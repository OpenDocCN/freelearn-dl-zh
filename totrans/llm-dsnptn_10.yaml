- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Checkpointing and Recovery
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查点和恢复
- en: '**Checkpointing** and **recovery** refer to the process of saving the state
    of a system, application, or model at specific intervals (checkpointing) and restoring
    it from a saved state in case of failure (recovery). In machine learning, checkpointing
    involves periodically saving model parameters, optimizer states, and training
    progress so that training can resume from the last checkpoint instead of starting
    over. This is especially useful for long-running tasks, where interruptions due
    to system crashes, power failures, or preempted cloud instances can otherwise
    result in significant losses.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**检查点**和**恢复**指的是在特定间隔保存系统、应用程序或模型状态的过程（检查点），以及在出现故障时从保存的状态中恢复（恢复）。在机器学习中，检查点包括定期保存模型参数、优化器状态和训练进度，以便可以从最后一个检查点恢复训练，而不是从头开始。这对于长时间运行的任务特别有用，否则由于系统崩溃、电源故障或抢占式云实例的中断可能会造成重大损失。'
- en: Checkpointing and recovery are crucial for ensuring **fault tolerance**, **efficiency**,
    and **reproducibility** in training large-scale models. Without checkpointing,
    an unexpected failure could waste hours or even days of computation. Additionally,
    it allows for **experiment reproducibility**, enabling researchers to revisit
    and fine-tune models from intermediate states, rather than redoing entire training
    runs. Efficient checkpointing strategies (e.g., saving at fixed intervals or when
    validation performance improves) help balance storage overhead while minimizing
    retraining costs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 检查点和恢复对于确保大规模模型训练的**容错性**、**效率**和**可重现性**至关重要。没有检查点，意外的故障可能会浪费数小时甚至数天的计算时间。此外，它还允许**实验可重现性**，使研究人员能够从中间状态重新访问和微调模型，而不是重新进行整个训练过程。高效的检查点策略（例如，在固定间隔或验证性能提高时保存）有助于平衡存储开销，同时最小化重新训练成本。
- en: In this chapter, we’ll explore strategies for determining optimal checkpoint
    frequency, efficient storage formats for large models, and techniques for recovering
    from various types of failures. You’ll also gain insights into checkpointing in
    distributed training scenarios and version control for model checkpoints.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨确定最佳检查点频率的策略、大型模型的高效存储格式以及从各种类型故障中恢复的技术。您还将深入了解分布式训练场景中的检查点以及模型检查点的版本控制。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Why is checkpointing important?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么检查点很重要？
- en: Checkpoint frequency and storage strategies
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查点频率和存储策略
- en: Efficient checkpoint formats
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效的检查点格式
- en: Recovering from failures
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从故障中恢复
- en: Checkpointing in distributed LLM training
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式 LLM 训练中的检查点
- en: Version control for LLM checkpoints
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 检查点的版本控制
- en: Automated checkpointing and recovery systems
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动检查点和恢复系统
- en: Why is checkpointing important?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么检查点很重要？
- en: Checkpointing is a common practice in LLM training due to the long duration
    and resource-intensive nature of the LLM training process.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 LLM 训练过程持续时间长且资源密集，检查点是一种常见的做法。
- en: 'Let’s implement a basic checkpointing system:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个基本的检查点系统：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This implementation demonstrates the basic structure of a checkpointing system.
    The `save_checkpoint` method saves the model state, optimizer state, and training
    progress information. The `load_checkpoint` method allows you to resume training
    from a saved checkpoint.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现展示了检查点系统的基本结构。`save_checkpoint` 方法保存模型状态、优化器状态和训练进度信息。`load_checkpoint`
    方法允许您从保存的检查点恢复训练。
- en: Checkpoint frequency and storage strategies
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查点频率和存储策略
- en: 'Determining the optimal checkpoint frequency involves striking a balance between
    *safety* and *efficiency*. Let’s explore different strategies and their implementation:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 确定最佳检查点频率需要在**安全性**和**效率**之间取得平衡。让我们探讨不同的策略及其实现：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This implementation introduces several checkpointing strategies:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现介绍了几种检查点策略：
- en: '**Regular checkpointing with a maximum number of checkpoints**: This prevents
    excessive disk usage by removing old checkpoints when the limit is reached'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大检查点数限制的常规检查点**: 当达到限制时，通过删除旧检查点来防止过多的磁盘使用'
- en: '**Time-based checkpointing**: This saves checkpoints at regular time intervals,
    which can be useful for long-running training processes'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于时间的检查点**: 这会在固定的时间间隔保存检查点，这对于长时间运行的训练过程非常有用'
- en: '**Best model checkpointing**: This saves the model with the best performance
    (lowest loss in this case), which is useful for model selection'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s a trade-off analysis of the three checkpointing strategies:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '**Regular checkpointing with a maximum number** **of checkpoints**:'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pros**: Prevents excessive storage usage and ensures periodic snapshots of
    training progress'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**: Might overwrite useful older checkpoints, potentially losing good
    models if performance fluctuates'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Best use case**: When storage is a constraint and periodic snapshots are
    needed for resumption'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time-based checkpointing**:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pros**: Ensures checkpoints are spaced out over time, which is useful for
    monitoring long training runs'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**: Can be inefficient if checkpoints are saved too frequently (wasting
    storage) or too infrequently (missing critical states)'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Best use case**: For long-running training processes where consistent snapshots
    are needed for debugging or rollback'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Best** **model checkpointing**:'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pros**: Retains the most promising model, which is useful for final model
    selection.'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**: If loss is noisy, a single “best” checkpoint may not be truly representative.
    Can fail to capture intermediate learning dynamics.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Best use case**: When selecting the most performant model is the priority
    over periodic snapshots.'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some factors to consider when selecting the strategy you wish to adopt:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '**Computational cost**: Frequent checkpointing increases disk I/O and CPU overhead'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Failure recovery**: Regular and time-based checkpointing help resume training
    after interruptions, whereas best-model checkpointing may not provide recent progress'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storage constraints**: Maintaining many checkpoints consumes storage; regular
    checkpointing with a limit is most efficient in managing this'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rate of model improvement**: If the model improves rapidly, frequent checkpoints
    may be useful; if the progress is slow, fewer but more strategic checkpoints may
    suffice'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The recommended approach for checkpointing LLMs is to combine strategies:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Use regular checkpointing (e.g., every few hours) to ensure progress is saved
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use best model checkpointing to retain the best-performing model
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a rolling window of recent checkpoints to balance storage efficiency and
    recovery options
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficient checkpoint formats
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For LLMs with billions of parameters, checkpoint size can become a significant
    concern. Let’s explore some strategies for efficient checkpoint storage:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary libraries and implement `EfficientLLMTrainer`:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This code defines an `EfficientLLMTrainer` class that extends `AdvancedLLMTrainer`
    (presumably a pre-existing class for training LLMs). The key function implemented
    is `save_checkpoint_efficient`, which efficiently saves model checkpoints in a
    compressed ZIP format.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s define the function (`load_checkpoint_efficient`) to load the checkpoint
    in ZIP format:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This function, `load_checkpoint_efficient`, is responsible for loading a previously
    saved checkpoint from a ZIP file and restoring the model and optimizer states.
    See the following example usage.
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Example usage:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This implementation uses ZIP compression to reduce the size of checkpoints.
    It also separates the model and optimizer state dictionaries from other metadata,
    allowing for more efficient storage and loading.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Other strategies for efficient checkpoint storage include the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '**Quantization**: Reducing the precision of model weights (e.g., from float32
    to float16) can significantly reduce the checkpoint size (see more about this
    strategy in [*Chapter 13*](B31249_13.xhtml#_idTextAnchor209))'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Incremental checkpointing**: Only save the changes since the last checkpoint,
    rather than the entire model state'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed storage**: In multi-GPU or multi-node setups, distribute the
    checkpoint across multiple storage devices'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud storage**: Use cloud storage solutions that offer fast I/O and automatic
    compression'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For very large models, you might also consider more advanced techniques, such
    as **model sharding**, where different parts of the model are saved separately
    and can be loaded on demand.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Recovering from failures
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Robust recovery mechanisms are crucial for LLM training. Let’s implement a
    system that can handle various types of failures:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `RobustLLMTrainer` class extends `EfficientLLMTrainer` to add resilience
    by handling interruptions (such as `SIGINT` for *Ctrl* + *C* and `SIGTERM` for
    termination) and saving checkpoints to prevent data loss. It initializes with
    a model, optimizer, checkpoint directory, and auto-save interval, then sets up
    signal handlers to trigger a graceful shutdown by saving progress before exiting.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: During training, it attempts to resume from the latest checkpoint if available.
    It loops through epochs and steps, running `train_fn` to compute loss and periodically
    saving checkpoints based on `autosave_interval`. If an exception occurs, it catches
    the error, saves the progress, and re-raises the exception to avoid silent failures.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: The `get_latest_checkpoint()` method retrieves the most recent checkpoint by
    sorting files in the checkpoint directory (though `os` is missing and should be
    imported). The script concludes with an example usage where a dummy loss function
    is defined, and training is started with `trainer.train(epochs=10,` `steps_per_epoch=1000,
    train_fn=train_step)`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'This implementation includes several robustness features:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '**Signal handling**: The trainer catches interrupt signals (*Ctrl* + *C*) and
    gracefully saves a checkpoint before exiting'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automatic resumption**: The trainer automatically finds and loads the latest
    checkpoint when starting training'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regular auto-saves**: Checkpoints are saved at regular intervals during training'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exception handling**: If an error occurs during training, a checkpoint is
    saved before the exception is re-raised'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These features help recover from various types of failures:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这些功能有助于从各种类型的故障中恢复：
- en: '**System crashes or power outages**: Regular auto-saves ensure that not too
    much progress is lost'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**系统崩溃或断电**：定期自动保存确保不会丢失太多进度'
- en: '**User interruptions**: Signal handling allows for graceful exits with the
    state saved'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户中断**：信号处理允许优雅地退出并保存状态'
- en: '**Code errors**: Exception handling ensures that progress is saved even if
    an unexpected error occurs'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码错误**：异常处理确保即使在发生意外错误的情况下也能保存进度'
- en: 'For even more robust recovery, consider implementing the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现更健壮的恢复，考虑实施以下措施：
- en: '**Checkpoint validation**: Verify the integrity of checkpoints before loading
    them'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检查点验证**：在加载检查点之前验证其完整性'
- en: '**Multiple backup checkpoints**: Keep several recent checkpoints in case the
    latest one is corrupted'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多个备份检查点**：保留几个最近的检查点以防最新的一个被损坏'
- en: '**Distributed checkpointing**: In multi-node setups, ensure that checkpoints
    are consistent across all nodes'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式检查点**：在多节点设置中，确保所有节点上的检查点一致性'
- en: Checkpointing in distributed LLM training
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式LLM训练中的检查点
- en: Distributed training introduces additional complexity to checkpointing.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练给检查点引入了额外的复杂性。
- en: 'Let’s break down the implementation of a basic distributed checkpointing system
    and understand each component:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解基本分布式检查点系统的实现，并理解每个组件：
- en: 'We first define the `DistributedLLMTrainer` class, which inherits from `RobustLLMTrainer`.
    The `DistributedLLMTrainer` class is designed for the distributed training of
    LLMs using PyTorch’s `torch.distributed` framework. It ensures that the model
    is trained across multiple devices (e.g., GPUs) or nodes efficiently:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先定义继承自 `RobustLLMTrainer` 的 `DistributedLLMTrainer` 类。`DistributedLLMTrainer`
    类是为使用PyTorch的 `torch.distributed` 框架进行LLM的分布式训练而设计的。它确保模型在多个设备（例如，GPU）或节点上高效训练：
- en: '[PRE6]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The initialization does the following:'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 初始化执行以下操作：
- en: Calls the parent class initializer.
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用父类初始化器。
- en: 'Sets up distributed training attributes:'
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置分布式训练属性：
- en: '`self.rank`: Identifies the current process'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`self.rank`：标识当前进程'
- en: '`self.world_size`: Indicates the total number of processes'
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`self.world_size`：指示进程总数'
- en: 'We then use the following methods to save and load checkpoints during distributed
    training:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们随后使用以下方法在分布式训练期间保存和加载检查点：
- en: '[PRE7]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following methods handle distributed checkpointing:'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下方法处理分布式检查点：
- en: '`save_checkpoint_distributed`: Only the main process (rank `0`) saves the checkpoint
    to avoid redundant writes, reduce disk I/O, and ensure consistency across processes.
    If all ranks are saved independently, it could lead to storage inefficiencies
    and potential race conditions. After saving, `dist.barrier()` synchronizes all
    processes to ensure they wait for the checkpoint to be written. When loading,
    only rank `0` reads the checkpoint to prevent redundant disk access; then, it
    broadcasts the loaded values to all other ranks using `dist.broadcast()`, ensuring
    every process starts from the same state before resuming training.'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_checkpoint_distributed`：只有主进程（rank `0`）保存检查点以避免冗余写入，减少磁盘I/O，并确保进程间的一致性。如果所有rank独立保存，可能会导致存储效率低下和潜在的竞争条件。保存后，`dist.barrier()`
    同步所有进程以确保它们等待检查点写入。在加载时，只有rank `0` 读取检查点以防止冗余磁盘访问；然后，使用 `dist.broadcast()` 将加载的值广播到所有其他rank，确保每个进程在继续训练前从相同的状态开始。'
- en: '`load_checkpoint_distributed`:'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_checkpoint_distributed`：'
- en: Only the main process loads the checkpoint
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有主进程加载检查点
- en: It broadcasts the loaded values to all other processes
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将加载的值广播到所有其他进程
- en: It ensures all processes have the same checkpoint data
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保所有进程具有相同的检查点数据
- en: 'Next, we implement distributed training:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们实现分布式训练：
- en: '[PRE8]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `train_distributed` method does the following:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`train_distributed` 方法执行以下操作：'
- en: Determines the starting point (epoch and step)
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定起始点（epoch和step）
- en: Broadcasts this information to all processes
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此信息广播到所有进程
- en: Runs the training loop with periodic checkpointing
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 带有定期检查点的运行训练循环
- en: Handles exceptions by saving a final checkpoint and cleaning up
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过保存最终检查点和清理来处理异常
- en: 'We then employ the following code to initialize distributed training with PyTorch,
    set up your model for parallel execution, and perform a simple distributed training
    loop:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们随后使用以下代码初始化分布式训练，设置模型以进行并行执行，并执行简单的分布式训练循环：
- en: '[PRE9]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This code includes the following:'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此代码包括以下内容：
- en: '`init_distributed`: Initializes the distributed training environment'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`distributed_train_step`: A dummy training function for demonstration'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`main`: Shows how to use `DistributedLLMTrainer` in practice'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Key considerations for distributed checkpointing include maintaining consistency
    by synchronizing all processes using barriers during checkpointing, ensuring that
    only the main process handles I/O operations to avoid conflicts, and effectively
    sharing important data by broadcasting it from the main process to the other processes.
    Additionally, the system incorporates robust error handling, allowing it to gracefully
    save checkpoints and clean up distributed resources in case of failures.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Next, let us focus on the version control aspect.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Version control for LLM checkpoints
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Version control for LLM checkpoints can help with managing different versions
    of your model during the development process. Here’s a simple implementation:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This implementation provides basic version control features:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '**Version tracking**: Each saved checkpoint can be associated with a version
    name'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Branching**: You can create new branches from existing checkpoints, allowing
    for experimentation'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Version history**: The version information is stored in a JSON file for easy
    inspection and management'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The key benefits of version control for LLM checkpoints are as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '**Experimentation**: You can easily try different training strategies or hyperparameters
    from a common starting point'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collaboration**: Team members can share and work on different versions of
    the model'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reproducibility**: Specific versions of the model can be referenced and recreated'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated checkpointing and recovery systems
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To make the checkpointing and recovery process more robust and hands-off, we
    can implement an automated system:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the required modules:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We imported two key modules here:'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`threading`: Enables the creation of threads for running tasks (such as auto-save
    and health checks) concurrently with the main training process'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`time`: Used to manage intervals between auto-saves and health checks, as well
    as timestamping saved checkpoints'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we define and initialize the class:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `AutomatedLLMTrainer` class inherits from a base class, `VersionControlledLLMTrainer`,
    which handles basic checkpointing logic. This class introduces automation for
    checkpointing and system health monitoring.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following is a list of parameters that manage auto-saving, system health
    checks, and training execution control:'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`autosave_interval`: The time (in seconds) between auto-save checkpoints.'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`health_check_interval`: The time between system health checks.'
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training_active`: A flag to indicate whether the training is ongoing. It is
    used to control the threads’ execution.'
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The constructor calls `super().__init__()` to inherit functionality from the
    parent class and sets up intervals for auto-saving and health checks.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Auto-save the thread for checkpointing:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This method starts a separate thread that periodically saves a checkpoint during
    training.
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此方法在训练期间启动一个单独的线程，定期保存检查点。
- en: 'The following is a list of components responsible for handling periodic auto-saving
    during training:'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是在训练期间处理周期性自动保存的组件列表：
- en: '`autosave_loop`: A function that continuously runs while `training_active`
    is `True`. Every `autosave_interval` seconds, it calls the `save_checkpoint_versioned()`
    method to save the current state.'
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`autosave_loop`：一个在`training_active`为`True`时持续运行的函数。每`autosave_interval`秒，它调用`save_checkpoint_versioned()`方法来保存当前状态。'
- en: '`threading.Thread`: The thread runs `autosave_loop` in the background, ensuring
    that auto-saves happen concurrently with the training process.'
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threading.Thread`：该线程在后台运行`autosave_loop`，确保自动保存与训练过程同时发生。'
- en: 'Next, we implement a health check thread. This method starts a health check
    thread that monitors system performance at regular intervals:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们实现一个健康检查线程。此方法启动一个健康检查线程，定期监控系统性能：
- en: '[PRE14]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here are the main elements of the preceding snippet:'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是前面代码片段的主要元素：
- en: '`health_check_loop`: A function that continuously runs during training. Every
    `health_check_interval` seconds, it checks the system health by calling `check_system_health()`.
    If a problem is detected, it triggers the recovery process.'
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`health_check_loop`：一个在训练期间持续运行的函数。每`health_check_interval`秒，它通过调用`check_system_health()`来检查系统健康。如果检测到问题，它将触发恢复过程。'
- en: '`check_system_health()`: This method needs to be defined to check the system’s
    performance metrics (e.g., GPU memory or CPU usage). If the health check fails,
    it calls `initiate_recovery()`.'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`check_system_health()`：此方法需要定义以检查系统的性能指标（例如，GPU内存或CPU使用率）。如果健康检查失败，它调用`initiate_recovery()`。'
- en: 'We perform system health check and recovery. The following placeholder method
    is where the system health checks will be implemented, for example, checking GPU
    memory, CPU utilization, disk space, or any other resource critical to the training
    process. It returns `True` if everything is fine, and `False` if there’s a problem:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们执行系统健康检查和恢复。以下占位符方法是系统健康检查将被实现的地方，例如，检查GPU内存、CPU利用率、磁盘空间或任何对训练过程至关重要的资源。如果没有问题，它返回`True`，如果有问题，则返回`False`：
- en: '[PRE15]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following method will contain logic for what to do if the system health
    check fails. It could, for instance, reload the last checkpoint, reduce the batch
    size, or take other corrective actions depending on the issue detected:'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下方法将包含在系统健康检查失败时应该执行什么逻辑。例如，它可以重新加载最后一个检查点，减少批量大小，或根据检测到的问题采取其他纠正措施：
- en: '[PRE16]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, we conduct automated training with checkpointing and health checks.
    This method manages the overall training process with automation. It activates
    the auto-save and health check threads and initiates distributed training via
    the parent class’s `train_distributed()` method:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们进行带有检查点和健康检查的自动训练。此方法通过自动化管理整个训练过程。它激活自动保存和健康检查线程，并通过父类的`train_distributed()`方法启动分布式训练：
- en: '[PRE17]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here’s a breakdown of the main code elements:'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面是主要代码元素的分解：
- en: '`self.training_active`: Set to `True` to indicate that training is running'
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.training_active`：设置为`True`以指示训练正在进行'
- en: '`try-finally block`: Ensures that no matter how the training ends (whether
    it completes or crashes), the `training_active` flag is set to `False` and both
    threads are properly terminated'
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`try-finally block`：确保无论训练如何结束（无论是完成还是崩溃），`training_active`标志都设置为`False`，并且两个线程都得到适当终止'
- en: This approach reduces manual intervention, enhances reliability, and offers
    flexibility in defining recovery logic based on the specific training needs.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法减少了手动干预，增强了可靠性，并提供了根据特定训练需求定义恢复逻辑的灵活性。
- en: Summary
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Implementing robust checkpointing and recovery systems is common practice for
    successful LLM training. By incorporating these techniques, you can ensure that
    your long-running training processes are resilient to failures, easily manageable,
    and conducive to experimentation and collaboration.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 实施健壮的检查点和恢复系统是成功进行LLM训练的常见做法。通过采用这些技术，你可以确保你的长时间运行训练过程能够抵御故障，易于管理，并有利于实验和协作。
- en: 'To expand our discussion, *Table 10.1* lists checkpointing strategies, trade-offs,
    and use cases:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了扩展我们的讨论，*表10.1*列出了检查点策略、权衡和使用案例：
- en: '| **Checkpointing** **Strategy** | **Description** | **Trade-Offs** | **Use
    Cases** |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| **检查点** **策略** | **描述** | **权衡** | **使用案例** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Regular (with max limit) | Saves at intervals (steps/epochs); keeps a maximum
    number. | Pros: Saves storage; periodic snapshots.Cons: Might overwrite good checkpoints.
    | Iterative model development; monitoring training progress; preventing complete
    data loss during long training runs. |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 定期（带最大限制） | 在间隔（步骤/周期）保存；保持最大数量。 | 优点：节省存储；定期快照。缺点：可能会覆盖好的检查点。 | 迭代模型开发；监控训练进度；防止长时间训练运行中数据完全丢失。
    |'
- en: '| Time-based | Saves at specified intervals (e.g., every 30 minutes). | Pros:
    Time-spaced snapshots.Cons: Inefficient if the interval is too short/long. | Long-running
    experiments where consistent, time-stamped checkpoints are crucial for debugging
    and analysis; ensuring recoverability in case of system failures. |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 基于时间 | 在指定间隔（例如，每30分钟）保存。 | 优点：时间间隔快照。缺点：如果间隔太短/长则效率低下。 | 长时间运行的实验，一致的、时间戳标记的检查点对于调试和分析至关重要；确保系统故障时的可恢复性。
    |'
- en: '| Best model | Saves only when the model achieves the best performance. | Pros:
    Retains the best model.Cons: May not be representative if loss is noisy; no intermediate
    snapshots. | Selecting the most performant model. |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 最佳模型 | 只有当模型达到最佳性能时才保存。 | 优点：保留最佳模型。缺点：如果损失值有噪声，可能不具有代表性；没有中间快照。 | 选择性能最佳模型。
    |'
- en: '| Efficient (compression) | Uses compression (e.g., ZIP) to reduce size. |
    Storage-constrained environments; handling large models where storage is a primary
    concern; archiving models for long-term storage. | Storage-constrained environments;
    archiving models for long-term storage. |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 高效（压缩） | 使用压缩（例如，ZIP）来减小大小。 | 适用于存储受限的环境；处理存储是主要关注点的大模型；长期存储模型存档。 | 存储受限环境；长期存储模型存档。
    |'
- en: '| Efficient (quantization) | Reduces precision of weights (e.g., float32 to
    float16). | Pros: Reduces size.Cons: Potential accuracy loss. | Deploying models
    on resource-limited devices; reducing checkpoint size for faster transfer and
    storage; accelerating model loading. |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 高效（量化） | 减少权重的精度（例如，从float32到float16）。 | 优点：减小大小。缺点：可能损失精度。 | 在资源受限的设备上部署模型；减小检查点大小以实现更快的传输和存储；加速模型加载。
    |'
- en: '| Efficient (incremental) | Saves only changes since the last checkpoint. |
    Pros: Can significantly reduce size.Cons: Complex; potentially fragile. | Training
    models with gradual parameter updates; large models where frequent full checkpoints
    are impractical; continuous learning scenarios. |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 高效（增量） | 只保存自上次检查点以来的更改。 | 优点：可以显著减小大小。缺点：复杂；可能脆弱。 | 使用逐步参数更新的模型训练；大模型，频繁完整检查点不切实际；连续学习场景。
    |'
- en: '| Distributed | In distributed training, only the main process (rank 0) saves;
    data is broadcast to others. | Pros: Avoids redundant writes; ensures consistency.Cons:
    Requires coordination. | Large-scale distributed training jobs; ensuring consistent
    model states across multiple workers; minimizing network overhead. |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 分布式 | 在分布式训练中，只有主进程（rank 0）保存；数据广播给其他人。 | 优点：避免冗余写入；确保一致性。缺点：需要协调。 | 大规模分布式训练作业；确保多个工作者之间模型状态的一致性；最小化网络开销。
    |'
- en: '| Version-controlled | Associates checkpoints with versions; supports branching.
    | Pros: Experimentation; reproducibility; rollback.Cons: Adds complexity. | Collaborative
    model development; tracking experimental variations; ensuring reproducibility
    for scientific research; managing model evolution. |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 版本控制 | 将检查点与版本关联；支持分支。 | 优点：实验；可重复性；回滚。缺点：增加复杂性。 | 协作模型开发；跟踪实验变化；确保科学研究的可重复性；管理模型演变。
    |'
- en: '| Automated (with health checks) | Auto-saves checkpoints; performs health
    checks; can initiate recovery. | Pros: Reduces manual work; enhances reliability.Cons:
    Requires health check/recovery implementation. | Mission-critical training jobs;
    automated recovery from failures; long-running experiments requiring high reliability.
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 自动化（带健康检查） | 自动保存检查点；执行健康检查；可以启动恢复。 | 优点：减少手动工作；增强可靠性。缺点：需要健康检查/恢复实现。 | 适用于关键任务训练作业；从故障中自动恢复；需要高可靠性的长时间运行实验。
    |'
- en: Table 10.1 – Checkpointing strategies, trade-offs, and use cases
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.1 – 检查点策略、权衡和使用案例
- en: In the next chapter, we’ll explore effective techniques for adapting pre-trained
    language models to specific tasks or domains.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨将预训练语言模型适应特定任务或领域的有效技术。
