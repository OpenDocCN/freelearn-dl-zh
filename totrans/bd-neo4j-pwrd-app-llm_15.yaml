- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying Your Application on the Google Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have come a long way in designing and developing your GenAI application.
    Now, it is time to take that next crucial step—deployment. While a true production-grade
    deployment involves various complexities such as CI/CD pipelines, scalability
    considerations, observability, cost optimization, and security hardening, this
    chapter is designed to give you a foundational, hands-on introduction to cloud
    deployment using Google Cloud Run. **Cloud Run** by Google Cloud provides a powerful
    yet developer-friendly way to deploy containerized applications without managing
    infrastructure, making it ideal for rapid prototyping and small-scale production
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: The deployment steps and services may vary slightly across other cloud platforms,
    such as AWS or Microsoft Azure, but we will focus on Google Cloud for brevity.
    However, once you are comfortable with the core concepts here, you are encouraged
    to experiment with similar workflows on other providers to broaden your cloud
    deployment expertise.
  prefs: []
  type: TYPE_NORMAL
- en: We will walk through the process of deploying the Haystack chatbot you built
    in [*Chapter 5*](Chapter_03.xhtml#_idTextAnchor021) as a serverless application
    on Google Cloud. The process of deploying the intelligent recommendation system
    using Spring AI is mentioned later in the chapter with the links to the resources
    to follow the steps. By the end of this chapter, you will have a working chatbot
    live on the cloud and the confidence to build on this foundation for more advanced
    deployments.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing your search chatbot using Haystack for deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containerizing the application with Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a Google Cloud project and services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying to Google Cloud Run
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing and verifying the deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To deploy your Haystack chatbot using Google Cloud Run, you need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An active Google Cloud account with billing enabled. If you are new to Google
    Cloud, you can start by creating an account at [ https://console.cloud.google.com/](https://console.cloud.google.com/)
    and take advantage of the free tier and credits offered for new users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Access to a Neo4j database:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you are using a local Neo4j instance, you must expose it publicly using
    `ngrok` or a similar tool, so that the deployed chatbot can connect to it. Here
    is an example to expose Neo4j’s `bolt` port:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Update your `.env` with the `ngrok` public URL:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: If you are using AuraDB Free, the preceding step can be ignored.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing your Haystack chatbot for deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be deploying our application on Google Cloud. The approach we will be
    talking about is similar from a deployment perspective on all the popular cloud
    environments. We will be looking at building a `docker compose` and running it
    in the cloud. Google Cloud was chosen because it is convenient rather than having
    any technical advantage as such. Once we deploy and run the application on Google
    Cloud, the official documentation links for other clouds to deploy the same `docker
    compose` will be provided. Just repeating those steps in the book will not make
    much of a difference.
  prefs: []
  type: TYPE_NORMAL
- en: Before jumping into containerization and deployment, it is important to ensure
    that your Haystack chatbot code is organized in a way that is compatible with
    serverless deployment. In this section, you will structure your code base appropriately
    and prepare the essential configuration files needed for deploying to Google Cloud
    Run.
  prefs: []
  type: TYPE_NORMAL
- en: We will be reusing the working search chatbot from [*Chapter 5*](Chapter_03.xhtml#_idTextAnchor021)
    by creating a copy of the main script (`search_chatbot.py`), renaming it `app.py`
    (as this is the default entry point many cloud services expect when serving a
    Python web application), and placing it in a simplified folder ready for containerization.
    Since the chatbot logic is already functional, we will skip local testing and
    move directly to packaging and deploying it to the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: These steps to help you containerize and deploy your Haystack chatbot to Google
    Cloud Run are also presented in the `README.md` file in the book’s GitHub repo
    at [https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch12](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch12).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, prepare a `requirements.txt` file that lists all the necessary Python
    dependencies your chatbot needs to run. This file allows the container to install
    the required packages during the build process. The content of this file will
    look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To manage sensitive credentials and environment-specific configurations securely,
    it is recommended to use a `.env` file. In the `ch12` directory of the GitHub
    repo, you will find a file named `example.env` that serves as a template. This
    file includes placeholders for key variables such as your OpenAI API key and Neo4j
    database credentials. To use it, simply create a copy of this file, rename it
    `.env`, and populate it with your actual values. The application leverages the
    `python-dotenv` library to load these variables at runtime, keeping secrets out
    of your code base while still making them accessible to your application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, you have set up the core components needed for deployment—your
    application script, dependencies, and environment variables. To ensure everything
    is organized correctly for containerization and deployment, your project directory
    should now follow this structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `example.env` file acts as a reference for users to create their own .`env`
    file with valid credentials and configuration values. With all the core components
    now in place—including your application script, dependencies, and environment
    setup—you are ready to containerize your application for deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Let us move on to the next step, which is to containerize your application with
    Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Containerizing the application with Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before deploying your Haystack chatbot to Google Cloud Run, the application
    must be packaged into a Docker container. **Containerization** allows you to bundle
    your code, dependencies, and environment into a single, portable unit that runs
    consistently across different systems—including the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you will create a Dockerfile, which defines the steps required
    to build a Docker image of your chatbot. This image will then be deployed to Cloud
    Run as a serverless web service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the Dockerfile used to containerize your Haystack chatbot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us break down what each line does:'
  prefs: []
  type: TYPE_NORMAL
- en: '`FROM python:3.11`: This sets the base image to Python 3.11, which includes
    everything needed to run Python applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EXPOSE 8080`: Cloud Run expects the application to listen on port `8080`.
    This line documents the port that the container will expose at runtime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WORKDIR /app`: This sets the working directory inside the container to `/app`.
    All subsequent commands will run from this directory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`COPY . ./`: This copies the entire contents of your local project directory
    into the container’s `/app` directory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RUN pip install -r requirements.txt`: This installs all Python dependencies
    listed in your `requirements.txt` file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CMD ["python", "app.py"]`: This specifies the command to run when the container
    starts—in this case, it runs your chatbot application using `app.py`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once your Dockerfile is in place, you now have a fully containerized version
    of your Haystack chatbot, ready to be deployed to the cloud. The next step is
    to configure your Google Cloud environment so that you can push your container
    and run it using Cloud Run.
  prefs: []
  type: TYPE_NORMAL
- en: Let us move on to setting up your Google Cloud project and services.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Google Cloud project and services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google Cloud provides a robust, developer-friendly platform for deploying modern
    applications, including GenAI-powered solutions. With tools such as Cloud Run,
    Artifact Registry, and Cloud Build, Google Cloud enables you to go from code to
    scalable, serverless deployment with minimal operational overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Although your Haystack chatbot uses OpenAI for language processing, Google Cloud
    plays a critical role in hosting the application, managing container builds, and
    securely storing your Docker images. In this section, you will configure your
    Google Cloud project, enable only the necessary services (such as Cloud Run, Cloud
    Build, and Artifact Registry), and prepare your environment for deployment.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this section, your project will be cloud-ready, with all the services
    and permissions in place to deploy your chatbot using Google Cloud’s serverless
    infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Let us get started by setting up your project and enabling the required APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the Google Cloud console ([https://console.cloud.google.com/](https://console.cloud.google.com/)),
    on the project selector page, select or create a Google Cloud project ([https://cloud.google.com/resource-manager/docs/creating-managing-projects](https://cloud.google.com/resource-manager/docs/creating-managing-projects)).
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that billing is enabled for your Cloud project. Learn how to check
    whether billing is enabled on a project at [https://cloud.google.com/billing/docs/how-to/verify-billing-enabled](https://cloud.google.com/billing/docs/how-to/verify-billing-enabled).
  prefs: []
  type: TYPE_NORMAL
- en: Launching Google Cloud Shell
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To simplify the setup and avoid installing any tools locally, we will use Google
    Cloud Shell, which comes pre-installed with Docker, the `gcloud` CLI, and Git.
    Here is how to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the Google Cloud console ([https://console.cloud.google.com/](https://console.cloud.google.com/)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the Cloud Shell icon in the top-right corner of the navigation bar (the
    terminal icon).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A terminal window will open at the bottom of your screen. This is a fully functional
    shell with access to your Google Cloud project and services.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Shell provisions a temporary VM with 5 GB of persistent storage—more than
    enough for this walkthrough.
  prefs: []
  type: TYPE_NORMAL
- en: Setting your active project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Make sure you are working on the right Google Cloud project. You can either
    create a new one or use an existing one. Set it using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You can verify the active project with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Enabling the required services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, enable the Google Cloud services required for deploying your container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'On successful execution of the command, you should see a message similar to
    the one shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The alternative to the preceding `gcloud` command is through the console by
    searching for each product. If any API is missed, you can enable it during the
    implementation. Refer to the documentation for `gcloud` commands and usage: [https://cloud.google.com/sdk/gcloud/reference/config/list](https://cloud.google.com/sdk/gcloud/reference/config/list).'
  prefs: []
  type: TYPE_NORMAL
- en: Adding your project files to Cloud Shell
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before continuing with the deployment steps, make sure your Haystack chatbot
    files are available in your Google Cloud Shell environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'You have two options to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Upload your existing files**: If you have been developing the project locally
    (e.g., as part of earlier chapters), you can upload your working directory to
    Cloud Shell using the **Upload** option in the Cloud Shell editor. Just click
    the **Open Editor** button (pencil icon), then use **File | Upload Files** or
    drag and drop your folder directly into the editor.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Clone from GitHub (recommended for clean setup)**: Alternatively, you can
    clone the *Chapter 12* code directly from the official book repository using the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once you are inside the `ch12` folder, you will find all the necessary files—`app.py`,
    `requirements.txt`, `Dockerfile`, and `example.env`.
  prefs: []
  type: TYPE_NORMAL
- en: If you decide to clone the repo, make sure to follow the step mentioned earlier
    to generate the `.env` file. Now that your project files are in place and ready
    inside Cloud Shell, it is time to move on to the final stage—deploying your Haystack
    chatbot to Google Cloud Run.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying to Google Cloud Run
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will walk through the full deployment workflow—from setting
    environment variables and configuring Artifact Registry to building your container
    and deploying it live using Cloud Run. Let us break it down step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up environment variables. Begin by exporting the key environment variables
    for your Google Cloud project and deployment region. Replace the placeholders
    with your actual values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an Artifact Registry instance and build the container. Configure your
    Artifact Registry repository and build your container image using Cloud Build:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the Docker repository:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Authenticate Docker with Artifact Registry:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, build and push your container image:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: This command uses your Dockerfile to package the app and pushes the resulting
    container image to Artifact Registry.
  prefs: []
  type: TYPE_NORMAL
- en: Deploy to Cloud Run. Before deploying, make sure your `.env` file contains all
    the required environment variables, such as `OPENAI_API_KEY`, `NEO4J_URI`, and
    any project-specific configuration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To pass these variables during deployment, convert your `.env` file into a
    format compatible with the `--set-env-vars` flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, deploy your application to Cloud Run:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once complete, Google Cloud Run will return a URL where your chatbot is live
    and accessible via the web.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations—your Haystack chatbot is now successfully deployed and running
    as a serverless application on Google Cloud!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us move on to the final step: testing and verifying the deployment to ensure
    everything works as expected.'
  prefs: []
  type: TYPE_NORMAL
- en: Testing and verifying the deployment on Google Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once your deployment is complete, Google Cloud Run will return a public service
    URL, typically in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Open this URL in your browser. You should see your Gradio-powered chatbot interface
    live on the web—identical in functionality to your local version. You can now
    interact with the chatbot, submit queries, and receive movie recommendations just
    as before, but this time, it is running fully in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'If something does not work as expected, keep the following checklist ready
    for troubleshooting:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dependency check**: Make sure your Dockerfile correctly installs all dependencies
    using `pip install -r requirements.txt`. Missing dependencies can result in build
    or runtime errors on Cloud Run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud Shell versus local environment**: If you are not using Google Cloud
    Shell, ensure your local environment is authenticated with Google Cloud using
    a service account that has appropriate permissions for Cloud Run, Artifact Registry,
    and (if applicable) Vertex AI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitor logs and metrics**: You can monitor your service’s logs, request
    history, and performance metrics directly from the Google Cloud console under
    **Cloud Run**. This is especially useful for debugging and performance tuning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud Run service management**: Navigate to **Cloud Run** in the Cloud console,
    where you will see a list of deployed services. Your chatbot (e.g., `movies-chatbot`)
    should appear here. Clicking on the service name will give you access to the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The public service URL
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment history
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Container configuration
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Environment variables
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Logs and error reports
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This visibility makes it easy to track and manage your application post-deployment.
  prefs: []
  type: TYPE_NORMAL
- en: With your chatbot now live, deployed on a scalable serverless platform, and
    publicly accessible, you have successfully completed the deployment journey. Your
    GenAI-powered movie recommendation chatbot is now ready to be used, shared, and
    further enhanced.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the chatbot to other clouds
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned in the initial section, once you have `docker compose` prepared,
    you can follow the instructions provided to deploy the same application to other
    clouds:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Azure deployment**: Please follow the instructions provided at this link:  [https://techcommunity.microsoft.com/blog/appsonazureblog/how-to-deploy-a-local-docker-container-to-azure-container-apps/3583888](https://techcommunity.microsoft.com/blog/appsonazureblog/how-to-deploy-a-local-docker-container-to-azure-container-apps/3583888).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS deployment**: Docker provides a clear guide on deploying `docker compose`
    files to AWS. Please follow this link for more details: [https://www.docker.com/blog/docker-compose-from-local-to-amazon-ecs/](https://www.docker.com/blog/docker-compose-from-local-to-amazon-ecs/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you follow the instructions in these links, you can see the similarity
    with the deployment approach when we chose to containerize the applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a lot of information available in each cloud’s official documentation
    to deploy Spring Boot applications to the cloud. For example, if you are looking
    to run the application we created in *Chapters 9* and *10*, you can follow the
    steps in the documentation provided by various cloud vendors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Google Cloud**: This article ([https://cloud.google.com/run/docs/quickstarts/build-and-deploy/deploy-java-service](https://cloud.google.com/run/docs/quickstarts/build-and-deploy/deploy-java-service))
    lists detailed steps to deploy on Google Cloud Run'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure**: This article ([https://learn.microsoft.com/en-us/azure/spring-apps/basic-standard/how-to-maven-deploy-apps](https://learn.microsoft.com/en-us/azure/spring-apps/basic-standard/how-to-maven-deploy-apps))
    shows how to deploy the Spring Boot application on Azure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS**: For Azure here are some articles for your reference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This article ([https://community.aws/content/2qk6oFuOPiA4G0N83ocU0REbtdN/step-by-step-guide-to-deploying-a-spring-boot-application-on-aws-ec2-with-best-practices](https://community.aws/content/2qk6oFuOPiA4G0N83ocU0REbtdN/step-by-step-guide-to-deploying-a-spring-boot-application-on-aws-ec2-with-best-practices))
    shows us how to deploy the Spring Boot application on AWS EC2
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This article ([https://www.geeksforgeeks.org/deploy-a-spring-boot-application-with-aws/](https://www.geeksforgeeks.org/deploy-a-spring-boot-application-with-aws/))
    shows how to deploy the Spring Boot application using AWS Elastic Beanstalk
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: These articles are good enough to guide you through deploying our GenAI application
    as, at its heart, it is a simple application that does not need scaling as such
    since it only performs the augmentation—that, too, as a batch process.
  prefs: []
  type: TYPE_NORMAL
- en: The production deployment of these applications is a more complex procedure
    that requires you to focus on various supporting elements, such as database, monitoring,
    and so on. Discussing the whole deployment is out of the scope of this book, but
    we will highlight the deployment architecture and key considerations to deploy
    your applications in production in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Preparing for deployment in production: key considerations'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at a typical architecture deployment for intelligent
    applications. There are a lot of other aspects we need to keep in mind when we
    are moving to production. For simplicity, we will refer to the augmentation application
    we built in *Chapters 9* and *10*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us look at all the tasks we did from loading data to reviewing the results:'
  prefs: []
  type: TYPE_NORMAL
- en: We loaded the data into a graph.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The graph was enhanced with seasonal relationships. We augmented the graph using
    the augmentation application—articles as well as customer behavior aspects.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We also utilized KNN similarity and community detection algorithms to enhance
    the graph and reviewed how this approach gives us better results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In a production deployment, all of these aspects may need to be automated and
    deployed as individual applications. Let us take a brief look at all of these
    aspects that we need to take care of.
  prefs: []
  type: TYPE_NORMAL
- en: When we are deploying intelligent applications to production, we need to make
    sure we take care of data ingestion, data consumption, the LLM or ML pipeline
    to augment the graph, and the graph database deployment architecture for scale.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first take a look at the deployment architecture in *Figure 12.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 — Deployment architecture of Neo4j intelligent applications](img/B31107_12_01-01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 — Deployment architecture of Neo4j intelligent applications
  prefs: []
  type: TYPE_NORMAL
- en: We can see there are two different Neo4j databases shown here. In Neo4j for
    regular interaction, we will have `Primary`, which can perform both `READ` and
    `WRITE` capabilities. For analytical, **Graph Data Science** (**GDS**), and other
    uses, we will use `Secondary`, which provides only the `READ` capability. We can
    have more than one `Primary` database to provide high availability and more than
    one `Secondary` database to provide horizontal scalability. We can see from the
    diagram that all the regular interactions, including data ingestion and data consumption,
    are handled by `Primary` and the analytical workload that augments the graph is
    handled by `Secondary`.
  prefs: []
  type: TYPE_NORMAL
- en: This type of deployment architecture also makes it easy to maintain and monitor
    the system. The Neo4j database comes with **Neo4j Ops Manager** ([https://neo4j.com/docs/ops-manager/current/](https://neo4j.com/docs/ops-manager/current/))
    for deploying and monitoring Neo4j servers. It provides dashboards to monitor
    the current health of the system as well as to set up alerts to be notified in
    case of errors.
  prefs: []
  type: TYPE_NORMAL
- en: For the other applications, we need to have similar monitoring, especially for
    data ingestion and augmentation applications. When they fail in the middle, we
    should be able to restart from where we have failed. The augmentation application
    is built to handle the data that way.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we are building the ingestion pipeline, we need to keep in mind these
    aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: What is our initial data size?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are our day-to-day changes (incremental data changes) and their size?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does incremental data come? Is it coming in near-real time, as a batch at
    regular intervals, a big batch at the end of the day, or interactive changes made
    by end users via the UI?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will look at best practices for these scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Initial data load
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we are migrating from another data source or database, we might have to move
    the data into Neo4j for the first time. Depending on this data size, we must decide
    whether we can take a transactional approach to load the data or leverage the
    offline data import approach called `neo4j-admin` import.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 9*](Chapter_09.xhtml#_idTextAnchor059), we used a transactional
    approach to load the data. If the amount of data we are loading is under a few
    million records, say 100 million, we can load this data in a reasonable amount
    of time. When we load the data transactionally, the Neo4j database needs to update
    the indexes and keep transaction logs apart, committing the data to the database.
    This adds a good amount of overhead to the process. But this approach gives us
    more flexibility and reusable code to use with incremental data loading. If we
    are loading data into a cluster, we can use this approach to make sure the data
    is available across the cluster, as the Neo4j database server makes sure the changes
    are replicated across the cluster. We have used the **LOAD CSV** option to load
    the data. You can read more about this at [https://neo4j.com/docs/cypher-manual/current/clauses/load-csv/](https://neo4j.com/docs/cypher-manual/current/clauses/load-csv/).
  prefs: []
  type: TYPE_NORMAL
- en: This approach is fine for proofs of concept and ad hoc data load purposes, but
    for production systems, the data ingestion should be performed using a client
    by connecting to the database using the Neo4j protocol. While the **LOAD CSV**
    option is simple and attractive, it uses up the database heap to load the data
    and perform the data ingestion, which might not be desirable. A basic Python client
    application that can ingest data into a graph can be found at [https://github.com/neo4j-field/pyingest](https://github.com/neo4j-field/pyingest).
    Note that this is a sample client, and you need to build one that suits your production
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: If the data sizes are bigger, then using the **Neo4j Admin Import** process
    would be best. For this purpose, we need to prepare CSV files for nodes and relationships
    in a specific format and use the `neo4j-admin` tool to prepare the database. You
    can read more about the CSV file formats and examples at [https://neo4j.com/docs/operations-manual/current/tools/neo4j-admin/neo4j-admin-import/](https://neo4j.com/docs/operations-manual/current/tools/neo4j-admin/neo4j-admin-import/).
  prefs: []
  type: TYPE_NORMAL
- en: Incremental data load
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The approaches for incremental data loading depend on the framework we will
    use to load the data. If there is a lot of streaming data, then leveraging a framework
    such as Apache Kafka might be a good idea. Also, it is easy to build applications
    to interact with databases to ingest the data using language frameworks such as
    Java, JavaScript, .NET, or Python.
  prefs: []
  type: TYPE_NORMAL
- en: You can read about building client applications for Neo4j at [https://neo4j.com/docs/create-applications/](https://neo4j.com/docs/create-applications/).
    One thing we need to keep in mind is to leverage the managed transactional functions
    so that the driver can retry the transactions as needed in a cluster when the
    cluster topology changes due to network failures or server failures. You can read
    more about this at [https://neo4j.com/docs/java-manual/current/transactions/](https://neo4j.com/docs/java-manual/current/transactions/).
    This link points to Java usage, but the same feature is available for all supported
    language frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Graph augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While we have built the article augmentation and customer augmentation as a
    Spring Boot application, there are other aspects that we have not investigated
    automating. After we generated embeddings for a specified season for the customer,
    we ran the ML aspects as individual commands such as KNN similarity and community
    detection. We might have to automate these aspects, too. Whenever new data is
    ingested into the graph, we might have to trigger the augmentation application
    to generate the embeddings to adapt to the new data we have loaded, and then trigger
    the GDS algorithms after that is completed. Note that the ML pipeline we looked
    at is simple chaining of KNN similarity and community detection; they can be much
    more complex based on the needs.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to read more about ML pipelines with Neo4j, you can visit [https://neo4j.com/docs/graph-data-science/current/machine-learning/machine-learning/](https://neo4j.com/docs/graph-data-science/current/machine-learning/machine-learning/).
  prefs: []
  type: TYPE_NORMAL
- en: We explored how to look at recommendations as Cypher queries as part of analysis
    and validation. Once we are satisfied with queries, we might have to build an
    application to provide recommendations as needed on demand.
  prefs: []
  type: TYPE_NORMAL
- en: We have built our application on the Spring Framework, which provides various
    capabilities and options to build a production-grade application that makes it
    easier to deploy and monitor applications. You can read more about how we can
    package the Spring applications for production deployment at [https://docs.spring.io/spring-boot/reference/packaging/index.html](https://docs.spring.io/spring-boot/reference/packaging/index.html).
    You can read more about the production-level features that help us monitor the
    application at [https://docs.spring.io/spring-boot/reference/actuator/index.html](https://docs.spring.io/spring-boot/reference/actuator/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: These are the key principles and considerations for deployment; for production
    deployment, multiple aspects need to be considered, including a proper deployment
    architecture along with application development. Several processes need to be
    deployed for monitoring and performance evaluation to make sure the application
    scales with usage as data and traffic grows.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this concluding chapter, you learned how to take your Haystack-powered GenAI
    chatbot from local development to a fully deployed, cloud-hosted application using
    Google Cloud Run. We walked through preparing your project structure, containerizing
    your application with Docker, configuring essential Google Cloud services, and
    deploying your chatbot in a scalable and serverless environment. You also learned
    how to verify the deployment, monitor performance, and troubleshoot common issues—equipping
    you with practical skills that extend far beyond just this project.
  prefs: []
  type: TYPE_NORMAL
- en: More importantly, this chapter brought everything full circle. From understanding
    knowledge graphs and vector search to integrating GenAI workflows using Haystack
    and Neo4j and, finally, deploying your application to the cloud—you now have a
    complete end-to-end blueprint for building intelligent, scalable, and production-ready
    GenAI applications.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve now wrapped up our journey of building Neo4j-powered apps with LLMs. Up
    next, we’ll take a quick look at the key takeaways of this journey.
  prefs: []
  type: TYPE_NORMAL
