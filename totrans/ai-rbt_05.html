<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-78"><a id="_idTextAnchor159"/>5</h1>
<h1 id="_idParaDest-79"><a id="_idTextAnchor160"/>Picking Up and Putting Away Toys using Reinforcement Learning and Genetic Algorithms</h1>
<p>This chapter is where the robots start to get challenging – and fun. What we want to do now is have the robot’s manipulator arm start picking up objects. Not only that, but instead of preprogramming arm moves and grasping actions, we want the robot to be able to learn how to pick up objects, and how to move its arm without hitting itself.</p>
<p>How would you teach a child to pick up toys in their room? Would you offer a reward for completing the task, such as “<em class="italic">If you pick up your toys, you will get a treat?</em>” Or would you offer a threat of punishment, such as “<em class="italic">If you don’t pick up your toys, you can’t play games on your tablet.</em>” This concept, offering positive feedback for good behavior and negative <a id="_idIndexMarker363"/>feedback for undesirable actions, is called <strong class="bold">reinforcement learning</strong>. That is one of the ways we will train our robot in this chapter.</p>
<p>If this sounds something like a game, where you get positive points for reaching a goal and lose points for missing a goal, then you are right. We have some concept of winning that we are trying to achieve, and we create some sort of point system to reinforce – that is to say, reward – behavior when the robot does what we want it to.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Designing the software</li>
<li>Setting up the solution</li>
<li>Introducing Q-learning for grasping objects</li>
<li>Introducing <strong class="bold">genetic algorithms</strong> (<strong class="bold">GAs</strong>) for path planning</li>
<li>Alternative robot arm ML approaches</li>
</ul>
<h1 id="_idParaDest-80"><a id="_idTextAnchor161"/>Technical requirements</h1>
<p>The exercise in this chapter does not require any new software or tools that we haven’t already seen in previous chapters. We will start by using Python and ROS 2. You will need an IDE for Python (IDLE or Visual Studio Code) to edit the source code.</p>
<p>Since this chapter is all about moving the robot arm, you will need a robot arm to execute the code. The one I used is the <strong class="bold">LewanSoul Robot xArm</strong>, which I purchased from Amazon.com. This arm uses digital servos, which makes the programming much easier, and provides us with position feedback, so we know what position the arm is in. The arm I purchased can be found at <a href="http://tinyurl.com/xarmRobotBook">http://tinyurl.com/xarmRobotBook</a> at the time of publication.</p>
<p class="callout-heading">Note</p>
<p class="callout">If you don’t want to buy a robot arm (or can’t), you can run this code against a simulation of a robot arm using ROS 2 and <strong class="bold">Gazebo</strong>, a simulation engine. You can find instructions at <a href="https://community.arm.com/arm-research/b/articles/posts/do-you-want-to-build-a-robot">https://community.arm.com/arm-research/b/articles/posts/do-you-want-to-build-a-robot</a>.</p>
<p>You’ll find the code for this chapter in the GitHub repository for this book, at <a href="https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e">https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e</a>.</p>
<h1 id="_idParaDest-81"><a id="_idTextAnchor162"/>Task analysis</h1>
<p>Our tasks for this chapter are pretty straightforward. We will use a robot arm to pick up the toys we <a id="_idIndexMarker364"/>identified in the previous chapter. This can be divided into the following tasks:</p>
<ul>
<li>First, we build <a id="_idIndexMarker365"/>an interface to control the robot arm. We are using <strong class="bold">ROS 2</strong> to connect the various parts of the robot together, so this interface is how the rest of the system sends commands and receives data from the arm. Then we get into teaching the arm to perform its function, which is picking up toys. The first level of capability is picking up or grasping toys. Each toy is slightly different, and the same strategy won’t work every time. Also, the toy might be in different orientations, so we have to adapt to how the toy is presented to the robot’s end effector (a fancy name for its hand). So rather than write a lot of custom code that may or may not work all the time, we want to create a structure so that the robot can learn for itself.</li>
<li>The next problem we face is to have the arm move. It’s not just that the arm has positions, but it also has to have a path from a start point to an end point. The arm is not a monolithic part – it’s composed of six different motors (as shown in <em class="italic">Figure 5</em><em class="italic">.3</em>) that each do something different. Two of the motors – the grip and the wrist – don’t move the arm at all; they only affect the hand. So our arm path is controlled by four motors. The other big problem is that the arm can collide with the body of the robot if we are not careful, so our path planning for the arm has to avoid collisions.<p class="list-inset">We will use a <a id="_idIndexMarker366"/>completely different technique for learning arm paths. A <strong class="bold">GA</strong> is a technique for machine learning that uses an analog of evolution to <em class="italic">evolve</em> complex behaviors out of simple movements.</p></li>
</ul>
<p>Now let’s talk a <a id="_idIndexMarker367"/>bit first about what we have to work with. We have a <code>600</code>, and (after a short time interval to permit the motor to move) we see that the servo position is <code>421</code>, then something is preventing the motor from reaching the goal we set for it. This information will be very valuable for training the robot arm.</p>
<p>We can use <strong class="bold">forward kinematics</strong>, which <a id="_idIndexMarker369"/>means summing up all the angles and levers of the arm to deduce where the hand is located (I’ll provide the code for that later in the chapter). We can use this hand location as our desired state – our <strong class="bold">reward criteria</strong>. We will <a id="_idIndexMarker370"/>give the robot points, or rewards, based on how close the hand is to the desired position and orientation we want. We want the robot to figure out what it takes to get to that position. We need to give the robot a way to test out different theories or actions that will result in the arm moving.</p>
<p>We will begin by just working with the robot hand, or to use the fancy robot term, the <strong class="bold">end effector</strong>.</p>
<p>The following <a id="_idIndexMarker371"/>diagram shows how we are trying to align our robot arm to pick up a toy by rotating the wrist:</p>
<div><div><img alt="Figure 5.1 – Storyboard for picking up a toy" src="img/B19846_05_1.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Storyboard for picking up a toy</p>
<p>For grasping, we have three actions to work with. We position the arm to pick up the toy, we adjust the angle of the hand by rotating the hand with the wrist servo, and we close the hand in order to grasp the object. If the hand closes completely, then we missed the toy and the hand is empty. If the toy is keeping the gripper from closing because we picked it up, then we have success and have grabbed the toy. We’ll be using this process to teach the robot to use different hand positions to pick up toys based on their shape.</p>
<h1 id="_idParaDest-82"><a id="_idTextAnchor163"/>Designing the software</h1>
<p>The first steps <a id="_idIndexMarker372"/>in designing the robot arm control software are to establish a coordinate frame (how we measure movement), after which we set up our solution space by creating states (arm positions) and actions (movements that change positions). The following diagram shows the coordinate frame for the robot arm:</p>
<div><div><img alt="Figure 5.2 – Robot arm coordinate frame" src="img/B19846_05_2.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – Robot arm coordinate frame</p>
<p>Let’s define the coordinate frame of the robot – our reference that we use to measure movement – as shown in the preceding diagram. The X direction is toward the front of the robot, so movement forward and backward is along the X-axis. Horizontal movement (left or right) is along the Y-axis. Vertical movement (up and down) is in the Z direction. We place the zero point – the origin of our coordinates – down the center of the robot arm with zero Z (Z=0) on the floor. So, if I say the robot hand is moving positively in X, then it is moving away from the front of the robot. If the hand (the end of the arm) is moving in Y, then it is moving left or right.</p>
<p>Now we must <a id="_idIndexMarker373"/>have a set of names that we will call the servo motors in the arm. We’ll do a bit of anthropomorphic naming, and give the arm parts anatomical titles. The motors are numbered in the control system and the servos on my robot arm are labeled:</p>
<ul>
<li><em class="italic">Motor 1</em> opens and closes the gripper. We may also call the gripper the hand.</li>
<li><em class="italic">Motor 2</em> is wrist rotate, which rotates the hand.</li>
<li><em class="italic">Motor 3</em> is the wrist pitch (up and down) direction.</li>
<li>We’ll call <em class="italic">Motor 4</em> the elbow. The elbow flexes the arm in the middle, just as you expect.</li>
<li><em class="italic">Motor 5</em> is the shoulder pitch servo, which moves the arm up and down, rotating around the Y-axis when the arm is pointing straight ahead.</li>
<li><em class="italic">Motor 6</em> is at the base of the arm, so we’ll call it the shoulder yaw (right or left) servo. It rotates the entire arm about the Z-axis. I’ve decided not to move this axis, since the entire base of the robot can rotate due to the omni wheels. We’ll just move the arm up and down to simplify the problem. The navigation system we develop in <a href="B19846_08.xhtml#_idTextAnchor235"><em class="italic">Chapter 8</em></a> will point the arm in the correct direction.</li>
</ul>
<p>We will start by defining an interface to the robot arm that the rest of the robot control system can use:</p>
<div><div><img alt="Figure 5.3 – Robot arm motor nomenclature" src="img/B19846_05_3.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Robot arm motor nomenclature</p>
<p>Here, pitch refers to up/down motion while yaw refers to right/left motion.</p>
<p>We’ll use <a id="_idIndexMarker374"/>two terms that are common in the robot world to describe how we calculate where the arm is based on the data we have:</p>
<ul>
<li><strong class="bold">Forward Kinematics</strong> (<strong class="bold">FK</strong>) is the process of starting at the base of the robot arm and <a id="_idIndexMarker375"/>working out toward the gripper, calculating the position and orientation of each joint in turn. We take <a id="_idIndexMarker376"/>the position of the joint and the angle it is at, and add the length of the arm between that joint and the next joint. The process of doing this calculation, which produces an X-Y-Z position and a pitch-roll-yaw orientation of the end of the robot’s fingers, is called forward kinematics because we calculate forward from the base and out to the arm.</li>
<li><strong class="bold">Inverse Kinematics</strong> (<strong class="bold">IK</strong>) takes a different approach. We know the position and orientation of either where the hand is, or where we want it to be. Then we calculate <a id="_idIndexMarker377"/>backward up the arm to determine what joint angles would produce that hand position. IK is a bit trickier because there <a id="_idIndexMarker378"/>may be more than one solution (combination of joint positions) that may produce a given hand result. Try this with your own arm. Grasp a doorknob. Now move your arm while keeping your hand on the doorknob. There are multiple combinations of your joints that result in your hand being in the same position and orientation. We won’t be using IK here in this book, but I wanted you to be familiar with the term, which is often used in robot arms to drive the position of robot end effectors (grippers or hands).</li>
</ul>
<p>For a more in-depth explanation of these concepts, you can refer to <a href="https://control.com/technical-articles/robot-manipulation-control-with-inverse-and-forward-kinematics/">https://control.com/technical-articles/robot-manipulation-control-with-inverse-and-forward-kinematics/</a>.</p>
<p>Next, let’s discuss how we can put the ar<a id="_idTextAnchor164"/><a id="_idTextAnchor165"/><a id="_idTextAnchor166"/>m in motion.</p>
<h1 id="_idParaDest-83"><a id="_idTextAnchor167"/>Setting up the solution</h1>
<p>We will call the act of setting the motors to a different position an <strong class="bold">action</strong>, and we will call the <a id="_idIndexMarker379"/>position of the robot arm and hand the <strong class="bold">state</strong>. An action applied to a state results in the arm being in a new state.</p>
<p>We are going to have the robot associate states (a beginning position of the hand) and an action (the motor commands used when at that state) with the probability of generating either a positive or negative <strong class="bold">outcome</strong> – we will be training the robot to figure out which sets of actions result in maximizing the <strong class="bold">reward</strong>. What’s a reward? It’s just an arbitrary value that we use to define whether the learning the robot accomplished was positive – something we wanted – or negative – something we did not want. If the action resulted in positive learning, then we increment the reward, and if it does not, then we decrement the reward. The robot will use an algorithm to both try and maximize the reward, and to incrementally learn a task.</p>
<p>Let’s understand this process better by exploring the role played by machine learning.</p>
<h2 id="_idParaDest-84"><a id="_idTextAnchor168"/>Machine learning for robot arms</h2>
<p>Since incremental learning was also part of neural networks, we will use some of the same tools <a id="_idIndexMarker380"/>we used before in our neural network to propagate a reward to each step in a chain of movements that result in the hand moving to some location. In reinforcement learning, this is called <strong class="bold">discounting the reward</strong> – distributing portions of rewards to the step in a multi-step process. Likewise, the combination of a state and an action is called a <strong class="bold">policy</strong> – because we are telling the robot, “when you are in this position, and want to go to that position, do<a id="_idTextAnchor169"/> this action.” Let’s understand this concept better by looking more closely at our process for learning with the robot arm:</p>
<ol>
<li>We set our goal position of the robot hand, which is the position of the robot hand in X and Z coordinates in millimeters from the rotational center of the arm.</li>
<li>The robot will try a series of movements to try and get close to that goal. We will not be giving the robot the motor positions it needs to get to that goal – the robot must learn. The initial movements will be totally randomly generated. We will restrict the delta movement (analogous to the learning rate from the previous chapter) to some small size so we don’t get wild flailing of the arm.</li>
<li>At each incremental movement, we will score the movement based on whether or not the arm moved closer to the goal position.</li>
<li>The robot will remember these movements by associating the beginning state and the action (movement) with the reward score.</li>
<li>Later, we will train a neural network to generate probabilities of positive outcomes based on the inputs of starting state and movement action. This will allow the arm to learn which sequences of movement achieve positive results. Then we will be able to predict which movement will result in the arm moving correctly based on the star<a id="_idTextAnchor170"/>ting position.</li>
<li>You can also surmise that we must add a reward for accomplishing the task quickly – we want the results to be efficient, and so we will add rewards for taking the shortest time to complete the task – or, you could say, we subtract a reward for each step needed to get to the goal so that the process with the fewest steps gets the most reward.</li>
<li>We calculate <a id="_idIndexMarker381"/>rewards using the <strong class="bold">Q-function</strong>, as follows:</li>
</ol>
<p><em class="italic">Q = Q(s,a)+ (reward(s,a)  + g  * </em><em class="italic">max(Q(s’,a’))</em></p>
<p class="list-inset">where <em class="italic">Q</em> represents the reward the robot will get (or expects to get) from a particular action. <em class="italic">Q(s,a)</em> is the final reward that we expect overall for an action given the starting state. <em class="italic">reward(s,a)</em> is the reward for that action (the small, incremental step we take now). <em class="italic">g</em> is a discount function that rewards getting to the <a id="_idIndexMarker382"/>goal quicker, that is, with a fewer number of steps (the more steps you have, the more <em class="italic">g</em> discounts (removes the reward)), and <em class="italic">max(Q(s’,a’))</em> selects the action that results in the largest reward out of the set of actions available at that state. In the equation, <em class="italic">s</em> and <em class="italic">a</em> represent the current state and action, and <em class="italic">s’</em> and <em class="italic">a’</em> represent the subsequent state and action, respectively. This is my version of Bellman’s equation for decision-making, with some adaptations for this particular problem. I added a discount for longer solutions (with more steps, thus taking longer to execute) to reward quicker arm movement (fewer steps), and left out the learning rate (alpha) as we are taking whole steps for each state (we don’t have intermediate states to learn).</p>
<p>Next, let’s understand how we can teach the robot arm how to<a id="_idTextAnchor171"/><a id="_idTextAnchor172"/> learn movement.</p>
<h2 id="_idParaDest-85"><a id="_idTextAnchor173"/>How do we pick actions?</h2>
<p>What actions <a id="_idIndexMarker383"/>can the robot arm perform? As shown in <em class="italic">Figure 5</em><em class="italic">.3</em>, we have six motors, and we have three options for each motor:</p>
<ul>
<li>We can do nothing – that is, not move at all</li>
<li>We can move counterclockwise, which will make our motor angle smaller</li>
<li>We can move clockwise, which makes our motor angle larger</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">Most servo motors treat positive position changes as clockwise rotation. Thus, if we command the rotation to change from 200 to 250 degrees, the motor will turn clockwise 50 degrees.</p>
<p>Our action <a id="_idIndexMarker384"/>space for each motion of the robot arm is to move each motor either clockwise, counterclockwise, or not at all. This gives us 729 combinations with 6 motors (<em class="italic">3</em>6 possible actions). That is quite a lot. The software interface we are going to build refers to the robot arm motors by number with <em class="italic">1</em> being the hand and <em class="italic">6</em> being the shoulder rotation motor.</p>
<p>Let’s reduce this number and just consider the motions of three of the motors – the <code>[-1, 0, 1]</code>. We will use a value of just +/-1 or 0 in the action matrix to step the motors in small increments. The x-y coordinates of the hand can be computed from the sums of the angles of each joint times the length of the arm.</p>
<p>Here is a Python function to compute the position of the robot hand, given that each arm segment is 10 cm long. You can substitute the length of your robot arm segments. This function turns the motor angles representing the hand position from degrees into x-y coordinates in centimeters:</p>
<pre class="source-code">
def forward_kinematics(theta1, theta2, theta3, segment_length):
 # Convert degrees to radians
    theta1_rad = math.radians(theta1)
    theta2_rad = math.radians(theta2)
    theta3_rad = math.radians(theta3)
    # Calculate positions of each joint
    x1 = segment_length * math.cos(theta1_rad)
    y1 = segment_length * math.sin(theta1_rad)
    x2 = x1 + segment_length * math.cos(theta1_rad + theta2_rad)
    y2 = y1 + segment_length * math.sin(theta1_rad + theta2_rad)
    x3 = x2 + segment_length * math.cos(theta1_rad + theta2_rad + theta3_rad)
    y3 = y2 + segment_length * math.sin(theta1_rad + theta2_rad + theta3_rad)
return x3, y3</pre> <p>The actions of the arm (possible movements) make up the action space of our robot arm, which is the set of all possible actions. What we will be doing in this chapter is investigating various ways of picking which action to perform and when in order to accomplish our tasks, and using machine<a id="_idTextAnchor174"/> learning to do it.</p>
<p>Another way <a id="_idIndexMarker386"/>of looking at this process is that we are generating a <strong class="bold">decision tree</strong>. You are probably familiar with the concept. We have a bit of <a id="_idIndexMarker387"/>a unique application when applying this to a robot arm, because our arm is a series of joints connected together, and moving one moves all of the other joints farther out on the arm. When we move Motor 5, Motors 4 and 3 move position in space, and their angles and distances to the ground and to our goal change. Each possible motor move adds 27 new branches to our decision tree, and can generate 27 new arm positions. All we have to do is pi<a id="_idTextAnchor175"/>ck which one to keep.</p>
<p>The rest of this chapter will deal with just how we go about selecting our motions. It’s time to start writing some code now. The first order of business is to create an interface to the robot arm that the rest of the robot can use.</p>
<h1 id="_idParaDest-86"><a id="_idTextAnchor176"/>Creating the interface to the arm</h1>
<p>As previously <a id="_idIndexMarker388"/>noted, we are using ROS 2 as <a id="_idIndexMarker389"/>our interface service, which creates a <strong class="bold">Modular Open System Architecture</strong> (<strong class="bold">MOSA</strong>). This turns our components into <em class="italic">plug-and-play</em> devices that can be added, removed, or modified, much like the apps on a smartphone. The secret to making that happen is to create a useful, generic interface, which we will do now.</p>
<p class="callout-heading">Note</p>
<p class="callout">I’m creating my own interface to ROS 2 that is just for this book. We won’t be using any other ROS packages with this arm – just what we create, so I wanted the very minimum interface to get the job done.</p>
<p>We’ll be creating this interface in Python. Follow these steps:</p>
<ol>
<li>First, create a <strong class="bold">package</strong> for the robot arm in ROS 2. A package is a portable organization unit for functionality in ROS 2. Since we have multiple programs and multiple functions for the robot arm, we can bundle them together:<pre class="source-code">
<strong class="bold">cd ~/ros2_ws/src</strong>
<strong class="bold">ros2 pkg create –build-type ament-cmake ros_xarm</strong>
<code>src</code> directory where we will store all of the parts we need.</p></li> <li>We need to install the drivers for xArm so we can use them in Python:<pre class="source-code">
<strong class="bold">pip install xarm</strong></pre></li> <li>Now we go to our new source directory:<pre class="source-code">
<code>xarm_mgr.py</code>, which is short for <code>xarm manager</code>.</p></li> <li>Open the editor and let’s start coding. First, we need some imports:<pre class="source-code">
import rclpy
import xarm
import time
from rlcpy.node import Node
from std_msgs.msg import String, Int32MultiArray, Int32</pre><p class="list-inset"><code>rclpy</code> is the ROS 2 Python interface. <code>xarm</code> is the interface to the robot arm, while <code>time</code> of course is a time module that we will use to set timers. Finally, we use some standard ROS message formats with which to communicate.</p></li> <li>Next, we are <a id="_idIndexMarker390"/>going to create some predefined named positions of the arm as shortcuts. This is a simple way to put the arm where we need it. I’ve defined five arm preset positions we can call:</li>
</ol>
<div><div><img alt="Figure 5.4 – Robot arm positions" src="img/B19846_05_4.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – Robot arm positions</p>
<p class="list-inset">Let’s describe these positions in some detail:</p>
<ul>
<li><em class="italic">High Carry</em> is the position we want the arm to be at when we are carrying an object such as a toy. The arm is over the robot, and the hand is elevated. This helps to keep the toy from falling out of the hand.</li>
<li><em class="italic">Neutral Carry</em> is the standard position when the robot is driving so that the arm is not in front of the camera.</li>
<li><em class="italic">Pick Up</em> is a combination of <em class="italic">Grasp</em> and <em class="italic">Grasp Close</em> (which are not shown individually in the figure). The former is an arm position that puts the hand on the ground <a id="_idIndexMarker391"/>so we can pick up an object. The arm is as far out in front of the robot as it will go and touches the ground. The latter just closes the end effector to grab a toy.</li>
<li><em class="italic">Drop Off</em> is the arm position high above the robot to put a toy in the toy box, which is quite tall.</li>
<li><em class="italic">Align</em> (not shown) is a utility mode to check the alignment of the arm. All the servos are set to their middle position and the arm should point at the ceiling in a straight line. If it does not, you need to adjust the arm using the utility that came with it.</li>
</ul>
<p class="list-inset">Let’s see how we can set up the ROS interface. The numbers are the servo motor positions (angles) in units from <code>0</code> (fully counterclockwise) to <code>1000</code> (fully clockwise). The <code>9999</code> code means to not change the servo in that position so we can create commands that don’t change the positions of parts of the arm, such as the gripper:</p>
<pre class="source-code">
<strong class="bold">HighCarry=[9999,500,195,858,618,9999]</strong>
<strong class="bold">MidCarry=[9999, 500, 500, 807, 443, 9999]</strong>
<strong class="bold">Grasp = [100,500,151,553,117,9999]</strong>
<strong class="bold">GraspClose=[700,9999,9999,9999,9999,9999]</strong>
<strong class="bold">Align=[500,500,500,500,500,500]</strong></pre> <ol>
<li value="6">Now we can start defining our robot arm control class. We’ll start with the class definition and the initialization function:<pre class="source-code">
class xarmControl(Node):
    def __init__(self):
        super().__init__('xarm_manager') # node name
        self.publisher = self.create_publisher(Int32MultiArray, 'xarm_pos', 10)
        self.armAngPub = self.create_publisher(Int32MultiArray, 'xarm_angle', 10)</pre><p class="list-inset">There is <a id="_idIndexMarker392"/>quite a bit going on here to set up our ROS interface for the robot arm:</p><ul><li>First of all, we call up the object class structure (<code>super</code>) to initialize our ROS 2 node with the name <code>xarm_manager</code>.</li><li>Then we create a publisher for the arm position information, helpfully called <code>xarm_pos</code>. Here, POS stands for position. This publishes the arm position in servo units, which go from <code>0</code> (fully counterclockwise) to <code>1000</code> (fully clockwise). We also publish the arm angles in degrees, in case we need that information, as <code>xarm_angle</code>. The center of the servo travel is 0 degrees (<code>500</code> in servo units). Counterclockwise positions are negative angles while clockwise positions are positive angles. I just used integer degrees (no decimal points) since we don’t need that level of precision for the arm. Our High Carry position in servo units is <code>[666,501,195,867,617,500]</code>, and in servo angles is <code>[41,0,-76,91,29,0]</code>. We publish our outputs and subscribe to our inputs.</li></ul></li> <li>Our inputs, or subscriptions, provide the outside interface to the arm. I thought through how the arm might be used, and came up with the interface I wanted to see. In our case, we have a very simple arm, and need only a few commands. First of all, we have a string command called <code>RobotCmd</code>, which lets us create commands to control the mode or state of the robot. This will be used for a lot of commands for the robot, and not just for the arm. I’ve created several arm mode commands that we’ll cover in a few paragraphs. The usefulness of <code>RobotCmd</code> is we can send any string on this input and process it on the receiving end. It’s a very flexible and useful interface. Note that for each subscriber, we create a function call to a callback routine. When the data is published on the interface, the callback routine is called in our program (<code>xarm_mgr.py</code>) automatically:<pre class="source-code">
self.cmdSubscribe = self.create_subscription(String, 'RobotCmd', self.cmdCallback,10)</pre></li> <li>The next <a id="_idIndexMarker393"/>part of the interface allows us to move the base of the arm in yaw, and operate the hand and wrist independently. In this chapter, we are starting with training just the gripper, so it helps to have an independent interface to rotate, open, and close the gripper. Operating the hand does not change the coordinate position of the gripper, so this can be separated. Likewise, we move the hand in yaw – right and left – to line up with toys to be grasped. We are going to start with this function locked off, and we’ll add the yaw function later. This is controlled by the computer vision system that we designed in the previous chapter, so it needs a separate interface. We have the <code>xarmWrist</code> command to rotate the wrist, <code>xarmEffector</code> to open and close the gripper fingers, and <code>xarmBase</code> to move the base of the arm right or left:<pre class="source-code">
        self.wristSubscribe = self.create_subscription(Int32, 'xarmWrist', self.wristCallback,10)
        self.effSubscribe = self.create_subscription(Int32, 'xarmEffector', self.effCallback,10)
        self.baseSubscribe = self.create_subscription(Int32, 'xarmBase', self.baseCallback,10)</pre></li> <li>The last command interface lets us move the arm to any position we specify. Normally, we command the arm to move using an array of numbers, like this: <code>[100,500,151,553,117,500]</code>. I’ve added a <em class="italic">secret feature</em> to this command. Since we may want to move the arm without either changing the yaw angle (which comes from the vision system) or the hand position (which may or may not be holding a toy), we can send commands that move the arm but don’t <a id="_idIndexMarker394"/>affect some of the servos, such as the hand. I used the value <code>9999</code> as the <em class="italic">don’t-move-this-servo</em> value. So if the arm position command reads <code>[9999, 9999, 500, 807, 443, 9999]</code> then the yaw position (<em class="italic">Motor 6</em>) and the hand position (<em class="italic">Motors 0 </em>and<em class="italic"> 1</em>) don’t change:<pre class="source-code">
        self.baseSubscribe = self.create_subscription(
Int32MultiArray, 'newArmPos', self.moveArmCallback,10)</pre></li> <li>Now that we have all of our publish and subscribe interfaces defined, we can open the USB interface to the robot arm and see whether it is responding. If not, we’ll throw an error message:<pre class="source-code">
        timer_period = 1.0 # seconds
        self.timer = self.create_timer(timer_period, self.timer_callback)
        self.i = 0 # counter
        try:
            self.arm = xarm.Controller('USB')
            print("ARM OPEN")
        except:
            self.get_logger().error("xarm_manager init NO ARM DETECTED")
            self.arm = None
            print("ERROR init: NO AR<a id="_idTextAnchor177"/>M DETECTED")
        return</pre></li> </ol>
<p class="callout-heading">Note</p>
<p class="callout">Here is the quick cheat guide to the servos in the <code>xarmPos</code> command array:</p>
<p class="callout"><code>[grip open/close, wrist rotate, wrist pitch, elbow pitch, shoulder pitch, </code><code>shoulder yaw]</code></p>
<ol>
<li value="11">Our next function in the source code is to set up a telemetry timer. We want to periodically publish the arm’s position for the rest of the robot to use. We’ll create a <a id="_idIndexMarker395"/>timer callback that executes periodically at a rate we specify. Let’s start with once a second. This is an informational value and we are not using it for control – the servo controller takes care of that. This is the code we need:<pre class="source-code">
        timer_period = 1.0 # seconds
        self.timer = self.create_timer(timer_period, self.timer_callback)
        self.i = 0 # counter</pre><p class="list-inset">The <code>timer_period</code> is the interval between interrupts. The <code>self.timer</code> class variable is a function pointer to the timer function, and we point it at another function, <code>self.timer_callback</code>, which we’ll define in the next code block. Every second, the interrupt will go off and call the <code>timer_callback</code> routine.</p></li> <li>Our next bit of code is part of the hardware interface. Since we are initializing the arm <a id="_idIndexMarker396"/>controller, we need to open the hardware connection to the arm, which is a USB port using the <strong class="bold">human interface device</strong> (<strong class="bold">HID</strong>) protocol:<pre class="source-code">
        try:
            self.arm = xarm.Controller('USB')
            print("ARM OPEN")
        except:
            self.get_logger().error("xarm_manager init NO ARM DETECTED")
            self.arm = None
            print("ERROR init: NO ARM DETECTED")
        return</pre><p class="list-inset">We first create a <code>try</code> block so we can handle any exceptions. The robot arm may not <a id="_idIndexMarker397"/>be powered on, or it may not be connected, so we have to be prepared to handle this. We create an arm object (<code>self.arm</code>) that will be our interface to the hardware. If the arm opens successfully, then we return. If not, we run through the <code>except</code> routine:</p><ul><li>First, we log that we did not find the arm in the ROS error log. The ROS logging function is very versatile and provides a handy place to store information that you need for debugging.</li><li>Then we set the arm to a null object (<code>None</code>) so that we don’t throw unnecessary errors later in the program, and we can test to see whether the arm is connected.</li></ul></li> <li>The next block of code is our timer callback that publishes telemetry about the arm. Remember that we defined two output messages, the arm position and arm angle. We can service them both here:<pre class="source-code">
    def timer_callback(self):
        msg = Int32MultiArray()
        # call arm and get positions
        armPos=[]
        for i in range(1,7):
            armPos.append(self.arm.getPosition(i))
        msg.data = armPos
        self.publisher.publish(msg)
        # get arm positions in degrees
        armPos=[]
        for i in range(1,7):
            armPos.append(int(self.arm.getPosition(i, True)))
        msg.data = armPos
        #print(armPos)
        self.armAngPub.publish(msg)</pre><p class="list-inset">We are <a id="_idIndexMarker398"/>using the <code>Int32MultiArray</code> datatype so that we can publish the arm position data as an array of integers. We collect the data from the arm by calling <code>self.arm.getPosition(servoNumber)</code>. We append the output to our array, and when we are done, call the ROS publish routine <code>(self.&lt;topic name&gt;.publish(msg))</code>. We do the same thing for the arm angle, which we can get by calling <code>arm.getPosition(servoNumber, True)</code> to return an angle instead of servo units.</p></li> <li>Now we can handle receiving commands from other programs. Next, we are going to be creating a control panel for the robot that can send commands and set modes for the robot:<pre class="source-code">
    def cmdCallback(self, msg):
        self.get_logger().info("xarm rec cmd %s" % msg.data)
        robotCmd = msg.data
        if robotCmd=="ARM HIGH_CARRY":
            self.setArm(HighCarry)
        if robotCmd=="ARM MID_CARRY":
            self.setArm(MidCarry)
        if robotCmd=="ARM GRASP_POS":
            self.setArm(Grasp)
        if robotCmd=="ARM GRASP_CLOSE":
            self.setArm(GraspClose)
        if robotCmd=="ARM ALIGN":
            self.setArm(Align)</pre><p class="list-inset">This section <a id="_idIndexMarker399"/>is pretty straightforward. We receive a string message containing a command, and we parse the message to see whether it is something this program recognizes. If so, we process the message and perform the appropriate command. If we get <code>ARM MID_CARRY</code>, which is a command to position the arm to the middle position, then we send a <code>setArm</code> command using the <code>MidCarry</code> global variable, which has the servo positions for all six motors.</p></li> <li>Next, we write the code for the robot to receive and execute the wrist servo command, which rotates the gripper. This command goes to <em class="italic">Motor 2</em>:<pre class="source-code">
    def wristCallback(self, msg):
        try:
            newArmPos = int(msg.data)
        except ValueError:
            self.get_logger().info("Invalid xarm wrist cmd %s" % msg.data)
            print("invalid wrist cmd ", msg.data)
            return
        # set limits
        newArmPos = float(min(90.0,newArmPos))
        newArmPos = float(max(-90.0,newArmPos))
        self.arm.setPosition(2,newArmPos, True)</pre><p class="list-inset">This function call is executed when the <code>xarmWrist</code> topic is published. This command just moves the wrist rotation, which we would use to align the fingers of the hand to the object we are picking up. I added some exception handling for invalid values, along with limit checking on the range of the input, which I consider a <a id="_idIndexMarker400"/>standard practice for external inputs. We don’t want the arm to do something weird with an invalid input, such as if someone was able to send a string on the <code>xarmWrist</code> topic instead of an integer. We also check whether the range of the data in the command is valid, which in this case is from <code>0</code> to <code>1000</code> servo units. If we get an out-of-bounds error, we clamp the command to the allowed range using the <code>min</code> and <code>max</code> functions.</p></li> <li>The end effector command and base command (which controls the left-right rotation of the entire arm) work exactly the same way:<pre class="source-code">
    def effCallback(self, msg):
    # set just the end effector position
        try:
            newArmPos = int(msg.data)
        except ValueError:
            self.get_logger().info("Invalid xarm effector cmd %s" % msg.data)
            return
        # set limits
        newArmPos = min(1000,newArmPos)
        newArmPos = max(0,newArmPos)
        self.arm.setPosition(1,newArmPos)
    def baseCallback(self, msg):
    # set just the base azimuth position
        try:
            newArmPos = int(msg.data)
        except ValueError:
            self.get_logger().info("Invalid xarm base cmd %s" % msg.data)
            return
        # set limits
        newArmPos = min(1000,newArmPos)
        newArmPos = max(0,newArmPos)
        self.arm.setPosition(6,newArmPos)</pre><p class="list-inset">The <code>setArm</code> command lets us send one comment to set the position of every servo motor at once, with one command. We send an array of six integers, and this program <a id="_idIndexMarker401"/>relays that to the servo motor controller.</p><p class="list-inset">As mentioned before, I put in a special value, <code>9999</code>, that tells this bit of code to not move that motor. This lets us send commands to the arm that move some of the servos, or just one of them. This lets us move the up/down axis and left/right axis of the end of the arm independently, which is important.</p><p class="list-inset">Another thing that is important is that while this bit of Python code executes almost instantly, the servo motors take a finite amount of time to move. We have to throw in some delays between the servo commands so that the servo controller can process them and send them to the right motor. I’ve discovered that the value <code>0.1</code> (1/10 of a second) between commands works. If you leave this value out, only one servo will move, and the arm will not process the rest of the commands. The servos use a serial interface in a daisy chain fashion, which means they relay messages to each other. Each servo is plugged into one other servo, which is a big improvement over all the servos being plugged in individually.</p></li> <li>We can finish <a id="_idIndexMarker402"/>up our arm control code with <code>MAIN</code> – the executable part of the program:<pre class="source-code">
#######################MAIN####################################
rclpy.init()
print("Arm Control Active")
xarmCtr = xarmControl()
# spin ROS 2
rclpy.spin(xarmCtr)
# destroy node explicitly
xarmCtr.destroy_node()
rclpy.shutdown()</pre><p class="list-inset">Here, we initialize <code>rclpy</code> (ROS 2 Python interface) to connect our program to the ROS infrastructure. Then we create an instance of our <code>xarm</code> control class we created. We’ll call it <code>xarmCtr</code>. Then we just have to tell ROS 2 to execute. We don’t even need a loop. The program will perform publish and subscribe calls, and our timer sends out telemetry, which is all included in our <code>xarmControl</code> object. When we fall out of spin, we are done with the program, so we shut down the ROS node, and then the program.</p></li> </ol>
<p>Now we are ready to start training our robot arm! To do this, we are going to use three different methods to train our arm to pick up objects. In the first stage, we will just train the robot hand – the end effector – to grasp objects. We will use Q-learning, a type of RL, to accomplish this. We will have the robot try to pick up items, and we will reward, or give points, if the robot is successful and subtract points if it fails. The software will try to maximize <a id="_idIndexMarker403"/>the reward to get the most points, just like playing a game. We will generate different policies, or action plans, to make this happen.</p>
<h1 id="_idParaDest-87"><a id="_idTextAnchor178"/>Introducing Q-learning for grasping objects</h1>
<p>Training a robot <a id="_idIndexMarker404"/>arm end effector to pick up an oddly shaped <a id="_idIndexMarker405"/>object using the <strong class="bold">Q-learning</strong> RL technique involves several steps. Here’s a step-by-step explanation of the process:</p>
<ol>
<li>Define the state space and action space:<ul><li><strong class="bold">Define the state space</strong>: This includes all the relevant information about the environment and the robot arm, such as the position and orientation of the object, the position and orientation of the end effector, and any other relevant sensor data</li><li><strong class="bold">Define the action space</strong>: These are the possible actions the robot arm can take, such as rotating the end effector, moving it in different directions, or adjusting its gripper</li></ul></li>
<li><strong class="bold">Set up the Q-table</strong>: Create a Q-table that represents the state-action pairs and initialize it with random values. The Q-table will have a row for each state and a column for each action. As we test each position that the arm moves to, we will store the reward that was computed by the Q-learning equation (introduced in the <em class="italic">Machine learning for robot arms</em> section) in this table so that we can refer to it later. We will search the Q-table by state and action to see which state-action pair results in the largest reward.</li>
<li><strong class="bold">Define the reward function</strong>: Define a reward function that provides feedback to the robot arm based on its actions. The reward function should encourage the arm to pick up the object successfully and discourage undesirable behavior.</li>
<li><strong class="bold">Start the training loop</strong>: Start the training loop, which consists of multiple episodes. Each episode represents one iteration of the training process:<ul><li>Reset the environment and set the initial state</li><li>Select an action based on the current state using an exploration-exploitation strategy such as epsilon-greedy, where you explore random actions with a certain probability (epsilon) or choose the action with the highest Q-value</li><li>Execute the selected action and observe the new state and the reward</li><li>Update the Q-value in the Q-table using the Q-learning update equation, which incorporates the reward, the maximum Q-value for the next state, and the learning rate (alpha) and discount factor (gamma) parameters</li><li>Update the current state to the new state</li><li>Repeat the previous steps until the episode terminates, either by successfully picking up the object or reaching a maximum number of steps</li></ul></li>
<li><strong class="bold">Exploration and exploitation</strong>: Adjust the exploration rate (represented by epsilon) over time to gradually reduce exploration and favor exploitation of the learned <a id="_idIndexMarker406"/>knowledge. This allows the robot arm to initially explore different actions and gradually focus on exploiting the learned information to improve performance.</li>
<li><strong class="bold">Repeat training</strong>: Continue the training loop for multiple episodes until the Q-values converge or the performance reaches a satisfactory level.</li>
<li><strong class="bold">Perform testing</strong>: After training, use the learned Q-values to make decisions on the actions to take in a testing environment. Apply the trained policy to the robot arm end effector, allowing it to pick up the oddly shaped object based on the learned knowledge.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Implementing Q-learning for training a robot arm end effector requires a combination of software and hardware components, such as simulation environments, robotic arm controllers, and sensory input interfaces. The specifics of the implementation can vary depending on the robot arm platform and the tools and libraries being used.</p>
<h2 id="_idParaDest-88"><a id="_idTextAnchor179"/>Writing the code</h2>
<p>Now we’ll <a id="_idIndexMarker407"/>implement the seven-step process we just described by building the code that will train the arm, using the robot arm interface we made in the previous section:</p>
<ol>
<li>First, we include our imports – the functions we’ll need to implement our training code:<pre class="source-code">
import rclpy
import time
import random
from rclpy.node import Node
from std_msgs.msg import String, Int32MultiArray, Int32
from sensor_msgs.msg import Image
from vision_msgs.msg import Detection2D
from vision_msgs.msg import ObjectHypothesisWithPose
from vision_msgs.msg import Detection2DArray
import math
import pickle</pre><p class="list-inset"><code>rclpy</code> is the ROS 2 Python interface. We use <code>Detection2D</code> to talk to the vision system from the previous chapter (YOLOV8). I’ll explain the <code>pickle</code> reference when we get to it.</p></li> <li>Next, let’s define some functions we’ll be using later:<pre class="source-code">
global learningRate = 0.1 # learning rate
def round4(x):
 return (math.round(x*4)/4)
# function to restrict a variable to a range. if x &lt; minx, x=min x,etc.
def rangeMinMax(x,minx,maxx):
 xx = max(minx,x)
 xx = min(maxx,xx)
 return xx
def sortByQ(listByAspect):
 return(listByAspect[2])</pre><p class="list-inset">The learning rate is used in reinforcement learning just like in other machine learning algorithms to adjust how <a id="_idIndexMarker408"/>fast the system makes changes as a result of inputs. We’ll start with <code>0.1</code>. If this value is too big, we will have big jumps in our training that can cause erratic outputs. If it’s too small, we’ll have to do a lot of repetitions. <code>actionSpace</code> is the list of possible hand actions that we are teaching. These values are the angle of the wrist in degrees. Note that <code>-90</code> and <code>+90</code> are the same as far as grasping is concerned.</p><p class="list-inset">The <code>round4</code> function is used to round off the aspect ratio of the bounding box. When we detect a toy, as you may remember, the object recognition system draws a box around it. We use that bounding box as a clue to how the toy is oriented relative to the robot. We want a limited number of aspect angles to train for, so we’ll round this off to the nearest <code>0.25</code>.</p><p class="list-inset">The <code>SortbyQ</code> function is a custom sort key that we’ll use to sort our training to put the highest reward – represented by the letter <code>Q</code> – first.</p></li> <li>In this step, we’ll declare the class that will teach the robot to grasp objects. We’ll call the class <code>LearningHand</code>, and we’ll make it a node in ROS 2:<pre class="source-code">
class LearningHand(Node):
    def __init__(self):
        super().__init__('armQLearn') # node name
        # we need to both publish and subscribe to the RobotCmd topic
self.armPosSub = self.create_subscription(Int32MultiArray, "xarm_pos", self.armPosCallback, 10)
        self.cmdSubscribe = self.create_subscription(String, 'RobotCmd', self.cmdCallback,10)
        self.cmdPub = self.create_publisher(String, 'RobotCmd', 10)
 self.wristPub = self.create_publisher(Int32,'xarmWrist', 10)
 # declare parameter for number of repetitions
 self.declare_parameter('ArmLearningRepeats', rclpy.Parameter.Type.INTEGER)
 # get the current value from configuration
 self.repeats = self.get_parameter('ArmLearningRepeats').get_parameter_value().int_value</pre><p class="list-inset">Here we <a id="_idIndexMarker409"/>initialize the object by passing up the <code>init</code> function to the parent class (with <code>super</code>). We give the node the name <code>armQLearn</code>, which is how the rest of the robot will find it.</p><p class="list-inset">Our ROS interface subscribes to several topics. We need to talk to the robot arm, so we subscribe to <code>xarm_pos</code> (arm position). We need to subscribe (like every program that talks to the robot) to <code>RobotCmd</code>, which is our master mode command channel. We also need to be able to send commands on <code>RobotCmd</code>, so we create a publisher on that topic. Finally, we use a ROS parameter to set the value of how many repetitions we want for each learning task.</p></li> <li>This next block of code completes the setup for the learning function:<pre class="source-code">
 self.mode = "idle"
 self.armInterface = ArmInterface()
 # define the state space
 self.stateActionPairs = []
 # state space is the target aspect and the hand angle
 # aspect is length / width length along x axis(front back) width on y axis)
 aspects = [0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75]
 handAngles = [90, -45, 0, 45] # note +90 and -90 are the same angle
 for jj in range(0,len(aspects)):
   for ii in range(0,4):
     self.stateActionPairs.append([aspects[jj], handAngles[ii],0.0])</pre><p class="list-inset">We set the <a id="_idIndexMarker410"/>learning system mode to <code>idle</code>, which just means “wait for the user to start learning.” We create the arm interface by instantiating the <code>ArmInterface</code> class object we imported. Next, we need to set up our learning matrix, which stores the possible aspects (things we can see) and the possible actions (things we can do). The last element, which we set to 0 here, is the <code>Q</code> value, which is where we store our training results.</p></li> <li>The following set of functions helps us to command the arm:<pre class="source-code">
 def sndCmd(self,msgStr):
     msg = String()
     msg.data = msgStr
     self.cmpPub.publish(msg)
 def setHandAngle(self,ang):
     msg = Int32()
     msg.data = ang
     self.wristPub.publish(msg)
 def armPosCallback(self,msg):
     self.currentArmPos = msg.data
 def setActionPairs(self,pairs):
     self.stateActionPairs = pairs</pre><p class="list-inset"><code>sndCmd</code> (send command) publishes on the <code>RobotCmd</code> topic and sets arm modes. <code>SetHandAngle</code>, as you expect, sets the angle of the wrist servo. <code>armPosCallback</code> receives the arm’s current position, which is published by the arm <a id="_idIndexMarker411"/>control program. <code>setActionPairs</code> allows us to create new action pairs to learn.</p></li> <li>Now we are ready to do the arm training. This is a combined human and robot activity, and is really a lot of fun to do. We’ll try the same aspect 20 times:<pre class="source-code">
 def training(self, aspect):
   # get the aspect from the vision system
   #aspect = 1.0 # start here
   stateActionPairs.sort(key=sortByQ) # sort by Q value
   if len(stateActionPairs)&lt;1:
     #error - no aspects found!
     #
     self.get_logger().error("qLearningHand No Aspect for
     Training")
     return
   else:
     mySetup = stateActionPairs[0] # using the highest q value
     handAngle = mySetup[1]
     myOldQ = mySetup[2]</pre><p class="list-inset">This initiates <a id="_idIndexMarker412"/>the training program on the robot arm. We start by training based on aspect. We first look at our <code>stateActionPairs</code> to sort on the highest <code>Q</code> value for this aspect. We use our custom <code>SortbyQ</code> function to sort the list of <code>stateActionPairs</code>. We set the hand angle to the angle with the highest <code>Q</code>, or expected reward.</p></li> <li>This part of the program is the physical motion the robot arm will go through:<pre class="source-code">
     sndCmd("ARM MID_CARRY")
     timer.pause(1.0)
     sndCmd("ARM GRASP")
     time.sleep(1.0)
     setHandAngle(handAngle)
     time.sleep(0.3)
     # close the gripper
     sndCmd("ARM GRASP_CLOSE")
     time.sleep(0.5)
     # now raise the arm
     sndCmd("ARM MID_CARRY")
     time.sleep(1.0)</pre><p class="list-inset">We start by telling the arm to move to the <em class="italic">Mid Carry</em> position – halfway up. Then we wait 1 second for the arm to complete its motion, and then we move the arm to the grasp position. The next step moves the wrist to the angle that we got from the <code>Q</code> function. Then we close the gripper with the <code>ARM GRASP_CLOSE</code> command. Now we raise the arm to see whether the gripper can lift the toy, using the <code>ARM MID_CARRY</code> instruction. If we are successful, the robot arm will now be holding toy. If not, the gripper will be empty.</p></li> <li>Now we <a id="_idIndexMarker413"/>can check to see whether the gripper has an object in it:<pre class="source-code">
     #check to see if grip is OK
     handPos = self.currentArmPos[0]
     gripSuccess = False
     if handPos &gt; 650: ## fail
           gripSuccess = -1 # reward value of not gripping
     else: # success!
           gripSuccess = +1 # reward value of gripping</pre><p class="list-inset">If the grip of the robot hand is correct, the toy will prevent the gripper from closing. We check the hand position (which the arm sends twice a second) to see the position. For my particular arm, the position that corresponds to 650 servo units or greater is completely closed. Your arm may vary, so check to see what the arm reports for a fully closed and empty gripper. We set the <code>gripSuccess</code> variable as appropriate.</p></li> <li>Now we do the machine learning part. We use my special modified Bellman equation introduced in the <em class="italic">Machine learning for robot arms</em> section to adjust the Q value for this state-action pair:<pre class="source-code">
 # the Bellman Equation
 ### Q(s, a) = Q(s, a) + α * [R + γ * max(Q(s', a')) - Q(s, a)]
 newQ = myOldQ + (learningRate*(gripSuccess))
 mySetup[2]=newQ</pre><p class="list-inset">Since we are not using a future reward value (we get the complete reward from this one action of closing the gripper and raising the arm), we don’t need the expected future reward, only a present reward. We multiply the <code>gripSuccess</code> value (<code>+1</code> or <code>-1</code>) by the learning rate and add this to the old Q score to get a new Q score. Each success increments the reward while any failure leads to a decrement.</p></li> <li>To finish <a id="_idIndexMarker414"/>our learning function, we insert the updated Q value back into the learning table that matches the aspect angle and the wrist angle we tested:<pre class="source-code">
 foundStateActionPair = False
 # re insert back into q learning array
 for i in range (0,len(stateActionPairs):
     thisStateAction = stateActionPairs[i]
     if thisStateAction[0] == mySetup[0] and 
thisStateAction[1] == mySetup[0]:
         foundStateActionPair=True
         stateActionPairs[2]=mySetup[2] # store the new q value in the table
     if not foundStateActionPair:
         # we don't have this in the table - let's add it
         stateActionPairs.append(mySetup)
 input("Reset and Press Enter") # wait for enter key to continue</pre><p class="list-inset">If this state-action pair is not in the table (which it should be), then we add it. I put this in just to keep the program from erroring out if we give a strange arm angle. Finally, we pause the program and wait for the user to hit the <em class="italic">Enter</em> key in order to continue.</p></li> <li>Let’s now look at the rest of the program, which is pretty straightforward. We have to do some housekeeping, service some calls, and make our main training loop:<pre class="source-code">
 def cmdCallBack(self,msg):
   robotCmd = msg.data
   if robotCmd == "GoLearnHand":
     self.mode = "start"
   if robotCmd == "StopLearnHand":
     self.mode = "idle"</pre><p class="list-inset">This <code>cmdCallBack</code> receives commands from the <code>RobotCmd</code> topic. The only two commands we service in this program are <code>GoLearnHand</code>, which starts the learning process, and <code>StopLearnHand</code>, which lets you stop training.</p></li> <li>This section <a id="_idIndexMarker415"/>is our arm interface to the robot arm and sets up the publish/subscribe interface we need to command the arm:<pre class="source-code">
class ArmInterface():
 init(self):
   self.armPosSub = self.create_subscription(Int32MultiArray, 'xarm_pos',self.armPosCallback, 10)
   self.armAngSub = self.create_subscription(Int32MultiArray, 'xarm_angle',self.armAngCallback, 10)
   self.armPosPub = self.create_publisher(Int32MultiArray, 'xarm')
 def armPosCallback(self,msg):
   self.armPos = msg.data
 def armAngCallback(self, msg):
   self.armAngle = msg.data
   # decoder ring: [grip, wrist angle, wrist pitch, elbow pitch, 
  sholder pitch, sholder yaw]
 def setArmPos(self,armPosArray):
   msg = Int32MultiArray
   msg.data = armPosArray
   self.armPosPub.publish(msg)</pre><p class="list-inset">We subscribe to <code>xarm_pos</code> (arm position in servo units) and <code>xarm_angle</code> (arm position in degrees). I added the ability to set the robot arm position on the <code>xarm</code> topic, but you may not need that.</p><p class="list-inset">For each <a id="_idIndexMarker416"/>subscription we need a callback function. We have <code>armPosCallback</code> and <code>armAngleCallback,</code> which will be called when the arm publishes its position, which I set to happen at 2 Hertz, or twice a second. You can increase this rate in the <code>xarm_mgr</code> program if you feel it necessary.</p></li> <li>Now we get to the main program. For a lot of ROS programs, this main section is pretty brief. We have an extra routine we need to put here. To save the training function after we do our training, I came up with this solution – to <em class="italic">pickle</em> the state-action pairs and put them into a file:<pre class="source-code">
### MAIN ####
# persistent training file to opeate the arm
ArmTrainingFileName = "armTrainingFile.txt"
armIf = ArmInterface()
armTrainer = LearningHand()
#open and read the file after the appending:
try:
 f = open(ArmTrainingFileName, "r")
 savedActionPairs = pickle.load(f)
 armTrainer.setActionPairs(savedActionPairs)
 f.close()
except:
 print("No Training file found")
 self.get_logger().error("qLearningHand No Training File Found armTrainingFile.txt")</pre><p class="list-inset">When we run this program, we need to load this file and set our action-pairs table to these saved values. I set up a <code>try</code>/<code>except</code> block to send an error message when this training file is not found. This will happen the first time you run the program, but we’ll create a new file in just a moment for next time.</p><p class="list-inset">We also <a id="_idIndexMarker417"/>instantiate our class variables for the arm trainer and the arm interface, which creates the main part of our training program.</p></li> <li>This is the meat of our training loop. We set the aspect and number of trial repetitions that we train on:<pre class="source-code">
aspectTest = [1.0, 0.5, 1.5,2]
trainingKnt = 20
for jj in aspectTest:
 for ii in range(0,trainingKnt):
   print("Starting Training on Aspect ", jj)
   armTrainer.training(jj)</pre><p class="list-inset">Start with the toy parallel to the front of the robot. Do 20 trials of picking it up, and then move the toy 45 degrees to the right for the next part. Then perform 20 more trials. Then the toy is moved to be 90 degrees to the robot. Run 20 trials. Finally set the toy at -45 degrees (to the left) for the final set and run 20 times. Welcome to machine learning!</p></li> <li>You’ll probably guess that the last thing we do is save our training data, like this:<pre class="source-code">
f = open("ArmTrainingFileName", "w")
# open file in write mode
pickle.dump(armTrainer.stateActionPairs,f)
print("Arm Training File Written")
f.close()</pre></li> </ol>
<p>This completes our training program. Repeat this training for as many types of toys as you have, and you should have a trained arm that consistently picks up toys at a variety of angles <a id="_idIndexMarker418"/>to the robot. Start with a selection of toys you want the robot to pick up. Set the angle of the toy to the robot at 0 – let’s say this is with the longest part of the toy parallel to the front of the robot. Then we send <code>GoLearnHand</code> on <code>RobotCmd</code> to put the robot arm in learning mode.</p>
<p>We have tried out Q-learning in a couple of different configurations, with a limited amount of success in training our robot. The main problem with Q-learning is that we have a very large number of possible states, or positions, that the robot arm can be in. This means that gaining a lot of knowledge about any one position by repeated trials is very difficult. Next, we are going to introduce a different approach using GAs to generate our movement actions.</p>
<h1 id="_idParaDest-89"><a id="_idTextAnchor180"/>Introducing GAs</h1>
<p>Moving the robot arm requires the coordination of three motors simultaneously to create a <a id="_idIndexMarker419"/>smooth movement. We need a mechanism to create different combinations of motor movement for the robot to test. We could just use random numbers, but that would be inefficient and could take thousands of trials to get to the level of training we want.</p>
<p>What if we had a way of trying different combinations of motor movement, and then pitting them against one another to pick the best one? It would be a sort of Darwinian <em class="italic">survival of the fittest</em> for arm movement scripts – such as a GA process. Let’s explore how we can apply this concept to our use case.</p>
<h2 id="_idParaDest-90"><a id="_idTextAnchor181"/>Understanding how the GA process works</h2>
<p>Here are <a id="_idIndexMarker420"/>the steps involved in our GA process:</p>
<ol>
<li>We do a trial run to go from position 1 (neutral carry) to position 2 (pickup). The robot moves the arm 100 times before getting the hand into the right position. Why 100? We need a large enough sample space to allow the algorithm to explore different solutions. With a value of 50, the solution did not converge satisfactorily, while a value of 200 yielded the same result as 100.</li>
<li>We score each movement based on the percentage of goal accomplishment, indicating how much this movement contributed to the goal.</li>
<li>We take the 10 best moves and put them in a database.</li>
<li>We run the test again and do the same thing – now we have 10 more <em class="italic">best moves</em> and 20 moves in the database.</li>
<li>We take the five best from the first set and cross them with the five best from the second set – plus five moves chosen at random and five more made up of totally random moves. Crossing two solutions refers to the process of taking a segment from the first set and a segment from the second set. In genetic terms, this is like taking half the <em class="italic">DNA</em> from each of two <em class="italic">parents</em> to make a new <em class="italic">child</em>.</li>
<li>We run that sequence of moves, and then take the 10 best individual moves and continue on.</li>
</ol>
<p>Through the process of selection, we should quickly get down to a sequence that performs the task. It may not be optimal, but it will work. We are managing our <em class="italic">gene pool</em> (a list of trial solutions to our problem) to create a solution to a problem by successive approximation. We want to keep a good mix of possibilities that can be combined in different ways to solve the problem of moving our arm to its goal.</p>
<p>We can actually use <a id="_idIndexMarker421"/>several methods of <strong class="bold">cross-breeding</strong> our movement sequences. What I described is a simple cross – half the first parent’s genetic material and half the second parent’s material (if you will pardon the biological metaphor). We could instead use quarters – ¼ first, ¼ second, ¼ first, ¼ second – to have two crosses. We could also randomly grab bits from one or the other. We will stick with the half/half strategy for now, but you are free to experiment to your heart’s content. In essence, in all of these options, we are taking a solution, breaking it in half, and randomly combining it with half of a solution from another trial.</p>
<p>You are <a id="_idIndexMarker422"/>about to issue an objection: what if the movement takes less than 10 steps? Easy – when we get to the goal, we stop, and discard the remaining steps.</p>
<p class="callout-heading">Note</p>
<p class="callout">We are not looking for a perfect or optimum task execution, but just something good enough to get the job done. For a lot of real-time robotics, we don’t have the luxury of time to create a perfect solution, so any solution that gets the job done is adequate.</p>
<p>Why did we add the five additional random sample moves, and five totally random moves? This also mimics natural selection – the power of mutation. Our genetic code (the DNA in our bodies) is not perfect, and sometimes inferior material gets passed along. We also experience random mutations from bad copies of genes, cosmic rays, and viruses. We are introducing some random factors to <em class="italic">bump</em> the tuning of our algorithm – the element of natural selection – in case we converge on a local minimum or miss some simple path because it has not occurred yet to our previous movements.</p>
<p>But why on Earth are we going to all this trouble? The GA process can do something very difficult for a piece of software – it can innovate or evolve new solutions out of primitive actions by basically trying stuff until it finds out what works and what does not. We have provided another machine learning process to add to our toolbox, but one that can create solutions we, the programmers, had not preconceived.</p>
<p>Now, let’s dive into the GA process. In the interest of transparency, we are going to build our own GA process from scratch.</p>
<p class="callout-heading">Note</p>
<p class="callout">We’ll be building our own tools in this version, but there are some prebuilt toolsets that can help <a id="_idIndexMarker423"/>you to create GAs, such as <code>pip </code><code>install deap</code>.</p>
<h2 id="_idParaDest-91"><a id="_idTextAnchor182"/>Building a GA process</h2>
<p>We loosely adopt the concept of the <em class="italic">survival of the fittest</em> to decide which plans are the fittest <a id="_idIndexMarker424"/>and get to survive and propagate. I’m giving you a sandbox in which to play genetic engineer, where you have access to all of the parts and nothing is hidden behind the curtain. You will fi<a id="_idTextAnchor183"/>nd that for our problem, the code is not all that complex:</p>
<ol>
<li>We’ll start by creating the <code>computefitness</code> function, the one that scores our genetic material. <strong class="bold">Fitness</strong> is our criteria for grading our algorithm. We can change the fitness to our heart’s content to tailor our output to our needs. In this case, we are making a path in space for the robot arm from the starting location to the ending goal location. We evaluate our path in terms of how close any point of the path comes to our goal. Just as in our previous programs, the movement of the robot is constituted as 27 combinations of the three motors going clockwise, counterclockwise, or not moving. We divide the movement into small steps, each three motor units (1.8 degrees) of so of motion. We string together a whole group of these steps to make a path. The fitness function steps along the path and computes the hand position at each step.</li>
<li>The <code>predictReward</code> function makes a trial computation of where the robot hand has moved as a result of that step. Let’s say we move <em class="italic">Motor 1</em> clockwise three steps, leave <em class="italic">Motor 2</em> alone, and move <em class="italic">Motor 3</em> counterclockwise three steps. This causes the hand to move slightly up and out. We score each step individually by how close it comes to the goal. Our score is computed out of 100; 100 is exactly at the goal, and we take away one point for each 1/100th of the distance the arm is away from the goal, up to a maximum of 340 mm. Why 340? That is the total length of the arm. We score the total movement a bit differently than you might think. Totaling up the rewards make no difference, as we want the point of closest approach to the goal. So we pick the single step with the highest reward and save that value. We throw away any steps after that, since they will only take us further away. Thus we automatically prune our paths to end at the goal.</li>
<li>I used <a id="_idIndexMarker425"/>the term <code>allele</code> to indicate a single step out of the total path, which I called <code>chrom</code>, short for chromosome:<pre class="source-code">
def computeFitness(population, goal, learningRate, initialPos): 
  fitness = []
  gamma = 0.6 
  state=initialPos 
  index = 0
  for chrom in population:
    value=0
    for allele in chrom:
      action = ACTIONMAT[allele]
      indivFit, state =
      predictReward(state,goal,action,learningRate) value += 
      indivFit
      if indivFit &gt; 95:
        # we are at the goal – snip the DNA here 
        break
    fitness.append([value,index]) 
    index += 1
  return fitness</pre></li> <li>How do we create our paths to start with? The <code>make_new_individual</code> function builds our initial population of chromosomes, or paths, out of random numbers. Each chromosome contains a path made up of a number from 0 to 26 that represents all the valid combinations of motor commands. We set the path length to be a random number from 10 to 60:<pre class="source-code">
def make_new_individual():
  # individual length of steps 
  lenInd = random.randint(10,60)
  chrom = [] # chromosome description 
  for ii in range(lenInd):
    chrom.append(randint(26)) 
  return chrom</pre></li> <li>We use the <code>roulette</code> function to pick a portion of our population to continue. Each generation, we select from the top 50% of scoring individuals to donate their DNA <a id="_idIndexMarker426"/>to create the next generation. We want the reward value of the path, or chromosome, to weigh the selection process; the higher the reward score, the better chance of having children. This is part of our selection process:<pre class="source-code">
# select an individual in proportion to its value
def roulette(items):
 total_weight = sum(item[0] 
 for item in items) 
 weight_to_target = random.uniform(0, total_weight) 
 for item in items:
  weight_to_target -= item[0] 
  if weight_to_target &lt;= 0: 
   return item
# main Program
INITIAL_POS = [127,127,127]
GOAL=[-107.39209423, -35.18324771]
robotArm=RobotArm() 
robotArm.setGoal(GOAL) 
population = 300
learningRate = 3
crossover_chance = .50
mutate_chance = .001 
pop = []</pre></li> <li>We start by building our initial population out of random parts. Their original fitness will be very low: about 13% or less. We maintain a pool of 300 individual paths, which we call chromosomes:<pre class="source-code">
for i in range(population): pop.append(make_new_individual())
  trainingData=[] epochs = 100</pre></li> <li>Here we set up the loop to go through 100 generations of our natural selection process. We begin by computing the fitness of each individual and adding that score to <a id="_idIndexMarker427"/>a fitness list with an index pointing back to the chromosome:<pre class="source-code">
for jj in range(epochs):
  # evaluate the population
  fitnessList = computeFitness(pop,GOAL,learningRate, INITIAL_POS)</pre></li> <li>We sort the fitness in inverse order to get the best individuals. The largest number should be first:<pre class="source-code">
fitnessList.sort(reverse=True)</pre></li> <li>We keep the top 50% of the population and discard the bottom 50%. The bottom half is out of the gene pool as being unfit:<pre class="source-code">
fitLen = 150
fitnessList = fitnessList[0:fitLen] # survival of the fittest...</pre></li> <li>We pull out <a id="_idIndexMarker428"/>the top performer from the whole list and put it into the <strong class="bold">hall of fame</strong> (<strong class="bold">HOF</strong>). This will eventually be the output of our process. In the <a id="_idIndexMarker429"/>meantime, we use the HOF or <strong class="bold">HOF fitness</strong> (<strong class="bold">HOFF</strong>) value as a measure of the fitness of this generation:<pre class="source-code">
 hoff = pop[fitnessList[0][1]]
 print("HOF = ",fitnessList[0])</pre></li> <li>We store the HOFF value in a <code>trainingData</code> list so we can graph the results at the end of the program:<pre class="source-code">
trainingData.append(fitnessList[0][0])
newPop = []
for ddex in fitnessList:<a id="_idTextAnchor184"/> newPop.append(pop[ddex[1]])
  print ("Survivors: ",len(newPop))</pre></li> <li>At this phase, we have deleted the bottom 50% of our population, removing the worst performers. Now we need to replace them with the children of the best performers of this generation. We are going to use crossover as our mating technique. There are several types of genetic mating that can produce successful offspring. Crossover is popular and a good place to start, as well as being easy <a id="_idIndexMarker430"/>to code. All we are doing is picking a spot in the genome and taking the first half from one parent, and the second half from the other. We pick our parents to <em class="italic">mate</em> randomly from the <a id="_idIndexMarker431"/>remaining population, weighted proportionally to their fitness. This is referred to as <strong class="bold">roulette wheel selection</strong>. The better individuals are weighted more heavily and are more likely to be selected for breeding. We create 140 new individuals as children of this generation:<pre class="source-code">
# crossover
# pick to individuals at random # on the basis of fitness
numCross = population-len(newPop)-10 print ("New Pop Crossovers",numCross) # #
# add 5 new random individuals for kk in range(10):
newPop.append(make_new_individual()) 
for kk in range(int(numCross)):
 p1 = roulette(fitnessList)[1] 
 p2 = roulette(fitnessList)[1]
 chrom1 = pop[p1]
 chrom2 = pop[p2]
 lenChrom = min(len(chrom1),len(chrom2)) xover = 
 randint(lenChrom)
 # xover is the point where the chromosomes cross over newChrom 
 = chrom1[0:xover]+chrom2[xover:]</pre></li> <li>Our next step is <strong class="bold">mutation</strong>. In real natural selection, there is a small chance that DNA will get corrupted or changed by cosmic rays, miscopying of the sequence, or other factors. Some mutations are beneficial, and some are not. We create our version <a id="_idIndexMarker432"/>of this process by having a small chance (1/100 or so) that one gene in our new child path is randomly changed into some other value:<pre class="source-code">
# now we do mutation bitDex = 0
for kk in range(len(newChrom)-1): 
  mutDraw = random.random()
  if mutDraw &lt; mutate_chance: # a mutation has occured! 
   bit = randint<a id="_idTextAnchor185"/>(26) 
   newChrom[kk]=bit
   print ("mutation") 
newPop.append(newChrom)</pre></li> <li>Now that we have done all our processing, we add this new child path to our population, and get ready for the next generation to be evaluated. We record some data and loop back to the start:<pre class="source-code">
# welcome the new baby from parent 1 (p1) and parent 2 (p2) print("Generation: ",jj,"New population = ",len(newPop)) pop=newPop
mp.plot(trainingData) mp.show()</pre></li> </ol>
<p>So, how did we do with our mad genetic experiment? The following output chart speaks for itself:</p>
<div><div><img alt="Figure 5.5 – Learning curve for the ﻿GA solution" src="img/B19846_05_5.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – Learning curve for the GA solution</p>
<p>The GA, for all it seems like a bit of voodoo programming, works quite well as a machine learning tool for this specific case of training our robot arm. Our solution peaked at 99.76% of the goal (about 2 mm) after just 90 generations or so, which is quite fast for an AI learning process. You can see the smooth nature of the learning that shows that this approach <a id="_idIndexMarker433"/>can be used to solve path-planning problems for our robot arm. I have to admit that I was quite skeptical about this process, bu<a id="_idTextAnchor186"/>t it seems to work quite well for this particular problem domain.</p>
<p>The programming really was not too hard, and you can spend some time improving the process by tweaking the parameters of the GA. What if we had a smaller population? What if w<a id="_idTextAnchor187"/>e changed the fitnes<a id="_idTextAnchor188"/>s criteria? Get in there, muck about, and see what you can learn.</p>
<h1 id="_idParaDest-92"><a id="_idTextAnchor189"/>Alternative robot arm ML approaches</h1>
<p>The realm of robot arm control via machine learning is really just getting started. There are a couple <a id="_idIndexMarker434"/>of research avenues I wanted to bring to your attention as you look for further study. One way to approach our understanding of robot movement is to consider the balance between <em class="italic">exploitation</em> and <em class="italic">exploration</em>. Exploitation is getting the robot to its goal as quickly as possible. Exploration is using the space around the robot to try new things. The path-planning program may have been stuck on a local minimum (think of this as a blind alley), and there could be better, more optimal solutions available that had not been considered.</p>
<p>There is also more than one way to teach a robot. We have been using a form of self-exploration in our training. What if we could show the robot what to do and have it learn by example? We could <a id="_idTextAnchor190"/>let the robot observe a human doing the same task, and have it try to emulate the results.<a id="_idTextAnchor191"/> Let’s discuss some alternative methods in the following sections.</p>
<h2 id="_idParaDest-93"><a id="_idTextAnchor192"/>Google’s SAC-X</h2>
<p>Google is <a id="_idIndexMarker435"/>trying a slightly different approach to the robot arm problem. In their <strong class="bold">Scheduled Auxiliary Control</strong> (<strong class="bold">SAC- X</strong>) program, they surmise that it can be quite difficult to assign reward points to individual <a id="_idIndexMarker436"/>movements of the robot arm. They break down a complex task into smaller auxiliary tasks, and give reward points for those supporting <a id="_idIndexMarker437"/>tasks to let the robot build up to a complicated challenge. If we were stacking blocks with a robot arm, we might separate picking up the block as one task, moving with the block in hand as another, and so on. Google referred to this as a <em class="italic">sparse reward</em> problem if reinforcement was only used on the main task, stacking a block on top of another. You can imagine how, in the process of teaching a robot to stack blocks, there would be thous<a id="_idTextAnchor193"/><a id="_idTextAnchor194"/><a id="_idTextAnchor195"/>ands of failed attempts before a successful move resulted in a reward.</p>
<h2 id="_idParaDest-94"><a id="_idTextAnchor196"/>Amazon Robotics Challenge</h2>
<p>Amazon has <a id="_idIndexMarker438"/>millions and millions of boxes, parts, bits, and <a id="_idIndexMarker439"/>other things on its shelves. The company needs to get the stuff from the shelves into small boxes so they can ship it to you as fast as possible when you order it. For the last few years, Amazon has sponsored the <em class="italic">Amazon Robotics Challenge</em>, where teams from universities were invited to use robot arms to pick up items off a shelf and, you guessed it, put them into a box.</p>
<p>When you <a id="_idIndexMarker440"/>consider that Amazon sells almost everything <a id="_idIndexMarker441"/>imaginable, this is a real challenge. In 2017, a team from Queensland, Australia, won <a id="_idTextAnchor197"/>the challenge with a low-cost arm and a really good hand-tracking system.</p>
<h1 id="_idParaDest-95"><a id="_idTextAnchor198"/>Summary</h1>
<p>Our task for this chapter was to use machine learning to teach the robot how to use its robot arm. We used two techniques with some variations. We used a variety of reinforcement learning techniques, or Q-learning, to develop a movement path by selecting individual actions based on the robot’s arm state. Each motion was scored individually as a reward, and as part of the overall path as a value. The process stored the results of the learning in a Q-matrix that could be used to generate a path. We improved our first cut of the reinforcement learning program by indexing, or encoding, the motions from a 27-element array of possible combinations of motors as numbers from 0 to 26, and likewise indexing the robot state to a state lookup table. This resulted in a 40x speedup of the learning process. Our Q-learning approach struggled with the large number of states that the robot arm could be in.</p>
<p>Our second technique was a GA. We created individual random paths to make a population. We created a fitness function to score each path against our goal and kept the top performers from each generation. We then crossed genetic material from two somewhat randomly selected individuals to create a new child path. The GA also simulated mutation by having a slight chance of random changes in the steps of a path. The results for the GA showed no problem with the state space complexity<a id="_idTextAnchor199"/> of our robot arm and generated a valid path after just a few generations.</p>
<p>Why do we go to all of this trouble? We use machine learning techniques when other empirical methods are either difficult, not reliable, or don’t produce solutions in a reasonable amount of time. We can also tackle much more complex tasks with these techniques that might be intractable to a brute-force or math-only solution.</p>
<p>In the next chapter, we’ll be adding a voice interface to the robot with natural language processing, so you can talk to the robot and it will listen – and talk back.</p>
<h1 id="_idParaDest-96"><a id="_idTextAnchor200"/>Questions</h1>
<ol>
<li>In Q-learning, what does the Q stand for?<p class="list-inset"><strong class="bold">Hint</strong>: You will have to research this yourself.</p></li>
<li>What could we do to limit the number of states that the Q-learning algorithm has to search through?</li>
<li>What effect does changing the learning rate have on the learning process?</li>
<li>What function or parameter serves to penalize longer paths in the Q-learning equation? What effect does increasing or decreasing this function have?</li>
<li>In the genetic algorithm, how would you go about penalizing longer paths so that shorter paths (fewer number of steps) would be preferred?</li>
<li>Look up the SARSA variation of Q-learning. How would you implement the SARSA technique into program 2.</li>
<li>What effect does changing the learning rate in the genetic algorithm have? What are the upper and lower bounds of the learning rat<a id="_idTextAnchor201"/>e?</li>
<li>In a genetic algorithm, what effect does reducing the population have?</li>
</ol>
<h1 id="_idParaDest-97"><a id="_idTextAnchor202"/>Further reading</h1>
<ul>
<li><em class="italic">Python Deep Learning</em> by Zocca, Spacagna, Slater, and Roelants, Packt Publishing</li>
<li><em class="italic">Artificial Intelligence with Python</em> by Prateek Joshi, Packt Publishing</li>
<li><em class="italic">AI Junkie: Genetic Algorithm – A Brief Overview</em>, retrieved from <a href="http://www.ai-junkie.com/ga/intro/gat2.html">http://www.ai-junkie.com/ga/intro/gat2.html</a></li>
<li><em class="italic">Basic Reinforcement Learning Tutorial 2: </em><em class="italic">SARSA</em>: <a href="https://github.com/vmayoral/basic_reinforcement_learning/tree/master/tutorial2">https://github.com/vmayoral/basic_reinforcement_learning/tree/master/tutorial2</a></li>
<li><em class="italic">Google DeepMind Blog: Learning b<a id="_idTextAnchor203"/><a id="_idTextAnchor204"/>y Playing (Robot Arm (</em><em class="italic">SAC-X))</em>: <a href="https://deepmind.com/blog/learning-playing/">https://deepmind.com/blog/learning-playing/</a></li>
</ul>
</div>
</body></html>