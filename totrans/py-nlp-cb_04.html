<html><head></head><body>
		<div id="_idContainer012" class="calibre2">
			<h1 id="_idParaDest-104" class="chapter-number"><a id="_idTextAnchor106" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1.1">4</span></h1>
			<h1 id="_idParaDest-105" class="calibre7"><a id="_idTextAnchor107" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.2.1">Classifying Texts</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.3.1">In this chapter, we will be classifying texts using different methods. </span><span class="kobospan" id="kobo.3.2">Classifying texts is a classic NLP problem. </span><span class="kobospan" id="kobo.3.3">This NLP task involves assigning a value to a text, for example, a topic (such as sport or business) or a sentiment, such as negative or positive, and any such task </span><span><span class="kobospan" id="kobo.4.1">needs evaluation.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.5.1">After reading this chapter, you will be able to preprocess and classify texts using keywords, unsupervised clustering, and two supervised algorithms: </span><strong class="bold"><span class="kobospan" id="kobo.6.1">support vector machines</span></strong><span class="kobospan" id="kobo.7.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.8.1">SVMs</span></strong><span class="kobospan" id="kobo.9.1">) and a </span><strong class="bold"><span class="kobospan" id="kobo.10.1">convolutional neural network</span></strong><span class="kobospan" id="kobo.11.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.12.1">CNN</span></strong><span class="kobospan" id="kobo.13.1">) model trained within the spaCy framework. </span><span class="kobospan" id="kobo.13.2">We will also use GPT-3.5 to </span><span><span class="kobospan" id="kobo.14.1">classify texts.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.15.1">For theoretical background on some of the concepts discussed in this section, please refer to </span><em class="italic"><span class="kobospan" id="kobo.16.1">Building Machine Learning Systems with Python</span></em><span class="kobospan" id="kobo.17.1"> by Coelho et al. </span><span class="kobospan" id="kobo.17.2">That book will explain the basics of building a machine learning project, such as training and test sets, as well as metrics used to evaluate such projects, including precision, recall, F1, </span><span><span class="kobospan" id="kobo.18.1">and accuracy.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.19.1">Here is a list of the recipes in </span><span><span class="kobospan" id="kobo.20.1">this chapter:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.21.1">Getting the dataset and </span><span><span class="kobospan" id="kobo.22.1">evaluation ready</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.23.1">Performing rule-based text classification </span><span><span class="kobospan" id="kobo.24.1">using keywords</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.25.1">Clustering sentences using K-Means – unsupervised </span><span><span class="kobospan" id="kobo.26.1">text classification</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.27.1">Using SVMs for supervised </span><span><span class="kobospan" id="kobo.28.1">text classification</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.29.1">Training a spaCy model for supervised </span><span><span class="kobospan" id="kobo.30.1">text classification</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.31.1">Classifying texts using </span><span><span class="kobospan" id="kobo.32.1">OpenAI models</span></span></li>
			</ul>
			<h1 id="_idParaDest-106" class="calibre7"><a id="_idTextAnchor108" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.33.1">Technical requirements</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.34.1">The code for this chapter can be found in the </span><strong class="source-inline"><span class="kobospan" id="kobo.35.1">Chapter04</span></strong><span class="kobospan" id="kobo.36.1"> folder in the GitHub repository of the book (</span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.37.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition</span></a><span class="kobospan" id="kobo.38.1">). </span><span class="kobospan" id="kobo.38.2">As always, we will use the </span><strong class="source-inline"><span class="kobospan" id="kobo.39.1">poetry</span></strong><span class="kobospan" id="kobo.40.1"> environment to install the necessary packages. </span><span class="kobospan" id="kobo.40.2">You can also install the required packages using the provided </span><strong class="source-inline"><span class="kobospan" id="kobo.41.1">requirements.txt</span></strong><span class="kobospan" id="kobo.42.1"> file. </span><span class="kobospan" id="kobo.42.2">We will use the Hugging Face </span><strong class="source-inline"><span class="kobospan" id="kobo.43.1">datasets</span></strong><span class="kobospan" id="kobo.44.1"> package to get datasets that we will use throughout </span><span><span class="kobospan" id="kobo.45.1">the chapter.</span></span></p>
			<h1 id="_idParaDest-107" class="calibre7"><a id="_idTextAnchor109" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.46.1">Getting the dataset and evaluation ready</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.47.1">In this recipe, we will load a </span><a id="_idIndexMarker171" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.48.1">dataset, prepare it for processing, and create an evaluation baseline. </span><span class="kobospan" id="kobo.48.2">This recipe builds on some of the recipes from </span><a href="B18411_03.xhtml#_idTextAnchor067" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.49.1">Chapter 3</span></em></span></a><span class="kobospan" id="kobo.50.1">, where we used </span><a id="_idIndexMarker172" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.51.1">different tools to represent text in a </span><span><span class="kobospan" id="kobo.52.1">computer-readable form.</span></span></p>
			<h2 id="_idParaDest-108" class="calibre5"><a id="_idTextAnchor110" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.53.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.54.1">For this recipe, we will use the Rotten Tomatoes reviews dataset, available through Hugging Face. </span><span class="kobospan" id="kobo.54.2">This dataset consists of user movie reviews that can be classified into positive and negative. </span><span class="kobospan" id="kobo.54.3">We will prepare the dataset for machine learning classification. </span><span class="kobospan" id="kobo.54.4">The preparation process in this case will involve loading the reviews, filtering out non-English language ones, tokenizing the text into words, and removing stopwords. </span><span class="kobospan" id="kobo.54.5">Before the machine learning algorithm can run, the text reviews need to be transformed into vectors. </span><span class="kobospan" id="kobo.54.6">This transformation process is described in detail in </span><a href="B18411_03.xhtml#_idTextAnchor067" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.55.1">Chapter 3</span></em></span></a><span><span class="kobospan" id="kobo.56.1">.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.57.1">The notebook is located </span><span><span class="kobospan" id="kobo.58.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.1_data_preparation.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.59.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.1_data_preparation.ipynb</span></span></a><span><span class="kobospan" id="kobo.60.1">.</span></span></p>
			<h2 id="_idParaDest-109" class="calibre5"><a id="_idTextAnchor111" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.61.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.62.1">We will classify whether the input review is of negative or positive sentiment. </span><span class="kobospan" id="kobo.62.2">We will first filter out non-English text, then tokenize it into words and remove stopwords and punctuation. </span><span class="kobospan" id="kobo.62.3">Finally, we will look at the class distribution and review the most common words in </span><span><span class="kobospan" id="kobo.63.1">each class.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.64.1">Here are </span><span><span class="kobospan" id="kobo.65.1">the steps:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.66.1">Run the simple </span><span><span class="kobospan" id="kobo.67.1">classifier file:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.68.1">
%run -i "../util/util_simple_classifier.ipynb"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.69.1">Import necessary classes. </span><span class="kobospan" id="kobo.69.2">We import the </span><strong class="source-inline1"><span class="kobospan" id="kobo.70.1">detect</span></strong><span class="kobospan" id="kobo.71.1"> function from </span><strong class="source-inline1"><span class="kobospan" id="kobo.72.1">langdetect</span></strong><span class="kobospan" id="kobo.73.1">, which will help us determine</span><a id="_idIndexMarker173" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.74.1"> the language of the review. </span><span class="kobospan" id="kobo.74.2">We also import the </span><strong class="source-inline1"><span class="kobospan" id="kobo.75.1">word_tokenize</span></strong><span class="kobospan" id="kobo.76.1"> function, which we will use to split the reviews into words. </span><span class="kobospan" id="kobo.76.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.77.1">FreqDist</span></strong><span class="kobospan" id="kobo.78.1"> class from NLTK will help us see the most frequent positive and negative words in</span><a id="_idIndexMarker174" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.79.1"> the reviews. </span><span class="kobospan" id="kobo.79.2">We will use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.80.1">stopwords</span></strong><span class="kobospan" id="kobo.81.1"> list, also from NLTK, to filter the stopwords from the text. </span><span class="kobospan" id="kobo.81.2">Finally, the </span><strong class="source-inline1"><span class="kobospan" id="kobo.82.1">punctuation</span></strong><span class="kobospan" id="kobo.83.1"> string from the </span><strong class="source-inline1"><span class="kobospan" id="kobo.84.1">string</span></strong><span class="kobospan" id="kobo.85.1"> package will help us to </span><span><span class="kobospan" id="kobo.86.1">filter punctuation:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.87.1">
from langdetect import detect
from nltk import word_tokenize
from nltk.probability import FreqDist
from nltk.corpus import stopwords
from string import punctuation</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.88.1">Load the training and test datasets using the function from the simple classifier file and print the two dataframes. </span><span class="kobospan" id="kobo.88.2">We see that the data contains a </span><strong class="source-inline1"><span class="kobospan" id="kobo.89.1">text</span></strong><span class="kobospan" id="kobo.90.1"> column and a </span><strong class="source-inline1"><span class="kobospan" id="kobo.91.1">label</span></strong><span class="kobospan" id="kobo.92.1"> column, where the text column </span><span><span class="kobospan" id="kobo.93.1">is lowercase:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.94.1">
(train_df, test_df) = load_train_test_dataset_pd("train", 
    "test")
print(train_df)
print(test_df)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.95.1">The output should look similar </span><span><span class="kobospan" id="kobo.96.1">to this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.97.1">                                                   text  label
0     the rock is destined to be the 21st century's ...      1
1     the gorgeously elaborate continuation of " the...      1
...                                                 ...    ...
</span><span class="kobospan1" id="kobo.97.2">8525  any enjoyment will be hinge from a personal th...      0
8526  if legendary shlockmeister ed wood had ever ma...      0
[8530 rows x 2 columns]
                                                   text  label
0     lovingly photographed in the manner of a golde...      1
1                 consistently clever and suspenseful .      1
...                                                 ...    ...
</span><span class="kobospan1" id="kobo.97.3">1061  a terrible movie that some people will neverth...      0
1062  there are many definitions of 'time waster' bu...      0
[1066 rows x 2 columns]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.98.1">Now we create a new column called </span><strong class="source-inline1"><span class="kobospan" id="kobo.99.1">lang</span></strong><span class="kobospan" id="kobo.100.1"> in the dataframes that will contain the language of the review. </span><span class="kobospan" id="kobo.100.2">We use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.101.1">detect</span></strong><span class="kobospan" id="kobo.102.1"> function to populate this column via the </span><strong class="source-inline1"><span class="kobospan" id="kobo.103.1">apply</span></strong><span class="kobospan" id="kobo.104.1"> method. </span><span class="kobospan" id="kobo.104.2">We</span><a id="_idIndexMarker175" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.105.1"> then filter the dataframe to only contain English-language reviews. </span><span class="kobospan" id="kobo.105.2">The final row counts of the training dataframe before and after the </span><a id="_idIndexMarker176" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.106.1">filtering show us that 178 rows were non-English. </span><span class="kobospan" id="kobo.106.2">This step may take a minute </span><span><span class="kobospan" id="kobo.107.1">to run:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.108.1">
train_df["lang"] = train_df["text"].apply(detect)
train_df = train_df[train_df['lang'] == 'en']
print(train_df)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.109.1">Now the output should look something </span><span><span class="kobospan" id="kobo.110.1">like this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.111.1">                                                   text  label lang
0     the rock is destined to be the 21st century's ...      1   en
1     the gorgeously elaborate continuation of " the...      1   en
...                                                 ...
</span><span class="kobospan1" id="kobo.111.2">    ...  ...
</span><span class="kobospan1" id="kobo.111.3">8528    interminably bleak , to say nothing of boring .      0   en
8529  things really get weird , though not particula...      0   en
[8364 rows x 3 columns]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.112.1">Now we will do the</span><a id="_idIndexMarker177" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.113.1"> same for the </span><span><span class="kobospan" id="kobo.114.1">test </span></span><span><a id="_idIndexMarker178" class="calibre6 pcalibre pcalibre1"/></span><span><span class="kobospan" id="kobo.115.1">dataframe:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.116.1">
test_df["lang"] = test_df["text"].apply(detect)
test_df = test_df[test_df['lang'] == 'en']</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.117.1">Now we will tokenize the text into words. </span><span class="kobospan" id="kobo.117.2">If you get an error saying that the </span><strong class="source-inline1"><span class="kobospan" id="kobo.118.1">english.pickle</span></strong><span class="kobospan" id="kobo.119.1"> tokenizer was not found, run the line </span><strong class="source-inline1"><span class="kobospan" id="kobo.120.1">nltk.download('punkt')</span></strong><span class="kobospan" id="kobo.121.1"> before running the rest of the code. </span><span class="kobospan" id="kobo.121.2">This code is also contained in the </span><strong class="source-inline1"><span class="kobospan" id="kobo.122.1">lang_utils </span></strong><span><strong class="source-inline1"><span class="kobospan" id="kobo.123.1">notebook</span></strong></span><span><span class="kobospan" id="kobo.124.1"> (</span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/lang_utils.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.125.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/lang_utils.ipynb</span></span></a><span><span class="kobospan" id="kobo.126.1">)</span></span><span><span class="kobospan" id="kobo.127.1">:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.128.1">
train_df["tokenized_text"] = train_df["text"].apply(
    word_tokenize)
print(train_df)
test_df["tokenized_text"] = test_df["text"].apply(word_tokenize)
print(test_df)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.129.1">The result </span><a id="_idIndexMarker179" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.130.1">will be similar </span><span><span class="kobospan" id="kobo.131.1">to</span></span><span><a id="_idIndexMarker180" class="calibre6 pcalibre pcalibre1"/></span><span><span class="kobospan" id="kobo.132.1"> this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.133.1">                                                   text  label lang  \
0     the rock is destined to be the 21st century's ...
</span><span class="kobospan1" id="kobo.133.2">      1   en
1     the gorgeously elaborate continuation of " the...      
1   en
...                                                 ...    ...
</span><span class="kobospan1" id="kobo.133.3">  ...
</span><span class="kobospan1" id="kobo.133.4">8528    interminably bleak , to say nothing of boring .      
0   en
8529  things really get weird , though not particula...      
0   en
                                         tokenized_text
0     [the, rock, is, destined, to, be, the, 21st, c...
</span><span class="kobospan1" id="kobo.133.5">1     [the, gorgeously, elaborate, continuation, of,...
</span><span class="kobospan1" id="kobo.133.6">...                                                 ...
</span><span class="kobospan1" id="kobo.133.7">8528  [interminably, bleak, ,, to, say, nothing, of,...
</span><span class="kobospan1" id="kobo.133.8">8529  [things, really, get, weird, ,, though, not, p...
</span><span class="kobospan1" id="kobo.133.9">[8352 rows x 4 columns]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.134.1">In this step, we will remove stopwords and punctuation. </span><span class="kobospan" id="kobo.134.2">First, we load the stopwords using the</span><a id="_idIndexMarker181" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.135.1"> NLTK package. </span><span class="kobospan" id="kobo.135.2">We then add </span><strong class="source-inline1"><span class="kobospan" id="kobo.136.1">'s</span></strong><span class="kobospan" id="kobo.137.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.138.1">``</span></strong><span class="kobospan" id="kobo.139.1"> to the list of stopwords. </span><span class="kobospan" id="kobo.139.2">You </span><a id="_idIndexMarker182" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.140.1">can add other words that you think are also stopwords. </span><span class="kobospan" id="kobo.140.2">We then define a function that will take a list of words as input and filter it, returning a new list that doesn’t contain stopwords or punctuation. </span><span class="kobospan" id="kobo.140.3">Finally, we apply this function to the training and test data. </span><span class="kobospan" id="kobo.140.4">From the printout, we can see that stopwords and punctuation </span><span><span class="kobospan" id="kobo.141.1">were removed:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.142.1">
stop_words = list(stopwords.words('english'))
stop_words.append("``")
stop_words.append("'s")
def remove_stopwords_and_punct(x):
    new_list = [w for w in x if w not in stop_words and w not in punctuation]
    return new_list
train_df["tokenized_text"] = train_df["tokenized_text"].apply(
    remove_stopwords_and_punct)
print(train_df)
test_df["tokenized_text"] = test_df["tokenized_text"].apply(
    remove_stopwords_and_punct)
print(test_df)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.143.1">The result will look</span><a id="_idIndexMarker183" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.144.1"> similar </span><span><span class="kobospan" id="kobo.145.1">to</span></span><span><a id="_idIndexMarker184" class="calibre6 pcalibre pcalibre1"/></span><span><span class="kobospan" id="kobo.146.1"> this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.147.1">                                                   text  label lang  \
0     the rock is destined to be the 21st century's ...
</span><span class="kobospan1" id="kobo.147.2">      1   en
1     the gorgeously elaborate continuation of " the...
</span><span class="kobospan1" id="kobo.147.3">      1   en
...                                                 ...
</span><span class="kobospan1" id="kobo.147.4">    ...  ...
</span><span class="kobospan1" id="kobo.147.5">8528    interminably bleak , to say nothing of boring .
</span><span class="kobospan1" id="kobo.147.6">      0   en
8529  things really get weird , though not particula...
</span><span class="kobospan1" id="kobo.147.7">      0   en
                                         tokenized_text
0     [rock, destined, 21st, century, new, conan, go...
</span><span class="kobospan1" id="kobo.147.8">1     [gorgeously, elaborate, continuation, lord, ri...
</span><span class="kobospan1" id="kobo.147.9">...                                                 ...
</span><span class="kobospan1" id="kobo.147.10">8528        [interminably, bleak, say, nothing, boring]
8529  [things, really, get, weird, though, particula...
</span><span class="kobospan1" id="kobo.147.11">[8352 rows x 4 columns]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.148.1">Now we will check the class balance over both datasets. </span><span class="kobospan" id="kobo.148.2">It is important that the number of items in each class is approximately the same, since if one class dominates, the model can just learn to always assign this dominating class without being wrong much </span><a id="_idIndexMarker185" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.149.1">of </span><span><span class="kobospan" id="kobo.150.1">the time:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.151.1">
print(train_df.groupby('label').count())
print(test_df.groupby('label').count())</span></pre><p class="calibre3"><span class="kobospan" id="kobo.152.1">We see that there </span><a id="_idIndexMarker186" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.153.1">are slightly, but not significantly, more negative reviews in the training data than positive, and the numbers are nearly equal in </span><span><span class="kobospan" id="kobo.154.1">test data.</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.155.1">text  lang  tokenized_text
label
0      4185  4185            4185
1      4167  4167            4167
       text  lang  tokenized_text
label
0       523   523             523
1       522   522             522</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.156.1">Let’s now save the cleaned data </span><span><span class="kobospan" id="kobo.157.1">to disk:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.158.1">
train_df.to_json("../data/rotten_tomatoes_train.json")
test_df.to_json("../data/rotten_tomatoes_test.json")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.159.1">In this step, we define a function that will take a list of words and the number of words as input and return a </span><strong class="source-inline1"><span class="kobospan" id="kobo.160.1">FreqDist</span></strong><span class="kobospan" id="kobo.161.1"> object. </span><span class="kobospan" id="kobo.161.2">It will also print out the top </span><em class="italic"><span class="kobospan" id="kobo.162.1">n</span></em><span class="kobospan" id="kobo.163.1"> most frequent words, where </span><em class="italic"><span class="kobospan" id="kobo.164.1">n</span></em><span class="kobospan" id="kobo.165.1"> is passed into the function and is </span><strong class="source-inline1"><span class="kobospan" id="kobo.166.1">200</span></strong> <span><span class="kobospan" id="kobo.167.1">by default:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.168.1">
def get_stats(word_list, num_words=200):
    freq_dist = FreqDist(word_list)
    print(freq_dist.most_common(num_words))
    return freq_dist</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.169.1">Now let’s use the preceding function and show the most common words in positive and negative reviews to </span><a id="_idIndexMarker187" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.170.1">see whether there are significant vocabulary differences between</span><a id="_idIndexMarker188" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.171.1"> the two classes. </span><span class="kobospan" id="kobo.171.2">We create two lists of words, one for positive and one for negative reviews. </span><span class="kobospan" id="kobo.171.3">We first filter the dataframe by label and then use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.172.1">sum</span></strong><span class="kobospan" id="kobo.173.1"> function to get the words from all </span><span><span class="kobospan" id="kobo.174.1">the reviews:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.175.1">
positive_train_words = train_df[
    train_df["label"] == 1].tokenized_text.sum()
negative_train_words = train_df[
    train_df["label"] == 0].tokenized_text.sum()
positive_fd = get_stats(positive_train_words)
negative_fd = get_stats(negative_train_words)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.176.1">In the output, we see that the words </span><strong class="source-inline"><span class="kobospan" id="kobo.177.1">film</span></strong><span class="kobospan" id="kobo.178.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.179.1">movie</span></strong><span class="kobospan" id="kobo.180.1"> and some other words also act as stopwords in this case, as they are the most common words in both sets. </span><span class="kobospan" id="kobo.180.2">We can add them to the stopwords list in step 7 and redo </span><span><span class="kobospan" id="kobo.181.1">the cleaning:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.182.1">[('film', 683), ('movie', 429), ("n't", 286), ('one', 280), ('--', 271), ('like', 209), ('story', 194), ('comedy', 160), ('good', 150), ('even', 144), ('funny', 137), ('way', 135), ('time', 127), ('best', 126), ('characters', 125), ('make', 124), ('life', 124), ('much', 122), ('us', 122), ('love', 118), ...]
[('movie', 641), ('film', 557), ("n't", 450), ('like', 354), ('one', 293), ('--', 264), ('story', 189), ('much', 177), ('bad', 173), ('even', 160), ('time', 146), ('good', 143), ('characters', 138), ('little', 137), ('would', 130), ('never', 122), ('comedy', 121), ('enough', 107), ('really', 105), ('nothing', 103), ('way', 102), ('make', 101), ...]</span></pre></li>			</ol>
			<h1 id="_idParaDest-110" class="calibre7"><a id="_idTextAnchor112" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.183.1">Performing rule-based text classification using keywords</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.184.1">In this recipe, we will use the </span><a id="_idIndexMarker189" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.185.1">vocabulary of the text to classify the Rotten Tomatoes reviews. </span><span class="kobospan" id="kobo.185.2">We will create a simple classifier that </span><a id="_idIndexMarker190" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.186.1">will have a vectorizer for each class. </span><span class="kobospan" id="kobo.186.2">That vectorizer will include the words characteristic to that class. </span><span class="kobospan" id="kobo.186.3">The classification will simply be vectorizing the text using each of the vectorizers and then using the class that has </span><span><span class="kobospan" id="kobo.187.1">more words.</span></span></p>
			<h2 id="_idParaDest-111" class="calibre5"><a id="_idTextAnchor113" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.188.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.189.1">We will use the </span><strong class="source-inline"><span class="kobospan" id="kobo.190.1">CountVectorizer</span></strong><span class="kobospan" id="kobo.191.1"> class and the </span><strong class="source-inline"><span class="kobospan" id="kobo.192.1">classification_report</span></strong><span class="kobospan" id="kobo.193.1"> function from </span><strong class="source-inline"><span class="kobospan" id="kobo.194.1">sklearn</span></strong><span class="kobospan" id="kobo.195.1">, as well as the </span><strong class="source-inline"><span class="kobospan" id="kobo.196.1">word_tokenize</span></strong><span class="kobospan" id="kobo.197.1"> method from NLTK. </span><span class="kobospan" id="kobo.197.2">All of these are included in the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.198.1">poetry</span></strong></span><span><span class="kobospan" id="kobo.199.1"> environment.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.200.1">The notebook is located </span><span><span class="kobospan" id="kobo.201.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.2_rule_based.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.202.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.2_rule_based.ipynb</span></span></a><span><span class="kobospan" id="kobo.203.1">.</span></span></p>
			<h2 id="_idParaDest-112" class="calibre5"><a id="_idTextAnchor114" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.204.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.205.1">In this recipe, we will create a separate vectorizer for each class. </span><span class="kobospan" id="kobo.205.2">We will then use those vectorizers to count the number of each class word in each review to </span><span><span class="kobospan" id="kobo.206.1">classify it:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.207.1">Run the simple </span><span><span class="kobospan" id="kobo.208.1">classifier file:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.209.1">
%run -i "../util/util_simple_classifier.ipynb"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.210.1">Do the </span><span><span class="kobospan" id="kobo.211.1">necessary imports:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.212.1">
from nltk import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import classification_report</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.213.1">Load the cleaned data from disk. </span><span class="kobospan" id="kobo.213.2">If you receive a </span><strong class="source-inline1"><span class="kobospan" id="kobo.214.1">FileNotFoundError</span></strong><span class="kobospan" id="kobo.215.1"> error at this step, you need to run the previous recipe, </span><em class="italic"><span class="kobospan" id="kobo.216.1">Getting the dataset and evaluation ready</span></em><span class="kobospan" id="kobo.217.1">, first, since those</span><a id="_idIndexMarker191" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.218.1"> files were created there after cleaning </span><span><span class="kobospan" id="kobo.219.1">the data:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.220.1">
train_df = pd.read_json("../data/rotten_tomatoes_train.json")
test_df = pd.read_json("../data/rotten_tomatoes_test.json")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.221.1">Here we create a list of </span><a id="_idIndexMarker192" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.222.1">words unique to each class. </span><span class="kobospan" id="kobo.222.2">We first concatenate all the words from the </span><strong class="source-inline1"><span class="kobospan" id="kobo.223.1">text</span></strong><span class="kobospan" id="kobo.224.1"> column, filtering on the relevant </span><strong class="source-inline1"><span class="kobospan" id="kobo.225.1">label</span></strong><span class="kobospan" id="kobo.226.1"> value (</span><strong class="source-inline1"><span class="kobospan" id="kobo.227.1">0</span></strong><span class="kobospan" id="kobo.228.1"> for negative reviews and </span><strong class="source-inline1"><span class="kobospan" id="kobo.229.1">1</span></strong><span class="kobospan" id="kobo.230.1"> for positive ones). </span><span class="kobospan" id="kobo.230.2">We then get the words that appear in both of those lists in the </span><strong class="source-inline1"><span class="kobospan" id="kobo.231.1">word_intersection</span></strong><span class="kobospan" id="kobo.232.1"> variable. </span><span class="kobospan" id="kobo.232.2">Finally, we create filtered word lists, one for each class, that do not contain words that appear in both classes. </span><span class="kobospan" id="kobo.232.3">Basically, we delete all the words that appear in both positive and negative reviews from the </span><span><span class="kobospan" id="kobo.233.1">respective lists:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.234.1">
positive_train_words = train_df[train_df["label"] 
    == 1].text.sum()
negative_train_words = train_df[train_df["label"] 
    == 0].text.sum()
word_intersection = set(positive_train_words) \ 
    &amp; set(negative_train_words)
positive_filtered = list(set(positive_train_words) 
    - word_intersection)
negative_filtered = list(set(negative_train_words) 
    - word_intersection)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.235.1">Next, we define a function to create vectorizers, one for each class. </span><span class="kobospan" id="kobo.235.2">The input to this function is a list of lists, where each one is a list of words that only appear in that class; we created these in the previous step. </span><span class="kobospan" id="kobo.235.3">For each of the word lists, we create a </span><strong class="source-inline1"><span class="kobospan" id="kobo.236.1">CountVectorizer</span></strong><span class="kobospan" id="kobo.237.1"> object that takes the word list as the </span><strong class="source-inline1"><span class="kobospan" id="kobo.238.1">vocabulary</span></strong><span class="kobospan" id="kobo.239.1"> parameter. </span><span class="kobospan" id="kobo.239.2">Providing this ensures that we only count those words for the purpose </span><span><span class="kobospan" id="kobo.240.1">of classification:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.241.1">
def create_vectorizers(word_lists):
    vectorizers = []
    for word_list in word_lists:
        vectorizer = CountVectorizer(vocabulary=word_list)
        vectorizers.append(vectorizer)
    return vectorizers</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.242.1">Create the vectorizers using the </span><span><span class="kobospan" id="kobo.243.1">preceding function:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.244.1">
vectorizers = create_vectorizers([negative_filtered,
    positive_filtered])</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.245.1">In this step, we create a </span><strong class="source-inline1"><span class="kobospan" id="kobo.246.1">vectorize</span></strong><span class="kobospan" id="kobo.247.1"> function that takes in a list of words and a list of vectorizers. </span><span class="kobospan" id="kobo.247.2">We first </span><a id="_idIndexMarker193" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.248.1">create a string from the word list, as the vectorizer expects a string. </span><span class="kobospan" id="kobo.248.2">For </span><a id="_idIndexMarker194" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.249.1">each vectorizer in the list, we apply it to the text and then sum the total count of words in that vectorizer. </span><span class="kobospan" id="kobo.249.2">Finally, we append that sum to a list of scores. </span><span class="kobospan" id="kobo.249.3">This will count words in the input per class. </span><span class="kobospan" id="kobo.249.4">We return this score list at the end of </span><span><span class="kobospan" id="kobo.250.1">the function:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.251.1">
def vectorize(text_list, vectorizers):
    text = " ".join(text_list)
    scores = []
    for vectorizer in vectorizers:
        output = vectorizer.transform([text])
        output_sum = sum(output.todense().tolist()[0])
        scores.append(output_sum)
    return scores</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.252.1">In this step, we define the </span><strong class="source-inline1"><span class="kobospan" id="kobo.253.1">classify</span></strong><span class="kobospan" id="kobo.254.1"> function, which takes a list of scores returned by the </span><strong class="source-inline1"><span class="kobospan" id="kobo.255.1">vectorize</span></strong><span class="kobospan" id="kobo.256.1"> function. </span><span class="kobospan" id="kobo.256.2">This function simply selects the maximum score from the list and returns</span><a id="_idIndexMarker195" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.257.1"> the index of that score corresponding to the </span><span><span class="kobospan" id="kobo.258.1">class label:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.259.1">
def classify(score_list):
    return max(enumerate(score_list),key=lambda x: x[1])[0]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.260.1">Here, we apply the preceding</span><a id="_idIndexMarker196" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.261.1"> functions to the training data. </span><span class="kobospan" id="kobo.261.2">We first vectorize the text and then classify it. </span><span class="kobospan" id="kobo.261.3">We create a new column for the result </span><span><span class="kobospan" id="kobo.262.1">called </span></span><span><strong class="source-inline1"><span class="kobospan" id="kobo.263.1">prediction</span></strong></span><span><span class="kobospan" id="kobo.264.1">:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.265.1">
train_df["prediction"] = train_df["text"].apply(
    lambda x: classify(vectorize(x, vectorizers)))
print(train_df)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.266.1">The output will look similar </span><span><span class="kobospan" id="kobo.267.1">to this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.268.1">                                                   text  label lang  \
0     [rock, destined, 21st, century, new, conan, go...      
1   en
1     [gorgeously, elaborate, continuation, lord, ri...      
1   en
...                                                 ...    ...
</span><span class="kobospan1" id="kobo.268.2">  ...
</span><span class="kobospan1" id="kobo.268.3">8528        [interminably, bleak, say, nothing, boring]      
0   en
8529  [things, really, get, weird, though, particula...      
0   en
      prediction
0              1
1              1
...          ...
</span><span class="kobospan1" id="kobo.268.4">8528           0
8529           0
[8364 rows x 4 columns]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.269.1">Now we measure the performance of the rule-based classifier by printing the classification report. </span><span class="kobospan" id="kobo.269.2">We input the </span><a id="_idIndexMarker197" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.270.1">assigned label and the prediction columns. </span><span class="kobospan" id="kobo.270.2">The result is an</span><a id="_idIndexMarker198" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.271.1"> overall accuracy score </span><span><span class="kobospan" id="kobo.272.1">of 87%:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.273.1">
print(classification_report(train_df['label'], 
    train_df['prediction']))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.274.1">This results in </span><span><span class="kobospan" id="kobo.275.1">the following:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.276.1">              precision    recall  f1-score   support
           0       0.79      0.99      0.88      4194
           1       0.99      0.74      0.85      4170
    accuracy                           0.87      8364
   macro avg       0.89      0.87      0.86      8364
weighted avg       0.89      0.87      0.86      8364</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.277.1">Here we do the same for the test data, and we see a significant reduction in accuracy, down to 62%. </span><span class="kobospan" id="kobo.277.2">This is because </span><a id="_idIndexMarker199" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.278.1">the vocabulary lists that we use to create the vectorizers only come </span><a id="_idIndexMarker200" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.279.1">from the training data and are not exhaustive. </span><span class="kobospan" id="kobo.279.2">They will lead to errors in </span><span><span class="kobospan" id="kobo.280.1">unseen data:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.281.1">
test_df["prediction"] = test_df["text"].apply(
    lambda x: classify(vectorize(x, vectorizers)))
print(classification_report(test_df['label'], 
    test_df['prediction']))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.282.1">The result will be </span><span><span class="kobospan" id="kobo.283.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.284.1">              precision    recall  f1-score   support
           0       0.59      0.81      0.68       523
           1       0.70      0.43      0.53       524
    accuracy                           0.62      1047
   macro avg       0.64      0.62      0.61      1047
weighted avg       0.64      0.62      0.61      1047</span></pre></li>			</ol>
			<h1 id="_idParaDest-113" class="calibre7"><a id="_idTextAnchor115" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.285.1">Clustering sentences using K-Means – unsupervised 
text classification</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.286.1">In this recipe, we will use the</span><a id="_idIndexMarker201" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.287.1"> BBC news dataset. </span><span class="kobospan" id="kobo.287.2">The dataset contains </span><a id="_idIndexMarker202" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.288.1">news pieces sorted by five topics: politics, tech, business, sport, and entertainment. </span><span class="kobospan" id="kobo.288.2">We will apply the unsupervised K-Means algorithm to sort the data into </span><span><span class="kobospan" id="kobo.289.1">unlabeled classes.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.290.1">After you read this recipe, you will be able to create your own unsupervised clustering model that will sort data into several classes. </span><span class="kobospan" id="kobo.290.2">You can then later apply it to any text data without having to first </span><span><span class="kobospan" id="kobo.291.1">label it.</span></span></p>
			<h2 id="_idParaDest-114" class="calibre5"><a id="_idTextAnchor116" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.292.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.293.1">We will use the </span><strong class="source-inline"><span class="kobospan" id="kobo.294.1">KMeans</span></strong><span class="kobospan" id="kobo.295.1"> algorithm to create our unsupervised model. </span><span class="kobospan" id="kobo.295.2">It is part of the </span><strong class="source-inline"><span class="kobospan" id="kobo.296.1">sklearn</span></strong><span class="kobospan" id="kobo.297.1"> package and is included in the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.298.1">poetry</span></strong></span><span><span class="kobospan" id="kobo.299.1"> environment.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.300.1">The BBC news dataset as we use it here was uploaded by a Hugging Face user, and the link and the dataset might </span><a id="_idIndexMarker203" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.301.1">change in time. </span><span class="kobospan" id="kobo.301.2">To avoid any potential issues, you can</span><a id="_idIndexMarker204" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.302.1"> use the BBC dataset uploaded to the book’s GitHub repository by loading it from the CSV file provided in the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.303.1">data</span></strong></span><span><span class="kobospan" id="kobo.304.1"> directory.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.305.1">The notebook is located </span><span><span class="kobospan" id="kobo.306.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.3_unsupervised_classification.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.307.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.3_unsupervised_classification.ipynb</span></span></a><span><span class="kobospan" id="kobo.308.1">.</span></span></p>
			<h2 id="_idParaDest-115" class="calibre5"><a id="_idTextAnchor117" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.309.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.310.1">In this recipe, we will preprocess the data, vectorize it, and then cluster it using K-Means. </span><span class="kobospan" id="kobo.310.2">Since there are usually no right answers for unsupervised modeling, evaluating the models is more difficult, but we will be able to look at some statistics, as well as the most common words in all </span><span><span class="kobospan" id="kobo.311.1">the clusters.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.312.1">Your steps should be formatted </span><span><span class="kobospan" id="kobo.313.1">like so:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.314.1">Run the simple </span><span><span class="kobospan" id="kobo.315.1">classification file:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.316.1">
%run -i "../util/util_simple_classifier.ipynb"
%run -i "../util/lang_utils.ipynb"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.317.1">Import the necessary functions </span><span><span class="kobospan" id="kobo.318.1">and packages:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.319.1">
from nltk import word_tokenize
from sklearn.cluster import KMeans
from nltk.probability import FreqDist
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import StratifiedShuffleSplit</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.320.1">We will load the BBC dataset. </span><span class="kobospan" id="kobo.320.2">We use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.321.1">load_dataset</span></strong><span class="kobospan" id="kobo.322.1"> function from Hugging Face’s </span><strong class="source-inline1"><span class="kobospan" id="kobo.323.1">datasets</span></strong><span class="kobospan" id="kobo.324.1"> package. </span><span class="kobospan" id="kobo.324.2">This </span><a id="_idIndexMarker205" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.325.1">function was imported in the simple classifier file we ran in step 1. </span><span class="kobospan" id="kobo.325.2">In the Hugging Face repository, datasets are usually split into training and testing. </span><span class="kobospan" id="kobo.325.3">We will load both, although in unsupervised learning, the test set is usually </span><span><span class="kobospan" id="kobo.326.1">not used:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.327.1">
train_dataset = load_dataset("SetFit/bbc-news", split="train")
test_dataset = load_dataset("SetFit/bbc-news", split="test")
train_df = train_dataset.to_pandas()
test_df = test_dataset.to_pandas()
print(train_df)
print(test_df)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.328.1">The result will look similar </span><span><span class="kobospan" id="kobo.329.1">to this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.330.1">                                                   text  label
     label_text
0     wales want rugby league training wales could f...      2
          sport
1     china aviation seeks rescue deal scandal-hit j...      1
       business
...                                                 ...    ...
</span><span class="kobospan1" id="kobo.330.2">            ...
</span><span class="kobospan1" id="kobo.330.3">1223  why few targets are better than many the econo...      1
       business
1224  boothroyd calls for lords speaker betty boothr...      4
       politics
[1225 rows x 3 columns]
                                                  text  label
     label_text
0    carry on star patsy rowlands dies actress pats...      3
  entertainment
1    sydney to host north v south game sydney will ...      2
          sport
..                                                 ...    ...
</span><span class="kobospan1" id="kobo.330.4">            ...
</span><span class="kobospan1" id="kobo.330.5">998  stormy year for property insurers a string of ...      1
       business
999  what the election should really be about  a ge...      4
       politics
[1000 rows x 3 columns]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.331.1">Now we will check the distribution of items per class for both training and test data. </span><span class="kobospan" id="kobo.331.2">Class balance is important in </span><a id="_idIndexMarker206" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.332.1">classification, as a disproportionally larger class will influence the </span><span><span class="kobospan" id="kobo.333.1">final classifier:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.334.1">
print(train_df.groupby('label_text').count())
print(test_df.groupby('label_text').count())</span></pre><p class="calibre3"><span class="kobospan" id="kobo.335.1">We see that the classes are pretty evenly split, but there are more examples in the </span><strong class="source-inline"><span class="kobospan" id="kobo.336.1">business</span></strong><span class="kobospan" id="kobo.337.1"> and </span><span><strong class="source-inline"><span class="kobospan" id="kobo.338.1">sport</span></strong></span><span><span class="kobospan" id="kobo.339.1"> categories:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.340.1">               text  label
label_text
business        286    286
entertainment   210    210
politics        242    242
sport           275    275
tech            212    212
               text  label
label_text
business        224    224
entertainment   176    176
politics        175    175
sport           236    236
tech            189    189</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.341.1">Since there is almost as much data in the test set as in the training set, we will combine the data and create a better train/test split. </span><span class="kobospan" id="kobo.341.2">We first concatenate the two dataframes. </span><span class="kobospan" id="kobo.341.3">We then create a </span><strong class="source-inline1"><span class="kobospan" id="kobo.342.1">StratifiedShuffleSplit</span></strong><span class="kobospan" id="kobo.343.1"> that will create a train/test split and will do it while preserving the class balance. </span><span class="kobospan" id="kobo.343.2">We specify that we only need one split (</span><strong class="source-inline1"><span class="kobospan" id="kobo.344.1">n_splits</span></strong><span class="kobospan" id="kobo.345.1">) and that the test data needs to be 20% of the whole dataset (</span><strong class="source-inline1"><span class="kobospan" id="kobo.346.1">test_size</span></strong><span class="kobospan" id="kobo.347.1">). </span><span class="kobospan" id="kobo.347.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.348.1">sss</span></strong><span class="kobospan" id="kobo.349.1"> object’s </span><strong class="source-inline1"><span class="kobospan" id="kobo.350.1">split</span></strong><span class="kobospan" id="kobo.351.1"> method returns a generator that contains the indices for the split. </span><span class="kobospan" id="kobo.351.2">We can then use these indices to get new training </span><a id="_idIndexMarker207" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.352.1">and test dataframes. </span><span class="kobospan" id="kobo.352.2">To do that, we filter on the relevant indices and then make a copy of the resulting dataframe slice. </span><span class="kobospan" id="kobo.352.3">If we didn’t make a copy, then we would be working on the original dataframe. </span><span class="kobospan" id="kobo.352.4">We then print out the class counts for both dataframes and see that there is more training and less </span><span><span class="kobospan" id="kobo.353.1">testing data:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.354.1">
combined_df = pd.concat([train_df, test_df],
    ignore_index=True, sort=False)
print(combined_df)
sss = StratifiedShuffleSplit(n_splits=1,
    test_size=0.2, random_state=0)
train_index, test_index = next(
    sss.split(combined_df["text"], combined_df["label"]))
train_df = combined_df[combined_df.index.isin(
    train_index)].copy()
test_df = combined_df[combined_df.index.isin(test_index)].copy()
print(train_df.groupby('label_text').count())
print(test_df.groupby('label_text').count())</span></pre><p class="calibre3"><span class="kobospan" id="kobo.355.1">The result should look </span><span><span class="kobospan" id="kobo.356.1">like</span></span><span><a id="_idIndexMarker208" class="calibre6 pcalibre pcalibre1"/></span><span><span class="kobospan" id="kobo.357.1"> this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.358.1">               text  label  text_tokenized  text_clean  cluster
label_text
business        408    408             408         408      330
entertainment   309    309             309         309      253
politics        333    333             333         333      263
sport           409    409             409         409      327
tech            321    321             321         321      262
               text  label  text_tokenized  text_clean  cluster
label_text
business        102    102             102         102       78
entertainment    77     77              77          77       56
politics         84     84              84          84       70
sport           102    102             102         102       82
tech             80     80              80          80       59</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.359.1">We will now preprocess the data: tokenize it and remove stopwords and punctuation. </span><span class="kobospan" id="kobo.359.2">The functions to do this (</span><strong class="source-inline1"><span class="kobospan" id="kobo.360.1">tokenize</span></strong><span class="kobospan" id="kobo.361.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.362.1">remove_stopword_punct</span></strong><span class="kobospan" id="kobo.363.1">) are imported in the </span><strong class="source-inline1"><span class="kobospan" id="kobo.364.1">language_utils</span></strong><span class="kobospan" id="kobo.365.1"> file we ran in step 1. </span><span class="kobospan" id="kobo.365.2">If you get an error that the </span><strong class="source-inline1"><span class="kobospan" id="kobo.366.1">english.pickle</span></strong><span class="kobospan" id="kobo.367.1"> tokenizer was not found, run the line </span><strong class="source-inline1"><span class="kobospan" id="kobo.368.1">nltk.download('punkt')</span></strong><span class="kobospan" id="kobo.369.1"> before running the rest of the code. </span><span class="kobospan" id="kobo.369.2">This code is also</span><a id="_idIndexMarker209" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.370.1"> contained in the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.371.1">lang_utils notebook</span></strong></span><span><span class="kobospan" id="kobo.372.1">:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.373.1">
train_df = tokenize(train_df, "text")
train_df = remove_stopword_punct(train_df, "text_tokenized")
test_df = tokenize(test_df, "text")
test_df = remove_stopword_punct(test_df, "text_tokenized")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.374.1">In this step, we create the vectorizer. </span><span class="kobospan" id="kobo.374.2">To do that, we get all the words from the training news articles. </span><span class="kobospan" id="kobo.374.3">First, we save the clean text in a separate column, </span><strong class="source-inline1"><span class="kobospan" id="kobo.375.1">text_clean</span></strong><span class="kobospan" id="kobo.376.1">, and then we save the two dataframes to disk. </span><span class="kobospan" id="kobo.376.2">Then we create a TF-IDF vectorizer that will count unigrams, bigrams, and trigrams (the </span><strong class="source-inline1"><span class="kobospan" id="kobo.377.1">ngram_range</span></strong><span class="kobospan" id="kobo.378.1"> parameter). </span><span class="kobospan" id="kobo.378.2">We then fit the vectorizer on the training data only. </span><span class="kobospan" id="kobo.378.3">The reason we fit it only on the training data is that if we fit it on both training and test data, it would lead to data leakage and we would get better test scores than actual performance </span><a id="_idIndexMarker210" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.379.1">on </span><span><span class="kobospan" id="kobo.380.1">unseen data:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.381.1">
train_df["text_clean"] = train_df["text_tokenized"].apply(
    lambda x: " ".join(list(x)))
test_df["text_clean"] = test_df["text_tokenized"].apply(
    lambda x: " ".join(list(x)))
train_df.to_json("../data/bbc_train.json")
test_df.to_json("../data/bbc_test.json")
vec = TfidfVectorizer(ngram_range=(1,3))
matrix = vec.fit_transform(train_df["text_clean"])</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.382.1">Now we can create the </span><strong class="source-inline1"><span class="kobospan" id="kobo.383.1">Kmeans</span></strong><span class="kobospan" id="kobo.384.1"> classifier for five clusters and then fit it on the matrix produced using the vectorizer from the preceding code. </span><span class="kobospan" id="kobo.384.2">We specify the number of clusters using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.385.1">n_clusters</span></strong><span class="kobospan" id="kobo.386.1"> parameter. </span><span class="kobospan" id="kobo.386.2">We also specify that the number of times the algorithm should run is 10 using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.387.1">n_init</span></strong><span class="kobospan" id="kobo.388.1"> parameter. </span><span class="kobospan" id="kobo.388.2">For higher-dimensional problems, it is recommended to do several runs. </span><span class="kobospan" id="kobo.388.3">After initializing the classifier, we fit it on the matrix we created using the vectorizer in step 7. </span><span class="kobospan" id="kobo.388.4">This will create the clustering of the </span><span><span class="kobospan" id="kobo.389.1">training data:</span></span></li>
			</ol>
			<p class="callout-heading"><span class="kobospan" id="kobo.390.1">Note</span></p>
			<p class="callout"><span class="kobospan" id="kobo.391.1">In real-life projects, you will not know the number of clusters in advance, as we do here. </span><span class="kobospan" id="kobo.391.2">You will need to use the elbow method or other methods to estimate the optimal number </span><span><span class="kobospan" id="kobo.392.1">of classes.</span></span></p>
			<pre class="source-code"><span class="kobospan1" id="kobo.393.1">
km = KMeans(n_clusters=5, n_init=10)
km.fit(matrix)</span></pre>			<ol class="calibre13">
				<li value="9" class="calibre14"><span class="kobospan" id="kobo.394.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.395.1">get_most_frequent_words</span></strong><span class="kobospan" id="kobo.396.1"> function will return a list of the most frequent words in a list. </span><span class="kobospan" id="kobo.396.2">The most frequent words list will provide us with a clue as to which topic the text is about. </span><span class="kobospan" id="kobo.396.3">We will use this function to print out the most frequent words in a cluster to understand which topic they refer to. </span><span class="kobospan" id="kobo.396.4">The function takes in input text, tokenizes it, and then creates a </span><strong class="source-inline1"><span class="kobospan" id="kobo.397.1">FreqDist</span></strong><span class="kobospan" id="kobo.398.1"> object. </span><span class="kobospan" id="kobo.398.2">We get the top word</span><a id="_idIndexMarker211" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.399.1"> frequency tuples by using its </span><strong class="source-inline1"><span class="kobospan" id="kobo.400.1">most_common</span></strong><span class="kobospan" id="kobo.401.1"> function and finally get only the word without the frequencies and return this as </span><span><span class="kobospan" id="kobo.402.1">a list:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.403.1">
def get_most_frequent_words(text, num_words):
    word_list = word_tokenize(text)
    freq_dist = FreqDist(word_list)
    top_words = freq_dist.most_common(num_words)
    top_words = [word[0] for word in top_words]
    return top_words</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.404.1">In this step, we define another function, </span><strong class="source-inline1"><span class="kobospan" id="kobo.405.1">print_most_common_words_by_cluster</span></strong><span class="kobospan" id="kobo.406.1">, which uses the </span><strong class="source-inline1"><span class="kobospan" id="kobo.407.1">get_most_frequent_words</span></strong><span class="kobospan" id="kobo.408.1"> function we defined in the previous step. </span><span class="kobospan" id="kobo.408.2">We take the dataframe, the </span><strong class="source-inline1"><span class="kobospan" id="kobo.409.1">KMeans</span></strong><span class="kobospan" id="kobo.410.1"> model, and the number of clusters as input parameters. </span><span class="kobospan" id="kobo.410.2">We then get the list of the clusters assigned to each data point and then create a column in the dataframe that specifies the assigned cluster. </span><span class="kobospan" id="kobo.410.3">For each cluster, we then filter the dataframe to get the text just for that cluster. </span><span class="kobospan" id="kobo.410.4">We use this text to then pass it into the </span><strong class="source-inline1"><span class="kobospan" id="kobo.411.1">get_most_frequent_words</span></strong><span class="kobospan" id="kobo.412.1"> function to get the list of the most frequent words in that cluster. </span><span class="kobospan" id="kobo.412.2">We print the cluster number and the list and return the input dataframe with the added cluster </span><span><span class="kobospan" id="kobo.413.1">number column:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.414.1">
def print_most_common_words_by_cluster(input_df, km, 
    num_clusters):
    clusters = km.labels_.tolist()
    input_df["cluster"] = clusters
    for cluster in range(0, num_clusters):
        this_cluster_text = input_df[
            input_df['cluster'] == cluster]
        all_text = " ".join(
            this_cluster_text['text_clean'].astype(str))
        top_200 = get_most_frequent_words(all_text, 200)
        print(cluster)
        print(top_200)
    return input_df</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.415.1">Here, we use the function we defined in the previous step on the training dataframe. </span><span class="kobospan" id="kobo.415.2">We also pass in the fitted </span><strong class="source-inline1"><span class="kobospan" id="kobo.416.1">KMeans</span></strong><span class="kobospan" id="kobo.417.1"> model and the number of clusters, </span><strong class="source-inline1"><span class="kobospan" id="kobo.418.1">5</span></strong><span class="kobospan" id="kobo.419.1">. </span><span class="kobospan" id="kobo.419.2">The printout gives </span><a id="_idIndexMarker212" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.420.1">us an idea of which cluster is which topic. </span><span class="kobospan" id="kobo.420.2">The cluster numbers might vary, but the cluster that has </span><strong class="source-inline1"><span class="kobospan" id="kobo.421.1">labour</span></strong><span class="kobospan" id="kobo.422.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.423.1">party</span></strong><span class="kobospan" id="kobo.424.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.425.1">election</span></strong><span class="kobospan" id="kobo.426.1"> as the most frequent words is the </span><strong class="source-inline1"><span class="kobospan" id="kobo.427.1">politics</span></strong><span class="kobospan" id="kobo.428.1"> cluster; the cluster with the words </span><strong class="source-inline1"><span class="kobospan" id="kobo.429.1">music</span></strong><span class="kobospan" id="kobo.430.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.431.1">award</span></strong><span class="kobospan" id="kobo.432.1">, and </span><strong class="source-inline1"><span class="kobospan" id="kobo.433.1">show</span></strong><span class="kobospan" id="kobo.434.1"> is the </span><strong class="source-inline1"><span class="kobospan" id="kobo.435.1">entertainment</span></strong><span class="kobospan" id="kobo.436.1"> cluster; the cluster with the words </span><strong class="source-inline1"><span class="kobospan" id="kobo.437.1">game</span></strong><span class="kobospan" id="kobo.438.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.439.1">England</span></strong><span class="kobospan" id="kobo.440.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.441.1">win</span></strong><span class="kobospan" id="kobo.442.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.443.1">play</span></strong><span class="kobospan" id="kobo.444.1">, and </span><strong class="source-inline1"><span class="kobospan" id="kobo.445.1">cup</span></strong><span class="kobospan" id="kobo.446.1"> is the </span><strong class="source-inline1"><span class="kobospan" id="kobo.447.1">sport</span></strong><span class="kobospan" id="kobo.448.1"> cluster; the cluster with the words </span><strong class="source-inline1"><span class="kobospan" id="kobo.449.1">sales</span></strong><span class="kobospan" id="kobo.450.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.451.1">growth</span></strong><span class="kobospan" id="kobo.452.1"> is the </span><strong class="source-inline1"><span class="kobospan" id="kobo.453.1">business</span></strong><span class="kobospan" id="kobo.454.1"> cluster; and the cluster with the words </span><strong class="source-inline1"><span class="kobospan" id="kobo.455.1">software</span></strong><span class="kobospan" id="kobo.456.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.457.1">net</span></strong><span class="kobospan" id="kobo.458.1">, and </span><strong class="source-inline1"><span class="kobospan" id="kobo.459.1">search</span></strong><span class="kobospan" id="kobo.460.1"> is the </span><strong class="source-inline1"><span class="kobospan" id="kobo.461.1">tech</span></strong><span class="kobospan" id="kobo.462.1"> cluster. </span><span class="kobospan" id="kobo.462.2">We also note that the words </span><strong class="source-inline1"><span class="kobospan" id="kobo.463.1">said</span></strong><span class="kobospan" id="kobo.464.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.465.1">Mr</span></strong><span class="kobospan" id="kobo.466.1"> are clearly stopwords, as they appear in most clusters close to </span><span><span class="kobospan" id="kobo.467.1">the top:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.468.1">
print_most_common_words_by_cluster(train_df, km, 5)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.469.1">The results will vary each time you run the training, but they might look like this (</span><span><span class="kobospan" id="kobo.470.1">output truncated):</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.471.1">0
['mr', 'said', 'would', 'labour', 'party', 'election', 'blair', 'government', ...]
1
['film', 'said', 'best', 'also', 'year', 'one', 'us', 'awards', 'music', 'new', 'number', 'award', 'show', ...]
2
['said', 'game', 'england', 'first', 'win', 'world', 'last', 'one', 'two', 'would', 'time', 'play', 'back', 'cup', 'players', ...]
3
['said', 'mr', 'us', 'year', 'people', 'also', 'would', 'new', 'one', 'could', 'uk', 'sales', 'firm', 'growth', ...]
4
['said', 'people', 'software', 'would', 'users', 'mr', 'could', 'new', 'microsoft', 'security', 'net', 'search', 'also', ...]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.472.1">In this step, we use the fitted model to predict the cluster for a test example. </span><span class="kobospan" id="kobo.472.2">We use the text in row 1 of the test dataframe. </span><span class="kobospan" id="kobo.472.3">It is a politics example. </span><span class="kobospan" id="kobo.472.4">We use the vectorizer to turn the text into a</span><a id="_idIndexMarker213" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.473.1"> vector and then use the K-Means model to predict the cluster. </span><span class="kobospan" id="kobo.473.2">The prediction is cluster 0, which in this case </span><span><span class="kobospan" id="kobo.474.1">is correct:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.475.1">
test_example = test_df.iloc[1, test_df.columns.get_loc('text')]
print(test_example)
vectorized = vec.transform([test_example])
prediction = km.predict(vectorized)
print(prediction)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.476.1">The result might look </span><span><span class="kobospan" id="kobo.477.1">like this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.478.1">lib dems  new election pr chief the lib dems have appointed a senior figure from bt to be the party s new communications chief for their next general election effort.  sandy walkington will now work with senior figures such as matthew taylor on completing the party manifesto. </span><span class="kobospan1" id="kobo.478.2">party chief executive lord rennard said the appointment was a  significant strengthening of the lib dem team . </span><span class="kobospan1" id="kobo.478.3">mr walkington said he wanted the party to be ready for any  mischief  rivals or the media tried to throw at it.   my role will be to ensure this new public profile is effectively communicated at all levels   he said.  i also know the party will be put under scrutiny in the media and from the other parties as never before - and we will need to show ourselves ready and prepared to counter the mischief and misrepresentation that all too often comes from the party s opponents.  the party is already demonstrating on every issue that it is the effective opposition.  mr walkington s new job title is director of general election communications.
</span><span class="kobospan1" id="kobo.478.4">[0]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.479.1">Finally, we save the model using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.480.1">joblib</span></strong><span class="kobospan" id="kobo.481.1"> package’s </span><strong class="source-inline1"><span class="kobospan" id="kobo.482.1">dump</span></strong><span class="kobospan" id="kobo.483.1"> function and then load it again using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.484.1">load</span></strong><span class="kobospan" id="kobo.485.1"> function. </span><span class="kobospan" id="kobo.485.2">We check the prediction of the loaded model, and it is the same as</span><a id="_idIndexMarker214" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.486.1"> the prediction of the model in memory. </span><span class="kobospan" id="kobo.486.2">This step will allow us to reuse the model in </span><span><span class="kobospan" id="kobo.487.1">the future:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.488.1">
dump(km, '../data/kmeans.joblib')
km_ = load('../data/kmeans.joblib')
prediction = km_.predict(vectorized)
print(prediction)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.489.1">The result might look </span><span><span class="kobospan" id="kobo.490.1">like this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.491.1">[0]</span></pre></li>			</ol>
			<h1 id="_idParaDest-116" class="calibre7"><a id="_idTextAnchor118" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.492.1">Using SVMs for supervised text classification</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.493.1">In this recipe, we will build a </span><a id="_idIndexMarker215" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.494.1">machine learning classifier that uses the SVM algorithm. </span><span class="kobospan" id="kobo.494.2">By the end of this recipe, you will have a </span><a id="_idIndexMarker216" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.495.1">working classifier that you will be able to test on new inputs and evaluate using the same </span><strong class="source-inline"><span class="kobospan" id="kobo.496.1">classification_report</span></strong><span class="kobospan" id="kobo.497.1"> tools we used in the previous sections. </span><span class="kobospan" id="kobo.497.2">We will use the same BBC news dataset we used with </span><span><strong class="source-inline"><span class="kobospan" id="kobo.498.1">KMeans</span></strong></span><span><span class="kobospan" id="kobo.499.1"> previously.</span></span></p>
			<h2 id="_idParaDest-117" class="calibre5"><a id="_idTextAnchor119" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.500.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.501.1">We will continue working with the same packages that we already installed in the previous recipes. </span><span class="kobospan" id="kobo.501.2">The packages needed are installed in the </span><strong class="source-inline"><span class="kobospan" id="kobo.502.1">poetry</span></strong><span class="kobospan" id="kobo.503.1"> environment or by installing the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.504.1">requirements.txt</span></strong></span><span><span class="kobospan" id="kobo.505.1"> file.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.506.1">The notebook is located </span><span><span class="kobospan" id="kobo.507.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.4-svm_classification.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.508.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.4-svm_classification.ipynb</span></span></a><span><span class="kobospan" id="kobo.509.1">.</span></span></p>
			<h2 id="_idParaDest-118" class="calibre5"><a id="_idTextAnchor120" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.510.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.511.1">We will load the cleaned training and test data that we had saved in the previous recipe. </span><span class="kobospan" id="kobo.511.2">We will then create the SVM classifier and train it. </span><span class="kobospan" id="kobo.511.3">We will use BERT encoding as </span><span><span class="kobospan" id="kobo.512.1">our vectorizer.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.513.1">Your steps should be formatted </span><span><span class="kobospan" id="kobo.514.1">like so:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.515.1">Run the simple </span><span><span class="kobospan" id="kobo.516.1">classifier file:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.517.1">
%run -i "../util/util_simple_classifier.ipynb"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.518.1">Import the necessary functions </span><span><span class="kobospan" id="kobo.519.1">and packages:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.520.1">
from sklearn.svm import SVC
from sentence_transformers import SentenceTransformer
from sklearn.metrics import confusion_matrix</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.521.1">Here, we load the training and test data. </span><span class="kobospan" id="kobo.521.2">If you get a </span><strong class="source-inline1"><span class="kobospan" id="kobo.522.1">FileNotFoundError</span></strong><span class="kobospan" id="kobo.523.1"> error in this step, run steps 1-7 from the previous recipe, </span><em class="italic"><span class="kobospan" id="kobo.524.1">Clustering sentences using K-Means – unsupervised text classification</span></em><span class="kobospan" id="kobo.525.1">. </span><span class="kobospan" id="kobo.525.2">We then shuffle the training data using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.526.1">sample</span></strong><span class="kobospan" id="kobo.527.1"> function. </span><span class="kobospan" id="kobo.527.2">Shuffling</span><a id="_idIndexMarker217" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.528.1"> ensures that we do not have long sequences of data of the</span><a id="_idIndexMarker218" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.529.1"> same class. </span><span class="kobospan" id="kobo.529.2">Finally, we print out the number of counts of examples by class. </span><span class="kobospan" id="kobo.529.3">We see that the classes are more or less balanced, which is important for training </span><span><span class="kobospan" id="kobo.530.1">a classifier:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.531.1">
train_df = pd.read_json("../data/bbc_train.json")
test_df = pd.read_json("../data/bbc_test.json")
train_df.sample(frac=1)
print(train_df.groupby('label_text').count())
print(test_df.groupby('label_text').count())</span></pre><p class="calibre3"><span class="kobospan" id="kobo.532.1">The result will look </span><span><span class="kobospan" id="kobo.533.1">like this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.534.1">               text  label  text_tokenized  text_clean  cluster
label_text
business        231    231             231         231      231
entertainment   181    181             181         181      181
politics        182    182             182         182      182
sport           243    243             243         243      243
tech            194    194             194         194      194
               text  label  text_tokenized  text_clean
label_text
business         58     58              58          58
entertainment    45     45              45          45
politics         45     45              45          45
sport            61     61              61          61
tech             49     49              49          49</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.535.1">Here, we load the sentence transformer </span><strong class="source-inline1"><span class="kobospan" id="kobo.536.1">all-MiniLM-L6-v2</span></strong><span class="kobospan" id="kobo.537.1"> model that will provide the vectors for us. </span><span class="kobospan" id="kobo.537.2">To learn more about the model, please read the </span><em class="italic"><span class="kobospan" id="kobo.538.1">Using BERT and OpenAI embeddings instead of word embeddings</span></em><span class="kobospan" id="kobo.539.1"> recipe in </span><a href="B18411_03.xhtml#_idTextAnchor067" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.540.1">Chapter 3</span></em></span></a><span class="kobospan" id="kobo.541.1">. </span><span class="kobospan" id="kobo.541.2">We then</span><a id="_idIndexMarker219" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.542.1"> define the </span><strong class="source-inline1"><span class="kobospan" id="kobo.543.1">get_sentence_vector</span></strong><span class="kobospan" id="kobo.544.1"> function, which returns the sentence</span><a id="_idIndexMarker220" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.545.1"> embedding for the </span><span><span class="kobospan" id="kobo.546.1">text input:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.547.1">
model = SentenceTransformer('all-MiniLM-L6-v2')
def get_sentence_vector(text, model):
    sentence_embeddings = model.encode([text])
    return sentence_embeddings[0]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.548.1">Define a function that will create an SVM object and train it given input data. </span><span class="kobospan" id="kobo.548.2">It takes in the input vectors and the gold labels, creates an SVC object with the RBF kernel and a regularization parameter of </span><strong class="source-inline1"><span class="kobospan" id="kobo.549.1">0.1</span></strong><span class="kobospan" id="kobo.550.1">, and trains it on the training data. </span><span class="kobospan" id="kobo.550.2">It then returns the </span><span><span class="kobospan" id="kobo.551.1">trained classifier:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.552.1">
def train_classifier(X_train, y_train):
    clf = SVC(C=0.1, kernel='rbf')
    clf = clf.fit(X_train, y_train)
    return clf</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.553.1">In this step, we create the list of labels for the classifier and the </span><strong class="source-inline1"><span class="kobospan" id="kobo.554.1">vectorize</span></strong><span class="kobospan" id="kobo.555.1"> method. </span><span class="kobospan" id="kobo.555.2">We then create the training and test datasets using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.556.1">create_train_test_data</span></strong><span class="kobospan" id="kobo.557.1"> method, which is located in the simple classifier file. </span><span class="kobospan" id="kobo.557.2">We then train the classifier </span><a id="_idIndexMarker221" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.558.1">using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.559.1">train_classifier</span></strong><span class="kobospan" id="kobo.560.1"> function and print the training </span><a id="_idIndexMarker222" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.561.1">and test metrics. </span><span class="kobospan" id="kobo.561.2">We see that the test metrics are really good, all </span><span><span class="kobospan" id="kobo.562.1">above 90%:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.563.1">
target_names=["tech", "business", "sport", 
    "entertainment", "politics"]
vectorize = lambda x: get_sentence_vector(x, model)
(X_train, X_test, y_train, y_test) = create_train_test_data(
    train_df, test_df, vectorize, column_name="text_clean")
clf = train_classifier(X_train, y_train)
print(classification_report(train_df["label"],
        y_train, target_names=target_names))
test_classifier(test_df, clf, target_names=target_names)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.564.1">The output will be </span><span><span class="kobospan" id="kobo.565.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.566.1">               precision    recall  f1-score   support
         tech       1.00      1.00      1.00       194
     business       1.00      1.00      1.00       231
        sport       1.00      1.00      1.00       243
entertainment       1.00      1.00      1.00       181
     politics       1.00      1.00      1.00       182
     accuracy                           1.00      1031
    macro avg       1.00      1.00      1.00      1031
 weighted avg       1.00      1.00      1.00      1031
               precision    recall  f1-score   support
         tech       0.92      0.98      0.95        49
     business       0.95      0.90      0.92        58
        sport       1.00      1.00      1.00        61
entertainment       1.00      0.98      0.99        45
     politics       0.96      0.98      0.97        45
     accuracy                           0.97       258
    macro avg       0.97      0.97      0.97       258
 weighted avg       0.97      0.97      0.96       258</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.567.1">In this step, we print out the confusion matrix to see where the classifier makes mistakes. </span><span class="kobospan" id="kobo.567.2">The rows represent the</span><a id="_idIndexMarker223" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.568.1"> correct labels, and the columns are the predicted labels. </span><span class="kobospan" id="kobo.568.2">We see the</span><a id="_idIndexMarker224" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.569.1"> most confusion (four examples) where the correct label is </span><strong class="source-inline1"><span class="kobospan" id="kobo.570.1">business</span></strong><span class="kobospan" id="kobo.571.1"> but </span><strong class="source-inline1"><span class="kobospan" id="kobo.572.1">tech</span></strong><span class="kobospan" id="kobo.573.1"> is predicted, and where </span><strong class="source-inline1"><span class="kobospan" id="kobo.574.1">business</span></strong><span class="kobospan" id="kobo.575.1"> is the correct label and </span><strong class="source-inline1"><span class="kobospan" id="kobo.576.1">politics</span></strong><span class="kobospan" id="kobo.577.1"> is predicted (two examples). </span><span class="kobospan" id="kobo.577.2">We also see that </span><strong class="source-inline1"><span class="kobospan" id="kobo.578.1">business</span></strong><span class="kobospan" id="kobo.579.1"> is predicted incorrectly for </span><strong class="source-inline1"><span class="kobospan" id="kobo.580.1">tech</span></strong><span class="kobospan" id="kobo.581.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.582.1">entertainment</span></strong><span class="kobospan" id="kobo.583.1">, and </span><strong class="source-inline1"><span class="kobospan" id="kobo.584.1">politics</span></strong><span class="kobospan" id="kobo.585.1"> once each. </span><span class="kobospan" id="kobo.585.2">These errors are also reflected in the metrics, where we see that both recall and precision for </span><strong class="source-inline1"><span class="kobospan" id="kobo.586.1">business</span></strong><span class="kobospan" id="kobo.587.1"> are affected. </span><span class="kobospan" id="kobo.587.2">The only category with perfect scores is </span><strong class="source-inline1"><span class="kobospan" id="kobo.588.1">sport</span></strong><span class="kobospan" id="kobo.589.1"> and it also has zeroes across the confusion matrix everywhere except the intersection of the correct row and predicted column. </span><span class="kobospan" id="kobo.589.2">We can use the confusion matrix to see which categories have the most confusion between themselves and take measures to rectify that </span><span><span class="kobospan" id="kobo.590.1">if needed:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.591.1">
print(confusion_matrix(test_df["label"], test_df["prediction"]))
[[48  1  0  0  0]
 [ 4 52  0  0  2]
 [ 0  0 61  0  0]
 [ 0  1  0 44  0]
 [ 0  1  0  0 44]]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.592.1">We will test the classifier on a </span><a id="_idIndexMarker225" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.593.1">new example. </span><span class="kobospan" id="kobo.593.2">We first vectorize the text and then use the trained model to</span><a id="_idIndexMarker226" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.594.1"> make a prediction and print the prediction. </span><span class="kobospan" id="kobo.594.2">The new article is about tech, and the prediction is class </span><strong class="source-inline1"><span class="kobospan" id="kobo.595.1">0</span></strong><span class="kobospan" id="kobo.596.1">, which is </span><span><span class="kobospan" id="kobo.597.1">indeed </span></span><span><strong class="source-inline1"><span class="kobospan" id="kobo.598.1">tech</span></strong></span><span><span class="kobospan" id="kobo.599.1">:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.600.1">
new_example = """iPhone 12: Apple makes jump to 5G
Apple has confirmed its iPhone 12 handsets will be its first to work on faster 5G networks.
</span><span class="kobospan1" id="kobo.600.2">The company has also extended the range to include a new "Mini" model that has a smaller 5.4in screen.
</span><span class="kobospan1" id="kobo.600.3">The US firm bucked a wider industry downturn by increasing its handset sales over the past year.
</span><span class="kobospan1" id="kobo.600.4">But some experts say the new features give Apple its best opportunity for growth since 2014, when it revamped its line-up with the iPhone 6.
</span><span class="kobospan1" id="kobo.600.5">"5G will bring a new level of performance for downloads and uploads, higher quality video streaming, more responsive gaming, real-time interactivity and so much more," said chief executive Tim Cook.
</span><span class="kobospan1" id="kobo.600.6">…"""
vector = vectorize(new_example)
prediction = clf.predict([vector])
print(prediction))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.601.1">The result will be </span><span><span class="kobospan" id="kobo.602.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.603.1">[0]</span></pre></li>			</ol>
			<h2 id="_idParaDest-119" class="calibre5"><a id="_idTextAnchor121" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.604.1">There’s more…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.605.1">There are many different machine learning algorithms</span><a id="_idIndexMarker227" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.606.1"> that can be used instead of the SVM algorithm. </span><span class="kobospan" id="kobo.606.2">Some of the others include </span><a id="_idIndexMarker228" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.607.1">regression, Naïve Bayes, and decision trees. </span><span class="kobospan" id="kobo.607.2">You can experiment with them and see which ones </span><span><span class="kobospan" id="kobo.608.1">perform better.</span></span></p>
			<h1 id="_idParaDest-120" class="calibre7"><a id="_idTextAnchor122" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.609.1">Training a spaCy model for supervised text classification</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.610.1">In this recipe, we will train a spaCy </span><a id="_idIndexMarker229" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.611.1">model on the BBC dataset, the same dataset we used in the previous recipe, to will predict the </span><span><span class="kobospan" id="kobo.612.1">text category.</span></span></p>
			<h2 id="_idParaDest-121" class="calibre5"><a id="_idTextAnchor123" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.613.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.614.1">We will use the spaCy package to train our model. </span><span class="kobospan" id="kobo.614.2">All the dependencies are taken care of by the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.615.1">poetry</span></strong></span><span><span class="kobospan" id="kobo.616.1"> environment.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.617.1">You will need to download the config file from the book’s GitHub repository, located at </span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/spacy_config.cfg" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.618.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/spacy_config.cfg</span></a><span class="kobospan" id="kobo.619.1">. </span><span class="kobospan" id="kobo.619.2">This file should be located at the path </span><strong class="source-inline"><span class="kobospan" id="kobo.620.1">../data/spacy_config.cfg</span></strong><span class="kobospan" id="kobo.621.1"> with respect to </span><span><span class="kobospan" id="kobo.622.1">the notebook.</span></span></p>
			<p class="callout-heading"><span class="kobospan" id="kobo.623.1">Note</span></p>
			<p class="callout"><span class="kobospan" id="kobo.624.1">You can modify the training config, or generate your own </span><span><span class="kobospan" id="kobo.625.1">at </span></span><a href="https://spacy.io/usage/training" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.626.1">https://spacy.io/usage/training</span></span></a><span><span class="kobospan" id="kobo.627.1">.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.628.1">The notebook is located </span><span><span class="kobospan" id="kobo.629.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.5-spacy_textcat.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.630.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.5-spacy_textcat.ipynb</span></span></a><span><span class="kobospan" id="kobo.631.1">.</span></span></p>
			<h2 id="_idParaDest-122" class="calibre5"><a id="_idTextAnchor124" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.632.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.633.1">The general structure of the training is similar to a plain machine learning model training, where we clean the data, create</span><a id="_idIndexMarker230" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.634.1"> the dataset, and split it into training and testing datasets. </span><span class="kobospan" id="kobo.634.2">We then train a model and test it on </span><span><span class="kobospan" id="kobo.635.1">unseen data:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.636.1">Run the simple </span><span><span class="kobospan" id="kobo.637.1">classifier file:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.638.1">
%run -i "../util/lang_utils.ipynb"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.639.1">Import the necessary functions </span><span><span class="kobospan" id="kobo.640.1">and packages:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.641.1">
import pandas as pd
from spacy.cli.train import train
from spacy.cli.evaluate import evaluate
from spacy.cli.debug_data import debug_data
from spacy.tokens import DocBin</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.642.1">Here we define the </span><strong class="source-inline1"><span class="kobospan" id="kobo.643.1">preprocess_data_entry</span></strong><span class="kobospan" id="kobo.644.1"> function, which will take the input text, its label, and the list of all labels. </span><span class="kobospan" id="kobo.644.2">It will then run the small spaCy model on the text. </span><span class="kobospan" id="kobo.644.3">This model was imported by running the language utilities file in step 1. </span><span class="kobospan" id="kobo.644.4">It is not important which model we use in this step, since we just want to have a </span><strong class="source-inline1"><span class="kobospan" id="kobo.645.1">Doc</span></strong><span class="kobospan" id="kobo.646.1"> object created from the text. </span><span class="kobospan" id="kobo.646.2">That is why we run the smallest model, so it takes less time. </span><span class="kobospan" id="kobo.646.3">We then create a one-hot encoding for the text class, setting the class label to </span><strong class="source-inline1"><span class="kobospan" id="kobo.647.1">1</span></strong><span class="kobospan" id="kobo.648.1"> and the rest to </span><strong class="source-inline1"><span class="kobospan" id="kobo.649.1">0</span></strong><span class="kobospan" id="kobo.650.1">. </span><span class="kobospan" id="kobo.650.2">We then create a label dictionary that maps the category name to its value. </span><span class="kobospan" id="kobo.650.3">We set the </span><strong class="source-inline1"><span class="kobospan" id="kobo.651.1">doc.cats</span></strong><span class="kobospan" id="kobo.652.1"> attribute to this dictionary and return the </span><strong class="source-inline1"><span class="kobospan" id="kobo.653.1">Doc</span></strong><span class="kobospan" id="kobo.654.1"> object. </span><span class="kobospan" id="kobo.654.2">spaCy requires this preprocessing of the data to train a </span><span><span class="kobospan" id="kobo.655.1">classification model:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.656.1">
def preprocess_data_entry(input_text, label, label_list):
    doc = small_model(input_text)
    cats = [0] * len(label_list)
    cats[label] = 1
    final_cats = {}
    for i, label in enumerate(label_list):
        final_cats[label] = cats[i]
    doc.cats = final_cats
    return doc</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.657.1">Now we prepare the training and test datasets. </span><span class="kobospan" id="kobo.657.2">We create the </span><strong class="source-inline1"><span class="kobospan" id="kobo.658.1">DocBin</span></strong><span class="kobospan" id="kobo.659.1"> objects for both training and test data that is required by the spaCy algorithm. </span><span class="kobospan" id="kobo.659.2">We then load the saved data from disk. </span><span class="kobospan" id="kobo.659.3">This is the data we saved in the K-Means recipe. </span><span class="kobospan" id="kobo.659.4">If you get a </span><strong class="source-inline1"><span class="kobospan" id="kobo.660.1">FileNotFoundError</span></strong><span class="kobospan" id="kobo.661.1"> error here, you need to run steps 1-7 from the </span><em class="italic"><span class="kobospan" id="kobo.662.1">Clustering sentences using K-Means – unsupervised text classification</span></em><span class="kobospan" id="kobo.663.1"> recipe. </span><span class="kobospan" id="kobo.663.2">We</span><a id="_idIndexMarker231" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.664.1"> then shuffle the training dataframe. </span><span class="kobospan" id="kobo.664.2">Then we preprocess each data point using the function we defined in the previous step. </span><span class="kobospan" id="kobo.664.3">We then add each datapoint to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.665.1">DocBin</span></strong><span class="kobospan" id="kobo.666.1"> object. </span><span class="kobospan" id="kobo.666.2">Finally, we save the two datasets </span><span><span class="kobospan" id="kobo.667.1">to disk:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.668.1">
train_db = DocBin()
test_db = DocBin()
label_list = ["tech", "business", "sport", 
    "entertainment", "politics"]
train_df = pd.read_json("../data/bbc_train.json")
test_df = pd.read_json("../data/bbc_test.json")
train_df.sample(frac=1)
for idx, row in train_df.iterrows():
    text = row["text"]
    label = row["label"]
    doc = preprocess_data_entry(text, label, label_list)
    train_db.add(doc)
for idx, row in test_df.iterrows():
    text = row["text"]
    label = row["label"]
    doc = preprocess_data_entry(text, label, label_list)
    test_db.add(doc)
train_db.to_disk('../data/bbc_train.spacy')
test_db.to_disk('../data/bbc_test.spacy')</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.669.1">Train the model using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.670.1">train</span></strong><span class="kobospan" id="kobo.671.1"> command. </span><span class="kobospan" id="kobo.671.2">In order for training to work, you will need to have the configuration file downloaded to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.672.1">data</span></strong><span class="kobospan" id="kobo.673.1"> folder. </span><span class="kobospan" id="kobo.673.2">This is explained in the </span><em class="italic"><span class="kobospan" id="kobo.674.1">Getting ready</span></em><span class="kobospan" id="kobo.675.1"> section of this recipe. </span><span class="kobospan" id="kobo.675.2">The training config specifies the location of the training and test datasets, so you need to run the previous step for the training to</span><a id="_idIndexMarker232" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.676.1"> work. </span><span class="kobospan" id="kobo.676.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.677.1">train</span></strong><span class="kobospan" id="kobo.678.1"> command saves the model in the </span><strong class="source-inline1"><span class="kobospan" id="kobo.679.1">model_last</span></strong><span class="kobospan" id="kobo.680.1"> subdirectory of the directory we specify in the input (</span><strong class="source-inline1"><span class="kobospan" id="kobo.681.1">../models/spacy_textcat_bbc/</span></strong><span class="kobospan" id="kobo.682.1"> in </span><span><span class="kobospan" id="kobo.683.1">this case):</span></span><pre class="source-code"><span class="kobospan1" id="kobo.684.1">
train("../data/spacy_config.cfg", output_path="../models/spacy_textcat_bbc")</span></pre><p class="calibre3"><span class="kobospan" id="kobo.685.1">The output will differ but might look like this (truncated for easier reading). </span><span class="kobospan" id="kobo.685.2">We see that the final accuracy of our trained model </span><span><span class="kobospan" id="kobo.686.1">is 85%:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.687.1">ℹ Saving to output directory: ../models/spacy_textcat_bbc
ℹ Using CPU
=========================== Initializing pipeline ===========================
✔ Initialized pipeline
4.5-spacy_textcat.ipynb
============================= Training pipeline =============================
ℹ Pipeline: ['tok2vec', 'textcat']
ℹ Initial learn rate: 0.001
E    #       LOSS TOK2VEC  LOSS TEXTCAT  CATS_SCORE  SCORE
---  ------  ------------  ------------  ----------  ------
  0       0          0.00          0.16        8.48    0.08
  0     200         20.77         37.26       35.58    0.36
  0     400         98.56         35.96       26.90    0.27
  0     600         49.83         37.31       36.60    0.37
… (truncated)
  4    4800       7571.47          9.64       80.25    0.80
  4    5000      16164.99         10.58       87.71    0.88
  5    5200       8604.43          8.20       84.98    0.85
✔ Saved pipeline to output directory
../models/spacy_textcat_bbc/model-last</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.688.1">Now we test the model on an unseen example. </span><span class="kobospan" id="kobo.688.2">We first load the model and then get an example from the test data. </span><span class="kobospan" id="kobo.688.3">We then check the text and its category. </span><span class="kobospan" id="kobo.688.4">We run the model on the input text and print the resulting probabilities. </span><span class="kobospan" id="kobo.688.5">The model will give a dictionary of categories with their respective probability scores. </span><span class="kobospan" id="kobo.688.6">These scores indicate the probability that the text belongs to the respective class. </span><span class="kobospan" id="kobo.688.7">The class with the highest probability is the one we should assign to the text. </span><span class="kobospan" id="kobo.688.8">The category dictionary is in</span><a id="_idIndexMarker233" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.689.1"> the </span><strong class="source-inline1"><span class="kobospan" id="kobo.690.1">doc.cats</span></strong><span class="kobospan" id="kobo.691.1"> attribute, just like when we were preparing the data, but in this case the model assigns it. </span><span class="kobospan" id="kobo.691.2">In this case, the text is about politics and the model correctly </span><span><span class="kobospan" id="kobo.692.1">classifies it:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.693.1">
nlp = spacy.load("../models/spacy_textcat_bbc/model-last")
input_text = test_df.iloc[1, test_df.columns.get_loc('text')]
print(input_text)
print(test_df["label_text"].iloc[[1]])
doc = nlp(input_text)
print("Predicted probabilities: ", doc.cats)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.694.1">The output will look similar </span><span><span class="kobospan" id="kobo.695.1">to this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.696.1">lib dems  new election pr chief the lib dems have appointed a senior figure from bt to be the party s new communications chief for their next general election effort.  sandy walkington will now work with senior figures such as matthew taylor on completing the party manifesto. </span><span class="kobospan1" id="kobo.696.2">party chief executive lord rennard said the appointment was a  significant strengthening of the lib dem team . </span><span class="kobospan1" id="kobo.696.3">mr walkington said he wanted the party to be ready for any  mischief  rivals or the media tried to throw at it.   my role will be to ensure this new public profile is effectively communicated at all levels   he said.  i also know the party will be put under scrutiny in the media and from the other parties as never before - and we will need to show ourselves ready and prepared to counter the mischief and misrepresentation that all too often comes from the party s opponents.  the party is already demonstrating on every issue that it is the effective opposition.  mr walkington s new job title is director of general election communications.
</span><span class="kobospan1" id="kobo.696.4">8    politics
Name: label_text, dtype: object
Predicted probabilities:  {'tech': 3.531841841208916e-08, 'business': 0.000641813559923321, 'sport': 0.00033847044687718153, 'entertainment': 0.00016174423217307776, 'politics': 0.9988579750061035}</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.697.1">In this step, we define a </span><strong class="source-inline1"><span class="kobospan" id="kobo.698.1">get_prediction</span></strong><span class="kobospan" id="kobo.699.1"> function, which takes text, a spaCy model, and the list of potential </span><a id="_idIndexMarker234" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.700.1">classes and outputs the category whose probability is the highest. </span><span class="kobospan" id="kobo.700.2">We then apply this function to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.701.1">text</span></strong><span class="kobospan" id="kobo.702.1"> column of the </span><span><span class="kobospan" id="kobo.703.1">test dataframe:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.704.1">
def get_prediction(input_text, nlp_model, target_names):
    doc = nlp_model(input_text)
    category = max(doc.cats, key = doc.cats.get)
    return target_names.index(category)
test_df["prediction"] = test_df["text"].apply(
    lambda x: get_prediction(x, nlp, label_list))</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.705.1">Now we print out the classification report based on the data from the test dataframe we generated in the previous step. </span><span class="kobospan" id="kobo.705.2">The overall accuracy of the model is 87%, and the reason it is a bit low is because we do not have enough data to train a </span><span><span class="kobospan" id="kobo.706.1">better model:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.707.1">
print(classification_report(test_df["label"],
    test_df["prediction"], target_names=target_names))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.708.1">The result should look similar </span><span><span class="kobospan" id="kobo.709.1">to this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.710.1">               precision    recall  f1-score   support
         tech       0.82      0.94      0.87        80
     business       0.94      0.83      0.89       102
        sport       0.89      0.89      0.89       102
entertainment       0.94      0.87      0.91        77
     politics       0.78      0.83      0.80        84
     accuracy                           0.87       445
    macro avg       0.87      0.87      0.87       445
 weighted avg       0.88      0.87      0.87       445</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.711.1">In this step, we do the same evaluation using the spaCy </span><strong class="source-inline1"><span class="kobospan" id="kobo.712.1">evaluate</span></strong><span class="kobospan" id="kobo.713.1"> command. </span><span class="kobospan" id="kobo.713.2">This command takes in the </span><a id="_idIndexMarker235" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.714.1">path to the model and the path to the test dataset and outputs the scores in a slightly different format. </span><span class="kobospan" id="kobo.714.2">We see that the scores from both steps </span><span><span class="kobospan" id="kobo.715.1">are consistent:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.716.1">
evaluate('../models/spacy_textcat_bbc/model-last', '../data/bbc_test.spacy')</span></pre><p class="calibre3"><span class="kobospan" id="kobo.717.1">The result should look similar </span><span><span class="kobospan" id="kobo.718.1">to this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.719.1">{'token_acc': 1.0,
 'token_p': 1.0,
 'token_r': 1.0,
 'token_f': 1.0,
 'cats_score': 0.8719339318444819,
 'cats_score_desc': 'macro F',
 'cats_micro_p': 0.8719101123595505,
 'cats_micro_r': 0.8719101123595505,
 'cats_micro_f': 0.8719101123595505,
 'cats_macro_p': 0.8746516896205309,
 'cats_macro_r': 0.8732906799083269,
 'cats_macro_f': 0.8719339318444819,
 'cats_macro_auc': 0.9800144873453936,
 'cats_f_per_type': {'tech': {'p': 0.8152173913043478,
   'r': 0.9375,
   'f': 0.872093023255814},
  'business': {'p': 0.9444444444444444,
   'r': 0.8333333333333334,
   'f': 0.8854166666666667},
  'sport': {'p': 0.8921568627450981,
   'r': 0.8921568627450981,
   'f': 0.8921568627450981},
  'entertainment': {'p': 0.9436619718309859,
   'r': 0.8701298701298701,
   'f': 0.9054054054054054},
  'politics': {'p': 0.7777777777777778,
   'r': 0.8333333333333334,
   'f': 0.8045977011494253}},
 'cats_auc_per_type': {'tech': 0.9842808219178081,
  'business': 0.9824501229063054,
  'sport': 0.9933544846510032,
  'entertainment': 0.9834839073969509,
  'politics': 0.9565030998549005},
 'speed': 6894.989948433934}</span></pre></li>			</ol>
			<h1 id="_idParaDest-123" class="calibre7"><a id="_idTextAnchor125" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.720.1">Classifying texts using OpenAI models</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.721.1">In this recipe, we will ask an</span><a id="_idIndexMarker236" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.722.1"> OpenAI model to provide the</span><a id="_idIndexMarker237" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.723.1"> classification of an input text. </span><span class="kobospan" id="kobo.723.2">We will use the same BBC dataset from </span><span><span class="kobospan" id="kobo.724.1">previous recipes.</span></span></p>
			<h2 id="_idParaDest-124" class="calibre5"><a id="_idTextAnchor126" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.725.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.726.1">To run this recipe, you will need to have the </span><strong class="source-inline"><span class="kobospan" id="kobo.727.1">openai</span></strong><span class="kobospan" id="kobo.728.1"> package installed, provided as part of the </span><strong class="source-inline"><span class="kobospan" id="kobo.729.1">poetry</span></strong><span class="kobospan" id="kobo.730.1"> environment, and the </span><strong class="source-inline"><span class="kobospan" id="kobo.731.1">requirements.txt</span></strong><span class="kobospan" id="kobo.732.1"> file. </span><span class="kobospan" id="kobo.732.2">You will also have to have an OpenAI API key. </span><span class="kobospan" id="kobo.732.3">Paste it into the provided field in the file utilities </span><span><span class="kobospan" id="kobo.733.1">notebook (</span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/file_utils.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.734.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/file_utils.ipynb</span></span></a><span><span class="kobospan" id="kobo.735.1">).</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.736.1">The notebook is located </span><span><span class="kobospan" id="kobo.737.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.6_openai_classification.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.738.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.6_openai_classification.ipynb</span></span></a><span><span class="kobospan" id="kobo.739.1">.</span></span></p>
			<p class="callout-heading"><span class="kobospan" id="kobo.740.1">Note</span></p>
			<p class="callout"><span class="kobospan" id="kobo.741.1">OpenAI frequently changes and retires existing models and introduces new ones. </span><span class="kobospan" id="kobo.741.2">The model we use in this recipe, </span><strong class="source-inline1"><span class="kobospan" id="kobo.742.1">gpt-3.5-turbo</span></strong><span class="kobospan" id="kobo.743.1">, might be obsolete by the time you read this. </span><span class="kobospan" id="kobo.743.2">In this case, please check the OpenAI documentation and select another </span><span><span class="kobospan" id="kobo.744.1">suitable model.</span></span></p>
			<h2 id="_idParaDest-125" class="calibre5"><a id="_idTextAnchor127" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.745.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.746.1">In this recipe, we will query the OpenAI API and provide a request for classification as the prompt. </span><span class="kobospan" id="kobo.746.2">We will then post-process the results and evaluate the Open AI model on </span><span><span class="kobospan" id="kobo.747.1">this task:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.748.1">Run the simple classifier and the file </span><span><span class="kobospan" id="kobo.749.1">utilities notebooks:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.750.1">
%run -i "../util/file_utils.ipynb"
%run -i "../util/util_simple_classifier.ipynb"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.751.1">Import the necessary </span><a id="_idIndexMarker238" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.752.1">functions and packages to create the OpenAI client using the </span><span><span class="kobospan" id="kobo.753.1">API key:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.754.1">
import re
from sklearn.metrics import classification_report
from openai import OpenAI
client = OpenAI(api_key=OPEN_AI_KEY)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.755.1">Load the training </span><a id="_idIndexMarker239" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.756.1">and test datasets using Hugging Face without preprocessing them for the number of classes, as we will not be training a </span><span><span class="kobospan" id="kobo.757.1">new model:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.758.1">
train_dataset = load_dataset("SetFit/bbc-news", split="train")
test_dataset = load_dataset("SetFit/bbc-news", split="test")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.759.1">Load and print the first example in the dataset and </span><span><span class="kobospan" id="kobo.760.1">its category:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.761.1">
example = test_dataset[0]["text"]
category = test_dataset[0]["label_text"]
print(example)
print(category)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.762.1">The result should be </span><span><span class="kobospan" id="kobo.763.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.764.1">carry on star patsy rowlands dies actress patsy rowlands  known to millions for her roles in the carry on films  has died at the age of 71.  rowlands starred in nine of the popular carry on films  alongside fellow regulars sid james  kenneth williams and barbara windsor. </span><span class="kobospan1" id="kobo.764.2">she also carved out a successful television career  appearing for many years in itv s well-loved comedy bless this house....
</span><span class="kobospan1" id="kobo.764.3">entertainment</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.765.1">Run the OpenAI model on this one</span><a id="_idIndexMarker240" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.766.1"> example. </span><span class="kobospan" id="kobo.766.2">In step 5, we query the OpenAI API asking it to classify </span><a id="_idIndexMarker241" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.767.1">this example. </span><span class="kobospan" id="kobo.767.2">We create the prompt and append the example text to it. </span><span class="kobospan" id="kobo.767.3">In the prompt, we specify to the model that it is to classify the input text as one of five classes and the output format. </span><span class="kobospan" id="kobo.767.4">If we don’t include these output instructions, it might add other words to it and return text such as </span><em class="italic"><span class="kobospan" id="kobo.768.1">The topic is entertainment</span></em><span class="kobospan" id="kobo.769.1">. </span><span class="kobospan" id="kobo.769.2">We select the </span><strong class="source-inline1"><span class="kobospan" id="kobo.770.1">gpt-3.5-turbo</span></strong><span class="kobospan" id="kobo.771.1"> model and specify the prompt, the temperature, and several other parameters. </span><span class="kobospan" id="kobo.771.2">We set the temperature to </span><strong class="source-inline1"><span class="kobospan" id="kobo.772.1">0</span></strong><span class="kobospan" id="kobo.773.1"> so that there is no or minimal variation in the model’s response. </span><span class="kobospan" id="kobo.773.2">We then print the response returned by the API. </span><span class="kobospan" id="kobo.773.3">The output might vary, but in most cases, it should return </span><em class="italic"><span class="kobospan" id="kobo.774.1">entertainment</span></em><span class="kobospan" id="kobo.775.1">, which </span><span><span class="kobospan" id="kobo.776.1">is correct:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.777.1">
prompt="""You are classifying texts by topics. </span><span class="kobospan1" id="kobo.777.2">There are 5 topics: tech, entertainment, business, politics and sport.
</span><span class="kobospan1" id="kobo.777.3">Output the topic and nothing else. </span><span class="kobospan1" id="kobo.777.4">For example, if the topic is business, your output should be "business".
</span><span class="kobospan1" id="kobo.777.5">Give the following text, what is its topic from the above list without any additional explanations: """ + example
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    temperature=0,
    max_tokens=256,
    top_p=1.0,
    frequency_penalty=0,
    presence_penalty=0,
    messages=[
        {"role": "system", "content": 
            "You are a helpful assistant."},
        {"role": "user", "content": prompt}
    ],
)
print(response.choices[0].message.content)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.778.1">The result might vary, but should look </span><span><span class="kobospan" id="kobo.779.1">like this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.780.1">entertainment</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.781.1">Create a function that will provide the</span><a id="_idIndexMarker242" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.782.1"> classification of an input text and return the category. </span><span class="kobospan" id="kobo.782.2">It takes input</span><a id="_idIndexMarker243" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.783.1"> text and calls the OpenAI API with the same prompt we used previously. </span><span class="kobospan" id="kobo.783.2">It then lowercases the response, strips it of extra white space, and </span><span><span class="kobospan" id="kobo.784.1">returns it:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.785.1">
def get_gpt_classification(input_text):
    prompt="""You are classifying texts by topics. </span><span class="kobospan1" id="kobo.785.2">There are 5 topics: tech, entertainment, business, politics and sport.
</span><span class="kobospan1" id="kobo.785.3">Output the topic and nothing else. </span><span class="kobospan1" id="kobo.785.4">For example, if the topic is business, your output should be "business".
</span><span class="kobospan1" id="kobo.785.5">Give the following text, what is its topic from the above list without any additional explanations: """ + input_text
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        temperature=0,
        max_tokens=256,
        top_p=1.0,
        frequency_penalty=0,
        presence_penalty=0,
        messages=[
            {"role": "system", "content": 
                "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
    )
    classification = response.choices[0].message.content
    classification = classification.lower().strip()
    return classification</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.786.1">In this step, we load test </span><a id="_idIndexMarker244" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.787.1">data. </span><span class="kobospan" id="kobo.787.2">We take the test dataset from Hugging Face and convert it into a </span><a id="_idIndexMarker245" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.788.1">dataframe. </span><span class="kobospan" id="kobo.788.2">We then shuffle the dataframe and select the first 200 examples. </span><span class="kobospan" id="kobo.788.3">The reason is that we want to reduce the cost of testing this classifier through the OpenAI API. </span><span class="kobospan" id="kobo.788.4">You can modify how much data you test this </span><span><span class="kobospan" id="kobo.789.1">method on:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.790.1">
test_df = test_dataset.to_pandas()
test_df.sample(frac=1)
test_data = test_df[0:200].copy()</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.791.1">In step 8, we use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.792.1">get_gpt_classification</span></strong><span class="kobospan" id="kobo.793.1"> function to create a new column in the test dataframe. </span><span class="kobospan" id="kobo.793.2">Depending on the number of test examples you have, it might take a few minutes </span><span><span class="kobospan" id="kobo.794.1">to run:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.795.1">
test_data["gpt_prediction"] = test_data["text"].apply(
    lambda x: get_gpt_classification(x))</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.796.1">Despite our instructions</span><a id="_idIndexMarker246" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.797.1"> to OpenAI to only provide the category as the answer, it might add some other </span><a id="_idIndexMarker247" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.798.1">words, so we define a function, </span><strong class="source-inline1"><span class="kobospan" id="kobo.799.1">get_one_word_match</span></strong><span class="kobospan" id="kobo.800.1">, that cleans OpenAI’s output. </span><span class="kobospan" id="kobo.800.2">In this function, we use a regular expression to match one of the class labels and return just that word from the original string. </span><span class="kobospan" id="kobo.800.3">We then apply this function to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.801.1">gpt_prediction</span></strong><span class="kobospan" id="kobo.802.1"> column in the </span><span><span class="kobospan" id="kobo.803.1">test dataframe:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.804.1">
def get_one_word_match(input_text):
    loc = re.search(
        r'tech|entertainment|business|sport|politics',
        input_text).span()
    return input_text[loc[0]:loc[1]]
test_data["gpt_prediction"] = test_data["gpt_prediction"].apply(
    lambda x: get_one_word_match(x))</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.805.1">Now we turn the label into </span><span><span class="kobospan" id="kobo.806.1">numerical format:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.807.1">
label_list = ["tech", "business", "sport", 
    "entertainment", "politics"]
test_data["gpt_label"] = test_data["gpt_prediction"].apply(
    lambda x: label_list.index(x))</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.808.1">We print the resulting dataframe. </span><span class="kobospan" id="kobo.808.2">We can see that we have all the information we need to perform an evaluation. </span><span class="kobospan" id="kobo.808.3">We have both the correct labels (the </span><strong class="source-inline1"><span class="kobospan" id="kobo.809.1">label</span></strong><span class="kobospan" id="kobo.810.1"> column) and the pred</span><a id="_idIndexMarker248" class="calibre6 pcalibre pcalibre1"/><a id="_idIndexMarker249" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.811.1">icted labels (the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.812.1">gpt_label</span></strong></span><span><span class="kobospan" id="kobo.813.1"> column):</span></span><pre class="source-code"><span class="kobospan1" id="kobo.814.1">
print(test_data)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.815.1">The result should look similar </span><span><span class="kobospan" id="kobo.816.1">to this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.817.1">                                                  text  label
     label_text  \
0    carry on star patsy rowlands dies actress pats...      3
  entertainment
1    sydney to host north v south game sydney will ...      2
          sport
..                                                 ...    ...
</span><span class="kobospan1" id="kobo.817.2">            ...
</span><span class="kobospan1" id="kobo.817.3">198  xbox power cable  fire fear  microsoft has sai...      0
           tech
199  prop jones ready for hard graft adam jones say...      2
          sport
    gpt_prediction  gpt_label
0    entertainment          3
1            sport          2
..             ...        ...
</span><span class="kobospan1" id="kobo.817.4">198           tech          0
199          sport          2</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.818.1">Now we can print the classification report that evaluates the </span><span><span class="kobospan" id="kobo.819.1">OpenAI classification:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.820.1">
print(classification_report(test_data["label"],
        test_data["gpt_label"], target_names=label_list))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.821.1">The results might</span><a id="_idIndexMarker250" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.822.1"> vary. </span><span class="kobospan" id="kobo.822.2">This is a sample output. </span><span class="kobospan" id="kobo.822.3">We see that</span><a id="_idIndexMarker251" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.823.1"> the overall accuracy is </span><span><span class="kobospan" id="kobo.824.1">good, 90%:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.825.1">               precision    recall  f1-score   support
         tech       0.97      0.80      0.88        41
     business       0.87      0.89      0.88        44
        sport       1.00      0.96      0.98        48
entertainment       0.88      0.90      0.89        40
     politics       0.76      0.96      0.85        27
     accuracy                           0.90       200
    macro avg       0.90      0.90      0.90       200
 weighted avg       0.91      0.90      0.90       200</span></pre></li>			</ol>
		</div>
	</body></html>