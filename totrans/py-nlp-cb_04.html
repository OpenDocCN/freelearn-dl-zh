<html><head></head><body>
		<div><h1 id="_idParaDest-104" class="chapter-number"><a id="_idTextAnchor106" class="calibre6 pcalibre pcalibre1"/>4</h1>
			<h1 id="_idParaDest-105" class="calibre7"><a id="_idTextAnchor107" class="calibre6 pcalibre pcalibre1"/>Classifying Texts</h1>
			<p class="calibre3">In this chapter, we will be classifying texts using different methods. Classifying texts is a classic NLP problem. This NLP task involves assigning a value to a text, for example, a topic (such as sport or business) or a sentiment, such as negative or positive, and any such task needs evaluation.</p>
			<p class="calibre3">After reading this chapter, you will be able to preprocess and classify texts using keywords, unsupervised clustering, and two supervised algorithms: <strong class="bold">support vector machines</strong> (<strong class="bold">SVMs</strong>) and a <strong class="bold">convolutional neural network</strong> (<strong class="bold">CNN</strong>) model trained within the spaCy framework. We will also use GPT-3.5 to classify texts.</p>
			<p class="calibre3">For theoretical background on some of the concepts discussed in this section, please refer to <em class="italic">Building Machine Learning Systems with Python</em> by Coelho et al. That book will explain the basics of building a machine learning project, such as training and test sets, as well as metrics used to evaluate such projects, including precision, recall, F1, and accuracy.</p>
			<p class="calibre3">Here is a list of the recipes in this chapter:</p>
			<ul class="calibre15">
				<li class="calibre14">Getting the dataset and evaluation ready</li>
				<li class="calibre14">Performing rule-based text classification using keywords</li>
				<li class="calibre14">Clustering sentences using K-Means – unsupervised text classification</li>
				<li class="calibre14">Using SVMs for supervised text classification</li>
				<li class="calibre14">Training a spaCy model for supervised text classification</li>
				<li class="calibre14">Classifying texts using OpenAI models</li>
			</ul>
			<h1 id="_idParaDest-106" class="calibre7"><a id="_idTextAnchor108" class="calibre6 pcalibre pcalibre1"/>Technical requirements</h1>
			<p class="calibre3">The code for this chapter can be found in the <code>Chapter04</code> folder in the GitHub repository of the book (<a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition</a>). As always, we will use the <code>poetry</code> environment to install the necessary packages. You can also install the required packages using the provided <code>requirements.txt</code> file. We will use the Hugging Face <code>datasets</code> package to get datasets that we will use throughout the chapter.</p>
			<h1 id="_idParaDest-107" class="calibre7"><a id="_idTextAnchor109" class="calibre6 pcalibre pcalibre1"/>Getting the dataset and evaluation ready</h1>
			<p class="calibre3">In this recipe, we will load a <a id="_idIndexMarker171" class="calibre6 pcalibre pcalibre1"/>dataset, prepare it for processing, and create an evaluation baseline. This recipe builds on some of the recipes from <a href="B18411_03.xhtml#_idTextAnchor067" class="calibre6 pcalibre pcalibre1"><em class="italic">Chapter 3</em></a>, where we used <a id="_idIndexMarker172" class="calibre6 pcalibre pcalibre1"/>different tools to represent text in a computer-readable form.</p>
			<h2 id="_idParaDest-108" class="calibre5"><a id="_idTextAnchor110" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">For this recipe, we will use the Rotten Tomatoes reviews dataset, available through Hugging Face. This dataset consists of user movie reviews that can be classified into positive and negative. We will prepare the dataset for machine learning classification. The preparation process in this case will involve loading the reviews, filtering out non-English language ones, tokenizing the text into words, and removing stopwords. Before the machine learning algorithm can run, the text reviews need to be transformed into vectors. This transformation process is described in detail in <a href="B18411_03.xhtml#_idTextAnchor067" class="calibre6 pcalibre pcalibre1"><em class="italic">Chapter 3</em></a>.</p>
			<p class="calibre3">The notebook is located at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.1_data_preparation.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.1_data_preparation.ipynb</a>.</p>
			<h2 id="_idParaDest-109" class="calibre5"><a id="_idTextAnchor111" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">We will classify whether the input review is of negative or positive sentiment. We will first filter out non-English text, then tokenize it into words and remove stopwords and punctuation. Finally, we will look at the class distribution and review the most common words in each class.</p>
			<p class="calibre3">Here are the steps:</p>
			<ol class="calibre13">
				<li class="calibre14">Run the simple classifier file:<pre class="source-code">
%run -i "../util/util_simple_classifier.ipynb"</pre></li>				<li class="calibre14">Import necessary classes. We import the <strong class="source-inline1">detect</strong> function from <strong class="source-inline1">langdetect</strong>, which will help us determine<a id="_idIndexMarker173" class="calibre6 pcalibre pcalibre1"/> the language of the review. We also import the <strong class="source-inline1">word_tokenize</strong> function, which we will use to split the reviews into words. The <strong class="source-inline1">FreqDist</strong> class from NLTK will help us see the most frequent positive and negative words in<a id="_idIndexMarker174" class="calibre6 pcalibre pcalibre1"/> the reviews. We will use the <strong class="source-inline1">stopwords</strong> list, also from NLTK, to filter the stopwords from the text. Finally, the <strong class="source-inline1">punctuation</strong> string from the <strong class="source-inline1">string</strong> package will help us to filter punctuation:<pre class="source-code">
from langdetect import detect
from nltk import word_tokenize
from nltk.probability import FreqDist
from nltk.corpus import stopwords
from string import punctuation</pre></li>				<li class="calibre14">Load the training and test datasets using the function from the simple classifier file and print the two dataframes. We see that the data contains a <strong class="source-inline1">text</strong> column and a <strong class="source-inline1">label</strong> column, where the text column is lowercase:<pre class="source-code">
(train_df, test_df) = load_train_test_dataset_pd("train", 
    "test")
print(train_df)
print(test_df)</pre><p class="calibre3">The output should look similar to this:</p><pre class="source-code">                                                   text  label
0     the rock is destined to be the 21st century's ...      1
1     the gorgeously elaborate continuation of " the...      1
...                                                 ...    ...
8525  any enjoyment will be hinge from a personal th...      0
8526  if legendary shlockmeister ed wood had ever ma...      0
[8530 rows x 2 columns]
                                                   text  label
0     lovingly photographed in the manner of a golde...      1
1                 consistently clever and suspenseful .      1
...                                                 ...    ...
1061  a terrible movie that some people will neverth...      0
1062  there are many definitions of 'time waster' bu...      0
[1066 rows x 2 columns]</pre></li>				<li class="calibre14">Now we create a new column called <strong class="source-inline1">lang</strong> in the dataframes that will contain the language of the review. We use the <strong class="source-inline1">detect</strong> function to populate this column via the <strong class="source-inline1">apply</strong> method. We<a id="_idIndexMarker175" class="calibre6 pcalibre pcalibre1"/> then filter the dataframe to only contain English-language reviews. The final row counts of the training dataframe before and after the <a id="_idIndexMarker176" class="calibre6 pcalibre pcalibre1"/>filtering show us that 178 rows were non-English. This step may take a minute to run:<pre class="source-code">
train_df["lang"] = train_df["text"].apply(detect)
train_df = train_df[train_df['lang'] == 'en']
print(train_df)</pre><p class="calibre3">Now the output should look something like this:</p><pre class="source-code">                                                   text  label lang
0     the rock is destined to be the 21st century's ...      1   en
1     the gorgeously elaborate continuation of " the...      1   en
...                                                 ...
    ...  ...
8528    interminably bleak , to say nothing of boring .      0   en
8529  things really get weird , though not particula...      0   en
[8364 rows x 3 columns]</pre></li>				<li class="calibre14">Now we will do the<a id="_idIndexMarker177" class="calibre6 pcalibre pcalibre1"/> same for the test <a id="_idIndexMarker178" class="calibre6 pcalibre pcalibre1"/>dataframe:<pre class="source-code">
test_df["lang"] = test_df["text"].apply(detect)
test_df = test_df[test_df['lang'] == 'en']</pre></li>				<li class="calibre14">Now we will tokenize the text into words. If you get an error saying that the <strong class="source-inline1">english.pickle</strong> tokenizer was not found, run the line <strong class="source-inline1">nltk.download('punkt')</strong> before running the rest of the code. This code is also contained in the <strong class="source-inline1">lang_utils </strong><strong class="source-inline1">notebook</strong> (<a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/lang_utils.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/lang_utils.ipynb</a>):<pre class="source-code">
train_df["tokenized_text"] = train_df["text"].apply(
    word_tokenize)
print(train_df)
test_df["tokenized_text"] = test_df["text"].apply(word_tokenize)
print(test_df)</pre><p class="calibre3">The result <a id="_idIndexMarker179" class="calibre6 pcalibre pcalibre1"/>will be similar to<a id="_idIndexMarker180" class="calibre6 pcalibre pcalibre1"/> this:</p><pre class="source-code">                                                   text  label lang  \
0     the rock is destined to be the 21st century's ...
      1   en
1     the gorgeously elaborate continuation of " the...      
1   en
...                                                 ...    ...
  ...
8528    interminably bleak , to say nothing of boring .      
0   en
8529  things really get weird , though not particula...      
0   en
                                         tokenized_text
0     [the, rock, is, destined, to, be, the, 21st, c...
1     [the, gorgeously, elaborate, continuation, of,...
...                                                 ...
8528  [interminably, bleak, ,, to, say, nothing, of,...
8529  [things, really, get, weird, ,, though, not, p...
[8352 rows x 4 columns]</pre></li>				<li class="calibre14">In this step, we will remove stopwords and punctuation. First, we load the stopwords using the<a id="_idIndexMarker181" class="calibre6 pcalibre pcalibre1"/> NLTK package. We then add <strong class="source-inline1">'s</strong> and <strong class="source-inline1">``</strong> to the list of stopwords. You <a id="_idIndexMarker182" class="calibre6 pcalibre pcalibre1"/>can add other words that you think are also stopwords. We then define a function that will take a list of words as input and filter it, returning a new list that doesn’t contain stopwords or punctuation. Finally, we apply this function to the training and test data. From the printout, we can see that stopwords and punctuation were removed:<pre class="source-code">
stop_words = list(stopwords.words('english'))
stop_words.append("``")
stop_words.append("'s")
def remove_stopwords_and_punct(x):
    new_list = [w for w in x if w not in stop_words and w not in punctuation]
    return new_list
train_df["tokenized_text"] = train_df["tokenized_text"].apply(
    remove_stopwords_and_punct)
print(train_df)
test_df["tokenized_text"] = test_df["tokenized_text"].apply(
    remove_stopwords_and_punct)
print(test_df)</pre><p class="calibre3">The result will look<a id="_idIndexMarker183" class="calibre6 pcalibre pcalibre1"/> similar to<a id="_idIndexMarker184" class="calibre6 pcalibre pcalibre1"/> this:</p><pre class="source-code">                                                   text  label lang  \
0     the rock is destined to be the 21st century's ...
      1   en
1     the gorgeously elaborate continuation of " the...
      1   en
...                                                 ...
    ...  ...
8528    interminably bleak , to say nothing of boring .
      0   en
8529  things really get weird , though not particula...
      0   en
                                         tokenized_text
0     [rock, destined, 21st, century, new, conan, go...
1     [gorgeously, elaborate, continuation, lord, ri...
...                                                 ...
8528        [interminably, bleak, say, nothing, boring]
8529  [things, really, get, weird, though, particula...
[8352 rows x 4 columns]</pre></li>				<li class="calibre14">Now we will check the class balance over both datasets. It is important that the number of items in each class is approximately the same, since if one class dominates, the model can just learn to always assign this dominating class without being wrong much <a id="_idIndexMarker185" class="calibre6 pcalibre pcalibre1"/>of the time:<pre class="source-code">
print(train_df.groupby('label').count())
print(test_df.groupby('label').count())</pre><p class="calibre3">We see that there <a id="_idIndexMarker186" class="calibre6 pcalibre pcalibre1"/>are slightly, but not significantly, more negative reviews in the training data than positive, and the numbers are nearly equal in test data.</p><pre class="source-code">text  lang  tokenized_text
label
0      4185  4185            4185
1      4167  4167            4167
       text  lang  tokenized_text
label
0       523   523             523
1       522   522             522</pre></li>				<li class="calibre14">Let’s now save the cleaned data to disk:<pre class="source-code">
train_df.to_json("../data/rotten_tomatoes_train.json")
test_df.to_json("../data/rotten_tomatoes_test.json")</pre></li>				<li class="calibre14">In this step, we define a function that will take a list of words and the number of words as input and return a <strong class="source-inline1">FreqDist</strong> object. It will also print out the top <em class="italic">n</em> most frequent words, where <em class="italic">n</em> is passed into the function and is <strong class="source-inline1">200</strong> by default:<pre class="source-code">
def get_stats(word_list, num_words=200):
    freq_dist = FreqDist(word_list)
    print(freq_dist.most_common(num_words))
    return freq_dist</pre></li>				<li class="calibre14">Now let’s use the preceding function and show the most common words in positive and negative reviews to <a id="_idIndexMarker187" class="calibre6 pcalibre pcalibre1"/>see whether there are significant vocabulary differences between<a id="_idIndexMarker188" class="calibre6 pcalibre pcalibre1"/> the two classes. We create two lists of words, one for positive and one for negative reviews. We first filter the dataframe by label and then use the <strong class="source-inline1">sum</strong> function to get the words from all the reviews:<pre class="source-code">
positive_train_words = train_df[
    train_df["label"] == 1].tokenized_text.sum()
negative_train_words = train_df[
    train_df["label"] == 0].tokenized_text.sum()
positive_fd = get_stats(positive_train_words)
negative_fd = get_stats(negative_train_words)</pre><p class="calibre3">In the output, we see that the words <code>film</code> and <code>movie</code> and some other words also act as stopwords in this case, as they are the most common words in both sets. We can add them to the stopwords list in step 7 and redo the cleaning:</p><pre class="source-code">[('film', 683), ('movie', 429), ("n't", 286), ('one', 280), ('--', 271), ('like', 209), ('story', 194), ('comedy', 160), ('good', 150), ('even', 144), ('funny', 137), ('way', 135), ('time', 127), ('best', 126), ('characters', 125), ('make', 124), ('life', 124), ('much', 122), ('us', 122), ('love', 118), ...]
[('movie', 641), ('film', 557), ("n't", 450), ('like', 354), ('one', 293), ('--', 264), ('story', 189), ('much', 177), ('bad', 173), ('even', 160), ('time', 146), ('good', 143), ('characters', 138), ('little', 137), ('would', 130), ('never', 122), ('comedy', 121), ('enough', 107), ('really', 105), ('nothing', 103), ('way', 102), ('make', 101), ...]</pre></li>			</ol>
			<h1 id="_idParaDest-110" class="calibre7"><a id="_idTextAnchor112" class="calibre6 pcalibre pcalibre1"/>Performing rule-based text classification using keywords</h1>
			<p class="calibre3">In this recipe, we will use the <a id="_idIndexMarker189" class="calibre6 pcalibre pcalibre1"/>vocabulary of the text to classify the Rotten Tomatoes reviews. We will create a simple classifier that <a id="_idIndexMarker190" class="calibre6 pcalibre pcalibre1"/>will have a vectorizer for each class. That vectorizer will include the words characteristic to that class. The classification will simply be vectorizing the text using each of the vectorizers and then using the class that has more words.</p>
			<h2 id="_idParaDest-111" class="calibre5"><a id="_idTextAnchor113" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will use the <code>CountVectorizer</code> class and the <code>classification_report</code> function from <code>sklearn</code>, as well as the <code>word_tokenize</code> method from NLTK. All of these are included in the <code>poetry</code> environment.</p>
			<p class="calibre3">The notebook is located at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.2_rule_based.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.2_rule_based.ipynb</a>.</p>
			<h2 id="_idParaDest-112" class="calibre5"><a id="_idTextAnchor114" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">In this recipe, we will create a separate vectorizer for each class. We will then use those vectorizers to count the number of each class word in each review to classify it:</p>
			<ol class="calibre13">
				<li class="calibre14">Run the simple classifier file:<pre class="source-code">
%run -i "../util/util_simple_classifier.ipynb"</pre></li>				<li class="calibre14">Do the necessary imports:<pre class="source-code">
from nltk import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import classification_report</pre></li>				<li class="calibre14">Load the cleaned data from disk. If you receive a <strong class="source-inline1">FileNotFoundError</strong> error at this step, you need to run the previous recipe, <em class="italic">Getting the dataset and evaluation ready</em>, first, since those<a id="_idIndexMarker191" class="calibre6 pcalibre pcalibre1"/> files were created there after cleaning the data:<pre class="source-code">
train_df = pd.read_json("../data/rotten_tomatoes_train.json")
test_df = pd.read_json("../data/rotten_tomatoes_test.json")</pre></li>				<li class="calibre14">Here we create a list of <a id="_idIndexMarker192" class="calibre6 pcalibre pcalibre1"/>words unique to each class. We first concatenate all the words from the <strong class="source-inline1">text</strong> column, filtering on the relevant <strong class="source-inline1">label</strong> value (<strong class="source-inline1">0</strong> for negative reviews and <strong class="source-inline1">1</strong> for positive ones). We then get the words that appear in both of those lists in the <strong class="source-inline1">word_intersection</strong> variable. Finally, we create filtered word lists, one for each class, that do not contain words that appear in both classes. Basically, we delete all the words that appear in both positive and negative reviews from the respective lists:<pre class="source-code">
positive_train_words = train_df[train_df["label"] 
    == 1].text.sum()
negative_train_words = train_df[train_df["label"] 
    == 0].text.sum()
word_intersection = set(positive_train_words) \ 
    &amp; set(negative_train_words)
positive_filtered = list(set(positive_train_words) 
    - word_intersection)
negative_filtered = list(set(negative_train_words) 
    - word_intersection)</pre></li>				<li class="calibre14">Next, we define a function to create vectorizers, one for each class. The input to this function is a list of lists, where each one is a list of words that only appear in that class; we created these in the previous step. For each of the word lists, we create a <strong class="source-inline1">CountVectorizer</strong> object that takes the word list as the <strong class="source-inline1">vocabulary</strong> parameter. Providing this ensures that we only count those words for the purpose of classification:<pre class="source-code">
def create_vectorizers(word_lists):
    vectorizers = []
    for word_list in word_lists:
        vectorizer = CountVectorizer(vocabulary=word_list)
        vectorizers.append(vectorizer)
    return vectorizers</pre></li>				<li class="calibre14">Create the vectorizers using the preceding function:<pre class="source-code">
vectorizers = create_vectorizers([negative_filtered,
    positive_filtered])</pre></li>				<li class="calibre14">In this step, we create a <strong class="source-inline1">vectorize</strong> function that takes in a list of words and a list of vectorizers. We first <a id="_idIndexMarker193" class="calibre6 pcalibre pcalibre1"/>create a string from the word list, as the vectorizer expects a string. For <a id="_idIndexMarker194" class="calibre6 pcalibre pcalibre1"/>each vectorizer in the list, we apply it to the text and then sum the total count of words in that vectorizer. Finally, we append that sum to a list of scores. This will count words in the input per class. We return this score list at the end of the function:<pre class="source-code">
def vectorize(text_list, vectorizers):
    text = " ".join(text_list)
    scores = []
    for vectorizer in vectorizers:
        output = vectorizer.transform([text])
        output_sum = sum(output.todense().tolist()[0])
        scores.append(output_sum)
    return scores</pre></li>				<li class="calibre14">In this step, we define the <strong class="source-inline1">classify</strong> function, which takes a list of scores returned by the <strong class="source-inline1">vectorize</strong> function. This function simply selects the maximum score from the list and returns<a id="_idIndexMarker195" class="calibre6 pcalibre pcalibre1"/> the index of that score corresponding to the class label:<pre class="source-code">
def classify(score_list):
    return max(enumerate(score_list),key=lambda x: x[1])[0]</pre></li>				<li class="calibre14">Here, we apply the preceding<a id="_idIndexMarker196" class="calibre6 pcalibre pcalibre1"/> functions to the training data. We first vectorize the text and then classify it. We create a new column for the result called <strong class="source-inline1">prediction</strong>:<pre class="source-code">
train_df["prediction"] = train_df["text"].apply(
    lambda x: classify(vectorize(x, vectorizers)))
print(train_df)</pre><p class="calibre3">The output will look similar to this:</p><pre class="source-code">                                                   text  label lang  \
0     [rock, destined, 21st, century, new, conan, go...      
1   en
1     [gorgeously, elaborate, continuation, lord, ri...      
1   en
...                                                 ...    ...
  ...
8528        [interminably, bleak, say, nothing, boring]      
0   en
8529  [things, really, get, weird, though, particula...      
0   en
      prediction
0              1
1              1
...          ...
8528           0
8529           0
[8364 rows x 4 columns]</pre></li>				<li class="calibre14">Now we measure the performance of the rule-based classifier by printing the classification report. We input the <a id="_idIndexMarker197" class="calibre6 pcalibre pcalibre1"/>assigned label and the prediction columns. The result is an<a id="_idIndexMarker198" class="calibre6 pcalibre pcalibre1"/> overall accuracy score of 87%:<pre class="source-code">
print(classification_report(train_df['label'], 
    train_df['prediction']))</pre><p class="calibre3">This results in the following:</p><pre class="source-code">              precision    recall  f1-score   support
           0       0.79      0.99      0.88      4194
           1       0.99      0.74      0.85      4170
    accuracy                           0.87      8364
   macro avg       0.89      0.87      0.86      8364
weighted avg       0.89      0.87      0.86      8364</pre></li>				<li class="calibre14">Here we do the same for the test data, and we see a significant reduction in accuracy, down to 62%. This is because <a id="_idIndexMarker199" class="calibre6 pcalibre pcalibre1"/>the vocabulary lists that we use to create the vectorizers only come <a id="_idIndexMarker200" class="calibre6 pcalibre pcalibre1"/>from the training data and are not exhaustive. They will lead to errors in unseen data:<pre class="source-code">
test_df["prediction"] = test_df["text"].apply(
    lambda x: classify(vectorize(x, vectorizers)))
print(classification_report(test_df['label'], 
    test_df['prediction']))</pre><p class="calibre3">The result will be as follows:</p><pre class="source-code">              precision    recall  f1-score   support
           0       0.59      0.81      0.68       523
           1       0.70      0.43      0.53       524
    accuracy                           0.62      1047
   macro avg       0.64      0.62      0.61      1047
weighted avg       0.64      0.62      0.61      1047</pre></li>			</ol>
			<h1 id="_idParaDest-113" class="calibre7"><a id="_idTextAnchor115" class="calibre6 pcalibre pcalibre1"/>Clustering sentences using K-Means – unsupervised 
text classification</h1>
			<p class="calibre3">In this recipe, we will use the<a id="_idIndexMarker201" class="calibre6 pcalibre pcalibre1"/> BBC news dataset. The dataset contains <a id="_idIndexMarker202" class="calibre6 pcalibre pcalibre1"/>news pieces sorted by five topics: politics, tech, business, sport, and entertainment. We will apply the unsupervised K-Means algorithm to sort the data into unlabeled classes.</p>
			<p class="calibre3">After you read this recipe, you will be able to create your own unsupervised clustering model that will sort data into several classes. You can then later apply it to any text data without having to first label it.</p>
			<h2 id="_idParaDest-114" class="calibre5"><a id="_idTextAnchor116" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will use the <code>KMeans</code> algorithm to create our unsupervised model. It is part of the <code>sklearn</code> package and is included in the <code>poetry</code> environment.</p>
			<p class="calibre3">The BBC news dataset as we use it here was uploaded by a Hugging Face user, and the link and the dataset might <a id="_idIndexMarker203" class="calibre6 pcalibre pcalibre1"/>change in time. To avoid any potential issues, you can<a id="_idIndexMarker204" class="calibre6 pcalibre pcalibre1"/> use the BBC dataset uploaded to the book’s GitHub repository by loading it from the CSV file provided in the <code>data</code> directory.</p>
			<p class="calibre3">The notebook is located at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.3_unsupervised_classification.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.3_unsupervised_classification.ipynb</a>.</p>
			<h2 id="_idParaDest-115" class="calibre5"><a id="_idTextAnchor117" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">In this recipe, we will preprocess the data, vectorize it, and then cluster it using K-Means. Since there are usually no right answers for unsupervised modeling, evaluating the models is more difficult, but we will be able to look at some statistics, as well as the most common words in all the clusters.</p>
			<p class="calibre3">Your steps should be formatted like so:</p>
			<ol class="calibre13">
				<li class="calibre14">Run the simple classification file:<pre class="source-code">
%run -i "../util/util_simple_classifier.ipynb"
%run -i "../util/lang_utils.ipynb"</pre></li>				<li class="calibre14">Import the necessary functions and packages:<pre class="source-code">
from nltk import word_tokenize
from sklearn.cluster import KMeans
from nltk.probability import FreqDist
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import StratifiedShuffleSplit</pre></li>				<li class="calibre14">We will load the BBC dataset. We use the <strong class="source-inline1">load_dataset</strong> function from Hugging Face’s <strong class="source-inline1">datasets</strong> package. This <a id="_idIndexMarker205" class="calibre6 pcalibre pcalibre1"/>function was imported in the simple classifier file we ran in step 1. In the Hugging Face repository, datasets are usually split into training and testing. We will load both, although in unsupervised learning, the test set is usually not used:<pre class="source-code">
train_dataset = load_dataset("SetFit/bbc-news", split="train")
test_dataset = load_dataset("SetFit/bbc-news", split="test")
train_df = train_dataset.to_pandas()
test_df = test_dataset.to_pandas()
print(train_df)
print(test_df)</pre><p class="calibre3">The result will look similar to this:</p><pre class="source-code">                                                   text  label
     label_text
0     wales want rugby league training wales could f...      2
          sport
1     china aviation seeks rescue deal scandal-hit j...      1
       business
...                                                 ...    ...
            ...
1223  why few targets are better than many the econo...      1
       business
1224  boothroyd calls for lords speaker betty boothr...      4
       politics
[1225 rows x 3 columns]
                                                  text  label
     label_text
0    carry on star patsy rowlands dies actress pats...      3
  entertainment
1    sydney to host north v south game sydney will ...      2
          sport
..                                                 ...    ...
            ...
998  stormy year for property insurers a string of ...      1
       business
999  what the election should really be about  a ge...      4
       politics
[1000 rows x 3 columns]</pre></li>				<li class="calibre14">Now we will check the distribution of items per class for both training and test data. Class balance is important in <a id="_idIndexMarker206" class="calibre6 pcalibre pcalibre1"/>classification, as a disproportionally larger class will influence the final classifier:<pre class="source-code">
print(train_df.groupby('label_text').count())
print(test_df.groupby('label_text').count())</pre><p class="calibre3">We see that the classes are pretty evenly split, but there are more examples in the <code>business</code> and <code>sport</code> categories:</p><pre class="source-code">               text  label
label_text
business        286    286
entertainment   210    210
politics        242    242
sport           275    275
tech            212    212
               text  label
label_text
business        224    224
entertainment   176    176
politics        175    175
sport           236    236
tech            189    189</pre></li>				<li class="calibre14">Since there is almost as much data in the test set as in the training set, we will combine the data and create a better train/test split. We first concatenate the two dataframes. We then create a <strong class="source-inline1">StratifiedShuffleSplit</strong> that will create a train/test split and will do it while preserving the class balance. We specify that we only need one split (<strong class="source-inline1">n_splits</strong>) and that the test data needs to be 20% of the whole dataset (<strong class="source-inline1">test_size</strong>). The <strong class="source-inline1">sss</strong> object’s <strong class="source-inline1">split</strong> method returns a generator that contains the indices for the split. We can then use these indices to get new training <a id="_idIndexMarker207" class="calibre6 pcalibre pcalibre1"/>and test dataframes. To do that, we filter on the relevant indices and then make a copy of the resulting dataframe slice. If we didn’t make a copy, then we would be working on the original dataframe. We then print out the class counts for both dataframes and see that there is more training and less testing data:<pre class="source-code">
combined_df = pd.concat([train_df, test_df],
    ignore_index=True, sort=False)
print(combined_df)
sss = StratifiedShuffleSplit(n_splits=1,
    test_size=0.2, random_state=0)
train_index, test_index = next(
    sss.split(combined_df["text"], combined_df["label"]))
train_df = combined_df[combined_df.index.isin(
    train_index)].copy()
test_df = combined_df[combined_df.index.isin(test_index)].copy()
print(train_df.groupby('label_text').count())
print(test_df.groupby('label_text').count())</pre><p class="calibre3">The result should look like<a id="_idIndexMarker208" class="calibre6 pcalibre pcalibre1"/> this:</p><pre class="source-code">               text  label  text_tokenized  text_clean  cluster
label_text
business        408    408             408         408      330
entertainment   309    309             309         309      253
politics        333    333             333         333      263
sport           409    409             409         409      327
tech            321    321             321         321      262
               text  label  text_tokenized  text_clean  cluster
label_text
business        102    102             102         102       78
entertainment    77     77              77          77       56
politics         84     84              84          84       70
sport           102    102             102         102       82
tech             80     80              80          80       59</pre></li>				<li class="calibre14">We will now preprocess the data: tokenize it and remove stopwords and punctuation. The functions to do this (<strong class="source-inline1">tokenize</strong>, <strong class="source-inline1">remove_stopword_punct</strong>) are imported in the <strong class="source-inline1">language_utils</strong> file we ran in step 1. If you get an error that the <strong class="source-inline1">english.pickle</strong> tokenizer was not found, run the line <strong class="source-inline1">nltk.download('punkt')</strong> before running the rest of the code. This code is also<a id="_idIndexMarker209" class="calibre6 pcalibre pcalibre1"/> contained in the <strong class="source-inline1">lang_utils notebook</strong>:<pre class="source-code">
train_df = tokenize(train_df, "text")
train_df = remove_stopword_punct(train_df, "text_tokenized")
test_df = tokenize(test_df, "text")
test_df = remove_stopword_punct(test_df, "text_tokenized")</pre></li>				<li class="calibre14">In this step, we create the vectorizer. To do that, we get all the words from the training news articles. First, we save the clean text in a separate column, <strong class="source-inline1">text_clean</strong>, and then we save the two dataframes to disk. Then we create a TF-IDF vectorizer that will count unigrams, bigrams, and trigrams (the <strong class="source-inline1">ngram_range</strong> parameter). We then fit the vectorizer on the training data only. The reason we fit it only on the training data is that if we fit it on both training and test data, it would lead to data leakage and we would get better test scores than actual performance <a id="_idIndexMarker210" class="calibre6 pcalibre pcalibre1"/>on unseen data:<pre class="source-code">
train_df["text_clean"] = train_df["text_tokenized"].apply(
    lambda x: " ".join(list(x)))
test_df["text_clean"] = test_df["text_tokenized"].apply(
    lambda x: " ".join(list(x)))
train_df.to_json("../data/bbc_train.json")
test_df.to_json("../data/bbc_test.json")
vec = TfidfVectorizer(ngram_range=(1,3))
matrix = vec.fit_transform(train_df["text_clean"])</pre></li>				<li class="calibre14">Now we can create the <strong class="source-inline1">Kmeans</strong> classifier for five clusters and then fit it on the matrix produced using the vectorizer from the preceding code. We specify the number of clusters using the <strong class="source-inline1">n_clusters</strong> parameter. We also specify that the number of times the algorithm should run is 10 using the <strong class="source-inline1">n_init</strong> parameter. For higher-dimensional problems, it is recommended to do several runs. After initializing the classifier, we fit it on the matrix we created using the vectorizer in step 7. This will create the clustering of the training data:</li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">In real-life projects, you will not know the number of clusters in advance, as we do here. You will need to use the elbow method or other methods to estimate the optimal number of classes.</p>
			<pre class="source-code">
km = KMeans(n_clusters=5, n_init=10)
km.fit(matrix)</pre>			<ol class="calibre13">
				<li value="9" class="calibre14">The <strong class="source-inline1">get_most_frequent_words</strong> function will return a list of the most frequent words in a list. The most frequent words list will provide us with a clue as to which topic the text is about. We will use this function to print out the most frequent words in a cluster to understand which topic they refer to. The function takes in input text, tokenizes it, and then creates a <strong class="source-inline1">FreqDist</strong> object. We get the top word<a id="_idIndexMarker211" class="calibre6 pcalibre pcalibre1"/> frequency tuples by using its <strong class="source-inline1">most_common</strong> function and finally get only the word without the frequencies and return this as a list:<pre class="source-code">
def get_most_frequent_words(text, num_words):
    word_list = word_tokenize(text)
    freq_dist = FreqDist(word_list)
    top_words = freq_dist.most_common(num_words)
    top_words = [word[0] for word in top_words]
    return top_words</pre></li>				<li class="calibre14">In this step, we define another function, <strong class="source-inline1">print_most_common_words_by_cluster</strong>, which uses the <strong class="source-inline1">get_most_frequent_words</strong> function we defined in the previous step. We take the dataframe, the <strong class="source-inline1">KMeans</strong> model, and the number of clusters as input parameters. We then get the list of the clusters assigned to each data point and then create a column in the dataframe that specifies the assigned cluster. For each cluster, we then filter the dataframe to get the text just for that cluster. We use this text to then pass it into the <strong class="source-inline1">get_most_frequent_words</strong> function to get the list of the most frequent words in that cluster. We print the cluster number and the list and return the input dataframe with the added cluster number column:<pre class="source-code">
def print_most_common_words_by_cluster(input_df, km, 
    num_clusters):
    clusters = km.labels_.tolist()
    input_df["cluster"] = clusters
    for cluster in range(0, num_clusters):
        this_cluster_text = input_df[
            input_df['cluster'] == cluster]
        all_text = " ".join(
            this_cluster_text['text_clean'].astype(str))
        top_200 = get_most_frequent_words(all_text, 200)
        print(cluster)
        print(top_200)
    return input_df</pre></li>				<li class="calibre14">Here, we use the function we defined in the previous step on the training dataframe. We also pass in the fitted <strong class="source-inline1">KMeans</strong> model and the number of clusters, <strong class="source-inline1">5</strong>. The printout gives <a id="_idIndexMarker212" class="calibre6 pcalibre pcalibre1"/>us an idea of which cluster is which topic. The cluster numbers might vary, but the cluster that has <strong class="source-inline1">labour</strong>, <strong class="source-inline1">party</strong>, <strong class="source-inline1">election</strong> as the most frequent words is the <strong class="source-inline1">politics</strong> cluster; the cluster with the words <strong class="source-inline1">music</strong>, <strong class="source-inline1">award</strong>, and <strong class="source-inline1">show</strong> is the <strong class="source-inline1">entertainment</strong> cluster; the cluster with the words <strong class="source-inline1">game</strong>, <strong class="source-inline1">England</strong>, <strong class="source-inline1">win</strong>, <strong class="source-inline1">play</strong>, and <strong class="source-inline1">cup</strong> is the <strong class="source-inline1">sport</strong> cluster; the cluster with the words <strong class="source-inline1">sales</strong> and <strong class="source-inline1">growth</strong> is the <strong class="source-inline1">business</strong> cluster; and the cluster with the words <strong class="source-inline1">software</strong>, <strong class="source-inline1">net</strong>, and <strong class="source-inline1">search</strong> is the <strong class="source-inline1">tech</strong> cluster. We also note that the words <strong class="source-inline1">said</strong> and <strong class="source-inline1">Mr</strong> are clearly stopwords, as they appear in most clusters close to the top:<pre class="source-code">
print_most_common_words_by_cluster(train_df, km, 5)</pre><p class="calibre3">The results will vary each time you run the training, but they might look like this (output truncated):</p><pre class="source-code">0
['mr', 'said', 'would', 'labour', 'party', 'election', 'blair', 'government', ...]
1
['film', 'said', 'best', 'also', 'year', 'one', 'us', 'awards', 'music', 'new', 'number', 'award', 'show', ...]
2
['said', 'game', 'england', 'first', 'win', 'world', 'last', 'one', 'two', 'would', 'time', 'play', 'back', 'cup', 'players', ...]
3
['said', 'mr', 'us', 'year', 'people', 'also', 'would', 'new', 'one', 'could', 'uk', 'sales', 'firm', 'growth', ...]
4
['said', 'people', 'software', 'would', 'users', 'mr', 'could', 'new', 'microsoft', 'security', 'net', 'search', 'also', ...]</pre></li>				<li class="calibre14">In this step, we use the fitted model to predict the cluster for a test example. We use the text in row 1 of the test dataframe. It is a politics example. We use the vectorizer to turn the text into a<a id="_idIndexMarker213" class="calibre6 pcalibre pcalibre1"/> vector and then use the K-Means model to predict the cluster. The prediction is cluster 0, which in this case is correct:<pre class="source-code">
test_example = test_df.iloc[1, test_df.columns.get_loc('text')]
print(test_example)
vectorized = vec.transform([test_example])
prediction = km.predict(vectorized)
print(prediction)</pre><p class="calibre3">The result might look like this:</p><pre class="source-code">lib dems  new election pr chief the lib dems have appointed a senior figure from bt to be the party s new communications chief for their next general election effort.  sandy walkington will now work with senior figures such as matthew taylor on completing the party manifesto. party chief executive lord rennard said the appointment was a  significant strengthening of the lib dem team . mr walkington said he wanted the party to be ready for any  mischief  rivals or the media tried to throw at it.   my role will be to ensure this new public profile is effectively communicated at all levels   he said.  i also know the party will be put under scrutiny in the media and from the other parties as never before - and we will need to show ourselves ready and prepared to counter the mischief and misrepresentation that all too often comes from the party s opponents.  the party is already demonstrating on every issue that it is the effective opposition.  mr walkington s new job title is director of general election communications.
[0]</pre></li>				<li class="calibre14">Finally, we save the model using the <strong class="source-inline1">joblib</strong> package’s <strong class="source-inline1">dump</strong> function and then load it again using the <strong class="source-inline1">load</strong> function. We check the prediction of the loaded model, and it is the same as<a id="_idIndexMarker214" class="calibre6 pcalibre pcalibre1"/> the prediction of the model in memory. This step will allow us to reuse the model in the future:<pre class="source-code">
dump(km, '../data/kmeans.joblib')
km_ = load('../data/kmeans.joblib')
prediction = km_.predict(vectorized)
print(prediction)</pre><p class="calibre3">The result might look like this:</p><pre class="source-code">[0]</pre></li>			</ol>
			<h1 id="_idParaDest-116" class="calibre7"><a id="_idTextAnchor118" class="calibre6 pcalibre pcalibre1"/>Using SVMs for supervised text classification</h1>
			<p class="calibre3">In this recipe, we will build a <a id="_idIndexMarker215" class="calibre6 pcalibre pcalibre1"/>machine learning classifier that uses the SVM algorithm. By the end of this recipe, you will have a <a id="_idIndexMarker216" class="calibre6 pcalibre pcalibre1"/>working classifier that you will be able to test on new inputs and evaluate using the same <code>classification_report</code> tools we used in the previous sections. We will use the same BBC news dataset we used with <code>KMeans</code> previously.</p>
			<h2 id="_idParaDest-117" class="calibre5"><a id="_idTextAnchor119" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will continue working with the same packages that we already installed in the previous recipes. The packages needed are installed in the <code>poetry</code> environment or by installing the <code>requirements.txt</code> file.</p>
			<p class="calibre3">The notebook is located at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.4-svm_classification.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.4-svm_classification.ipynb</a>.</p>
			<h2 id="_idParaDest-118" class="calibre5"><a id="_idTextAnchor120" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">We will load the cleaned training and test data that we had saved in the previous recipe. We will then create the SVM classifier and train it. We will use BERT encoding as our vectorizer.</p>
			<p class="calibre3">Your steps should be formatted like so:</p>
			<ol class="calibre13">
				<li class="calibre14">Run the simple classifier file:<pre class="source-code">
%run -i "../util/util_simple_classifier.ipynb"</pre></li>				<li class="calibre14">Import the necessary functions and packages:<pre class="source-code">
from sklearn.svm import SVC
from sentence_transformers import SentenceTransformer
from sklearn.metrics import confusion_matrix</pre></li>				<li class="calibre14">Here, we load the training and test data. If you get a <strong class="source-inline1">FileNotFoundError</strong> error in this step, run steps 1-7 from the previous recipe, <em class="italic">Clustering sentences using K-Means – unsupervised text classification</em>. We then shuffle the training data using the <strong class="source-inline1">sample</strong> function. Shuffling<a id="_idIndexMarker217" class="calibre6 pcalibre pcalibre1"/> ensures that we do not have long sequences of data of the<a id="_idIndexMarker218" class="calibre6 pcalibre pcalibre1"/> same class. Finally, we print out the number of counts of examples by class. We see that the classes are more or less balanced, which is important for training a classifier:<pre class="source-code">
train_df = pd.read_json("../data/bbc_train.json")
test_df = pd.read_json("../data/bbc_test.json")
train_df.sample(frac=1)
print(train_df.groupby('label_text').count())
print(test_df.groupby('label_text').count())</pre><p class="calibre3">The result will look like this:</p><pre class="source-code">               text  label  text_tokenized  text_clean  cluster
label_text
business        231    231             231         231      231
entertainment   181    181             181         181      181
politics        182    182             182         182      182
sport           243    243             243         243      243
tech            194    194             194         194      194
               text  label  text_tokenized  text_clean
label_text
business         58     58              58          58
entertainment    45     45              45          45
politics         45     45              45          45
sport            61     61              61          61
tech             49     49              49          49</pre></li>				<li class="calibre14">Here, we load the sentence transformer <strong class="source-inline1">all-MiniLM-L6-v2</strong> model that will provide the vectors for us. To learn more about the model, please read the <em class="italic">Using BERT and OpenAI embeddings instead of word embeddings</em> recipe in <a href="B18411_03.xhtml#_idTextAnchor067" class="calibre6 pcalibre pcalibre1"><em class="italic">Chapter 3</em></a>. We then<a id="_idIndexMarker219" class="calibre6 pcalibre pcalibre1"/> define the <strong class="source-inline1">get_sentence_vector</strong> function, which returns the sentence<a id="_idIndexMarker220" class="calibre6 pcalibre pcalibre1"/> embedding for the text input:<pre class="source-code">
model = SentenceTransformer('all-MiniLM-L6-v2')
def get_sentence_vector(text, model):
    sentence_embeddings = model.encode([text])
    return sentence_embeddings[0]</pre></li>				<li class="calibre14">Define a function that will create an SVM object and train it given input data. It takes in the input vectors and the gold labels, creates an SVC object with the RBF kernel and a regularization parameter of <strong class="source-inline1">0.1</strong>, and trains it on the training data. It then returns the trained classifier:<pre class="source-code">
def train_classifier(X_train, y_train):
    clf = SVC(C=0.1, kernel='rbf')
    clf = clf.fit(X_train, y_train)
    return clf</pre></li>				<li class="calibre14">In this step, we create the list of labels for the classifier and the <strong class="source-inline1">vectorize</strong> method. We then create the training and test datasets using the <strong class="source-inline1">create_train_test_data</strong> method, which is located in the simple classifier file. We then train the classifier <a id="_idIndexMarker221" class="calibre6 pcalibre pcalibre1"/>using the <strong class="source-inline1">train_classifier</strong> function and print the training <a id="_idIndexMarker222" class="calibre6 pcalibre pcalibre1"/>and test metrics. We see that the test metrics are really good, all above 90%:<pre class="source-code">
target_names=["tech", "business", "sport", 
    "entertainment", "politics"]
vectorize = lambda x: get_sentence_vector(x, model)
(X_train, X_test, y_train, y_test) = create_train_test_data(
    train_df, test_df, vectorize, column_name="text_clean")
clf = train_classifier(X_train, y_train)
print(classification_report(train_df["label"],
        y_train, target_names=target_names))
test_classifier(test_df, clf, target_names=target_names)</pre><p class="calibre3">The output will be as follows:</p><pre class="source-code">               precision    recall  f1-score   support
         tech       1.00      1.00      1.00       194
     business       1.00      1.00      1.00       231
        sport       1.00      1.00      1.00       243
entertainment       1.00      1.00      1.00       181
     politics       1.00      1.00      1.00       182
     accuracy                           1.00      1031
    macro avg       1.00      1.00      1.00      1031
 weighted avg       1.00      1.00      1.00      1031
               precision    recall  f1-score   support
         tech       0.92      0.98      0.95        49
     business       0.95      0.90      0.92        58
        sport       1.00      1.00      1.00        61
entertainment       1.00      0.98      0.99        45
     politics       0.96      0.98      0.97        45
     accuracy                           0.97       258
    macro avg       0.97      0.97      0.97       258
 weighted avg       0.97      0.97      0.96       258</pre></li>				<li class="calibre14">In this step, we print out the confusion matrix to see where the classifier makes mistakes. The rows represent the<a id="_idIndexMarker223" class="calibre6 pcalibre pcalibre1"/> correct labels, and the columns are the predicted labels. We see the<a id="_idIndexMarker224" class="calibre6 pcalibre pcalibre1"/> most confusion (four examples) where the correct label is <strong class="source-inline1">business</strong> but <strong class="source-inline1">tech</strong> is predicted, and where <strong class="source-inline1">business</strong> is the correct label and <strong class="source-inline1">politics</strong> is predicted (two examples). We also see that <strong class="source-inline1">business</strong> is predicted incorrectly for <strong class="source-inline1">tech</strong>, <strong class="source-inline1">entertainment</strong>, and <strong class="source-inline1">politics</strong> once each. These errors are also reflected in the metrics, where we see that both recall and precision for <strong class="source-inline1">business</strong> are affected. The only category with perfect scores is <strong class="source-inline1">sport</strong> and it also has zeroes across the confusion matrix everywhere except the intersection of the correct row and predicted column. We can use the confusion matrix to see which categories have the most confusion between themselves and take measures to rectify that if needed:<pre class="source-code">
print(confusion_matrix(test_df["label"], test_df["prediction"]))
[[48  1  0  0  0]
 [ 4 52  0  0  2]
 [ 0  0 61  0  0]
 [ 0  1  0 44  0]
 [ 0  1  0  0 44]]</pre></li>				<li class="calibre14">We will test the classifier on a <a id="_idIndexMarker225" class="calibre6 pcalibre pcalibre1"/>new example. We first vectorize the text and then use the trained model to<a id="_idIndexMarker226" class="calibre6 pcalibre pcalibre1"/> make a prediction and print the prediction. The new article is about tech, and the prediction is class <strong class="source-inline1">0</strong>, which is indeed <strong class="source-inline1">tech</strong>:<pre class="source-code">
new_example = """iPhone 12: Apple makes jump to 5G
Apple has confirmed its iPhone 12 handsets will be its first to work on faster 5G networks.
The company has also extended the range to include a new "Mini" model that has a smaller 5.4in screen.
The US firm bucked a wider industry downturn by increasing its handset sales over the past year.
But some experts say the new features give Apple its best opportunity for growth since 2014, when it revamped its line-up with the iPhone 6.
"5G will bring a new level of performance for downloads and uploads, higher quality video streaming, more responsive gaming, real-time interactivity and so much more," said chief executive Tim Cook.
…"""
vector = vectorize(new_example)
prediction = clf.predict([vector])
print(prediction))</pre><p class="calibre3">The result will be as follows:</p><pre class="source-code">[0]</pre></li>			</ol>
			<h2 id="_idParaDest-119" class="calibre5"><a id="_idTextAnchor121" class="calibre6 pcalibre pcalibre1"/>There’s more…</h2>
			<p class="calibre3">There are many different machine learning algorithms<a id="_idIndexMarker227" class="calibre6 pcalibre pcalibre1"/> that can be used instead of the SVM algorithm. Some of the others include <a id="_idIndexMarker228" class="calibre6 pcalibre pcalibre1"/>regression, Naïve Bayes, and decision trees. You can experiment with them and see which ones perform better.</p>
			<h1 id="_idParaDest-120" class="calibre7"><a id="_idTextAnchor122" class="calibre6 pcalibre pcalibre1"/>Training a spaCy model for supervised text classification</h1>
			<p class="calibre3">In this recipe, we will train a spaCy <a id="_idIndexMarker229" class="calibre6 pcalibre pcalibre1"/>model on the BBC dataset, the same dataset we used in the previous recipe, to will predict the text category.</p>
			<h2 id="_idParaDest-121" class="calibre5"><a id="_idTextAnchor123" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will use the spaCy package to train our model. All the dependencies are taken care of by the <code>poetry</code> environment.</p>
			<p class="calibre3">You will need to download the config file from the book’s GitHub repository, located at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/spacy_config.cfg" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/spacy_config.cfg</a>. This file should be located at the path <code>../data/spacy_config.cfg</code> with respect to the notebook.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can modify the training config, or generate your own at <a href="https://spacy.io/usage/training" class="calibre6 pcalibre pcalibre1">https://spacy.io/usage/training</a>.</p>
			<p class="calibre3">The notebook is located at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.5-spacy_textcat.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.5-spacy_textcat.ipynb</a>.</p>
			<h2 id="_idParaDest-122" class="calibre5"><a id="_idTextAnchor124" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">The general structure of the training is similar to a plain machine learning model training, where we clean the data, create<a id="_idIndexMarker230" class="calibre6 pcalibre pcalibre1"/> the dataset, and split it into training and testing datasets. We then train a model and test it on unseen data:</p>
			<ol class="calibre13">
				<li class="calibre14">Run the simple classifier file:<pre class="source-code">
%run -i "../util/lang_utils.ipynb"</pre></li>				<li class="calibre14">Import the necessary functions and packages:<pre class="source-code">
import pandas as pd
from spacy.cli.train import train
from spacy.cli.evaluate import evaluate
from spacy.cli.debug_data import debug_data
from spacy.tokens import DocBin</pre></li>				<li class="calibre14">Here we define the <strong class="source-inline1">preprocess_data_entry</strong> function, which will take the input text, its label, and the list of all labels. It will then run the small spaCy model on the text. This model was imported by running the language utilities file in step 1. It is not important which model we use in this step, since we just want to have a <strong class="source-inline1">Doc</strong> object created from the text. That is why we run the smallest model, so it takes less time. We then create a one-hot encoding for the text class, setting the class label to <strong class="source-inline1">1</strong> and the rest to <strong class="source-inline1">0</strong>. We then create a label dictionary that maps the category name to its value. We set the <strong class="source-inline1">doc.cats</strong> attribute to this dictionary and return the <strong class="source-inline1">Doc</strong> object. spaCy requires this preprocessing of the data to train a classification model:<pre class="source-code">
def preprocess_data_entry(input_text, label, label_list):
    doc = small_model(input_text)
    cats = [0] * len(label_list)
    cats[label] = 1
    final_cats = {}
    for i, label in enumerate(label_list):
        final_cats[label] = cats[i]
    doc.cats = final_cats
    return doc</pre></li>				<li class="calibre14">Now we prepare the training and test datasets. We create the <strong class="source-inline1">DocBin</strong> objects for both training and test data that is required by the spaCy algorithm. We then load the saved data from disk. This is the data we saved in the K-Means recipe. If you get a <strong class="source-inline1">FileNotFoundError</strong> error here, you need to run steps 1-7 from the <em class="italic">Clustering sentences using K-Means – unsupervised text classification</em> recipe. We<a id="_idIndexMarker231" class="calibre6 pcalibre pcalibre1"/> then shuffle the training dataframe. Then we preprocess each data point using the function we defined in the previous step. We then add each datapoint to the <strong class="source-inline1">DocBin</strong> object. Finally, we save the two datasets to disk:<pre class="source-code">
train_db = DocBin()
test_db = DocBin()
label_list = ["tech", "business", "sport", 
    "entertainment", "politics"]
train_df = pd.read_json("../data/bbc_train.json")
test_df = pd.read_json("../data/bbc_test.json")
train_df.sample(frac=1)
for idx, row in train_df.iterrows():
    text = row["text"]
    label = row["label"]
    doc = preprocess_data_entry(text, label, label_list)
    train_db.add(doc)
for idx, row in test_df.iterrows():
    text = row["text"]
    label = row["label"]
    doc = preprocess_data_entry(text, label, label_list)
    test_db.add(doc)
train_db.to_disk('../data/bbc_train.spacy')
test_db.to_disk('../data/bbc_test.spacy')</pre></li>				<li class="calibre14">Train the model using the <strong class="source-inline1">train</strong> command. In order for training to work, you will need to have the configuration file downloaded to the <strong class="source-inline1">data</strong> folder. This is explained in the <em class="italic">Getting ready</em> section of this recipe. The training config specifies the location of the training and test datasets, so you need to run the previous step for the training to<a id="_idIndexMarker232" class="calibre6 pcalibre pcalibre1"/> work. The <strong class="source-inline1">train</strong> command saves the model in the <strong class="source-inline1">model_last</strong> subdirectory of the directory we specify in the input (<strong class="source-inline1">../models/spacy_textcat_bbc/</strong> in this case):<pre class="source-code">
train("../data/spacy_config.cfg", output_path="../models/spacy_textcat_bbc")</pre><p class="calibre3">The output will differ but might look like this (truncated for easier reading). We see that the final accuracy of our trained model is 85%:</p><pre class="source-code">ℹ Saving to output directory: ../models/spacy_textcat_bbc
ℹ Using CPU
=========================== Initializing pipeline ===========================
✔ Initialized pipeline
4.5-spacy_textcat.ipynb
============================= Training pipeline =============================
ℹ Pipeline: ['tok2vec', 'textcat']
ℹ Initial learn rate: 0.001
E    #       LOSS TOK2VEC  LOSS TEXTCAT  CATS_SCORE  SCORE
---  ------  ------------  ------------  ----------  ------
  0       0          0.00          0.16        8.48    0.08
  0     200         20.77         37.26       35.58    0.36
  0     400         98.56         35.96       26.90    0.27
  0     600         49.83         37.31       36.60    0.37
… (truncated)
  4    4800       7571.47          9.64       80.25    0.80
  4    5000      16164.99         10.58       87.71    0.88
  5    5200       8604.43          8.20       84.98    0.85
✔ Saved pipeline to output directory
../models/spacy_textcat_bbc/model-last</pre></li>				<li class="calibre14">Now we test the model on an unseen example. We first load the model and then get an example from the test data. We then check the text and its category. We run the model on the input text and print the resulting probabilities. The model will give a dictionary of categories with their respective probability scores. These scores indicate the probability that the text belongs to the respective class. The class with the highest probability is the one we should assign to the text. The category dictionary is in<a id="_idIndexMarker233" class="calibre6 pcalibre pcalibre1"/> the <strong class="source-inline1">doc.cats</strong> attribute, just like when we were preparing the data, but in this case the model assigns it. In this case, the text is about politics and the model correctly classifies it:<pre class="source-code">
nlp = spacy.load("../models/spacy_textcat_bbc/model-last")
input_text = test_df.iloc[1, test_df.columns.get_loc('text')]
print(input_text)
print(test_df["label_text"].iloc[[1]])
doc = nlp(input_text)
print("Predicted probabilities: ", doc.cats)</pre><p class="calibre3">The output will look similar to this:</p><pre class="source-code">lib dems  new election pr chief the lib dems have appointed a senior figure from bt to be the party s new communications chief for their next general election effort.  sandy walkington will now work with senior figures such as matthew taylor on completing the party manifesto. party chief executive lord rennard said the appointment was a  significant strengthening of the lib dem team . mr walkington said he wanted the party to be ready for any  mischief  rivals or the media tried to throw at it.   my role will be to ensure this new public profile is effectively communicated at all levels   he said.  i also know the party will be put under scrutiny in the media and from the other parties as never before - and we will need to show ourselves ready and prepared to counter the mischief and misrepresentation that all too often comes from the party s opponents.  the party is already demonstrating on every issue that it is the effective opposition.  mr walkington s new job title is director of general election communications.
8    politics
Name: label_text, dtype: object
Predicted probabilities:  {'tech': 3.531841841208916e-08, 'business': 0.000641813559923321, 'sport': 0.00033847044687718153, 'entertainment': 0.00016174423217307776, 'politics': 0.9988579750061035}</pre></li>				<li class="calibre14">In this step, we define a <strong class="source-inline1">get_prediction</strong> function, which takes text, a spaCy model, and the list of potential <a id="_idIndexMarker234" class="calibre6 pcalibre pcalibre1"/>classes and outputs the category whose probability is the highest. We then apply this function to the <strong class="source-inline1">text</strong> column of the test dataframe:<pre class="source-code">
def get_prediction(input_text, nlp_model, target_names):
    doc = nlp_model(input_text)
    category = max(doc.cats, key = doc.cats.get)
    return target_names.index(category)
test_df["prediction"] = test_df["text"].apply(
    lambda x: get_prediction(x, nlp, label_list))</pre></li>				<li class="calibre14">Now we print out the classification report based on the data from the test dataframe we generated in the previous step. The overall accuracy of the model is 87%, and the reason it is a bit low is because we do not have enough data to train a better model:<pre class="source-code">
print(classification_report(test_df["label"],
    test_df["prediction"], target_names=target_names))</pre><p class="calibre3">The result should look similar to this:</p><pre class="source-code">               precision    recall  f1-score   support
         tech       0.82      0.94      0.87        80
     business       0.94      0.83      0.89       102
        sport       0.89      0.89      0.89       102
entertainment       0.94      0.87      0.91        77
     politics       0.78      0.83      0.80        84
     accuracy                           0.87       445
    macro avg       0.87      0.87      0.87       445
 weighted avg       0.88      0.87      0.87       445</pre></li>				<li class="calibre14">In this step, we do the same evaluation using the spaCy <strong class="source-inline1">evaluate</strong> command. This command takes in the <a id="_idIndexMarker235" class="calibre6 pcalibre pcalibre1"/>path to the model and the path to the test dataset and outputs the scores in a slightly different format. We see that the scores from both steps are consistent:<pre class="source-code">
evaluate('../models/spacy_textcat_bbc/model-last', '../data/bbc_test.spacy')</pre><p class="calibre3">The result should look similar to this:</p><pre class="source-code">{'token_acc': 1.0,
 'token_p': 1.0,
 'token_r': 1.0,
 'token_f': 1.0,
 'cats_score': 0.8719339318444819,
 'cats_score_desc': 'macro F',
 'cats_micro_p': 0.8719101123595505,
 'cats_micro_r': 0.8719101123595505,
 'cats_micro_f': 0.8719101123595505,
 'cats_macro_p': 0.8746516896205309,
 'cats_macro_r': 0.8732906799083269,
 'cats_macro_f': 0.8719339318444819,
 'cats_macro_auc': 0.9800144873453936,
 'cats_f_per_type': {'tech': {'p': 0.8152173913043478,
   'r': 0.9375,
   'f': 0.872093023255814},
  'business': {'p': 0.9444444444444444,
   'r': 0.8333333333333334,
   'f': 0.8854166666666667},
  'sport': {'p': 0.8921568627450981,
   'r': 0.8921568627450981,
   'f': 0.8921568627450981},
  'entertainment': {'p': 0.9436619718309859,
   'r': 0.8701298701298701,
   'f': 0.9054054054054054},
  'politics': {'p': 0.7777777777777778,
   'r': 0.8333333333333334,
   'f': 0.8045977011494253}},
 'cats_auc_per_type': {'tech': 0.9842808219178081,
  'business': 0.9824501229063054,
  'sport': 0.9933544846510032,
  'entertainment': 0.9834839073969509,
  'politics': 0.9565030998549005},
 'speed': 6894.989948433934}</pre></li>			</ol>
			<h1 id="_idParaDest-123" class="calibre7"><a id="_idTextAnchor125" class="calibre6 pcalibre pcalibre1"/>Classifying texts using OpenAI models</h1>
			<p class="calibre3">In this recipe, we will ask an<a id="_idIndexMarker236" class="calibre6 pcalibre pcalibre1"/> OpenAI model to provide the<a id="_idIndexMarker237" class="calibre6 pcalibre pcalibre1"/> classification of an input text. We will use the same BBC dataset from previous recipes.</p>
			<h2 id="_idParaDest-124" class="calibre5"><a id="_idTextAnchor126" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">To run this recipe, you will need to have the <code>openai</code> package installed, provided as part of the <code>poetry</code> environment, and the <code>requirements.txt</code> file. You will also have to have an OpenAI API key. Paste it into the provided field in the file utilities notebook (<a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/file_utils.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/file_utils.ipynb</a>).</p>
			<p class="calibre3">The notebook is located at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.6_openai_classification.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.6_openai_classification.ipynb</a>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">OpenAI frequently changes and retires existing models and introduces new ones. The model we use in this recipe, <strong class="source-inline1">gpt-3.5-turbo</strong>, might be obsolete by the time you read this. In this case, please check the OpenAI documentation and select another suitable model.</p>
			<h2 id="_idParaDest-125" class="calibre5"><a id="_idTextAnchor127" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">In this recipe, we will query the OpenAI API and provide a request for classification as the prompt. We will then post-process the results and evaluate the Open AI model on this task:</p>
			<ol class="calibre13">
				<li class="calibre14">Run the simple classifier and the file utilities notebooks:<pre class="source-code">
%run -i "../util/file_utils.ipynb"
%run -i "../util/util_simple_classifier.ipynb"</pre></li>				<li class="calibre14">Import the necessary <a id="_idIndexMarker238" class="calibre6 pcalibre pcalibre1"/>functions and packages to create the OpenAI client using the API key:<pre class="source-code">
import re
from sklearn.metrics import classification_report
from openai import OpenAI
client = OpenAI(api_key=OPEN_AI_KEY)</pre></li>				<li class="calibre14">Load the training <a id="_idIndexMarker239" class="calibre6 pcalibre pcalibre1"/>and test datasets using Hugging Face without preprocessing them for the number of classes, as we will not be training a new model:<pre class="source-code">
train_dataset = load_dataset("SetFit/bbc-news", split="train")
test_dataset = load_dataset("SetFit/bbc-news", split="test")</pre></li>				<li class="calibre14">Load and print the first example in the dataset and its category:<pre class="source-code">
example = test_dataset[0]["text"]
category = test_dataset[0]["label_text"]
print(example)
print(category)</pre><p class="calibre3">The result should be as follows:</p><pre class="source-code">carry on star patsy rowlands dies actress patsy rowlands  known to millions for her roles in the carry on films  has died at the age of 71.  rowlands starred in nine of the popular carry on films  alongside fellow regulars sid james  kenneth williams and barbara windsor. she also carved out a successful television career  appearing for many years in itv s well-loved comedy bless this house....
entertainment</pre></li>				<li class="calibre14">Run the OpenAI model on this one<a id="_idIndexMarker240" class="calibre6 pcalibre pcalibre1"/> example. In step 5, we query the OpenAI API asking it to classify <a id="_idIndexMarker241" class="calibre6 pcalibre pcalibre1"/>this example. We create the prompt and append the example text to it. In the prompt, we specify to the model that it is to classify the input text as one of five classes and the output format. If we don’t include these output instructions, it might add other words to it and return text such as <em class="italic">The topic is entertainment</em>. We select the <strong class="source-inline1">gpt-3.5-turbo</strong> model and specify the prompt, the temperature, and several other parameters. We set the temperature to <strong class="source-inline1">0</strong> so that there is no or minimal variation in the model’s response. We then print the response returned by the API. The output might vary, but in most cases, it should return <em class="italic">entertainment</em>, which is correct:<pre class="source-code">
prompt="""You are classifying texts by topics. There are 5 topics: tech, entertainment, business, politics and sport.
Output the topic and nothing else. For example, if the topic is business, your output should be "business".
Give the following text, what is its topic from the above list without any additional explanations: """ + example
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    temperature=0,
    max_tokens=256,
    top_p=1.0,
    frequency_penalty=0,
    presence_penalty=0,
    messages=[
        {"role": "system", "content": 
            "You are a helpful assistant."},
        {"role": "user", "content": prompt}
    ],
)
print(response.choices[0].message.content)</pre><p class="calibre3">The result might vary, but should look like this:</p><pre class="source-code">entertainment</pre></li>				<li class="calibre14">Create a function that will provide the<a id="_idIndexMarker242" class="calibre6 pcalibre pcalibre1"/> classification of an input text and return the category. It takes input<a id="_idIndexMarker243" class="calibre6 pcalibre pcalibre1"/> text and calls the OpenAI API with the same prompt we used previously. It then lowercases the response, strips it of extra white space, and returns it:<pre class="source-code">
def get_gpt_classification(input_text):
    prompt="""You are classifying texts by topics. There are 5 topics: tech, entertainment, business, politics and sport.
Output the topic and nothing else. For example, if the topic is business, your output should be "business".
Give the following text, what is its topic from the above list without any additional explanations: """ + input_text
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        temperature=0,
        max_tokens=256,
        top_p=1.0,
        frequency_penalty=0,
        presence_penalty=0,
        messages=[
            {"role": "system", "content": 
                "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
    )
    classification = response.choices[0].message.content
    classification = classification.lower().strip()
    return classification</pre></li>				<li class="calibre14">In this step, we load test <a id="_idIndexMarker244" class="calibre6 pcalibre pcalibre1"/>data. We take the test dataset from Hugging Face and convert it into a <a id="_idIndexMarker245" class="calibre6 pcalibre pcalibre1"/>dataframe. We then shuffle the dataframe and select the first 200 examples. The reason is that we want to reduce the cost of testing this classifier through the OpenAI API. You can modify how much data you test this method on:<pre class="source-code">
test_df = test_dataset.to_pandas()
test_df.sample(frac=1)
test_data = test_df[0:200].copy()</pre></li>				<li class="calibre14">In step 8, we use the <strong class="source-inline1">get_gpt_classification</strong> function to create a new column in the test dataframe. Depending on the number of test examples you have, it might take a few minutes to run:<pre class="source-code">
test_data["gpt_prediction"] = test_data["text"].apply(
    lambda x: get_gpt_classification(x))</pre></li>				<li class="calibre14">Despite our instructions<a id="_idIndexMarker246" class="calibre6 pcalibre pcalibre1"/> to OpenAI to only provide the category as the answer, it might add some other <a id="_idIndexMarker247" class="calibre6 pcalibre pcalibre1"/>words, so we define a function, <strong class="source-inline1">get_one_word_match</strong>, that cleans OpenAI’s output. In this function, we use a regular expression to match one of the class labels and return just that word from the original string. We then apply this function to the <strong class="source-inline1">gpt_prediction</strong> column in the test dataframe:<pre class="source-code">
def get_one_word_match(input_text):
    loc = re.search(
        r'tech|entertainment|business|sport|politics',
        input_text).span()
    return input_text[loc[0]:loc[1]]
test_data["gpt_prediction"] = test_data["gpt_prediction"].apply(
    lambda x: get_one_word_match(x))</pre></li>				<li class="calibre14">Now we turn the label into numerical format:<pre class="source-code">
label_list = ["tech", "business", "sport", 
    "entertainment", "politics"]
test_data["gpt_label"] = test_data["gpt_prediction"].apply(
    lambda x: label_list.index(x))</pre></li>				<li class="calibre14">We print the resulting dataframe. We can see that we have all the information we need to perform an evaluation. We have both the correct labels (the <strong class="source-inline1">label</strong> column) and the pred<a id="_idIndexMarker248" class="calibre6 pcalibre pcalibre1"/><a id="_idIndexMarker249" class="calibre6 pcalibre pcalibre1"/>icted labels (the <strong class="source-inline1">gpt_label</strong> column):<pre class="source-code">
print(test_data)</pre><p class="calibre3">The result should look similar to this:</p><pre class="source-code">                                                  text  label
     label_text  \
0    carry on star patsy rowlands dies actress pats...      3
  entertainment
1    sydney to host north v south game sydney will ...      2
          sport
..                                                 ...    ...
            ...
198  xbox power cable  fire fear  microsoft has sai...      0
           tech
199  prop jones ready for hard graft adam jones say...      2
          sport
    gpt_prediction  gpt_label
0    entertainment          3
1            sport          2
..             ...        ...
198           tech          0
199          sport          2</pre></li>				<li class="calibre14">Now we can print the classification report that evaluates the OpenAI classification:<pre class="source-code">
print(classification_report(test_data["label"],
        test_data["gpt_label"], target_names=label_list))</pre><p class="calibre3">The results might<a id="_idIndexMarker250" class="calibre6 pcalibre pcalibre1"/> vary. This is a sample output. We see that<a id="_idIndexMarker251" class="calibre6 pcalibre pcalibre1"/> the overall accuracy is good, 90%:</p><pre class="source-code">               precision    recall  f1-score   support
         tech       0.97      0.80      0.88        41
     business       0.87      0.89      0.88        44
        sport       1.00      0.96      0.98        48
entertainment       0.88      0.90      0.89        40
     politics       0.76      0.96      0.85        27
     accuracy                           0.90       200
    macro avg       0.90      0.90      0.90       200
 weighted avg       0.91      0.90      0.90       200</pre></li>			</ol>
		</div>
	</body></html>