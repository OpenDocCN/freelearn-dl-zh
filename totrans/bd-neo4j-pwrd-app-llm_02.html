<html><head></head><body><div><div><div><h1 class="chapterNumber"><a id="_idTextAnchor014"/>2</h1>
    <h1 id="_idParaDest-29" class="chapterTitle">Demystifying RAG</h1>
    <p class="normal">In the previous chapter, we explored the evolution of LLMs and how they have changed the GenAI landscape. We also discussed some of their pitfalls. We will explore how we can avoid these pitfalls using <strong class="keyWord">Retrieval-Augmented Generation</strong> (<strong class="keyWord">RAG</strong>) in this chapter. We will take a look at what RAG means, what<a id="_idIndexMarker034"/> its architecture is, and how it fits into the LLM workflow in building improved intelligent applications.</p>
    <p class="normal">In this chapter, we are going to cover the following main topics:Â </p>
    <ul>
      <li class="bulletList">Understanding the power of RAG</li>
      <li class="bulletList">Deconstructing the RAG flow</li>
      <li class="bulletList">Retrieving external information for your RAG</li>
      <li class="bulletList">Building an end-to-end RAG flow</li>
    </ul>
    <h1 id="_idParaDest-30" class="heading-1">Technical requirements</h1>
    <p class="normal">This chapter requires familiarity with the Python programming language (version <code class="inlineCode">3.6</code> or higher is recommended) and basic concepts of deep learning. </p>
    <p class="normal">We will be leveraging popular AI toolkits such as Hugging Faceâ€™s <a id="_idIndexMarker035"/>Transformers library (<a href="https://huggingface.co/docs/transformers/en/index">https://huggingface.co/docs/transformers/en/index</a>) to build and experiment with RAG. While not mandatory, having a basic understanding of Git version control can be helpful. </p>
    <p class="normal">Git allows you to easily clone the code repository for this chapter and track any changes you make. Do not worry about finding or typing the code yourself! We have created a dedicated public repository on GitHub, <a href="https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch2">https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch2</a>, allowing you to easily clone it and follow along with the chapterâ€™s hands-on exercises.</p>
    <p class="normal">This repository contains all the necessary scripts, files, and configurations required to implement the RAG model and integrate Neo4j with advanced knowledge graph capabilities.</p>
    <p class="normal">To follow along, make sure you have the following Python libraries installed in your environment:</p>
    <ul>
      <li class="bulletList"><strong class="screenText">Transformers</strong>: Install the Hugging Face <a id="_idIndexMarker036"/>Transformers library for handling model-related functionalities: <code class="inlineCode">pip install transformers</code>.</li>
      <li class="bulletList"><strong class="screenText">PyTorch</strong>: Install PyTorch<a id="_idIndexMarker037"/> as the backend for computation. Follow the instructions at <a href="https://pytorch.org/get-started/locally/">https://pytorch.org/get-started/locally/</a> to install the appropriate version for your system.</li>
      <li class="bulletList"><strong class="screenText">scikit-learn</strong>: For similarity <a id="_idIndexMarker038"/>calculations, install <code class="inlineCode">scikit-learn</code> using the <code class="inlineCode">pip install scikit-learn</code> command.</li>
      <li class="bulletList"><strong class="screenText">NumPy</strong>: Install NumPy<a id="_idIndexMarker039"/> for numerical operations: <code class="inlineCode">pip install numpy</code>.</li>
      <li class="bulletList"><strong class="screenText">SentencePiece</strong>: SentencePiece<a id="_idIndexMarker040"/> is required for text tokenization with certain models. You can install it using the instructions provided in the official GitHub repository: <a href="https://github.com/google/sentencepiece#installation">https://github.com/google/sentencepiece#installation</a>. For most Python environments, install it via <code class="inlineCode">pip</code>: <code class="inlineCode">pip install sentencepiece</code>.</li>
      <li class="bulletList"><strong class="screenText">rank_bm25</strong>: The <code class="inlineCode">rank_bm25</code> library<a id="_idIndexMarker041"/> is required to implement the BM25 algorithm for keyword-based retrieval. You can install it using <code class="inlineCode">pip</code>: <code class="inlineCode">pip install rank_bm25</code>.</li>
      <li class="bulletList"><strong class="screenText">datasets</strong>: The <code class="inlineCode">datasets</code> library<a id="_idIndexMarker042"/> from Hugging Face provides efficient tools for loading, processing, and transforming datasets. It supports large-scale datasets with minimal memory usage. You can install it using <code class="inlineCode">pip install datasets</code>.</li>
      <li class="bulletList"><strong class="screenText">pandas<a id="_idTextAnchor015"/></strong>: <code class="inlineCode">pandas</code> is a <a id="_idIndexMarker043"/>powerful data analysis library in Python, used for manipulating tabular data. In this example, it helps preprocess the dataset by converting it into a DataFrame for easier manipulation. Install it using <code class="inlineCode">pip install pandas</code>.</li>
      <li class="bulletList"><strong class="screenText">faiss-CPU</strong>: <code class="inlineCode">faiss-cpu</code> is a library <a id="_idIndexMarker044"/>for efficient similarity search and clustering of dense vectors. It is used in this example for building a retriever that fetches relevant passages during inference. Visit the Faiss GitHub repository (<a href="https://github.com/facebookresearch/faiss">https://github.com/facebookresearch/faiss</a>) for <a id="_idIndexMarker045"/>documentation and examples. Install it using <code class="inlineCode">pip</code>: <code class="inlineCode">pip install faiss-cpu</code>.</li>
      <li class="bulletList"><strong class="screenText">Accelerate</strong>: Accelerate<a id="_idIndexMarker046"/> is a library by Hugging Face that simplifies distributed training and inference. It ensures optimal hardware utilization across CPUs, GPUs, and multi-node setups. Install it using <code class="inlineCode">pip install accelerate</code>.</li>
    </ul>
    <p class="normal">By ensuring your environment is configured with these tools, you can seamlessly explore the hands-on exercises provided in this chapter.</p>
    <div><p class="normal"><strong class="keyWord">Note</strong></p>
      <p class="normal">All the sections in this chapter focus on the relevant code snippets. For the <br/>complete code, please refer to the bookâ€™s GitHub repository: <a href="https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch2">https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch2</a>.</p>
    </div>
    <h1 id="_idParaDest-31" class="heading-1">Understanding the power of RAG</h1>
    <p class="normal">RAG<a id="_idIndexMarker047"/> was introduced by Meta researchers in 2020 (<a href="https://arxiv.org/abs/2005.11401v4">https://arxiv.org/abs/2005.11401v4</a>) as a framework that allows GenAI models to leverage external data that is not part of model training to enhance the output. </p>
    <p class="normal">It is a widely known fact that LLMs suffer from hallucinations. One of the classic real-world examples of LLMs hallucinating is the case of Levidow, Levidow &amp; Oberman, the New York law firm that was fined for submitting a legal brief containing fake citations generated by OpenAIâ€™s ChatGPT in a case against Colombian airline Avianca. They were subsequently fined thousands of dollars, and they are likely to have lost more in reputational damage. You can read more about it here: <a href="https://news.sky.com/story/lawyers-fined-after-citing-bogus-cases-from-chatgpt-research-12908318">https://news.sky.com/story/lawyers-fined-after-citing-bogus-cases-from-chatgpt-research-12908318</a>.</p>
    <p class="normal">LLM hallucinations<a id="_idIndexMarker048"/> can arise from several factors, such as the following:</p>
    <ul>
      <li class="bulletList"><strong class="screenText">Overfitting to training data</strong>: During training, the LLM might overfit to statistical patterns in the training data. This can lead the model to prioritize replicating those patterns over generating factually accurate content.</li>
      <li class="bulletList"><strong class="screenText">Lack of causal reasoning</strong>: LLMs excel at identifying statistical relationships between words but may struggle to understand cause-and-effect relationships. This can lead to outputs that are grammatically correct but factually implausible.</li>
      <li class="bulletList"><strong class="screenText">Temperature configuration</strong>: LLMs can be configured with a parameter called <strong class="keyWord">temperature</strong>, a number between <code class="inlineCode">0</code> and <code class="inlineCode">1</code> that controls the randomness in text generation. Higher temperatures increase creativity but also the likelihood of hallucinations as the model deviates from expected responses.</li>
      <li class="bulletList"> <strong class="screenText">Missing information</strong>: If the information required to generate an accurate response is not included in the training data, the model might generate plausible sounding but incorrect answers.</li>
      <li class="bulletList"><strong class="screenText">Flawed or biased training data</strong>: The quality of the training process plays a significant role. If the dataset contains biases or inaccuracies, the model may perpetuate <a id="_idIndexMarker049"/>these issues, leading to hallucinations.</li>
    </ul>
    <p class="normal">While hallucinations are a significant challenge, several methods can help mitigate them to some extent:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Prompt engineering</strong>: This involves<a id="_idIndexMarker050"/> carefully crafting and iteratively refining the instructions or queries given to the LLM to elicit consistent and accurate responses. For instance, asking an LLM
        <pre class="programlisting code"><code class="hljs-code"> List five key benefits of Neo4j for knowledge graphs 
</code></pre>
      </li>
    </ul>
    <p class="normal">provides more structure and precision compared to the broad query like:</p>
    <pre class="programlisting code"><code class="hljs-code">Tell me about Neo4j 
</code></pre>
    <p class="normal">The former query specifies the expected output, leading the model to focus on a concise and relevant list of benefits, while the latter might yield a verbose or tangential response. Prompt engineering helps guide the model to stay within the desired scope of information and reduces the chances of it producing irrelevant or fabricated outputs. For a detailed exploration of prompt engineering techniques and best practices, check out this <a id="_idIndexMarker051"/>guide: <a href="https://cloud.google.com/discover/what-is-prompt-engineering">https://cloud.google.com/discover/what-is-prompt-engineering</a>.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">In-context learning</strong> (<strong class="keyWord">few-shot prompting</strong>): In this method, examples are included within the prompt to guide<a id="_idIndexMarker052"/> the LLM toward accurate, task-specific responses. For instance, when asking for a product comparison, providing a few examples of properly structured comparisons within the prompt helps the model mimic the pattern. This approach leverages the modelâ€™s capability to infer context and adjust its responses based on the given examples, making it effective for domain-specific tasks.</li>
      <li class="bulletList"><strong class="keyWord">Fine-tuning</strong>: This involves training an<a id="_idIndexMarker053"/> already pre-trained LLM further on a specific dataset to adapt it to specialized domains or tasks. This process enhances the modelâ€™s ability to generate domain-specific, relevant, and accurate responses. One popular method for fine-tuning is <strong class="keyWord">Reinforcement Learning with Human Feedback</strong> (<strong class="keyWord">RLHF</strong>), where <a id="_idIndexMarker054"/>human evaluators guide the model by scoring its outputs. These scores are used to adjust the modelâ€™s behavior, aligning it with human expectations. For example, fine-tuning an LLM on a companyâ€™s internal documentation ensures it produces accurate and relevant outputs tailored to the organizationâ€™s specific needs. If prompted with
        <pre class="programlisting code"><code class="hljs-code">Explain the onboarding process for new hires
</code></pre>
      </li>
    </ul>
    <p class="normal">a fine-tuned model might provide a detailed explanation consistent with the companyâ€™s policies, whereas a general model might offer a vague or unrelated response. Let us take another example scenario to understand how using RLHF, you can improve responses. </p>
    <p class="normal">Suppose the LLM was initially asked:</p>
    <pre class="programlisting code"><code class="hljs-code">What are the benefits of using XYZ software? 
</code></pre>
    <p class="normal">The response might include generic benefits that do not align with the softwareâ€™s unique features. With RLHF, human evaluators score the response based on accuracy, relevance, and completeness. For instance, the initial response could be:</p>
    <pre class="programlisting code"><code class="hljs-code">XYZ software improves productivity, enhances collaboration, and reduces costs.
</code></pre>
    <p class="normal">The feedback may be:</p>
    <pre class="programlisting code"><code class="hljs-code">Too generic; lacks specifics about XYZ software.
</code></pre>
    <p class="normal">After fine-tuning with human feedback, the result could be a more accurate and tailored response, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">XYZ software offers real-time data synchronization, customizable workflows, and advanced security features, making it ideal for enterprise resource planning.
</code></pre>
    <p class="normal">RLHF is <a id="_idIndexMarker055"/>especially valuable in reducing hallucinations because it emphasizes learning from human-curated feedback.</p>
    <p class="normal">While these methods provide significant improvements, they still fall short in one critical area: enabling organizations to use domain-specific knowledge to rapidly build accurate, contextual, and explainable GenAI applications. The solution lies in <strong class="keyWord">grounding</strong> â€“ a concept that ties the modelâ€™s responses to real-world facts or data. This<a id="_idIndexMarker056"/> approach forms the foundation of a new paradigm in text generation called RAG. By dynamically retrieving factual information from reliable knowledge sources, RAG ensures outputs are both accurate and contextually aligned. RAG tries to address LLM hallucinations by incorporating relevant information from factual knowledge repositories. </p>
    <p class="normal">The term retrieval-augmented generation, or <a id="_idIndexMarker057"/>RAG for short, was first introduced by researchers at <strong class="keyWord">Facebook AI Research</strong> (<strong class="keyWord">FAIR</strong>) in a paper titled <em class="italic">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</em>: <a href="https://arxiv.org/abs/2005.11401">https://arxiv.org/abs/2005.11401</a>, submitted in May 2020.</p>
    <p class="normal">The paper proposed <a id="_idIndexMarker058"/>RAG as a hybrid architecture (refer to <em class="italic">Figure 2.1</em>) that combines a neural retriever with a sequence-to-sequence generator. The <strong class="keyWord">retriever</strong> fetches <a id="_idIndexMarker059"/>relevant documents from an external knowledge base, which are then used as context for the generator to produce outputs grounded in factual data. This approach was shown to significantly improve performance on knowledge-intensive NLP tasks, such as open-domain question-answering and dialogue systems, by reducing the reliance on the modelâ€™s internal knowledge and enhancing factual accuracy. RAG addresses the previously mentioned shortcomings of LLMs by introducing a critical element: the ability to retrieve relevant knowledge from supplementary or domain-specific data sources.</p>
    <figure class="mediaobject"><img src="img/B31107_02_1.png" alt="Figure 2.1 â€” RAG architecture proposed in the Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks research paper by FAIR" width="1650" height="502"/></figure>
    <p class="packt_figref">Figure 2.1 â€” RAG architecture proposed in the Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks research paper by FAIR</p>
    <p class="normal">Additionally, RAG pipelines offer the <a id="_idIndexMarker060"/>potential to reduce model size while maintaining accuracy. Instead of embedding all knowledge within the modelâ€™s parametersâ€”which would require extensive resourcesâ€”RAG allows the model to retrieve information dynamically, keeping it lightweight and scalable.</p>
    <p class="normal">The next section in this chapter will delve deeper into the inner workings of RAG, exploring how it bridges the gap between raw generation and knowledge-grounded text production.</p>
    <h1 id="_idParaDest-32" class="heading-1">Deconstructing the RAG flow</h1>
    <p class="normal">Let us now deconstruct the building <a id="_idIndexMarker061"/>blocks of a RAG model and help you understand how it functions.</p>
    <p class="normal">First, we will take a look at the regular LLM application flow. <em class="italic">Figure 2.2</em> illustrates this basic flow.</p>
    <figure class="mediaobject"><img src="img/B31107_02_2.png" alt="Figure 2.2 â€” The basic flow of information in a chat application with an LLM" width="495" height="310"/></figure>
    <p class="packt_figref">Figure 2.2 â€” The basic flow of information in a chat application with an LLM</p>
    <p class="normal">Here is what<a id="_idIndexMarker062"/> happens when a user prompts an LLM</p>
    <ol>
      <li class="numberedList" value="1"><strong class="screenText">User sends a prompt</strong>: The process begins with a user sending a prompt to an LLM chat API. This prompt could be a question, an instruction, or any other request for information or content generation.</li>
      <li class="numberedList"><strong class="screenText">LLM API processes the prompt</strong>: The LLM chat API receives the userâ€™s prompt and transmits it to an LLM. LLMs are AI models trained on massive amounts of text data, allowing them to communicate and generate human-like text in response to a wide range of prompts and questions.</li>
      <li class="numberedList"><strong class="screenText">LLM generates a response</strong>: The LLM then processes the prompt and formulates a response. This response is sent back to the LLM chat API, which then transmits it to the user.</li>
    </ol>
    <p class="normal">From this flow, we can see that the LLM is responsible for providing the answer and there is no other process in between. This is the most common usage without RAG in the request-response flow. </p>
    <p class="normal">Now let us take a look at where RAG fits into this workflow. </p>
    <figure class="mediaobject"><img src="img/B31107_02_3.png" alt="Figure 2.3 â€” The flow of information in a chat application with the RAG model" width="530" height="595"/></figure>
    <p class="packt_figref">Figure 2.3 â€” The flow of information in a chat application with the RAG model</p>
    <p class="normal">We can see from <em class="italic">Figure 2.3</em> that<a id="_idIndexMarker063"/> we have an intermediate data source before the actual LLM service invocation that can provide the context for the LLM request: </p>
    <ol>
      <li class="numberedList" value="1"><strong class="screenText">User sends prompt</strong>: The process starts with the user sending a prompt or question through a chat interface. This prompt could be anything the user wants information about or needs help with.</li>
      <li class="numberedList"><strong class="screenText">RAG model processes prompt</strong>: The prompt is received by a chat API, which then relays it to the RAG model. The RAG model has two main components working together: the <em class="italic">retriever</em> (discussed in Step <em class="italic">3</em>) and the <em class="italic">encoder-decoder</em> (discussed in Step <em class="italic">4</em>).</li>
      <li class="numberedList"><strong class="screenText">Retriever</strong>: This component searches through a knowledge repository, which may include unstructured documents, passages, or structured data such as tables or knowledge graphs. Its role is to locate the most relevant information needed to address the userâ€™s prompt.</li>
    </ol>
    <p class="normal">We will cover a simple example of the retriever component. You can review the full code available at <a href="https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/dpr.py">https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/dpr.py</a></p>
    <pre>context encoder model</em> and <em class="italic">tokenizer</em> from Hugging Faceâ€™s Transformers library:</pre>
    <ol>
      <li class="numberedList" value="1">Let us define a set of documents that we want to store in a document store. We are using a few predefined sentences here for demonstration purposes:
        <pre class="programlisting code"><code class="hljs-code">documents = [
Â Â Â Â "The IPL 2024 was a thrilling season with unexpected results.",
.....
Â Â Â Â "Dense Passage Retrieval (') is a state-of-the-art technique for information retrieval."
]
</code></pre>
      </li>
      <li class="numberedList">Next, we will store the content defined previously in a content store. Then we will generate an embedding for each of the documents and store them in the content store:
        <pre class="programlisting code"><code class="hljs-code">def encode_documents(documents):
Â Â Â Â inputs = tokenizer(
Â Â Â Â Â Â Â Â documents, return_tensors='pt', 
Â Â Â Â Â Â Â Â padding=True, truncation=True)
Â Â Â Â with torch.no_grad():
Â Â Â Â Â Â Â Â outputs = model(**inputs)
Â Â Â Â return outputs.pooler_output.numpy()
 
document_embeddings = encode_documents(documents)
</code></pre>
      </li>
      <li class="numberedList">Now, let us define a method that retrieves the content from the document store based on query input. We will generate an embedding of the request and query the content store to retrieve the relevant result. We are leveraging vector search here to get the relevant results:
        <pre class="programlisting code"><code class="hljs-code">def retrieve_documents(query, num_results=3):
Â Â Â Â inputs = tokenizer(query, return_tensors='pt', 
Â Â Â Â Â Â Â Â padding=True, truncation=True)
Â Â Â Â with torch.no_grad():
Â Â Â Â Â Â Â Â query_embedding = model(**inputs).pooler_output
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â .numpy()
Â Â Â Â similarity_scores = cosine_similarity(
Â Â Â Â Â Â Â Â query_embedding, document_embeddings).flatten()
Â Â Â Â top_indices = similarity_scores.argsort()[-num_results:]
Â Â Â Â Â Â Â Â [::-1]
Â Â Â Â top_docs = [
Â Â Â Â Â Â Â Â (documents[i], similarity_scores[i]) 
Â Â Â Â Â Â Â Â for i in top_indices]
Â Â Â Â return top_doc
</code></pre>
      </li>
    </ol>
    <p class="normal">We can see for a given query what kind of output we would receive as an example.</p>
    <p class="normal">The following is sample input:</p>
    <pre class="programlisting code"><code class="hljs-code">Query: What is Dense Passage Retrieval?
</code></pre>
    <p class="normal">And here is<a id="_idIndexMarker065"/> the sample output:</p>
    <pre class="programlisting code"><code class="hljs-code">Top Results:
Score: 0.7777, Document: Dense Passage Retrieval (') is a state-of-the-art technique for information retrieval.
...
</code></pre>
    <div><p class="normal"><strong class="keyWord">Note</strong></p>
      <p class="normal">Retriever implementations can be quite complex. They could involve <br/>using efficient search algorithms such as BM25, TF-IDF, or neural retrievers <a id="_idIndexMarker066"/><br/>such as <strong class="keyWord">Dense Passage Retrieval</strong>. You can read more about it at <a href="https://github.com/facebookresearch/">https://github.com/facebookresearch/</a>.</p>
    </div>
    <ol>
      <li class="numberedList" value="4"><strong class="screenText">Encoder-decoder/augmented generation</strong>: The encoder part of this component processes the prompt along with the retrieved informationâ€”whether structured or unstructuredâ€”to create a comprehensive representation. The decoder then uses this representation to generate a response that is accurate, contextually rich, and tailored to the userâ€™s prompt.</li>
    </ol>
    <p class="normal">This involves invoking the LLM API with the input query and context information. Let us take a look at an example of how this works. The following example shows how we can invoke a query with contextual information. This example showcases the use of T5Tokenizer model: </p>
    <ol>
      <li class="numberedList" value="1">Let us define an LLM first. We will be using the T5 model from Hugging Face:
        <pre class="programlisting code"><code class="hljs-code">tokenizer = T5Tokenizer.from_pretrained('t5-small', 
Â Â Â Â legacy=False)
model = T5ForConditionalGeneration.from_pretrained(
Â Â Â Â 't5-small')
</code></pre>
      </li>
      <li class="numberedList">Define the query and documents for the RAG flow. Normally, we leverage a retriever for the RAG flow. We are going to use hardcoded values for demonstration purposes here:
        <pre class="programlisting code"><code class="hljs-code">query = "What are the benefits of solar energy?"
retrieved_passages = """
Solar energy is a renewable resource and reduces electricity bills.
......
"""
</code></pre>
      </li>
      <li class="numberedList">We will <a id="_idIndexMarker067"/>define a method that takes the input query and the retrieved passages to use the LLM API to demonstrate the RAG approach:
        <pre class="programlisting code"><code class="hljs-code">def generate_response(query, retrieved_passages):
Â Â Â Â Â Â Â Â input_text = f"Answer this question based on the provided context: {query} Context: {retrieved_passages}" 
Â Â Â Â inputs = tokenizer(input_text, return_tensors='pt', 
Â Â Â Â Â Â Â Â padding=True, 
Â Â Â Â Â Â Â Â truncation=True, max_length=512
Â Â Â Â ).to(device)
Â Â Â Â with torch.no_grad():
Â Â Â Â Â Â Â Â outputs = model.generate(
Â Â Â Â Â Â Â Â Â Â Â Â **inputs,
Â Â Â Â Â Â Â Â Â Â Â Â max_length=300,Â Â # Allow longer responses
Â Â Â Â Â Â Â Â Â Â Â Â num_beams=3,Â Â Â Â Â # Use beam search for better results
Â Â Â Â Â Â Â Â Â Â Â Â early_stopping=True
Â Â Â Â Â Â Â Â )
Â Â Â Â return tokenizer.decode(outputs[0], 
Â Â Â Â Â Â Â Â skip_special_tokens=True)
</code></pre>
      </li>
    </ol>
    <div><p class="normal"><strong class="keyWord"> Note</strong></p>
      <p class="normal">We are employing the <strong class="keyWord">T5 model</strong>â€™s beam search decoding to produce an <br/>accurate and contextually relevant response. <strong class="keyWord">Beam search decoding</strong> is a search algorithm used to find the most likely sequence of tokens (words) during<a id="_idIndexMarker068"/> text generation. Unlike greedy decoding, which selects the most <br/>probable token at each step, beam search maintains multiple potential <br/>sequences (called <strong class="keyWord">beams</strong>) and explores them simultaneously. This <br/>increases the chances of finding a high-quality result, as it avoids committing to suboptimal choices too early in the generation process. You can learn more about beam search in Transformers in this article: <a href="https://huggingface.co/blog/constrained-beam-search">https://huggingface.co/blog/constrained-beam-search</a>.</p>
    </div>
    <p class="normal">Now, let us<a id="_idIndexMarker069"/> invoke this method and review the response.</p>
    <ol>
      <li class="numberedList" value="5"><strong class="screenText">Chat API delivers response</strong>: The following code will invoke the <code class="inlineCode">generate_response</code> method and deliver the chat response for the input query:
        <pre class="programlisting code"><code class="hljs-code">response = generate_response(query, retrieved_passages) 
print("Query:", query) 
print("Retrieved Passages:", retrieved_passages) 
print("Generated Response:", response)
</code></pre>
      </li>
    </ol>
    <p class="normal">When we run this example, the outcome is as follows. </p>
    <p class="normal">The following is sample input:</p>
    <pre class="programlisting code"><code class="hljs-code">Query: What are the benefits of solar energy?
</code></pre>
    <p class="normal">The retrieved passages are as follows: </p>
    <pre class="programlisting code"><code class="hljs-code">Solar energy is a renewable resource and reduces electricity bills.
......
</code></pre>
    <p class="normal">The following is the sample output:</p>
    <pre class="programlisting code"><code class="hljs-code">Generated Response: it is environmentally friendly and helps combat climate change
</code></pre>
    <p class="normal">You can find the full code for this example at <a href="https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/augmented_generation.py">https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/augmented_generation.py</a>. </p>
    <ol>
      <li class="numberedList" value="6"><strong class="screenText">Integration and fine-tuning</strong>: Now let us look at a code snippet that combines the retriever and the LLM invocation as the full RAG flow. The following code demonstrates this:
        <pre class="programlisting code"><code class="hljs-code">def rag_pipeline(query):
Â Â Â Â retrieved_docs = retrieve_documents(query)
Â Â Â Â response = generate_response(query, retrieved_docs)
Â Â Â Â return response
 
query = "How does climate change affect biodiversity?"
generated_text = rag_pipeline(query)
print("Final Generated Text:", generated_text)
</code></pre>
      </li>
    </ol>
    <p class="normal">From the code, we <a id="_idIndexMarker070"/>can see that the flow is simple. We retrieve the documents needed to leverage the RAG flow using the retriever and pass the input query and the retrieved documents to the LLM API invocation.</p>
    <p class="normal">In this deep dive into the RAG architecture, we focused on its mechanics and demonstrated the functioning of its core components. By combining efficient information retrieval with advanced language generation models, RAG produces contextually appropriate and knowledge-enriched responses. As we<a id="_idIndexMarker071"/> transition to the next section, we will discuss the <strong class="keyWord">retrieval process</strong>. </p>
    <h1 id="_idParaDest-33" class="heading-1">Retrieving external information for your RAG</h1>
    <p class="normal">Understanding how RAG leverages external knowledge is crucial for appreciating its ability to generate factually accurate and informative responses. This section discusses various <strong class="keyWord">retrieval techniques</strong>, strategies for integrating retrieved information, and practical examples to illustrate these concepts.</p>
    <h2 id="_idParaDest-34" class="heading-2">Understanding retrieval techniques and strategies</h2>
    <p class="normal">The success of a RAG <a id="_idIndexMarker072"/>model hinges on its ability to retrieve relevant information from a vast external knowledge base using one of the commonly used retrieval techniques. These retrieval methods are essential for sourcing relevant information from large datasets. Common techniques include traditional methods such as BM25 and modern neural approaches such as DPR. Broadly speaking, these techniques can be classified into<a id="_idIndexMarker073"/> three <a id="_idIndexMarker074"/>categories: <strong class="keyWord">vector similarity search</strong>, <strong class="keyWord">keyword matching</strong>, and <strong class="keyWord">passage retrieval</strong>. We will discuss each of them in the following <a id="_idIndexMarker075"/>subsections.</p>
    <h3 id="_idParaDest-35" class="heading-3">Vector similarity search </h3>
    <p class="normal">The text or query you pass to the <a id="_idIndexMarker076"/>LLM is converted into a vector representation called an <strong class="keyWord">embedding</strong>. The vector similarity search compares <a id="_idIndexMarker077"/>the vector embeddings to retrieve the <a id="_idIndexMarker078"/>closest match. The idea is that related and similar text will have similar embeddings. This technique works as follows:Â Â </p>
    <ol>
      <li class="numberedList" value="1">Build an embedding of the input query. We tokenize the input query and generate the vector embedding representation of it:
        <pre class="programlisting code"><code class="hljs-code">query_inputs = question_tokenizer(query, return_tensors="pt")
with torch.no_grad():
Â Â query_embeddings = question_encoder(
Â Â Â Â Â Â Â Â **query_inputs
Â Â Â Â ).pooler_output
</code></pre>
      </li>
      <li class="numberedList">Build the embeddings of the documents. We generate an embedding for each document using the tokenizer and associate each embedding with its document:
        <pre class="programlisting code"><code class="hljs-code">for doc in documents:
Â Â Â Â doc_inputs = context_tokenizer(doc, return_tensors="pt")
Â Â Â Â with torch.no_grad():
Â Â Â Â Â Â Â Â doc_embeddings.append(
Â Â Â Â Â Â Â Â Â Â Â Â context_encoder(**doc_inputs).pooler_output)
doc_embeddings = torch.cat(doc_embeddings)
</code></pre>
      </li>
      <li class="numberedList">Find similar documents using dot product calculation. This step uses the input query embedding and searches the document embeddings for results similar to the input query:
        <pre class="programlisting code"><code class="hljs-code">scores = torch.matmul(query_embeddings, doc_embeddings.T).squeeze()
</code></pre>
      </li>
      <li class="numberedList">Sort the documents by the relevancy score and return the results. The results contain the matching documents along with a score representing how similar it is to the input query. We will order the results in the order we want, most similar to least similar:
        <pre class="programlisting code"><code class="hljs-code">ranked_docs = sorted(
Â Â Â Â zip(documents, scores), key=lambda x: x[1], reverse=True)
</code></pre>
      </li>
    </ol>
    <p class="normal">Let us run this example to see what the results would look like. </p>
    <p class="normal">The following is a sample input query:</p>
    <pre class="programlisting code"><code class="hljs-code">What are the benefits of solar energy?
</code></pre>
    <p class="normal">The following is the sample output (ranked documents):</p>
    <pre class="programlisting code"><code class="hljs-code">Document: Solar energy is a renewable source of power., Score: 80.8264
....
Document: Graph databases like Neo4j are used to model complex relationships., Score: 52.8945
</code></pre>
    <p class="normal">The preceding <a id="_idIndexMarker079"/>code demonstrates how to <a id="_idIndexMarker080"/>use DPR to encode a query and a set of documents into high-dimensional vector representations. By computing similarity scores, such as the dot product between the query vector and document vectors, the model evaluates the relevance of each document to the query. The documents are then ranked based on their similarity scores, with the most relevant ones appearing at the top. This process highlights the power of vector-based retrieval in effectively identifying contextually relevant information from a diverse set of documents, even when they include a mix of related and unrelated content.</p>
    <p class="normal">The full version of this example is available in the GitHub repository: <a href="https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/vector_similarity_search.py">https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/vector_similarity_search.py</a>.</p>
    <h3 id="_idParaDest-36" class="heading-3">Keyword matching </h3>
    <p class="normal"><strong class="keyWord">Keyword matching</strong> is a <a id="_idIndexMarker081"/>simpler approach that identifies documents containing keywords <a id="_idIndexMarker082"/>present in the user prompt. While efficient, it can be susceptible to noise and misses documents with relevant synonyms. BM25 is a keyword-based probabilistic retri<a id="_idTextAnchor016"/>eval function that scores documents based on the query terms appearing in each document, considering term frequency and document length. The flow of this approach looks as follows:</p>
    <ol>
      <li class="numberedList" value="1">Build the BM25 corpus using the documents. We will tokenize documents and build a corpus from this. We will build the BM25 corpus:
        <pre class="programlisting code"><code class="hljs-code">tokenized_corpus = [doc.split() for doc in corpus]
# Initialize BM25 with the tokenized corpus
bm25 = BM25Okapi(tokenized_corpus, k1=1.5, b=0.75)
</code></pre>
      </li>
      <li class="numberedList">Tokenize the query to search using it:
        <pre class="programlisting code"><code class="hljs-code">tokenized_query = query.split()
</code></pre>
      </li>
      <li class="numberedList">Query the BM25 corpus using the tokenized query. This returns the scores for matching documents:
        <pre class="programlisting code"><code class="hljs-code">scores = bm25.get_scores(tokenized_query)
</code></pre>
      </li>
      <li class="numberedList">We will take these scores, order the documents in the required order, and return them:
        <pre class="programlisting code"><code class="hljs-code">ranked_docs = sorted(zip(corpus, scores), key=lambda x: x[1], 
Â Â Â Â reverse=True)
</code></pre>
      </li>
    </ol>
    <p class="normal">When we run this example, the results would look like this for the given input.</p>
    <p class="normal">The following is a sample input query:</p>
    <pre class="programlisting code"><code class="hljs-code">quick fox
</code></pre>
    <p class="normal">The following is the sample output:</p>
    <pre class="programlisting code"><code class="hljs-code">Ranked Documents:
Document: The quick brown fox jumps over the lazy dog., Score: 0.6049
.....
Document: Artificial intelligence is transforming the world., Score: 0.0000
</code></pre>
    <p class="normal">The BM25<a id="_idIndexMarker083"/> algorithm ranks documents based on their <a id="_idIndexMarker084"/>relevance to a query. It relies on the term frequency (how often a keyword appears in a document) and document length, applying a probabilistic scoring function to evaluate relevance. Unlike vector similarity search, which represents both queries and documents as dense numerical vectors in high-dimensional space and measures similarity using mathematical functions such as the dot product, BM25 operates directly on discrete word matches. This means BM25 is efficient and interpretable but can struggle with semantic relationships, as it cannot recognize synonyms or contextual meanings. In contrast, vector similarity search, such as DPR, excels in identifying conceptual similarities even when exact keywords differ, making it more suitable for tasks requiring deep semantic understanding. This snippet illustrates BM25â€™s utility for straightforward keyword-matching tasks where efficiency and explainability are critical.</p>
    <p class="normal">The complete example is available in the GitHub repository: <a href="https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/keyword_matching.py">https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/keyword_matching.py</a>.</p>
    <h3 id="_idParaDest-37" class="heading-3">Passage retrieval </h3>
    <p class="normal">Instead of retrieving entire<a id="_idIndexMarker085"/> documents, RAG can focus on specific passages within documents<a id="_idIndexMarker086"/> that directly address the userâ€™s query. This allows for more precise information extraction. The initial flow of this approach is very similar to the vector search approach. We get the ranked documents using the approach shown in vector search and then extract relevant passages as shown in the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code"># Extract passages for the reader
passages = [doc for doc, score in ranked_docs]
 
# Prepare inputs for the reader
inputs = reader_tokenizer(
Â Â Â Â questions=query,
Â Â Â Â titles=["Passage"] * len(passages),
Â Â Â Â texts=passages,
Â Â Â Â return_tensors="pt",
Â Â Â Â padding=True,
Â Â Â Â truncation=True
)
# Use the reader to extract the most relevant passage
with torch.no_grad():
Â Â Â Â outputs = reader(**inputs)
# Extract the passage with the highest score
max_score_index = torch.argmax(outputs.relevance_logits)
most_relevant_passage = passages[max_score_index]
</code></pre>
    <p class="normal">When we run this example for the given input query, the results look as follows.</p>
    <p class="normal">The following is a sample input query:</p>
    <pre class="programlisting code"><code class="hljs-code">What are the benefits of solar energy?
</code></pre>
    <p class="normal">The following is the sample output:</p>
    <pre class="programlisting code"><code class="hljs-code">Ranked Documents:
Document: Solar energy is a renewable source of power., Score: 80.8264
.....
Document: It has low maintenance costs., Score: 57.9905
 
Most Relevant Passage: Solar panels help combat climate change and reduce carbon footprint.
</code></pre>
    <p class="normal">The <a id="_idIndexMarker087"/>preceding example illustrates the <strong class="keyWord">passage-retrieval approach</strong>, which is<a id="_idIndexMarker088"/> more granular than document-level retrieval, focusing on extracting specific passages that directly address the userâ€™s query. By leveraging a <em class="italic">reader model</em> in combination with a <em class="italic">retriever</em>, this approach enhances relevance and specificity, as it identifies not only the most relevant document but also the exact passage within it that best answers the query. </p>
    <p class="normal">Even if a passage has a slightly lower retriever score, the reader may prioritize it because it evaluates relevance more precisely at the word and span levels, considering contextual nuances. The retriever typically calculates a similarity score using the dot product of the query and passage embeddings:</p>
    <figure class="mediaobject"><a id="_idIndexMarker089"/><img src="img/B21107_02_001.png" alt="" width="622" height="165"/></figure>
    <p class="normal">Here, ğ‘ is <a id="_idIndexMarker090"/>the query embedding, <a id="_idIndexMarker091"/><img src="img/B21107_02_002.png" alt="" width="36" height="56"/> is the <a id="_idIndexMarker092"/>passage embedding for the <a id="_idIndexMarker093"/><img src="img/B21107_02_003.png" alt="" width="51" height="47"/> passage, and ğ‘‘ is the dimensionality of the embeddings.</p>
    <p class="normal">The reader, however, refines this further by analyzing the text content of each passage. It assigns a <strong class="keyWord">relevance score</strong> or <strong class="screenText">logit</strong> (also known as <strong class="keyWord">confidence score</strong>) based on the likelihood that a given passage<a id="_idIndexMarker094"/> contains the answer. This relevance score is computed from the raw <a id="_idIndexMarker095"/>outputs (logits) of the reader model, which considers word-level and span-level interactions between the query and the passage. The formula for the relevance score can be expressed as follows:</p>
    <figure class="mediaobject"><a id="_idIndexMarker096"/><img src="img/B21107_02_004.png" alt="" width="875" height="55"/></figure>
    <p class="normal">Here, we have the following:</p>
    <ul>
      <li class="bulletList">logits(<a id="_idIndexMarker097"/><img src="img/B21107_02_002.png" alt="" width="36" height="56"/>) refers to the raw scores assigned to the passage, <a id="_idIndexMarker098"/><img src="img/B21107_02_002.png" alt="" width="36" height="56"/>, by the reader</li>
      <li class="bulletList">softmax converts these raw scores into probabilities, emphasizing the passage most likely to be relevant (<a href="https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html">https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html</a>)</li>
    </ul>
    <p class="normal">By combining both stages, the system can identify passages that are not only semantically similar (<em class="italic">retriever stage</em>) but also contextually aligned with the queryâ€™s intent (<em class="italic">reader stage</em>). </p>
    <p class="normal">This dual-stage process highlights the strength of passage retrieval in generating highly targeted responses in information retrieval pipelines.</p>
    <p class="normal">The complete example is available in the GitHub repository: <a href="https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/passage_retrieval.py">https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/passage_retrieval.py</a>.</p>
    <h2 id="_idParaDest-38" class="heading-2">Integrating the retrieved information</h2>
    <p class="normal">For the last step in the <a id="_idIndexMarker099"/>RAG flow, let us look at how we can combine the retriever information with the generation model in a way that synthesizes contextually relevant and coherent responses. Unlike earlier examples, this approach explicitly integrates multiple retrieved passages with the query. By doing so, it creates a single input for the generation model. This allows the model to synthesize a unified and enriched response that goes beyond merely selecting or ranking passages:</p>
    <pre class="programlisting code"><code class="hljs-code">def integrate_and_generate(query, retrieved_docs):
Â Â Â Â # Combine query and retrieved documents into a single input
Â Â Â Â input_text = f"Answer this question based on the following context: {query} Context: {' '.join(retrieved_docs)}"
Â Â Â Â 
Â Â Â Â # Tokenize input for T5
Â Â Â Â inputs = t5_tokenizer(input_text, return_tensors="pt", 
Â Â Â Â Â Â Â Â padding=True, truncation=True, max_length=512)
Â Â Â Â 
Â Â Â Â # Generate a response
Â Â Â Â with torch.no_grad():
Â Â Â Â Â Â Â Â outputs = t5_model.generate(**inputs, max_length=100)
Â Â Â Â 
Â Â Â Â # Decode and return the generated response
Â Â Â Â return t5_tokenizer.decode(outputs[0], skip_special_tokens=True)
</code></pre>
    <p class="normal">The following is a <a id="_idIndexMarker100"/>sample input query:</p>
    <pre class="programlisting code"><code class="hljs-code">What are the benefits of solar energy?
</code></pre>
    <p class="normal">The following is the sample output:</p>
    <pre class="programlisting code"><code class="hljs-code">Ranked Documents:
Document: Solar energy is a renewable source of power., Score: 80.8264
....
Document: It has low maintenance costs., Score: 57.9905
 
Most Relevant Passage: Solar panels help combat climate change and reduce carbon footprint.
</code>
sized response. The <code class="inlineCode">generate()</code> function processes the combined input (query and passages) through the encoder to produce contextual embeddings, <em class="italic">â„</em>. These embeddings are then used by the decoder, which generates each token sequentially based on probabilities:</pre>
    <figure class="mediaobject"><a id="_idIndexMarker101"/><img src="img/B21107_02_007.png" alt="" width="750" height="58"/></figure>
    <p class="normal">Here, <a id="_idIndexMarker102"/><img src="img/B21107_02_008.png" alt="" width="47" height="52"/> is the token at position <a id="_idIndexMarker103"/><img src="img/B21107_02_009.png" alt="" width="18" height="45"/>, <a id="_idIndexMarker104"/><img src="img/B21107_02_010.png" alt="" width="40" height="49"/> is the hidden state, and <a id="_idIndexMarker105"/><img src="img/B21107_02_011.png" alt="" width="42" height="47"/> is the modelâ€™s weight matrix. Beam search ensures th<a id="_idTextAnchor018"/>e selection of the most likely sequence by maximizing the overall probability across tokens. Unlike previous examples where individual passages were selected or ranked, this code explicitly combines multiple retrieved documents into a single input alongside the query. This enables the T5 model to process the combined context holistically and produce a coherent response that incorporates information from multiple sources, making it particularly effective for queries requiring synthesis or summarization across multiple passages.</p>
    <p class="normal">To refer to the full version of this code, please refer: <a href="https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/integrate_and_generate.py">https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/integrate_and_generate.py</a></p>
    <p class="normal">By exploring various<a id="_idIndexMarker106"/> retrieval techniques and their integration with generation models, we have seen how RAG architectures leverage external knowledge to produce accurate and informative responses. </p>
    <p class="normal">In the next section, let us look at the holistic flow from reading input documents from a source and leveraging those documents for a retriever flow, instead of the simple hardcoded sentences we looked at in the examples in this section.</p>
    <h1 id="_idParaDest-39" class="heading-1">Building an end-to-end RAG flow</h1>
    <p class="normal">In the <a id="_idIndexMarker107"/>previous sections, we delved into the various steps in the RAG flow individually with simple data to demonstrate the usage. It would be a good idea to take a step back and use a real-world dataset, albeit a simple one, to complete the whole flow. For this, we will use the GitHub issues dataset (<a href="https://huggingface.co/datasets/lewtun/github-issues">https://huggingface.co/datasets/lewtun/github-issues</a>). We will look at how we can read this data and use it in the RAG flow. This would lay the foundation for the full end-to-end RAG flow implementation in later chapters. </p>
    <p class="normal">In this example, we will load GitHub comments to be able to answer questions, such as how we can load data offline. We need to follow these steps to load the data and set up the retriever:</p>
    <ol>
      <li class="numberedList" value="1"><strong class="screenText">Preparing the data</strong>: First, we need to prepare our dataset. We will use the Hugging Face <code class="inlineCode">datasets</code> library:
        <pre class="programlisting code"><code class="hljs-code"># Load the GitHub issues dataset
issues_dataset = load_dataset("lewtun/github-issues", split="train")
 
# Filter out pull requests and keep only issues with comments
issues_dataset = issues_dataset.filter(
Â Â Â Â lambda x: not x["is_pull_request"] and len(x["comments"]) &gt; 0)
</code></pre>
      </li>
      <li class="numberedList"><strong class="screenText">Select the relevant columns</strong>: Keep only the columns required for analysis:
        <pre class="programlisting code"><code class="hljs-code"># Define columns to keep
columns_to_keep = ["title", "body", "html_url", "comments"]
columns_to_remove = set(issues_dataset.column_names) - \ 
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â set(columns_to_keep)
# Remove unnecessary columns
issues_dataset = issues_dataset.remove_columns(columns_to_remove)
</code></pre>
      </li>
      <li class="numberedList"><strong class="screenText">Convert the dataset into a pandas DataFrame</strong>: Convert the dataset into a <code class="inlineCode">pandas</code> DataFrame for easier manipulation:
        <pre class="programlisting code"><code class="hljs-code"># Set format to pandas and convert the dataset
issues_dataset.set_format("pandas")
df = issues_dataset[:]
</code></pre>
      </li>
      <li class="numberedList"><strong class="screenText">Explode comments, convert them back into a dataset, and process</strong>: Flatten the comments into<a id="_idIndexMarker108"/> individual rows, convert the DataFrame back into a dataset, and compute the length of each comment. This step makes this data amenable to use with the retriever flow:
        <pre class="programlisting code"><code class="hljs-code"># Explode comments into separate rows
comments_df = df.explode("comments", ignore_index=True) 
# Convert the DataFrame back to a Dataset
comments_dataset = Dataset.from_pandas(comments_df)
Â Â 
# Compute the length of each comment
comments_dataset = comments_dataset.map(
Â Â Â Â lambda x: {"comment_length": len(x["comments"].split())}, 
Â Â Â Â num_proc=1)
# Filter out short comments
comments_dataset = comments_dataset.filter(
Â Â Â Â lambda x: x["comment_length"] &gt; 15)
</code></pre>
      </li>
      <li class="numberedList"><strong class="screenText">Concatenate text for embeddings</strong>: Let us prepare the document text by concatenating <a id="_idIndexMarker109"/>the relevant text fields. We will take individual fields from each row and prepare the text that represents the document text for that row. These documents are stored in an embedding store for retriever usage purposes:
        <pre class="programlisting code"><code class="hljs-code"># Function to concatenate text fields
def concatenate_text(examples):
Â Â Â Â return {
Â Â Â Â Â Â Â "text": examples["title"] + " \n " + 
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â examples["body"] + " \n " + 
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â examples["comments"]
Â Â Â Â }
# Apply the function to create a text field
comments_dataset = comments_dataset.map(concatenate_text, 
Â Â Â Â num_proc=1)
</code></pre>
      </li>
      <li class="numberedList"><strong class="screenText">Load model and tokenizer</strong>: Let us load the LLM that we will use to convert the documents into the embeddings and store them in an embedding store for the retriever flow:
        <pre class="programlisting code"><code class="hljs-code"># Load pre-trained model and tokenizer
model_ckpt = "sentence-transformers/all-MiniLM-L6-v2"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt).to("cpu")
</code></pre>
      </li>
      <li class="numberedList"><strong class="screenText">Define the embedding function</strong>: Define the embedding function that leverages the model we defined previously to generate the embedding. We can invoke this method<a id="_idIndexMarker110"/> iteratively to generate the embeddings for all the documents we have, one document at a time:
        <pre class="programlisting code"><code class="hljs-code"># Function to get embeddings for a list of texts
def get_embeddings(text_list):
Â Â Â Â encoded_input = tokenizer(text_list, padding=True, 
Â Â Â Â Â Â Â Â truncation=True, return_tensors="pt").to("cpu")
Â Â Â Â with torch.no_grad():
Â Â Â Â Â Â Â Â model_output = model(**encoded_input)
Â Â Â Â return cls_pooling(model_output).numpy()
</code></pre>
      </li>
      <li class="numberedList"><strong class="screenText">Compute embeddings</strong>: Compute embeddings for the dataset. Now that we have the embedding function defined, let us call it for all the documents we have in our comments <a id="_idIndexMarker111"/>dataset. Note that we are storing the embedding in a new column named <code class="inlineCode">embedding</code> in the same dataset:
        <pre class="programlisting code"><code class="hljs-code"># Compute embeddings for the dataset
comments_dataset = comments_dataset.map(
Â Â Â Â lambda batch: {"embeddings": [get_embeddings([text])[0] 
        for text in batch["text"]]},
Â Â Â Â batched=True,
Â Â Â Â batch_size=100,
Â Â Â Â num_proc=1
)
</code></pre>
      </li>
      <li class="numberedList"><strong class="screenText">Perform semantic search</strong>: Let us perform the retriever flow for the question. This will retrieve all the questions related to the question we have asked. We can use these documents to refine the response as needed:
        <pre class="programlisting code"><code class="hljs-code"># Define a query
question = "How can I load a dataset offline?"
# Compute the embedding for the query
query_embedding = get_embeddings([question]).reshape(1, -1) 
# Find the nearest examples
embeddings = np.vstack(comments_dataset["embeddings"])
similarities = cosine_similarity(
Â Â Â Â query_embedding, embeddings
).flatten()
# Display the results
top_indices = np.argsort(similarities)[::-1][:5]
for idx in top_indices:
Â Â Â Â result = comments_dataset[int(idx)]Â Â # Convert NumPy integer to native Python integer
Â Â Â Â print(f"COMMENT: {result['comments']}")
Â Â Â Â print(f"SCORE: {similarities[idx]}")
Â Â Â Â print(f"TITLE: {result['title']}")
Â Â Â Â print(f"URL: {result['html_url']}")
Â Â Â Â print("=" * 50)
</code></pre>
      </li>
    </ol>
    <p class="normal">The preceding code<a id="_idIndexMarker112"/> showcases the complete flow, from how we load the data into a data store, which can form the basis for the retriever, to retrieving documents, which can be used to provide more context for the LLM when it is generating the answer.</p>
    <p class="normal">Now let us see how the output looks when we run this application. We have the question hardcoded in the example code, and it is:</p>
    <pre class="programlisting code"><code class="hljs-code">How can I load a dataset offline?.
</code></pre>
    <p class="normal">The following is the sample output:</p>
    <pre class="programlisting code"><code class="hljs-code">COMMENT: Yes currently you need an internet connection because the lib tries to check for the etag of the dataset script ...
SCORE: 0.9054292969045314
TITLE: Downloaded datasets are not usable offline
URL: https://github.com/huggingface/datasets/issues/761
==================================================
COMMENT: Requiring online connection is a deal breaker in some cases ...
SCORE: 0.9052456782359709
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================
</code></pre>
    <p class="normal">This hands-on <a id="_idIndexMarker113"/>example demonstrated the practical application of an end-to-end RAG architectur<a id="_idTextAnchor019"/>e, leveraging powerful retrieval techniques to enhance language generation. The preceding code is adapted from the Hugging Face NLP course, available at <a href="https://huggingface.co/learn/nlp-course/chapter5/6?fw=tf">https://huggingface.co/learn/nlp-course/chapter5/6?fw=tf</a>.</p>
    <p class="normal">The complete Python file, along with a detailed explanation of how to run it, is available at <a href="https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/full_rag_pipeline.py">https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/full_rag_pipeline.py</a>. </p>
    <h1 id="_idParaDest-40" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we deep-dived into the world of RAG models. We started by understanding the core principles of RAG and how they differ from traditional generative AI models. This foundational knowledge is crucial as it sets the stage for appreciating the enhanced capabilities that RAG brings to the table. </p>
    <p class="normal">Next, we took a closer look at the architecture of RAG models, deconstructing their components through detailed code examples. By examining the encoder, retriever, and decoder, you gained insights into the inner workings of these models and how they integrate retrieved information to produce more contextually relevant and coherent outputs. </p>
    <p class="normal">We then explored how RAG harnesses the power of information retrieval. These techniques help RAG effectively leverage external knowledge sources to improve the quality of a generated text. This is particularly useful for applications requiring high accuracy and context awareness. You also learned how to a simple RAG model using popular libraries such as Transformers and Hugging Face.</p>
    <p class="normal">As we move forward to the next chapter, <a href="Preface.xhtml#_idTextAnchor012"><em class="italic">Chapter 3</em></a>, we will build on this foundation. You will learn about graph data modeling and how to create knowledge graphs with Neo4j.</p>
  </div>
</div></div></body></html>