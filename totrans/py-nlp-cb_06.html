<html><head></head><body>
		<div id="_idContainer025" class="calibre2">
			<h1 id="_idParaDest-152" class="chapter-number"><a id="_idTextAnchor156" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1.1">6</span></h1>
			<h1 id="_idParaDest-153" class="calibre7"><a id="_idTextAnchor157" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.2.1">Topic Modeling</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.3.1">In this chapter, we will cover </span><strong class="bold"><span class="kobospan" id="kobo.4.1">topic modeling</span></strong><span class="kobospan" id="kobo.5.1">, or </span><a id="_idIndexMarker320" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.6.1">the classification of topics present in a corpus of text. </span><span class="kobospan" id="kobo.6.2">Topic modeling is a very useful technique that can give us an idea about which topics appear in a document set. </span><span class="kobospan" id="kobo.6.3">For example, topic modeling is used for trend discovery on social media. </span><span class="kobospan" id="kobo.6.4">Also, in many cases, it is useful to do topic modeling as part of the preliminary data analysis of a dataset to understand which topics appear </span><span><span class="kobospan" id="kobo.7.1">in it.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.8.1">There are many different algorithms available to do this. </span><span class="kobospan" id="kobo.8.2">All of them try to find similarities between different texts and put them into several clusters. </span><span class="kobospan" id="kobo.8.3">These different clusters indicate </span><span><span class="kobospan" id="kobo.9.1">different topics.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.10.1">You will learn how to create and use topic models via various techniques with the BBC news dataset in this chapter. </span><span class="kobospan" id="kobo.10.2">This dataset has news that falls within the following topics: politics, sport, business, tech, and entertainment. </span><span class="kobospan" id="kobo.10.3">Thus, we know that in each case, we need to have five topic clusters. </span><span class="kobospan" id="kobo.10.4">This is not going to be the case in real-life scenarios, and you will need to estimate the number of topic clusters. </span><span class="kobospan" id="kobo.10.5">A good reference on how to do this is </span><em class="italic"><span class="kobospan" id="kobo.11.1">The Hundred-Page Machine Learning Book</span></em><span class="kobospan" id="kobo.12.1"> by Andriy Burkov (</span><span><span class="kobospan" id="kobo.13.1">p. </span><span class="kobospan" id="kobo.13.2">112).</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.14.1">This chapter contains the </span><span><span class="kobospan" id="kobo.15.1">following recipes:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.16.1">LDA topic modeling </span><span><span class="kobospan" id="kobo.17.1">with </span></span><span><strong class="source-inline1"><span class="kobospan" id="kobo.18.1">gensim</span></strong></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.19.1">Community detection clustering </span><span><span class="kobospan" id="kobo.20.1">with SBERT</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.21.1">K-Means topic modeling </span><span><span class="kobospan" id="kobo.22.1">with BERT</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.23.1">Topic modeling </span><span><span class="kobospan" id="kobo.24.1">using BERTopic</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.25.1">Using contextualized </span><span><span class="kobospan" id="kobo.26.1">topic models</span></span></li>
			</ul>
			<h1 id="_idParaDest-154" class="calibre7"><a id="_idTextAnchor158" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.27.1">Technical requirements</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.28.1">In this chapter, we will work with the same BBC dataset that we worked with in </span><a href="B18411_04.xhtml#_idTextAnchor106" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.29.1">Chapter 4</span></em></span></a><span class="kobospan" id="kobo.30.1">. </span><span class="kobospan" id="kobo.30.2">The dataset is located in the book </span><span><span class="kobospan" id="kobo.31.1">GitHub repository</span></span><span><span class="kobospan" id="kobo.32.1">:</span></span></p>
			<p class="calibre3"><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/bbc-text.csv" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.33.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/bbc-text.csv</span></span></a></p>
			<p class="calibre3"><span class="kobospan" id="kobo.34.1">It is also available through </span><span><span class="kobospan" id="kobo.35.1">Hugging Face:</span></span></p>
			<p class="calibre3"><a href="https://huggingface.co/datasets/SetFit/bbc-news" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.36.1">https://huggingface.co/datasets/SetFit/bbc-news</span></span></a></p>
			<p class="callout-heading"><span class="kobospan" id="kobo.37.1">Note</span></p>
			<p class="callout"><span class="kobospan" id="kobo.38.1">This dataset is used in this book with permission from the researchers. </span><span class="kobospan" id="kobo.38.2">The original paper associated with this dataset is </span><span><span class="kobospan" id="kobo.39.1">as follows:</span></span></p>
			<p class="callout"><span class="kobospan" id="kobo.40.1">Derek Greene and Pádraig Cunningham. </span><span class="kobospan" id="kobo.40.2">“Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering”, in Proc. </span><span class="kobospan" id="kobo.40.3">23rd International Conference on Machine Learning (</span><span><span class="kobospan" id="kobo.41.1">ICML’06), 2006.</span></span></p>
			<p class="callout"><span class="kobospan" id="kobo.42.1">All rights, including copyright, in the text content of the original articles are owned by </span><span><span class="kobospan" id="kobo.43.1">the BBC.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.44.1">Please make sure to download all the Python notebooks in the </span><strong class="source-inline"><span class="kobospan" id="kobo.45.1">util</span></strong><span class="kobospan" id="kobo.46.1"> folder on GitHub into the </span><strong class="source-inline"><span class="kobospan" id="kobo.47.1">util</span></strong><span class="kobospan" id="kobo.48.1"> folder on your computer. </span><span class="kobospan" id="kobo.48.2">The directory structure on your computer should mirror the setup in the GitHub repository. </span><span class="kobospan" id="kobo.48.3">We will be accessing files in this directory in several recipes in </span><span><span class="kobospan" id="kobo.49.1">this chapter.</span></span></p>
			<h1 id="_idParaDest-155" class="calibre7"><a id="_idTextAnchor159" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.50.1">LDA topic modeling with gensim</span></h1>
			<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.51.1">Latent Dirichlet Allocation</span></strong><span class="kobospan" id="kobo.52.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.53.1">LDA</span></strong><span class="kobospan" id="kobo.54.1">) is </span><a id="_idIndexMarker321" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.55.1">one of the oldest algorithms for topic modeling. </span><span class="kobospan" id="kobo.55.2">It is a statistical generative model that calculates the </span><a id="_idIndexMarker322" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.56.1">probabilities of different words. </span><span class="kobospan" id="kobo.56.2">In general, LDA is a good choice of model for </span><span><span class="kobospan" id="kobo.57.1">longer texts.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.58.1">We will use one of the </span><a id="_idIndexMarker323" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.59.1">main topic modeling algorithms, LDA, to create a topic model for the BBC news texts. </span><span class="kobospan" id="kobo.59.2">We know that the BBC news dataset has five topics: tech, politics, business, entertainment, and sport. </span><span class="kobospan" id="kobo.59.3">Thus, we will use five as the expected number </span><span><span class="kobospan" id="kobo.60.1">of clusters.</span></span></p>
			<h2 id="_idParaDest-156" class="calibre5"><a id="_idTextAnchor160" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.61.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.62.1">We will</span><a id="_idIndexMarker324" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.63.1"> be using the </span><strong class="source-inline"><span class="kobospan" id="kobo.64.1">gensim</span></strong><span class="kobospan" id="kobo.65.1"> package, which is part of the </span><a id="_idIndexMarker325" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.66.1">poetry environment. </span><span class="kobospan" id="kobo.66.2">You can also install the </span><strong class="source-inline"><span class="kobospan" id="kobo.67.1">requirements.txt</span></strong><span class="kobospan" id="kobo.68.1"> file to get </span><span><span class="kobospan" id="kobo.69.1">the package.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.70.1">The dataset is located at </span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/bbc-text.csv" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.71.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/bbc-text.csv</span></a><span class="kobospan" id="kobo.72.1"> and should be downloaded to the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.73.1">data</span></strong></span><span><span class="kobospan" id="kobo.74.1"> folder.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.75.1">The notebook is located </span><span><span class="kobospan" id="kobo.76.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.1_topic_modeling_gensim.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.77.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.1_topic_modeling_gensim.ipynb</span></span></a><span><span class="kobospan" id="kobo.78.1">.</span></span></p>
			<h2 id="_idParaDest-157" class="calibre5"><a id="_idTextAnchor161" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.79.1">How to do it...</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.80.1">An LDA model requires the data to be clean. </span><span class="kobospan" id="kobo.80.2">This means that stopwords and other unnecessary tokens need to be removed from the text. </span><span class="kobospan" id="kobo.80.3">This includes digits and punctuation. </span><span class="kobospan" id="kobo.80.4">If this step is skipped, topics that center around stopwords, digits, or punctuation </span><span><span class="kobospan" id="kobo.81.1">might appear.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.82.1">We will load the data, clean it, and preprocess it using the </span><strong class="source-inline"><span class="kobospan" id="kobo.83.1">simple_preprocess</span></strong><span class="kobospan" id="kobo.84.1"> function, which is available through the </span><strong class="source-inline"><span class="kobospan" id="kobo.85.1">gensim</span></strong><span class="kobospan" id="kobo.86.1"> package. </span><span class="kobospan" id="kobo.86.2">Then we will create the LDA model. </span><span class="kobospan" id="kobo.86.3">Any topic model requires the engineer to estimate the number of topics in advance. </span><span class="kobospan" id="kobo.86.4">We will use five, as we know that there are five topics present in the data. </span><span class="kobospan" id="kobo.86.5">For more information on how to estimate the number of topics, please see the introductory section of </span><span><span class="kobospan" id="kobo.87.1">this chapter.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.88.1">The steps are </span><span><span class="kobospan" id="kobo.89.1">as follows:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.90.1">Do the </span><span><span class="kobospan" id="kobo.91.1">necessary imports:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.92.1">
import pandas as pd
import nltk
import re
from nltk.corpus import stopwords
from gensim.utils import simple_preprocess
from gensim.models.ldamodel import LdaModel
import gensim.corpora as corpora
from pprint import pprint
from gensim.corpora import MmCorpus</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.93.1">Load the</span><a id="_idIndexMarker326" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.94.1"> stopwords and the BBC news data, then print the resulting dataframe. </span><span class="kobospan" id="kobo.94.2">Here, we use the standard stopword list from NLTK. </span><span class="kobospan" id="kobo.94.3">As we saw in </span><a href="B18411_04.xhtml#_idTextAnchor106" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.95.1">Chapter 4</span></em></span></a><span class="kobospan" id="kobo.96.1">, in the </span><em class="italic"><span class="kobospan" id="kobo.97.1">Clustering sentences using K-Means: unsupervised text classification</span></em><span class="kobospan" id="kobo.98.1"> recipe, the </span><strong class="source-inline1"><span class="kobospan" id="kobo.99.1">said</span></strong><span class="kobospan" id="kobo.100.1"> word is also considered a stopword in this dataset, so we must manually </span><a id="_idIndexMarker327" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.101.1">add it to </span><span><span class="kobospan" id="kobo.102.1">the list.</span></span><pre class="source-code"><span class="kobospan1" id="kobo.103.1">
stop_words = stopwords.words('english')
stop_words.append("said")
bbc_df = pd.read_csv("../data/bbc-text.csv")
print(bbc_df)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.104.1">The result will look similar </span><span><span class="kobospan" id="kobo.105.1">to this:</span></span></p></li>			</ol>
			<div class="calibre2">
				<div id="_idContainer019" class="img---figure">
					<span class="kobospan" id="kobo.106.1"><img src="image/B18411_06_1.jpg" alt="" role="presentation" class="calibre4"/></span>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.107.1">Figure 6.1 – The BBC news dataframe output</span></p>
			<ol class="calibre13">
				<li value="3" class="calibre14"><span class="kobospan" id="kobo.108.1">In this step, we will create the </span><strong class="source-inline1"><span class="kobospan" id="kobo.109.1">clean_text</span></strong><span class="kobospan" id="kobo.110.1"> function. </span><span class="kobospan" id="kobo.110.2">This function removes extra whitespace in the first line and digits in the second line from text. </span><span class="kobospan" id="kobo.110.3">It then uses the </span><strong class="source-inline1"><span class="kobospan" id="kobo.111.1">simple_preprocess</span></strong><span class="kobospan" id="kobo.112.1"> function from </span><strong class="source-inline1"><span class="kobospan" id="kobo.113.1">gensim</span></strong><span class="kobospan" id="kobo.114.1">. </span><span class="kobospan" id="kobo.114.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.115.1">simple_preprocess</span></strong><span class="kobospan" id="kobo.116.1"> function splits the text into a list of tokens, lowercases them, and </span><a id="_idIndexMarker328" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.117.1">removes tokens that are too long or too short. </span><span class="kobospan" id="kobo.117.2">We then remove stopwords </span><a id="_idIndexMarker329" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.118.1">from </span><span><span class="kobospan" id="kobo.119.1">the list:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.120.1">
def clean_text(input_string):
    input_string = re.sub(r'[^\w\s]', ' ', input_string)
    input_string = re.sub(r'\d', '', input_string)
    input_list = simple_preprocess(input_string)
    input_list = [word for word in input_list if word not in 
        stop_words]
    return input_list</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.121.1">We will now apply the function to the data. </span><span class="kobospan" id="kobo.121.2">The text column now contains a list of words that are in lowercase and </span><span><span class="kobospan" id="kobo.122.1">no stopwords:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.123.1">
bbc_df['text'] = bbc_df['text'].apply(lambda x: clean_text(x))
print(bbc_df)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.124.1">The result will look similar </span><span><span class="kobospan" id="kobo.125.1">to this:</span></span></p></li>			</ol>
			<div class="calibre2">
				<div id="_idContainer020" class="img---figure">
					<span class="kobospan" id="kobo.126.1"><img src="image/B18411_06_2.jpg" alt="" role="presentation" class="calibre4"/></span>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.127.1">Figure 6.2 – The processed BBC news output</span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.128.1">Here, we </span><a id="_idIndexMarker330" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.129.1">will use the </span><strong class="source-inline"><span class="kobospan" id="kobo.130.1">gensim.corpora.Dictionary</span></strong><span class="kobospan" id="kobo.131.1"> class to create a mapping from a word to its integer ID. </span><span class="kobospan" id="kobo.131.2">This is necessary to then create a bag-of-words representation of the text. </span><span class="kobospan" id="kobo.131.3">Then, using</span><a id="_idIndexMarker331" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.132.1"> this mapping, we create the corpus as a bag of words. </span><span class="kobospan" id="kobo.132.2">To learn more about the bag-of-words concept, please see </span><a href="B18411_03.xhtml#_idTextAnchor067" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.133.1">Chapter 3</span></em></span></a><span class="kobospan" id="kobo.134.1">, </span><em class="italic"><span class="kobospan" id="kobo.135.1">Putting Documents into a Bag of Words</span></em><span class="kobospan" id="kobo.136.1">. </span><span class="kobospan" id="kobo.136.2">In this recipe, instead of using </span><strong class="source-inline"><span class="kobospan" id="kobo.137.1">sklean</span></strong><span class="kobospan" id="kobo.138.1">’s </span><strong class="source-inline"><span class="kobospan" id="kobo.139.1">CountVectorizer</span></strong><span class="kobospan" id="kobo.140.1"> class, we will use the classes provided by the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.141.1">gensim</span></strong></span><span><span class="kobospan" id="kobo.142.1"> package:</span></span></p>
			<pre class="source-code"><span class="kobospan1" id="kobo.143.1">
texts = bbc_df['text'].values
id_dict = corpora.Dictionary(texts)
corpus = [id_dict.doc2bow(text) for text in texts]</span></pre>			<ol class="calibre13">
				<li value="5" class="calibre14"><span class="kobospan" id="kobo.144.1">In this step, we will initialize and train the LDA model. </span><span class="kobospan" id="kobo.144.2">We will pass in the preprocessed and vectorized data (</span><strong class="source-inline1"><span class="kobospan" id="kobo.145.1">corpus</span></strong><span class="kobospan" id="kobo.146.1">), the word-to-ID mapping (</span><strong class="source-inline1"><span class="kobospan" id="kobo.147.1">id_dict</span></strong><span class="kobospan" id="kobo.148.1">), the number of topics, which we initialized to five, the chunk size, and the number of passes. </span><span class="kobospan" id="kobo.148.2">The chunk size determines the number of documents used in each training chunk, and the number of passes specifies the number of passes through the corpus during training. </span><span class="kobospan" id="kobo.148.3">You can experiment with these hyperparameters to see whether they improve the model. </span><span class="kobospan" id="kobo.148.4">The parameters used here, 100 documents per chunk and 20 passes, were chosen experimentally to produce a </span><span><span class="kobospan" id="kobo.149.1">good model:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.150.1">
num_topics = 5
lda_model = LdaModel(corpus=corpus,
                     id2word=id_dict,
                     num_topics=num_topics,
                     chunksize=100,
                     passes=20)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.151.1">Here, we </span><a id="_idIndexMarker332" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.152.1">will print the resulting topics using the </span><a id="_idIndexMarker333" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.153.1">pretty print (</span><strong class="source-inline"><span class="kobospan" id="kobo.154.1">pprint</span></strong><span class="kobospan" id="kobo.155.1">) function that arranges them nicely. </span><span class="kobospan" id="kobo.155.2">The results will vary each time you train the model. </span><span class="kobospan" id="kobo.155.3">For this training pass, topic 0 is tech, topic 1 is sport, topic 2 is business, topic 3 is entertainment, and topic 4 </span><span><span class="kobospan" id="kobo.156.1">is politics:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.157.1">pprint(lda_model.print_topics())</span></pre><p class="calibre3"><span class="kobospan" id="kobo.158.1">The results will vary. </span><span class="kobospan" id="kobo.158.2">Our output looks </span><span><span class="kobospan" id="kobo.159.1">like this:</span></span></p></li>			</ol>
			<div class="calibre2">
				<div id="_idContainer021" class="img---figure">
					<span class="kobospan" id="kobo.160.1"><img src="image/B18411_06_3.jpg" alt="" role="presentation" class="calibre4"/></span>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.161.1">Figure 6.3 – Our LDA output</span></p>
			<h2 id="_idParaDest-158" class="calibre5"><a id="_idTextAnchor162" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.162.1">There’s more...</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.163.1">Now let’s save</span><a id="_idIndexMarker334" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.164.1"> the </span><a id="_idIndexMarker335" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.165.1">model and apply it to </span><span><span class="kobospan" id="kobo.166.1">novel input:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.167.1">Define the </span><strong class="source-inline1"><span class="kobospan" id="kobo.168.1">save_model</span></strong><span class="kobospan" id="kobo.169.1"> function. </span><span class="kobospan" id="kobo.169.2">To save the model, we need the model itself, the path where we want to save it, the vectorizer (</span><strong class="source-inline1"><span class="kobospan" id="kobo.170.1">id_dict</span></strong><span class="kobospan" id="kobo.171.1">), the path where we want to save the vectorizer, the corpus, and the path where the corpus will be saved. </span><span class="kobospan" id="kobo.171.2">The function will save these three components to their </span><span><span class="kobospan" id="kobo.172.1">corresponding paths:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.173.1">
def save_model(lda, lda_path, id_dict, dict_path, 
    corpus, corpus_path):
    lda.save(lda_path)
    id_dict.save(dict_path)
    MmCorpus.serialize(corpus_path, corpus)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.174.1">Save the model, the word-to-ID dictionary, and the vectorized corpus by using the function we </span><span><span class="kobospan" id="kobo.175.1">just defined:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.176.1">
model_path = "../models/bbc_gensim/lda.model"
dict_path = "../models/bbc_gensim/id2word.dict"
corpus_path = "../models/bbc_gensim/corpus.mm"
save_model(lda_model, model_path, id_dict, dict_path, 
    corpus, corpus_path)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.177.1">Load the </span><a id="_idIndexMarker336" class="calibre6 pcalibre pcalibre1"/><span><span class="kobospan" id="kobo.178.1">saved model:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.179.1">
lda_model = LdaModel.load(model_path)
id_dict = corpora.Dictionary.load(dict_path)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.180.1">Define a </span><a id="_idIndexMarker337" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.181.1">new example for testing. </span><span class="kobospan" id="kobo.181.2">This example is on the topic </span><span><span class="kobospan" id="kobo.182.1">of sports:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.183.1">
new_example = """Manchester United players slumped to the turf
at full-time in Germany on Tuesday in acknowledgement of what their
latest pedestrian first-half display had cost them. </span><span class="kobospan1" id="kobo.183.2">The 3-2 loss at
RB Leipzig means United will not be one of the 16 teams in the draw
for the knockout stages of the Champions League. </span><span class="kobospan1" id="kobo.183.3">And this is not the
only price for failure. </span><span class="kobospan1" id="kobo.183.4">The damage will be felt in the accounts, in
the dealings they have with current and potentially future players
and in the faith the fans have placed in manager Ole Gunnar Solskjaer.
</span><span class="kobospan1" id="kobo.183.5">With Paul Pogba's agent angling for a move for his client and ex-United
defender Phil Neville speaking of a "witchhunt" against his former team-mate
Solskjaer, BBC Sport looks at the ramifications and reaction to a big loss for United."""</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.184.1">In this </span><a id="_idIndexMarker338" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.185.1">step, we </span><a id="_idIndexMarker339" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.186.1">will preprocess the text using the same </span><strong class="source-inline1"><span class="kobospan" id="kobo.187.1">clean_text</span></strong><span class="kobospan" id="kobo.188.1"> function, then convert it into a bag-of-words vector and run it through the model. </span><span class="kobospan" id="kobo.188.2">The prediction is a list of tuples, where the first element in each tuple is the number of the topic and the second element is the probability that this text belongs to this particular topic. </span><span class="kobospan" id="kobo.188.3">In this example, we can see that topic 1 is the most probable, which is sport, and is the </span><span><span class="kobospan" id="kobo.189.1">correct identification:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.190.1">
input_list = clean_text(new_example)
bow = id_dict.doc2bow(input_list)
topics = lda_model[bow]
print(topics)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.191.1">The results will vary. </span><span class="kobospan" id="kobo.191.2">Our results look </span><span><span class="kobospan" id="kobo.192.1">like this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.193.1">[(1, 0.7338447), (2, 0.15261793), (4, 0.1073401)]</span></pre></li>			</ol>
			<h1 id="_idParaDest-159" class="calibre7"><a id="_idTextAnchor163" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.194.1">Community detection clustering with SBERT</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.195.1">In this</span><a id="_idIndexMarker340" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.196.1"> recipe, we will use the community detection</span><a id="_idIndexMarker341" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.197.1"> algorithm</span><a id="_idIndexMarker342" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.198.1"> included with the </span><strong class="bold"><span class="kobospan" id="kobo.199.1">SentenceTransformers</span></strong><span class="kobospan" id="kobo.200.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.201.1">SBERT</span></strong><span class="kobospan" id="kobo.202.1">) package. </span><span class="kobospan" id="kobo.202.2">SBERT will allow us to easily encode sentences using the BERT model. </span><span class="kobospan" id="kobo.202.3">See the </span><em class="italic"><span class="kobospan" id="kobo.203.1">Using BERT and OpenAI embeddings instead of word embeddings</span></em><span class="kobospan" id="kobo.204.1"> recipe in </span><a href="B18411_03.xhtml#_idTextAnchor067" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.205.1">Chapter 3</span></em></span></a><span class="kobospan" id="kobo.206.1"> for a more detailed explanation of how to use the </span><span><span class="kobospan" id="kobo.207.1">sentence transformers.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.208.1">This algorithm is frequently used to find </span><strong class="bold"><span class="kobospan" id="kobo.209.1">communities</span></strong><span class="kobospan" id="kobo.210.1"> in social media but can also be used for topic modeling. </span><span class="kobospan" id="kobo.210.2">The advantage of this algorithm is that it is very fast. </span><span class="kobospan" id="kobo.210.3">It works best on shorter texts, such as texts found on social media. </span><span class="kobospan" id="kobo.210.4">It also only discovers the main topics </span><a id="_idIndexMarker343" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.211.1">in the </span><a id="_idIndexMarker344" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.212.1">document dataset, as opposed to LDA, which clusters all available text. </span><span class="kobospan" id="kobo.212.2">One use of the community detection algorithm is finding duplicate posts on </span><span><span class="kobospan" id="kobo.213.1">social media.</span></span></p>
			<h2 id="_idParaDest-160" class="calibre5"><a id="_idTextAnchor164" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.214.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.215.1">We will use the SBERT package in this recipe. </span><span class="kobospan" id="kobo.215.2">It is included in the poetry environment. </span><span class="kobospan" id="kobo.215.3">You can also install it together with other packages by installing the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.216.1">requirements.txt</span></strong></span><span><span class="kobospan" id="kobo.217.1"> file.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.218.1">The notebook is located </span><span><span class="kobospan" id="kobo.219.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.2_community_detection.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.220.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.2_community_detection.ipynb</span></span></a><span><span class="kobospan" id="kobo.221.1">.</span></span></p>
			<h2 id="_idParaDest-161" class="calibre5"><a id="_idTextAnchor165" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.222.1">How to do it...</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.223.1">We will transform the text using the BERT sentence transformers model and then use the community detection clustering algorithm on the </span><span><span class="kobospan" id="kobo.224.1">resulting embeddings.</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.225.1">Do the necessary imports. </span><span class="kobospan" id="kobo.225.2">Here, you might need to download the stopwords corpus from NLTK. </span><span class="kobospan" id="kobo.225.3">Please see the </span><em class="italic"><span class="kobospan" id="kobo.226.1">Removing stopwords </span></em><span class="kobospan" id="kobo.227.1">recipe in </span><a href="B18411_01.xhtml#_idTextAnchor013" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.228.1">Chapter 1</span></em></span></a><span class="kobospan" id="kobo.229.1"> for detailed instructions on how to </span><span><span class="kobospan" id="kobo.230.1">do this.</span></span><pre class="source-code"><span class="kobospan1" id="kobo.231.1">
import pandas as pd
import nltk
import re
from nltk.corpus import stopwords
from sentence_transformers import SentenceTransformer, util</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.232.1">Run the </span><strong class="source-inline1"><span class="kobospan" id="kobo.233.1">language </span></strong><span><strong class="source-inline1"><span class="kobospan" id="kobo.234.1">utilities</span></strong></span><span><span class="kobospan" id="kobo.235.1"> file:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.236.1">
%run -i "../util/lang_utils.ipynb"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.237.1">Load the BBC data and </span><span><span class="kobospan" id="kobo.238.1">print it:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.239.1">
bbc_df = pd.read_csv("../data/bbc-text.csv")
print(bbc_df)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.240.1">The result will look like </span><span><em class="italic"><span class="kobospan" id="kobo.241.1">Figure 6</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.242.1">.1</span></em></span><span><span class="kobospan" id="kobo.243.1">.</span></span></p></li>				<li class="calibre14"><span class="kobospan" id="kobo.244.1">Load the</span><a id="_idIndexMarker345" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.245.1"> model and create the embeddings. </span><span class="kobospan" id="kobo.245.2">See the </span><em class="italic"><span class="kobospan" id="kobo.246.1">Using BERT and OpenAI embeddings instead of word embeddings</span></em><span class="kobospan" id="kobo.247.1"> recipe in </span><a href="B18411_03.xhtml#_idTextAnchor067" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.248.1">Chapter 3</span></em></span></a><span class="kobospan" id="kobo.249.1"> for more information on sentence embeddings. </span><span class="kobospan" id="kobo.249.2">The community detection algorithm requires the embeddings to be in the form of tensors; hence, we must set </span><strong class="source-inline1"><span class="kobospan" id="kobo.250.1">convert_to_tensor</span></strong> <span><span class="kobospan" id="kobo.251.1">to </span></span><span><strong class="source-inline1"><span class="kobospan" id="kobo.252.1">True</span></strong></span><span><span class="kobospan" id="kobo.253.1">:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.254.1">
model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(bbc_df["text"], convert_to_tensor=True)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.255.1">In this</span><a id="_idIndexMarker346" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.256.1"> step, we will create the clusters. </span><span class="kobospan" id="kobo.256.2">We will specify the threshold for similarity to be </span><strong class="source-inline1"><span class="kobospan" id="kobo.257.1">0.7</span></strong><span class="kobospan" id="kobo.258.1"> on a scale from 0 to 1. </span><span class="kobospan" id="kobo.258.2">This will make sure that the resulting communities are very similar to each other. </span><span class="kobospan" id="kobo.258.3">The minimum community size is 10; that means that a minimum of 10 news articles are required to form a cluster. </span><span class="kobospan" id="kobo.258.4">If we want larger, more general clusters, we should use a larger number for the minimum community size. </span><span class="kobospan" id="kobo.258.5">A more granular clustering should use a smaller number. </span><span class="kobospan" id="kobo.258.6">Any cluster with fewer members will not appear in the output. </span><span class="kobospan" id="kobo.258.7">The result is a list of lists, where each inner list represents a cluster and lists the row IDs of cluster members in the </span><span><span class="kobospan" id="kobo.259.1">original dataframe:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.260.1">
clusters = util.community_detection(
    embeddings, threshold=0.7, min_community_size=10)
print(clusters)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.261.1">The result will vary and might look </span><span><span class="kobospan" id="kobo.262.1">like this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.263.1">[[117, 168, 192, 493, 516, 530, 638, 827, 883, 1082, 1154, 1208, 1257, 1359, 1553, 1594, 1650, 1898, 1938, 2059, 2152], [76, 178, 290, 337, 497, 518, 755, 923, 1057, 1105, 1151, 1172, 1242, 1560, 1810, 1813, 1882, 1942, 1981], [150, 281, 376, 503, 758, 900, 1156, 1405, 1633, 1636, 1645, 1940, 1946, 1971], [389, 399, 565, 791, 1014, 1018, 1259, 1288, 1440, 1588, 1824, 1917, 2024], [373, 901, 1004, 1037, 1041, 1323, 1499, 1534, 1580, 1621, 1751, 2178], [42, 959, 1063, 1244, 1292, 1304, 1597, 1915, 2081, 2104, 2128], [186, 193, 767, 787, 1171, 1284, 1625, 1651, 1797, 2148], [134, 388, 682, 1069, 1476, 1680, 2106, 2129, 2186, 2198]]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.264.1">Here, we </span><a id="_idIndexMarker347" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.265.1">will define a function that prints the most common words by cluster. </span><span class="kobospan" id="kobo.265.2">We will </span><a id="_idIndexMarker348" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.266.1">take the clusters created by the community detection algorithm and the original dataframe. </span><span class="kobospan" id="kobo.266.2">For each cluster, we will first select the sentences that represent it and then get the most frequent words by using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.267.1">get_most_frequent_words</span></strong><span class="kobospan" id="kobo.268.1"> function, which we defined in </span><a href="B18411_04.xhtml#_idTextAnchor106" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.269.1">Chapter 4</span></em></span></a><span class="kobospan" id="kobo.270.1">, in the </span><em class="italic"><span class="kobospan" id="kobo.271.1">Clustering sentences using K-Means: unsupervised text classification</span></em><span class="kobospan" id="kobo.272.1"> recipe. </span><span class="kobospan" id="kobo.272.2">This function is also located in the </span><strong class="source-inline1"><span class="kobospan" id="kobo.273.1">lang_utils</span></strong><span class="kobospan" id="kobo.274.1"> notebook that we ran in the </span><span><span class="kobospan" id="kobo.275.1">second step:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.276.1">
def print_words_by_cluster(clusters, input_df):
    for i, cluster in enumerate(clusters):
        print(f"\nCluster {i+1}, {len(cluster)} elements ")
        sentences = input_df.iloc[cluster]["text"]
        all_text = " ".join(sentences)
        freq_words = get_most_frequent_words(all_text)
        print(freq_words)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.277.1">Now, use the function on the model output (</span><strong class="source-inline1"><span class="kobospan" id="kobo.278.1">truncated</span></strong><span class="kobospan" id="kobo.279.1">). </span><span class="kobospan" id="kobo.279.2">We can see that there are</span><a id="_idIndexMarker349" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.280.1"> many more specific </span><a id="_idIndexMarker350" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.281.1">clusters than just the five topics in the original </span><span><span class="kobospan" id="kobo.282.1">BBC dataset:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.283.1">
Cluster 1, 21 elements
['mr', 'labour', 'brown', 'said', 'blair', 'election', 'minister', 'prime', 'chancellor', 'would', 'party', 'new', 'campaign', 'told', 'government', ...]
Cluster 2, 19 elements
['yukos', 'us', 'said', 'russian', 'oil', 'gazprom', 'court', 'rosneft', 'russia', 'yugansk', 'company', 'bankruptcy', 'auction', 'firm', 'unit', ...]
Cluster 3, 14 elements
['kenteris', 'greek', 'thanou', 'iaaf', 'said', 'athens', 'tests', 'drugs', 'olympics', 'charges', 'also', 'decision', 'test', 'athletics', 'missing', ...]
Cluster 4, 13 elements
['mr', 'tax', 'howard', 'labour', 'would', 'said', 'tory', 'election', 'government', 'taxes', 'blair', 'spending', 'tories', 'party', 'cuts',...]
Cluster 5, 12 elements
['best', 'film', 'aviator', 'director', 'actor', 'foxx', 'swank', 'actress', 'baby', 'million', 'dollar', 'said', 'win', 'eastwood', 'jamie',...]
Cluster 6, 11 elements
['said', 'prices', 'market', 'house', 'uk', 'figures', 'mortgage', 'housing', 'year', 'lending', 'november', 'price', 'december', 'rise', 'rose', ...]
Cluster 7, 10 elements
['lse', 'deutsche', 'boerse', 'bid', 'euronext', 'said', 'exchange', 'london', 'offer', 'stock', 'would', 'also', 'shareholders', 'german', 'market',...]
Cluster 8, 10 elements
['dollar', 'us', 'euro', 'said', 'currency', 'deficit', 'analysts', 'trading', 'yen', 'record', 'exports', 'economic', 'trade', 'markets', 'european',...]</span></pre></li>			</ol>
			<h1 id="_idParaDest-162" class="calibre7"><a id="_idTextAnchor166" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.284.1">K-Means topic modeling with BERT</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.285.1">In </span><a id="_idIndexMarker351" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.286.1">this recipe, we will use the K-Means algorithm to do unsupervised topic classification, using the BERT embeddings to encode the data. </span><span class="kobospan" id="kobo.286.2">This recipe shares many commonalities with the </span><em class="italic"><span class="kobospan" id="kobo.287.1">Clustering sentences using K-Means – unsupervised text classification</span></em><span class="kobospan" id="kobo.288.1"> recipe in </span><a href="B18411_04.xhtml#_idTextAnchor106" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.289.1">Chapter 4</span></em></span></a><span><span class="kobospan" id="kobo.290.1">.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.291.1">The </span><a id="_idIndexMarker352" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.292.1">K-Means algorithm is </span><a id="_idIndexMarker353" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.293.1">used to find similar clusters with any kind of data and is an easy way to see trends in the data. </span><span class="kobospan" id="kobo.293.2">It is frequently used while performing preliminary data analysis to quickly check the different types of data that appear in a dataset. </span><span class="kobospan" id="kobo.293.3">We can use it with text data and encode the data using a sentence </span><span><span class="kobospan" id="kobo.294.1">transformer model.</span></span></p>
			<h2 id="_idParaDest-163" class="calibre5"><a id="_idTextAnchor167" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.295.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.296.1">We will be using the </span><strong class="source-inline"><span class="kobospan" id="kobo.297.1">sklearn.cluster.KMeans</span></strong><span class="kobospan" id="kobo.298.1"> object to do the unsupervised clustering, as well as using HuggingFace </span><strong class="source-inline"><span class="kobospan" id="kobo.299.1">sentence transformers</span></strong><span class="kobospan" id="kobo.300.1">. </span><span class="kobospan" id="kobo.300.2">Both packages are part of the </span><span><span class="kobospan" id="kobo.301.1">poetry environment.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.302.1">The notebook is located </span><span><span class="kobospan" id="kobo.303.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.3-kmeans_with_bert.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.304.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.3-kmeans_with_bert.ipynb</span></span></a><span><span class="kobospan" id="kobo.305.1">.</span></span></p>
			<h2 id="_idParaDest-164" class="calibre5"><a id="_idTextAnchor168" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.306.1">How to do it...</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.307.1">In the recipe, we</span><a id="_idIndexMarker354" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.308.1"> will load the BBC dataset and encode it </span><a id="_idIndexMarker355" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.309.1">by using the sentence transformers package. </span><span class="kobospan" id="kobo.309.2">We will then use the K-Means clustering algorithm to create five clusters. </span><span class="kobospan" id="kobo.309.3">After that, we will test the model on the test set to see how well it would perform on </span><span><span class="kobospan" id="kobo.310.1">unseen data:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.311.1">Do the </span><span><span class="kobospan" id="kobo.312.1">necessary imports:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.313.1">
import re
import string
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from nltk.probability import FreqDist
from nltk.corpus import stopwords
from sentence_transformers import SentenceTransformer</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.314.1">Run the language utilities file. </span><span class="kobospan" id="kobo.314.2">This will allow us to reuse the </span><strong class="source-inline1"><span class="kobospan" id="kobo.315.1">print_most_common_words_by_cluster</span></strong><span class="kobospan" id="kobo.316.1"> function in </span><span><span class="kobospan" id="kobo.317.1">this recipe:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.318.1">
%run -i "../util/lang_utils.ipynb"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.319.1">Read and print </span><span><span class="kobospan" id="kobo.320.1">the data:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.321.1">
bbc_df = pd.read_csv("../data/bbc-text.csv")
print(bbc_df)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.322.1">The result should look like the one in </span><span><em class="italic"><span class="kobospan" id="kobo.323.1">Figure 6</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.324.1">.1</span></em></span><span><span class="kobospan" id="kobo.325.1">.</span></span></p></li>				<li class="calibre14"><span class="kobospan" id="kobo.326.1">In this step, we will split the data into </span><strong class="source-inline1"><span class="kobospan" id="kobo.327.1">training</span></strong><span class="kobospan" id="kobo.328.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.329.1">testing</span></strong><span class="kobospan" id="kobo.330.1">. </span><span class="kobospan" id="kobo.330.2">We limit the testing size to 10% of the whole dataset. </span><span class="kobospan" id="kobo.330.3">The length of the training set is 2002 and the length of the test set </span><span><span class="kobospan" id="kobo.331.1">is 223:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.332.1">
bbc_train, bbc_test = train_test_split(bbc_df, test_size=0.1)
print(len(bbc_train))
print(len(bbc_test))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.333.1">The result will be </span><span><span class="kobospan" id="kobo.334.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.335.1">2002
223</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.336.1">Here, we </span><a id="_idIndexMarker356" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.337.1">will assign the list of texts to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.338.1">documents</span></strong><span class="kobospan" id="kobo.339.1"> variable. </span><span class="kobospan" id="kobo.339.2">We </span><a id="_idIndexMarker357" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.340.1">will then read in the </span><strong class="source-inline1"><span class="kobospan" id="kobo.341.1">all-MiniLM-L6-v2</span></strong><span class="kobospan" id="kobo.342.1"> model to use for the sentence embeddings and encode the text data. </span><span class="kobospan" id="kobo.342.2">Next, we will initialize a KMeans model with five clusters, setting the </span><strong class="source-inline1"><span class="kobospan" id="kobo.343.1">n_init</span></strong><span class="kobospan" id="kobo.344.1"> parameter to </span><strong class="source-inline1"><span class="kobospan" id="kobo.345.1">auto</span></strong><span class="kobospan" id="kobo.346.1">, which determines the number of times the algorithm is run. </span><span class="kobospan" id="kobo.346.2">We will also set the </span><strong class="source-inline1"><span class="kobospan" id="kobo.347.1">init</span></strong><span class="kobospan" id="kobo.348.1"> parameter to </span><strong class="source-inline1"><span class="kobospan" id="kobo.349.1">k-means++</span></strong><span class="kobospan" id="kobo.350.1">. </span><span class="kobospan" id="kobo.350.2">This parameter ensures faster convergence of the algorithm. </span><span class="kobospan" id="kobo.350.3">We will then train the </span><span><span class="kobospan" id="kobo.351.1">initialized model:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.352.1">
documents = bbc_train['text'].values
model = SentenceTransformer('all-MiniLM-L6-v2')
encoded_data = model.encode(documents)
km = KMeans(n_clusters=5, n_init='auto', init='k-means++')
km.fit(encoded_data)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.353.1">Print out the most common words </span><span><span class="kobospan" id="kobo.354.1">by topic:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.355.1">
print_most_common_words_by_cluster(documents, km, 5)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.356.1">The results will vary; our results look </span><span><span class="kobospan" id="kobo.357.1">like this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.358.1">0
['said', 'people', 'new', 'also', 'mr', 'technology', 'would', 'one', 'mobile', ...]
1
['said', 'game', 'england', 'first', 'win', 'world', 'last', 'would', 'one', 'two', 'time',...]
2
['said', 'film', 'best', 'music', 'also', 'year', 'us', 'one', 'new', 'awards', 'show',...]
3
['said', 'mr', 'would', 'labour', 'government', 'people', 'blair', 'party', 'election', 'also', 'minister', ...]
4
['said', 'us', 'year', 'mr', 'would', 'also', 'market', 'company', 'new', 'growth', 'firm', 'economy', ...]</span></pre><p class="calibre3"><span class="kobospan" id="kobo.359.1">We can see that the</span><a id="_idIndexMarker358" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.360.1"> mapping of topics is as follows: 0 is tech, 1 is sport, 2 is entertainment, 3 is politics, and 4 </span><span><span class="kobospan" id="kobo.361.1">is business.</span></span></p></li>				<li class="calibre14"><span class="kobospan" id="kobo.362.1">We can </span><a id="_idIndexMarker359" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.363.1">now use the test data to see how well the model performs on unseen data. </span><span class="kobospan" id="kobo.363.2">First, we must create a prediction column in the test dataframe and populate it with the cluster number for each of the </span><span><span class="kobospan" id="kobo.364.1">test inputs:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.365.1">
bbc_test["prediction"] = bbc_test["text"].apply(
    lambda x: km.predict(model.encode([x]))[0])
print(bbc_test)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.366.1">The results will vary; this is </span><span><span class="kobospan" id="kobo.367.1">our output:</span></span></p></li>			</ol>
			<div class="calibre2">
				<div id="_idContainer022" class="img---figure">
					<span class="kobospan" id="kobo.368.1"><img src="image/B18411_06_5.jpg" alt="" role="presentation" class="calibre4"/></span>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.369.1">Figure 6.4 – The result of running K-Means on the test dataframe</span></p>
			<ol class="calibre13">
				<li value="8" class="calibre14"><span class="kobospan" id="kobo.370.1">Now, we will </span><a id="_idIndexMarker360" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.371.1">create a mapping between</span><a id="_idIndexMarker361" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.372.1"> the cluster number and the topic name, which we discovered manually by looking at the most frequent words for each cluster. </span><span class="kobospan" id="kobo.372.2">We will then create a column with the predicted topic name for every text in the test set by using the mapping and the </span><strong class="source-inline1"><span class="kobospan" id="kobo.373.1">prediction</span></strong><span class="kobospan" id="kobo.374.1"> column we created in the previous step. </span><span class="kobospan" id="kobo.374.2">Now, we can compare the predictions of the model with the true value of the data. </span><span class="kobospan" id="kobo.374.3">We will use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.375.1">classification_report</span></strong><span class="kobospan" id="kobo.376.1"> function from </span><strong class="source-inline1"><span class="kobospan" id="kobo.377.1">sklearn</span></strong><span class="kobospan" id="kobo.378.1"> to get the corresponding </span><a id="_idIndexMarker362" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.379.1">statistics. </span><span class="kobospan" id="kobo.379.2">Finally, we will print out the classification report for </span><span><span class="kobospan" id="kobo.380.1">the predictions:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.381.1">
topic_mapping = {0:"tech", 1:"sport",
    2:"entertainment", 3:"politics", 4:"business"}
bbc_test["pred_category"] = bbc_test["prediction"].apply(
    lambda x: topic_mapping[x])
print(classification_report(bbc_test["category"],
    bbc_test["pred_category"]))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.382.1">The result will be </span><span><span class="kobospan" id="kobo.383.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.384.1">               precision    recall  f1-score   support
     business       0.98      0.96      0.97        55
entertainment       0.95      1.00      0.97        38
     politics       0.97      0.93      0.95        42
        sport       0.98      0.96      0.97        47
         tech       0.93      0.98      0.95        41
     accuracy                           0.96       223
    macro avg       0.96      0.97      0.96       223
 weighted avg       0.96      0.96      0.96       223</span></pre><p class="calibre3"><span class="kobospan" id="kobo.385.1">The </span><a id="_idIndexMarker363" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.386.1">scores are very high – almost perfect. </span><span class="kobospan" id="kobo.386.2">Most of this speaks to the quality of the sentence embedding model that </span><span><span class="kobospan" id="kobo.387.1">we used.</span></span></p></li>				<li class="calibre14"><span class="kobospan" id="kobo.388.1">Define a </span><span><span class="kobospan" id="kobo.389.1">new example:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.390.1">
new_example = """Manchester United players slumped to the turf
at full-time in Germany on Tuesday in acknowledgement of what their
latest pedestrian first-half display had cost them. </span><span class="kobospan1" id="kobo.390.2">The 3-2 loss at
RB Leipzig means United will not be one of the 16 teams in the draw
for the knockout stages of the Champions League. </span><span class="kobospan1" id="kobo.390.3">And this is not the
only price for failure. </span><span class="kobospan1" id="kobo.390.4">The damage will be felt in the accounts, in
the dealings they have with current and potentially future players
and in the faith the fans have placed in manager Ole Gunnar Solskjaer.
</span><span class="kobospan1" id="kobo.390.5">With Paul Pogba's agent angling for a move for his client and ex-United
defender Phil Neville speaking of a "witchhunt" against his former team-mate
Solskjaer, BBC Sport looks at the ramifications and reaction to a big loss for United."""</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.391.1">Print </span><a id="_idIndexMarker364" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.392.1">the </span><a id="_idIndexMarker365" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.393.1">prediction for the </span><span><span class="kobospan" id="kobo.394.1">new example:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.395.1">
predictions = km.predict(model.encode([new_example]))
print(predictions[0])</span></pre><p class="calibre3"><span class="kobospan" id="kobo.396.1">The output will be </span><span><span class="kobospan" id="kobo.397.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.398.1">1</span></pre></li>			</ol>
			<p class="calibre3"><span class="kobospan" id="kobo.399.1">  Cluster number 1 corresponds to sport, which is the </span><span><span class="kobospan" id="kobo.400.1">correct classification.</span></span></p>
			<h1 id="_idParaDest-165" class="calibre7"><a id="_idTextAnchor169" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.401.1">Topic modeling using BERTopic</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.402.1">In this recipe, we</span><a id="_idIndexMarker366" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.403.1"> will explore the BERTopic package that provides many different and versatile tools for topic modeling and visualization. </span><span class="kobospan" id="kobo.403.2">It is especially useful if you would like to do different visualizations of the topic clusters created. </span><span class="kobospan" id="kobo.403.3">This topic modeling algorithm uses BERT embeddings to encode the data, hence the “BERT” in the name. </span><span class="kobospan" id="kobo.403.4">You can learn more about the algorithm and its constituent parts </span><span><span class="kobospan" id="kobo.404.1">at </span></span><a href="https://maartengr.github.io/BERTopic/algorithm/algorithm.html" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.405.1">https://maartengr.github.io/BERTopic/algorithm/algorithm.html</span></span></a><span><span class="kobospan" id="kobo.406.1">.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.407.1">The</span><a id="_idIndexMarker367" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.408.1"> BERTopic package, by default, uses the HDBSCAN algorithm to create clusters from the data in an unsupervised fashion. </span><span class="kobospan" id="kobo.408.2">You can learn more about how the HDBSCAN algorithm works at </span><a href="https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.409.1">https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html</span></a><span class="kobospan" id="kobo.410.1">. </span><span class="kobospan" id="kobo.410.2">However, it is also possible to customize the inner workings of a BERTopic object to use other algorithms. </span><span class="kobospan" id="kobo.410.3">It is also possible to substitute other custom components into its pipeline. </span><span class="kobospan" id="kobo.410.4">In this recipe, we will use the default settings, and you can experiment with </span><span><span class="kobospan" id="kobo.411.1">other components.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.412.1">The resulting topics are of very high quality. </span><span class="kobospan" id="kobo.412.2">There might be several reasons for this. </span><span class="kobospan" id="kobo.412.3">One of them is the result of using BERT embeddings, which we saw in </span><a href="B18411_04.xhtml#_idTextAnchor106" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.413.1">Chapter 4</span></em></span></a><span class="kobospan" id="kobo.414.1">, to positively impact the </span><span><span class="kobospan" id="kobo.415.1">classification results</span></span></p>
			<h2 id="_idParaDest-166" class="calibre5"><a id="_idTextAnchor170" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.416.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.417.1">We will use the BERTopic package to create topic models for the BBC dataset. </span><span class="kobospan" id="kobo.417.2">The package is included in the poetry environment and is also part of the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.418.1">requirements.txt</span></strong></span><span><span class="kobospan" id="kobo.419.1"> file.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.420.1">The notebook is located </span><span><span class="kobospan" id="kobo.421.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.3-kmeans_with_bert.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.422.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.3-kmeans_with_bert.ipynb</span></span></a><span><span class="kobospan" id="kobo.423.1">.</span></span></p>
			<h2 id="_idParaDest-167" class="calibre5"><a id="_idTextAnchor171" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.424.1">How to do it...</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.425.1">In this recipe, we will load the BBC dataset and preprocess it again. </span><span class="kobospan" id="kobo.425.2">The preprocessing step will involve tokenizing the data and removing stopwords. </span><span class="kobospan" id="kobo.425.3">Then we will create the topic model using BERTopic and inspect the results. </span><span class="kobospan" id="kobo.425.4">We will also test the topic model on unseen data and use </span><strong class="source-inline"><span class="kobospan" id="kobo.426.1">classification_report</span></strong><span class="kobospan" id="kobo.427.1"> to see the </span><span><span class="kobospan" id="kobo.428.1">accuracy statistics:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.429.1">Do the </span><span><span class="kobospan" id="kobo.430.1">necessary imports:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.431.1">
import pandas as pd
import numpy as np
from bertopic import BERTopic
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.432.1">Run</span><a id="_idIndexMarker368" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.433.1"> the language </span><span><span class="kobospan" id="kobo.434.1">utilities file:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.435.1">
%run -i "../util/lang_utils.ipynb"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.436.1">Define and </span><a id="_idIndexMarker369" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.437.1">amend the stopwords, then read in the </span><span><span class="kobospan" id="kobo.438.1">BBC data:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.439.1">
stop_words = stopwords.words('english')
stop_words.append("said")
stop_words.append("mr")
bbc_df = pd.read_csv("../data/bbc-text.csv")</span></pre><p class="calibre3"><span class="kobospan" id="kobo.440.1">In this step, we will preprocess the data. </span><span class="kobospan" id="kobo.440.2">We will first tokenize it using the </span><strong class="source-inline"><span class="kobospan" id="kobo.441.1">word_tokenize</span></strong><span class="kobospan" id="kobo.442.1"> method from NLTK as shown in the </span><em class="italic"><span class="kobospan" id="kobo.443.1">Dividing sentences into words – tokenization</span></em><span class="kobospan" id="kobo.444.1"> recipe in </span><a href="B18411_01.xhtml#_idTextAnchor013" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.445.1">Chapter 1</span></em></span></a><span class="kobospan" id="kobo.446.1">. </span><span class="kobospan" id="kobo.446.2">Then remove stopwords and finally put the text back together into a string. </span><span class="kobospan" id="kobo.446.3">We must do the last step because the BERTopic uses a sentence embedding model, and that model requires a string, not a list </span><span><span class="kobospan" id="kobo.447.1">of words:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.448.1">bbc_df["text"] = bbc_df["text"].apply(
    lambda x: word_tokenize(x))
bbc_df["text"] = bbc_df["text"].apply(
    lambda x: [w for w in x if w not in stop_words])
bbc_df["text"] = bbc_df["text"].apply(lambda x: " ".join(x))</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.449.1">Here, we will split the dataset into training and test sets, specifying the size of the test </span><a id="_idIndexMarker370" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.450.1">set to be 10%. </span><span class="kobospan" id="kobo.450.2">As a result, we will get 2002 datapoints for training and 223 </span><span><span class="kobospan" id="kobo.451.1">for testing:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.452.1">
bbc_train, bbc_test = train_test_split(bbc_df, test_size=0.1)
print(len(bbc_train))
print(len(bbc_test))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.453.1">The result will be </span><span><span class="kobospan" id="kobo.454.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.455.1">2002
223</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.456.1">Extract the lists of texts from </span><span><span class="kobospan" id="kobo.457.1">the dataframe:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.458.1">
docs = bbc_train["text"].values</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.459.1">In this </span><a id="_idIndexMarker371" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.460.1">step, we willinitialize the </span><strong class="source-inline1"><span class="kobospan" id="kobo.461.1">BERTopic</span></strong><span class="kobospan" id="kobo.462.1"> object and then fit it on the documents extracted in </span><em class="italic"><span class="kobospan" id="kobo.463.1">step 6</span></em><span class="kobospan" id="kobo.464.1">. </span><span class="kobospan" id="kobo.464.2">We will specify the number of topics to be six, one more than the five that we are looking for. </span><span class="kobospan" id="kobo.464.3">This is because a key difference between BERTopic and other topic modeling algorithms is that it has a special </span><strong class="bold"><span class="kobospan" id="kobo.465.1">discard</span></strong><span class="kobospan" id="kobo.466.1"> topic numbered -1. </span><span class="kobospan" id="kobo.466.2">We could also specify a larger number of topics. </span><span class="kobospan" id="kobo.466.3">In that case, they would be narrower than the general five categories of business, politics, entertainment, tech, </span><span><span class="kobospan" id="kobo.467.1">and sport:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.468.1">
topic_model = BERTopic(nr_topics=6)
topics, probs = topic_model.fit_transform(docs)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.469.1">Here, we will print out the information about the resulting topic model. </span><span class="kobospan" id="kobo.469.2">Other than the </span><strong class="source-inline1"><span class="kobospan" id="kobo.470.1">discard</span></strong><span class="kobospan" id="kobo.471.1"> topic, the topics align well with the gold labels assigned by human annotators. </span><span class="kobospan" id="kobo.471.2">The function prints out the most representative words for each topic, as well as the most </span><span><span class="kobospan" id="kobo.472.1">representative documents:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.473.1">
print(topic_model.get_topic_info())</span></pre><p class="calibre3"><span class="kobospan" id="kobo.474.1">The</span><a id="_idIndexMarker372" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.475.1"> results</span><a id="_idIndexMarker373" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.476.1"> will vary; here is an </span><span><span class="kobospan" id="kobo.477.1">example result:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.478.1">   Topic  Count                                 Name  \
0     -1    222             -1_also_company_china_us
1      0    463             0_england_game_win_first
2      1    393      1_would_labour_government_blair
3      2    321             2_film_best_music_awards
4      3    309  3_people_mobile_technology_software
5      4    294             4_us_year_growth_economy
                                      Representation  \
0  [also, company, china, us, would, year, new, p...
</span><span class="kobospan1" id="kobo.478.2">1  [england, game, win, first, club, world, playe...
</span><span class="kobospan1" id="kobo.478.3">2  [would, labour, government, blair, election, p...
</span><span class="kobospan1" id="kobo.478.4">3  [film, best, music, awards, show, year, band, ...
</span><span class="kobospan1" id="kobo.478.5">4  [people, mobile, technology, software, digital...
</span><span class="kobospan1" id="kobo.478.6">5  [us, year, growth, economy, economic, company,...
</span><span class="kobospan1" id="kobo.478.7">                                 Representative_Docs
0  [us retail sales surge december us retail sale...
</span><span class="kobospan1" id="kobo.478.8">1  [ireland win eclipses refereeing errors intern...
</span><span class="kobospan1" id="kobo.478.9">2  [lib dems unveil election slogan liberal democ...
</span><span class="kobospan1" id="kobo.478.10">3  [scissor sisters triumph brits us band scissor...
</span><span class="kobospan1" id="kobo.478.11">4  [mobiles media players yet mobiles yet ready a...
</span><span class="kobospan1" id="kobo.478.12">5  [consumer spending lifts us growth us economic...</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.479.1">In this step, we </span><a id="_idIndexMarker374" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.480.1">will print the topics. </span><span class="kobospan" id="kobo.480.2">We can see</span><a id="_idIndexMarker375" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.481.1"> from the words that the zeroth topic is sport, the first topic is politics, the second topic is entertainment, the third topic is tech, and the fourth topic </span><span><span class="kobospan" id="kobo.482.1">is business.</span></span></li>
			</ol>
			<div class="calibre2">
				<div id="_idContainer023" class="img---figure">
					<span class="kobospan" id="kobo.483.1"><img src="image/B18411_06_6.jpg" alt="" role="presentation" class="calibre4"/></span>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.484.1">Figure 6.5 – The topics generated by BERTopic</span></p>
			<ol class="calibre13">
				<li value="9" class="calibre14"><span class="kobospan" id="kobo.485.1">In this </span><a id="_idIndexMarker376" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.486.1">step, we will generate the topic labels </span><a id="_idIndexMarker377" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.487.1">using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.488.1">generate_topic_labels</span></strong><span class="kobospan" id="kobo.489.1"> function. </span><span class="kobospan" id="kobo.489.2">We will input the number of words to use for the topic label, the separator (in this case, this is an underscore), and whether to include the topic number. </span><span class="kobospan" id="kobo.489.3">As a result, we will get a list of topic names. </span><span class="kobospan" id="kobo.489.4">We can see from the resulting topics that we could include </span><em class="italic"><span class="kobospan" id="kobo.490.1">would</span></em><span class="kobospan" id="kobo.491.1"> as </span><span><span class="kobospan" id="kobo.492.1">a stopword:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.493.1">
topic_model.generate_topic_labels(
    nr_words=5, topic_prefix=True, separator='_')</span></pre><p class="calibre3"><span class="kobospan" id="kobo.494.1">The result will be similar to </span><span><span class="kobospan" id="kobo.495.1">the following:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.496.1">['-1_also_company_china_us_would',
 '0_england_game_win_first_club',
 '1_would_labour_government_blair_election',
 '2_film_best_music_awards_show',
 '3_people_mobile_technology_software_digital',
 '4_us_year_growth_economy_economic']</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.497.1">Here, we</span><a id="_idIndexMarker378" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.498.1"> will define the </span><strong class="source-inline1"><span class="kobospan" id="kobo.499.1">get_prediction</span></strong><span class="kobospan" id="kobo.500.1"> function that gives us the topic number for a text input and a corresponding model. </span><span class="kobospan" id="kobo.500.2">The</span><a id="_idIndexMarker379" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.501.1"> function transforms the input text and outputs a tuple of two lists. </span><span class="kobospan" id="kobo.501.2">One is a list of topic numbers and the other is the list of probabilities of assigning each topic. </span><span class="kobospan" id="kobo.501.3">The lists are sorted in the order of the most probable topic, so we can take the first element of the first list as the predicted topic and </span><span><span class="kobospan" id="kobo.502.1">return it:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.503.1">
def get_prediction(input_text, model):
    pred = model.transform(input_text)
    pred = pred[0][0]
    return pred</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.504.1">In this step, we will define a column for predictions in the test dataframe and then use the function we defined in the previous step to get predictions for each text in the dataframe. </span><span class="kobospan" id="kobo.504.2">We will then create a mapping of topic numbers to gold topic labels that we can use to test the effectiveness of the </span><span><span class="kobospan" id="kobo.505.1">topic model:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.506.1">
bbc_test["prediction"] = bbc_test["text"].apply(
    lambda x: get_prediction(x, topic_model))
topic_mapping = {0:"sport", 1:"politics",
    2:"entertainment", 3:"tech", 4:"business", -1:"discard"}</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.507.1">Here, we will create a new column in the test dataframe to record the predicted topic name using the mapping we created. </span><span class="kobospan" id="kobo.507.2">We will then filter the test set to only use entries that have not been predicted to be the discard </span><span><span class="kobospan" id="kobo.508.1">topic -1:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.509.1">
bbc_test["pred_category"] = bbc_test["prediction"].apply(
    lambda x: topic_mapping[x])
test_data = bbc_test.loc[bbc_test['prediction'] != -1]
print(classification_report(test_data["category"],
    test_data["pred_category"]))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.510.1">The</span><a id="_idIndexMarker380" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.511.1"> result </span><a id="_idIndexMarker381" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.512.1">will be similar </span><span><span class="kobospan" id="kobo.513.1">to this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.514.1">               precision    recall  f1-score   support
     business       0.95      0.86      0.90        21
entertainment       0.97      1.00      0.98        30
     politics       0.94      1.00      0.97        46
        sport       1.00      1.00      1.00        62
         tech       0.96      0.88      0.92        25
     accuracy                           0.97       184
    macro avg       0.96      0.95      0.95       184
 weighted avg       0.97      0.97      0.97       184</span></pre><p class="calibre3"><span class="kobospan" id="kobo.515.1">The test scores are very high. </span><span class="kobospan" id="kobo.515.2">This is reflective of the encoding model, the BERTopic model, which is also a sentence transformer model, as in the </span><span><span class="kobospan" id="kobo.516.1">previous recipe.</span></span></p></li>				<li class="calibre14"><span class="kobospan" id="kobo.517.1">In this step, we will define a new example to test with the model and print it. </span><span class="kobospan" id="kobo.517.2">We will use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.518.1">iloc</span></strong><span class="kobospan" id="kobo.519.1"> function from the </span><strong class="source-inline1"><span class="kobospan" id="kobo.520.1">pandas</span></strong><span class="kobospan" id="kobo.521.1"> package to access the first element of the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.522.1">bbc_test</span></strong></span><span><span class="kobospan" id="kobo.523.1"> dataframe:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.524.1">
new_input = bbc_test["text"].iloc[0]
print(new_input)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.525.1">The result will be as </span><span><span class="kobospan" id="kobo.526.1">follows (truncated):</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.527.1">howard dismisses tory tax fears michael howard dismissed fears conservatives plans £4bn tax cuts modest . </span><span class="kobospan1" id="kobo.527.2">defended package saying plan tories first budget hoped able go . </span><span class="kobospan1" id="kobo.527.3">tories monday highlighted £35bn wasteful spending would stop allow tax cuts reduced borrowing spending key services . </span><span class="kobospan1" id="kobo.527.4">...</span></pre><p class="calibre3"><span class="kobospan" id="kobo.528.1">The</span><a id="_idIndexMarker382" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.529.1"> example</span><a id="_idIndexMarker383" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.530.1"> is about politics, which should be </span><span><span class="kobospan" id="kobo.531.1">topic 1.</span></span></p></li>				<li class="calibre14"><span class="kobospan" id="kobo.532.1">Obtain a prediction from the model and </span><span><span class="kobospan" id="kobo.533.1">print it:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.534.1">
print(topic_model.transform(new_input))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.535.1">The result will be a </span><span><span class="kobospan" id="kobo.536.1">correct prediction:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.537.1">([1], array([1.]))</span></pre></li>			</ol>
			<h2 id="_idParaDest-168" class="calibre5"><a id="_idTextAnchor172" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.538.1">There’s more...</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.539.1">Now, we can find topics that are similar to a particular word, phrase, or sentence. </span><span class="kobospan" id="kobo.539.2">This way, we could easily find a topic to which a text corresponds within the dataset. </span><span class="kobospan" id="kobo.539.3">We will use a word, a phrase, and a sentence to see how well the model can show the </span><span><span class="kobospan" id="kobo.540.1">corresponding topics:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.541.1">Find topics most similar to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.542.1">sports</span></strong><span class="kobospan" id="kobo.543.1"> word with the corresponding similarity scores. </span><span class="kobospan" id="kobo.543.2">Combine the topic numbers and similarity scores in a list of tuples and print them. </span><span class="kobospan" id="kobo.543.3">The tuples are a combination of the topic number and the similarity score between the text that is passed in and the </span><span><span class="kobospan" id="kobo.544.1">particular topic:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.545.1">
topics, similarity = topic_model.find_topics("sports", top_n=5)
sim_topics = list(zip(topics, similarity))
print(sim_topics)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.546.1">The most similar topic is topic 0, which </span><span><span class="kobospan" id="kobo.547.1">is sport:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.548.1">[(0, 0.29033981040460977), (3, 0.049293092462828376), (-1, -0.0047265937178774895), (2, -0.02074380026102955), (4, -0.03699168959416969)]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.549.1">Repeat </span><a id="_idIndexMarker384" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.550.1">the</span><a id="_idIndexMarker385" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.551.1"> preceding step for the </span><strong class="source-inline1"><span class="kobospan" id="kobo.552.1">business and economics</span></strong> <span><span class="kobospan" id="kobo.553.1">example phrase:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.554.1">
topics, similarity = topic_model.find_topics(
    "business and economics",
    top_n=5)
sim_topics = list(zip(topics, similarity))
print(sim_topics)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.555.1">Here, the most similar topic is topic 4, which </span><span><span class="kobospan" id="kobo.556.1">is business:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.557.1">[(4, 0.29003573983158404), (-1, 0.26259758927249205), (3, 0.15627005753581313), (1, 0.05491237184012845), (0, 0.010567363445904386)]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.558.1">Now repeat the same process for the following example sentence: </span><strong class="source-inline1"><span class="kobospan" id="kobo.559.1">"YouTube removed a snippet of code that publicly disclosed whether a channel receives ad payouts, obscuring which creators benefit most from the platform."</span></strong><span class="kobospan" id="kobo.560.1">. </span><span class="kobospan" id="kobo.560.2">We would expect this to be most similar to the </span><span><span class="kobospan" id="kobo.561.1">tech topic:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.562.1">
input_text = """YouTube removed a snippet of code that publicly disclosed whether a channel receives ad payouts,
obscuring which creators benefit most from the platform."""
</span><span class="kobospan1" id="kobo.562.2">topics, similarity = topic_model.find_topics(
    input_text, top_n=5)
sim_topics = list(zip(topics, similarity))
print(sim_topics)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.563.1">In the output, we can see that the most similar topic is topic 3, which </span><span><span class="kobospan" id="kobo.564.1">is tech:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.565.1">[(3, 0.2540850599909866), (-1, 0.172097560474608), (2, 0.1367798346494483), (4, 0.10243553209139492), (1, 0.06954579004136925)]</span></pre></li>			</ol>
			<h1 id="_idParaDest-169" class="calibre7"><a id="_idTextAnchor173" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.566.1">Using contextualized topic models</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.567.1">In this </span><a id="_idIndexMarker386" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.568.1">recipe, we will look at another topic model algorithm: contextualized topic models. </span><span class="kobospan" id="kobo.568.2">To produce a more effective topic model, it combines embeddings with a bag-of-words </span><span><span class="kobospan" id="kobo.569.1">document representation.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.570.1">We will show you how to use the trained topic model with input in other languages. </span><span class="kobospan" id="kobo.570.2">This feature is especially useful because we can create a topic model in one language, for example, one that has many resources available, and then apply it on another language that does not have as many resources. </span><span class="kobospan" id="kobo.570.3">To achieve this, we will utilize a multilingual embedding model in order to encode </span><span><span class="kobospan" id="kobo.571.1">the data.</span></span></p>
			<h2 id="_idParaDest-170" class="calibre5"><a id="_idTextAnchor174" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.572.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.573.1">We will need the </span><strong class="source-inline"><span class="kobospan" id="kobo.574.1">contextualized-topic-models</span></strong><span class="kobospan" id="kobo.575.1"> package for this recipe. </span><span class="kobospan" id="kobo.575.2">It is part of the poetry environment and the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.576.1">requirements.txt</span></strong></span><span><span class="kobospan" id="kobo.577.1"> file.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.578.1">The notebook is located </span><span><span class="kobospan" id="kobo.579.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.5-contextualized-tm.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.580.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.5-contextualized-tm.ipynb</span></span></a><span><span class="kobospan" id="kobo.581.1">.</span></span></p>
			<h2 id="_idParaDest-171" class="calibre5"><a id="_idTextAnchor175" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.582.1">How to do it...</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.583.1">In this recipe, we will load the data, then </span><a id="_idIndexMarker387" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.584.1">divide it into sentences, preprocess it, and use the </span><strong class="bold"><span class="kobospan" id="kobo.585.1">gsdmm</span></strong><span class="kobospan" id="kobo.586.1"> model </span><a id="_idIndexMarker388" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.587.1">to cluster the sentences into topics. </span><span class="kobospan" id="kobo.587.2">If you would like more information about the algorithm, please </span><a id="_idIndexMarker389" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.588.1">see the package documentation </span><span><span class="kobospan" id="kobo.589.1">at </span></span><a href="https://pypi.org/project/contextualized-topic-models/" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.590.1">https://pypi.org/project/contextualized-topic-models/</span></span></a><span><span class="kobospan" id="kobo.591.1">.</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.592.1">Do the </span><span><span class="kobospan" id="kobo.593.1">necessary imports:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.594.1">
import pandas as pd
from nltk.corpus import stopwords
from contextualized_topic_models.utils.preprocessing import( 
    WhiteSpacePreprocessingStopwords)
from contextualized_topic_models.models.ctm import ZeroShotTM
from contextualized_topic_models.utils.data_preparation import( 
    TopicModelDataPreparation)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.595.1">Suppress </span><span><span class="kobospan" id="kobo.596.1">the warnings:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.597.1">
import warnings
warnings.filterwarnings('ignore')
warnings.filterwarnings("ignore", category = DeprecationWarning)
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.598.1">Create the stopwords list and read in </span><span><span class="kobospan" id="kobo.599.1">the data:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.600.1">
stop_words = stopwords.words('english')
stop_words.append("said")
bbc_df = pd.read_csv("../data/bbc-text.csv")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.601.1">In this step, we will create the preprocessor object and use it to preprocess the documents. </span><span class="kobospan" id="kobo.601.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.602.1">contextualized-topic-models</span></strong><span class="kobospan" id="kobo.603.1"> package provides different preprocessors that prepare the data to be used in the topic model algorithm. </span><span class="kobospan" id="kobo.603.2">This preprocessor tokenizes the documents, removes the stopwords, and puts them back into a string. </span><span class="kobospan" id="kobo.603.3">It returns the list of preprocessed documents, the list</span><a id="_idIndexMarker390" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.604.1"> of original documents, the dataset vocabulary, and a list of document indices in the </span><span><span class="kobospan" id="kobo.605.1">original dataframe:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.606.1">
documents = bbc_df["text"]
preprocessor = WhiteSpacePreprocessingStopwords(
    documents, stopwords_list=stop_words)
preprocessed_documents,unpreprocessed_documents,vocab,indices =\
    preprocessor.preprocess()</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.607.1">Here, we will create the </span><strong class="source-inline1"><span class="kobospan" id="kobo.608.1">TopicModelDataPreparation</span></strong><span class="kobospan" id="kobo.609.1"> object. </span><span class="kobospan" id="kobo.609.2">We will pass the embedding model name as the parameter. </span><span class="kobospan" id="kobo.609.3">This is a multilingual model that can encode text in various languages with good results. </span><span class="kobospan" id="kobo.609.4">We will then fit it on the documents. </span><span class="kobospan" id="kobo.609.5">It uses an embedding model to turn the texts into embeddings and also creates a bag-of-words model. </span><span class="kobospan" id="kobo.609.6">The output is a </span><strong class="source-inline1"><span class="kobospan" id="kobo.610.1">CTMDataset</span></strong><span class="kobospan" id="kobo.611.1"> object that represents the training dataset in the format required by the topic model </span><span><span class="kobospan" id="kobo.612.1">training algorithm:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.613.1">
tp = TopicModelDataPreparation(
    "distiluse-base-multilingual-cased")
training_dataset = tp.fit(
    text_for_contextual=unpreprocessed_documents,
    text_for_bow=preprocessed_documents)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.614.1">In this step, we will create the topic model using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.615.1">ZeroShotTM</span></strong><span class="kobospan" id="kobo.616.1"> object. </span><span class="kobospan" id="kobo.616.2">The term </span><strong class="bold"><span class="kobospan" id="kobo.617.1">zero shot</span></strong><span class="kobospan" id="kobo.618.1"> means </span><a id="_idIndexMarker391" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.619.1">that the model has no prior information about the documents. </span><span class="kobospan" id="kobo.619.2">We will input the size of the vocabulary for the bag-of-words model, the size of the embeddings vector, the number of topics (the </span><strong class="source-inline1"><span class="kobospan" id="kobo.620.1">n_components</span></strong><span class="kobospan" id="kobo.621.1"> parameter), and the number of epochs to train the model for. </span><span class="kobospan" id="kobo.621.2">We will use five topics, since the BBC dataset has that many topics. </span><span class="kobospan" id="kobo.621.3">When you apply this algorithm to your data, you will need to experiment with different numbers</span><a id="_idIndexMarker392" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.622.1"> of topics. </span><span class="kobospan" id="kobo.622.2">Finally, we will fit the initialized topic model on the </span><span><span class="kobospan" id="kobo.623.1">training dataset:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.624.1">
ctm = ZeroShotTM(bow_size=len(tp.vocab),
    contextual_size=512, n_components=5,
    num_epochs=100)
ctm.fit(training_dataset)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.625.1">Here, we will inspect the topics. </span><span class="kobospan" id="kobo.625.2">We can see that they fit well with the golden labels. </span><span class="kobospan" id="kobo.625.3">Topic 0 is tech, topic 1 is sport, topic 2 is business, topic 3 is entertainment, and topic 4 </span><span><span class="kobospan" id="kobo.626.1">is politics:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.627.1">
ctm.get_topics()</span></pre><p class="calibre3"><span class="kobospan" id="kobo.628.1">The results will vary; this is the output </span><span><span class="kobospan" id="kobo.629.1">we get:</span></span></p></li>			</ol>
			<div class="calibre2">
				<div id="_idContainer024" class="img---figure">
					<span class="kobospan" id="kobo.630.1"><img src="image/B18411_06_7.jpg" alt="" role="presentation" class="calibre4"/></span>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.631.1">Figure 6.6 – The contextualized model output</span></p>
			<ol class="calibre13">
				<li value="8" class="calibre14"><span class="kobospan" id="kobo.632.1">Now, we will initialize a new news piece, this time in Spanish, to see how effective the topic model trained on English-language documents will be on a news article in </span><a id="_idIndexMarker393" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.633.1">a different language. </span><span class="kobospan" id="kobo.633.2">This particular news piece should fall into the tech topic. </span><span class="kobospan" id="kobo.633.3">We will preprocess it using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.634.1">TopicModelDataPreparation</span></strong><span class="kobospan" id="kobo.635.1"> object. </span><span class="kobospan" id="kobo.635.2">To then use the model on the encoded text, we need to create a dataset object. </span><span class="kobospan" id="kobo.635.3">That is why we have to include the Spanish news piece in a list and then pass it on for data preparation. </span><span class="kobospan" id="kobo.635.4">Finally, we must pass the dataset (that consists of only one element) through </span><span><span class="kobospan" id="kobo.636.1">the model:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.637.1">
spanish_news_piece = """IBM anuncia el comienzo de la "era de la utilidad cuántica" y anticipa un superordenador en 2033.
</span><span class="kobospan1" id="kobo.637.2">La compañía asegura haber alcanzado un sistema de computación que no se puede simular con procedimientos clásicos."""
</span><span class="kobospan1" id="kobo.637.3">testing_dataset = tp.transform([spanish_news_piece])</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.638.1">In this step, we will get the topic distribution for the testing dataset we created in the previous step. </span><span class="kobospan" id="kobo.638.2">The result is a list of lists, where each individual list represents the probability that a particular text belongs to that topic. </span><span class="kobospan" id="kobo.638.3">The probabilities have the same indices in individual lists as the </span><span><span class="kobospan" id="kobo.639.1">topic numbers:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.640.1">
ctm.get_doc_topic_distribution(testing_dataset)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.641.1">In this case, the highest probability is for topic 0, which is </span><span><span class="kobospan" id="kobo.642.1">indeed tech:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.643.1">array([[0.5902461,0.09361929,0.14041995,0.07586181,0.0998529 ]],
      dtype=float32)</span></pre></li>			</ol>
			<h2 id="_idParaDest-172" class="calibre5"><a id="_idTextAnchor176" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.644.1">See also</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.645.1">For more information about contextualized topic models, </span><span><span class="kobospan" id="kobo.646.1">see </span></span><a href="https://contextualized-topic-models.readthedocs.io/en/latest/index.html" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.647.1">https://contextualized-topic-models.readthedocs.io/en/latest/index.html</span></span></a><span><span class="kobospan" id="kobo.648.1">.</span></span></p>
		</div>
	</body></html>