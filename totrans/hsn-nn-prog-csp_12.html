<html><head></head><body>
        

                            
                    <h1 class="header-title">Replacing Back Propagation with PSO</h1>
                
            
            
                
<p class="mce-root">One of the latest examples of success with neural networks is the field of study known as <strong>Swarm Intelligence</strong>. Even though this field of study has been around for many years, advances in computer hardware combined with our understanding of studying animals has helped us to take this fascinating field out of the laboratory and into many different directions and real-world scenarios.</p>
<p>In this chapter, we are going to cover the following:</p>
<ul>
<li>Basic theory on Particle Swarm Optimization, or PSO for short</li>
<li>The open-source machine-learning framework Encog</li>
<li>Replacing the conventional back propagation with PSO</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>You will require Microsoft Visual Studio and also might want refer <a href="https://github.com/encog">https://github.com/encog</a>.</p>
<p>Check out the following video to see Code in Action: <a href="http://bit.ly/2QPd6Qo">http://bit.ly/2QPd6Qo</a>.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Basic theory</h1>
                
            
            
                
<p>OK, pop quiz time. What do a flock of birds, a school of fish, and a swarm of bees all have in common? Swarm intelligence—knowing how to cooperatively live and work near each other while optimally achieving the same objective. It's not about the intelligence of the individual, but rather the achievements of the group. No one individual has a clear path or directive, no one is at the top shouting orders, yet the optimal goal is always accomplished. Swarms of bees find new nests by doing waggle dances. Birds fly in great harmony, each taking turns being the leader. Fish swim collectively in beautiful architectures we call schools. But if we as humans always need someone at the top giving orders, and we still collectively don't always agree, how is it that millions of these little creatures have been doing it for years and we can't? Oops, going off on a tangent there, sorry!</p>
<p>Let's start off with a few quick definitions that will be used throughout to ensure we are all on the same page going forward.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Swarm intelligence</h1>
                
            
            
                
<p>Swarm intelligence is the collective behavior of self-organizing systems, decentralized in nature. The swarm itself exhibits social cognitive behavior and achieves a goal that individual contributors would not achieve by themselves. The collective achieves the goals rather than the efforts of any individual contributor. This leads us to PSO itself.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Particle Swarm Optimization</h1>
                
            
            
                
<p><strong>Particle Swarm Optimization</strong> is a method (a population-based algorithm) that solves a problem by optimizing it iteratively while trying to improve a potential solution regarding its optimal quality. Every individual particle in the PSO algorithm learns from itself and another particle with a good fitness value. Each particle, which represents a solution, flies through the search space with a velocity that is dynamically adjusted according to its own and its companion's historical behaviors. The particles tend to fly toward better search areas over the course of the search process.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Types of Particle Swarm Optimizations</h1>
                
            
            
                
<p>The following is a list of just some of the variants of Particle Swarm Optimization:</p>
<ul>
<li>Traditional Particle Swarm Optimization</li>
<li>Canonical Particle Swarm Optimization</li>
<li>Fully informed Particle Swarm Optimization</li>
</ul>
<p>Let's now talk a little bit about the theory behind swarm intelligence, and then we'll move into two of the more specialized types of study in that field: Particle Swarm Optimization and ant swarm optimization, which are both direct drop-in replacements for back propagation!</p>
<p>However fascinating and intriguing you find this, please remember that nothing is perfect and there's no single shiny bullet that works for everything. This is a fascinating theory of study and entire books have been written on the subject. However, we always need to keep in mind the <strong>No Free Lunch Theorem for Optimization</strong>.</p>
<p>The No Free Lunch Theorem for Optimization states that no one can propose any one specific algorithm for solving all optimization problems. The success of an algorithm in solving one specific set of problems does not guarantee that it solves all optimization problems. More concretely, all optimization techniques perform equally well on average if you consider every optimization problem despite the performance on a subset of problems.</p>
<p>In a very well written paper titled <em>A time performance comparison of Particle Swarm Optimization in mobile devices</em>, written by Luis Antonio Beltrán Prieto, Zuzana Komínkova-Oplatková, Rubén Torres Frías, and Juan Luis Escoto Hernández, Particle Swarm Optimization is described like this:</p>
<p>"PSO is an optimization technique developed by Kennedy and Eberhart inspired by the collective behavior of animal groups, such as swarms of insects, to build a swarm of particles, i.e., a set of candidate solutions which flow through the parameter space generating trajectories driven by the best individuals. The initial population (swarm) consists of random solutions (particles) for the problem and is considered as a population of homogeneous agents which interact locally with other individuals without any central control. As a result, collective behavior is generated, thus evolution relies on cooperation and competition among individuals through the different epochs (generations). Each particle defines trajectories in the parameter space according to a motion function which is affected by velocity, inertia, cognitive coefficient and social coefficient. The objective is to find the global best solutions by stochastic weighting of the aforementioned elements. The process is iterative until a stopping criterion is met."</p>
<p>More intuitive analogies for Particle Swarm Optimization are birds and how they behave collaboratively, or a swarm of bees and how they determine which flowers to visit or which humans to attack! If you've ever watched a flock of birds flying or inadvertently knocked down a bee's nest then you know exactly what I am referring to.</p>
<p>Now, instead of dealing in just theory, let's take a hypothetical journey, a Treasure Hunt. I am intentionally going to make this as verbose as possible to ensure the analogy fits the problem space. It goes something like this.</p>
<p>You and several of your friends are in a mountainous region trying to find a hidden treasure worth a lot of money. We are not quite sure where the treasure is located, but we do know that it is in the deepest valley in the region. This equates to the minimum elevation in terms of height above sea level.</p>
<p>Let us also state that all our friends can communicate with one another using their cell phones (let's assume we have cell service here!). Let's also assume for now that our cell phones have GPS apps on them that tell us the elevation we are currently at. We will search each day for the treasure until we find it, and at the end of each day we will have either found the treasure and are rich, or we need to update our information and try again the next day. So, here's what each person has:</p>
<ul>
<li>A cell phone with a GPS app to determine elevation.</li>
<li>Pen and paper to track our information at the end of each day. On this we will write the best position we have found (individually), which is our personal best, or <strong>PBEST</strong>. We will also write on this paper the best position that the entire team has found thus far, being our global best value or <strong>GBEST</strong>.</li>
</ul>
<p>The following are the rules that we need to follow in our search:</p>
<ul>
<li>Each person will start in a random location and in a random direction. We determine our elevation right away and write it on our paper. It would be best for us if each person was spread out as much as possible so that we can be efficient and cover more ground, but this is not necessary.</li>
<li>Our journey will take <em>T</em> number of days, to which at this point we are now aware of what that value is or will be.</li>
<li>Every morning we will plan our day.</li>
<li>Communications can only take place at the end of each day.</li>
<li>Each morning, everyone compares the elevations they are at and updates <strong>GBEST</strong> on their paper.</li>
<li><strong>GBEST</strong> is the only piece of information each person can share (location and elevation).</li>
</ul>
<ul>
<li>Each person will update <strong>PBEST</strong> on their paper if they find a better position.</li>
<li><strong>PBEST</strong> information is not shared; no one cares about anything but GBEST.</li>
<li>Take notes of this one; to move each day, each person takes (for instance) <em>x</em> steps in the direction of the last day, <em>y</em> steps in the direction towards <strong>PBEST</strong>, and <em>z</em> steps in the direction of <strong>GBEST</strong>. Confused?</li>
<li>Steps are random as we need some form of randomness in the search to make a stochastic search pattern for everyone as a collective group (that is, a flock or swarm of people).</li>
</ul>
<p>With these few rules behind us, we can start our journey to the treasure. The team as a collective will keep locating different regions while watching the GBEST location found thus far. There is no guarantee of course that we will find the treasure, or that we will find it in the minimal number of days, but generally our search should be effective. Remember, no individual knows the exact location of the treasure, but cooperates with the swarm to develop collective intelligence to help find the treasure faster. For sure, it's better than a completely random search!</p>
<p>Let's try and plot out our steps in pseudo-pseudo-code:</p>
<ol>
<li>Initialize a population of random solutions. For x number of decision variables, we have an x-space in which our solution exists as particles. Each particle has n variables and stores the best fitness for itself and the team.</li>
<li>For each iteration (either a number or a fitness value), calculate the fitness and store the best fitness variable (<strong>PBEST</strong>) and communicate this to the swarm.</li>
<li>Identify <strong>GBEST</strong> by comparing all the information we have received from the collective swarm.</li>
<li>Identify what will take us in the direction of <strong>GBEST</strong> considering our <strong>PBEST</strong> and <strong>GBEST</strong>.</li>
<li>Move in a specific time step in the direction of our velocity vector.</li>
<li>Over time, each team member (our particles in the swarm) will identify better <strong>GBEST</strong> variables and navigate towards them, thus also improving their <strong>PBEST</strong> at the same time.</li>
</ol>
<p>With Particle Swarm Optimization we have three basic components that we should briefly discuss. They are, in no particular order:</p>
<ul>
<li><strong>Position</strong>: Similar to the location in the preceding analogy, referring to the parameter values. This refers to where a particle (bird, bee, fish, and so on) is in an x-dimensional search space.</li>
<li><strong>Velocity</strong>: Similar to the movement direction in the preceding analogy, it is used for storing velocity, which will update each particle's position.</li>
<li><strong>Fitness</strong>: Similar to the elevation in the preceding analogy, this shows how fit the particle is.</li>
</ul>
<p class="mce-root"/>
<p>Velocity is the main part of our Particle Swarm Optimization. It considers the current position of the particle, the best position found by the swarm (<strong>GBEST</strong>) (all particles), and the best position of the current particle (<strong>PBEST</strong>). Mathematically, it breaks down like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/c772b9c1-f58f-405b-a68a-af3d00dcf41c.png" style="width:42.75em;height:1.25em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/dfda3de2-bfd5-4853-8746-103f27d6c048.png" style="width:16.08em;height:1.25em;"/></p>
<p>There are also three hyperparameters that we should mention as you will be hearing about them a lot.</p>
<p><strong>Inertia Weight</strong> (<strong>W</strong>): The inertia weight controls the impact of the previous historical velocities on the current velocity. It regulates the trade-off between the global and local exploration abilities. If the inertia is high, particles are constrained in changing their direction and thus turn around much slower. This implies a larger exploration area and less possibility of convergence towards the optimum.</p>
<p>If inertia is small, then only a small amount of momentum is present from the previous time-step; this allows for much faster changes in direction. The problem here is that it could take quite a bit longer to converge.</p>
<p>By decreasing the inertia weight, it is easier to obtain a better global search ability and make the particles enter the optimal value area earlier. This means it will then be easier to have a better search ability and optimum value.</p>
<ul>
<li><strong>C1</strong>: <strong>Cognitive intelligence</strong></li>
<li><strong><em>C2</em></strong>: <strong>Social intelligence</strong></li>
</ul>
<p>It should be noted that C1 and C2 are positive constants that control how far an individual particle can move within a single iteration. Lower values will allow particles to stray further from the targeted regions before being reined in. Higher values will result in shorter, more abrupt movements toward, or past, the targeted region. By default, we will set both values to 2.0.</p>
<p>You should experiment with the cognitive intelligence and social intelligence values, as sometimes different values lead to improved performance.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Original Particle Swarm Optimization strategy</h1>
                
            
            
                
<p>As the particles (bees, birds, fish, termites) are moving along in the pre-designated search space to determine the best position, during each iteration of the cycle (where a <em>cycle</em> may be referred to as <em>max iterations</em>), each particle updates its velocity and position. Once the new velocity has been determined, it is used to compute the new particle position for the next time step.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Particle Swarm Optimization search strategy</h1>
                
            
            
                
<p>For every particle over time, we will track the inertia (current velocity), the personal best (referred to as PBEST) and the global best (referred to as <strong>GBEST</strong>). As we mentioned, as we move through time to our global minimum, we will be tracking our personal best location, as well as the global best location of the swarm. This information will be communicated to the rest of the group so that the swarms' best location information can be communicated back to the group after each iteration is completed. We need to be either following the swarm or leading it in order to achieve our goals.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Particle Swarm Optimization search strategy pseudo-code</h1>
                
            
            
                
<p>The following is pseudo-code for the logic we will be using to find our global minimum (the location of the treasure):</p>
<pre>Initialize our hyperparameters<br/>Initialize the population of particles<br/>Do<br/>For each particle<br/>Calculate the objective<br/>Update <strong>PBEST<br/></strong>Update <strong>GBEST<br/></strong>End for<br/>Update <strong>Inertia</strong> weight<br/>For each particle<br/>Update <strong>Velocity</strong> (V)<br/>Update <strong>Position</strong> (X)<br/>End for<br/>While the end condition is not satisfied<br/>Return <strong>GBEST</strong> as the best global optimum estimation</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Parameter effects on optimization</h1>
                
            
            
                
<p>There are many varying theories for what each variable in Particle Swarm Optimization should look like. There are the theoretically acceptable values, and then there are the values determined over time with testing. The following are some of the recommendations I am making for your consideration.</p>
<p>The original (canonical) version of the Particle Swarm Optimization algorithm used values of 1, 2, and 2 respectively for Inertia, C1, and C2. These values do seem to work quite well. I have also found through testing, as have others, that values of 0.5, 1.5, and 1.5 respectively work even better, providing the best convergence rate depending upon the function and strategy used. Other values lead to slower or complete non-convergence. You, the reader, should perform your own testing based upon the strategy and function you prefer and determine which values you find suitable for your purpose.</p>
<p>Please note that, depending upon the strategy and function you select, your values should be different to provide proper convergence. For instance, using a minimization strategy and a Step function, I have seen optimal convergence happen using a global value of 0.729 for inertia. The cognitive intelligence (C1) is usually the same as the social intelligence (C2) with a pre-determined value of 2. I should point out however that, as you will see when we get to the chapter on building and using the visual workbench, the default value I use for C1 and C2 is 1.49445.</p>
<p>It is important to note that any of the values shown here are not pulled out of thin air. They come from a tremendous amount of optimization testing. In addition, they closely align with those tested by Clerc and Kennedy (2002) for implementation of constriction coefficients. Please feel free to use your own values and always keep in mind the No Free Lunch theorem.</p>
<p class="CDPAlignLeft CDPAlign">The following is an example of how swarm optimization is affected by weight, social, and cognitive parameters:</p>
<table style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1255 image-border" src="img/8bebba5d-9b6d-4822-92af-efba3568a005.png" style="width:6.83em;height:7.33em;"/></td>
<td class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1254 image-border" src="img/13400d88-1349-4592-9c6e-ca3224db16a7.png" style="width:9.75em;height:8.33em;"/></td>
<td class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1253 image-border" src="img/ff8e8709-92ba-416b-a9d6-cbe78668ffff.png" style="width:13.00em;height:13.75em;"/></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">Iteration=31,w=0,c1=c2=2 </td>
<td class="CDPAlignCenter CDPAlign">Iteration=31,w=0.59,c1=c2=2</td>
<td class="CDPAlignCenter CDPAlign">Iteration=31,w=1,c1=c2=2</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Replacing back propagation with Particle Swarm Optimization</h1>
                
            
            
                
<p>And now we come to the moment of truth. How does any of this apply to my code? In order to answer this question, we are going to use the open source Encog machine learning framework for our next demonstration. You can download our sample project following the instructions for the web location of the files for the book. Please make sure you have it loaded and open in Visual Studio before proceeding:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/0ad5422d-0e11-4afb-b539-ff935d1bec5b.png" style="width:16.92em;height:19.92em;"/></p>
<p>We are going to create a sample application that will demonstrate replacing back propagation with Particle Swarm Optimization. If all goes well, from the outside looking in you will not notice a difference.</p>
<p>You will be able to run this sample out of the box and follow along. We will be using the XOR problem solver, but instead of using back propagation it will be using the Particle Swarm Optimization we've been discussing. Let's dig a little deeper into the code. The following is the data that we will be using to implement this example:</p>
<pre>/// Input for the XOR function.<br/>public static double[][] XORInput = {new[] {0.0, 0.0},new[] {1.0, 0.0},new[] {0.0, 1.0},new[] {1.0, 1.0}};<br/>/// Ideal output for the XOR function.<br/>public static double[][] XORIdeal = {new[] {0.0},new[] {1.0},new[] {1.0},new[] {0.0}};</pre>
<p>Pretty straightforward.</p>
<p>Now let's look at the sample application itself. The following is how the XORPSO implementation is done:</p>
<pre>///Create a basic training data set using the supplied data shown above<br/>IMLDataSet trainingSet = new BasicMLDataSet(XORInput, XORIdeal);<br/>///Create a simple feed forward network<br/>BasicNetwork<strong>network</strong> = EncogUtility.SimpleFeedForward(2, 2, 0, 1, false);<br/>///Create a scoring/fitness object<br/>ICalculateScore score = new TrainingSetScore(trainingSet);<br/>///Create a network weight initializer<br/>IRandomizer randomizer = new NguyenWidrowRandomizer();<br/>///Create the NN PSO trainer. This is our replacement function from back prop<br/>IMLTrain train = new NeuralPSO(<strong>network</strong>, randomizer, score, 20);<br/>///Train the application until it reaches an error rate of 0.01<br/>EncogUtility.TrainToError(train, 0.01);<br/><strong>network</strong> = (BasicNetwork)train.Method;<br/>///Print out the results<br/>EncogUtility.Evaluate(<strong>network</strong>, trainingSet);</pre>
<p>When we run this sample application here is what it looks like. You will notice that it appears exactly like the normal XOR sample from the outside looking in:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1261 image-border" src="img/584424e8-4d92-497d-a14d-de57a029955e.png" style="width:27.42em;height:33.25em;"/></p>
<p>You will notice that, when training is completed, we are very close to our ideal scores.</p>
<p>Now let's talk about the internals. Let's look at some of the internal variables used to make this work. The following is where you will see why we spent time early on with our basic theory. It should all be familiar to you now.</p>
<p>Declare the variable <kbd>m_populationSize</kbd>. A typical range is 20 - 40 for many problems. More difficult problems may need a much higher value. It must be low enough to keep the training process computationally efficient:</p>
<pre>protected int m_populationSize = 30;</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>This determines the size of the search space. The positional components of particle will be bounded to [-maxPos, maxPos]. A well chosen range can improve the performance. <kbd>-1</kbd> is a special value that represents boundless search space:</p>
<pre>protected double m_maxPosition = -1;</pre>
<p>This maximum change one particle can take during one iteration imposes a limit on the maximum absolute value of the velocity components of a particle, and affects the granularity of the search. If too high, particles can fly past the optimum solution. If too low, particles can get stuck in local minima. It is usually set to a fraction of the dynamic range of the search space (10% was shown to be good for high dimensional problems). -1 is a special value that represents boundless velocities:</p>
<pre>protected double m_maxVelocity = 2;</pre>
<p> For c1, cognitive learning rate &gt;= 0 (the tendency to return to the personal best position):</p>
<pre>protected double m_c1 = 2.0;</pre>
<p>For c2, social learning rate &gt;= 0 (tendency to move towards the swarm best position):</p>
<pre>protected double m_c2 = 2.0;</pre>
<p>Inertia weight, w, controls global (higher-value) versus local exploration of the search space. It is analogous to temperature in simulated annealing and must be chosen carefully or gradually decreased over time. The value is usually between 0 and 1:</p>
<pre>protected double m_inertiaWeight = 0.4;</pre>
<p>All these variables should be familiar to you. Next, the heart of what we are doing involves the <kbd>UpdateParticle</kbd> function, shown as follows. This function is responsible for updating the velocity, position, and personal best position of a particle:</p>
<pre>public void UpdateParticle(int particleIndex, bool init)<br/>{<br/>int i = particleIndex;<br/>double[] <strong>particlePosition</strong> = null;<br/>if (init)<br/>{</pre>
<p>Create a new particle with random values (except the first particle, which has the same value as the network passed to the algorithm):</p>
<pre>if (m_networks[i] == null)<br/>{<br/>m_networks[i] = (BasicNetwork)m_bestNetwork.Clone();<br/>if (i &gt; 0) m_randomizer.Randomize(m_networks[i]);<br/>}<br/><strong>particlePosition</strong> = GetNetworkState(i);<br/>m_bestVectors[i] = <strong>particlePosition</strong>;</pre>
<p>Randomize the velocity:</p>
<pre>m_va.Randomise(m_velocities[i], m_maxVelocity);<br/>}<br/>else<br/>{<br/><strong>particlePosition</strong> = GetNetworkState(i);<br/>UpdateVelocity(i, <strong>particlePosition</strong>);</pre>
<p>Velocity clamping:</p>
<pre>m_va.ClampComponents(m_velocities[i], m_maxVelocity);</pre>
<p>New position <img class="fm-editor-equation" src="img/f955e8d5-b0d4-4511-b36a-a2f15a21981e.png" style="width:9.67em;height:1.33em;"/>:</p>
<pre>m_va.Add(<strong>particlePosition</strong>, m_velocities[i]);</pre>
<p>Pin the particle against the boundary of the search space (only for components exceeding <kbd>maxPosition</kbd>):</p>
<pre>m_va.ClampComponents(<strong>particlePosition</strong>, m_maxPosition);<br/>SetNetworkState(i, <strong>particlePosition</strong>);<br/>}<br/>UpdatePersonalBestPosition(i, <strong>particlePosition</strong>);<br/>}</pre>
<p>Each particle will need to have its velocity updated, as you can see in the preceding code. This function will use the inertia weight, cognitive, and social terms to compute the velocity of the particle. This function encompasses the standard Particle Swarm Optimization formula as we described in the pseudo-code earlier in this chapter:</p>
<pre>protected void UpdateVelocity(int particleIndex, double[] particlePosition)<br/>{<br/>int i = particleIndex;<br/>double[] vtmp = new double[particlePosition.Length];</pre>
<p>Standard PSO formula for inertia weight:</p>
<pre>m_va.Mul(m_velocities[i], m_inertiaWeight);</pre>
<p>Standard PSO formula for cognitive term:</p>
<pre>m_va.Copy(vtmp, m_bestVectors[i])<br/>m_va.Sub(vtmp, particlePosition);<br/>m_va.MulRand(vtmp, m_c1);<br/>m_va.Add(m_velocities[i], vtmp);</pre>
<p>Standard PSO formula for social term:</p>
<pre>if (i != m_bestVectorIndex)<br/>{<br/>m_va.Copy(vtmp, m_pseudoAsynchronousUpdate ? m_bestVectors[m_bestVectorIndex] : m_bestVector);<br/>m_va.Sub(vtmp, particlePosition);<br/>m_va.MulRand(vtmp, m_c2);<br/>m_va.Add(m_velocities[i], vtmp);<br/>}<br/>}</pre>
<p>And this is how we substituted Particle Swarm Optimization for the standard backward propagation. Simple, right?</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter we learned some basic theory behind Particle Swarm Optimization. We learned how this algorithm applies to, and has been influenced by flocks of birds, swarms of bees, schools of fish, and more. We also saw how we could replace the standard back propagation formula with Particle Swarm Optimization.</p>
<p>In the next chapter, we are going to learn how to delve into function optimization and show you how you can find optimal parameters, a process that will save you countless hours of testing!</p>


            

            
        
    </body></html>