<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">GRUs Compared to LSTMs, RNNs, and Feedforward networks</h1>
                </header>
            
            <article>
                
<p>In this chapter, we're going to talk about <strong>gated recurrent units</strong> (<strong>GRU</strong>). We will also compare them to LSTMs, which we learned about in the previous chapter. As you know, LSTMs have been around since 1987 and are among the most widely used models in Deep Learning for NLP today. GRUs, however, were first presented in 2014, are a simpler variant of LSTMs that share many of the same properties, train easier and faster, and typically have less computational complexity.</p>
<p>In this chapter, we will learn about the following:</p>
<ul>
<li>GRUs</li>
<li>How GRUs differ from LSTMs</li>
<li>How to implement a GRU</li>
<li>GRU, LTSM, RNN, and Feedforward comparisons</li>
<li>Network differences</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You will be required to have a basic knowledge of .NET development using Microsoft Visual Studio and C#. You will need to download the code for this chapter from the book website.</p>
<p><span>Check out the following video to see Code in Action: </span><a href="http://bit.ly/2OHd7o5">http://bit.ly/2OHd7o5</a>.</p>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">QuickNN</h1>
                </header>
            
            <article>
                
<p><span>To follow along with the code, you should have the QuickNN solution open inside Microsoft Visual Studio. We will be using this code to explain in detail some of the finer points as well as comparisons between coding the different networks. Here is the solution you should have loaded:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-622 image-border" src="assets/4b001255-96d0-4503-b3a4-d79cf7be7ca0.png" style="width:18.33em;height:21.83em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Solution</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding GRUs</h1>
                </header>
            
            <article>
                
<p>GRUs are a cousin to the long short-term memory recurrent neural networks. Both LSTM and GRU networks have additional parameters that control when and how their internal memory is updated. Both can capture long- and short-term dependencies in sequences. The GRU networks, however, involve less parameters than their LSTM cousins, and as a result, are faster to train. The GRU learns how to use its reset and forget gates in order to make longer term predictions while enforcing memory protection. Let's look at a simple diagram of a GRU:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-623 image-border" src="assets/2efeafa7-af09-4338-bf4c-bba5286c6e1f.png" style="width:28.25em;height:20.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">GRU</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Differences between LSTM and GRU</h1>
                </header>
            
            <article>
                
<p>There are a few subtle differences between a LSTM and a GRU, although to be perfectly honest, there are more similarities than differences! For starters, a GRU has one less gate than an LSTM. As you can see in the following diagram, an LSTM has an input gate, a forget gate, and an output gate. A GRU, on the other hand, has only two gates, a reset gate and an update gate. The reset gate determines how to combine new inputs with the previous memory, and the update gate defines how much of the previous memory remains:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-624 image-border" src="assets/2cd83c9c-b843-4099-9df8-98272cf6d59c.png" style="width:38.50em;height:15.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">LSTM vs GRU</div>
<p>Another interesting fact is that if we set the reset gate to all 1s and the update gate to all 0s, do you know what we have? If you guessed a plain old recurrent neural network, you'd be right!</p>
<p>Here are the key differences between a LSTM and a GRU:</p>
<ul>
<li>A GRU has two gates, a LSTM has three.</li>
<li>GRUs do not have an internal memory cell that is different from the exposed hidden state. This is because the output gate that the LSTM has does.</li>
<li>The input and forget gates are coupled by an update gate that weighs the old and new content.</li>
<li>The reset gate is applied directly to the previous hidden state.</li>
<li>We do not apply a second non-linearity when computing the GRU output.</li>
<li>There is no output gate, the weighted sum is what becomes the output.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using a GRU versus a LSTM</h1>
                </header>
            
            <article>
                
<p>Since GRUs are relatively new, the question usually arises as to which network type to use and when. To be honest, there really is no clear winner here, as GRUs have yet to mature, and LSTM variants seem to pop up every month. GRUs do have fewer parameters, and theoretically may train a bit faster than a LSTM. It may also theoretically need less data than a LSTM. On the other hand, if you have a lot of data, the extra power of the LSTM may work better for you.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Coding different networks</h1>
                </header>
            
            <article>
                
<p>In this section, we are going to look at the sample code we described earlier in this chapter. We specifically are going to look at how we build different networks. The <kbd>NetworkBuilder</kbd> is our main object for building the four different types of networks we need for this exercise. You can feel free to modify it and add additional networks if you so desire. Currently, it supports the following networks:</p>
<ul>
<li>LSTM</li>
<li>RNN</li>
<li>GRU</li>
<li>Feedforward</li>
</ul>
<p>The one thing that you will notice in our sample network is that the only difference between networks is how the network itself is created via the <kbd>NetworkBuilder</kbd>. All the remaining code stays the same. You will also note if you look through the example source code that the number of iterations or epochs is much lower in the GRU sample. This is because GRUs are typically easier to train and therefore require fewer iterations. While our normal RNN training is complete somewhere around 50,000 iterations (we let it go to 100,000 just in case), our GRU training loop completes usually in under 10,000 iterations, which is a very large computational saving.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Coding an LSTM</h1>
                </header>
            
            <article>
                
<p>To construct a LSTM, we simply call the <kbd>MakeLstm()</kbd> function of our <kbd>NetworkBuilder</kbd>. This function will ingest several input parameters and return to us a network object:</p>
<pre>INetwork nn = NetworkBuilder.MakeLstm(inputDimension,<br/>hiddenDimension,hiddenLayers,outputDimension,data.GetModelOutputUnitToUse(),<br/>initParamsStdDev, rng);</pre>
<p>As you can see, internally this calls our <kbd>MakeLSTM()</kbd> function inside the <kbd>NetworkBuilder</kbd> object. Here is a look at that code:</p>
<pre>public static NeuralNetwork MakeLstm(int inputDimension, int hiddenDimension, int hiddenLayers, int outputDimension, INonlinearity decoderUnit, double initParamsStdDev, Random rng)<br/>{<br/>List&lt;ILayer&gt; layers = new List&lt;ILayer&gt;();<br/>for (int <strong>h</strong> = 0; <strong>h</strong>&lt;hiddenLayers; <strong>h</strong>++)<br/>{</pre>
<p>Add all of the hidden layers:</p>
<pre>layers.Add(<strong>h</strong> == 0? new LstmLayer(inputDimension, hiddenDimension, initParamsStdDev, rng): new LstmLayer(hiddenDimension, hiddenDimension, initParamsStdDev, rng));<br/>}</pre>
<p>Add the feed forward layer:</p>
<pre>layers.Add(new FeedForwardLayer(hiddenDimension, outputDimension, decoderUnit, initParamsStdDev, rng));</pre>
<p>Create the network:</p>
<pre>return new NeuralNetwork(layers);<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Coding a GRU</h1>
                </header>
            
            <article>
                
<p>To construct a gated recurrent unit, we simply call the <kbd>MakeGru()</kbd> function of our <kbd>NetworkBuilder</kbd> as shown here:</p>
<pre>INetwork nn = NetworkBuilder.MakeGru(inputDimension,<br/>hiddenDimension,<br/>hiddenLayers,<br/>outputDimension,<br/>data.GetModelOutputUnitToUse(),<br/>initParamsStdDev, rng);</pre>
<p>The <kbd>MakeGru()</kbd> function calls the same named function internally to construct our GRU network. Here is a look at how it does it:</p>
<pre>public static NeuralNetwork MakeGru(int inputDimension, int hiddenDimension, int hiddenLayers, int outputDimension, INonlinearity decoderUnit, double initParamsStdDev, Random rng)<br/>{<br/>List&lt;ILayer&gt; layers = new List&lt;ILayer&gt;();<br/>for (int h = 0; h&lt;hiddenLayers; h++)<br/> {<br/> layers.Add(h == 0<br/> ? newGruLayer(inputDimension, hiddenDimension, initParamsStdDev, rng)<br/> : newGruLayer(hiddenDimension, hiddenDimension, initParamsStdDev, rng));<br/> }<br/>layers.Add(new FeedForwardLayer(hiddenDimension, outputDimension, decoderUnit, initParamsStdDev, rng));<br/>return new NeuralNetwork(layers);<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Comparing LSTM, GRU, Feedforward, and RNN operations</h1>
                </header>
            
            <article>
                
<p>In order to help you see the difference in both the creation and results of all the network objects we have been dealing with, I created the sample code that follows. This sample will allow you to see the difference in training times for all four of the network types we have here. As stated previously, the GRU is the easiest to train and therefore will complete faster (in less iterations) than the other networks. When executing the code, you will see that the GRU achieves the optimal error rate typically in under 10,000 iterations, while a conventional RNN and/or LSTM can take 50,000 or more iterations to converge properly.</p>
<p>Here is what our sample code looks like:</p>
<pre>static void Main(string[] args)<br/>{<br/>Console.WriteLine("Running GRU sample", Color.Yellow);<br/>Console.ReadKey();<br/>ExampleGRU.Run();<br/>Console.ReadKey();<br/>Console.WriteLine("Running LSTM sample", Color.Yellow);<br/>Console.ReadKey();<br/>ExampleLSTM.Run();<br/>Console.ReadKey();<br/>Console.WriteLine("Running RNN sample", Color.Yellow);<br/>Console.ReadKey();<br/>ExampleRNN.Run();<br/>Console.ReadKey();<br/>Console.WriteLine("Running Feed Forward sample", Color.Yellow);<br/>Console.ReadKey();<br/>ExampleFeedForward.Run();<br/>Console.ReadKey();<br/>}</pre>
<p>And here is the output from the sample running:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-625 image-border" src="assets/b9cbe876-a369-4124-8446-6fb6922bd871.png" style="width:44.92em;height:32.58em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Output 1</div>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-626 image-border" src="assets/5aa428f9-17b8-4150-876f-db6080f6bc73.png" style="width:45.00em;height:32.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Output 2</div>
<p>Now, let's look at how we create the GRU network and run the program. In the following code segment, we will use our XOR Dataset generator to generate random data for us. For our network, we will have 2 inputs, 1 hidden layer with 3 neurons, and 1 output. Our learning rate is set to 0.001 and our standard deviation is set to 0.08.</p>
<p>We call our <kbd>NetworkBuilder</kbd><strong><em> </em></strong>object, which is responsible for creating all our network variants. We pass all our described parameters to the <kbd>NetworkBuilder</kbd>. Once our network object is created we pass this variable to our trainer and train the network. Once the network training is completed we then test our network to ensure our results are satisfactory. When we create our Graph object for testing, we are sure to pass false to the constructor to let it know that we do not need back propagation:</p>
<pre>public class ExampleGRU<br/> {<br/>public static void Run()<br/> {<br/>Random rng = new Random();<br/>DataSet data = new XorDataSetGenerator();<br/>int inputDimension = 2;<br/>int hiddenDimension = 3;<br/>int outputDimension = 1;<br/>int hiddenLayers = 1;<br/>double learningRate = 0.001;<br/>double initParamsStdDev = 0.08;<br/><br/>INetwork nn = NetworkBuilder.MakeGru(inputDimension,<br/>hiddenDimension, hiddenLayers, outputDimension, newSigmoidUnit(),<br/>initParamsStdDev, rng);<br/><br/>int reportEveryNthEpoch = 10;<br/>int trainingEpochs = 10000; // GRU's typically need less training<br/>Trainer.train&lt;NeuralNetwork&gt;(trainingEpochs, learningRate, nn, data, reportEveryNthEpoch, rng);<br/>Console.WriteLine("Training Completed.", Color.Green);<br/>Console.WriteLine("Test: 1,1", Color.Yellow);<br/>Matrix input = new Matrix(new double[] { 1, 1 });<br/>Matrix output = nn.Activate(input, new Graph(false));<br/>Console.WriteLine("Test: 1,1. Output:" + output.W[0], Color.Yellow);<br/>Matrix input1 = new Matrix(new double[] { 0, 1 });<br/>Matrix output1 = nn.Activate(input1, new Graph(false));<br/>Console.WriteLine("Test: 0,1. Output:" + output1.W[0], Color.Yellow);<br/>Console.WriteLine("Complete", Color.Yellow);<br/> }<br/> }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Network differences</h1>
                </header>
            
            <article>
                
<p>As mentioned earlier, the only difference between our networks are the layers that are created and added to the network object. In an LSTM we will add LSTM layers, and in a GRU, unsurprisingly, we will add GRU layers, and so forth. All four types of creation functions are displayed as follows for you to compare:</p>
<pre>public static NeuralNetwork MakeLstm(int inputDimension, int hiddenDimension, int hiddenLayers, int outputDimension, INonlinearity decoderUnit, double initParamsStdDev, Random rng)<br/>{<br/>    List&lt;ILayer&gt; layers = new List&lt;ILayer&gt;();<br/>    for (int h = 0; h&lt;hiddenLayers; h++)<br/>    {<br/>        layers.Add(h == 0<br/>         ? new LstmLayer(inputDimension, hiddenDimension, initParamsStdDev, rng)<br/>         : new LstmLayer(hiddenDimension, hiddenDimension, initParamsStdDev, rng));<br/>    }<br/>    layers.Add(new FeedForwardLayer(hiddenDimension, outputDimension, decoderUnit,         initParamsStdDev, rng));<br/>    return new NeuralNetwork(layers);<br/>}<br/><br/><br/>public static NeuralNetwork MakeFeedForward(int inputDimension, int hiddenDimension, inthiddenLayers, int outputDimension, INonlinearity hiddenUnit, INonlinearity decoderUnit, double initParamsStdDev, Random rng)<br/>{<br/>    List&lt;ILayer&gt; layers = new List&lt;ILayer&gt;();<br/>    for (int h = 0; h&lt;hiddenLayers; h++)<br/>    {<br/>        layers.Add(h == 0? new FeedForwardLayer(inputDimension, hiddenDimension,         hiddenUnit, initParamsStdDev, rng): new FeedForwardLayer(hiddenDimension,         hiddenDimension, hiddenUnit, initParamsStdDev, rng));<br/>    }<br/>    layers.Add(new FeedForwardLayer(hiddenDimension, outputDimension,             decoderUnit, initParamsStdDev, rng));<br/>    return new NeuralNetwork(layers);<br/> }<br/><br/><br/>public static NeuralNetwork MakeGru(int inputDimension, int hiddenDimension, int hiddenLayers, int outputDimension, INonlinearity decoderUnit, double initParamsStdDev, Random rng)<br/>{<br/>    List&lt;ILayer&gt; layers = new List&lt;ILayer&gt;();<br/>    for (int h = 0; h&lt;hiddenLayers; h++)<br/>    {<br/>        layers.Add(h == 0? new GruLayer(inputDimension, hiddenDimension, initParamsStdDev, rng): new GruLayer(hiddenDimension, hiddenDimension, initParamsStdDev, rng));<br/>    }<br/>    layers.Add(new FeedForwardLayer(hiddenDimension, outputDimension, decoderUnit, initParamsStdDev, rng));<br/>    return new NeuralNetwork(layers);<br/>}<br/><br/><br/>public static NeuralNetwork MakeRnn(int inputDimension, int hiddenDimension, int hiddenLayers, int outputDimension, INonlinearity hiddenUnit, INonlinearity decoderUnit, double initParamsStdDev, Random rng)<br/>{<br/>    List&lt;ILayer&gt; layers = new List&lt;ILayer&gt;();<br/>    for (int h = 0; h&lt;hiddenLayers; h++)<br/>    {<br/>        layers.Add(h == 0? new RnnLayer(inputDimension, hiddenDimension, hiddenUnit, initParamsStdDev, rng)<br/>        : new RnnLayer(hiddenDimension, hiddenDimension, hiddenUnit, initParamsStdDev, rng));<br/>    }<br/>    layers.Add(new FeedForwardLayer(hiddenDimension, outputDimension, decoderUnit, initParamsStdDev, rng));<br/>    return new NeuralNetwork(layers);<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned about GRUs. We showed how they compared to, and differed from, LSTM Networks. We also showed you an example program that tested all the network types we discussed and produced their outputs. We also compared how these networks are created.</p>
<p>I hope you enjoyed your journey with me throughout this book. As we as authors try and better understand what readers would like to see and hear, I welcome your constructive comments and feedback, which will only help to make the book and source code better. Till the next book, happy coding!</p>


            </article>

            
        </section>
    </body></html>