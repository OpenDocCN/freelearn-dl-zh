- en: '15'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cross-Validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Cross-validation** is a statistical technique used to assess how well a machine
    learning model generalizes to unseen data. It involves partitioning a dataset
    into multiple subsets or “folds,” training the model on some of these subsets
    while testing it on the remaining ones. This process is repeated to ensure a reliable
    performance estimate. This helps detect overfitting and provides a more robust
    evaluation than a single train-test split. In the context of LLMs, cross-validation
    must be adapted to address the complexities of pre-training, fine-tuning, few-shot
    learning, and domain generalization, making it an essential tool for evaluating
    model performance across varied tasks and data distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will explore cross-validation strategies specifically designed
    for LLMs. We’ll delve into methods for creating appropriate data splits for pre-training
    and fine-tuning, as well as strategies for few-shot and zero-shot evaluation.
    You’ll learn how to assess domain and task generalization in LLMs and handle the
    unique challenges of cross-validation in the context of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you’ll be equipped with robust cross-validation
    techniques to reliably assess your LLM’s performance and generalization capabilities
    across various domains and tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Pre-training and fine-tuning data splits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Few-shot and zero-shot evaluation strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Domain and task generalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continual learning evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-validation challenges and best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pre-training and fine-tuning data splits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In LLMs, data splits refer to the division of datasets into training, validation,
    and test sets to ensure the model learns generalizable patterns rather than memorizing
    data. This is essential for evaluating performance fairly, tuning model parameters,
    and preventing data leakage. Proper splitting is especially important in LLMs
    due to their scale, the diversity of tasks, and the need to assess domain and
    task generalization.
  prefs: []
  type: TYPE_NORMAL
- en: Stratified sampling for pre-training data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Stratified sampling** is a sampling method that first divides the population
    into smaller subgroups (**strata**) based on shared characteristics and then randomly
    samples from within each stratum to ensure proportional representation of all
    groups in the final sample. This is particularly useful when dealing with imbalanced
    datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When creating data splits for pre-training, it’s important to ensure that each
    split represents the diversity of the entire dataset. Here’s an example of how
    you might implement **stratified sampling** for pre-training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This code uses `StratifiedShuffleSplit` to create a stratified split of the
    pre-training data, ensuring that the distribution of domains (or any other relevant
    categorical variable) is similar in both the training and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: Time-based splitting for fine-tuning data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For fine-tuning tasks that involve time-sensitive data, it’s often beneficial
    to use **time-based splitting**.
  prefs: []
  type: TYPE_NORMAL
- en: Time-based splitting is a data partitioning strategy where the dataset is divided
    according to chronological order, ensuring that earlier data is used for training
    and later data for validation or testing. This approach is especially important
    for fine-tuning tasks involving time-sensitive data—such as financial forecasting,
    user behavior modeling, or event prediction—where future information should not
    influence past training. By preserving the natural temporal sequence, time-based
    splitting helps evaluate how well a model can generalize to future, unseen scenarios,
    closely mimicking real-world deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach helps evaluate how well the model generalizes to future data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This function splits the data based on a specified date, which is particularly
    useful for tasks where the model needs to generalize to future events or trends.
  prefs: []
  type: TYPE_NORMAL
- en: Oversampling and weighting techniques for data balancing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When working with datasets that have an uneven distribution of categories, such
    as imbalanced domains or label frequencies, **oversampling** and **weighting techniques**
    can help ensure that the model learns effectively from all classes. Oversampling
    involves replicating examples from underrepresented categories to increase their
    presence in the training data, preventing the model from ignoring them. This can
    be done using methods such as random oversampling or synthetic data generation
    (e.g., SMOTE for structured data). On the other hand, weighting techniques adjust
    the loss function by assigning higher importance to underrepresented categories,
    so the model learns from them without necessarily increasing the dataset size.
    Both approaches help mitigate bias, improving the model’s ability to generalize
    across all categories, rather than favoring the most frequent ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a short code example demonstrating oversampling and class weighting
    techniques using PyTorch and sklearn, applied to a text classification task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The code demonstrates two common techniques for addressing class imbalance
    in classification tasks: class weighting and oversampling. First, it uses `compute_class_weight`
    of `sklearn` to calculate weights inversely proportional to class frequencies,
    assigning higher importance to underrepresented classes (e.g., class 2, which
    appears less often). These weights are passed to PyTorch’s `CrossEntropyLoss`,
    so that during training, misclassifying rare classes penalizes the model more
    than misclassifying common ones. Second, it performs oversampling by computing
    a per-sample weight based on the inverse frequency of each sample’s class, which
    ensures that samples from minority classes have a higher probability of being
    selected during training. These sample weights are used to initialize PyTorch’s
    `WeightedRandomSampler`, which enables the `DataLoader` to sample training data
    in a balanced way across classes without having to physically duplicate data.
    Together, these techniques help the model learn to treat all classes fairly, improving
    its generalization on imbalanced datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot and zero-shot evaluation strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Few-shot and zero-shot evaluation strategies enable LLMs to generalize across
    tasks without requiring extensive retraining. Zero-shot learning is useful for
    tasks where no labeled examples are available, while few-shot learning enhances
    performance by providing limited guidance. These methods are key to making LLMs
    adaptable and scalable for real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a comparison between the two strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Aspect** | **Zero-shot** | **Few-shot** |'
  prefs: []
  type: TYPE_TB
- en: '| **Description** | No examples; model must infer task from prompt alone |
    Provides a small number of labeled examples in the prompt |'
  prefs: []
  type: TYPE_TB
- en: '| **Strengths** | No labeled data needed, highly flexible | Higher accuracy,
    better task comprehension |'
  prefs: []
  type: TYPE_TB
- en: '| **Weaknesses** | Lower accuracy, risk of ambiguity | Requires careful example
    selection, still less effective than fine-tuning |'
  prefs: []
  type: TYPE_TB
- en: '| **Use Cases** | Open-ended Q&A, commonsense reasoning, general-knowledge
    tasks | Text classification, translation, summarization, code generation |'
  prefs: []
  type: TYPE_TB
- en: Table 15.1 – Few-shot vs. zero-shot
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how to implement each of these strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In **few-shot evaluation**, we provide the model with a small number of examples
    before asking it to perform a task. Here’s an example of how you might implement
    few-shot evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This code demonstrates how to perform few-shot evaluation on a sentiment analysis
    task using a pre-trained GPT-2 model.
  prefs: []
  type: TYPE_NORMAL
- en: The `few_shot_evaluate` function takes a GPT-2 model, tokenizer, task description,
    examples, and a test instance as input. It constructs a `tokenizer.encode`, converting
    it into numerical tokens suitable for the model. The function then uses `model.generate`
    inside a `torch.no_grad()` block to generate text without computing gradients,
    making inference more efficient. The model generates a response with a maximum
    length of `100` tokens, ensuring it stays concise. The generated text is then
    decoded using `tokenizer.decode`, with `skip_special_tokens=True` to remove unwanted
    tokens. Finally, the function extracts the part of the response after the last
    occurrence of `"Output:"` to isolate the model’s generated answer, trimming any
    extra whitespace. This approach effectively enables **few-shot learning**, where
    the model leverages provided examples to make a more informed prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Zero-shot evaluation** tests a model’s ability to perform a task without
    any specific examples. Here’s how you might implement zero-shot evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This function demonstrates zero-shot evaluation on a text classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `zero_shot_evaluate` function performs `task_description` with `test_instance`,
    ensuring that the model understands the task and what it needs to classify. The
    phrase `"Output:"` is appended to signal where the model should generate its response.
    The prompt is then tokenized using tokenizer.encode, converting it into numerical
    input tensors that the model can process. The function uses `torch.no_grad()`
    to disable gradient computation, making inference more efficient. The `model.generate`
    function takes the tokenized prompt and generates an output sequence with a maximum
    length of `100` tokens while returning only one sequence. The generated output
    is then decoded back into text using `tokenizer.decode`, ensuring that any special
    tokens are removed. Finally, the function extracts and returns the portion of
    the generated text that appears after `"Output:"`, which represents the model’s
    predicted classification. In the example usage, the function is applied to a classification
    task where the model is asked to categorize a given text snippet`—"NASA''s Mars
    rover has discovered traces of ancient microbial life."`—into one of the predefined
    categories: `Science`, `Politics`, `Sports`, or `Entertainment`. The model, without
    seeing any labeled examples, infers the correct category based on its prior knowledge.
    The output is then printed, demonstrating the model’s zero-shot classification
    ability.'
  prefs: []
  type: TYPE_NORMAL
- en: Domain and task generalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Assessing how well an LLM generalizes across different domains and tasks is
    crucial for understanding its true capabilities. Let’s explore some techniques
    for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating domain adaptation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To evaluate **domain adaptation**, we can test the model on data from a different
    domain than it was trained on. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is how we evaluate domain adaptation and print out the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Putting it together, we can use the preceding code to evaluate the model’s performance
    on both the source domain (what it was trained on) and the target domain, calculating
    the drop in performance as a measure of domain adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating task generalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To assess **task generalization**, we can evaluate the model on a variety of
    tasks it wasn’t specifically fine-tuned for. Here’s an example using the GLUE
    benchmark (which we discussed in [*Chapter 14*](B31249_14.xhtml#_idTextAnchor230)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is how to run the evaluation based on the previously defined
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Putting together the preceding code, we can evaluate the model on multiple GLUE
    tasks to assess its ability to generalize across different NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Continual learning evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Continual learning** is the ability of a model to learn new tasks without
    forgetting previously learned ones. Here’s an example of how you might evaluate
    continual learning in LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up our continual learning framework by initializing the model, the tokenizer,
    and the main function structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the preprocessing function that handles different input formats for
    various GLUE tasks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Preprocess and prepare the dataset for each task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Provide the training setup and execution for each task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Conduct an evaluation across all previously seen tasks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the evaluation and display the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Putting the preceding code blocks together, we show how to fine-tune the model
    on a sequence of tasks and evaluate its performance on all previously seen tasks
    after each fine-tuning step, allowing us to assess how well it retains knowledge
    of earlier tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation challenges and best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LLMs present unique challenges for cross-validation due to their scale and
    the nature of their training data. Here are some key challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data contamination**: Avoiding test set overlap with pre-training data is
    difficult given the vast and diverse web data LLMs are trained on, making it hard
    to ensure a truly unseen validation set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computational cost**: Traditional methods such as k-fold cross-validation
    are often infeasible due to the immense computational resources required for models
    of this scale'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Domain shift**: LLMs may show inconsistent performance when exposed to data
    from underrepresented or entirely new domains, complicating the evaluation of
    generalizability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt sensitivity**: The performance of LLMs can vary significantly based
    on subtle differences in prompt wording, adding another layer of variability to
    the validation process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on these challenges, here are some best practices for LLM cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mitigate data contamination**: Use rigorous data deduplication methods to
    identify and remove overlaps between the pre-training corpus and validation datasets.
    Tools such as MinHash or Bloom filters can efficiently detect near-duplicates
    in large datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MinHash
  prefs: []
  type: TYPE_NORMAL
- en: '**MinHash** is a probabilistic technique for quickly estimating how similar
    two sets are by converting large sets into smaller, representative fingerprints
    (**hashes**) where the probability of hash collision is proportional to the similarity
    between the original sets, making it particularly useful for detecting near-duplicate
    content in large datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '**MinHashLSH** is based on MinHash and **locality-sensitive hashing** (**LSH**),
    which groups similar items into the same “buckets” to enable fast lookup and comparison.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code example demonstrates data deduplication using MinHash and
    MinHashLSH for detecting near-duplicates in datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**Reduce computational cost**: Use stratified sampling or a single-split validation
    (e.g., train-validation-test) approach to minimize computational overhead. Alternatively,
    employ smaller model checkpoints or distilled versions of the LLM during experimentation
    before scaling up.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code example shows stratified sampling for efficient validation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Handle domain shift**: Construct validation datasets with explicit representation
    from diverse domains. Fine-tune models with representative domain-specific data
    to reduce performance gaps in underrepresented areas.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This code example demonstrates handling domain shift through domain-specific
    validation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Address prompt sensitivity**: Perform prompt engineering systematically.
    Use techniques such as prompt paraphrasing, instruction tuning, or ensemble evaluation
    across multiple prompts to ensure robustness and minimize the variability introduced
    by prompt changes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code example shows systematic prompt engineering with multiple
    variants:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following code example shows how to combine all these approaches into a
    single evaluation pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cross-validation for LLMs requires careful consideration of their unique characteristics
    and capabilities. By implementing these advanced techniques and best practices,
    you can obtain a more robust and comprehensive assessment of your LLM’s performance
    across various domains and tasks.
  prefs: []
  type: TYPE_NORMAL
- en: As we move forward, the next chapter will delve into the crucial topic of interpretability
    in LLMs. We’ll explore techniques for understanding and explaining the outputs
    and behaviors of LLMs.
  prefs: []
  type: TYPE_NORMAL
