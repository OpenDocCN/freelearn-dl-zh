["```py\nimport gensim\nprint(f'gensim: {gensim.__version__}')\n> gensim: 3.4.0\n```", "```py\nC:\\Users\\nirantk\\Anaconda3\\envs\\fastai\\lib\\site-packages\\Gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n```", "```py\nfrom tqdm import tqdm\nclass TqdmUpTo(tqdm):\n    def update_to(self, b=1, bsize=1, tsize=None):\n        if tsize is not None: self.total = tsize\n        self.update(b * bsize - self.n)\n\ndef get_data(url, filename):\n    \"\"\"\n    Download data if the filename does not exist already\n    Uses Tqdm to show download progress\n    \"\"\"\n    import os\n    from urllib.request import urlretrieve\n\n    if not os.path.exists(filename):\n\n        dirname = os.path.dirname(filename)\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n\n        with TqdmUpTo(unit='B', unit_scale=True, miniters=1, desc=url.split('/')[-1]) as t:\n            urlretrieve(url, filename, reporthook=t.update_to)\n```", "```py\nembedding_url = 'http://nlp.stanford.edu/data/glove.6B.zip'\nget_data(embedding_url, 'data/glove.6B.zip')\n```", "```py\n# # We need to run this only once, can unzip manually unzip to the data directory too\n# !unzip data/glove.6B.zip \n# !mv glove.6B.300d.txt data/glove.6B.300d.txt \n# !mv glove.6B.200d.txt data/glove.6B.200d.txt \n# !mv glove.6B.100d.txt data/glove.6B.100d.txt \n# !mv glove.6B.50d.txt data/glove.6B.50d.txt\n```", "```py\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nglove_input_file = 'data/glove.6B.300d.txt'\nword2vec_output_file = 'data/glove.6B.300d.word2vec.txt'\n```", "```py\nimport os\nif not os.path.exists(word2vec_output_file):\n    glove2word2vec(glove_input_file, word2vec_output_file)\n```", "```py\nfrom gensim.models import KeyedVectors\nfilename = word2vec_output_file\n```", "```py\n%%time\n# load the Stanford GloVe model from file, this is Disk I/O and can be slow\npretrained_w2v_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n# binary=False format for human readable text (.txt) files, and binary=True for .bin files\n```", "```py\n(king - man) + woman = ?\n```", "```py\n# calculate: (king - man) + woman = ?\nresult = pretrained_w2v_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n```", "```py\nprint(result)\n> [('queen', 0.6713277101516724)]\n```", "```py\nresult = pretrained_w2v_model.most_similar(positive=['quora', 'facebook'], negative=['linkedin'], topn=1)\nprint(result)\n```", "```py\n[('twitter', 0.37966805696487427)]\n```", "```py\npretrained_w2v_model.most_similar('india')\n```", "```py\n[('indian', 0.7355823516845703),\n ('pakistan', 0.7285579442977905),\n ('delhi', 0.6846907138824463),\n ('bangladesh', 0.6203191876411438),\n ('lanka', 0.609517514705658),\n ('sri', 0.6011613607406616),\n ('kashmir', 0.5746493935585022),\n ('nepal', 0.5421023368835449),\n ('pradesh', 0.5405811071395874),\n ('maharashtra', 0.518537700176239)]\n```", "```py\ntry: \n  pretrained_w2v_model.wv.most_similar('nirant')\nexcept Exception as e: \n  print(e)  \n```", "```py\n\"word 'nirant' not in vocabulary\"\n```", "```py\nted_dataset = \"https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip\"\nget_data(ted_dataset, \"data/ted_en.zip\")\n```", "```py\nimport zipfile\nimport lxml.etree\nwith zipfile.ZipFile('data/ted_en.zip', 'r') as z:\n    doc = lxml.etree.parse(z.open('ted_en-20160408.xml', 'r'))\ninput_text = '\\n'.join(doc.xpath('//content/text()'))\n\n```", "```py\ninput_text[:500]\n> \"Here are two reasons companies fail: they only do more of the same, or they only do what's new.\\nTo me the real, real solution to quality growth is figuring out the balance between two activities: exploration and exploitation. Both are necessary, but it can be too much of a good thing.\\nConsider Facit. I'm actually old enough to remember them. Facit was a fantastic company. They were born deep in the Swedish forest, and they made the best mechanical calculators in the world. Everybody used them. A\"\n```", "```py\nimport re\n# remove parenthesis \ninput_text_noparens = re.sub(r'\\([^)]*\\)', '', input_text)\n\n# store as list of sentences\nsentences_strings_ted = []\nfor line in input_text_noparens.split('\\n'):\n    m = re.match(r'^(?:(?P<precolon>[^:]{,20}):)?(?P<postcolon>.*)$', line)\n    sentences_strings_ted.extend(sent for sent in m.groupdict()['postcolon'].split('.') if sent)\n\n# store as list of lists of words\nsentences_ted = []\nfor sent_str in sentences_strings_ted:\n    tokens = re.sub(r\"[^a-z0-9]+\", \" \", sent_str.lower()).split()\n    sentences_ted.append(tokens)\n```", "```py\nprint(sentences_ted[:2])\n```", "```py\nimport json\n# with open('ted_clean_sentences.json', 'w') as fp:\n#     json.dump(sentences_ted, fp)\n\nwith open('ted_clean_sentences.json', 'r') as fp:\n    sentences_ted = json.load(fp)\n\n```", "```py\nfrom gensim.models.fasttext import FastText\n```", "```py\nfasttext_ted_model = FastText(sentences_ted, size=100, window=5, min_count=5, workers=-1, sg=1)\n # sg = 1 denotes skipgram, else CBOW is used\n```", "```py\nfasttext_ted_model.wv.most_similar(\"india\")\n\n[('indians', 0.5911639928817749),\n ('indian', 0.5406097769737244),\n ('indiana', 0.4898717999458313),\n ('indicated', 0.4400438070297241),\n ('indicate', 0.4042605757713318),\n ('internal', 0.39166826009750366),\n ('interior', 0.3871103823184967),\n ('byproducts', 0.3752930164337158),\n ('princesses', 0.37265270948410034),\n ('indications', 0.369659960269928)]\n```", "```py\nfrom gensim.models.word2vec import Word2Vec\n```", "```py\nword2vec_ted_model = Word2Vec(sentences=sentences_ted, size=100, window=5, min_count=5, workers=-1, sg=1)\n```", "```py\nword2vec_ted_model.wv.most_similar(\"india\")\n\n[('cent', 0.38214215636253357),\n ('dichotomy', 0.37258434295654297),\n ('executing', 0.3550642132759094),\n ('capabilities', 0.3549191951751709),\n ('enormity', 0.3421599268913269),\n ('abbott', 0.34020164608955383),\n ('resented', 0.33033430576324463),\n ('egypt', 0.32998529076576233),\n ('reagan', 0.32638251781463623),\n ('squeezing', 0.32618749141693115)]\n```", "```py\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nimport gensim\nfrom pprint import pprint\nimport multiprocessing\n```", "```py\ntalks = doc.xpath('//content/text()')\n```", "```py\ndef read_corpus(talks, tokens_only=False):\n    for i, line in enumerate(talks):\n        if tokens_only:\n            yield gensim.utils.simple_preprocess(line)\n        else:\n            # For training data, add tags\n            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])\n```", "```py\nread_corpus(talks)\n```", "```py\n<generator object read_corpus at 0x0000024741DBA990>\n```", "```py\nted_talk_docs = list(read_corpus(talks))\n```", "```py\nted_talk_docs[0]\n\nTaggedDocument(words=['here', 'are', 'two', 'reasons', 'companies', 'fail', ...., 'you', 'already', 'know', 'don', 'forget', 'the', 'beauty', 'is', 'in', 'the', 'balance', 'thank', 'you', 'applause'], tags=[0])\n```", "```py\ncores = multiprocessing.cpu_count()\nprint(cores)\n8\n```", "```py\nmodel = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, iter=5, workers=cores)\n```", "```py\nmodel.build_vocab(ted_talk_docs)\n```", "```py\nsentence_1 = 'Modern medicine has changed the way we think about healthcare, life spans and by extension career and marriage'\n\nsentence_2 = 'Modern medicine is not just a boon to the rich, making the raw chemicals behind these is also pollutes the poorest neighborhoods'\n\nsentence_3 = 'Modern medicine has changed the way we think about healthcare, and increased life spans, delaying weddings'\n```", "```py\nmodel.docvecs.similarity_unseen_docs(model, sentence_1.split(), sentence_3.split())\n> -0.18353473068679\n\nmodel.docvecs.similarity_unseen_docs(model, sentence_1.split(), sentence_2.split())\n> -0.08177642293252027\n```", "```py\n%time model.train(ted_talk_docs, total_examples=model.corpus_count, epochs=model.epochs)\nWall time: 6.61 s\n```", "```py\nmodel.infer_vector(sentence_1.split())\n\narray([-0.03805782,  0.09805363, -0.07234333,  0.31308332,  0.09668373,\n       -0.01471598, -0.16677614, -0.08661497, -0.20852503, -0.14948   ,\n       -0.20959479,  0.17605443,  0.15131783, -0.17354141, -0.20173495,\n        0.11115499,  0.38531387, -0.39101505,  0.12799   ,  0.0808568 ,\n        0.2573657 ,  0.06932276,  0.00427534, -0.26196653,  0.23503092,\n        0.07589306, -0.01828301,  0.38289976, -0.04719075, -0.19283117,\n        0.1305226 , -0.1426582 , -0.05023642, -0.11381021,  0.04444459,\n       -0.04242943,  0.08780348,  0.02872207, -0.23920575,  0.00984556,\n        0.0620702 , -0.07004016,  0.15629964,  0.0664391 ,  0.10215732,\n        0.19148728, -0.02945088,  0.00786009, -0.05731675, -0.16740018,\n       -0.1270729 ,  0.10185472,  0.16655563,  0.13184668,  0.18476236,\n       -0.27073956, -0.04078012, -0.12580603,  0.02078131,  0.23821649,\n        0.09743162, -0.1095973 , -0.22433399, -0.00453655,  0.29851952,\n       -0.21170728,  0.1928157 , -0.06223159, -0.044757  ,  0.02430432,\n        0.22560015, -0.06163954,  0.09602281,  0.09183675, -0.0035969 ,\n        0.13212039,  0.03829316,  0.02570504, -0.10459486,  0.07317936,\n        0.08702451, -0.11364868, -0.1518436 ,  0.04545208,  0.0309107 ,\n       -0.02958601,  0.08201223,  0.26910907, -0.19102073,  0.00368607,\n       -0.02754402,  0.3168101 , -0.00713515, -0.03267708, -0.03792975,\n        0.06958092, -0.03290432,  0.03928463, -0.10203536,  0.01584929],\n      dtype=float32)\n```", "```py\nmodel.docvecs.similarity_unseen_docs(model, sentence_1.split(), sentence_3.split())\n0.9010817740272721\n\nmodel.docvecs.similarity_unseen_docs(model, sentence_1.split(), sentence_2.split())\n0.7461058869759862\n```", "```py\nmodel.docvecs.similarity_unseen_docs(model, sentence_2.split(), sentence_3.split())\n0.8189999598358203\n```", "```py\nranks = []\nfor idx in range(len(ted_talk_docs)):\n    inferred_vector = model.infer_vector(ted_talk_docs[idx].words)\n    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n    rank = [docid for docid, sim in sims].index(idx)\n    ranks.append(rank)\n```", "```py\nimport collections\ncollections.Counter(ranks) # Results vary due to random seeding + very small corpus\nCounter({0: 2079, 1: 2, 4: 1, 5: 2, 2: 1})\n```", "```py\ndoc_slice = ' '.join(ted_talk_docs[idx].words)[:500]\nprint(f'Document ({idx}): «{doc_slice}»\\n')\nprint(f'SIMILAR/DISSIMILAR DOCS PER MODEL {model}')\nfor label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n      doc_slice = ' '.join(ted_talk_docs[sims[index][0]].words)[:500]\n      print(f'{label} {sims[index]}: «{doc_slice}»\\n')\n\n```", "```py\nDocument (2084): «if you re here today and very happy that you are you've all heard about how sustainable development will save us from ourselves however when we're not at ted we're often told that real sustainability policy agenda is just not feasible especially in large urban areas like new york city and that because most people with decision making powers in both the public and the private sector really don't feel as though they are in danger the reason why here today in part is because of dog an abandoned puppy»\n\nSIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)\n MOST (2084, 0.893369197845459): «if you are here today and very happy that you are you've all heard about how sustainable development will save us from ourselves however when we are not at ted we are often told that real sustainability policy agenda is just not feasible especially in large urban areas like new york city and that because most people with decision making powers in both the public and the private sector really don feel as though they re in danger the reason why here today in part is because of dog an abandoned puppy»\n\nMEDIAN (1823, 0.42069244384765625): «so going to talk today about collecting stories in some unconventional ways this is picture of me from very awkward stage in my life you might enjoy the awkwardly tight cut off pajama bottoms with balloons anyway it was time when was mainly interested in collecting imaginary stories so this is picture of me holding one of the first watercolor paintings ever made and recently I've been much more interested in collecting stories from reality so real stories and specifically interested in collecting »\n\nLEAST (270, 0.12334088981151581): «on june precisely at in balmy winter afternoon in so paulo brazil typical south american winter afternoon this kid this young man that you see celebrating here like he had scored goal juliano pinto years old accomplished magnificent deed despite being paralyzed and not having any sensation from mid chest to the tip of his toes as the result of car crash six years ago that killed his brother and produced complete spinal cord lesion that left juliano in wheelchair juliano rose to the occasion and»\n```"]