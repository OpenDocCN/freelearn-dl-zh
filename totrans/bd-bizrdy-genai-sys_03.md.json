["```py\n!curl -L https://raw.githubusercontent.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/master/commons/grequests.py --output grequests.py \n```", "```py\ndownload(\"commons\",\"requirements02.py\") \n```", "```py\n# Run the setup script to install and import dependencies\n%run requirements02 \n```", "```py\nimport subprocess\nimport sys\ndef run_command(command):\n    try:\n        subprocess.check_call(command)\n    except subprocess.CalledProcessError as e:\n        print(f\"Command failed: {' '.join(command)}\\nError: {e}\")\n        sys.exit(1)\n# Uninstall the 'pinecone-client' package\nprint(\"Uninstalling 'pinecone-client'...\")\nrun_command(\n    [sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"pinecone-client\"]\n)\n# Install the specific version of 'pinecone-client'\nprint(\"Installing 'pinecone-client' version 5.0.1...\")\nrun_command(\n    [\n        sys.executable, \"-m\", \"pip\", \"install\",\\\n        \"--force-reinstall\", \"pinecone-client==5.0.1\"\n    ]\n) \n```", "```py\n# Verify the installation\ntry:\n    import pinecone\n    print(\n        f\"'pinecone-client' version {pinecone.__version__} is installed.\"\n)\nexcept ImportError:\n    print(\n        \"Failed to import the 'pinecone-client' library after installation.\"\n)\n    sys.exit(1) \n```", "```py\nUninstalling 'pinecone-client'...\nInstalling 'pinecone-client' version 5.0.1...\n'pinecone-client' version 5.0.1 is installed. \n```", "```py\ndownload(\"commons\",\"pinecone_setup.py\") \n```", "```py\n# Import libraries\nimport openai\nimport os\nfrom google.colab import userdata\n# Function to initialize the Pinecone API key\ndef initialize_pinecone_api():\n    # Access the secret by its name\n    PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n\n    if not PINECONE_API_KEY:\n        raise ValueError(\"PINECONE_API_KEY is not set in userdata!\")\n\n    # Set the API key in the environment and OpenAI\n    os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n    print(\"PINECONE_API_KEY initialized successfully.\") \n```", "```py\nif google_secrets==True:\n    import pinecone_setup\n    pinecone_setup.initialize_pinecone_api() \n```", "```py\nif google_secrets==False: # Uncomment the code and choose any method you wish to initialize the Pinecone API key\n    import os\n    #PINECONE_API_KEY=[YOUR PINECONE_API_KEY]\n    #os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n    #openai.api_key = os.getenv(\"PINECONE_API_KEY\")\n    #print(\"Pinecone API key initialized successfully.\") \n```", "```py\ndownload(\"Chapter03\",\"scenario.csv\") \n```", "```py\nimport time\nstart_time = time.time()  # Start timing\n# File path\nfile_path = 'scenario.csv'\n# Read the file, skip the header, and clean the lines\nchunks = []\nwith open(file_path, 'r') as file:\n    next(file)  # Skip the header line\n    chunks = [line.strip() for line in file]  # Read and clean lines as chunks \n```", "```py\nresponse_time = time.time() - start_time  # Measure response time\nprint(f\"Response Time: {response_time:.2f} seconds\")  # Print response time \n```", "```py\nTotal number of chunks: 3\nResponse Time: 0.00 seconds \n```", "```py\n# Optionally, print the first three chunks for verification\nfor i, chunk in enumerate(chunks[:3], start=1):\n    print(chunk) \n```", "```py\n['ID,SCENARIO\\n',\n '100,Semantic analysis.This is not an analysis but a semantic search. Provide more information on the topic.\\n',\n '200,Sentiment analysis  Read the content return a sentiment analysis nalysis on this text and provide a score with the label named : Sentiment analysis score followed by a numerical value between 0 and 1  with no + or - sign and  add an explanation to justify the score.\\n',\n '300,Semantic analysis.This is not an analysis but a semantic search. Provide more information on the topic.\\n'] \n```", "```py\nimport openai\nimport time\nembedding_model=\"text-embedding-3-small\"\n#embedding_model=\"text-embedding-3-large\"\n#embedding_model=\"text-embedding-ada-002\" \n```", "```py\n# Initialize the OpenAI client\nclient = openai.OpenAI() \n```", "```py\ndef get_embedding(texts, model=\"text-embedding-3-small\") \n```", "```py\ntexts = [text.replace(\"\\n\", \" \") for text in texts] \n```", "```py\nresponse = client.embeddings.create(input=texts, model=model) \n```", "```py\nembeddings = [res.embedding for res in response.data]  # Extract embeddings \n```", "```py\nreturn embeddings \n```", "```py\ndef embed_chunks(\n    chunks, embedding_model=\"text-embedding-3-small\",\n    batch_size=1000, pause_time=3\n): \n```", "```py\n**start_time** = time.time()  # Start timing the operation\n**embeddings** = []  # Initialize an empty list to store the embeddings\n**counter** = 1  # Batch counter \n```", "```py\n# Process chunks in batches\n    for i in range(0, len(chunks), batch_size):\n        chunk_batch = chunks[i:i + batch_size]  # Select a batch of chunks \n```", "```py\n # Get the embeddings for the current batch\n        current_embeddings = get_embedding(\n            chunk_batch, model=embedding_model\n        ) \n```", "```py\n# Append the embeddings to the final list\n        embeddings.extend(current_embeddings) \n```", "```py\n # Print batch progress and pause\n        print(f\"Batch {counter} embedded.\")\n        counter += 1\n        time.sleep(pause_time)  # Optional: adjust or remove this depending on rate limits \n```", "```py\n # Print total response time\n    response_time = time.time() - start_time\n    print(f\"Total Response Time: {response_time:.2f} seconds\") \n```", "```py\nembeddings = embed_chunks(chunks) \n```", "```py\nBatch 1 embedded.\nTotal Response Time: 4.09 seconds \n```", "```py\nprint(\"First embedding:\", embeddings[0]) \n```", "```py\nFirst embedding: [0.017762450501322746, 0.041617266833782196, -0.024105189368128777,â€¦ \n```", "```py\n# Check the lengths of the chunks and embeddings\nnum_chunks = len(chunks)\nprint(f\"Number of chunks: {num_chunks}\")\nprint(f\"Number of embeddings: {len(embeddings)}\") \n```", "```py\nNumber of chunks: 3\nNumber of embeddings: 3 \n```", "```py\nfrom pinecone import Pinecone, ServerlessSpec \n```", "```py\n# Retrieve the API key from environment variables\napi_key = os.environ.get('PINECONE_API_KEY')\nif not api_key:\n    raise ValueError(\"PINECONE_API_KEY is not set in the environment!\")\n# Initialize the Pinecone client\npc = Pinecone(api_key=api_key) \n```", "```py\nfrom pinecone import ServerlessSpec\nindex_name = \"genai-v1\"\nnamespace=\"genaisys\" \n```", "```py\ncloud = os.environ.get('PINECONE_CLOUD') or 'aws'\nregion = os.environ.get('PINECONE_REGION') or 'us-east-1'\nspec = ServerlessSpec(cloud=cloud, region=region) \n```", "```py\nimport time\nimport pinecone\n# check if index already exists (it shouldn't if this is first time)\nif index_name not in pc.list_indexes().names(): \n```", "```py\n # if does not exist, create index\n    pc.create_index(\n        index_name,\n        dimension=1536,  # dimension of the embedding model\n        metric='cosine',\n        spec=spec\n    )\n    # wait for index to be initialized\n    time.sleep(1) \n```", "```py\n{'dimension': 1536,\n 'index_fullness': 0.0,\n 'namespaces': {},\n 'total_vector_count': 0} \n```", "```py\nIndex stats\n{'dimension': 1536,\n 'index_fullness': 0.0,\n 'namespaces': {'genaisys': {'vector_count': 3}},\n 'total_vector_count': 3} \n```", "```py\n# connect to index\nindex = pc.Index(index_name)\n# view index stats\nindex.describe_index_stats() \n```", "```py\n{'dimension': 1536,\n 'index_fullness': 0.0,\n 'namespaces': {'genaisys': {'vector_count': 0}},\n 'total_vector_count': 0} \n```", "```py\nimport pinecone\nimport time\nimport sys\nstart_time = time.time()  # Start timing before the request \n```", "```py\n# Function to calculate the size of a batch\ndef get_batch_size(data, limit=4000000):  # limit set to 4MB to be safe\n    total_size = 0\n    batch_size = 0\n    for item in data:\n        item_size = sum([sys.getsizeof(v) for v in item.values()])\n        if total_size + item_size > limit:\n            break\n        total_size += item_size\n        batch_size += 1\n    return batch_size \n```", "```py\n# Upsert function with namespace\ndef upsert_to_pinecone(batch, batch_size, namespace=\"genaisys\"):\n    \"\"\"\n    Upserts a batch of data to Pinecone under a specified namespace.\n    \"\"\"\n    try:\n        index.upsert(vectors=batch, namespace=namespace)\n        print(\n            f\"Upserted {batch_size} vectors to namespace '{namespace}'.\"\n        )\n    except Exception as e:\n        print(f\"Error during upsert: {e}\") \n```", "```py\ndef batch_upsert(data): \n```", "```py\n# Function to upsert data in batches\ndef batch_upsert(data):\n    total = len(data)\n    i = 0\n    while i < total:\n        batch_size = get_batch_size(data[i:])\n        batch = data[i:i + batch_size]\n        if batch:\n            upsert_to_pinecone(batch, batch_size, namespace=\"genaisys\")\n            i += batch_size\n            print(f\"Upserted {i}/{total} items...\")  # Display current progress\n        else:\n            break\n    print(\"Upsert complete.\") \n```", "```py\n# Generate IDs for each data item\nids = [str(i) for i in range(1, len(chunks) + 1)] \n```", "```py\n# Prepare data for upsert\ndata_for_upsert = [\n    {\"id\": str(id), \"values\": emb, \"metadata\": {\"text\": chunk}}\n    for id, (chunk, emb) in zip(ids, zip(chunks, embeddings))\n] \n```", "```py\n# Upsert data in batches\nbatch_upsert(data_for_upsert) \n```", "```py\nUpserted 3 vectors to namespace 'genaisys'.\nUpserted 3/3 items...\nUpsert complete.\nUpsertion response time: 0.45 seconds \n```", "```py\n#You might have to run this cell after a few seconds to give Pinecone\n#the time to update the index information\nprint(\"Index stats\")\nprint(index.describe_index_stats(include_metadata=True)) \n```", "```py\nIndex stats\n{'dimension': 1536,\n 'index_fullness': 0.0,\n 'namespaces': {'genaisys': {'vector_count': 3}},\n 'total_vector_count': 3} \n```", "```py\ndownload(\"Chapter03\",\"data01.txt\")\n# Load the CSV file\nfile_path = '/content/data01.txt' \n```", "```py\ntry:\n    with open(file_path, 'r') as file:\n        text = file.read()\n    text\nexcept FileNotFoundError:\n    text = \"Error: File not found. Please check the file path.\"\nprint(text) \n```", "```py\nThe CTO was explaing that a business-ready generative AI system (GenAISys) offers functionality similar to ChatGPT-like platformsâ€¦ \n```", "```py\n# Import libraries\nfrom openai import OpenAI\n# Initialize OpenAI Client\nclient = OpenAI()\n# Function to chunk text using GPT-4o\ndef chunk_text_with_gpt4o(text):\n    # Prepare the messages for GPT-4o\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are an assistant skilled at splitting long texts into meaningful, semantically coherent chunks of 50-100 words each.\"},\n        {\"role\": \"user\", \"content\": f\"Split the following text into meaningful chunks:\\n\\n{text}\"}\n    ]\nNow we send the request to the API:\n    # Make the GPT-4o API call\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",  # GPT-4o model\n        messages=messages,\n        temperature=0.2,  # Low randomness for consistent chunks\n        max_tokens=1024  # Sufficient tokens for the chunked response\n    ) \n```", "```py\n # Extract and clean the response\n    chunked_text = response.choices[0].message.content\n    chunks = chunked_text.split(\"\\n\\n\")  # Assume GPT-4o separates chunks with double newlines\n    return chunks \n```", "```py\n# Chunk the text\nchunks = chunk_text_with_gpt4o(text)\n# Display the chunks\nprint(\"Chunks:\")\nfor i, chunk in enumerate(chunks):\n    print(f\"\\nChunk {i+1}:\")\n    print(chunk) \n```", "```py\nChunks:\nChunk 1:\nThe CTO was explaining that â€¦\nChunk 2:\nGenAISys relies on a generative AI modelâ€¦\nChunk 3:\nWe defined memoryless, short-term, long-termâ€¦ \n```", "```py\ndef query_vector_store(query_text, namespace):\n    print(\"Querying vector store...\") \n```", "```py\n # Retrieve query results\n    query_results = get_query_results(query_text, namespace) \n```", "```py\n # Process and display the results\n    print(\"Processed query results:\")\n    text, target_id = display_results(query_results)\n    return text, target_id \n```", "```py\ndef display_results(query_results):\n    for match in query_results['matches']:\n        print(f\"ID: {match['id']}, Score: {match['score']}\")\n        if 'metadata' in match and 'text' in match['metadata']:\n            text=match['metadata']['text']\n            #print(f\"Text: {match['metadata']['text']}\")\n            target_id = query_results['matches'][0]['id']  # Get the ID from the first match\n            #print(f\"Target ID: {target_id}\")\n        else:\n            print(\"No metadata available.\")\n      return text, target_id \n```", "```py\ndef get_query_results(query_text, namespace):\n    # Generate the query vector from the query text\n    query_vector = get_embedding(query_text)  # Replace with your method to generate embeddings \n```", "```py\n # Perform the query\n    query_results = index.query(\n        vector=query_vector,\n        namespace=namespace,\n        top_k=1,  # Adjust as needed\n        include_metadata=True\n    ) \n```", "```py\n # Return the results\n    return query_results \n```", "```py\nimport openai\nclient = openai.OpenAI()\nembedding_model = \"text-embedding-3-small\"\ndef get_embedding(text, model=embedding_model):\n    text = text.replace(\"\\n\", \" \")\n    response = client.embeddings.create(input=[text], model=model)\n    embedding = response.data[0].embedding\n    return embedding \n```", "```py\n# Define your namespace and query text\nnamespace = \"genaisys\"  # Example namespace\nquery_text = \"The customers like the idea of travelling and learning. Provide your sentiment.\" \n```", "```py\n# Call the query function\ntext, target_id = query_vector_store(query_text, namespace)\n# Display the final output\nprint(\"Final output:\")\nprint(f\"Text: {text}\")\nprint(f\"Target ID: {target_id}\") \n```", "```py\nQuerying vector store...\nProcessed query results:\nID: 2, Score: 0.221010014\nQuerying response time: 0.54 seconds\nFinal output:\nText: 200,Sentiment analysis  Read the content return a sentiment analysis nalysis on this text and provide a score with the label named : Sentiment analysis score followed by a numerical value between 0 and 1  with no + or - sign and  add an explanation to justify the score.\nTarget ID: 2 \n```", "```py\n# Define your namespace and query text\nnamespace = \"data01\"  # Example namespace\nquery_text = \"What did the CTO say about the different types of memory?\"\nThe result is printed:\n# Display the final output\nprint(\"Final output:\")\nprint(f\"Text: {text}\")\nprint(f\"Target ID: {target_id}\") \n```", "```py\nQuerying vector store...\nProcessed query results:\nID: 3, Score: 0.571151137\nQuerying response time: 0.45 seconds\nFinal output:\nText: We defined memoryless, short-term, long-term memory, and cross-topic memory. For the hybrid travel marketing campaign, we will distinguish semantic memory (facts) from episodic memory (personal events in time, for example). The CTO said that we will need to use episodic memories of past customer trips to make the semantic aspects of our trips more engaging.\nTarget ID: 3 \n```"]