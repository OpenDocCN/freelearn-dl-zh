<html><head></head><body>
        

                            
                    <h1 class="header-title">Tidying your Text</h1>
                
            
            
                
<p>Data cleaning is one of the most important and time-consuming tasks when it comes to <strong>natural language processing</strong> (<strong>NLP</strong>):</p>
<p>"There's the joke that 80 percent of data science is cleaning the data and 20 percent is complaining about cleaning the data."</p>
<div><p>– Kaggle founder and CEO Anthony Goldbloom in a <a href="https://www.theverge.com/2017/11/1/16589246/machine-learning-data-science-dirty-data-kaggle-survey-2017">Verge Interview</a></p>
</div>
<p>In this chapter, we will discuss some of the most common text pre-processing ideas. This task is universal, tedious, and unavoidable. Most people working in data science or NLP understand that it's an underrated value addition. Some of these tasks don't work well in isolation but have a powerful effect when used in the right combination and order. This chapter will introduce several new words and tools, since the field has a rich history from two worlds. It borrows from both traditional NLP and machine learning. We'll meet spaCy, a fast industry-grade toolkit for natural language processing in Python. We will use it for tokenization, sentence extraction, and lemmatization.</p>
<p>We will learn to use regex functions, which are useful for text mining. Python's regex replaces can be slow for larger data sizes. Instead, we will use FlashText for substitution and expansion.  </p>
<p>This is the only book to cover FlashText. More broadly, we will share how you can start thinking about manipulating and cleaning text. This is not an in-depth coverage of any one technique, but a jump start for you to think about what might work for you.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Bread and butter – most common tasks</h1>
                
            
            
                
<p>There are several well-known text cleaning ideas. They have all made their way into the most popular tools today such as NLTK, Stanford CoreNLP, and spaCy. I like spaCy for two main reasons:</p>
<ul>
<li>It's an industry-grade NLP, unlike NLTK, which is mainly meant for teaching.</li>
<li>It has good speed-to-performance trade-off. spaCy is written in Cython, which gives it C-like performance with Python code.</li>
</ul>
<p>spaCy is actively maintained and developed, and incorporates the best methods available for most challenges.</p>
<p>By the end of this section, you will be able to do the following:</p>
<ul>
<li>Understand tokenization and do it manually yourself using spaCy</li>
<li>Understand why stop word removal and case standardization works, with spaCy examples</li>
<li>Differentiate between stemming and lemmatization, with spaCy lemmatization examples</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Loading the data</h1>
                
            
            
                
<p>I have always liked <em>The Adventures of Sherlock Holmes</em> by Sir Arthur Conan Doyle. Let's download the book and save it locally:</p>
<pre>url = 'http://www.gutenberg.org/ebooks/1661.txt.utf-8'<br/>file_name = 'sherlock.txt'</pre>
<p>Let's actually download the file. You only need to do this once, but this download utility can be used whenever you are downloading other datasets, too:</p>
<pre class="mce-root">import urllib.request<br/># Download the file from `url` and save it locally under `file_name`:<br/>with urllib.request.urlopen(url) as response:<br/>    with open(file_name, 'wb') as out_file:<br/>        data = response.read() # a `bytes` object<br/>        out_file.write(data)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Moving on, let's check whether we got the correct file in place with shell syntax inside our Jupyter notebook. This ability to run basic shell commands – on both Windows and Linux – is really useful:</p>
<pre>!ls *.txt</pre>
<p>The preceding command returns the following output:</p>
<pre>sherlock.txt</pre>
<p>The file contains header and footer information from Project Gutenberg. We are not interested in this, and will discard the copyright and other legal notices. This is what we want to do:</p>
<ol>
<li>Open the file.</li>
<li>Delete the header and footer information.</li>
<li>Save the new file as <kbd>sherlock_clean.txt</kbd>.</li>
</ol>
<p>I opened the text file and found that I need to remove the first 33 lines. Let's do that using shell commands – which also work on Windows inside Jupyter notebook. You remember this now, don't you? Marching on:</p>
<pre>!sed -i 1,33d sherlock.txt</pre>
<p>I used the <kbd>sed</kbd> syntax.  The <kbd>-i</kbd> flag tells you to make the necessary changes. <kbd>1,33d</kbd> instructs you to delete lines 1 to 33.</p>
<p>Let's double-check this. We expect the book to now begin with the iconic book title/cover:</p>
<pre>!head -5 sherlock.txt</pre>
<p>This shows the first five lines of the book. They are as we expect:</p>
<pre class="mce-root">THE ADVENTURES OF SHERLOCK HOLMES<br/><br/><br/>   by<br/><br/><br/>   SIR ARTHUR CONAN DOYLE</pre>
<p class="mce-root"/>
<p>What do I see?</p>
<p>Before I move on to text cleaning for any NLP task, I would like to spend a few seconds taking a quick glance at the data itself. I noted down some of the things I spotted in the following list. Of course, a keener eye will be able to see a lot more than I did:</p>
<ul>
<li>Dates are written in a mixed format: <em>twentieth of March, 1888</em>; times are too: <em>three o'clock</em>.</li>
<li>The text is wrapped at around 70 columns, so no line can be longer than 70 characters.</li>
<li>There are a lot of proper nouns. These include names such as <em>Atkinson</em> and <em>Trepoff</em>, in addition to locations such as <em>Trincomalee</em> and <em>Baker Street</em>.</li>
<li>The index is in Roman numerals such as <em>I</em> and <em>IV</em>, and not <em>1</em> and <em>4</em>.</li>
<li>There is a lot of dialogues such as <em>You have carte blanche,</em> with no narrative around them. This storytelling style switches freely from being narrative to dialogue-driven.</li>
<li>The grammar and vocabulary is slightly unusual because of the time when Doyle wrote.  </li>
</ul>
<p>These subjective observations are helpful in understanding the nature and edge cases in your text. Let's move on and load the book into Python for processing:</p>
<pre class="mce-root"># let's get this data into Python<br/><br/>text = open(file_name, 'r', encoding='utf-8').read() # note that I add an encoding='utf-8' parameter to preserve information<br/><br/>print(text[:5])</pre>
<p>This returns the first five characters:</p>
<pre>THE A</pre>
<p>Let's quickly verify that we have loaded the data into useful data types.</p>
<div><p>To check our own data types, use the following command:</p>
<pre>print(f'The file is loaded as datatype: {type(text)} and has {len(text)} characters in it')</pre>
<p>The preceding command returns the following output:</p>
<pre>The file is loaded as datatype: &lt;class 'str'&gt; and has 581204 characters in it</pre></div>
<p>There is a major improvement between Py2.7 and Py3.6 on how strings are handled. They are now all Unicode by default.</p>
<p>In Python 3, <kbd>str</kbd> are Unicode strings, and it is more convenient for the NLP of non-English texts.</p>
<p>Here is a small relevant example to highlight the differences between the two:</p>
<pre>from collections import Counter<br/>Counter('Möbelstück')<br/><br/>In Python 2: Counter({'\xc3': 2, 'b': 1, 'e': 1, 'c': 1, 'k': 1, 'M': 1, 'l': 1, 's': 1, 't': 1, '\xb6': 1, '\xbc': 1})<br/>In Python 3: Counter({'M': 1, 'ö': 1, 'b': 1, 'e': 1, 'l': 1, 's': 1, 't': 1, 'ü': 1, 'c': 1, 'k': 1})</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Exploring the loaded data</h1>
                
            
            
                
<p>How many unique characters can we see?</p>
<p>For reference, ASCII has 127 characters in it, so we expect this to have, at most, 127 characters:</p>
<pre>unique_chars = list(set(text))<br/>unique_chars.sort()<br/>print(unique_chars)<br/>print(f'There are {len(unique_chars)} unique characters, including both ASCII and Unicode character')</pre>
<p>The preceding code returns the following output:</p>
<pre>   ['\n', ' ', '!', '"', '$', '%', '&amp;', "'", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'â', 'è', 'é']<br/>   There are 85 unique characters, including both ASCII and Unicode character</pre>
<p>For our machine learning models, we often need the words to occur as individual tokens or single words. Let's explain what this means in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Tokenization</h1>
                
            
            
                
<p class="mce-root"/>
<p>Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation.<br/>
Here is an example of tokenization:</p>
<pre>Input: Friends, Romans, Countrymen, lend me your ears;<br/>Output: <img src="img/d3613e7a-8343-47b0-8444-8e06341128e8.png" style="width:3.42em;height:1.42em;"/> <img src="img/03cb260e-495f-4f7d-b3b2-c145fb8361be.png" style="width:3.42em;height:1.33em;"/> <img src="img/f0e848de-92b5-42a8-bd0f-11f9b35119db.png" style="width:5.00em;height:1.33em;"/> <img src="img/b1526f8a-d573-4ed1-ae37-3309dc5e76dc.png" style="width:2.33em;height:1.50em;"/> <img src="img/d4fb119b-f301-4c49-bb54-dddaf15de17d.png" style="width:1.67em;height:1.33em;"/> <img src="img/f58a30f4-0f22-4090-9cbe-389635ffa323.png" style="width:2.33em;height:1.42em;"/> <img src="img/dd75a099-7df7-4aee-b135-287da81f350d.png" style="width:2.17em;height:1.42em;"/>.</pre>
<p>It is, in fact, sometimes useful to distinguish between tokens and words. But here, for ease of understanding, we will use them interchangeably.</p>
<p>We will convert the raw text into a list of words. This should preserve the original ordering of the text.</p>
<p>There are several ways to do this, so let's try a few of them out. We will program two methods from scratch to build our intuition, and then check how spaCy handles tokenization.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Intuitive – split by whitespace</h1>
                
            
            
                
<p>The following lines of code simply segment or <em>split</em> the entire text body on space <kbd>' '</kbd>:</p>
<pre class="mce-root">words = text.split()<br/>print(len(words))<br/><br/>   107431</pre>
<p>Let's preview a rather large segment from our list of tokens:</p>
<pre>print(words[90:200])  #start with the first chapter, ignoring the index for now<br/>   ['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', 'THE', 'woman.', 'I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name.', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole', 'of', 'her', 'sex.', 'It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', 'akin', 'to', 'love', 'for', 'Irene', 'Adler.', 'All', 'emotions,', 'and', 'that', 'one', 'particularly,', 'were', 'abhorrent', 'to', 'his', 'cold,', 'precise', 'but', 'admirably', 'balanced', 'mind.', 'He', 'was,', 'I', 'take', 'it,', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', 'the', 'world', 'has', 'seen,', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', 'false', 'position.', 'He', 'never', 'spoke', 'of', 'the', 'softer', 'passions,', 'save', 'with', 'a', 'gibe', 'and', 'a', 'sneer.', 'They', 'were', 'admirable', 'things', 'for']</pre>
<p>The way punctuation is split here is not desirable. It often appears with the word itself, such as the full stop at end of <kbd>Adler<strong>.</strong></kbd> and a comma being part of <kbd>emotions<strong>,</strong></kbd>. Quite often we want words to be separated from punctuation, because words convey a lot more meaning than punctuation in most datasets.</p>
<p>Let's look at a shorter example:</p>
<pre>'red-headed woman on the street'.split()</pre>
<p>The following is the output from the preceding code:</p>
<pre>['red-headed', 'woman', 'on', 'the', 'street']</pre>
<p>Note how the words <em>red-headed</em> were not split. This is something we may or may not want to keep. We will come back to this, so keep this in mind.</p>
<p>One way to tackle this punctuation challenge is to simply extract words and discard everything else. This means that we will discard all non-ASCII characters and punctuation.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The hack – splitting by word extraction</h1>
                
            
            
                
<p>Word extraction can be done in several ways. In turn, we can use word extraction for splitting the words into tokens. We will look at Regex, or Regular Expressions for doing word extractions. It is a pattern driven string search mechanism where the pattern grammar is defined by the user.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Introducing Regexes</h1>
                
            
            
                
<p>Regular expressions can be a little challenging at first, but they are very powerful. They are generic abstractions, and work across multiple languages beyond Python:</p>
<pre>import re<br/>re.split('\W+', 'Words, words, words.')<br/>&gt; ['Words', 'words', 'words', '']</pre>
<p class="mce-root"/>
<p>The regular expression <kbd>\W+</kbd> means <em>a word character (A-Z</em> etc.<em>) repeated one or more times:</em></p>
<pre class="mce-root">words_alphanumeric = re.split('\W+', text)<br/>print(len(words_alphanumeric), len(words))</pre>
<p>The output of the preceding code is <kbd>(109111, 107431)</kbd>.</p>
<p>Let’s preview the words we extracted:</p>
<pre>print(words_alphanumeric[90:200])</pre>
<p>The following is the output we got from the preceding code:</p>
<pre>   ['BOHEMIA', 'I', 'To', 'Sherlock', 'Holmes', 'she', 'is', 'always', 'THE', 'woman', 'I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole', 'of', 'her', 'sex', 'It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', 'akin', 'to', 'love', 'for', 'Irene', 'Adler', 'All', 'emotions', 'and', 'that', 'one', 'particularly', 'were', 'abhorrent', 'to', 'his', 'cold', 'precise', 'but', 'admirably', 'balanced', 'mind', 'He', 'was', 'I', 'take', 'it', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', 'the', 'world', 'has', 'seen', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', 'false', 'position', 'He', 'never', 'spoke', 'of', 'the', 'softer', 'passions', 'save', 'with', 'a', 'gibe', 'and', 'a', 'sneer', 'They', 'were', 'admirable']</pre>
<p>We notice how <kbd>Adler</kbd> no longer has the punctuation mark alongside it. This is what we wanted. Mission accomplished? </p>
<p>What was the trade-off we made here? To understand that, let's look at another example:</p>
<pre>words_break = re.split('\W+', "Isn't he coming home for dinner with the red-headed girl?")<br/>print(words_break)</pre>
<p>The following is the output we got from the preceding code:</p>
<pre> ['Isn', 't', 'he', 'coming', 'home', 'for', 'dinner', 'with', 'the', 'red', 'headed', 'girl', '']</pre>
<p class="mce-root"/>
<p>We have split <kbd>Isn't</kbd> to <kbd>Isn</kbd> and <kbd>t</kbd>. This isn't good if you're working with, say, email or Twitter data, because you would have a lot more of these contractions and abbreviations. As a minor annoyance, we have an extra empty token, <kbd>''</kbd>, at the end. Similarly, because we neglected punctuation, <kbd>red-headed</kbd> is broken into two words: <kbd>red</kbd> and <kbd>headed</kbd>. We have no straightforward way to restore this connection if we are only given the tokenized version.</p>
<p>We can write custom rules in our tokenization strategy to cover most of these edge cases. Or, we can use something that has already been written for us.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">spaCy for tokenization</h1>
                
            
            
                
<p>spaCy loads the English <em>model</em> using the preceding <kbd>.load</kbd> syntax. This tells spaCy what rules, logic, weights, and other information to use:</p>
<pre> %%time<br/> import spacy<br/> # python -m spacy download en<br/> # uncomment above line to download the model<br/> nlp = spacy.load('en')</pre>
<p>While we use only <kbd>'en'</kbd> or English examples in this book, spaCy supports these features for more languages. I have used their multi-language tokenizer for Hindi as well, and have been satisfied with the same:</p>
<p>The <kbd>%%time</kbd> syntax measures the CPU and Wall time at your runtime execution for the cell in a Jupyter not  ebook.</p>
<pre>doc = nlp(text)</pre>
<p>This creates a spaCy object, <kbd>doc</kbd>. The object stores pre-computed linguistic features, including tokens. Some NLP libraries, especially in the Java and C ecosystem, compute linguistic features such as tokens, lemmas, and parts of speech when that specific function is called. Instead, spaCy computes them all at initialization when the <kbd>text</kbd> is passed to it.</p>
<p>spaCy pre-computes most linguistic features – all you have to do is retrieve them from the object.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We can retrieve them by calling the object iterator. In the following code, we call the iterator and <em>list</em> it:</p>
<pre>print(list(doc)[150:200])</pre>
<p>The following is the output from the preceding code:</p>
<pre>[whole, of, her, sex, ., It, was, not, that, he, felt,<br/>   , any, emotion, akin, to, love, for, Irene, Adler, ., All, emotions, ,, and, that,<br/>   , one, particularly, ,, were, abhorrent, to, his, cold, ,, precise, but,<br/>   , admirably, balanced, mind, ., He, was, ,, I, take, it, ,]</pre>
<p>Conveniently, spaCy tokenizes all punctuation and words. They are returned as individual tokens. Let's try the example that we didn't like earlier:</p>
<pre>words = nlp("Isn't he coming home for dinner with the red-headed girl?")<br/>print([token for token in words])<br/>&gt; [Is, n't, he, coming, home, for, dinner, with, the, red, -, headed, girl, ?]</pre>
<p>Here are the observations:</p>
<ul>
<li>spaCy got the <kbd>Isn't</kbd> split correct: <kbd>Is</kbd> and <kbd>n't</kbd>.</li>
<li><kbd>red-headed</kbd> was broken into three tokens: <kbd>red</kbd>, <kbd>-</kbd>, and <kbd>headed</kbd>. Since the punctuation information isn't lost, we can restore the original <kbd>red-headed</kbd> token if we want to.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">How does the spaCy tokenizer work?</h1>
                
            
            
                
<p>The simplest explanation is from the spaCy docs (<a href="https://spacy.io/usage/spacy-101">spacy-101</a>) itself.</p>
<p>First, the raw text is split on whitespace characters, similar to text.split (<kbd>' '</kbd>). Then, the tokenizer processes the text from left to right. On each substring, it performs two checks:</p>
<ul>
<li><em>Does the substring match a tokenizer exception rule?</em> For example, <em>don't</em> does not contain whitespace, but should be split into two tokens, <em>do</em> and <em>n't</em>, while <em>U.K.</em> should always remain one token.</li>
<li><em>Can a prefix, suffix, or infix be split off?</em> For example, punctuation such as commas, periods, hyphens, or quotes:</li>
</ul>
<div><img class="alignnone size-full wp-image-377 image-border" src="img/3c84ff70-690a-4ebb-b74b-29065083cf1d.png" style=""/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Sentence tokenization</h1>
                
            
            
                
<p>We can also use spaCy to extract one sentence at a time, instead of one word at a time:</p>
<pre>sentences = list(doc.sents)<br/>print(sentences[14:18])</pre>
<p>The following is the output from the preceding code:</p>
<pre> [she is always THE woman., I have seldom heard<br/>   him mention her under any other name., In his eyes she eclipse<br/>   and predominates the whole of her sex., It was not that he felt<br/>   any emotion akin to love for Irene Adler.]</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Stop words removal and case change</h1>
                
            
            
                
<p>These simple ideas are widespread and fairly effective for a lot of tasks. They are particularly useful in reducing the number of unique tokens in a document for your processing.  </p>
<p class="mce-root"/>
<p>spaCy has already marked each token as a stop word or not and stored it in the <kbd>is_stop</kbd> attribute of each token. This makes it very handy for text cleaning. Let's take a quick look:</p>
<pre>sentence_example = "the AI/AGI uprising cannot happen without the progress of NLP"<br/>[(token, token.is_stop, token.is_punct) for token in nlp(sentence_example)]<br/><br/>   [(the, True, False),<br/>    (AI, False, False),<br/>    (/, False, True),<br/>    (AGI, True, False),<br/>    (uprising, False, False),<br/>    (can, True, False),<br/>    (not, True, False),<br/>    (happen, False, False),<br/>    (without, True, False),<br/>    (the, True, False),<br/>    (progress, False, False),<br/>    (of, True, False),<br/>    (NLP, True, False)]</pre>
<p>Getting back to our Sherlock example, let’s take a look at the first few lines and whether they count as stop words or not:</p>
<pre>for token in doc[:5]:<br/>   print(token, token.is_stop, token.is_punct)   <br/><br/>Output:<br/>   THE False False<br/>   ADVENTURES False False<br/>   OF False False<br/>   SHERLOCK False False<br/>   HOLMES False False</pre>
<p>Interesting – while <em>the</em> and <em>of</em> were marked as stop words, <kbd>THE</kbd> and <kbd>OF</kbd> were not. This is not a bug, but by design. spaCy doesn't remove words that are different because of their capitals or title case automatically.</p>
<p>Instead, we can force this behavior by converting our original text to lowercase before we pass it to spaCy:</p>
<pre>text_lower = text.lower()  # native python function<br/>doc_lower = nlp(text_lower)<br/>for token in doc_lower[:5]:<br/>   print(token, token.is_stop)<br/><br/>Output:<br/> the True<br/> adventures False<br/> of True<br/> sherlock False<br/> holmes False</pre>
<p>Let's look at what stop words exist in the spaCy dictionary, and then how to extend the same programmatically: </p>
<pre>from spacy.lang.en.stop_words import STOP_WORDS<br/>f'spaCy has a dictionary of {len(list(STOP_WORDS))} stop words'<br/><br/>   'spaCy has a dictionary of 305 stop words'</pre>
<p>We want to expand the stop words dictionary according to our domain and problem. For instance, if you were using this code to process the text of an NLP book, we might want to add words such as <kbd>NLP</kbd>, <em>Processing</em>, <kbd>AGI</kbd>, <em>Data,</em> and so on to the stop words list. </p>
<p>spaCy has an intuitive <kbd>.add()</kbd> API to do this:</p>
<pre>domain_stop_words = ["NLP", "Processing", "AGI"]<br/>for word in domain_stop_words:<br/>   STOP_WORDS.add(word)</pre>
<p>Let's try running the same example as earlier with these added stop words:</p>
<pre>[(token, token.is_stop, token.is_punct) for token in nlp(sentence_example)]</pre>
<p>The following is the output from running the preceding code:</p>
<pre>    [(the, True, False),<br/>    (AI, False, False),<br/>    (/, False, True),<br/>    (AGI, True, False),<br/>    (uprising, False, False),<br/>    (can, True, False),<br/>    (not, True, False),<br/>    (happen, False, False),<br/>    (without, True, False),<br/>    (the, True, False),<br/>    (progress, False, False),<br/>    (of, True, False),<br/>    (NLP, True, False)]</pre>
<p>Exactly as expected, <kbd>NLP</kbd> and <kbd>AGI</kbd> are now marked as stop words too.</p>
<p>Let's pull out string tokens which are not stop words into a Python list or similar data structure.</p>
<p class="mce-root"/>
<p>Some NLP tasks that come after text pre-processing expect string tokens and not spaCy token objects as a datatype. Removing both stop words and punctuation here for demonstration:</p>
<pre>[str(token) for token in nlp(sentence_example) if not token.is_stop and not token.is_punct]<br/> ['AI', 'uprising', 'happen', 'progress']</pre>
<p>Or just removing stop words, while retaining punctuation:</p>
<pre>[str(token) for token in nlp(sentence_example) if not token.is_stop]<br/>['AI'], '/', 'uprising', 'happen', 'progress']</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Stemming and lemmatization</h1>
                
            
            
                
<p>Stemming and lemmatization are very two very popular ideas that are used to reduce the vocabulary size of your corpus.</p>
<div><strong>Stemming</strong> usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.<br/>
<br/>
<strong>Lemmatization</strong> usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.<br/>
<br/>
If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw, depending on whether the use of the token was as a verb or a noun.</div>
<p>- Dr. Christopher Manning et al, 2008, [<a href="https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html">IR-Book</a>]<br/>
(Chris Manning is a Professor in machine learning at the Departments of Computer Science and Linguistics at Stanford University)</p>


            

            
        
    

        

                            
                    <h1 class="header-title">spaCy for lemmatization</h1>
                
            
            
                
<p>spaCy only supports lemmatization. As discussed by spaCy creator Matt Honnibal in <a href="https://github.com/explosion/spaCy/issues/327">issue #327</a> on GitHub, stemmers are rarely a good idea.</p>
<p class="mce-root"/>
<p>We want to treat <kbd>meet/NOUN</kbd> differently from <kbd>meeting/VERB</kbd>. Unlike Stanford NLTK, which was created to <em>teach and introduce </em>as many NLP ideas as possible, spaCy takes an opinionated stand against stemming.</p>
<p>spaCy does lemmatization for you by default when you process the text with the <kbd>nlp</kbd> object. This information is stored in the <kbd>lemma</kbd> attribute for each token. spaCy stores the internal hash or identifier, which spaCy stores in <kbd>token.lemma</kbd>. This numerical hash has no meaning for us. This numerical representation helps spaCy access and manipulate information much faster than its other Pythonic components. </p>
<p>An underscore at the attribute end, such as <kbd>lemma_</kbd>, tells spaCy that we are looking for something that is human-readable:</p>
<pre>lemma_sentence_example = "Their Apples &amp; Banana fruit salads are amazing. Would you like meeting me at the cafe?"<br/>[(token, token.lemma_, token.lemma, token.pos_ ) for token in nlp(lemma_sentence_example)]<br/><br/>Printing this gives the following output: <br/><br/>   [(Their, '-PRON-', 561228191312463089, 'ADJ'),<br/>    (Apples, 'apples', 14374618037326464786, 'PROPN'),<br/>    (&amp;, '&amp;', 15473034735919704609, 'CCONJ'),<br/>    (Banana, 'banana', 2525716904149915114, 'PROPN'),<br/>    (fruit, 'fruit', 17674554054627885835, 'NOUN'),<br/>    (salads, 'salad', 16382906660984395826, 'NOUN'),<br/>    (are, 'be', 10382539506755952630, 'VERB'),<br/>    (amazing, 'amazing', 12968186374132960503, 'ADJ'),<br/>    (., '.', 12646065887601541794, 'PUNCT'),<br/>    (Would, 'would', 6992604926141104606, 'VERB'),<br/>    (you, '-PRON-', 561228191312463089, 'PRON'),<br/>    (like, 'like', 18194338103975822726, 'VERB'),<br/>    (meeting, 'meet', 6880656908171229526, 'VERB'),<br/>    (me, '-PRON-', 561228191312463089, 'PRON'),<br/>    (at, 'at', 11667289587015813222, 'ADP'),<br/>    (the, 'the', 7425985699627899538, 'DET'),<br/>    (cafe, 'cafe', 10569699879655997926, 'NOUN'),<br/>    (?, '?', 8205403955989537350, 'PUNCT')]</pre>
<p>There's quite a few things going on here. Let's discuss them.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">-PRON-</h1>
                
            
            
                
<p>spaCy has a slightly annoying lemma (recall that lemma is the output of lemmatization): -PRON-. This is used as the lemma for all pronouns such as <kbd>Their</kbd>, <kbd>you</kbd>, <kbd>me</kbd>, and <kbd>I</kbd>.  Other NLP tools lemmatize these to <kbd>I</kbd> instead of a placeholder, such as <kbd>-PRON-</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Case-insensitive</h1>
                
            
            
                
<p>While checking for stop words, spaCy did not automatically lowercase our input. On the other hand, lemmatization does this for us. It converted "Apple" to "apple" and "Banana" to "banana".</p>
<p>This is one of the ways spaCy makes our lives easier, though slightly inconsistent. While removing stop words, we want to preserve THE in "THE ADVENTURES OF SHERLOCK HOLMES" while removing <em>the </em>in "the street was black". The opposite is usually true in lemmatization; we care more about how the word was used in context and use a proper lemma accordingly. </p>


            

            
        
    

        

                            
                    <h1 class="header-title">Conversion – meeting  to meet</h1>
                
            
            
                
<p>Lemmatization is aware of the linguistic role that words play in context. "Meeting" is converted to "meet" because it's a verb. spaCy does expose part of speech tagging and other linguistic features for us to use. We will learn how to query those soon.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">spaCy compared with NLTK and CoreNLP</h1>
                
            
            
                
<p>The following is a comparison of the NLTK and CoreNLP:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><strong>Feature</strong></td>
<td><strong>Spacy</strong></td>
<td><strong>NLTK</strong></td>
<td><strong>CoreNLP</strong></td>
</tr>
<tr>
<td>Native Python support/API</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
</tr>
<tr>
<td>Multi-language support</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
</tr>
<tr>
<td>Tokenization</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
</tr>
<tr>
<td>Part-of-speech tagging</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
</tr>
<tr>
<td>Sentence segmentation</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
</tr>
<tr>
<td>Dependency parsing</td>
<td>Y</td>
<td>N</td>
<td>Y</td>
</tr>
<tr>
<td>Entity recognition</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
</tr>
<tr>
<td>Integrated word vectors</td>
<td>Y</td>
<td>N</td>
<td>N</td>
</tr>
<tr>
<td>Sentiment analysis</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
</tr>
<tr>
<td>Coreference resolution</td>
<td>N</td>
<td>N</td>
<td>Y</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Correcting spelling</h1>
                
            
            
                
<p>One of the most frequently seen text challenges is correcting spelling errors. This is all the more true when data is entered by casual human users, for instance, shipping addresses or similar.</p>
<p>Let's look at an example. We want to correct Gujrat, Gujart, and other minor misspellings to Gujarat. There are several good ways to do this, depending on your dataset and level of expertise. We will discuss two or three popular ways, and discuss their pros and cons.</p>
<p>Before I begin, we need to pay homage to the legendary <a href="https://norvig.com/spell-correct.html" target="_blank">Peter Norvig's Spell Correct</a>. It's still worth a read on how to <em>think</em> about solving a problem and <em>exploring</em> implementations. Even the way he refactors his code and writes functions is educational.</p>
<p>His spell-correction module is not the simplest or best way of doing this. I recommend two packages: one with a bias toward simplicity, one with a bias toward giving you all the knives, bells, and whistles to try:</p>
<ul>
<li><strong><a href="https://github.com/seatgeek/fuzzywuzzy" target="_blank">FuzzyWuzzy</a></strong> is easy to use. It gives a simple similarity score between two strings, capped to 100. Higher numbers mean that the words are more similar.</li>
<li><strong><a href="https://github.com/jamesturk/jellyfish" target="_blank">Jellyfish</a></strong> supports six edit distance functions and four phonetic encoding options that you can use as per your use case.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">FuzzyWuzzy</h1>
                
            
            
                
<p>Let's see how we can use FuzzyWuzzy to correct our misspellings.</p>
<p class="mce-root">Use the following code to install FuzzyWuzzy on your machine:</p>
<pre class="mce-root">import sys<br/><br/>!{sys.executable} -m pip install fuzzywuzzy<br/># alternative for 4-10x faster computation: <br/><br/># !{sys.executable} -m pip install fuzzywuzzy[speedup]</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">FuzzyWuzzy has two main modules that will come in useful: fuzz and process. Let's import fuzz first:</p>
<pre class="mce-root">from fuzzywuzzy import fuzz<br/># Trying the ratio and partial_ratio <br/>fuzz.ratio("Electronic City Phase One", "Electronic City Phase One, Bangalore")<br/># 82<br/>fuzz.partial_ratio("Electronic City Phase One", "Electronic City Phase One, Bangalore")<br/># 100</pre>
<p class="mce-root">We can see how the ratio function is confused by the trailing <kbd>Bangalore</kbd> used in the preceding address, but really the two strings refer to the same address/entity. This is captured by <kbd>partial_ratio</kbd>.</p>
<p class="mce-root">Do you see how both <kbd>ratio</kbd> and <kbd>partial_ratio</kbd> are sensitive to the ordering of the words? This is useful for comparing addresses that follow some order. On the other hand, if we want to compare something else, for example, person names, it might give counter-intuitive results:</p>
<pre class="mce-root">fuzz.ratio('Narendra Modi', 'Narendra D. Modi')<br/># 90<br/>fuzz.partial_ratio('Narendra Modi', 'Narendra D. Modi')<br/># 77</pre>
<p class="mce-root">As you can see, just because we had an extra <kbd>D.</kbd> token, our logic is not applicable anymore. We want something that is less order-sensitive. The authors of FuzzyWuzzy have us covered.</p>
<p class="mce-root">FuzzyWuzzy supports functions that tokenize our input on space and remove punctuation, numbers, and non-ASCII characters. This is then used to calculate similarity. Let's try this out:</p>
<pre class="mce-root">fuzz.token_sort_ratio('Narendra Modi', 'Narendra D. Modi')<br/># 93<br/>fuzz.token_set_ratio('Narendra Modi', 'Narendra D. Modi')<br/># 100</pre>
<p class="mce-root">This will work perfectly for us. In case we have a list of options and we want to find the closest match(es), we can use the process module:</p>
<pre class="mce-root">from fuzzywuzzy import process<br/>query = 'Gujrat'<br/><br/>choices = ['Gujarat', 'Gujjar', 'Gujarat Govt.']<br/><br/># Get a list of matches ordered by score, default limit to 5<br/>print(process.extract(query, choices))<br/># [('Gujarat', 92), ('Gujarat Govt.', 75), ('Gujjar', 67)]<br/><br/># If we want only the top one result to be # returned:<br/>process.extractOne(query, choices)<br/># ('Gujarat', 92)</pre>
<p class="mce-root">Let's look at another example. Here, we have <kbd>Bangalore</kbd> misspelled as <kbd>Banglore</kbd> – we are missing an <kbd>a</kbd>:</p>
<pre class="mce-root">query = 'Banglore'<br/>choices = ['Bangalore', 'Bengaluru']<br/>print(process.extract(query, choices))<br/># [('Bangalore', 94), ('Bengaluru', 59)]<br/>process.extractOne(query, choices)<br/># ('Bangalore', 94)</pre>
<p class="mce-root">Let's take an example of a common search typo in online shopping. Users have misspelled <kbd>chilli</kbd> as <kbd>chili</kbd>; note the missing <kbd>l</kbd>:</p>
<pre class="mce-root">query = 'chili'<br/>choices = ['chilli', 'chilled', 'chilling']<br/>print(process.extract(query, choices))<br/># [('chilli', 91), ('chilling', 77), ('chilled', 67)]<br/>process.extractOne(query, choices)<br/># ('chilli', 91)</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Jellyfish</h1>
                
            
            
                
<p>Jellyfish supports reasonably fast implementations of almost all popular edit distance functions (Recall how the edit distance functions tell you how similar two sequences/strings are). While FuzzyWuzzy supported mainly Levenshtein distance, this package supports some more string comparison utilities:</p>
<ul>
<li>Levenshtein distance</li>
<li>Damerau-Levenshtein distance</li>
<li>Jaro distance</li>
<li>Jaro-Winkler distance</li>
<li>Match rating approach comparison</li>
<li>Hamming distance</li>
</ul>
<p class="text_cell_render border-box-sizing rendered_html">Additionally, it supports <strong>phonetic encodings</strong> for English.</p>
<p class="mce-root"/>
<p>Use the following code to install Jellyfish on your machine:</p>
<pre>import sys<br/># !{sys.executable} -m pip install jellyfish</pre>
<div><div><p class="prompt input_prompt">Let's try importing the package and setting up some examples to try out:</p>
<div><div><div><pre>import jellyfish<br/><br/>correct_example = ('Narendra Modi', 'Narendra Modi')<br/>damodardas_example = ('Narendra Modi', 'Narendra D. Modi')<br/>modi_typo_example = ('Narendra Modi', 'Narendar Modi')<br/>gujarat_typo_example = ('Gujarat', 'Gujrat')<br/><br/>examples = [correct_example, damodardas_example, modi_typo_example, gujarat_typo_example]</pre></div>
</div>
</div>
</div>
</div>
<div><div><p class="prompt input_prompt">We want to try multiple distance functions with all of our examples. The smarter thing to do is build a utility function for this. Let's do that now: </p>
<div><div><div><pre>def calculate_distance(function, examples=examples):<br/>    for ele in examples:<br/>        print(f'{ele}: {function(*ele)}')</pre></div>
</div>
</div>
</div>
</div>
<div><div><p class="prompt input_prompt">Note that <kbd>calculate_distance</kbd> takes the distance function as input. We can leave <kbd>examples</kbd> as implicitly picked from what we had declared previously in the global namespace. </p>
<p>Levenshtein distance, which is probably the most famous string similarity function, is sometimes synonymous with edit distance function, but we consider this to be a particular implementation of the edit distance family of functions:</p>
<div><div><div><pre>calculate_distance(jellyfish.levenshtein_distance) <br/># ('Narendra Modi', 'Narendra Modi'): 0<br/># ('Narendra Modi', 'Narendra D. Modi'): 3<br/># ('Narendra Modi', 'Narendar Modi'): 2<br/># ('Gujarat', 'Gujrat'): 1</pre></div>
</div>
</div>
</div>
</div>
<div><div><p class="prompt input_prompt">The Damerau–Levenshtein distance adds transpositions to the Levenshtein edit operations of insertion, deletion, and substitution. Let's try this out and see if it changes anything for us: </p>
<div><div><div><pre>calculate_distance(jellyfish.damerau_levenshtein_distance)<br/># ('Narendra Modi', 'Narendra Modi'): 0<br/># ('Narendra Modi', 'Narendra D. Modi'): 3<br/># ('Narendra Modi', 'Narendar Modi'): 1<br/># ('Gujarat', 'Gujrat'): 1</pre></div>
</div>
</div>
</div>
</div>
<div><div><p class="prompt input_prompt">We note that the <kbd>Narendra</kbd> and <kbd>Narendar</kbd> distance value changed from <kbd>3</kbd> to <kbd>2</kbd>. This is because we now count at least <kbd>a</kbd> to be transposed with <kbd>r</kbd> or vice versa. The other character is a substitution, so 1+1 = 2. </p>
</div>
</div>
<div><p>The next distance function that we will try is hamming distance. This counts the minimum number of substitutions required to change one string into the other:</p>
</div>
<div><div><div><div><div><pre>calculate_distance(jellyfish.hamming_distance)<br/># ('Narendra Modi', 'Narendra Modi'): 0<br/># ('Narendra Modi', 'Narendra D. Modi'): 7<br/># ('Narendra Modi', 'Narendar Modi'): 2<br/># ('Gujarat', 'Gujrat'): 4</pre></div>
</div>
</div>
</div>
</div>
<div><p class="prompt input_prompt"><strong>Jaro and Jaro-Winkler</strong> return a value of similarity – and not dissimilarity. This means that the perfect match returns 1.0 and a totally unrelated match would tend to be 0:</p>
</div>
<div><div><div><div><div><pre>calculate_distance(jellyfish.jaro_distance)<br/># ('Narendra Modi', 'Narendra Modi'): 1.0<br/># ('Narendra Modi', 'Narendra D. Modi'): 0.9375<br/># ('Narendra Modi', 'Narendar Modi'): 0.9743589743589745<br/># ('Gujarat', 'Gujrat'): 0.8968253968253969</pre></div>
</div>
</div>
</div>
</div>
<div><div><p class="prompt input_prompt">Trying the other variation of Jaro similarity, that is, Jaro-Winkler, we get the following:</p>
<div><div><div><pre>calculate_distance(jellyfish.jaro_winkler)<br/># ('Narendra Modi', 'Narendra Modi'): 1.0<br/># ('Narendra Modi', 'Narendra D. Modi'): 0.9625<br/># ('Narendra Modi', 'Narendar Modi'): 0.9846153846153847<br/># ('Gujarat', 'Gujrat'): 0.9277777777777778</pre></div>
</div>
</div>
</div>
</div>
<p>These are extremely useful and diverse techniques. Yet, their overemphasis on written text creates one problem that is unique to English. We don't write English in the same way we speak. This means that we do not capture the range of all similarities. To solve this challenge, which is typically encountered in chatbots used by non-native English speakers, we can look at the phonetic similarity of words, which is what we will do next.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Phonetic word similarity</h1>
                
            
            
                
<p>The way we say a word makes up its phonetics. Phonetics is the information of speech sounds. For instance, soul and sole sound identical in a lot of British-derived accents, such as Indian accents.</p>
<p>Quite often, words might be misspelled a little bit because the typist was trying to make it <em>sound right</em>. In this case, we leverage this phonetic information to map this typo back to the correct spelling.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">What is a phonetic encoding?</h1>
                
            
            
                
<p>We can convert a word into a representation of its pronunciation. Of course, this might vary by accents, and by the conversion technique as well.</p>
<p>Yet, over time, two or three popular ways have emerged so that we can do this. Each of these methods takes a single string and returns a coded representation. I encourage you to Google each of these terms:</p>
<ul>
<li><strong>American Soundex (the 1930s)</strong>: Implemented in popular database software such as PostgreSQL, MySQL, and SQLite</li>
<li><strong>NYSIIS (New York State Identification and Intelligence System) (the 1970s)</strong></li>
<li>Metaphone (the 1990s)</li>
<li><strong>Match rating codex (the early 2000s)</strong></li>
</ul>
<p>Let's take a quick preview of the same:</p>
<div><div><div><div><div><pre>jellyfish.soundex('Jellyfish')<br/># 'J412'</pre></div>
</div>
</div>
</div>
</div>
<div><div><p class="prompt input_prompt">For NYSIIS, we will use the following: </p>
<div><div><div><pre>jellyfish.nysiis('Jellyfish')<br/># 'JALYF'</pre></div>
</div>
</div>
</div>
</div>
<div><div><p class="prompt input_prompt">Using the slightly more updated metaphone, we get the following output: </p>
<div><div><div><pre>jellyfish.metaphone('Jellyfish')<br/># 'JLFX'</pre></div>
</div>
</div>
</div>
</div>
<div><div><p class="prompt input_prompt">The matching rate codex gives us the following output:</p>
<div><div><div><pre>jellyfish.match_rating_codex('Jellyfish')<br/># 'JLYFSH'</pre></div>
</div>
</div>
</div>
</div>
<div><p class="prompt input_prompt">We can now use the string comparison utility that we saw earlier to compare two strings phonetically.</p>
<div><div><pre><strong>Metaphone + Levenshtein</strong></pre>
<p>For instance, <kbd>write</kbd> and <kbd>right</kbd><q> </q>should have zero phonetic Levenshtein distance because they are pronounced in the same way. Let's try this out:</p>
</div>
</div>
</div>
<div><div><div><div><div><pre>jellyfish.levenshtein_distance(jellyfish.metaphone('write'), jellyfish.metaphone('right'))# <br/># 0</pre></div>
</div>
</div>
</div>
</div>
<div><p class="prompt input_prompt">This worked as expected. Let's add some examples to our old examples list:</p>
</div>
<div><div><div><div><div><pre>examples+= [('write', 'right'), ('Mangalore', 'Bangalore'), ('Delhi', 'Dilli')] # adding a few examples to show how cool this is</pre></div>
</div>
</div>
</div>
</div>
<div><div><p class="prompt input_prompt">Let's encapsulate this into a utility function, like we did earlier. We will use two function parameters now: <kbd>phonetic_func</kbd> and <kbd>distance_func</kbd>:</p>
<div><div><div><pre>def calculate_phonetic_distance(phonetic_func, distance_func, examples=examples):
    print("Word\t\tSound\t\tWord\t\t\tSound\t\tPhonetic Distance")
    for ele in examples:
        correct, typo = ele[0], ele[1]
        phonetic_correct, phonetic_typo = phonetic_func(correct), phonetic_func(typo)
        phonetic_distance = distance_func(phonetic_correct, phonetic_typo)
        print(f'{correct:&lt;10}\t{phonetic_correct:&lt;10}\t{typo:&lt;20}\t{phonetic_typo:&lt;10}\t{phonetic_distance:&lt;10}') 
        
calculate_phonetic_distance(phonetic_func=jellyfish.metaphone, distance_func=jellyfish.levenshtein_distance)        </pre></div>
</div>
</div>
</div>
<div><div><div><p class="prompt">This returns the following table: </p>
<div><pre>Word               Sound           Word                    Sound           Phonetic Distance
Narendra Modi   NRNTR MT        Narendra Modi           NRNTR MT        0         
Narendra Modi   NRNTR MT        Narendra D. Modi        NRNTR T MT      2         
Narendra Modi   NRNTR MT        Narendar Modi           NRNTR MT        0         
Gujarat         KJRT            Gujrat                  KJRT            0         
write           RT              right                   RT              0         
Mangalore       MNKLR           Bangalore               BNKLR           1         
Delhi           TLH             Dilli                   TL              1         </pre></div>
</div>
</div>
</div>
</div>
<div><p class="prompt input_prompt">Note that Delhi and Dilli are separated, which is not nice. On the other hand, Narendra and Narendar are marked as similar to zero edit distance, which is quite cool. Let's try a different technique and see how it goes.</p>
<div><div><p><strong>American soundex</strong></p>
<p>We note that the Soundex is aware of common similar-sounding words and gives them separate phonetic encoding. This allows us to separate <kbd>right</kbd> from <kbd>write</kbd>.</p>
<p>This will only work on American/English words though. Indian sounds such as <kbd>Narendra Modi</kbd> and <kbd>Narendra D. Modi</kbd> are now considered similar:</p>
</div>
</div>
</div>
<div><div><div><div><div><pre>calculate_phonetic_distance(phonetic_func=jellyfish.soundex, distance_func=jellyfish.levenshtein_distance)        </pre></div>
</div>
</div>
</div>
<div><div><div><p class="prompt">Note the changes from the previous code in the following table:</p>
<div><pre>Word            Sound           Word                    Sound           Phonetic Distance
Narendra Modi   N653            Narendra Modi           N653            0         
Narendra Modi   N653            Narendra D. Modi        N653            0         
Narendra Modi   N653            Narendar Modi           N653            0         
Gujarat         G263            Gujrat                  G263            0         
write           W630            right                   R230            2         
Mangalore       M524            Bangalore               B524            1         
Delhi           D400            Dilli                   D400            0    </pre></div>
</div>
</div>
</div>
</div>


            

            
        
    

        

                            
                    <h1 class="header-title">Runtime complexity</h1>
                
            
            
                
<div><div><div><p>We now have the ability to find the correct spellings of words or mark them as similar. While processing a large corpus, we can extract all unique words and compare each token against every other token.</p>
<p>It would take O(n<sup>2</sup>), where <em>n</em> is the number of unique tokens in a corpus. This might make the process too slow for a large corpus.</p>
<p>The alternative is to use a standard dictionary and expand the same for your corpus. If the dictionary has <em>m</em> unique words, this process now will be O(m∗n). Assuming that m&lt;&lt;n*m&lt;&lt;n<sup>2</sup>, this will be much faster than the previous approach.</p>
</div>
</div>
</div>


            

            
        
    

        

                            
                    <h1 class="header-title">Cleaning a corpus with FlashText</h1>
                
            
            
                
<p>But what about a web-scale corpus with millions of documents and a few thousand keywords? Regex can take several days to run over such exact searches because of its linear time complexity. How can we improve this?</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We can use FlashText for this very specific use case:</p>
<ul>
<li class="CDPAlignLeft CDPAlign">A few million documents with a few thousand keywords</li>
<li class="CDPAlignLeft CDPAlign">Exact keyword matches – either by replacing or searching for the presence of those keywords</li>
</ul>
<p>Of course, there are several different possible solutions to this problem. I recommend this for its simplicity and focus on solving one problem. It does not require us to learn new syntax or set up specific tools such as ElasticSearch.</p>
<p>The following table gives you a comparison of using Flashtext versus compiled regex for searching:</p>
<div><img src="img/a629566c-ec3d-427b-a4de-f22c8b7ee9a0.png"/></div>
<p class="mce-root"/>
<p>The following tables gives you a comparison of using FlashText versus compiled regex for substitutions:</p>
<div><img src="img/d090d72d-edef-4635-9035-0979e2af849c.png"/></div>
<p>We note that while the time taken by Regex scales almost linearly, Flashtext is relatively flat. Now, we know that we need Flashtext for speed and scale. FlashText has seen a lot of love from the community. Adopters include <a href="https://github.com/NIHOPA/NLPre">NLProc</a>  – the NLP Preprocessing Toolkit from the National Institute of Health.</p>
<p>Follow these instructions to install FlashText onto your machine.</p>
<p>First, we will install pip on our conda environment. We will do this from our notebook:</p>
<div><div><div><div><div><pre># import sys
# !{sys.executable} -m pip install flashtext</pre></div>
</div>
</div>
</div>
</div>
<div><p class="prompt input_prompt">The FlashText source code is available on GitHub (<a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-Python-Quick-Start/tree/master/Chapter02">https://github.com/PacktPublishing/Natural-Language-Processing-with-Python-Quick-Start/tree/master/Chapter02</a>), and the documents are pretty easy to navigate and use. We will only consider two basic examples here. Let's figure out the syntax for finding keywords that exist in a corpus:</p>
</div>
<div><div><div><div><div><pre>from flashtext.keyword import KeywordProcessor
keyword_processor = KeywordProcessor()
keyword_processor.add_keyword('Delhi', 'NCR') # notice we are adding tuples here
keyword_processor.add_keyword('Bombay', 'Mumbai')
keywords_found = keyword_processor.extract_keywords('I love the food in Delhi and the people in Bombay')
keywords_found
# ['NCR', 'Mumbai']</pre></div>
</div>
</div>
</div>
</div>
<div><p class="prompt input_prompt">How about we replace them now?</p>
</div>
<div><div><div><div><div><pre>from flashtext.keyword import KeywordProcessor
keyword_processor = KeywordProcessor()
keyword_processor.add_keyword('Delhi', 'NCR')
keyword_processor.add_keyword('Bombay', 'Mumbai')
replaced_sentence = keyword_processor.replace_keywords('I love the food in Delhi and the people in Bombay')
replaced_sentence
# 'I love the food in NCR and the people in Mumbai'</pre></div>
</div>
</div>
</div>
</div>
<div><div><div><p>Unfortunately, FlashText only supports English for the time being. Regex can search for keywords based on special characters such as <kbd>^,$,*,\d</kbd>, which are not supported in FlashText. So, to match partial words such as <kbd>word\dvec</kbd>, we would still have to use regex. However, FlashText is still excellent for extracting complete words like <kbd>word2vec</kbd>.</p>
</div>
</div>
</div>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>This chapter covered a lot of new ground. We started by performing linguistic processing on our text. We met <strong>spaCy</strong>, which we will continue to dive deeper into as we move on in this book. We covered the following foundational ideas from linguistics, tokenization doing this with and without spaCy, stop word removal, case standardization, lemmatization (we skipped stemming) – using spaCy and its peculiarities such as<em>-PRON-</em></p>
<p>But what do we do with spaCy, other than text cleaning? Can we build something? Yes!</p>
<p>Not only can we extend our simple linguistics based text cleaning using spaCy pipelines but also do parts of speech tagging, named entity recognition, and other common tasks. We will look at this in the next chapter. </p>
<p>We looked at spelling correction or the closest word match problem. We discussed <strong>FuzzyWuzzy</strong> and <strong>Jellyfish</strong> in this context. To ensure that we can scale beyond more than a few hundred keywords, we also looked at <strong>FlashText</strong>. I encourage you to dive deeper into any of these excellent libraries to learn about the best software engineering practices. </p>
<p>In the next chapter, we will tie all these together with other linguistic tools to build an end-to-end toy program. </p>


            

            
        
    </body></html>