<html><head></head><body>
		<div><h1 id="_idParaDest-42" class="chapter-number"><a id="_idTextAnchor042" class="calibre6 pcalibre pcalibre1"/>2</h1>
			<h1 id="_idParaDest-43" class="calibre7"><a id="_idTextAnchor043" class="calibre6 pcalibre pcalibre1"/>Playing with Grammar</h1>
			<p class="calibre3">Grammar is one of the main building blocks of language. Each human language, and programming language for that matter, has a set of rules that every person speaking it must follow, otherwise risking not being understood. These grammatical rules can be uncovered using NLP and are useful for extracting data from sentences. For example, using information about the grammatical structure of text, we can parse out subjects, objects, and relations between different entities.</p>
			<p class="calibre3">In this chapter, you will learn how to use different packages to reveal the grammatical structure of words and sentences, as well as extract certain parts of sentences. These are the topics covered in this chapter:</p>
			<ul class="calibre15">
				<li class="calibre14">Counting nouns – plural and singular nouns</li>
				<li class="calibre14">Getting the dependency parse</li>
				<li class="calibre14">Extracting noun chunks</li>
				<li class="calibre14">Extracting the subjects and objects of the sentence</li>
				<li class="calibre14">Finding patterns in text using grammatical information</li>
			</ul>
			<h1 id="_idParaDest-44" class="calibre7"><a id="_idTextAnchor044" class="calibre6 pcalibre pcalibre1"/>Technical requirements</h1>
			<p class="calibre3">Please follow the installation requirements given in <a href="B18411_01.xhtml#_idTextAnchor013" class="calibre6 pcalibre pcalibre1"><em class="italic">Chapter 1</em></a> to run the notebooks in this chapter.</p>
			<h1 id="_idParaDest-45" class="calibre7"><a id="_idTextAnchor045" class="calibre6 pcalibre pcalibre1"/>Counting nouns – plural and singular nouns</h1>
			<p class="calibre3">In this recipe, we will do two<a id="_idIndexMarker055" class="calibre6 pcalibre pcalibre1"/> things: determine whether a noun is plural or singular and turn plural nouns<a id="_idIndexMarker056" class="calibre6 pcalibre pcalibre1"/> into singular, and vice versa.</p>
			<p class="calibre3">You might need these two things for a variety of tasks. For example, you might want to count the word statistics, and for that, you most likely need to count the singular and plural nouns together. In order to count the plural nouns together with singular ones, you need a way to recognize that a word is plural or singular.</p>
			<h2 id="_idParaDest-46" class="calibre5"><a id="_idTextAnchor046" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">To determine whether a noun is singular or plural, we will use <code>spaCy</code> via two different methods: by looking at the <a id="_idIndexMarker057" class="calibre6 pcalibre pcalibre1"/>difference between the lemma and the actual word and by looking at the <code>morph</code> attribute. To inflect these nouns, or turn singular nouns into plural or vice versa we will use the <code>textblob</code> package. We will also see how to determine the noun’s number using GPT-3 through the OpenAI API. The code for this section is located at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter02" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter02</a>.</p>
			<h2 id="_idParaDest-47" class="calibre5"><a id="_idTextAnchor047" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">We will first use <code>spaCy</code>’s lemma information to infer whether a noun is singular or plural. Then, we will use the <code>morph</code> attribute of <code>Token</code> objects. We will then create a function that uses one of those methods. Finally, we will use GPT-3.5 to find out the number of nouns:</p>
			<ol class="calibre13">
				<li class="calibre14">Run  the code in the file and language <a id="_idIndexMarker058" class="calibre6 pcalibre pcalibre1"/>utility notebooks. If you run into an error saying that the small or large models do not exist, you need to open the <strong class="source-inline1">lang_utils.ipynb</strong> file, uncomment, and run the statement that downloads the model:<pre class="source-code">
%run -i "../util/file_utils.ipynb"
%run -i "../util/lang_utils.ipynb"</pre></li>				<li class="calibre14">Initialize the <strong class="source-inline1">text</strong> variable and process it using the <strong class="source-inline1">spaCy</strong> small model to get the resulting <strong class="source-inline1">Doc</strong> object:<pre class="source-code">
text = "I have five birds"
doc = small_model(text)</pre></li>				<li class="calibre14">In this step, we loop through the <strong class="source-inline1">Doc</strong> object. For each token in the object, we check whether it’s a noun and whether the lemma is the same as the word itself. Since the lemma is the basic form of the word, if the lemma is different from the word, that token is plural:<pre class="source-code">
for token in doc:
    if (token.pos_ == "NOUN" and token.lemma_ != token.text):
        print(token.text, "plural")</pre><p class="calibre3">The result should be as follows:</p><pre class="source-code">birds plural</pre></li>				<li class="calibre14">Now, we will check the number of a noun using a different method: the <strong class="source-inline1">morph</strong> features of a <strong class="source-inline1">Token</strong> object. The <strong class="source-inline1">morph</strong> features are the morphological features of a word, such<a id="_idIndexMarker059" class="calibre6 pcalibre pcalibre1"/> as number, case, and so on. Since we know that<a id="_idIndexMarker060" class="calibre6 pcalibre pcalibre1"/> token <strong class="source-inline1">3</strong> is a noun, we directly access the <strong class="source-inline1">morph</strong> features and get the <strong class="source-inline1">Number</strong> to get the same result as previo<a id="_idTextAnchor048" class="calibre6 pcalibre pcalibre1"/>usly:<pre class="source-code">
doc = small_model("I have five birds.")
print(doc[3].morph.get("Number"))</pre><p class="calibre3">Here is the result:</p><pre class="source-code">['Plur']</pre></li>				<li class="calibre14">In this step, we prepare to define a function that returns a tuple, <strong class="source-inline1">(noun, number)</strong>. In order to better encode the noun number, we use an <strong class="source-inline1">Enum</strong> class that assigns numbers to different values. We assign <strong class="source-inline1">1</strong> to singular and <strong class="source-inline1">2</strong> to plural. Once we create the class, we can directly refer to the noun number variables as <strong class="source-inline1">Noun_number.SINGULAR</strong> and <strong class="source-inline1">Noun_number.PLURAL</strong>:<pre class="source-code">
class Noun_number(Enum):
    SINGULAR = 1
    PLURAL = 2</pre></li>				<li class="calibre14">In this step, we define the function. It takes as input the text, the <strong class="source-inline1">spaCy</strong> model, and the method of determining the noun<a id="_idIndexMarker061" class="calibre6 pcalibre pcalibre1"/> number. The two methods are <strong class="source-inline1">lemma</strong> and <strong class="source-inline1">morph</strong>, the same two methods<a id="_idIndexMarker062" class="calibre6 pcalibre pcalibre1"/> we used in <em class="italic">steps 3</em> and <em class="italic">4</em>, respectively. The function outputs a list of tuples, each of the format <strong class="source-inline1">(&lt;noun text&gt;, &lt;noun number&gt;)</strong>, where the noun number is expressed using the <strong class="source-inline1">Noun_number</strong> class defined in <em class="italic">step 5</em>:<pre class="source-code">
def get_nouns_number(text, model, method="lemma"):
    nouns = []
    doc = model(text)
    for token in doc:
        if (token.pos_ == "NOUN"):
            if method == "lemma":
                if token.lemma_ != token.text:
                    nouns.append((token.text, 
                        Noun_number.PLURAL))
                else:
                    nouns.append((token.text,
                        Noun_number.SINGULAR))
            elif method == "morph":
                if token.morph.get("Number") == "Sing":
                    nouns.append((token.text,
                        Noun_number.PLURAL))
                else:
                    nouns.append((token.text,
                        Noun_number.SINGULAR))
    return nouns</pre></li>				<li class="calibre14">We can use the preceding function and see its performance with different <strong class="source-inline1">spaCy</strong> models. In this step, we use <a id="_idIndexMarker063" class="calibre6 pcalibre pcalibre1"/>the small <strong class="source-inline1">spaCy</strong> model with the function we just defined. Using <a id="_idIndexMarker064" class="calibre6 pcalibre pcalibre1"/>both methods, we see that the <strong class="source-inline1">spaCy</strong> model gets the number of the irregular noun <strong class="source-inline1">geese</strong> incorrectly:<pre class="source-code">
text = "Three geese crossed the road"
nouns = get_nouns_number(text, small_model, "morph")
print(nouns)
nouns = get_nouns_number(text, small_model)
print(nouns)</pre><p class="calibre3">The result should be as follows:</p><pre class="source-code">[('geese', &lt;Noun_number.SINGULAR: 1&gt;), ('road', &lt;Noun_number.SINGULAR: 1&gt;)]
[('geese', &lt;Noun_number.SINGULAR: 1&gt;), ('road', &lt;Noun_number.SINGULAR: 1&gt;)]</pre></li>				<li class="calibre14">Now, let’s do the same using the large model. If you have not yet downloaded the large model, do so by running the first line. Otherwise, you can comment it out. Here, we see that although the <strong class="source-inline1">morph</strong> method still incorrectly assigns singular to <strong class="source-inline1">geese</strong>, the <strong class="source-inline1">lemma</strong> method provides the correct answer:<pre class="source-code">
!python -m spacy download en_core_web_lg
large_model = spacy.load("en_core_web_lg")
nouns = get_nouns_number(text, large_model, "morph")
print(nouns)
nouns = get_nouns_number(text, large_model)
print(nouns)</pre><p class="calibre3">The result should be as follows:</p><pre class="source-code">[('geese', &lt;Noun_number.SINGULAR: 1&gt;), ('road', &lt;Noun_number.SINGULAR: 1&gt;)]
[('geese', &lt;Noun_number.PLURAL: 2&gt;), ('road', &lt;Noun_number.SINGULAR: 1&gt;)]</pre></li>				<li class="calibre14">Let’s now use GPT-3.5 to get the noun <a id="_idIndexMarker065" class="calibre6 pcalibre pcalibre1"/>number. In the results, we see that GPT-3.5 gives us an identical <a id="_idIndexMarker066" class="calibre6 pcalibre pcalibre1"/>result and correctly identifies both the number for <strong class="source-inline1">geese</strong> and the number for <strong class="source-inline1">road</strong>:<pre class="source-code">
from openai import OpenAI
client = OpenAI(api_key=OPEN_AI_KEY)
prompt="""Decide whether each noun in the following text is singular or plural.
Return the list in the format of a python tuple: (word, number). Do not provide any additional explanations.
Sentence: Three geese crossed the road."""
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    temperature=0,
    max_tokens=256,
    top_p=1.0,
    frequency_penalty=0,
    presence_penalty=0,
    messages=[
        {"role": "system", "content": "You are a helpful 
            assistant."},
        {"role": "user", "content": prompt}
    ],
)
print(response.choices[0].message.content)</pre><p class="calibre3">The result should be as follows:</p><pre class="source-code">('geese', 'plural')
('road', 'singular')</pre></li>			</ol>
			<h2 id="_idParaDest-48" class="calibre5"><a id="_idTextAnchor049" class="calibre6 pcalibre pcalibre1"/>There’s more…</h2>
			<p class="calibre3">We can also change the nouns <a id="_idIndexMarker067" class="calibre6 pcalibre pcalibre1"/>from plural to singular, and vice versa. We will use the <code>textblob</code> package for that. The package should be installed automatically via the Poetry <a id="_idIndexMarker068" class="calibre6 pcalibre pcalibre1"/>environment:</p>
			<ol class="calibre13">
				<li class="calibre14">Import the <strong class="source-inline1">TextBlob</strong> class from the package:<pre class="source-code">
from textblob import TextBlob</pre></li>				<li class="calibre14">Initialize a list of text variables and process them using the <strong class="source-inline1">TextBlob</strong> class via a list comprehension:<pre class="source-code">
texts = ["book", "goose", "pen", "point", "deer"]
blob_objs = [TextBlob(text) for text in texts]</pre></li>				<li class="calibre14">Use the <strong class="source-inline1">pluralize</strong> function of the object to get the plural. This function returns a list and we access its first element. Print the result:<pre class="source-code">
plurals = [blob_obj.words.pluralize()[0] 
    for blob_obj in blob_objs]
print(plurals)</pre><p class="calibre3">The result should be as follows:</p><pre class="source-code">['books', 'geese', 'pens', 'points', 'deer']</pre></li>				<li class="calibre14">Now, we will do the reverse. We use the preceding <strong class="source-inline1">plurals</strong> list to turn the plural nouns into <strong class="source-inline1">TextBlob</strong> objects:<pre class="source-code">
blob_objs = [TextBlob(text) for text in plurals]</pre></li>				<li class="calibre14">Turn the nouns into singular using the <strong class="source-inline1">singularize</strong> function and print:<pre class="source-code">
singulars = [blob_obj.words.singularize()[0] 
    for blob_obj in blob_objs]
print(singulars)</pre><p class="calibre3">The result should be the same as the list we started with in <em class="italic">step 2</em>:</p><pre class="source-code">['book', 'goose', 'pen', 'point', 'deer']</pre></li>			</ol>
			<h1 id="_idParaDest-49" class="calibre7"><a id="_idTextAnchor050" class="calibre6 pcalibre pcalibre1"/>Getting the dependency parse</h1>
			<p class="calibre3">A dependency parse is a tool that shows dependencies in a sentence. For example, in the sentence <em class="italic">The cat wore a hat</em>, the root of the sentence is the verb, <em class="italic">wore</em>, and both the subject, <em class="italic">the cat</em>, and the object, <em class="italic">a hat</em>, are dependents. The dependency parse can be very useful in many NLP tasks since it shows the grammatical structure of the sentence, with the subject, the main verb, the object, and so on. It can then be used in downstream processing.</p>
			<p class="calibre3">The <code>spaCy</code> NLP engine does the dependency <a id="_idIndexMarker069" class="calibre6 pcalibre pcalibre1"/>parse as part of its overall analysis. The dependency parse tags explain the role of each word in the sentence. <code>ROOT</code> is the main word that all other words depend on, usually the verb.</p>
			<h2 id="_idParaDest-50" class="calibre5"><a id="_idTextAnchor051" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will use <code>spaCy</code> to create the dependency parse. The required packages are part of the Poetry environment.</p>
			<h2 id="_idParaDest-51" class="calibre5"><a id="_idTextAnchor052" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">We will take a few sentences from the <code>sherlock_holmes1.txt</code> file to illustrate the dependency parse. The steps are as follows:</p>
			<ol class="calibre13">
				<li class="calibre14">Run the file and language utility notebooks:<pre class="source-code">
%run -i "../util/file_utils.ipynb"
%run -i "../util/lang_utils.ipynb"</pre></li>				<li class="calibre14">Define the sentence we will be parsing:<pre class="source-code">
sentence = 'I have seldom heard him mention her under any other name.'</pre></li>				<li class="calibre14">Define a function that will print the word, its grammatical function embedded in the <strong class="source-inline1">dep_</strong> attribute, and the explanation of that attribute. The <strong class="source-inline1">dep_</strong> attribute of the <strong class="source-inline1">Token</strong> object shows the grammatical function of the word in the sentence:<pre class="source-code">
def print_dependencies(sentence, model):
    doc = model(sentence)
    for token in doc:
        print(token.text, "\t", token.dep_, "\t", 
            spacy.explain(token.dep_))</pre></li>				<li class="calibre14">Now, let’s use this function on the<a id="_idIndexMarker070" class="calibre6 pcalibre pcalibre1"/> first sentence in our list. We can see that the verb <strong class="source-inline1">heard</strong> is the <strong class="source-inline1">ROOT</strong> word of the sentence, with all other words depending on it:<pre class="source-code">
print_dependencies(sentence, small_model)</pre><p class="calibre3">The result should be as follows:</p><pre class="source-code">I    nsubj    nominal subject
have    aux    auxiliary
seldom    advmod    adverbial modifier
heard    ROOT    root
him    nsubj    nominal subject
mention    ccomp    clausal complement
her    dobj    direct object
under    prep    prepositional modifier
any    det    determiner
other    amod    adjectival modifier
name    pobj    object of preposition
.    punct    punctuation</pre></li>				<li class="calibre14">To explore the dependency parse structure, we can use the attributes of the <strong class="source-inline1">Token</strong> class. Using the <strong class="source-inline1">ancestors</strong> and <strong class="source-inline1">children</strong> attributes, we can get the tokens that this token<a id="_idIndexMarker071" class="calibre6 pcalibre pcalibre1"/> depends on and the tokens that depend on it, respectively. The function to print the ancestors is as follows:<pre class="source-code">
def print_ancestors(sentence, model):
    doc = model(sentence)
    for token in doc:
        print(token.text, [t.text for t in token.ancestors])</pre></li>				<li class="calibre14">Now, let’s use this function on the first sentence in our list:<pre class="source-code">
print_ancestors(sentence, small_model)</pre><p class="calibre3">The output will be as follows. In the result, we see that <code>heard</code> has no ancestors since it is the main word in the sentence. All other words depend on it, and in fact, contain <code>heard</code> in their ancestor lists.</p><p class="calibre3">The dependency chain can be seen by following the ancestor links for each word. For example, if we look at the word <code>name</code>, we see that its ancestors are <code>under</code>, <code>mention</code>, and <code>heard</code>. The immediate parent of <code>name</code> is <code>under</code>, the parent of <code>under</code> is <code>mention</code>, and the parent of <code>mention</code> is <code>heard</code>. A dependency chain will always lead to the root, or the main word, of the sentence:</p><pre class="source-code">I ['heard']
have ['heard']
seldom ['heard']
heard []
him ['mention', 'heard']
mention ['heard']
her ['mention', 'heard']
under ['mention', 'heard']
any ['name', 'under', 'mention', 'heard']
other ['name', 'under', 'mention', 'heard']
name ['under', 'mention', 'heard']
. ['heard']</pre></li>				<li class="calibre14">To see all the<a id="_idIndexMarker072" class="calibre6 pcalibre pcalibre1"/> children, use the following function. This function prints out each word and the words that depend on it, its <strong class="bold">children</strong>:<pre class="source-code">
def print_children(sentence, model):
    doc = model(sentence)
    for token in doc:
        print(token.text,[t.text for t in token.children])</pre></li>				<li class="calibre14">Now, let’s use this function on the first sentence in our list:<pre class="source-code">
print_children(sentence, small_model)</pre><p class="calibre3">The result should be as follows. Now, the word <code>heard</code> has a list of words that depend on it since it is the main word in the sentence:</p><pre class="source-code">I []
have []
seldom []
heard ['I', 'have', 'seldom', 'mention', '.']
him []
mention ['him', 'her', 'under']
her []
under ['name']
any []
other []
name ['any', 'other']
. []</pre></li>				<li class="calibre14">We can also see left and right children in separate lists. In the following function, we print the children <a id="_idIndexMarker073" class="calibre6 pcalibre pcalibre1"/>as two separate lists, left and right. This can be useful when doing grammatical transformations in the sentence:<pre class="source-code">
def print_lefts_and_rights(sentence, model):
    doc = model(sentence)
    for token in doc:
        print(token.text,
            [t.text for t in token.lefts],
            [t.text for t in token.rights])</pre></li>				<li class="calibre14">Let’s use this function on the first sentence in our list:<pre class="source-code">
print_lefts_and_rights(sentence, small_model)</pre><p class="calibre3">The result should be as follows:</p><pre class="source-code">I [] []
have [] []
seldom [] []
heard ['I', 'have', 'seldom'] ['mention', '.']
him [] []
mention ['him'] ['her', 'under']
her [] []
under [] ['name']
any [] []
other [] []
name ['any', 'other'] []
. [] []</pre></li>				<li class="calibre14">We can also see the <a id="_idIndexMarker074" class="calibre6 pcalibre pcalibre1"/>subtree that the token is in by using this function:<pre class="source-code">
def print_subtree(sentence, model):
    doc = model(sentence)
    for token in doc:
        print(token.text, [t.text for t in token.subtree])</pre></li>				<li class="calibre14">Let’s use this function on the first sentence in our list:<pre class="source-code">
print_subtree(sentence, small_model)</pre><p class="calibre3">The result should be as follows. From the subtrees that each word is part of, we can see the grammatical phrases that<a id="_idIndexMarker075" class="calibre6 pcalibre pcalibre1"/><a id="_idIndexMarker076" class="calibre6 pcalibre pcalibre1"/> appear in<a id="_idIndexMarker077" class="calibre6 pcalibre pcalibre1"/> the sentence, such as the <code>any other name</code>, and the <code>under any </code><code>other name</code>:</p><pre class="source-code">I ['I']
have ['have']
seldom ['seldom']
heard ['I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', '.']
him ['him']
mention ['him', 'mention', 'her', 'under', 'any', 'other', 'name']
her ['her']
under ['under', 'any', 'other', 'name']
any ['any']
other ['other']
name ['any', 'other', 'name']
. ['.']</pre></li>			</ol>
			<h2 id="_idParaDest-52" class="calibre5"><a id="_idTextAnchor053" class="calibre6 pcalibre pcalibre1"/>See also</h2>
			<p class="calibre3">The dependency parse can be<a id="_idIndexMarker078" class="calibre6 pcalibre pcalibre1"/> visualized graphically using the <code>displaCy</code> package, which is part of <code>spaCy</code>. Please see <a href="B18411_08.xhtml#_idTextAnchor205" class="calibre6 pcalibre pcalibre1"><em class="italic">Chapter </em></a><em class="italic">7</em>, <em class="italic">Visualizing Text Data</em>, for a detailed recipe on how to do the visualization.</p>
			<h1 id="_idParaDest-53" class="calibre7"><a id="_idTextAnchor054" class="calibre6 pcalibre pcalibre1"/>Extracting noun chunks</h1>
			<p class="calibre3">Noun chunks are known in linguistics as noun phrases. They<a id="_idIndexMarker079" class="calibre6 pcalibre pcalibre1"/> represent nouns and any words that depend on and accompany nouns. For example, in the sentence <em class="italic">The big red apple fell on the scared cat</em>, the noun<a id="_idIndexMarker080" class="calibre6 pcalibre pcalibre1"/> chunks are <em class="italic">the big red apple</em> and <em class="italic">the scared cat</em>. Extracting these noun chunks is instrumental to many other downstream NLP tasks, such as named entity recognition and processing entities and relations between them. In this recipe, we will explore how to extract named entities from a text.</p>
			<h2 id="_idParaDest-54" class="calibre5"><a id="_idTextAnchor055" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will use the <code>spaCy</code> package, which has a function for extracting noun chunks, and the text from the <code>sherlock_holmes_1.txt</code> file as an example.</p>
			<h2 id="_idParaDest-55" class="calibre5"><a id="_idTextAnchor056" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">Use the following steps to get the noun chunks from a text:</p>
			<ol class="calibre13">
				<li class="calibre14">Run the file and language utility notebooks:<pre class="source-code">
%run -i "../util/file_utils.ipynb"
%run -i "../util/lang_utils.ipynb"</pre></li>				<li class="calibre14">Define the function<a id="_idIndexMarker081" class="calibre6 pcalibre pcalibre1"/> that will print out the noun chunks. The noun chunks are contained in the <strong class="source-inline1">doc.noun_chunks</strong> class variable:<pre class="source-code">
def print_noun_chunks(text, model):
    doc = model(text)
    for noun_chunk in doc.noun_chunks:
        print(noun_chunk.text)</pre></li>				<li class="calibre14">Read the text from the <strong class="source-inline1">sherlock_holmes_1.txt</strong> file and use the function on the resulting text:<pre class="source-code">
sherlock_holmes_part_of_text = read_text_file("../data/sherlock_holmes_1.txt")
print_noun_chunks(sherlock_holmes_part_of_text, small_model)</pre><p class="calibre3">This is the partial result. See the output of the notebook at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter02/noun_chunks_2.3.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter02/noun_chunks_2.3.ipynb</a> for the full printout. The function gets the pronouns, nouns, and noun phrases that are in the text correctly:</p><pre class="source-code">Sherlock Holmes
she
the_ woman
I
him
her
any other name
his eyes
she
the whole
…</pre></li>			</ol>
			<h2 id="_idParaDest-56" class="calibre5"><a id="_idTextAnchor057" class="calibre6 pcalibre pcalibre1"/>There’s more…</h2>
			<p class="calibre3">Noun chunks are <code>spaCy</code> <code>Span</code> objects and have all their properties. See the official documentation at <a href="https://spacy.io/api/token" class="calibre6 pcalibre pcalibre1">https://spacy.io/api/token</a>.</p>
			<p class="calibre3">Let’s explore some properties of noun chunks:</p>
			<ol class="calibre13">
				<li class="calibre14">We will define a function that will print out the different properties of noun chunks. It will print the text<a id="_idIndexMarker082" class="calibre6 pcalibre pcalibre1"/> of the noun chunk, its start and end indices within the <strong class="source-inline1">Doc</strong> object, the sentence it belongs to (useful when there is more than one sentence), the root of the noun chunk (its main word), and the chunk’s similarity to the word <strong class="source-inline1">emotions</strong>. Finally, it will print out the similarity of the whole input sentence to <strong class="source-inline1">emotions</strong>:<pre class="source-code">
def explore_properties(sentence, model):
    doc = model(sentence)
    other_span = "emotions"
    other_doc = model(other_span)
    for noun_chunk in doc.noun_chunks:
        print(noun_chunk.text)
        print("Noun chunk start and end", "\t",
            noun_chunk.start, "\t", noun_chunk.end)
        print("Noun chunk sentence:", noun_chunk.sent)
        print("Noun chunk root:", noun_chunk.root.text)
        print(f"Noun chunk similarity to '{other_span}'",
            noun_chunk.similarity(other_doc))
    print(f"Similarity of the sentence '{sentence}' to 
        '{other_span}':",
        doc.similarity(other_doc))</pre></li>				<li class="calibre14">Set the sentence to <strong class="source-inline1">All emotions, and that one particularly, were abhorrent to his cold, precise but admirably </strong><strong class="source-inline1">balanced mind</strong>:<pre class="source-code">
sentence = "All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind."</pre></li>				<li class="calibre14">Use the <strong class="source-inline1">explore_properties</strong> function on <a id="_idIndexMarker083" class="calibre6 pcalibre pcalibre1"/>the sentence using the small model:<pre class="source-code">
explore_properties(sentence, small_model)</pre><p class="calibre3">This is the result:</p><pre class="source-code">All emotions
Noun chunk start and end    0    2
Noun chunk sentence: All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind.
Noun chunk root: emotions
Noun chunk similarity to 'emotions' 0.4026421588260174
his cold, precise but admirably balanced mind
Noun chunk start and end    11    19
Noun chunk sentence: All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind.
Noun chunk root: mind
Noun chunk similarity to 'emotions' -0.036891259527462
Similarity of the sentence 'All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind.' to 'emotions': 0.03174900767577446</pre><p class="calibre3">You will also see a warning message <a id="_idIndexMarker084" class="calibre6 pcalibre pcalibre1"/>similar to this one due to the fact that the small model does not ship with word vectors of its own:</p><pre class="source-code">/tmp/ipykernel_1807/2430050149.py:10: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.
  print(f"Noun chunk similarity to '{other_span}'", noun_chunk.similarity(other_doc))</pre></li>				<li class="calibre14">Now, let’s apply the same function to the same sentence with the large model:<pre class="source-code">
sentence = "All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind."
explore_properties(sentence, large_model)</pre><p class="calibre3">The large model does come with its own word vectors and does not result in a warning:</p><pre class="source-code">All emotions
Noun chunk start and end    0    2
Noun chunk sentence: All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind.
Noun chunk root: emotions
Noun chunk similarity to 'emotions' 0.6302678068015664
his cold, precise but admirably balanced mind
Noun chunk start and end    11    19
Noun chunk sentence: All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind.
Noun chunk root: mind
Noun chunk similarity to 'emotions' 0.5744456705692561
Similarity of the sentence 'All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind.' to 'emotions': 0.640366414527618</pre><p class="calibre3">We see that the similarity of the <code>All emotions</code> noun chunk is high in relation to the word <code>emotions</code>, as compared<a id="_idIndexMarker085" class="calibre6 pcalibre pcalibre1"/> to the similarity of the <code>his cold, precise but admirably balanced mind</code> noun chunk.</p></li>			</ol>
			<p class="callout-heading">Important note</p>
			<p class="callout">A larger <strong class="source-inline1">spaCy</strong> model, such as <strong class="source-inline1">en_core_web_lg</strong>, takes up more space but is more precise.</p>
			<h2 id="_idParaDest-57" class="calibre5"><a id="_idTextAnchor058" class="calibre6 pcalibre pcalibre1"/>See also</h2>
			<p class="calibre3">The topic of semantic similarity will be explored in more detail in <a href="B18411_03.xhtml#_idTextAnchor067" class="calibre6 pcalibre pcalibre1"><em class="italic">Chapter 3</em></a>.</p>
			<h1 id="_idParaDest-58" class="calibre7"><a id="_idTextAnchor059" class="calibre6 pcalibre pcalibre1"/>Extracting subjects and objects of the sentence</h1>
			<p class="calibre3">Sometimes, we might need to find <a id="_idIndexMarker086" class="calibre6 pcalibre pcalibre1"/>the subject and direct objects of the sentence, and that is easily accomplished with the <code>spaCy</code> package.</p>
			<h2 id="_idParaDest-59" class="calibre5"><a id="_idTextAnchor060" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will be using the dependency tags from <code>spaCy</code> to find subjects and objects. The code uses the <code>spaCy</code> engine to parse the sentence. Then, the subject function loops through the tokens, and if the dependency tag contains <code>subj</code>, it returns that token’s subtree, a <code>Span</code> object. There are different subject tags, including <code>nsubj</code> for regular subjects and <code>nsubjpass</code> for subjects of passive sentences, thus we want to look for both.</p>
			<h2 id="_idParaDest-60" class="calibre5"><a id="_idTextAnchor061" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">We will use the <code>subtree</code> attribute of tokens to find the complete noun chunk that is the subject or direct object of the verb (see the <em class="italic">Getting the dependency parse</em> recipe). We will define functions to find the subject, direct object, dative phrase, and prepositional phrases:</p>
			<ol class="calibre13">
				<li class="calibre14">Run the file and language utility notebooks:<pre class="source-code">
%run -i "../util/file_utils.ipynb"
%run -i "../util/lang_utils.ipynb"</pre></li>				<li class="calibre14">We will use two functions to find the subject and the direct object of the sentence. These functions will loop through the tokens and return the subtree that contains the token with <strong class="source-inline1">subj</strong> or <strong class="source-inline1">dobj</strong> in the dependency tag, respectively. Here is the subject function. It looks for the token that has a dependency tag that contains <strong class="source-inline1">subj</strong> and then returns the subtree that contains that token. There are several subject dependency tags, including <strong class="source-inline1">nsubj</strong> and <strong class="source-inline1">nsubjpass</strong> (for the subject of a passive sentence), so we look for the most general pattern:<pre class="source-code">
def get_subject_phrase(doc):
    for token in doc:
        if ("subj" in token.dep_):
            subtree = list(token.subtree)
            start = subtree[0].i
            end = subtree[-1].i + 1
            return doc[start:end]</pre></li>				<li class="calibre14">Here is the direct object function. It works similarly to <strong class="source-inline1">get_subject_phrase</strong> but looks for the <strong class="source-inline1">dobj</strong> dependency tag instead of a tag that contains <strong class="source-inline1">subj</strong>. If the<a id="_idIndexMarker087" class="calibre6 pcalibre pcalibre1"/> sentence does not have a direct object, it will return <strong class="source-inline1">None</strong>:<pre class="source-code">
def get_object_phrase(doc):
    for token in doc:
        if ("dobj" in token.dep_):
            subtree = list(token.subtree)
            start = subtree[0].i
            end = subtree[-1].i + 1
            return doc[start:end]</pre></li>				<li class="calibre14">Assign a list of sentences to a variable, loop through them, and use the preceding functions to print out their subjects and objects:<pre class="source-code">
sentences = [
    "The big black cat stared at the small dog.",
    "Jane watched her brother in the evenings.",
    "Laura gave Sam a very interesting book."
]
for sentence in sentences:
    doc = small_model(sentence)
    subject_phrase = get_subject_phrase(doc)
    object_phrase = get_object_phrase(doc)
    print(sentence)
    print("\tSubject:", subject_phrase)
    print("\tDirect object:", object_phrase)</pre><p class="calibre3">The result will be as follows. Since the first sentence does not have a direct object, <code>None</code> is printed out. For the <a id="_idIndexMarker088" class="calibre6 pcalibre pcalibre1"/>sentence <code>The big black cat stared at the small dog</code>, the subject is <code>the big black cat</code> and there is no direct object (<code>the small dog</code> is the object of the preposition <code>at</code>). For the sentence <code>Jane watched her brother in the evenings</code>, the subject is <code>Jane</code> and the direct object is <code>her brother</code>. In the sentence <code>Laura gave Sam a very interesting book</code>, the subject is <code>Laura</code> and the direct object is <code>a very </code><code>interesting book</code>:</p><pre class="source-code">The big black cat stared at the small dog.
  Subject: The big black cat
  Direct object: None
Jane watched her brother in the evenings.
  Subject: Jane
  Direct object: her brother
Laura gave Sam a very interesting book.
  Subject: Laura
  Direct object: a very interesting book</pre></li>			</ol>
			<h2 id="_idParaDest-61" class="calibre5"><a id="_idTextAnchor062" class="calibre6 pcalibre pcalibre1"/>There’s more…</h2>
			<p class="calibre3">We can look for other objects, for example, the dative objects of verbs such as <em class="italic">give</em> and objects of prepositional phrases. The functions will look very similar, with the main difference being the dependency tags: <code>dative</code> for the dative object function, and <code>pobj</code> for the prepositional object function. The prepositional object function will return a list since there can be more than one prepositional phrase in a sentence:</p>
			<ol class="calibre13">
				<li class="calibre14">The dative object<a id="_idIndexMarker089" class="calibre6 pcalibre pcalibre1"/> function checks the tokens for the <strong class="source-inline1">dative</strong> tag. It returns <strong class="source-inline1">None</strong> if there are no dative objects:<pre class="source-code">
def get_dative_phrase(doc):
    for token in doc:
        if ("dative" in token.dep_):
            subtree = list(token.subtree)
            start = subtree[0].i
            end = subtree[-1].i + 1
            return doc[start:end]</pre></li>				<li class="calibre14">We can also combine the subject, object, and dative functions into one with an argument that specifies which object to look for:<pre class="source-code">
def get_phrase(doc, phrase):
    # phrase is one of "subj", "obj", "dative"
    for token in doc:
        if (phrase in token.dep_):
            subtree = list(token.subtree)
            start = subtree[0].i
            end = subtree[-1].i + 1
            return doc[start:end]</pre></li>				<li class="calibre14">Let us now define a sentence with a dative object and run the function for all three types of phrases:<pre class="source-code">
sentence = "Laura gave Sam a very interesting book."
doc = small_model(sentence)
subject_phrase = get_phrase(doc, "subj")
object_phrase = get_phrase(doc, "obj")
dative_phrase = get_phrase(doc, "dative")
print(sentence)
print("\tSubject:", subject_phrase)
print("\tDirect object:", object_phrase)
print("\tDative object:", dative_phrase)</pre><p class="calibre3">The result will be as<a id="_idIndexMarker090" class="calibre6 pcalibre pcalibre1"/> follows. The dative object is <code>Sam</code>:</p><pre class="source-code">Laura gave Sam a very interesting book.
  Subject: Laura
  Direct object: a very interesting book
  Dative object: Sam</pre></li>				<li class="calibre14">Here is the prepositional object function. It returns a list of objects of prepositions, which will be empty if there are none:<pre class="source-code">
def get_prepositional_phrase_objs(doc):
    prep_spans = []
    for token in doc:
        if ("pobj" in token.dep_):
            subtree = list(token.subtree)
            start = subtree[0].i
            end = subtree[-1].i + 1
            prep_spans.append(doc[start:end])
    return prep_spans</pre></li>				<li class="calibre14">Let’s define a list of sentences and run the two functions on them:<pre class="source-code">
sentences = [
    "The big black cat stared at the small dog.",
    "Jane watched her brother in the evenings."
]
for sentence in sentences:
    doc = small_model(sentence)
    subject_phrase = get_phrase(doc, "subj")
    object_phrase = get_phrase(doc, "obj")
    dative_phrase = get_phrase(doc, "dative")
    prepositional_phrase_objs = \
        get_prepositional_phrase_objs(doc)
    print(sentence)
    print("\tSubject:", subject_phrase)
    print("\tDirect object:", object_phrase)
    print("\tPrepositional phrases:", prepositional_phrase_objs)</pre><p class="calibre3">The result will be <a id="_idIndexMarker091" class="calibre6 pcalibre pcalibre1"/>as follows:</p><pre class="source-code">The big black cat stared at the small dog.
  Subject: The big black cat
  Direct object: the small dog
  Prepositional phrases: [the small dog]
Jane watched her brother in the evenings.
  Subject: Jane
  Direct object: her brother
  Prepositional phrases: [the evenings]</pre><p class="calibre3">There is one prepositional phrase in each sentence. In the sentence <code>The big black cat stared at the small dog</code>, it is <code>at the small dog</code>, and in the sentence <code>Jane watched her brother in the evenings</code>, it is <code>in </code><code>the evenings</code>.</p></li>			</ol>
			<p class="calibre3">It is left as an exercise for you<a id="_idIndexMarker092" class="calibre6 pcalibre pcalibre1"/> to find the actual prepositional phrases with prepositions intact instead of just the noun phrases that are dependent on these prepositions.</p>
			<h1 id="_idParaDest-62" class="calibre7"><a id="_idTextAnchor063" class="calibre6 pcalibre pcalibre1"/>Finding patterns in text using grammatical information</h1>
			<p class="calibre3">In this section, we will use the <code>spaCy</code> <code>Matcher</code> object to find patterns in the text. We will use the grammatical properties of<a id="_idIndexMarker093" class="calibre6 pcalibre pcalibre1"/> the words to create these patterns. For example, we might be looking for verb phrases instead of noun phrases. We can specify grammatical patterns to match verb phrases.</p>
			<h2 id="_idParaDest-63" class="calibre5"><a id="_idTextAnchor064" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will be using the <code>spaCy</code> <code>Matcher</code> object to specify and find patterns. It can match different properties, not just grammatical. You can find out more in the documentation at <a href="https://spacy.io/usage/rule-based-matching/" class="calibre6 pcalibre pcalibre1">https://spacy.io/usage/rule-based-matching/</a>.</p>
			<h2 id="_idParaDest-64" class="calibre5"><a id="_idTextAnchor065" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">Your steps should be formatted like so:</p>
			<ol class="calibre13">
				<li class="calibre14">Run the file and language utility notebooks:<pre class="source-code">
%run -i "../util/file_utils.ipynb"
%run -i "../util/lang_utils.ipynb"</pre></li>				<li class="calibre14">Import the <strong class="source-inline1">Matcher</strong> object and initialize it. We need to put in the vocabulary object, which is the same as the vocabulary of the model we will be using to process the text:<pre class="source-code">
from spacy.matcher import Matcher
matcher = Matcher(small_model.vocab)</pre></li>				<li class="calibre14">Create a list of patterns and add them to the matcher. Each pattern is a list of dictionaries, where each dictionary describes a token. In our patterns, we only specify the part of speech for each token. We then add these patterns to the <strong class="source-inline1">Matcher</strong> object.  The patterns we will be using are a verb by itself (for example, <em class="italic">paints</em>), an auxiliary followed by a verb (for example, <strong class="source-inline1">was observing</strong>), an auxiliary<a id="_idIndexMarker094" class="calibre6 pcalibre pcalibre1"/> followed by an adjective (for example, <strong class="source-inline1">were late</strong>), and an auxiliary followed by a verb and a preposition (for example, <strong class="source-inline1">were staring at</strong>). This is not an exhaustive list; feel free to come up with other examples:<pre class="source-code">
patterns = [
    [{"POS": "VERB"}],
    [{"POS": "AUX"}, {"POS": "VERB"}],
    [{"POS": "AUX"}, {"POS": "ADJ"}],
    [{"POS": "AUX"}, {"POS": "VERB"}, {"POS": "ADP"}]
]
matcher.add("Verb", patterns)</pre></li>				<li class="calibre14">Read in the small part of the <em class="italic">Sherlock Holmes</em> text and process it using the small model:<pre class="source-code">
sherlock_holmes_part_of_text = read_text_file("../data/sherlock_holmes_1.txt")
doc = small_model(sherlock_holmes_part_of_text)</pre></li>				<li class="calibre14">Now, we find the matches using the <strong class="source-inline1">Matcher</strong> object and the processed text. We then loop through the matches and print out the match ID, the string ID (the identifier of the pattern), the start and end of the match, and the text of the match:<pre class="source-code">
matches = matcher(doc)
for match_id, start, end in matches:
    string_id = small_model.vocab.strings[match_id]
    span = doc[start:end]
    print(match_id, string_id, start, end, span.text)</pre><p class="calibre3">The result will be<a id="_idIndexMarker095" class="calibre6 pcalibre pcalibre1"/> as follows:</p><pre class="source-code">14677086776663181681 Verb 14 15 heard
14677086776663181681 Verb 17 18 mention
14677086776663181681 Verb 28 29 eclipses
14677086776663181681 Verb 31 32 predominates
14677086776663181681 Verb 43 44 felt
14677086776663181681 Verb 49 50 love
14677086776663181681 Verb 63 65 were abhorrent
14677086776663181681 Verb 80 81 take
14677086776663181681 Verb 88 89 observing
14677086776663181681 Verb 94 96 has seen
14677086776663181681 Verb 95 96 seen
14677086776663181681 Verb 103 105 have placed
14677086776663181681 Verb 104 105 placed
14677086776663181681 Verb 114 115 spoke
14677086776663181681 Verb 120 121 save
14677086776663181681 Verb 130 132 were admirable
14677086776663181681 Verb 140 141 drawing
14677086776663181681 Verb 153 154 trained
14677086776663181681 Verb 157 158 admit
14677086776663181681 Verb 167 168 adjusted
14677086776663181681 Verb 171 172 introduce
14677086776663181681 Verb 173 174 distracting
14677086776663181681 Verb 178 179 throw
14677086776663181681 Verb 228 229 was</pre></li>			</ol>
			<p class="calibre3">The code finds some of the verb <a id="_idIndexMarker096" class="calibre6 pcalibre pcalibre1"/>phrases in the text. Sometimes, it finds a partial match that is part of another match. Weeding out these partial matches is left as an exercise.</p>
			<h2 id="_idParaDest-65" class="calibre5"><a id="_idTextAnchor066" class="calibre6 pcalibre pcalibre1"/>See also</h2>
			<p class="calibre3">We can use other attributes apart from parts of speech. It is possible to match on the text itself, its length, whether it is alphanumeric, the punctuation, the word’s case, the <code>dep_</code> and <code>morph</code> attributes, lemma, entity type, and others. It is also possible to use regular expressions on the patterns. For <a id="_idIndexMarker097" class="calibre6 pcalibre pcalibre1"/>more information, see the spaCy documentation: <a href="https://spacy.io/usage/rule-based-matching" class="calibre6 pcalibre pcalibre1">https://spacy.io/usage/rule-based-matching</a>.</p>
		</div>
	</body></html>