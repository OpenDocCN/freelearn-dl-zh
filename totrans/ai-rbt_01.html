<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-16"><a id="_idTextAnchor015"/>1</h1>
<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/>The Foundation of Robotics and Artificial Intelligence</h1>
<p>In this book, I invite you to go on a journey with me to discover how to add <strong class="bold">Artificial Intelligence</strong> (<strong class="bold">AI</strong>) to a<a id="_idIndexMarker000"/> mobile robot. The basic difference between what I will <a id="_idIndexMarker001"/>call an <strong class="bold">AI robot</strong> and a more <strong class="bold">regular robot</strong> is the<a id="_idIndexMarker002"/> ability of the robot and its software to make decisions and to learn and adapt to its environment based on data from its sensors. To be a bit more specific, we are leaving the world of pre-coded robot design behind. Instead of programming all of the robot’s behaviors in advance, the robot, or more correctly, the robot software, will learn from examples we provide, or from interacting with the outside world. The robot software will not control its behavior as much as the data that we use to train the AI system will.</p>
<p>The AI robot will use its learning process to make predictions about the environment and how to achieve goals, and then use those predictions to create behavior. We will be trying out several forms of AI on our journey, including supervised and unsupervised learning, reinforcement learning, neural networks, and genetic algorithms. We will create a digital robot assistant that can talk and understand commands (and tell jokes), and we will create an <strong class="bold">Artificial Personality</strong> (<strong class="bold">AP</strong>) for our robot. We will learn how to teach our robot to navigate without a map, grasp objects by trial and error, and see in three dimensions.</p>
<p>In this chapter, we will cover the following key topics:</p>
<ul>
<li>The basic principles of robotics and AI</li>
<li>What is AI and autonomy (and what is it not)?</li>
<li>Are recent developments in AI anything new?</li>
<li>What is a robot?</li>
<li>Introducing our sample problem</li>
<li>When do you need AI for your robot?</li>
<li>Introducing the robot and our development environment</li>
</ul>
<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>Technical requirements</h1>
<p>The technical requirements for completing the tasks in this chapter are described in the <em class="italic">Preface</em> at the beginning of this book.</p>
<p>All of the code for this book is available on the GitHub repository, available at <a href="https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e/">https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e/</a>.</p>
<h1 id="_idParaDest-19"><a id="_idTextAnchor018"/>The basic principle of robotics and AI</h1>
<p>AI applied to <strong class="bold">robotics development</strong> requires<a id="_idIndexMarker003"/> a different set of skills from <a id="_idIndexMarker004"/>you, the<a id="_idIndexMarker005"/> robot designer or developer. You may have made robots before. You probably have a quadcopter or a 3D printer (which is, in fact, a robot). The familiar<a id="_idIndexMarker006"/> world of <strong class="bold">Proportional-Integral-Derivative</strong> (<strong class="bold">PID</strong>) controllers, sensor loops, and state machines is augmented by <strong class="bold">Artificial Neural Networks</strong> (<strong class="bold">ANNs</strong>), expert systems, genetic algorithms, and searching path planners. We want a robot that does not just react to its environment as a reflex action but has goals and intent – and can learn and adapt to the environment and is taught or trained rather than programmed. Some of the problems we can solve this way would be difficult, intractable, or impossible otherwise.</p>
<p>What we are going to do in this book is introduce a problem – picking up toys in a playroom – that we will use as our example throughout the book as we learn a series of techniques for applying AI to our robot. It is important to understand that, in this book, the journey is far more important than the destination. At the end of the book, you should have gained some important skills with broad applicability, not just learned how to pick up toys.</p>
<p>What we are going to do is first provide some tools and background to match the infrastructure that was used to develop the examples in the book. This is both to provide an even playing field and to not assume any practical knowledge on your part. To execute some of the advanced neural networks that we are going to build, we will use the GPUs in the Jetson.</p>
<p>In the rest of this chapter, we will discuss some basics about robotics and AI, and then proceed to develop two important tools that we will use in all of the examples in the rest of the book. We will introduce the concept of soft real-time control, and then provide a <a id="_idIndexMarker007"/>framework, or <a id="_idIndexMarker008"/>model, for creating autonomy for our robot<a id="_idIndexMarker009"/> called the <strong class="bold">Observe-Orient-Decide-Act</strong> (<strong class="bold">OODA</strong>) loop.</p>
<h1 id="_idParaDest-20"><a id="_idTextAnchor019"/>What is AI and autonomy (and what is it not)?</h1>
<p>What would<a id="_idIndexMarker010"/> be the definition of AI? In general, it means a machine that<a id="_idIndexMarker011"/> exhibits some characteristics of intelligence – thinking, reasoning, planning, learning, and adapting. It can also mean a software program that can simulate thinking or reasoning. Let’s try some examples: a robot that avoids obstacles by simple rules (if the obstacle is to the right, go left) is not AI. A program that learns, by example, to recognize a cat in a video is AI. A robot arm that is operated by a joystick does not use AI, but a robot arm that adapts to different objects in order to pick them up is an application of AI.</p>
<p>There are two defining characteristics of AI robots that you must be aware of. First of all, AI robots are primarily <strong class="bold">trained</strong> to perform tasks, by providing examples, rather than being programmed step by step. For example, we will teach the robot’s software to recognize toys – things we want it to pick up – by training a neural network with examples of what toys look like. We will provide a training set of pictures with the toys in the images. We will specifically annotate what parts of the images are toys, and the robot will learn from that. Then we will test the robot to see that it learned what we wanted it to, somewhat like a teacher would test a student. The second characteristic is <strong class="bold">emergent behavior</strong>, in <a id="_idIndexMarker012"/>which the robot exhibits evolving actions that were not explicitly programmed into it. We provide the robot with controlling software that is inherently non-linear and self-organizing. The robot may suddenly exhibit some bizarre or unusual reaction to an event or situation that might appear to be odd, quirky, or even emotional. I worked with a self-driving car that we swore had delicate sensibilities and moved very daintily, earning it the nickname <em class="italic">Ferdinand</em>, after the sensitive, flower-loving bull from a cartoon, which was strange in a nine-ton truck that appeared to like plants. These behaviors are just caused by interactions of the various software components and control algorithms and do not represent anything more than that.</p>
<p>One concept you will hear in AI circles is the <strong class="bold">Turing test</strong>. The Turing test was proposed by Alan Turing in 1950, in a paper entitled <em class="italic">Computing Machinery and Intelligence</em>. He postulated that a human interrogator would question a hidden, unseen AI system, along with another human. If the human posing the questions was unable to tell which person was the computer and which was the human, then that AI computer would pass the test. This test supposes that the AI would be fully capable of listening to a conversation, understanding the content, and giving the same sort of answers a person would. Current <strong class="bold">AI chatbots</strong> can <a id="_idIndexMarker013"/>easily pass the Turing test and you may have interacted several times this week with AI on the phone without realizing it.</p>
<p>One group from the <strong class="bold">Association for the Advancement of Artificial Intelligence</strong> (<strong class="bold">AAAI</strong>) proposed<a id="_idIndexMarker014"/> that a more suitable test for AI might be the assembly of flatpack furniture – using the supplied instructions. However, to date, no robot has passed this test.</p>
<p>Our objective in this book is not to pass the Turing test, but rather to take some novel approaches to solving problems using techniques in machine learning, planning, goal seeking, pattern recognition, grouping, and clustering. Many of these problems would be very difficult to solve any other way. AI software that could pass the Turing test would be an<a id="_idIndexMarker015"/> example of <strong class="bold">general AI</strong>, or a full, working intelligent artificial brain, and, just like you, general AI does not need to be specifically trained to solve any particular problem. To date, general AI has not been created, but what we do have is <strong class="bold">narrow AI</strong> or <a id="_idIndexMarker016"/>software that simulates thinking in a very narrow application, such as recognizing objects, or picking good stocks to buy.</p>
<p>While we are <em class="italic">not</em> building general AI in this book, that means we are not going to be worried about our creations developing a mind of their own or getting out of control. That comes from the<a id="_idIndexMarker017"/> realm<a id="_idIndexMarker018"/> of science fiction and bad movies, rather than the reality of computers today. I am firmly of the mind that anyone preaching about the <em class="italic">evils</em> of AI or predicting that robots will take over the world has likely not seen the dismal state of AI research in terms of solving general problems or creating something resembling actual intelligence.</p>
<h1 id="_idParaDest-21"><a id="_idTextAnchor020"/>Are recent developments in AI anything new?</h1>
<p><em class="italic">What has been is what will be, and what has been done is what will be done, and there is nothing new under the sun – Ecclesiastes 1:9, King </em><em class="italic">James Bible</em></p>
<p>The modern<a id="_idIndexMarker019"/> practice of AI is not new. Most of these techniques were developed in the 1960s and 1970s and fell out of favor because the computing machinery of the day was insufficient for the complexity of software or the number of calculations required. They only waited for computers to get bigger and for another very significant event – the invention of the <strong class="bold">internet</strong>. In previous decades, if you needed 10,000 digitized pictures of cats to compile a database to train a neural network, the task would be almost impossible – you could take a lot of cat pictures, or scan images from books. Today, a Google search for cat pictures returns 126,000,000 results in 0.44 seconds. Finding cat pictures, or anything else, is just a search away, and you have your training set for your neural network – unless you need to train on a very specific set of objects that don’t happen to be on the internet, as we will see in this book, in which case we will once again be taking a lot of pictures with another modern aid not found in the sixties, a digital camera. The happy combination of very fast computers, cheap, plentiful storage, and access to almost unlimited data of every sort has produced a renaissance in AI.</p>
<p>Another modern development has occurred on the other end of the computer spectrum. While anyone can now have what we would have called a supercomputer back in 2000 on their desk at home, the development of the smartphone has driven a whole series of innovations that are just being felt in technology. Your wonder of a smartphone has accelerometers and gyroscopes made of tiny silicon chips called <strong class="bold">Micro-Electromechanical Systems</strong> (<strong class="bold">MEMS</strong>). It also has a high-resolution but very small digital camera <a id="_idIndexMarker020"/>and a multi-core computer processor that takes very little power to run. It also contains (probably) three radios – a Wi-Fi wireless network, a cellular phone, and a Bluetooth transceiver. As good as these parts are at making your iPhone fun to use, they have also found their way into parts available for robots. That is fun for us because what used to be only available for research labs and universities is now for sale to individual users. If you happen to have a university or research lab or work for a technology company with multi-million-dollar development budgets, you will also learn something from this book, and find tools and ideas that hopefully will inspire your robotics creations or power new products with exciting capabilities.</p>
<p>Now that you’re<a id="_idIndexMarker021"/> familiar with the concept of AI for robotics, let’s look at what a robot actually is.</p>
<h1 id="_idParaDest-22"><a id="_idTextAnchor021"/>What is a robot?</h1>
<p>The word <strong class="bold">robot</strong> entered <a id="_idIndexMarker022"/>the modern language from the play <em class="italic">R.U.R</em> by the Czech author Karel Capek, which was published back in 1920. <em class="italic">Roboti</em> is a Czech word meaning <em class="italic">forced servitude</em>. In the play, an industrialist learns how to build artificial people – not mechanical, metal men, but made of flesh and blood, and coming from a factory fully grown. The English translation of the name <em class="italic">R.U.R</em> as <em class="italic">Rossum’s Universal Robots</em> introduced the word <em class="italic">robot</em> to the world.</p>
<p>For the purposes of this book, a robot is a machine that is capable of sensing and reacting to its environment, and that has some human- or animal-like function. We generally think of a robot as an automated, self-directing mobile machine that can interact with the environment. That is to say, a robot <a id="_idIndexMarker023"/>has a <strong class="bold">physical form</strong> and exhibits some form<a id="_idIndexMarker024"/> of <strong class="bold">autonomy</strong>, or the ability to make decisions for itself based on observation of the external environment.</p>
<p>Next, let’s discuss the problem we will be trying to solve in this book.</p>
<h1 id="_idParaDest-23"><a id="_idTextAnchor022"/>Our sample problem – clean up this room!</h1>
<p>In the course of this book, we <a id="_idIndexMarker025"/>will be using a single problem set that I feel most people can relate to easily, while still representing a real challenge for the most seasoned roboticist. We will be using AI and robotics techniques to pick up toys in my house after my grandchildren have visited. That sound you just heard was the gasp from the professional robotics engineers and researchers in the audience – this is a tough problem. Why is this a tough problem, and why is it ideal for this book?</p>
<p>Let’s discuss the problem and break it down a bit. Later, in <a href="B19846_02.xhtml#_idTextAnchor032"><em class="italic">Chapter 2</em></a>, we will do a full task analysis, learn how to write use cases, and create storyboards to develop our approach, but we can start here with some general observations.</p>
<p>Robotics designers first start with the environment – where does the robot work? We divide environments into two categories: structured and unstructured. A structured environment, like the playing field for a FIRST robotics competition (a contest for robots built by high school students in the US, where all of the playing field is known in advance), an assembly line, or a lab bench, has everything in an organized space. You might have heard the saying <em class="italic">“A place for everything and everything in its place”</em> – that is a <strong class="bold">structured environment</strong>. Another way to think about it is that we know in advance where everything<a id="_idIndexMarker026"/> is or is going to be. We know what color things are, where they are placed in space, and what shape they are. A name for this type of information is <em class="italic">a priori</em> knowledge – things we know in advance. Having advanced knowledge of the environment in robotics is sometimes absolutely essential. Assembly line robots expect parts to arrive in an exact position and orientation to be grasped and placed into position. In other words, we have arranged the world to suit the robot.</p>
<p>In the world of my house, this is simply not an option. If I could get my grandchildren to put their toys in exactly the same spot each time, then we would not need a robot for this task. We have a set of objects that are fairly fixed – we only have so many toys for them to play with. We occasionally add things or lose toys, or something falls down the stairs, but the toys are elements of a set of fixed objects. What they are not is positioned or oriented in any particular manner – they are just where they were left when the kids finished playing with them and went home. We also have a fixed set of furniture, but some<a id="_idIndexMarker027"/> parts move – the footstool or chairs can be moved around. This is an <strong class="bold">unstructured environment</strong>, where <a id="_idIndexMarker028"/>the robot and the software have to adapt, not the toys or furniture.</p>
<p>The problem is to have the robot drive around the room and pick up toys. Here are some objectives for this task:</p>
<ul>
<li>We want the<a id="_idIndexMarker029"/> user to <strong class="bold">interact</strong> with the robot by <strong class="bold">talking</strong> to it. We want the robot to understand what we want it to do, which is to say, what our intent is for the commands we are giving it.</li>
<li>Once commanded to start, the robot will have to <strong class="bold">identify an object</strong> as being a toy or not being a toy. We only want to pick up toys.</li>
<li>The robot must <strong class="bold">avoid hazards</strong>, the most important being the stairs going down to the first floor. Robots have a particular problem with negative obstacles (dropoffs, curbs, cliffs, stairs, etc.), and that is exactly what we have here.</li>
<li>Once the robot finds a toy, it has to determine how to <strong class="bold">pick the toy up</strong> with its robot arm. Can it grasp the object directly, or must it scoop the item up, or push it along? We expect that the robot will try different ways to pick up toys and may need several trial-and-error attempts.</li>
<li>Once the toy is picked up by the robot arm, the robot needs to <strong class="bold">carry the toy</strong> to a toy box. The robot must recognize the toy box in the room, remember where it is for repeat trips, and then position itself to place the toy in the box. Again, more than one attempt may be required.</li>
<li>After the toy is dropped off, the robot returns to <strong class="bold">patrolling the room</strong> looking for more toys. At some point, hopefully, all of the toys will be retrieved. It may have to ask us, the<a id="_idIndexMarker030"/> human, whether the room is acceptable, or whether it needs to continue cleaning.</li>
</ul>
<p>What will we learn from this problem? We will be using this backdrop to examine a variety of AI techniques and tools. The purpose of the book is to teach you how to develop AI solutions with robots. It is the process and the approach that is the critical information here, not the problem and not the robot I developed for the book. We will be demonstrating <a id="_idIndexMarker031"/>techniques for making a moving machine that can learn and adapt to its environment. I would expect that you will pick and choose which chapters to read and in which order, according to your interests and your needs, and as such, each of the chapters will be standalone lessons.</p>
<p>The first three chapters are foundation material that supports the rest of the book by setting up the problem and providing a firm framework to attach the rest of the material.</p>
<h2 id="_idParaDest-24"><a id="_idTextAnchor023"/>The basics of robotics</h2>
<p>Not all of the chapters or<a id="_idIndexMarker032"/> topics in this book are considered <em class="italic">classical</em> AI approaches, but they do represent different ways of approaching machine learning and decision-making problems. We will be exploring together the following topics:</p>
<ul>
<li><strong class="bold">Control theory and timing</strong>: We will build a firm foundation for robot control by understanding control theory and timing. We will be using a soft real-time control scheme with what I <a id="_idIndexMarker033"/>call a <strong class="bold">frame-based control loop</strong>. This technique has a fancy technical name – <strong class="bold">rate monotonic scheduling</strong> – but I think you will find the concept intuitive and easy to understand.</li>
<li><strong class="bold">OODA loop</strong>: At the most basic level, AI is a way for the robot to make decisions about its actions. We will introduce a model for decision-making that comes from the US Air Force, called the <strong class="bold">OODA loop</strong>. This <a id="_idIndexMarker034"/>describes how a robot (or a person) makes decisions. Our robot will have two of these loops, an inner loop <a id="_idIndexMarker035"/>or <strong class="bold">introspective loop</strong>, and an<a id="_idIndexMarker036"/> outward-looking <strong class="bold">environment sensor loop</strong>. The lower, inner loop takes priority over the slower, outer loop, just as the autonomic parts of your body (such as the heartbeat, breathing, and eating) take precedence over your task functions (such as going to work, paying bills, and mowing the yard). This makes our system a type<a id="_idIndexMarker037"/> of <strong class="bold">subsumption architecture</strong>, a biologically inspired control paradigm named by Rodney Brooks of <a id="_idIndexMarker038"/>MIT, one of the founders of iRobot and Rethink Robotics, and the designer of a robot named Baxter.</li>
</ul>
<div><div><img alt="Figure 1.1 – My version of the OODA loop" src="img/B19846_01_1.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – My version of the OODA loop</p>
<p class="callout-heading">Note</p>
<p class="callout">The OODA loop was invented by Col. John Boyd, a man also called <em class="italic">The Father of the F-16</em>. Col. Boyd’s ideas are still widely quoted today, and his OODA loop is used to describe robot AI, military planning, or marketing strategies with equal utility. OODA provides a model for how a thinking machine that interacts with its environment might work.</p>
<p>Our robot works not by simply following commands or instructions step by step but by setting goals and then working to achieve those goals. The robot is free to set its own path or determine how to get to its goal. We will tell the robot <em class="italic">pick up that toy</em> and the robot will decide which toy, how to get in range, and how to pick up the toy. If we, the human robot owner, instead tried to treat the robot as a teleoperated hand, we would have to give the robot many individual instructions, such as <em class="italic">move forward</em>, <em class="italic">move right</em>, <em class="italic">extend arm</em>, <em class="italic">and open hand</em>, each individually, and without giving the robot any idea why we were making those motions. In a goal-oriented structure, the robot will be aware of which objects are toys and which are not and it will know how to find the toy box and how to put toys in the box. This is the difference between an autonomous robot and a radio-controlled teleoperated device.</p>
<p>Before designing the specifics of our robot and its software, we have to match its capabilities to the environment and the problem it must solve. The book will introduce some tools for designing the robot and managing the development of the software. We will use two tools from <a id="_idIndexMarker039"/>the discipline of systems engineering to accomplish this – <strong class="bold">use cases</strong> and <strong class="bold">storyboards</strong>. I will make this process as streamlined as possible. More advanced types of systems engineering are used by NASA, aerospace, and automobile companies to design rockets, cars, and aircraft – this gives you a taste of those types of structured processes.</p>
<h2 id="_idParaDest-25"><a id="_idTextAnchor024"/>The techniques used in this book</h2>
<p>The following sections <a id="_idIndexMarker040"/>will each detail step-by-step<a id="_idIndexMarker041"/> examples of applying AI techniques to a robotics problem:</p>
<ul>
<li>We start<a id="_idIndexMarker042"/> with <strong class="bold">object recognition</strong>. We need our robot to recognize objects, and then classify them as either <em class="italic">toys</em> to be picked up or <em class="italic">not toys</em> to be left alone. We will use a trained <strong class="bold">ANN</strong> to recognize<a id="_idIndexMarker043"/> objects from a video camera from various angles and lighting conditions. We will be using the process<a id="_idIndexMarker044"/> of <strong class="bold">transfer learning</strong> to extend an existing object recognition<a id="_idIndexMarker045"/> system, <strong class="bold">YOLOv8</strong>, to recognize our toys quickly and reliably.</li>
<li>The next task, once a toy is identified, is to pick it up. Writing a general-purpose <em class="italic">pick up anything</em> program for a robot arm is a difficult task involving a lot of higher mathematics (use the internet to look up <em class="italic">inverse kinematics</em> to see what I mean). What if we let the robot sort this out for itself? We use <strong class="bold">genetic algorithms</strong> that <a id="_idIndexMarker046"/>permit the robot to invent its own behaviors and learn to use its arm on its own. Then we will employ <strong class="bold">deep reinforcement learning</strong> (<strong class="bold">DRL</strong>) to let <a id="_idIndexMarker047"/>the robot teach itself how to grasp various objects using an <strong class="bold">end effector</strong> (robot <a id="_idIndexMarker048"/>speak for a hand).</li>
<li>Our robot needs to understand commands and instructions from its owner (us). We use <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) to not<a id="_idIndexMarker049"/> just recognize speech but to <a id="_idIndexMarker050"/>understand <a id="_idIndexMarker051"/>our intent for the robot to create goals consistent with what we want it to do. We use a neat technique that I call the <em class="italic">fill in the blank</em> method to allow the robot to reason from the context of a command. This process is useful for a lot of robot planning tasks.</li>
<li>The robot’s next problem is navigating rooms while avoiding the stairs and other hazards. We will use a combination of a unique, mapless navigation technique with 3D vision provided by a special stereo camera to see and avoid obstacles.</li>
<li>The robot will need to be able to find the toy box to put items away, as well as have a general framework for planning moves in the future. We will use <strong class="bold">decision trees</strong> for <a id="_idIndexMarker052"/>path planning, as well as discussing <strong class="bold">pruning</strong> or<a id="_idIndexMarker053"/> quickly rejecting bad plans. If you imagine what a computer chess program algorithm must do, looking several moves ahead and scoring good moves versus bad moves before selecting a strategy, that will give you an idea of the power of this technique. This type of decision tree has many uses and can handle many dimensions of strategies. We’ll be using it as one of two ways to find a path to our toy box to put toys away.</li>
<li>Our final task brings a different set of tools not normally used in robotics, or at least not the way we are going to employ them.<p class="list-inset">I have five wonderful, talented, and delightful grandchildren who love to come and visit. You’ll be hearing a lot about them throughout the book. The oldest grandson is 10 years old, and autistic, as is my granddaughter, the third child, who is 8, as well as the youngest boy, who is 6 as I write this. I introduced my eldest grandson, William, to the robot – and he immediately wanted to have a conversation with it. He asked, <em class="italic">“What’s your name?”</em> and <em class="italic">“What do you do?”</em> He was disappointed when the robot made no reply. So for the grandkids, we will be developing an engine for the robot to carry out a short conversation – we will be creating a robot personality to interact with children. William had one more request for this robot – he wants it to tell and respond to <em class="italic">knock, knock</em> jokes, so we will use that as a prototype of special dialog.</p></li>
</ul>
<p>While developing a robot with actual feelings is far beyond the state of the art in robotics or AI today, we can simulate having a personality with a finite state machine and some Monte Carlo modeling. We will also give the robot a model for human interaction so that the robot will take into account the child’s mood as well. I like to call this type of<a id="_idIndexMarker054"/> software <a id="_idIndexMarker055"/>an <strong class="bold">AP</strong> to distinguish it from our<a id="_idIndexMarker056"/> AI. AI builds a model of thinking, and an AP builds a model of emotion for our robot.</p>
<p>Now that you’re familiar with the problem we will be addressing in this book, let’s briefly discuss when and why you might need AI for your robot.</p>
<h1 id="_idParaDest-26"><a id="_idTextAnchor025"/>When do you need AI for your robot?</h1>
<p>We generally <a id="_idIndexMarker057"/>describe AI as a technique for <a id="_idIndexMarker058"/>modeling or simulating processes that emulate how our brains make decisions. Let’s discuss how AI can be used in robotics to provide capabilities that may be difficult for <em class="italic">traditional</em> programming techniques to achieve. One of those is identifying objects in images or pictures. If you connect a camera to a computer, the computer receives not an image, but an array of numbers that represent pixels (picture elements). If we are trying to determine whether a certain object, say a toy, is located in the image, then this can be quite tricky. You can find shapes, such as circles or squares, but a teddy bear? Moreover, what if the teddy bear is upside down, or lying flat on a surface? This is the sort of problem that an AI program can solve when nothing else can.</p>
<p>Our traditional approach for creating robot behaviors is to figure out what function we want and to write code to make that happen. When we have a simple function, such as driving around an obstacle, then this approach works well, and we can get results with a little tuning.</p>
<p>Some examples of AI and <strong class="bold">ML</strong> for robotics include:</p>
<ul>
<li><strong class="bold">NLP</strong>: Using AI/ML to<a id="_idIndexMarker059"/> allow the robot to understand and respond to natural human speech and commands. This makes interacting with the robot much more intuitive.</li>
<li><strong class="bold">Computer vision</strong>: Using AI to let the robot see and recognize objects or people’s faces, read text, and so on. This helps the robot operate in real-world environments.</li>
<li><strong class="bold">Motion planning</strong>: AI can help the robot plan optimal paths and motions to navigate around obstacles and people. This makes the robot’s movements more efficient and human-like.</li>
<li><strong class="bold">Reinforcement learning</strong>: The robot can learn how to do, and improve at doing, tasks through trial and error using AI reinforcement learning algorithms. This means less explicit programming is needed.</li>
</ul>
<p>The main rule of thumb is to use AI/ML whenever you want the robot to perform robustly in a complex, dynamic real-world environment. The AI gives it more perceptual and decision-making capabilities.</p>
<p>Now let’s look at one function we need for this robot – recognizing that an object is either a toy (and needs to be picked up) or is not. Creating a standard function for this via programming is quite difficult. Regular computer vision processes separate an image into shapes, colors, or areas. Our problem is the toys don’t have predictable shapes (circles, squares, or triangles), they don’t have consistent colors, and they are not all the same size. What we would rather do is to teach the robot what is a toy and what is not. That is what we would do with a person. We just need a process for teaching the robot how to use a camera to recognize a particular object. Fortunately, this is an area of AI that has been deeply studied, and there are already techniques to accomplish this, which we will use in <a href="B19846_04.xhtml#_idTextAnchor126"><em class="italic">Chapter 4</em></a>. We will <a id="_idIndexMarker060"/>use a <strong class="bold">convolutional neural network</strong> (<strong class="bold">CNN</strong>) to recognize toys from camera images. This is a type<a id="_idIndexMarker061"/> of <strong class="bold">supervised learning</strong>, where we use examples to show the software what type of object we want to recognize, and then create a customized function that <em class="italic">predicts</em> the class (or type) of object based on the pixels that represent it in an image. One of the principles of AI that we will be applying is <strong class="bold">gradual learning</strong> using <strong class="bold">gradient descent</strong>. This <a id="_idIndexMarker062"/>means that instead of trying to make the <a id="_idIndexMarker063"/>computer learn a skill all in one go, we will train it a little bit at a time, gently training <a id="_idIndexMarker064"/>a<a id="_idIndexMarker065"/> function to output what we want by looking at errors (or loss) and making small changes. We use the principle of gradient descent – looking at the slope of the change in errors – to determine which way to adjust the training.</p>
<p>You may be thinking at this point, <em class="italic">“If that works for learning to classify pictures, then maybe it can be used to classify other things,"</em> and you would be right. We’ll use a similar approach – with somewhat different neural networks – to teach the robot to answer to its name, by recognizing the sound.</p>
<p>So, in general, when do we need to use AI in a robot? When we need to emulate some sort of decision-making process that would be difficult or impossible to create with procedural steps (i.e., programming). It’s easy to see that neural networks are emulations of animal thought processes since they are a (greatly) simplified model of how neurons interact. Other AI techniques can be more difficult to understand.</p>
<p>One common theme could be that AI consistently uses <em class="italic">programming by example</em> as a technique to replace code with a common framework and variables with data. Instead of <em class="italic">programming by process</em>, we are programming by showing the software what result we want and having the software come up with how to get to that result. So for <strong class="bold">object recognition</strong> using <a id="_idIndexMarker066"/>pictures, we provide pictures of objects <em class="italic">and</em> the answer to what kind of object is represented by the picture. We repeat this over and over and train the software – by modifying the parameters in the code.</p>
<p>Another type of behavior we can create with AI has to do with behaviors. There are a lot of tasks that can be thought of as games. We can easily imagine how this works. Let’s say you want your children to pick up the toys in their room. You could command them to do it – which may or may not work. Or, you could make it a game by awarding points for each toy picked up, and giving a reward (such as giving a dollar) based on the number of points scored. What did we add by doing this? We added a <strong class="bold">metric</strong>, or measurement tool, to let the children know how well they are doing – a point system. And, more critically, we added a reward for specific behaviors. This can be a process we can use to modify or create behaviors in a robot. This is formally called <strong class="bold">reinforcement learning</strong>. While <a id="_idIndexMarker067"/>we can’t give a robot an emotional reward (as robots don’t have wants or needs), we can program the robot to seek to maximize a reward function. Then we can use the same process of making a small adjustment in parameters that change the reward, see whether that improves the score, and then either keep that change (when learning results in more reward, our reinforcement) or discard it if the score goes down. This type of process works well for robot motion, and for controlling robot arms.</p>
<p>I must tell you that the task set out in this book – to pick up toys in an unstructured environment – is nearly impossible to perform without AI techniques. It could be done by modifying the environment – say, by putting RFID tags in the toys – but not otherwise. That, then, is<a id="_idIndexMarker068"/> the purpose of this book – to <a id="_idIndexMarker069"/>show how certain tasks, which are difficult or impossible to solve otherwise, can be completed using the combination of AI and robotics.</p>
<p>Next, let’s discuss our robot and the development environment that we’ll be using in this book.</p>
<h1 id="_idParaDest-27"><a id="_idTextAnchor026"/>Introducing the robot and our development environment</h1>
<p>This is a book <a id="_idIndexMarker070"/>about <a id="_idIndexMarker071"/>robots and AI, so we really need to have a robot to use for all of our practical examples. As we will discuss in <a href="B19846_02.xhtml#_idTextAnchor032"><em class="italic">Chapter 2</em></a> at some length, I have selected robot hardware and software that will be accessible to the average reader. The particular brand and type are not important, and I’ve upgraded Albert considerably since the first edition was published some five years ago. In the interest of keeping things up to date, we are putting all of the hardware details in the GitHub repository for this book.</p>
<p>As shown in the following photographs taken from two different perspectives, my robot has new omnidirectional wheels, a mechanical six-degree-of-freedom arm, and a computer brain:</p>
<div><div><img alt="Figure 1.2 – Albert the robot has wheels and a mechanical arm" src="img/B19846_01_2.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – Albert the robot has wheels and a mechanical arm</p>
<p>I’ll call it <em class="italic">Albert</em>, since it needs some sort of name, and I like the reference to Prince Albert, consort of Queen Victoria, who was famous for taking marvelous care of their nine children. All nine of<a id="_idIndexMarker072"/> his <a id="_idIndexMarker073"/>children survived to adulthood, which was a rarity in the Victorian age, and he had 42 grandchildren. He went by his middle name; his actual first name was Francis.</p>
<p>Our tasks in this book center around picking up toys in an interior space, so our robot has a solid base with four motors and omni wheels for driving over carpet. Our steering method is the tank type, or differential drive, where we steer by sending different commands to the wheel motors. If we want to go straight ahead, we set all four motors to the same forward speed. If we want to travel backward, we reverse both motors the same amount. Turns are accomplished by moving one side forward and the other backward (which makes the robot turn in place) or by giving one side more forward drive than the other. We can make any sort of turn this way. The omni wheels allow us to do some other tricks as well – we can turn the wheels toward each other and translate directly sideways, and even turn in a circle while pointing at the same spot on the ground. We will mostly drive like a truck or car but will use the <em class="italic">y</em>-axis motion occasionally to line things up. Speaking of axes, I’ll use the <em class="italic">x</em> axis to mean that the robot will move straight ahead, the <em class="italic">y</em> axis refers to horizontal movement from side to side, and the <em class="italic">z</em> axis is up and down, which we need for the robot’s arm.</p>
<p>In order to pick up toys, we need some sort of manipulator, so I’ve included a six-axis robot arm that imitates a shoulder–elbow–wrist–hand combination that is quite dexterous and, since it is made out of standard digital servos, quite easy to wire and program.</p>
<p>The main control of the Albert robot is the Nvidia Nano <strong class="bold">single-board computer</strong> (<strong class="bold">SBC</strong>), which <a id="_idIndexMarker074"/>talks to the operator via a USB Wi-Fi dongle. The Nvidia talks to an Arduino Mega 2560 microcontroller and motor controller that we will use to control <a id="_idIndexMarker075"/>motors via <strong class="bold">Pulse Width Modulation</strong> (<strong class="bold">PWM</strong>) pulses. The following figure depicts the internal<a id="_idIndexMarker076"/> components <a id="_idIndexMarker077"/>of the robot:</p>
<div><div><img alt="Figure 1.3 – Block diagram of the robot" src="img/B19846_01_3.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – Block diagram of the robot</p>
<p>We will be primarily concerned with the Nvidia Nano SBC, which is the brains of our robot. We will set up the rest of the components once and not change them for the entire book.</p>
<p>The Nvidia Nano acts as the main interface between our control station, which is a PC running Windows, and the robot itself via a Wi-Fi network. Just about any low-power, Linux-based SBC can perform this job, such as a BeagleBone Black, Odroid XU4, or an Intel Edison. One of the advantages of the Nano is that it can use its <strong class="bold">Graphics Processing Units</strong> (<strong class="bold">GPUs</strong>) to <a id="_idIndexMarker078"/>speed up the processing of neural networks.</p>
<p>Connected to the SBC is an Arduino with a motor controller. The Nano talks through a USB port addressed as a serial port. We also need a 5V regulator to provide the proper power from the 11.1V rechargeable lithium battery power pack into the robot. My power pack is a rechargeable 3S1P (three cells in series and one in parallel) 2700 Ah battery (normally used for quadcopter drones) and came with the appropriate charger. As with any lithium battery, follow all of the directions that come with the battery pack and recharge it in a <a id="_idIndexMarker079"/>metal <a id="_idIndexMarker080"/>box or container in case of fire.</p>
<h2 id="_idParaDest-28"><a id="_idTextAnchor027"/>Software components (ROS, Python, and Linux)</h2>
<p>I am going to direct <a id="_idIndexMarker081"/>you once again to the Git repository to see all of the software that runs the robot, but I’ll cover the basics here to remind you. The base operating system for the robot is Linux running on an Nvidia Nano SBC, as we said. We are using the ROS 2 to connect all of our various software components together, and it also does a wonderful job of taking care of all of the finicky networking tasks such as setting up sockets and establishing connections. It also comes with a great library of already prepared functions that we can just take advantage of, such as a joystick interface. ROS 2 is not a true operating system that controls the whole computer like Linux or Windows does, but rather is a backbone of communications and interface standards and utilities that make putting together a robot a lot simpler. The name I like to use for this type of system is <strong class="bold">Modular Open System Architecture</strong> (<strong class="bold">MOSA</strong>). ROS 2 uses a <em class="italic">publish/subscribe</em> technique<a id="_idIndexMarker082"/> to move data from one place to another that truly decouples the programs that produce data (such as sensors and cameras) from those programs that use data, such as controls and displays. We’ll be making a lot of our own stuff and only using a few ROS functions. Packt has several great books for learning ROS; my favorite is <em class="italic">Effective Robotics Programming </em><em class="italic">with ROS</em>.</p>
<p>The programming language we will use throughout this book, with a couple of minor exceptions, will<a id="_idIndexMarker083"/> be <strong class="bold">Python</strong>. Python is a great language for this purpose for two great reasons: it is widely used in the robotics community in conjunction with ROS, and it is also widely accepted in the machine learning and AI community. This double whammy makes using Python irresistible. Python is an interpreted language, which has three amazing advantages for us:</p>
<ul>
<li><strong class="bold">Portability</strong>: Python is<a id="_idIndexMarker084"/> very portable between Windows, Mac, and Linux. Usually, you can get by with just a line or two of changes if you use a function out of the operating system, such as opening a file. Python has access to a huge collection of C/C++ libraries that also add to its utility.</li>
<li><strong class="bold">No compilation</strong>: As an interpreted language, Python does not require a compile step. Some of the programs we are developing in this book are pretty involved, and if we wrote them in C or C++, it would take 10 or 20 minutes of build time each time we made a change. You can do a lot with that much time, which you can spend getting your program to run and not waiting for the <em class="italic">make</em> process to finish.</li>
<li><strong class="bold">Isolation</strong>: This is a benefit that does not get talked about much but having had a lot of experience with crashing operating systems with robots, I can tell you that the fact that Python’s interpreter is isolated from the core operating system means that having one of your Python ROS programs crash the computer is very rare. A computer crash means rebooting the computer and also probably losing all of the data you need to diagnose the crash. I had a professional robot project that we moved from Python to C++, and immediately the operating system crashes began, which shot the reliability of our robot. If a Python program crashes, another program can monitor that and restart it. If the operating system has crashed, there is not much you can do without some extra hardware that can push the <em class="italic">Reset</em> button for you.</li>
</ul>
<p>Before we dive into the coding of our base control system, let’s talk about the theory we will use to create a robust, modular, and flexible control system for robotics.</p>
<h2 id="_idParaDest-29"><a id="_idTextAnchor028"/>Robot control systems and a decision-making framework</h2>
<p>As I mentioned <a id="_idIndexMarker085"/>earlier in<a id="_idIndexMarker086"/> this chapter, we are going to use two sets of tools in the sections: <strong class="bold">soft real-time control</strong> and the <strong class="bold">OODA loop</strong>. One<a id="_idIndexMarker087"/> gives us a base for controlling<a id="_idIndexMarker088"/> the robot easily and consistently, and the other provides a basis for the robot’s autonomy.</p>
<h3>How to control your robot</h3>
<p>The basic concept of<a id="_idIndexMarker089"/> how a robot works, especially one that drives, is simple. There is a master control loop that does the same thing over and over – reads data from the sensors and motor controller, looks for commands from the operator (or the robot’s autonomy functions), makes any changes to the state of the robot based on those commands, and then sends instructions to the motors or effectors to make the robot move.</p>
<div><div><img alt="Figure 1.4 – Robot control loop" src="img/B19846_01_4.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – Robot control loop</p>
<p>The preceding diagram illustrates how we have instantiated the OODA loop in the software and hardware of our robot. The robot can either act autonomously or accept commands from a control station connected via a wireless network.</p>
<p>What we need to do is perform this control loop in a consistent manner all of the time. We need to set a base frame rate or basic update frequency that sets the timing of our control loop. This makes all the systems of the robot perform together. Without some sort of time manager, each control cycle of the robot takes a different amount of time to complete, and any sort of path planning, position estimate, or arm movement becomes very complicated. ROS<a id="_idIndexMarker090"/> does not provide a time manager as it is inherently non-synchronous; if required, we have to create one ourselves.</p>
<h3>Using control loops</h3>
<p>In order to have<a id="_idIndexMarker091"/> control of our robot, we have to establish some sort of control or feedback loop. Let’s say that we tell the robot to move 12 inches (30 cm) forward. The robot must send a command to the motors to start moving forward, and then have some sort of mechanism to measure 12 inches of travel. We can use several means, but let’s just use a clock. The robot moves 3 inches (7.5 cm) per second. We need the control loop to start the movement, and then each update cycle, or time through the loop, check the time and see whether four seconds have elapsed. If they have, then it sends a <em class="italic">stop</em> command to the motors. The timer is the <em class="italic">control</em>, four seconds is the <em class="italic">set point</em>, and the motor is the <em class="italic">system</em> that is controlled. The process also generates an error signal that tells us what control to apply (in this case, to stop). Let’s look at a simple control loop:</p>
<div><div><img alt="Figure 1.5 – Sample control loop – maintaining the temperature of a pot of water" src="img/B19846_01_5.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.5 – Sample control loop – maintaining the temperature of a pot of water</p>
<p>Based on the preceding figure, what we want is a constant temperature<a id="_idIndexMarker092"/> in the <strong class="bold">pot of water</strong>. The <strong class="bold">valve</strong> controls<a id="_idIndexMarker093"/> the heat produced by the <strong class="bold">fire</strong>, which warms the <strong class="bold">pot of water</strong>. The <strong class="bold">temperature sensor</strong> detects whether<a id="_idIndexMarker094"/> the water is too cold, too hot, or just right. The controller uses this information to control the valve for more heat. This type of schema is <a id="_idIndexMarker095"/>called a <strong class="bold">closed loop </strong><strong class="bold">control system</strong>.</p>
<p>You can think of this also in terms of a process. We start the process, and then get feedback to show our progress so that we know when to stop or modify the process. We could be doing speed control, where we need the robot to move at a specific speed, or pointing control, where the robot aims or turns in a specific direction.</p>
<p>Let’s look at another example. We have a robot with a self-charging docking station, with a set <a id="_idIndexMarker096"/>of <strong class="bold">light-emitting diodes</strong> (<strong class="bold">LEDs</strong>) on the top as an optical target. We want the robot to drive straight into the docking station. We use the camera to see the target LEDs on the docking station. The camera generates an error signal, which is used to guide the robot toward <a id="_idIndexMarker097"/>the LEDs. The distance between the LEDs also gives us a rough range to the dock. This process is illustrated in the following figure:</p>
<div><div><img alt="Figure 1.6 – Target tracking for a self-docking charging station" src="img/B19846_01_6.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.6 – Target tracking for a self-docking charging station</p>
<p>Let’s understand this in some more detail:</p>
<ul>
<li>Let’s say that the LEDs in the figure are off to the left of the center 50% and the distance from the robot to the target is 3 feet (1 m). We send that information through a control loop to the motors – turn left a bit and drive forward a bit.</li>
<li>We then check again, and the LEDs are closer to the center (40%) and the distance to the target is 2.9 feet or 90 cm. Our error signal is a bit less, and the distance is a bit less. We’ll have to<a id="_idIndexMarker098"/> develop a <strong class="bold">scaling factor</strong> to determine how many pixels equate to how much <strong class="bold">turn rate</strong>, which is measured as a percentage of full power. Since we are using a fixed camera and lens, this will be a constant.</li>
<li>Now we send a slower turn and a slower movement to the motors this update cycle. We end up exactly in the center and come to zero speed just as we touch the docking station.</li>
</ul>
<p>For those people currently saying, <em class="italic">“But if you use a PID controller …”</em>, yes, you are correct – you also know that I’ve just described a <em class="italic">P</em> or <em class="italic">proportional</em> control scheme. We can add more bells and whistles to help prevent the robot from overshooting or undershooting the target <a id="_idIndexMarker099"/>due to its own weight and inertia and to damp out oscillations caused by those overshoots.</p>
<p>A <strong class="bold">PID controller</strong> is a <a id="_idIndexMarker100"/>type of control system that uses three types of inputs to manage a closed-loop control<a id="_idIndexMarker101"/> system. A <strong class="bold">proportional control</strong> uses a multiple of the detected error to drive a control.</p>
<p>For example, in our pot of water, we measure the error in the temperature. If the desired temperature is 100°C and we measure 90°C with our thermometer, then the error in the temperature is 10 °C. We need to add more heat by opening the valve in proportion to the temperature error. If the error is 0, the change in the value is 0. Let’s say that we try changing the value of the valve by 10% for a 10°C error. So we multiply 10°C by 0.01 to set our valve position to +0.1. This 0.01 value is<a id="_idIndexMarker102"/> our <em class="italic">P</em> term or <strong class="bold">proportional constant</strong>.</p>
<p>In our next sample, we see that the temperature of our pot is now 93°C and our error is 7°C. We change our valve position to +0.07, slightly less than before. We will probably find that by using this method, we will overshoot the desired temperature due to the hysteresis of the water – it takes a while for the water to heat up, creating a delay in the response. We will end up overheating the water and overshooting our desired temperature. One way to help prevent that is with the <strong class="bold">D</strong> term of the PID controller, that is, a <strong class="bold">derivative</strong> term. You remember that a derivative describes the slope of the line of a function – in this case, the temperature curve we measure. The <em class="italic">y</em> axis of our temperature graph is time, so we have <em class="italic">delta temperature/delta time</em>. To add a <em class="italic">D</em> term to our controller, we also add in the difference between the error of the last sample and the error of this sample (<em class="italic">-10 – (-7) = -3</em>). We add this to our control by multiplying this value times a constant, <em class="italic">D</em>. The integral term is just the cumulative sum of the error multiplied by a constant we’ll call <em class="italic">I</em>. We can modify the <em class="italic">P</em>, <em class="italic">I</em>, and <em class="italic">D</em> constants to adjust (tune) our PID controller to provide the proper response for our control loop – with no overshoots, undershoots, or drifts. More explanation is available at <a href="https://jjrobots.com/pid/">https://jjrobots.com/pid/</a>. The point of these examples is to point out the concept of control in a machine – we have to take measurements, compare them to our desired result, compute the error signal, and then make any corrections to the controls over and over many times a<a id="_idIndexMarker103"/> second, and doing that consistently is the concept of real-time control.</p>
<h4>Types of control loops</h4>
<p>In order to perform <a id="_idIndexMarker104"/>our control loop at a consistent time interval (or to use the proper term, deterministically), we have two ways of controlling our program <a id="_idIndexMarker105"/>execution: <strong class="bold">soft real time</strong> and <strong class="bold">hard real time</strong>. Hard<a id="_idIndexMarker106"/> real-time control systems require assistance from the hardware of the computer – that is where the <em class="italic">hard</em> part of the title comes from. Hard real time generally<a id="_idIndexMarker107"/> requires a <strong class="bold">real-time operating system</strong> (<strong class="bold">RTOS</strong>) or complete control over all of the computer cycles in the processor. The problem we are faced with is that a computer running an operating system is constantly getting interrupted by other processes, chaining threads, switching contexts, and performing tasks. Your experience with desktop computers, or even smartphones, is that the same process, such as starting up a word processor program, always seems to take a different amount of time whenever you start it up.</p>
<p>This sort of behavior is intolerable in a real-time system where we need to know in advance exactly how long a process will take down to the microsecond. You can easily imagine the problems if we created an autopilot for an airliner that, instead of managing the aircraft’s direction and altitude, was constantly getting interrupted by disk drive access or network calls that played havoc with the control loops giving you a smooth ride or making a touchdown on the runway.</p>
<p>An RTOS system allows the programmers and developers to have complete control over when and how the processes execute and which routines are allowed to interrupt and for how long. Control loops in RTOS systems always take the exact same number of computer cycles (and thus time) every loop, which makes them reliable and dependable when the output is critical. It is important to know that in a hard real-time system, the hardware enforces timing constraints and makes sure that the computer resources are available when they are needed.</p>
<p>We can actually do hard real time in an Arduino microcontroller because it has no operating system and can only do one task at a time or run only one program at a time. Our robot will also have a more capable processor in the form of an Nvidia Nano running Linux. This computer, which has some real power, does a number of tasks simultaneously to support the operating system, run the network interface, send graphics to the output HDMI port, provide a user interface, and even support multiple users.</p>
<p>Soft real time is a bit more of a relaxed approach, and is more appropriate to our playroom-cleaning robot than a safety-critical hard real-time system – plus, RTOSs can be expensive (there are open source versions) and require special training for you. What we are going to do is treat our control loop as a feedback system. We will leave some extra room – say about 10% – at the end of each cycle to allow the operating system to do its work, which should leave us with a consistent control loop that executes at a constant time interval. Just like our control loop example that we just discussed, we will take a measurement, determine the error, and apply a correction to each cycle.</p>
<p>We are not just worried about our update rate. We also must worry<a id="_idIndexMarker108"/> about <strong class="bold">jitter</strong>, or random variability in the timing loop caused by the operating system getting interrupted and doing other things. An interrupt will cause our timing loop to take longer, causing a random jump in our<a id="_idIndexMarker109"/> cycle time. We have to design our control loops to handle a certain amount of jitter for soft real time, but these are comparatively infrequent events.</p>
<h4>Running a control loop</h4>
<p>The process of running <a id="_idIndexMarker110"/>a control loop is fairly simple in practice. We start by initializing our timer, which needs to be the high-resolution clock. We are writing our control loop in Python, so we will use the <code>time.time()</code> function, which is specifically designed to measure our internal program timing performance (set frame rate, do loop, measure time, generate error, sleep for error, loop). Each time we call <code>time.time()</code>, we get a floating-point number, which is the number of seconds from the Unix clock and has microsecond resolution on the Nvidia Nano.</p>
<p>The concept for this process is to divide our processing into a set of fixed time intervals we will <a id="_idIndexMarker111"/>call <strong class="bold">frames</strong>. Everything we do will fit within an integral number of frames. Our basic running speed will process 30 <strong class="bold">frames per second</strong> (<strong class="bold">fps</strong>). That is how fast we will be updating the robot’s position estimate, reading sensors, and sending commands to motors. We have other functions that run slower than the 30 frames, so we can divide them between frames in even multiples. Some functions run every frame (30 fps) and are called and executed every frame.</p>
<p>Let’s say that we have a sonar sensor that can only update 10 times a second. We call the <em class="italic">read sonar</em> function every third frame. We assign all our functions to be some multiple of our basic 30 fps frame rate, so we have 30, 15, 10, 7.5, 6, 5, 4.28, 2, and 1 fps if we call the functions every frame, every second frame, every third frame, and so on. We can even do less than 1 fps – a function called every 60 frames executes once every 2 seconds.</p>
<p>The tricky bit is we <a id="_idIndexMarker112"/>need to make sure that each process fits into one frame time – which is 1/30 of a second or 0.033 seconds or 33 milliseconds. If the process takes longer than that, we have to either divide it up into parts or run it in a separate thread or program where we can start the process in one frame and get the result in another. It is also important to try and balance the frames so that not all processing lands in the same frame. The following figure shows a task scheduling system based on a 30 fps basic rate. Here, we have four tasks to take care of: task <em class="italic">A</em> runs at 15 fps, task <em class="italic">B</em> runs at 6 fps (every five frames), task <em class="italic">C</em> runs at 10 fps (every three frames), and task <em class="italic">D</em> runs at 30 fps (every frame):</p>
<div><div><img alt="Figure 1.7 – Frame-based task schedule" src="img/B19846_01_7.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.7 – Frame-based task schedule</p>
<p>Our first pass (the top of the figure) at the schedule has all four tasks landing on the same frame at frames 1, 13, and 25. We can improve the balance of the load on the control program if we delay the start of task <em class="italic">B</em> on the second frame as shown in the bottom half of the diagram.</p>
<p>This is akin to how measures in music work, where a measure is a certain amount of time, and different<a id="_idIndexMarker113"/> notes have different intervals – one whole note can only appear once per measure, a half note can appear twice, all the way down to 64th notes. Just like a composer makes sure that each measure has the right number of beats, we can make sure that our control loop has a balanced measure of processes to execute each frame.</p>
<p>Let’s start by writing a little program to control our timing loop and to let you play with these principles.</p>
<p>This is exciting – our first bit of coding together. This program just demonstrates the timing control loop we are going to use in the main robot control program and is here to let you play around with some parameters and see the results. This is the simplest version I think is possible of a soft time-controlled loop, so feel free to improve and embellish it. I’ve made you a flowchart to help you understand this a little better:</p>
<div><div><img alt="Figure 1.8 – Flowchart of soft real-time controller" src="img/B19846_01_8.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.8 – Flowchart of soft real-time controller</p>
<p>Let’s look more <a id="_idIndexMarker114"/>closely at the terms used in the preceding diagram:</p>
<ul>
<li><strong class="bold">FrameTime</strong>: The time we have allocated to execute one iteration of the loop</li>
<li><strong class="bold">StartTime</strong>: When the loop/frame begins</li>
<li><strong class="bold">Do a Bunch of Math</strong>: The program that you are managing</li>
<li><strong class="bold">StopTime</strong>: When the frame completes</li>
<li><strong class="bold">Remaining Time</strong>: The difference between the elapsed time and the desired frame time</li>
<li><strong class="bold">Elapsed Time</strong>: The time it takes to actually run through the loop once</li>
<li><strong class="bold">Frame Sleep Time</strong>: We use <strong class="bold">Remaining Time</strong> to tell the computer to sleep so that the frame takes exactly the amount of time we want.</li>
</ul>
<p>Now we’ll begin with coding. This is pretty straightforward Python code – we won’t get fancy until later:</p>
<ol>
<li>We start by importing our libraries. It is not surprising that we start with the <code>time</code> module. We also will use the <code>mean</code> function from <code>numpy</code> (Python numerical analysis) and <code>matplotlib</code> to draw our graph at the end. We will also be doing<a id="_idIndexMarker115"/> some math calculations to simulate our processing and create a load on the frame rate:<pre class="source-code">
import time
from numpy import mean
import matplotlib.pyplot as plt
import math
#</pre></li> <li>Now we have some parameters to control our test. This is where you can experiment with different timings. Our basic control is <code>FRAMERATE</code> – how many updates per second do we want to try? Let’s start with <code>30</code>, as we did in the example we discussed earlier:<pre class="source-code">
# set our frame rate - how many cycles per second to run our loop?
FRAMERATE = 30
# how long does each frame take in seconds?
FRAME = 1.0/FRAMERATE
# initialize myTimer
# This is one of our timer variables where we will store the clock time from the operating system.
myTimer = 0.0</pre></li> <li>The duration of the test is set by the <code>counter</code> variable. The time the test will take is the <code>FRAME</code> time times the number of cycles in <code>counter</code>. In our example, 2,000 frames divided by 30 fps is 66.6 seconds, or a bit over a minute to run the test:<pre class="source-code">
# how many cycles to test? counter*FRAME = runtime in seconds
counter = 2000</pre><p class="list-inset">We will be controlling our timing loop in two ways:</p><ul><li>We will first measure the amount of time it takes to perform the calculations for this frame. We have a stub of a program with some trigonometry functions we will call to put a load on the computer. Robot control functions, such as computing the angles needed in a robot arm, need lots of <a id="_idIndexMarker116"/>trig to work. This is available from <code>import math</code> in the header of the program.</li></ul></li> </ol>
<p class="callout-heading">Note</p>
<p class="callout">We will measure the time for our control function to run, which will take some part of our frame. We then compute how much of our frame remains, and tell the computer to sleep this process for the rest of the time. Using the <code>sleep</code> function releases the computer to go and take care of other business in the operating system, and is a better way to mark time rather than running a tight loop of some sort to waste the rest of our frame time.</p>
<ul>
<li>The second way we control our loop is by measuring the complete frame – compute time plus rest time – and looking to see whether we are over or under our frame time. We use <code>TIME_CORRECTION</code> for this function to trim our sleep time to account for variability in the sleep function and any delays getting back from the operating system:<pre class="source-code">
# factor for our timing loop computations
TIME_CORRECTION= 0.0</pre></li> </ul>
<ol>
<li value="4">We will collect some data to draw a jitter graph at the end of the program. We use the <code>dataStore</code> structure for this. Let’s put a header on the screen to tell you the program has begun, since it takes a while to finish:<pre class="source-code">
# place to store data
dataStore = []
# Operator information ready to go
# We create a heading to show that the program is starting its test
print "START COUNTING: FRAME TIME", FRAME, "RUN TIME:",FRAME*counter</pre></li> <li>In this step, we <a id="_idIndexMarker117"/>are going to set up some variables to measure our timing. As we mentioned, the objective is to have a bunch of compute frames, each the same length. Each frame has two parts: a <code>myTime</code> is the <em class="italic">top of frame</em> time, when the frame begins. <code>newTime</code> is the end of the work period timer. We use <code>masterTime</code> to compute the total time the program is running:<pre class="source-code">
# initialize the precision clock
 myTime = newTime = time.time()
 # save the starting time for later
 masterTime=myTime
 # begin our timing loop
 for ii in range(counter):</pre></li> <li>This section is our <strong class="bold">payload</strong> – the section of the code doing the work. This might be an arm angle calculation, a state estimate, or a command interpreter. We’ll stick in some trig functions and some math to get the CPU to do some work for us. Normally, this <em class="italic">working</em> section is the majority of our frame, so let’s repeat these math terms 1,000 times:<pre class="source-code">
    # we start our frame - this represents doing some detailed 
    math calculations
    # this is just to burn up some CPU cycles
    for jj in range(1000):
          x = 100
          y = 23 + ii
          z = math.cos(x)
          z1 = math.sin(y)
    #
    # read the clock after all compute is done
    # this is our working frame time
    #</pre></li> <li>Now we read the <a id="_idIndexMarker118"/>clock to find the working time. We can now compute how long we need to sleep the process before the next frame. The important part is that <em class="italic">working time + sleep time = frame time</em>. I’ll call this <code>timeError</code>:<pre class="source-code">
    newTime = time.time()
    # how much time has elapsed so far in this frame
    # time = UNIX clock in seconds
    # so we have to subract our starting time to get the elapsed
    time
    myTimer = newTime-myTime
    # what is the time left to go in the frame?
    timeError = FRAME-myTimer</pre><p class="list-inset">We carry forward some information from the previous frame here. <code>TIME_CORRECTION</code> is our adjustment for any timing errors in the previous frame time. We initialized it earlier to zero before we started our loop so we don’t get an undefined variable error here. We also do some range checking because we can get some large jitters in our timing caused by the operating system that can <a id="_idIndexMarker119"/>cause our sleep timer to crash if we try to sleep a negative amount of time:</p></li> </ol>
<p class="callout-heading">Note</p>
<p class="callout">We use the Python <code>max</code> function as a quick way to clamp the value of sleep time to be zero or greater. It returns the greater of two arguments. The alternative is something like <em class="italic">if a&lt; 0 : </em><em class="italic">a=0</em>.</p>
<pre class="source-code">
    # OK time to sleep
    # the TIME CORRECTION helps account for all of this clock
    reading
    # this also corrects for sleep timer errors
    # we are using a porpotional control to get the system to
    converge
    # if you leave the divisor out, then the system oscillates
    out of control
    sleepTime = timeError + (TIME_CORRECTION/2.0)
    # quick way to eliminate any negative numbers
    # which are possible due to jitter
    # and will cause the program to crash
    sleepTime=max(sleepTime,0.0)</pre> <ol>
<li value="8">So, here is our actual sleep command. The <code>sleep</code> command does not always provide a precise time interval, so we will be checking for errors:<pre class="source-code">
    # put this process to sleep
    time.sleep(sleepTime)</pre></li> <li>This is the time correction section. We figure out how long our frame time was in total (working and sleeping) and subtract it from what we want the frame time to be (<code>FrameTime</code>). Then we set our time correction to that value. I’m also going to save the measured frame time into a data store so we can graph how we did later using <code>matplotlib</code>. This technique is one of Python’s more useful features:<pre class="source-code">
    #print timeError,TIME_CORRECTION
    # set our timer up for the next frame
    time2=time.time()
    measuredFrameTime = time2-myTime
    ##print measuredFrameTime,
    TIME_CORRECTION=FRAME-(measuredFrameTime)
    dataStore.append(measuredFrameTime*1000)
    #TIME_CORRECTION=max(-FRAME,TIME_CORRECTION)
    #print TIME_CORRECTION
    myTime = time.time()</pre><p class="list-inset">This <a id="_idIndexMarker120"/>completes the looping section of the program. This example does 2,000 cycles of 30 frames a second and finishes in 66.6 seconds. You can experiment with different cycle times and frame rates.</p></li> <li>Now that we have completed the program, we can make a little report and a graph. We print out the frame time and total runtime, compute the average frame time (total time/counter), and display the average error we encountered, which we can get by averaging the data in <code>dataStore</code>:<pre class="source-code">
# Timing loop test is over - print the results
#
# get the total time for the program
endTime = time.time() - masterTime
# compute the average frame time by dividing total time by our number of frames
avgTime = endTime / counter
#print report
 print "FINISHED COUNTING"
 print "REQUESTED FRAME TIME:",FRAME,"AVG FRAME TIME:",avgTime
 print "REQUESTED TOTAL TIME:",FRAME*counter,"ACTUAL TOTAL TIME:", endTime
 print "AVERAGE ERROR",FRAME-avgTime, "TOTAL_ERROR:",(FRAME*counter) - endTime
 print "AVERAGE SLEEP TIME: ",mean(dataStore),"AVERAGE RUN TIME",(FRAME*1000)-mean(dataStore)
 # loop is over, plot result
 # this lets us see the "jitter" in the result
 plt.plot(dataStore)
 plt.show()</pre><p class="list-inset">The results from our<a id="_idIndexMarker121"/> program are shown in the following code block. Note that the average error is just 0.00018 of a second, or 0.18 milliseconds out of a frame of 33 milliseconds:</p><pre class="source-code">START COUNTING: FRAME TIME 0.0333333333333 RUN TIME: 66.6666666667
FINISHED COUNTING
REQUESTED FRAME TIME: 0.0333333333333 AVG FRAME TIME: 0.0331549999714
REQUESTED TOTAL TIME: 66.6666666667 ACTUAL TOTAL TIME: 66.3099999428
AVERAGE ERROR 0.000178333361944 TOTAL_ERROR: 0.356666723887
AVERAGE SLEEP TIME: 33.1549999714 AVERAGE RUN TIME 0.178333361944</pre></li> </ol>
<p>The following figure shows the timing graph of our program:</p>
<div><div><img alt="Figure 1.9 – Timing graph of our program" src="img/B19846_01_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.9 – Timing graph of our program</p>
<p>The <em class="italic">spikes</em> in the image are<a id="_idIndexMarker122"/> jitter caused by operating system interrupts. You can see the program controls the frame time in a fairly narrow range. If we did not provide control, the frame time would get greater and greater as the program executed. The graph shows that the frame time stays in a narrow range that keeps returning to the correct value.</p>
<p>Now that we have exercised our programming muscles, we can apply this knowledge to the main control loop for our robot with soft real-time control. This control loop has two primary functions:</p>
<ul>
<li>Respond to commands from the control station</li>
<li>Interface to the robot’s motors and sensors in the Arduino Mega</li>
</ul>
<p>We will discuss this in detail in <a href="B19846_07.xhtml#_idTextAnchor221"><em class="italic">Chapter 7</em></a>.</p>
<h1 id="_idParaDest-30"><a id="_idTextAnchor029"/>Summary</h1>
<p>In this chapter, we introduced the subject of AI, which will be emphasized throughout this book. We identified the main difference between an AI robot and a <em class="italic">regular</em> robot, which is that an AI robot may be nondeterministic. This is to say it may have a different response to the same stimulus, due to learning. We introduced the problem we will use throughout the book, which is picking up toys in a playroom and putting them into a toy box. Next, we discussed two critical tools for AI robotics: the OODA loop, which provides a model for how our robot makes decisions, and the soft real-time control loop, which manages and controls the speed of execution of our program. We applied these techniques in a timing loop demonstration and began to develop our main robot control program.</p>
<p>In the next chapter, we will teach the robot to recognize toys – the objects we want the robot to pick up and put away. We will use computer vision with a video camera to find and recognize the toys left on the floor.</p>
<h1 id="_idParaDest-31"><a id="_idTextAnchor030"/>Questions</h1>
<ol>
<li>What does the acronym <em class="italic">PID</em> stand for? Is this considered an AI software method?</li>
<li>What is the Turing test? Do you feel this is a valid method of assessing AI?</li>
<li>Why do you think robots have a problem with negative obstacles such as stairs and potholes?</li>
<li>In the OODA loop, what does the <em class="italic">Orient</em> step do?</li>
<li>From the discussion of Python and its advantages, compute the following. Your program needs 50 changes tested. Assuming each change requires a recompile step and one run to test, a C Make compile takes 450 seconds and a Python <code>run</code> command takes 3 seconds. How much time do you sit idle waiting on the compiler?</li>
<li>What does RTOS stand for?</li>
<li>Your robot has the following scheduled tasks: telemetry at 10 Hz, GPS at 5 Hz, inertial measurements at 50 Hz, and motor control at 20 Hz. At what frequency would you schedule the base task, and what intervals would you use for the slower tasks (i.e., 10 Hz base, motors every three frames, telemetry every two frames, etc.)?</li>
<li>Given that a frame rate scheduler has the fastest task at 20 fps, how would you schedule a task that needs to run at 7 fps? How about one that runs at 3.5 fps?</li>
<li>What is a blocking call function? Why is it bad to use blocking calls in a real-time system like a robot?</li>
</ol>
<h1 id="_idParaDest-32"><a id="_idTextAnchor031"/>Further reading</h1>
<p>You can refer to the following resources for further details:</p>
<ul>
<li><em class="italic">Effective Robotics Programming with ROS – Third Edition</em>, by Anil Mahtani, Luis Sanchez, and Enreque Fernandez Perdomo, Packt Publishing, 2016</li>
<li><em class="italic">Introduction to AI Robotics – Second Edition</em> by Robin R. Murphy, Bradford Books, 2019</li>
<li><em class="italic">Real-Time scheduling: from hard to soft real-time systems</em>, a whitepaper by Palopoli Lipari, 2015 (<a href="https://arxiv.org/pdf/1512.01978.pdf">https://arxiv.org/pdf/1512.01978.pdf</a>)</li>
<li><em class="italic">Boyd: The Fighter Pilot Who Changed the Art of War</em>, by Robert Coram, Little, Brown and Company, 2002</li>
</ul>
</div>
</body></html>