["```py\n    from vllm import LLM, SamplingParams\n    from datasets import load_dataset\n    from tqdm.auto import tqdm\n    import gc \n    ```", "```py\n    def generate_answers(model_id, dataset_name):\n        dataset = load_dataset(dataset_name, split=\"test\") \n    ```", "```py\n     def format(sample):\n            return \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{}\\n\\n### Response:\\n\".format(sample[\"instruction\"])\n        dataset = dataset.map(lambda sample: {\"prompt\": format(sample)}) \n    ```", "```py\n     llm = LLM(model=model_id, max_model_len=4096)\n        sampling_params = SamplingParams(temperature=0.8, top_p=0.95, min_p=0.05, max_tokens=4096)\n        outputs = llm.generate(dataset[\"prompt\"], sampling_params) \n    ```", "```py\n     answers = [output.outputs[0].text for output in outputs]\n        dataset = dataset.add_column(\"answers\", answers) \n    ```", "```py\n     print(f\"Uploading results for {model_id}\")\n        dataset.push_to_hub(f\"mlabonne/{model_id.split('/')[-1]}-results\")\n        gc.collect()\n        return dataset \n    ```", "```py\n    model_ids = [\n        'mlabonne/TwinLlama-3.1-8B',\n        'mlabonne/TwinLlama-3.1-8B-DPO',\n        'meta-llama/Meta-Llama-3.1-8B-Instruct'\n    ]\n    for model_id in model_ids:\n        generate_answers(model_id, \"mlabonne/llmtwin\") \n    ```", "```py\n    import json\n    from typing import List\n    from datasets import Dataset, load_dataset\n    from openai import OpenAI\n    from tqdm.auto import tqdm\n    import concurrent.futures \n    ```", "```py\n    def evaluate_answer(\n        instruction: str, answer: str, client: OpenAI\n    ) -> dict:\n        prompt = f\"\"\"You are an expert judge. Please evaluate the quality of a given answer to an instruction based on two criteria:\n    1\\. Accuracy: How factually correct is the information presented in the answer? You are a technical expert in this topic.\n    2\\. Style: Is the tone and writing style appropriate for a blog post or social media content? It should use simple but technical words and avoid formal or academic language. \n    ```", "```py\n    Accuracy scale:\n    1 (Poor): Contains factual errors or misleading information\n    2 (Good): Mostly accurate with minor errors or omissions\n    3 (Excellent): Highly accurate and comprehensive\n    Style scale:\n    1 (Poor): Too formal, uses some overly complex words\n    2 (Good): Good balance of technical content and accessibility, but still uses formal words and expressions\n    3 (Excellent): Perfectly accessible language for blog/social media, uses simple but precise technical terms when necessary \n    ```", "```py\n    Example of bad style: The Llama2 7B model constitutes a noteworthy progression in the field of artificial intelligence, serving as the successor to its predecessor, the original Llama architecture.\n    Example of excellent style: Llama2 7B outperforms the original Llama model across multiple benchmarks.\n    Instruction: {instruction}\n    Answer: {answer}\n    Provide your evaluation in JSON format with the following structure:\n    {{\n        \"accuracy\": {{\n            \"analysis\": \"...\",\n            \"score\": 0\n        }},\n        \"style\": {{\n            \"analysis\": \"...\",\n            \"score\": 0\n        }}\n    }}\n    \"\"\" \n    ```", "```py\n     completion = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a helpful assistant who evaluates answers based on accuracy and style. Provide your response in JSON format with a short analysis and score for each criterion.\",\n                },\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            response_format={\"type\": \"json_object\"},\n            max_tokens=1000,\n            temperature=0.8,\n        ) \n    ```", "```py\n    def evaluate_batch(batch, start_index):\n        client = OpenAI(api_key=OPENAI_KEY)\n        return [\n            (i, evaluate_answer(instr, ans, client))\n            for i, (instr, ans) in enumerate(batch, start=start_index)\n        ] \n    ```", "```py\n    def evaluate_answers(model_id: str, num_threads: int = 10, batch_size: int = 5) -> Dataset:\n        dataset = load_dataset(f\"mlabonne/{model_id.split('/')[-1]}-results\", split=\"all\") \n    ```", "```py\n     batches = [\n            (i, list(zip(dataset[\"instruction\"][i:i+batch_size], dataset[\"answers\"][i:i+batch_size])))\n            for i in range(0, len(dataset), batch_size)\n        ] \n    ```", "```py\n     evaluations = [None] * len(dataset)\n        with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n            futures = [executor.submit(evaluate_batch, batch, start_index) for start_index, batch in batches]\n            for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n                for index, evaluation in future.result():\n                    evaluations[index] = evaluation \n    ```", "```py\n     if 'evaluation' in dataset.column_names:\n            dataset = dataset.remove_columns(['evaluation'])\n        dataset = dataset.add_column(\"evaluation\", evaluations) \n    ```", "```py\n     accuracy_scores = []\n        style_scores = []\n        for evaluation in dataset['evaluation']:\n            try:\n                eval_dict = json.loads(evaluation) if isinstance(evaluation, str) else evaluation\n                accuracy_score = eval_dict['accuracy']['score']\n                style_score = eval_dict['style']['score']\n                accuracy_scores.append(accuracy_score)\n                style_scores.append(style_score)\n            except (json.JSONDecodeError, KeyError, TypeError):\n                accuracy_scores.append(None)\n                style_scores.append(None) \n    ```", "```py\n     if 'accuracy' in dataset.column_names:\n            dataset = dataset.remove_columns(['accuracy'])\n        dataset = dataset.add_column('accuracy', accuracy_scores)\n        if 'style' in dataset.column_names:\n            dataset = dataset.remove_columns(['style'])\n        dataset = dataset.add_column('style', style_scores) \n    ```", "```py\n     dataset.push_to_hub(f\"mlabonne/{model_id.split('/')[-1]}-results\")\n        return dataset \n    ```", "```py\n    model_ids = [\n        'mlabonne/TwinLlama-3.1-8B',\n        'mlabonne/TwinLlama-3.1-8B-DPO',\n        'meta-llama/Meta-Llama-3.1-8B-Instruct'\n    ]\n    for model_id in model_ids:\n        evaluate_answers(model_id) \n    ```", "```py\nTwinLlama-3.1-8B - Accuracy: 2.45\nTwinLlama-3.1-8B - Style: 2.04\nTwinLlama-3.1-8B-DPO - Accuracy: 2.46\n**TwinLlama-3.1-8B-DPO - Style: 2.12**\n**Llama-3.1-8B-Instruct - Accuracy: 2.62**\nLlama-3.1-8B-Instruct - Style: 1.86 \n```"]