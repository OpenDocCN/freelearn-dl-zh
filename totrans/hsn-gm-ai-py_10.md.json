["```py\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Categorical\n\n#Hyperparameters\nlearning_rate = 0.0002\ngamma = 0.98\n\nclass REINFORCE(nn.Module):\n  def __init__(self, input_shape, num_actions):\n    super(REINFORCE, self).__init__()\n    self.data = []\n\n    self.fc1 = nn.Linear(input_shape, 128)\n    self.fc2 = nn.Linear(128, num_actions)\n    self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n\n def act(self, x):\n   x = F.relu(self.fc1(x))\n   x = F.softmax(self.fc2(x), dim=0)\n   return x\n\n def put_data(self, item):\n   self.data.append(item)\n\n def train_net(self):\n   R = 0\n   for r, log_prob in self.data[::-1]:\n     R = r + gamma * R\n     loss = -log_prob * R\n     self.optimizer.zero_grad()\n     loss.backward()\n     self.optimizer.step()\n   self.data = []\n\nenv = gym.make('LunarLander-v2')\npi = REINFORCE(env.observation_space.shape[0], env.action_space.n)\nscore = 0.0\nprint_interval = 100\niterations = 10000\nmin_play_reward = 20\n\ndef play_game():\n  done = False\n  state = env.reset()\n  its = 500\n  while(not done and its > 0):\n    its -= 1\n    prob = pi.act(torch.from_numpy(state).float())\n    m = Categorical(prob)\n    action = m.sample()\n    next_state, reward, done, _ = env.step(action.item())\n    env.render()\n    state = next_state\n\nfor iteration in range(iterations):\n  s = env.reset()\n  for t in range(501):\n    prob = pi.act(torch.from_numpy(s).float())\n    m = Categorical(prob)\n    action = m.sample()\n    s_prime, r, done, info = env.step(action.item())\n    pi.put_data((r,torch.log(prob[action])))\n\n    s = s_prime\n    score += r\n    if done:\n      if score/print_interval > min_play_reward:\n        play_game()\n      break\n  pi.train_net()\n  if iteration%print_interval==0 and iteration!=0:\n    print(\"# of episode :{}, avg score : {}\".format(iteration, score/print_interval))\n    score = 0.0 \n\nenv.close()\n```", "```py\ndef train_net(self):\n   R = 0\n   for r, log_prob in self.data[::-1]:\n     R = r + gamma * R\n     loss = -log_prob * R\n     self.optimizer.zero_grad()\n     loss.backward()\n     self.optimizer.step()\n   self.data = []\n```", "```py\nenv = gym.make('LunarLander-v2')\npi = REINFORCE(env.observation_space.shape[0], env.action_space.n)\nscore = 0.0\nprint_interval = 100\niterations = 10000\nmin_play_reward = 20\n```", "```py\nprob = pi.act(torch.from_numpy(s).float())\nm = Categorical(prob)\naction = m.sample()\ns_prime, r, done, info = env.step(action.item())\npi.put_data((r,torch.log(prob[action])))\n```", "```py\nclass ActorCritic(nn.Module):\n def __init__(self, input_shape, num_actions):\n   super(ActorCritic, self).__init__()\n   self.data = []\n   self.fc1 = nn.Linear(input_shape,256)\n   self.fc_pi = nn.Linear(256,num_actions)\n   self.fc_v = nn.Linear(256,1)\n   self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n\n def pi(self, x, softmax_dim = 0):\n   x = F.relu(self.fc1(x))\n   x = self.fc_pi(x)\n   prob = F.softmax(x, dim=softmax_dim)\n   return prob\n\n def v(self, x):\n   x = F.relu(self.fc1(x))\n   v = self.fc_v(x)\n   return v\n\n def put_data(self, transition):\n   self.data.append(transition)\n\n def make_batch(self):\n   s_lst, a_lst, r_lst, s_prime_lst, done_lst = [], [], [], [], []\n   for transition in self.data:\n     s,a,r,s_prime,done = transition\n     s_lst.append(s)\n     a_lst.append([a])\n     r_lst.append([r/100.0])\n     s_prime_lst.append(s_prime)\n     done_mask = 0.0 if done else 1.0  \n     done_lst.append([done_mask])\n\n     s_batch, a_batch, r_batch, s_prime_batch, done_batch\n       = torch.tensor(s_lst, dtype=torch.float),\n       torch.tensor(a_lst), \\\n       torch.tensor(r_lst, dtype=torch.float),\n       torch.tensor(s_prime_lst,   dtype=torch.float), \\ \n       torch.tensor(done_lst, dtype=torch.float)\n\n     self.data = []\n     return s_batch, a_batch, r_batch, s_prime_batch, done_batch\n\n def train_net(self):\n   s, a, r, s_prime, done = self.make_batch()\n   td_target = r + gamma * self.v(s_prime) * done\n   delta = td_target - self.v(s)\n   pi = self.pi(s, softmax_dim=1)\n   pi_a = pi.gather(1,a)\n   loss = -torch.log(pi_a) * delta.detach() \n     + F.smooth_l1_loss(self.v(s), td_target.detach())\n   self.optimizer.zero_grad()\n   loss.mean().backward()\n   self.optimizer.step()\n```", "```py\nfor iteration in range(iterations):\n done = False\n s = env.reset()\n while not done:\n   for t in range(n_rollout):\n     prob = model.pi(torch.from_numpy(s).float())\n     m = Categorical(prob)\n     a = m.sample().item()\n     s_prime, r, done, info = env.step(a) \n     model.put_data((s,a,r,s_prime,done))\n\n     s = s_prime\n     score += r\n     if done:\n       if score/print_interval > min_play_reward:\n         play_game()\n       break \n\n     model.train_net()\n\n     if iteration%print_interval==0 and iteration!=0:\n       print(\"# of episode :{},\n         avg score : {:.1f}\".format(iteration, score/print_interval))\n       score = 0.0\n\nenv.close()\n```", "```py\ns, a, r, s_prime, done = self.make_batch()\ntd_target = r + gamma * self.v(s_prime) * done\n```", "```py\ndelta = td_target - self.v(s) \n```", "```py\npi = self.pi(s, softmax_dim=1)\npi_a = pi.gather(1,a)\n```", "```py\nloss = -torch.log(pi_a) * delta.detach() \n     + F.smooth_l1_loss(self.v(s), td_target.detach())\n```", "```py\nself.optimizer.zero_grad()\nloss.mean().backward()\nself.optimizer.step()\n```", "```py\nlr_mu = 0.0005\nlr_q = 0.001\ngamma = 0.99\nbatch_size = 32\nbuffer_limit = 50000\ntau = 0.005\n```", "```py\nenv = gym.make('Pendulum-v0')\nmemory = ReplayBuffer()\n\nq, q_target = QNet(), QNet()\nq_target.load_state_dict(q.state_dict())\nmu, mu_target = MuNet(), MuNet()\nmu_target.load_state_dict(mu.state_dict())\n\nscore = 0.0\nprint_interval = 20\nmin_play_reward = 0\niterations = 10000\n\nmu_optimizer = optim.Adam(mu.parameters(), lr=lr_mu)\nq_optimizer = optim.Adam(q.parameters(), lr=lr_q)\nou_noise = OrnsteinUhlenbeckNoise(mu=np.zeros(1))\n```", "```py\nfor iteration in range(iterations):\n  s = env.reset() \n  for t in range(300):\n```", "```py\na = mu(torch.from_numpy(s).float()) \na = a.item() + ou_noise()[0]\ns_prime, r, done, info = env.step([a]) memory.put((s,a,r/100.0,s_prime,done))\nscore +=r\ns = s_prime\n```", "```py\nif memory.size()>2000:\n  for i in range(10):\n    train(mu, mu_target, q, q_target, memory, q_optimizer, mu_optimizer)\n    soft_update(mu, mu_target)\n    soft_update(q, q_target)\n```", "```py\ndef soft_update(net, net_target):\n  for param_target, param in zip(net_target.parameters(), net.parameters()):\n   param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)\n```", "```py\nclass QNet(nn.Module):\n  def __init__(self):\n    super(QNet, self).__init__()\n\n    self.fc_s = nn.Linear(3, 64)\n    self.fc_a = nn.Linear(1,64)\n    self.fc_q = nn.Linear(128, 32)\n    self.fc_3 = nn.Linear(32,1)\n\n  def forward(self, x, a):\n    h1 = F.relu(self.fc_s(x))\n    h2 = F.relu(self.fc_a(a))\n    cat = torch.cat([h1,h2], dim=1)\n    q = F.relu(self.fc_q(cat))\n    q = self.fc_3(q)\n    return q\n```", "```py\nclass MuNet(nn.Module):\n  def __init__(self):\n    super(MuNet, self).__init__()\n\n    self.fc1 = nn.Linear(3, 128)\n    self.fc2 = nn.Linear(128, 64)\n    self.fc_mu = nn.Linear(64, 1)\n\n  def forward(self, x):\n    x = F.relu(self.fc1(x))\n    x = F.relu(self.fc2(x))\n    mu = torch.tanh(self.fc_mu(x))*2 \n    return mu\n```", "```py\ndef train(mu, mu_target, q, q_target, memory, q_optimizer, mu_optimizer):\n  s,a,r,s_prime,done_mask = memory.sample(batch_size)\n```", "```py\ntarget = r + gamma * q_target(s_prime, mu_target(s_prime))\nq_loss = F.smooth_l1_loss(q(s,a), target.detach())\nq_optimizer.zero_grad()\nq_loss.backward()\nq_optimizer.step()\n```", "```py\nmu_loss = -q(s,mu(s)).mean() \nmu_optimizer.zero_grad()\nmu_loss.backward()\nmu_optimizer.step()\n```", "```py\nenv = gym.make(args.env_name)\n\nnum_inputs = env.observation_space.shape[0]\nnum_actions = env.action_space.shape[0]\n\nenv.seed(args.seed)\ntorch.manual_seed(args.seed)\n\npolicy_net = Policy(num_inputs, num_actions)\nvalue_net = Value(num_inputs)\n```", "```py\nwhile num_steps < args.batch_size:\n```", "```py\ndef conjugate_gradients(Avp, b, nsteps, residual_tol=1e-10):\n  x = torch.zeros(b.size())\n  r = b.clone()\n  p = b.clone()\n  rdotr = torch.dot(r, r)\n  for i in range(nsteps):\n    _Avp = Avp(p)\n    alpha = rdotr / torch.dot(p, _Avp)\n    x += alpha * p\n    r -= alpha * _Avp\n    new_rdotr = torch.dot(r, r)\n    betta = new_rdotr / rdotr\n    p = r + betta * p\n    rdotr = new_rdotr\n    if rdotr < residual_tol:\n      break\n  return x\n```", "```py\nstepdir = conjugate_gradients(Fvp, -loss_grad, 10)\n```", "```py\ndef linesearch(model, f, x, fullstep, expected_improve_rate,   \n  max_backtracks=10, accept_ratio=.1):\n  fval = f(True).data   \n  print(\"fval before\", fval.item())\n  for (_n_backtracks, stepfrac) in enumerate(.5**np.arange(max_backtracks)):\n    xnew = x + stepfrac * fullstep\n    set_flat_params_to(model, xnew)\n    newfval = f(True).data\n    actual_improve = fval - newfval\n    expected_improve = expected_improve_rate * stepfrac\n    ratio = actual_improve / expected_improve\n    print(\"a/e/r\", actual_improve.item(), expected_improve.item(), \n     ratio.item())\n\n  if ratio.item() > accept_ratio and actual_improve.item() > 0:\n    print(\"fval after\", newfval.item())\n    return True, xnew\n\n  return False, x\n```", "```py\nsuccess, new_params = linesearch(model, get_loss, prev_params, fullstep,\n   neggdotstepdir / lm[0])\n```", "```py\nneggdotstepdir = (-loss_grad * stepdir).sum(0, keepdim=True)\n```", "```py\n set_flat_params_to(model, new_params)\n```", "```py\ndef set_flat_params_to(model, flat_params):\n  prev_ind = 0\n  for param in model.parameters():\n    flat_size = int(np.prod(list(param.size())))\n    param.data.copy_(\n      flat_params[prev_ind:prev_ind + flat_size].view(param.size()))\n    prev_ind += flat_size\n```", "```py\ntrpo_step(policy_net, get_loss, get_kl, args.max_kl, args.damping)\n```", "```py\nbatch = memory.sample()\nupdate_params(batch)\n```", "```py\nfor i in reversed(range(rewards.size(0))):\n  returns[i] = rewards[i] + args.gamma * prev_return * masks[i]\n  deltas[i] = rewards[i] + args.gamma * prev_value *\n    masks[i] - values.data[i]\n  advantages[i] = deltas[i] + args.gamma * args.tau * \n    prev_advantage * masks[i]\n```"]