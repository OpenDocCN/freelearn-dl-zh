<html><head></head><body>
  <div id="_idContainer086">
   <h1 class="chapter-number" id="_idParaDest-103">
    <a id="_idTextAnchor102">
    </a>
    <span class="koboSpan" id="kobo.1.1">
     7
    </span>
   </h1>
   <h1 id="_idParaDest-104">
    <a id="_idTextAnchor103">
    </a>
    <span class="koboSpan" id="kobo.2.1">
     Enhancing NLP Tasks Using LLMs with spacy-llm
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.3.1">
     In this chapter, we will build upon the knowledge gained in
    </span>
    <a href="B22441_06.xhtml#_idTextAnchor087">
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.4.1">
        Chapter 6
       </span>
      </em>
     </span>
    </a>
    <span class="koboSpan" id="kobo.5.1">
     , and explore how to integrate
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.6.1">
      large language models
     </span>
    </strong>
    <span class="koboSpan" id="kobo.7.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.8.1">
      LLMs
     </span>
    </strong>
    <span class="koboSpan" id="kobo.9.1">
     ) into spaCy pipelines using the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.10.1">
      spacy-llm
     </span>
    </strong>
    <span class="koboSpan" id="kobo.11.1">
     library.
    </span>
    <span class="koboSpan" id="kobo.11.2">
     We
    </span>
    <a id="_idIndexMarker354">
    </a>
    <span class="koboSpan" id="kobo.12.1">
     will start by understanding the basics of LLMs and prompt engineering, and how these powerful models can be leveraged to perform a wide range of NLP tasks within spaCy.
    </span>
    <span class="koboSpan" id="kobo.12.2">
     We’ll demonstrate how to configure and use pre-built LLM tasks such as text summarization, and then take a step further by creating a custom task to extract contextual information from text.
    </span>
    <span class="koboSpan" id="kobo.12.3">
     This will involve using Jinja templates for prompt creation and writing custom spaCy components that can efficiently handle complex NLP tasks.
    </span>
    <span class="koboSpan" id="kobo.12.4">
     By the end of this chapter, you will have a deeper understanding of how to enhance traditional NLP pipelines with the flexibility and power
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.13.1">
      of LLMs.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.14.1">
     In this chapter, we’re going to cover the following
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.15.1">
      main topics:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.16.1">
      LLMs and prompt
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.17.1">
       engineering basics
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.18.1">
      Text summarization with LLMs
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.19.1">
       and spaCy
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.20.1">
      Creating custom LLM tasks with
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.21.1">
       Jinja template
      </span>
     </span>
    </li>
   </ul>
   <h1 id="_idParaDest-105">
    <a id="_idTextAnchor104">
    </a>
    <span class="koboSpan" id="kobo.22.1">
     Technical requirements
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.23.1">
     In this chapter, we will be working with spaCy and the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.24.1">
      spacy-llm
     </span>
    </strong>
    <span class="koboSpan" id="kobo.25.1">
     library to create and run our pipelines.
    </span>
    <span class="koboSpan" id="kobo.25.2">
     You can find the code used in this chapter
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.26.1">
      at
     </span>
    </span>
    <a href="https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition">
     <span class="No-Break">
      <span class="koboSpan" id="kobo.27.1">
       https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition
      </span>
     </span>
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.28.1">
      .
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-106">
    <a id="_idTextAnchor105">
    </a>
    <span class="koboSpan" id="kobo.29.1">
     LLMs and prompt engineering basics
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.30.1">
     As we saw in
    </span>
    <a href="B22441_06.xhtml#_idTextAnchor087">
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.31.1">
        Chapter 6
       </span>
      </em>
     </span>
    </a>
    <span class="koboSpan" id="kobo.32.1">
     ,
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.33.1">
      language modeling
     </span>
    </strong>
    <span class="koboSpan" id="kobo.34.1">
     is the task of
    </span>
    <a id="_idIndexMarker355">
    </a>
    <span class="koboSpan" id="kobo.35.1">
     predicting
    </span>
    <a id="_idIndexMarker356">
    </a>
    <span class="koboSpan" id="kobo.36.1">
     the next token given the
    </span>
    <a id="_idIndexMarker357">
    </a>
    <span class="koboSpan" id="kobo.37.1">
     sequence of previous tokens.
    </span>
    <span class="koboSpan" id="kobo.37.2">
     The example we used was that given the sequence of words
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.38.1">
      Yesterday I visited a
     </span>
    </strong>
    <span class="koboSpan" id="kobo.39.1">
     , a language model can predict the next token to be something such as
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.40.1">
      church
     </span>
    </strong>
    <span class="koboSpan" id="kobo.41.1">
     ,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.42.1">
      hospital
     </span>
    </strong>
    <span class="koboSpan" id="kobo.43.1">
     ,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.44.1">
      school
     </span>
    </strong>
    <span class="koboSpan" id="kobo.45.1">
     , and so on.
    </span>
    <span class="koboSpan" id="kobo.45.2">
     Conventional language models are usually trained in a supervised manner to perform a specific task.
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.46.1">
      Pre-trained language models
     </span>
    </strong>
    <span class="koboSpan" id="kobo.47.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.48.1">
      PLM
     </span>
    </strong>
    <span class="koboSpan" id="kobo.49.1">
     ) are
    </span>
    <a id="_idIndexMarker358">
    </a>
    <span class="koboSpan" id="kobo.50.1">
     trained in a self-supervised manner, with the aim of learning a generic representation of the language.
    </span>
    <span class="koboSpan" id="kobo.50.2">
     These PLM models are then fine-tuned to perform a specific downstream task.
    </span>
    <span class="koboSpan" id="kobo.50.3">
     This self-supervised pre-training made PLM models much more powerful than regular
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.51.1">
      language models.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.52.1">
     The LLMs are an evolution of PLMs that have many more model parameters and larger training datasets.
    </span>
    <span class="koboSpan" id="kobo.52.2">
     The GPT-3 model, for example, has 175B parameters.
    </span>
    <span class="koboSpan" id="kobo.52.3">
     Its successor, GPT3.5, was the base for the ChatGPT model released in November 2022.
    </span>
    <span class="koboSpan" id="kobo.52.4">
     LLMs can serve as general-purpose tools, capable of tasks from language translation to coding assistance.
    </span>
    <span class="koboSpan" id="kobo.52.5">
     Their ability to understand and generate human-like text has led to impactful applications in medicine, education, science, math, law, and more.
    </span>
    <span class="koboSpan" id="kobo.52.6">
     In medicine, LLMs support doctors with evidence-based recommendations and enhance patient interactions.
    </span>
    <span class="koboSpan" id="kobo.52.7">
     In education, they customize learning experiences and assist teachers in creating content.
    </span>
    <span class="koboSpan" id="kobo.52.8">
     In science, LLMs speed up research and scientific writing.
    </span>
    <span class="koboSpan" id="kobo.52.9">
     In law, they analyze legal documents and clarify
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.53.1">
      complex terms.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.54.1">
     We can also use LLMs for regular NLP tasks such as
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.55.1">
      Named Entity Recognition
     </span>
    </strong>
    <span class="koboSpan" id="kobo.56.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.57.1">
      NER
     </span>
    </strong>
    <span class="koboSpan" id="kobo.58.1">
     ), text
    </span>
    <a id="_idIndexMarker359">
    </a>
    <span class="koboSpan" id="kobo.59.1">
     categorization, and text summarization.
    </span>
    <span class="koboSpan" id="kobo.59.2">
     Basically, these models can do almost anything we ask them to.
    </span>
    <span class="koboSpan" id="kobo.59.3">
     But this doesn’t come for free since training them requires extensive computational resources, and the large number of layers and parameters make them produce answers much more slowly than non-LLM models.
    </span>
    <span class="koboSpan" id="kobo.59.4">
     LLMs can
    </span>
    <a id="_idIndexMarker360">
    </a>
    <span class="koboSpan" id="kobo.60.1">
     also
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.61.1">
      hallucinate
     </span>
    </strong>
    <span class="koboSpan" id="kobo.62.1">
     : produce responses that at first seem plausible but are in fact incorrect or not aligned with the facts or context.
    </span>
    <span class="koboSpan" id="kobo.62.2">
     This phenomenon occurs because the models generate text based on patterns learned from their training data, rather than verifying information against an external source.
    </span>
    <span class="koboSpan" id="kobo.62.3">
     As a result, they might create statements that sound reasonable but are misleading, inaccurate, or entirely fictional.
    </span>
    <span class="koboSpan" id="kobo.62.4">
     Given all that, LLMs are useful but we should always analyze if they are the best solution to the project
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.63.1">
      at hand.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.64.1">
     To interact with LLMs, we use prompts.
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.65.1">
      Prompts
     </span>
    </strong>
    <span class="koboSpan" id="kobo.66.1">
     should guide the models to generate answers or make the model take action.
    </span>
    <span class="koboSpan" id="kobo.66.2">
     Prompts usually have
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.67.1">
      these elements:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.68.1">
       Instruction
      </span>
     </strong>
     <span class="koboSpan" id="kobo.69.1">
      : The task you want the model
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.70.1">
       to execute
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.71.1">
       Context
      </span>
     </strong>
     <span class="koboSpan" id="kobo.72.1">
      : External information or additional context that should be useful to produce
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.73.1">
       better answers
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.74.1">
       Input data
      </span>
     </strong>
     <span class="koboSpan" id="kobo.75.1">
      : The input/question we want to
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.76.1">
       be answered
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.77.1">
       Output indicator
      </span>
     </strong>
     <span class="koboSpan" id="kobo.78.1">
      : The type of format we want to see as the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.79.1">
       model output
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.80.1">
     With
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.81.1">
      spacy-llm
     </span>
    </strong>
    <span class="koboSpan" id="kobo.82.1">
     , we define prompts as tasks.
    </span>
    <span class="koboSpan" id="kobo.82.2">
     When building spaCy pipelines with LLMs, each LLM component is defined using a
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.83.1">
      task
     </span>
    </strong>
    <span class="koboSpan" id="kobo.84.1">
     and a
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.85.1">
      model
     </span>
    </strong>
    <span class="koboSpan" id="kobo.86.1">
     .
    </span>
    <span class="koboSpan" id="kobo.86.2">
     The
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.87.1">
      task
     </span>
    </strong>
    <span class="koboSpan" id="kobo.88.1">
     defines the prompt and functionality to parse the resulting response.
    </span>
    <span class="koboSpan" id="kobo.88.2">
     The
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.89.1">
      model
     </span>
    </strong>
    <span class="koboSpan" id="kobo.90.1">
     defines the LLM model and how to connect
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.91.1">
      to it.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.92.1">
     Now that you
    </span>
    <a id="_idIndexMarker361">
    </a>
    <span class="koboSpan" id="kobo.93.1">
     know
    </span>
    <a id="_idIndexMarker362">
    </a>
    <span class="koboSpan" id="kobo.94.1">
     what LLMs are and how to interact with them, let’s use a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.95.1">
      spacy-llm
     </span>
    </strong>
    <span class="koboSpan" id="kobo.96.1">
     component in a pipeline.
    </span>
    <span class="koboSpan" id="kobo.96.2">
     In the next section, we’re going to create a pipeline to summarize texts using
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.97.1">
      an LLM.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-107">
    <a id="_idTextAnchor106">
    </a>
    <span class="koboSpan" id="kobo.98.1">
     Text summarization with LLMs and spacy-llm
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.99.1">
     Each
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.100.1">
      spacy-llm
     </span>
    </strong>
    <span class="koboSpan" id="kobo.101.1">
     component
    </span>
    <a id="_idIndexMarker363">
    </a>
    <span class="koboSpan" id="kobo.102.1">
     has
    </span>
    <a id="_idIndexMarker364">
    </a>
    <span class="koboSpan" id="kobo.103.1">
     a task
    </span>
    <a id="_idIndexMarker365">
    </a>
    <span class="koboSpan" id="kobo.104.1">
     definition.
    </span>
    <span class="koboSpan" id="kobo.104.2">
     spaCy
    </span>
    <a id="_idIndexMarker366">
    </a>
    <span class="koboSpan" id="kobo.105.1">
     has some pre-defined tasks, and we can also create own tasks.
    </span>
    <span class="koboSpan" id="kobo.105.2">
     In this section, we’re going to use the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.106.1">
      spacy.Summarization.v1
     </span>
    </strong>
    <span class="koboSpan" id="kobo.107.1">
     task.
    </span>
    <span class="koboSpan" id="kobo.107.2">
     Each task is defined using a prompt.
    </span>
    <span class="koboSpan" id="kobo.107.3">
     Here is the prompt for this task, available
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.108.1">
      at
     </span>
    </span>
    <a href="https://github.com/explosion/spacy-llm/blob/main/spacy_llm/tasks/templates/summarization.v1.jinja">
     <span class="No-Break">
      <span class="koboSpan" id="kobo.109.1">
       https://github.com/explosion/spacy-llm/blob/main/spacy_llm/tasks/templates/summarization.v1.jinja
      </span>
     </span>
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.110.1">
      :
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.111.1">
You are an expert summarization system. </span><span class="koboSpan" id="kobo.111.2">Your task is to accept Text as input and summarize the Text in a concise way.
</span><span class="koboSpan" id="kobo.111.3">{%- if max_n_words -%}
{# whitespace #}
The summary must not, under any circumstances, contain more than {{ max_n_words }} words.
</span><span class="koboSpan" id="kobo.111.4">{%- endif -%}
{# whitespace #}
{%- if prompt_examples -%}
{# whitespace #}
Below are some examples (only use these as a guide):
{# whitespace #}
{%- for example in prompt_examples -%}
{# whitespace #}
Text:
'''
{{ example.text }}
'''
Summary:
'''
{{ example.summary }}
'''
{# whitespace #}
{%- endfor -%}
{# whitespace #}
{%- endif -%}
{# whitespace #}
Here is the Text that needs to be summarized:
'''
{{ text }}
'''
Summary:</span></pre>
   <p>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.112.1">
      spacy-llm
     </span>
    </strong>
    <span class="koboSpan" id="kobo.113.1">
     uses
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.114.1">
      Jinja
     </span>
    </strong>
    <span class="koboSpan" id="kobo.115.1">
     templates to define the instructions and examples.
    </span>
    <span class="koboSpan" id="kobo.115.2">
     Jinja uses placeholders to insert data dynamically in the templates.
    </span>
    <span class="koboSpan" id="kobo.115.3">
     The most common placeholders are
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.116.1">
      {{ }}
     </span>
    </strong>
    <span class="koboSpan" id="kobo.117.1">
     ,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.118.1">
      {% %}
     </span>
    </strong>
    <span class="koboSpan" id="kobo.119.1">
     , and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.120.1">
      {# #}
     </span>
    </strong>
    <span class="koboSpan" id="kobo.121.1">
     .
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.122.1">
      {{ }}
     </span>
    </strong>
    <span class="koboSpan" id="kobo.123.1">
     is used to add variables or expressions,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.124.1">
      {% %}
     </span>
    </strong>
    <span class="koboSpan" id="kobo.125.1">
     is used with flow control statements, and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.126.1">
      {# #}
     </span>
    </strong>
    <span class="koboSpan" id="kobo.127.1">
     is used to add comments.
    </span>
    <span class="koboSpan" id="kobo.127.2">
     Let’s see how some of these placeholders are used in the
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.128.1">
       spacy.Summarization.v1
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.129.1">
      template.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.130.1">
     We can ask the model to output a summary with a certain maximum number of words.
    </span>
    <span class="koboSpan" id="kobo.130.2">
     The default value for
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.131.1">
      max_n_words
     </span>
    </strong>
    <span class="koboSpan" id="kobo.132.1">
     is
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.133.1">
      null
     </span>
    </strong>
    <span class="koboSpan" id="kobo.134.1">
     .
    </span>
    <span class="koboSpan" id="kobo.134.2">
     If this parameter is set in our config, the template will include
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.135.1">
      this number:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.136.1">
{%- if max_n_words -%}
{# whitespace #}
The summary must not, under any circumstances, contain more than {{ max_n_words }} words.
</span><span class="koboSpan" id="kobo.136.2">{%- endif -%}</span></pre>
   <p>
    <strong class="bold">
     <span class="koboSpan" id="kobo.137.1">
      Few-shot prompting
     </span>
    </strong>
    <span class="koboSpan" id="kobo.138.1">
     is a technique that consists of providing some examples (usually 1 to 5) of desirable inputs and outputs to show the model how we want the results.
    </span>
    <span class="koboSpan" id="kobo.138.2">
     These examples
    </span>
    <a id="_idIndexMarker367">
    </a>
    <span class="koboSpan" id="kobo.139.1">
     help
    </span>
    <a id="_idIndexMarker368">
    </a>
    <span class="koboSpan" id="kobo.140.1">
     the
    </span>
    <a id="_idIndexMarker369">
    </a>
    <span class="koboSpan" id="kobo.141.1">
     model
    </span>
    <a id="_idIndexMarker370">
    </a>
    <span class="koboSpan" id="kobo.142.1">
     better understand the patterns without the need for fine-tuning with lots of examples.
    </span>
    <span class="koboSpan" id="kobo.142.2">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.143.1">
      spacy.Summarization.v1
     </span>
    </strong>
    <span class="koboSpan" id="kobo.144.1">
     task has an
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.145.1">
      examples
     </span>
    </strong>
    <span class="koboSpan" id="kobo.146.1">
     parameter to generate
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.147.1">
      few-shot examples.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.148.1">
     Now that we know how the summarization template task works it’s time to work on the other element of
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.149.1">
      spacy-llm
     </span>
    </strong>
    <span class="koboSpan" id="kobo.150.1">
     , the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.151.1">
      model
     </span>
    </strong>
    <span class="koboSpan" id="kobo.152.1">
     .
    </span>
    <span class="koboSpan" id="kobo.152.2">
     We’r
    </span>
    <a id="_idIndexMarker371">
    </a>
    <span class="koboSpan" id="kobo.153.1">
     e going to
    </span>
    <a id="_idIndexMarker372">
    </a>
    <span class="koboSpan" id="kobo.154.1">
     use Anthropic’s
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.155.1">
      Claude 2
     </span>
    </strong>
    <span class="koboSpan" id="kobo.156.1">
     model.
    </span>
    <span class="koboSpan" id="kobo.156.2">
     To make sure the credentials to connect to this model are available, you can open the console on your computer and run
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.157.1">
      export ANTHROPIC_API_KEY="..."
     </span>
    </strong>
    <span class="koboSpan" id="kobo.158.1">
     .
    </span>
    <span class="koboSpan" id="kobo.158.2">
     Now let’s install the package with the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.159.1">
      python3 -m pip install spacy-llm==0.7.2
     </span>
    </strong>
    <span class="koboSpan" id="kobo.160.1">
     command.
    </span>
    <span class="koboSpan" id="kobo.160.2">
     We will load the pipeline using a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.161.1">
      config.cfg
     </span>
    </strong>
    <span class="koboSpan" id="kobo.162.1">
     file (if you need a refresher, you can go back to the
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.163.1">
      Working with spaCy config.cfg files
     </span>
    </em>
    <span class="koboSpan" id="kobo.164.1">
     section in
    </span>
    <a href="B22441_06.xhtml#_idTextAnchor087">
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.165.1">
        Chapter 6
       </span>
      </em>
     </span>
    </a>
    <span class="koboSpan" id="kobo.166.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.166.2">
     Let’s build the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.167.1">
      configuration file:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.168.1">
      First, we’ll define the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.169.1">
       nlp
      </span>
     </strong>
     <span class="koboSpan" id="kobo.170.1">
      section, where we should define the language and the components of our pipeline.
     </span>
     <span class="koboSpan" id="kobo.170.2">
      We’re only using the
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.171.1">
        llm
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.172.1">
       component:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.173.1">
[nlp]
lang = "en"
pipeline = ["llm"]</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.174.1">
      Now, it’s time to specify the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.175.1">
       components
      </span>
     </strong>
     <span class="koboSpan" id="kobo.176.1">
      section.
     </span>
     <span class="koboSpan" id="kobo.176.2">
      To initialize the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.177.1">
       llm
      </span>
     </strong>
     <span class="koboSpan" id="kobo.178.1">
      component, we’ll write
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.179.1">
       factory = "llm"
      </span>
     </strong>
     <span class="koboSpan" id="kobo.180.1">
      ; then, we will specify the task and
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.181.1">
       the model:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.182.1">
[components]
[components.llm]
factory = "llm"
[components.llm.task]
@llm_tasks = "spacy.Summarization.v1"
examples = null
max_n_words = null
[components.llm.model]
@llm_models = "spacy.Claude-2.v2"
config = {"max_tokens_to_sample": 1024}</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.183.1">
      To load this
     </span>
     <a id="_idIndexMarker373">
     </a>
     <span class="koboSpan" id="kobo.184.1">
      pipeline, we’ll
     </span>
     <a id="_idIndexMarker374">
     </a>
     <span class="koboSpan" id="kobo.185.1">
      pass
     </span>
     <a id="_idIndexMarker375">
     </a>
     <span class="koboSpan" id="kobo.186.1">
      the
     </span>
     <a id="_idIndexMarker376">
     </a>
     <span class="koboSpan" id="kobo.187.1">
      path to this config file to the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.188.1">
       assemble
      </span>
     </strong>
     <span class="koboSpan" id="kobo.189.1">
      method from
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.190.1">
       spacy_llm.util
      </span>
     </strong>
     <span class="koboSpan" id="kobo.191.1">
      .
     </span>
     <span class="koboSpan" id="kobo.191.2">
      Let’s ask the model to summarize the
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.192.1">
       LLMs and prompt engineering basics
      </span>
     </em>
     <span class="koboSpan" id="kobo.193.1">
      section of
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.194.1">
       this chapter:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.195.1">
from spacy_llm.util import assemble
nlp = assemble("config.cfg")
content = """
As we saw on Chapter 6, Language Modeling is the task of predicting the next token given the sequence of previous tokens.
</span><span class="koboSpan" id="kobo.195.2">[...]
Now that you know what LLMs are and how to interact with them, let's use a spacy-llm component in a pipeline. </span><span class="koboSpan" id="kobo.195.3">In the next section we're going to create a pipeline to summarize texts using a LLM.
</span><span class="koboSpan" id="kobo.195.4">"""
doc = nlp(content)
print(doc._.summary)</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.196.1">
      The
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.197.1">
       spacy.Summarization.v1
      </span>
     </strong>
     <span class="koboSpan" id="kobo.198.1">
      task adds the summary to the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.199.1">
       ._.summary
      </span>
     </strong>
     <span class="koboSpan" id="kobo.200.1">
      extension
     </span>
     <a id="_idIndexMarker377">
     </a>
     <span class="koboSpan" id="kobo.201.1">
      attribute
     </span>
     <a id="_idIndexMarker378">
     </a>
     <span class="koboSpan" id="kobo.202.1">
      by
     </span>
     <a id="_idIndexMarker379">
     </a>
     <span class="koboSpan" id="kobo.203.1">
      default.
     </span>
     <span class="koboSpan" id="kobo.203.2">
      Here
     </span>
     <a id="_idIndexMarker380">
     </a>
     <span class="koboSpan" id="kobo.204.1">
      is the response from
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.205.1">
       the model:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.206.1">
'Here is a concise summary of the key points from the text:\n\nLanguage models predict the next token in a sequence. </span><span class="koboSpan" id="kobo.206.2">Pre-trained language models (PLMs) are trained in a self-supervised way to learn general representations of language. </span><span class="koboSpan" id="kobo.206.3">PLMs are fine-tuned for downstream tasks. </span><span class="koboSpan" id="kobo.206.4">Large language models (LLMs) like GPT-3 have billions of parameters and are trained on huge datasets. </span><span class="koboSpan" id="kobo.206.5">LLMs can perform a variety of tasks including translation, coding assistance, scientific writing, and legal analysis. </span><span class="koboSpan" id="kobo.206.6">However, LLMs require lots of compute resources, are slow, and can sometimes "hallucinate" plausible but incorrect information. </span><span class="koboSpan" id="kobo.206.7">We interact with LLMs using prompts that provide instructions, context, input data, and indicate the desired output format. </span><span class="koboSpan" id="kobo.206.8">Spacy-llm allows defining LLM components in spaCy pipelines using tasks to specify prompts and models to connect to the LLM. </span><span class="koboSpan" id="kobo.206.9">The text then explains we will create a pipeline to summarize text using a LLM component.'</span></pre>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.207.1">
     Nice, right?
    </span>
    <span class="koboSpan" id="kobo.207.2">
     You can check the other parameters to customize the task here
    </span>
    <a href="https://spacy.io/api/large-language-models#summarization-v1">
     <span class="koboSpan" id="kobo.208.1">
      https://spacy.io/api/large-language-models#summarization-v1
     </span>
    </a>
    <span class="koboSpan" id="kobo.209.1">
     .
    </span>
    <span class="koboSpan" id="kobo.209.2">
     Some of the other available
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.210.1">
      spacy-llm
     </span>
    </strong>
    <span class="koboSpan" id="kobo.211.1">
     tasks include
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.212.1">
      spacy.EntityLinker.v1
     </span>
    </strong>
    <span class="koboSpan" id="kobo.213.1">
     ,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.214.1">
      spacy.NER.v3
     </span>
    </strong>
    <span class="koboSpan" id="kobo.215.1">
     ,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.216.1">
      spacy.SpanCat.v3
     </span>
    </strong>
    <span class="koboSpan" id="kobo.217.1">
     ,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.218.1">
      spacy.TextCat.v3
     </span>
    </strong>
    <span class="koboSpan" id="kobo.219.1">
     , and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.220.1">
      spacy.Sentiment.v1
     </span>
    </strong>
    <span class="koboSpan" id="kobo.221.1">
     .
    </span>
    <span class="koboSpan" id="kobo.221.2">
     But beyond using these pre-built tasks, you can also create your own, which not only enhances the power of
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.222.1">
      spacy-llm
     </span>
    </strong>
    <span class="koboSpan" id="kobo.223.1">
     but also provides an organized way to follow best practices
    </span>
    <a id="_idIndexMarker381">
    </a>
    <span class="koboSpan" id="kobo.224.1">
     when
    </span>
    <a id="_idIndexMarker382">
    </a>
    <span class="koboSpan" id="kobo.225.1">
     building
    </span>
    <a id="_idIndexMarker383">
    </a>
    <span class="koboSpan" id="kobo.226.1">
     NLP
    </span>
    <a id="_idIndexMarker384">
    </a>
    <span class="koboSpan" id="kobo.227.1">
     pipelines.
    </span>
    <span class="koboSpan" id="kobo.227.2">
     Let’s learn how to do that in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.228.1">
      next section.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-108">
    <a id="_idTextAnchor107">
    </a>
    <span class="koboSpan" id="kobo.229.1">
     Creating custom spacy-llm tasks
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.230.1">
     In this section, we’re
    </span>
    <a id="_idIndexMarker385">
    </a>
    <span class="koboSpan" id="kobo.231.1">
     going to create a task where, given a quote from
    </span>
    <a href="https://dummyjson.com/docs/quotes">
     <span class="koboSpan" id="kobo.232.1">
      https://dummyjson.com/docs/quotes
     </span>
    </a>
    <span class="koboSpan" id="kobo.233.1">
     , the model should provide the context of the quote.
    </span>
    <span class="koboSpan" id="kobo.233.2">
     Here’s
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.234.1">
      an example:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.235.1">
Quote: We must balance conspicuous consumption with conscious capitalism.
</span><span class="koboSpan" id="kobo.235.2">Context: Business ethics.</span></pre>
   <p>
    <span class="koboSpan" id="kobo.236.1">
     The first step of creating a custom
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.237.1">
      spacy-llm
     </span>
    </strong>
    <span class="koboSpan" id="kobo.238.1">
     task is to create the prompt and save it as a Jinja template.
    </span>
    <span class="koboSpan" id="kobo.238.2">
     Here is the template for
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.239.1">
      this task:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.240.1">
You are an expert at extracting context from text.
</span><span class="koboSpan" id="kobo.240.2">Your tasks is to accept a quote as input and provide the context of the quote.
</span><span class="koboSpan" id="kobo.240.3">This context will be used to group the quotes together.
</span><span class="koboSpan" id="kobo.240.4">Do not put any other text in your answer and provide the context in 3 words max. </span><span class="koboSpan" id="kobo.240.5">The quote should have one context only.
</span><span class="koboSpan" id="kobo.240.6">{# whitespace #}
{# whitespace #}
Here is the quote that needs classification
{# whitespace #}
{# whitespace #}
Quote:
'''
{{ text }}</span></pre>
   <p>
    <span class="koboSpan" id="kobo.241.1">
     We’ll save this in a file named
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.242.1">
      templates/quote_context_extract.jinja
     </span>
    </strong>
    <span class="koboSpan" id="kobo.243.1">
     .
    </span>
    <span class="koboSpan" id="kobo.243.2">
     The next step is to create the class for the task.
    </span>
    <span class="koboSpan" id="kobo.243.3">
     This class should implement
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.244.1">
      two functions:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.245.1">
       generate_prompts(docs: Iterable[Doc]) -&gt; Iterable[str]
      </span>
     </strong>
     <span class="koboSpan" id="kobo.246.1">
      : This function converts a list of spaCy
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.247.1">
       Doc
      </span>
     </strong>
     <span class="koboSpan" id="kobo.248.1">
      objects into a list
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.249.1">
       of prompts.
      </span>
     </span>
    </li>
    <li>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.250.1">
       parse_responses(docs: Iterable[Doc], responses: Iterable[str]) -&gt; Iterable[Doc]
      </span>
     </strong>
     <span class="koboSpan" id="kobo.251.1">
      : This function parses LLM outputs into a spaCy
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.252.1">
        Doc
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.253.1">
       object.
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.254.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.255.1">
      generate_prompts()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.256.1">
     method will use our Jinja template and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.257.1">
      parse_responses()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.258.1">
     method will receive the model’s response and add the context extension attribution
    </span>
    <a id="_idIndexMarker386">
    </a>
    <span class="koboSpan" id="kobo.259.1">
     to our
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.260.1">
      Doc
     </span>
    </strong>
    <span class="koboSpan" id="kobo.261.1">
     .
    </span>
    <span class="koboSpan" id="kobo.261.2">
     Let’s create the
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.262.1">
       QuoteContextExtractTask
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.263.1">
      class:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.264.1">
      First, we import all the methods we’ll need and set the directory for
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.265.1">
       the templates:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.266.1">
from pathlib import Path
from spacy_llm.registry import registry
import jinja2
from typing import Iterable
from spacy.tokens import Doc
TEMPLATE_DIR = Path("templates")</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.267.1">
      Now, let’s create a method to read the text of the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.268.1">
       Jinja template:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.269.1">
def read_template(name: str) -&gt; str:
    """Read the text from a Jinja template using pathlib"""
    path = TEMPLATE_DIR / f"{name}.jinja"
    if not path.exists():
        raise ValueError(f"{name} is not a valid template.")
    return path.read_text()</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.270.1">
      Finally, we can start to create the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.271.1">
       QuoteContextExtractTask
      </span>
     </strong>
     <span class="koboSpan" id="kobo.272.1">
      class.
     </span>
     <span class="koboSpan" id="kobo.272.2">
      Let’s start creating the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.273.1">
       __init__()
      </span>
     </strong>
     <span class="koboSpan" id="kobo.274.1">
      method.
     </span>
     <span class="koboSpan" id="kobo.274.2">
      The class should be initiated with the name of the Jinja template and a field string to set the name of the extension
     </span>
     <a id="_idIndexMarker387">
     </a>
     <span class="koboSpan" id="kobo.275.1">
      attribute the task will add to the
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.276.1">
        Doc
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.277.1">
       object:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.278.1">
class QuoteContextExtractTask:
    def __init__(self, template: str = "quotecontextextract",
                 field: str = "context"):
        self._template = read_template(template)
        self._field = field</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.279.1">
      Now we will create a method to build the prompt.
     </span>
     <span class="koboSpan" id="kobo.279.2">
      Jinja uses an
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.280.1">
       Environment
      </span>
     </strong>
     <span class="koboSpan" id="kobo.281.1">
      object to load the templates from the file.
     </span>
     <span class="koboSpan" id="kobo.281.2">
      We will use the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.282.1">
       from_string()
      </span>
     </strong>
     <span class="koboSpan" id="kobo.283.1">
      method to build the template from text and yield it.
     </span>
     <span class="koboSpan" id="kobo.283.2">
      Each time this method runs internally, it will render the template replacing the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.284.1">
       {{text}}
      </span>
     </strong>
     <span class="koboSpan" id="kobo.285.1">
      variable of the template with the value
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.286.1">
       of
      </span>
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.287.1">
        doc.text
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.288.1">
       :
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.289.1">
def generate_prompts(self, docs: Iterable[Doc]) -&gt; Iterable[str]:
    environment = jinja2.Environment()
    _template = environment.from_string(self._template)
    for doc in docs:
        prompt = _template.render(
            text=doc.text,
        )
        yield prompt</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.290.1">
      We can now write the last method of the class.
     </span>
     <span class="koboSpan" id="kobo.290.2">
      The
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.291.1">
       parse_responses()
      </span>
     </strong>
     <span class="koboSpan" id="kobo.292.1">
      method will add the response of the model to the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.293.1">
       Doc
      </span>
     </strong>
     <span class="koboSpan" id="kobo.294.1">
      object.
     </span>
     <span class="koboSpan" id="kobo.294.2">
      First, we create a helper method to add the extension attribute if it doesn’t exist.
     </span>
     <span class="koboSpan" id="kobo.294.3">
      To set the extension attribute, we will use Python’s
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.295.1">
       setattr ()
      </span>
     </strong>
     <span class="koboSpan" id="kobo.296.1">
      method so we
     </span>
     <a id="_idIndexMarker388">
     </a>
     <span class="koboSpan" id="kobo.297.1">
      can dynamically set the attribute using the class
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.298.1">
       field variable:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.299.1">
  def _check_doc_extension(self):
      """Add extension if need be."""
</span><span class="koboSpan" id="kobo.299.2">      if not Doc.has_extension(self._field):
          Doc.set_extension(self._field, default=None)
    def parse_responses(
        self, docs: Iterable[Doc], responses: Iterable[str]
    ) -&gt; Iterable[Doc]:
        self._check_doc_extension()
        for doc, prompt_response in zip(docs, responses):
            try:
                setattr(
                    doc._,
                    self._field,
                    prompt_response[0].strip(),
                )
            except ValueError:
                setattr(doc._, self._field, None)
            yield doc</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.300.1">
      To use this class in our
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.301.1">
       config.cfg
      </span>
     </strong>
     <span class="koboSpan" id="kobo.302.1">
      file, we need to add the task to the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.303.1">
       spacy-llm
      </span>
     </strong>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.304.1">
        llm_tasks
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.305.1">
       register:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.306.1">
@registry.llm_tasks("my_namespace.QuoteContextExtractTask.v1")
def make_quote_extraction() -&gt; "QuoteContextExtractTask":
    return QuoteContextExtractTask()</span></pre>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.307.1">
     And we’re done!
    </span>
    <span class="koboSpan" id="kobo.307.2">
     Save
    </span>
    <a id="_idIndexMarker389">
    </a>
    <span class="koboSpan" id="kobo.308.1">
     this to a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.309.1">
      quote.py
     </span>
    </strong>
    <span class="koboSpan" id="kobo.310.1">
     class.
    </span>
    <span class="koboSpan" id="kobo.310.2">
     Here is the full code that should be in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.311.1">
      this script:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.312.1">
from pathlib import Path
from spacy_llm.registry import registry
import jinja2
from typing import Iterable
from spacy.tokens import Doc
TEMPLATE_DIR = Path("templates")
def read_template(name: str) -&gt; str:
    """Read the text from a Jinja template using pathlib"""
    path = TEMPLATE_DIR / f"{name}.jinja"
    if not path.exists():
        raise ValueError(f"{name} is not a valid template.")
    return path.read_text()
@registry.llm_tasks("my_namespace.QuoteContextExtractTask.v1")
def make_quote_extraction() -&gt; "QuoteContextExtractTask":
    return QuoteContextExtractTask()
class QuoteContextExtractTask:
    def __init__(self, template: str = "quote_context_extract",
                 field: str = "context"):
        self._template = read_template(template)
        self._field = field
    def generate_prompts(self, 
        docs: Iterable[Doc]) -&gt; Iterable[str]:
        environment = jinja2.Environment()
        _template = environment.from_string(self_template)
        for doc in docs:
            prompt = _template.render(
                text=doc.text,
            )
            yield prompt
    def _check_doc_extension(self):
        """Add extension if need be."""
</span><span class="koboSpan" id="kobo.312.2">        if not Doc.has_extension(self._field):
            Doc.set_extension(self._field, default=None)
  def parse_responses(
      self, docs: Iterable[Doc], responses: Iterable[str]
  ) -&gt; Iterable[Doc]:
        self._check_doc_extension()
        for doc, prompt_response in zip(docs, responses):
            try:
                setattr(
                    doc._,
                    self._field,
                    prompt_response[0].strip(),
                ),
            except ValueError:
                setattr(doc._, self._field, None)
            yield doc</span></pre>
   <ol>
    <li value="7">
     <span class="koboSpan" id="kobo.313.1">
      Now, let’s
     </span>
     <a id="_idIndexMarker390">
     </a>
     <span class="koboSpan" id="kobo.314.1">
      test our custom task by first creating the
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.315.1">
        config_custom_task.cfg
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.316.1">
       file:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.317.1">
[nlp]
lang = "en"
pipeline = ["llm"]
[components]
[components.llm]
factory = "llm"
[components.llm.task]
@llm_tasks = "my_namespace.QuoteContextExtractTask.v1"
[components.llm.model]
@llm_models = "spacy.Claude-2.v2"
config = {"max_tokens_to_sample": 1024}</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.318.1">
      Finally, we can
     </span>
     <a id="_idIndexMarker391">
     </a>
     <span class="koboSpan" id="kobo.319.1">
      assemble the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.320.1">
       nlp
      </span>
     </strong>
     <span class="koboSpan" id="kobo.321.1">
      object using this file and print out the context for a quote.
     </span>
     <span class="koboSpan" id="kobo.321.2">
      Don’t forget to import the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.322.1">
       QuoteContextExtractTask
      </span>
     </strong>
     <span class="koboSpan" id="kobo.323.1">
      from
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.324.1">
       quote.py
      </span>
     </strong>
     <span class="koboSpan" id="kobo.325.1">
      so spaCy knows where to load
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.326.1">
       this from:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.327.1">
from spacy_llm.util import assemble
from quote import QuoteContextExtractTask
nlp = assemble("config_custom_task.cfg")
quote = "Life isn't about getting and having, it's about giving and being."
</span><span class="koboSpan" id="kobo.327.2">doc = nlp(quote)
print("Context:", doc._.context)
&gt;&gt;&gt; Context: self-improvement</span></pre>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.328.1">
     In this section, you’ve created a custom
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.329.1">
      spacy-llm
     </span>
    </strong>
    <span class="koboSpan" id="kobo.330.1">
     task to extract the context from a given quote.
    </span>
    <span class="koboSpan" id="kobo.330.2">
     This approach not only allows you to craft highly specific NLP tasks tailored to your needs but also
    </span>
    <a id="_idIndexMarker392">
    </a>
    <span class="koboSpan" id="kobo.331.1">
     provides a structured way to integrate best practices in software engineering, such as modularity and reusability, into your
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.332.1">
      NLP pipelines.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-109">
    <a id="_idTextAnchor108">
    </a>
    <span class="koboSpan" id="kobo.333.1">
     Summary
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.334.1">
     In this chapter, we explored how to leverage
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.335.1">
      large language models
     </span>
    </strong>
    <span class="koboSpan" id="kobo.336.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.337.1">
      LLMs
     </span>
    </strong>
    <span class="koboSpan" id="kobo.338.1">
     ) within spaCy pipelines using the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.339.1">
      spacy-llm
     </span>
    </strong>
    <span class="koboSpan" id="kobo.340.1">
     library.
    </span>
    <span class="koboSpan" id="kobo.340.2">
     We reviewed the basics of LLMs and prompt engineering, emphasizing their role as versatile tools capable of performing a wide range of NLP tasks, from text categorization to summarization.
    </span>
    <span class="koboSpan" id="kobo.340.3">
     However, we also noted the limitations of LLMs, such as their high computational cost and the tendency to hallucinate.
    </span>
    <span class="koboSpan" id="kobo.340.4">
     We then demonstrated how to integrate LLMs into spaCy pipelines by defining tasks and models.
    </span>
    <span class="koboSpan" id="kobo.340.5">
     Specifically, we implemented a summarization task and, subsequently, created a custom task to extract the context from a quote.
    </span>
    <span class="koboSpan" id="kobo.340.6">
     This process involved creating Jinja templates for prompts and defining methods for generating and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.341.1">
      parsing responses.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.342.1">
     In the next chapter, we’re going back to more traditional machine learning and learn how to annotate data and train a pipeline component from scratch
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.343.1">
      using spaCy.
     </span>
    </span>
   </p>
  </div>
 </body></html>