["```py\n$ aws bedrock list-foundation-models\n```", "```py\n$ aws bedrock-runtime invoke-model \\\n  --model-id mistral.mistral-7b-instruct-v0:2 \\\n  --body \"{\\\"prompt\\\":\\\"<s>[INST]100 words tweet on MLOps with\nAmazon SageMaker [/INST]\\\", \\\"max_tokens\\\":200, \\\"temperature\\\":0.5}\" \\\n  --cli-binary-format raw-in-base64-out \\\n  output.txt\n```", "```py\nimport boto3\nbedrock_client = boto3.client(service_name='bedrock')\nbedrock_client.list_foundation_models()\n```", "```py\nimport boto3\nbedrock_client = boto3.client(service_name='bedrock')\nbedrock_client.get_foundation_model(modelIdentifier='meta.llama3-70b-instruct-v1:0')\n```", "```py\nimport boto3\nimport json\nmodel_id = 'meta.llama3-70b-instruct-v1:0' # change this to use a different version from the model provider\nprompt_data = \"What is the significance of the number 42?\"\n# Following the request syntax of invoke_model, you can create request body with the below prompt and respective inference parameters.\npayload = json.dumps({\n    'prompt': prompt_data,\n    'max_gen_len': 512,\n    'top_p': 0.5,\n    'temperature': 0.5,\n})\nbedrock_runtime = boto3.client(\n    service_name='bedrock-runtime',\n    region_name='us-east-1'\n)\nresponse = bedrock_runtime.invoke_model(\n    body=payload,\n    modelId=model_id,\n    accept='application/json',\n    contentType='application/json'\n)\nresponse_body = json.loads(response.get('body').read())\nprint(response_body.get('generation'))\n```", "```py\nimport boto3\nimport json\nbedrock_client = boto3.client('bedrock-runtime',region_name='us-east-1')\nprompt = \"\"\"\nTask: Compose an email to customer support team.\nOutput:\n\"\"\"\nmessages = [{ \"role\":'user', \"content\":[{'type':'text','text': prompt}]}]\nmax_tokens=512\ntop_p=1\ntemp=0.5\nsystem = \"You are an AI Assistant\"\nbody=json.dumps(\n        {\n            \"anthropic_version\": \"bedrock-2023-05-31\",\n            \"max_tokens\": max_tokens,\n            \"messages\": messages,\n            \"temperature\": temp,\n            \"top_p\": top_p,\n            \"system\": system\n        }\n    )\nmodelId = \"anthropic.claude-3-sonnet-20240229-v1:0\"\naccept = \"application/json\"\ncontentType = \"application/json\"\nresponse = bedrock_client.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\nresponse_body = json.loads(response.get('body').read())\nprint(response_body)\n```", "```py\nfrom IPython.display import clear_output, display, display_markdown, Markdown\nimport boto3, json\nbrt = boto3.client(service_name='bedrock-runtime', region_name='us-east-1'\n)\npayload = json.dumps({\n    'prompt': '\\n\\nHuman: write a blog on quantum computing in 500 words.\\n\\nAssistant:',\n    'max_tokens_to_sample': 4096\n})\nresponse = brt.invoke_model_with_response_stream(\n    modelId='anthropic.claude-v2',\n    body=payload,\n    accept='application/json',\n    contentType='application/json'\n)\nstreaming = response.get('body')\noutput = []\nif streaming:\n    for event in streaming:\n        chunk = event.get('chunk')\n        if chunk:\n            chunk_object = json.loads(chunk.get('bytes').decode())\n            text = chunk_object['completion']\n            clear_output(wait=True)\n            output.append(text)\n            display_markdown(Markdown(''.join(output)))\n```", "```py\n# Install the latest version for boto3 to leverage Converse API. We start with uninstalling the previous version\n%pip install boto3==1.34.131\n# Import the respective libraries\nimport boto3\nimport botocore\nimport os\nimport json\nimport sys\n#Ensure you have the latest version of boto3 to invoke Converse API\nprint(boto3.__version__)\n#Create client side Amazon Bedrock connection with Boto3 library\nregion = os.environ.get(\"AWS_REGION\")\nbedrock_client = boto3.client(service_name='bedrock-runtime',region_name=region)\nmodel_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n# Inference parameters\ntop_k = 100\ntemp = 0.3\n# inference model request fields\nmodel_fields = {\"top_k\": top_k}\n# Base inference parameters\ninference_configuration = {\"temperature\": temp}\n# Setup the system prompts and messages to send to the model.\nsystem_prompts = [{\"text\": \"You are an expert stylist that recommends different attire for the user based on the occasion.\"}]\nmessage_1 = {\n    \"role\": \"user\",\n    \"content\": [{\"text\": \"Give me top 3 trending style and attire recommendations for my son's graduation party\"}]\n  }\nmessages = []\n# Start the conversation with the 1st message.\nmessages.append(message_1)\n# Send the message.\nresponse = bedrock_client.converse(\n        modelId=model_id,\n        messages=messages,\n        system=system_prompts,\n        inferenceConfig=inference_configuration,\n        additionalModelRequestFields=model_fields\n    )\n# Add the response message to the conversation.\noutput_message = response['output']['message']\nprint(output_message['content'][0]['text'])\n```", "```py\n# Send the message.\nmodel_response = bedrock_client.converse_stream(\n        modelId=model_id,\n        messages=messages,\n        system=system_prompts,\n        inferenceConfig=inference_config,\n        additionalModelRequestFields=additional_model_fields\n    )\n# # Add the response message to the conversation.\nstream = model_response.get('stream')\nif stream:\n    for event in stream:\n        if 'contentBlockDelta' in event:\nprint(event['contentBlockDelta']['delta']['text'], end=\"\")\n```", "```py\n# Installing LangChain\n!pip install langchain\n#import the respective libraries and packages\nimport os\nimport sys\nimport json\nimport boto3\nimport botocore\n# You need to specify LLM for LangChain Bedrock class, and can pass arguments for inference.\nfrom langchain.llms.bedrock import Bedrock\n#Create boto3 client for Amazon Bedrock-runtime\nbedrock_client = boto3.client(service_name=\"bedrock-runtime\", region_name='us-east-1')\n#Provide the respective model ID of the FM you want to use\nmodelId=\"amazon.titan-tg1-large\"\n#Pass the Model ID and respective arguments to the LangChain Bedrock Class\nllm = Bedrock(\n    model_id=modelId,\n    model_kwargs={\n        \"maxTokenCount\": 4096,\n        \"stopSequences\": [],\n        \"temperature\": 0,\n        \"topP\": 1,\n    },\n    client=bedrock_client,\n)\n#Provide Sample prompt data\nprompt_data = \"Tell me about LangChain\"\n#Invoke the LLM\nresponse = llm(prompt_data)\nprint(response)\n```", "```py\n#import the respective libraries and packages\nimport os\nimport sys\nimport boto3\nimport json\nimport botocore\n# You need to specify LLM for LangChain Bedrock class, and can pass arguments for inference.\nfrom langchain_aws import BedrockLLM\n#Create boto3 client for Amazon Bedrock-runtime\nbedrock_client = boto3.client(service_name=\"bedrock-runtime\", region_name='us-east-1')\nfrom langchain.prompts import PromptTemplate\n# Create a prompt template that has multiple input variables\nmulti_var_prompt = PromptTemplate(\n    input_variables=[\"leasingAgent\", \"tenantName\", \"feedbackFromTenant\"],\n    template=\"\"\"\n<s>[INST] Write an email from the Leasing Agent {leasingAgent} to {tenantName} in response to the following feedback that was received from the customer:\n<customer_feedback>\n{feedbackFromTenant}\n</customer_feedback> [/INST]\\\n\"\"\"\n)\n# Pass in values to the input variables\nprompt_data = multi_var_prompt.format(leasingAgent=\"Jane\",\n                                 tenantName=\"Isabella\",\n                                 feedbackFromTenant=\"\"\"Hi Jane,\n     I have been living in this apartment for 2 years now, and I wanted to appreciate how lucky I am to be living here. I have hardly faced any issues in my apartment, but when any issue occurs, administration staff is always there to fix the problem, and are very polite. They also run multiple events throughout the year for all the tenants which helps us socialize. The best part of the apartment is it's location and it is very much affordable.\n     \"\"\")\n#Provide the respective model ID of the FM you want to use\nmodelId = 'mistral.mistral-large-2402-v1:0' # change this to use a different version from the model provider\n#Pass the Model ID and respective parameters to the Langchain Bedrock Class\nllm = BedrockLLM(\n    model_id=modelId,\n    model_kwargs={\n        \"max_tokens\": 4096,\n        \"temperature\": 0.5,\n        \"top_p\": 0.5,\n        \"top_k\":50,\n    },\n    client=bedrock_client,\n)\n```", "```py\nresponse = llm(prompt_data)\nemail = response[response.index('\\n')+1:]\nprint(email)\n```"]