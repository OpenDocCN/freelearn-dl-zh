<html><head></head><body>
<div><h1 class="chapterTitle" id="_idParaDest-178">Appendix</h1>
<p class="normal">In the main chapters of this book, we explored the power of OpenAI’s models through the lens of ChatGPT, diving into its conversational interface and understanding how it can revolutionize the way we interact with AI. However, the world of OpenAI extends beyond ChatGPT’s familiar chat-based experience. To fully harness the potential of these models, it’s crucial to understand the broader tools and interfaces OpenAI provides.</p>
<p class="normal">This appendix is dedicated to exploring one such tool: the <strong class="keyWord">OpenAI Playground</strong>. The Playground offers a versatile<a id="_idIndexMarker661"/> environment to experiment with OpenAI’s models, granting more control over parameters, outputs, and behaviors. Whether you want to fine-tune responses, test different use cases, or simply gain a deeper understanding of the models’ capabilities, the Playground is an invaluable resource.</p>
<p class="normal">In this appendix, we will:</p>
<ul>
<li class="bulletList">Walk through the Playground interface and its key features.</li>
<li class="bulletList">Illustrate how to interact with OpenAI models directly from the Playground.</li>
<li class="bulletList">Offer tips and best practices to maximize your outcomes when using the Playground.</li>
</ul>
<p class="normal">By the end of this appendix, you’ll have the knowledge and confidence to use OpenAI’s Playground and its models, going beyond ChatGPT.</p>
<h1 class="heading-1" id="_idParaDest-179">Trying OpenAI models in the Playground</h1>
<p class="normal">To access <a id="_idIndexMarker662"/>an OpenAI <a id="_idIndexMarker663"/>Playground, you need to create an OpenAI account and navigate through <a href="https://platform.openai.com/playground">to https://platform.openai.com/playgro</a>und. This is how the landing page looks:</p>
<figure class="mediaobject"><img alt="A screenshot of a chat  Description automatically generated" src="img/Appendix_01.png"/></figure>
<p class="packt_figref">Figure 1: OpenAI Playground at https://platform.openai.com/playground</p>
<p class="normal">As you can see from <em class="italic">Figure 1</em>, the Playground offers a UI where the user can start interacting with the model, which<a id="_idIndexMarker664"/> you can select at the top of your chat interface. Note that, whenever consuming models via the OpenAI Playground, you<a id="_idIndexMarker665"/> will be charged a fee depending on the amount of interactions. You can find the pricing page at https://openai.com/api/pricing/.</p>
<p class="normal">Before diving deeper into the main sections of the Playground, let’s first define some jargon you will see in this chapter:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Tokens</strong>: Tokens can be considered as word fragments or segments that are used by the API to process input prompts. Unlike complete words, tokens may contain trailing spaces or even word segments. As a general rule of thumb, one token in English is approximately equivalent to four characters, or three-quarters of a word (you can refer to the following link to convert words to tokens in the context of OpenAI models: https://platform.openai.com/tokenizer).</li>
<li class="bulletList"><strong class="keyWord">Prompt</strong>: In the context <a id="_idIndexMarker666"/>of <strong class="keyWord">natural language processing</strong> (<strong class="keyWord">NLP</strong>) and Generative AI, a prompt refers to a piece of text that is given as input to an AI language model to generate a response or output. The prompt can be a question, a statement, or a sentence, and it is used to provide context and direction to the language model.</li>
<li class="bulletList"><strong class="keyWord">Context</strong>: In the field of GPT, context refers to the words and sentences that come before the user’s prompt. This context is used by the language model to generate the most probable next word or phrase, based on the patterns and relationships found in the training data.</li>
<li class="bulletList"><strong class="keyWord">Model confidence</strong>: Model confidence refers to the level of certainty or probability that an AI model assigns to a particular prediction or output. In the context of NLP, model confidence is often used to indicate how confident the AI model is in the correctness or relevance of its generated response to a given input prompt.</li>
<li class="bulletList"><strong class="keyWord">Tools</strong>: With tools, we provide the model with an extra skill that it can invoke to accomplish the user’s task. A function will always have a description in natural language so that the model knows when to invoke it.</li>
</ul>
<p class="normal">In the Playground, there<a id="_idIndexMarker667"/> are four main sections to <a id="_idIndexMarker668"/>interact with the models. Let’s explore them in the next sections.</p>
<h2 class="heading-2" id="_idParaDest-180">Chat</h2>
<p class="normal">Here, you can test all <a id="_idIndexMarker669"/>the chat models available today, including both text-only models (like GPT-3.5) and multimodal models (like GPT-4o). You can provide a system message – the set of instructions that you provide your model with – all in natural language.</p>
<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal">In the context of LLMs, the system message is an instruction provided at the beginning of a conversation to establish the model’s role, behavior, and response guidelines. This message sets the overarching context, guiding the model’s interactions to align with specific objectives or constraints. For example, a system message might specify that the model should act as a friendly travel advisor or maintain a formal tone. This configuration can be set at the backend level by the AI developer, so that the end user will not have access to it and, henceforth, will not be able to “force” the model to behave differently.</p>
</div>
<p class="normal">You can also compare the output of two different models, given the same question. The following is an example of how to do that:</p>
<figure class="mediaobject"><img alt="A screenshot of a chat  Description automatically generated" src="img/Appendix_02.png"/></figure>
<p class="packt_figref">Figure 2: An example of comparison between two models</p>
<p class="normal">For each model, you can also <a id="_idIndexMarker670"/>play with some parameters that you can configure. Here is a list:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Temperature</strong> (ranging from 0 to 2): This controls the randomness of the model’s response. A low-level temperature makes your model more deterministic, meaning that it will tend to give the same output to the same question. For example, if I ask my model multiple times <em class="italic">What is OpenAI?</em> with the temperature set as 0, it will give, most of the time, the same answer. On the other hand, if I do the same with a temperature greater than 0, it will try to modify its answers each time, in terms of wording and style.</li>
<li class="bulletList"><strong class="keyWord">Max tokens</strong>: This controls the length (in terms of tokens) of the model’s response to the user’s prompt.</li>
<li class="bulletList"><strong class="keyWord">Stop sequences</strong> (user input): This makes responses end at the desired point, such as the end of a sentence or list.</li>
<li class="bulletList"><strong class="keyWord">Top probabilities</strong> (ranging from 0 to 1): This controls which tokens the model will consider when generating a response. This means that the model will select from the smallest set of tokens whose cumulative probability adds up to 90% of the distribution.</li>
<li class="bulletList"><strong class="keyWord">Frequency penalty</strong> (ranging from 0 to 1): This controls the repetition of the same tokens in the generated response. The higher the penalty, the lower the probability of seeing the same tokens more than once in the same response. The penalty reduces the chance proportionally, based on how often a token has appeared in the text so far (this is the key difference from the following parameter).</li>
<li class="bulletList"><strong class="keyWord">Presence penalty</strong> (ranging from 0 to 2): This is similar to the previous parameter but stricter. It reduces the chance of repeating any token that has appeared in the text at all so far. As it is stricter than the frequency penalty, the presence penalty also increases the <a id="_idIndexMarker671"/>likelihood of introducing new topics in a response.</li>
</ul>
<h2 class="heading-2" id="_idParaDest-181">Assistants</h2>
<p class="normal"><strong class="keyWord">OpenAI Assistants</strong> can <a id="_idIndexMarker672"/>be seen as a way to develop AI <a id="_idIndexMarker673"/>agents faster and more easily. In fact, Assistants can be defined as entities powered by an LLM, with a set of instructions to follow and a set of tools or plugins to use.</p>
<p class="normal">In the case of OpenAI Assistants, they come with three pre-built tools:</p>
<ul>
<li class="bulletList"><strong class="screenText">File Search</strong>: This allows the user to upload custom documents so that the Assistant can navigate through them to accomplish the user’s query. It operates with a RAG-based framework.</li>
<li class="bulletList"><strong class="screenText">Function Calling</strong>: This allows the user to define a set of custom functions that can be invoked by the Assistant to accomplish a given task.</li>
<li class="bulletList"><strong class="screenText">Code Interpreter</strong>: This refers to the capability of the Assistant to run code either against provided documents (for example, in the case of spreadsheets or analytical papers that require mathematical computations) or simply to solve complex tasks provided by the user (for example, complex mathematical problems).</li>
</ul>
<p class="normal">In the following screenshot, you can see an example of an Assistant called <strong class="screenText">Chat with PDF</strong>, which specializes in responding to provided documents (in my case, I uploaded the paper <em class="italic">LLaMA: Open and Efficient Foundation Language Models</em> by Hugo Touvron et al.).</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="img/Appendix_03.png"/></figure>
<p class="packt_figref">Figure 3: Example of an OpenAI Assistant</p>
<p class="normal">As you can see from the preceding screenshot, the Assistant was able to answer my question, retrieving knowledge from the provided document. In fact, my question was pretty vague, since the term <em class="italic">toxicity</em> can <a id="_idIndexMarker674"/>refer to multiple<a id="_idIndexMarker675"/> domains; nevertheless, the Assistant knows to watch over the provided documents as the primary source of information.</p>
<h2 class="heading-2" id="_idParaDest-182">Completions</h2>
<p class="normal">This section<a id="_idIndexMarker676"/> refers <a id="_idIndexMarker677"/>to a class of models called <strong class="keyWord">base models</strong>, like GPT-3. They are the basis on which the so-called “assistant models” (or chat models, as we saw previously) are built. For example, the chat model GPT-3.5 Turbo (the model behind ChatGPT) is a fine-tuned version of the base model GPT-3.</p>
<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal">Completions (base) models are designed for generating single responses to prompts, making them suitable for tasks like text generation and summarization without maintaining context over multiple interactions. Chat (assistant) models, on the other hand, are optimized for interactive conversations, capable of maintaining context across multiple turns, and are ideal for applications like chatbots and virtual assistants.</p>
</div>
<p class="normal">Below you can see an example of a typical completion task in the Playground:</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="img/Appendix_04.png"/></figure>
<p class="packt_figref">Figure 4: Example of completion task in OpenAI Playground</p>
<p class="normal">As you can see, using my words “Today I went to a grocery store and” the model completed the sentence with <a id="_idIndexMarker678"/>the most likely words.</p>
<p class="normal">Today, completion models are rarely used as they are outperformed by chat models, yet they can be further fine-tuned to tailored use cases (we will cover fine-tuning later on in this section).</p>
<h2 class="heading-2" id="_idParaDest-183">Text to speech</h2>
<p class="normal">In addition to <em class="italic">Whisper</em>, the<a id="_idIndexMarker679"/> aforementioned<a id="_idIndexMarker680"/> speech-to-text model, OpenAI also released a <strong class="keyWord">text-to-speech</strong> (<strong class="keyWord">TTS</strong>) model that can be tested directly in the Playground.</p>
<p class="normal">Let’s see an example:</p>
<figure class="mediaobject"><img alt="A screenshot of a video chat  Description automatically generated" src="img/Appendix_05.png"/></figure>
<p class="packt_figref">Figure 5: Example of using OpenAI’s TTS models in the Playground</p>
<p class="normal">As you can see from the above screenshot, you can select the voice, model, speed, and format of the generated audio.</p>
<p class="normal">All the previous models come pre-built, in the sense that they have already been pre trained on a huge knowledge base.</p>
<p class="normal">However, there are<a id="_idIndexMarker681"/> some ways you can <a id="_idIndexMarker682"/>make your model more customized and tailored for your use case.</p>
<h2 class="heading-2" id="_idParaDest-184">Customizing your model</h2>
<p class="normal">The first method of <a id="_idIndexMarker683"/>tailoring your model for your use case is embedded in the way the model is designed, and it involves providing your model with the context in the few-shot learning approach.</p>
<p class="normal">For example, you could ask the model to generate an article whose template and lexicon recall another one you have already written. For this, you can provide the model with your query of generating an article and also with the former article as a reference or context, so that the model is better prepared for your request.</p>
<p class="normal">Here is an example of it:</p>
<figure class="mediaobject"><img alt="A screenshot of a chat  Description automatically generated" src="img/Appendix_06.png"/></figure>
<p class="packt_figref">Figure 6: An example of a conversation within the OpenAI Playground with the few-shot learning approach</p>
<p class="normal">In the previous<a id="_idIndexMarker684"/> example, I instructed the model to output only the label of the tweet’s sentiment, providing it with three examples of how to do that.</p>
<p class="normal">The second method of customizing your model is more sophisticated and is called <strong class="keyWord">fine-tuning</strong>. Fine-tuning is <a id="_idIndexMarker685"/>the process of adapting a pre trained model to a new task.</p>
<p class="normal">In fine-tuning, the parameters of the pre trained model are altered, either by adjusting the existing parameters or by adding new parameters, to better fit the data for the new task. This is done by training the model on a smaller labeled dataset that is specific to the new task. The key idea behind fine-tuning is to leverage the knowledge learned from the pre trained model and fine-tune it to the new task, rather than training a model from scratch. Have a look at the following figure:</p>
<figure class="mediaobject"><img alt="" src="img/Appendix_07.png"/></figure>
<p class="packt_figref">Figure 7: Model fine-tuning</p>
<p class="normal">In the preceding figure, you can see a schema on how fine-tuning works on OpenAI pre-built models. The idea is that you have available a pre trained model with general-purpose weights or parameters. Then, you<a id="_idIndexMarker686"/> feed your model with custom data, typically in the form of <em class="italic">key-value</em> prompts and completions, as shown here:</p>
<pre class="programlisting code"><code class="hljs-code">{"prompt": "&lt;prompt text&gt;", "completion": "&lt;ideal generated text&gt;"}
{"prompt": "&lt;prompt text&gt;", "completion": "&lt;ideal generated text&gt;"}
{"prompt": "&lt;prompt text&gt;", "completion": "&lt;ideal generated text&gt;"}
</code></pre>
<p class="normal">Once the training is done, you will have a customized model that performs particularly well for a given task, for example, the classification of your company’s documentation.</p>
<p class="normal">The nice thing about fine-tuning is that you can make pre-built models tailored to your use cases, without the need to re-train them from scratch, yet leveraging smaller training datasets and hence needing less training time and computing. At the same time, the model keeps its generative power and <a id="_idIndexMarker687"/>accuracy learned via the original training, the one that was carried out on the massive dataset.</p>
<h1 class="heading-1" id="_idParaDest-185">Summary</h1>
<p class="normal">The OpenAI Playground presents a powerful tool for experimenting with advanced AI models through zero- or few-shot learning and fine-tuning techniques. The Playground allows users to interact directly with pre trained models, making it easier to customize and enhance them for specific tasks, such as sentiment analysis or document classification.</p>
<p class="normal">For developers looking to build AI applications that leverage OpenAI’s API, mastering these techniques is crucial to ascertain whether a specific model’s configuration will meet a specific application’s requirements.</p>
<p class="normal">Despite the focus of this book being mainly on ChatGPT, enterprise-scale scenarios (which we covered in <em class="italic">Chapter 10</em>) require more customized approaches when it comes to AI use cases; that’s why familiarizing yourself with the concept of the Playground and OpenAI models’ APIs is of a great value to embrace the mindset of this new wave of AI-powered application development.</p>
<h1 class="heading-1">Join our communities on Discord and Reddit</h1>
<p class="normal">Have questions about the book or want to contribute to discussions on Generative AI and LLMs? Join our Discord server at <a href="Appendix.xhtml">https://packt.link/I1tSU</a> and our Reddit channel at <a href="Appendix.xhtml">https://packt.link/jwAmA</a> to connect, share, and collaborate with like-minded enthusiasts.</p>
<p class="normal"><img alt="" src="img/Discord.png"/> <img alt="" src="img/QR_Code757615820155951000.png"/></p>
</div>


<div><p class="BM-packtLogo"><img alt="" src="img/New_Packt_Logo1.png"/></p>
<p class="normal"><a href="http://packt.com">packt.com</a></p>
<p class="normal">Subscribe to our online digital library for full access to over 7,000 books and videos, as well as industry leading tools to help you plan your personal development and advance your career. For more information, please visit our website.</p>
<h1 class="heading-1" id="_idParaDest-186">Why subscribe?</h1>
<ul>
<li class="bulletList">Spend less time learning and more time coding with practical eBooks and Videos from over 4,000 industry professionals</li>
<li class="bulletList">Improve your learning with Skill Plans built especially for you</li>
<li class="bulletList">Get a free eBook or video every month</li>
<li class="bulletList">Fully searchable for easy access to vital information</li>
<li class="bulletList">Copy and paste, print, and bookmark content</li>
</ul>
<p class="normal">At <a href="http://www.packt.com">www.packt.com</a>, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts and offers on Packt books and eBooks.</p>
<p class="eop"/>
<h1 class="mainHeading" id="_idParaDest-187">Other Books You May Enjoy</h1>
<p class="normal">If you enjoyed this book, you may be interested in these other books by Packt:</p>
<p class="BM-bookCover"><a href="https://www.packtpub.com/en-in/product/generating-creative-images-with-dall-e-3-9781835089903"><img alt="" src="img/9781835087718.png"/></a></p>
<p class="normal"><strong class="keyWord">Generating Creative Images With DALL-E 3</strong></p>
<p class="normal">Holly Picano</p>
<p class="normal">ISBN: 9781835087718</p>
<ul>
<li class="bulletList">Master DALL-E 3’s architecture and training methods</li>
<li class="bulletList">Create fine prints and other AI-generated art with precision</li>
<li class="bulletList">Seamlessly blend AI with traditional artistry</li>
<li class="bulletList">Address ethical dilemmas in AI art</li>
<li class="bulletList">Explore the future of digital creativity</li>
<li class="bulletList">Implement practical optimization techniques for your artistic endeavors</li>
</ul>
<p class="eop"/>
<p class="BM-bookCover"><a href="https://www.packtpub.com/en-in/product/building-ai-applications-with-openai-apis-9781835884010"><img alt="" src="img/9781835884003.png"/></a></p>
<p class="normal"><strong class="keyWord">Building AI Applications with OpenAI APIs</strong></p>
<p class="normal">Martin Yanev</p>
<p class="normal">ISBN: 9781835884003</p>
<ul>
<li class="bulletList">Develop a solid foundation in using the OpenAI API for NLP tasks</li>
<li class="bulletList">Build, deploy, and integrate payments into various desktop and SaaS AI applications</li>
<li class="bulletList">Integrate ChatGPT with frameworks such as Flask, Django, and Microsoft Office APIs</li>
<li class="bulletList">Unleash your creativity by integrating DALL-E APIs to generate stunning AI art within your desktop apps</li>
<li class="bulletList">Experience the power of Whisper API’s speech recognition and text-to-speech features</li>
<li class="bulletList">Find out how to fine-tune ChatGPT models for your specific use case</li>
<li class="bulletList">Master AI embeddings to measure the relatedness of text strings</li>
</ul>
<p class="eop"/>
<h1 class="heading-1" id="_idParaDest-188">Packt is searching for authors like you</h1>
<p class="normal">If you’re interested in becoming an author for Packt, please visit <a href="http://authors.packtpub.com">authors.packtpub.com</a> and apply today. We have worked with thousands of developers and tech professionals, just like you, to help them share their insight with the global tech community. You can make a general application, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.</p>
</div>
<div><p class="eop"/>
<h1 class="heading-1" id="_idParaDest-189">Share your thoughts</h1>
<p class="normal">Now you’ve finished <em class="italic">Practical Generative AI with ChatGPT, Second Edition</em>, we’d love to hear your thoughts! If you purchased the book from Amazon, please <a href="https://packt.link/r/1836647859">click here to go straight to the Amazon review page</a> for this book and share your feedback or leave a review on the site that you purchased it from.</p>
<p class="normal">Your review is important to us and the tech community and will help us make sure we’re delivering excellent quality content.</p>
</div>
</body></html>