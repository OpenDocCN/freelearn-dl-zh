- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Blocks of Intelligent Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the rapidly evolving landscape of software development, a new class of applications
    is emerging: intelligent applications. **Intelligent applications** are a superset
    of traditional full stack applications. These applications use **artificial intelligence**
    (**AI**) to deliver highly personalized, context-aware experiences that go beyond
    the capabilities of traditional software.'
  prefs: []
  type: TYPE_NORMAL
- en: Intelligent applications understand complex, unstructured data and use this
    understanding to make decisions and create natural, adaptive interactions.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this chapter is to provide you with an overview of the logical and
    technical building blocks of intelligent applications. The chapter explores how
    intelligent applications extend the capability of traditional full-stack applications,
    the core structures that define them, and how these components function to create
    dynamic, context-aware experiences. By the end of this chapter, you will understand
    how these components fit together to form an intelligent application.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The building blocks of intelligent applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs as reasoning engines for intelligent applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector embedding models and vector databases as semantic long-term memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model hosting infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter is theoretical. It covers the logical components of intelligent
    applications and how they fit together.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter assumes fundamental knowledge of traditional full stack application
    development components, such as servers, clients, databases, and APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Defining intelligent applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditional applications typically consist of a client-side user interface,
    a server-side backend, and a database for data storage and retrieval. They perform
    tasks following a strict set of instructions. Intelligent applications require
    a client, server, and database as well, but they augment the traditional stack
    with AI components.
  prefs: []
  type: TYPE_NORMAL
- en: Intelligent applications stand out by understanding complex, unstructured data
    to enable natural, adaptive interactions and decision-making. Intelligent applications
    can engage in open-ended interactions, generate novel content, and make autonomous
    decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of intelligent applications include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Chatbots that provide natural language responses based on external data using
    **retrieval-augmented generation** (**RAG**). For example, Perplexity.ai ([https://www.perplexity.ai/](https://www.perplexity.ai/))
    is an AI-powered search engine and chatbot that provides users with AI-generated
    answers to their queries based on sources retrieved from the web.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Content generators that let you use natural language prompts to create media
    such as images, video, and audio. There are a variety of intelligent content generators
    focusing on different media types, such as Suno ([https://suno.com/](https://suno.com/))
    for text-to-song, Midjourney ([https://www.midjourney.com/home](https://www.midjourney.com/home))
    for text-to-image, and Runway ([https://runwayml.com/](https://runwayml.com/))
    for text-to-video.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommendation systems that use customer data to provide personalized suggestions
    based on their preferences and history. These suggestions can be augmented with
    natural language to further personalize the customer experience. An example of
    this is Spotify’s AI DJ ([https://support.spotify.com/us/article/dj/](https://support.spotify.com/us/article/dj/)),
    which creates a personalized radio station, including LLM-generated DJ interludes,
    based on your listening history.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These examples are a few early glances at the new categories of intelligent
    applications that developers have only started to build. In the next section,
    you will learn more about the core components of intelligent applications.
  prefs: []
  type: TYPE_NORMAL
- en: The building blocks of intelligent applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the heart of intelligent applications are two key building blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The reasoning engine**: The reasoning engine is the brain of an intelligent
    application, responsible for understanding user input, generating appropriate
    responses, and making decisions based on available information. The reasoning
    engine is typically powered by **large language models** (**LLMs**)—AI models
    that perform text completion. LLMs can understand user intent, generate human-like
    responses, and perform complex cognitive tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semantic memory**: Semantic memory refers to the application’s ability to
    store and retrieve information in a way that preserves its meaning and relationships,
    enabling the reasoning engine to access relevant context as needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Semantic memory consists of two core components:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**AI vector embedding model**: AI vector embedding models represent the semantic
    meaning of unstructured data, such as text or images, in large arrays of numbers.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vector database**: Vector databases efficiently store and retrieve vectors
    to support semantic search and context retrieval.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The reasoning engine can retrieve and store relevant information from the semantic
    memory, using unstructured data to inform its outputs.
  prefs: []
  type: TYPE_NORMAL
- en: The LLMs and embedding models that power intelligent applications have different
    hardware requirements than traditional applications, especially at scale. Intelligent
    applications require specialized model hosting infrastructure that can handle
    the unique hardware and scalability requirements of AI workloads. Intelligent
    applications also incorporate continuous learning, safety monitoring, and human
    feedback to ensure quality and integrity.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are the vital organ for intelligent applications. The next section will
    provide a deeper understanding of the role of LLMs in intelligent applications.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs – reasoning engines for intelligent apps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs are the key technology of intelligent applications, unlocking whole new
    classes of AI-powered systems. These models are trained on vast amounts of text
    data to understand language, generate human-like text, answer questions, and engage
    in dialogue.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs undergo continuous improvement with the release of new models. featuring
    billions or trillions of parameters and enhanced reasoning, memory, and multi-modal
    capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Use cases for LLM reasoning engines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs have emerged as a powerful general-purpose technology for AI systems, analogous
    to the **central processing unit** (**CPU**) in traditional computing. Much like
    CPUs, LLMs serve as general-purpose computational engines that can be programmed
    for many tasks and play a similar role in language-based reasoning and generation.
    The general-purpose nature of LLMs lets developers use their capabilities for
    a wide range of reasoning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'A crop of techniques to leverage the diverse abilities of LLMs have emerged,
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt engineering**: Using carefully crafted prompts, developers can steer
    LLMs to perform a wide range of language tasks. A key advantage of prompt engineering
    is its iterative nature. Since prompts are fundamentally just text, it’s easy
    to rapidly experiment with different prompts and see the results. Advanced prompt
    engineering techniques, such as chain-of-thought prompting (which encourages the
    model to break down its reasoning into a series of steps) and multi-shot prompting
    (which provides the model with example input/output pairs), can further enhance
    the quality and reliability of LLM-generated text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-tuning**: Fine-tuning involves starting with a pre-trained general-purpose
    model and further training it on a smaller dataset relevant to the target task.
    This can yield better results than prompt engineering alone, but it comes with
    certain caveats, such as being more expensive and time-consuming. You should only
    fine-tune after exhausting what you can achieve through prompt engineering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retrieval augmentation**: Retrieval augmentation interfaces LLMs with external
    knowledge, allowing them to draw on up-to-date, domain-specific information. In
    this approach, relevant information is retrieved from a knowledge base and injected
    into the prompt, enabling the LLM to generate contextually relevant outputs. Retrieval
    augmentation mitigates the limitations of the static pre-training of LLMs, keeping
    their knowledge updated and reducing the likelihood of the model hallucinating
    incorrect information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these techniques, you can use LLMs for a diverse array of tasks. The next
    section explores current use cases for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Diverse capabilities of LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While fundamentally *just* language models, LLMs have shown surprising emergent
    capabilities ([https://arxiv.org/pdf/2307.06435](https://arxiv.org/pdf/2307.06435)).
    As of writing in spring 2024, state-of-the-art language models are capable of
    performing tasks of the following categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text generation and completion**: Given a prompt, LLMs can generate coherent
    continuations, making them useful for tasks such as content creation, text summarization,
    and code completion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open-ended dialogue and chat**: LLMs can engage in back-and-forth conversations,
    maintaining context and handling open-ended user queries and follow-up questions.
    This capability is foundational for chatbots, virtual assistants, tutoring systems,
    and similar applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Question answering**: LLMs can provide direct answers to user questions,
    perform research, and synthesize information to address queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification and sentiment analysis**: LLMs can classify text into predefined
    categories and assess sentiment, emotion, and opinion. This enables applications
    such as content moderation and customer feedback analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data transformation and extraction**: LLMs can map unstructured text into
    structured formats and extract key information, such as named entities, relationships,
    and events. This makes LLMs valuable for tasks such as data mining, knowledge
    graph construction, and **robotic process** **automation** (**RPA**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As LLMs continue to grow in scale and sophistication, new capabilities are constantly
    emerging, often in surprising ways that were not directly intended by the original
    training objective.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the ability of GPT-3 to generate functioning code was an unexpected
    discovery. With advancements in the field of LLMs, we can expect to see more impressive
    and versatile capabilities emerge, further expanding the potential of intelligent
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-modal language models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Multi-modal language models** hold particular promise for expanding the capabilities
    of language models. Multi-modal models can process and generate images, speech,
    and video in addition to text, and have become an important component of intelligent
    applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of new application categories made possible with multi-modal models
    include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating content based on multiple input types, such as a chatbot where users
    can provide both images and text as inputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced data analysis, such as a medical diagnosis tool that analyzes X-rays
    along with medical records.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time translation, taking audio or images of one language and translating
    it to another language.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Such examples highlight how multi-modal language models can enhance the possible
    use cases for language models.
  prefs: []
  type: TYPE_NORMAL
- en: A paradigm shift in AI development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The rise of LLMs represents a paradigm shift in the development of AI-powered
    applications. Previously, many reasoning tasks required specially trained models,
    which were time-intensive and computationally expensive to create. Developing
    these models often necessitated dedicated **machine learning** (**ML**) engineering
    teams with specialized expertise.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the general-purpose nature of LLMs allows most software engineers
    to leverage their capabilities through simple API calls and prompt engineering.
    While there is still an art and science to optimizing LLM-based workflows for
    production deployability, the process is significantly faster and more accessible
    compared to traditional ML approaches.
  prefs: []
  type: TYPE_NORMAL
- en: This shift has dramatically reduced the total cost of ownership and development
    timelines for AI-powered applications. NLP tasks that previously could take months
    of work by a sophisticated ML engineering team can now be achieved by a single
    software engineer with access to an LLM API and some prompt engineering skills.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, LLMs have unlocked entirely new classes of applications that were
    previously not possible or practical to develop. The ability of LLMs to understand
    and generate human-like text, engage in open-ended dialogue, and perform complex
    reasoning tasks has opened up a wide range of possibilities for intelligent applications
    across industries.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll learn more about LLMs in [*Chapter 3*](B22495_03.xhtml#_idTextAnchor041),
    *Large Language Models*, which discusses their history and how they operate.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding models and vector databases – semantic long-term memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to the reasoning capabilities provided by LLMs, intelligent applications
    require semantic long-term memory for storing and retrieving information.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic memory typically consists of two core components—AI vector embedding
    models and vector databases. Vector embedding models represent the semantic meaning
    of unstructured data, such as text or images, in large arrays of numbers. Vector
    databases efficiently store and retrieve these vectors to support semantic search
    and context retrieval. These components work together to enable the reasoning
    engine to access relevant context and information as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Embedding models** are AI models that map text and other data types, such
    as images and audio, into high-dimensional vector representations. These vector
    representations capture the semantic meaning of the input data, allowing for efficient
    similarity comparisons and semantic search, typically using cosine similarity
    as the distance metric.'
  prefs: []
  type: TYPE_NORMAL
- en: Embedding models encode semantic meaning into a machine-interpretable format.
    By representing similar concepts as nearby points in the vector space, embedding
    models let us measure the semantic similarity between pieces of unstructured data
    and perform semantic search across a large corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-trained embedding models are widely available and can be fine-tuned for
    specific domains or use cases. Compared to LLMs, embedding models tend to be more
    affordable and can run on limited hardware, making them accessible to a wider
    range of applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common applications of embedding models include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Semantic search and retrieval**: Embedding models can be used as a component
    in larger AI systems to retrieve relevant context for LLMs, especially in RAG
    architectures. RAG is a particularly important use case for the intelligent applications
    discussed in this book and will be covered in more detail in [*Chapter 8*](B22495_08.xhtml#_idTextAnchor180),
    *Implementing Vector Search in* *AI Applications*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recommendation systems**: By representing items and user preferences as embeddings,
    recommendation systems can identify similar items and generate personalized recommendations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering and topic modeling**: Embedding models can help discover latent
    topics and themes in large datasets, which can be useful for analyzing user interactions
    with intelligent applications, such as identifying frequently asked questions
    in a chatbot.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anomaly** **detection**: By identifying outlier vectors that are semantically
    distant from the norm, embedding models can be used for anomaly detection in various
    domains.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Analyzing relationships between entities**: Embedding models can uncover
    hidden relationships and connections between entities based on their semantic
    similarity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will explore the technical details and practical considerations of embedding
    models in [*Chapter 4*](B22495_04.xhtml#_idTextAnchor061), *Embedding Models*.
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Vector databases** are specialized data stores optimized for storing and
    searching high-dimensional vectors. They provide fast, **approximate nearest neighbor**
    (**ANN**) search capabilities that allow intelligent applications to quickly store
    and retrieve relevant information based on spatial proximity.'
  prefs: []
  type: TYPE_NORMAL
- en: ANN search is necessary because performing exact similarity calculations against
    every vector in the database becomes computationally expensive as the database
    grows in size. Vector databases use algorithms, such as **hierarchical navigable
    small worlds** (**HNSW**), to efficiently find approximate nearest neighbors,
    making vector search feasible at scale.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to ANN search, vector databases typically support filtering and
    exact search on metadata associated with the vectors. The exact functionality
    and performance of these features vary across different vector database products.
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases provide an intelligent application with low-latency retrieval
    of relevant information given a query. Using the semantic meaning of the content
    for search, vector databases align with the way LLMs reason about information,
    enabling the application to apply the same unstructured data format for long-term
    memory as it does for reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: In applications that use RAG, the vector database plays a crucial role. The
    application generates a query embedding, which is used to retrieve relevant context
    from the vector database. Multiple relevant chunks are then provided as context
    to the LLM, which uses this information to generate informed and relevant responses.
  prefs: []
  type: TYPE_NORMAL
- en: You will learn about the technical details and practical considerations of vector
    databases in [*Chapter 5*](B22495_05.xhtml#_idTextAnchor115), *Vector Databases*.
  prefs: []
  type: TYPE_NORMAL
- en: Model hosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To implement AI models in your intelligent application, you must host them on
    computers, either in a data center or the cloud. This process is known as **model
    hosting**. Hosting AI models for applications presents a different set of requirements
    compared to hosting traditional software. Running AI models at scale requires
    powerful **graphics processing units** (**GPUs**) and configuring the software
    environment to load and execute the model efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: The key challenges in model hosting include high computational requirements
    and hardware costs, limited availability of GPU resources, complexity in managing
    and scaling the hosting infrastructure, and potential vendor lock-in or limited
    flexibility when using proprietary solutions. As a result, hardware and cost constraints
    must be factored into the application design process more than ever.
  prefs: []
  type: TYPE_NORMAL
- en: Self-hosting models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The term **self-hosting models** refers to the practice of deploying and running
    AI models, such as LLMs, on an organization’s own infrastructure and hardware
    resources. In this approach, the organization is responsible for setting up and
    maintaining the necessary computational resources, software environment, and infrastructure
    required to load and execute the models.
  prefs: []
  type: TYPE_NORMAL
- en: Self-hosting AI models requires a significant upfront investment in specialized
    hardware, which can be cost-prohibitive for many organizations. Managing the model
    infrastructure also imposes an operational burden that requires ML expertise,
    which many software teams lack. This can divert the focus from the core application
    and business logic.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling self-hosted models to ensure availability can be challenging, as models
    can be large and take time to load into memory. Organizations may need to provision
    significant excess capacity to handle peak loads. Additionally, maintaining and
    updating models is a complex task, as models can go stale over time and require
    retraining or fine-tuning. With the active research in the field, new models and
    techniques constantly emerge, making it difficult for organizations to keep up.
  prefs: []
  type: TYPE_NORMAL
- en: Model hosting providers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The challenges associated with self-hosting have made model hosting providers
    a popular choice for intelligent application development.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model hosting providers** are cloud-based services that offer a platform
    for deploying, running, and managing AI models, such as LLMs, on their infrastructure.
    These providers handle the complexities of setting up, maintaining, and scaling
    the infrastructure required to load and execute the models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model hosting providers offer several benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Outsourced hardware and infrastructure management**: Model hosting providers
    handle provisioning, scaling, availability, security, and other infrastructure
    concerns, allowing application teams to focus on their core product.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost efficiency and flexible pricing**: With model hosting providers, organizations
    pay only for what they use and can scale resources up and down as needed, reducing
    upfront investment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Access to a wide range of models**: Providers curate and host many state-of-the-art
    models, continuously integrating the latest research. They often add additional
    features and optimizations to the raw models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support and expertise**: Providers can offer consultation on model selection,
    prompt engineering, application architecture, and assistance with fine-tuning,
    data preparation, evaluation, and other aspects of AI development.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rapid prototyping and experimentation**: Model hosting providers enable developers
    to quickly test different models and approaches, adapting to new developments
    in the fast-moving AI/ML space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability and reliability**: Providers build robust, highly available,
    and auto-scaling infrastructure to meet the demands of production-scale intelligent
    applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of model hosting providers include those from model developers such
    as OpenAI, Anthropic, and Cohere, as well as offerings from cloud providers such
    as AWS Bedrock, Google Vertex AI, and Azure AI Studio.
  prefs: []
  type: TYPE_NORMAL
- en: Your (soon-to-be) intelligent app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With LLMs, embedding models, vector databases, and model hosting, you have
    the key building blocks for creating intelligent applications. While the specific
    architecture will vary depending on your use case, a common pattern emerges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LLMs** for reasoning and generation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embeddings** and **vector search** for retrieval and memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model hosting** to serve these components at scale'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This AI stack is integrated with traditional application components, such as
    backend services, APIs, frontend user interfaces, databases, and data pipelines.
    Additionally, intelligent applications often include components for AI-specific
    concerns, such as prompt management and optimization, data preparation and embedding
    generation, and AI safety, testing, and monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: The rest of this section walks through an example architecture for a RAG-powered
    chatbot, showcasing how these components work together. The subsequent chapters
    will dive deeper into the end-to-end process of building production-grade intelligent
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Sample application – RAG chatbot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider a simple chatbot application that leverages RAG that lets users talk
    to some documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are seven key components of this application:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Chatbot UI**: A website with a simple chatbot UI that communicates with the
    web server'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Web server**: A Python Flask server to manage conversations between the user
    and the LLM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data ingestion extract, transform, load (ETL) pipeline**: A Python script
    that ingests data from the data sources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text-embedding-3-small` model, hosted by OpenAI'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gpt-4-turbo` model, hosted by OpenAI'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vector store**: MongoDB Atlas Vector Search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MongoDB Atlas**: A database-as-a-service for persisting conversations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This simple example application does not include evaluation or observability
    modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this architecture, there are two key data flows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Chat interaction**: The user communicates with the chatbot with RAG'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data ingestion**: Bringing data from its original sources into the vector
    database'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the chat interaction, the chatbot UI communicates with the chatbot web server,
    which in turn interacts with the LLM, embedding model, and vector store. This
    occurs for every message that the user sends to the chatbot. *Figure 2**.1* shows
    the data flow for the chatbot application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: An example of a basic RAG chatbot conversation data flow'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data flow illustrated in *Figure 2**.1* can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The user sends a message to the chatbot from the web UI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The web UI creates a request to the server with the user’s message.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The web server sends a request to the embedding model API to create a vector
    embedding for the user query. The embedding model API responds with the corresponding
    vector embedding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The web server performs a vector search in the vector database using the query
    vector embedding. The vector store responds with the matching vector search results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The server constructs a message that the LLM will respond to. This message consists
    of a system prompt and a new message that includes the user’s original message
    and the content retrieved from the vector search. The LLM then responds to the
    user message.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The server saves the conversation state to the database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The server returns the LLM-generated message to the user in a response to the
    original request from the web UI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A data ingestion pipeline prepares and enriches data, generates embeddings
    using the embedding model, and populates the vector store and traditional database.
    This pipeline runs as a batch job every 24 hours. *Figure 2**.2* shows an example
    of a data ingestion pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: An example of a RAG chatbot data ingestion ETL pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the data flow shown in *Figure 2**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: The data ingestion ETL pipeline pulls in data from various data sources.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The ETL pipeline cleans the data into a consistent format. It also breaks the
    data into chunks of data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The ETL pipeline calls the embedding model API to generate a vector embedding
    for each data chunk.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The ETL pipeline stores the chunks along with their vector embeddings in a vector
    database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The vector database indexes the embeddings for use with vector search.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While a simple architecture like this can be used to build compelling prototypes,
    transitioning from prototype to production and continuously iterating on the application
    requires addressing many additional considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data ingestion strategy**: Acquiring, cleaning, and preparing the data that
    will be ingested into the vector store or database for retrieval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advanced retrieval patterns**: Incorporating techniques for efficient and
    accurate retrieval of relevant information from the vector store or database,
    such as combining semantic search with traditional filtering, AI-based reranking,
    and query mutation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation and testing**: Adding modules for evaluating model outputs, testing
    end-to-end application flows, and monitoring for potential biases or errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability and performance optimization**: Implementing optimizations such
    as caching, load balancing, and efficient resource management to handle increasing
    workloads and ensure consistent responsiveness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security and privacy**: Securing the application to ensure that users can
    only interact with data that they have permission to, so that user data is handled
    in accordance with relevant policies, standards, and laws.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User experience and interaction design**: Incorporating new generative AI
    interfaces and interaction patterns, such as streaming responses, answer confidence,
    and source citation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous improvement and model updates**: Building processes and systems
    to update AI models safely and reliably and hyperparameters in the intelligent
    application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implications of intelligent applications for software engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The rise of intelligent applications has significant implications for how software
    is made. Developing these intelligent applications requires an extension of traditional
    development skills. The AI engineer must possess an understanding of prompt engineering,
    vector search, and evaluation, as well as familiarity with the latest AI techniques
    and architectures. While a complete understanding of the underlying neural networks
    is not necessary, basic knowledge of **natural language processing** (**NLP**)
    is helpful.
  prefs: []
  type: TYPE_NORMAL
- en: Intelligent application development also introduces new challenges and considerations,
    such as data management and integration with AI components, testing and debugging
    of AI-driven functionality, and addressing the ethical, safety, and security implications
    of AI outputs. The compute-heavy nature of AI workloads also necessitates focusing
    on scalability and cost optimization. Developers building traditional software
    generally do not need to face such concerns.
  prefs: []
  type: TYPE_NORMAL
- en: To address these challenges, software development teams must adapt their processes
    and adopt novel approaches and best practices. This entails implementing AI governance,
    bridging the gap between software and ML/AI teams, and adjusting the development
    lifecycle for intelligent app needs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Intelligent applications represent a new paradigm in software development, combining
    AI with traditional application components to deliver highly personalized, context-aware
    experiences. This chapter details the core components of intelligent applications,
    highlighting the pivotal role of LLMs as reasoning engines. LLMs serve as versatile
    computational tools capable of performing diverse tasks, including chat, summarization,
    and classification, due to their general-purpose design.
  prefs: []
  type: TYPE_NORMAL
- en: Complementing these reasoning engines are embedding models and vector databases,
    which function as the semantic memory of intelligent applications. These components
    enable the reasoning engine to retrieve pertinent context and information as needed.
    Additionally, the hosting of AI models demands dedicated infrastructure, as their
    unique hardware requirements differ significantly from traditional software needs.
    Using building blocks such as LLMs, embedding models, vector databases, and model
    hosting infrastructure, developers can create applications that understand complex,
    unstructured data, engage in open-ended interactions, generate novel content,
    and make autonomous decisions. Building these intelligent applications demands
    a new set of tools, approaches, and best practices.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will examine how LLMs work and the role they play in building
    intelligent applications.
  prefs: []
  type: TYPE_NORMAL
- en: Part 1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Foundations of AI: LLMs, Embedding Models, Vector Databases, and Application
    Design'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This set of chapters provides in-depth and practical knowledge on the techniques
    and principles underpinning AI-intensive applications. You will progress quickly
    from fundamental concepts to real-world use cases and learn best practices for
    building your AI solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part of the book includes the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B22495_03.xhtml#_idTextAnchor041), *Large Language Models*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B22495_04.xhtml#_idTextAnchor061), *Embedding Models*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B22495_05.xhtml#_idTextAnchor115), *Vector Databases*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B22495_06.xhtml#_idTextAnchor137), *AI/ML Application Design*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
