- en: Chapter 3. Advanced Classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: A simple classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language model classifier with tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naïve Bayes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extractors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multithreaded cross validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning parameters in logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizing feature extraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining feature extractors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifier-building life cycle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linguistic tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thresholding classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train a little, learn a little – active learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Annotation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduces more sophisticated classifiers that use different learning
    techniques as well as richer observations about the data (features). We will also
    address the best practices for building machine-learning systems as well as data
    annotation and approaches that minimize the amount of training data needed.
  prefs: []
  type: TYPE_NORMAL
- en: A simple classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe is a thought experiment that should help make clear what machine
    learning does. Recall the *Training your own language model classifier* recipe
    in [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"),
    *Simple Classifiers*, to train your own sentiment classifier in the recipe. Consider
    what a conservative approach to the same problem might be—build `Map<String,String>`
    from the inputs to the correct class. This recipe will explore how this might
    work and what its consequences might be.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Brace yourself; this will be spectacularly stupid but hopefully informative.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the following in the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The usual anemic prompt appears, with some user input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It correctly gets the language as `e` or English. However, everything else
    is about to fail. Next, we will use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We just dropped the final `y` on `#Disney`, and as a result, we got a big confused
    classifier. What happened?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section should really be called *How it doesn't work*, but let's dive into
    the details anyway.
  prefs: []
  type: TYPE_NORMAL
- en: Just to be clear, this recipe is not recommended as an actual solution to a
    classification problem that requires any flexibility at all. However, it introduces
    a minimal example of how to work with LingPipe's `Classification` class as well
    as makes clear what an extreme case of overfitting looks like; this in turn, helps
    demonstrate how machine learning is different from most of standard computer engineering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting with the `main()` method, we will get into standard code slinging
    that should be familiar to you from [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple
    Classifiers"), *Simple Classifiers*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Nothing novel is going on here—we are just training up a classifier, as shown
    in [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"),
    *Simple Classifiers*, and then supplying the classifier to the `Util.consoleInputBestCategory()`
    method. Looking at the class code reveals what is going on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'So, the `handle()` method takes the `text` and `classification` pair and stuffs
    them in `HashMap`. The classifier does nothing else to learn from the data so
    training amounts to memorization of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `classify()` method just does a lookup into `Map` and returns the value
    if there is one, otherwise, we will get the category `n` as the return classification.
  prefs: []
  type: TYPE_NORMAL
- en: What is good about the preceding code is that you have a minimalist example
    of a `BaseClassifier` implementation, and you can see how the `handle()` method
    adds data to the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: What is bad about the preceding code is the utter rigidity of the mapping from
    training data to categories. If the exact example is not seen in training, then
    the `n` category is assumed.
  prefs: []
  type: TYPE_NORMAL
- en: This is an extreme example of overfitting, but it essentially conveys what it
    means to have an overfit model. An overfit model is tailored too close to the
    training data and cannot generalize well to new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s think a bit more about what is so wrong about the preceding classifier
    for language identification—the issue is that entire sentences/tweets are the
    wrong unit of processing. Words/tokens are a much better measure of what language
    is being used. Some improvements that will be borne out in the later recipes are:'
  prefs: []
  type: TYPE_NORMAL
- en: Break the text into words/tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of a match/no-match decision, consider a more nuanced approach. A simple
    *which language matches more words* will be a huge improvement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As languages get closer, for example, British versus American English, probabilities
    can be called for that. Pay attention to likely discriminating words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While this recipe might be comically inappropriate for the task at hand, consider
    trying a sentiment for an even more ludicrous example. It embodies a core assumption
    of much of computer science that the world of inputs is discrete and finite. Machine
    learning can be viewed as a response to a world where this is not the case.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Oddly enough, we often have a need for such a classifier in commercial systems—we
    call it the management classifier; it runs preemptively on data. It has happened
    that a senior VP is unhappy with the system output for some example. This classifier
    then can be trained with the exact case that allows for immediate system fixing
    and satisfaction of the VP.
  prefs: []
  type: TYPE_NORMAL
- en: Language model classifier with tokens
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Chapter 1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"),
    *Simple Classifiers*, covered classification without knowing what tokens/words
    were, with a language model per category—we used character slices or ngrams to
    model the text. [Chapter 2](part0027_split_000.html#page "Chapter 2. Finding and
    Working with Words"), *Finding and Working with Words*, discussed at length the
    process of finding tokens in text, and now we can use them to build a classifier.
    Most of the time, we use tokenized input to classifiers, so this recipe is an
    important introduction to the concept.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This recipe will tell us how to train and use a tokenized language model classifier,
    but it will ignore issues such as evaluation, serialization, deserialization,
    and so on. You can refer to the recipes in [Chapter 1](part0014_split_000.html#page
    "Chapter 1. Simple Classifiers"), *Simple Classifiers*, for examples. This code
    of this recipe is in `com.lingpipe.cookbook.chapter3.TrainAndRunTokenizedLMClassifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The exception of the following code is the same as found in the *Training your
    own language model classifier* recipe in [Chapter 1](part0014_split_000.html#page
    "Chapter 1. Simple Classifiers"), *Simple Classifiers*. The `DynamicLMClassifier`
    class provides a static method for the creation of a tokenized LM classifier.
    Some setup is required. The `maxTokenNgram` variable sets the largest size of
    token sequences used in the classifier—smaller datasets usually benefit from lower
    order (number of tokens) ngrams. Next, we will set up a `tokenizerFactory` method,
    selecting the workhorse tokenizer from [Chapter 2](part0027_split_000.html#page
    "Chapter 2. Finding and Working with Words"), *Finding and Working with Words*.
    Finally, we will specify the categories that the classifier uses:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, the classifier is constructed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the code from the command line or your IDE:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In application, the `DynamicLMClassifier` classifier does not see a great deal
    of use in commercial application. This classifier might be a good choice for an
    author-identification classifier (that is, one that classifies whether a given
    piece of text is written by an author or by someone else) that was highly sensitive
    to turns of phrase and exact word usage. The Javadoc is well worth consulting
    to better understand what this class does.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Naïve Bayes is probably the world's most famous classification technology, and
    just to keep you on your toes, we provide two separate implementations with lots
    of configurability. One of the most well-known applications of a Naïve Bayes classifier
    is for spam filtering in an e-mail.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason the word *naïve* is used is that the classifier assumes that words
    (features) occur independent of one another—this is clearly a naïve assumption,
    but lots of useful and not-so-useful technologies have been based on the approach.
    Some notable features of the traditional naïve Bayes include:'
  prefs: []
  type: TYPE_NORMAL
- en: Character sequences are converted to bags of tokens with counts. No whitespaces
    are considered, and the order of the tokens does not matter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naïve Bayes classifiers require two or more categories into which input texts
    are categorized. These categories must be both exhaustive and mutually exclusive.
    This indicates that a document used for training must only belong to one category.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The math is very simple: `p(category|tokens) = p(category,tokens)/p(tokens)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The class is configurable for various kinds of unknown token models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A naïve Bayes classifier estimates two things. First, it estimates the probability
    of each category, independent of any tokens. This is carried out based on the
    number of training examples presented for each category. Second, for each category,
    it estimates the probability of seeing each token in that category. Naïve Bayes
    is so useful and important that we will show you exactly how it works and plug
    through the formulas. The example we have is to classify hot and cold weather
    based on the text.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will work out the math to calculate the probability of a category
    given a word sequence. Second, we will plug in an example and then verify it using
    the classifier we build.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s lay out the basic formula to calculate the probability of a category
    given a text input. A token-based naïve Bayes classifier computes the joint token
    count and category probabilities as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Conditional probabilities are derived by applying Bayes''s rule to invert the
    probability calculation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will get to expand all these terms. If we look at `p(tokens|cat)`,
    this is where the naïve assumption comes into play. We assume that each token
    is independent, and thus, the probability of all the tokens is the product of
    the probability of each token:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The probability of the tokens themselves, that is, `p(tokens)`, the denominator
    in the preceding equation. This is just the sum of their probability in each category
    weighted by the probability of the category itself:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: When building a naïve Bayes classifier, `p(tokens)` doesn't need to be explicitly
    calculated. Instead, we can use `p(tokens|cat) * p(cat)` and assign the tokens
    to the category with the higher product.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we have laid out each element of our equation, we can look at how these
    probabilities are calculated. We can calculate both these probabilities using
    simple frequencies.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The probability of a category is calculated by counting the number of times
    the category showed up in the training instances divided by the total number of
    training instances. As we know that Naïve Bayes classifiers have exhaustive and
    mutually-exclusive categories, the sum of the frequency of each category must
    equal the total number of training instances:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The probability of a token in a category is computed by counting the number
    of times the token appeared in a category divided by the number of times all the
    other tokens appeared in this category:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These probabilities are calculated to provide what is called the **maximum likelihood
    estimate** of the model. Unfortunately, these estimates provide zero probability
    for tokens that were not seen during the training. You can see this very easily
    in the calculation of an unseen token probability. Since it wasn't seen, it will
    have a frequency count of 0, and the numerator of our original equation goes to
    0.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In order to overcome this, we will use a technique known as **smoothing** that
    assigns a prior and then computes a maximum a posteriori estimate rather than
    a maximum likelihood estimate. A very common smoothing technique is called additive
    smoothing, and it just involves adding a prior count to every count in the training
    data. Two sets of counts are added: the first is a token count added to all the
    token frequency calculations, and the second is a category count, which is added
    to all the category count calculations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This obviously changes the `p(cat)` and the `p(token|cat)` values. Let''s call
    the `alpha` prior that is added to the category count and the `beta` prior that
    is added to the token count. When we call the `alpha` prior, our previous calculations
    will change to:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When we call the `beta` prior, the calculations will change to:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have set up our equations, let's look at a concrete example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We''ll build a classifier to classify whether the forecast calls for hot or
    cold weather based on a set of phrases:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'There are a total of seven tokens in these five training items:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`super`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`steamy`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`today`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`boiling`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`out`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`freezing`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`icy`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Of these, all the tokens appear once, except `steamy`, which appears twice in
    the `hot` category and `out`, which appears once in each category. This is our
    training data. Now, let's calculate the probability of an input text being in
    the `hot` or `cold` category . Let's say our input is the word `super`. Let's
    set the category prior `alpha` to `1` and the token prior `beta` also to `1`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'So, we will calculate the probabilities of `p(hot|super)` and `p(cold|super)`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will take into consideration all the tokens, including the ones that haven''t
    been seen in the `hot` category:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will give us a denominator equal to a sum of these inputs:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, `p(super|hot) = 2/13` is one part of the equation. We still need to calculate
    `p(hot)` and `p (super)`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the `hot` category, we have three documents or cases, and for the `cold`
    category, we have two documents in our training data. So, `freq(hot) = 3` and
    `freq(cold) = 2`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To calculate `p(super|cold)`, we need to repeat the same steps:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This gives us the probability of the token `super`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now have all the pieces together to calculate `p(hot|super)` and `p(cold|super)`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we want to repeat this for the input stream `super super`, the following
    calculations can be used:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Remember our naïve assumption: the probability of the tokens is the product
    of the probabilities, since we assume that they are independent of each other.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's verify our calculations by training up the naïve Bayes classifier and
    using the same input.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s verify some of these calculations in code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In your IDE, run the `TrainAndRunNaiveBayesClassifier` class in the code package
    of this chapter, or using the command line, type the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the prompt, let''s use our first example, `super`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As we can see, our calculations were correct. For the case of a word, `hello`,
    that doesn''t exist in our training; we will fall back to the prevalence of the
    categories modified by the category''s prior counts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Again, for the case of `super super`, our calculations were correct.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The source that generates the preceding output is in `src/com/lingpipe/chapter3/TrainAndRunNaiveBays.java`.
    The code should be straightforward, so we will not covering it in this recipe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more details on configuring naïve Bayes, including length normalizing, refer
    to the Javadoc at [http://alias-i.com/lingpipe/docs/api/index.html?com/aliasi/classify/TradNaiveBayesClassifier.html](http://alias-i.com/lingpipe/docs/api/index.html?com/aliasi/classify/TradNaiveBayesClassifier.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can refer to the expectation maximization tutorial at [http://alias-i.com/lingpipe/demos/tutorial/em/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/em/read-me.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extractors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, we have been using characters and words to train our models. We
    are about to introduce a classifier (logistic regression) that allows for other
    observations about the data to inform the classifier—for example, whether a word
    is actually a date. Feature extractors are used in CRF taggers and K-means clustering.
    This recipe will introduce feature extractors independent of any technology that
    uses them.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is not much to this recipe, but the upcoming *Logistic regression* recipe
    has many moving parts, and this is one of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fire up your IDE or type in the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Type a string into our standard I/O loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Features are then produced:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that there is no order information here. Does it keep a count or not?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The feature extractor keeps count with `my=2`, and it does not normalize the
    case (`My` is different from `my`). Refer to the later recipes in this chapter
    on how to modify feature extractors—they are very flexible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LingPipe provides solid infrastructure for the creation of feature extractors.
    The code for this recipe is in `src/com/lingipe/chapter3/SimpleFeatureExtractor.java`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code constructs `TokenFeatureExtractor` with `TokenizerFactory`.
    It is one of the 13 `FeatureExtractor` implementations provided in LingPipe.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will apply the I/O loop and print out the feature, which is `Map<String,
    ? extends Number>`. The `String` element is the feature name. In this case, the
    actual token is the name. The second element of the map is a value that extends
    `Number`, in this case, the count of how many times the token was seen in the
    text.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The feature name needs to only be a unique name—we could have prepended each
    feature name with `SimpleFeatExt_` to keep track of where the feature came from,
    which is helpful in complex feature-extraction scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression is probably responsible for the majority of industrial classifiers,
    with the possible exception of naïve Bayes classifiers. It almost certainly is
    one of the best performing classifiers available, albeit at the cost of slow training
    and considerable complexity in configuration and tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression is also known as maximum entropy, neural network classification
    with a single neuron, and others. So far in this book, the classifiers have been
    based on the underlying characters or tokens, but logistic regression uses unrestricted
    feature extraction, which allows for arbitrary observations of the situation to
    be encoded in the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe closely follows a more complete tutorial at [http://alias-i.com/lingpipe/demos/tutorial/logistic-regression/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/logistic-regression/read-me.html).
  prefs: []
  type: TYPE_NORMAL
- en: How logistic regression works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All that logistic regression does is take a vector of feature weights over the
    data, apply a vector of coefficients, and do some simple math, which results in
    a probability for each class encountered in training. The complicated bit is in
    determining what the coefficients should be.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some of the features produced by our training recipe for
    21 tweets annotated for English `e` and non-English `n`. There are relatively
    few features because feature weights are being pushed to `0.0` by our prior, and
    once a weight is `0.0`, then the feature is removed. Note that one category, `n`,
    is set to `0.0` for all the features of the `n-1` category—this is a property
    of the logistic regression process that fixes once categories features to `0.0`
    and adjust all other categories features with respect to that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Take the string, `I luv Disney`, which will only have two non-zero features:
    `I=0.37` and `Disney=0.15` for `e` and zeros for `n`. Since there is no feature
    that matches `luv`, it is ignored. The probability that the tweet is English breaks
    down to:'
  prefs: []
  type: TYPE_NORMAL
- en: '*vectorMultiply(e,[I,Disney]) = exp(.37*1 + .15*1) = 1.68*'
  prefs: []
  type: TYPE_NORMAL
- en: '*vectorMultiply(n,[I,Disney]) = exp(0*1 + 0*1) = 1*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will rescale to a probability by summing the outcomes and dividing it:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p(e|,[I,Disney]) = 1.68/(1.68 +1) = 0.62*'
  prefs: []
  type: TYPE_NORMAL
- en: '*p(e|,[I,Disney]) = 1/(1.68 +1) = 0.38*'
  prefs: []
  type: TYPE_NORMAL
- en: This is how the math works on running a logistic regression model. Training
    is another issue entirely.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe assumes the same framework that we have been using all along to
    get training data from `.csv` files, train the classifier, and run it from the
    command line.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up to train the classifier is a bit complex because of the number of
    parameters and objects used in training. We will discuss all the 10 arguments
    to the training method as found in `com.lingpipe.cookbook.chapter3.TrainAndRunLogReg`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `main()` method starts with what should be familiar classes and methods—if
    they are not familiar, have a look at *How to train and evaluate with cross validation*
    and *Introduction to Introduction to tokenizer Factories – finding words in a
    character stream*, recipes from [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple
    Classifiers"), *Simple Classifiers*, and [Chapter 2](part0027_split_000.html#page
    "Chapter 2. Finding and Working with Words"), *Finding and Working with Words*,
    respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are using `XValidatingObjectCorpus` when a simpler implementation
    such as `ListCorpus` will do. We will not take advantage of any of its cross-validation
    features, because the `numFolds` param as `0` will have training visit the entire
    corpus. We are trying to keep the number of novel classes to a minimum, and we
    tend to always use this implementation in real-world gigs anyway.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will start to build the configuration for our classifier. The `FeatureExtractor<E>`
    interface provides a mapping from data to features; this will be used to train
    and run the classifier. In this case, we are using a `TokenFeatureExtractor()`
    method, which creates features based on the tokens found by the tokenizer supplied
    during construction. This is similar to what naïve Bayes reasons over. The previous
    recipe goes into more detail about what the feature extractor is doing if this
    is not clear:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The `minFeatureCount` item is usually set to a number higher than 1, but with
    small training sets, this is needed to get any performance. The thought behind
    filtering feature counts is that logistic regression tends to overfit low-count
    features that, just by chance, exist in one category of training data. As training
    data grows, the `minFeatureCount` value is adjusted usually by paying attention
    to cross-validation performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The `addInterceptFeature` Boolean controls whether a category feature exists
    that models the prevalence of the category in training. The default name of the
    intercept feature is `*&^INTERCEPT%$^&**`, and you will see it in the weight vector
    output if it is being used. By convention, the intercept feature is set to `1.0`
    for all inputs. The idea is that if a category is just very common or very rare,
    there should be a feature that captures just this fact, independent of other features
    that might not be as cleanly distributed. This models the category probability
    in naïve Bayes in some way, but the logistic regression algorithm will decide
    how useful it is as it does with all other features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: These Booleans control what happens to the intercept feature if it is used.
    Priors, in the following code, are typically not applied to the intercept feature;
    this is the result if this parameter is true. Set the Boolean to `false`, and
    the prior will be applied to the intercept.
  prefs: []
  type: TYPE_NORMAL
- en: Next is the `RegressionPrior` instance, which controls how the model is fit.
    What you need to know is that priors help prevent logistic regression from overfitting
    the data by pushing coefficients towards 0\. There is a non-informative prior
    that does not do this with the consequence that if there is a feature that applies
    to just one category it will be scaled to infinity, because the model keeps fitting
    better as the coefficient is increased in the numeric estimation. Priors, in this
    context, function as a way to not be over confident in observations about the
    world.
  prefs: []
  type: TYPE_NORMAL
- en: Another dimension in the `RegressionPrior` instance is the expected variance
    of the features. Low variance will push coefficients to zero more aggressively.
    The prior returned by the static `laplace()` method tends to work well for NLP
    problems. For more information on what is going on here, consult the relevant
    Javadoc and the logistic regression tutorial referenced at the beginning of the
    recipe—there is a lot going on, but it can be managed without a deep theoretical
    understanding. Also, see the *Tuning parameters in logistic regression* recipe
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will control how the algorithm searches for an answer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '`AnnealingSchedule` is best understood by consulting the Javadoc, but what
    it does is change how much the coefficients are allowed to vary when fitting the
    model. The `minImprovement` parameter sets the amount the model fit has to improve
    to not terminate the search, because the algorithm has converged. The `minEpochs`
    parameter sets a minimal number of iterations, and `maxEpochs` sets an upper limit
    if the search does not converge as determined by `minImprovement`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next is some code that allows for basic reporting/logging. `LogLevel.INFO`
    will report a great deal of information about the progress of the classifier as
    it tries to converge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Here ends the *Getting ready* section of one of our most complex classes—next,
    we will train and run the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It has been a bit of work setting up to train and run this class. We will just
    go through the steps to get it up and running; the upcoming recipes will address
    its tuning and evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that there is a more complex 14-argument train method as well the one
    that extends configurability. This is the 10-argument version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `train()` method, depending on the `LogLevel` constant, will produce from
    nothing with `LogLevel.NONE` to the prodigious output with `LogLevel.ALL`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While we are not going to use it, we show how to serialize the trained model
    to disk. The *How to serialize a LingPipe object – classifier example* recipe
    in [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"),
    *Simple Classifiers*, explains what is going on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once trained, we will apply the standard classification loop with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the preceding code in the IDE of your choice or use the command-line command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result is a big dump of information about the training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `epoch` reporting goes on until either the number of epochs is met or the
    search converges. In the following case, the number of epochs was met:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can play with the classifier a bit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should look familiar; it is exactly the same result as the worked example
    at the start of the recipe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That's it! You have trained up and used the world's most relevant industrial
    classifier. However, there's a lot more to harnessing the power of this beast.
  prefs: []
  type: TYPE_NORMAL
- en: Multithreaded cross validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cross validation (refer to the *How to train and evaluate with cross validation*
    recipe in [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"),
    *Simple Classifiers*) can be very slow, which interferes with tuning systems.
    This recipe will show you a simple but effective way to access all the available
    cores on your system to more quickly process each fold.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe explains multi-threaded cross validation in the context of the next
    recipe, so don't be confused by the fact that the same class is repeated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Engage your IDE or type in the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The system then responds with the following output (you might have to scroll
    to the top of the window):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The default training data is 21 tweets annotated for English `e` and non-English
    `n`. In the preceding output, we saw a report of each fold that runs as a thread
    and the resulting confusion matrix. That's it! We just did multithreaded cross
    validation. Let's see how this works.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All the action happens in the `Util.xvalLogRegMultiThread()` method, which
    we invoke from `src/com/lingpipe/cookbook/chapter3/TuneLogRegParams.java`. The
    details of `TuneLogRegParams` are covered in the next recipe. This recipe will
    focus on the `Util` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: All 10 parameters used to configure logistic regression are controllable (you
    can refer to the previous recipe for explanation), with the addition of `numFolds`,
    which controls how many folds there will be, `numThreads`, which controls how
    many threads can be run at the same time, and finally, `categories`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the relevant method in `src/com/lingpipe/cookbook/Util.java`,
    we see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The method starts with the matching arguments for configuration information
    of logistic regression and running cross validation. Since cross validation is
    most often used in system tuning, all the relevant bits are exposed to modification.
    Everything is final because we are using an anonymous inner class to create threads.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we will set up `crossFoldEvaluator` that will collect the results from
    each thread:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will get down to the business of creating threads for each fold, `i`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `XValidatingObjectCorpus` class is set up for multithreaded access by creating
    a thread-safe version of the corpus for reads with the `itemView()` method. This
    method returns a corpus that can have the fold set, but no data can be added.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Each thread is a `runnable` object, where the actual work of training and evaluating
    the fold is handled in the `run()` method:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this code, we started with training the classifier, which, in turn, requires
    a `try/catch` statement to handle `IOException` thrown by the `LogisticRegressionClassifier.train()`
    method. Next, we will create `withinFoldEvaluator` that will be populated within
    the thread without a synchronization issue:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is important that `storeInputs` be `true` so that the fold results can be
    added to `crossFoldEvaluator`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This method, also in `Util`, iterates over all the true positives and false
    negatives for each category and adds them to `crossFoldEvaluator`. Note that this
    is synchronized: this means that only one thread can access the method at a time,
    but given that classification has already been done, it should not be much of
    a bottleneck:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The method takes the true positives and false negatives from each category and
    adds them to the `crossFoldEval` evaluator. These are essentially copy operations
    that do not take long to compute.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Returning to `xvalLogRegMultiThread`, we will handle the exception and add
    the completed `Runnable` to our list of `Thread`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With all the threads set up, we will invoke `runThreads()` as well as print
    the confusion matrix that results. We will not go into the source of `runThreads()`,
    because it is a straightforward Java management of threads, and `printConfusionMatrix`
    has been covered in [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple
    Classifiers"), *Simple Classifiers*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That's it for really speeding up cross validation on multicore machines. It
    can make a big difference when tuning systems.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning parameters in logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression presents an intimidating array of parameters to tweak for
    better performance, and working with it is a bit of black art. Having built thousands
    of these classifiers, we are still learning how to do it better. This recipe will
    point you in the general right direction, but the topic probably deserves its
    own book.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe involves extensive changes to the source of `src/com/lingpipe/chapter3/TuneLogRegParams.java`.
    We will just run one configuration of it here, with most of the exposition in
    the *How it works…* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Engage your IDE or type the following in the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The system then responds with cross-validation output confusion matrix for
    our default data in `data/disney_e_n.csv`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will report on false positives for each category—this will cover all
    the mistakes made:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This output is followed by the features, their coefficients, and a count—remember
    that we will see `n-1` categories, because one of the category''s features is
    set to `0.0` for all features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we have our standard I/O that allows for examples to be tested:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is the basic structure that we will work with. In the upcoming sections,
    we will explore the impact of varying parameters more closely.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This recipe assumes that you are familiar with logistic regression training
    and configuration from two recipes back and cross validation, which is the previous
    recipe. The overall structure of the code is presented in an outline form, with
    the tuning parameters retained. Modifying each parameter will be discussed later
    in the recipe—below we start with the `main()` method ignoring some code as indicated
    by ''`...`'' and the tunable code shown for tokenization and feature extraction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Next the priors are set up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Priors have a strong influence on the behavior coefficient assignment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code controls the search space of logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code runs cross validation to see how the system is doing—note
    the elided parameters with `...`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we will set the number of folds to `0`, which will have
    the train method visit the entire corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, for each category, we will print out the features and their coefficients
    for the just trained classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will have the usual console classifier I/O:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Tuning feature extraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The features that are fed into logistic regression have a huge impact on the
    performance of the system. We will cover feature extraction in greater detail
    in the later recipes, but we will bring to bear one of the most useful and somewhat
    counter-intuitive approaches here, because it is very easy to execute—use character
    ngrams instead of words/tokens. Let''s look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: This output indicates that the classifier is tied between `e` English and `n`
    non-English as a decision. Scrolling back through the features, we will see that
    there are no matches for any of the words in the input. There are some substring
    matches on the English side. `The` has the substring `he` for the feature word
    `the`. For language ID, it makes sense to consider subsequences, but as a matter
    of experience, it can be a big help for sentiment and other problems as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Modifying the tokenizer to be two-to-four-character ngrams is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the proper distinction being made:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: The overall performance on cross validation drops a bit. For very small training
    sets, such as 21 tweets, this is not unexpected. Generally, the cross-validation
    performance with a consultation of what the mistakes look like and a look at the
    false positives will help guide this process.
  prefs: []
  type: TYPE_NORMAL
- en: 'In looking at the false positives, it is clear that `Disney` is a source of
    problems, because the coefficients on features show it to be evidence for English.
    Some of the false positives are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the features for `e`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: In the absence of more training data, the features `!`, `Disney`, and `"` should
    be removed to help this classifier perform better, because none of these features
    are language specific, whereas `I` and `to` are, although not unique to English.
    This can be done by filtering the data or creating the appropriate tokenizer factory,
    but the best move is to probably get more data.
  prefs: []
  type: TYPE_NORMAL
- en: The `minFeature` count becomes useful when there is much more data, and you
    don't want logistic regression focusing on a very-low-count phenomenon because
    it tends to lead to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting the `addInterceptFeature` parameter to `true` will add a feature that
    always fires. This will allow logistic regression to have a feature sensitive
    to the number of examples for each category. It is not the marginal likelihood
    of the category, as logistic regression will adjust the weight like any other
    feature—but the following priors show how it can be further tuned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: The intercept is the strongest feature for `n` in the end, and the overall cross-validation
    performance suffered in this case.
  prefs: []
  type: TYPE_NORMAL
- en: Priors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The role of priors is to restrict the tendency of logistic regression to perfectly
    fit the training data. The ones we use try in varying degrees to push coefficients
    to zero. We will start with the `nonInformativeIntercept` prior, which controls
    whether the intercept feature is subject to the normalizing influences of the
    prior—if true, then the intercept is not subject to the prior, which was the case
    in the preceding example. Setting it to `false` moved it much closer to zero from
    `-0.17`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will adjust the variance of the prior. This sets an expected variation
    for the weights. A low variance means that coefficients are expected not to vary
    much from zero. In the preceding code, the variance was set to `2`. This is the
    result of setting it to `.01`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: This is a drop from 104 features with variance `2` to one feature for variance
    `.01`, because once a feature has dropped to `0`, it is removed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Increasing the variance changes our top `e` features from `2` to `4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: This is a total of 119 features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a variance of `2` and a `gaussian` prior:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: Oddly, we spend very little time worrying about which prior we use, but variance
    has a big role in performance, because it can cut down the feature space quickly.
    Laplace is a commonly accepted prior for NLP applications.
  prefs: []
  type: TYPE_NORMAL
- en: Consult the Javadoc and logistic regression tutorial for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Annealing schedule and epochs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As logistic regression converges, the annealing schedule controls how the search
    space is explored and terminated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: When tuning, we will increase the first parameter to the annealing schedule
    by order of magnitude (`.0025,.025,..`) if the search is taking too long—often,
    we can increase the training speed without impacting the cross-validation performance.
    Also, the `minImprovement` value can be increased to have the convergence end
    earlier, which can both increase the training speed and prevent the model from
    overfitting—this is called **early stopping**. Again, your guiding light in this
    situation is to look at the cross-validation performance when making changes.
  prefs: []
  type: TYPE_NORMAL
- en: The epochs required to achieve convergence can get quite high, so if the classifier
    is iterating to `maxEpochs -1`, this means that more epochs are required to converge.
    Be sure to set the `reporter.setLevel(LogLevel.INFO);` property or a more informative
    level to get the convergence report. This is another way to additionally force
    early stopping.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter tuning is a black art that can only be learned through practice. The
    quality and quantity of training data is probably the dominant factor in classifier
    performance, but tuning can make a big difference as well.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing feature extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logistic regression allows for arbitrary features to be used. Features are
    any observations that can be made about data being classified. Some examples are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Words/tokens from the text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We found that character ngrams work very well in lieu of words or stemmed words.
    For small data sets of less than 10,000 words of training, we will use 2-4 grams.
    Bigger training data can merit a longer gram, but we have never had good results
    above 8-gram characters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output from another component can be a feature, for example, a part-of-speech
    tagger.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metadata known about the text, for example, the location of a tweet or time
    of the day it was created.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognition of dates and numbers abstracted from the actual value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The source for this recipe is in `src/com/lingpipe/cookbook/chapter3/ContainsNumberFeatureExtractor.java`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature extractors are straightforward to build. The following is a feature
    extractor that returns a `CONTAINS_NUMBER` feature with weight `1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'By adding a `main()` method, we can test the feature extractor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code yields the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That's it. The next recipe will show you how to combine feature extractors.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Designing features is a bit of an art. Logistic regression is supposed to be
    robust in the face of irrelevant features, but overwhelming it with really dumb
    features will likely detract from performance.
  prefs: []
  type: TYPE_NORMAL
- en: One way to think about what features you need is to wonder what evidence from
    the text or environment helps you, the human, decide what the correct classification
    is. Try and ignore your world knowledge when looking at the text. If world knowledge,
    that is, France is a country, is important, then try and model this world knowledge
    with a gazetteer to generate `CONTAINS_COUNTRY_MENTION`.
  prefs: []
  type: TYPE_NORMAL
- en: Be aware that features are strings, and the only notion of equivalence is the
    exact string match. The `12:01pm` feature is completely distinct from `12:02pm`,
    although, to a human, these strings are very close, because we understand time.
    To get the similarity of these two features, you must have something like a `LUNCH_TIME`
    feature that is computed using time.
  prefs: []
  type: TYPE_NORMAL
- en: Combining feature extractors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature extractors can be combined in much the same way as tokenizers in [Chapter
    2](part0027_split_000.html#page "Chapter 2. Finding and Working with Words"),
    *Finding and Working with Words*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe will show you how to combine the feature extractor from the previous
    recipe with a very common feature extractor over character ngrams.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start with a `main()` method in `src/com/lingpipe/cookbook/chapter3/CombinedFeatureExtractor.java`
    that we will use to run the feature extractor. The following lines set up features
    that result from the tokenizer using the LingPipe class, `TokenFeatureExtractor`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then, we will construct the feature extractor from the previous recipe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, the LingPipe class joining feature extractors, `AddFeatureExtractor`,
    joins the two into a third:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The remaining code gets the features and prints them out:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run the following command
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Javadoc references a broad range of feature extractors and combiners/filters
    to help manage the task of feature extraction. One slightly confusing aspect of
    the class is that the `FeatureExtractor` interface is in the `com.aliasi.util`
    package, and the implementing classes are all in `com.aliasi.features`.
  prefs: []
  type: TYPE_NORMAL
- en: Classifier-building life cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the top-level building, a classifier usually proceeds as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Create training data—refer to the following recipe for more about this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build training and evaluation infrastructure with sanity check.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Establish baseline performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select optimization metric for classifier—this is what the classifier is trying
    to do and will guide tuning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Optimize classifier via techniques such as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parameter tuning
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Thresholding
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Linguistic tuning
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding training data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Refining classifier definition
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This recipe will present the first four steps in concrete terms, and there are
    recipes in this chapter for the optimization step.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Nothing happens without training data for classifiers. Look at the *Annotation*
    recipe at the end of the chapter for tips on creating training data. You can also
    use an active learning framework to incrementally generate a training corpus (covered
    later in this chapter), which is the data used in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Next, reduce the risk by starting with the dumbest possible implementation to
    make sure that the problem being solved is scoped correctly, and that the overall
    architecture makes sense. Connect the assumed inputs to assumed outputs with simple
    code. We promise that most of the time, one or the other will not be what you
    thought it would be.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe assumes that you are familiar with the evaluation concepts in [Chapter
    1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"), *Simple Classifiers*,
    such as cross validation and confusion matrices, in addition to the logistic regression
    recipes covered so far.
  prefs: []
  type: TYPE_NORMAL
- en: The entire source is at `src/com/lingpipe/cookbook/chapter3/ClassifierBuilder.java`.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe also assumes that you can compile and run the code within your preferred
    development environment. The result of all the changes we are making is in `src/com/lingpipe/cookbook/chapter3/ClassifierBuilderFinal.java`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Big caveat in this recipe—we are using a tiny dataset to make basic points on
    classifier building. The sentiment classifier we are trying to build would benefit
    from 10 times more data.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start with a collection of tweets that have been deduplicated and are the
    result of the *Train a little, learn a little – active learning* recipe that will
    follow this recipe. The starting point of the recipe is the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: Sanity check – test on training data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first thing to do is get the system running and test on training data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have left a print statement that advertises what is going on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running `ClassifierBuilder` will yield the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding confusion matrix is nearly a perfect system output, which validates
    that the system is basically working. This is the best system output you will
    ever see; never let management see it, or they will think this level of performance
    is either doable or done.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Establishing a baseline with cross validation and metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now it is time to see what is really going on.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have small data, then set the number of folds to `10` so that 90 percent
    of the data is used for training. If you have large data or are in a huge rush,
    then set it to `2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Comment out or remove the training on test code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plumb in a cross-validation loop or just uncomment the loop in our source:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Recompiling and running the code will give us the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The classifier labels mean `p=positiveSentiment`, `n=negativeSentiment`, and
    `o=other`, which covered other languages or neutral sentiment. The first row of
    the confusion matrix indicates that the system gets `45` true positives, `8` false
    negatives that it thinks are `n`, and `17` false negatives that it thinks are
    `o`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To get the false positives for `p`, we need to look at the first column. We
    see that the system thought that `16` `n` annotations were `p` and `18` `o` annotations
    were `p`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: The confusion matrix is the most honest and straightforward way to view/present
    results for classifiers. Performance metrics such as precision, recall, F-measure,
    and accuracy are all very slippery and often used incorrectly. When presenting
    results, always have a confusion matrix handy, because if we are in the audience
    or someone like us is, we will ask to see it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Perform the same analysis for the other categories, and you will have an assessment
    of system performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Picking a single metric to optimize against
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'While the confusion matrix establishes the overall performance of the classifier,
    it is too complex to use as a tuning guide. You don''t want to have to digest
    the entire matrix every time you adjust a feature. You and your team must agree
    on a single number that, if it goes up, the system is considered better. The following
    metrics apply to binary classifiers; if there are more than two categories, then
    you will have to sum them somehow. Some common metrics we see are:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**F-measure**: F-measure is an attempt to reward reductions in false negatives
    and false positives at the same time:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*F-measure = 2*TP / (2*TP + FP + FN)*'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: It is mainly an academic measure to declare that one system is better than another.
    It sees little use in industry.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Recall at 90 percent precision**: The goal is to provide as much coverage
    as possible while not making more than 10 percent false positives. This is when
    the system does not want to look bad very often; this applies to spell checkers,
    question answering systems, and sentiment dashboards.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision at 99.9 percent recall**: This metric supports *needle in the haystack*
    or *needle in the needle stack* kind of problems. The user cannot afford to miss
    anything and is willing to perhaps slog through lots of false positives as long
    as they don''t miss anything. The system is better if the false positive rate
    is lower. Use cases are intelligence analysts and medical researchers.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining this metric comes from a mixture of business/research needs, technical
    capability, available resources, and willpower. If a customer wants a high recall
    and high-precision system, our first question will be to ask what the budget is
    per document. If it is high enough, we will suggest hiring experts to correct
    system output, which is the best combination of what computers are good at (exhaustiveness)
    and what humans are good at (discrimination). Generally, budgets don't support
    this, so the balancing act begins, but we have deployed systems in just this way.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For this recipe, we will pick a maximizing recall at 50-percent precision for
    `n` (negative), because we want to be sure to intercept any negative sentiment
    and will tolerate false positives. We will choose 65 percent for a `p` positive,
    because the good news is less actionable, and who doesn't love Disney? We don't
    care what `o` (other performance) is—the category exists for linguistic reasons,
    independent of the business use. This metric a likely metric for a sentiment-dashboard
    application. This means that the system will produce one mistake for every two
    guesses of a negative-sentiment category and 13 out of 20 for positive sentiment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implementing the evaluation metric
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Perform the following steps to implement the evaluation metric:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start with reporting precision/recall for all categories with the `Util.printPrecRecall`
    method after printing out the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will now look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The precision for `n` exceeds our objective of `.5`–since we want to maximize
    recall at `.5`, we can make a few more mistakes before we get to the limit. You
    can refer to the *Thresholding classifiers* recipe to find out how to do this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The precision for `p` is 57 percent, and this is too low for our business objective.
    Logistic regression classifiers, however, provide a conditional probability that
    might allow us to meet the precision needs just by paying attention to the probability.
    Adding the following line of code will allow us to see the results sorted by conditional
    probability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding line of code starts by getting a `ScoredPrecisionRecallEvaluation`
    value from the evaluator. A double-scored curve (`[][])` is gotten from that object
    with the Boolean interpolate set to false, because we want the curve to be unadulterated.
    You can look at the Javadoc for what is going on. Then, we will use a print route
    from the same class to print out the curve. The output will look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is sorted by score, in the third column, which in this case, happens
    to be a conditional probability, so the max value is 1 and min value is 0\. Notice
    that the recall grows as correct cases are found (the second line), and it never
    goes down. However, when a mistake is made like in the fourth line, precision
    drops to `.6`, because 3 out of 5 cases are correct so far. The precision actually
    goes below `.65` before the last value is found—in bold, with a score of `.73`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'So, without any tuning, we can report that we can achieve 30 percent recall
    for `p` at our accepted precision limit of 65 percent. This requires that we threshold
    the classifier at `.73` for the category, which means if we reject scores less
    than `.73` for `p`, some comments are:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We got lucky. Usually, the first classifier runs do not reveal an immediately
    useful threshold with default values.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression classifiers have a very nice property that they provide;
    they also provide conditional probability estimates for thresholding. Not all
    classifiers have this property—language models and naïve Bayes classifiers tend
    to push scores towards 0 or 1, making thresholding difficult.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: As the training data is highly biased (this is from the *Train a little, learn
    a little – active learning* recipe that follows), we cannot trust this threshold.
    The classifier will have to be pointed at fresh data to set the threshold. Refer
    to the *Thresholding classifiers* recipe to see how this is done.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This classifier has seen very little data and will not be a good candidate for
    deployment despite the supporting evaluation. We would be more comfortable with
    at least 1,000 tweets from a diverse set of dates.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At this point in the process, we either accept the results by verifying that
    the performance is acceptable on fresh data or turn to improving the classifier
    by techniques covered by other recipes in this chapter. The final step of the
    recipe is to train up the classifier on all training data and write to disk:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: We will use the resulting model in the *Thresholding classifiers* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Linguistic tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will address issues around tuning the classifier by paying attention
    to the mistakes made by the system and making linguistic adjustments by adjusting
    parameters and features. We will continue with the sentiment use case from the
    previous recipe and work with the same data. We will start with a fresh class
    at `src/com/lingpipe/cookbook/chapter3/LinguisticTuning.java`.
  prefs: []
  type: TYPE_NORMAL
- en: We have very little data. In the real world, we will insist on more training
    data—at least 100 of the smallest category, negative, are needed with a natural
    distribution of positives and others.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will jump right in and run some data—the default is `data/activeLearningCompleted/disneySentimentDedupe.2.csv`,
    but you can specify your own file in the command line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following in your command line or IDE equivalent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For each fold, the features for the classifier will be printed. The output
    will look like the following for each category (just the first few features for
    each):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Starting with the `n` category, note that there are no features. It is a property
    of logistic regression that one category's features are all set to `0.0`, and
    the remaining `n-1` category's features are offset accordingly. This cannot be
    controlled, which is a bit annoying because the `n` or negative category can be
    the focus of linguistic tuning given how badly it performs in the example. Not
    to be deterred, we will move on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that the output is intended to make it easy to use a `find` command to
    locate feature output in the extensive reporting output. To find a feature search
    on `category <feature name>` to see if there is a nonzeroed report, search on
    `category <feature name> NON_ZERO`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We are looking for a few things in these features. First of all, there are
    apparently odd features that are getting big scores—the output is ranked in positive
    to negative for the category. What we want to look for is some signal in the feature
    weights—so `love` makes sense as being associated with a positive sentiment. Looking
    at features like this can really be surprising and counter intuitive. The uppercase
    `I` and lowercase `i` suggest that the text should be downcased. We will make
    this change and see if it helps. Our current performance is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The code change is to add a `LowerCaseTokenizerFactory` item to the current
    `IndoEuropeanTokenizerFactory` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the code, and we will pick up some precision and recall:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The features are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'What''s the next move? The `minFeature` count is very low at `1`. Let''s raise
    it to `2` and see what happens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This hurts performance by a few cases, so we will return to `1`. However, experience
    dictates that the minimum count goes up as more data is found to prevent overfitting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It is time for the secret sauce—change the tokenizer to `NGramTokenizer`; it
    tends to work better than standard tokenizers—we are now rolling with the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This worked. We will pick up a few more cases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'However, the features are now pretty hard to scan:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have found over the course of time that character ngrams are the features
    of choice for text-classifier problems. They seem to nearly always help, and they
    helped here. Look at the features, and you can recover that `love` is still contributing
    but in little bits, such as `lov`, `ov`, and `lo`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is another approach that deserves a mention, which is some of the tokens
    produced by `IndoEuropeanTokenizerFactory` are most likely useless, and they are
    just confusing the issue. Using a stop-word list, focusing on more useful tokenization,
    and perhaps applying a stemmer such as the Porter stemmer might work as well.
    This has been the traditional approach to these kinds of problems—we have never
    had that much luck with them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It is a good time to check on the performance of the `n` category; we have
    been messing about with the model and should check it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output also reports false positives for `p` and `n`. We really don''t care
    much about `o`, except when it shows up as a false positive for the other categories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Looking at false positives, we can suggest changes to feature extraction. Recognizing
    quotes from `~Walt Disney` might help the classifier with `IS_DISNEY_QUOTE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Also, looking at errors can point out errors in annotation, one can argue that
    the following is actually positive:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At this point, the system is somewhat tuned. The configuration should be saved
    someplace and the next steps are considered. They include the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Declare victory and deploy. Before deploying, be sure to test on novel data
    using all training data to train. The *Thresholding classifiers* recipe will be
    very useful.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Annotate more data. Use the active learning framework in the following recipe
    to help identify high-confidence cases that are wrong and right. This will likely
    help more than anything with performance, especially with low-count data such
    as the kind we have been working with.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at the epoch report, the system is never converging on its own. Increase
    the limit to 10,000 and see if this helps things.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The result of our tuning efforts was to improve the performance from:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is not a bad uptick in performance in exchange for looking at some data
    and thinking a bit about how to help the classifier do its job.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Thresholding classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression classifiers are often deployed with a threshold rather than
    the provided `classifier.bestCategory()` method. This method picks the category
    with the highest conditional probability, which, in a 3-way classifier, can be
    just above one-third. This recipe will show you how to adjust classifier performance
    by explicitly controlling how the best category is determined.
  prefs: []
  type: TYPE_NORMAL
- en: 'This recipe will consider the 3-way case with the `p`, `n`, and `o` labels
    and work with the classifier produced by the *Classifier-building life cycle*
    recipe earlier in this chapter. The cross-validation evaluation produced is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: We will run novel data to set thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our business use case is that recall be maximized while `p` has `.65` precision
    and `n` has `.5` precision for reasons discussed in the *Classifier-building life
    cycle* recipe. The `o` category is not important in this case. The `p` category
    appears to be too low with `.57`, and the `n` category can increase recall as
    the precision is above `.5`.
  prefs: []
  type: TYPE_NORMAL
- en: We cannot use the cross-validation results unless care has been taken to produce
    a proper distribution of annotations—the active learning approach used tends to
    not produce such distributions. Even with a good distribution, the fact that the
    classifier was likely tuned with cross validation means that it is most likely
    overfit to that dataset because tuning decisions were made to maximize performance
    of those sets that are not general to new data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We need to point the trained classifier at new data—the rule of thumb is to
    train by hook or crook but always threshold on fresh. We followed the *Getting
    data from the Twitter API* recipe in [Chapter 1](part0014_split_000.html#page
    "Chapter 1. Simple Classifiers"), *Simple Classifiers*, and downloaded new data
    from Twitter with the `disney` query. Nearly a year has passed since our initial
    search, so the tweets are most likely non-overlapping. The resulting 1,500 tweets
    were put into `data/freshDisney.csv`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensure that you don't run this code on data that is not backed up. The I/O is
    simple rather than robust. The code overwrites the input file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Invoke `RunClassifier` on your IDE or run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Open the `.csv` file in your favorite spreadsheet. All tweets should have a
    score and a guessed category in the standard annotation format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort with the primary sort on the `GUESS` column in the ascending or descending
    order and then sort on `SCORE` in the descending order. The result should be each
    category with higher scores descending to lower scores. This is how we set up
    top-down annotations.![How to do it...](img/00004.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Setting up sort of data for top-down annotation. All categories are grouped
    together, and a descending sort of the score is established.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For the categories that you care about, in this case, `p` and `n`, annotate
    truth from the highest score to the lowest scores until it is likely that the
    precision goal has been broached. For example, annotate `n` until you either run
    out of `n` guesses, or you have enough mistakes that you have `.50` precision.
    A mistake is when the truth is `o` or `p`. Do the same for `p` until you have
    a precision of `.65`, or you run out of number of `p`. For our canned example,
    we have put the annotations in `data/freshDisneyAnnotated.csv`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following command or the equivalent in your IDE (note that we are supplying
    the input file and not using the default):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This command will produce the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'First off, this is a surprisingly good system performance for our minimally
    trained classifier. `p` is very close to the target precision of `.65` without
    thresholding, and coverage is not bad: it is found as 141 true positives out of
    1,500 tweets. As we have not annotated all 1,500 tweets, we cannot truly say what
    the recall of the classifier is, so the term is overloaded in common use. The
    `n` category is not doing as well, but it is still pretty good. Our annotation
    did no annotations for the `o` category, so the system column is all zeros.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we will look at the precision/recall/score curve for thresholding guidance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Most values have been elided to save space in the preceding output. We saw
    that the point at which the classifier passes `.65` precision has a score of `.525`.
    This means that we can expect 65-percent precision if we threshold at `.525` with
    a bunch of caveats:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is a single-point sample without a confidence estimate. There are more
    sophisticated ways to arrive at a threshold that is beyond the scope of this recipe.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Time is a big contributor to variance in performance.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 10-percent variance in performance for well-developed classifiers is not uncommon
    in practice. Factor this into performance requirements.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The nice thing about the preceding curve is that it looks like we can provide
    a `.80` precision classifier at a threshold of `.76` with nearly 30 percent of
    the coverage of the `.65` precision classifier if we decide that higher precision
    is called for.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `n` case has a curve that looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It looks like a threshold of `.549` gets the job done. The rest of the recipe
    will show how you to set up the thresholded classifier now that we have the thresholds.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The code behind `RunClassifier.java` offers nothing of novelty in the context
    of this chapter, so it is left to you to work through.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal is to create a classifier that will assign `p` to a tweet if it scores
    above `.525` for that category and `n` if scores above `.549` for that category;
    otherwise, it gets `o`. Wrong….management saw the p/r curve and now insists that
    `p` must be 80-percent precise, which means that the threshold will be `.76`.
  prefs: []
  type: TYPE_NORMAL
- en: The solution is very simple. If a score for `p` is below `.76`, then it will
    be rescored down to `0.0`. Likewise, if a score for `n` is below `.54`, then it
    will be rescored down to `0.0`. The effect of this is that `o` will be the best
    category for all below-threshold cases, because `.75` `p` can at best be `.25`
    `n`, which remains below the `n` threshold, and `.53` `n` can at most be `.47`
    `p`, which is below that category's threshold. This can get complicated if all
    categories are thresholded, or the thresholds are low.
  prefs: []
  type: TYPE_NORMAL
- en: Stepping back, we are taking a conditional classifier where all the category
    scores must sum to 1 and breaking this contract, because we will take any estimate
    for `p` that is below `.76` and bust it down to `0.0`. It is a similar story for
    `n`. The resulting classifier will now have to be `ScoredClassifier` because this
    is the next most specific contract in the LingPipe API that we can uphold.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this class is in `src/com/lingpipe/cookbook/chapter3/ThresholdedClassifier`.
    At the top level, we have the class, relevant member variable, and constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will implement the only required method for `ScoredClassification`,
    and this is where the magic happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: The complicated bit about scored classifications is that scores have to be assigned
    to all categories even if the score is `0.0`. The mapping from a conditional classification,
    where all scores sum to `1.0`, does not lend itself to a generic solution, which
    is why the preceding ad hoc implementation is used.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also a `main()` method that spools up the relevant bits for `ThresholdedClassifier`
    and applies them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: The thresholds are doing exactly as designed; `p` is `.79` precision, which
    is close enough for consulting, and `n` is spot on. The source for the `main()`
    method should be straightforward given the context of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: That's it. Almost never do we release a nonthresholded classifier, and best
    practices require that thresholds be set on held-out data, preferably from later
    epochs than the training data. Logistic regression is quite robust against skewed
    training data, but the ointment that cleanses the flaws of skewed data is novel
    data annotated top down to precision objectives. Yes, it is possible to threshold
    with cross validation, but it suffers from the flaws that overfit due to tuning,
    and you would screw up your distributions. Recall-oriented objectives are another
    matter.
  prefs: []
  type: TYPE_NORMAL
- en: Train a little, learn a little – active learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Active learning is a super power to quickly develop classifiers. It has saved
    many a project in the real world. The idea is very simple and can be broken down
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Assemble a packet of raw data that is way bigger than you can annotate manually.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Annotate an embarrassingly small amount of the raw data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the classifier on the embarrassingly small amount of training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the trained classifier on all the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Put the classifier output into a `.csv` file ranked by confidence of best category.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Correct another embarrassingly small amount of data, starting with the most
    confident classifications.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the process until the performance is acceptable, or you run out of data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If successful, be sure to evaluate/threshold on fresh data, because the active
    learning process can introduce biases to the evaluation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What this process does is help the classifier distinguish the cases where it
    is making higher confidence mistakes and correcting it. It also works as a classification-driven
    search engine of sorts, where the positive training data functions as the query,
    and the remaining data functions as the index being searched.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, active learning is applied to the near-miss cases where the classifier
    is unsure of the correct class. In this case, the corrections will apply to the
    lowest confidence classifications. We came up with the high-confidence correction
    approach because we were under pressure to increase precision with a thresholded
    classifier that only accepted high-confidence decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is going on here is that we are using the classifier to find more data
    that looks like what it knows about. For problems where the target classes are
    rare in the unannotated data, it can very quickly help the system identify more
    examples of the class. For example, in a binary-classification task with marginal
    probability of 1 percent for the target class in the raw data, this is almost
    certainly the way to go. You cannot ask annotators to reliably mark a 1-in-100
    phenomenon over time. While this is the right way to do it, the end result is
    that it will not be done because of the effort involved.
  prefs: []
  type: TYPE_NORMAL
- en: Like most cheats, shortcuts, and super powers, the question to ask is what is
    the price paid. In the duality of precision and recall, recall suffers with this
    approach. This is because the approach biases annotation towards known cases.
    Cases that have very different wording are unlikely to be found, so coverage can
    suffer.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s get started with active learning:'
  prefs: []
  type: TYPE_NORMAL
- en: Collect some training data in our `.csv` format from [Chapter 1](part0014_split_000.html#page
    "Chapter 1. Simple Classifiers"), *Simple Classifiers*, or use our example data
    in `data/activeLearning/disneyDedupe.0.csv`. Our data builds on the Disney tweets
    from [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"),
    *Simple Classifiers*. Sentiment is a good candidate for active learning, because
    it benefits from quality training data and creating quality training data can
    be difficult. Use the `.csv` file format from the Twitter search downloader if
    you are using your own data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the `.csv` deduplication routine from the *Eliminate near duplicates with
    Jaccard distance* recipe of [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple
    Classifiers"), *Simple Classifiers* to get rid of near-duplicate tweets. We have
    already done this with our example data. We went from 1,500 tweets to 1,343.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you have your own data, annotate around 25 examples in the `TRUTH` column
    according to the standard annotation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`p` stands for positive sentiment'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n` stands for negative sentiment'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`o` stands for other, which means that no sentiment is expressed, or the tweet
    is not in English'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Be sure to get a few examples of each category
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Our example data is already annotated for this step. If you are using your own
    data, be sure to use the format of the first file (that has the `0.csv` format),
    with no other `.` in the path.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How to do it…](img/00005.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Examples of tweets annotated. Note that all categories have examples.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following command. Do not do this on your own annotated data without
    backing up the file. Our I/O routine is written for simplicity, not robustness.
    You have been warned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Pointed at the supplied annotated data, this will print the following to the
    console with a final suggestion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This recipe will show you how to make it better, mainly by making it bigger
    in smart ways. Let''s see where we stand:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The data has been annotated a bit for three categories
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Of 1,343 tweets, there have been 25 annotated, 13 of which are `o`, which we
    don't particularly care about given the use case, but they still are important
    because they are not `p` or `n`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This is not nearly enough annotated data to build a reliable classifier with,
    but we can use it to help annotate more data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The last line encourages more annotation and the name of a file to annotate
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The precision and recall are reported for each category, that is, the result
    of cross validation over the training data. There is also a confusion matrix.
    At this point, we are not expecting very good performance, but `p` and `o` are
    doing quite well. The `n` category is not doing well at all.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, fire up a spreadsheet, and import and view the indicated `.csv` file
    using a UTF-8 encoding. OpenOffice shows us the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How to do it…](img/00006.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Initial output of the active learning approach
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Reading from the left-hand side to the right-hand side, we will see the `SCORE`
    column, which reflects the classifier's confidence; its most likely category,
    shown in the `GUESS` column, is correct. The next column is the `TRUTH` class
    as determined by a human. The last `TEXT` column is the tweet being classified.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'All 1,343 tweets have been classified in one of two ways:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the tweet had an annotation, that is, an entry in the `TRUTH` column, then
    the annotation was made when the tweet was in the test fold of a 10-fold cross
    validation. Line 13 is just such a case. In this case, the classification was
    `o`, but the truth was `p`, so it would be a false negative for `p`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the tweet was not annotated, that is, no entry in the `TRUTH` column, then
    it was classified using all the available training data. All other examples in
    the shown spreadsheet are handled this way. They don't inform the evaluation at
    all. We will annotate these tweets to help improve classifier performance.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will annotate high-confidence tweets irrespective of category, as shown
    in the following screenshot:![How to do it…](img/00007.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Corrected output for active learning output. Note the dominance of the o category.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Annotating down to line 19, we will notice that most of the tweets are `o` and
    are dominating the process. There are only three `p` and no `n`. We need to get
    some `n` annotations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can focus on likely candidate `n` annotations by selecting the entire sheet,
    except for the headers, and sorting by column **B** or `GUESS`. Scrolling to the
    `n` guesses, we should see the highest confidence examples. In the following screenshot,
    we have annotated all the `n` guesses because the category needs data. Our annotations
    are in `data/activeLearningCompleted/disneySentimentDedupe.1.csv`. If you want
    to exactly duplicate the recipe, you will have to copy this file to the `activeLearning`
    directory.![How to do it…](img/00008.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Annotations sorted by category with very few n or negative categories.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Scrolling to the `p` guesses, we annotated a bunch as well.![How to do it…](img/00009.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Positive labels with corrections and surprising number of negatives
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There are eight negative cases that we found in the `p` guesses mixed in with
    lots of `p` and some `o` annotations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will save the file without changing the filename and run the same program
    as we did earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is a fairly typical output early in the annotation process. Positive `p`,
    the easy category, is dragging along at 49-percent precision and 45-percent recall.
    Negative `n` is even worse. Undaunted, we will do another round of annotation
    on the output file indicating focus on `n` guesses to help that category improve
    performance. We will save and rerun the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This last round of annotation got us over the edge (remember to copy over our
    annotation from `activeLearningCompleted/disneySentimentDedupe.2.csv` if you are
    mirroring the recipe exactly). We annotated high-confidence examples from both
    `p` and `n`, adding nearly 100 examples. The first best annotation for `n` is
    above 50-percent precision with 41-percent recall. We assume that there will be
    a tunable threshold that meets our 80-percent requirement for `p` and declares
    victory in 211 moves, which is much less than the total 1,343 annotations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That's it. This is a real-world example and the first example we have tried
    for the book, so the data is not cooked. The approach tends to work, although
    no promises; some data resists even the most focused efforts of a well-equipped
    computational linguist.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Be sure to store the final `.csv` file some place safe. It would be a shame
    to lose all that directed annotation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before releasing this classifier we would want to run the classifier, which
    trains on all annotated data, on new text to verify performance and set thresholds.
    This annotation process introduces biases over the data that will not be reflected
    in the real world. In particular, we have biased annotation for `n` and `p` and
    added `o` as we saw them. This is not the actual distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This recipe has some subtlety because of the simultaneous evaluation and creation
    of ranked output for annotation. The code starts with constructs that should be
    familiar to you:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: The `getLatestEpochFile` method looks for the highest numbered file that ends
    with `csv`, shares the root with the filename, and returns it. On no account will
    we use this routine for anything serious. The method is standard Java, so we won't
    cover it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the latest file, we will do some reporting, read it in our standard
    `.csv` annotated files, and load a cross-validating corpus. All these routines
    are explained elsewhere in locations specified in the `Util` source. Finally,
    we will get the categories that were found in the `.csv` annotated file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will configure some standard logistic-regression-training parameters
    and create the evaluator for cross-fold evaluation. Note that the Boolean for
    `storeInputs` is `true`, which will facilitate recording results. The *How to
    train and evaluate with cross validation* recipe in [Chapter 1](part0014_split_000.html#page
    "Chapter 1. Simple Classifiers"), *Simple Classifiers*, has a complete explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will execute standard cross validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of cross validation, the evaluator has all the classifications stored
    in `visitTest()`. Next, we will transfer this data to an accumulator, which creates
    and stores rows that will be put into the output spreadsheet and redundantly stores
    the score; this score will be used in a sort to control the order of annotations
    printed out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will iterate over each category and create a list of the false negatives
    and true positives for the category—these are the cases that the truth category
    is the category label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, all the in-category test cases are used to create rows for the accumulator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the code will print out some standard evaluator output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: All the mentioned steps only apply to annotated data. We will now turn to getting
    best category and scores for all the unannotated data in the `.csv` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will set the number of folds on the cross-validating corpus to `0`,
    which means that `vistTrain()` will visit the entire corpus of annotations—unannotated
    data is not contained in the corpus. The logistic regression classifier is trained
    in the usual way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: 'Armed with a classifier, the code iterates over all the `data` items, one row
    at a time. The first step is to check for an annotation. If the value is not the
    empty string, then the data was in the aforementioned corpus and used as training
    data so that the loop skips to the next row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: If the row is unannotated, then the score and `bestCategory()` method is added
    at the appropriate points, and the row is added to the accumulator with the score.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of the code increments the index of the filename and writes out the
    accumulator data with a bit of reporting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: This is how it works. Remember that the biases that can be introduced by this
    approach invalidate evaluation numbers. Always run on fresh held-out data to get
    a proper sense of the classifier's performance.
  prefs: []
  type: TYPE_NORMAL
- en: Annotation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most valuable services we provide is teaching our customers how to
    create gold-standard data, also known as training data. Nearly every successful-driven
    NLP project we have done has involved a good deal of customer-driven annotation.
    The quality of the NLP is entirely dependent on the quality of the training data.
    Creating training data is a fairly straightforward process, but it requires attention
    to detail and significant resources. From a budget perspective, you can expect
    to spend as much as the development team on annotation, if not more.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use sentiment over tweets as our example, and we will assume a business
    context, but even academic efforts will have similar dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Get 10 examples of what you expect the system to do. For our example, this means
    getting 10 tweets that reflect the scope of what the system is expected to do.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make some effort to pick from the range of what you expect as inputs/outputs.
    Feel free to cherry-pick strong examples, but do not make up examples. Humans
    are terrible at creating example data. Seriously, don't do it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Annotate these tweets for the expected categories.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Have a meeting with all the stakeholders in the annotation. This includes user-experience
    designers, business folks, developers, and end users. The goal of this meeting
    is to expose all the relevant parties to what the system will actually do—the
    system will take the 10 examples and produce the category label. You will be amazed
    at how much clarity this step establishes. Here are the kinds of clarity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upstream/downstream users of the classifier will have a clear idea of what they
    are expected to produce or consume. For example, the system consumes UTF-8-encoded
    English tweets and produces an ASCII single character of `p`, `n`, or `u`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For a sentiment, people tend to want a severity score, which is very hard to
    get. You can expect annotation costs to double at least. Is it worth it? A score
    of confidence can be provided, but that is confidence that the category is correct
    *not* the severity of the sentiment. This meeting will force the discussion.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: During this meeting explain that each category will likely need at least 100
    examples, if not 500, to do a reasonable job. Also explain that switching domains
    might require new annotations. NLP is extremely easy for your human colleagues,
    and as a result, they tend to underestimate what it takes to build systems.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Don't neglect to include whoever is paying for all this. I suppose you should
    not have your parents involved if this is your undergraduate thesis.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Write down an annotation standard that explains the intention behind each category.
    It doesn''t need to be very complex, but it needs to exist. The annotation standard
    should be circulated around the stakeholders. Bonus points if you have one at
    the mentioned meetings; if so, it will likely be different at the end, but this
    is fine. An example is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A tweet is positive `p` if the sentiment is unambiguously positive about Disney.
    A positive sentiment that applies to a non-Disney tweet is not `p` but `u`. An
    example is the `n` tweet indicates clearly negative intent towards Disney. Examples
    include that all other tweets are `u`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples in the annotation standard do the best job of communicating the intent.
    Humans do a better job with examples rather than descriptions in our experience.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Create your collection of unannotated data. The best practice here is for the
    data to be random from the expected source. This works fine for categories with
    noticeable prevalence in data, say 10 percent or more, but we have built classifiers
    that occur at a rate of 1/2,000,000 for question-answering systems. For rare categories,
    you can use a search engine to help find instances of the category—for example,
    search for `luv` to find positive tweets. Alternatively, you can use a classifier
    trained on a few examples, run it on data, and look at the high-scoring positives—we
    covered this in the previous recipe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Recruit at least two annotators to annotate data. The reason we need at least
    two is that the task has to be shown to be reproducible by humans. If people can''t
    reliably do the task, then you can''t expect a computer to do it. This is where
    we execute some code. Type in the following command in the command line or invoke
    your annotators in you IDE—this will run with our default files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE153]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE154]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The code reports disagreements and prints out a confusion matrix. Precision
    and recall are useful metrics as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is little novel data in the code in `src/com/lingpipe/cookbook/chapter3/InterAnnotatorAgreement.java`.
    One slight twist is that we used `BaseClassifierEvaluator` to do the evaluation
    work without a classifier ever being specified—the creation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: 'The evaluator is populated with classifications directly rather than the usual
    `Corpus.visitTest()` method, as done elsewhere in the book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: If the recipe requires further explanation, consult the *Evaluation of classifiers—the
    confusion matrix* recipe in [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple
    Classifiers"), *Simple Classifiers*.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Annotation is a very complex area that deserves its own book, and fortunately,
    there is a good one, *Natural Language Annotation for Machine Learning*, *James
    Pustejovsky and Amber Stubbs*, *O'Reilly Media*. To get annotations done, there
    is Amazon's Mechanical Turk service as well as companies that specialize in the
    creation of training data such as CrowdFlower. However, be careful of outsourcing
    because classifiers are very dependent on the quality of data.
  prefs: []
  type: TYPE_NORMAL
- en: Conflict resolution between annotators is a challenging area. Many errors will
    be due to attention lapses, but some will persist as legitimate areas of disagreement.
    Two easy resolution strategies are either to throw out the data or keep both annotations.
  prefs: []
  type: TYPE_NORMAL
