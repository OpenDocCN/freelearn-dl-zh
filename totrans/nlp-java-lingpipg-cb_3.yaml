- en: Chapter 3. Advanced Classifiers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。高级分类器
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下配方：
- en: A simple classifier
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个简单的分类器
- en: Language model classifier with tokens
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于标记的语言模型分类器
- en: Naïve Bayes
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Feature extractors
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征提取器
- en: Logistic regression
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Multithreaded cross validation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多线程交叉验证
- en: Tuning parameters in logistic regression
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归中的调参
- en: Customizing feature extraction
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定制特征提取
- en: Combining feature extractors
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合特征提取器
- en: Classifier-building life cycle
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类器构建生命周期
- en: Linguistic tuning
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言学调谐
- en: Thresholding classifiers
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阈值分类器
- en: Train a little, learn a little – active learning
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一点，学习一点——主动学习
- en: Annotation
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注释
- en: Introduction
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: This chapter introduces more sophisticated classifiers that use different learning
    techniques as well as richer observations about the data (features). We will also
    address the best practices for building machine-learning systems as well as data
    annotation and approaches that minimize the amount of training data needed.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了更复杂的分类器，这些分类器使用不同的学习技术以及关于数据的更丰富观察（特征）。我们还将讨论构建机器学习系统的最佳实践以及数据标注和减少所需训练数据量的方法。
- en: A simple classifier
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个简单的分类器
- en: This recipe is a thought experiment that should help make clear what machine
    learning does. Recall the *Training your own language model classifier* recipe
    in [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"),
    *Simple Classifiers*, to train your own sentiment classifier in the recipe. Consider
    what a conservative approach to the same problem might be—build `Map<String,String>`
    from the inputs to the correct class. This recipe will explore how this might
    work and what its consequences might be.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方是一个思想实验，应该有助于阐明机器学习做什么。回想一下[第1章](part0014_split_000.html#page "第1章。简单分类器")中*训练自己的语言模型分类器*的配方，在配方中训练自己的情感分类器。考虑一下对同一问题的保守方法可能是什么——从输入到正确类别的`Map<String,String>`。这个配方将探讨这可能如何工作以及可能产生的后果。
- en: How to do it...
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: Brace yourself; this will be spectacularly stupid but hopefully informative.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好；这将是非常愚蠢但希望是有信息的。
- en: 'Enter the following in the command line:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令行中输入以下内容：
- en: '[PRE0]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The usual anemic prompt appears, with some user input:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 常见的贫血提示出现，伴随着一些用户输入：
- en: '[PRE1]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It correctly gets the language as `e` or English. However, everything else
    is about to fail. Next, we will use the following code:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它正确地识别出语言为`e`或英语。然而，其他所有事情都将失败。接下来，我们将使用以下代码：
- en: '[PRE2]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We just dropped the final `y` on `#Disney`, and as a result, we got a big confused
    classifier. What happened?
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们刚刚在`#Disney`上省略了最后的`y`，结果我们得到了一个很大的混淆分类器。发生了什么？
- en: How it works...
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This section should really be called *How it doesn't work*, but let's dive into
    the details anyway.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这一节实际上应该被称为*它不工作的方式*，但无论如何让我们深入细节。
- en: Just to be clear, this recipe is not recommended as an actual solution to a
    classification problem that requires any flexibility at all. However, it introduces
    a minimal example of how to work with LingPipe's `Classification` class as well
    as makes clear what an extreme case of overfitting looks like; this in turn, helps
    demonstrate how machine learning is different from most of standard computer engineering.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清楚起见，这个配方不推荐作为任何需要灵活性的分类问题的实际解决方案。然而，它引入了如何与LingPipe的`Classification`类一起工作的最小示例，以及清楚地说明了过度拟合的极端案例；这反过来又有助于展示机器学习与标准计算机工程的不同之处。
- en: 'Starting with the `main()` method, we will get into standard code slinging
    that should be familiar to you from [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple
    Classifiers"), *Simple Classifiers*:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从`main()`方法开始，我们将进入您从[第1章](part0014_split_000.html#page "第1章。简单分类器")中熟悉的常规代码编写：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Nothing novel is going on here—we are just training up a classifier, as shown
    in [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"),
    *Simple Classifiers*, and then supplying the classifier to the `Util.consoleInputBestCategory()`
    method. Looking at the class code reveals what is going on:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有发生任何新颖的事情——我们只是在训练一个分类器，正如[第1章](part0014_split_000.html#page "第1章。简单分类器")中所示，*简单分类器*，然后将分类器提供给`Util.consoleInputBestCategory()`方法。查看类代码可以揭示发生了什么：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'So, the `handle()` method takes the `text` and `classification` pair and stuffs
    them in `HashMap`. The classifier does nothing else to learn from the data so
    training amounts to memorization of the data:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`handle()`方法接受`text`和`classification`对，并将它们放入`HashMap`中。分类器不会从数据中学习其他任何事情，因此训练相当于数据的记忆：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `classify()` method just does a lookup into `Map` and returns the value
    if there is one, otherwise, we will get the category `n` as the return classification.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`classify()`方法只是在`Map`中进行查找，如果存在则返回值，否则，我们将得到类别`n`作为返回分类。'
- en: What is good about the preceding code is that you have a minimalist example
    of a `BaseClassifier` implementation, and you can see how the `handle()` method
    adds data to the classifier.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个代码的优点在于，你有一个`BaseClassifier`实现的简约示例，你可以看到`handle()`方法是如何向分类器添加数据的。
- en: What is bad about the preceding code is the utter rigidity of the mapping from
    training data to categories. If the exact example is not seen in training, then
    the `n` category is assumed.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个代码的缺点在于从训练数据到类别的映射过于僵化。如果训练中没有看到确切的例子，那么就假设为`n`类别。
- en: This is an extreme example of overfitting, but it essentially conveys what it
    means to have an overfit model. An overfit model is tailored too close to the
    training data and cannot generalize well to new data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个过度拟合的极端例子，但它本质上传达了拥有一个过度拟合模型的意义。一个过度拟合的模型过于贴近训练数据，无法很好地推广到新数据。
- en: 'Let''s think a bit more about what is so wrong about the preceding classifier
    for language identification—the issue is that entire sentences/tweets are the
    wrong unit of processing. Words/tokens are a much better measure of what language
    is being used. Some improvements that will be borne out in the later recipes are:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再深入思考一下前一个用于语言识别的分类器有什么问题——问题是整个句子/推文是错误的处理单元。单词/标记是衡量正在使用哪种语言的更好指标。在后面的食谱中将会体现的一些改进包括：
- en: Break the text into words/tokens.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本分解成单词/标记。
- en: Instead of a match/no-match decision, consider a more nuanced approach. A simple
    *which language matches more words* will be a huge improvement.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 而不是简单的匹配/不匹配决策，考虑一个更细致的方法。简单的“哪种语言匹配更多单词”将是一个巨大的改进。
- en: As languages get closer, for example, British versus American English, probabilities
    can be called for that. Pay attention to likely discriminating words.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当语言越来越接近时，例如，英国英语与美国英语，可以调用概率。注意可能的区分性单词。
- en: While this recipe might be comically inappropriate for the task at hand, consider
    trying a sentiment for an even more ludicrous example. It embodies a core assumption
    of much of computer science that the world of inputs is discrete and finite. Machine
    learning can be viewed as a response to a world where this is not the case.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个食谱可能对于当前任务来说有些滑稽不合适，但考虑尝试一个更加荒谬的例子。它体现了一个计算机科学的核心假设，即输入的世界是离散且有限的。机器学习可以被视为对这样一个世界的回应。
- en: There's more…
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Oddly enough, we often have a need for such a classifier in commercial systems—we
    call it the management classifier; it runs preemptively on data. It has happened
    that a senior VP is unhappy with the system output for some example. This classifier
    then can be trained with the exact case that allows for immediate system fixing
    and satisfaction of the VP.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 令人奇怪的是，我们在商业系统中往往需要这样的分类器——我们称之为管理分类器；它在数据上预先运行。曾经发生过一位高级副总裁对系统输出的某些示例不满意的情况。这个分类器随后可以用确切的案例进行训练，以便立即修复系统并满足副总裁的需求。
- en: Language model classifier with tokens
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于标记的语言模型分类器
- en: '[Chapter 1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"),
    *Simple Classifiers*, covered classification without knowing what tokens/words
    were, with a language model per category—we used character slices or ngrams to
    model the text. [Chapter 2](part0027_split_000.html#page "Chapter 2. Finding and
    Working with Words"), *Finding and Working with Words*, discussed at length the
    process of finding tokens in text, and now we can use them to build a classifier.
    Most of the time, we use tokenized input to classifiers, so this recipe is an
    important introduction to the concept.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[第1章](part0014_split_000.html#page "第1章。简单分类器")，*简单分类器*，在不知道标记/单词是什么的情况下进行了分类，每个类别都有一个语言模型——我们使用了字符切片或ngram来对文本建模。[第2章](part0027_split_000.html#page
    "第2章。查找和使用单词")，*查找和使用单词*，详细讨论了在文本中查找标记的过程，现在我们可以使用它们来构建分类器。大多数时候，我们使用标记化输入到分类器中，所以这个食谱是概念的重要介绍。'
- en: How to do it...
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'This recipe will tell us how to train and use a tokenized language model classifier,
    but it will ignore issues such as evaluation, serialization, deserialization,
    and so on. You can refer to the recipes in [Chapter 1](part0014_split_000.html#page
    "Chapter 1. Simple Classifiers"), *Simple Classifiers*, for examples. This code
    of this recipe is in `com.lingpipe.cookbook.chapter3.TrainAndRunTokenizedLMClassifier`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱将告诉我们如何训练和使用一个分词语言模型分类器，但它将忽略诸如评估、序列化、反序列化等问题。您可以参考[第1章](part0014_split_000.html#page
    "第1章。简单分类器")中的食谱，*简单分类器*，以获取示例。这个食谱的代码在`com.lingpipe.cookbook.chapter3.TrainAndRunTokenizedLMClassifier`：
- en: 'The exception of the following code is the same as found in the *Training your
    own language model classifier* recipe in [Chapter 1](part0014_split_000.html#page
    "Chapter 1. Simple Classifiers"), *Simple Classifiers*. The `DynamicLMClassifier`
    class provides a static method for the creation of a tokenized LM classifier.
    Some setup is required. The `maxTokenNgram` variable sets the largest size of
    token sequences used in the classifier—smaller datasets usually benefit from lower
    order (number of tokens) ngrams. Next, we will set up a `tokenizerFactory` method,
    selecting the workhorse tokenizer from [Chapter 2](part0027_split_000.html#page
    "Chapter 2. Finding and Working with Words"), *Finding and Working with Words*.
    Finally, we will specify the categories that the classifier uses:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码的异常与[第1章](part0014_split_000.html#page "第1章。简单分类器")中“训练自己的语言模型分类器”食谱中找到的相同，*简单分类器*。`DynamicLMClassifier`类提供了一个用于创建分词语言模型分类器的静态方法。需要一些设置。`maxTokenNgram`变量设置了分类器中使用的最大标记序列大小——较小的数据集通常从低阶（标记数量）ngram中受益。接下来，我们将设置一个`tokenizerFactory`方法，选择来自[第2章](part0027_split_000.html#page
    "第2章。查找和使用单词")的“查找和使用单词”中的工作马分类器。最后，我们将指定分类器使用的类别：
- en: '[PRE6]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, the classifier is constructed:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，构建分类器：
- en: '[PRE7]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Run the code from the command line or your IDE:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过命令行或您的IDE运行代码：
- en: '[PRE8]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: There's more...
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: In application, the `DynamicLMClassifier` classifier does not see a great deal
    of use in commercial application. This classifier might be a good choice for an
    author-identification classifier (that is, one that classifies whether a given
    piece of text is written by an author or by someone else) that was highly sensitive
    to turns of phrase and exact word usage. The Javadoc is well worth consulting
    to better understand what this class does.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用中，`DynamicLMClassifier`分类器在商业应用中并没有得到广泛的使用。这个分类器可能是一个很好的选择，用于作者识别分类器（即，判断给定的文本是由作者还是其他人写的）对短语和精确的单词使用非常敏感。Javadoc值得咨询，以更好地了解这个类的作用。
- en: Naïve Bayes
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Naïve Bayes is probably the world's most famous classification technology, and
    just to keep you on your toes, we provide two separate implementations with lots
    of configurability. One of the most well-known applications of a Naïve Bayes classifier
    is for spam filtering in an e-mail.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯可能是世界上最著名的分类技术，为了保持你的警惕，我们提供了两个具有许多可配置性的独立实现。朴素贝叶斯分类器最著名的应用之一是用于电子邮件中的垃圾邮件过滤。
- en: 'The reason the word *naïve* is used is that the classifier assumes that words
    (features) occur independent of one another—this is clearly a naïve assumption,
    but lots of useful and not-so-useful technologies have been based on the approach.
    Some notable features of the traditional naïve Bayes include:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单词“naïve”的原因是分类器假设单词（特征）彼此独立——这显然是一个简单的假设，但许多有用和不那么有用的技术都是基于这种方法。传统朴素贝叶斯的一些显著特点包括：
- en: Character sequences are converted to bags of tokens with counts. No whitespaces
    are considered, and the order of the tokens does not matter.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符序列被转换为带有计数的标记袋。不考虑空白字符，并且标记的顺序无关紧要。
- en: Naïve Bayes classifiers require two or more categories into which input texts
    are categorized. These categories must be both exhaustive and mutually exclusive.
    This indicates that a document used for training must only belong to one category.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器需要两个或更多类别，输入文本被分类到这些类别中。这些类别必须是详尽无遗且相互排斥的。这意味着用于训练的文档必须只属于一个类别。
- en: 'The math is very simple: `p(category|tokens) = p(category,tokens)/p(tokens)`.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数学非常简单：`p(category|tokens) = p(category,tokens)/p(tokens)`。
- en: The class is configurable for various kinds of unknown token models.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该类可以根据各种未知标记模型进行配置。
- en: A naïve Bayes classifier estimates two things. First, it estimates the probability
    of each category, independent of any tokens. This is carried out based on the
    number of training examples presented for each category. Second, for each category,
    it estimates the probability of seeing each token in that category. Naïve Bayes
    is so useful and important that we will show you exactly how it works and plug
    through the formulas. The example we have is to classify hot and cold weather
    based on the text.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器估计两件事。首先，它估计每个类别的概率，独立于任何标记。这是基于每个类别提供的训练示例的数量来执行的。其次，对于每个类别，它估计在该类别中看到每个标记的概率。朴素贝叶斯非常有用且重要，我们将向您展示它的工作原理以及如何使用公式。我们的例子是根据文本对炎热和寒冷天气进行分类。
- en: First, we will work out the math to calculate the probability of a category
    given a word sequence. Second, we will plug in an example and then verify it using
    the classifier we build.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将计算出给定词序列的类别概率。其次，我们将插入一个示例，然后使用我们构建的分类器进行验证。
- en: Getting ready
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Let''s lay out the basic formula to calculate the probability of a category
    given a text input. A token-based naïve Bayes classifier computes the joint token
    count and category probabilities as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们列出基本公式来计算给定文本输入的类别概率。基于标记的朴素贝叶斯分类器计算联合标记计数和类别概率如下：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Conditional probabilities are derived by applying Bayes''s rule to invert the
    probability calculation:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 条件概率是通过应用贝叶斯定理来反转概率计算得到的：
- en: '[PRE10]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, we will get to expand all these terms. If we look at `p(tokens|cat)`,
    this is where the naïve assumption comes into play. We assume that each token
    is independent, and thus, the probability of all the tokens is the product of
    the probability of each token:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将扩展所有这些术语。如果我们看`p(tokens|cat)`，这就是朴素假设发挥作用的地方。我们假设每个标记是独立的，因此所有标记的概率是每个标记概率的乘积：
- en: '[PRE11]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The probability of the tokens themselves, that is, `p(tokens)`, the denominator
    in the preceding equation. This is just the sum of their probability in each category
    weighted by the probability of the category itself:'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标记本身的概率，即`p(tokens)`，是前面方程中的分母。这仅仅是它们在每个类别中的概率之和，并按该类别的概率本身进行加权：
- en: '[PRE12]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: When building a naïve Bayes classifier, `p(tokens)` doesn't need to be explicitly
    calculated. Instead, we can use `p(tokens|cat) * p(cat)` and assign the tokens
    to the category with the higher product.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当构建朴素贝叶斯分类器时，`p(tokens)`不需要显式计算。相反，我们可以使用`p(tokens|cat) * p(cat)`并将标记分配给具有更高乘积的类别。
- en: Now that we have laid out each element of our equation, we can look at how these
    probabilities are calculated. We can calculate both these probabilities using
    simple frequencies.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经列出了方程的每个元素，我们可以看看这些概率是如何计算的。我们可以使用简单的频率来计算这两个概率。
- en: 'The probability of a category is calculated by counting the number of times
    the category showed up in the training instances divided by the total number of
    training instances. As we know that Naïve Bayes classifiers have exhaustive and
    mutually-exclusive categories, the sum of the frequency of each category must
    equal the total number of training instances:'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个类别的概率是通过计算该类别在训练实例中出现的次数除以训练实例的总数来计算的。正如我们所知，朴素贝叶斯分类器具有穷尽性和互斥性类别，因此每个类别的频率之和必须等于训练实例的总数：
- en: '[PRE13]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The probability of a token in a category is computed by counting the number
    of times the token appeared in a category divided by the number of times all the
    other tokens appeared in this category:'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个类别中标记的概率是通过计算该标记在类别中出现的次数除以所有其他标记在该类别中出现的次数来计算的：
- en: '[PRE14]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: These probabilities are calculated to provide what is called the **maximum likelihood
    estimate** of the model. Unfortunately, these estimates provide zero probability
    for tokens that were not seen during the training. You can see this very easily
    in the calculation of an unseen token probability. Since it wasn't seen, it will
    have a frequency count of 0, and the numerator of our original equation goes to
    0.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些概率被计算出来以提供所谓的**最大似然估计**的模型。不幸的是，这些估计为在训练期间未看到的标记提供了零概率。你可以在计算未看到标记的概率中非常容易地看到这一点。由于它没有被看到，它将有一个频率计数为0，并且我们原始方程中的分子将变为0。
- en: 'In order to overcome this, we will use a technique known as **smoothing** that
    assigns a prior and then computes a maximum a posteriori estimate rather than
    a maximum likelihood estimate. A very common smoothing technique is called additive
    smoothing, and it just involves adding a prior count to every count in the training
    data. Two sets of counts are added: the first is a token count added to all the
    token frequency calculations, and the second is a category count, which is added
    to all the category count calculations.'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了克服这一点，我们将使用一种称为**平滑**的技术，它分配一个先验，然后计算最大后验估计而不是最大似然估计。一个非常常见的平滑技术称为加性平滑，它只是将先验计数添加到训练数据中的每个计数。添加了两组计数：第一组是添加到所有标记频率计算中的标记计数，第二组是类别计数，它添加到所有类别计数计算中。
- en: 'This obviously changes the `p(cat)` and the `p(token|cat)` values. Let''s call
    the `alpha` prior that is added to the category count and the `beta` prior that
    is added to the token count. When we call the `alpha` prior, our previous calculations
    will change to:'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这显然改变了`p(cat)`和`p(token|cat)`的值。让我们称添加到类别计数中的`alpha`先验和添加到标记计数中的`beta`先验为`alpha`。当我们调用`alpha`先验时，我们的先前计算将变为：
- en: '[PRE15]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'When we call the `beta` prior, the calculations will change to:'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们调用`beta`先验时，计算将变为：
- en: '[PRE16]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now that we have set up our equations, let's look at a concrete example.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经建立了方程，让我们看看一个具体的例子。
- en: 'We''ll build a classifier to classify whether the forecast calls for hot or
    cold weather based on a set of phrases:'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将构建一个分类器，根据一组短语来分类预报是热天还是冷天：
- en: '[PRE17]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'There are a total of seven tokens in these five training items:'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这五个训练项中总共有七个标记：
- en: '`super`'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`super`'
- en: '`steamy`'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`steamy`'
- en: '`today`'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`today`'
- en: '`boiling`'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`boiling`'
- en: '`out`'
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out`'
- en: '`freezing`'
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`freezing`'
- en: '`icy`'
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`icy`'
- en: Of these, all the tokens appear once, except `steamy`, which appears twice in
    the `hot` category and `out`, which appears once in each category. This is our
    training data. Now, let's calculate the probability of an input text being in
    the `hot` or `cold` category . Let's say our input is the word `super`. Let's
    set the category prior `alpha` to `1` and the token prior `beta` also to `1`.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这些中，所有标记都出现了一次，除了`steamy`在`hot`类别中出现了两次，以及`out`在每个类别中都出现了一次。这是我们训练数据。现在，让我们计算输入文本属于`hot`或`cold`类别的概率。假设我们的输入是单词`super`。让我们将类别先验`alpha`设置为`1`，将标记先验`beta`也设置为`1`。
- en: 'So, we will calculate the probabilities of `p(hot|super)` and `p(cold|super)`:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，我们将计算`p(hot|super)`和`p(cold|super)`的概率：
- en: '[PRE18]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We will take into consideration all the tokens, including the ones that haven''t
    been seen in the `hot` category:'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将考虑所有标记，包括在`hot`类别中尚未见过的标记：
- en: '[PRE19]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This will give us a denominator equal to a sum of these inputs:'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将给我们一个等于这些输入之和的分母：
- en: '[PRE20]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, `p(super|hot) = 2/13` is one part of the equation. We still need to calculate
    `p(hot)` and `p (super)`:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，`p(super|hot) = 2/13`是方程的一部分。我们还需要计算`p(hot)`和`p(super)`：
- en: '[PRE21]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'For the `hot` category, we have three documents or cases, and for the `cold`
    category, we have two documents in our training data. So, `freq(hot) = 3` and
    `freq(cold) = 2`:'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于`hot`类别，我们有三个文档或案例，而对于`cold`类别，我们在训练数据中有两个文档。因此，`freq(hot) = 3`和`freq(cold)
    = 2`：
- en: '[PRE22]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To calculate `p(super|cold)`, we need to repeat the same steps:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要计算`p(super|cold)`，我们需要重复相同的步骤：
- en: '[PRE23]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This gives us the probability of the token `super`:'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这给我们带来了标记`super`的概率：
- en: '[PRE24]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We now have all the pieces together to calculate `p(hot|super)` and `p(cold|super)`:'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了计算`p(hot|super)`和`p(cold|super)`的所有部分：
- en: '[PRE25]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'If we want to repeat this for the input stream `super super`, the following
    calculations can be used:'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果我们想要对输入流`super super`重复此操作，可以使用以下计算：
- en: '[PRE26]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Remember our naïve assumption: the probability of the tokens is the product
    of the probabilities, since we assume that they are independent of each other.'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 记住我们的朴素假设：标记的概率是概率的乘积，因为我们假设它们彼此独立。
- en: Let's verify our calculations by training up the naïve Bayes classifier and
    using the same input.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过训练朴素贝叶斯分类器并使用相同的输入来验证我们的计算：
- en: How to do it...
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s verify some of these calculations in code:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在代码中验证一些这些计算：
- en: 'In your IDE, run the `TrainAndRunNaiveBayesClassifier` class in the code package
    of this chapter, or using the command line, type the following command:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的IDE中，运行本章代码包中的`TrainAndRunNaiveBayesClassifier`类，或者使用命令行，输入以下命令：
- en: '[PRE27]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In the prompt, let''s use our first example, `super`:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在提示中，让我们使用我们的第一个例子，`super`：
- en: '[PRE28]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'As we can see, our calculations were correct. For the case of a word, `hello`,
    that doesn''t exist in our training; we will fall back to the prevalence of the
    categories modified by the category''s prior counts:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如我们所见，我们的计算是正确的。对于在训练中不存在的单词`hello`的情况，我们将回退到由类别先验计数修改的类别的普遍性：
- en: '[PRE29]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Again, for the case of `super super`, our calculations were correct.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次，对于`super super`的情况，我们的计算是正确的。
- en: '[PRE30]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The source that generates the preceding output is in `src/com/lingpipe/chapter3/TrainAndRunNaiveBays.java`.
    The code should be straightforward, so we will not covering it in this recipe.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成前面输出的源代码在`src/com/lingpipe/chapter3/TrainAndRunNaiveBays.java`。代码应该是直截了当的，所以我们不会在这个配方中涵盖它。
- en: See also
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考内容
- en: For more details on configuring naïve Bayes, including length normalizing, refer
    to the Javadoc at [http://alias-i.com/lingpipe/docs/api/index.html?com/aliasi/classify/TradNaiveBayesClassifier.html](http://alias-i.com/lingpipe/docs/api/index.html?com/aliasi/classify/TradNaiveBayesClassifier.html)
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关配置朴素贝叶斯的更多详细信息，包括长度归一化，请参阅[http://alias-i.com/lingpipe/docs/api/index.html?com/aliasi/classify/TradNaiveBayesClassifier.html](http://alias-i.com/lingpipe/docs/api/index.html?com/aliasi/classify/TradNaiveBayesClassifier.html)上的Javadoc
- en: You can refer to the expectation maximization tutorial at [http://alias-i.com/lingpipe/demos/tutorial/em/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/em/read-me.html)
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以参考[http://alias-i.com/lingpipe/demos/tutorial/em/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/em/read-me.html)上的期望最大化教程
- en: Feature extractors
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征提取器
- en: Up until now, we have been using characters and words to train our models. We
    are about to introduce a classifier (logistic regression) that allows for other
    observations about the data to inform the classifier—for example, whether a word
    is actually a date. Feature extractors are used in CRF taggers and K-means clustering.
    This recipe will introduce feature extractors independent of any technology that
    uses them.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直使用字符和单词来训练我们的模型。我们将介绍一个分类器（逻辑回归），它允许对数据进行其他观察，以告知分类器——例如，一个单词是否实际上是一个日期。特征提取器在CRF标记器和K-means聚类中使用。这个配方将介绍独立于任何使用它们的技术的特征提取器。
- en: How to do it...
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: There is not much to this recipe, but the upcoming *Logistic regression* recipe
    has many moving parts, and this is one of them.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方没有太多内容，但即将到来的*逻辑回归*配方有很多组成部分，这是其中之一。
- en: 'Fire up your IDE or type in the command line:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动您的IDE或输入命令行：
- en: '[PRE32]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Type a string into our standard I/O loop:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的标准I/O循环中输入一个字符串：
- en: '[PRE33]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Features are then produced:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后产生特征：
- en: '[PRE34]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note that there is no order information here. Does it keep a count or not?
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，这里没有顺序信息。它是否保持计数？
- en: '[PRE35]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The feature extractor keeps count with `my=2`, and it does not normalize the
    case (`My` is different from `my`). Refer to the later recipes in this chapter
    on how to modify feature extractors—they are very flexible.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征提取器使用`my=2`进行计数，并且它不规范化大小写（`My`与`my`不同）。请参考本章后面的配方，了解如何修改特征提取器——它们非常灵活。
- en: How it works…
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'LingPipe provides solid infrastructure for the creation of feature extractors.
    The code for this recipe is in `src/com/lingipe/chapter3/SimpleFeatureExtractor.java`:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: LingPipe为创建特征提取器提供了坚实的基础设施。这个配方的代码在`src/com/lingipe/chapter3/SimpleFeatureExtractor.java`：
- en: '[PRE36]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The preceding code constructs `TokenFeatureExtractor` with `TokenizerFactory`.
    It is one of the 13 `FeatureExtractor` implementations provided in LingPipe.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码使用`TokenizerFactory`构建了`TokenFeatureExtractor`。它是LingPipe提供的13个`FeatureExtractor`实现之一。
- en: Next, we will apply the I/O loop and print out the feature, which is `Map<String,
    ? extends Number>`. The `String` element is the feature name. In this case, the
    actual token is the name. The second element of the map is a value that extends
    `Number`, in this case, the count of how many times the token was seen in the
    text.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将应用I/O循环并打印出特征，它是`Map<String, ? extends Number>`。`String`元素是特征名称。在这种情况下，实际的标记是名称。映射的第二个元素是一个扩展`Number`的值，在这种情况下，是标记在文本中出现的次数。
- en: '[PRE37]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The feature name needs to only be a unique name—we could have prepended each
    feature name with `SimpleFeatExt_` to keep track of where the feature came from,
    which is helpful in complex feature-extraction scenarios.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 特征名称只需要是一个唯一的名称——我们可以在每个特征名称前加上`SimpleFeatExt_`以跟踪特征来源，这在复杂的特征提取场景中很有帮助。
- en: Logistic regression
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Logistic regression is probably responsible for the majority of industrial classifiers,
    with the possible exception of naïve Bayes classifiers. It almost certainly is
    one of the best performing classifiers available, albeit at the cost of slow training
    and considerable complexity in configuration and tuning.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归可能是大多数工业分类器的主要责任，可能除外的是朴素贝叶斯分类器。它几乎肯定是目前表现最好的分类器之一，尽管代价是训练速度慢，配置和调整相当复杂。
- en: Logistic regression is also known as maximum entropy, neural network classification
    with a single neuron, and others. So far in this book, the classifiers have been
    based on the underlying characters or tokens, but logistic regression uses unrestricted
    feature extraction, which allows for arbitrary observations of the situation to
    be encoded in the classifier.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归也被称为最大熵、单神经元神经网络分类以及其他名称。到目前为止，本书中的分类器都是基于底层字符或标记，但逻辑回归使用无限制的特征提取，这允许将任意观察到的情境编码到分类器中。
- en: This recipe closely follows a more complete tutorial at [http://alias-i.com/lingpipe/demos/tutorial/logistic-regression/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/logistic-regression/read-me.html).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法与[http://alias-i.com/lingpipe/demos/tutorial/logistic-regression/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/logistic-regression/read-me.html)上的更完整的教程非常相似。
- en: How logistic regression works
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归的工作原理
- en: All that logistic regression does is take a vector of feature weights over the
    data, apply a vector of coefficients, and do some simple math, which results in
    a probability for each class encountered in training. The complicated bit is in
    determining what the coefficients should be.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归所做的只是对数据中的特征权重向量应用一个系数向量，并进行一些简单的数学运算，从而得到训练过程中遇到的每个类别的概率。复杂之处在于确定系数应该是什么。
- en: 'The following are some of the features produced by our training recipe for
    21 tweets annotated for English `e` and non-English `n`. There are relatively
    few features because feature weights are being pushed to `0.0` by our prior, and
    once a weight is `0.0`, then the feature is removed. Note that one category, `n`,
    is set to `0.0` for all the features of the `n-1` category—this is a property
    of the logistic regression process that fixes once categories features to `0.0`
    and adjust all other categories features with respect to that:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们为21条标注为英语`e`和非英语`n`的推文训练的某些特征。由于我们的先验将特征权重推到`0.0`，因此特征相对较少，一旦权重为`0.0`，则删除该特征。请注意，一个类别`n`被设置为`0.0`，对于`n-1`类别的所有特征——这是逻辑回归过程的一个属性，一旦将类别特征固定为`0.0`，就调整所有其他类别特征相对于该值：
- en: '[PRE38]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Take the string, `I luv Disney`, which will only have two non-zero features:
    `I=0.37` and `Disney=0.15` for `e` and zeros for `n`. Since there is no feature
    that matches `luv`, it is ignored. The probability that the tweet is English breaks
    down to:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 以字符串“`I luv Disney`”为例，它将只有两个非零特征：`I=0.37`和`Disney=0.15`对于`e`，而`n`为0。由于没有与`luv`匹配的特征，它被忽略。推文是英语的概率分解如下：
- en: '*vectorMultiply(e,[I,Disney]) = exp(.37*1 + .15*1) = 1.68*'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '*vectorMultiply(e,[I,Disney]) = exp(.37*1 + .15*1) = 1.68*'
- en: '*vectorMultiply(n,[I,Disney]) = exp(0*1 + 0*1) = 1*'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '*vectorMultiply(n,[I,Disney]) = exp(0*1 + 0*1) = 1*'
- en: 'We will rescale to a probability by summing the outcomes and dividing it:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过求和结果并除以总数来将其转换为概率：
- en: '*p(e|,[I,Disney]) = 1.68/(1.68 +1) = 0.62*'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*p(e|,[I,Disney]) = 1.68/(1.68 +1) = 0.62*'
- en: '*p(e|,[I,Disney]) = 1/(1.68 +1) = 0.38*'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '*p(e|,[I,Disney]) = 1/(1.68 +1) = 0.38*'
- en: This is how the math works on running a logistic regression model. Training
    is another issue entirely.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是运行逻辑回归模型时数学工作的方式。训练是另一个完全不同的问题。
- en: Getting ready
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe assumes the same framework that we have been using all along to
    get training data from `.csv` files, train the classifier, and run it from the
    command line.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法假设了我们一直在使用的相同框架，从`.csv`文件中获取训练数据，训练分类器，并在命令行中运行它。
- en: Setting up to train the classifier is a bit complex because of the number of
    parameters and objects used in training. We will discuss all the 10 arguments
    to the training method as found in `com.lingpipe.cookbook.chapter3.TrainAndRunLogReg`.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 设置训练分类器相对复杂，因为训练中使用的参数和对象数量众多。我们将讨论`com.lingpipe.cookbook.chapter3.TrainAndRunLogReg`中训练方法的10个参数。
- en: 'The `main()` method starts with what should be familiar classes and methods—if
    they are not familiar, have a look at *How to train and evaluate with cross validation*
    and *Introduction to Introduction to tokenizer Factories – finding words in a
    character stream*, recipes from [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple
    Classifiers"), *Simple Classifiers*, and [Chapter 2](part0027_split_000.html#page
    "Chapter 2. Finding and Working with Words"), *Finding and Working with Words*,
    respectively:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()` 方法从应该熟悉的类和方法开始——如果它们不熟悉，请查看 *如何使用交叉验证进行训练和评估* 和 *介绍到介绍分词器工厂——在字符流中查找单词*，这些是从
    [第1章](part0014_split_000.html#page "第1章。简单分类器")，*简单分类器* 和 [第2章](part0027_split_000.html#page
    "第2章。查找和使用单词*)，*查找和使用单词* 中摘录的食谱：'
- en: '[PRE39]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note that we are using `XValidatingObjectCorpus` when a simpler implementation
    such as `ListCorpus` will do. We will not take advantage of any of its cross-validation
    features, because the `numFolds` param as `0` will have training visit the entire
    corpus. We are trying to keep the number of novel classes to a minimum, and we
    tend to always use this implementation in real-world gigs anyway.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当我们可以使用更简单的实现，如 `ListCorpus` 时，我们正在使用 `XValidatingObjectCorpus`。我们不会利用其交叉验证的任何功能，因为
    `numFolds` 参数设置为 `0` 将导致训练访问整个语料库。我们试图将新类别的数量保持在最低，而且我们通常在实际工作中总是使用这种实现。
- en: 'Now, we will start to build the configuration for our classifier. The `FeatureExtractor<E>`
    interface provides a mapping from data to features; this will be used to train
    and run the classifier. In this case, we are using a `TokenFeatureExtractor()`
    method, which creates features based on the tokens found by the tokenizer supplied
    during construction. This is similar to what naïve Bayes reasons over. The previous
    recipe goes into more detail about what the feature extractor is doing if this
    is not clear:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将开始为我们的分类器构建配置。`FeatureExtractor<E>` 接口提供了从数据到特征的映射；这将被用于训练和运行分类器。在这种情况下，我们使用
    `TokenFeatureExtractor()` 方法，该方法基于在构建时提供的分词器找到的标记创建特征。这类似于朴素贝叶斯推理。前面的食谱更详细地介绍了特征提取器正在做什么，如果这还不清楚的话：
- en: '[PRE40]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The `minFeatureCount` item is usually set to a number higher than 1, but with
    small training sets, this is needed to get any performance. The thought behind
    filtering feature counts is that logistic regression tends to overfit low-count
    features that, just by chance, exist in one category of training data. As training
    data grows, the `minFeatureCount` value is adjusted usually by paying attention
    to cross-validation performance:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`minFeatureCount` 项目通常设置为一个大于1的数字，但在小型训练集中，这是获得任何性能所必需的。过滤特征计数的想法是，逻辑回归倾向于过度拟合低计数的特征，这些特征只是偶然存在于训练数据的一个类别中。随着训练数据的增长，`minFeatureCount`
    值通常通过关注交叉验证性能来调整：'
- en: '[PRE41]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The `addInterceptFeature` Boolean controls whether a category feature exists
    that models the prevalence of the category in training. The default name of the
    intercept feature is `*&^INTERCEPT%$^&**`, and you will see it in the weight vector
    output if it is being used. By convention, the intercept feature is set to `1.0`
    for all inputs. The idea is that if a category is just very common or very rare,
    there should be a feature that captures just this fact, independent of other features
    that might not be as cleanly distributed. This models the category probability
    in naïve Bayes in some way, but the logistic regression algorithm will decide
    how useful it is as it does with all other features:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`addInterceptFeature` 布尔值控制是否存在一个类别特征，该特征用于建模训练中该类别的普遍性。截距特征的默认名称是 `*&^INTERCEPT%$^&**`，如果正在使用它，你将在权重向量输出中看到它。按照惯例，对于所有输入，截距特征被设置为
    `1.0`。其想法是，如果一个类别非常普遍或非常罕见，应该有一个特征来捕捉这一事实，而与其他可能分布不干净的其它特征无关。这种方式在朴素贝叶斯中某种意义上建模了类别概率，但逻辑回归算法将决定它作为所有其他特征一样有用：'
- en: '[PRE42]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: These Booleans control what happens to the intercept feature if it is used.
    Priors, in the following code, are typically not applied to the intercept feature;
    this is the result if this parameter is true. Set the Boolean to `false`, and
    the prior will be applied to the intercept.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这些布尔值控制如果使用截距特征时会发生什么。在下面的代码中，先验通常不应用于截距特征；这是如果此参数为真的结果。将布尔值设置为 `false`，先验将应用于截距。
- en: Next is the `RegressionPrior` instance, which controls how the model is fit.
    What you need to know is that priors help prevent logistic regression from overfitting
    the data by pushing coefficients towards 0\. There is a non-informative prior
    that does not do this with the consequence that if there is a feature that applies
    to just one category it will be scaled to infinity, because the model keeps fitting
    better as the coefficient is increased in the numeric estimation. Priors, in this
    context, function as a way to not be over confident in observations about the
    world.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是 `RegressionPrior` 实例，它控制模型如何拟合。您需要知道的是，先验有助于通过将系数推向 0 来防止逻辑回归过度拟合数据。存在一个非信息性先验，它不会这样做，结果是如果有一个只适用于一个类别的特征，它将被缩放到无穷大，因为模型在数值估计中增加系数时拟合得越来越好。在这个上下文中，先验充当了一种方式，以避免对世界的观察过于自信。
- en: Another dimension in the `RegressionPrior` instance is the expected variance
    of the features. Low variance will push coefficients to zero more aggressively.
    The prior returned by the static `laplace()` method tends to work well for NLP
    problems. For more information on what is going on here, consult the relevant
    Javadoc and the logistic regression tutorial referenced at the beginning of the
    recipe—there is a lot going on, but it can be managed without a deep theoretical
    understanding. Also, see the *Tuning parameters in logistic regression* recipe
    in this chapter.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `RegressionPrior` 实例的另一个维度中是特征的预期方差。低方差会更有力地将系数推向零。静态 `laplace()` 方法返回的先验通常对
    NLP 问题很有用。有关这里发生情况的更多信息，请参阅相关的 Javadoc 和食谱开头引用的逻辑回归教程——这里有很多事情发生，但无需深入的理论理解就可以管理。此外，请参阅本章中的“逻辑回归中的参数调整”食谱。
- en: '[PRE43]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Next, we will control how the algorithm searches for an answer.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将控制算法搜索答案的方式。
- en: '[PRE44]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '`AnnealingSchedule` is best understood by consulting the Javadoc, but what
    it does is change how much the coefficients are allowed to vary when fitting the
    model. The `minImprovement` parameter sets the amount the model fit has to improve
    to not terminate the search, because the algorithm has converged. The `minEpochs`
    parameter sets a minimal number of iterations, and `maxEpochs` sets an upper limit
    if the search does not converge as determined by `minImprovement`.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查阅 Javadoc 可以更好地理解 `AnnealingSchedule`，但它所做的就是改变在拟合模型时允许系数变化的程度。`minImprovement`
    参数设置模型拟合必须改进的量，以避免终止搜索，因为算法已经收敛。`minEpochs` 参数设置最小迭代次数，而 `maxEpochs` 设置搜索没有根据
    `minImprovement` 确定的收敛时上限。
- en: 'Next is some code that allows for basic reporting/logging. `LogLevel.INFO`
    will report a great deal of information about the progress of the classifier as
    it tries to converge:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是一段允许进行基本报告/记录的代码。`LogLevel.INFO` 将报告分类器在尝试收敛过程中的大量信息：
- en: '[PRE45]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Here ends the *Getting ready* section of one of our most complex classes—next,
    we will train and run the classifier.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们最复杂的课程之一“准备”部分的结束——接下来，我们将训练并运行分类器。
- en: How to do it...
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'It has been a bit of work setting up to train and run this class. We will just
    go through the steps to get it up and running; the upcoming recipes will address
    its tuning and evaluation:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 设置训练和运行此类已经做了一些工作。我们将只通过步骤来使其运行；即将到来的食谱将解决其调整和评估：
- en: 'Note that there is a more complex 14-argument train method as well the one
    that extends configurability. This is the 10-argument version:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，还有一个更复杂的 14 参数的 `train` 方法以及扩展可配置性的一个方法。这是 10 参数版本：
- en: '[PRE46]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The `train()` method, depending on the `LogLevel` constant, will produce from
    nothing with `LogLevel.NONE` to the prodigious output with `LogLevel.ALL`.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据 `LogLevel` 常量，`train()` 方法将根据 `LogLevel.NONE` 从无到 `LogLevel.ALL` 的巨大输出产生。
- en: 'While we are not going to use it, we show how to serialize the trained model
    to disk. The *How to serialize a LingPipe object – classifier example* recipe
    in [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"),
    *Simple Classifiers*, explains what is going on:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然我们不会使用它，但我们展示了如何将训练好的模型序列化到磁盘。[第 1 章](part0014_split_000.html#page "第 1 章。简单分类器")，“如何序列化
    LingPipe 对象 – 分类器示例”食谱解释了这里发生的情况：
- en: '[PRE47]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Once trained, we will apply the standard classification loop with:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练完成后，我们将使用以下标准分类循环应用：
- en: '[PRE48]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Run the preceding code in the IDE of your choice or use the command-line command:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您选择的 IDE 中运行前面的代码或使用命令行命令：
- en: '[PRE49]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The result is a big dump of information about the training:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果是关于训练的大量信息：
- en: '[PRE50]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The `epoch` reporting goes on until either the number of epochs is met or the
    search converges. In the following case, the number of epochs was met:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`epoch` 报告会持续进行，直到达到指定的 epoch 数量或搜索收敛。在以下情况下，达到了 epoch 数量：'
- en: '[PRE51]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now, we can play with the classifier a bit:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以稍微玩一下这个分类器：
- en: '[PRE52]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: This should look familiar; it is exactly the same result as the worked example
    at the start of the recipe.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这看起来很熟悉；这正是菜谱开头的工作示例的结果。
- en: That's it! You have trained up and used the world's most relevant industrial
    classifier. However, there's a lot more to harnessing the power of this beast.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你已经训练并使用了世界上最具相关性的工业分类器。然而，要充分利用这个巨兽的力量还有很多。
- en: Multithreaded cross validation
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多线程交叉验证
- en: Cross validation (refer to the *How to train and evaluate with cross validation*
    recipe in [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"),
    *Simple Classifiers*) can be very slow, which interferes with tuning systems.
    This recipe will show you a simple but effective way to access all the available
    cores on your system to more quickly process each fold.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证（参考第 1 章 *如何使用交叉验证进行训练和评估* 菜谱，*简单分类器*），可能会非常慢，这会干扰系统调优。这个菜谱将向你展示一种简单但有效的方法，以访问系统上所有可用的核心，以便更快地处理每个折叠。
- en: How to do it...
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: This recipe explains multi-threaded cross validation in the context of the next
    recipe, so don't be confused by the fact that the same class is repeated.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这个菜谱在下一个菜谱的上下文中解释了多线程交叉验证，所以不要因为同一个类被重复而感到困惑。
- en: 'Engage your IDE or type in the command line:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动你的 IDE 或在命令行中输入命令：
- en: '[PRE53]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The system then responds with the following output (you might have to scroll
    to the top of the window):'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 系统随后会响应以下输出（你可能需要滚动到窗口顶部）：
- en: '[PRE54]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The default training data is 21 tweets annotated for English `e` and non-English
    `n`. In the preceding output, we saw a report of each fold that runs as a thread
    and the resulting confusion matrix. That's it! We just did multithreaded cross
    validation. Let's see how this works.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认的训练数据是 21 条标注为英语 `e` 和非英语 `n` 的推文。在前面的输出中，我们看到了作为线程运行的每个折叠的报告和结果混淆矩阵。就是这样！我们刚刚完成了多线程交叉验证。让我们看看它是如何工作的。
- en: How it works…
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'All the action happens in the `Util.xvalLogRegMultiThread()` method, which
    we invoke from `src/com/lingpipe/cookbook/chapter3/TuneLogRegParams.java`. The
    details of `TuneLogRegParams` are covered in the next recipe. This recipe will
    focus on the `Util` method:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 所有操作都在 `Util.xvalLogRegMultiThread()` 方法中发生，我们从 `src/com/lingpipe/cookbook/chapter3/TuneLogRegParams.java`
    调用此方法。`TuneLogRegParams` 的细节将在下一个菜谱中介绍。这个菜谱将专注于 `Util` 方法：
- en: '[PRE55]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: All 10 parameters used to configure logistic regression are controllable (you
    can refer to the previous recipe for explanation), with the addition of `numFolds`,
    which controls how many folds there will be, `numThreads`, which controls how
    many threads can be run at the same time, and finally, `categories`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 用于配置逻辑回归的所有 10 个参数都是可控制的（你可以参考前面的菜谱进行解释），包括 `numFolds`，它控制将有多少个折叠，`numThreads`，它控制可以同时运行多少个线程，以及最后的
    `categories`。
- en: 'If we look at the relevant method in `src/com/lingpipe/cookbook/Util.java`,
    we see:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看 `src/com/lingpipe/cookbook/Util.java` 中的相关方法，我们会看到：
- en: '[PRE56]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The method starts with the matching arguments for configuration information
    of logistic regression and running cross validation. Since cross validation is
    most often used in system tuning, all the relevant bits are exposed to modification.
    Everything is final because we are using an anonymous inner class to create threads.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 方法从为逻辑回归配置信息匹配的参数开始，运行交叉验证。由于交叉验证最常用于系统调优，所有相关部分都暴露出来以供修改。一切都是最终的，因为我们使用匿名内部类来创建线程。
- en: 'Next, we will set up `crossFoldEvaluator` that will collect the results from
    each thread:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将设置 `crossFoldEvaluator`，它将收集每个线程的结果：
- en: '[PRE57]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now, we will get down to the business of creating threads for each fold, `i`:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将着手为每个折叠创建线程，`i`：
- en: '[PRE58]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The `XValidatingObjectCorpus` class is set up for multithreaded access by creating
    a thread-safe version of the corpus for reads with the `itemView()` method. This
    method returns a corpus that can have the fold set, but no data can be added.
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`XValidatingObjectCorpus` 类通过创建用于读取的线程安全版本的数据集，通过 `itemView()` 方法设置为多线程访问。此方法返回一个可以设置折叠的数据集，但不能添加数据。'
- en: 'Each thread is a `runnable` object, where the actual work of training and evaluating
    the fold is handled in the `run()` method:'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个线程都是一个 `runnable` 对象，其中折叠的训练和评估的实际工作在 `run()` 方法中处理：
- en: '[PRE59]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'In this code, we started with training the classifier, which, in turn, requires
    a `try/catch` statement to handle `IOException` thrown by the `LogisticRegressionClassifier.train()`
    method. Next, we will create `withinFoldEvaluator` that will be populated within
    the thread without a synchronization issue:'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在此代码中，我们首先训练分类器，这反过来又需要一个 `try/catch` 语句来处理 `LogisticRegressionClassifier.train()`
    方法抛出的 `IOException`。接下来，我们将创建 `withinFoldEvaluator`，它将在线程中填充，而不会出现同步问题：
- en: '[PRE60]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'It is important that `storeInputs` be `true` so that the fold results can be
    added to `crossFoldEvaluator`:'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 确保 `storeInputs` 为 `true` 非常重要，这样可以将折叠结果添加到 `crossFoldEvaluator`：
- en: '[PRE61]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This method, also in `Util`, iterates over all the true positives and false
    negatives for each category and adds them to `crossFoldEvaluator`. Note that this
    is synchronized: this means that only one thread can access the method at a time,
    but given that classification has already been done, it should not be much of
    a bottleneck:'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此方法，同样在 `Util` 中，遍历每个类别的所有真实正例和假负例，并将它们添加到 `crossFoldEvaluator`。请注意，这是同步的：这意味着一次只有一个线程可以访问该方法，但由于分类已经完成，这不应该成为瓶颈：
- en: '[PRE62]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The method takes the true positives and false negatives from each category and
    adds them to the `crossFoldEval` evaluator. These are essentially copy operations
    that do not take long to compute.
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该方法将每个类别的真实正例和假负例添加到 `crossFoldEval` 评估器中。这些基本上是复制操作，计算起来不费时。
- en: 'Returning to `xvalLogRegMultiThread`, we will handle the exception and add
    the completed `Runnable` to our list of `Thread`:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回到 `xvalLogRegMultiThread`，我们将处理异常并将完成的 `Runnable` 添加到我们的 `Thread` 列表中：
- en: '[PRE63]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'With all the threads set up, we will invoke `runThreads()` as well as print
    the confusion matrix that results. We will not go into the source of `runThreads()`,
    because it is a straightforward Java management of threads, and `printConfusionMatrix`
    has been covered in [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple
    Classifiers"), *Simple Classifiers*:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在设置好所有线程后，我们将调用 `runThreads()` 并打印出由此产生的混淆矩阵。我们不会深入探讨 `runThreads()` 的来源，因为它只是简单的
    Java 线程管理，而 `printConfusionMatrix` 在 [第 1 章](part0014_split_000.html#page "Chapter 1. Simple
    Classifiers")，*简单分类器* 中已经介绍过：
- en: '[PRE64]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: That's it for really speeding up cross validation on multicore machines. It
    can make a big difference when tuning systems.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是加快多核机器上交叉验证速度的全部内容。在调整系统时，这可以产生很大的影响。
- en: Tuning parameters in logistic regression
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整逻辑回归中的参数
- en: Logistic regression presents an intimidating array of parameters to tweak for
    better performance, and working with it is a bit of black art. Having built thousands
    of these classifiers, we are still learning how to do it better. This recipe will
    point you in the general right direction, but the topic probably deserves its
    own book.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归提供了一系列令人畏惧的参数来调整以获得更好的性能，与之合作有点像黑魔法。我们已经构建了数千个这样的分类器，但我们仍在学习如何做得更好。这个配方将指引你走向一般正确的方向，但这个主题可能值得一本自己的书。
- en: How to do it...
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: This recipe involves extensive changes to the source of `src/com/lingpipe/chapter3/TuneLogRegParams.java`.
    We will just run one configuration of it here, with most of the exposition in
    the *How it works…* section.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方涉及到对 `src/com/lingpipe/chapter3/TuneLogRegParams.java` 源代码的广泛修改。我们在这里只运行它的一个配置，大部分解释都在
    *它是如何工作的…* 部分中。
- en: 'Engage your IDE or type the following in the command line:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动你的 IDE 或在命令行中输入以下内容：
- en: '[PRE65]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The system then responds with cross-validation output confusion matrix for
    our default data in `data/disney_e_n.csv`:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 系统随后会响应默认数据 `data/disney_e_n.csv` 的交叉验证输出混淆矩阵：
- en: '[PRE66]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Next, we will report on false positives for each category—this will cover all
    the mistakes made:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将报告每个类别的假正例——这将涵盖所有错误：
- en: '[PRE67]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'This output is followed by the features, their coefficients, and a count—remember
    that we will see `n-1` categories, because one of the category''s features is
    set to `0.0` for all features:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此输出之后是特征、它们的系数和计数——记住，我们将看到 `n-1` 个类别，因为其中一个类别的特征被设置为所有特征的 `0.0`：
- en: '[PRE68]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Finally, we have our standard I/O that allows for examples to be tested:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们有我们的标准输入/输出，允许测试示例：
- en: '[PRE69]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: This is the basic structure that we will work with. In the upcoming sections,
    we will explore the impact of varying parameters more closely.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是我们将要工作的基本结构。在接下来的章节中，我们将更深入地探讨参数变化的影响。
- en: How it works…
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'This recipe assumes that you are familiar with logistic regression training
    and configuration from two recipes back and cross validation, which is the previous
    recipe. The overall structure of the code is presented in an outline form, with
    the tuning parameters retained. Modifying each parameter will be discussed later
    in the recipe—below we start with the `main()` method ignoring some code as indicated
    by ''`...`'' and the tunable code shown for tokenization and feature extraction:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 本菜谱假设你已经熟悉从两个菜谱之前的逻辑回归训练和配置以及交叉验证，这是之前的菜谱。代码的整体结构以提纲形式呈现，保留了调整参数。将讨论如何修改每个参数将在菜谱的后面部分——下面我们开始从`main()`方法开始，忽略一些由`...`指示的代码，并显示用于分词和特征提取的可调整代码：
- en: '[PRE70]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Next the priors are set up:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来设置先验概率：
- en: '[PRE71]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Priors have a strong influence on the behavior coefficient assignment:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 先验概率对行为系数分配有强烈的影响：
- en: '[PRE72]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The preceding code controls the search space of logistic regression:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码控制着逻辑回归的搜索空间：
- en: '[PRE73]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: The preceding code runs cross validation to see how the system is doing—note
    the elided parameters with `...`.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码运行交叉验证以查看系统表现如何——注意省略的参数使用`...`表示。
- en: 'In the following code, we will set the number of folds to `0`, which will have
    the train method visit the entire corpus:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们将折数设置为`0`，这将使训练方法访问整个语料库：
- en: '[PRE74]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Then, for each category, we will print out the features and their coefficients
    for the just trained classifier:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于每个类别，我们将打印出刚刚训练好的分类器的特征及其系数：
- en: '[PRE75]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Finally, we will have the usual console classifier I/O:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将有常规的控制台分类器输入/输出：
- en: '[PRE76]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Tuning feature extraction
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调整特征提取
- en: 'The features that are fed into logistic regression have a huge impact on the
    performance of the system. We will cover feature extraction in greater detail
    in the later recipes, but we will bring to bear one of the most useful and somewhat
    counter-intuitive approaches here, because it is very easy to execute—use character
    ngrams instead of words/tokens. Let''s look at an example:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 输入到逻辑回归中的特征对系统的性能有巨大影响。我们将在后面的菜谱中更详细地介绍特征提取，但在这里我们将使用一种非常实用且有些反直觉的方法，因为它非常容易执行——使用字符n-gram而不是单词/标记。让我们看一个例子：
- en: '[PRE77]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: This output indicates that the classifier is tied between `e` English and `n`
    non-English as a decision. Scrolling back through the features, we will see that
    there are no matches for any of the words in the input. There are some substring
    matches on the English side. `The` has the substring `he` for the feature word
    `the`. For language ID, it makes sense to consider subsequences, but as a matter
    of experience, it can be a big help for sentiment and other problems as well.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出表明，分类器在`e`英语和`n`非英语之间做出决策。通过滚动回查看特征，我们将看到输入中的任何单词都没有匹配项。在英语方面有一些子串匹配。`The`对于特征词`the`有子串`he`。对于语言识别，考虑子串是有意义的，但根据经验，它对于情感和其他问题也可能有很大的帮助。
- en: 'Modifying the tokenizer to be two-to-four-character ngrams is done as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 将分词器修改为两到四个字符的n-gram如下：
- en: '[PRE78]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'This results in the proper distinction being made:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了适当的区分：
- en: '[PRE79]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: The overall performance on cross validation drops a bit. For very small training
    sets, such as 21 tweets, this is not unexpected. Generally, the cross-validation
    performance with a consultation of what the mistakes look like and a look at the
    false positives will help guide this process.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在交叉验证中的整体性能略有下降。对于非常小的训练集，例如21条推文，这是预料之中的。通常，通过查看错误的样子以及查看假阳性，交叉验证的性能将有助于指导这个过程。
- en: 'In looking at the false positives, it is clear that `Disney` is a source of
    problems, because the coefficients on features show it to be evidence for English.
    Some of the false positives are:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看假阳性时，很明显`Disney`是问题之源，因为特征上的系数表明它是英语的证据。一些假阳性如下：
- en: '[PRE80]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The following are the features for `e`:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是为`e`的特征：
- en: '[PRE81]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: In the absence of more training data, the features `!`, `Disney`, and `"` should
    be removed to help this classifier perform better, because none of these features
    are language specific, whereas `I` and `to` are, although not unique to English.
    This can be done by filtering the data or creating the appropriate tokenizer factory,
    but the best move is to probably get more data.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有更多训练数据的情况下，应该删除特征`!`、`Disney`和`"`以帮助此分类器表现更好，因为这些特征都不是语言特定的，而`I`和`to`是，尽管它们不是英语特有的。这可以通过过滤数据或创建适当的分词器工厂来完成，但最好的办法可能是获取更多数据。
- en: The `minFeature` count becomes useful when there is much more data, and you
    don't want logistic regression focusing on a very-low-count phenomenon because
    it tends to lead to overfitting.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据量很大，且你不想逻辑回归专注于非常低频的现象，因为它往往会引起过拟合时，`minFeature`计数变得有用。
- en: 'Setting the `addInterceptFeature` parameter to `true` will add a feature that
    always fires. This will allow logistic regression to have a feature sensitive
    to the number of examples for each category. It is not the marginal likelihood
    of the category, as logistic regression will adjust the weight like any other
    feature—but the following priors show how it can be further tuned:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 将`addInterceptFeature`参数设置为`true`将添加一个始终触发的特征。这将允许逻辑回归具有对每个类别的示例数量敏感的特征。这并不是类别的边缘似然，因为逻辑回归会像任何其他特征一样调整权重——但是以下先验展示了如何进一步调整：
- en: '[PRE82]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: The intercept is the strongest feature for `n` in the end, and the overall cross-validation
    performance suffered in this case.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，截距是`n`的最强特征，在这种情况下，整体交叉验证性能有所下降。
- en: Priors
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 先验
- en: 'The role of priors is to restrict the tendency of logistic regression to perfectly
    fit the training data. The ones we use try in varying degrees to push coefficients
    to zero. We will start with the `nonInformativeIntercept` prior, which controls
    whether the intercept feature is subject to the normalizing influences of the
    prior—if true, then the intercept is not subject to the prior, which was the case
    in the preceding example. Setting it to `false` moved it much closer to zero from
    `-0.17`:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 先验的作用是限制逻辑回归完美拟合训练数据的倾向。我们使用的那些先验在程度上试图将系数推向零。我们将从`nonInformativeIntercept`先验开始，它控制截距特征是否受到先验的归一化影响——如果为真，则截距不受先验影响，这在先前的例子中就是这样。将其设置为`false`使其从`-0.17`移动到接近零：
- en: '[PRE83]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Next, we will adjust the variance of the prior. This sets an expected variation
    for the weights. A low variance means that coefficients are expected not to vary
    much from zero. In the preceding code, the variance was set to `2`. This is the
    result of setting it to `.01`:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将调整先验的方差。这为权重设定了一个预期的变化。低方差意味着系数预计不会从零变化很大。在先前的代码中，方差被设置为`2`。这是将其设置为`.01`的结果：
- en: '[PRE84]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: This is a drop from 104 features with variance `2` to one feature for variance
    `.01`, because once a feature has dropped to `0`, it is removed.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这是从方差为`2`的104个特征下降到方差为`.01`的一个特征，因为一旦一个特征下降到`0`，它就会被移除。
- en: 'Increasing the variance changes our top `e` features from `2` to `4`:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 增加方差将使我们的前`e`个特征从`2`增加到`4`：
- en: '[PRE85]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: This is a total of 119 features.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这总共是119个特征。
- en: 'Consider a variance of `2` and a `gaussian` prior:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个方差为`2`和`gaussian`先验：
- en: '[PRE86]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'We will get the following output:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下输出：
- en: '[PRE87]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Oddly, we spend very little time worrying about which prior we use, but variance
    has a big role in performance, because it can cut down the feature space quickly.
    Laplace is a commonly accepted prior for NLP applications.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 奇怪的是，我们花很少的时间担心使用哪个先验，但方差在性能中起着重要作用，因为它可以快速减少特征空间。拉普拉斯是NLP应用中普遍接受的先验。
- en: Consult the Javadoc and logistic regression tutorial for more information.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅Javadoc和逻辑回归教程以获取更多信息。
- en: Annealing schedule and epochs
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 退火计划和时期
- en: 'As logistic regression converges, the annealing schedule controls how the search
    space is explored and terminated:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 随着逻辑回归收敛，退火计划控制着搜索空间的探索和终止方式：
- en: '[PRE88]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: When tuning, we will increase the first parameter to the annealing schedule
    by order of magnitude (`.0025,.025,..`) if the search is taking too long—often,
    we can increase the training speed without impacting the cross-validation performance.
    Also, the `minImprovement` value can be increased to have the convergence end
    earlier, which can both increase the training speed and prevent the model from
    overfitting—this is called **early stopping**. Again, your guiding light in this
    situation is to look at the cross-validation performance when making changes.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整时，如果搜索过程耗时过长，我们将按数量级（`.0025,.025,...`）增加退火计划的第一参数——通常，我们可以增加训练速度而不会影响交叉验证性能。此外，`minImprovement`值可以增加，以便收敛结束得更快，这既可以增加训练速度，又可以防止模型过拟合——这被称为**提前停止**。再次强调，在这种情况下，你的指导方针是查看在做出更改时的交叉验证性能。
- en: The epochs required to achieve convergence can get quite high, so if the classifier
    is iterating to `maxEpochs -1`, this means that more epochs are required to converge.
    Be sure to set the `reporter.setLevel(LogLevel.INFO);` property or a more informative
    level to get the convergence report. This is another way to additionally force
    early stopping.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 实现收敛所需的epoch数可能会相当高，所以如果分类器正在迭代到`maxEpochs -1`，这意味着需要更多的epoch才能收敛。确保设置`reporter.setLevel(LogLevel.INFO);`属性或更详细的信息级别以获取收敛报告。这是强制提前停止的另一种方式。
- en: Parameter tuning is a black art that can only be learned through practice. The
    quality and quantity of training data is probably the dominant factor in classifier
    performance, but tuning can make a big difference as well.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 参数调整是一门只能通过实践学习的黑艺术。训练数据的质量和数量可能是分类器性能的主要因素，但调整也可以产生重大影响。
- en: Customizing feature extraction
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义特征提取
- en: 'Logistic regression allows for arbitrary features to be used. Features are
    any observations that can be made about data being classified. Some examples are
    as follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归允许使用任意特征。特征是关于正在分类的数据可以做出的任何观察。以下是一些例子：
- en: Words/tokens from the text.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本中的单词/标记。
- en: We found that character ngrams work very well in lieu of words or stemmed words.
    For small data sets of less than 10,000 words of training, we will use 2-4 grams.
    Bigger training data can merit a longer gram, but we have never had good results
    above 8-gram characters.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们发现，在代替单词或词干的情况下，字符n-gram工作得非常好。对于小于10,000个训练单词的小数据集，我们将使用2-4个gram。更大的训练数据可能需要更长的gram，但我们从未在超过8-gram字符的情况下获得良好的结果。
- en: Output from another component can be a feature, for example, a part-of-speech
    tagger.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自另一个组件的输出可以是特征，例如，一个词性标注器。
- en: Metadata known about the text, for example, the location of a tweet or time
    of the day it was created.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于文本已知的元数据，例如，推文的地点或创建时间。
- en: Recognition of dates and numbers abstracted from the actual value.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从实际值中抽象出的日期和数字的识别。
- en: How to do it…
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做……
- en: The source for this recipe is in `src/com/lingpipe/cookbook/chapter3/ContainsNumberFeatureExtractor.java`.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 此菜谱的源代码位于`src/com/lingpipe/cookbook/chapter3/ContainsNumberFeatureExtractor.java`。
- en: 'Feature extractors are straightforward to build. The following is a feature
    extractor that returns a `CONTAINS_NUMBER` feature with weight `1`:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征提取器很容易构建。以下是一个返回带有权重`1`的`CONTAINS_NUMBER`特征的提取器：
- en: '[PRE89]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'By adding a `main()` method, we can test the feature extractor:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过添加`main()`方法，我们可以测试特征提取器：
- en: '[PRE90]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Now run the following command:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在运行以下命令：
- en: '[PRE91]'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'The preceding code yields the following output:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '[PRE92]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: That's it. The next recipe will show you how to combine feature extractors.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。下一个菜谱将向您展示如何组合特征提取器。
- en: There's more…
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: Designing features is a bit of an art. Logistic regression is supposed to be
    robust in the face of irrelevant features, but overwhelming it with really dumb
    features will likely detract from performance.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 设计特征有点像一门艺术。逻辑回归在面临无关特征时应该很稳健，但用真正愚蠢的特征来压倒它可能会降低性能。
- en: One way to think about what features you need is to wonder what evidence from
    the text or environment helps you, the human, decide what the correct classification
    is. Try and ignore your world knowledge when looking at the text. If world knowledge,
    that is, France is a country, is important, then try and model this world knowledge
    with a gazetteer to generate `CONTAINS_COUNTRY_MENTION`.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 思考你需要哪些特征的一种方式是思考文本或环境中哪些证据有助于你，作为人类，决定正确的分类。尝试在查看文本时忽略你的世界知识。如果世界知识，即法国是一个国家，很重要，那么尝试用
    gazetteer 来模拟这种世界知识，以生成`CONTAINS_COUNTRY_MENTION`。
- en: Be aware that features are strings, and the only notion of equivalence is the
    exact string match. The `12:01pm` feature is completely distinct from `12:02pm`,
    although, to a human, these strings are very close, because we understand time.
    To get the similarity of these two features, you must have something like a `LUNCH_TIME`
    feature that is computed using time.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，特征是字符串，唯一的概念是精确的字符串匹配。`12:01pm`特征与`12:02pm`完全不同，尽管对人类来说，这些字符串非常接近，因为我们理解时间。要获取这两个特征之间的相似度，你必须有像`LUNCH_TIME`这样的特征，它是使用时间计算出来的。
- en: Combining feature extractors
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组合特征提取器
- en: Feature extractors can be combined in much the same way as tokenizers in [Chapter
    2](part0027_split_000.html#page "Chapter 2. Finding and Working with Words"),
    *Finding and Working with Words*.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取器可以像 [第 2 章](part0027_split_000.html#page "第 2 章。查找和使用单词") 中的分词器一样组合，*查找和使用单词*。
- en: How to do it…
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: This recipe will show you how to combine the feature extractor from the previous
    recipe with a very common feature extractor over character ngrams.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱将向您展示如何将前一个食谱中的特征提取器与一个非常常见的基于字符 n-gram 的特征提取器相结合。
- en: 'We will start with a `main()` method in `src/com/lingpipe/cookbook/chapter3/CombinedFeatureExtractor.java`
    that we will use to run the feature extractor. The following lines set up features
    that result from the tokenizer using the LingPipe class, `TokenFeatureExtractor`:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在 `src/com/lingpipe/cookbook/chapter3/CombinedFeatureExtractor.java` 中的 `main()`
    方法开始，我们将使用它来运行特征提取器。以下行设置了使用 LingPipe 类 `TokenFeatureExtractor` 通过分词器生成的特征：
- en: '[PRE93]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: Then, we will construct the feature extractor from the previous recipe.
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将从前一个食谱中构建特征提取器。
- en: '[PRE94]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Next, the LingPipe class joining feature extractors, `AddFeatureExtractor`,
    joins the two into a third:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，LingPipe 类 `AddFeatureExtractor` 将两个特征提取器合并为一个第三个：
- en: '[PRE95]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'The remaining code gets the features and prints them out:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剩余的代码获取特征并打印出来：
- en: '[PRE96]'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: Run the following command
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令
- en: '[PRE97]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'The output looks like this:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出看起来像这样：
- en: '[PRE98]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: There's more…
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The Javadoc references a broad range of feature extractors and combiners/filters
    to help manage the task of feature extraction. One slightly confusing aspect of
    the class is that the `FeatureExtractor` interface is in the `com.aliasi.util`
    package, and the implementing classes are all in `com.aliasi.features`.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: Javadoc 引用了广泛的功能提取器和组合器/过滤器，以帮助管理特征提取的任务。这个类的一个稍微令人困惑的方面是，`FeatureExtractor`
    接口位于 `com.aliasi.util` 包中，而实现类都在 `com.aliasi.features`。
- en: Classifier-building life cycle
  id: totrans-360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类器构建生命周期
- en: 'At the top-level building, a classifier usually proceeds as follows:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶级构建中，分类器通常按以下方式进行：
- en: Create training data—refer to the following recipe for more about this.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建训练数据——有关更多信息，请参阅以下食谱。
- en: Build training and evaluation infrastructure with sanity check.
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用健全性检查构建训练和评估基础设施。
- en: Establish baseline performance.
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立基线性能。
- en: Select optimization metric for classifier—this is what the classifier is trying
    to do and will guide tuning.
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择分类器的优化指标——这是分类器试图完成的事情，并将指导调整。
- en: 'Optimize classifier via techniques such as:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过以下技术优化分类器：
- en: Parameter tuning
  id: totrans-367
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数调整
- en: Thresholding
  id: totrans-368
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阈值
- en: Linguistic tuning
  id: totrans-369
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言学调整
- en: Adding training data
  id: totrans-370
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加训练数据
- en: Refining classifier definition
  id: totrans-371
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精炼分类器定义
- en: This recipe will present the first four steps in concrete terms, and there are
    recipes in this chapter for the optimization step.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱将以具体术语展示前四个步骤，本章中还有针对优化步骤的食谱。
- en: Getting ready
  id: totrans-373
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Nothing happens without training data for classifiers. Look at the *Annotation*
    recipe at the end of the chapter for tips on creating training data. You can also
    use an active learning framework to incrementally generate a training corpus (covered
    later in this chapter), which is the data used in this recipe.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 没有训练数据，分类器不会发生任何作用。查看本章末尾的 *Annotation* 食谱，以获取创建训练数据的技巧。您还可以使用主动学习框架逐步生成训练语料库（本章后面将介绍），这是本食谱中使用的数据。
- en: Next, reduce the risk by starting with the dumbest possible implementation to
    make sure that the problem being solved is scoped correctly, and that the overall
    architecture makes sense. Connect the assumed inputs to assumed outputs with simple
    code. We promise that most of the time, one or the other will not be what you
    thought it would be.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过从最愚蠢的可能实现开始来降低风险，以确保所解决的问题范围正确，整体架构合理。用简单的代码将假设的输入连接到假设的输出。我们保证，大多数情况下，其中一个或另一个将不是你所想的。
- en: This recipe assumes that you are familiar with the evaluation concepts in [Chapter
    1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"), *Simple Classifiers*,
    such as cross validation and confusion matrices, in addition to the logistic regression
    recipes covered so far.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱假设您熟悉 [第 1 章](part0014_split_000.html#page "第 1 章。简单分类器") 中介绍的评价概念，如交叉验证和混淆矩阵，以及迄今为止涵盖的逻辑回归食谱。
- en: The entire source is at `src/com/lingpipe/cookbook/chapter3/ClassifierBuilder.java`.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 整个源代码位于 `src/com/lingpipe/cookbook/chapter3/ClassifierBuilder.java`。
- en: This recipe also assumes that you can compile and run the code within your preferred
    development environment. The result of all the changes we are making is in `src/com/lingpipe/cookbook/chapter3/ClassifierBuilderFinal.java`.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱还假设你可以在你首选的开发环境中编译和运行代码。我们做出的所有更改的结果在`src/com/lingpipe/cookbook/chapter3/ClassifierBuilderFinal.java`中。
- en: Note
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Big caveat in this recipe—we are using a tiny dataset to make basic points on
    classifier building. The sentiment classifier we are trying to build would benefit
    from 10 times more data.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中有一个大前提——我们使用一个非常小的数据集来在分类器构建上阐述基本观点。我们试图构建的情感分类器将从10倍更多的数据中受益。
- en: How to do it…
  id: totrans-381
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'We start with a collection of tweets that have been deduplicated and are the
    result of the *Train a little, learn a little – active learning* recipe that will
    follow this recipe. The starting point of the recipe is the following code:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一组去重后的推文开始，这些推文是接下来这个食谱中将要遵循的“训练一点，学习一点——主动学习”食谱的结果。食谱的起点是以下代码：
- en: '[PRE99]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: Sanity check – test on training data
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理性检查 – 在训练数据上测试
- en: 'The first thing to do is get the system running and test on training data:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的事情是让系统运行起来，并在训练数据上测试：
- en: 'We have left a print statement that advertises what is going on:'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们留下了一条打印语句，宣传正在发生的事情：
- en: '[PRE100]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Running `ClassifierBuilder` will yield the following:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`ClassifierBuilder`将产生以下结果：
- en: '[PRE101]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: The preceding confusion matrix is nearly a perfect system output, which validates
    that the system is basically working. This is the best system output you will
    ever see; never let management see it, or they will think this level of performance
    is either doable or done.
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的混淆矩阵几乎是一个完美的系统输出，这验证了系统基本上是正常工作的。这是你将看到的最好的系统输出；永远不要让管理层看到它，否则他们会认为这种性能水平是可以做到的或者已经做到了。
- en: Establishing a baseline with cross validation and metrics
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用交叉验证和指标建立基线
- en: Now it is time to see what is really going on.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候看看真正发生的事情了。
- en: 'If you have small data, then set the number of folds to `10` so that 90 percent
    of the data is used for training. If you have large data or are in a huge rush,
    then set it to `2`:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你数据量小，那么将折数设置为`10`，这样就有90%的数据用于训练。如果你数据量大或者时间紧迫，那么将其设置为`2`：
- en: '[PRE102]'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Comment out or remove the training on test code:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取消注释或删除测试数据上的训练代码：
- en: '[PRE103]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Plumb in a cross-validation loop or just uncomment the loop in our source:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 插入一个交叉验证循环或者只是取消注释我们源代码中的循环：
- en: '[PRE104]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Recompiling and running the code will give us the following output:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新编译并运行代码将给出以下输出：
- en: '[PRE105]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'The classifier labels mean `p=positiveSentiment`, `n=negativeSentiment`, and
    `o=other`, which covered other languages or neutral sentiment. The first row of
    the confusion matrix indicates that the system gets `45` true positives, `8` false
    negatives that it thinks are `n`, and `17` false negatives that it thinks are
    `o`:'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分类器的标签意味着`p=positiveSentiment`，`n=negativeSentiment`，和`o=other`，这涵盖了其他语言或中性情感。混淆矩阵的第一行表明，系统得到了`45`个真正的正例，`8`个它认为是`n`的假阴性，以及`17`个它认为是`o`的假阴性：
- en: '[PRE106]'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'To get the false positives for `p`, we need to look at the first column. We
    see that the system thought that `16` `n` annotations were `p` and `18` `o` annotations
    were `p`:'
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要获取`p`的假阳性，我们需要查看第一列。我们看到系统认为`16`个`n`注释是`p`，`18`个`o`注释是`p`：
- en: '[PRE107]'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: Tip
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: The confusion matrix is the most honest and straightforward way to view/present
    results for classifiers. Performance metrics such as precision, recall, F-measure,
    and accuracy are all very slippery and often used incorrectly. When presenting
    results, always have a confusion matrix handy, because if we are in the audience
    or someone like us is, we will ask to see it.
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 混淆矩阵是查看/展示分类器结果最诚实和直接的方式。性能指标如精确度、召回率、F度量、准确度都非常难以捉摸，并且经常被错误地使用。在展示结果时，始终准备好混淆矩阵，因为如果我们是听众或者类似的人，我们都会要求看到它。
- en: Perform the same analysis for the other categories, and you will have an assessment
    of system performance.
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对其他类别执行相同的分析，你将有一个系统性能的评估。
- en: Picking a single metric to optimize against
  id: totrans-408
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择一个单一指标进行优化
- en: 'Perform the following steps:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'While the confusion matrix establishes the overall performance of the classifier,
    it is too complex to use as a tuning guide. You don''t want to have to digest
    the entire matrix every time you adjust a feature. You and your team must agree
    on a single number that, if it goes up, the system is considered better. The following
    metrics apply to binary classifiers; if there are more than two categories, then
    you will have to sum them somehow. Some common metrics we see are:'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然混淆矩阵建立了分类器的整体性能，但它太复杂，不能用作调整指南。你不想每次调整一个特征时都要消化整个矩阵。你和你的团队必须同意一个单一数字，如果它上升，系统就被认为是更好的。以下指标适用于二元分类器；如果有超过两个类别，那么你将不得不以某种方式将它们相加。我们看到的常见指标包括：
- en: '**F-measure**: F-measure is an attempt to reward reductions in false negatives
    and false positives at the same time:'
  id: totrans-411
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F-measure**：F-measure是尝试同时奖励减少误负和误正：'
- en: '*F-measure = 2*TP / (2*TP + FP + FN)*'
  id: totrans-412
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*F-measure = 2*TP / (2*TP + FP + FN)*'
- en: It is mainly an academic measure to declare that one system is better than another.
    It sees little use in industry.
  id: totrans-413
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这主要是一个学术指标，用来声明一个系统比另一个系统更好。在工业界几乎很少使用。
- en: '**Recall at 90 percent precision**: The goal is to provide as much coverage
    as possible while not making more than 10 percent false positives. This is when
    the system does not want to look bad very often; this applies to spell checkers,
    question answering systems, and sentiment dashboards.'
  id: totrans-414
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在90%精确度下的召回率**：目标是尽可能提供覆盖范围，同时不超过10%的误报。这是系统不希望频繁出错的时刻；这适用于拼写检查器、问答系统和情感仪表板。'
- en: '**Precision at 99.9 percent recall**: This metric supports *needle in the haystack*
    or *needle in the needle stack* kind of problems. The user cannot afford to miss
    anything and is willing to perhaps slog through lots of false positives as long
    as they don''t miss anything. The system is better if the false positive rate
    is lower. Use cases are intelligence analysts and medical researchers.'
  id: totrans-415
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在99.9%召回率下的精确度**：这个指标支持“大海捞针”或“针尖上的针堆”这类问题。用户不能容忍错过任何信息，并且愿意忍受大量的误报，只要不错过任何信息。如果误报率较低，系统会更好。用例包括情报分析师和医学研究人员。'
- en: Determining this metric comes from a mixture of business/research needs, technical
    capability, available resources, and willpower. If a customer wants a high recall
    and high-precision system, our first question will be to ask what the budget is
    per document. If it is high enough, we will suggest hiring experts to correct
    system output, which is the best combination of what computers are good at (exhaustiveness)
    and what humans are good at (discrimination). Generally, budgets don't support
    this, so the balancing act begins, but we have deployed systems in just this way.
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定这个指标来源于业务/研究需求、技术能力、可用资源和意志力的混合。如果客户想要高召回率和高精确度的系统，我们首先会问每个文档的预算是多少。如果预算足够高，我们会建议雇佣专家来纠正系统输出，这是计算机擅长（全面性）和人类擅长（辨别力）的最佳结合。通常，预算不支持这一点，因此平衡行为开始了，但我们确实以这种方式部署了系统。
- en: For this recipe, we will pick a maximizing recall at 50-percent precision for
    `n` (negative), because we want to be sure to intercept any negative sentiment
    and will tolerate false positives. We will choose 65 percent for a `p` positive,
    because the good news is less actionable, and who doesn't love Disney? We don't
    care what `o` (other performance) is—the category exists for linguistic reasons,
    independent of the business use. This metric a likely metric for a sentiment-dashboard
    application. This means that the system will produce one mistake for every two
    guesses of a negative-sentiment category and 13 out of 20 for positive sentiment.
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于这个配方，我们将选择在50%精确度下最大化召回率，对于`n`（负面），因为我们想确保拦截任何负面情绪，并且可以容忍误报。我们将选择65%的`p`（正面），因为好消息的可操作性较低，而且谁不喜欢迪士尼呢？我们不在乎`o`（其他性能）是什么——这个类别存在是出于语言原因，与业务用途无关。这是一个可能的情感仪表板应用指标。这意味着系统将为每个负面情绪类别的两个猜测犯一个错误，以及为正面情绪的20个猜测犯13个错误。
- en: Implementing the evaluation metric
  id: totrans-418
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现评估指标
- en: 'Perform the following steps to implement the evaluation metric:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以实现评估指标：
- en: 'We will start with reporting precision/recall for all categories with the `Util.printPrecRecall`
    method after printing out the confusion matrix:'
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在打印出混淆矩阵后，我们将使用`Util.printPrecRecall`方法报告所有类别的精确度/召回率：
- en: '[PRE108]'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'The output will now look like this:'
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出现在将看起来像这样：
- en: '[PRE109]'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: The precision for `n` exceeds our objective of `.5`–since we want to maximize
    recall at `.5`, we can make a few more mistakes before we get to the limit. You
    can refer to the *Thresholding classifiers* recipe to find out how to do this.
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`n`的精度超过了我们的目标`.5`——因为我们希望在`.5`处最大化召回率，我们可以在达到极限之前犯一些错误。你可以参考*阈值分类器*配方来了解如何做到这一点。'
- en: 'The precision for `p` is 57 percent, and this is too low for our business objective.
    Logistic regression classifiers, however, provide a conditional probability that
    might allow us to meet the precision needs just by paying attention to the probability.
    Adding the following line of code will allow us to see the results sorted by conditional
    probability:'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`p`的精度为57%，这对于我们的业务目标来说太低了。然而，逻辑回归分类器提供了一种条件概率，这可能允许我们仅通过关注概率来满足精度需求。添加以下代码行将允许我们按条件概率排序查看结果：'
- en: '[PRE110]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'The preceding line of code starts by getting a `ScoredPrecisionRecallEvaluation`
    value from the evaluator. A double-scored curve (`[][])` is gotten from that object
    with the Boolean interpolate set to false, because we want the curve to be unadulterated.
    You can look at the Javadoc for what is going on. Then, we will use a print route
    from the same class to print out the curve. The output will look like this:'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上一行代码首先从评估器中获取一个`ScoredPrecisionRecallEvaluation`值。从这个对象中获取一个双评分曲线（`[][])`，布尔插值设置为false，因为我们希望曲线保持纯净。你可以查看Javadoc了解具体发生了什么。然后，我们将使用同一类中的打印路由来打印出曲线。输出将看起来像这样：
- en: '[PRE111]'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: The output is sorted by score, in the third column, which in this case, happens
    to be a conditional probability, so the max value is 1 and min value is 0\. Notice
    that the recall grows as correct cases are found (the second line), and it never
    goes down. However, when a mistake is made like in the fourth line, precision
    drops to `.6`, because 3 out of 5 cases are correct so far. The precision actually
    goes below `.65` before the last value is found—in bold, with a score of `.73`.
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出按分数排序，在第三列，在这种情况下，恰好是条件概率，所以最大值是1，最小值是0。注意，随着正确案例的发现（第二行），召回率会增长，并且它永远不会下降。然而，当出现错误，如第四行时，精度会下降到`.6`，因为到目前为止有3个案例是正确的。实际上，在找到最后一个值之前，精度实际上已经低于`.65`——在粗体中，分数为`.73`。
- en: 'So, without any tuning, we can report that we can achieve 30 percent recall
    for `p` at our accepted precision limit of 65 percent. This requires that we threshold
    the classifier at `.73` for the category, which means if we reject scores less
    than `.73` for `p`, some comments are:'
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，没有任何调整，我们可以报告，在65%的接受精度极限下，我们可以实现30%的召回率`p`。这要求我们将分类器在该类别上的阈值设置为`.73`，这意味着如果我们拒绝小于`.73`的`p`分数，一些评论是：
- en: We got lucky. Usually, the first classifier runs do not reveal an immediately
    useful threshold with default values.
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们很幸运。通常情况下，第一次分类器的运行不会立即揭示一个有用的阈值，默认值通常是不够的。
- en: Logistic regression classifiers have a very nice property that they provide;
    they also provide conditional probability estimates for thresholding. Not all
    classifiers have this property—language models and naïve Bayes classifiers tend
    to push scores towards 0 or 1, making thresholding difficult.
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归分类器有一个非常好的特性，它们提供；它们还提供用于阈值化的条件概率估计。并非所有分类器都有这个特性——语言模型和朴素贝叶斯分类器倾向于将分数推向0或1，这使得阈值化变得困难。
- en: As the training data is highly biased (this is from the *Train a little, learn
    a little – active learning* recipe that follows), we cannot trust this threshold.
    The classifier will have to be pointed at fresh data to set the threshold. Refer
    to the *Thresholding classifiers* recipe to see how this is done.
  id: totrans-433
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于训练数据高度偏差（这是来自后续的*训练一点，学习一点——主动学习*配方），我们无法信任这个阈值。分类器必须指向新鲜数据来设置阈值。请参考*阈值分类器*配方了解如何进行。
- en: This classifier has seen very little data and will not be a good candidate for
    deployment despite the supporting evaluation. We would be more comfortable with
    at least 1,000 tweets from a diverse set of dates.
  id: totrans-434
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个分类器看到的数据非常少，尽管有支持性的评估，但不会是一个好的部署候选。我们更愿意至少有来自不同日期的1,000条推文。
- en: 'At this point in the process, we either accept the results by verifying that
    the performance is acceptable on fresh data or turn to improving the classifier
    by techniques covered by other recipes in this chapter. The final step of the
    recipe is to train up the classifier on all training data and write to disk:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: We will use the resulting model in the *Thresholding classifiers* recipe.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: Linguistic tuning
  id: totrans-438
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will address issues around tuning the classifier by paying attention
    to the mistakes made by the system and making linguistic adjustments by adjusting
    parameters and features. We will continue with the sentiment use case from the
    previous recipe and work with the same data. We will start with a fresh class
    at `src/com/lingpipe/cookbook/chapter3/LinguisticTuning.java`.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: We have very little data. In the real world, we will insist on more training
    data—at least 100 of the smallest category, negative, are needed with a natural
    distribution of positives and others.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-441
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will jump right in and run some data—the default is `data/activeLearningCompleted/disneySentimentDedupe.2.csv`,
    but you can specify your own file in the command line.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following in your command line or IDE equivalent:'
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'For each fold, the features for the classifier will be printed. The output
    will look like the following for each category (just the first few features for
    each):'
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: Starting with the `n` category, note that there are no features. It is a property
    of logistic regression that one category's features are all set to `0.0`, and
    the remaining `n-1` category's features are offset accordingly. This cannot be
    controlled, which is a bit annoying because the `n` or negative category can be
    the focus of linguistic tuning given how badly it performs in the example. Not
    to be deterred, we will move on.
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that the output is intended to make it easy to use a `find` command to
    locate feature output in the extensive reporting output. To find a feature search
    on `category <feature name>` to see if there is a nonzeroed report, search on
    `category <feature name> NON_ZERO`.
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We are looking for a few things in these features. First of all, there are
    apparently odd features that are getting big scores—the output is ranked in positive
    to negative for the category. What we want to look for is some signal in the feature
    weights—so `love` makes sense as being associated with a positive sentiment. Looking
    at features like this can really be surprising and counter intuitive. The uppercase
    `I` and lowercase `i` suggest that the text should be downcased. We will make
    this change and see if it helps. Our current performance is:'
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'The code change is to add a `LowerCaseTokenizerFactory` item to the current
    `IndoEuropeanTokenizerFactory` class:'
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'Run the code, and we will pick up some precision and recall:'
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'The features are as follows:'
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'What''s the next move? The `minFeature` count is very low at `1`. Let''s raise
    it to `2` and see what happens:'
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: This hurts performance by a few cases, so we will return to `1`. However, experience
    dictates that the minimum count goes up as more data is found to prevent overfitting.
  id: totrans-459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这导致性能下降几个案例，因此我们将返回到`1`。然而，经验表明，随着更多数据的发现，最小计数会上升以防止过拟合。
- en: 'It is time for the secret sauce—change the tokenizer to `NGramTokenizer`; it
    tends to work better than standard tokenizers—we are now rolling with the following
    code:'
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是时候加入秘密调料了——将分词器更改为`NGramTokenizer`；它通常比标准分词器效果更好——我们现在正在使用以下代码：
- en: '[PRE120]'
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'This worked. We will pick up a few more cases:'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这有效了。我们将挑选一些更多的情况：
- en: '[PRE121]'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'However, the features are now pretty hard to scan:'
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然而，现在的特征非常难以扫描：
- en: '[PRE122]'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: We have found over the course of time that character ngrams are the features
    of choice for text-classifier problems. They seem to nearly always help, and they
    helped here. Look at the features, and you can recover that `love` is still contributing
    but in little bits, such as `lov`, `ov`, and `lo`.
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们发现，随着时间的推移，字符n-gram是文本分类问题中首选的特征。它们似乎几乎总是有帮助，而且在这里也很有帮助。看看特征，你可以恢复出`love`仍然在以小块的方式贡献，例如`lov`、`ov`和`lo`。
- en: There is another approach that deserves a mention, which is some of the tokens
    produced by `IndoEuropeanTokenizerFactory` are most likely useless, and they are
    just confusing the issue. Using a stop-word list, focusing on more useful tokenization,
    and perhaps applying a stemmer such as the Porter stemmer might work as well.
    This has been the traditional approach to these kinds of problems—we have never
    had that much luck with them.
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一种值得注意的方法是，`IndoEuropeanTokenizerFactory`生成的某些标记很可能无用，它们只是混淆了问题。使用停用词表，关注更有用的分词，并可能应用像Porter词干提取器这样的词干提取器可能也会有效。这是处理这类问题的传统方法——我们从未在这方面取得过太多成功。
- en: 'It is a good time to check on the performance of the `n` category; we have
    been messing about with the model and should check it:'
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是检查`n`类别性能的好时机；我们一直在调整模型，应该检查它：
- en: '[PRE123]'
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: 'The output also reports false positives for `p` and `n`. We really don''t care
    much about `o`, except when it shows up as a false positive for the other categories:'
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出还报告了`p`和`n`的误报。我们真的不太关心`o`，除非它作为其他类别的误报出现：
- en: '[PRE124]'
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE124]'
- en: Looking at false positives, we can suggest changes to feature extraction. Recognizing
    quotes from `~Walt Disney` might help the classifier with `IS_DISNEY_QUOTE`.
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看误报情况，我们可以建议对特征提取进行修改。识别来自`~华特·迪士尼`的引语可能有助于分类器使用`IS_DISNEY_QUOTE`。
- en: 'Also, looking at errors can point out errors in annotation, one can argue that
    the following is actually positive:'
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，查看错误可以指出注释中的错误，有人可能会认为以下内容实际上是正面的：
- en: '[PRE125]'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'At this point, the system is somewhat tuned. The configuration should be saved
    someplace and the next steps are considered. They include the following:'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 到目前为止，系统已经进行了某种程度的调整。应将配置保存在某个地方，并考虑下一步。它们包括以下内容：
- en: Declare victory and deploy. Before deploying, be sure to test on novel data
    using all training data to train. The *Thresholding classifiers* recipe will be
    very useful.
  id: totrans-476
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 宣布胜利并部署。在部署之前，务必使用所有训练数据在新型数据上进行测试。*阈值分类器*食谱将非常有用。
- en: Annotate more data. Use the active learning framework in the following recipe
    to help identify high-confidence cases that are wrong and right. This will likely
    help more than anything with performance, especially with low-count data such
    as the kind we have been working with.
  id: totrans-477
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标注更多数据。使用以下食谱中的主动学习框架来帮助识别错误和正确的高置信度案例。这可能会比其他任何方法更有助于性能，特别是对于低计数数据，如我们一直在处理的数据。
- en: Looking at the epoch report, the system is never converging on its own. Increase
    the limit to 10,000 and see if this helps things.
  id: totrans-478
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看纪元报告，系统从未自行收敛。将限制提高到10,000并看看这是否有帮助。
- en: 'The result of our tuning efforts was to improve the performance from:'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们调整努力的成果是提高性能，从：
- en: '[PRE126]'
  id: totrans-480
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: 'To the following:'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 到以下：
- en: '[PRE127]'
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: This is not a bad uptick in performance in exchange for looking at some data
    and thinking a bit about how to help the classifier do its job.
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这不是在查看一些数据和思考如何帮助分类器完成其工作的情况下，以换取性能提升的坏交易。
- en: Thresholding classifiers
  id: totrans-484
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 阈值分类器
- en: Logistic regression classifiers are often deployed with a threshold rather than
    the provided `classifier.bestCategory()` method. This method picks the category
    with the highest conditional probability, which, in a 3-way classifier, can be
    just above one-third. This recipe will show you how to adjust classifier performance
    by explicitly controlling how the best category is determined.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归分类器通常使用阈值而不是提供的`classifier.bestCategory()`方法进行部署。该方法选择具有最高条件概率的类别，在一个三分类器中，这个概率可能略高于三分之一。本食谱将向您展示如何通过显式控制最佳类别的确定方式来调整分类器性能。
- en: 'This recipe will consider the 3-way case with the `p`, `n`, and `o` labels
    and work with the classifier produced by the *Classifier-building life cycle*
    recipe earlier in this chapter. The cross-validation evaluation produced is:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱将考虑具有`p`、`n`和`o`标签的三分类情况，并使用本章前面提到的*分类器构建生命周期*食谱中产生的分类器。产生的交叉验证评估结果如下：
- en: '[PRE128]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: We will run novel data to set thresholds.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行新的数据来设置阈值。
- en: How to do it...
  id: totrans-489
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: Our business use case is that recall be maximized while `p` has `.65` precision
    and `n` has `.5` precision for reasons discussed in the *Classifier-building life
    cycle* recipe. The `o` category is not important in this case. The `p` category
    appears to be too low with `.57`, and the `n` category can increase recall as
    the precision is above `.5`.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的业务用例是，在**分类器构建生命周期**中讨论的原因下，`p`具有`.65`的精确度，`n`具有`.5`的精确度，同时最大化召回率。在这种情况下，`o`类别并不重要。`p`类别似乎精确度太低，为`.57`，而`n`类别可以在精确度高于`.5`的情况下提高召回率。
- en: We cannot use the cross-validation results unless care has been taken to produce
    a proper distribution of annotations—the active learning approach used tends to
    not produce such distributions. Even with a good distribution, the fact that the
    classifier was likely tuned with cross validation means that it is most likely
    overfit to that dataset because tuning decisions were made to maximize performance
    of those sets that are not general to new data.
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不能使用交叉验证结果，除非已经注意到了产生适当的注释分布——所使用的主动学习方法往往不会产生这样的分布。即使有良好的分布，由于分类器很可能经过交叉验证进行调优，因此它很可能过度拟合到该数据集，因为调优决策是为了最大化那些不适用于新数据的集合并不是一般性的性能。
- en: We need to point the trained classifier at new data—the rule of thumb is to
    train by hook or crook but always threshold on fresh. We followed the *Getting
    data from the Twitter API* recipe in [Chapter 1](part0014_split_000.html#page
    "Chapter 1. Simple Classifiers"), *Simple Classifiers*, and downloaded new data
    from Twitter with the `disney` query. Nearly a year has passed since our initial
    search, so the tweets are most likely non-overlapping. The resulting 1,500 tweets
    were put into `data/freshDisney.csv`.
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要将训练好的分类器指向新的数据——一般来说，无论通过什么方式都要进行训练，但始终在新鲜数据上设置阈值。我们遵循了[第1章](part0014_split_000.html#page
    "第1章。简单分类器")中“从Twitter API获取数据”的食谱，*简单分类器*，并使用`disney`查询从Twitter下载了新的数据。自从我们最初的搜索以来，已经过去将近一年，因此推文很可能不会重叠。结果得到的1,500条推文被放入`data/freshDisney.csv`。
- en: Ensure that you don't run this code on data that is not backed up. The I/O is
    simple rather than robust. The code overwrites the input file.
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保不要在未备份的数据上运行此代码。I/O相对简单而不是健壮。代码会覆盖输入文件。
- en: 'Invoke `RunClassifier` on your IDE or run the following command:'
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的IDE中调用`RunClassifier`或运行以下命令：
- en: '[PRE129]'
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: Open the `.csv` file in your favorite spreadsheet. All tweets should have a
    score and a guessed category in the standard annotation format.
  id: totrans-496
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的最喜欢的电子表格中打开`.csv`文件。所有推文都应该有一个分数和一个猜测的类别，格式为标准注释格式。
- en: Sort with the primary sort on the `GUESS` column in the ascending or descending
    order and then sort on `SCORE` in the descending order. The result should be each
    category with higher scores descending to lower scores. This is how we set up
    top-down annotations.![How to do it...](img/00004.jpeg)
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照升序或降序对`GUESS`列进行主要排序，然后按`SCORE`降序排序。结果应该是每个类别的分数从高到低。这是我们设置自上而下注释的方式。![如何做到这一点...](img/00004.jpeg)
- en: Setting up sort of data for top-down annotation. All categories are grouped
    together, and a descending sort of the score is established.
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置用于自上而下注释的数据排序。所有类别都分组在一起，并建立分数的降序排序。
- en: For the categories that you care about, in this case, `p` and `n`, annotate
    truth from the highest score to the lowest scores until it is likely that the
    precision goal has been broached. For example, annotate `n` until you either run
    out of `n` guesses, or you have enough mistakes that you have `.50` precision.
    A mistake is when the truth is `o` or `p`. Do the same for `p` until you have
    a precision of `.65`, or you run out of number of `p`. For our canned example,
    we have put the annotations in `data/freshDisneyAnnotated.csv`.
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于你关心的类别，在这个例子中是`p`和`n`，从最高得分标注到最低得分，直到很可能已经达到了精度目标。例如，标注`n`直到你用完`n`的猜测，或者你有足够的错误使得精度达到`.50`。错误是指真实值为`o`或`p`。对于`p`也做同样的操作，直到你达到`.65`的精度，或者用完`p`的数量。在我们的示例中，我们将标注放在了`data/freshDisneyAnnotated.csv`。
- en: 'Run the following command or the equivalent in your IDE (note that we are supplying
    the input file and not using the default):'
  id: totrans-500
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令或你IDE中的等效命令（注意，我们提供了输入文件，而没有使用默认设置）：
- en: '[PRE130]'
  id: totrans-501
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: 'This command will produce the following output:'
  id: totrans-502
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个命令将产生以下输出：
- en: '[PRE131]'
  id: totrans-503
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: 'First off, this is a surprisingly good system performance for our minimally
    trained classifier. `p` is very close to the target precision of `.65` without
    thresholding, and coverage is not bad: it is found as 141 true positives out of
    1,500 tweets. As we have not annotated all 1,500 tweets, we cannot truly say what
    the recall of the classifier is, so the term is overloaded in common use. The
    `n` category is not doing as well, but it is still pretty good. Our annotation
    did no annotations for the `o` category, so the system column is all zeros.'
  id: totrans-504
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，这是一个令人惊讶的好系统性能，对于我们的最少训练分类器来说。`p`非常接近没有阈值时的目标精度`.65`，而且覆盖率也不错：在1500条推文中找到了141个真实正例。由于我们没有标注所有1500条推文，我们无法真正地说出分类器的召回率是多少，所以这个术语在常见用法中被过度使用。`n`类别表现不佳，但仍然相当不错。我们的标注没有对`o`类别进行标注，所以系统列都是零。
- en: 'Next, we will look at the precision/recall/score curve for thresholding guidance:'
  id: totrans-505
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将查看用于阈值指导的精度/召回/得分曲线：
- en: '[PRE132]'
  id: totrans-506
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: 'Most values have been elided to save space in the preceding output. We saw
    that the point at which the classifier passes `.65` precision has a score of `.525`.
    This means that we can expect 65-percent precision if we threshold at `.525` with
    a bunch of caveats:'
  id: totrans-507
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了节省空间，前面的输出中省略了大多数值。我们注意到，分类器达到`.65`精度的点得分为`.525`。这意味着，如果我们以`.525`为阈值，我们可以期望达到65%的精度，但有一些注意事项：
- en: This is a single-point sample without a confidence estimate. There are more
    sophisticated ways to arrive at a threshold that is beyond the scope of this recipe.
  id: totrans-508
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个没有置信度估计的单点样本。有更复杂的方法来确定阈值，但这超出了本食谱的范围。
- en: Time is a big contributor to variance in performance.
  id: totrans-509
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间是性能方差的一个大因素。
- en: 10-percent variance in performance for well-developed classifiers is not uncommon
    in practice. Factor this into performance requirements.
  id: totrans-510
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实践中，对于开发良好的分类器，性能的10%方差并不罕见。将这一点纳入性能要求中。
- en: The nice thing about the preceding curve is that it looks like we can provide
    a `.80` precision classifier at a threshold of `.76` with nearly 30 percent of
    the coverage of the `.65` precision classifier if we decide that higher precision
    is called for.
  id: totrans-511
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的曲线的优点在于，如果我们决定需要更高的精度，那么在`.76`的阈值下，我们可以提供一个`.80`精度的分类器，并且几乎覆盖了`.65`精度分类器的30%。
- en: 'The `n` case has a curve that looks like this:'
  id: totrans-512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`n`的情况有一个看起来像这样的曲线：'
- en: '[PRE133]'
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: It looks like a threshold of `.549` gets the job done. The rest of the recipe
    will show how you to set up the thresholded classifier now that we have the thresholds.
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看起来阈值`.549`就能完成任务。接下来的食谱将展示如何设置阈值分类器，现在我们已经有了阈值。
- en: The code behind `RunClassifier.java` offers nothing of novelty in the context
    of this chapter, so it is left to you to work through.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '`RunClassifier.java`背后的代码在本章的上下文中没有提供任何新颖之处，所以留给你去实现。'
- en: How it works…
  id: totrans-516
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The goal is to create a classifier that will assign `p` to a tweet if it scores
    above `.525` for that category and `n` if scores above `.549` for that category;
    otherwise, it gets `o`. Wrong….management saw the p/r curve and now insists that
    `p` must be 80-percent precise, which means that the threshold will be `.76`.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是创建一个分类器，如果某个推文的该类别得分为`.525`以上，则将其分配为`p`；如果得分为`.549`以上，则分配为`n`；否则，分配为`o`。错误……管理层看到了p/r曲线，现在坚持认为`p`必须达到80%的精度，这意味着阈值将是`.76`。
- en: The solution is very simple. If a score for `p` is below `.76`, then it will
    be rescored down to `0.0`. Likewise, if a score for `n` is below `.54`, then it
    will be rescored down to `0.0`. The effect of this is that `o` will be the best
    category for all below-threshold cases, because `.75` `p` can at best be `.25`
    `n`, which remains below the `n` threshold, and `.53` `n` can at most be `.47`
    `p`, which is below that category's threshold. This can get complicated if all
    categories are thresholded, or the thresholds are low.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案非常简单。如果 `p` 的分数低于 `.76`，则将其重新评分到 `0.0`。同样，如果 `n` 的分数低于 `.54`，则将其重新评分到 `0.0`。这种效果是，对于所有低于阈值的案例，`o`
    将是最佳类别，因为 `.75` 的 `p` 最多是 `.25` 的 `n`，这仍然低于 `n` 的阈值，而 `.53` 的 `n` 最多是 `.47` 的
    `p`，这低于该类别的阈值。如果所有类别都设置了阈值，或者阈值很低，这可能会变得复杂。
- en: Stepping back, we are taking a conditional classifier where all the category
    scores must sum to 1 and breaking this contract, because we will take any estimate
    for `p` that is below `.76` and bust it down to `0.0`. It is a similar story for
    `n`. The resulting classifier will now have to be `ScoredClassifier` because this
    is the next most specific contract in the LingPipe API that we can uphold.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，我们正在处理一个条件分类器，其中所有类别的分数必须加起来为 1，并打破这个合同，因为我们将对任何低于 `.76` 的 `p` 估计进行下推到
    `0.0`。对于 `n` 也是如此。结果分类器现在将必须是 `ScoredClassifier`，因为这是 LingPipe API 中我们可以维持的下一个最具体的合同。
- en: 'The code for this class is in `src/com/lingpipe/cookbook/chapter3/ThresholdedClassifier`.
    At the top level, we have the class, relevant member variable, and constructor:'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类的代码位于 `src/com/lingpipe/cookbook/chapter3/ThresholdedClassifier`。在顶层，我们有类、相关成员变量和构造函数：
- en: '[PRE134]'
  id: totrans-521
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: 'Next, we will implement the only required method for `ScoredClassification`,
    and this is where the magic happens:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现 `ScoredClassification` 所需的唯一方法，这就是魔法发生的地方：
- en: '[PRE135]'
  id: totrans-523
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: The complicated bit about scored classifications is that scores have to be assigned
    to all categories even if the score is `0.0`. The mapping from a conditional classification,
    where all scores sum to `1.0`, does not lend itself to a generic solution, which
    is why the preceding ad hoc implementation is used.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 关于评分分类的复杂之处在于，即使分数是 `0.0`，也必须将分数分配给所有类别。从条件分类（所有分数加起来为 `1.0`）到通用解决方案的映射并不适用，这就是为什么使用先前的临时实现的原因。
- en: 'There is also a `main()` method that spools up the relevant bits for `ThresholdedClassifier`
    and applies them:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一个 `main()` 方法，它会加载 `ThresholdedClassifier` 相关的片段并应用它们：
- en: '[PRE136]'
  id: totrans-526
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: The thresholds are doing exactly as designed; `p` is `.79` precision, which
    is close enough for consulting, and `n` is spot on. The source for the `main()`
    method should be straightforward given the context of this chapter.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 阈值正在按设计执行；`p` 是 `.79` 精度，对于咨询来说足够接近，而 `n` 则非常准确。考虑到本章的上下文，`main()` 方法的来源应该是直截了当的。
- en: That's it. Almost never do we release a nonthresholded classifier, and best
    practices require that thresholds be set on held-out data, preferably from later
    epochs than the training data. Logistic regression is quite robust against skewed
    training data, but the ointment that cleanses the flaws of skewed data is novel
    data annotated top down to precision objectives. Yes, it is possible to threshold
    with cross validation, but it suffers from the flaws that overfit due to tuning,
    and you would screw up your distributions. Recall-oriented objectives are another
    matter.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。我们几乎从不发布非阈值分类器，最佳实践要求在保留数据上设置阈值，最好是训练数据之后的时期。逻辑回归对倾斜的训练数据相当稳健，但清除倾斜数据缺陷的药膏是新颖的数据，这些数据从上到下精确标注到目标。是的，可以使用交叉验证进行阈值设置，但它存在由于调整而过度拟合的缺陷，并且你会搞砸你的分布。以回忆为导向的目标是另一回事。
- en: Train a little, learn a little – active learning
  id: totrans-529
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 少量训练，少量学习——主动学习
- en: 'Active learning is a super power to quickly develop classifiers. It has saved
    many a project in the real world. The idea is very simple and can be broken down
    as follows:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习是一种快速开发分类器的超级能力。它已经在现实世界的许多项目中挽救了许多项目。这个想法非常简单，可以分解如下：
- en: Assemble a packet of raw data that is way bigger than you can annotate manually.
  id: totrans-531
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 组装一个远远超过你手动标注能力的原始数据包。
- en: Annotate an embarrassingly small amount of the raw data.
  id: totrans-532
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅标注一小部分原始数据。
- en: Train the classifier on the embarrassingly small amount of training data.
  id: totrans-533
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在那令人尴尬的小量训练数据上训练分类器。
- en: Run the trained classifier on all the data.
  id: totrans-534
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有数据上运行训练好的分类器。
- en: Put the classifier output into a `.csv` file ranked by confidence of best category.
  id: totrans-535
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将分类器的输出按最佳类别的置信度排序放入一个 `.csv` 文件中。
- en: Correct another embarrassingly small amount of data, starting with the most
    confident classifications.
  id: totrans-536
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更正另一小部分令人尴尬的数据，从最自信的分类开始。
- en: Evaluate the performance.
  id: totrans-537
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估性能。
- en: Repeat the process until the performance is acceptable, or you run out of data.
  id: totrans-538
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复此过程，直到性能可接受，或者数据用尽。
- en: If successful, be sure to evaluate/threshold on fresh data, because the active
    learning process can introduce biases to the evaluation.
  id: totrans-539
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果成功，请务必在新鲜数据上评估/阈值，因为主动学习过程可能会引入偏差到评估中。
- en: What this process does is help the classifier distinguish the cases where it
    is making higher confidence mistakes and correcting it. It also works as a classification-driven
    search engine of sorts, where the positive training data functions as the query,
    and the remaining data functions as the index being searched.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程所做的就是帮助分类器区分它做出更高置信度错误的案例，并对其进行纠正。它还充当一种分类驱动的搜索引擎，其中正训练数据充当查询，其余数据充当被搜索的索引。
- en: Traditionally, active learning is applied to the near-miss cases where the classifier
    is unsure of the correct class. In this case, the corrections will apply to the
    lowest confidence classifications. We came up with the high-confidence correction
    approach because we were under pressure to increase precision with a thresholded
    classifier that only accepted high-confidence decisions.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，主动学习应用于分类器不确定正确类别的近错案例。在这种情况下，更正将应用于置信度最低的分类。我们提出了高置信度更正方法，因为我们面临着在仅接受高置信度决策的阈值分类器中提高精度的压力。
- en: Getting ready
  id: totrans-542
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: What is going on here is that we are using the classifier to find more data
    that looks like what it knows about. For problems where the target classes are
    rare in the unannotated data, it can very quickly help the system identify more
    examples of the class. For example, in a binary-classification task with marginal
    probability of 1 percent for the target class in the raw data, this is almost
    certainly the way to go. You cannot ask annotators to reliably mark a 1-in-100
    phenomenon over time. While this is the right way to do it, the end result is
    that it will not be done because of the effort involved.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生的事情是，我们正在使用分类器来找到更多看起来像它所知道的数据。对于目标类别在未标注数据中很少见的问题，它可以非常快速地帮助系统识别更多该类别的示例。例如，在一个二分类任务中，目标类别的原始数据中边际概率为1%，这几乎肯定是一条可行的途径。你不能要求标注者随着时间的推移可靠地标记1/100的现象。虽然这是正确的方法，但最终结果可能因为涉及的努力而不会这样做。
- en: Like most cheats, shortcuts, and super powers, the question to ask is what is
    the price paid. In the duality of precision and recall, recall suffers with this
    approach. This is because the approach biases annotation towards known cases.
    Cases that have very different wording are unlikely to be found, so coverage can
    suffer.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 就像大多数作弊、捷径和超级能力一样，需要问的问题是付出了什么代价。在精确度和召回率的二元对立中，召回率会受到影响。这是因为这种方法倾向于已知案例的标注。具有非常不同措辞的案例不太可能被发现，因此覆盖率可能会受到影响。
- en: How to do it…
  id: totrans-545
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Let''s get started with active learning:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始主动学习：
- en: Collect some training data in our `.csv` format from [Chapter 1](part0014_split_000.html#page
    "Chapter 1. Simple Classifiers"), *Simple Classifiers*, or use our example data
    in `data/activeLearning/disneyDedupe.0.csv`. Our data builds on the Disney tweets
    from [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"),
    *Simple Classifiers*. Sentiment is a good candidate for active learning, because
    it benefits from quality training data and creating quality training data can
    be difficult. Use the `.csv` file format from the Twitter search downloader if
    you are using your own data.
  id: totrans-547
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[第1章](part0014_split_000.html#page "第1章。简单分类器")，*简单分类器*收集一些我们的`.csv`格式的训练数据，或者使用`data/activeLearning/disneyDedupe.0.csv`中的示例数据。我们的数据建立在[第1章](part0014_split_000.html#page
    "第1章。简单分类器")，*简单分类器*中的迪士尼推文之上。情感分析是主动学习的良好候选，因为它受益于高质量的训练数据，而创建高质量的训练数据可能很困难。如果你使用自己的数据，请使用Twitter搜索下载器的`.csv`文件格式。
- en: Run the `.csv` deduplication routine from the *Eliminate near duplicates with
    Jaccard distance* recipe of [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple
    Classifiers"), *Simple Classifiers* to get rid of near-duplicate tweets. We have
    already done this with our example data. We went from 1,500 tweets to 1,343.
  id: totrans-548
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行[第1章](part0014_split_000.html#page "第1章。简单分类器")，*简单分类器*中*消除近重复项使用Jaccard距离*食谱的`.csv`去重程序，以消除近重复推文。我们已经用示例数据做了这件事。我们从1,500条推文减少到1,343条。
- en: 'If you have your own data, annotate around 25 examples in the `TRUTH` column
    according to the standard annotation:'
  id: totrans-549
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你有自己的数据，根据标准标注在`TRUTH`列中标注大约25个示例：
- en: '`p` stands for positive sentiment'
  id: totrans-550
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p`代表积极情感'
- en: '`n` stands for negative sentiment'
  id: totrans-551
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n`代表消极情感'
- en: '`o` stands for other, which means that no sentiment is expressed, or the tweet
    is not in English'
  id: totrans-552
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`o`代表其他，这意味着没有表达情感，或者推文不是英文。'
- en: Be sure to get a few examples of each category
  id: totrans-553
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保获取每个类别的几个示例
- en: Our example data is already annotated for this step. If you are using your own
    data, be sure to use the format of the first file (that has the `0.csv` format),
    with no other `.` in the path.
  id: totrans-554
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的示例数据已经为此步骤进行了标注。如果你使用自己的数据，请确保使用第一个文件（具有`0.csv`格式）的格式，路径中不包含其他`.`。
- en: '![How to do it…](img/00005.jpeg)'
  id: totrans-555
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做…](img/00005.jpeg)'
- en: Examples of tweets annotated. Note that all categories have examples.
  id: totrans-556
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 已标注的推文示例。注意，所有类别都有示例。
- en: 'Run the following command. Do not do this on your own annotated data without
    backing up the file. Our I/O routine is written for simplicity, not robustness.
    You have been warned:'
  id: totrans-557
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令。在没有备份文件的情况下，不要在自己的标注数据上执行此操作。我们的I/O例程是为简单而编写的，而不是为了健壮性。我们已经警告过你：
- en: '[PRE137]'
  id: totrans-558
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE137]'
- en: 'Pointed at the supplied annotated data, this will print the following to the
    console with a final suggestion:'
  id: totrans-559
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指向提供的标注数据，这将打印以下内容到控制台，并给出最终建议：
- en: '[PRE138]'
  id: totrans-560
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE138]'
- en: 'This recipe will show you how to make it better, mainly by making it bigger
    in smart ways. Let''s see where we stand:'
  id: totrans-561
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个配方将向你展示如何通过以更智能的方式使其更大来改进它。让我们看看我们目前的情况：
- en: The data has been annotated a bit for three categories
  id: totrans-562
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据已经为三个类别进行了一些标注
- en: Of 1,343 tweets, there have been 25 annotated, 13 of which are `o`, which we
    don't particularly care about given the use case, but they still are important
    because they are not `p` or `n`
  id: totrans-563
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在1,343条推文中，有25条进行了标注，其中13条是`o`，鉴于用例，我们并不特别关心这些，但它们仍然很重要，因为它们不是`p`或`n`。
- en: This is not nearly enough annotated data to build a reliable classifier with,
    but we can use it to help annotate more data
  id: totrans-564
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这并不是构建可靠分类器的足够标注数据，但我们可以用它来帮助标注更多数据
- en: The last line encourages more annotation and the name of a file to annotate
  id: totrans-565
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一行鼓励进行更多标注，并指出要标注的文件名
- en: The precision and recall are reported for each category, that is, the result
    of cross validation over the training data. There is also a confusion matrix.
    At this point, we are not expecting very good performance, but `p` and `o` are
    doing quite well. The `n` category is not doing well at all.
  id: totrans-566
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个类别都报告了精确度和召回率，即对训练数据进行交叉验证的结果。还有一个混淆矩阵。到目前为止，我们并不期望有很好的性能，但`p`和`o`做得相当好。`n`类别做得非常不好。
- en: 'Next, fire up a spreadsheet, and import and view the indicated `.csv` file
    using a UTF-8 encoding. OpenOffice shows us the following:'
  id: totrans-567
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，打开电子表格，使用UTF-8编码导入并查看指示的`.csv`文件。OpenOffice显示以下内容：
- en: '![How to do it…](img/00006.jpeg)'
  id: totrans-568
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做…](img/00006.jpeg)'
- en: Initial output of the active learning approach
  id: totrans-569
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 活动学习方法的初始输出
- en: Reading from the left-hand side to the right-hand side, we will see the `SCORE`
    column, which reflects the classifier's confidence; its most likely category,
    shown in the `GUESS` column, is correct. The next column is the `TRUTH` class
    as determined by a human. The last `TEXT` column is the tweet being classified.
  id: totrans-570
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从左侧向右侧读取，我们将看到`SCORE`列，它反映了分类器的置信度；最可能的类别，在`GUESS`列中显示，是正确的。下一列是人工确定的`TRUTH`类别。最后一列`TEXT`是正在分类的推文。
- en: 'All 1,343 tweets have been classified in one of two ways:'
  id: totrans-571
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有1,343条推文都通过两种方式之一进行了分类：
- en: If the tweet had an annotation, that is, an entry in the `TRUTH` column, then
    the annotation was made when the tweet was in the test fold of a 10-fold cross
    validation. Line 13 is just such a case. In this case, the classification was
    `o`, but the truth was `p`, so it would be a false negative for `p`.
  id: totrans-572
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果推文有标注，即`TRUTH`列中的条目，那么标注是在推文处于10折交叉验证的测试折叠时进行的。第13行就是这样一种情况。在这种情况下，分类是`o`，但事实是`p`，所以对于`p`来说，这将是一个假阴性。
- en: If the tweet was not annotated, that is, no entry in the `TRUTH` column, then
    it was classified using all the available training data. All other examples in
    the shown spreadsheet are handled this way. They don't inform the evaluation at
    all. We will annotate these tweets to help improve classifier performance.
  id: totrans-573
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果推文没有标注，即`TRUTH`列中没有条目，那么它就是使用所有可用的训练数据进行分类的。显示的电子表格中的所有其他示例都是这样处理的。它们根本不影响评估。我们将标注这些推文以帮助提高分类器性能。
- en: Next, we will annotate high-confidence tweets irrespective of category, as shown
    in the following screenshot:![How to do it…](img/00007.jpeg)
  id: totrans-574
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将注释高置信度的推文，无论类别如何，如下面的屏幕截图所示：![如何做…](img/00007.jpeg)
- en: Corrected output for active learning output. Note the dominance of the o category.
  id: totrans-575
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 活动学习输出的更正输出。注意o类别的优势。
- en: Annotating down to line 19, we will notice that most of the tweets are `o` and
    are dominating the process. There are only three `p` and no `n`. We need to get
    some `n` annotations.
  id: totrans-576
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注释到第19行，我们会注意到大多数推文都是`o`，并且主导着整个过程。只有三个`p`，没有`n`。我们需要得到一些`n`注释。
- en: We can focus on likely candidate `n` annotations by selecting the entire sheet,
    except for the headers, and sorting by column **B** or `GUESS`. Scrolling to the
    `n` guesses, we should see the highest confidence examples. In the following screenshot,
    we have annotated all the `n` guesses because the category needs data. Our annotations
    are in `data/activeLearningCompleted/disneySentimentDedupe.1.csv`. If you want
    to exactly duplicate the recipe, you will have to copy this file to the `activeLearning`
    directory.![How to do it…](img/00008.jpeg)
  id: totrans-577
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过选择整个工作表（除了标题外）并按列**B**或`GUESS`排序，来专注于可能的候选`n`注释。滚动到`n`猜测，我们应该看到置信度最高的示例。在下面的屏幕截图中，我们注释了所有的`n`猜测，因为该类别需要数据。我们的注释在`data/activeLearningCompleted/disneySentimentDedupe.1.csv`中。如果你想完全复制这个配方，你必须将此文件复制到`activeLearning`目录中。![如何做…](img/00008.jpeg)
- en: Annotations sorted by category with very few n or negative categories.
  id: totrans-578
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 按类别排序的注释，其中n或负类别非常少。
- en: Scrolling to the `p` guesses, we annotated a bunch as well.![How to do it…](img/00009.jpeg)
  id: totrans-579
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动到`p`猜测，我们也注释了一大堆。![如何做…](img/00009.jpeg)
- en: Positive labels with corrections and surprising number of negatives
  id: totrans-580
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 带有更正的正面标签和令人惊讶的负数数量
- en: There are eight negative cases that we found in the `p` guesses mixed in with
    lots of `p` and some `o` annotations.
  id: totrans-581
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在`p`猜测中发现了八个负案例，混合在许多`p`和一些`o`注释中。
- en: 'We will save the file without changing the filename and run the same program
    as we did earlier:'
  id: totrans-582
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将保存文件，不更改文件名，并运行我们之前所做的相同程序：
- en: '[PRE139]'
  id: totrans-583
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE139]'
- en: 'The output will be as follows:'
  id: totrans-584
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE140]'
  id: totrans-585
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE140]'
- en: 'This is a fairly typical output early in the annotation process. Positive `p`,
    the easy category, is dragging along at 49-percent precision and 45-percent recall.
    Negative `n` is even worse. Undaunted, we will do another round of annotation
    on the output file indicating focus on `n` guesses to help that category improve
    performance. We will save and rerun the file:'
  id: totrans-586
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是在注释过程中的一个相当典型的输出。正面`p`，一个简单的类别，以49%的准确率和45%的召回率缓慢前进。负面的`n`甚至更糟。尽管如此，我们将在输出文件上再进行一轮注释，重点关注`n`猜测，以帮助该类别提高性能。我们将保存并重新运行文件：
- en: '[PRE141]'
  id: totrans-587
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE141]'
- en: This last round of annotation got us over the edge (remember to copy over our
    annotation from `activeLearningCompleted/disneySentimentDedupe.2.csv` if you are
    mirroring the recipe exactly). We annotated high-confidence examples from both
    `p` and `n`, adding nearly 100 examples. The first best annotation for `n` is
    above 50-percent precision with 41-percent recall. We assume that there will be
    a tunable threshold that meets our 80-percent requirement for `p` and declares
    victory in 211 moves, which is much less than the total 1,343 annotations.
  id: totrans-588
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这最后一轮注释使我们达到了极限（如果你要完全复制这个配方，请记住将我们的注释从`activeLearningCompleted/disneySentimentDedupe.2.csv`复制过来）。我们从`p`和`n`中注释了高置信度示例，增加了近100个示例。`n`的第一个最佳注释的准确率超过50%，召回率为41%。我们假设将会有一个可调的阈值，以满足我们对`p`的80%要求，并在211步中宣布胜利，这比总共1,343个注释要少得多。
- en: That's it. This is a real-world example and the first example we have tried
    for the book, so the data is not cooked. The approach tends to work, although
    no promises; some data resists even the most focused efforts of a well-equipped
    computational linguist.
  id: totrans-589
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就这样。这是一个真实世界的例子，也是我们为这本书尝试的第一个例子，所以数据是未经处理的。这种方法往往有效，尽管不能保证；有些数据甚至对装备精良的计算语言学家最专注的努力也表现出抵抗力。
- en: Be sure to store the final `.csv` file some place safe. It would be a shame
    to lose all that directed annotation.
  id: totrans-590
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一定要将最终的`.csv`文件保存在安全的地方。丢失所有这些有针对性的注释将是件遗憾的事。
- en: Before releasing this classifier we would want to run the classifier, which
    trains on all annotated data, on new text to verify performance and set thresholds.
    This annotation process introduces biases over the data that will not be reflected
    in the real world. In particular, we have biased annotation for `n` and `p` and
    added `o` as we saw them. This is not the actual distribution.
  id: totrans-591
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在发布这个分类器之前，我们希望运行这个分类器，它训练所有注释数据，并在新文本上验证性能并设置阈值。这个注释过程会在数据中引入偏差，这些偏差在现实世界中不会反映出来。特别是，我们对`n`和`p`进行了有偏注释，并添加了`o`，正如我们所看到的。这并不是实际的分布。
- en: How it works...
  id: totrans-592
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'This recipe has some subtlety because of the simultaneous evaluation and creation
    of ranked output for annotation. The code starts with constructs that should be
    familiar to you:'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方由于同时评估和创建注释的排名输出而有一些微妙之处。代码从您应该熟悉的构造开始：
- en: '[PRE142]'
  id: totrans-594
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: The `getLatestEpochFile` method looks for the highest numbered file that ends
    with `csv`, shares the root with the filename, and returns it. On no account will
    we use this routine for anything serious. The method is standard Java, so we won't
    cover it.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '`getLatestEpochFile`方法寻找以`csv`结尾的最高编号文件，与文件名共享根目录，并返回它。我们决不会用这个程序做任何严肃的事情。这个方法是标准的Java，所以我们将不会涉及它。'
- en: 'Once we have the latest file, we will do some reporting, read it in our standard
    `.csv` annotated files, and load a cross-validating corpus. All these routines
    are explained elsewhere in locations specified in the `Util` source. Finally,
    we will get the categories that were found in the `.csv` annotated file:'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了最新的文件，我们将进行一些报告，读取我们的标准`.csv`注释文件，并加载一个交叉验证语料库。所有这些程序在其他地方有详细说明，位置由`Util`源中指定的位置给出。最后，我们将得到在`.csv`注释文件中找到的类别：
- en: '[PRE143]'
  id: totrans-597
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: 'Next, we will configure some standard logistic-regression-training parameters
    and create the evaluator for cross-fold evaluation. Note that the Boolean for
    `storeInputs` is `true`, which will facilitate recording results. The *How to
    train and evaluate with cross validation* recipe in [Chapter 1](part0014_split_000.html#page
    "Chapter 1. Simple Classifiers"), *Simple Classifiers*, has a complete explanation:'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将配置一些标准的逻辑回归训练参数，并创建用于交叉验证评估的评估器。请注意，`storeInputs`的布尔值为`true`，这将便于记录结果。《第1章》（`part0014_split_000.html#page
    "Chapter 1. Simple Classifiers"`）中关于`*How to train and evaluate with cross validation*`的配方有完整的解释：
- en: '[PRE144]'
  id: totrans-599
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: 'Then, we will execute standard cross validation:'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将执行标准的交叉验证：
- en: '[PRE145]'
  id: totrans-601
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: 'At the end of cross validation, the evaluator has all the classifications stored
    in `visitTest()`. Next, we will transfer this data to an accumulator, which creates
    and stores rows that will be put into the output spreadsheet and redundantly stores
    the score; this score will be used in a sort to control the order of annotations
    printed out:'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 在交叉验证结束时，评估器在`visitTest()`中存储了所有的分类。接下来，我们将把这些数据转移到累加器中，它创建并存储将要放入输出电子表格的行，并冗余地存储分数；这个分数将用于排序以控制打印出的注释的顺序：
- en: '[PRE146]'
  id: totrans-603
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: 'Then, we will iterate over each category and create a list of the false negatives
    and true positives for the category—these are the cases that the truth category
    is the category label:'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将遍历每个类别，并为该类别创建一个包含假阴性和真阳性的列表——这些是真实类别与类别标签相同的案例：
- en: '[PRE147]'
  id: totrans-605
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: 'Next, all the in-category test cases are used to create rows for the accumulator:'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，所有属于类别的测试用例都用于为累加器创建行：
- en: '[PRE148]'
  id: totrans-607
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: 'Next, the code will print out some standard evaluator output:'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，代码将打印出一些标准的评估器输出：
- en: '[PRE149]'
  id: totrans-609
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: All the mentioned steps only apply to annotated data. We will now turn to getting
    best category and scores for all the unannotated data in the `.csv` file.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 所述的所有步骤仅适用于注释数据。我们现在将转向获取`.csv`文件中所有未注释数据的最佳类别和分数。
- en: 'First, we will set the number of folds on the cross-validating corpus to `0`,
    which means that `vistTrain()` will visit the entire corpus of annotations—unannotated
    data is not contained in the corpus. The logistic regression classifier is trained
    in the usual way:'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将设置交叉验证语料库的折数数量为`0`，这意味着`vistTrain()`将访问整个注释语料库——未注释的数据不包含在语料库中。逻辑回归分类器以通常的方式进行训练：
- en: '[PRE150]'
  id: totrans-612
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: 'Armed with a classifier, the code iterates over all the `data` items, one row
    at a time. The first step is to check for an annotation. If the value is not the
    empty string, then the data was in the aforementioned corpus and used as training
    data so that the loop skips to the next row:'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 带着分类器，代码逐行遍历所有的`data`项。第一步是检查注释。如果值不是空字符串，则数据位于上述语料库中，并用作训练数据，因此循环跳到下一行：
- en: '[PRE151]'
  id: totrans-614
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: If the row is unannotated, then the score and `bestCategory()` method is added
    at the appropriate points, and the row is added to the accumulator with the score.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 如果行未标注，则在适当的位置添加分数和`bestCategory()`方法，并将行添加到累加器中，带有分数。
- en: 'The rest of the code increments the index of the filename and writes out the
    accumulator data with a bit of reporting:'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的其余部分增加文件名索引并输出带有少量报告的累加器数据：
- en: '[PRE152]'
  id: totrans-617
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: This is how it works. Remember that the biases that can be introduced by this
    approach invalidate evaluation numbers. Always run on fresh held-out data to get
    a proper sense of the classifier's performance.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是它的工作方式。记住，这种方法可能引入的偏差会使得评估数字无效。始终在新鲜保留数据上运行以获得分类器性能的正确感觉。
- en: Annotation
  id: totrans-619
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标注
- en: One of the most valuable services we provide is teaching our customers how to
    create gold-standard data, also known as training data. Nearly every successful-driven
    NLP project we have done has involved a good deal of customer-driven annotation.
    The quality of the NLP is entirely dependent on the quality of the training data.
    Creating training data is a fairly straightforward process, but it requires attention
    to detail and significant resources. From a budget perspective, you can expect
    to spend as much as the development team on annotation, if not more.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供的一项最有价值的服务是教我们的客户如何创建黄金标准数据，也称为训练数据。我们进行的几乎所有以成功为导向的自然语言处理项目都涉及大量的客户驱动标注。自然语言处理的质量完全取决于训练数据的质量。创建训练数据是一个相当直接的过程，但它需要关注细节和大量资源。从预算角度来看，你可以预期在标注上花费的金额与开发团队一样多，甚至更多。
- en: How to do it...
  id: totrans-621
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: We will use sentiment over tweets as our example, and we will assume a business
    context, but even academic efforts will have similar dimensions.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以推文上的情感为例，并假设一个商业环境，但即使是学术努力也将有类似的维度。
- en: Get 10 examples of what you expect the system to do. For our example, this means
    getting 10 tweets that reflect the scope of what the system is expected to do.
  id: totrans-623
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取你期望的系统将执行的前10个示例。对于我们的例子，这意味着获取10条反映系统预期执行范围的推文。
- en: Make some effort to pick from the range of what you expect as inputs/outputs.
    Feel free to cherry-pick strong examples, but do not make up examples. Humans
    are terrible at creating example data. Seriously, don't do it.
  id: totrans-624
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽量从你期望的输入/输出范围内挑选。你可以自由挑选强有力的例子，但不要编造例子。人类在创建示例数据方面非常糟糕。真的，不要这么做。
- en: Annotate these tweets for the expected categories.
  id: totrans-625
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标注这些推文为预期的类别。
- en: 'Have a meeting with all the stakeholders in the annotation. This includes user-experience
    designers, business folks, developers, and end users. The goal of this meeting
    is to expose all the relevant parties to what the system will actually do—the
    system will take the 10 examples and produce the category label. You will be amazed
    at how much clarity this step establishes. Here are the kinds of clarity:'
  id: totrans-626
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与标注中的所有利益相关者召开会议。这包括用户体验设计师、商业人士、开发人员和最终用户。这次会议的目标是让所有相关方了解系统实际上将做什么——系统将使用这10个示例并产生类别标签。你会对这一步骤带来的多少清晰度感到惊讶。以下是一些清晰度：
- en: Upstream/downstream users of the classifier will have a clear idea of what they
    are expected to produce or consume. For example, the system consumes UTF-8-encoded
    English tweets and produces an ASCII single character of `p`, `n`, or `u`.
  id: totrans-627
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类器的上游/下游用户将清楚地了解他们期望产生或消费的内容。例如，系统消耗UTF-8编码的英文推文，并产生一个ASCII单字符`p`、`n`或`u`。
- en: For a sentiment, people tend to want a severity score, which is very hard to
    get. You can expect annotation costs to double at least. Is it worth it? A score
    of confidence can be provided, but that is confidence that the category is correct
    *not* the severity of the sentiment. This meeting will force the discussion.
  id: totrans-628
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于情感，人们往往希望得到一个严重程度分数，这非常难以获得。你可以预期标注成本至少翻倍。这值得吗？可以提供一个置信度分数，但这只是对类别正确的置信度，而不是情感的严重程度。这次会议将迫使进行讨论。
- en: During this meeting explain that each category will likely need at least 100
    examples, if not 500, to do a reasonable job. Also explain that switching domains
    might require new annotations. NLP is extremely easy for your human colleagues,
    and as a result, they tend to underestimate what it takes to build systems.
  id: totrans-629
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这次会议中解释说，每个类别可能至少需要100个示例，如果不是500个，才能做合理的工作。还解释说，切换领域可能需要新的标注。自然语言处理对于你的同事来说非常容易，因此他们往往低估了构建系统所需的努力。
- en: Don't neglect to include whoever is paying for all this. I suppose you should
    not have your parents involved if this is your undergraduate thesis.
  id: totrans-630
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要忽视包括支付所有这些费用的人。我想如果你这是本科论文，你应该不让你的父母参与。
- en: 'Write down an annotation standard that explains the intention behind each category.
    It doesn''t need to be very complex, but it needs to exist. The annotation standard
    should be circulated around the stakeholders. Bonus points if you have one at
    the mentioned meetings; if so, it will likely be different at the end, but this
    is fine. An example is:'
  id: totrans-631
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录一个注释标准，解释每个类别的意图。它不需要非常复杂，但必须存在。注释标准应该分发给所有利益相关者。如果在提到的会议上已经有了这样的标准，那么加分；如果是这样，最终可能有所不同，但这没关系。例如：
- en: A tweet is positive `p` if the sentiment is unambiguously positive about Disney.
    A positive sentiment that applies to a non-Disney tweet is not `p` but `u`. An
    example is the `n` tweet indicates clearly negative intent towards Disney. Examples
    include that all other tweets are `u`.
  id: totrans-632
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果推文的情感明确地是关于迪士尼的积极，那么这条推文就是积极的`p`。如果一个积极的情感适用于非迪士尼的推文，那么它不是`p`而是`u`。一个例子是`n`推文明确表示对迪士尼的负面意图。其他所有推文都是`u`。
- en: Examples in the annotation standard do the best job of communicating the intent.
    Humans do a better job with examples rather than descriptions in our experience.
  id: totrans-633
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在注释标准中的示例最能有效地传达意图。根据我们的经验，人类用示例比用描述做得更好。
- en: Create your collection of unannotated data. The best practice here is for the
    data to be random from the expected source. This works fine for categories with
    noticeable prevalence in data, say 10 percent or more, but we have built classifiers
    that occur at a rate of 1/2,000,000 for question-answering systems. For rare categories,
    you can use a search engine to help find instances of the category—for example,
    search for `luv` to find positive tweets. Alternatively, you can use a classifier
    trained on a few examples, run it on data, and look at the high-scoring positives—we
    covered this in the previous recipe.
  id: totrans-634
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建你的未注释数据集。这里的最佳实践是数据来自预期的来源，并且是随机的。这对于数据中具有明显普遍性的类别来说效果很好，比如说10%或更多，但我们已经构建了在问答系统中以1/2,000,000的比率出现的分类器。对于罕见类别，你可以使用搜索引擎来帮助找到该类别的实例——例如，搜索`luv`以找到积极的推文。或者，你可以使用在几个示例上训练的分类器，在数据上运行它，并查看得分高的积极项——我们已经在之前的食谱中介绍了这一点。
- en: 'Recruit at least two annotators to annotate data. The reason we need at least
    two is that the task has to be shown to be reproducible by humans. If people can''t
    reliably do the task, then you can''t expect a computer to do it. This is where
    we execute some code. Type in the following command in the command line or invoke
    your annotators in you IDE—this will run with our default files:'
  id: totrans-635
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 招募至少两名注释员来注释数据。我们需要至少两名注释员的原因是，这项任务必须证明人类可以重复进行。如果人们无法可靠地完成任务，那么你不能期望计算机能够完成。这就是我们执行代码的地方。在命令行中输入以下命令或在你的IDE中调用你的注释员——这将使用我们的默认文件运行：
- en: '[PRE153]'
  id: totrans-636
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '[PRE154]'
  id: totrans-637
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE154]'
- en: The code reports disagreements and prints out a confusion matrix. Precision
    and recall are useful metrics as well.
  id: totrans-638
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码报告分歧并打印出混淆矩阵。精确率和召回率也是有用的指标。
- en: How it works…
  id: totrans-639
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'There is little novel data in the code in `src/com/lingpipe/cookbook/chapter3/InterAnnotatorAgreement.java`.
    One slight twist is that we used `BaseClassifierEvaluator` to do the evaluation
    work without a classifier ever being specified—the creation is as follows:'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 在`src/com/lingpipe/cookbook/chapter3/InterAnnotatorAgreement.java`中的代码几乎没有新数据。有一点细微的区别是，我们使用了`BaseClassifierEvaluator`来执行评估工作，而不需要指定任何分类器——创建方式如下：
- en: '[PRE155]'
  id: totrans-641
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: 'The evaluator is populated with classifications directly rather than the usual
    `Corpus.visitTest()` method, as done elsewhere in the book:'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 评估者直接填充分类，而不是像书中其他地方所做的那样使用`Corpus.visitTest()`方法：
- en: '[PRE156]'
  id: totrans-643
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: If the recipe requires further explanation, consult the *Evaluation of classifiers—the
    confusion matrix* recipe in [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple
    Classifiers"), *Simple Classifiers*.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 如果食谱需要进一步解释，请参考[第1章](part0014_split_000.html#page "第1章. 简单分类器")中的*分类器评估——混淆矩阵*食谱，*简单分类器*。
- en: There's more…
  id: totrans-645
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: Annotation is a very complex area that deserves its own book, and fortunately,
    there is a good one, *Natural Language Annotation for Machine Learning*, *James
    Pustejovsky and Amber Stubbs*, *O'Reilly Media*. To get annotations done, there
    is Amazon's Mechanical Turk service as well as companies that specialize in the
    creation of training data such as CrowdFlower. However, be careful of outsourcing
    because classifiers are very dependent on the quality of data.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 注释是一个非常复杂的领域，值得拥有一本自己的书，幸运的是，有一本很好的书，*《机器学习自然语言注释》*，作者是*詹姆斯·普斯特约夫斯基和安布尔·斯塔布斯*，由*奥莱利媒体*出版。为了完成注释工作，可以使用亚马逊的Mechanical
    Turk服务，以及专注于创建训练数据的公司，如CrowdFlower。然而，外包时要小心，因为分类器非常依赖于数据的质量。
- en: Conflict resolution between annotators is a challenging area. Many errors will
    be due to attention lapses, but some will persist as legitimate areas of disagreement.
    Two easy resolution strategies are either to throw out the data or keep both annotations.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 注释者之间的冲突解决是一个具有挑战性的领域。许多错误将归因于注意力分散，但一些错误将作为合法的不同意见领域持续存在。两种简单的解决策略要么是丢弃数据，要么是保留两个注释。
