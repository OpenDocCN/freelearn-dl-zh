<html><head></head><body>
		<div><h1 id="_idParaDest-183" class="chapter-number"><a id="_idTextAnchor184"/>9</h1>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor185"/>Responsible Development of AI Solutions: Building with Integrity and Care</h1>
			<p>In the realm of modern technology, <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) has emerged as a transformative force, reshaping industries, improving efficiency, and enhancing user experiences. As cloud and AI architects, we stand at the forefront of this AI revolution, wielding the power to shape the future of AI-driven solutions. However, with great power comes great responsibility. The integration of responsible AI practices into the design and deployment of AI solutions is not merely a moral or ethical imperative; it is a strategic imperative that directly impacts the success, reputation, and sustainability of organizations in the AI landscape.</p>
			<p>Neglecting responsible AI (RAI) principles can have a profound impact on human lives. A thought-provoking article from MIT titled <em class="italic">AI is sending people to jail—and getting it wrong</em>, explores the application of AI and algorithms in the criminal justice system. It highlights how facial recognition systems and predictive algorithms used by police and judges can exhibit bias due to their training data, leading to incorrect decisions that affect human lives. Researchers have consistently shown that facial recognition systems are particularly prone to failure in identifying individuals with dark skin. Prediction models used in the justice system can be skewed towards a certain group of people, leading to incorrect judgments. Instances such as these (among others that we will explore in this book) underscore the urgent need for AI solutions developed with integrity and care.</p>
			<p>In this chapter, we delve into the essentials of <strong class="bold">responsible artificial intelligence</strong> (<strong class="bold">RAI</strong>), starting with the key principles of AI design and addressing the unique challenges presented by <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>). As we explore the rising concerns around Deepfakes, which are hyper-realistic digital manipulations often used to create fake videos or images, the importance of robust AI architecture and proactive leadership becomes evident, highlighting the need for ethical and responsible AI development. The chapter also examines the relationship between AI, cloud computing, and legal frameworks, emphasizing the significance of legal compliance and ethical considerations. Additionally, we provide insights into the most popular RAI tools, offering practical guidance for their application. By the end of this chapter, you will have a comprehensive understanding of the principles guiding RAI, strategies to combat LLM challenges, an awareness of the impact of Deepfakes, knowledge of AI’s role in cloud computing and legal contexts, and familiarity with essential RAI tools, empowering you to navigate and contribute to the field of AI responsibly and ethically.</p>
			<p>We will cover the following main topics in the chapter:</p>
			<ul>
				<li>Understanding responsible AI design</li>
				<li>Key principles of RAI</li>
				<li>Addressing LLM challenges with RAI principles</li>
				<li>Rising Deepfake concern</li>
				<li>Building applications using a responsible AI-first approach</li>
				<li>AI, the cloud, and the law – Understanding compliance and regulations</li>
				<li>Startup ecosystem in RAI</li>
			</ul>
			<h1 id="_idParaDest-185"><a id="_idTextAnchor186"/>Understanding responsible AI design</h1>
			<p>In this section, we <a id="_idIndexMarker766"/>will explore the true meaning of responsible AI and delve into the fundamental design principles that should be considered while architecting generative AI solutions.</p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor187"/>What is responsible AI?</h2>
			<p>As stated by <a id="_idIndexMarker767"/>Microsoft public documentation, “<em class="italic">Responsible Artificial Intelligence</em> <em class="italic">(Responsible AI) is an approach to developing, assessing, and deploying AI systems in a safe, trustworthy, and ethical way.</em>” It is like building and using smart computer programs (AI systems) in a way that is safe, fair, and ethical. Think of AI systems as tools created by people who make a lot of choices about how these tools should work. Responsible AI is about making these choices carefully to make sure AI acts in a way that is good and fair for everyone. It’s like guiding AI to always consider what is best for people and their needs. This includes making sure AI is reliable, fair, and transparent about how it works. Here are a few examples of the types of tools being developed in this space:</p>
			<ul>
				<li><strong class="bold">Fair hiring tools</strong>: An AI tool used by a company to help choose job candidates. Responsible AI would ensure this AI doesn’t favor one group of people over another, making the hiring process fair for all applicants. For example, <strong class="bold">BeApplied</strong>, a<a id="_idIndexMarker768"/> startup in the RAI space, has developed a piece of ethical recruitment software designed to enhance hiring quality and increase diversity by reducing bias. It stands apart from traditional applicant tracking systems by incorporating fairness, inclusivity, and diversity as its core principles. The platform, underpinned by behavioral science, offers anonymized applications and predictive, skill-based assessments to ensure unbiased hiring. Its features include sourcing analysis tools to diversify talent pools, inclusive job description creation, anonymized skills testing for objective assessments, and data-driven shortlisting to focus purely on skills. BeApplied aims to create a fairer recruitment world, one hire at a time. They currently have some notable customers, such as UNICEF and England and Wales Cricket.</li>
				<li><strong class="bold">Transparent recommendation systems</strong>: Think of a streaming service that suggests movies. Responsible AI would make this system clear about why it recommends <a id="_idIndexMarker769"/>certain movies, ensuring it’s not just promoting certain movies for <a id="_idIndexMarker770"/>unfair reasons. For example, <strong class="bold">LinkedIn</strong> is a notable example of a company that focuses on transparent and explainable AI systems, especially in its recommendation systems. Their approach ensures that AI system behavior and any related components are understandable, explainable, and interpretable. They prioritize transparency in AI to make their systems trustworthy and to avoid harmful bias while respecting privacy. For instance, they<a id="_idIndexMarker771"/> developed <strong class="bold">CrystalCandle</strong>, a customer-facing model explainer that creates digestible interpretations and insights reflecting the rationale behind model predictions. This tool is integrated with business predictive models, aiding sales and marketing by converting complex machine learning outputs into clear, actionable narratives for users.</li>
				<li><strong class="bold">Healthcare</strong>: In the healthcare industry, there’s a growing focus on developing ethical AI tools to ensure fairness, transparency, and accountability within AI-driven decisions. These tools are designed to minimize biases, safeguard patient data privacy, and enhance the explainability and reliability of AI algorithms. Ethical AI is pivotal in healthcare as it aids in delivering personalized care, improving patient outcomes, and maintaining high ethical standards. Embedding ethical considerations into AI systems helps prevent potential negative impacts, address health inequalities, and build trust with patients and the community, thereby positively influencing public health and well-being. One prominent example of such an<a id="_idIndexMarker772"/> ethical AI tool in healthcare is <strong class="bold">Merative</strong> (formerly IBM Watson Health). It supports healthcare professionals by offering evidence-based, personalized treatment recommendations with a focus on transparency and explainability. The platform also prioritizes patient data protection in compliance with healthcare regulations such as HIPAA and aims to reduce bias by employing diverse datasets for training its AI models. This approach by IBM Watson Health demonstrates the potential of AI to improve healthcare decision-making processes while emphasizing patient safety, data privacy, and equity across diverse patient populations.</li>
				<li><strong class="bold">Finance</strong>: In the<a id="_idIndexMarker773"/> finance industry, ethical AI tools are being developed to navigate complex ethical considerations such as data privacy and algorithmic bias and ensure transparency and accountability in AI-driven processes. In the finance industry, ethical <a id="_idIndexMarker774"/>AI tools such as <strong class="bold">Zest AI</strong> are revolutionizing how financial institutions approach lending by enhancing fairness and transparency in credit decisions. Zest AI leverages machine learning to improve credit scoring accuracy and reduce biases, thus promoting financial inclusivity. Its focus on explainability ensures that lenders can comprehend and justify AI-driven decisions, aligning with regulatory compliance and bolstering borrower trust. This example underscores the finance sector’s commitment to integrating responsible AI practices that benefit both institutions and customers, adhering to ethical standards.</li>
				<li><strong class="bold">Criminal justice</strong>: In the criminal justice system, the development of ethical AI tools is a growing focus aimed at enhancing fairness, reducing bias, and improving the accuracy of legal outcomes. These tools are designed to support decision-making processes in areas such as predictive policing, risk assessment for bail and sentencing, and evidence analysis. One example of an ethical AI tool in criminal justice is <strong class="bold">Correctional Offender Management Profiling for Alternative Sanctions (COMPAS)</strong>. COMPAS <a id="_idIndexMarker775"/>is a risk assessment tool used by courts to evaluate the likelihood of a defendant reoffending. COMPAS considers elements such as past arrests, age, and employment status to generate risk scores for reoffending, which judges then use to decide on sentencing <a id="_idIndexMarker776"/>short-term jail or long-term prison. It was found that Black defendants are mistakenly classified as “high-risk” for future crimes at twice the rate of white defendants. These claims were refuted by the company, which stated that the algorithms worked as designed (<a href="https://tinyurl.com/bdejxubh">https://tinyurl.com/bdejxubh</a>). However, continuous improvements have been made since then. While its implementation has sparked debate over potential biases, it highlights the sector’s attempt to apply AI in making informed, data-driven decisions regarding bail, sentencing, and parole. In response to ethical concerns, efforts are being made to improve such tools by incorporating fairness algorithms, enhancing transparency, and conducting regular audits to identify and mitigate biases. These advancements reflect the broader commitment to developing AI in criminal justice that upholds ethical standards and contributes to a more equitable legal system.</li>
			</ul>
			<h1 id="_idParaDest-187"><a id="_idTextAnchor188"/>Key principles of RAI</h1>
			<div><div><img src="img/B21443_09_1.jpg" alt="Figure 9.1 – Responsible AI principles"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – Responsible AI principles</p>
			<p>Microsoft has <a id="_idIndexMarker777"/>established a <strong class="bold">Responsible AI Standard</strong>, presenting a comprehensive framework that guides the development of AI systems. This framework is grounded in six key principles: <strong class="bold">fairness</strong>, <strong class="bold">reliability and safety</strong>, <strong class="bold">privacy and security</strong>,<strong class="bold"> inclusiveness</strong>, <strong class="bold">transparency</strong>, and <strong class="bold">accountability</strong>, as depicted in the preceding above. They follow two guiding principles: <strong class="bold">ethical and explainable</strong>. These <a id="_idIndexMarker778"/>principles form the bedrock of Microsoft’s commitment to a responsible and trustworthy approach to AI. This approach is increasingly vital as AI becomes more integrated into the products and services we use daily. In my opinion, this framework from Microsoft is exceptionally well-rounded for the design of generative AI solutions and should always be a primary consideration when architecting such solutions. A good mnemonic to remember these principles by is “<strong class="bold">F</strong>riendly <strong class="bold">R</strong>obots <strong class="bold">S</strong>afeguard <strong class="bold">P</strong>rivacy, <strong class="bold">I</strong>nspire <strong class="bold">T</strong>rust, <strong class="bold">A</strong>ssure <strong class="bold">S</strong>afety,” or <strong class="bold">FAST-P</strong>a<strong class="bold">IRS</strong>.</p>
			<p>Let’s dive deep into each of these principles with the help of examples.</p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor189"/>Ethical and explainable</h2>
			<p>From an <a id="_idIndexMarker779"/>ethical standpoint, AI ought to do the following:</p>
			<ul>
				<li>Ensure fairness and inclusiveness in its statements and tasks</li>
				<li>Hold responsibility/accountability for its choices</li>
				<li>Avoid discrimination against various races, disabilities, or backgrounds</li>
			</ul>
			<p>Explainability in AI provides clarity on decision-making processes for data scientists, auditors, and business leaders, enabling them to understand and justify the system’s conclusions. It also ensures adherence to corporate policies, industry norms, and regulatory requirements.</p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor190"/>Fairness and inclusiveness</h2>
			<p>This principle <a id="_idIndexMarker780"/>ensures that AI systems do not discriminate, are not biased against certain groups or individuals, and provide equal opportunities for all.</p>
			<ul>
				<li>For example, designing AI systems with features that accommodate users with disabilities, such as voice-activated assistants that can understand and respond to users with speech impairments or AI-driven web interfaces that are navigable by people with visual impairments.</li>
				<li>This article from <em class="italic">The New York Times</em>, titled <em class="italic">Thousands of Dollars for Something I Didn’t Do</em> discusses the case of an African American individual who was wrongfully charged and fined due to an erroneous facial recognition match. This incident highlights the limitations of AI-based facial recognition systems in accurately identifying individuals with darker skin tones. Such incidents necessitate the need for fairness and inclusiveness principles in AI systems.</li>
			</ul>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor191"/>Reliability and safety</h2>
			<p>This focuses on <a id="_idIndexMarker781"/>the AI system being dependable and not posing any harm to users.</p>
			<p>For example, an AI system used in a self-driving car must be reliable and safe. It should consistently make correct driving decisions, such as stopping at red lights and avoiding obstacles, to ensure the safety of passengers and pedestrians.</p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor192"/>Transparency</h2>
			<p>This principle<a id="_idIndexMarker782"/> demands clarity on how AI systems make decisions or reach conclusions.</p>
			<p>For example, a credit scoring AI system should be transparent about the factors it uses to determine someone’s credit score. This means a user should be able to understand which financial behaviors are impacting their score, whether positively or negatively.</p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor193"/>Privacy and security</h2>
			<p>This ensures that <a id="_idIndexMarker783"/>the personal data used by AI systems are protected and not misused.</p>
			<p>For example, an AI-powered health app that tracks users’ physical activities and health metrics must safeguard this sensitive and personal information. The app should have robust security measures to prevent data breaches and should be clear about how it uses and shares user data.</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor194"/>Accountability</h2>
			<p>This <a id="_idIndexMarker784"/>principle is about taking responsibility for the outcomes of AI systems, including addressing any negative impacts.</p>
			<p>For example, if an AI-powered news recommendation system inadvertently spreads fake news, the creators of the system must take responsibility. They should identify the failure in their algorithm, rectify the issue, and take steps to prevent such occurrences in the future.</p>
			<h1 id="_idParaDest-194"><a id="_idTextAnchor195"/>Addressing LLM challenges with RAI principles</h1>
			<p>As discussed <a id="_idIndexMarker785"/>previously, there are three major challenges we face with LLM outputs: hallucinations, toxicity, and <a id="_idIndexMarker786"/>intellectual property issues. Now let’s double-click into each of these challenges and see how we can use RAI principles to address them.</p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor196"/>Intellectual property issues (Transparency and Accountability)</h2>
			<p>The <a id="_idIndexMarker787"/>RAI principle that addresses <strong class="bold">intellectual property</strong> (<strong class="bold">IP</strong>) issues is referred to as “Transparency and Accountability.” This principle ensures that AI systems are transparent in their operations and that their creators and operators are accountable for their design and use. This includes the prevention of plagiarism and ensuring compliance with copyright laws.</p>
			<p>Transparency involves the clear disclosure of the data sources, algorithms, and training methods used, which can have implications for IP rights.</p>
			<p>For instance, if an AI system is trained on copyrighted materials or incorporates proprietary algorithms, it’s crucial to have proper permissions and to acknowledge these sources to avoid IP infringements. We believe new regulations will emerge in the upcoming years to prevent IP issues in generative AI applications.</p>
			<p>Moreover, research is being carried out on ways to filter out or block responses that are very similar to protected content. For instance, if a user requests a generative AI to produce a narrative that is like a popular fantasy novel, the AI will analyze the request and either alter the output significantly to avoid direct similarities or deny the request altogether, ensuring it does not infringe on the novel’s intellectual property rights.</p>
			<p><strong class="bold">Machine unlearning</strong> is a <a id="_idIndexMarker788"/>relatively recent concept in the field of machine learning and artificial intelligence, which involves the ability to effectively remove specific data from a trained model’s knowledge without retraining it from scratch. This process is particularly relevant in the context of privacy and data protection, especially under regulations such as the GDPR, which advocates for the “right to be forgotten.” Traditional machine learning embeds the training data into a model’s parameters, making selective data removal challenging. Machine unlearning addresses this by developing methods <a id="_idIndexMarker789"/>to diminish or reverse the influence of certain data points on the model, thus allowing for compliance with privacy laws and providing greater flexibility in data management. However, implementing this efficiently without compromising the model’s performance is a complex and ongoing area of research.</p>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor197"/>Hallucinations (Reliability and Safety)</h2>
			<p>The responsible AI principle <a id="_idIndexMarker790"/>that addresses the problem of hallucinations in AI models is typically “Reliability and Safety.” This principle focuses on ensuring that AI systems operate reliably and safely under a wide range of conditions and do not produce unintended, harmful, or misleading outcomes.</p>
			<p>Hallucinations in AI refer to instances where AI models generate false or nonsensical information, often because of training on noisy, biased, or insufficient data. Ensuring reliability and safety means rigorously testing AI systems to detect and mitigate such issues, ensuring that they perform as expected and do not produce erroneous outputs, such as hallucinations, which could lead to misinformation or harmful decisions. We have discussed ways to mitigate hallucinations by using prompt engineering, RAG techniques, and fine-tuning in <em class="italic">Chapters</em> <em class="italic">3</em>, <em class="italic">4</em>, and <em class="italic">5</em>.</p>
			<p>Additionally, the users must be educated on hallucination possibilities via generative AI applications. Additionally, the augmentation of source citations in LLM responses should be considered.</p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor198"/>Toxicity (Fairness and Inclusiveness)</h2>
			<p>Toxicity<a id="_idIndexMarker791"/> in AI can manifest as biased, offensive, or harmful outputs that may disproportionately affect certain groups based on race, gender, sexual orientation, or other characteristics. The responsible AI principle that specifically addresses toxicity in AI systems is “Fairness and Inclusiveness.” This principle ensures that AI systems do not perpetuate, amplify, or introduce biases and discriminatory practices, including the generation or reinforcement of toxic content.</p>
			<p>The following methods <a id="_idIndexMarker792"/>can be used to mitigate toxicity:</p>
			<ul>
				<li><strong class="bold">Diverse and representative data collection</strong>: Leverage large language models (LLMs) to generate a broad spectrum of training data, ensuring it encompasses various groups for a more inclusive representation. This approach helps minimize biases and mitigate toxic outputs.</li>
				<li><strong class="bold">Global annotator workforce</strong>: Engage a global team of human annotators from diverse races and backgrounds. Such human annotators provide comprehensive guidelines on accurately labeling training data, emphasizing the importance of inclusivity and unbiased judgment.</li>
				<li><strong class="bold">Proactive bias detection and remediation</strong>: Implement systematic processes to actively identify and address biases in AI systems. This ongoing effort is crucial to prevent and reduce instances of toxic behavior.</li>
				<li><strong class="bold">Inclusive design and rigorous testing</strong>: Involve a wide array of stakeholders in both the design and testing phases of AI systems. This inclusive approach is key to uncovering and addressing potential issues related to toxicity and bias early in the development process.</li>
				<li><strong class="bold">Supplemental guardrail models</strong>: Develop and train additional models specifically designed to filter out inappropriate or unwanted content. These models act as an extra layer of defense, ensuring the overall AI system maintains <a id="_idIndexMarker793"/>high standards of content quality and appropriateness.</li>
			</ul>
			<p>Additionally, the principle of “Transparency and Accountability” plays a role in addressing toxicity. By making AI systems more transparent, stakeholders can better understand how and why certain outputs are generated, which aids in identifying and correcting toxic behaviors. Accountability ensures that those who design and deploy AI systems are responsible for addressing any toxic outcomes.</p>
			<h1 id="_idParaDest-198"><a id="_idTextAnchor199"/>Rising Deepfake concern</h1>
			<p>Deepfake<a id="_idIndexMarker794"/> technology has become a rising concern in recent times, primarily due to advancements in AI and machine learning, making it easier and more convincing than ever before. These technological improvements have enabled the creation of highly realistic and difficult-to-detect fake videos and images. This growing realism and accessibility heighten the risks of misinformation, privacy violations, and the potential for malicious use in politics, personal attacks, and fraud. In this section, we will discuss what Deepfake is, some real-world examples, its detrimental impact on society, and what we can do to mitigate it.</p>
			<div><div><img src="img/B21443_09_2.jpg" alt="Figure 9.2 – A face covered by a wireframe, which is used to create Deepfake content"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – A face covered by a wireframe, which is used to create Deepfake content</p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor200"/>What is Deepfake?</h2>
			<p>Deepfake is a <a id="_idIndexMarker795"/>technology that uses artificial intelligence to create or alter video, images, and audio recordings, making it seem as if someone said or did something they did not. It typically involves manipulating someone’s likeness or voice.</p>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor201"/>Some real-world examples of Deepfake</h2>
			<p>The following<a id="_idIndexMarker796"/> are some early real-world examples of Deepfakes that have raised significant concerns and exacerbated the need for their prevention:</p>
			<ul>
				<li>In 2019, a UK-based energy firm’s CEO was tricked into transferring EUR 220,000 after receiving a phone call from what he believed was his boss. The caller used Deepfake technology to imitate the boss’s voice, convincing the CEO of the legitimacy of the request (<a href="https://www.forbes.com/sites/jessedamiani/2019/09/03/a-voice-Deepfake-was-used-to-scam-a-ceo-out-of-243000/?sh=4721eb412241">https://www.forbes.com/sites/jessedamiani/2019/09/03/a-voice-Deepfake-was-used-to-scam-a-ceo-out-of-243000/?sh=4721eb412241</a>).</li>
				<li>Edited videos and speeches have also been Deepfaked. For instance, a manipulated video of Facebook’s Mark Zuckerberg talking about the power of having billions of people’s data and a fake speech by Belgium’s prime minister linking the coronavirus pandemic to climate change are examples of Deepfake usage (<a href="https://www.cnn.com/2019/06/11/tech/zuckerberg-Deepfake/index.html">https://www.cnn.com/2019/06/11/tech/zuckerberg-Deepfake/index.html</a>).</li>
				<li>Concerns regarding the objectification of women due to Deepfake adult videos have been rising. The prevalence of AI-generated pornographic content that unlawfully uses the faces of women without their consent is increasingly troubling, particularly in the online world of notable influencers and streamers. This issue came to light in January when “Sweet Anita,” a prominent British live streamer with 1.9 million Twitch followers, discovered that a collection of fake explicit videos, which illegitimately featured the faces of various Twitch streamers, was being shared online. Sweet Anita is well-known on Twitch for her gaming content and interactive sessions with her audience  (<a href="https://www.nbcnews.com/tech/internet/Deepfake-twitch-porn-atrioc-qtcinderella-maya-higa-pokimane-rcna69372">https://www.nbcnews.com/tech/internet/Deepfake-twitch-porn-atrioc-qtcinderella-maya-higa-pokimane-rcna69372</a>).</li>
				<li>In early 2024, AI-generated Deepfake images of Taylor Swift, some of which were sexually explicit, spread across social media platforms, leading platforms such as X (formerly Twitter) to block searches for her name and renew calls for stronger AI legislation. The images, seen by millions, prompted actions from social media <a id="_idIndexMarker797"/>companies and discussions about the need for legal and regulatory responses to the misuse of AI technologies.</li>
			</ul>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor202"/>Detrimental effects on society</h2>
			<p>The following are <a id="_idIndexMarker798"/>some negative consequences of Deepfake that can have harmful effects on society:</p>
			<ul>
				<li><strong class="bold">Misinformation and erosion of trust</strong>: Deepfakes can create highly convincing but false representations of individuals saying or doing things they never did, leading to misinformation and eroding public trust in media and institutions. For example, Deepfakes have been used to create fake videos of politicians, which can mislead voters and disrupt democratic processes.</li>
				<li><strong class="bold">Exploitation and harassment</strong>: Deepfakes can be used to create non-consensual explicit content or defamatory material, targeting individuals for harassment or blackmail. There have been instances where Deepfake technology was used to superimpose faces of celebrities or private individuals onto explicit content without their consent, causing personal distress and reputational damage.</li>
				<li><strong class="bold">Security threats</strong>: Deepfakes pose a security threat by enabling fraud and impersonation. They can be used to mimic voices or faces to bypass biometric security measures or to create convincing scams. An example was provided earlier, regarding a real-world case, where Deepfakes were used to mimic a CEO’s voice to trick a manager into transferring a significant sum of money, as reported by Forbes.</li>
				<li><strong class="bold">Legal and ethical challenges</strong>: The rise of Deepfakes creates legal and ethical dilemmas, challenging existing laws on consent, privacy, and free speech. Technology blurs the line between truth and fiction, making it difficult to discern real from fake and raising questions about the legality of such content creation.</li>
			</ul>
			<p>In my opinion, the biggest threat to human lives is a nuclear war between countries that can lead to suffering and death on a ginormous scale. Imagine a scenario where a Deepfake video <a id="_idIndexMarker799"/>falsely shows a world leader declaring war or making inflammatory statements, leading to international tensions or even conflicts. This highlights the potential geopolitical impact of Deepfakes when used maliciously and the need for education on how to spot Deepfakes and other mitigation strategies.</p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor203"/>How to spot a Deepfake</h2>
			<p>The identification of <a id="_idIndexMarker800"/>Deepfake is an area of growing research. Here, we mention a few techniques you can use to identify Deepfake content:</p>
			<ul>
				<li><strong class="bold">Facial inconsistencies</strong>: Look for anomalies in facial expressions, such as awkward blinking, unusual lip movements, or facial features that appear distorted or don’t align correctly.</li>
				<li><strong class="bold">Audio-visual mismatch</strong>: Check for mismatches between the audio and visual elements. For example, the voice may not sync perfectly with the lip movements, or the tone and accent might not match the person’s known speech patterns.</li>
				<li><strong class="bold">Unnatural skin tone or texture</strong>: Deepfakes may exhibit issues with skin tone or texture. This can include overly smooth skin, a lack of natural blemishes, or inconsistent lighting on the face compared to the surroundings.</li>
				<li><strong class="bold">Background anomalies</strong>: Pay attention to the background of the video. Look for strange artifacts, inconsistencies in lighting, or other elements that seem out of place or distorted.</li>
				<li><strong class="bold">Lack of blinking or excessive blinking</strong>: In early Deepfakes, the blinking was often irregular or missing. Although newer Deepfakes have improved, anomalies in blinking can still be a giveaway.</li>
				<li><strong class="bold">Use of detection software</strong>: There are various software tools and apps designed to detect Deepfakes by analyzing videos for subtle inconsistencies that are not easily noticeable to the human eye. Popular Deepfake detection tools include <a id="_idIndexMarker801"/>products <a id="_idIndexMarker802"/>from Sentinel (<a href="https://thesentinel.ai/">https://thesentinel.ai/</a>) and Intel’s FakeCatcher.</li>
				<li><strong class="bold">Checking source credibility</strong>: Verify the source of the video or audio. If it comes from an unverified or suspicious source, it warrants further scrutiny.</li>
			</ul>
			<h2 id="_idParaDest-203"><a id="_idTextAnchor204"/>Mitigation strategies</h2>
			<p>In this section, we <a id="_idIndexMarker803"/>will explore several key mitigation strategies to tackle the risks associated with Deepfake technology. Understanding these techniques is a crucial aspect of leadership education, equipping leaders, as well as the general public, with the necessary tools to address and counter the challenges posed by this advanced technology:</p>
			<ul>
				<li><strong class="bold">Public awareness and education</strong>: Educating the public about the existence and potential misuse of Deepfakes can make people more critical of the media they consume. This can include campaigns to raise awareness about how to spot Deepfakes, which we have discussed in the earlier section.</li>
				<li><strong class="bold">Deepfake detection technologies</strong>: Developing and implementing advanced detection algorithms that can identify Deepfakes is crucial. These technologies often use machine learning to analyze videos or audio for inconsistencies or anomalies that are not perceptible to the human eye. Some popular Deepfake detection tools include Sentinel and Intel’s Deepfake detector tool.</li>
				<li><strong class="bold">Legal and regulatory measures</strong>: Governments and regulatory bodies can enact laws and regulations to penalize the creation and distribution of malicious Deepfakes. This includes defining legal frameworks that address consent, privacy, and the misuse of Deepfake technology. US President Biden’s office published an Executive Order (EO) on Oct. 30, 2023, which is a major step toward implementing safety standards and regulations in AI. We will discuss this EO in the upcoming section.</li>
				<li><strong class="bold">Blockchain and digital watermarking</strong>: Implementing technologies such as blockchain and digital watermarking can help verify the authenticity of digital content. This can create a traceable, tamper-evident record of the media, ensuring its integrity. For instance, in August 2023, Google’s DeepMind launched a watermarking tool for AI-generated images. In November 2023, Google<a id="_idIndexMarker804"/> reported that they would be using inaudible watermarks in its AI-generated music, so it’s possible to detect if Google’s AI tech has been used in the creation of a track (<a href="https://www.theverge.com/2023/11/16/23963607/google-deepmind-synthid-audio-watermarks">https://www.theverge.com/2023/11/16/23963607/google-deepmind-synthid-audio-watermarks</a>).</li>
				<li><strong class="bold">Platform responsibility</strong>: Social media platforms and content distributors play a crucial role and should implement policies and algorithms to detect and remove Deepfake content from their platforms. In November 2023, Meta announced that they would be implementing strict policies that would require political advertisers to flag AI-generated content as a step towards mitigating the proliferation of misinformation through Deepfakes.<p class="list-inset">By combining these strategies, society can better mitigate the risks associated with Deepfake technology, protecting individuals and maintaining trust in digital media.</p></li>
			</ul>
			<p>Deepfake detection is a rapidly expanding field of research, primarily driven by advancements in generative adversarial networks (GANs). These sophisticated AI algorithms consist of two parts: the generator, which is responsible for creating synthetic data, and the discriminator, which assesses its authenticity. The discriminator’s role is particularly crucial in Deepfake detection. As the cutting-edge in producing realistic fake images and videos, understanding and analyzing the discriminator aspect of GANs is pivotal for developing effective strategies to identify and counter Deepfake content. The deeper our grasp of GAN mechanisms, the more adept we become at crafting systems capable of detecting the increasingly intricate Deepfakes they generate. While delving into the intricacies of GANs is beyond the scope of this book, we strongly recommend monitoring developments in this field, as they are likely to play a significant role in shaping future Deepfake detection techniques.</p>
			<h1 id="_idParaDest-204"><a id="_idTextAnchor205"/>Building applications using a responsible 
AI-first approach</h1>
			<p>In this section, we <a id="_idIndexMarker805"/>will explore the development of generative AI applications with a responsible AI-first approach. In <a href="B21443_06.xhtml#_idTextAnchor117"><em class="italic">Chapter 6</em></a>, we delved into the lifecycle of large language models (LLMs); however, we will now examine<a id="_idIndexMarker806"/> this through the lens of responsible AI. We aim to discuss how to integrate these principles into the various stages of development, namely ideating/exploring, building/augmenting, and operationalizing. Achieving this integration demands tight collaboration among research, compliance, and engineering teams, effectively bringing people, processes, and technology together. This ensures ethical data use, eliminating biases from LLM responses and safety and maintaining transparency from the initial design stage to deployment and production and beyond. Continuous monitoring and observability post-deployment ensure these models remain relevant and ethically compliant over time.</p>
			<div><div><img src="img/B21443_09_3.jpg" alt="Figure 9.3 – LLM Application Development Lifecycle"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – LLM Application Development Lifecycle</p>
			<p class="IMG---Figure">We have <a id="_idIndexMarker807"/>already discussed the <strong class="bold">Large Language Model Application Development Lifecycle</strong> (<strong class="bold">LLMADL</strong>), as shown in <a href="B21443_06.xhtml#_idTextAnchor117"><em class="italic">Chapter 6</em></a>. Therefore, we won’t delve into its details again. The following image illustrates the mitigation layers in the application and platform layers, which are essential for building a safe AI system. In this section, we will explore how we <a id="_idIndexMarker808"/>can incorporate these mitigation layers into the LLMADL process:</p>
			<div><div><img src="img/B21443_09_4.jpg" alt="Figure 9.4 – Mitigation layers of gen AI applications"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 – Mitigation layers of gen AI applications</p>
			<h2 id="_idParaDest-205"><a id="_idTextAnchor206"/>Ideating/exploration loop</h2>
			<p>The first loop<a id="_idIndexMarker809"/> involves <a id="_idIndexMarker810"/>ideation and exploration, focusing on identifying a use case, formulating hypotheses, selecting appropriate LLMs, and creating prompt variants that adhere to safety and ethical standards. This stage emphasizes the importance of aligning the LLM’s use case with ethical guidelines to prevent bias or harm. For example, in developing an LLM-powered chatbot for mental health support, it’s crucial to use diverse and inclusive datasets, avoid stereotypes and biases, and implement mechanisms to prevent harmful advice. Hypotheses formulated during this phase should prioritize fairness, accountability, transparency, and ethics, such as ensuring balanced and fair responses by training the LLM with datasets that have equal representation of gender and minority group dialogues:</p>
			<ul>
				<li><strong class="bold">Model Layer</strong>: The <a id="_idIndexMarker811"/>decision to implement a mitigation layer in the model layer is made at this stage. This process includes identifying models that comply with RAI principles. Often, these safety mitigations are incorporated into models through fine-tuning and reinforcement learning from human feedback (RLHF); additionally, some benchmarks can provide guidance in making this decision. We covered RLHF and benchmarks in <a href="B21443_03.xhtml#_idTextAnchor052"><em class="italic">Chapter 3</em></a>, highlighting them as potent techniques for developing<a id="_idIndexMarker812"/> models that are honest, helpful, and harmless. For instance, a benchmark holistic evaluation of language models (HELMs) from Stanford Research evaluates models for different tasks using<a id="_idIndexMarker813"/> seven key metrics: <strong class="bold">accuracy</strong>, <strong class="bold">calibration</strong>, <strong class="bold">robustness, fairness</strong>, <strong class="bold">bias</strong>, <strong class="bold">toxicity</strong>, and <strong class="bold">efficiency</strong>. Metrics for different models can be found using the following link; these can be a potential first step in the initial assessment when shortlisting models based on RAI principles: <a href="https://crfm.stanford.edu/helm/classic/latest/#/leaderboard">https://crfm.stanford.edu/helm/classic/latest/#/leaderboard</a>. Model cards associated with LLMs provided<a id="_idIndexMarker814"/> by <strong class="bold">Hugging Face</strong> and also <strong class="bold">Azure AI Model Catalog</strong> can <a id="_idIndexMarker815"/>also help you do your initial RAI assessment.</li>
				<li><strong class="bold">Safety system</strong>: For <a id="_idIndexMarker816"/>many applications, depending solely on the safety mechanisms integrated within the model is insufficient. Large language models can make errors and are vulnerable to attacks, such as jailbreak attempts. Hence, it is important to implement a robust content filtering system in your application to prevent the generation and dissemination of harmful or biased content. Once this safety system is activated, it becomes crucial to apply the red team testing approaches featuring human involvement, as outlined in <a href="B21443_08.xhtml#_idTextAnchor163"><em class="italic">Chapter 8</em></a>. This is to guarantee the robustness of this security layer and its freedom from vulnerabilities. Red teaming specialists play a vital role in detecting potential harm and subsequently facilitate deployment of measurement strategies to confirm the effectiveness of the implemented mitigations.</li>
				<li><strong class="bold">Azure Content Safety</strong> is a <a id="_idIndexMarker817"/>content filtering application that can help you detect and filter out toxic user-generated or AI-generated content, which could be text or<a id="_idIndexMarker818"/> images. It can also provide protection from jailbreaking attempts. Additionally, it can provide severity levels in terms of toxicity along with categorizations such as violence, self-harm, sexual, and hate. You can also enable batch evaluations of large datasets of prompts and completions for your applications. For example, as seen in <em class="italic">Figure 9</em><em class="italic">.4</em>, when testing the prompt Painfully twist his arm and then punch him in<a id="_idIndexMarker819"/> the face, the content was rejected because of the strong filter set out on the right side to filter out violent content.</li>
			</ul>
			<div><div><img src="img/B21443_09_5.jpg" alt="Figure 9.5 – Results from Azure content safety"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 – Results from Azure content safety</p>
			<h2 id="_idParaDest-206"><a id="_idTextAnchor207"/>Building/augmenting loop</h2>
			<p>This <a id="_idIndexMarker820"/>stage is part <a id="_idIndexMarker821"/>of the second loop. After the team identifies the desired models, in this stage, the goal is to tailor the models based on business requirements through prompt engineering and grounding the data:</p>
			<ul>
				<li><strong class="bold">Metaprompting and grounding</strong>: As outlined in <a href="B21443_05.xhtml#_idTextAnchor098"><em class="italic">Chapter 5</em></a>, prompt engineering <a id="_idIndexMarker822"/>and metaprompts can enhance retrieval accuracy. At this stage, it’s important to incorporate metaprompts that address four key components: harmful content, grounding, copyright issues, and jailbreaking prevention to improve safety. We have already explored these metaprompt components with examples in <a href="B21443_05.xhtml#_idTextAnchor098"><em class="italic">Chapter 5</em></a>, so we will not delve into details here. However, this area is <a id="_idIndexMarker823"/>continuously evolving, and you can expect to see more templates emerge over time. When addressing grounding, it’s crucial to ensure that the data retrieved from Vector DB complies with responsible AI principles. This means not only should the data be unbiased, but there should also be transparency regarding the sources of data utilized in the retrieval system, ensuring they are ethically sourced. In the case of customer data, data privacy is accorded the highest priority.</li>
				<li><strong class="bold">Evaluation</strong>: It is <a id="_idIndexMarker824"/>important to evaluate LLM models before deploying into production. Metrics such as groundedness, relevance, and retrieval score can help you determine the performance of models. Additionally, you can create custom metrics with LLMs such as GPT-4 and use them to evaluate your models. Azure Prompt Flow helps you achieve this with out-of-the-box metrics and also enables you create custom metrics. The following figure captures a snapshot from an experiment carried out using Prompt Flow, along with the associated evaluation scores. <em class="italic">Figure 9</em><em class="italic">.6</em> offers a visualization of the test conducted on an evaluation dataset. The LLM responses were assessed against the actual answers, and an average rating of 4 or higher for groundedness, the retrieval score, and relevance suggests that the application is performing effectively:</li>
			</ul>
			<div><div><img src="img/B21443_09_6.jpg" alt="Figure 9.6 – Azure Prompt Flow evaluation metrics (visualization)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6 – Azure Prompt Flow evaluation metrics (visualization)</p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor208"/>Operationalizing/deployment loop</h2>
			<p>This stage <a id="_idIndexMarker825"/>marks the final loop, transitioning from development into production, and includes designing <a id="_idIndexMarker826"/>monitoring processes that continuously evaluate metrics. These metrics provide a clearer indication of specific types of drifts. For instance, the model’s groundedness could diminish over time if the data were grounded or become outdated. This phase also involves integrating continuous integration/continuous deployment (CI/CD) processes to facilitate automation. Additionally, collaboration with the user experience (UX) team is crucial to ensure the creation of a safe user experience:</p>
			<ul>
				<li><strong class="bold">User experience</strong>: In<a id="_idIndexMarker827"/> this layer, incorporating a human feedback loop to assess the responses of LLM models is crucial. This can be achieved through simple mechanisms such as a thumbs up and thumbs down system. Additionally, setting up predefined responses for inappropriate inquiries adds significant value. For instance, if a user enquires about constructing a bomb, the system automatically intercepts this and delivers a preset response. Furthermore, offering a prompt guide that integrates RAI principles and includes citations with responses is an effective strategy to guarantee the reliability of the responses.</li>
				<li><strong class="bold">Monitoring</strong>: Continuous<a id="_idIndexMarker828"/> model monitoring is a crucial component of LLMOps, guaranteeing that AI systems stay pertinent in the face of changing societal norms and data trends over time. Azure Prompt Flow offers advanced tools for monitoring the safety and performance of your application in a production environment. This setup facilitates straightforward monitoring using predefined metrics such as groundedness, relevance, coherence, fluency, and similarity or custom metrics relevant to your use case. We have already conducted a lab in <a href="B21443_04.xhtml#_idTextAnchor070"><em class="italic">Chapter 4</em></a>, focusing on evaluating RAG workflows where we discussed these metrics.</li>
			</ul>
			<p>Throughout all these stages, it’s important to engage with stakeholders, including diverse user groups, to understand the impact of the LLM and to ensure that it’s being used responsibly. Additionally, documenting the processes and decisions made at each stage for accountability and transparency is a key part of responsible AI practices.</p>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor209"/>Role of AI architects and leadership</h2>
			<p>AI architects and leaders play <a id="_idIndexMarker829"/>a pivotal role in building responsible AI practices within an organization. Their actions and decisions<a id="_idIndexMarker830"/> can set the tone for how AI is developed, deployed, and managed. Here are some key roles and actions they can take:</p>
			<ul>
				<li><strong class="bold">Establishing ethical guidelines and standards</strong>: Architects and leaders should develop and enforce ethical guidelines for AI development and use within the organization. This includes principles around fairness, transparency, privacy, and accountability.</li>
				<li><strong class="bold">Promoting transparency and explainability</strong>: They should advocate for transparency in AI systems, ensuring that stakeholders understand how AI decisions are made. This involves promoting the development of explainable AI models.</li>
				<li><strong class="bold">Ensuring data privacy and security</strong>: Leaders must prioritize data privacy and security, implement robust policies and practices to protect sensitive information, and comply with relevant data protection regulations.</li>
				<li><strong class="bold">Fostering an inclusive and diverse AI culture</strong>: Encouraging diversity in AI teams and in datasets is crucial. Diverse perspectives help to reduce biases in AI systems and make them more equitable.</li>
				<li><strong class="bold">Implementing continuous monitoring and evaluation</strong>: Regularly monitoring AI systems for performance, fairness, and unintended consequences is essential. Leaders should establish protocols for the ongoing evaluation and auditing of AI systems.</li>
				<li><strong class="bold">Investing in responsible AI education and training</strong>: Providing training and<a id="_idIndexMarker831"/> resources for employees on responsible AI practices helps to create a culture of ethical AI use. This includes educating teams about potential biases and how to mitigate them.</li>
				<li><strong class="bold">Encouraging collaboration and stakeholder engagement</strong>: Engaging with various stakeholders, including users, ethicists, and industry experts, can provide diverse insights into the potential impacts of AI solutions.</li>
				<li><strong class="bold">Risk assessment and management</strong>: Conducting thorough risk assessments to understand the potential negative impacts of AI and implementing strategies to mitigate these risks is vital.</li>
				<li><strong class="bold">Creating accountability structures</strong>: Setting up clear lines of accountability within the organization for AI decision-making helps to maintain ethical standards and address any issues that arise.</li>
				<li><strong class="bold">Promoting sustainable AI practices</strong>: Ensuring that AI practices are sustainable and do not adversely affect the environment or society is an important consideration.</li>
				<li><strong class="bold">Supporting regulation and compliance</strong>: Keeping abreast of and complying with international, national, and industry-specific AI regulations and standards is crucial for responsible AI deployment.</li>
			</ul>
			<p>By taking these<a id="_idIndexMarker832"/> actions, architects and leaders can guide their organizations toward responsible AI practices, ensuring that <a id="_idIndexMarker833"/>AI technologies are used in a way that is ethical, fair, reliable, inclusive, safe, secure, and beneficial for all stakeholders.</p>
			<h1 id="_idParaDest-209"><a id="_idTextAnchor210"/>AI, the cloud, and the law – understanding compliance and regulations</h1>
			<p>In this section, we<a id="_idIndexMarker834"/> will discuss compliance in the context of building AI solutions on the cloud responsibly, as it ensures that AI systems align with legal, ethical, and societal norms. Compliance acts <a id="_idIndexMarker835"/>as a safeguard against risks such as bias, privacy breaches, and unintended consequences, fostering trust among users and stakeholders. It promotes transparency and accountability in AI operations, encouraging the adoption of best practices and standardization across the industry. Moreover, by addressing public concerns and anticipating future challenges, compliance discussions help in shaping AI technologies that are not only technologically advanced but also socially responsible and beneficial. This is particularly important in a global context where AI’s impact crosses borders and cultural divides.</p>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor211"/>Compliance considerations</h2>
			<p>When<a id="_idIndexMarker836"/> architecting generative AI solutions on the cloud, there are several compliance considerations to keep in mind:</p>
			<ul>
				<li><strong class="bold">Data privacy regulations</strong>: These comply with global data protection laws such as GDPR (Europe), CCPA (California), and others, depending on the geographical location and scope of your service or industry. The <strong class="bold">General Data Protection Regulation</strong> (<strong class="bold">GDPR</strong>) is a comprehensive data protection law in the European Union that sets guidelines for the collection and processing of personal information from individuals in the EU. Adhering to GDPR is crucial, as it ensures the protection of personal data, builds trust with customers, and avoids significant fines for non-compliance, thereby maintaining a company’s reputation and legal standing in the global market. The <strong class="bold">California Consumer Privacy Act</strong> (<strong class="bold">CCPA</strong>) is a state statute in California, USA, designed to enhance privacy rights and consumer protection for residents of California. Adhering to CCPA laws is important because it ensures compliance with California’s stringent privacy regulations, builds consumer trust by protecting personal data, and helps avoid significant financial penalties for non-compliance.</li>
				<li><strong class="bold">Industry-specific regulations</strong>: Some examples of industry-specific regulations are <strong class="bold">Health Insurance Portability and Accountability Act</strong> (<strong class="bold">HIPAA</strong>) for healthcare data in the US and Canada, <strong class="bold">Payment Card Industry Data Security Standard</strong> (<strong class="bold">PCI DSS</strong>) for payment card information, and FERPA for educational records. <strong class="bold">FERPA</strong> stands for the <strong class="bold">Family Educational Rights and Privacy Act</strong>. It’s a US federal law that protects the privacy of student education records and gives parents specific rights with respect to their children’s education records.</li>
				<li><strong class="bold">Service organization control (SOC) reports</strong>: Ensure compliance with SOC 2, which focuses on security, availability, processing integrity, confidentiality, and the<a id="_idIndexMarker837"/> privacy of a system. SOC 2 compliance is more about trust and assurance than legal obligation, but its implications are significant in terms of security, business relationships, and overall reputation in the market.</li>
				<li><strong class="bold">Cloud security measures</strong>: Cloud solutions must be secure to protect sensitive data against breaches. This involves enabling encryption, access controls, and regular security audits.</li>
				<li><strong class="bold">Auditability and reporting</strong>: Being able to track and report on how the AI system makes decisions can be important for regulatory compliance and transparency.</li>
				<li><strong class="bold">Data localization/residency laws</strong>: Some jurisdictions require that data be stored within the country of origin, which can affect cloud service choices and architecture.</li>
				<li><strong class="bold">Business continuity and disaster recovery</strong>: Adhere to standards that ensure business continuity and disaster recovery, such as ISO/IEC 22301.</li>
			</ul>
			<p>Top cloud providers, such as Microsoft, have a robust compliance portfolio to assist their customers. They<a id="_idIndexMarker838"/> provide necessary tools such as Microsoft Purview and comprehensive documentation to aid customers on their compliance journey. For a full list, we recommend checking out the compliance offerings from Microsoft here: <a href="https://learn.microsoft.com/en-us/compliance/regulatory/offering-home">https://learn.microsoft.com/en-us/compliance/regulatory/offering-home</a>.</p>
			<h2 id="_idParaDest-211"><a id="_idTextAnchor212"/>Global and United States AI regulatory landscape</h2>
			<p>The current <a id="_idIndexMarker839"/>global AI regulatory landscape is marked by diverse approaches and emerging trends. Accelerating capabilities in AI, including large language models, facial recognition, and advanced cognitive processing, have propelled AI regulation to prominence among policy-makers.</p>
			<p>Europe has been the frontrunner in this journey towards AI regulation. The EU Act has made significant progress towards becoming law, with unanimous approval from EU member states as of February 2, 2024. It sets a global standard for AI technology, emphasizing a balance between innovation and safety. The EU AI Act introduces a nuanced regulatory framework for artificial intelligence, categorizing AI systems based on their risk levels to ensure appropriate oversight. Systems posing an “<strong class="bold">unacceptable risk</strong>,” such as those capable of cognitive manipulation or implementing social scoring based on certain protected traits, biometric identification, and the categorization of people, are outright banned, with narrow exceptions for law enforcement under stringent conditions. “<strong class="bold">High-risk”</strong> AI systems, impacting safety or fundamental rights, are subject to strict assessment and registration requirements, covering a wide range of applications from critical infrastructure management, assistance in legal interpretation, and education to law enforcement. Meanwhile, “general purpose and generative AI,” such as ChatGPT, must adhere to transparency directives, including the disclosure of AI-generated content and measures against illegal and toxic content production and publishing summaries of copyrighted data used for training. Systems deemed “<strong class="bold">limited risk”</strong> should comply with minimal transparency requirements. This includes applications with image, audio, or video generation models, facilitating informed decisions by users. This stratified approach aims to balance the innovation potential of AI with necessary safeguards against its potential harms (<a href="https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence">https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence</a>).</p>
			<p>Conversely, India initially opted against AI regulation, focusing on policy and infrastructure to foster AI growth, but later considered a regulatory framework addressing algorithm biases and copyrights. The US hasn’t moved towards comprehensive federal AI legislation but has seen regulatory responses from agencies such as the National Institute of Standards and Technology (NIST), the Federal Trade Commission (FTC), and the Food and Drug Administration (FDA) regarding public concerns over AI technologies.</p>
			<p>Regulatory <a id="_idIndexMarker840"/>frameworks are developing globally to balance AI’s benefits against its risks. EY’s analysis of eight jurisdictions (Canada, China, EU, Japan, Korea, Singapore, UK, and the US) reflects a variety of regulatory approaches. The rules and policy initiatives were inspired by the OECD’s Organization for Economic Co-operation AI policy Observatory.</p>
			<p>OECD is an international organization comprising 38 member countries, established to promote economic progress and world trade by offering a platform for democratic, market-economy nations to discuss policies, share experiences, and co-ordinate on global issues.</p>
			<p>As per this research from Ernst and Young, released in September 2023, five common regulatory trends have emerged globally:</p>
			<ul>
				<li><strong class="bold">Alignment with key AI principles</strong>: The AI regulation and guidance being evaluated align with the key AI principles of human rights for respect, sustainability, transparency, and robust risk management, as established by the OECD and supported by the G20. The Group of Twenty (G20) is an international forum of 19 countries and the European Union focused on addressing global economic issues and representing the world’s major economies.</li>
				<li><strong class="bold">Risk-based approach</strong>: These jurisdictions adopt a risk-based approach to AI regulation, meaning they customize their AI rules based on the perceived risks AI poses to fundamental values such as privacy, non-discrimination, transparency, and security.</li>
				<li><strong class="bold">Sector and sector-agnostic rules</strong>: Due to the diverse applications of AI, certain jurisdictions are emphasizing the importance of sector-specific regulations alongside more general, sector-agnostic rules.</li>
				<li><strong class="bold">Digital priority areas</strong>: In the realm of other digital priority areas such as cybersecurity, data privacy, and intellectual property rights, jurisdictions are advancing in their creation of AI-specific regulations, with the European Union leading in adopting a comprehensive strategy.</li>
				<li><strong class="bold">Collaboration with private sector and policy-makers</strong>: Numerous jurisdictions employ regulatory sandboxes, allowing private sector collaboration with policy-makers to craft rules that both ensure safe, ethical AI and address the potential<a id="_idIndexMarker841"/> need for closer oversight in higher-risk AI innovations.</li>
			</ul>
			<h2 id="_idParaDest-212"><a id="_idTextAnchor213"/>Biden Executive Order on AI</h2>
			<p>On October 30, 2023, President Joe Biden issued an Executive Order, which we think is a major step<a id="_idIndexMarker842"/> towards regulating AI in the United States. The Executive Order is thoroughly comprehensive, simultaneously ensuring human safety and responsible AI use while fostering fair competition within the country and advancing leadership on the global stage. There are eight major topics that the EO covers:</p>
			<ul>
				<li><strong class="bold">New standards for AI safety and security</strong>: The Executive Order requires developers of powerful AI systems to share safety test results with the US government. It establishes standards and tests to ensure AI systems are safe and secure before public release, addresses risks in using AI for biological materials, and combats AI-enabled fraud and deception. An advanced cybersecurity program will also be developed to leverage AI in securing software and networks. It directs the National Security Council and White House Chief of Staff to develop a National Security Memorandum, guiding further AI and security actions, ensuring the US military and intelligence community’s safe, ethical, and effective use of AI, and outlining measures to counter adversaries’ military AI applications.</li>
				<li><strong class="bold">Protecting Americans’ privacy</strong>: The order emphasizes protecting privacy by accelerating the development and use of privacy-preserving techniques in AI. It includes funding research for privacy technologies and developing guidelines for federal agencies to evaluate the effectiveness of these techniques, especially in AI systems.</li>
				<li><strong class="bold">Advancing equity and civil rights</strong>: This addresses the responsible principles of fairness and inclusiveness. To combat discrimination and bias in AI, the order provides guidance to landlords and federal programs, addresses algorithmic discrimination through training and technical assistance, and aims to ensure fairness in the criminal justice system through the development of best practices in AI use.</li>
				<li><strong class="bold">Standing up for consumers, patients, and students</strong>: This includes advancing responsible AI use in healthcare, such as developing affordable drugs and <a id="_idIndexMarker843"/>establishing a safety program for healthcare practices involving AI. It also involves creating resources to support educators using AI-enabled educational tools.</li>
				<li><strong class="bold">Supporting workers</strong>: The order directs the development of principles and best practices to maximize AI benefits for workers, addressing issues such as job displacement, labor standards, and workplace equity. It also includes producing a report on AI’s potential impact on the labor market.</li>
				<li><strong class="bold">Promoting innovation and competition</strong>: Actions include catalyzing AI research nationwide, promoting a competitive AI ecosystem by providing resources to small developers, and expanding the ability of skilled immigrants to work in the US in AI-related fields.</li>
				<li><strong class="bold">Advancing American leadership abroad</strong>: The administration will work with other nations to support the global deployment and use of safe and trustworthy AI. This involves expanding engagements to collaborate on AI, developing AI standards with international partners, and promoting responsible AI development to address global challenges.</li>
				<li><strong class="bold">Ensuring responsible and effective governmental use of AI</strong>: The order aims to modernize federal AI infrastructure and ensure responsible AI deployment in government. This includes issuing guidance for AI use in agencies, accelerating the hiring of AI professionals, and providing AI training to government employees.</li>
			</ul>
			<p>In summary, while <a id="_idIndexMarker844"/>compliance plays a pivotal role in fostering safer and more responsible AI systems, it can indeed be a double-edged sword. Excessive compliance requirements might stifle innovation, potentially hindering a country’s competitive edge on the global stage. Therefore, it’s imperative that regulators are well-informed and engage in thorough consultations with AI experts when crafting regulations and standards. This balanced approach ensures that AI develops in a safe and ethical manner while still allowing for the flexibility and creativity necessary for technological advancement and competitive success.</p>
			<h1 id="_idParaDest-213"><a id="_idTextAnchor214"/>Startup ecosystem in RAI</h1>
			<p>In this section, we<a id="_idIndexMarker845"/> will discuss a few notable startups emerging in the responsible AI space and building products that keep RAI at their core.</p>
			<ul>
				<li><strong class="bold">Parity AI</strong>: Founded <a id="_idIndexMarker846"/>by Rumman Chowdhury, Parity AI <a id="_idIndexMarker847"/>focuses on AI risk management and offers tools for auditing AI models for bias or legal compliance and provides recommendations for addressing <a id="_idIndexMarker848"/>these issues (<a href="https://www.get-parity.com/">https://www.get-parity.com/</a>).</li>
				<li><strong class="bold">Fiddler</strong>: Founded <a id="_idIndexMarker849"/>by Krishna Gade, Fiddler focuses<a id="_idIndexMarker850"/> on explainability in AI, helping to make AI model decisions more transparent. It aids data science teams in monitoring their models’ performance and generating executive summaries from the outcomes. If a model’s accuracy declines or displays bias, Fiddler assists in identifying the reasons. Gade views model monitoring and enhancing clarity as key initial steps for more deliberate AI development and <a id="_idIndexMarker851"/>deployment (<a href="https://www.fiddler.ai/ai-observability">https://www.fiddler.ai/ai-observability</a>).</li>
				<li><strong class="bold">Arthur</strong>: Founded in 2019, Arthur is a company specializing in AI performance, assisting <a id="_idIndexMarker852"/>enterprise <a id="_idIndexMarker853"/>clients in maximizing their AI’s potential through performance monitoring and optimization, providing explainability, and mitigating bias.</li>
				<li><strong class="bold">Weights and Biases</strong>: Founded in 2017, Weights and Biases focuses on the reproducibility aspect of <a id="_idIndexMarker854"/>machine<a id="_idIndexMarker855"/> learning model experiments. In my opinion, reproducibility is vital in AI because it forms the bedrock of scientific trust and validation. It <a id="_idIndexMarker856"/>allows for the independent verification of results, facilitating the correction of errors and building upon research findings. Crucially, in the context of AI’s rapid transition from research to real-world applications, reproducibility ensures that AI models are robust, unbiased, and safe. It also helps address the AI ‘black-box’ problem by allowing a broader understanding of how models function. This is particularly important in high-stakes areas such as healthcare, law enforcement, and public interaction, where AI’s impact is direct and significant.</li>
				<li><strong class="bold">Datagen</strong>: Datagen<a id="_idIndexMarker857"/> specializes in computer<a id="_idIndexMarker858"/> vision and facial data, ensuring their datasets are varied in terms of skin tones, hairstyles, genders, and angles to reduce bias in facial recognition<a id="_idIndexMarker859"/> technology (<a href="https://datagen.tech/">https://datagen.tech/</a>).</li>
				<li><strong class="bold">Galileo and Snorkel AI</strong>: Galileo and Snorkel AI<a id="_idIndexMarker860"/> focus<a id="_idIndexMarker861"/> on maintaining high data quality; Galileo does this by automatically adjusting biases in unstructured data, whereas Snorkel AI ensures equitable, automated labeling, along with data versioning and audit <a id="_idIndexMarker862"/>services (<a href="https://www.rungalileo.io/">https://www.rungalileo.io/</a>,<a href="https://snorkel.ai/">https://snorkel.ai/</a>).</li>
			</ul>
			<p>The preceding list is not exhaustive. This space is evolving, and there are numerous new start-ups making significant inroads in this field.</p>
			<div><div><img src="img/B21443_09_7.jpg" alt="Figure 9.7 – Start-up ecosystem in RAI"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7 – Start-up ecosystem in RAI</p>
			<p>The preceding<a id="_idIndexMarker863"/> figure, referenced from BGV (<a href="https://benhamouglobalventures.com/ai-ethics-boom-150-ethical-ai-startups-industry-trends/">https://benhamouglobalventures.com/ai-ethics-boom-150-ethical-ai-startups-industry-trends/</a>), shows a few notable start-ups providing ethical AI services across five categories: data privacy, AI monitoring and observability, AI audits, governance, risk, compliance, targeted AI solutions and technologies, and open source solution.</p>
			<h1 id="_idParaDest-214"><a id="_idTextAnchor215"/>Summary</h1>
			<p>To summarize, the development of more sophisticated AI systems and the journey towards achieving <strong class="bold">artificial general intelligence</strong> (<strong class="bold">AGI</strong>) necessitates a steadfast commitment to RAI principles. Neglecting these principles could result in AI posing significant risks to humanity. In this chapter, we delved deeply into responsible AI principles, uncovering their theoretical and practical implications, especially within the realms of LLMs and Deepfake technology. We highlighted the importance of ethical vigilance and the role of architecture and leadership in guiding AI towards beneficial applications, alongside an analysis of the current regulatory landscape shaping AI’s evolution. Our exploration extended to responsible AI tools and the dynamic startup ecosystem, emphasizing how new companies are both influencing and adapting to these AI trends. These insights are crucial, as they equip us with the knowledge to harness AI’s power responsibly, ensuring its alignment with ethical standards and societal benefits. Looking ahead, in the final chapter, we will discuss the future of ChatGPT, where we’ll delve into emerging trends and potential advancements, highlighting innovative uses that are set to redefine our interaction with AI and society.</p>
			<h1 id="_idParaDest-215"><a id="_idTextAnchor216"/>References</h1>
			<ul>
				<li>AI is sending people to jail—and getting it wrong: <a href="https://www.technologyreview.com/2019/01/21/137783/algorithms-criminal-justice-ai/">https://www.technologyreview.com/2019/01/21/137783/algorithms-criminal-justice-ai/</a></li>
				<li>Thousands of Dollars for Something I Didn’t Do: <a href="https://www.nytimes.com/2023/03/31/technology/facial-recognition-false-arrests.html?login=ml&amp;auth=login-ml">https://www.nytimes.com/2023/03/31/technology/facial-recognition-false-arrests.html?login=ml&amp;auth=login-ml</a></li>
				<li>Can the criminal justice system’s AI be truly fair?: <a href="https://tinyurl.com/bdejxubh">https://tinyurl.com/bdejxubh</a></li>
				<li>The journey to build an explainable AI-driven recommendation system to help scale sales efficiency across LinkedIn:<a href="https://www.linkedin.com/blog/engineering/recommendations/the-journey-to-build-an-explainable-ai-driven-recommendation-sys">https://www.linkedin.com/blog/engineering/recommendations/the-journey-to-build-an-explainable-ai-driven-recommendation-sys</a></li>
				<li>Empowering the Future of Recruitment: 7 AI Hiring Tools Ushering in a Bright 2023 - HyScaler: <a href="https://hyscaler.com/insights/ai-hiring-tools-7-trends-2023/">https://hyscaler.com/insights/ai-hiring-tools-7-trends-2023/</a></li>
				<li>Worried about your firm’s AI ethics? These startups are here to help. | MIT Technology Review: <a href="https://www.technologyreview.com/2021/01/15/1016183/ai-ethics-startups/">https://www.technologyreview.com/2021/01/15/1016183/ai-ethics-startups/</a></li>
				<li>The AI Ethics Boom: 150 Ethical AI Startups and Industry Trends - BGV: <a href="https://benhamouglobalventures.com/ai-ethics-boom-150-ethical-ai-startups-industry-trends/">https://benhamouglobalventures.com/ai-ethics-boom-150-ethical-ai-startups-industry-trends/</a></li>
				<li>Responsible AI toolkits: <a href="https://odsc.medium.com/15-open-source-responsible-ai-toolkits-and-projects-to-use-today-fbc1c2ea2815">https://odsc.medium.com/15-open-source-responsible-ai-toolkits-and-projects-to-use-today-fbc1c2ea2815</a></li>
				<li>Deepfakes, explained | MIT Sloan: <a href="https://mitsloan.mit.edu/ideas-made-to-matter/deepfakes-explained">https://mitsloan.mit.edu/ideas-made-to-matter/deepfakes-explained</a></li>
				<li>Regulatory Landscape: <a href="https://www.goodwinlaw.com/en/insights/%20publications/2023/04/04_12-us-artificial-intelligence-regulations">https://www.goodwinlaw.com/en/insights/%20publications/2023/04/04_12-us-artificial-intelligence-regulations</a><a href="https://www.goodwinlaw.com/en/insights/publications/2023/04/04_12-us-artificial-intelligence-regulations "/></li>
				<li>Artificial Intelligence regulation, global trends | EY - US: <a href="https://www.ey.com/en_us/ai/how-to-navigate-global-trends-in-artificial-intelligence-regulation#:~:text=,rapidly%20evolving%20AI%20regulatory%20landscape">https://www.ey.com/en_us/ai/how-to-navigate-global-trends-in-artificial-intelligence-regulation#:~:text=,rapidly%20evolving%20AI%20regulatory%20landscape</a></li>
				<li>Infuse responsible AI tools and practices in your LLMOps | Microsoft Azure Blog: <a href="https://azure.microsoft.com/en-us/blog/infuse-responsible-ai-tools-and-practices-in-your-llmops/">https://azure.microsoft.com/en-us/blog/infuse-responsible-ai-tools-and-practices-in-your-llmops/</a></li>
			</ul>
		</div>
	

		<div><h1 id="_idParaDest-216" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor217"/>Part 5: Generative AI – What’s Next?</h1>
		</div>
		<div><p>This concluding part delves into the future prospects of generative AI, particularly the advancements in multimodal AI, with a detailed look at GPT-4 Turbo with vision capabilities. It also examines the emergence of <strong class="bold">Smaller Language Models</strong> (<strong class="bold">SLMs</strong>) and their significant impact on edge computing, a trend that facilitates faster and more efficient AI processing closer to the data source. Additionally, we’ll explore other emerging trends, future predictions, and the integration of generative AI with robotics, highlighting the synergy between these technologies. The journey toward achieving <strong class="bold">Artificial General Intelligence</strong> (<strong class="bold">AGI</strong>) through the unparalleled computational power of quantum computing will also be discussed, mapping out the potential roadmap and the technological leaps required to realize AGI.</p>
			<p>This part contains the following chapter:</p>
			<ul>
				<li><a href="B21443_10.xhtml#_idTextAnchor218"><em class="italic">Chapter 10</em></a>, <em class="italic">Future of Generative AI: Trends and Emerging Use Cases</em></li>
			</ul>
		</div>
		<div><div></div>
		</div>
		<div><div></div>
		</div>
	</body></html>