<html><head></head><body>
		<div><h1 id="_idParaDest-36" class="chapter-number"><a id="_idTextAnchor035"/><st c="0">2</st></h1>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor036"/><st c="2">Code Lab – An Entire RAG Pipeline</st></h1>
			<p><st c="35">This code lab lays the foundation for the rest of the code in this book. </st><st c="109">We will spend this entire chapter giving you an entire </st><strong class="bold"><st c="164">retrieval-augmented generation</st></strong><st c="194"> (</st><strong class="bold"><st c="196">RAG</st></strong><st c="199">) pipeline. </st><st c="212">Then, as we step through the book, we will look at different parts of the code, adding enhancements along the way so that you have a comprehensive understanding of how your code can evolve to tackle more and more </st><st c="425">difficult problems.</st></p>
			<p><st c="444">We will spend this chapter walking through each component of the RAG pipeline, including the </st><st c="538">following aspects:</st></p>
			<ul>
				<li><st c="556">No interface</st></li>
				<li><st c="569">Setting up a large language model (LLM) account </st><st c="618">with OpenAI</st></li>
				<li><st c="629">Installing the required </st><st c="654">Python packages</st></li>
				<li><st c="669">Indexing data by web crawling, splitting documents, and embedding </st><st c="736">the chunks</st></li>
				<li><st c="746">Retrieving relevant documents using vector </st><st c="790">similarity search</st></li>
				<li><st c="807">Generating responses by integrating retrieved context into </st><st c="867">LLM prompts</st></li>
			</ul>
			<p><st c="878">As we step through the code, you will gain a comprehensive understanding of each step in the RAG process programmatically by using tools such as LangChain, Chroma DB, and OpenAI’s APIs. </st><st c="1065">This will provide you with a strong foundation that we will build upon in subsequent chapters, enhancing and evolving the code to tackle increasingly </st><st c="1215">complex problems.</st></p>
			<p><st c="1232">In later chapters, we will explore techniques that can help improve and customize the pipeline for different use cases and overcome common challenges that arise when building RAG-powered applications. </st><st c="1434">Let’s dive in and </st><st c="1452">start building!</st></p>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor037"/><st c="1467">Technical requirements</st></h1>
			<p><st c="1490">The code for this chapter is available </st><st c="1530">here: </st><a href="https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_02 "><st c="1536">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_02</st></a></p>
			<p><st c="1633">You will need to run this chapter’s code in an environment that’s been set up to run Jupyter notebooks. </st><st c="1738">Experience with Jupyter notebooks is a prerequisite for using this book, and it is too difficult to cover it in a short amount of text. </st><st c="1874">There are numerous ways to set up a notebook environment. </st><st c="1932">There are online versions, versions you can download, notebook environments that universities provide students, and different interfaces you can use. </st><st c="2082">If you are doing this at a company, they will likely have an environment you will want to get familiar with. </st><st c="2191">Each of these options takes very different instructions to set up, and those instructions change often. </st><st c="2295">If you need to brush up on your knowledge about this type of environment, you can start on the Jupyter website: </st><a href="https://docs.jupyter.org/en/latest/"><st c="2407">https://docs.jupyter.org/en/latest/</st></a><st c="2442">. Start here, then ask your favorite LLM for more help to get your environment </st><st c="2521">set up.</st></p>
			<p><st c="2528">What do I use? </st><st c="2544">When I use my Chromebook, often when I am traveling, I use a notebook set up in one of the cloud environments. </st><st c="2655">I prefer Google Colab or their Colab Enterprise notebooks, which you can find in the Vertex AI section of Google Cloud Platform. </st><st c="2784">But these environments cost money, often exceeding $20 a month if you are active. </st><st c="2866">If you are as active as me, it can exceed $1,000 </st><st c="2915">per month!</st></p>
			<p><st c="2925">As a cost-effective alternative for when I am that active, I use Docker Desktop on my Mac, which hosts a Kubernetes cluster locally, and set up my notebook environment in the cluster. </st><st c="3110">All these approaches have several environmental requirements that are often changing. </st><st c="3196">It is best to do a little research and figure out what works best for your situation. </st><st c="3282">There are similar solutions for </st><st c="3314">Windows-based computers.</st></p>
			<p><st c="3338">Ultimately, the primary requirement is to find an environment in which you can run a Jupyter notebook using Python 3. </st><st c="3457">The code we will provide will indicate what other packages you will need </st><st c="3530">to install.</st></p>
			<p class="callout-heading"><st c="3541">Note</st></p>
			<p class="callout"><st c="3546">All of this code assumes you are working in a Jupyter notebook. </st><st c="3611">You could do this directly in a Python file (</st><code><st c="3656">.py</st></code><st c="3659">), but you may have to change some of it. </st><st c="3702">Running this in a notebook gives you the ability to step through it cell by cell and see what happens at each point to better understand the </st><st c="3843">entire process.</st></p>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor038"/><st c="3858">No interface!</st></h1>
			<p><st c="3872">In the following coding example, we are not going to work with interfaces; we will cover that in </st><a href="B22475_06.xhtml#_idTextAnchor114"><em class="italic"><st c="3970">Chapter 6</st></em></a><st c="3979">. In the meantime, we will simply create a string variable that represents the prompt users would enter and use that as a fill-in for a full-fledged </st><st c="4128">interface input.</st></p>
			<h1 id="_idParaDest-40"><a id="_idTextAnchor039"/><st c="4144">Setting up a large language model (LLM) account</st></h1>
			<p><st c="4192">For the</st><a id="_idIndexMarker062"/><st c="4200"> general public, OpenAI’s ChatGPT models</st><a id="_idIndexMarker063"/><st c="4240"> are currently the most popular and well-known LLMs. </st><st c="4293">However, there are many other LLMs available in the market that fit a myriad of purposes. </st><st c="4383">You do not always need to use the most expensive, most powerful LLM. </st><st c="4452">Some LLMs focus on one area, such as the Meditron LLMs, which are medical research-focused fine-tuned versions of Llama 2. </st><st c="4575">If you are in the medical area, you may want to use that LLM instead as it may do better than a big general LLM in your domain. </st><st c="4703">Often, LLMs can be used to double-check other LLMs, so you have to have more than one in those cases. </st><st c="4805">I strongly encourage you to not just use the first LLM you have worked with and to look for the LLM that best suits your needs. </st><st c="4933">But to keep things simpler this early in this book, I am going to talk about setting up </st><st c="5021">OpenAI’s ChatGPT:</st></p>
			<ol>
				<li><st c="5038">Go to the </st><strong class="bold"><st c="5049">API</st></strong><st c="5052"> section of the OpenAI </st><st c="5075">website: </st><a href="https://openai.com/api/"><st c="5084">https://openai.com/api/</st></a><st c="5107">.</st></li>
				<li><st c="5108">If you have not set up an account yet, do so now. </st><st c="5159">The web page can change often, but look for where to </st><st c="5212">sign up.</st></li>
			</ol>
			<p class="callout-heading"><st c="5220">Warning</st></p>
			<p class="callout"><st c="5228">Using OpenAI’s API costs money! </st><st c="5261">Use </st><st c="5265">it sparingly!</st></p>
			<ol>
				<li value="3"><st c="5278">Once you’ve signed up, go to the documentation at </st><a href="https://platform.openai.com/docs/quickstart"><st c="5329">https://platform.openai.com/docs/quickstart</st></a><st c="5372"> and follow the instructions to set up your first </st><st c="5422">API key.</st></li>
				<li><st c="5430">When creating an API key, give it a memorable name and select the type of permissions you want to implement (</st><strong class="bold"><st c="5540">All</st></strong><st c="5544">, </st><strong class="bold"><st c="5546">Restricted</st></strong><st c="5556">, or </st><strong class="bold"><st c="5561">Read Only</st></strong><st c="5570">). </st><st c="5574">If you do not know what option to select, it is best to go with </st><strong class="bold"><st c="5638">All</st></strong><st c="5641"> for now. </st><st c="5651">However, be aware of the other options – you may want to share various responsibilities with other team members but restrict certain types </st><st c="5790">of access:</st><ol><li class="Alphabets"><strong class="bold"><st c="5800">All</st></strong><st c="5804">: This key will have read/write access to all of the </st><st c="5858">OpenAI APIs.</st></li><li class="Alphabets"><strong class="bold"><st c="5870">Restricted</st></strong><st c="5881">: A list of available APIs will appear, providing you with granular control over which APIs the key has access to. </st><st c="5997">You have the option of giving just read or write access to each API. </st><st c="6066">Make sure you have at least enabled the models and embedding APIs you will use in </st><st c="6148">these demos.</st></li><li class="Alphabets"><strong class="bold"><st c="6160">Read Only</st></strong><st c="6170">: This option gives you read-only access to </st><st c="6215">all APIs.</st></li></ol></li>
				<li><st c="6224">Copy the key provided. </st><st c="6248">You will add this to your code shortly. </st><st c="6288">In the meantime, keep in mind that if this key is shared with anyone else, whomever you provide this key can use it and you will be charged. </st><st c="6429">So, this is a key that you want to consider top secret and take the proper precautions to prevent unauthorized use </st><st c="6544">of it.</st></li>
				<li><st c="6550">The OpenAI API requires you to buy credits in advance to use the API. </st><st c="6621">Buy what you are comfortable with, and then for more safety, make sure the </st><strong class="bold"><st c="6696">Enable auto recharge</st></strong><st c="6716"> option is off. </st><st c="6732">This </st><a id="_idIndexMarker064"/><st c="6737">will ensure you </st><a id="_idIndexMarker065"/><st c="6753">are only spending what you intend </st><st c="6787">to spend.</st></li>
			</ol>
			<p><st c="6796">With that, you have set up the key component that will serve as the </st><em class="italic"><st c="6865">brains</st></em><st c="6871"> in your RAG pipeline: the LLM! </st><st c="6903">Next, we will set up your development environment so that you can connect to </st><st c="6980">the LLM.</st></p>
			<h1 id="_idParaDest-41"><a id="_idTextAnchor040"/><st c="6988">Installing the necessary packages</st></h1>
			<p><st c="7022">Make sure these packages are installed in your Python environment. </st><st c="7090">Add the following lines of code in the first cell of </st><st c="7143">your notebook:</st></p>
			<pre class="source-code"><st c="7157">
%pip install langchain_community
%pip install langchain_experimental
%pip install langchain-openai
%pip install langchainhub
%pip install chromadb
%pip install langchain
%pip install beautifulsoup4</st></pre>
			<p><st c="7355">The preceding code installs several Python libraries using the </st><code><st c="7419">pip</st></code><st c="7422"> package manager, something you will need to run the code I am providing. </st><st c="7496">Here’s a breakdown of </st><st c="7518">each library:</st></p>
			<ul>
				<li><code><st c="7531">langchain_community</st></code><st c="7551">: This is a</st><a id="_idIndexMarker066"/><st c="7563"> community-driven package for the LangChain library, which is an open source framework for building applications with LLMs. </st><st c="7687">It provides a set of tools and components for working with LLMs and integrating them into </st><st c="7777">various applications.</st></li>
				<li><code><st c="7798">langchain_experimental</st></code><st c="7821">: The </st><code><st c="7828">langchain_experimental</st></code><st c="7850"> library</st><a id="_idIndexMarker067"/><st c="7858"> offers additional capabilities and tools beyond the core LangChain library that are not yet fully stable or production-ready but are still available for experimentation </st><st c="8028">and exploration.</st></li>
				<li><code><st c="8044">langchain-openai</st></code><st c="8061">: This </st><a id="_idIndexMarker068"/><st c="8069">package provides integration between LangChain and OpenAI’s language models. </st><st c="8146">It allows you to easily incorporate OpenAI’s models, such as ChatGPT 4 or the OpenAI embeddings service, into your </st><st c="8261">LangChain applications.</st></li>
				<li><code><st c="8284">langchainhub</st></code><st c="8297">: This </st><a id="_idIndexMarker069"/><st c="8305">package provides a collection of pre-built components and templates for LangChain applications. </st><st c="8401">It includes various agents, memory components, and utility functions that can be used to accelerate the development of </st><st c="8520">LangChain-based applications.</st></li>
				<li><code><st c="8549">chromadb</st></code><st c="8558">: This is the </st><a id="_idIndexMarker070"/><st c="8573">package name for Chroma DB, a high-performance embedding/vector database designed for efficient similarity search </st><st c="8687">and retrieval.</st></li>
				<li><code><st c="8701">langchain</st></code><st c="8711">: This is the</st><a id="_idIndexMarker071"/><st c="8725"> core LangChain library itself. </st><st c="8757">It provides a framework and a set of abstractions for building applications with LLMs. </st><st c="8844">LangChain includes the components needed for an effective RAG pipeline, including prompting, memory management, agents, and other integrations with various external tools </st><st c="9015">and services.</st></li>
			</ul>
			<p><st c="9028">After running the preceding first line, you will need to restart your kernel to be able to access all of the new packages you just installed in the environment. </st><st c="9190">Depending on what environment you are in, this can be done in a variety of ways. </st><st c="9271">Typically, you will see a refresh button you can use or a </st><strong class="bold"><st c="9329">Restart kernel</st></strong><st c="9343"> option in </st><st c="9354">the menu.</st></p>
			<p><st c="9363">If you have trouble finding a way to restart the kernel, add this cell and </st><st c="9439">run it:</st></p>
			<pre class="source-code"><st c="9446">
import IPython
app = IPython.Application.instance(;
app.kernel.do_shutdown(True)</st></pre>
			<p><st c="9527">This is a code version for performing a kernel restart in an IPython environment (notebooks). </st><st c="9622">You shouldn’t need it, but it is here for you just </st><st c="9673">in case!</st></p>
			<p><st c="9681">Once you have installed these packages and restarted your kernel, you are ready to start coding! </st><st c="9779">Let’s start with importing many of the packages you just installed in </st><st c="9849">your environment.</st></p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor041"/><st c="9866">Imports</st></h2>
			<p><st c="9874">Now, let’s import all of the libraries needed to</st><a id="_idIndexMarker072"/><st c="9923"> perform the RAG-related tasks. </st><st c="9955">I have provided comments at the top of each group of imports to indicate what area of RAG the imports are relevant to. </st><st c="10074">This, combined with the description in the following list, provides a basic introduction to everything you need for your first </st><st c="10201">RAG pipeline:</st></p>
			<pre class="source-code"><st c="10214">
import os
from langchain_community.document_loaders import WebBaseLoader
import bs4
import openai
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain import hub
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
import chromadb
from langchain_community.vectorstores import Chroma
from langchain_experimental.text_splitter import SemanticChunker</st></pre>
			<p><st c="10644">Let’s step through each of </st><st c="10672">these imports:</st></p>
			<ul>
				<li><code><st c="10686">import os</st></code><st c="10696">: This</st><a id="_idIndexMarker073"/><st c="10703"> provides a way to interact with the operating system. </st><st c="10758">It is useful for performing operations such as accessing environment variables and working with </st><st c="10854">file paths.</st></li>
				<li><code><st c="10865">from langchain_community.document_loaders import WebBaseLoader</st></code><st c="10928">: The </st><code><st c="10935">WebBaseLoader</st></code><st c="10948"> class is a document loader that can fetch and load web pages </st><st c="11010">as documents.</st></li>
				<li><code><st c="11023">import bs4</st></code><st c="11034">: The </st><code><st c="11041">bs4</st></code><st c="11044"> module, which stands for </st><strong class="bold"><st c="11070">Beautiful Soup 4</st></strong><st c="11086">, is a popular library for web scraping and parsing HTML </st><a id="_idIndexMarker074"/><st c="11143">or XML documents. </st><st c="11161">Since we will be working with a web page, this gives us a simple way to pull out the title, content, and </st><st c="11266">headers separately.</st></li>
				<li><code><st c="11285">import openai</st></code><st c="11299">: This provides an interface to interact with OpenAI’s language models </st><st c="11371">and APIs.</st></li>
				<li><code><st c="11380">from langchain_openai import ChatOpenAI, OpenAIEmbeddings</st></code><st c="11438">: This imports both </st><code><st c="11459">ChatOpenAI</st></code><st c="11469"> (for the LLM) and </st><code><st c="11488">OpenAIEmbeddings</st></code><st c="11504"> (for the embeddings), which are specific implementations of language models and embeddings that use OpenAI’s models that work directly </st><st c="11640">with LangChain.</st></li>
				<li><code><st c="11655">from langchain import hub</st></code><st c="11681">: The </st><code><st c="11688">hub</st></code><st c="11691"> component provides access to various pre-built components and utilities for working with </st><st c="11781">language models.</st></li>
				<li><code><st c="11797">from langchain_core.output_parsers import StrOutputParser</st></code><st c="11855">: This component parses the output generated by the language model and extracts the relevant information. </st><st c="11962">In this case, it assumes that the language model’s output is a string and returns </st><st c="12044">it as-is.</st></li>
				<li><code><st c="12053">from langchain_core.runnables import RunnablePassthrough</st></code><st c="12110">: This component passes through the question or query without any modifications. </st><st c="12192">It allows the question to be used as-is in the subsequent steps of </st><st c="12259">the chain.</st></li>
				<li><code><st c="12269">Import chromadb</st></code><st c="12285">: As mentioned previously, </st><code><st c="12313">chromadb</st></code><st c="12321"> imports the Chroma DB vector store, a high-performance embedding/vector database designed for efficient similarity search </st><st c="12444">and retrieval.</st></li>
				<li><code><st c="12458">from langchain_community.vectorstores import Chroma</st></code><st c="12510">: This provides an interface to interact with the Chroma vector database </st><st c="12584">using LangChain.</st></li>
				<li><code><st c="12600">from langchain_experimental.text_splitter import SemanticChunker</st></code><st c="12665">: A text splitter is typically a function that we use to split the text into small chunks based on a specified chunk size and overlap. </st><st c="12801">This splitter is called </st><code><st c="12825">SemanticChunker</st></code><st c="12840">, an experimental text-splitting utility provided by the </st><code><st c="12897">Langchain_experimental</st></code><st c="12919"> library. </st><st c="12929">The main purpose of </st><code><st c="12949">SemanticChunker</st></code><st c="12964"> is to break down long text into more manageable pieces while preserving the</st><a id="_idIndexMarker075"/><st c="13040"> semantic coherence and context of </st><st c="13075">each chunk.</st></li>
			</ul>
			<p><st c="13086">These imports provide the essential Python packages that will be needed to set up your RAG pipeline. </st><st c="13188">Your next step will be to connect your environment to </st><st c="13242">OpenAI’s API.</st></p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/><st c="13255">OpenAI connection</st></h2>
			<p><st c="13273">The following line of code is a very</st><a id="_idIndexMarker076"/><st c="13310"> simple demonstration of how your API key will be ingested into the system. </st><st c="13386">However, this is not a secure way to use an API key. </st><st c="13439">There are many ways to do this more securely. </st><st c="13485">If you have a preference, go ahead and implement it now, but otherwise, we will cover a popular way to make this more secure in </st><a href="B22475_05.xhtml#_idTextAnchor095"><em class="italic"><st c="13613">Chapter 5</st></em></a><em class="italic"><st c="13622">.</st></em></p>
			<p><st c="13623">You are going to need replace </st><code><st c="13654">sk-###################</st></code><st c="13676"> with your actual OpenAI </st><st c="13701">API key:</st></p>
			<pre class="source-code"><st c="13709">
os.environ['OPENAI_API_KEY'] = 'sk-###################'
openai.api_key = os.environ['OPENAI_API_KEY']</st></pre>
			<p class="callout-heading"><st c="13811">Important</st></p>
			<p class="callout"><st c="13821">This is just a simple example; please use a secure approach to hide your </st><st c="13895">API key!</st></p>
			<p><st c="13903">You have probably guessed that this OpenAI API key will be used to connect to the ChatGPT LLM. </st><st c="13999">But ChatGPT is not the only service we will use from OpenAI. </st><st c="14060">This API key is also used to access the OpenAI embedding service. </st><st c="14126">In the next section, which focuses on coding the indexing stage of the RAG process, we will utilize the OpenAI embedding service to convert your content into vector embeddings, a key aspect of the </st><st c="14323">RAG pipeline.</st></p>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor043"/><st c="14336">Indexing</st></h1>
			<p><st c="14345">The next few steps represent the </st><em class="italic"><st c="14379">indexing</st></em><st c="14387"> stage, where we obtain our target data, pre-process it, and vectorize it. </st><st c="14462">These</st><a id="_idIndexMarker077"/><st c="14467"> steps are often done </st><em class="italic"><st c="14489">offline</st></em><st c="14496">, meaning they are done to </st><a id="_idIndexMarker078"/><st c="14523">prepare the application for usage later. </st><st c="14564">But in some cases, it may make sense to do this all in real time, such as in rapidly changing data environments where the data that is used is relatively small. </st><st c="14725">In this particular example, the steps are </st><st c="14767">as follows:</st></p>
			<ol>
				<li><st c="14778">Web loading </st><st c="14791">and crawling.</st></li>
				<li><st c="14804">Splitting the data into digestible chunks for the Chroma DB </st><st c="14865">vectorizing algorithm.</st></li>
				<li><st c="14887">Embedding and indexing </st><st c="14911">those chunks.</st></li>
				<li><st c="14924">Adding those chunks and embeddings to the Chroma DB </st><st c="14977">vector store.</st></li>
			</ol>
			<p><st c="14990">Let’s start with the first step: web loading </st><st c="15036">and crawling.</st></p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor044"/><st c="15049">Web loading and crawling</st></h2>
			<p><st c="15074">To start, we need to pull </st><a id="_idIndexMarker079"/><st c="15101">in our data. </st><st c="15114">This could be anything of </st><a id="_idIndexMarker080"/><st c="15140">course, but we have to </st><st c="15163">start somewhere!</st></p>
			<p><st c="15179">For our example, I am providing a web page example based on some of the content from </st><a href="B22475_01.xhtml#_idTextAnchor015"><em class="italic"><st c="15265">Chapter 1</st></em></a><st c="15274">. I have adopted the original structure from an example provided by LangChain </st><st c="15352">at </st><a href="https://lilianweng.github.io/posts/2023-06-23-agent/"><st c="15355">https://lilianweng.github.io/posts/2023-06-23-agent/</st></a><st c="15407">.</st></p>
			<p><st c="15408">You can try that web page as well if it is still available when you read this, but be sure to change the question you use to query the content to a question more suitable to the content on that page. </st><st c="15609">You also need to restart your kernel if you change web pages; otherwise, it will include content from both web pages if you rerun the loader. </st><st c="15751">That may be what you want, but I’m just letting </st><st c="15799">you know!</st></p>
			<p><st c="15808">I also encourage you to try this with other web pages and see what challenges these other pages present. </st><st c="15914">This example involves a very clean piece of data compared to most web pages, which tend to be rife with ads and other content you do not want showing up. </st><st c="16068">But maybe you can find a relatively clean blog post and pull that in? </st><st c="16138">Maybe you can create your own? </st><st c="16169">Try different web pages </st><st c="16193">and see!</st></p>
			<pre class="source-code"><st c="16201">
loader = WebBaseLoader(
    web_paths=("https://kbourne.github.io/chapter1.html",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
           class_=("post-content", "post-title",
                   "post-header")
        )
    ),
)
docs = loader.load()</st></pre>
			<p><st c="16407">The preceding</st><a id="_idIndexMarker081"/><st c="16421"> code starts with using the </st><code><st c="16449">WebBaseLoader</st></code><st c="16462"> class from the </st><code><st c="16478">langchain_community document_loaders</st></code><st c="16514"> module</st><a id="_idIndexMarker082"/><st c="16521"> to load web pages as documents. </st><st c="16554">Let’s break </st><st c="16566">it down:</st></p>
			<ol>
				<li><st c="16574">Creating the </st><code><st c="16588">WebBaseLoader</st></code><st c="16601"> instance: The </st><code><st c="16616">WebBaseLoader</st></code><st c="16629"> class is instantiated with the </st><st c="16661">following parameters:</st><ul><li><code><st c="16682">web_paths</st></code><st c="16692">: A tuple containing the URLs of the web pages to be loaded. </st><st c="16754">In this case, it contains a single </st><st c="16789">URL: </st><code><st c="16794">https://kbourne.github.io/chapter1.html</st></code><st c="16833">.</st></li><li><code><st c="16834">bs_kwargs</st></code><st c="16844">: A dictionary of keyword arguments to be passed to the </st><code><st c="16901">BeautifulSoup</st></code><st c="16914"> parser.</st></li><li><code><st c="16922">parse_only</st></code><st c="16933">: A </st><code><st c="16938">bs4.SoupStrainer</st></code><st c="16954"> object specifies the HTML elements to parse. </st><st c="17000">In this case, it is set to parse only the elements with the CSS classes, such as </st><code><st c="17081">post-content</st></code><st c="17093">, </st><code><st c="17095">post-title</st></code><st c="17105">, </st><st c="17107">and </st><code><st c="17111">post-header</st></code><st c="17122">.</st></li></ul></li>
				<li><st c="17123">The </st><code><st c="17128">WebBaseLoader</st></code><st c="17141"> instance initiates a series of steps that represent the loading of the document into your environment: The load method is called on </st><code><st c="17274">loader</st></code><st c="17280">, the </st><code><st c="17286">WebBaseLoader</st></code><st c="17299"> instance that fetches and loads the specified web pages as documents. </st><st c="17370">Internally, </st><code><st c="17382">loader</st></code><st c="17388"> is doing </st><st c="17398">a lot!</st><p class="list-inset"><st c="17404">Here are the steps it performs just based on this small amount </st><st c="17468">of code:</st></p><ol><li class="upper-roman"><st c="17476">Makes HTTP requests to the specified URLs to fetch the </st><st c="17532">web pages.</st></li><li class="upper-roman"><st c="17542">Parses the HTML content of the web pages using </st><code><st c="17590">BeautifulSoup</st></code><st c="17603">, considering only the elements specified by the </st><code><st c="17652">parse_only</st></code><st c="17662"> parameter.</st></li><li class="upper-roman"><st c="17673">Extracts the relevant text content from the parsed </st><st c="17725">HTML elements.</st></li><li class="upper-roman"><st c="17739">Creates </st><code><st c="17748">Document</st></code><st c="17756"> objects for each web page that contain the extracted text content, along with metadata such as the </st><st c="17856">source URL.</st></li></ol></li>
			</ol>
			<p><st c="17867">The resulting </st><code><st c="17882">Document</st></code><st c="17890"> objects are stored in the </st><code><st c="17917">docs</st></code><st c="17921"> variable for further use in </st><st c="17950">our code!</st></p>
			<p><st c="17959">The classes that </st><a id="_idIndexMarker083"/><st c="17977">we are passing to </st><code><st c="17995">bs4</st></code><st c="17998"> (</st><code><st c="18000">post-content</st></code><st c="18012">, </st><code><st c="18014">post-title</st></code><st c="18024">, and </st><code><st c="18030">post-header</st></code><st c="18041">) are CSS classes. </st><st c="18061">If you are using an HTML page</st><a id="_idIndexMarker084"/><st c="18090"> that does not have those CSS classes, this will not work. </st><st c="18149">So, if you are using a different URL and are not getting data, take a look at what the CSS tags are in the HTML you are crawling. </st><st c="18279">Many web pages do use this pattern, but not all! </st><st c="18328">Crawling web pages presents many challenges </st><st c="18372">like this.</st></p>
			<p><st c="18382">Once you have collected the documents from your data source, you need to pre-process them. </st><st c="18474">In this case, this </st><st c="18493">involves splitting.</st></p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor045"/><st c="18512">Splitting</st></h2>
			<p><st c="18522">If you are using the provided URL, you </st><a id="_idIndexMarker085"/><st c="18562">will only parse the elements with the </st><code><st c="18600">post-content</st></code><st c="18612">, </st><code><st c="18614">post-title</st></code><st c="18624">, and </st><code><st c="18630">post-header</st></code><st c="18641"> CSS classes. </st><st c="18655">This will extract the text content from the main article body (usually identified by the </st><code><st c="18744">post-content</st></code><st c="18756"> class), the title of the blog post (usually identified by the </st><code><st c="18819">post-title</st></code><st c="18829"> class), and any header information (usually identified by the </st><code><st c="18892">post-header</st></code><st c="18903"> class).</st></p>
			<p><st c="18911">In case you were curious, this is what this document looks like on the web (</st><em class="italic"><st c="18988">Figure 2</st></em><em class="italic"><st c="18997">.1</st></em><st c="18999">):</st></p>
			<div><div><img src="img/B22475_02_01.jpg" alt="Figure 2.1 – A web page that we will process"/><st c="19002"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="21860">Figure 2.1 – A web page that we will process</st></p>
			<p><st c="21904">It goes down many pages too! </st><st c="21934">There</st><a id="_idIndexMarker086"/><st c="21939"> is a lot of content here, too much for an LLM to process directly. </st><st c="22007">So, we will need to split the document into </st><st c="22051">digestible chunks:</st></p>
			<pre class="source-code"><st c="22069">
text_splitter = SemanticChunker(OpenAIEmbeddings())
splits = text_splitter.split_documents(docs)</st></pre>
			<p><st c="22166">There are many text splitters available in LangChain, but I chose to start with an experimental, but very interesting, option called </st><code><st c="22300">SemanticChunker</st></code><st c="22315">. As I mentioned previously, when talking about the imports, </st><code><st c="22376">SemanticChunker</st></code><st c="22391"> focuses on breaking down long text into more manageable pieces while preserving the semantic coherence and context of </st><st c="22510">each chunk.</st></p>
			<p><st c="22521">Other text splitters typically take an arbitrary chunk length that is not context-aware, something that creates issues when important content gets split by the chunker. </st><st c="22691">There are ways to address this that we will talk about in </st><a href="B22475_11.xhtml#_idTextAnchor229"><em class="italic"><st c="22749">Chapter 11</st></em></a><st c="22759">, but for now, just know that </st><code><st c="22789">SemanticChunker</st></code><st c="22804"> focuses on accounting for context rather than just arbitrary length in your chunks. </st><st c="22889">It should also be noted that it is still considered experimental and it is under continual development. </st><st c="22993">In </st><a href="B22475_11.xhtml#_idTextAnchor229"><em class="italic"><st c="22996">Chapter 11</st></em></a><st c="23006">, we will put it to the test against probably the other most important text splitter, </st><code><st c="23092">RecursiveCharacter TextSplitter</st></code><st c="23123">, and see which splitter works best with </st><st c="23164">this content.</st></p>
			<p><st c="23177">It should also be noted that the </st><code><st c="23211">SemanticChunker</st></code><st c="23226"> splitter you use in this code uses </st><code><st c="23262">OpenAIEmbeddings</st></code><st c="23278">, and it costs money to process the embeddings. </st><st c="23326">The OpenAI embedding models currently cost between $0.02 and $0.13 per million tokens, depending on what model you use. </st><st c="23446">At the time of writing, if do not designate an embedding model, OpenAI will use the </st><code><st c="23530">text-embedding-ada-002</st></code><st c="23552"> model by default, which costs $0.02 per million tokens. </st><st c="23609">If you want to avoid the cost, fall back to </st><code><st c="23653">RecursiveCharacter TextSplitter</st></code><st c="23684">, something we will cover in </st><a href="B22475_11.xhtml#_idTextAnchor229"><em class="italic"><st c="23713">Chapter 11</st></em></a><st c="23723">.</st></p>
			<p><st c="23724">I encourage you to go ahead and try different splitters and see what happens! </st><st c="23803">For example, do you think you get better results from </st><code><st c="23857">RecursiveCharacter TextSplitter</st></code><st c="23888"> than from </st><code><st c="23899">SemanticChunker</st></code><st c="23914">, which we are using here? </st><st c="23941">Maybe speed is more important </st><a id="_idIndexMarker087"/><st c="23971">than quality in your particular case – which one </st><st c="24020">is faster?</st></p>
			<p><st c="24030">Once you have chunked up your content, the next step is to convert it into the vector embeddings we have talked so </st><st c="24146">much about!</st></p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor046"/><st c="24157">Embedding and indexing the chunks</st></h2>
			<p><st c="24191">The next few steps </st><a id="_idIndexMarker088"/><st c="24211">represent the retrieval and generation steps, where we will use Chroma DB</st><a id="_idIndexMarker089"/><st c="24284"> as the vector database. </st><st c="24309">As mentioned multiple times now, Chroma DB is a great vector store! </st><st c="24377">I selected this vector store because it is easy to run locally and it works well for demos like this, but it is a fairly powerful vector store. </st><st c="24521">As you may recall when we talked about vocabulary and the difference between vector stores and vector databases, Chroma DB is indeed both! </st><st c="24660">Chroma is one of many options for your vector store though. </st><st c="24720">In </st><a href="B22475_07.xhtml#_idTextAnchor122"><em class="italic"><st c="24723">Chapter 7</st></em></a><st c="24732">, we will discuss many of the vector store options and reasons to choose one over the other. </st><st c="24825">Some of these options even provide free vector </st><st c="24872">embedding generation.</st></p>
			<p><st c="24893">We are using OpenAI embeddings here as well, which will use your OpenAI key to send your chunks of data to the OpenAI API, convert them into embeddings, and then send them back in their mathematical form. </st><st c="25099">Note that this </st><em class="italic"><st c="25114">does</st></em><st c="25118"> cost money! </st><st c="25131">It is a fraction of a penny for each embedding, but it is worth noting. </st><st c="25203">So, please use caution when using this code if you are doing this on a tight budget! </st><st c="25288">In </st><a href="B22475_07.xhtml#_idTextAnchor122"><em class="italic"><st c="25291">Chapter 7</st></em></a><st c="25300">, we will review some ways to use free vectorization services to generate these embeddings </st><st c="25391">for free:</st></p>
			<pre class="source-code"><st c="25400">
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=OpenAIEmbeddings())
retriever = vectorstore.as_retriever()</st></pre>
			<p><st c="25524">First, we create </st><a id="_idIndexMarker090"/><st c="25542">the Chroma vector store with the </st><code><st c="25575">Chroma.from_documents</st></code><st c="25596"> method, which is called to create a Chroma vector store from the split documents. </st><st c="25679">This is one of many methods we can use to create a Chroma database. </st><st c="25747">This typically depends on the source, but for this particular method, it takes the </st><st c="25830">following parameters:</st></p>
			<ul>
				<li><code><st c="25851">documents</st></code><st c="25861">: The list of split documents (splits) obtained from the previous </st><st c="25928">code snippet</st></li>
				<li><code><st c="25940">embedding</st></code><st c="25950">: An instance of the </st><code><st c="25972">OpenAIEmbeddings</st></code><st c="25988"> class, which is used to generate embeddings for </st><st c="26037">the documents</st></li>
			</ul>
			<p><st c="26050">Internally, the method is doing a </st><st c="26085">few things:</st></p>
			<ol>
				<li><st c="26096">It iterates over each </st><code><st c="26119">Document</st></code><st c="26127"> object in the </st><st c="26142">splits list.</st></li>
				<li><st c="26154">For each </st><code><st c="26164">Document</st></code><st c="26172"> object, it uses the provided </st><code><st c="26202">OpenAIEmbeddings</st></code><st c="26218"> instance to generate an </st><st c="26243">embedding vector.</st></li>
				<li><st c="26260">It stores the document text and its corresponding embedding vector in the Chroma </st><st c="26342">vector database.</st></li>
			</ol>
			<p><st c="26358">At this point, you now have a vector database called </st><code><st c="26412">vectorstore</st></code><st c="26423">, and it is full of embeddings, which are…? </st><st c="26467">That’s right – mathematical representations of all of the content from the web page you just crawled! </st><st c="26569">So cool!</st></p>
			<p><st c="26577">But what is this next part – a retriever? </st><st c="26620">Is this of the canine variety? </st><st c="26651">Nope. </st><st c="26657">This is creating the mechanism that you will use to perform vector similarity searches on your new vector database. </st><st c="26773">You call the </st><code><st c="26786">as_retriever</st></code><st c="26798"> method right on the </st><code><st c="26819">vectorstore</st></code><st c="26830"> instance to create the retriever. </st><st c="26865">The retriever is an object that provides a convenient interface for performing these similarity searches and retrieving the relevant documents from the vector database based on </st><st c="27042">those searches.</st></p>
			<p><st c="27057">If you just want to perform the document retrieval process, you can. </st><st c="27127">This is not officially part of the code, but if you want to test this out, add this in an extra cell and </st><st c="27232">run it:</st></p>
			<pre class="source-code"><st c="27239">
query = "How does RAG compare with fine-tuning?"
</st><st c="27289">relevant_docs = retriever.get_relevant_documents(query)
relevant_docs</st></pre>
			<p><st c="27358">The output </st><a id="_idIndexMarker091"/><st c="27370">should be what I list later in this code when I indicate what is passed to the LLM, but it is essentially a list of the content stored in the </st><code><st c="27512">vectorstore</st></code><st c="27523"> vector database that is most similar to </st><st c="27564">the query.</st></p>
			<p><st c="27574">Aren’t you impressed? </st><st c="27597">This is a simple example of course, but this is the foundation for much more powerful tools that you can use to access your data and supercharge generative AI applications for </st><st c="27773">your organization!</st></p>
			<p><st c="27791">However, at this point in the application, you have only created the receiver. </st><st c="27871">You have not used it within the RAG pipeline yet. </st><st c="27921">We will review how to do </st><st c="27946">that next!</st></p>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor047"/><st c="27956">Retrieval and generation</st></h1>
			<p><st c="27981">In the code, the retrieval and generation stages</st><a id="_idIndexMarker092"/><st c="28030"> are combined within the chain we set up to represent the entire RAG process. </st><st c="28108">This leverages pre-built components from the </st><strong class="bold"><st c="28153">LangChain Hub</st></strong><st c="28166">, such as </st><strong class="bold"><st c="28176">prompt templates</st></strong><st c="28192">, and</st><a id="_idIndexMarker093"/><st c="28197"> integrates them with a selected LLM. </st><st c="28235">We will also</st><a id="_idIndexMarker094"/><st c="28247"> utilize the </st><strong class="bold"><st c="28260">LangChain Expression Language</st></strong><st c="28289"> (</st><strong class="bold"><st c="28291">LCEL</st></strong><st c="28295">) to </st><a id="_idIndexMarker095"/><st c="28301">define a chain of operations that retrieves relevant documents based on an input question, formats the retrieved content, and feeds it into the LLM to generate a response. </st><st c="28473">Overall, the steps we take in retrieval and generation are </st><st c="28532">as follows:</st></p>
			<ol>
				<li><st c="28543">Take in a </st><st c="28554">user query.</st></li>
				<li><st c="28565">Vectorize that </st><st c="28581">user query.</st></li>
				<li><st c="28592">Perform a similarity search of the vector store to find the closest vectors to the user query vector, as well as their </st><st c="28712">associated content.</st></li>
				<li><st c="28731">Pass the</st><a id="_idIndexMarker096"/><st c="28740"> retrieved content into a prompt template, a process known </st><st c="28799">as </st><strong class="bold"><st c="28802">hydrating</st></strong><st c="28811">.</st></li>
				<li><st c="28812">Pass that </st><em class="italic"><st c="28823">hydrated</st></em><st c="28831"> prompt to </st><st c="28842">the LLM.</st></li>
				<li><st c="28850">Once you</st><a id="_idIndexMarker097"/><st c="28859"> receive a response from the LLM, present it to </st><st c="28907">the user.</st></li>
			</ol>
			<p><st c="28916">From a coding standpoint, we will start by defining the prompt template so that we have something to hydrate when we receive the user query. </st><st c="29058">We will cover this in the </st><st c="29084">next section.</st></p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor048"/><st c="29097">Prompt templates from the LangChain Hub</st></h2>
			<p><st c="29137">The LangChain Hub</st><a id="_idIndexMarker098"/><st c="29155"> is a collection of pre-built components and templates that can be easily integrated into LangChain applications. </st><st c="29269">It provides a centralized repository for </st><a id="_idIndexMarker099"/><st c="29310">sharing and discovering reusable components, such as prompts, agents, and utilities. </st><st c="29395">Here, we are calling a prompt template from the LangChain Hub and assigning it to </st><code><st c="29477">prompt</st></code><st c="29483">, a prompt template representing what we will pass to </st><st c="29537">the LLM:</st></p>
			<pre class="source-code"><st c="29545">
prompt = hub.pull("jclemens24/rag-prompt")
print(prompt)</st></pre>
			<p><st c="29602">This code retrieves a pre-built prompt template from the LangChain Hub using the </st><code><st c="29684">pull</st></code><st c="29688"> method of the </st><code><st c="29703">hub</st></code><st c="29706"> module. </st><st c="29715">The prompt template is identified by the </st><code><st c="29756">jclemens24/rag-prompt</st></code><st c="29777"> string. </st><st c="29786">This identifier follows the </st><em class="italic"><st c="29814">repository/component</st></em><st c="29834"> convention, where </st><em class="italic"><st c="29853">repository</st></em><st c="29863"> represents the organization or user hosting the component, and </st><em class="italic"><st c="29927">component</st></em><st c="29936"> represents the specific component being pulled. </st><st c="29985">The </st><code><st c="29989">rag-prompt</st></code><st c="29999"> component indicates it is a prompt designed for </st><st c="30048">RAG applications.</st></p>
			<p><st c="30065">If you print out the prompt with </st><code><st c="30099">print(prompt)</st></code><st c="30112">, you can see what is used here, as well as what the </st><st c="30165">inputs are:</st></p>
			<pre class="source-code"><st c="30176">
input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. </st><st c="30378">Use the following pieces of retrieved-context to answer the question. </st><st c="30448">If you don't know the answer, just say that you don't know.\nQuestion: {question} \nContext: {context} \nAnswer:"))]</st></pre>
			<p><st c="30564">This is the initial part of the prompt that gets passed to the LLM, which in this case, tells </st><st c="30659">it this:</st></p>
			<pre class="source-code"><st c="30667">
"You are an assistant for question-answering tasks. </st><st c="30720">Use the following pieces of retrieved-context to answer the question. </st><st c="30790">If you don't know the answer, just say that you don't know.
</st><st c="30850">Question: {question}
Context: {context}
Answer:"</st></pre>
			<p><st c="30898">Later, you add</st><a id="_idIndexMarker100"/><st c="30913"> the </st><code><st c="30918">question</st></code><st c="30926"> and </st><code><st c="30931">context</st></code><st c="30938"> variables to </st><em class="italic"><st c="30952">hydrate</st></em><st c="30959"> the prompt, but starting with this format optimizes it to work better for </st><st c="31034">RAG applications.</st></p>
			<p class="callout-heading"><st c="31051">Note</st></p>
			<p class="callout"><st c="31056">The </st><code><st c="31061">jclemens24/rag-prompt</st></code><st c="31082"> string is one version of the predefined starting prompts. </st><st c="31141">Visit the LangChain Hub to find many more – you may even find one that better fits your </st><st c="31229">needs: </st><a href="https://smith.langchain.com/hub/search?q=rag-prompt"><st c="31236">https://smith.langchain.com/hub/search?q=rag-prompt</st></a><st c="31287">.</st></p>
			<p class="callout"><st c="31288">You can also use your own! </st><st c="31316">I can count over 30 options at the time </st><st c="31356">of writing!</st></p>
			<p><st c="31367">The prompt template is a key part of the RAG pipeline as it represents how you communicate with the LLM to receive the response you are seeking. </st><st c="31513">But in most RAG pipelines, getting the prompt into a format so that it can work with the prompt template is not as straightforward as just passing it a string. </st><st c="31673">In this example, the </st><code><st c="31694">context</st></code><st c="31701"> variable represents the content we get from the retriever and that is not in a string format </st><a id="_idIndexMarker101"/><st c="31795">yet! </st><st c="31800">We will walk through how to convert our retrieved content into the proper string format we </st><st c="31891">need next.</st></p>
			<h2 id="_idParaDest-50"><a id="_idTextAnchor049"/><st c="31901">Formatting a function so that it matches the next step’s input</st></h2>
			<p><st c="31964">First, we will set up a</st><a id="_idIndexMarker102"/><st c="31988"> function that takes the list of retrieved documents (docs) </st><st c="32048">as input:</st></p>
			<pre class="source-code"><st c="32057">
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)</st></pre>
			<p><st c="32133">Inside this function, a generator expression, </st><code><st c="32180">(doc.page_content for doc in docs)</st></code><st c="32214">, is used to extract the </st><code><st c="32239">page_content</st></code><st c="32251"> attribute from each document object. </st><st c="32289">The </st><code><st c="32293">page_content</st></code><st c="32305"> attribute represents the text content of </st><st c="32347">each document.</st></p>
			<p class="callout-heading"><st c="32361">Note</st></p>
			<p class="callout"><st c="32366">In this case, a </st><em class="italic"><st c="32383">document</st></em><st c="32391"> is not the entire document that you crawled earlier. </st><st c="32445">It is just one small section of it, but we generally call </st><st c="32503">these documents.</st></p>
			<p><st c="32519">The </st><code><st c="32524">join</st></code><st c="32528"> method is called on the </st><code><st c="32553">\n\n</st></code><st c="32557"> string to concatenate </st><code><st c="32580">page_content</st></code><st c="32592"> of each document with two newline characters between each document’s content. </st><st c="32671">The formatted string is returned by the </st><code><st c="32711">format_docs</st></code><st c="32722"> function to represent the </st><code><st c="32749">context</st></code><st c="32756"> key in the dictionary that is piped into the </st><st c="32802">prompt object.</st></p>
			<p><st c="32816">The purpose of this function is to format the output of the retriever into the string format that it will need to be in for the next step in the chain, after the retriever step. </st><st c="32995">We will explain this further in a moment, but short functions like this are often necessary for LangChain chains to match up inputs and outputs across the </st><st c="33150">entire chain.</st></p>
			<p><st c="33163">Next, we will review the last step before we can create our LangChain chain – that is, defining the LLM we will use in </st><st c="33283">that chain.</st></p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor050"/><st c="33294">Defining your LLM</st></h2>
			<p><st c="33312">Let’s set up the</st><a id="_idIndexMarker103"/><st c="33329"> LLM model you </st><st c="33344">will use:</st></p>
			<pre class="source-code"><st c="33353">
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)</st></pre>
			<p><st c="33411">The preceding code creates an instance of the </st><code><st c="33458">ChatOpenAI</st></code><st c="33468"> class from the </st><code><st c="33484">langchain_openai</st></code><st c="33500"> module, which serves as an interface to OpenAI’s language models, specifically the</st><a id="_idIndexMarker104"/><st c="33583"> GPT-4o mini model. </st><st c="33603">Even though this model is newer, it was released at a significant discount to the older models. </st><st c="33699">Using this model will help keep your inference costs down while still allowing you to use a recent model! </st><st c="33805">If you would like to try a different version of ChatGPT, such as </st><code><st c="33870">gpt-4</st></code><st c="33875">, you can just change the model name. </st><st c="33913">Look up the newest models on the OpenAI API website – they add </st><st c="33976">them often!</st></p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor051"/><st c="33987">Setting up a LangChain chain using LCEL</st></h2>
			<p><st c="34027">This </st><em class="italic"><st c="34033">chain</st></em><st c="34038"> is in </st><a id="_idIndexMarker105"/><st c="34045">a code format specific to LangChain called</st><a id="_idIndexMarker106"/><st c="34087"> LCEL. </st><st c="34094">You will see me using LCEL throughout the code from here on out. </st><st c="34159">Not only does it make the code easier to read and more concise, but it opens up new techniques focused on improving the speed and efficiency of your </st><st c="34308">LangChain code.</st></p>
			<p><st c="34323">If you walk through this chain, you’ll see it provides a great representation of the entire </st><st c="34416">RAG process:</st></p>
			<pre class="source-code"><st c="34428">
rag_chain = (
    {"context": retriever | format_docs,
     "question": RunnablePassthrough()}
         | prompt
         | llm
         | StrOutputParser()
)</st></pre>
			<p><st c="34551">All of these components have already been described, but to summarize, the </st><code><st c="34627">rag_chain</st></code><st c="34636"> variable represents a chain of operations using the LangChain framework. </st><st c="34710">Let’s walk through each step of the chain, digging into what is happening at </st><st c="34787">each point:</st></p>
			<ol>
				<li><code><st c="34990">rag_chain</st></code><st c="34999"> variable in a moment, we will pass it a “question.” As shown in the preceding code, the chain starts with a dictionary that defines two keys: </st><code><st c="35142">"context"</st></code><st c="35151"> and </st><code><st c="35156">"question"</st></code><st c="35166">. The question part is pretty straightforward, but where does the context come from? </st><st c="35251">The </st><code><st c="35255">"context"</st></code><st c="35264"> key assigned is the result of the </st><code><st c="35299">retriever</st></code><st c="35308"> | </st><code><st c="35311">format_docs</st></code><st c="35322"> operation.</st></li><li class="Alphabets"><st c="35333">Does </st><code><st c="35339">format_docs</st></code><st c="35350"> sound familiar? </st><st c="35367">Yes! </st><st c="35372">That’s because we just set up that function previously. </st><st c="35428">Here, we use that function alongside </st><code><st c="35465">retriever</st></code><st c="35474">. The </st><code><st c="35480">|</st></code><st c="35481"> operator, called a pipe, between the retriever and </st><code><st c="35533">format_docs</st></code><st c="35544"> indicates that we are chaining these operations together. </st><st c="35603">So, in this case, the </st><code><st c="35625">retriever</st></code><st c="35634"> object is </st><em class="italic"><st c="35645">piped</st></em><st c="35650"> into the </st><code><st c="35660">format_docs</st></code><st c="35671"> function. </st><st c="35682">We are running the </st><code><st c="35701">retriever</st></code><st c="35710"> operation here, which is the vector similarity search. </st><st c="35766">The similarity search should return a set of matches; that set of matches is what is passed to the function. </st><st c="35875">Our </st><code><st c="35879">format_docs</st></code><st c="35890"> function, as described earlier, is then used on the content provided by the retriever to format all the results of that retriever into a single string. </st><st c="36043">That complete string is then assigned to the </st><em class="italic"><st c="36088">context</st></em><st c="36095">, which as you may remember is a variable in our prompt. </st><st c="36152">The expected input format of the next step is a dictionary with two keys – that is, </st><code><st c="36236">"context"</st></code><st c="36245"> and </st><code><st c="36250">"question"</st></code><st c="36260">. The values that are assigned to these keys are expected to be strings. </st><st c="36333">So, we can’t just pass retriever output, which is a list of objects. </st><st c="36402">This is why we use the </st><code><st c="36425">format_docs</st></code><st c="36436"> function – to convert the retriever results into the string we need for the next step. </st><st c="36524">Let’s go back to the </st><em class="italic"><st c="36545">question</st></em><st c="36553"> that was passed into the chain, which is already in the string format we require. </st><st c="36636">We don’t need any formatting! </st><st c="36666">So, we use the </st><code><st c="36681">RunnablePassthrough()</st></code><st c="36702"> object to just let that input (the </st><em class="italic"><st c="36738">question</st></em><st c="36746"> provided) pass through as the string that it is already formatted as. </st><st c="36817">That object takes the </st><em class="italic"><st c="36839">question</st></em><st c="36847"> we pass into the </st><code><st c="36865">rag_chain</st></code><st c="36874"> variable and passes it through without any modification. </st><st c="36932">We now have our first step in the chain, which is defining the two variables that the prompt in the next </st><st c="37037">step accepts.</st></li></ol></li>
				<li><st c="37050">We can see another pipe (</st><code><st c="37076">|</st></code><st c="37078">) followed by the </st><code><st c="37096">prompt</st></code><st c="37102"> object, and we </st><em class="italic"><st c="37118">pipe</st></em><st c="37122"> the variables (in a dictionary) into that prompt object. </st><st c="37180">This is known as hydrating the prompt. </st><st c="37219">As mentioned previously, the </st><code><st c="37248">prompt</st></code><st c="37254"> object is a prompt template that defines what we will pass to the LLM, and it typically includes input variables (context and question) that are filled/hydrated first. </st><st c="37423">The result of this second step is the full prompt text as a string, with the variables filling in the placeholders for context and question. </st><st c="37564">Then, we have another pipe (</st><code><st c="37592">|</st></code><st c="37594">) and the </st><code><st c="37604">llm</st></code><st c="37607"> object that we defined earlier. </st><st c="37640">As we have seen already, this step in the chain takes the output from the previous step, which is the prompt string that includes all the information from previous steps. </st><st c="37811">The </st><code><st c="37815">llm</st></code><st c="37818"> object represents the language model</st><a id="_idIndexMarker108"/><st c="37855"> we set up, which in this case is </st><code><st c="37889">ChatGPT 4o</st></code><st c="37899">. The formatted prompt string is passed as input to the language model, which generates a response based on the provided context </st><st c="38028">and question.</st></li>
				<li><st c="38041">It almost seems like this would be enough, but when you use an LLM API, it is not just sending you the text you might see when you type something into ChatGPT. </st><st c="38202">It is in a JSON format and has a lot of other data included with it. </st><st c="38271">So, to keep things simple, we are going to </st><em class="italic"><st c="38314">pipe</st></em><st c="38318"> the LLM’s output to the next step and use LangChain’s </st><code><st c="38373">StrOutputParser()</st></code><st c="38390"> object. </st><st c="38399">Note that </st><code><st c="38409">StrOutputParser()</st></code><st c="38426"> is a utility class in LangChain that parses the key output of the language model into a string format. </st><st c="38530">Not only does it strip away all the information you did not want to deal with right now, but it ensures that the generated response is returned as </st><st c="38677">a string.</st></li>
			</ol>
			<p><st c="38686">Let’s take a moment to appreciate everything we just did here. </st><st c="38750">This </st><em class="italic"><st c="38755">chain</st></em><st c="38760"> we created using LangChain represents the core code for our entire RAG pipeline, and it is just a few </st><st c="38863">strings long!</st></p>
			<p><st c="38876">When the user uses your application, it will start with the user query. </st><st c="38949">But from a coding standpoint, we set up everything else so that we can process the query properly. </st><st c="39048">At this point, we are ready to accept the user query, so let’s review this last step in </st><st c="39136">our code.</st></p>
			<h1 id="_idParaDest-53"><a id="_idTextAnchor052"/><st c="39145">Submitting a question for RAG</st></h1>
			<p><st c="39175">So far, you have</st><a id="_idIndexMarker109"/><st c="39192"> defined the chain, but you haven’t run it. </st><st c="39236">So, let’s run the entire RAG pipeline in this one line, using a query you are </st><st c="39314">feeding in:</st></p>
			<pre class="source-code"><st c="39325">
rag_chain.invoke("What are the advantages of using RAG?")</st></pre>
			<p><st c="39383">As mentioned when stepping through what happens in the chain, </st><code><st c="39446">"What are the advantages of using RAG?"</st></code><st c="39485"> is the string we are going to pass into the chain to begin with. </st><st c="39551">The first step in the chain expects this string as the </st><em class="italic"><st c="39606">question</st></em><st c="39614"> we discussed in the previous section as one of the two expected variables. </st><st c="39690">In some applications, this may not be in the proper format and will need an extra function to prepare it, but for this application, it is already in the string format we are expecting, so we pass it right into that </st><code><st c="39905">RunnablePassThrough()</st></code><st c="39926"> object.</st></p>
			<p><st c="39934">In the future, this prompt will include a query from a user interface, but for now, we will represent it as this variable string. </st><st c="40065">Keep in mind that this is not the only text the LLM will see; you added a more robust prompt defined by </st><code><st c="40169">prompt</st></code><st c="40175"> previously, hydrated by the </st><code><st c="40204">"context"</st></code><st c="40213"> and </st><code><st c="40218">"</st></code><code><st c="40219">question"</st></code><st c="40228"> variables.</st></p>
			<p><st c="40239">And that is it from a coding standpoint! </st><st c="40281">But what happens when you run the code? </st><st c="40321">Let’s review the output you can expect from this RAG </st><st c="40374">pipeline code.</st></p>
			<h1 id="_idParaDest-54"><a id="_idTextAnchor053"/><st c="40388">Final output</st></h1>
			<p><st c="40401">The</st><a id="_idIndexMarker110"/><st c="40405"> final output will look something </st><st c="40439">like this:</st></p>
			<pre class="source-code"><st c="40449">
"The advantages of using Retrieval Augmented Generation (RAG) include:\n\n1. </st><st c="40527">**Improved Accuracy and Relevance:** RAG enhances the accuracy and relevance of responses generated by large language models (LLMs) by fetching and incorporating specific information from databases or datasets in real time. </st><st c="40751">This ensures outputs are based on both the model's pre-existing knowledge and the most current and relevant data provided.\n\n2. </st><st c="40880">**Customization and Flexibility:** RAG allows for the customization of responses based on domain-specific needs by integrating a company's internal databases into the model's response generation process. </st><st c="41084">This level of customization is invaluable for creating personalized experiences and for applications requiring high specificity and detail.\n\n3. </st><st c="41230">**Expanding Model Knowledge Beyond Training Data:** RAG overcomes the limitations of LLMs, which are bound by the scope of their training data. </st><st c="41374">By enabling models to access and utilize information not included in their initial training sets, RAG effectively expands the knowledge base of the model without the need for retraining. </st><st c="41561">This makes LLMs more versatile and adaptable to new domains or rapidly evolving topics."</st></pre>
			<p><st c="41649">This has some </st><a id="_idIndexMarker111"/><st c="41664">basic formatting in it, so when it’s displayed, it will look like this (including the bullets and </st><st c="41762">bolded text):</st></p>
			<p><code><st c="41775">The advantages of using Retrieval Augmented Generation (</st></code><code><st c="41832">RAG) include:</st></code></p>
			<ul>
				<li><code><st c="41846">Improved Accuracy and Relevance: RAG enhances the accuracy and relevance of responses generated by large language models (LLMs) by fetching and incorporating specific information from databases or datasets in real time. </st><st c="42067">This ensures outputs are based on both the model's pre-existing knowledge and the most current and relevant </st></code><code><st c="42175">data provided.</st></code></li>
				<li><code><st c="42189">Customization and Flexibility: RAG allows for the customization of responses based on domain-specific needs by integrating a company's internal databases into the model's response generation process. </st><st c="42390">This level of customization is invaluable for creating personalized experiences and for applications requiring high specificity </st></code><code><st c="42518">and detail.</st></code></li>
				<li><code><st c="42529">Expanding Model Knowledge Beyond Training Data: RAG overcomes the limitations of LLMs, which are bound by the scope of their training data. </st><st c="42670">By enabling models to access and utilize information not included in their initial training sets, RAG effectively expands the knowledge base of the model without the need for retraining. </st><st c="42857">This makes LLMs more versatile and adaptable to new domains or rapidly </st></code><code><st c="42928">evolving topics.</st></code></li>
			</ul>
			<p><st c="42944">In your use cases, you will need to make decisions by asking questions such as, could a less expensive model do a good enough job at a significantly reduced cost? </st><st c="43108">Or do I need to spend the extra money to get more robust responses? </st><st c="43176">Your prompt may have said to keep it very brief and you end up with the same shorter response as a less expensive model anyway, so why spend the extra money? </st><st c="43334">This is a common consideration when using these models, and in many cases, the largest, most expensive models are not always what is needed to meet the requirements of </st><st c="43502">the application.</st></p>
			<p><st c="43518">Here’s what the LLM will </st><a id="_idIndexMarker112"/><st c="43544">see when you combine this with the RAG-focused prompt </st><st c="43598">from earlier:</st></p>
			<pre class="source-code"><st c="43611">
"You are an assistant for question-answering tasks. </st><st c="43664">Use the following pieces of retrieved context to answer the question. </st><st c="43734">If you don't know the answer, just say that you don't know.
</st><st c="43794">Question:    What are the Advantages of using RAG?
</st><st c="43842">Context:    Can you imagine what you could do with all of the benefits mentioned above, but combined with all of the data within your company, about everything your company has ever done, about your customers and all of their interactions, or about all of your products and services combined with a knowledge of what a specific customer's needs are? </st><st c="44189">You do not have to imagine it, that is what RAG does! </st><st c="44243">Even smaller companies are not able to access much of their internal data resources very effectively. </st><st c="44345">Larger companies are swimming in petabytes of data that are not readily accessible or are not being fully utilized. </st><st c="44461">Before RAG, most of the services you saw that connected customers or employees with the data resources of the company were just scratching the surface of what is possible compared to if they could access ALL of the data in the company. </st><st c="44697">With the advent of RAG and generative AI in general, corporations are on the precipice of something really, really big. </st><st c="44817">Comparing RAG with Model Fine-Tuning#\nEstablished Large Language Models (LLM), what we call the foundation models, can be learned in two ways:\n Fine-tuning - With fine-tuning, you are adjusting the weights and/or biases that define the model\'s intelligence based
[TRUNCATED FOR BREVITY!]
Answer:"</st></pre>
			<p><st c="45116">As you can see, the context is quite large—it returns all of the most relevant information from the original document to help the LLM determine how to answer the new question. </st><st c="45293">The context</st><a id="_idIndexMarker113"/><st c="45304"> is what was returned by the vector similarity search, something we will talk about in more depth in </st><a href="B22475_08.xhtml#_idTextAnchor152"><em class="italic"><st c="45405">Chapter 8</st></em></a><st c="45414">.</st></p>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor054"/><st c="45415">Complete code</st></h1>
			<p><st c="45429">Here is the code in </st><a id="_idIndexMarker114"/><st c="45450">its entirety:</st></p>
			<pre class="source-code"><st c="45463">
%pip install langchain_community
%pip install langchain_experimental
%pip install langchain-openai
%pip install langchainhub
%pip install chromadb
%pip install langchain
%pip install beautifulsoup4</st></pre>
			<p><st c="45661">Restart the kernel before running the </st><st c="45700">following code:</st></p>
			<pre class="source-code"><st c="45715">
import os
from langchain_community.document_loaders import WebBaseLoader
import bs4
import openai
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain import hub
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
import chromadb
from langchain_community.vectorstores import Chroma
from langchain_experimental.text_splitter import SemanticChunker
os.environ['OPENAI_API_KEY'] = 'sk-###################'
openai.api_key = os.environ['OPENAI_API_KEY']
#### INDEXING ####
loader = WebBaseLoader(
    web_paths=("https://kbourne.github.io/chapter1.html",),
    bs_kwargs=dict(parse_only=bs4.SoupStrainer(
                       class_=("post-content",
                               "post-title",
                               "post-header")
                   )
         ),
)
docs = loader.load()
text_splitter = SemanticChunker(OpenAIEmbeddings())
splits = text_splitter.split_documents(docs)
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=OpenAIEmbeddings())
retriever = vectorstore.as_retriever()
#### RETRIEVAL and GENERATION ####
prompt = hub.pull("jclemens24/rag-prompt")
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)
llm = ChatOpenAI(model_name="gpt-4o-mini")
rag_chain = (
    {"context": retriever | format_docs,
     "question": RunnablePassthrough()}
         | prompt
         | llm
         | StrOutputParser()
)
rag_chain.invoke("What are the Advantages of using RAG?")</st></pre>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor055"/><st c="47070">Summary</st></h1>
			<p><st c="47078">This chapter provided a comprehensive code lab that walked through the implementation of a complete RAG pipeline. </st><st c="47193">We began by installing the necessary Python packages, including LangChain, Chroma DB, and various LangChain extensions. </st><st c="47313">Then, we learned how to set up an OpenAI API key, load documents from a web page using </st><code><st c="47400">WebBaseLoader</st></code><st c="47413">, and preprocess the HTML content with BeautifulSoup to extract </st><st c="47477">relevant sections.</st></p>
			<p><st c="47495">Next, the loaded documents were split into manageable chunks using </st><code><st c="47563">SemanticChunker</st></code><st c="47578"> from LangChain’s experimental module. </st><st c="47617">These chunks were then embedded into vector representations using OpenAI’s embedding model and stored in a Chroma DB </st><st c="47734">vector database.</st></p>
			<p><st c="47750">Next, we introduced the concept of a retriever, which is used to perform a vector similarity search on the embedded documents based on a given query. </st><st c="47901">We stepped through the retrieval and generation stages of RAG, which in this case are combined into a LangChain chain using the LCEL. </st><st c="48035">The chain integrates pre-built prompt templates from the LangChain Hub, a selected LLM, and utility functions for formatting retrieved documents and parsing </st><st c="48192">LLM outputs.</st></p>
			<p><st c="48204">Finally, we learned how to submit a question to the RAG pipeline and receive a generated response that incorporates the retrieved context. </st><st c="48344">We saw the output from the LLM model and discussed key considerations for choosing the appropriate model based on accuracy, depth, </st><st c="48475">and cost.</st></p>
			<p><st c="48484">Finally, the complete code for a RAG pipeline was provided! </st><st c="48545">That’s it – you can close this book for now and still be able to build an entire RAG application. </st><st c="48643">Good luck! </st><st c="48654">But before you go, there are still many concepts to review so that you can optimize your RAG pipeline. </st><st c="48757">If you do a quick search of the web for </st><code><st c="48797">trouble with RAG</st></code><st c="48813"> or something similar, you will likely find millions of questions and problems highlighted where RAG applications have issues with all but the simplest of applications. </st><st c="48982">There are also many other solutions that RAG can solve that need the code just provided to be adjusted. </st><st c="49086">The rest of this book is dedicated to helping you build up knowledge that will help you get past any of these problems and form many new solutions. </st><st c="49234">If you hit a similar challenge, don’t despair! </st><st c="49281">There is a solution! </st><st c="49302">It just might take the time to go beyond </st><a href="B22475_02.xhtml#_idTextAnchor035"><em class="italic"><st c="49343">Chapter 2</st></em></a><st c="49352">!</st></p>
			<p><st c="49353">In the next chapter, we will take some of the practical applications we discussed in </st><a href="B22475_01.xhtml#_idTextAnchor015"><em class="italic"><st c="49438">Chapter 1</st></em></a><st c="49447"> and dive much deeper into how they are being implemented in various organizations. </st><st c="49531">We will also provide some hands-on code related to one of the most common practical applications of RAG: providing the sources of the content that the RAG application is quoting </st><st c="49709">to you.</st></p>
		</div>
	<div></body></html>