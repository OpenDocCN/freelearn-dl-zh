<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-194"><a id="_idTextAnchor207"/>11</h1>
<h1 id="_idParaDest-195"><a id="_idTextAnchor208"/>Evaluating and Monitoring Models with Amazon Bedrock</h1>
<p>To work with the best-performing model for your generative AI solution, you need to evaluate the models that are available to you. This chapter explores various techniques to assess the performance of different models.</p>
<p>The chapter introduces two primary evaluation methods provided by Amazon Bedrock: automatic model evaluation and human evaluation. We will do a detailed walk-through of these two methods. In addition, we will look at open source tools such as <strong class="bold">Foundation Models Evaluation</strong> (<strong class="bold">FMEval</strong>) and <strong class="bold">RAG Assessment</strong> (<strong class="bold">Ragas</strong>) for model<a id="_idIndexMarker896"/> evaluation<a id="_idIndexMarker897"/> and evaluating RAG pipelines.</p>
<p>The second part of the chapter goes into monitoring. We will explore how to leverage Amazon CloudWatch for real-time monitoring of model performance, latency, and token counts. We will further look at model invocation logging to capture requests, responses, and metadata for model invocations. Furthermore, we will highlight the integration of Amazon Bedrock with AWS CloudTrail for auditing API calls and with Amazon EventBridge for event-driven monitoring and automation of model customization jobs.</p>
<p>By the end of this chapter, you will be able to understand how to evaluate FMs and monitor their performance.</p>
<p>Here are the key topics that will be covered in this chapter:</p>
<ul>
<li>Evaluating models</li>
<li>Monitoring Amazon Bedroc<a id="_idTextAnchor209"/><a id="_idTextAnchor210"/><a id="_idTextAnchor211"/>k</li>
</ul>
<h1 id="_idParaDest-196"><a id="_idTextAnchor212"/>Technical requirements</h1>
<p>This chapter requires you to have access to an AWS account. If you don’t have it already, you can go to <a href="https://aws.amazon.com/getting-started/">https://aws.amazon.com/getting-started/</a> and create an AWS account.</p>
<p>Secondly, you will need to set up AWS Python SDK (Boto3): <a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html">https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html</a></p>
<p>You can carry out the Python setup in any way: install it on your local machine, or use AWS Cloud9, or utilize AWS Lambda, or leverage Amazon SageMaker.</p>
<p class="callout-heading">Note</p>
<p class="callout">There will be a charge associated with the invocation and customization of the FMs of Amazon Bedrock. Please refer to <a href="https://aws.amazon.com/bedrock/pricing/">https://aws.amazon.com/bedrock/pricing/</a> to learn more.</p>
<h1 id="_idParaDest-197"><a id="_idTextAnchor213"/>Evaluating models</h1>
<p>By now, we have<a id="_idIndexMarker898"/> gained a comprehensive understanding of Amazon Bedrock’s capabilities, exploring techniques such as prompt engineering, RAG, and model customization. We have also examined various architectural design patterns and analyzed the responses generated by different models. With the vast selection of FMs available within Amazon Bedrock, identifying the most suitable option for your specific use case and business requirements can be challenging. To address this, we will now focus on the topic of model evaluation and how to compare the outputs of different models to choose the one that best meets the needs of your application and business. This is a critical initial phase in implementing any generative AI solution.</p>
<div><div><img alt="Figure 11.1 – The generative AI life cycle" src="img/B22045_11_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – The generative AI life cycle</p>
<p>As shown in <em class="italic">Figure 11</em><em class="italic">.1</em>, after defining the specific business use case that you’re aiming to solve with generative AI, the <strong class="bold">choice</strong> stage<a id="_idIndexMarker899"/> involves both selecting potential models from the available options and rigorously evaluating these candidate models. Following <a id="_idIndexMarker900"/>this, the <strong class="bold">responsible AI</strong> stage focuses on ensuring data privacy and security, as well as implementing guardrails for responsible model behavior, which we will cover in <a href="B22045_12.xhtml#_idTextAnchor226"><em class="italic">Chapter 12</em></a>.</p>
<p>Before talking about model evaluation, one quick way to compare the responses from the models on Amazon<a id="_idIndexMarker901"/> Bedrock’s <strong class="bold">Chat playground</strong> screen is to use the <strong class="bold">Compare </strong><strong class="bold">mode</strong> toggle.</p>
<div><div><img alt="Figure 11.2 – Compare mode in Bedrock’s Chat playground" src="img/B22045_11_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – Compare mode in Bedrock’s Chat playground</p>
<div><div><img alt="Figure 11.3 – Model metrics in Compare mode" src="img/B22045_11_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – Model metrics in Compare mode</p>
<p>As shown in <em class="italic">Figure 11</em><em class="italic">.2</em>, you can enable compare mode in <strong class="bold">Chat playground</strong> and add up to three models to view a side-by-side comparison of model responses to the same prompt. In addition, you can view the metrics of each of the selected models (as shown in <em class="italic">Figure 11</em><em class="italic">.3</em>) and compare latency, input token count, output token count, and the associated cost. We will<a id="_idIndexMarker902"/> cover the model metrics in depth in the <em class="italic">Monitoring Amazon </em><em class="italic">Bedrock</em> section.</p>
<h2 id="_idParaDest-198"><a id="_idTextAnchor214"/>Using Amazon Bedrock</h2>
<p>Within Amazon<a id="_idIndexMarker903"/> Bedrock, you <a id="_idIndexMarker904"/>can create model evaluation jobs to compare the responses from the models for use cases such as text generation, summarization, Q&amp;A, and so on.</p>
<p>Model evaluation within Amazon Bedrock primarily consists of two options:</p>
<ul>
<li>Automatic model evaluation</li>
<li>Human evaluation</li>
</ul>
<p>Let us dive deeper into both of these evaluation techniques.</p>
<h2 id="_idParaDest-199"><a id="_idTextAnchor215"/>Automatic model evaluation</h2>
<p>With automatic<a id="_idIndexMarker905"/> model evaluation, an evaluation algorithm script is run behind the scenes on either Amazon Bedrock’s <a id="_idIndexMarker906"/>provided <strong class="bold">built-in dataset </strong>or on a <strong class="bold">custom dataset</strong> that you provide<a id="_idIndexMarker907"/> for the recommended metrics (accuracy, toxicity, and robustness). Let us go through the steps for creating an automatic model evaluation job:</p>
<ol>
<li><strong class="bold">Evaluation name</strong>: This refers to choosing a descriptive name that accurately reflects the purpose of the job. This name should be unique within your AWS account in the specific AWS region.</li>
<li><strong class="bold">Model selector</strong>: Select the model that you would like to evaluate (as shown in <em class="italic">Figure 11</em><em class="italic">.4</em>). At the time of writing this book, automatic model evaluation is carried out on a single model.</li>
</ol>
<div><div><img alt="Figure 11.4 – Model selector" src="img/B22045_11_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – Model selector</p>
<p class="list-inset">Within <strong class="bold">Model selector</strong>, you can optionally modify the inference parameters, such as <strong class="bold">Temperature</strong>, <strong class="bold">Top P</strong>, <strong class="bold">Response length</strong>, and so on (as shown in <em class="italic">Figure 11</em><em class="italic">.5</em>). You can get this screen by clicking on <strong class="bold">Inference configuration: </strong><strong class="bold">Default update</strong>.</p>
<p class="list-inset">Modifying the values of this inference configuration will alter the output of the model. To learn more<a id="_idIndexMarker908"/> about the inference configuration<a id="_idIndexMarker909"/> parameters, you can go back to <a href="B22045_02.xhtml#_idTextAnchor034"><em class="italic">Chapter 2</em></a> of this book.</p>
<div><div><img alt="Figure 11.5 – Inference configuration" src="img/B22045_11_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – Inference configuration</p>
<ol>
<li value="3"><strong class="bold">Task type</strong>: Currently, the following task types are supported while using Amazon Bedrock Model evaluation:<ul><li>General text generation</li><li>Text summarization</li><li>Question and answer</li><li>Text classification</li></ul></li>
<li><strong class="bold">Metrics and datasets</strong>: There are a total of three metrics provided by Amazon Bedrock that you can choose to use to measure the performance of the model:<ul><li><strong class="bold">Accuracy</strong>: The capability to encode factual knowledge about the real world is a critical aspect of generative AI models. This metric evaluates the model’s ability <a id="_idIndexMarker910"/>to generate outputs that align with established facts and data. It assesses<a id="_idIndexMarker911"/> the model’s understanding of the subject matter and its ability to synthesize information accurately. A high accuracy score indicates that the model’s outputs are reliable and can be trusted for tasks that require factual precision.</li><li><strong class="bold">Toxicity</strong>: This refers to the propensity of the model to generate harmful, offensive, or inappropriate content. This metric gauges the model’s tendency to produce outputs that could be considered unethical, biased, or discriminatory. Evaluating toxicity is crucial for ensuring the responsible and ethical deployment of AI systems, particularly in applications that involve direct interaction with users or the dissemination of information to the public.</li><li><strong class="bold">Robustness</strong>: Robustness is a measure of the model’s resilience to minor, semantic-preserving changes in the input data. It assesses the degree to which the model’s output remains consistent and reliable when faced with slight variations or perturbations in the input. This metric is particularly important for generative AI models that operate in dynamic or noisy environments, where the input data may be subject to minor fluctuations or disturbances. A robust model is less likely to produce erratic or inconsistent outputs in<a id="_idIndexMarker912"/> response to small input changes.</li></ul></li>
</ol>
<p>The text classification task supports the accuracy and robustness metrics, while the other tasks support all three metrics.</p>
<p>For every task type and metric that you choose, Amazon Bedrock provides you with built-in datasets. For example, for the general text generation task type, you will get the following built-in datasets:</p>
<ul>
<li>TREX: <a href="https://hadyelsahar.github.io/t-rex/">https://hadyelsahar.github.io/t-rex/</a></li>
<li>BOLD: <a href="https://github.com/amazon-science/bold">https://github.com/amazon-science/bold</a></li>
<li>WikiText2: <a href="https://huggingface.co/datasets/wikitext">https://huggingface.co/datasets/wikitext</a></li>
<li>English Wikipedia: <a href="https://en.wikipedia.org/wiki/Wikipedia:Database_download">https://en.wikipedia.org/wiki/Wikipedia:Database_download</a></li>
<li>RealToxicityPrompts: <a href="https://github.com/allenai/real-toxicity-prompts">https://github.com/allenai/real-toxicity-prompts</a></li>
</ul>
<p>For a complete list of built-in datasets based on different metrics and task types, you can go through <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-prompt-datasets-builtin.html">https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-prompt-datasets-builtin.html</a>.</p>
<p>If you would like <a id="_idIndexMarker913"/>to use your own custom dataset, it needs to be <em class="italic">JSON line</em> (<em class="italic">.jsonl</em>) format. Each line within the dataset must be a valid JSON object and you can include up to 1,000 prompts per evaluation job.</p>
<p>To construct your custom prompt dataset, you’ll need to incorporate the following keys:</p>
<ul>
<li><strong class="bold">prompt</strong>: This key is mandatory and serves as the input for various tasks, such as general text generation, question answering, text summarization, and classification. Depending on the task, the value associated with this key will vary – it could be a prompt for the model to respond to, a question to answer, text to summarize, or content to classify.</li>
<li><code>referenceResponse</code> is required to provide the ground truth response against which your model’s output will be evaluated. For tasks such as question answering, accuracy evaluation, and robustness testing, this key will hold the correct answer or expected response.</li>
<li><code>category</code> key. This optional key allows you to group prompts and their corresponding reference responses, enabling a more granular analysis of your model’s performance across different domains or categories.</li>
</ul>
<p>To illustrate the <a id="_idIndexMarker914"/>usage of these keys, consider the following example for a question answering task:</p>
<pre class="source-code">
{"prompt":"What is the process that converts raw materials into finished goods?", "category":"Manufacturing", "referenceResponse":"Manufacturing"}
{"prompt":"What is the study of methods to improve workplace efficiency?", "category":"Manufacturing", "referenceResponse":"Industrial Engineering"}
{"prompt":"What is the assembly of parts into a final product?", "category":"Manufacturing", "referenceResponse":"Assembly"}
{"prompt":"A computerized system that monitors and controls production processes is called", "category":"Manufacturing", "referenceResponse":"SCADA"}
{"prompt":"A system that minimizes waste and maximizes efficiency is called", "category":"Manufacturing", "referenceResponse":"Lean Manufacturing"}</pre>
<p>In this JSON line, the <code>prompt</code> key contains the <code>What is the process that converts raw materials into finished goods?</code> question, while the <code>referenceResponse</code> key holds the correct answer, <code>Manufacturing</code>. Additionally, the category key is set to <code>Manufacturing</code>, allowing you to group this prompt and response with others related to manufacturing.</p>
<p>Once you have created a custom prompt dataset, you will need to store the dataset file in an Amazon S3 bucket and specify the correct S3 path (such as <strong class="bold">s3://test/data/</strong>) when creating the model evaluation job (as shown in <em class="italic">Figure 11</em><em class="italic">.6</em>).</p>
<div><div><img alt="Figure 11.6 – Choosing a prompt dataset" src="img/B22045_11_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6 – Choosing a prompt dataset</p>
<p>Please note that the S3 <a id="_idIndexMarker915"/>bucket should have the <a id="_idIndexMarker916"/>following <strong class="bold">Cross-Origin Resource Sharing</strong> (<strong class="bold">CORS</strong>) policy attached:</p>
<pre class="source-code">
[{
"AllowedHeaders": ["*"],
"AllowedMethods": ["GET","POST","PUT","DELETE"],
"AllowedOrigins": ["*"],
"ExposeHeaders": ["Access-Control-Allow-Origin"]
}]</pre>
<p>The CORS policy is a set of rules that specify which origins (domains or websites) are allowed to access the S3 bucket. To learn more about CORS, you can check <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors.html</a>.</p>
<p>By meticulously crafting your custom prompt dataset, you can ensure that your LLMs are thoroughly evaluated against a diverse range of scenarios, covering various tasks, domains, and complexity levels.</p>
<div><div><img alt="Figure 11.7 – Metrics and datasets" src="img/B22045_11_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7 – Metrics and datasets</p>
<p><em class="italic">Figure 11</em><em class="italic">.7</em> depicts <a id="_idIndexMarker917"/>the <strong class="bold">Metrics and Datasets</strong> options for the general text generation task type.</p>
<p>Let us look at the other parameters that you can specify while creating the model evaluation job:</p>
<ul>
<li><strong class="bold">Evaluation results</strong>: Here, you<a id="_idIndexMarker918"/> can specify the S3 path where the results of the evaluation job should be stored. We will cover the evaluation results in the next section.</li>
<li><strong class="bold">IAM role and KMS key</strong>: Certain permissions are required to perform actions such as accessing data from an S3 bucket or storing the evaluation results. Here is the policy that is needed, at minimum, for an automatic model evaluation job:<pre class="source-code">
{</pre><pre class="source-code">
    "Version": "2012-10-17",</pre><pre class="source-code">
    "Statement": [</pre><pre class="source-code">
        {</pre><pre class="source-code">
            "Sid": "BedrockConsole",</pre><pre class="source-code">
            "Effect": "Allow",</pre><pre class="source-code">
            "Action": [</pre><pre class="source-code">
               "bedrock:CreateEvaluationJob",</pre><pre class="source-code">
               "bedrock:GetEvaluationJob",</pre><pre class="source-code">
               "bedrock:ListEvaluationJobs",</pre><pre class="source-code">
               "bedrock:StopEvaluationJob",</pre><pre class="source-code">
               "bedrock:GetCustomModel",</pre><pre class="source-code">
               "bedrock:ListCustomModels",</pre><pre class="source-code">
               "bedrock:CreateProvisionedModel</pre><pre class="source-code">
Throughput",</pre><pre class="source-code">
               "bedrock:UpdateProvisionedModel</pre><pre class="source-code">
Throughput",</pre><pre class="source-code">
               "bedrock:GetProvisionedModel</pre><pre class="source-code">
Throughput",</pre><pre class="source-code">
               "bedrock:ListProvisionedModel</pre><pre class="source-code">
Throughputs",</pre><pre class="source-code">
               "bedrock:ListTagsForResource",</pre><pre class="source-code">
               "bedrock:UntagResource",</pre><pre class="source-code">
               "bedrock:TagResource"</pre><pre class="source-code">
            ],</pre><pre class="source-code">
            "Resource": "*"</pre><pre class="source-code">
        },</pre><pre class="source-code">
        {</pre><pre class="source-code">
            "Sid": "AllowConsoleS3AccessForModelEvaluation",</pre><pre class="source-code">
            "Effect": "Allow",</pre><pre class="source-code">
            "Action": [</pre><pre class="source-code">
              "s3:GetObject",</pre><pre class="source-code">
              "s3:GetBucketCORS",</pre><pre class="source-code">
              "s3:ListBucket",</pre><pre class="source-code">
              "s3:ListBucketVersions",</pre><pre class="source-code">
              "s3:GetBucketLocation"</pre><pre class="source-code">
            ],</pre><pre class="source-code">
            "Resource": "*"</pre><pre class="source-code">
        }</pre><pre class="source-code">
    ]</pre><pre class="source-code">
}</pre></li>
</ul>
<p>You can find details on the<a id="_idIndexMarker919"/> permissions needed for a model evaluation job at <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-security.html?icmpid=docs_bedrock_help_panel_model_evaluation">https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-security.html?icmpid=docs_bedrock_help_panel_model_evaluation</a>.</p>
<h2 id="_idParaDest-200"><a id="_idTextAnchor216"/>Model evaluation results</h2>
<p>Once you start the <a id="_idIndexMarker920"/>model evaluation job, you can view the results for each of the metrics as shown in <em class="italic">Figure 11</em><em class="italic">.8</em>.</p>
<div><div><img alt="Figure 11.8 – An evaluation summary" src="img/B22045_11_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8 – An evaluation summary</p>
<p>In addition, the metrics results are stored in the S3 bucket that you have specified, as shown in <em class="italic">Figure 11</em><em class="italic">.9</em>.</p>
<div><div><img alt="Figure 11.9 – Metrics results in the S3 bucket" src="img/B22045_11_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.9 – Metrics results in the S3 bucket</p>
<p>Let us understand how the evaluation is performed for each of the task types.</p>
<h3>Text generation</h3>
<p>For text generation<a id="_idIndexMarker921"/> task type, here is how the evaluation is performed:</p>
<ul>
<li><strong class="bold">Accuracy</strong>: This metric is evaluated using the <strong class="bold">Real World Knowledge</strong> (<strong class="bold">RWK</strong>) score, which <a id="_idIndexMarker922"/>assesses the model’s ability to understand the real world. The RWK score measures the extent to which a language model can produce output that is consistent with real-world facts and common sense. It assesses the model’s ability to reason about the physical world, understand social norms, and avoid generating nonsensical or contradictory statements. A high RWK score indicates that the model is performing accurately.</li>
<li><strong class="bold">Robustness</strong>: Semantic robustness is<a id="_idIndexMarker923"/> the metric used to measure robustness in this task type. It is calculated using the word error rate, which quantifies how much the model’s output changes in response to minor, semantic-preserving perturbations in the input. A low semantic robustness score signifies that the model is performing well, as it is robust to such perturbations.</li>
<li><strong class="bold">Toxicity</strong>: This metric is calculated using the detoxify algorithm (<a href="https://github.com/unitaryai/detoxify">https://github.com/unitaryai/detoxify</a>), which measures the presence of toxic content in the model’s output. A low toxicity value indicates that the selected model is not<a id="_idIndexMarker924"/> generating a significant amount of harmful or offensive content.</li>
</ul>
<h3>Text summarization</h3>
<p>For text summarization <a id="_idIndexMarker925"/>task type, here is how the evaluation is performed:</p>
<ul>
<li><strong class="bold">Accuracy</strong>: The BERTScore is used to evaluate accuracy in this task type. It is calculated using pre-trained contextual embeddings from BERT models and matches words in candidate and reference sentences by cosine similarity.</li>
<li><strong class="bold">Robustness</strong>: This metric is expressed as a percentage and is calculated by taking the difference between the BERTScores of a perturbed prompt and the original prompt, which is then divided by the BERTScore of the original prompt and multiplied by 100. The lower the score, the more robust the selected model is.</li>
<li><strong class="bold">Toxicity</strong>: As with the general text generation task type, the detoxify algorithm is used to calculate the toxicity of the model’s output, with a low value indicating minimal toxic content generation.</li>
</ul>
<h3>Question and answer</h3>
<p>For question and answer <a id="_idIndexMarker926"/>task type, here is how the evaluation is performed:</p>
<ul>
<li><strong class="bold">Accuracy</strong>: The F1 score is used to evaluate accuracy in this task type. It is calculated by dividing the precision score (the ratio of correct predictions to all predictions) by the recall score (the ratio of correct predictions to the total number of relevant predictions). Higher F1 scores indicate better performance.</li>
<li><strong class="bold">Robustness</strong>: This metric is expressed as a percentage and is calculated by taking the difference between the F1 scores of a perturbed prompt and the original prompt, which is then divided by the F1 score of the original prompt and multiplied by 100. A lower score indicates that the selected model is more robust.</li>
<li><strong class="bold">Toxicity</strong>: As with the other task types, the detoxify algorithm is used to calculate the toxicity of the model’s output, with a low value indicating minimal toxic content generation.</li>
</ul>
<h3>Text classification</h3>
<p>For text classification<a id="_idIndexMarker927"/> task type, here is how the evaluation is performed:</p>
<ul>
<li><strong class="bold">Accuracy</strong>: In this task type, accuracy is calculated by comparing the predicted class to its ground truth label. A higher accuracy score indicates that the model is correctly classifying text based on the provided ground truth labels.</li>
<li><strong class="bold">Robustness</strong>: This metric is expressed as a percentage and is calculated by taking the difference between the classification accuracy scores of a perturbed prompt and the original prompt, which is then divided by the classification accuracy score of the original prompt and multiplied by 100. A lower score indicates that the selected model is more robust.</li>
</ul>
<p>Let us analyze the output of the accuracy and robustness metrics for the built-in dataset from T-Rex: (<a href="https://hadyelsahar.github.io/t-rex/">https://hadyelsahar.github.io/t-rex/</a>):</p>
<pre class="source-code">
{"automatedEvaluationResult":{"scores":[{"metricName":"Accuracy","result":1},{"metricName":"Robustness","result":0.0828}]},"inputRecord":{"prompt":"al-Hira is the capital of","referenceResponse":"Lakhmid"},"modelResponses":[{"response":" Al-Hira was an ancient city in Mesopotamia located south of what is now Kufa in modern-day Iraq. It served as the capital of the Arab Lakhmids from the 3rd to the 6th century CE.\n\nSome key facts about al-Hira:\n\n- It was founded in the 3rd century CE and served as the capital of the Lakhmids, who were Arab vassals of the Sassanian Persian Empire. \n\n- Under the leadership of the Lakhmid kings, al-Hira flourished as a center of Arab culture and politics in pre-Islamic Arabia. It was a major center of Nestorian Christianity in the region.\n\n- The city was located along important trade routes linking Mesopotamia to the Arabian peninsula. Its markets attracted merchants from throughout Arabia and beyond.\n\n- Al-Hira and the Lakhmids were defeated by the Sassanians in 602 CE. The city subsequently went into decline and was eventually abandoned. \n\n- The ruins of al-Hira are located about 94 km south of modern-day Baghdad, Iraq. Sections of its walls and some buildings have been excavated by archaeologists.\n\nSo in summary, al-Hira functioned as the capital of the Arab Lakhmids kingdom for several centuries until its downfall in the early 7th century CE. Its ruins stand as an","modelIdentifier":"anthropic.claude-v2"}]}</pre>
<p>The output shows that a prompt (<code>al-Hira is the capital of</code>) is provided to the Anthropic Claude v2 model, and that the model’s response (<code>Al-Hira was an ancient city in Mesopotamia...</code>) is assessed against a reference response (<code>Lakhmid</code>). The<a id="_idIndexMarker928"/> evaluation computes scores for metrics such as accuracy and robustness, providing insights into the model’s performance on this specific input.</p>
<h2 id="_idParaDest-201"><a id="_idTextAnchor217"/>Using human evaluation</h2>
<p>Human evaluation <a id="_idIndexMarker929"/>allows you to incorporate human input into<a id="_idIndexMarker930"/> the evaluation process, so the models are not only accurate but also align with real-world expectations and requirements. There are two types of human evaluation:</p>
<ul>
<li>Bringing your own work team</li>
<li>Using an AWS-managed work team</li>
</ul>
<h3>Bringing your own work team</h3>
<p>Similar to automatic <a id="_idIndexMarker931"/>model evaluation, when you choose human evaluation with your own work team, Amazon Bedrock guides you through a straightforward setup process, allowing you to select the models you want to evaluate, the task type (for example, text summarization), and the evaluation metrics. It also shows you how to upload your custom prompt dataset. Let us consider the step-by-step process for setting up human evaluation by bringing your own team:</p>
<ol>
<li><strong class="bold">Evaluation name</strong>: Choose a<a id="_idIndexMarker932"/> descriptive name that accurately represents the purpose of the job. This name should be unique within your AWS account in the specific AWS region. Along with the name, you can optionally provide the description and tags.</li>
<li><strong class="bold">Model selector</strong>: Select the model that you would like to evaluate (as shown in <em class="italic">Figure 11</em><em class="italic">.10</em>). At the time of writing this book, human model evaluation with bringing your own team can only performed on up to two models. Within the model selector, you can optionally modify the inference parameters such as temperature, Top P, response length, and so on.</li>
</ol>
<div><div><img alt="Figure 11.10 – The model selector" src="img/B22045_11_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.10 – The model selector</p>
<ol>
<li value="3"><strong class="bold">Task Type</strong>: Currently, the<a id="_idIndexMarker933"/> following task types are supported in this mode:<ul><li>General text<a id="_idIndexMarker934"/> generation</li><li>Text summarization</li><li>Question and answer</li><li>Text classification</li><li>Custom</li></ul><p class="list-inset">The last task type of these, Custom, allows you to specify custom evaluation metrics that the human workers can use.</p><p class="list-inset">Based on the task type, you will see the list of evaluation metrics and rating methods that you would have to choose from, as shown in <em class="italic">Figures 11.11</em> and <em class="italic">11.12</em>.</p></li>
</ol>
<div><div><img alt="Figure 11.11 – Evaluation metrics" src="img/B22045_11_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.11 – Evaluation metrics</p>
<div><div><img alt="Figure 11.12 – Rating method options" src="img/B22045_11_12.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.12 – Rating method options</p>
<ol>
<li value="4"><strong class="bold">Specifying paths</strong>: Next, you will need to specify the s3 path to your custom prompt dataset. As we have seen in the previous subsection, a custom prompt dataset needs to be in the <em class="italic">.jsonl</em> format. Here is an example of the custom prompt dataset:<pre class="source-code">
{"prompt":"What is the process that converts raw materials into finished goods?", "category":"Manufacturing", "referenceResponse":"Manufacturing"}</pre><pre class="source-code">
{"prompt":"What is the study of methods to improve workplace efficiency?", "category":"Manufacturing", "referenceResponse":"Industrial Engineering"}</pre><pre class="source-code">
{"prompt":"What is the assembly of parts into a final product?", "category":"Manufacturing", "referenceResponse":"Assembly"}</pre><p class="list-inset">Please note that <a id="_idIndexMarker935"/>the s3 path to your dataset<a id="_idIndexMarker936"/> requires you to have your <strong class="bold">Cross Origin Resource Sharing (CORS)</strong> settings configured as shown in <em class="italic">Figure 11</em><em class="italic">.13</em>.</p></li>
</ol>
<div><div><img alt="Figure 11.13 – The CORS policy window" src="img/B22045_11_13.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.13 – The CORS policy window</p>
<p class="list-inset">To learn more, you can visit <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-security-cors.html">https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-security-cors.html</a>.</p>
<ol>
<li value="5"><strong class="bold">IAM role and KMS key</strong>: Certain <a id="_idIndexMarker937"/>permissions are required to perform actions such as accessing data from an S3 bucket or storing the evaluation results. You can find more details on the IAM permissions needed for the model evaluation job at <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-security.html?icmpid=docs_bedrock_help_panel_model_evaluation">https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-security.html?icmpid=docs_bedrock_help_panel_model_evaluation</a>.</li>
<li><strong class="bold">Setting up a work team</strong>: Next, you will need to set up a work team (as shown in <em class="italic">Figure 11</em><em class="italic">.14</em>). This involves inviting the appropriate team members. The console provides you with sample email templates for inviting new workers and existing workers, which you can use as a reference when sending out the invitations. The <a id="_idIndexMarker938"/>worker receives the link to the private worker portal where they complete the labeling task.</li>
</ol>
<div><div><img alt="Figure 11.14 – Setting up a custom work team" src="img/B22045_11_14.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.14 – Setting up a custom work team</p>
<ol>
<li value="7">Next, you would need to specify the instructions (as shown in <em class="italic">Figure 11</em><em class="italic">.15</em>) of the task to the <a id="_idIndexMarker939"/>workers. These instructions will be visible to the human workers in the private worker <a id="_idIndexMarker940"/>portal where they will perform the labeling task.</li>
</ol>
<p class="IMG---Figure"> </p>
<div><div><img alt="Figure 11.15 – Providing instructions to human workers" src="img/B22045_11_15.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.15 – Providing instructions to human workers</p>
<p>After you have reviewed and created the job, the human worker team will receive an email along with the link to the portal to perform the task. Once the task has been completed by the workers, Amazon Bedrock will provide an evaluation report card. Let us understand the report card in detail:</p>
<ul>
<li><code>1</code> to <code>5</code>. When using this method, it is essential to provide clear instructions that define the meaning of each rating point. For instance, a rating of <code>1</code> could indicate a poor or irrelevant response, while a rating of <code>5</code> could signify an excellent and highly relevant output. The results are then presented as a histogram showcasing the distribution of ratings across the dataset:<ul><li>Define the scale points explicitly (for example, 1 = poor, 2 = fair, 3 = good, 4 = very good, 5 = excellent)</li><li>Provide clear guidelines for evaluators on how to interpret and apply the scale</li><li>Present the results as a histogram, allowing for easy visual interpretation of the rating distribution</li></ul></li>
<li><strong class="bold">Choice buttons</strong>: When you choose the choice buttons method, evaluators are presented with two model responses and asked to select their preferred option. This approach is particularly useful when comparing the performance of multiple models on the same task. The results are typically reported as a percentage, indicating<a id="_idIndexMarker942"/> the proportion of responses that evaluators preferred for each model.</li>
<li><code>1</code> (most preferred). This method provides a more nuanced understanding of the relative performance of different models. The results are presented as a histogram showing the distribution of rankings across the dataset.</li>
<li><strong class="bold">Thumbs up/down</strong>: Evaluators rate each response as acceptable or unacceptable. The final report showcases the percentage of responses that received a <strong class="bold">thumbs-up</strong> rating for each model, enabling a straightforward assessment of acceptability.</li>
</ul>
<p>Another form of <a id="_idIndexMarker943"/>human evaluation method is through an AWS-managed work team.</p>
<h3>Using an AWS-managed work team</h3>
<p>If you opt for an<a id="_idIndexMarker944"/> AWS-managed <a id="_idIndexMarker945"/>team, you can simply describe your model evaluation needs, including the task type, expertise level required, and approximate number of prompts. Based on these details, an AWS expert will then reach out to discuss your project requirements in detail, providing a custom quote and project timeline tailored to your specific needs.</p>
<p><em class="italic">Figure 11</em><em class="italic">.16</em> shows how you can create a managed workforce with all the details such as task type and expertise required.</p>
<div><div><img alt="Figure 11.16 – AWS-managed team for model evaluation" src="img/B22045_11_16.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.16 – AWS-managed team for model evaluation</p>
<p>An AWS-managed workforce is useful when you do not want to manage or assign tasks to your own workforce and when you require an AWS team to perform evaluations on your behalf.</p>
<p>Aside from using <a id="_idIndexMarker946"/>Bedrock’s model <a id="_idIndexMarker947"/>evaluation job, there are other open source techniques that you can utilize for model evaluation, such as <code>fmeval</code> and Ragas.</p>
<h3>FMEval</h3>
<p>FMEval is<a id="_idIndexMarker948"/> the open <a id="_idIndexMarker949"/>source library made available by AWS, which you can access at <a href="https://github.com/aws/fmeval">https://github.com/aws/fmeval</a>.</p>
<p>This library enables comprehensive evaluation of LLMs across various aspects such as accuracy, toxicity, semantic robustness, and prompt stereotyping. It offers a range of algorithms tailored for assessing LLMs’ performance on different tasks, ensuring a thorough understanding of their capabilities and limitations.</p>
<p>If you plan to use your own dataset for evaluation, you’ll need to configure a <code>DataConfig</code> object, like in the following code block. This object specifies the dataset name, URI, and MIME type, as well as the locations of the input prompts, target outputs, and other relevant columns. By customizing the <code>DataConfig</code> object, you can tailor the evaluation <a id="_idIndexMarker950"/>process to your specific dataset and task requirements:</p>
<pre class="source-code">
from fmeval.data_loaders.data_config import DataConfig
from fmeval.constants import MIME_TYPE_JSONLINES
from fmeval.model_runners.bedrock_model_runner import BedrockModelRunner
fmconfig = DataConfig(
    dataset_name="dataset",
    dataset_uri="dataset.jsonl",
    dataset_mime_type=MIME_TYPE_JSONLINES,
    model_input_location="question",
    target_output_location="answer",
)</pre>
<p>The library provides a flexible <code>ModelRunner</code> interface, allowing for seamless integration with Amazon Bedrock, and is used to perform invocations on the model. The following code block shows how an invocation can be carried out:</p>
<pre class="source-code">
bedrock_model_runner = BedrockModelRunner(
    model_id='anthropic.claude-v2',
    output='completion',
    content_template='{"prompt": $prompt, "max_tokens_to_sample": 500}'
)</pre>
<p>If you want to learn more about <code>fmeval</code>, you can visit <a href="https://github.com/aws/fmeval/tree/main">https://github.com/aws/fmeval/tree/main</a>.</p>
<p>In addition, you can try out <code>fmeval</code> with Amazon Bedrock. Here are some sample examples<a id="_idIndexMarker951"/> that <a id="_idIndexMarker952"/>you can test with Anthropic Claude v2:</p>
<ul>
<li><a href="https://github.com/aws/fmeval/blob/main/examples/bedrock-claude-factual-knowledge.ipynb">https://github.com/aws/fmeval/blob/main/examples/bedrock-claude-factual-knowledge.ipynb</a></li>
<li><a href="https://github.com/aws/fmeval/blob/main/examples/bedrock-claude-summarization-accuracy.ipynb">https://github.com/aws/fmeval/blob/main/examples/bedrock-claude-summarization-accuracy.ipynb</a></li>
</ul>
<h3>Ragas</h3>
<p>Ragas is a framework <a id="_idIndexMarker953"/>designed to assess the performance of <a id="_idIndexMarker954"/>your RAG pipelines, which combine language models with external data sources to enhance their output. It offers practical tools grounded in the latest research to analyze the text generated by your language model, giving you valuable insights into the effectiveness of your RAG pipeline.</p>
<p>Here are some key features and benefits of using Ragas:</p>
<ul>
<li><strong class="bold">Automated evaluation metrics</strong>: Ragas <a id="_idIndexMarker955"/>offers a suite of automated metrics tailored specifically for assessing the quality of RAG-generated text. These metrics go beyond traditional measures such as perplexity and BLEU, providing a more nuanced understanding of the generated output’s coherence, relevance, and factual accuracy.</li>
<li><strong class="bold">Customizable evaluation strategies</strong>: Recognizing that every RAG pipeline is unique, Ragas allows for flexible and customizable evaluation strategies. You can tailor the evaluation process to align with your specific use case, data domain, and performance requirements.</li>
<li><strong class="bold">Integration with CI/CD pipelines</strong>: Ragas is designed to integrate with <strong class="bold">CI/CD</strong> (<strong class="bold">Continuous Integration and Continuous Deployment</strong>) pipelines. This integration<a id="_idIndexMarker956"/> enables continuous monitoring and evaluation of your RAG pipeline’s performance, ensuring that any deviations or regressions are promptly detected and addressed.</li>
<li><strong class="bold">Interpretable insights</strong>: Ragas generates interpretable and actionable insights, highlighting areas where your RAG pipeline excels and identifying potential weaknesses or bottlenecks. These insights can guide your optimization efforts, helping you refine<a id="_idIndexMarker957"/> and enhance your pipeline’s performance iteratively.</li>
</ul>
<p>Ragas provides a<a id="_idIndexMarker958"/> list of metrics that you can import. Here is how:</p>
<pre class="source-code">
from ragas.metrics import (
    context_precision,
    faithfulness,
    context_recall,
)
from ragas.metrics.critique import harmfulness
metrics = [
    faithfulness,
    context_recall,
    context_precision,
    harmfulness,
]</pre>
<p>These metrics can then be passed to the <code>evaluate</code> function in Ragas, along with the Bedrock model and embeddings. Here is how:</p>
<pre class="source-code">
from ragas import evaluate
results = evaluate(
    df["eval"].select(range(3)),
    metrics=metrics,
    llm=bedrock_model,
    embeddings=bedrock_embeddings,
)
results</pre>
<p>In the preceding code snippet, <code>df</code> is assumed to be a pandas DataFrame containing the data you want to evaluate. Note that <code>llm=bedrock_model</code> and <code>embeddings=bedrock_embeddings</code> are the instances of the Bedrock and embeddings models, respectively, that we have created beforehand.</p>
<p>For the complete tutorial on how to use Amazon Bedrock with Ragas, you can go to <a href="https://docs.ragas.io/en/stable/howtos/customisations/aws-bedrock.html">https://docs.ragas.io/en/stable/howtos/customisations/aws-bedrock.html</a>.</p>
<p>Now that we have seen various techniques to perform Amazon Bedrock model evaluation, let us <a id="_idIndexMarker959"/>look at monitoring and logging solutions integrated with Amazon Bedrock.</p>
<h1 id="_idParaDest-202"><a id="_idTextAnchor219"/>Monitoring Amazon Bedrock</h1>
<p>Monitoring the<a id="_idIndexMarker960"/> performance and usage of your generative AI applications is crucial for ensuring optimal functionality, maintaining security and privacy standards, and gaining insights for future enhancements. Amazon Bedrock seamlessly integrates with Amazon CloudWatch, CloudTrail, and EventBridge, which provides a comprehensive monitoring and logging solution.</p>
<h2 id="_idParaDest-203"><a id="_idTextAnchor220"/>Amazon CloudWatch</h2>
<p>Amazon CloudWatch<a id="_idIndexMarker961"/> is a monitoring and observability<a id="_idIndexMarker962"/> service that collects and visualizes data from various AWS resources, including Amazon Bedrock. By leveraging CloudWatch, you can gain valuable insights into your Bedrock models’ performance so you can identify and address potential issues proactively. Through CloudWatch, you can track usage metrics and construct tailored dashboards for auditing purposes, ensuring transparency and accountability throughout the AI model development process.</p>
<p>One of the key features of using CloudWatch with Amazon Bedrock is that you can gain insights into model usage across multiple accounts and FMs within a single account. You can monitor critical aspects such as model invocations and token counts, so you can make informed decisions and optimize resource allocation effectively. If you would like to configure monitoring across multiple accounts in one or more regions, you can check the Amazon CloudWatch documentation at <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Cross-Account-Methods.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Cross-Account-Methods.html</a>.</p>
<p>This provides all the steps that you will need to take to enable this feature.</p>
<p>Furthermore, Bedrock offers a feature called model <em class="italic">invocation logging</em>. This functionality allows users to collect metadata, requests, and responses for all model invocations within the account. While this feature is disabled by default, you can easily enable it by going to <strong class="bold">Settings</strong> in the Bedrock console and toggling <strong class="bold">Model invocation logging</strong>. By enabling this, you allow Bedrock to publish invocation logs for enhanced visibility and analysis.</p>
<p>Let us look at how CloudWatch can be leveraged to monitor Bedrock in near real-time, utilizing metrics and logs to trigger alarms and initiate actions when predefined thresholds are exceeded.</p>
<h2 id="_idParaDest-204"><a id="_idTextAnchor221"/>Bedrock metrics</h2>
<p>Amazon Bedrock’s CloudWatch<a id="_idIndexMarker963"/> metrics cover a wide range of performance indicators, including the number of invocations, invocation latency, invocation client and server errors, invocation throttling instances, input and output tokens, and much more. You can see the full list of supported metrics at <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/monitoring-cw.html#runtime-cloudwatch-metrics">https://docs.aws.amazon.com/bedrock/latest/userguide/monitoring-cw.html#runtime-cloudwatch-metrics</a>.</p>
<p>With these metrics, you can compare latency between different models and measure token counts to assist in purchasing provisioned throughput, as well as detect and alert for throttling events.</p>
<p>When you are using a chat playground, you can view these metrics after running the prompt, as <a id="_idIndexMarker964"/>shown in <em class="italic">Figure 11</em><em class="italic">.17</em>.</p>
<div><div><img alt="Figure 11.17 – Model metrics" src="img/B22045_11_17.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.17 – Model metrics</p>
<p>In addition, you can define metric criteria, which allow you to provide specific conditions or thresholds for the model metrics. You can set criteria such as <strong class="bold">latency less than 100ms</strong> or <strong class="bold">output token count greater than 500</strong> based on your requirements. These criteria can be used to evaluate and compare the performance of different models against your desired metrics. When comparing multiple models, setting metric criteria helps identify which models meet or fail to meet your specified conditions, which helps in the selection of the most suitable model for your use case.</p>
<p>Let us also look at these metrics in the CloudWatch metrics dashboard.</p>
<div><div><img alt="Figure 11.18 – The CloudWatch metrics dashboard" src="img/B22045_11_18.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.18 – The CloudWatch metrics dashboard</p>
<p>In <em class="italic">Figure 11</em><em class="italic">.18</em>, you can <a id="_idIndexMarker965"/>see the CloudWatch metrics for the Anthropic Claude 3 Sonnet model: <strong class="bold">Invocations</strong> (sample count), <strong class="bold">InvocationLatency</strong> (in milliseconds), <strong class="bold">OutputTokenCount</strong> (sample count), and <strong class="bold">InputTokenCount</strong> (sample count). Let us understand what these terminologies (metrics and statistics) mean:</p>
<ul>
<li><strong class="bold">Sample count</strong>: This statistic represents total data points or observations recorded within a specified timeframe.</li>
<li><code>Converse</code>, <code>ConverseStream</code>, <code>InvokeModel</code>, and <code>InvokeModelWithResponseStream</code> APIs within a given time frame.</li>
<li><strong class="bold">InvocationLatency</strong>: This metric refers to the delay or amount of time that has elapsed between when the invocation request is made and when the response is received.</li>
</ul>
<p><strong class="bold">OutputTokenCount</strong> and <strong class="bold">InputTokenCount</strong> are useful metrics when you are analyzing and calculating the cost of model invocations. A token is essentially a small group of characters from an input prompt and the response. <strong class="bold">OutputTokenCount</strong> signifies the total count of tokens in the response provided by the model, whereas <strong class="bold">InputTokenCount</strong> signifies the total count of tokens in the input and prompt provided to the model.</p>
<p>To streamline <a id="_idIndexMarker966"/>monitoring and analysis, Bedrock’s logs and metrics can be presented in a single view using CloudWatch dashboards. These dashboards provide a comprehensive overview of the same KPIs, including the number of invocations over time by model, invocation latency by model, and token counts for input and output. The following figure shows the dashboard view of the metrics in a one-week time frame for the two models, Anthropic Claude v2 and Anthropic Claude v3 Sonnet.</p>
<div><div><img alt="Figure 11.19 – The CloudWatch dashboard" src="img/B22045_11_19.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.19 – The CloudWatch dashboard</p>
<p>For organizations with multiple AWS accounts, Bedrock supports CloudWatch cross-account observability, enabling the creation of rich cross-account dashboards in monitoring<a id="_idIndexMarker967"/> accounts. This feature ensures a centralized view of performance metrics across various accounts, facilitating better oversight and decision-making.</p>
<h2 id="_idParaDest-205"><a id="_idTextAnchor222"/>Model invocation logging</h2>
<p>Model invocation<a id="_idIndexMarker968"/> logging allows you to capture and analyze the requests and responses generated by the models, along with the metadata of all the invocation calls that are made. It provides a comprehensive view of how your models are utilized, enabling you to monitor their performance, identify potential issues, and optimize their usage.</p>
<p>Enabling model invocation logging is a straightforward process. You can configure it through the Amazon Bedrock console or via the API.</p>
<div><div><img alt="Figure 11.20 – Enabling model invocation logging" src="img/B22045_11_20.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.20 – Enabling model invocation logging</p>
<p><em class="italic">Figure 11</em><em class="italic">.20</em> shows the console view of <strong class="bold">Model invocation logging</strong> in the Amazon Bedrock console. You will need to enable the feature. The first step is to choose the type of data you want to log, such as text, images, or embeddings. Next, you’ll need to select the destination for your logs, which can be Amazon S3, Amazon CloudWatch Logs, or both, and provide the path.</p>
<p>If you opt for Amazon S3, your logs will be stored as compressed JSON files, each containing a batch of invocation records. These files can be queried using Amazon Athena or sent to various AWS services such as Amazon EventBridge. On the other hand, if you choose Amazon CloudWatch Logs, your invocation logs will be delivered to a specified log group as JSON events. This allows you to leverage CloudWatch log insights for querying and analyzing your logs in real time.</p>
<p>One of the key advantages of model invocation logging is its ability to capture large input and output data. For data exceeding 100 KB , or data in binary formats (for example, images, audio, and so on), Amazon Bedrock automatically uploads it to your designated Amazon S3 bucket. This ensures that no valuable information is lost, even for large or non-text data.</p>
<p>By leveraging thi<a id="_idIndexMarker969"/>s feature, you can optimize your models, identify potential issues, and ensure that your systems are operating efficiently and effectively.</p>
<p>Here is a sample model invocation logs in the CloudWatch logs console:</p>
<pre class="source-code">
{
    "schemaType": "ModelInvocationLog",
    "schemaVersion": "1.0",
    "timestamp": "2024-06-01T02:26:35Z",
    "accountId": "123456789012",
    "identity": {
        "arn": "arn:aws:sts::123456789012:assumed-role/Xyz/Abc"
    },
    "region": "us-east-1",
    "requestId": "9e0ff76a-7cac-67gg-43rg-5g643qwer85r",
    "operation": "ConverseStream",
    "modelId": "anthropic.claude-v2",
    "input": {
        "inputContentType": "application/json",
        "inputBodyJson": {
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "text": "Write a poem on stock market"
                        }
                    ]
                }
            ],
            "inferenceConfig": {
                "maxTokens": 2048,
                "temperature": 0.5,
                "topP": 1,
                "stopSequences": [
                    "\n\nHuman:"
                ]
            },
            "additionalModelRequestFields": {
                "top_k": 250
            }
        },
        "inputTokenCount": 15
    },
    "output": {
        "outputContentType": "application/json",
        "outputBodyJson": {
            "output": {
                "message": {
                    "role": "assistant",
                    "content": [
                        {
                            "text": "Here is a poem about the stock market:\n\nThe Stocks Go Up and Down\n\nThe stocks go up and the stocks go down\nGains and losses all around\nSome days are green, some days are red\nWondering where this rollercoaster will lead\n\nBuy low and sell high, that's what they say\nBut the market has a mind of its own each day\nOne wrong move and your profits fade away\nPatience and research are the prudent way\n\nBulls charge ahead with optimism bright  \nWhile bears retreat in a fearful plight\nAnalysts and investors try to read the signs\nOf economic trends and corporate lines\n\nThe risky trader seeks a quick buck\nWhile the long-term holder trusts in luck \nDay by day the tickers rise and fall\nAs we check our portfolios, hoping they won't stall\n\nSo place your bets and say your prayers\nThe market gods will judge what's fair\nBut one truth will always remain\nIn the stock market, uncertainty reigns"
                        }
                    ]
                }
            },
            "stopReason": "end_turn",
            "metrics": {
                "latencyMs": 8619
            },
            "usage": {
                "inputTokens": 15,
                "outputTokens": 218,
                "totalTokens": 233
            }
        },
        "outputTokenCount": 218
    }
}</pre>
<p>The preceding log snippet provides the <code>ConverseStream</code> conversational request made to the <code>anthropic.claude-v2</code> model. It captures various data points such as the input prompt, output response, performance metrics, and usage statistics. With this comprehensive<a id="_idIndexMarker970"/> logging, you can perform effective analysis and evaluation of the model’s capabilities and behavior.</p>
<h2 id="_idParaDest-206"><a id="_idTextAnchor223"/>AWS CloudTrail</h2>
<p>AWS CloudTrail is <a id="_idIndexMarker971"/>a compliance and auditing service that <a id="_idIndexMarker972"/>allows you to capture and analyze all API calls made within your AWS environment. Here’s how you can leverage CloudTrail to gain invaluable insights.</p>
<p>Amazon Bedrock seamlessly integrates with AWS CloudTrail, capturing every API call as an event. These events encompass actions initiated from the Amazon Bedrock console, as well as programmatic calls made through the Amazon Bedrock API operations. In CloudTrail, you gain a comprehensive record of who initiated the request, the source IP address, the timestamp, and additional details surrounding the request.</p>
<p>Amazon Bedrock logs two distinct categories of events with CloudTrail: data events and management events. When it comes to data events, CloudTrail doesn’t log Amazon Bedrock Runtime API operations (<code>InvokeModel</code> and <code>InvokeModelWithResponseStream</code>) as data events by default. However, it does log all actions related to agents for Amazon Bedrock Runtime API operations, categorized as data events:</p>
<ul>
<li>To log <code>InvokeAgent</code> calls, you would need to configure advanced event selectors on the CloudTrail trail to record data events for the <code>AWS::Bedrock::AgentAlias</code> resource type.</li>
<li>To log <code>Retrieve</code> and <code>RetrieveAndGenerate</code> calls, configure advanced event selectors to record data events for the <code>AWS::Bedrock::KnowledgeBase</code> resource type.</li>
</ul>
<p>Advanced Event Selector enables the creation of precision and granular filters for monitoring and managing CloudTrail activities related to both management and data events.</p>
<p><strong class="bold">Data events</strong> provide<a id="_idIndexMarker973"/> insights into resource operations such as reading or writing to resources such as Amazon Bedrock Knowledge Bases or agent aliases. These events are not logged by default due to their high volume but logging can be enabled through advanced event selectors.</p>
<p>On the other hand, <strong class="bold">management events</strong> capture control plane operations such as API calls for creating, updating, or<a id="_idIndexMarker974"/> deleting Amazon Bedrock resources. CloudTrail automatically logs these management events, providing a comprehensive audit trail of administrative activities within your Amazon Bedrock environment.</p>
<p>If you would like to learn more about CloudTrail, please check the AWS documentation: <a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html</a>.</p>
<p>Leveraging AWS CloudTrail in conjunction with Amazon Bedrock provides you with a powerful auditing and monitoring solution. By capturing and analyzing API calls, you can maintain<a id="_idIndexMarker975"/> visibility <a id="_idIndexMarker976"/>in your Amazon Bedrock environment, ensure adherence to best practices, and promptly address any potential security or operational concerns.</p>
<h2 id="_idParaDest-207"><a id="_idTextAnchor224"/>EventBridge</h2>
<p>Amazon EventBridge provides a solution for tracking and responding to events in near-real time. It acts <a id="_idIndexMarker977"/>as a centralized event bus, ingesting and<a id="_idIndexMarker978"/> processing state change data from various sources including Amazon Bedrock. Whenever there’s a shift in the status of a model customization job you’ve initiated, Bedrock publishes a new event to EventBridge. This event contains detailed information about the job, such as its current state, output model ARN, and any failure messages.</p>
<p>Here’s how you can harness the power of Amazon EventBridge to monitor Amazon Bedrock events effectively:</p>
<ul>
<li><strong class="bold">Event streaming and delivery</strong>: Amazon Bedrock emits events on a best-effort basis whenever there’s a state change in a model customization job you’ve initiated. These events are streamed to Amazon EventBridge, which acts as a centralized event bus, ingesting and processing event data from various AWS services and external sources.</li>
<li><strong class="bold">Event pattern matching</strong>: Within Amazon EventBridge, you can create rules that define event patterns based on specific criteria such as the source service, event type, or job status. By crafting rules tailored to your needs, you can filter and capture only the events that are relevant to your Amazon Bedrock workflows.</li>
<li><strong class="bold">Automated responses and integrations</strong>: Once an event matches a rule you’ve defined, Amazon EventBridge routes it to one or more targets you’ve specified. These targets can be various AWS services such as AWS Lambda functions, Amazon <strong class="bold">Simple Queue Service</strong> (<strong class="bold">SQS</strong>) queues, or <a id="_idIndexMarker979"/>Amazon <strong class="bold">Simple Notification Service</strong> (<strong class="bold">SNS</strong>) topics. With this<a id="_idIndexMarker980"/> flexibility, you can trigger automated actions, invoke downstream workflows, or receive notifications based on event data.</li>
<li><strong class="bold">Monitoring and alerting</strong>: One common use case for Amazon EventBridge is setting up alerting mechanisms for critical events. For instance, you can configure a rule to send an email notification to a designated address whenever a model customization job fails, enabling you to promptly investigate and address the issue.</li>
<li><strong class="bold">Event data enrichment</strong>: The event data emitted by Amazon Bedrock contains valuable information about the model customization job, such as the job ARN, output model ARN, job status, and failure messages (if applicable). By leveraging this data, you can build robust monitoring and alerting systems tailored to your specific requirements.</li>
</ul>
<p>To receive and process Amazon Bedrock events through Amazon EventBridge, you will need to create <em class="italic">rules</em> and <em class="italic">targets</em>. Rules define the event patterns to match, while targets specify the<a id="_idIndexMarker981"/> actions <a id="_idIndexMarker982"/>to be taken when an event matches a rule. Let’s learn more about how to do this:</p>
<ul>
<li>To create a rule, follow the ensuing steps:<ol><li class="upper-roman">Open the Amazon EventBridge console.</li><li class="upper-roman">Choose <strong class="bold">Create rule</strong>.</li><li class="upper-roman">Provide a name for your rule.</li><li class="upper-roman">Select <strong class="bold">Event pattern</strong>, as shown in <em class="italic">Figure 11</em><em class="italic">.21</em></li><li class="upper-roman">Define the event pattern to match Amazon Bedrock events (for instance, set <strong class="bold">source</strong> to <strong class="bold">aws.bedrock</strong> and <strong class="bold">detail-type</strong> to <strong class="bold">Model Customization Job </strong><strong class="bold">State Change</strong>).</li></ol></li>
</ul>
<div><div><img alt="Figure 11.21 – The Event pattern window" src="img/B22045_11_21.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.21 – The Event pattern window</p>
<ul>
<li>Follow the ensuing steps for configuring targets:<ol><li class="upper-roman" value="1">Choose the target type (for example, AWS Lambda, Amazon SNS, Amazon SQS).</li><li class="upper-roman">Specify the target resource (for example, a Lambda function ARN or an SNS topic ARN).</li><li class="upper-roman">Optionally, add additional configurations or transformations for the target.</li></ol></li>
</ul>
<p>One practical <a id="_idIndexMarker983"/>use case<a id="_idIndexMarker984"/> is to receive email notifications whenever there is a change in the status of your model customization jobs. Here’s how you can set it up:</p>
<ol>
<li>Create an Amazon SNS topic.</li>
<li>Subscribe your email address to the SNS topic.</li>
<li>Create an Amazon EventBridge rule with the following event pattern:<pre class="source-code">
   ```</pre><pre class="source-code">
   {</pre><pre class="source-code">
     "source": ["aws.bedrock"],</pre><pre class="source-code">
     "detail-type": ["Model Customization Job State Change"]</pre><pre class="source-code">
   }</pre><pre class="source-code">
   ```</pre></li>
<li>Set the SNS topic as the target for the rule.</li>
</ol>
<p>With this set up, you’ll receive email notifications whenever there is a state change in your Amazon Bedrock model customization jobs, keeping you informed about job progress and potential failures.</p>
<p>Additionally, the Amazon EventBridge integration with Amazon Bedrock opens up various advanced use cases, such as the following:</p>
<ul>
<li>Triggering Lambda functions to perform custom actions based on job events (for example, sending notifications to a Slack channel, updating a dashboard, or triggering downstream workflows)</li>
<li>Integrating with Amazon Step Functions to orchestrate complex workflows based on job events</li>
<li>Sending job events to Amazon Kinesis data streams for real-time processing and analysis</li>
<li>Archiving job events in Amazon S3 or Amazon CloudWatch logs for auditing and compliance purposes</li>
</ul>
<p>By leveraging<a id="_idIndexMarker985"/> Amazon<a id="_idIndexMarker986"/> EventBridge to monitor and respond to Amazon Bedrock events, you can enhance the security, automation, and visibility of your machine learning operations. With the ability to define custom rules and integrate with various AWS services, you can create a robust and secure environment tailored to your specific needs.</p>
<h1 id="_idParaDest-208"><a id="_idTextAnchor225"/>Summary</h1>
<p>In this chapter, we learned about various methods for evaluating and monitoring the Amazon Bedrock models.</p>
<p>We began by exploring the two primary model evaluation methods offered by Amazon Bedrock: automatic model evaluation and human evaluation. The automatic model evaluation process involves running an evaluation algorithm script on either a built-in or custom dataset, assessing metrics such as accuracy, toxicity, and robustness. Human evaluation, on the other hand, incorporates human input into the evaluation process, ensuring that the models not only deliver accurate results but also that those results align with real-world expectations and requirements.</p>
<p>Furthermore, we discussed open source tools such as <code>fmeval</code> and Ragas, which provide additional evaluation capabilities that are specifically tailored for LLMs and RAG pipelines.</p>
<p>Moving on to the section on monitoring, we discussed how Amazon CloudWatch can be leveraged to gain valuable insights into model performance, latency, and token counts. We explored the various metrics provided by Amazon Bedrock and considered how they can be visualized and monitored through CloudWatch dashboards. Additionally, we covered model invocation logging, a powerful feature that allows you to capture and analyze requests, responses, and metadata for all model invocations. Next, we looked at the integration of Amazon Bedrock with AWS CloudTrail and EventBridge. CloudTrail provides a comprehensive audit trail of API calls made within your AWS environment, enabling you to monitor and ensure adherence to best practices. EventBridge, on the other hand, allows you to track and respond to events in near-real time, enabling automated responses and integrations based on the state changes of your model customization jobs.</p>
<p>Ensuring security and privacy is the top priority at Amazon and also in today’s digital landscape. In the next chapter, we are going to look at how security and privacy can be ensured in Amazon Bedrock.</p>
</div>
</body></html>