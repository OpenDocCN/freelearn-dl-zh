<html><head></head><body><html:html>&#13;
 <html:head>&#13;
  <html:title>&#13;
   Customizing and Deploying Our LlamaIndex Project&#13;
  </html:title>&#13;
 </html:head>&#13;
 <html:body>&#13;
  <html:div class="epub-source">&#13;
   <html:h1 id="_idParaDest-199">&#13;
    Customizing and Deploying Our LlamaIndex Project&#13;
   </html:h1>&#13;
   <html:div id="_idContainer110">&#13;
    <html:p>&#13;
     Customizing&#13;
     <html:strong class="bold">&#13;
      Retrieval-Augmented Generation&#13;
     </html:strong>&#13;
     (&#13;
     <html:strong class="bold">&#13;
      RAG&#13;
     </html:strong>&#13;
     ) components and optimizing performance&#13;
     <html:a id="_idIndexMarker929">&#13;
     </html:a>&#13;
     is critical to building robust, production-ready applications with LlamaIndex. This chapter explores methods for leveraging&#13;
     <html:a id="_idIndexMarker930">&#13;
     </html:a>&#13;
     open source models, intelligent routing across&#13;
     <html:strong class="bold">&#13;
      large language models&#13;
     </html:strong>&#13;
     (&#13;
     <html:strong class="bold">&#13;
      LLMs&#13;
     </html:strong>&#13;
     ), and using community-built modules to increase flexibility and cost-effectiveness. Advanced tracing, evaluation methods, and deployment options are explored to gain deep insight, ensure reliable operation, and streamline the development&#13;
     <html:span class="No-Break">&#13;
      life cycle.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Throughout this chapter, we’re going to cover the following&#13;
     <html:span class="No-Break">&#13;
      main topics:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      Customizing our&#13;
      <html:span class="No-Break">&#13;
       RAG components&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      Using advanced tracing and&#13;
      <html:span class="No-Break">&#13;
       evaluation techniques&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      Introduction to deployment&#13;
      <html:span class="No-Break">&#13;
       with Streamlit&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      Hands-on – a step-by-step&#13;
      <html:span class="No-Break">&#13;
       deployment guide&#13;
      </html:span>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:a id="_idTextAnchor199">&#13;
    </html:a>&#13;
   </html:div>&#13;
  </html:div>&#13;
 </html:body>&#13;
</html:html>
<html:html>&#13;
 <html:head>&#13;
  <html:title>&#13;
   Technical requirements&#13;
  </html:title>&#13;
 </html:head>&#13;
 <html:body>&#13;
  <html:div class="epub-source">&#13;
   <html:h1 id="_idParaDest-200">&#13;
    Technical requirements&#13;
   </html:h1>&#13;
   <html:div id="_idContainer110">&#13;
    <html:p>&#13;
     For this chapter, you will need to install the following package in&#13;
     <html:span class="No-Break">&#13;
      your environment:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      <html:em class="italic">&#13;
       Arize AI&#13;
      </html:em>&#13;
      <html:span class="No-Break">&#13;
       <html:em class="italic">&#13;
        Phoenix&#13;
       </html:em>&#13;
      </html:span>&#13;
      <html:span class="No-Break">&#13;
       :&#13;
      </html:span>&#13;
      <html:a>&#13;
       <html:span class="No-Break">&#13;
        https://pypi.org/project/arize-phoenix/&#13;
       </html:span>&#13;
      </html:a>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:p>&#13;
     Three additional integration packages are required in order to run the&#13;
     <html:span class="No-Break">&#13;
      sample code:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      <html:em class="italic">&#13;
       Hugging Face&#13;
      </html:em>&#13;
      <html:span class="No-Break">&#13;
       <html:em class="italic">&#13;
        embeddings&#13;
       </html:em>&#13;
      </html:span>&#13;
      <html:span class="No-Break">&#13;
       :&#13;
      </html:span>&#13;
      <html:a>&#13;
       <html:span class="No-Break">&#13;
        https://pypi.org/project/llama-index-embeddings-huggingface/&#13;
       </html:span>&#13;
      </html:a>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:em class="italic">&#13;
       Zephyr query&#13;
      </html:em>&#13;
      <html:span class="No-Break">&#13;
       <html:em class="italic">&#13;
        engine&#13;
       </html:em>&#13;
      </html:span>&#13;
      <html:span class="No-Break">&#13;
       :&#13;
      </html:span>&#13;
      <html:a>&#13;
       <html:span class="No-Break">&#13;
        https://pypi.org/project/llama-index-packs-zephyr-query-engine/&#13;
       </html:span>&#13;
      </html:a>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:em class="italic">&#13;
       Neutrino&#13;
      </html:em>&#13;
      <html:span class="No-Break">&#13;
       <html:em class="italic">&#13;
        LLM&#13;
       </html:em>&#13;
      </html:span>&#13;
      <html:span class="No-Break">&#13;
       :&#13;
      </html:span>&#13;
      <html:a>&#13;
       <html:span class="No-Break">&#13;
        https://pypi.org/project/llama-index-llms-neutrino/&#13;
       </html:span>&#13;
      </html:a>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:p>&#13;
     All code samples from this chapter can be found in the&#13;
     <html:code class="literal">&#13;
      ch9&#13;
     </html:code>&#13;
     subfolder of the book’s&#13;
     <html:span class="No-Break">&#13;
      GitHub repository:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     <html:a>&#13;
      <html:span class="No-Break">&#13;
       https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex&#13;
      </html:span>&#13;
     </html:a>&#13;
    </html:p>&#13;
    <html:a id="_idTextAnchor200">&#13;
    </html:a>&#13;
   </html:div>&#13;
  </html:div>&#13;
 </html:body>&#13;
</html:html>
<html:html>&#13;
 <html:head>&#13;
  <html:title>&#13;
   Customizing our RAG components&#13;
  </html:title>&#13;
 </html:head>&#13;
 <html:body>&#13;
  <html:div class="epub-source">&#13;
   <html:h1 id="_idParaDest-201">&#13;
    Customizing our RAG components&#13;
   </html:h1>&#13;
   <html:div id="_idContainer110">&#13;
    client = OpenAI(base_url="http://localhost:1234/v1")&#13;
    from llama_index.llms.openai import OpenAI&#13;
llm = OpenAI(&#13;
    api_base='http://localhost:1234/v1',&#13;
    temperature=0.7&#13;
)&#13;
print(llm.complete('Who is Lionel Messi?'))&#13;
    pip install llama-index-llms-neutrino&#13;
    from llama_index.core.llms import ChatMessage&#13;
from llama_index.llms.neutrino import Neutrino&#13;
llm = Neutrino(&#13;
    api_key="&lt;your-Neutrino_API_key&gt;",&#13;
    router="&lt;Neutrino-router_ID&gt;"&#13;
)&#13;
    while True:&#13;
    user_message = input("Ask a question: ")&#13;
    if user_message.lower() == 'exit':&#13;
        print("Exiting chat")&#13;
        break&#13;
    response = llm.complete(user_message)&#13;
    print(f"LLM answer: {response}")&#13;
    print(f"Answered by: {response.raw['model']}")&#13;
    from llama_index.core import Settings&#13;
Settings.llm = llm&#13;
    pip install llama-index-embeddings-huggingface&#13;
    from zephyr_pack.base import ZephyrQueryEnginePack&#13;
from llama_index.readers import SimpleDirectoryReader&#13;
reader = SimpleDirectoryReader('files')&#13;
documents = reader.load_data()&#13;
zephyr_qe = ZephyrQueryEnginePack(documents)&#13;
response=zephyr_qe.run(&#13;
    "Enumerate famous buildings in ancient Rome"&#13;
    )&#13;
print(response)&#13;
    pip install chromadb&#13;
    llamaindex-cli rag --files files -q "What can you tell me about ancient Rome?" --verbose&#13;
    llamaindex-cli rag --chat&#13;
    <html:p>&#13;
     For starters, let’s talk about which components&#13;
     <html:a id="_idIndexMarker931">&#13;
     </html:a>&#13;
     of a RAG workflow can be customized in LlamaIndex. The short answer is&#13;
     <html:em class="italic">&#13;
      pretty much all of them, as we have seen already in the previous chapters&#13;
     </html:em>&#13;
     . The fact that the framework itself is flexible and allows customization of all the core components is a definite advantage. But leaving aside the framework itself, the core of a RAG workflow is actually the LLM and the embedding model it uses. In all the examples given so far, we have used the default configuration of LlamaIndex – which is based on OpenAI models. But, as we already briefly discussed in&#13;
     <html:a>&#13;
      <html:span class="No-Break">&#13;
       <html:em class="italic">&#13;
        Chapter 3&#13;
       </html:em>&#13;
      </html:span>&#13;
     </html:a>&#13;
     ,&#13;
     <html:em class="italic">&#13;
      Kickstarting Your Journey with LlamaIndex&#13;
     </html:em>&#13;
     , there are both good reasons and enough options available to choose other models – both commercial variants offered by established companies in this market, and open source models, which can be hosted locally, offering private alternatives, and substantially reducing the costs of a&#13;
     <html:span class="No-Break">&#13;
      large-scale implementation.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     But first,&#13;
     <html:span class="No-Break">&#13;
      some background.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:a id="_idTextAnchor201">&#13;
    </html:a>&#13;
    <html:h2 id="_idParaDest-202">&#13;
     How LLaMA and LLaMA 2 changed the open source landscape&#13;
    </html:h2>&#13;
    <html:p>&#13;
     In early 2023, Meta AI introduced the&#13;
     <html:strong class="bold">&#13;
      Large Language Model Meta AI&#13;
     </html:strong>&#13;
     (&#13;
     <html:strong class="bold">&#13;
      LLaMA&#13;
     </html:strong>&#13;
     ) family, offering&#13;
     <html:a id="_idIndexMarker932">&#13;
     </html:a>&#13;
     a notable leap in accessibility for LLMs&#13;
     <html:a id="_idIndexMarker933">&#13;
     </html:a>&#13;
     by releasing model weights&#13;
     <html:a id="_idIndexMarker934">&#13;
     </html:a>&#13;
     to the research community. Following this, LLaMA 2 was launched in July 2023, with improvements such as increased data for training and expanded model sizes, alongside models fine-tuned for dialogue under less restrictive commercial use conditions. Meta developed and launched three versions of LLaMA 2 with 7, 13, and 70 billion parameters, respectively. While the basic structure of these models stayed similar to the original LLaMA versions, they were trained with 40% additional data compared to the original models, in order to enhance their&#13;
     <html:span class="No-Break">&#13;
      foundational capabilities.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Despite some controversy regarding its open source status, the initiative marked a significant contribution to the open source ecosystem, triggering a new wave of community-based research and application development. The model consistently showcased competitive performance in tests against other leading LLMs, proving its&#13;
     <html:span class="No-Break">&#13;
      advanced capabilities.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Further down the line, these releases have led to the creation of tools such as&#13;
     <html:em class="italic">&#13;
      llama.cpp&#13;
     </html:em>&#13;
     by Georgi Gerganov (&#13;
     <html:a>&#13;
      https://github.com/ggerganov/llama.cpp&#13;
     </html:a>&#13;
     ), enabling the operation of these sophisticated models on more modest hardware, thus democratizing access to cutting-edge&#13;
     <html:span class="No-Break">&#13;
      AI technologies.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p class="callout-heading">&#13;
     Quick note&#13;
    </html:p>&#13;
    <html:p class="callout">&#13;
     <html:em class="italic">&#13;
      llama.cpp&#13;
     </html:em>&#13;
     is an efficient C/C++ implementation&#13;
     <html:a id="_idIndexMarker935">&#13;
     </html:a>&#13;
     of Meta’s LLaMA architecture for LLM inference. Hugely popular in the open source community, with more than 43,000 stars on GitHub and over 930 releases, this foundational framework has sparked the development of many other similar tools and services such as Ollama, Local.AI, and others. These updates and advances signaled that AI research was changing, focusing more on making information freely available and making sure AI models can run on simpler computers and other edge devices. This opened&#13;
     <html:a id="_idIndexMarker936">&#13;
     </html:a>&#13;
     up more possibilities for using&#13;
     <html:strong class="bold">&#13;
      generative AI&#13;
     </html:strong>&#13;
     (&#13;
     <html:strong class="bold">&#13;
      GenAI&#13;
     </html:strong>&#13;
     ) and encouraged new ideas and&#13;
     <html:span class="No-Break">&#13;
      improvements everywhere.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     I won’t go into a detailed discussion&#13;
     <html:a id="_idIndexMarker937">&#13;
     </html:a>&#13;
     of all the currently available&#13;
     <html:a id="_idIndexMarker938">&#13;
     </html:a>&#13;
     tools for running local LLMs. This is because there is already a plethora of available methods by which various open source models can be run on the local system. And not just local LLMs: there’s also an increasing number of service providers offering access either to their own proprietary AI models or providing cloud-hosted access to open source models, and the good news is that LlamaIndex already provides built-in support for many of them. You can always consult the official documentation of the framework for a detailed overview of the supported models, along with examples&#13;
     <html:a id="_idIndexMarker939">&#13;
     </html:a>&#13;
     of how they can be&#13;
     <html:span class="No-Break">&#13;
      used:&#13;
     </html:span>&#13;
     <html:a>&#13;
      <html:span class="No-Break">&#13;
       https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules.html&#13;
      </html:span>&#13;
     </html:a>&#13;
     <html:span class="No-Break">&#13;
      .&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Instead, I will try to offer you an alternative that I personally find very convenient for two important reasons: it is very easy to implement, and your existing code can be reused with only a few minimal changes. For beginner coders and tinkerers wanting to quickly experiment with an idea&#13;
     <html:a id="_idIndexMarker940">&#13;
     </html:a>&#13;
     or build simple&#13;
     <html:a id="_idIndexMarker941">&#13;
     </html:a>&#13;
     prototypes, this may be one of the&#13;
     <html:span class="No-Break">&#13;
      best solutions.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:a id="_idTextAnchor202">&#13;
    </html:a>&#13;
    <html:h2 id="_idParaDest-203">&#13;
     Running a local LLM using LM Studio&#13;
    </html:h2>&#13;
    <html:p>&#13;
     Built on top&#13;
     <html:a id="_idIndexMarker942">&#13;
     </html:a>&#13;
     of the&#13;
     <html:code class="literal">&#13;
      llama.cpp&#13;
     </html:code>&#13;
     library,&#13;
     <html:strong class="bold">&#13;
      LM Studio&#13;
     </html:strong>&#13;
     (&#13;
     <html:a>&#13;
      https://lmstudio.ai/&#13;
     </html:a>&#13;
     ) provides a very user-friendly&#13;
     <html:a id="_idIndexMarker943">&#13;
     </html:a>&#13;
     graphical interface&#13;
     <html:a id="_idIndexMarker944">&#13;
     </html:a>&#13;
     for LLMs. It allows us to download, configure, and locally run almost any open source model available on Hugging Face. A great resource, especially for non-technical users, LM Studio offers two ways of interacting with a local LLM: through a chat UI similar to OpenAI’s ChatGPT or via an OpenAI-compatible local server. This second option makes it particularly useful because we can easily adapt any LlamaIndex application natively designed to use OpenAI’s LLMs with very few modifications. We’ll get to that in a moment, but first, let’s see how to get things started with&#13;
     <html:span class="No-Break">&#13;
      LM Studio.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     To start using this tool, you’ll first have to download and install the right version, depending on your operating system. Releases are available for Mac, Windows, and Linux. The installation steps are self-explanatory and well documented on&#13;
     <html:span class="No-Break">&#13;
      their website.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Once installed, the LM Studio GUI starts with a&#13;
     <html:strong class="bold">&#13;
      Model Discovery&#13;
     </html:strong>&#13;
     screen where you can type any model or model family name and get a list of matching&#13;
     <html:a id="_idIndexMarker945">&#13;
     </html:a>&#13;
     model builds available for download. We’ll use the popular&#13;
     <html:strong class="bold">&#13;
      Zephyr-7B&#13;
     </html:strong>&#13;
     model for our example (&#13;
     <html:a>&#13;
      https://huggingface.co/HuggingFaceH4/zephyr-7b-beta&#13;
     </html:a>&#13;
     ). I have specifically chosen Zephyr because, albeit a compact model, it demonstrates the effectiveness of distilling&#13;
     <html:a id="_idIndexMarker946">&#13;
     </html:a>&#13;
     an LLM into a more manageable size. Derived from&#13;
     <html:strong class="bold">&#13;
      Mistral-7B&#13;
     </html:strong>&#13;
     , Zephyr-7B establishes a new benchmark for chat models with 7 billion&#13;
     <html:a id="_idIndexMarker947">&#13;
     </html:a>&#13;
     parameters, surpassing the performance of&#13;
     <html:strong class="bold">&#13;
      LLAMA2-CHAT-70B&#13;
     </html:strong>&#13;
     on the Hugging&#13;
     <html:a id="_idIndexMarker948">&#13;
     </html:a>&#13;
     Face&#13;
     <html:em class="italic">&#13;
      LMSYS Chatbot Arena Leaderboard&#13;
     </html:em>&#13;
     (&#13;
     <html:a>&#13;
      https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard&#13;
     </html:a>&#13;
     ).&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       Figure 9&#13;
      </html:em>&#13;
     </html:span>&#13;
     <html:em class="italic">&#13;
      .1&#13;
     </html:em>&#13;
     shows a typical output when searching for the&#13;
     <html:span class="No-Break">&#13;
      <html:code class="literal">&#13;
       zephyr-7b&#13;
      </html:code>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      keyword:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:div>&#13;
     <html:div class="IMG---Figure" id="_idContainer100">&#13;
      <html:img src="../Images/B21861_09_1.jpg"/>&#13;
     </html:div>&#13;
    </html:div>&#13;
    <html:p class="IMG---Caption" lang="en-US">&#13;
     Figure 9.1 – LM Studio screenshot displaying search results&#13;
    </html:p>&#13;
    <html:p>&#13;
     In the search results screen, you’ll see&#13;
     <html:span class="No-Break">&#13;
      two panels:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      The one on the left contains all models that match your search query. In our case, these are different builds of the&#13;
      <html:span class="No-Break">&#13;
       Zephyr-7B model&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      The right panel lists all the&#13;
      <html:strong class="bold">&#13;
       Generative Pre-trained Transformer-Generated Unified Format&#13;
      </html:strong>&#13;
      (&#13;
      <html:strong class="bold">&#13;
       GGUF&#13;
      </html:strong>&#13;
      ) file versions available&#13;
      <html:a id="_idIndexMarker949">&#13;
      </html:a>&#13;
      <html:span class="No-Break">&#13;
       for download&#13;
      </html:span>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:p class="callout-heading">&#13;
     About GGUF files&#13;
    </html:p>&#13;
    <html:p class="callout">&#13;
     GGUF is a specific file format&#13;
     <html:a id="_idIndexMarker950">&#13;
     </html:a>&#13;
     used for storing models for inference. Enhancing model sharing and usage efficiency, this format has quickly become a popular way of storing and distributing models throughout the open&#13;
     <html:span class="No-Break">&#13;
      source community.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     For most models, you’ll get an entire&#13;
     <html:a id="_idIndexMarker951">&#13;
     </html:a>&#13;
     list of GGUF files&#13;
     <html:a id="_idIndexMarker952">&#13;
     </html:a>&#13;
     available. Each one will have its own characteristics, but probably&#13;
     <html:a id="_idIndexMarker953">&#13;
     </html:a>&#13;
     the most important characteristic is the&#13;
     <html:span class="No-Break">&#13;
      <html:strong class="bold">&#13;
       quantization&#13;
      </html:strong>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      level.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:h3>&#13;
     Understanding LLM quantization&#13;
    </html:h3>&#13;
    <html:p>&#13;
     Running an open source LLM&#13;
     <html:a id="_idIndexMarker954">&#13;
     </html:a>&#13;
     on typical consumer hardware can prove challenging mainly because of its large memory footprint and high computational requirements. While some consumer-grade GPUs can aid in this regard, they may not be as effective as enterprise-level hardware in handling the demands of LLMs. That’s why we need quantization. The goal of applying quantization – a post-training optimization technique  – to an AI model is to optimize it for better performance and efficiency, particularly in terms of speed and memory usage, without significantly compromising its accuracy or&#13;
     <html:span class="No-Break">&#13;
      output quality.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     The quantization process achieves this by converting the model’s parameters – typically stored as 32-bit floating-point&#13;
     <html:a id="_idIndexMarker955">&#13;
     </html:a>&#13;
     numbers – to lower-bit representations, such as&#13;
     <html:strong class="bold">&#13;
      16-bit floating-point&#13;
     </html:strong>&#13;
     (&#13;
     <html:code class="literal">&#13;
      FP16&#13;
     </html:code>&#13;
     ),&#13;
     <html:strong class="bold">&#13;
      8-bit integers&#13;
     </html:strong>&#13;
     (&#13;
     <html:code class="literal">&#13;
      INT8&#13;
     </html:code>&#13;
     ), or even lower. It’s a kind&#13;
     <html:a id="_idIndexMarker956">&#13;
     </html:a>&#13;
     of approximation process that works by reducing the numerical precision used to represent the model’s parameters, combined with complex techniques to maintain as much accuracy as possible. Modern quantization techniques are designed to minimize accuracy loss, often resulting in models that are nearly as accurate as their&#13;
     <html:span class="No-Break">&#13;
      full-precision counterparts.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p class="callout-heading">&#13;
     A simple analogy to help you better understand the concept&#13;
    </html:p>&#13;
    <html:p class="callout">&#13;
     Imagine you have a recipe that calls for very precise measurements, such as&#13;
     <html:code class="literal">&#13;
      1.4732&#13;
     </html:code>&#13;
     cups of flour. In practice, you might round this to 1.5 cups, as the difference is negligible in most cases and the difference will not affect the end result. This is similar to quantization, where we reduce the precision of the model’s parameters to make the model more efficient while maintaining acceptable accuracy. But instead of cups of flour, we reduce the numerical precision of the model’s parameters. Instead of using 16 bits to store a parameter as 23.7, we could quantize it into 8 bits as 23. This directly translates to less memory usage and faster processing times. However, there is a trade-off between model size, speed,&#13;
     <html:span class="No-Break">&#13;
      and accuracy.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     With an acceptable loss of accuracy, this process can significantly reduce the size of the model and the computational resources required for both training and inference phases, making it more feasible to deploy these models on consumer hardware. Generally, the lower the bit representation (such as&#13;
     <html:code class="literal">&#13;
      INT4&#13;
     </html:code>&#13;
     or even binary), the smaller and faster the model becomes, but at a higher risk of&#13;
     <html:span class="No-Break">&#13;
      accuracy loss.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Being built on top of llama.cpp, LM Studio can take advantage of any compatible GPUs that could be used during the inference process. This feature is commonly called&#13;
     <html:em class="italic">&#13;
      GPU offloading&#13;
     </html:em>&#13;
     and means that computing operations&#13;
     <html:a id="_idIndexMarker957">&#13;
     </html:a>&#13;
     can be partially or even entirely transferred from the CPU to the GPU. Given the fact that a modern GPU is capable of handling highly parallel computing tasks more efficiently than CPUs, this can dramatically speed up the inference process. It also reduces the load on the CPU, thus providing an overall balanced improvement of system performance. The main limitation when attempting GPU offloading is the amount of video memory available on your GPU. In order to run efficiently, the GPU must load the model in the video&#13;
     <html:span class="No-Break">&#13;
      memory first.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Because of this, apart from the quantization&#13;
     <html:a id="_idIndexMarker958">&#13;
     </html:a>&#13;
     level, the GGUF files in the right panel will also have a flag showing three possible compatibility scenarios, each represented by a&#13;
     <html:span class="No-Break">&#13;
      different color:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Green&#13;
      </html:strong>&#13;
      : This means your GPU has enough video memory to load the model and execute the inference. In most cases, this is the&#13;
      <html:span class="No-Break">&#13;
       ideal scenario&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Blue&#13;
      </html:strong>&#13;
      : Not ideal, but still provides a considerable uplift&#13;
      <html:span class="No-Break">&#13;
       in performance&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Gray&#13;
      </html:strong>&#13;
      : This may or may not work depending on the&#13;
      <html:span class="No-Break">&#13;
       model architecture&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Red&#13;
      </html:strong>&#13;
      : Unfortunately, this means you won’t be able to run this version on your machine, the most probable reason being that its size exceeds your total&#13;
      <html:span class="No-Break">&#13;
       system memory&#13;
      </html:span>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:p class="callout-heading">&#13;
     Pro tip&#13;
    </html:p>&#13;
    <html:p class="callout">&#13;
     A very handy tool for approximating the required VRAM for a particular model given a particular quantization level&#13;
     <html:a id="_idIndexMarker959">&#13;
     </html:a>&#13;
     can be found on the Hugging Face&#13;
     <html:span class="No-Break">&#13;
      website:&#13;
     </html:span>&#13;
     <html:a>&#13;
      <html:span class="No-Break">&#13;
       https://huggingface.co/spaces/hf-accelerate/model-memory-usage&#13;
      </html:span>&#13;
     </html:a>&#13;
    </html:p>&#13;
    <html:h3>&#13;
     So, which model should you choose?&#13;
    </html:h3>&#13;
    <html:p>&#13;
     The general rule of thumb&#13;
     <html:a id="_idIndexMarker960">&#13;
     </html:a>&#13;
     is that with a lower quantization level, less memory will be required and the inference process will be faster. The trade-off is decreased accuracy. For example, a 3-bit quantization will always result in less accuracy than a&#13;
     <html:span class="No-Break">&#13;
      6-bit quantization.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Once you’ve made a decision on the exact model version, the next step is to download the model on your machine. But first, make sure you have the necessary space on your hard drive. There’s a handy status&#13;
     <html:a id="_idIndexMarker961">&#13;
     </html:a>&#13;
     bar on the bottom of the UI to monitor the status of&#13;
     <html:span class="No-Break">&#13;
      the download.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     After the download is complete, moving to the&#13;
     <html:strong class="bold">&#13;
      Chats&#13;
     </html:strong>&#13;
     screen will display something similar to&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       Figure 9&#13;
      </html:em>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       .2&#13;
      </html:em>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      :&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:div>&#13;
     <html:div class="IMG---Figure" id="_idContainer101">&#13;
      <html:img src="../Images/B21861_09_2.jpg"/>&#13;
     </html:div>&#13;
    </html:div>&#13;
    <html:p class="IMG---Caption" lang="en-US">&#13;
     Figure 9.2 – LM Studio’s chat UI&#13;
    </html:p>&#13;
    <html:p>&#13;
     This is the interaction method that I mentioned at the beginning of this section – the one resembling the ChatGPT interface. In this screen, you’ll be able to do&#13;
     <html:span class="No-Break">&#13;
      the following:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:ol>&#13;
     <html:li>&#13;
      Select the desired AI model from a list of all downloaded ones. To choose your model, use the&#13;
      <html:em class="italic">&#13;
       model selector&#13;
      </html:em>&#13;
      on top of the screen. You’ll have to wait for a few moments until the model is loaded&#13;
      <html:span class="No-Break">&#13;
       into memory.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      Configure any available parameters of the model using the&#13;
      <html:em class="italic">&#13;
       configuration panel&#13;
      </html:em>&#13;
      on the right side. We’ll talk in more detail about that in&#13;
      <html:span class="No-Break">&#13;
       a moment.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      See a list of previous chats on the&#13;
      <html:span class="No-Break">&#13;
       left side.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      Chat with the model using a familiar interface inspired&#13;
      <html:span class="No-Break">&#13;
       by ChatGPT.&#13;
      </html:span>&#13;
     </html:li>&#13;
    </html:ol>&#13;
    <html:p>&#13;
     There are a number of parameters that you can tweak in the configuration panel. The most important ones are&#13;
     <html:span class="No-Break">&#13;
      the following:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Preset&#13;
      </html:strong>&#13;
      : Some models come with predefined configurations that you can load from presets. For an easy start, I would recommend selecting the model’s specific preset from the list. For example, there is a Zephyr preset that can be used with all&#13;
      <html:span class="No-Break">&#13;
       Zephyr-based models&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       System Prompt&#13;
      </html:strong>&#13;
      : This prompt will set the initial context of&#13;
      <html:span class="No-Break">&#13;
       the conversation&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       GPU Offload&#13;
      </html:strong>&#13;
      : Allows you to configure the number of model layers to be offloaded to the GPU. Depending on the model you’re using and your available GPU, you may want to gradually experiment with increasing values while checking for model stability. Higher values can sometimes produce errors. If you feel confident, use -1 to offload all the model’s layers to&#13;
      <html:span class="No-Break">&#13;
       the GPU&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Context Length&#13;
      </html:strong>&#13;
      : Allows you to define the maximum context window to&#13;
      <html:span class="No-Break">&#13;
       be used&#13;
      </html:span>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:p>&#13;
     Changing some of these parameters may trigger a model reload, so you’ll have to be patient until it completes&#13;
     <html:a id="_idIndexMarker962">&#13;
     </html:a>&#13;
     the process. Once you have customized everything, the floor is yours – enjoy chatting with your&#13;
     <html:span class="No-Break">&#13;
      local LLM.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:h3>&#13;
     So far, so good, but where’s the RAG part in all this?&#13;
    </html:h3>&#13;
    <html:p>&#13;
     For that, we’ll have to go&#13;
     <html:a id="_idIndexMarker963">&#13;
     </html:a>&#13;
     to the&#13;
     <html:strong class="bold">&#13;
      Local Inference Server&#13;
     </html:strong>&#13;
     screen, which you can do by pressing the double-arrow icon on the left-side menu. You’ll be presented with a UI similar to&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       Figure 9&#13;
      </html:em>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       .3&#13;
      </html:em>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      :&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:div>&#13;
     <html:div class="IMG---Figure" id="_idContainer102">&#13;
      <html:img src="../Images/Image96391.jpg"/>&#13;
     </html:div>&#13;
    </html:div>&#13;
    <html:p class="IMG---Caption" lang="en-US">&#13;
     Figure 9.3 – The local Inference Server interface in LM Studio&#13;
    </html:p>&#13;
    <html:p>&#13;
     The configuration options from the right-side panel&#13;
     <html:a id="_idIndexMarker964">&#13;
     </html:a>&#13;
     are almost identical to the ones in the&#13;
     <html:strong class="bold">&#13;
      Chat&#13;
     </html:strong>&#13;
     screen. In the beginning, you can leave the&#13;
     <html:em class="italic">&#13;
      server configuration&#13;
     </html:em>&#13;
     options as default. The&#13;
     <html:em class="italic">&#13;
      usage&#13;
     </html:em>&#13;
     section tells you how to interact with the API. One of the great aspects of LM Studio is that it emulates the OpenAI API. That means your already existing code will need very few changes to work with a local LLM hosted through&#13;
     <html:span class="No-Break">&#13;
      LM Studio.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     All you have to do at this point is to click the&#13;
     <html:strong class="bold">&#13;
      Start Server&#13;
     </html:strong>&#13;
     button, and you’re good&#13;
     <html:span class="No-Break">&#13;
      to go.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p class="callout-heading">&#13;
     Quick note&#13;
    </html:p>&#13;
    <html:p class="callout">&#13;
     Please keep in mind that while the API server is running, the chat UI will be disabled, so you won’t be able to use both at the&#13;
     <html:span class="No-Break">&#13;
      same time.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Let’s see exactly what we need to change in our code if we want to port it to a local LLM using this method. If we look at the recommendation in the&#13;
     <html:em class="italic">&#13;
      usage&#13;
     </html:em>&#13;
     section, we’ll see that a single change&#13;
     <html:span class="No-Break">&#13;
      is necessary:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     However, because LlamaIndex has its own implementation of the OpenAI API client, in our case, we’ll have to use the&#13;
     <html:code class="literal">&#13;
      api_base&#13;
     </html:code>&#13;
     parameter&#13;
     <html:span class="No-Break">&#13;
      like this:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     As you can see, the only real change&#13;
     <html:a id="_idIndexMarker965">&#13;
     </html:a>&#13;
     we have to make is pointing the&#13;
     <html:code class="literal">&#13;
      llm&#13;
     </html:code>&#13;
     instance toward our local server instead of the OpenAI one. The rest of the code remains unchanged. After running this example, you’ll see actual requests coming from our code and responses coming from the API in LM Studio’s log screen. If you want to permanently reconfigure the LLM in the entire code, you’ll have to define a&#13;
     <html:code class="literal">&#13;
      Settings&#13;
     </html:code>&#13;
     object and use it to configure global settings, as I showed you in&#13;
     <html:a>&#13;
      <html:span class="No-Break">&#13;
       <html:em class="italic">&#13;
        Chapter 3&#13;
       </html:em>&#13;
      </html:span>&#13;
     </html:a>&#13;
     ,&#13;
     <html:em class="italic">&#13;
      Kickstarting Your Journey with LlamaIndex&#13;
     </html:em>&#13;
     , in the&#13;
     <html:em class="italic">&#13;
      Customizing the LLM used by&#13;
     </html:em>&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       LlamaIndex&#13;
      </html:em>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      section.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Neat, isn’t it? Our data is now completely private, and we don’t have to pay for using an AI model in our RAG workflows anymore. Of course, there’s still a cost, albeit in electricity rather than tokens. The capability to run local models on modest hardware unlocks numerous possibilities that extend beyond mere text generation. This includes the opportunity to embrace&#13;
     <html:a id="_idIndexMarker966">&#13;
     </html:a>&#13;
     fully multimodal experiences with models such as&#13;
     <html:strong class="bold">&#13;
      LLaVa&#13;
     </html:strong>&#13;
     (&#13;
     <html:a>&#13;
      https://huggingface.co/docs/transformers/main/en/model_doc/llava&#13;
     </html:a>&#13;
     ), allowing for a wider range of applications: a wonderful tool that serves as an excellent resource for rapid prototyping or exploring&#13;
     <html:span class="No-Break">&#13;
      diverse ideas.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     However, keep in mind that LM Studio&#13;
     <html:a id="_idIndexMarker967">&#13;
     </html:a>&#13;
     is governed by a licensing model, which restricts its use to personal, non-commercial purposes. To utilize LM Studio for commercial applications, obtaining permission from the developers&#13;
     <html:span class="No-Break">&#13;
      is necessary.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:a id="_idTextAnchor203">&#13;
    </html:a>&#13;
    <html:h2 id="_idParaDest-204">&#13;
     Routing between LLMs using services such as Neutrino or OpenRouter&#13;
    </html:h2>&#13;
    <html:p>&#13;
     Sometimes, a single LLM&#13;
     <html:a id="_idIndexMarker968">&#13;
     </html:a>&#13;
     may not be ideal for every single&#13;
     <html:a id="_idIndexMarker969">&#13;
     </html:a>&#13;
     interaction. In complex RAG scenarios, finding the best mix between cost, latency, and precision could prove to be a difficult task when forced to choose a single LLM for everything. But what if we could find a way to mix different LLMs in the same app and dynamically choose which one to use for each individual interaction? That&#13;
     <html:a id="_idIndexMarker970">&#13;
     </html:a>&#13;
     is the exact purpose of third-party services such as&#13;
     <html:strong class="bold">&#13;
      Neutrino&#13;
     </html:strong>&#13;
     (&#13;
     <html:a>&#13;
      https://www.neutrinoapp.com/&#13;
     </html:a>&#13;
     ) and&#13;
     <html:strong class="bold">&#13;
      OpenRouter&#13;
     </html:strong>&#13;
     (&#13;
     <html:a>&#13;
      https://openrouter.ai/&#13;
     </html:a>&#13;
     ). These types of services&#13;
     <html:a id="_idIndexMarker971">&#13;
     </html:a>&#13;
     can significantly enhance a RAG workflow by providing intelligent routing capabilities for queries across&#13;
     <html:span class="No-Break">&#13;
      different LLMs.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Neutrino’s smart model router, for&#13;
     <html:a id="_idIndexMarker972">&#13;
     </html:a>&#13;
     instance, allows you to intelligently route queries to the most suited LLM for the prompt, optimizing both response quality and cost efficiency. This can be particularly useful in a RAG workflow where different types of queries may require different LLM strengths or specialties. For example, one model might be more effective at understanding and parsing the initial user query, while another might be better suited for generating responses based on retrieved documents. By employing a router, we can dynamically select the most suitable model for each task without hardcoding model choices into our application, thus enhancing flexibility and potentially improving the overall performance of our RAG system.&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       Figure 9&#13;
      </html:em>&#13;
     </html:span>&#13;
     <html:em class="italic">&#13;
      .4&#13;
     </html:em>&#13;
     describes the working mechanism of a&#13;
     <html:span class="No-Break">&#13;
      Neutrino router:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:div>&#13;
     <html:div class="IMG---Figure" id="_idContainer103">&#13;
      <html:img src="../Images/B21861_09_4.jpg"/>&#13;
     </html:div>&#13;
    </html:div>&#13;
    <html:p class="IMG---Caption" lang="en-US">&#13;
     Figure 9.4 – A diagram of the Neutrino smart routing feature&#13;
    </html:p>&#13;
    <html:p>&#13;
     The great news is that both Neutrino&#13;
     <html:a id="_idIndexMarker973">&#13;
     </html:a>&#13;
     and OpenRouter are supported as integration packages&#13;
     <html:a id="_idIndexMarker974">&#13;
     </html:a>&#13;
     in LlamaIndex. Let’s have a look at a simple example that uses a custom&#13;
     <html:a id="_idIndexMarker975">&#13;
     </html:a>&#13;
     Neutrino router to dynamically&#13;
     <html:a id="_idIndexMarker976">&#13;
     </html:a>&#13;
     choose between different LLMs depending on the user query. To run this example, make sure you first install the Neutrino integration package by running the&#13;
     <html:span class="No-Break">&#13;
      following command:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Once the package is installed, you should first sign up for an account and obtain an API key on the Neutrino website. The next step is to create an LLM router by selecting your desired LLMs as well as a&#13;
     <html:em class="italic">&#13;
      fallback&#13;
     </html:em>&#13;
     LLM. The fallback model will be used by default in case of errors or whenever the router cannot determine which LLM to use. During the router setup, you will also have the option of choosing to use Neutrino as a provider for the AI models or utilize your own API keys for each LLM. The last step in the router setup process requires you to provide a&#13;
     <html:em class="italic">&#13;
      router ID&#13;
     </html:em>&#13;
     . This ID will be used in the code to specify the router used by&#13;
     <html:span class="No-Break">&#13;
      the service.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Here is how we can use the Neutrino router&#13;
     <html:span class="No-Break">&#13;
      in LlamaIndex:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     The code first initializes the Neutrino router in the form of a LlamaIndex&#13;
     <html:code class="literal">&#13;
      llm&#13;
     </html:code>&#13;
     object, for which you’ll need to provide your Neutrino API key and the ID of the router you have defined. Next, it runs in a loop, continually taking questions from the user until the&#13;
     <html:code class="literal">&#13;
      'exit'&#13;
     </html:code>&#13;
     keyword&#13;
     <html:span class="No-Break">&#13;
      is received:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     The questions are submitted&#13;
     <html:a id="_idIndexMarker977">&#13;
     </html:a>&#13;
     to the Neutrino router, and, in return, the script&#13;
     <html:a id="_idIndexMarker978">&#13;
     </html:a>&#13;
     not only prints&#13;
     <html:a id="_idIndexMarker979">&#13;
     </html:a>&#13;
     the answer&#13;
     <html:a id="_idIndexMarker980">&#13;
     </html:a>&#13;
     but also the name of the LLM that was chosen by the router to generate the answer. You can play around and experiment with different types of questions. Based on whichever models you selected when you defined the router, you’ll see that it will send the questions to different LLMs, depending on their capabilities. Another, more general approach in using such a router would be to use the&#13;
     <html:code class="literal">&#13;
      Settings&#13;
     </html:code>&#13;
     class to create a global configuration using that&#13;
     <html:span class="No-Break">&#13;
      <html:code class="literal">&#13;
       llm&#13;
      </html:code>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      object:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     This has the advantage that it configures every subsequent LlamaIndex component in our code to use the&#13;
     <html:span class="No-Break">&#13;
      Neutrino router.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p class="callout-heading">&#13;
     Pro tip&#13;
    </html:p>&#13;
    <html:p class="callout">&#13;
     If you’re not entirely satisfied with the decisions made by the router, Neutrino also gives you the ability to fine-tune your defined router by uploading a list of examples on which the router can be&#13;
     <html:span class="No-Break">&#13;
      trained:&#13;
     </html:span>&#13;
     <html:a>&#13;
      <html:span class="No-Break">&#13;
       https://platform.neutrinoapp.com/training-studio&#13;
      </html:span>&#13;
     </html:a>&#13;
    </html:p>&#13;
    <html:p>&#13;
     And Neutrino is just one example. OpenRouter works in a similar way, but it’s mostly focused on optimizing the cost of LLM calls, not necessarily&#13;
     <html:span class="No-Break">&#13;
      the quality.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     There are also other providers&#13;
     <html:a id="_idIndexMarker981">&#13;
     </html:a>&#13;
     offering similar&#13;
     <html:a id="_idIndexMarker982">&#13;
     </html:a>&#13;
     services, and the concept&#13;
     <html:a id="_idIndexMarker983">&#13;
     </html:a>&#13;
     is bound to become more and more popular as hundreds of new AI models&#13;
     <html:a id="_idIndexMarker984">&#13;
     </html:a>&#13;
     emerge every week. The ability to use LLM routing services enhances the RAG workflow by abstracting the complexity of model selection and management. As a result, we can focus on building and optimizing our applications instead of managing the underlying&#13;
     <html:span class="No-Break">&#13;
      AI models.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:a id="_idTextAnchor204">&#13;
    </html:a>&#13;
    <html:h2 id="_idParaDest-205">&#13;
     What about customizing embedding models?&#13;
    </html:h2>&#13;
    <html:p>&#13;
     Another important component&#13;
     <html:a id="_idIndexMarker985">&#13;
     </html:a>&#13;
     that can be considered for customization in a RAG scenario is the underlying embedding model. Intensively used in scenarios where vector store indexes are employed, the embedding model can also be a source of concern regarding cost and privacy. That is why we may sometimes prefer using a local model in our RAG workflow. Again, the good news is that LlamaIndex provides out-of-the-box support for more than 30 embedding models. They can be used by installing embedding&#13;
     <html:a id="_idIndexMarker986">&#13;
     </html:a>&#13;
     integration packages, documented on the&#13;
     <html:em class="italic">&#13;
      LlamaHub&#13;
     </html:em>&#13;
     <html:span class="No-Break">&#13;
      website:&#13;
     </html:span>&#13;
     <html:a>&#13;
      <html:span class="No-Break">&#13;
       https://llamahub.ai/?tab=embeddings&#13;
      </html:span>&#13;
     </html:a>&#13;
     <html:span class="No-Break">&#13;
      .&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     You can find a very simple example of how to configure LlamaIndex to use a local embedding model from Hugging Face in&#13;
     <html:a>&#13;
      <html:span class="No-Break">&#13;
       <html:em class="italic">&#13;
        Chapter 5&#13;
       </html:em>&#13;
      </html:span>&#13;
     </html:a>&#13;
     ,&#13;
     <html:em class="italic">&#13;
      Indexing with LlamaIndex&#13;
     </html:em>&#13;
     , in the&#13;
     <html:em class="italic">&#13;
      Understanding&#13;
     </html:em>&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       embeddings&#13;
      </html:em>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      section.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:a id="_idTextAnchor205">&#13;
    </html:a>&#13;
    <html:h2 id="_idParaDest-206">&#13;
     Leveraging the Plug and Play convenience of using Llama Packs&#13;
    </html:h2>&#13;
    <html:p>&#13;
     The fact that LlamaIndex&#13;
     <html:a id="_idIndexMarker987">&#13;
     </html:a>&#13;
     offers us such a rich framework&#13;
     <html:a id="_idIndexMarker988">&#13;
     </html:a>&#13;
     of low-level elements and methods for RAG is a double-edged sword. On the one hand, it is extremely useful to have a tool available for almost any practical problem you have to solve. On the other hand, to successfully implement these tools, we must first spend a fair amount of time familiarizing ourselves with each one. Then comes the fine-tuning&#13;
     <html:a id="_idIndexMarker989">&#13;
     </html:a>&#13;
     and optimization phase&#13;
     <html:a id="_idIndexMarker990">&#13;
     </html:a>&#13;
     for each component. We are already talking about a significant effort in the development and optimization process. Sometimes, in order to be able to test an idea with a rapid prototype, it would be preferable if we already had some advanced ready-made modules. Imagine some&#13;
     <html:em class="italic">&#13;
      Lego&#13;
     </html:em>&#13;
     pieces already structured into functional sub-assemblies: a roof, a window, a bus stop, and so on. Well, we have that&#13;
     <html:span class="No-Break">&#13;
      to hand.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Created and continually improved by the flourishing LlamaIndex community,&#13;
     <html:strong class="bold">&#13;
      Llama Packs&#13;
     </html:strong>&#13;
     are pre-packaged modules that can be used to quickly build LLM applications. Just like some pre-built Lego structures, they provide reusable components such as LLMs, embedding models, and vector indexes that have been preconfigured to work together for various use cases in building a RAG pipeline. They are ready-to-use modules that can be downloaded and initialized with parameters to achieve a specific goal outside of&#13;
     <html:span class="No-Break">&#13;
      the box.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p class="callout-heading">&#13;
     Example&#13;
    </html:p>&#13;
    <html:p class="callout">&#13;
     A pack could contain a full RAG pipeline to enable semantic search over text or an entire agent construct that could be immediately invoked in&#13;
     <html:span class="No-Break">&#13;
      our app.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Llama Packs act as templates that can be inspected, customized, and extended as needed. The code for each pack is available, so developers can modify it or take inspiration to build their own applications. The beauty of this concept is that it provides&#13;
     <html:strong class="bold">&#13;
      Plug and Play&#13;
     </html:strong>&#13;
     (&#13;
     <html:strong class="bold">&#13;
      PnP&#13;
     </html:strong>&#13;
     ) solutions without bloating the main code base of the framework. You can still use various integration packages together with the core components of LlamaIndex, and you can definitely customize any of these packs according to&#13;
     <html:span class="No-Break">&#13;
      your needs.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     You’ll find a collection of all the published Llama Packs, together with all the other integration packages, available on LlamaHub (&#13;
     <html:a>&#13;
      https://llamahub.ai/?tab=llama_packs&#13;
     </html:a>&#13;
     ). There’s a&#13;
     <html:em class="italic">&#13;
      README&#13;
     </html:em>&#13;
     file for each pack that provides details about its usage, and most of them also have detailed examples that you can follow and&#13;
     <html:span class="No-Break">&#13;
      experiment with.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Using them is very straightforward. Because, in this section, we talk about customizations in general and, among other options, moving our RAG workflows to local, open source models, I’m going to show you an example in the same line. We’ll explore a Llama Pack that allows for the creation of a query engine that relies entirely on locally hosted AI models. The pack implements&#13;
     <html:code class="literal">&#13;
      HuggingFaceH4/zephyr-7b-beta&#13;
     </html:code>&#13;
     as the LLM used for inference and&#13;
     <html:code class="literal">&#13;
      BAAI/bge-base-en-v1.5&#13;
     </html:code>&#13;
     as the embedding model. The pack&#13;
     <html:a id="_idIndexMarker991">&#13;
     </html:a>&#13;
     is called Zephyr Query Engine Pack, and&#13;
     <html:a id="_idIndexMarker992">&#13;
     </html:a>&#13;
     you can find it&#13;
     <html:span class="No-Break">&#13;
      here:&#13;
     </html:span>&#13;
     <html:a>&#13;
      <html:span class="No-Break">&#13;
       https://llamahub.ai/l/llama_packs-zephyr_query_engine&#13;
      </html:span>&#13;
     </html:a>&#13;
     <html:span class="No-Break">&#13;
      .&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     In a similar way to how LM Studio works, this pack can leverage existing GPUs to accelerate the inference process. Let’s see how&#13;
     <html:span class="No-Break">&#13;
      it works.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     The first step in using any Llama Pack&#13;
     <html:a id="_idIndexMarker993">&#13;
     </html:a>&#13;
     is to download the actual modules&#13;
     <html:a id="_idIndexMarker994">&#13;
     </html:a>&#13;
     on your local environment. This can be accomplished in three&#13;
     <html:span class="No-Break">&#13;
      different ways:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      By installing the corresponding integration package. In our example, that would be accomplished with the&#13;
      &#13;
      <html:span class="No-Break">&#13;
       following command:&#13;
      </html:span>&#13;
      <html:p class="list-inset">&#13;
       This method is simple and permanently installs the required pack into your local environment. Its only disadvantage is that you cannot inspect and modify the pack code. For that purpose, the other two methods&#13;
       <html:span class="No-Break">&#13;
        are recommended.&#13;
       </html:span>&#13;
      </html:p>&#13;
     </html:li>&#13;
     <html:li>&#13;
      By using the&#13;
      &#13;
      <html:strong class="bold">&#13;
       command-line interface&#13;
      </html:strong>&#13;
      (&#13;
      <html:strong class="bold">&#13;
       CLI&#13;
      </html:strong>&#13;
      ). Here’s&#13;
      <html:span class="No-Break">&#13;
       an&#13;
      </html:span>&#13;
      <html:span class="No-Break">&#13;
       <html:a id="_idIndexMarker995">&#13;
       </html:a>&#13;
      </html:span>&#13;
      <html:span class="No-Break">&#13;
       example:&#13;
      </html:span>&#13;
      <html:p class="list-inset">&#13;
       We’ll discuss the CLI tool in more detail in the&#13;
       <html:span class="No-Break">&#13;
        next section.&#13;
       </html:span>&#13;
      </html:p>&#13;
     </html:li>&#13;
     <html:li>&#13;
      Directly in the code, using the&#13;
      from llama_index.llama_pack import download_llama_pack&#13;
download_llama_pack(&#13;
    "ZephyrQueryEnginePack", "./zephyr_pack"&#13;
)&#13;
     <html:code class="literal">&#13;
       download_llama_pack()&#13;
      </html:code>&#13;
      method and specifying a download location&#13;
      <html:span class="No-Break">&#13;
       like this:&#13;
      </html:span>&#13;
      </html:li>&#13;
    </html:ul>&#13;
    <html:p>&#13;
     Once downloaded into your local environment, the pack contents will be stored in a subfolder called&#13;
     <html:code class="literal">&#13;
      zephyr_pack&#13;
     </html:code>&#13;
     . You can inspect and modify anything in the code, adjusting it to your own needs. You will also need to install the Hugging Face&#13;
     <html:code class="literal">&#13;
      embeddings&#13;
     </html:code>&#13;
     integration package before running&#13;
     <html:span class="No-Break">&#13;
      the example:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Here’s a simple example of how to use this pack&#13;
     <html:span class="No-Break">&#13;
      after downloading:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Notice that we’re using the&#13;
     <html:code class="literal">&#13;
      run()&#13;
     </html:code>&#13;
     method, which, in this case, is a wrapper for the&#13;
     <html:code class="literal">&#13;
      query()&#13;
     </html:code>&#13;
     method used by the regular&#13;
     <html:span class="No-Break">&#13;
      query engine.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     This is just one of the more than 50 packs already available on LlamaHub at this moment. And the number keeps growing. The great news is that all of them are well documented and follow pretty much the same implementation model. So, next time you’re faced with a practical scenario that needs combining low-level components into more advanced elements, instead of reinventing the wheel, I encourage you to spend some time browsing LlamaHub for a potential ready-made solution for your problem. Llama Packs accelerates LLM app development&#13;
     <html:a id="_idIndexMarker996">&#13;
     </html:a>&#13;
     by letting developers tap into pre-built components tailored for common&#13;
     <html:a id="_idIndexMarker997">&#13;
     </html:a>&#13;
     use cases. Both ready-made solutions and customizable templates are available to&#13;
     <html:span class="No-Break">&#13;
      kickstart projects.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:a id="_idTextAnchor206">&#13;
    </html:a>&#13;
    <html:h2 id="_idParaDest-207">&#13;
     Using the Llama CLI&#13;
    </html:h2>&#13;
    <html:p>&#13;
     Another very useful tool in the LlamaIndex&#13;
     <html:a id="_idIndexMarker998">&#13;
     </html:a>&#13;
     arsenal is the&#13;
     <html:code class="literal">&#13;
      llamaindex-cli&#13;
     </html:code>&#13;
     utility. Installed together with the LlamaIndex libraries, the tool can be accessed very easily from the command line and can be used for various purposes, including&#13;
     <html:span class="No-Break">&#13;
      the following:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      Downloading Llama Packs, as seen in the previous section. The syntax to download a Llama Pack is given&#13;
      &#13;
     <html:span class="No-Break">&#13;
       as follows:&#13;
      </html:span>&#13;
      </html:li>&#13;
     <html:li>&#13;
      Upgrading source code from versions older than LlamaIndex v.&#13;
      &#13;
      &#13;
     <html:code class="literal">&#13;
       0.10&#13;
      </html:code>&#13;
      . Due to the fact that version 0.10 brought many changes related to the code structure and how to use certain modules in the framework, the authors of LlamaIndex provided developers with this automatic upgrade tool. Basically, it automatically modifies the code written on older versions and updates it to the new structure introduced with v0.10 for an easier transition. The syntax used for this feature is the following to process all sources in a given&#13;
      <html:span class="No-Break">&#13;
       folder simultaneously:&#13;
      </html:span>&#13;
      <html:p class="list-inset">&#13;
       Or execute the following command to upgrade a&#13;
       <html:span class="No-Break">&#13;
        single file:&#13;
       </html:span>&#13;
      </html:p>&#13;
      </html:li>&#13;
     <html:li>&#13;
      By far the most interesting capability is enabled by using the&#13;
      <html:code class="literal">&#13;
       rag&#13;
      </html:code>&#13;
      argument. This feature allows you to build a RAG workflow directly from the command line without having to write any code. By default, the command-line RAG mode uses local storage for embeddings based on a Chroma DB database and OpenAI’s GPT-3.5 Turbo model as LLM. For privacy reasons, keep in mind that this means that all data we upload will be sent to OpenAI&#13;
      <html:span class="No-Break">&#13;
       by default.&#13;
      </html:span>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:h3>&#13;
     How RAG works in the command line&#13;
    </html:h3>&#13;
    <html:p>&#13;
     Before we can use the RAG&#13;
     <html:a id="_idIndexMarker999">&#13;
     </html:a>&#13;
     mode from the command&#13;
     <html:a id="_idIndexMarker1000">&#13;
     </html:a>&#13;
     line, we must first install the ChromaDB library in our&#13;
     <html:span class="No-Break">&#13;
      local environment:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     The&#13;
     <html:code class="literal">&#13;
      llamaindex-cli&#13;
     </html:code>&#13;
     utility offers a variety of command-line parameters that enable users to interact with language models and manage local files efficiently. Here are descriptions of the most important&#13;
     <html:span class="No-Break">&#13;
      command-line parameters:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      <html:code class="literal">&#13;
       --help&#13;
      </html:code>&#13;
      : Displays a help message, providing an overview of available commands and&#13;
      <html:span class="No-Break">&#13;
       their usage.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:code class="literal">&#13;
       --files &lt;FILES&gt;&#13;
      </html:code>&#13;
      : Defines the name of the file or directory from where the tool will ingest our proprietary data. The contents will be ingested and embedded into the local vector database, enabling the RAG CLI tool to index the specified files and later retrieve context from them at&#13;
      <html:span class="No-Break">&#13;
       query time.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:code class="literal">&#13;
       --question &lt;QUESTION&gt;&#13;
      </html:code>&#13;
      : Specifies the question you want to ask about the ingested files. Used for querying indexed content, leveraging the power of the LLM to extract information from our&#13;
      <html:span class="No-Break">&#13;
       proprietary data.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:code class="literal">&#13;
       --chat&#13;
      </html:code>&#13;
      : Opens a chat&#13;
      <html:strong class="bold">&#13;
       read-eval-print loop&#13;
      </html:strong>&#13;
      (&#13;
      <html:strong class="bold">&#13;
       REPL&#13;
      </html:strong>&#13;
      ) for an interactive Q&amp;A session within&#13;
      <html:a id="_idIndexMarker1001">&#13;
      </html:a>&#13;
      the terminal. This provides a conversational interface to query the&#13;
      <html:span class="No-Break">&#13;
       ingested documents.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:code class="literal">&#13;
       --verbose&#13;
      </html:code>&#13;
      : Enables verbose output during execution, offering detailed information about the tool’s operations that can be useful for troubleshooting and understanding the tool’s&#13;
      <html:span class="No-Break">&#13;
       inner workings.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:code class="literal">&#13;
       --clear&#13;
      </html:code>&#13;
      : Clears out all currently embedded data from the local vector database. Because a Chroma database is used to store the embeddings, these will persist across the sessions. The&#13;
      <html:code class="literal">&#13;
       --clear&#13;
      </html:code>&#13;
      command is the equivalent of&#13;
      <html:span class="No-Break">&#13;
       a reset.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:code class="literal">&#13;
       --create-llama&#13;
      </html:code>&#13;
      : Initiates the creation of a LlamaIndex application based on the selected files. This parameter extends the tool’s functionality beyond simple Q&amp;A, enabling the development of full-stack applications with a backend and frontend, leveraging the ingested data. You’ll find a complete&#13;
      <html:a id="_idIndexMarker1002">&#13;
      </html:a>&#13;
      example of how to use it&#13;
      <html:span class="No-Break">&#13;
       here:&#13;
      </html:span>&#13;
      <html:a>&#13;
       <html:span class="No-Break">&#13;
        https://www.npmjs.com/package/create-llama#example&#13;
       </html:span>&#13;
      </html:a>&#13;
      <html:span class="No-Break">&#13;
       .&#13;
      </html:span>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:p>&#13;
     Talking about examples, let’s have a look at a simple way to have a conversation with our files using the CLI RAG feature. We’ll use the contents of the&#13;
     <html:code class="literal">&#13;
      ch9\files&#13;
     </html:code>&#13;
     folder from our GitHub repository. So, make sure you’re running this script from inside that folder, which should contain some&#13;
     <html:span class="No-Break">&#13;
      sample files:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Alternatively, once the files have been ingested, for an interactive chat session with the data, you can use the&#13;
     <html:span class="No-Break">&#13;
      following command:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     And just in case you need&#13;
     <html:a id="_idIndexMarker1003">&#13;
     </html:a>&#13;
     to customize the mechanics&#13;
     <html:a id="_idIndexMarker1004">&#13;
     </html:a>&#13;
     of the CLI RAG, a complete example can be found in the official documentation&#13;
     <html:a id="_idIndexMarker1005">&#13;
     </html:a>&#13;
     of the framework,&#13;
     <html:span class="No-Break">&#13;
      here:&#13;
     </html:span>&#13;
     <html:a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html" target="_blank">&#13;
      <html:span class="No-Break">&#13;
       https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html&#13;
      </html:span>&#13;
     </html:a>&#13;
     <html:span class="No-Break">&#13;
      .&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Next, it’s time to dive deeper into the logic of our&#13;
     <html:span class="No-Break">&#13;
      LlamaIndex applications.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:a id="_idTextAnchor207">&#13;
    </html:a>&#13;
   </html:div>&#13;
  </html:div>&#13;
 </html:body>&#13;
</html:html>
<html:html>&#13;
 <html:head>&#13;
  <html:title>&#13;
   Using advanced tracing and evaluation techniques&#13;
  </html:title>&#13;
 </html:head>&#13;
 <html:body>&#13;
  <html:div class="epub-source">&#13;
   <html:h1 id="_idParaDest-208">&#13;
    Using advanced tracing and evaluation techniques&#13;
   </html:h1>&#13;
   <html:div id="_idContainer110">&#13;
    pip install "arize-phoenix[llama-index]" llama-hub html2text&#13;
    from llama_index.core import (&#13;
    SimpleDirectoryReader,&#13;
    VectorStoreIndex,&#13;
    set_global_handler&#13;
)&#13;
import phoenix as px&#13;
    px.launch_app()&#13;
set_global_handler("arize_phoenix")&#13;
    documents = SimpleDirectoryReader('files').load_data()&#13;
index = VectorStoreIndex.from_documents(documents)&#13;
qe = index.as_query_engine()&#13;
response = qe.query("Tell me about ancient Rome")&#13;
print(response)&#13;
    input("Press &lt;ENTER&gt; to exit")&#13;
    from llama_index.core import (&#13;
    SimpleDirectoryReader,&#13;
    VectorStoreIndex,&#13;
    set_global_handler&#13;
)&#13;
import phoenix as px&#13;
px.launch_app()&#13;
set_global_handler("arize_phoenix")&#13;
documents = SimpleDirectoryReader('files').load_data()&#13;
index = VectorStoreIndex.from_documents(documents)&#13;
qe = index.as_query_engine()&#13;
response = qe.query("Tell me about ancient Rome")&#13;
print(response)&#13;
    from phoenix.session.evaluation import (&#13;
    get_qa_with_reference,&#13;
    get_retrieved_documents&#13;
)&#13;
from phoenix.experimental.evals import (&#13;
    HallucinationEvaluator,&#13;
    RelevanceEvaluator,&#13;
    QAEvaluator,&#13;
    OpenAIModel,&#13;
    run_evals&#13;
)&#13;
from phoenix.trace import DocumentEvaluations, SpanEvaluations&#13;
    model = OpenAIModel(model="gpt-4-turbo-preview")&#13;
    retrieved_documents_df = get_retrieved_documents(px.Client())&#13;
queries_df = get_qa_with_reference(px.Client())&#13;
    hallucination_evaluator = HallucinationEvaluator(model)&#13;
qa_correctness_evaluator = QAEvaluator(model)&#13;
relevance_evaluator = RelevanceEvaluator(model)&#13;
hallucination_eval_df, qa_correctness_eval_df = run_evals(&#13;
    dataframe=queries_df,&#13;
    evaluators=[hallucination_evaluator, qa_correctness_evaluator],&#13;
    provide_explanation=True,&#13;
)&#13;
relevance_eval_df = run_evals(&#13;
    dataframe=retrieved_documents_df,&#13;
    evaluators=[relevance_evaluator],&#13;
    provide_explanation=True,&#13;
)[0]&#13;
    px.Client().log_evaluations(&#13;
    SpanEvaluations(&#13;
        eval_name="Hallucination",&#13;
        dataframe=hallucination_eval_df),&#13;
    SpanEvaluations(&#13;
        eval_name="QA Correctness",&#13;
        dataframe=qa_correctness_eval_df),&#13;
    DocumentEvaluations(&#13;
        eval_name="Relevance",&#13;
        dataframe=relevance_eval_df),&#13;
)&#13;
input("Press &lt;ENTER&gt; to exit")&#13;
    <html:p>&#13;
     The process of building&#13;
     <html:a id="_idIndexMarker1006">&#13;
     </html:a>&#13;
     an LLM-based application&#13;
     <html:a id="_idIndexMarker1007">&#13;
     </html:a>&#13;
     using a tool such as LlamaIndex is very developer-friendly since the framework abstracts away a lot of technical stuff. But at the same time, this complicates things, for the very same reason. When things don’t work as planned, developers need to have effective ways of understanding why. They have to peel back all these layers of abstraction in order to pinpoint the root causes. In other words, we need to be able to see the inner mechanics of our code, understand how different components interact, and be able to identify underlying issues. That’s where tracing becomes a really important feature. On the other hand, because we have so many tools available and so many ways of building our solution, we need a way to benchmark different combinations and determine the best mix of tools and orchestrations. That’s where evaluation comes into play. Evaluation is essential for comparing various tool and method combinations until we find the right configuration for our specific needs. Together, tracing and evaluation form the backbone of a successful RAG development process, ensuring both transparency and&#13;
     <html:span class="No-Break">&#13;
      optimal performance.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     In&#13;
     <html:a>&#13;
      <html:span class="No-Break">&#13;
       <html:em class="italic">&#13;
        Chapter 3&#13;
       </html:em>&#13;
      </html:span>&#13;
     </html:a>&#13;
     ,&#13;
     <html:em class="italic">&#13;
      Kickstarting Your Journey with LlamaIndex&#13;
     </html:em>&#13;
     , we already discussed simple logging methods that we can use to better understand what’s happening under the hood of our LlamaIndex apps. Now, it’s time to discover a much more advanced way in which we can understand and evaluate RAG applications. In this section, I will explain advanced tracing&#13;
     <html:a id="_idIndexMarker1008">&#13;
     </html:a>&#13;
     and evaluation using the&#13;
     <html:strong class="bold">&#13;
      Phoenix framework&#13;
     </html:strong>&#13;
     developed by Arize AI (&#13;
     <html:a>&#13;
      https://phoenix.arize.com/&#13;
     </html:a>&#13;
     ). Integrating LlamaIndex with specialized tracing and evaluation tools provides a sophisticated approach to understanding and optimizing RAG applications. Phoenix provides the necessary instrumentation together with a great visualization UI, making our RAG execution workflow really simple&#13;
     <html:span class="No-Break">&#13;
      to understand.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     To make use of the advanced&#13;
     <html:a id="_idIndexMarker1009">&#13;
     </html:a>&#13;
     capabilities&#13;
     <html:a id="_idIndexMarker1010">&#13;
     </html:a>&#13;
     of the Phoenix framework, we must first install some necessary libraries in&#13;
     <html:span class="No-Break">&#13;
      our environment:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:a id="_idTextAnchor208">&#13;
    </html:a>&#13;
    <html:h2 id="_idParaDest-209">&#13;
     Tracing our RAG workflows using Phoenix&#13;
    </html:h2>&#13;
    <html:p>&#13;
     In Phoenix, tracing&#13;
     <html:a id="_idIndexMarker1011">&#13;
     </html:a>&#13;
     is built on&#13;
     <html:a id="_idIndexMarker1012">&#13;
     </html:a>&#13;
     the concept of&#13;
     <html:strong class="bold">&#13;
      spans&#13;
     </html:strong>&#13;
     and&#13;
     <html:strong class="bold">&#13;
      traces&#13;
     </html:strong>&#13;
     , which are fundamental for capturing the detailed execution flow of applications. A span represents a specific operation or unit of work within the application, tracking the start and end times, along with metadata that provides context about the operation. These spans are nested within traces, which aggregate multiple spans to depict the entire journey of a request through the application. This hierarchical structure allows developers to drill down into specific operations, understanding how each component contributes to the overall process. Phoenix’s tracing capabilities are designed to seamlessly integrate with LlamaIndex, enabling developers to instrument their RAG applications with&#13;
     <html:span class="No-Break">&#13;
      minimal effort.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Because it features a client-server architecture, Phoenix is able to gather traces both locally and remotely. We can automatically collect telemetry data about each operation, including data ingestion, indexing, retrieval, processing, and any subsequent LLM calls. In the background, this data is collected by the Phoenix server, where it can be visualized and analyzed in&#13;
     <html:span class="No-Break">&#13;
      real time.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Once the necessary requirements have been installed, using Phoenix is really easy. There are many advanced capabilities that you can explore with this framework, but I will show you the most simple and straightforward way to use it for tracing the execution of a LlamaIndex application. We’ll make use of a special method called&#13;
     <html:code class="literal">&#13;
      set_global_handler&#13;
     </html:code>&#13;
     , which conveniently configures LlamaIndex to use a certain tracing tool for every operation – in our case, the&#13;
     <html:span class="No-Break">&#13;
      Phoenix framework.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Make sure you install&#13;
     <html:a id="_idIndexMarker1013">&#13;
     </html:a>&#13;
     the required packages before running&#13;
     <html:a id="_idIndexMarker1014">&#13;
     </html:a>&#13;
     <html:span class="No-Break">&#13;
      the example:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Here is&#13;
     <html:span class="No-Break">&#13;
      the code:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Apart from the obvious imports that will provide our basic RAG functionality, we’re also importing&#13;
     <html:code class="literal">&#13;
      set_global_handler&#13;
     </html:code>&#13;
     and the Phoenix library. The next part will be responsible for starting the Phoenix server and configuring LlamaIndex to use it as a global&#13;
     <html:span class="No-Break">&#13;
      callback handler:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     From now on, every single operation performed by our app will generate traces that will get collected by the Phoenix server. Let’s build a simple query engine based on a&#13;
     <html:code class="literal">&#13;
      VectorStoreIndex&#13;
     </html:code>&#13;
     index and run a&#13;
     <html:span class="No-Break">&#13;
      random query:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Because we need the server to be live in order to visualize the trace, we keep the script running with&#13;
     <html:span class="No-Break">&#13;
      this line:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Now, with the script still running in the background, we can access the Phoenix UI at this URL: http://localhost:6006/.&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       Figure 9&#13;
      </html:em>&#13;
     </html:span>&#13;
     <html:em class="italic">&#13;
      .5&#13;
     </html:em>&#13;
     shows what you’ll find in the Phoenix&#13;
     <html:span class="No-Break">&#13;
      server UI:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:div>&#13;
     <html:div class="IMG---Figure" id="_idContainer104">&#13;
      <html:img src="../Images/B21861_09_5.jpg"/>&#13;
     </html:div>&#13;
    </html:div>&#13;
    <html:p class="IMG---Caption" lang="en-US">&#13;
     Figure 9.5 – A screenshot from the Phoenix server UI depicting our tracing output&#13;
    </html:p>&#13;
    <html:p>&#13;
     Looking at this screenshot, we can see&#13;
     <html:a id="_idIndexMarker1015">&#13;
     </html:a>&#13;
     that the Phoenix server UI&#13;
     <html:a id="_idIndexMarker1016">&#13;
     </html:a>&#13;
     helps us visualize a complete trace of our code, divided into multiple spans. If you have successfully executed the previous sample code, our trace should consist of three different spans, each displayed on a separate line.&#13;
    </html:p>&#13;
    <html:p>&#13;
     Let’s talk about the columns in&#13;
     <html:span class="No-Break">&#13;
      the screenshot:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      The first column,&#13;
      <html:code class="literal">&#13;
       kind&#13;
      </html:code>&#13;
      , contains the type of the span. It can be&#13;
      <html:code class="literal">&#13;
       chain&#13;
      </html:code>&#13;
      ,&#13;
      <html:code class="literal">&#13;
       retriever&#13;
      </html:code>&#13;
      ,&#13;
      <html:code class="literal">&#13;
       re-ranker&#13;
      </html:code>&#13;
      ,&#13;
      <html:code class="literal">&#13;
       llm&#13;
      </html:code>&#13;
      ,&#13;
      <html:code class="literal">&#13;
       embedding&#13;
      </html:code>&#13;
      ,&#13;
      <html:code class="literal">&#13;
       tool&#13;
      </html:code>&#13;
      , or&#13;
      <html:code class="literal">&#13;
       agent&#13;
      </html:code>&#13;
      . We are already familiar with what these concepts represent in LlamaIndex, except for a chain. In Phoenix, a chain can be either the starting point for a series of operations in an LLM application or a connector linking different steps within the application workflow. In our example, the screenshot contains three spans: two chains and an embedding. They are displayed in the reverse order of their operation, beginning with the&#13;
      <html:span class="No-Break">&#13;
       last one.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      The second column,&#13;
      <html:em class="italic">&#13;
       name&#13;
      </html:em>&#13;
      , provides a more detailed description of the span. We can see that in our example, the first span represents a&#13;
      <html:em class="italic">&#13;
       query&#13;
      </html:em>&#13;
      , the second one is an&#13;
      <html:em class="italic">&#13;
       embedding&#13;
      </html:em>&#13;
      , and the third is a&#13;
      <html:em class="italic">&#13;
       Node-parsing&#13;
      </html:em>&#13;
      operation. The logic of our code is now clear: it first parsed the ingested documents into Nodes, then created a vector index by embedding the Nodes, and the final step was to run a query against&#13;
      <html:span class="No-Break">&#13;
       that index.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      The next two columns,&#13;
      <html:em class="italic">&#13;
       input,&#13;
      </html:em>&#13;
      and&#13;
      <html:em class="italic">&#13;
       output&#13;
      </html:em>&#13;
      , show exactly what went as an input into the span and what was the final output produced by it. In our example, we only have values in these fields for the query span as this does not apply to the&#13;
      <html:span class="No-Break">&#13;
       other ones.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      The&#13;
      <html:em class="italic">&#13;
       evaluations&#13;
      </html:em>&#13;
      column displays the results of the evaluation for each span. For now, that column should be empty as we have not yet executed any evaluation. We’ll cover this topic in the&#13;
      <html:span class="No-Break">&#13;
       next section.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:em class="italic">&#13;
       start time&#13;
      </html:em>&#13;
      provides an exact timestamp for&#13;
      <html:span class="No-Break">&#13;
       each span.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:em class="italic">&#13;
       latency&#13;
      </html:em>&#13;
      measures the total execution time for each span. This is really useful when trying to optimize our code for&#13;
      <html:span class="No-Break">&#13;
       increased performance.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      As the name implies,&#13;
      <html:em class="italic">&#13;
       total tokens&#13;
      </html:em>&#13;
      count the total number of tokens used by the&#13;
      <html:span class="No-Break">&#13;
       corresponding operation.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      The last column,&#13;
      <html:em class="italic">&#13;
       status&#13;
      </html:em>&#13;
      , indicates whether the operation was completed successfully&#13;
      <html:span class="No-Break">&#13;
       or not.&#13;
      </html:span>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:p>&#13;
     Here comes the best&#13;
     <html:a id="_idIndexMarker1017">&#13;
     </html:a>&#13;
     part. If we now&#13;
     <html:a id="_idIndexMarker1018">&#13;
     </html:a>&#13;
     click on the&#13;
     <html:em class="italic">&#13;
      kind&#13;
     </html:em>&#13;
     column of the query span – the first one in our list – we’ll get a detailed visualization, similar to the one depicted in&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       Figure 9&#13;
      </html:em>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       .6&#13;
      </html:em>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      :&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:div>&#13;
     <html:div class="IMG---Figure" id="_idContainer105">&#13;
      <html:img src="../Images/B21861_09_6.jpg"/>&#13;
     </html:div>&#13;
    </html:div>&#13;
    <html:p class="IMG---Caption" lang="en-US">&#13;
     Figure 9.6 – Trace details visualized on the Phoenix server UI&#13;
    </html:p>&#13;
    <html:p>&#13;
     As you can see, we can now get a detailed understanding of each individual step performed during this span. In this case, we see a decomposed view of the query engine operation: first, the retrieval part, and then the final response synthesis using the LLM. By clicking on each individual step, we can explore its attributes and underlying mechanics. And because Phoenix runs locally, all this data&#13;
     <html:span class="No-Break">&#13;
      remains private.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p class="callout-heading">&#13;
     Practical exercise&#13;
    </html:p>&#13;
    <html:p class="callout">&#13;
     Here’s a useful exercise you could attempt now. Try to reconfigure some of the samples discussed in the previous chapters to use the Phoenix framework. You’ll get a better understanding of how different components work in LlamaIndex and also have a chance to familiarize yourself with this&#13;
     <html:span class="No-Break">&#13;
      great tool.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     And if you want to go deeper&#13;
     <html:a id="_idIndexMarker1019">&#13;
     </html:a>&#13;
     and explore more advanced tracing&#13;
     <html:a id="_idIndexMarker1020">&#13;
     </html:a>&#13;
     features of Phoenix, you’ll find everything you need in their official&#13;
     <html:span class="No-Break">&#13;
      documentation:&#13;
     </html:span>&#13;
     <html:a>&#13;
      <html:span class="No-Break">&#13;
       https://docs.arize.com/phoenix/&#13;
      </html:span>&#13;
     </html:a>&#13;
     <html:span class="No-Break">&#13;
      .&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Next, let’s talk about how we can use Phoenix to evaluate and optimize our&#13;
     <html:span class="No-Break">&#13;
      RAG apps.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:a id="_idTextAnchor209">&#13;
    </html:a>&#13;
    <html:h2 id="_idParaDest-210">&#13;
     Evaluating our RAG system&#13;
    </html:h2>&#13;
    <html:p>&#13;
     When developing LLM-based&#13;
     <html:a id="_idIndexMarker1021">&#13;
     </html:a>&#13;
     systems, proper evaluation is essential for checking how well a RAG pipeline works. In general, LLM applications have to deal with very diverse inputs, and there is usually not a single, absolute answer they are supposed to return. That means evaluating them can prove to be a&#13;
     <html:span class="No-Break">&#13;
      challenging task.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     In general, evaluating a RAG pipeline involves assessing key aspects such as&#13;
     <html:span class="No-Break">&#13;
      the following:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Retrieval quality&#13;
      </html:strong>&#13;
      : Evaluating the relevance and effectiveness of the retrieved Nodes in providing the necessary information to answer&#13;
      <html:span class="No-Break">&#13;
       the query&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Generation quality&#13;
      </html:strong>&#13;
      : Assessing the quality of the final output, including its correctness, coherence, and adherence to the&#13;
      <html:span class="No-Break">&#13;
       provided context&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Faithfulness&#13;
      </html:strong>&#13;
      : Ensuring that the generated output is faithful to the retrieved information and does not introduce hallucinations&#13;
      <html:span class="No-Break">&#13;
       or inconsistencies&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Efficiency&#13;
      </html:strong>&#13;
      : Measuring the computational efficiency and scalability of the RAG pipeline, especially in real-world scenarios with&#13;
      <html:span class="No-Break">&#13;
       large-scale datasets&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Robustness&#13;
      </html:strong>&#13;
      : Testing the RAG system’s ability to handle diverse queries, edge cases, and potential&#13;
      <html:span class="No-Break">&#13;
       adversarial inputs&#13;
      </html:span>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:p>&#13;
     To address these evaluation challenges, several tools and frameworks have been developed to facilitate the evaluation process. These tools aim to provide automated metrics, reference-based comparisons, and also human-in-the-loop evaluation methodologies. By leveraging these evaluation frameworks, we can obtain insights into the strengths and weaknesses of our RAG pipelines, identify areas for improvement, and iterate on their designs to enhance&#13;
     <html:span class="No-Break">&#13;
      overall performance.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Because in the previous section, we’ve seen how Phoenix can help us with its tracing functionality, I’d like to continue building on the previous example and first explore some of the evaluation features provided by&#13;
     <html:span class="No-Break">&#13;
      this framework.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:h3>&#13;
     Using the Phoenix framework evaluation features&#13;
    </html:h3>&#13;
    <html:p>&#13;
     Since manual labeling&#13;
     <html:a id="_idIndexMarker1022">&#13;
     </html:a>&#13;
     and testing evaluation data&#13;
     <html:a id="_idIndexMarker1023">&#13;
     </html:a>&#13;
     can be very time-consuming, Phoenix uses GPT-4 as a reference to decide on the correctness of our RAG’s answers. This framework provides out-of-the-box support for batch processing, custom datasets, and pre-tested evaluation templates. Unlike traditional, more basic evaluation libraries that lack rigor for production environments, Phoenix ensures data science rigor, high throughput, and flexibility across different environments, making it significantly faster and more adaptable for evaluating both the model and the context in which it is used. Phoenix can be used&#13;
     <html:a id="_idIndexMarker1024">&#13;
     </html:a>&#13;
     for evaluating two important dimensions of a RAG workflow:&#13;
     <html:strong class="bold">&#13;
      retrieval&#13;
     </html:strong>&#13;
     and&#13;
     <html:span class="No-Break">&#13;
      <html:strong class="bold">&#13;
       LLM inference&#13;
      </html:strong>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      .&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     For retrieval, Phoenix evaluates&#13;
     <html:a id="_idIndexMarker1025">&#13;
     </html:a>&#13;
     the relevancy of the retrieved context. In other words,  it verifies if the retrieved Nodes actually contain an answer to the query or not. When evaluating LLM inference, the framework checks three&#13;
     <html:span class="No-Break">&#13;
      main attributes:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Correctness&#13;
      </html:strong>&#13;
      : This verifies if the system has accurately answered&#13;
      <html:span class="No-Break">&#13;
       a question&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Hallucinations&#13;
      </html:strong>&#13;
      : This aims to identify any unrealistic or fabricated responses by the LLM in relation to the context it&#13;
      <html:span class="No-Break">&#13;
       was provided&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Toxicity&#13;
      </html:strong>&#13;
      : This checks for any harmful content in the AI’s responses, including racism, bias, or&#13;
      <html:span class="No-Break">&#13;
       general toxicity&#13;
      </html:span>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:p>&#13;
     Because a complex RAG scenario could sometimes rely on many individual spans, being able to individually evaluate each one becomes an essential feature. This way, we can isolate the source of errors and stop them from propagating further in the flow. Since it uses an LLM for running evaluations, Phoenix returns not just the test result but also an explanation provided by the model. This can be very useful for understanding the root cause of a failed evaluation and pinpointing the misbehaving component in our&#13;
     <html:span class="No-Break">&#13;
      RAG application.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Let’s have a look at a simple example to understand how Phoenix can be used for evaluation. In order to minimize costs and keep the code simple, we’re going to use the previous approach we used for the tracing example. We’re going to ingest the contents of our&#13;
     <html:code class="literal">&#13;
      ch9/files&#13;
     </html:code>&#13;
     folder, create a vector index, and run a simple query against the index. In a real scenario, you would probably run these evaluators against a much larger dataset in order to cover as many edge cases as possible and increase the probability of finding underlying issues in the pipeline. Here is&#13;
     <html:span class="No-Break">&#13;
      an example:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     So far, the first part is identical&#13;
     <html:a id="_idIndexMarker1026">&#13;
     </html:a>&#13;
     to the previous&#13;
     <html:a id="_idIndexMarker1027">&#13;
     </html:a>&#13;
     example. It’s time to add the part responsible for the evaluation. We’ll begin by importing the necessary Phoenix components. Two functions,&#13;
     <html:code class="literal">&#13;
      get_retrieved_documents()&#13;
     </html:code>&#13;
     and&#13;
     <html:code class="literal">&#13;
      get_qa_with_reference()&#13;
     </html:code>&#13;
     , will be responsible for fetching the documents retrieved by queries and the queries with their reference answers for evaluation. We’re also importing three of the Phoenix evaluators:&#13;
     <html:code class="literal">&#13;
      HallucinationEvaluator&#13;
     </html:code>&#13;
     ,&#13;
     <html:code class="literal">&#13;
      QAEvaluator&#13;
     </html:code>&#13;
     , and&#13;
     <html:code class="literal">&#13;
      RelevanceEvaluator&#13;
     </html:code>&#13;
     . These evaluators will assess the hallucination in responses, the correctness of question-answer pairs, and the relevance of retrieved documents, respectively. We also need to import&#13;
     <html:code class="literal">&#13;
      run_evals()&#13;
     </html:code>&#13;
     , which will be responsible for performing the evaluation tasks and returning DataFrames containing the evaluation results. Finally, the&#13;
     <html:code class="literal">&#13;
      DocumentEvaluations&#13;
     </html:code>&#13;
     and&#13;
     <html:code class="literal">&#13;
      SpanEvaluations&#13;
     </html:code>&#13;
     classes will be used to encapsulate evaluation results and display these results in the Phoenix&#13;
     <html:span class="No-Break">&#13;
      server UI:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Now that the imports&#13;
     <html:a id="_idIndexMarker1028">&#13;
     </html:a>&#13;
     are complete, it’s time to prepare&#13;
     <html:a id="_idIndexMarker1029">&#13;
     </html:a>&#13;
     our evaluations. First, we declare the LLM that will be used to perform evaluations. This should always be the best&#13;
     <html:span class="No-Break">&#13;
      available model:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Once the evaluation model has been defined, it’s time to prepare our data. We’ll fetch the retrieved documents and queries in separate data frames. These data frames will later become the input for&#13;
     <html:span class="No-Break">&#13;
      evaluator functions:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Now that we have the data, we need to define evaluator functions and run the&#13;
     <html:span class="No-Break">&#13;
      actual evaluations:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     When running the evaluators, notice&#13;
     <html:a id="_idIndexMarker1030">&#13;
     </html:a>&#13;
     that I’m setting&#13;
     <html:a id="_idIndexMarker1031">&#13;
     </html:a>&#13;
     the&#13;
     <html:code class="literal">&#13;
      provide_explanation&#13;
     </html:code>&#13;
     argument to&#13;
     <html:code class="literal">&#13;
      True&#13;
     </html:code>&#13;
     . This ensures that explanations for the evaluation scores are included in the response from the LLM. The last part involves encapsulating the results in corresponding&#13;
     <html:code class="literal">&#13;
      SpanEvaluations&#13;
     </html:code>&#13;
     and&#13;
     <html:code class="literal">&#13;
      DocumentEvaluations&#13;
     </html:code>&#13;
     classes and sending them to the Phoenix server so that they can be properly displayed in&#13;
     <html:span class="No-Break">&#13;
      the UI:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Just like in the previous example, the input at the end keeps the script running until the user decides to exit by pressing the&#13;
     <html:em class="italic">&#13;
      Enter&#13;
     </html:em>&#13;
     key. This allows us to view and interact with the Phoenix app before closing it. If everything went smoothly, accessing the UI at  http://localhost:6006/ should reveal an output similar to what we can see in&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       Figure 9&#13;
      </html:em>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       .7&#13;
      </html:em>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      :&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:div>&#13;
     <html:div class="IMG---Figure" id="_idContainer106">&#13;
      <html:img src="../Images/Image96429.jpg"/>&#13;
     </html:div>&#13;
    </html:div>&#13;
    <html:p class="IMG---Caption" lang="en-US">&#13;
     Figure 9.7 – Visualizing evaluation results in the Phoenix server UI&#13;
    </html:p>&#13;
    <html:p>&#13;
     As you can see, the&#13;
     <html:em class="italic">&#13;
      evaluations&#13;
     </html:em>&#13;
     column has been updated with the values returned by the evaluators we just executed. We can now see the results, as well as the rationale for each&#13;
     <html:span class="No-Break">&#13;
      individual score.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     The topic of evaluating RAG apps is huge and could probably become the subject of an entirely separate book. There are many nuances and different approaches that could be considered regarding evaluation. I’ve only shown you a tool – Phoenix – but there are many other options for this purpose, including LlamaIndex’s own instrumentation. If you’re planning to explore this topic deeper, I encourage you to start by reading the LlamaIndex&#13;
     <html:a id="_idIndexMarker1032">&#13;
     </html:a>&#13;
     official documentation here:&#13;
     <html:a>&#13;
      https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html&#13;
     </html:a>&#13;
     . Also, get a better understanding of the complete&#13;
     <html:a id="_idIndexMarker1033">&#13;
     </html:a>&#13;
     capabilities of the Phoenix framework&#13;
     <html:a id="_idIndexMarker1034">&#13;
     </html:a>&#13;
     by reading their official&#13;
     <html:a id="_idIndexMarker1035">&#13;
     </html:a>&#13;
     documentation&#13;
     <html:span class="No-Break">&#13;
      here:&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      https://docs.arize.com/phoenix/&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      .&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:h3>&#13;
     Other alternatives for evaluation – RAGAS&#13;
    </html:h3>&#13;
    <html:p>&#13;
     While Phoenix provides a comprehensive evaluation&#13;
     <html:a id="_idIndexMarker1036">&#13;
     </html:a>&#13;
     framework for RAG pipelines, there are other alternatives available. Another notable framework is&#13;
     <html:strong class="bold">&#13;
      Retrieval-Augmented Generation Assessment&#13;
     </html:strong>&#13;
     (&#13;
     <html:strong class="bold">&#13;
      RAGAS&#13;
     </html:strong>&#13;
     ), which is based on the techniques introduced by Es et al. (2023) in their paper,&#13;
     <html:em class="italic">&#13;
      RAGAS: Automated Evaluation of Retrieval Augmented Generation&#13;
     </html:em>&#13;
     (&#13;
     <html:a>&#13;
      https://doi.org/10.48550/arXiv.2309.15217&#13;
     </html:a>&#13;
     ). The RAGAS framework provides a practical implementation of these evaluation methods, along with additional features&#13;
     <html:span class="No-Break">&#13;
      and integrations.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     RAGAS is specifically designed for evaluating and analyzing RAG systems. It offers a standardized approach to assess various aspects of a RAG pipeline, including retrieval quality, generation quality, and the interplay between the retrieval and&#13;
     <html:span class="No-Break">&#13;
      generation components.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Key features of RAGAS&#13;
     <html:a id="_idIndexMarker1037">&#13;
     </html:a>&#13;
     include&#13;
     <html:span class="No-Break">&#13;
      the following:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Retrieval evaluation&#13;
      </html:strong>&#13;
      : RAGAS assesses the quality of the retrieval component by measuring the relevance of the retrieved Nodes&#13;
      <html:a id="_idIndexMarker1038">&#13;
      </html:a>&#13;
      to the given query using metrics such as&#13;
      <html:strong class="bold">&#13;
       Recall@k&#13;
      </html:strong>&#13;
      – the proportion of relevant Nodes retrieved within the top&#13;
      <html:em class="italic">&#13;
       k&#13;
      </html:em>&#13;
      results, where k is a user-defined parameter. Another metric that measures retrieval quality is the&#13;
      <html:strong class="bold">&#13;
       Mean Reciprocal Rank&#13;
      </html:strong>&#13;
      (&#13;
      <html:strong class="bold">&#13;
       MRR&#13;
      </html:strong>&#13;
      ) – measuring how quickly&#13;
      <html:a id="_idIndexMarker1039">&#13;
      </html:a>&#13;
      the system finds the first&#13;
      <html:span class="No-Break">&#13;
       relevant Node.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Generation evaluation&#13;
      </html:strong>&#13;
      : RAGAS also evaluates the quality of the generated text using a combination of automatic metrics&#13;
      <html:a id="_idIndexMarker1040">&#13;
      </html:a>&#13;
      and human evaluation. The automatic metrics include&#13;
      <html:strong class="bold">&#13;
       Bilingual Evaluation Understudy&#13;
      </html:strong>&#13;
      (&#13;
      <html:strong class="bold">&#13;
       BLEU&#13;
      </html:strong>&#13;
      ), which measures the similarity between the generated text and a reference text by comparing overlapping word sequences, and&#13;
      <html:strong class="bold">&#13;
       Recall-Oriented Understudy for Gisting Evaluation&#13;
      </html:strong>&#13;
      (&#13;
      <html:strong class="bold">&#13;
       ROUGE&#13;
      </html:strong>&#13;
      ), which calculates the overlap of words&#13;
      <html:a id="_idIndexMarker1041">&#13;
      </html:a>&#13;
      and word sequences between the generated text and the reference text. To complement these automatic metrics, RAGAS also incorporates human evaluation to assess aspects such as fluency, coherence, and relevance of the generated output, providing a comprehensive assessment of the&#13;
      <html:span class="No-Break">&#13;
       generation quality.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Retrieval-generation interplay&#13;
      </html:strong>&#13;
      : The framework also analyzes the interplay between the retrieval and generation components by measuring how much the generated text relies on the retrieved Nodes. It introduces&#13;
      <html:a id="_idIndexMarker1042">&#13;
      </html:a>&#13;
      metrics such as&#13;
      <html:strong class="bold">&#13;
       Retrieval Dependency&#13;
      </html:strong>&#13;
      (&#13;
      <html:strong class="bold">&#13;
       RD&#13;
      </html:strong>&#13;
      ), which quantifies how much the generated text depends&#13;
      <html:a id="_idIndexMarker1043">&#13;
      </html:a>&#13;
      on the retrieved Nodes, and&#13;
      <html:strong class="bold">&#13;
       Retrieval Relevance&#13;
      </html:strong>&#13;
      (&#13;
      <html:strong class="bold">&#13;
       RR&#13;
      </html:strong>&#13;
      ), which measures the relevance of the retrieved Nodes to the generated text to quantify&#13;
      <html:span class="No-Break">&#13;
       this relationship.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Simulation&#13;
      </html:strong>&#13;
      : RAGAS includes a simulation&#13;
      <html:a id="_idIndexMarker1044">&#13;
      </html:a>&#13;
      component that allows us to simulate different retrieval scenarios and analyze their impact on the generation quality. This helps in understanding the robustness and generalization ability of RAG models under various retrieval conditions. By manipulating the retrieval results, users can test how the RAG model performs under scenarios such as retrieving irrelevant, partially relevant, or noisy data. The simulation feature provides insights into the interplay between the retrieval and generation components, enabling us to identify strengths and weaknesses and guide improvements in the&#13;
      <html:span class="No-Break">&#13;
       RAG model.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Fine-grained analysis&#13;
      </html:strong>&#13;
      : RAGAS enables fine-grained analysis of RAG pipelines by providing tools to visualize and interpret the retrieval-generation process, such as attention weights and individual&#13;
      <html:span class="No-Break">&#13;
       Node contributions.&#13;
      </html:span>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:p>&#13;
     A key advantage of this framework is that it enables reference-free evaluation of RAG pipelines, meaning it does not rely on ground truth annotations. This allows for more efficient and scalable&#13;
     <html:span class="No-Break">&#13;
      evaluation cycles.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Compared to Phoenix, RAGAS offers a more focused evaluation framework specifically tailored for RAG systems. While Phoenix provides a general-purpose evaluation platform with features such as tracing, hallucination detection, and relevance assessment, RAGAS goes deeper into the intricacies of retrieval-generation interplay and also offers simulation capabilities. The framework provides seamless integration with LlamaIndex, simplifying the evaluation of LlamaIndex-based RAG systems. To keep things simple, I have not included any code examples in this case, but you can find detailed examples and documentation on the official project’s page, at this&#13;
     <html:span class="No-Break">&#13;
      URL:&#13;
     </html:span>&#13;
     <html:a>&#13;
      <html:span class="No-Break">&#13;
       https://docs.ragas.io/en/stable/howtos/integrations/llamaindex.html&#13;
      </html:span>&#13;
     </html:a>&#13;
     <html:span class="No-Break">&#13;
      .&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     It’s worth noting that RAGAS is a more recent framework compared to Phoenix, and while it shows great promise, it may take some time for it to see the same level of adoption in the&#13;
     <html:span class="No-Break">&#13;
      research community.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p class="callout-heading">&#13;
     Important note&#13;
    </html:p>&#13;
    <html:p class="callout">&#13;
     One thing to always keep in mind in terms of evaluation is the concept of model drift, which we have already covered in&#13;
     <html:a>&#13;
      <html:span class="No-Break">&#13;
       <html:em class="italic">&#13;
        Chapter 7&#13;
       </html:em>&#13;
      </html:span>&#13;
     </html:a>&#13;
     ,&#13;
     <html:em class="italic">&#13;
      Querying Our Data, Part 2 – Postprocessing and Response Synthesis&#13;
     </html:em>&#13;
     section. Model drift can impact our RAG pipeline when the LLM’s behavior gradually deviates from its intended purpose or when the quality of the generated output deteriorates. Regular or even continuous evaluation can help detect and mitigate this phenomenon, ensuring the RAG system remains reliable and effective in&#13;
     <html:span class="No-Break">&#13;
      production environments.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     By mastering tracing and evaluation techniques, you’ll be able to create a complete system for finding and fixing problems in an LLM application. Using evaluations and tracing together, you can spot where things go wrong, figure out why, and see which part of your application needs to&#13;
     <html:span class="No-Break">&#13;
      be improved.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     It’s now time to focus our attention&#13;
     <html:a id="_idIndexMarker1045">&#13;
     </html:a>&#13;
     on our side project: the PITS tutor. In this chapter, we’ll finally get to deploy its components and run it as a standalone application. But first, let’s have a short introduction to the different deployment options provided&#13;
     <html:span class="No-Break">&#13;
      by&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      <html:strong class="bold">&#13;
       Streamlit&#13;
      </html:strong>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      .&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:a id="_idTextAnchor210">&#13;
    </html:a>&#13;
   </html:div>&#13;
  </html:div>&#13;
 </html:body>&#13;
</html:html>
<html:html>&#13;
 <html:head>&#13;
  <html:title>&#13;
   Introduction to deployment with Streamlit&#13;
  </html:title>&#13;
 </html:head>&#13;
 <html:body>&#13;
  <html:div class="epub-source">&#13;
   <html:h1 id="_idParaDest-211">&#13;
    Introduction to deployment with Streamlit&#13;
   </html:h1>&#13;
   <html:div id="_idContainer110">&#13;
    <html:p>&#13;
     As I explained in&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       Chapter 2&#13;
      </html:em>&#13;
     </html:span>&#13;
     ,&#13;
     <html:em class="italic">&#13;
      LlamaIndex: The Hidden Jewel - An Introduction to the LlamaIndex Ecosystem&#13;
     </html:em>&#13;
     , I chose Streamlit&#13;
     <html:a id="_idIndexMarker1046">&#13;
     </html:a>&#13;
     as the backbone for our side project because of its simplicity and the many deployment options it provides. Streamlit offers an easy approach to deploying your applications, making it possible to share your work with a broader audience with minimal effort. If you successfully followed the installation steps in&#13;
     <html:a>&#13;
      <html:span class="No-Break">&#13;
       <html:em class="italic">&#13;
        Chapter 2&#13;
       </html:em>&#13;
      </html:span>&#13;
     </html:a>&#13;
     , your local environment should already be ready for the next steps. However, just in case, before proceeding, make sure you have completed the necessary installation mentioned in&#13;
     <html:a>&#13;
      <html:span class="No-Break">&#13;
       <html:em class="italic">&#13;
        Chapter 2&#13;
       </html:em>&#13;
      </html:span>&#13;
     </html:a>&#13;
     , in the&#13;
     <html:em class="italic">&#13;
      Discovering Streamlit – the perfect tool for quick build and&#13;
     </html:em>&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       deployment&#13;
      </html:em>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      section.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Now that we’re all set up, let’s explore the deployment options available for Streamlit applications. Beyond the simplest method of running apps on your local machine, Streamlit offers a variety of web deployment solutions&#13;
     <html:a id="_idIndexMarker1047">&#13;
     </html:a>&#13;
     to cater to different needs&#13;
     <html:span class="No-Break">&#13;
      and preferences:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Streamlit Community Cloud&#13;
      </html:strong>&#13;
      : This user-friendly platform is the most straightforward option for deploying Streamlit apps, enabling users to deploy directly from their GitHub repositories in just a few clicks. It requires minimal configuration, and once deployed, your app will be accessible via a unique URL on Streamlit Community Cloud, making it easy to share&#13;
      <html:span class="No-Break">&#13;
       with others.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Custom cloud services&#13;
      </html:strong>&#13;
      : For those seeking greater control over their deployment environment, Streamlit&#13;
      <html:a id="_idIndexMarker1048">&#13;
      </html:a>&#13;
      apps can be deployed on various cloud services, including&#13;
      <html:strong class="bold">&#13;
       Amazon Web Services&#13;
      </html:strong>&#13;
      (&#13;
      <html:strong class="bold">&#13;
       AWS&#13;
      </html:strong>&#13;
      ),&#13;
      <html:strong class="bold">&#13;
       Google Cloud Platform&#13;
      </html:strong>&#13;
      (&#13;
      <html:strong class="bold">&#13;
       GCP&#13;
      </html:strong>&#13;
      ), and Azure. Deployment&#13;
      <html:a id="_idIndexMarker1049">&#13;
      </html:a>&#13;
      on these platforms might involve additional steps such as containerizing your app with Docker and configuring cloud-specific services such as AWS Elastic Beanstalk, Google App Engine, or Azure&#13;
      <html:span class="No-Break">&#13;
       App Service.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Self-hosting&#13;
      </html:strong>&#13;
      : If you have your own servers, opting to self-host your Streamlit applications gives you maximum control over the deployment environment and resources. This method involves setting up a server environment capable of running Python applications, installing Streamlit, and configuring your network for Streamlit app access. The self-hosting option answers to specific requirements for security, performance, or customization that cloud platforms&#13;
      <html:span class="No-Break">&#13;
       cannot meet.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Heroku&#13;
      </html:strong>&#13;
      : Heroku (https://www.heroku.com/) is another well-known platform&#13;
      <html:a id="_idIndexMarker1050">&#13;
      </html:a>&#13;
      for deploying Streamlit apps&#13;
      <html:a id="_idIndexMarker1051">&#13;
      </html:a>&#13;
      due to its simplicity and a free tier suitable for small projects&#13;
      <html:span class="No-Break">&#13;
       and prototypes.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Streamlit in Snowflake&#13;
      </html:strong>&#13;
      : For use cases prioritizing security and&#13;
      <html:strong class="bold">&#13;
       role-based access control&#13;
      </html:strong>&#13;
      (&#13;
      <html:strong class="bold">&#13;
       RBAC&#13;
      </html:strong>&#13;
      ), Streamlit’s integration with Snowflake&#13;
      <html:a id="_idIndexMarker1052">&#13;
      </html:a>&#13;
      offers a secure coding and deployment environment within the Snowflake platform. You can easily sign up for a trial Snowflake account, create a warehouse and database for your apps, and deploy Streamlit applications directly&#13;
      <html:a id="_idIndexMarker1053">&#13;
      </html:a>&#13;
      <html:span class="No-Break">&#13;
       within Snowflake.&#13;
      </html:span>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:p>&#13;
     Each of these deployment options offers unique benefits, with different advantages in terms of level of control, scalability, security requirements, and budget constraints. However, I have chosen to show you the simplest option and probably the most appropriate choice for our PITS application: deployment in Streamlit Community Cloud. However, for a commercial-ready solution, the other options would have been a&#13;
     <html:span class="No-Break">&#13;
      better choice.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:a id="_idTextAnchor211">&#13;
    </html:a>&#13;
   </html:div>&#13;
  </html:div>&#13;
 </html:body>&#13;
</html:html>
<html:html>&#13;
 <html:head>&#13;
  <html:title>&#13;
   HANDS-ON – a step-by-step deployment guide&#13;
  </html:title>&#13;
 </html:head>&#13;
 <html:body>&#13;
  <html:div class="epub-source">&#13;
   <html:h1 id="_idParaDest-212">&#13;
    HANDS-ON – a step-by-step deployment guide&#13;
   </html:h1>&#13;
   <html:div id="_idContainer110">&#13;
    from user_onboarding import user_onboarding&#13;
from session_functions import load_session, delete_session, save_session&#13;
from logging_functions import reset_log&#13;
from quiz_UI import show_quiz&#13;
from training_UI import show_training_UI&#13;
import streamlit as st&#13;
    def main():&#13;
    st.set_page_config(layout="wide")&#13;
    st.sidebar.title('P.I.T.S.')&#13;
    st.sidebar.markdown('### Your Personalized Intelligent Tutoring System')&#13;
        if 'show_quiz' in st.session_state and &#13;
    st.session_state['show_quiz']:&#13;
        show_quiz(st.session_state['study_subject'])&#13;
    elif 'resume_session' in st.session_state and &#13;
    st.session_state['resume_session']:&#13;
        st.session_state['show_quiz'] = False&#13;
        show_training_UI(st.session_state['user_name'], &#13;
        st.session_state['study_subject'])&#13;
    elif not load_session(st.session_state):&#13;
        user_onboarding()&#13;
     else:&#13;
        st.write(f"Welcome back {st.session_state['user_name']}!")&#13;
        col1, col2 = st.columns(2)&#13;
        if col1.button(f"Resume your study of &#13;
        {st.session_state['study_subject']}"):&#13;
            st.session_state['resume_session'] = True&#13;
            st.rerun()&#13;
        if col2.button('Start a new session'):&#13;
            delete_session(st.session_state)&#13;
            reset_log()&#13;
            for key in list(st.session_state.keys()):&#13;
                del st.session_state[key]&#13;
            st.rerun()&#13;
    <html:p>&#13;
     It’s time to share our PITS tutoring&#13;
     <html:a id="_idIndexMarker1054">&#13;
     </html:a>&#13;
     application with the world. However, as a quick side note, keep in mind that the current version is far from being ready for use in a multi-user, real-world environment. To keep the code base small and the deployment steps simple, I have designed PITS as a pure experiment in LlamaIndex. After all, the purpose of this book was not to delve into the architectural intricacies of building a full-fledged Streamlit application but rather to explain the tools and features that are available in LlamaIndex. This is the main reason why some of the PITS source files are not explained in detail in this book. Rest assured, however, that you will find plenty of comments in these modules, and if the comments available in the GitHub code aren’t enough, you can always explore the official Streamlit documentation and get a better&#13;
     <html:a id="_idIndexMarker1055">&#13;
     </html:a>&#13;
     understanding of the framework&#13;
     <html:span class="No-Break">&#13;
      here:&#13;
     </html:span>&#13;
     <html:a>&#13;
      <html:span class="No-Break">&#13;
       https://docs.streamlit.io/&#13;
      </html:span>&#13;
     </html:a>&#13;
     <html:span class="No-Break">&#13;
      .&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     However, a brief introduction to the way Streamlit applications is built is in order. We’ll use one of the PITS UI files as an example, and I’ll walk you through the code to give you a basic understanding of the principles of Streamlit applications. Here is the code for&#13;
     <html:code class="literal">&#13;
      app.py&#13;
     </html:code>&#13;
     , our main program in the PITS structure. This code is responsible for orchestrating the execution of the various components that make up the tutoring application. It acts as the central hub, routing users through the onboarding process, handling session management, and dynamically presenting the quiz and training interfaces based on user interactions and&#13;
     <html:span class="No-Break">&#13;
      session data:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     We start by importing&#13;
     <html:a id="_idIndexMarker1056">&#13;
     </html:a>&#13;
     the necessary modules and components, including Streamlit. We also import several custom functions from the other modules, such as&#13;
     <html:code class="literal">&#13;
      user_onboarding&#13;
     </html:code>&#13;
     ,&#13;
     <html:code class="literal">&#13;
      load_session&#13;
     </html:code>&#13;
     ,&#13;
     <html:code class="literal">&#13;
      delete_session&#13;
     </html:code>&#13;
     ,&#13;
     <html:code class="literal">&#13;
      save_session&#13;
     </html:code>&#13;
     ,&#13;
     <html:code class="literal">&#13;
      reset_log&#13;
     </html:code>&#13;
     ,&#13;
     <html:code class="literal">&#13;
      show_quiz&#13;
     </html:code>&#13;
     , and&#13;
     <html:code class="literal">&#13;
      show_training_UI&#13;
     </html:code>&#13;
     , each serving a specific role in the application’s flow. Following the imports, the&#13;
     <html:code class="literal">&#13;
      main()&#13;
     </html:code>&#13;
     function encapsulates the&#13;
     <html:span class="No-Break">&#13;
      application’s logic:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     The use of&#13;
     <html:code class="literal">&#13;
      st.set_page_config&#13;
     </html:code>&#13;
     at the beginning establishes the basic layout of our web application. Streamlit provides a sidebar feature, and we’ll make use of that to streamline our UI. Next, the application’s flow is primarily controlled through conditional statements that check for the presence of certain keys in Streamlit’s&#13;
     <html:code class="literal">&#13;
      (st.session_state)&#13;
     </html:code>&#13;
     session state. This session state acts as persistent storage across reruns of the app within the same browser session, allowing the application to remember user choices, entered information, and other&#13;
     <html:span class="No-Break">&#13;
      stateful data:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p class="callout-heading">&#13;
     A quick note on Streamlit’s session state&#13;
    </html:p>&#13;
    <html:p class="callout">&#13;
     Web applications are inherently&#13;
     <html:a id="_idIndexMarker1057">&#13;
     </html:a>&#13;
     stateless, meaning each request and response between the client and server are independent. Streamlit’s session state allows us to overcome this by providing a way to maintain state across reruns of the app within the same browser session. This is essential for creating an interactive and user-friendly experience, as it allows the application to remember user choices, inputs, and actions without requiring the user to re-enter data after&#13;
     <html:span class="No-Break">&#13;
      every interaction.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     I’ll briefly explain what happens in the previous part of&#13;
     <html:span class="No-Break">&#13;
      the code:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Quiz display logic&#13;
      </html:strong>&#13;
      : If the user has opted to take a quiz (&#13;
      <html:code class="literal">&#13;
       'show_quiz' in st.session_state&#13;
      </html:code>&#13;
      ), the quiz interface is displayed by&#13;
      <html:span class="No-Break">&#13;
       calling&#13;
      </html:span>&#13;
      <html:span class="No-Break">&#13;
       <html:code class="literal">&#13;
        show_quiz()&#13;
       </html:code>&#13;
      </html:span>&#13;
      <html:span class="No-Break">&#13;
       .&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Resuming sessions&#13;
      </html:strong>&#13;
      : If the user has already chosen to resume an existing session (&#13;
      <html:code class="literal">&#13;
       st.session_state['resume_session']=True&#13;
      </html:code>&#13;
      ), the app will take them directly to the&#13;
      <html:span class="No-Break">&#13;
       training UI.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       User onboarding and session management&#13;
      </html:strong>&#13;
      :&#13;
      <html:code class="literal">&#13;
       load_session(st.session_state)&#13;
      </html:code>&#13;
      checks whether session data exists. If not, the user is directed to the onboarding process&#13;
      <html:span class="No-Break">&#13;
       through&#13;
      </html:span>&#13;
      <html:span class="No-Break">&#13;
       <html:code class="literal">&#13;
        user_onboarding().&#13;
       </html:code>&#13;
      </html:span>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:p>&#13;
     Next, let’s see what happens&#13;
     <html:a id="_idIndexMarker1058">&#13;
     </html:a>&#13;
     when an existing session is found but&#13;
     <html:code class="literal">&#13;
      show quiz&#13;
     </html:code>&#13;
     is&#13;
     <html:code class="literal">&#13;
      False&#13;
     </html:code>&#13;
     and the user hasn’t clicked on the&#13;
     <html:strong class="bold">&#13;
      Resume session&#13;
     </html:strong>&#13;
     <html:span class="No-Break">&#13;
      button yet:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     The first operation in this&#13;
     <html:code class="literal">&#13;
      else&#13;
     </html:code>&#13;
     block is displaying a welcome back message. The app then displays two buttons, allowing the user to decide whether they want to resume the existing training session or start a fresh one. Choosing to start a new session will basically reset everything and rerun the entire code to start the application from the beginning. Resuming the session at this point will determine&#13;
     <html:a id="_idIndexMarker1059">&#13;
     </html:a>&#13;
     the app to run&#13;
     <html:code class="literal">&#13;
      show_training_UI&#13;
     </html:code>&#13;
     and continue the existing&#13;
     <html:span class="No-Break">&#13;
      training session.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:a id="_idTextAnchor212">&#13;
    </html:a>&#13;
    <html:h2 id="_idParaDest-213">&#13;
     Deploying our PITS project on Streamlit Community Cloud&#13;
    </html:h2>&#13;
    <html:p>&#13;
     Because of the way&#13;
     <html:a id="_idIndexMarker1060">&#13;
     </html:a>&#13;
     the internal folder&#13;
     <html:a id="_idIndexMarker1061">&#13;
     </html:a>&#13;
     structure of the Streamlit Community Cloud environment is implemented, we’ll have to make a few modifications to our PITS folder structure. The plan is to deploy the application straight from a GitHub repository. However, one of the requirements for deploying from GitHub into the Community Cloud environment is that the main&#13;
     <html:code class="literal">&#13;
      .py&#13;
     </html:code>&#13;
     file is hosted in the&#13;
     <html:code class="literal">&#13;
      root&#13;
     </html:code>&#13;
     folder of the repository. That is not the case for PITS as the folder structure is a bit different.&#13;
     <html:code class="literal">&#13;
      app.py&#13;
     </html:code>&#13;
     , which is the main file in our case, is currently found in the&#13;
     <html:code class="literal">&#13;
      Building-Data-Driven-Applications-with-LlamaIndex\PITS_APP&#13;
     </html:code>&#13;
     folder. To fix that, we’ll first make a copy of the&#13;
     <html:code class="literal">&#13;
      PITS_APP&#13;
     </html:code>&#13;
     subfolder, and then we’ll initiate a new GitHub repository from that new folder. To keep things simple and require minimum changes, I will guide you on how to create a new repository containing just the PITS app and then deploy it from your own&#13;
     <html:span class="No-Break">&#13;
      GitHub account:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:ol>&#13;
     <html:li>&#13;
      First, let’s create a copy&#13;
      &#13;
     <html:a id="_idIndexMarker1062">&#13;
      </html:a>&#13;
      of our local&#13;
      <html:code class="literal">&#13;
       PITS_APP&#13;
      </html:code>&#13;
      subfolder. Open Command Prompt&#13;
      <html:a id="_idIndexMarker1063">&#13;
      </html:a>&#13;
      and navigate to the&#13;
      <html:code class="literal">&#13;
       Building-Data-Driven-Applications-with-LlamaIndex&#13;
      </html:code>&#13;
      folder of your cloned repository. From that folder, type the&#13;
      <html:span class="No-Break">&#13;
       following command:&#13;
      </html:span>&#13;
      </html:li>&#13;
     <html:li>&#13;
      This will create a folder on your&#13;
      <html:code class="literal">&#13;
       C:&#13;
      </html:code>&#13;
      drive containing only the source files of the PITS application. If you navigate to the newly created folder and list its contents with the&#13;
      <html:code class="literal">&#13;
       dir&#13;
      </html:code>&#13;
      command, the output should look like&#13;
      <html:span class="No-Break">&#13;
       <html:em class="italic">&#13;
        Figure 9&#13;
       </html:em>&#13;
      </html:span>&#13;
      <html:span class="No-Break">&#13;
       <html:em class="italic">&#13;
        .8&#13;
       </html:em>&#13;
      </html:span>&#13;
      <html:span class="No-Break">&#13;
       :&#13;
      </html:span>&#13;
     </html:li>&#13;
    </html:ol>&#13;
    <html:div>&#13;
     <html:div class="IMG---Figure" id="_idContainer107">&#13;
      <html:img src="../Images/B21861_09_8.jpg"/>&#13;
     </html:div>&#13;
    </html:div>&#13;
    <html:p class="IMG---Caption" lang="en-US">&#13;
     Figure 9.8 – The contents of the C:\PITS_APP folder&#13;
    </html:p>&#13;
    <html:ol>&#13;
     <html:li>&#13;
      The next step is to sign in to your GitHub account and create a new repository. Let’s name it&#13;
      <html:code class="literal">&#13;
       PITS_ONLINE&#13;
      </html:code>&#13;
      , as in&#13;
      <html:span class="No-Break">&#13;
       <html:em class="italic">&#13;
        Figure 9&#13;
       </html:em>&#13;
      </html:span>&#13;
      <html:span class="No-Break">&#13;
       <html:em class="italic">&#13;
        .9&#13;
       </html:em>&#13;
      </html:span>&#13;
      <html:span class="No-Break">&#13;
       :&#13;
      </html:span>&#13;
     </html:li>&#13;
    </html:ol>&#13;
    <html:div>&#13;
     <html:div class="IMG---Figure" id="_idContainer108">&#13;
      <html:img src="../Images/B21861_09_9.jpg"/>&#13;
     </html:div>&#13;
    </html:div>&#13;
    <html:p class="IMG---Caption" lang="en-US">&#13;
     Figure 9.9 – Creating a new GitHub repository named PITS_ONLINE&#13;
    </html:p>&#13;
    <html:ol>&#13;
     <html:li>&#13;
      Once created, note the repository&#13;
      &#13;
     <html:a id="_idIndexMarker1064">&#13;
      </html:a>&#13;
      URL for the next steps. Next, we’ll initialize&#13;
      <html:a id="_idIndexMarker1065">&#13;
      </html:a>&#13;
      a new local repository in the desired folder. Open your CLI and navigate to the folder you want to turn into a separate repository –&#13;
      <html:code class="literal">&#13;
       C:\PITS_APP&#13;
      </html:code>&#13;
      – then execute the&#13;
      <html:span class="No-Break">&#13;
       following command:&#13;
      </html:span>&#13;
      </html:li>&#13;
     <html:li>&#13;
      Next, add and commit the existing files by running the&#13;
      &#13;
     <html:span class="No-Break">&#13;
       following command:&#13;
      </html:span>&#13;
      </html:li>&#13;
     <html:li>&#13;
      It’s now time to link your local repository to the GitHub repository you created. Replace the URL with your GitHub URL and append&#13;
      &#13;
     <html:code class="literal">&#13;
       .git&#13;
      </html:code>&#13;
      at the end in the&#13;
      <html:span class="No-Break">&#13;
       following command:&#13;
      </html:span>&#13;
      </html:li>&#13;
     <html:li>&#13;
      And finally, we push the contents to the new online repository with the&#13;
      &#13;
     <html:span class="No-Break">&#13;
       following command:&#13;
      </html:span>&#13;
      </html:li>&#13;
    </html:ol>&#13;
    <html:p>&#13;
     If everything went smoothly you should now have a brand-new GitHub repository containing the PITS&#13;
     <html:span class="No-Break">&#13;
      source code.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Let’s handle the Community Cloud&#13;
     <html:span class="No-Break">&#13;
      deployment next.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Deploying Streamlit applications into their Community Cloud environment is a fairly simple and straightforward process. To begin our deployment, the first step is to sign up for a free Streamlit account here:&#13;
     <html:a>&#13;
      https://share.streamlit.io/signup&#13;
     </html:a>&#13;
     . The best option is to use your GitHub account both for signing up and signing in to your Streamlit account. Once logged in, simply click on the&#13;
     <html:strong class="bold">&#13;
      New app&#13;
     </html:strong>&#13;
     button to begin the deployment process. You’ll be taken to a screen similar to what you can see in&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       Figure 9&#13;
      </html:em>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       .10&#13;
      </html:em>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      :&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:div>&#13;
     <html:div class="IMG---Figure" id="_idContainer109">&#13;
      <html:img src="../Images/B21861_09_10.jpg"/>&#13;
     </html:div>&#13;
    </html:div>&#13;
    <html:p class="IMG---Caption" lang="en-US">&#13;
     Figure 9.10 – Deploying an application into Streamlit Community Cloud&#13;
    </html:p>&#13;
    <html:p>&#13;
     If you signed in to Streamlit&#13;
     <html:a id="_idIndexMarker1066">&#13;
     </html:a>&#13;
     using GitHub, you should already&#13;
     <html:a id="_idIndexMarker1067">&#13;
     </html:a>&#13;
     have the&#13;
     <html:code class="literal">&#13;
      PITS_ONLINE&#13;
     </html:code>&#13;
     repository listed as an option. Select it, then, under the&#13;
     <html:strong class="bold">&#13;
      Main file path&#13;
     </html:strong>&#13;
     field, change the default value to&#13;
     <html:code class="literal">&#13;
      app.py&#13;
     </html:code>&#13;
     and then click&#13;
     <html:strong class="bold">&#13;
      Deploy&#13;
     </html:strong>&#13;
     . From here, the Streamlit deployment service takes over and prepares the required environment for your application. This might take a while, but if you want to check on the progress, you can always expand the&#13;
     <html:strong class="bold">&#13;
      Manage app&#13;
     </html:strong>&#13;
     section on the bottom right of your screen. When everything is ready, the application should&#13;
     <html:span class="No-Break">&#13;
      start automatically.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     You can now ingest your existing training materials, have PITS generate slides and narrations about your desired study topic, and ask its chatbot any questions related to&#13;
     <html:span class="No-Break">&#13;
      the contents.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p class="callout-heading">&#13;
     Important note&#13;
    </html:p>&#13;
    <html:p class="callout">&#13;
     Don’t forget, you’re using your own API key. To keep costs under control, you should first experiment on a limited scale by uploading some small training resources and always keeping an eye on the OpenAI API usage. The good news is that the majority of the cost is incurred during slides and narration generation. However, once that is completed, the resulting material is stored and reused in&#13;
     <html:span class="No-Break">&#13;
      future sessions.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Simple, isn’t it? Although offering an environment with limited resources, the Streamlit Community Cloud service makes it really easy to deploy simple apps and share quick prototypes. Your app is now online and can easily be shared with&#13;
     <html:span class="No-Break">&#13;
      other users.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     If anything went wrong, though, and you didn’t manage to complete the deployment, head over to the official&#13;
     <html:a id="_idIndexMarker1068">&#13;
     </html:a>&#13;
     documentation, and look for a solution:&#13;
     <html:a>&#13;
      https://docs.streamlit.io/streamlit-community-cloud/deploy-your-app&#13;
     </html:a>&#13;
     . In the Streamlit documentation, you’ll also find additional deployment options and configurations available&#13;
     <html:a id="_idIndexMarker1069">&#13;
     </html:a>&#13;
     that might be useful for your&#13;
     <html:span class="No-Break">&#13;
      future&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      <html:a id="_idIndexMarker1070">&#13;
      </html:a>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      projects.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:a id="_idTextAnchor213">&#13;
    </html:a>&#13;
   </html:div>&#13;
  </html:div>&#13;
 </html:body>&#13;
</html:html>
<html:html>&#13;
 <html:head>&#13;
  <html:title>&#13;
   Summary&#13;
  </html:title>&#13;
 </html:head>&#13;
 <html:body>&#13;
  <html:div class="epub-source">&#13;
   <html:h1 id="_idParaDest-214">&#13;
    Summary&#13;
   </html:h1>&#13;
   <html:div id="_idContainer110">&#13;
    <html:p>&#13;
     In this chapter, we explored customizing and enhancing RAG workflows with LlamaIndex. We covered techniques to leverage open source LLMs such as Zephyr using tools such as LM Studio, offering cost-effective and privacy-focused alternatives to commercial models. The chapter discussed intelligent routing across multiple LLMs with services such as Neutrino and OpenRouter for optimized performance. Community-built Llama Packs were highlighted as powerful ways to rapidly prototype and build advanced components, and the chapter introduced the Llama CLI for streamlining RAG development and&#13;
     <html:span class="No-Break">&#13;
      deployment workflows.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     We talked about advanced tracing with Phoenix, allowing us to gain deep insight into application execution flows and pinpoint problems through visualization. The evaluation of RAG systems was covered using Phoenix’s relevance, hallucination, and QA correctness evaluators, ensuring the robust performance of our LlamaIndex apps. Streamlit’s deployment options, especially the Community Cloud service for easy application sharing, simplified the deployment process. A step-by-step guide demonstrated how to deploy the PITS tutoring application to&#13;
     <html:span class="No-Break">&#13;
      the cloud.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     With a strong grasp of customization, evaluation, and deployment techniques, developers can now build production-ready, optimized RAG applications tailored to their&#13;
     <html:span class="No-Break">&#13;
      unique requirements.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Our journey continues with an exploration of the role of prompt engineering in enhancing the effectiveness of GenAI within the&#13;
     <html:span class="No-Break">&#13;
      LlamaIndex framework.&#13;
     </html:span>&#13;
    </html:p>&#13;
   </html:div>&#13;
  </html:div>&#13;
 </html:body>&#13;
</html:html></body></html>