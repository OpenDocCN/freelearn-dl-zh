- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mastering Linear Algebra, Probability, and Statistics for Machine Learning and
    NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Natural language processing** (**NLP**) and **machine learning** (**ML**)
    are two fields that have significantly benefited from mathematical concepts, particularly
    linear algebra and probability theory. These fundamental tools enable the analysis
    of the relationships between variables, forming the basis of many NLP and ML models.
    This chapter provides a comprehensive introduction to linear algebra and probability
    theory, including their practical applications in NLP and ML. The chapter commences
    with an overview of vectors and matrices and covers essential operations. Additionally,
    the basics of statistics, required for understanding the concepts and models in
    subsequent chapters, will be explained. Finally, the chapter introduces the fundamentals
    of optimization, which are critical for solving NLP problems and understanding
    the relationships between variables. By the end of this chapter, you will have
    a solid foundation in linear algebra and probability theory and understand their
    essential applications in NLP and ML.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to linear algebra
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eigenvalues and eigenvectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic probability for machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to linear algebra
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s start by first understanding scalars, vectors, and matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalars**: A scalar is a single numerical value that usually comes from the
    real domain in most ML applications. Examples of scalars in NLP include the frequency
    of a word in a text corpus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vectors**: A vector is a collection of numerical elements. Each of these
    elements can be termed as an entry, component, or dimension, and the count of
    these components defines the vector’s dimensionality. Within NLP, a vector could
    hold components related to elements such as word frequency, sentiment ranking,
    and more. NLP and ML are two domains that have reaped substantial benefits from
    mathematical disciplines, particularly linear algebra and probability theory.
    These foundational tools aid in evaluating the correlation between variables and
    are at the heart of numerous NLP and ML models. This segment presents a detailed
    primer on linear algebra and probability theory, along with their practical usage
    in NLP and ML. For instance, a text document’s three-dimensional vector representation
    might be expressed as a real-number array, such as [word frequency, sentiment
    ranking, complexity].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Matrices**: A matrix can be perceived as a rectangular collection of numerical
    elements composed of rows and columns. To retrieve an element from the matrix,
    one needs to denote its row and column indices. In the field of NLP, a data matrix
    might include rows that align with distinct text documents and columns that align
    with different text attributes, such as word frequency, sentiment, and so on.
    The dimensions of such a matrix are represented by the notation *n × d*, where
    *n* is the number of rows (i.e., text documents), and *d* is the number of columns
    (i.e., attributes).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s move on to the basic operations for scalars, vectors, and matrices next.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic operations for scalars, vectors, and matrices—addition and subtraction—can
    be carried out on vectors with the same dimensions. Let’s have two vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/1.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">y</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">x</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>-</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi mathvariant="bold">y</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/3.png)'
  prefs: []
  type: TYPE_IMG
- en: For example, if we have two vectors, a = [4,1] and b = [2,4], then *a + b =
    [**6,5]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s visualize this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Adding two vectors (a = [4,1] and  b = [2,4]) means that a  +
    b = [6,5]](img/B18949_02_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – Adding two vectors (a = [4,1] and b = [2,4]) means that a + b =
    [6,5]
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to scale a vector by multiplying it by a scalar. This operation
    is performed by multiplying each component of the vector by the scalar value.
    For example, let’s consider a n-dimensional vector, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4.png).
    The process of scaling this vector by a factor of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>a</mml:mi></mml:math>](img/5.png)
    can be represented mathematically as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/1.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>a</mml:mi><mml:mo>∙</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/7.png)'
  prefs: []
  type: TYPE_IMG
- en: This operation results in a new vector that has the same dimensionality as the
    original vector but with each component multiplied by the scalar value ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>a</mml:mi></mml:math>](img/8.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of multiplications between vectors: dot product (![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∙</mml:mo></mml:math>](img/9.png))
    and cross product (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>×</mml:mo></mml:math>](img/10.png)).
    The dot product is the one we use often in ML algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dot product is a mathematical operation that can be applied to two vectors,
    x = [x 1, x 2, … , x n] and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/11.png).
    It has many practical applications, one of which is to help determine their similarity.
    It is defined as the sum of the product of the corresponding elements of the two
    vectors. The dot product of *x* and *y* is represented by the symbol *x**![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∙</mml:mo></mml:math>](img/12.png)*
    *y* (having a dot in the middle) and is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>∙</mml:mo><mml:mi
    mathvariant="bold">y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>.</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/13.png)'
  prefs: []
  type: TYPE_IMG
- en: where *n* represents the dimensionality of the vectors. The dot product is a
    scalar quantity and can be used to measure the angle between two vectors, as well
    as the projection of one vector onto another. It also serves a vital function
    in numerous ML algorithms, including linear regression and neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dot product is commutative, meaning that the order of the vectors does
    not affect the result. This means that *x* *![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∙</mml:mo></mml:math>](img/12.png)*
    *y = y**![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∙</mml:mo></mml:math>](img/12.png)*
    *x*. Furthermore, the dot product maintains the distributive property of scalar
    multiplication, implying the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>∙</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>+</mml:mo><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi
    mathvariant="bold">x</mml:mi><mml:mo>∙</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>+</mml:mo><mml:mi
    mathvariant="bold">x</mml:mi><mml:mo>∙</mml:mo><mml:mi mathvariant="bold">z</mml:mi></mml:math>](img/16.png)'
  prefs: []
  type: TYPE_IMG
- en: The dot product of a vector with itself is also known as its squared norm or
    Euclidean norm. The norm, symbolized by *𝑛𝑜𝑟𝑚**(x)*, signifies the length of the
    vector and is computed as
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi
    mathvariant="bold">x</mml:mi><mml:mo>∙</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math>](img/17.png)'
  prefs: []
  type: TYPE_IMG
- en: The normalization of vectors can be achieved by dividing them by their norm,
    also known as the Euclidean norm or the length of the vector. This results in
    a vector with a unit length, denoted by *x’*. The normalization process can be
    shown as
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi
    mathvariant="bold">x</mi><mo>′</mo></mrow><mspace width="0.25em" /><mo>=</mo><mspace
    width="0.25em" /><mfrac><mi mathvariant="bold">x</mi><mfenced open="‖" close="‖"><mi
    mathvariant="bold">x</mi></mfenced></mfrac><mspace width="0.25em" /><mo>=</mo><mspace
    width="0.25em" /><mfrac><mi mathvariant="bold">x</mi><msqrt><mrow><mi mathvariant="bold">x</mi><mo>∙</mo><mi
    mathvariant="bold">x</mi></mrow></msqrt></mfrac></mrow></mrow></math>](img/18.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">x</mml:mi></mml:math>](img/19.png) is the original vector and
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="‖" close="‖" separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/20.png)
    represents its norm. It should be noted that normalizing a vector has the effect
    of retaining its direction while setting its length to 1, allowing the meaningful
    comparison of vectors in different spaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cosine similarity between two vectors ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi
    mathvariant="bold-italic">x</mi><mo>=</mo><mspace width="0.25em" /><mfenced open="["
    close="]"><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mspace
    width="0.25em" /><mo>…</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mspace
    width="0.25em" /></mrow></mfenced></mrow></mrow></math>](img/21.png)and ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold-italic">y</mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mi> </mml:mi></mml:mrow></mml:mfenced></mml:math>](img/22.png)
    is mathematically represented as the dot product of the two vectors after they
    have been normalized to unit length. This can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>C</mi><mi>o</mi><mi>s</mi><mfenced
    open="(" close=")"><mrow><mi mathvariant="bold">x</mi><mo>,</mo><mi mathvariant="bold">y</mi></mrow></mfenced><mspace
    width="0.25em" /><mo>=</mo><mspace width="0.25em" /><mfrac><mfenced open="(" close=")"><mrow><mi
    mathvariant="bold">x</mi><mo>∙</mo><mi mathvariant="bold">y</mi></mrow></mfenced><mfenced
    open="(" close=")"><mrow><mfenced open="‖" close="‖"><mi mathvariant="bold">x</mi></mfenced><mo>∙</mo><mfenced
    open="‖" close="‖"><mi mathvariant="bold">y</mi></mfenced></mrow></mfenced></mfrac><mspace
    width="0.25em" /><mo>=</mo><mspace width="0.25em" /><mfrac><mfenced open="(" close=")"><mrow><mi
    mathvariant="bold">x</mi><mo>∙</mo><mi mathvariant="bold">y</mi></mrow></mfenced><mfenced
    open="(" close=")"><mrow><msqrt><mrow><mi mathvariant="bold">x</mi><mo>∙</mo><mi
    mathvariant="bold">x</mi></mrow></msqrt><mo>∙</mo><msqrt><mrow><mi mathvariant="bold">y</mi><mo>∙</mo><mi
    mathvariant="bold">y</mi></mrow></msqrt></mrow></mfenced></mfrac></mrow></mrow></math>](img/23.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="‖" close="‖" separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/24.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="‖" close="‖" separators="|"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/25.png)
    are the norms of the vectors *x* and *y*, respectively. This computed cosine similarity
    between *x* and *y* is equivalent to the cosine of the angle between the two vectors,
    denoted as *θ*.
  prefs: []
  type: TYPE_NORMAL
- en: Vectors with a dot product of *0* are deemed orthogonal, implying that in the
    case of having both non-*0* vectors, the angle between them is 90 degrees. We
    can conclude that a *0* vector is orthogonal to any vector. A group of vectors
    is considered orthogonal if each pair of them is orthogonal and each vector possesses
    a norm of *1*. Such orthonormal sets prove to be valuable in numerous mathematical
    contexts. For instance, they come into play when transforming between different
    orthogonal co-ordinate systems, where the new co-ordinates of a point are computed
    in relation to the modified direction set. This approach, known as co-ordinate
    transformation in the field of analytical geometry, finds widespread application
    in the realm of linear algebra.
  prefs: []
  type: TYPE_NORMAL
- en: Basic operations on matrices and vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Matrix transpose** is the process of obtaining the transpose of a matrix
    and involves interchanging its rows and columns. This means that the element originally
    at the *(i, j)*th position in the matrix now occupies the *(j, i)*th position
    in its transpose. As a result, a matrix that was originally of size *n × m* becomes
    an *m × n* matrix when transposed. The notation used to represent the transpose
    of matrix *X* is ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math>](img/26.png).
    Here’s an illustrative example of a matrix transposition operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">X</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1,1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1,2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2,1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2,2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>3,1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>3,2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math>](img/27.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1,1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2,1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>3,1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1,2</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2,2</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>3,2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math>](img/28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Crucially, the transpose ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi
    mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math>](img/29.png)
    of matrix ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math>](img/26.png)
    reverts to the original matrix *X*. Moreover, it is clear that row vectors can
    be transposed into column vectors and vice versa. Additionally, the following
    holds true for both matrices and vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi
    mathvariant="bold">X</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>+</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math>](img/31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It’s also noteworthy that dot products are commutative for matrices and vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msup><mi
    mathvariant="bold">X</mi><mi>T</mi></msup><mo>∙</mo><mspace width="0.25em" /><mi
    mathvariant="bold">Y</mi><mspace width="0.25em" /><mo>=</mo><mspace width="0.25em"
    /><msup><mi mathvariant="bold">Y</mi><mrow><mi>T</mi><mspace width="0.25em" /></mrow></msup><mo>∙</mo><mi
    mathvariant="bold">X</mi></mrow></mrow></math>](img/32.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msup><mi
    mathvariant="bold">x</mi><mi>T</mi></msup><mo>∙</mo><mspace width="0.25em" /><mi
    mathvariant="bold">y</mi><mspace width="0.25em" /><mo>=</mo><mspace width="0.25em"
    /><msup><mi mathvariant="bold">y</mi><mrow><mi>T</mi><mspace width="0.25em" /></mrow></msup><mo>∙</mo><mi
    mathvariant="bold">x</mi></mrow></mrow></math>](img/33.png)'
  prefs: []
  type: TYPE_IMG
- en: Matrix definitions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we’ll cover the different type of matrix definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Symmetric matrix**: A symmetric matrix is a type of square matrix where the
    transpose of the matrix is equal to the original matrix. In mathematical terms,
    if a matrix *X* is symmetric, then ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">X</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math>](img/34.png).
    For example,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">X</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>2</mml:mn></mml:mtd><mml:mtd><mml:mn>3</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>2</mml:mn></mml:mtd><mml:mtd><mml:mn>4</mml:mn></mml:mtd><mml:mtd><mml:mn>5</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>3</mml:mn></mml:mtd><mml:mtd><mml:mn>5</mml:mn></mml:mtd><mml:mtd><mml:mn>7</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math>](img/35.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: is symmetric.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Rectangular diagonal matrix**: This is a matrix that is *m × n* in dimensions,
    with non-*0* values only on the main diagonal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Upper (or Lower) triangular matrix**: A matrix is called an upper (triangular)
    matrix if all the entries (i,j) below (above) its main diagonal are 0\. Next,
    we are going to describe matrix operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determinants
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The determinant of a square matrix provides a notion of its impact on the volume
    of a *d*-dimensional object when multiplied by its co-ordinate vectors. The determinant,
    symbolized as *det(A)*, represents the (signed) volume of the parallelepiped formed
    by the row or column vectors of the matrix. This interpretation holds consistently,
    as the volume determined by the row and column vectors is mathematically identical.
    When a diagonalizable matrix *A* interacts with a group of co-ordinate vectors,
    the ensuing distortion is termed anisotropic scaling. The determinant can aid
    in establishing the scale factors of this conversion. The determinant of a square
    matrix carries crucial insights about the linear alteration accomplished by the
    multiplication with the matrix. Particularly, the sign of the determinant mirrors
    the impact of the transformation on the basis of the system’s orientation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculating determinant is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: For a *1×1* matrix *A*, its determinant is equivalent to the single scalar present
    within it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For larger matrices, the determinant can be calculated by securing a column,
    *j*, and then broadening using the elements within that column. As another option,
    it’s possible to fix a row, *i*, and expand along that particular row. Regardless
    of whether you opt to fix a row or column, the end result, which is the determinant
    of the matrix, will remain consistent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: with *j* as a fixed value ranging from *1* to *d*,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">det</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">det</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/36.png)'
  prefs: []
  type: TYPE_IMG
- en: Or, with the fixed *i*,
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>det</mi><mfenced
    open="(" close=")"><mi mathvariant="bold">A</mi></mfenced><mspace width="0.25em"
    /><mo>=</mo><mspace width="0.25em" /><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mspace
    width="0.25em" /><mo>=</mo><mspace width="0.25em" /><mn>1</mn></mrow><mi>d</mi></munderover><msup><mfenced
    open="(" close=")"><mrow><mo>−</mo><mn>1</mn></mrow></mfenced><mfenced open="("
    close=")"><mrow><mi>i</mi><mo>+</mo><mi>j</mi></mrow></mfenced></msup></mrow><msub><mi>a</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>det</mi><mfenced
    open="(" close=")"><msub><mi mathvariant="bold">A</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mfenced></mrow></mrow></math>](img/37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Based on the following equations, we can see that some of the cases can be
    easily calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Diagonal matrix**: For a diagonal matrix, the determinant is the product
    of its diagonal elements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Triangular matrix**: In the context of a triangular matrix, the determinant
    is found by multiplying all its diagonal elements. If all components of a matrix’s
    row or column are 0, the determinant is also 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a *2 × 2* matrix of
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">A</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>a</mml:mi></mml:mtd><mml:mtd><mml:mi>b</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>c</mml:mi></mml:mtd><mml:mtd><mml:mi>d</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math>](img/38.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Its determinant can be computed as *ad - bc*. If we consider a *3 ×* *3* matrix,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">A</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>a</mml:mi></mml:mtd><mml:mtd><mml:mi>b</mml:mi></mml:mtd><mml:mtd><mml:mi>c</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>d</mml:mi></mml:mtd><mml:mtd><mml:mi>e</mml:mi></mml:mtd><mml:mtd><mml:mi>f</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>g</mml:mi></mml:mtd><mml:mtd><mml:mi>h</mml:mi></mml:mtd><mml:mtd><mml:mi>i</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math>](img/39.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'The determinant is calculated as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">det</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>a</mml:mi><mml:mo>.</mml:mo><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">det</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>h</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>d</mml:mi><mml:mo>.</mml:mo><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">det</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mi>b</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>h</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>g</mml:mi><mml:mo>.</mml:mo><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">det</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mi>b</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>e</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/40.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mo>=</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>a</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>-</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>h</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mfenced><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>d</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>-</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>h</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:mfenced><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>b</mml:mi><mml:mi>f</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>-</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/41.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mo>=</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>a</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>a</mml:mi><mml:mi>h</mml:mi><mml:mi>f</mml:mi><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>d</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>d</mml:mi><mml:mi>h</mml:mi><mml:mi>c</mml:mi><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>g</mml:mi><mml:mi>b</mml:mi><mml:mi>f</mml:mi><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:math>](img/42.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s now move on to eigenvalues and vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Eigenvalues and eigenvectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A vector *x*, belonging to a *d × d* matrix *A*, is an **eigenvector** if it
    satisfies the equation *Ax = λx*, where *λ* represents the eigenvalue associated
    with the matrix. This relationship delineates the link between matrix *A* and
    its corresponding eigenvector *x*, which can be perceived as the “stretching direction”
    of the matrix. In the case where *A* is a matrix that can be diagonalized, it
    can be deconstructed into a *d × d* invertible matrix, *V*, and a diagonal *d
    × d* matrix, *Δ*, such that
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">A</mml:mi><mml:mi mathvariant="bold"> </mml:mi><mml:mo>=</mml:mo><mml:mi
    mathvariant="bold"> </mml:mi><mml:mi mathvariant="bold">V</mml:mi><mml:mi mathvariant="bold"> </mml:mi><mml:mi
    mathvariant="bold">Δ</mml:mi><mml:mi mathvariant="bold"> </mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math>](img/43.png)'
  prefs: []
  type: TYPE_IMG
- en: The columns of *V* encompass *d* eigenvectors, while the diagonal entries of
    *Δ* house the corresponding eigenvalues. The linear transformation *Ax* can be
    visually understood through a sequence of three operations. Initially, the multiplication
    of *x* by ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math>](img/44.png)
    calculates *x*’s co-ordinates in a non-orthogonal basis associated with *V*’s
    columns. Subsequently, the multiplication of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math>](img/45.png)
    x by *Δ* scales these co-ordinates using the factors in *Δ*, aligned with the
    eigenvectors’ directions. Finally, the multiplication with *V* restores the co-ordinates
    to the original basis, resulting in an anisotropic scaling along the *d* eigenvector
    directions.
  prefs: []
  type: TYPE_NORMAL
- en: Diagonalizable matrices signify transformations involving anisotropic scaling
    along *d*-linearly independent directions. When *V* ‘s columns are orthonormal
    vectors, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi mathvariant="bold">V</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></math>](img/46.png)equals
    its transpose, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math>](img/47.png),
    indicating scaling along mutually orthogonal directions. In such cases, matrix
    *A* is always diagonalizable and exhibits symmetry when *V*’s columns are orthonormal
    vectors, as affirmed by the following relationship.
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi
    mathvariant="bold">V</mml:mi><mml:mi> </mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">Δ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi
    mathvariant="bold">V</mml:mi><mml:mi mathvariant="bold-italic"> </mml:mi><mml:mi
    mathvariant="bold">Δ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi
    mathvariant="bold">A</mml:mi></mml:math>](img/48.png)'
  prefs: []
  type: TYPE_IMG
- en: Numerical methods for finding eigenvectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The conventional method to ascertain the eigenvectors of a *d × d* matrix *A*
    involves locating the *d* roots, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>λ</mi><mn>1</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>λ</mi><mi>d</mi></msub></mrow></mrow></math>](img/49.png)of
    the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>d</mi><mi>e</mi><mi>t</mi><mo>(</mo><mi
    mathvariant="bold">A</mi><mo>−</mo><mi>λ</mi><mi mathvariant="bold">I</mi><mo>)</mo><mo>=</mo><mn>0</mn></mrow></mrow></mrow></math>](img/50.png)'
  prefs: []
  type: TYPE_IMG
- en: Some of these roots might be repeated. The subsequent step involves solving
    linear systems in the form ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mo>-</mml:mo><mml:mi>λ</mml:mi><mml:mi
    mathvariant="bold">I</mml:mi></mml:mrow></mml:mfenced><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/51.png),
    typically achieved using the Gaussian elimination method. However, this method
    might not always be the most stable or precise, as solvers of polynomial equations
    can exhibit ill-conditioning and numerical instability in practical applications.
    Indeed, a prevalent technique for resolving high-degree polynomial equations in
    engineering involves constructing a companion matrix possessing the same characteristic
    polynomial as the original polynomial and then determining its eigenvalues.
  prefs: []
  type: TYPE_NORMAL
- en: Eigenvalue decomposition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Eigenvalue decomposition, also known as the eigen-decomposition or the diagonalization
    of a matrix, is a powerful mathematical tool used in linear algebra and computational
    mathematics. The goal of eigenvalue decomposition is to decompose a given matrix
    into a product of matrices that represent the eigenvectors and eigenvalues of
    the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'The eigenvalue decomposition of matrix *A* is a factorization of the matrix
    into the product of two matrices: the matrix *V* and the matrix *D*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*V* has column which are the eigenvectors of matrix *A*, and *D* is a diagonal
    matrix that contains the corresponding eigenvalues on its diagnol.'
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvalue problem is to find the non-0 vectors, *v*, and the scalars, *λ*,
    such that *Av* *= λv*, where *A* is a square matrix, and thus *v* is an eigenvector
    of *A*. The scalar *λ* is called the eigenvalue of matrix *A*. The eigenvalue
    problem can be written in matrix form as *Av = λIv*, where *I* is the identity
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: The process of determining eigenvalues is intimately linked to the characteristic
    equation of matrix *A*, which is the polynomial equation derived from ![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>d</mi><mi>e</mi><mi>t</mi><mo>(</mo><mi
    mathvariant="bold">A</mi><mo>−</mo><mi>λ</mi><mi mathvariant="bold">I</mi><mo>)</mo><mo>=</mo><mn>0</mn></mrow></mrow></mrow></math>](img/52.png).
    The characteristic equation can be solved for the eigenvalues, *λ*, which are
    the roots of the equation. Once the eigenvalues are found, the eigenvectors can
    be found by solving the system of linear equations ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mo>-</mml:mo><mml:mi>λ</mml:mi><mml:mi
    mathvariant="bold">I</mml:mi></mml:mrow></mml:mfenced><mml:mi mathvariant="bold">v</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/53.png).
  prefs: []
  type: TYPE_NORMAL
- en: One important property of eigenvalue decomposition is that it allows us to diagonalize
    a matrix, which means that we can transform the matrix into a diagonal form by
    using an appropriate eigenvectors matrix. The diagonal form of a matrix is useful
    because it allows us to calculate the trace and determinant of the matrix easily.
  prefs: []
  type: TYPE_NORMAL
- en: Another important property of eigenvalue decomposition is that it provides insight
    into the structure of the matrix. For example, the eigenvalues of a symmetric
    matrix are always real, and the eigenvectors are orthogonal, which means that
    they are perpendicular to each other. In the case of non-symmetric matrices, the
    eigenvalues can be complex, and the eigenvectors are not necessarily orthogonal.
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvalue decomposition of a matrix has many applications in mathematics,
    physics, engineering, and computer science. In numerical analysis, eigenvalue
    decomposition is used to find the solution of linear systems, compute the eigenvalues
    of a matrix, and find the eigenvectors of a matrix. In physics, eigenvalue decomposition
    is used to analyze the stability of systems, such as the stability of an equilibrium
    point in a differential equation. In engineering, eigenvalue decomposition is
    used to study the dynamics of systems, such as the vibrations of a mechanical
    system.
  prefs: []
  type: TYPE_NORMAL
- en: Within the field of computer science, eigenvalue decomposition finds versatile
    applications across various domains, including machine learning and data analysis.
    In machine learning, eigenvalue decomposition plays a pivotal role in enabling
    **principal** **component analysis** (**PCA**), a technique employed for dimensionality
    reduction in extensive datasets. In the realm of data analysis, eigenvalue decomposition
    is harnessed to calculate the **singular value decomposition** (**SVD**), a potent
    tool for dissecting and understanding complex datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Singular value decomposition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The problem of minimizing ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi
    mathvariant="bold">A</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:math>](img/54.png),
    where *x* is a column vector that has a unit norm, and *A* is a symmetric *d ×
    d* data matrix, is a typical problem encountered in numerous machine learning
    contexts. This problem type is often found in applications such as principal component
    analysis, singular value decomposition, and spectral clustering, all of which
    involve feature engineering and dimensionality reduction. The optimization problem
    can be articulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Minimize
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi
    mathvariant="bold">A</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:math>](img/55.png)'
  prefs: []
  type: TYPE_IMG
- en: Subject to
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msup><mml:mrow><mml:mfenced open="‖" close="‖" separators="|"><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mn>1</mml:mn></mml:math>](img/56.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can solve the optimization problem as a maximization or minimization form.
    Imposing the constraint that vector *x* must be a unit vector significantly changes
    the nature of the optimization problem. In contrast to the prior section, the
    positive semi-definiteness of matrix *A* is no longer crucial for determining
    the solution. Even when *A* is indefinite, the constraint on the norm of vector
    *x* ensures a well-defined solution, preventing the involvement of vectors with
    unbounded magnitudes or trivial solutions, such as the *0* vector. **Value singular
    decomposition** (**SVD**) is a mathematical technique that takes a rectangular
    matrix, *A*, and decomposes it into three matrices: *U*, *S*, and ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math>](img/57.png).
    Matrix *A* is defined as an *n × p* matrix. The theorem of SVD states that *A*
    can be represented as the product of three matrices: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi
    mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi
    mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi
    mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub></mml:math>](img/58.png),
    where ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi
    mathvariant="bold">U</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:math>](img/59.png),
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi
    mathvariant="bold">V</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi
    mathvariant="normal">p</mml:mi></mml:mrow></mml:msub></mml:math>](img/60.png),
    and *U* and *V* are orthogonal matrices.'
  prefs: []
  type: TYPE_NORMAL
- en: The *U* matrix’s columns are known as the left singular vectors, while the rows
    of the transpose of the *V* matrix ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math>](img/47.png)
    are the right singular vectors. The *S* matrix, with singular values, is a diagonal
    matrix of the same size as *A*. SVD decomposes the original data into a co-ordinate
    system where the defining vectors are orthonormal (both orthogonal and normal).
    SVD computation involves identifying the eigenvalues and eigenvectors of matrices
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">A</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">T</mml:mi></mml:mrow></mml:msup></mml:math>](img/62.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi
    mathvariant="bold">A</mml:mi></mml:math>](img/63.png). Matrix *V*’s columns consist
    of eigenvectors from ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi
    mathvariant="bold">A</mml:mi></mml:math>](img/64.png), and matrix *U*’s columns
    consist of eigenvectors from ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">A</mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow></mml:msup></mml:math>](img/65.png).
    Singular values in the *S* matrix are derived from the square roots of eigenvalues
    from either ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">A</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math>](img/66.png)
    or ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi
    mathvariant="bold">A</mml:mi></mml:math>](img/67.png), organized in decreasing
    order. These singular values are real numbers. If *A* is a real matrix, *U* and
    *V* will also be real.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the calculation of SVD, an example is provided. Consider a *4
    × 2* matrix. The eigenvalues of the matrix can be found by computing ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">A</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math>](img/68.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi
    mathvariant="bold">A</mml:mi></mml:math>](img/69.png) and then determining the
    eigenvectors of these matrices. *U*’s columns are formed by the eigenvectors of
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">A</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math>](img/68.png),
    and *V*’s columns are formed by the eigenvectors of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi
    mathvariant="bold">A</mml:mi></mml:math>](img/71.png). The *S* matrix comprises
    the square root of eigenvalues from either ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">A</mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math>](img/68.png)
    or ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi
    mathvariant="bold">A</mml:mi></mml:math>](img/73.png). Eigenvalues are found by
    solving the characteristic equation in the given example ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="|" close="|" separators="|"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mo>-</mml:mo><mml:mi>λ</mml:mi><mml:mi
    mathvariant="bold">I</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/74.png),
    where *W* is the matrix, *I* is the unit matrix, and *λ* is the eigenvalue. The
    eigenvectors are then found by solving the set of equations derived from the eigenvalue
    equations. The final matrices *U,* *S*, and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math>](img/57.png)
    are then obtained by combining the eigenvectors and singular values.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that the singular values are in descending order, with ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>></mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>></mml:mo><mml:mo>…</mml:mo></mml:math>](img/76.png)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now move on to basic probability for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Basic probability for machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Probability provides information about the likelihood of an event occurring.
    In this field, there are several key terms that are important to understand:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Trial or experiment**: An action that results in a certain outcome with a
    certain likelihood'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sample space**: This encompasses all potential outcomes of a given experiment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event**: This denotes a non-empty portion of the sample space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, in technical terms, probability is a measure of the likelihood of
    an event occurring when an experiment is conducted.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this very simple case, the probability of event *A* with one outcome is
    equal to the chance of event *A* divided by the chance of all possible events.
    For example, in flipping a fair coin, there are two outcomes with the same chance:
    heads and tails. The chance of having heads will be *1/(1+1) = ½*.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to calculate the probability, given an event, *A*, with *n* outcomes
    and a sample space, *S*, the probability of event A is calculated as
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mfenced><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/77.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>E</mi><mn>1</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>E</mi><mi>n</mi></msub></mrow></mrow></math>](img/78.png)
    represents the outcomes in *A*. Assuming all results of the experiment have equal
    probability, and the selection of one does not influence the selection of others
    in subsequent rounds (meaning they are statistically independent), then
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mi>A</mi></mfenced><mspace width="0.25em" /><mo>=</mo><mspace
    width="0.25em" /><mfrac><mrow><mi>N</mi><mi>o</mi><mo>.</mo><mspace width="0.25em"
    /><mi>o</mi><mi>f</mi><mspace width="0.25em" /><mi>o</mi><mi>u</mi><mi>t</mi><mi>c</mi><mi>o</mi><mi>m</mi><mi>e</mi><mi>s</mi><mspace
    width="0.25em" /><mi>i</mi><mi>n</mi><mspace width="0.25em" /><mi>A</mi></mrow><mrow><mi>N</mi><mi>o</mi><mo>.</mo><mspace
    width="0.25em" /><mi>o</mi><mi>f</mi><mspace width="0.25em" /><mi>o</mi><mi>u</mi><mi>t</mi><mi>c</mi><mi>o</mi><mi>m</mi><mi>e</mi><mi>s</mi><mspace
    width="0.25em" /><mi>i</mi><mi>n</mi><mspace width="0.25em" /><mi>S</mi></mrow></mfrac></mrow></mrow></math>](img/79.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence, the value of probability ranges from *0* to *1*, with the sample space
    embodying the complete set of potential outcomes, denoted as *P(S) =* *1*.
  prefs: []
  type: TYPE_NORMAL
- en: Statistically independent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the realm of statistics, two events are defined as independent if the occurrence
    of one event doesn’t influence the likelihood of the other event’s occurrence.
    To put it formally, events *A* and *B* are independent precisely when *P(A and
    B) = P(A)P(B)*, where *P(A)* and *P(B)* are the respective probabilities of events
    *A* and *B* happening.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this example to clarify the concept of statistical independence: imagine
    we possess two coins, one fair (an equal chance of turning up heads or tails)
    and the other biased (showing a head is more likely than a tail). If we flip the
    fair coin and the biased coin, these two events are statistically independent
    because the outcome of one coin flip doesn’t alter the probability of the other
    coin turning up heads or tails. Specifically, the likelihood of both coins showing
    heads is the product of the individual probabilities: *(1/2) * (3/4) =* *3/8*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Statistical independence is a pivotal concept in statistics and probability
    theory, frequently leveraged in machine learning to outline the connections between
    variables within a dataset. By comprehending these relationships, machine learning
    algorithms can better spot patterns and deliver more precise predictions. We will
    describe the relationship between different types of events in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Complementary event**: The complementary event to *A*, signified as *A’*,
    encompasses the probability of all potential outcomes in the sample space not
    included in A. It’s critical to understand that *A* and *A’* are statistically
    independent:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mi>A</mi><mo>′</mo><mo>)</mo><mo>=</mo><mn>1</mn><mo>−</mo><mi>P</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow></mrow></mrow></math>](img/80.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Union and intersection**: The complementary event to *A*, signified as *A’*,
    encompasses the probability of all potential outcomes in the sample space not
    included in *A*. It’s critical to understand that *A* and *A’* are statistically
    independent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mutually exclusive**: When two events have no shared outcomes, they are viewed
    as mutually exclusive. In other words, if *A* and *B* are mutually exclusive events,
    then *P(A* *∩* *B) = 0*. This conclusion can be drawn from the addition rule of
    probability, as *A* and *B* are disjointed events:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mi>A</mi><mi>U</mi><mi>B</mi><mo>)</mo><mo>=</mo><mi>P</mi><mo>(</mo><mi>A</mi><mo>)</mo><mo>+</mo><mi>P</mi><mo>(</mo><mi>B</mi><mo>)</mo></mrow></mrow></mrow></math>](img/81.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Independent**: Two events are deemed independent when the occurrence of one
    doesn’t impact the occurrence of the other. If *A* and *B* are two independent
    events, then'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mi>A</mi><mo>∩</mo><mi>B</mi><mo>)</mo><mo>=</mo><mi>P</mi><mo>(</mo><mi>A</mi><mo>)</mo><mo>∙</mo><mi>P</mi><mo>(</mo><mi>B</mi><mo>)</mo></mrow></mrow></mrow></math>](img/82.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, we are going to describe the discrete random variable, its distribution,
    and how to use it to calculate the probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Discrete random variables and their distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A discrete random variable refers to a variable that can assume a finite or
    countably infinite number of potential outcomes. Examples of such variables might
    be the count of heads resulting from a coin toss, the tally of cars crossing a
    toll booth within a specific time span, or the number of blonde-haired students
    in a classroom.
  prefs: []
  type: TYPE_NORMAL
- en: The probability distribution of a discrete random variable assigns a certain
    likelihood to each potential outcome the variable could adopt. For instance, in
    the case of a coin toss, the probability distribution assigns a 0.5 probability
    to both *0* and *1*, representing tails and heads, respectively. For the car toll
    booth scenario, the distribution could be assigning a probability of *0.1* to
    no cars passing, *0.3* to one car, *0.4* to two cars, *0.15* to three cars, and
    *0.05* to four or more cars.
  prefs: []
  type: TYPE_NORMAL
- en: A graphical representation of the probability distribution of a discrete random
    variable can be achieved through a **probability mass function** (**PMF**), which
    correlates each possible outcome of the variable to its likelihood of occurrence.
    This function is usually represented as a bar chart or histogram, with each bar
    signifying the probability of a specific value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The PMF is bound by two key principles:'
  prefs: []
  type: TYPE_NORMAL
- en: It must be non-negative across all potential values of the random variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The total sum of probabilities for all possible outcomes should equate to 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The expected value of a discrete random variable offers an insight into its
    central tendency, computed as the probability-weighted average of its possible
    outcomes. This expected value is signified as *E[X]*, with *X* representing the
    random variable.
  prefs: []
  type: TYPE_NORMAL
- en: Probability density function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **probability density function** (**PDF**) is a tool used to describe the
    distribution of a continuous random variable. It can be used to calculate the
    probability of a value falling within a specific range. In simpler terms, it helps
    determine the chances of a continuous variable, *X*, having a value within the
    interval [*a, b*], or in statistical terms,
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mi>A</mi><mo><</mo><mi>X</mi><mo><</mo><mi>B</mi><mo>)</mo></mrow></mrow></mrow></math>](img/83.png)'
  prefs: []
  type: TYPE_IMG
- en: For continuous variables, the probability of a single value occurring is always
    0, which is in contrast to discrete variables that can assign non-*0* probabilities
    to distinct values. PDFs provide a way to estimate the likelihood of a value falling
    within a given range instead of a single value.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you can use a PDF to find the chances of the next IQ score measured
    falling between *100* and *120*.
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 2.2 – Probability density function for IQ from 100–120](img/B18949_02_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Probability density function for IQ from 100–120
  prefs: []
  type: TYPE_NORMAL
- en: 'To ascertain the distribution of a discrete random variable, one can either
    provide its PMF or **cumulative distribution function** (**CDF**). For continuous
    random variables, we primarily utilize the CDF, as it is well established. However,
    the PMF is not suitable for these types of variables because *P(X=x)* equals *0*
    for all *x* in the set of real numbers, given that *X* can assume any real value
    between *a* and *b*. Therefore, we typically define the PDF instead. The PDF resembles
    the concept of mass density in physics, signifying the concentration of probability.
    Its unit is the probability per unit length. To get a grasp of the PDF, let’s
    analyze a continuous random variable, *X*, and establish the function *fX(x)*
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>f</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mo>=</mo><munder><mi>lim</mi><mrow><mo>∆</mo><mo>→</mo><msup><mn>0</mn><mo>+</mo></msup></mrow></munder><mfrac><mrow><mi>P</mi><mo>(</mo><mi>x</mi><mo><</mo><mi>X</mi><mo>≤</mo><mfenced
    open="(" close=")"><mrow><mi>x</mi><mo>+</mo><mo>∆</mo></mrow></mfenced><mo>)</mo></mrow><mo>∆</mo></mfrac></mrow></mrow></math>](img/84.png)'
  prefs: []
  type: TYPE_IMG
- en: If the limit exists.
  prefs: []
  type: TYPE_NORMAL
- en: The function ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>f</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced></mrow></mrow></math>](img/85.png)provides
    the probability density at a given point, *x*. This is equivalent to the limit
    of the ratio of the probability of the interval *(x, x + Δ]* to the length of
    the interval as that length approaches 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s contemplate a continuous random variable, *X*, possessing an absolutely
    continuous CDF, denoted as ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/86.png).
    If ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/86.png)
    is differentiable at *x*, the function ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/88.png)
    is referred to as the PDF of *X*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>f</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mspace width="0.25em" /><mo>=</mo><munder><mi>lim</mi><mrow><mo>∆</mo><mo>→</mo><msup><mn>0</mn><mo>+</mo></msup></mrow></munder><mfrac><mrow><msub><mi>F</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mrow><mi>x</mi><mo>+</mo><mo>∆</mo></mrow></mfenced><mo>−</mo><msub><mi>F</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced></mrow><mo>∆</mo></mfrac><mo>=</mo><mfrac><mrow><mi>d</mi><msub><mi>F</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced></mrow><mrow><mi>d</mi><mi>x</mi></mrow></mfrac><mo>=</mo><msub><mrow><mi>F</mi><mo>′</mo></mrow><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced></mrow></mrow></math>](img/89.png)'
  prefs: []
  type: TYPE_IMG
- en: Assuming ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/86.png)
    is differentiable at ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/91.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s consider a continuous uniform random variable, *X*, with
    uniform ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:math>](img/92.png)
    distribution. Its CDF is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mi mathvariant="normal"> </mml:mi><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfrac><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>a</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo><</mml:mo><mml:mi>x</mml:mi><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo><</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>b</mml:mi></mml:math>](img/93.png)'
  prefs: []
  type: TYPE_IMG
- en: which is *0* for any x outside the bounds.
  prefs: []
  type: TYPE_NORMAL
- en: 'By using integration, the CDF can be obtained from the PDF:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mi mathvariant="normal"> </mml:mi><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mrow><mml:munderover><mml:mo stretchy="false">∫</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi
    mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:mrow></mml:math>](img/94.png)'
  prefs: []
  type: TYPE_IMG
- en: Additionally, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>a</mml:mi><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo><</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>X</mml:mi><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>≤</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:mfenced><mml:mi mathvariant="normal"> </mml:mi><mml:mo>-</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:mi mathvariant="normal"> </mml:mi><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mrow><mml:msubsup><mml:mo stretchy="false">∫</mml:mo><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:mrow></mml:math>](img/95.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, if we integrate over the entire real line, we will get 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mrow><mml:munderover><mml:mo stretchy="false">∫</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi
    mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:mrow><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mn>1</mml:mn></mml:math>](img/96.png)'
  prefs: []
  type: TYPE_IMG
- en: Explicitly, when integrating the PDF across the entire real number line, the
    result should equal *1*. This signifies that the area beneath the PDF curve must
    equate to *1*, or *P(S) = 1*, which remains true for the uniform distribution.
    The PDF signifies the density of probability; thus, it must be non-negative and
    can exceed *1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a continuous random variable, *X*, with PDF represented as ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/97.png).
    The ensuing properties are applicable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mi mathvariant="normal"> </mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:math>](img/98.png)'
  prefs: []
  type: TYPE_IMG
- en: for all real x
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mrow><mml:munderover><mml:mo stretchy="false">∫</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi
    mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:mrow><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mn>1</mml:mn></mml:math>](img/96.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, we’ll move on to cover maximum likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: Maximum likelihood estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Maximum likelihood is a statistical approach, that is used to estimate the parameters
    of a probability distribution. The objective is to identify the parameter values
    that maximize the likelihood of observing the data, essentially determining the
    parameters most likely to have generated the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have a random sample, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>X</mi><mo>=</mo><mo>{</mo><msub><mi>X</mi><mn>1</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>X</mi><mi>n</mi></msub><mo>}</mo></mrow></mrow></mrow></math>](img/100.png),
    from a population with a probability distribution ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:mi
    mathvariant="bold-italic">θ</mml:mi><mml:mo>)</mml:mo></mml:math>](img/101.png),
    where *θ* is a vector of parameters. The likelihood of observing the sample, *X*,
    given the parameters, *θ*, is defined as the product of the individual probabilities
    of observing each data point:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>L</mi><mo>(</mo><mi
    mathvariant="bold-italic">θ</mi><mo>|</mo><mi>X</mi><mo>)</mo><mo>=</mo><mi>f</mi><mo>(</mo><mi>X</mi><mo>|</mo><mi
    mathvariant="bold-italic">θ</mi><mo>)</mo></mrow></mrow></mrow></math>](img/102.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In case of having independent and identically distributed observations, the
    likelihood function can be expressed as the product of the univariate density
    functions, each evaluated at the corresponding observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>L</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi
    mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mfenced><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mfenced><mml:mo>…</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/103.png)'
  prefs: []
  type: TYPE_IMG
- en: The **maximum likelihood estimate** (**MLE**) is the parameter vector value
    that offers the maximum value for the likelihood function across the parameter
    space.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, it’s more convenient to employ the natural logarithm of the likelihood
    function, referred to as the **log-likelihood**. The peak of the log-likelihood
    happens at the identical parameter vector value as the likelihood function’s maximum,
    and the conditions required for a maximum (or minimum) are acquired by equating
    the log-likelihood derivatives with respect to each parameter to 0\. If the log-likelihood
    is differentiable with respect to the parameters, these conditions result in a
    set of equations that can be solved numerically to derive the MLE. One common
    use case or scenario where MLE significantly impacts ML model performance is in
    linear regression. When building a linear regression model, MLE is often used
    to estimate the coefficients that define the relationship between input features
    and the target variable. MLE helps find the values for the coefficients that maximize
    the likelihood of observing the given data under the assumed linear regression
    model, improving the accuracy of the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The MLEs of the parameters, *θ*, are the values that maximize the likelihood
    function. In other words, the MLEs are the values of *θ* that make the observed
    data, *X*, most probable.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find the MLEs, we typically take the natural logarithm of the likelihood
    function, as it is often easier to work with the logarithm of a product than with
    the product itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">ln</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">ln</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">ln</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mo>…</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">ln</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/104.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The MLEs are determined by equating the partial derivatives of the log-likelihood
    function with respect to each parameter to *0* and then solving these equations
    for the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mo>∂</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">ln</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced><mml:mo>/</mml:mo><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/105.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mo>∂</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">ln</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced><mml:mo>/</mml:mo><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/106.png)'
  prefs: []
  type: TYPE_IMG
- en: '...'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mo>∂</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">ln</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced><mml:mo>/</mml:mo><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/107.png)'
  prefs: []
  type: TYPE_IMG
- en: where *k* is the number of parameters in *θ*. The goal of a maximum likelihood
    estimator is to find *θ* such that
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi
    mathvariant="bold-italic">θ</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><munder><mi>max</mi><mi
    mathvariant="bold-italic">θ</mi></munder><mrow><mrow><mi>L</mi><mo>(</mo><mi mathvariant="bold-italic">θ</mi><mo>|</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></mrow></mrow></math>](img/108.png)'
  prefs: []
  type: TYPE_IMG
- en: Once the MLEs have been found, they can be used to make predictions about the
    population based on the sample data. Maximum likelihood is widely used in many
    fields, including psychology, economics, engineering, and biology. It serves as
    a potent tool for comprehending the connections among variables and for predicting
    outcomes based on observed data. For example, building a word predictor using
    maximum likelihood estimation.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we introduce the problem of word autocompletion, also known as **word
    prediction**, which is a feature in where an application predicts the next word
    a user is typing. The aim of word prediction is to save time and make typing easier
    by predicting what the user is likely to type next based on their previous inputs
    and other contextual factors. Word prediction can be found in various forms in
    many applications, including search engines, text editors, and mobile device keyboards,
    and is designed to save time and increase the accuracy of inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Given a group of words that the user typed, how would we suggest the next word?
  prefs: []
  type: TYPE_NORMAL
- en: If the words were **The United States of**, then it would be trivial to assume
    that the next word would be **America**. However, what about finding the next
    word for **How are**? One could suggest several next words.
  prefs: []
  type: TYPE_NORMAL
- en: There usually isn’t just one clear next word. Thus, we’d want to suggest the
    most likely word or perhaps even the most likely words. In that case, we would
    be interested in suggesting a probabilistic representation of the possible next
    words and picking the next word as the one that is most probable.
  prefs: []
  type: TYPE_NORMAL
- en: The maximum likelihood estimator provides us with that precise capability. It
    can tell us which word is most probable given the previous words that the user
    typed.
  prefs: []
  type: TYPE_NORMAL
- en: In order to calculate the MLE, we need to calculate the probability function
    of all word combinations. We can do that by processing large texts and counting
    how many times each combination of words exists.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider reviewing a large cohort of text that has the following occurrences:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **“****you”** | **“****they”** | **“****those”** | **“****the”** | **Any**
    **other word** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| “how are …” | 16 | 14 | 0 | 100 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| not “how are…” | 200 | 100 | 300 | 1,000 | 30,000 |'
  prefs: []
  type: TYPE_TB
- en: Table 2.1 – Sample of n-grams occurrences in a document
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, there are 16 occurrences in the text where the sequence “how
    are you” appears. There are 140 sequences that have a length of three that start
    with the words “how are.” That is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mn>16</mn><mo>+</mo><mn>14</mn><mo>+</mo><mn>0</mn><mo>+</mo><mn>100</mn><mo>+</mo><mn>10</mn><mo>=</mo><mn>140</mn></mrow></mrow></math>](img/109.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are 216 sequences that have a length of three and that end with the word
    “you”. That is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mn>16</mn><mo>+</mo><mn>200</mn><mo>=</mo><mn>216</mn></mrow></mrow></math>](img/110.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let’s suggest a formula for the most likely next word.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the common maximum likelihood estimation for the probablistic variable
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math>](img/111.png),
    the formula would be to find a value for ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math>](img/112.png)
    which maximizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>P</mi><mfenced
    open="(" close="|"><msub><mi>W</mi><mn>3</mn></msub></mfenced><msub><mi>W</mi><mn>1</mn></msub><mo>,</mo><msub><mi>W</mi><mn>2</mn></msub><mo>)</mo></mrow></mrow></mrow></math>](img/113.png)'
  prefs: []
  type: TYPE_IMG
- en: However, this common formula has a few characteristics that wouldn’t be advantagous
    to our application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the next formula which has specific advantages that are necessary
    for our use case. It is the maximum likelihood formula for parametric estimation,
    meaning, estimating deterministic parameters. It suggests finding a value for
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math>](img/112.png)
    which maximizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>P</mi><mfenced
    open="(" close="|"><mrow><msub><mi>W</mi><mn>1</mn></msub><mo>,</mo><msub><mi>W</mi><mn>2</mn></msub></mrow></mfenced><msub><mi>W</mi><mn>3</mn></msub><mo>)</mo></mrow></mrow></mrow></math>](img/115.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math>](img/112.png)
    is by no means a deterministic parameter, however, this formula suits our use
    case as it reduces common word bias emphasizing contextual fit, and adjusts for
    word specificity, thus enhancing the relevance of our predictions. We will elaborate
    more on these traits in the conclusion of this exercise.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s enhance this formula so to make it easier to calculate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>P</mi><mfenced
    open="(" close="|"><mrow><msub><mi>W</mi><mn>1</mn></msub><mo>,</mo><msub><mi>W</mi><mn>2</mn></msub></mrow></mfenced><msub><mi>W</mi><mn>3</mn></msub><mo>)</mo><mo>=</mo><mfrac><mrow><mi>P</mi><mo>(</mo><msub><mi>W</mi><mn>1</mn></msub><mo>,</mo><msub><mi>W</mi><mn>2</mn></msub><mo>,</mo><msub><mi>W</mi><mn>3</mn></msub><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><msub><mi>W</mi><mn>3</mn></msub><mo>)</mo></mrow></mfrac></mrow></mrow></mrow></math>](img/117.png)'
  prefs: []
  type: TYPE_IMG
- en: In our case,![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/118.png)
    is “how” and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/119.png)
    is “are.”
  prefs: []
  type: TYPE_NORMAL
- en: 'There are five candidates for the next word; let’s calculate the probability
    for each of them:'
  prefs: []
  type: TYPE_NORMAL
- en: P(“how”, “are” | “you”) = 16 / (200 + 16) = 16/216 = 2/27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(“how”, “are” | “they”) = 14 / (100 +14) = 14/114 = 7/57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(“how”, “are” | “those”) = 0 / 300 = 0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(“how”, “are” | “the”) = 100 / (1000 + 100) = 100/1100 = 1/11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(“how”, “are” | any other word) = 10 / (30,000 + 10) = 10/30010 = 1/3001
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Out of all the options, the highest value of probability is 7/57 and it is achieved
    when “they” is the next word.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the intuition behind this maximum likelihood estimator is having the
    suggested next word make the words that the user typed most likely. One could
    wonder, why not take the word that is most probable given the first two words,
    meaning, the orginal maximum likelihood formula for probabilistic variables? From
    the table, we see that given the words “how are,” the most frequent third word
    is “the,” with a probability of 100/140\. However, this approach wouldn’t take
    into account the fact that the word “the” is extremely prevalent altogether, as
    it is most frequently used in the text in general. Thus, its high frequency isn’t
    due to its relationship to the first two words; it is because it is simply a very
    common word in general. The maximum likelyhood formula we chose takes that into
    account.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bayesian estimation is a statistical approach that involves updating our beliefs
    or probabilities about a quantity of interest based on new data. The term “Bayesian”
    refers to Thomas Bayes, an 18th-century statistician who first developed the concept
    of Bayesian probability.
  prefs: []
  type: TYPE_NORMAL
- en: In Bayesian estimation, we start with prior beliefs about the quantity of interest,
    which are expressed as a probability distribution. These prior beliefs are updated
    as we collect new data. The updated beliefs are represented as a posterior distribution.
    The Bayesian framework provides a systematic way of updating prior beliefs with
    new data, taking into account the degree of uncertainty in both the prior beliefs
    and the new data.
  prefs: []
  type: TYPE_NORMAL
- en: The posterior distribution is calculated using Bayes’ theorem, which is the
    fundamental equation of Bayesian estimation. Bayes’ theorem states that
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi mathvariant="normal">Θ</mi><mo>|</mo><mi>X</mi></mrow></mfenced><mo>=</mo><mfrac><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>X</mi><mo>|</mo><mi mathvariant="normal">Θ</mi></mrow></mfenced><mi>P</mi><mfenced
    open="(" close=")"><mi mathvariant="normal">Θ</mi></mfenced></mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mi>X</mi></mfenced></mrow></mfrac></mrow></mrow></math>](img/120.png)'
  prefs: []
  type: TYPE_IMG
- en: where *Θ* is the quantity of interest, *X* is the new data, *P(Θ|X)* is the
    posterior distribution, *P(X|Θ)* is the likelihood of the data given the parameter
    value, *P(Θ)* is the prior distribution, and *P(X)* is the marginal likelihood
    or evidence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The marginal likelihood is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∫</mml:mo><mml:mrow><mml:mi> </mml:mi></mml:mrow><mml:mrow><mml:mi> </mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>∙</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mi
    mathvariant="normal">Θ</mml:mi></mml:math>](img/121.png)'
  prefs: []
  type: TYPE_IMG
- en: where the integral is taken over the entire space of *Θ*. The marginal likelihood
    is often used as a normalizing constant, ensuring that the posterior distribution
    integrates to *1*.
  prefs: []
  type: TYPE_NORMAL
- en: In Bayesian estimation, the choice of prior distribution is important, as it
    reflects our beliefs about the quantity of interest before collecting any data.
    The prior distribution can be chosen based on prior knowledge or previous studies.
    If no prior knowledge is available, a non-informative prior can be used, such
    as a uniform distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Once the posterior distribution is calculated, it can be used to make predictions
    about the quantity of interest. As an example, the posterior distribution’s mean
    can serve as a point estimate, whereas the posterior distribution itself can be
    employed to establish credible intervals. These intervals represent the probable
    range within which the true value of the target quantity resides.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter was about linear algebra and probability for ML, and it covers
    the fundamental mathematical concepts that are essential to understanding many
    machine learning algorithms. The chapter began with a review of linear algebra,
    covering topics such as matrix multiplication, determinants, eigenvectors, and
    eigenvalues. It then moved on to discuss probability theory, introducing the basic
    concepts of random variables and probability distributions. We also covered key
    concepts in statistical inference, such as maximum likelihood estimation and Bayesian
    inference.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover the fundamentals of machine learning for
    NLP, including topics such as data exploration, feature engineering, selection
    methods, and model training and validation.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please find the additional reading content as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Householder reflection matrix**: A Householder reflection matrix, or Householder
    matrix, is a type of linear transformation utilized in numerical linear algebra
    due to its computational effectiveness and numerical stability. This matrix is
    used to perform reflections of a given vector about a plane or hyperplane, transforming
    the vector so that it only has non-*0* components in one specific dimension. The
    **Householder matrix** (**H**) is defined by'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">H</mml:mi><mml:mi mathvariant="bold"> </mml:mi><mml:mo>=</mml:mo><mml:mi
    mathvariant="bold"> </mml:mi><mml:mi mathvariant="bold">I</mml:mi><mml:mi mathvariant="bold"> </mml:mi><mml:mo>-</mml:mo><mml:mi
    mathvariant="bold"> </mml:mi><mml:mn>2</mml:mn><mml:mi mathvariant="bold"> </mml:mi><mml:mi
    mathvariant="bold">u</mml:mi><mml:mi mathvariant="bold"> </mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">u</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math>](img/122.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *I* is the identity matrix, and *u* is a unit vector defining the reflection
    plane.
  prefs: []
  type: TYPE_NORMAL
- en: The main purpose of Householder transformations is to perform QR factorization
    and to reduce matrices to a tridiagonal or Hessenberg form. The properties of
    being symmetric and orthogonal make the Householder matrix computationally efficient
    and numerically stable.
  prefs: []
  type: TYPE_NORMAL
- en: '**Diagonalizable**: A matrix is said to be diagonalizable if it can be written
    in the form ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi
    mathvariant="bold">D</mi><mo>=</mo><msup><mi mathvariant="bold">P</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi
    mathvariant="bold">A</mi><mi mathvariant="bold">P</mi></mrow></mrow></math>](img/123.png)![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi mathvariant="bold">D</mi><mo>=</mo><msup><mi
    mathvariant="bold">P</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi mathvariant="bold">A</mi><mi
    mathvariant="bold">P</mi></mrow></mrow></math>](img/124.png), where *A* is the
    original matrix, *D* is a diagonal matrix, and *P* is a matrix for which the columns
    are the eigenvectors of *A*. Diagonalization simplifies many calculations in linear
    algebra, as computations with diagonal matrices are often more straightforward.
    For a matrix to be diagonalizable, it must have enough distinct eigenvectors to
    form a basis for its space, which is usually the case when all of its eigenvalues
    are distinct.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Invertible**: An invertible matrix, also known as a non-singular matrix or
    a non-degenerate matrix, is a square matrix that has an inverse. If a matrix,
    *A*, is invertible, there exists another matrix, often denoted as ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math>](img/125.png),
    such that when they are multiplied together, they yield the identity matrix. In
    other words, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi
    mathvariant="bold">A</mi><msup><mi mathvariant="bold">A</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>=</mo><msup><mi
    mathvariant="bold">A</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi mathvariant="bold">A</mi><mo>=</mo><mi
    mathvariant="bold">I</mi></mrow></mrow></math>](img/126.png), where *I* is the
    identity matrix. The identity matrix is a special square matrix with *1*s on its
    main diagonal and *0*s everywhere else. The existence of an inverse heavily depends
    on the determinant of the matrix—a matrix is invertible if and only if its determinant
    is not 0\. Invertible matrices are crucial in numerous areas of math, including
    solving systems of linear equations, matrix factorization, and many applications
    in engineering and physics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gaussian elimination method**: Gaussian elimination is a fundamental algorithm
    in linear algebra for solving systems of linear equations. It accomplishes this
    by transforming the system to an equivalent one in which the equations are simpler
    to solve. This method uses a sequence of operations to modify the system of equations,
    with the objective of creating a row-echelon or reduced row-echelon form. Here’s
    a simplified step-by-step process of Gaussian elimination: First, swap the rows
    to move any rows with a leading coefficient (the first non-*0* number from the
    left, also called the pivot) so as to have *1* at the top. Then, multiply or divide
    any rows by a scalar to create a leading coefficient of *1* if not already present.
    Finally, add or subtract rows to create *0*s below and above the pivot. Once the
    matrix is in row-echelon form (all *0* rows are at the bottom, and each leading
    coefficient is to the right of the leading coefficient of the row above it), we
    can use back substitution to find the variables. If we further simplify the matrix
    to a reduced row-echelon form (each leading coefficient is the only non-*0* entry
    in its column), the solutions can be read directly from the matrix. Gaussian elimination
    can also be used to find the rank of a matrix, calculate the determinant, and
    carry out matrix inversion if the system is square and has a unique solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trace**: The trace of a square matrix is the sum of its diagonal elements.
    It’s denoted as *Tr(A)* or *trace(A)*, where *A* is a square matrix. For example,
    if'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">A</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>a</mml:mi></mml:mtd><mml:mtd><mml:mi>b</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>c</mml:mi></mml:mtd><mml:mtd><mml:mi>d</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math>](img/38.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '*Tr(A) = a +* *d*.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alter O, Brown PO, Botstein D. (2000) *Singular value decomposition for genome-wide
    expression data processing and modeling*. Proc Natl Acad Sci U S A, 97, 10101-6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Golub, G.H., and Van Loan, C.F. (1989) *Matrix Computations*, 2nd ed. (Baltimore:
    Johns Hopkins University Press).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Greenberg, M. (2001) *Differential equations & Linear algebra* (Upper Saddle
    River, N.J. : Prentice Hall).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Strang, G. (1998) *Introduction to linear algebra* (Wellesley, MA : Wellesley-Cambridge
    Press).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lax, Peter D. *Linear algebra and its applications*. Vol. 78\. John Wiley &
    Sons, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dangeti, Pratap. *Statistics for machine learning*. Packt Publishing Ltd, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DasGupta, Anirban. *Probability for statistics and machine learning: fundamentals
    and advanced topics*. New York: Springer, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
