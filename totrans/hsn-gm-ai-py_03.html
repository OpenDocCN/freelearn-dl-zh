<html><head></head><body>
        

                            
                    <h1 class="header-title">Dynamic Programming and the Bellman Equation</h1>
                
            
            
                
<p><strong>Dynamic programming</strong> (<strong>DP</strong>) was the second major thread to influence modern <strong>reinforcement learning</strong> (<strong>RL</strong>) after trial-and-error learning. In this chapter, we will look at the foundations of DP and explore how they influenced the field of RL. We will also look at how the Bellman equation and the concept of optimality have interwoven with RL. From there, we will look at policy and value iteration methods to solve a class of problems well suited for DP. Finally, we will look at how to use the concepts we have learned in this chapter to teach an agent to play the FrozenLake environment from OpenAI Gym.</p>
<p>Here are the main topics we will cover in this chapter:</p>
<ul>
<li>Introducing DP</li>
<li>Understanding the Bellman equation</li>
<li>Building policy iteration </li>
<li>Building value iteration</li>
<li>Playing with policy versus value iteration</li>
</ul>
<p>For this chapter, we look at how to solve a finite <strong>Markov decision process</strong> (<strong>MDP</strong>) with DP using the Bellman equation of optimality. This chapter is meant as a history lesson and background to DP and Bellman. If you are already quite familiar with DP, then you may want to bypass this chapter since we will just explore entry-level DP, covering just enough background to see how it has influenced and bettered RL.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Introducing DP</h1>
                
            
            
                
<p>DP was developed by Richard E. Bellman in the 1950s as a way to optimize and solve complex decision problems. The method was first applied to engineering control problems but has since found uses in all disciplines requiring the analytical modeling of problems and subproblems. In effect, all DP is about is solving subproblems and then finding relationships to connect those to solve bigger problems. It does all of this by first applying the Bellman optimality equation and then solving it.</p>
<p>Before we get to solving a finite MDP with DP, we will want to understand, in a little more detail, what it is we are talking about. Let's look at a simple example of the difference between normal recursion and DP in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Regular programming versus DP</h1>
                
            
            
                
<p>We will do a comparison by first solving a problem using regular methods and then with DP. Along the way, we will identify key elements that make our solution a DP one. What most experienced programmers find is that they likely have done DP in some capacity, so don't be surprised if this all sounds really familiar. Let's open up the <kbd>Chapter_2_1.py</kbd> example and follow the exercise:</p>
<ol>
<li>The code is an example of finding the <em>n</em><sup>th</sup> number in the Fibonacci sequence using recursion and is shown as follows:</li>
</ol>
<pre style="padding-left: 60px">def Fibonacci(n): <br/>    if n&lt;0: <br/>        print("Outside bounds")<br/>    elif n==1:<br/>        return 0 # n==1, returns 0  <br/>    elif n==2: <br/>        return 1 # n==2, returns 1<br/>    else: <br/>        return Fibonacci(n-1)+Fibonacci(n-2) <br/>print(Fibonacci(9))</pre>
<ol start="2">
<li>Recall that we can resolve the <em>n</em><sup>th</sup> element in the Fibonacci sequence by summing the two previous digits in the sequence. We consider that when <kbd>n == 1</kbd>, the value is <kbd>0</kbd>, and when <kbd>n == 2</kbd>, the returned value is <kbd>1</kbd>. Hence, the third element in the sequence would be the sum of <kbd>Fibonacci(1)</kbd> and <kbd>Fibonacci(2)</kbd> and would return a value of <kbd>1</kbd>. This reflects in the code shown in the following line:</li>
</ol>
<pre style="padding-left: 60px">return Fibonacci(n-1)+Fibonacci(n-2)</pre>
<ol start="3">
<li>Hence, the solution for finding the fourth element using the linear programming version of recursion is shown in the following diagram:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-469 image-border" src="img/73bd0b7b-9d48-4a87-bf49-535ef14ad31e.png" style="width:35.58em;height:20.00em;"/></p>
<p>Solving the fourth element of the Fibonacci sequence</p>
<p>We can note the following: </p>
<ul>
<li>The preceding diagram shows each recursive call to the <kbd>Fibonacci</kbd> function required to calculate previous elements. Notice how this method requires the solution to solve or call the <kbd>Fibonacci(2)</kbd> function twice. Of course, that additional call in this example is trivial but these additional calls can quickly add up.</li>
<li>Run the code as you normally would and see the printed result for the ninth element.</li>
</ul>
<p>To appreciate how inefficient recursion can be, we have modified our previous example and saved it as <kbd>Chapter_2_2.py</kbd>. Open up that example now and follow the next exercise:</p>
<ol>
<li>The modified code, with the extra line highlighted, is shown for reference:</li>
</ol>
<pre style="padding-left: 60px">def Fibonacci(n):     <br/>    if n&lt;0: <br/>        print("Outside bounds")<br/>    elif n==1: <br/>        return 0 # n==1, returns 0 <br/>    elif n==2: <br/>        return 1 # n==2, returns 1<br/>    else:   <br/>        <strong>print("Solving for {}".format(n))</strong><br/>        return Fibonacci(n-1)+Fibonacci(n-2)<br/>print(Fibonacci(9))</pre>
<ol start="2">
<li>All we are doing is printing out when we need to calculate two more sequences to return the sum. Run the code and notice the output, as shown in the following screenshot:</li>
</ol>
<p style="color: black" class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-855 image-border" src="img/3261baec-4253-4b54-aa74-1cc31741d5c0.png" style="width:33.75em;height:38.75em;"/></p>
<p>Output from example Chapter_2_1.py </p>
<ol start="3">
<li>Notice, for just calculating the ninth element of the Fibonacci sequence using recursion, the number of times just <kbd>Fibonacci(3)</kbd> is called. </li>
</ol>
<p>Now, the solution works and, as they say, it is pretty and clean. In fact, as a programmer, you may have been taught to worship this style of coding at one time. However, we can clearly see how inefficient this method is to scale and that is where DP comes in, which we will discuss in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Enter DP and memoization</h1>
                
            
            
                
<p>A key concept in DP is to break down larger problems into smaller subproblems, then solve said smaller problems and store the result. The fancy name for this activity is called <strong>memoization</strong> and the best way to showcase how this works is with an example. Open up <kbd>Chapter_2_3.py</kbd> and follow the exercise:</p>
<ol>
<li>For reference, the entire block of code from our previous example has been modified as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>fibSequence = [0,1]</strong><br/><br/>def Fibonacci(n):     <br/>    if n&lt;0: <br/>        print("Outside bounds")<br/>   <strong> elif n&lt;= len(fibSequence): </strong><br/><strong>        return fibSequence[n-1]</strong> <br/>    else:   <br/>        print("Solving for {}".format(n))<br/>        <strong>fibN = Fibonacci(n-1) + Fibonacci(n-2)</strong><br/><strong>        fibSequence.append(fibN)</strong><br/><strong>        return fibN</strong> <br/>  <br/>print(Fibonacci(9))</pre>
<ol start="2">
<li>Again, the highlighted lines denote code changes, but, in this case, we will go over each code change in more detail starting with the first change, as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>fibSequence = [0,1]</strong></pre>
<ol start="3">
<li>This new line creates a new starting Fibonacci list with our two base numbers, <kbd>0</kbd> and <kbd>1</kbd>. We will still use a recursive function, but now we also store the results of every unique calculation for later:</li>
</ol>
<pre style="padding-left: 60px"><strong>elif n&lt;= len(fibSequence): <br/></strong><strong>        return fibSequence[n-1]</strong></pre>
<ol start="4">
<li>The next code change is where the algorithm returns a previously stored value, as in <kbd>0</kbd> or <kbd>1</kbd>, or as calculated and then stored in the <kbd>fibSequence</kbd> list:</li>
</ol>
<pre style="padding-left: 60px"><strong>fibN = Fibonacci(n-1) + Fibonacci(n-2)</strong><strong>        fibSequence.append(fibN)<br/></strong><strong>return fibN</strong> </pre>
<ol start="5">
<li>The last group of code changes now saves the recursive calculation, adding the new value to the entire sequence. This now requires the algorithm to only calculate each <em>n</em><sup>th</sup> value of the sequence once.</li>
<li>Run the code as you normally would and look at the results shown in the following screenshot:</li>
</ol>
<div><img class="aligncenter size-full wp-image-471 image-border" src="img/a270b391-9ecb-4969-a97d-2e3ae9f7833d.png" style="width:31.75em;height:13.08em;"/><br/>
<br/>
Example output from example Chapter_2_3.py</div>
<p>Notice how we are now only calculating the numbers in the sequence and not repeating any calculations. Clearly, this method is far superior to the linear programming example that we looked at earlier, and the code actually doesn't look bad either. Now, as we said, if this type of solution seems obvious, then you probably already understand more about DP than you realize. </p>
<p>We will only cover a very simple introduction to DP as it relates to RL in this book. As you can see, DP is a powerful technique that can benefit any developer who is serious about optimizing code. </p>
<p>In the next section, we look further at the work of Bellman and the equation that was named after him.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding the Bellman equation</h1>
                
            
            
                
<p>Bellman worked on solving finite MDP with DP, and it was during these efforts he derived his famed equation. The beauty behind this equation—and more abstractly, the concept, in general—is that it describes a method of optimizing the value or quality of a state. In other words, it describes how we can determine the optimal value/quality for being in a given state given the action and choices of successive states. Before breaking down the equation itself, let's first reconsider the finite MDP in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Unraveling the finite MDP</h1>
                
            
            
                
<p>Consider the finite MDP we developed in <a href="5553d896-c079-4404-a41b-c25293c745bb.xhtml"/><a href="5553d896-c079-4404-a41b-c25293c745bb.xhtml">Chapter 1</a>, <em>Understanding Rewards Learning</em>, that described your morning routine. Don't to worry if you didn't complete that exercise previously as we will consider a more specific example, as follows:</p>
<div><img class="aligncenter size-full wp-image-472 image-border" src="img/aa23c01a-8b06-4551-899e-7a906e68aea9.png" style="width:43.92em;height:27.25em;"/><br/>
<br/>
MDP for waking up and getting on the bus</div>
<p>The preceding finite MDP describes a possible routine for someone waking up and getting ready to get on a bus to go to school or work. In this MDP, we define a beginning state (<strong>BEGIN</strong>) and an ending state, that is, getting on the bus (<strong>END</strong>). The <strong>R =</strong> denotes the reward allotted when moving to that state and the number closest to the end of the action line denotes the probability of taking that action. We can denote the optimum path through this finite MDP, as shown in the following diagram:</p>
<div><img class="aligncenter size-full wp-image-473 image-border" src="img/9710a44e-0116-493e-9cb3-32311140ee1b.png" style="width:50.00em;height:8.42em;"/><br/>
<br/>
The optimum solution to the MDP</div>
<p>Mathematically, we can describe the optimum result, therefore, as the sum of all rewards traversed through the environment to obtain the total reward. More formally, we can also write this mathematically like so:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/076725cd-a610-4a0b-a86d-6601c01a0ee5.png" style="width:17.83em;height:1.33em;"/></p>
<p class="CDPAlignLeft CDPAlign">However, and this is where Bellman comes in, we can't consider all rewards as equal. Without exploring the math in more exhaustive detail, the Bellman equation introduced the concept that future rewards should be discounted. This is also quite intuitive when you think about it. The experience or effect we feel from future actions is weakened depending on how far in the future we need to decide. This is the same concept we apply with the Bellman equation. Hence, we can now apply a discounting factor (gamma) to the previous equation and show the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/6b648417-d51e-4680-a5be-3499d76264c7.png" style="width:28.42em;height:3.58em;"/></p>
<p>Gamma (<img class="fm-editor-equation" src="img/9906fcb0-b342-4e56-809a-bbd16b5926cc.png" style="width:0.75em;height:1.08em;"/>), shown in the equation, represents a future rewards discount factor. The value can be from <kbd>0.0</kbd> to <kbd>1.0</kbd>. If the value is <kbd>1.0</kbd>, then we consider no discount of future rewards, whereas a value of <kbd>0.1</kbd> would heavily discount future rewards. In most RL problems, we keep this number fairly high and well above <kbd>0.9</kbd>. This leads us to the next section where we discuss how the Bellman equation can optimize a problem.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The Bellman optimality equation</h1>
                
            
            
                
<p>The Bellman equation shows us that you can solve any MDP by first finding the optimal policy that allows an agent to traverse that MDP. Recall that a policy defines the decisions for each action that will guide an agent through an MDP. Ideally, what we want to find is the optimal policy: a policy that can maximize the value for each state and determine which states to traverse for maximum reward. When we combine this with other concepts and apply more math wizardry and then combine it with the Bellman optimality equation, we get the following optimal policy equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/02370093-e53e-42ff-951f-2c3bd8fedd75.png" style="width:26.75em;height:2.08em;"/></p>
<p class="CDPAlignLeft CDPAlign">That strange term at the very beginning (<sub><img class="fm-editor-equation" src="img/fb95776c-d075-47d2-bdd1-fc3aa828e7b3.png" style="width:3.08em;height:1.17em;"/></sub>) is a way of describing a function that maximizes the rewards given a set of states and actions considering that we discount future rewards by a factor called gamma. Notice how we also use <img class="fm-editor-equation" src="img/cd5e7dcf-31c0-49dd-b6ce-9a3d6dee4650.png" style="width:0.83em;height:0.83em;"/> to denote the policy equation but we often think of this as a quality and may refer to this as <em>q</em> or <em>Q</em>. If you think back to our previous peek at the Q-learning equation, then now you can clearly see how the rewards' discount factor, gamma, comes into play. </p>
<p>In the next section, we will look at methods to solve an MDP using DP and a method of policy iteration given our understanding of the Bellman optimality principle and resulting policy equation.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Building policy iteration </h1>
                
            
            
                
<p>For us to determine the best policy, we first need a method to evaluate the given policy for a state. We can use a method of evaluating the policy by searching through all of the states of an MDP and further evaluating all actions. This will provide us with a value function for the given state that we can then use to perform successive updates of a new value function iteratively. Mathematically, we can then use the previous Bellman optimality equation and derive a new update to a state value function, as shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/0c9818aa-db72-4266-8766-866221321deb.png" style="width:18.00em;height:1.50em;"/></p>
<p>In the preceding equation, the <sub><img class="fm-editor-equation" src="img/d19906d9-7bab-41e1-98ee-e739b3c8d6ff.png" style="width:0.75em;height:0.92em;"/></sub> symbol represents an expectation and denotes the expected state value update to a new value function. Inside this expectation, we can see this dependent on the returned reward plus the previous discounted value for the next state given an already chosen action. That means that our algorithm will iterate over every state and action evaluating a new state value using the preceding update equation. This process is called backing up or planning, and it is helpful for us to visualize how this algorithm works using backup diagrams. The following is an example of the backup diagrams for action value and state value backups:</p>
<div><img class="aligncenter size-full wp-image-474 image-border" src="img/817a2069-e71b-4177-9bc6-e1c9ecd6f6c5.png" style="width:31.67em;height:14.75em;"/><br/>
<br/>
Backup diagrams for action value and state value backups</div>
<p>Diagram <strong>(a)</strong> or <sub><img class="fm-editor-equation" src="img/1feda0df-b44f-4614-9d34-598e29d447b8.png" style="width:1.83em;height:1.25em;"/></sub>is the part of the backup or evaluation that tries each action and hence provides us with action values. The second part of the evaluation comes from the update and is shown in diagram <strong>(b)</strong> for <sub><img class="fm-editor-equation" src="img/134a97e6-b70d-4a6e-a633-5f315102b150.png" style="width:1.75em;height:1.42em;"/></sub>. Recall, the update evaluates the forward states by evaluating each of the state actions. The diagrams represent the point of evaluation with a filled-in solid circle. Notice how the action value only focuses on the forward actions while the state value focuses on the action value for each forward state. Of course, it will be helpful to look at how this comes together in code. However, before we get to that, we want to do some housekeeping in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Installing OpenAI Gym</h1>
                
            
            
                
<p>To help to encourage research and development in RL, the OpenAI group provides an open source platform for RL training called Gym. Gym, provided by OpenAI, comes with plenty of sample test environments that we can venture through while we work through this book. Also, other RL developers have developed other environments using the same standard interface Gym uses. Hence, by learning to use Gym, we will also be able to explore other cutting-edge RL environments later in this book.</p>
<p>The installation for Gym is quite simple, but, at the same time, we want to avoid any small mistakes that may cause you frustration later. Therefore, it is best to use the following instructions to set up and install an RL environment for development.</p>
<p>It is highly recommended that you use Anaconda for Python development with this book. Anaconda is a free open source cross-platform tool that can significantly increase your ease of development. Please stick with Anaconda unless you consider yourself an experienced Python developer. Google <kbd>python anaconda</kbd> to download and install it.</p>
<p>Follow the exercise to set up and install a Python environment with Gym:</p>
<ol>
<li>Open a new Anaconda Prompt or Python shell. Do this as an admin or be sure to execute the commands as an admin if required.</li>
<li>From the command line, run the following:</li>
</ol>
<pre style="padding-left: 60px"><strong>conda create -n chapter2 python=3.6</strong></pre>
<ol start="3">
<li>This will create a new virtual environment for your development. A virtual environment allows you to isolate dependencies and control your versioning. If you are not using Anaconda, you can use the Python virtual environment to create a new environment. You should also notice that we are forcing the environment to use Python 3.6. Again, this makes sure we know what version of Python we are using.</li>
<li>After the installation, we activate the environment with the following:</li>
</ol>
<pre style="padding-left: 60px"><strong>activate chapter2</strong></pre>
<ol start="5">
<li>Next, we install Gym with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>pip install gym</strong></pre>
<ol start="6">
<li>Gym will install several dependencies along with the various sample environments we will train on later. </li>
</ol>
<p>Before we get too far ahead though, let's now test our Gym installation with code in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Testing Gym</h1>
                
            
            
                
<p>In the next exercise, we will write code to test Gym and an environment called FrozenLake, which also happens to be our test environment for this chapter. Open the <kbd>Chapter_2_4.py</kbd> code example and follow the exercise:</p>
<ol>
<li>For reference, the code is shown as follows:</li>
</ol>
<pre style="padding-left: 60px">from os import system, name<br/>import time<br/>import gym<br/>import numpy as np<br/><br/>env = gym.make('FrozenLake-v0')<br/>env.reset()<br/><br/>def clear():<br/>    if name == 'nt': <br/>        _ = system('cls')    <br/>    else: <br/>        _ = system('clear')<br/><br/>for _ in range(1000):<br/>    clear()<br/>    env.render()<br/>    time.sleep(.5)<br/>    env.step(env.action_space.sample()) # take a random action<br/>env.close()</pre>
<ol start="2">
<li>At the top, we have the imports to load the <kbd>system</kbd> modules as well as <kbd>gym</kbd>, <kbd>time</kbd>, and <kbd>numpy</kbd>. <kbd>numpy</kbd> is a helper library we use to construct tensors. Tensors are a math/programming concept that can describe single values or multidimensional arrays of numbers. </li>
<li>Next, we build and reset the environment with the following code:</li>
</ol>
<pre style="padding-left: 60px">env = gym.make('FrozenLake-v0')<br/>env.reset()</pre>
<ol start="4">
<li>After that, we have a <kbd>clear</kbd> function, which we use to clear rendering that is not critical to this example. The code should be self-explanatory as well.</li>
</ol>
<ol start="5">
<li>This brings us to the <kbd>for</kbd> loop and where all of the actions, so to speak, happen. The line of most importance is shown as follows:</li>
</ol>
<pre style="padding-left: 60px">env.step(env.action_space.sample())</pre>
<ol start="6">
<li>The <kbd>env</kbd> variable represents the environment, and, in the line, we are letting the algorithm take a random action every step or iteration. In this example, the agent learns nothing and just moves at random, for now.</li>
<li>Run the code as you normally would and pay attention to the output. An example of the output screen is shown in the following:</li>
</ol>
<div><img class="aligncenter size-full wp-image-475 image-border" src="img/6fa318c8-8f0f-46ac-95f7-a85a95896e68.png" style="width:15.17em;height:12.25em;"/><br/>
<br/>
Example render from the FrozenLake environment</div>
<p>Since the algorithm/agent moves randomly, it is quite likely to hit a hole, denoted by <kbd>H</kbd> and just stay there. For reference, the legend for FrozenLake is given here:</p>
<ul>
<li><strong>S</strong> = start: This is where the agent starts when reset is called.</li>
<li><strong>F</strong> = frozen: This allows the agent to move across this area.</li>
<li><strong>H</strong> = hole: This is a hole in the ice; if the agent moves here, it falls in.</li>
<li><strong>G</strong> = goal: This is the goal the agent wants to reach, and, when it does, it receives a reward of 1.0.</li>
</ul>
<p>Now that we have Gym set up, we can move to evaluate the policy in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Policy evaluation</h1>
                
            
            
                
<p>Unlike the trial-and-error learning, you have already been introduced to DP methods that work as a form of static learning or what we may call planning. Planning is an appropriate definition here since the algorithm evaluates the entire MDP and hence all states and actions beforehand. Hence, these methods require full knowledge of the environment including all finite states and actions. While this works for known finite environments such as the one we are playing within this chapter, these methods are not substantial enough for real-world physical problems. We will, of course, solve real-world problems later in this book. For now, though, let's look at how to evaluate a policy from the previous update equations in code. Open <kbd>Chapter_2_5.py</kbd> and follow the exercise:</p>
<ol>
<li>For reference, the entire block of code, <kbd>Chapter_2_5.py</kbd>, is shown as follows:</li>
</ol>
<pre style="padding-left: 60px">from os import system, name<br/>import time<br/>import gym<br/>import numpy as np<br/>env = gym.make('FrozenLake-v0')<br/>env.reset()<br/><br/>def clear():<br/>    if name == 'nt': <br/>        _ = system('cls')    <br/>    else: <br/>        _ = system('clear')<br/><br/>def act(V, env, gamma, policy, state, v):<br/>    for action, action_prob in enumerate(policy[state]):                <br/>        for state_prob, next_state, reward, end in env.P[state][action]:                                        <br/>            v += action_prob * state_prob * (reward + gamma * V[next_state])                    <br/>            V[state] = v<br/>            <br/>def eval_policy(policy, env, gamma=1.0, theta=1e-9, terms=1e9):     <br/>    V = np.zeros(env.nS)  <br/>    delta = 0<br/>    for i in range(int(terms)): <br/>        for state in range(env.nS):            <br/>            act(V, env, gamma, policy, state, v=0.0)         <br/>        clear()<br/>        print(V)<br/>        time.sleep(1) <br/>        v = np.sum(V)<br/>        if v - delta &lt; theta:<br/>            return V<br/>        else:<br/>            delta = v<br/>    return V<br/><br/>policy = np.ones([env.env.nS, env.env.nA]) / env.env.nA<br/>V = eval_policy(policy, env.env)<br/><br/>print(policy, V)</pre>
<ol start="2">
<li>At the beginning of the code, we perform the same initial steps as our test example. We load <kbd>import</kbd> statements and initialize and load the environment, then define the <kbd>clear</kbd> function.</li>
<li>Next, move to the very end of the code and notice how we are initializing the policy using <kbd>numpy as np</kbd> to fill a tensor of the size of the environment, <kbd>state</kbd> x <kbd>action</kbd>. We then divide the tensor by the number of actions in a state—<kbd>4</kbd>, in this case. This gives us a distributed probability of <kbd>0.25</kbd> per action. Remember that the combined action probability in a Markov property needs to sum up to <kbd>1.0</kbd> or 100%.</li>
<li>Now, move up the <kbd>eval_policy</kbd> function and focus on the double loop, as shown in the following code block:</li>
</ol>
<pre style="padding-left: 60px">for i in range(int(terms)):<br/>  for state in range(env.nS):<br/>    act(V, env, gamma, policy, state, v=0.0)<br/>  clear()<br/>  print(V)<br/>  time.sleep(1)<br/>  v = np.sum(V)<br/>  if v - delta &lt; theta:<br/>    return V<br/>  else:<br/>    delta = v<br/>return V</pre>
<ol start="5">
<li>The first <kbd>for</kbd> loop loops on the number of terms or iterations before termination. We set a limit here to prevent endless looping. In the inner loop, all of the states in the environment are iterated through and acted on using the <kbd>act</kbd> function. After that, we use our previous render code to show the updated values. At the end of the first <kbd>for</kbd> loop, we check whether the calculated total change in the <kbd>v</kbd> value is less than a particular threshold, <kbd>theta</kbd>. If the change in value is less than the threshold, the function returns the calculate value function, <kbd>V</kbd>. </li>
<li>At the core of the algorithm is the <kbd>act</kbd> function and where the update equation operates; the inside of this function is shown as follows:</li>
</ol>
<pre style="padding-left: 60px">for action, action_prob in enumerate(policy[state]):   <br/>  for state_prob, next_state, reward, end<br/> in env.P[state][action]:   <br/>    <strong>v += action_prob * state_prob * (reward + gamma * V[next_state]) #update </strong><br/>    V[state] = v</pre>
<ol start="7">
<li>The first <kbd>for</kbd> loop iterates through all of the actions in the policy for the given state. Recall that we start by initializing the policy to <kbd>0.25</kbd> for every <kbd>action</kbd> function, <kbd>action_prob = 0.25</kbd>. Then, we loop through every transition from the state and action and apply the update. The update is shown in the highlighted equation. Finally, the <kbd>value</kbd> function, <kbd>V</kbd>, for the current state is updated to <kbd>v</kbd>.</li>
<li>Run the code and observe the output. Notice how the <kbd>value</kbd> function is continually updated. At the end of the run, you should see something similar to the following screenshot:</li>
</ol>
<div><img class="aligncenter size-full wp-image-476 image-border" src="img/a8ef6f06-a5b9-492d-9aa7-b3aa4267b4aa.png" style="width:47.00em;height:33.83em;"/></div>
<p>Running example Chapter_2_5.py</p>
<p>If it seems off that the policy is not updated, that is actually okay, for now. The important part here is to see how we update the <kbd>value</kbd> function. In the next section, we will look at how we can improve the policy.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Policy improvement</h1>
                
            
            
                
<p>With policy evaluation under our belt, it is time to move on to improving the policy by looking ahead. Recall we do this by looking at one state ahead of the current state and then evaluating all possible actions. Let's look at how this works in code. Open up the <kbd>Chapter_2_6.py</kbd> example and follow the exercise:</p>
<ol>
<li>For brevity, the following code excerpt from <kbd>Chapter_2_6.py</kbd> shows just the new sections of code that were added to that last example:</li>
</ol>
<pre style="padding-left: 60px">def evaluate(V, action_values, env, gamma, state):<br/>    for action in range(env.nA):<br/>        for prob, next_state, reward, terminated in env.P[state][action]:<br/>            action_values[action] += prob * (reward + gamma * V[next_state])<br/>    return action_values<br/><br/>def lookahead(env, state, V, gamma):<br/>    action_values = np.zeros(env.nA)<br/>    return evaluate(V, action_values, env, gamma, state)<br/><br/>def improve_policy(env, gamma=1.0, terms=1e9):    <br/>    policy = np.ones([env.nS, env.nA]) / env.nA<br/>    evals = 1<br/>    for i in range(int(terms)):<br/>        stable = True       <br/>        V = eval_policy(policy, env, gamma=gamma)<br/>        for state in range(env.nS):<br/>            current_action = np.argmax(policy[state])<br/>            action_value = lookahead(env, state, V, gamma)<br/>            best_action = np.argmax(action_value)<br/>            if current_action != best_action:<br/>                stable = False               <br/>                policy[state] = np.eye(env.nA)[best_action]<br/>            evals += 1                <br/>            if stable:<br/>                return policy, V<br/><br/><strong>#replaced bottom code from previous sample with</strong><br/><strong>policy, V = improve_policy(env.env) </strong><br/><strong>print(policy, V)</strong></pre>
<ol start="2">
<li>Added to the last example are three new functions: <kbd>improve_policy</kbd>, <kbd>lookahead</kbd>, and <kbd>evaluate</kbd>. <kbd>improve_policy</kbd> uses a limited loop to loop through the states in the current environment; before looping through each state, it calls <kbd>eval_policy</kbd> to update the <kbd>value</kbd> function by passing in the current <kbd>policy</kbd>, <kbd>environment</kbd>, and <kbd>gamma</kbd> parameters (discount factor). Then, it calls the <kbd>lookahead</kbd> function, which internally calls an <kbd>evaluate</kbd> function that updates the action values for the state. <kbd>evaluate</kbd> is a modified version of the <kbd>act</kbd> function.</li>
<li>While both functions, <kbd>eval_policy</kbd> and <kbd>improve_policy</kbd>, use limited terms for loops to prevent endless looping, they still use very large limits; in the example, the default is <kbd>1e09</kbd>. Therefore, we still want to determine a condition to hopefully stop the loop much earlier than the term's limit. In policy evaluation, we controlled this by observing the change or delta in the value function. In policy improvement, we now look to improve the actual policy and, to do this, we assume a greedy policy. In other words, we want to improve our policy to always pick the highest value action, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">action_value = lookahead(env, state, V, gamma)best_action = np.argmax(action_value)<br/>if current_action != best_action:                <br/>  stable = False   <br/>  policy[state] = np.eye(env.nA)[best_action]<br/>evals += 1 <br/><br/>if stable:<br/>  return policy, V</pre>
<ol start="4">
<li>The preceding block of code first uses the <kbd>numpy</kbd> function—<kbd>np.argmax</kbd> on the list of <kbd>action_value</kbd> returns from the <kbd>lookahead</kbd> function. This returns the max or <kbd>best_action</kbd>, or in other words, the greedy action. We then consider whether <kbd>current_action</kbd> is not equal to <kbd>best_action</kbd>; if it is not, then we consider the policy is not stable by setting <kbd>stable</kbd> to <kbd>false</kbd>. Since the action is not the best, we also update <kbd>policy</kbd> with the identity tensor using <kbd>np.eye</kbd> for the shape defined. This step simply assigns the policy a value of <kbd>1.0</kbd> for the best/greedy actions and <kbd>0.0</kbd> for all others.</li>
<li>At the end of the code, you can see that we now just call <kbd>improve_policy</kbd> and print the results of the policy and value functions.</li>
</ol>
<ol start="6">
<li>Run the code as you normally would and observe the output, as shown in the following screenshot:</li>
</ol>
<div><img class="aligncenter size-full wp-image-477 image-border" src="img/9e42541a-6250-41d1-93bd-c867138337d3.png" style="width:63.58em;height:37.08em;"/></div>
<p>Example output for Chapter_2_6.py</p>
<p>This sample will take a while longer to run and you should see the <kbd>value</kbd> function improve as the sample runs. When the sample completes, it will print the value function and policy. You can now see how the policy clearly indicates the best action for each state with a <kbd>1.0</kbd> value. The reason some states still have the <kbd>0.25</kbd> value for all actions is that the algorithm sees no need to evaluate or improve the policy in those states. They were likely states that were holes or were outside the optimal path.</p>
<p>Policy evaluation and improvement is one method we can use for planning with DP, but, in the next section, we will look at a second method called value iteration.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Building value iteration</h1>
                
            
            
                
<p>Iterating over values may seem a step back to what we referred to as policy iteration in the last section, but it is actually more of a side step or companion method. In value iteration, we loop through all states in the entire MDP looking for the best value for each state, and when we find that, we stop or break. However, we don't stop there and we continue by looking ahead of all states and then assuming a deterministic probability of 100% for the best action. This yields a new policy that may perform better than the previous policy iteration demonstration. The differences between both methods are subtle and best understood with a code example. Open up <kbd>Chapter_2_7.py</kbd> and follow the next exercise:</p>
<ol>
<li>This code example builds on the previous example. New code changes in example <kbd>Chapter_2_7.py</kbd> are shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">def value_iteration(env, gamma=1.0, theta=1e-9, terms=1e9):<br/>    V = np.zeros(env.nS)<br/>    for i in range(int(terms)):<br/>        delta = 0<br/>        for state in range(env.nS):<br/>            action_value = lookahead(env, state, V, gamma)<br/>            best_action_value = np.max(action_value)<br/>            delta = max(delta, np.abs(V[state] - best_action_value))<br/>            V[state] = best_action_value             <br/>        if delta &lt; theta: break<br/>    policy = np.zeros([env.nS, env.nA])<br/>    for state in range(env.nS):<br/>        action_value = lookahead(env, state, V, gamma)<br/>        best_action = np.argmax(action_value)<br/>        policy[state, best_action] = 1.0<br/>    return policy, V<br/><br/>#policy, V = improve_policy(env.env) <br/>#print(policy, V)<br/><br/>policy, V = value_iteration(env.env)<br/>print(policy, V)</pre>
<ol start="2">
<li>The bulk of this code is quite similar to code we already reviewed in the previous examples, but there are some subtle differences worth noting.</li>
</ol>
<ol start="3">
<li>First, this time, inside the limited terms loop, we iterated through the states and performed a straight lookahead with the <kbd>lookahead</kbd> function. The details of this code are as follows:</li>
</ol>
<pre style="padding-left: 60px">for state in range(env.nS):<br/> action_value = lookahead(env, state, V, gamma)<br/> best_action_value = np.max(action_value)<br/> delta = max(delta, np.abs(V[state] - best_action_value))<br/> V[state] = best_action_value <br/></pre>
<ol start="4">
<li>The slight difference in the preceding code versus policy evaluation and improvement is that, this time, we do an immediate lookahead and iterate over action values and then update the <kbd>value</kbd> function based on the best value. In this block of code, we also calculate a new <kbd>delta</kbd> value or amount of change from the previous best action value:</li>
</ol>
<pre style="padding-left: 60px">if delta &lt; theta: break</pre>
<ol start="5">
<li>After the loop, there is an <kbd>if</kbd> statement that checks whether the calculated <kbd>delta</kbd> value or the amount of action value change is below a particular threshold, <kbd>theta</kbd>. If <kbd>delta</kbd> is sufficiently small, we break the limited terms loop:</li>
</ol>
<pre style="padding-left: 60px">policy = np.zeros([env.nS, env.nA])<br/>  for state in range(env.nS):<br/>    action_value = lookahead(env, state, V, gamma)<br/>    best_action = np.argmax(action_value)<br/>    policy[state, best_action] = 1.0<br/>return policy, V</pre>
<ol start="6">
<li>From there, we initialize <kbd>policy</kbd> this time to all zeros with the <kbd>numpy np.zeros</kbd> function. Then, we loop through all of the states again and perform another one-step lookahead using the <kbd>lookahead</kbd> function. This returns a list of action values, which we determine the max index of, <kbd>best_action</kbd>. We then set <kbd>policy</kbd> to <kbd>1.0</kbd>; we assume the best action is always the one chosen for the state. Finally, we return the new policy and the <kbd>value</kbd> function, <kbd>V</kbd>.</li>
</ol>
<ol start="7">
<li>Run the code as you have and examine the output as shown in the following screenshot:</li>
</ol>
<div><img class="aligncenter size-full wp-image-478 image-border" src="img/ba298c25-3bc4-4d1e-99d5-7c319438e071.png" style="width:49.83em;height:27.92em;"/><br/>
<br/>
Example output from Chapter_2_8.py</div>
<p>This time, we don't do any policy iteration or improvement so the sample runs faster. You should also note how the policy has been updated for all states. Recall, in policy iteration, only the relevant states the algorithm/agent could move through were evaluated.</p>
<p>In the next section, we turn an actual agent loose on the environment using the policy calculated with policy iteration and improvement versus value iteration in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Playing with policy versus value iteration</h1>
                
            
            
                
<p>Policy and value iteration methods are quite similar and looked at as companion methods. As such, to evaluate which method to use, we often need to apply both methods to the problem in question. In the next exercise, we will evaluate both policy and value iteration methods side by side in the FrozenLake environment:</p>
<ol>
<li>Open the <kbd>Chapter_2_8.py</kbd> example. This example builds on the previous code examples, so we will only show the new additional code:</li>
</ol>
<pre style="padding-left: 60px">def play(env, episodes, policy):<br/>    wins = 0<br/>    total_reward = 0<br/>    for episode in range(episodes):<br/>        term = False<br/>        state = env.reset()<br/>        while not term:<br/>            action = np.argmax(policy[state])<br/>            next_state, reward, term, info = env.step(action)<br/>            total_reward += reward<br/>            state = next_state<br/>            if term and reward == 1.0:<br/>                wins += 1<br/>    average_reward = total_reward / episodes<br/>    return wins, total_reward, average_reward<br/><br/>policy, V = improve_policy(env.env)<br/>print(policy, V)<br/><br/>wins, total, avg = play(env.env, 1000, policy)<br/>print(wins)<br/><br/>policy, V = value_iteration(env.env)<br/>print(policy, V)<br/><br/>wins, total, avg = play(env.env, 1000, policy)<br/>print(wins)</pre>
<ol start="2">
<li>The additional code consists of a new function, <kbd>play</kbd>, and different test code at the end. At the code at the end, we first calculate a policy using the <kbd>improve_policy</kbd> function, which performs policy iteration:</li>
</ol>
<pre style="padding-left: 60px">wins, total, avg = play(env.env, 1000, policy)print(wins)</pre>
<ol start="3">
<li>Next, we evaluate the number of wins for <kbd>policy</kbd> by using the <kbd>play</kbd> function. After this, we print the number of wins.</li>
<li>Then, we evaluate a new policy using value iteration, again using the <kbd>play</kbd> function to evaluate the number of wins and printing the result:</li>
</ol>
<pre style="padding-left: 60px">for episode in range(episodes):<br/>  term = False<br/>  state = env.reset()<br/>  while not term:<br/>    action = np.argmax(policy[state])<br/>    <strong>next_state, reward, term, info = env.step(action)</strong><br/>    total_reward += reward<br/>    state = next_state<br/>    if term and reward == 1.0:<br/>      wins += 1<br/>average_reward = total_reward / episodes    return wins, total_reward, average_reward</pre>
<ol start="5">
<li>Inside the <kbd>play</kbd> function, we loop through the number of episodes. Each episode is considered to be one attempt by the agent to move from the start to the goal. In this example, the termination of an episode happens when the agent encounters a hole or the goal. If it reaches the goal, it receives a reward of <kbd>1.0</kbd>. Most of the code is self-explanatory, aside from the moment an agent conducts an action and is shown again as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>next_state, reward, term, info = env.step(action)</strong></pre>
<ol start="6">
<li>Recall, in our Gym environment test, we just randomly stepped the agent around. Now, in the preceding code, we execute a specific action set by the policy. The return from taking the action is <kbd>next_state</kbd>, <kbd>reward</kbd> (if any), <kbd>term</kbd> or termination, and an <kbd>info</kbd> variable. This line of code entirely controls the agent and allows it to move and interact with the environment:</li>
</ol>
<pre style="padding-left: 60px">total_reward += reward<br/>state = next_state<br/>if term and reward == 1.0:<br/>  wins += 1</pre>
<ol start="7">
<li>After the agent takes a step, we then update <kbd>total_reward</kbd> and <kbd>state</kbd>. Then, we test to see whether the agent won, the environment was terminated, and the returned reward was <kbd>1.0</kbd>. Otherwise, the agent continues. The agent may also terminate an episode from falling into a hole.</li>
</ol>
<ol start="8">
<li>Run the code as you normally would and examine the output, as shown in the following screenshot:</li>
</ol>
<div><img class="aligncenter size-full wp-image-479 image-border" src="img/049500f1-b224-4bf4-b2e0-adc3c96c9331.png" style="width:55.00em;height:54.58em;"/><br/>
<br/>
Example output from example Chapter_2_8.py</div>
<p>Notice the difference in results. This is the difference in policy iteration with value iteration over the FrozenLake problem. You can play with and adjust the parameters of <kbd>theta</kbd> and <kbd>gamma</kbd> to see whether you get better results for either method. Again, much like RL itself, you will need to perform a little trial and error on your own to determine the best DP method to use.</p>
<p>In the next section, we look at some additional exercises that can help you further your understanding of the material.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Exercises</h1>
                
            
            
                
<p>Completing the exercises in this section is entirely optional, but, hopefully, you can start to appreciate that we, as reinforcement learners ourselves, learn best by doing. Do your best and attempt to complete at least 2-3 exercises from the following:</p>
<ol>
<li>Consider other problems you could use DP with? How would you break the problem up into subproblems and calculate each subproblem?</li>
<li>Code up another example that compares a problem programmed linearly versus dynamically. Use the example from <em>Exercise 1</em>. The code examples, <kbd>Chapter_2_2.py</kbd> and <kbd>Chapter_2_3.py</kbd>, are good examples of side-by-side comparisons.</li>
<li>Look through the OpenAI documentation and explore other RL environments.</li>
<li>Create, render, and explore other RL environments from Gym using the sample test code from <kbd>Chapter_2_4.py</kbd>.</li>
<li>Explain the process/algorithm of evaluating and improving a policy using DP.</li>
<li>Explain the difference between policy iteration and value iteration.</li>
<li>Open the <kbd>Chapter_2_5.py</kbd> policy iteration example and adjust the <kbd>theta</kbd> and <kbd>gamma</kbd> parameters. What effect do these have on learning rates and values?</li>
<li>Open the <kbd>Chapter_2_6.py</kbd> policy improvement example and adjust the <kbd>theta</kbd> and <kbd>gamma</kbd> parameters. What effect do these have on learning rates and values?</li>
<li>Open the <kbd>Chapter_2_7.py</kbd> value iteration example and adjust the <kbd>theta</kbd> and <kbd>gamma</kbd> parameters. What effect do these have on learning rates and values?</li>
<li>Complete all of the policy and value iteration examples using the <kbd>FrozenLake 8x8</kbd> environment. This a much larger version of the lake problem. Now, which method performs better?</li>
</ol>
<p>Use these exercises to strengthen your knowledge of the material we just covered in this chapter. In this next section, we will summarize what we covered in this chapter.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we took an in-depth look at DP and the Bellman equation. The Bellman equation with DP has influenced RL significantly by introducing the concept of future rewards and optimization. We covered the contribution of Bellman in this chapter by first taking a deep look at DP and how to solve a problem dynamically. Then, we advanced to understanding the Bellman optimality equation and how it can be used to account for future rewards as well as determine expected state and action values using iterative methods. In particular, we focused on the implementation in Python of policy iteration and improvement. Then, from there, we looked at value iteration. Finally, we concluded this chapter by setting up an agent test against the FrozenLake environment using a policy generated by both policy and value iteration. For this chapter, we looked at a specific class of problems well suited to DP that also helped us to derive other concepts in RL such as discounted rewards.</p>
<p>In the next chapter, we continue with this theme by looking at Monte Carlo methods.</p>


            

            
        
    </body></html>