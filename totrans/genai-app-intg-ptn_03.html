<html><head></head><body>
<div><div><h1 class="chapterNumber">3</h1>
<h1 class="chapterTitle" id="_idParaDest-35">Designing Patterns for Interacting with Generative AI</h1>
<p class="normal">In the previous chapters, we<a id="_idIndexMarker105"/> explored the world of <strong class="keyWord">generative AI</strong> (<strong class="keyWord">GenAI</strong>), including the types of use cases and applications that can be developed using this exciting new technology. We also discussed evaluating the business value that GenAI can potentially bring to the table for different organizations and industries.</p>
<p class="normal">In this chapter, we will dive deeper into the practical considerations around integrating GenAI capabilities into real-world applications. A key question that arises is, where and how should we incorporate GenAI models within an application’s architecture and workflow? There are a few different approaches we can take, depending on factors like the application type, existing infrastructure, team skills, and more.</p>
<figure class="mediaobject"><img alt="" height="300" src="img/B22175_03_01.png" width="347"/></figure>
<p class="packt_figref">Figure 3.1: Image generated by AI to depict AI integration</p>
<p class="normal">We will start by examining how user requests or inputs can serve as entry points for generating content or predictions using AI models in near-real time. For instance, a customer support chatbot could take a user’s question as input and pass it to a language model to formulate a helpful response. Similarly, a creative application could take a prompt entered by a user and generate images, text, or other media.</p>
<p class="normal">Next, we’ll explore exit points – the points where applications return AI-generated outputs back to users or incorporate them into business workflows. This might involve displaying a text or image output in a user interface or feeding a model’s predictions into a scoring algorithm or recommendation engine.</p>
<p class="normal">Additionally, we’ll highlight the importance of monitoring and logging when integrating AI. Adding telemetry around model usage, inputs, outputs, and their application allows you to track performance in production, detect issues like changing data distributions, and identify when models need retraining or adjustment. Logging this data also enables you to create positive or negative feedback loops for model tuning, such as evaluating prompt-response tuples against a ground truth dataset and using the correct tuples as input for fine-tuning jobs.</p>
<p class="normal">By understanding these integration approaches and their practical applications, you’ll be well equipped to seamlessly incorporate the unique capabilities of GenAI into your applications, delivering maximum business value while being aware of the technology’s limitations.</p>
<p class="normal">In a nutshell, we will cover the following topics in this chapter:</p>
<ul>
<li class="bulletList">We will define a 5-component framework that can easily be applied when building GenAI applications</li>
<li class="bulletList">Identifying strategic entry points for AI models to enhance real-time user interactions across different application types, from customer service chatbots to creative tools</li>
<li class="bulletList">Defining effective prompt pre-processing to maximize inference request performance</li>
<li class="bulletList">Defining effective inference result post-processing and presentation for surfacing AI-generated outputs to users or incorporating them into business workflows, ensuring a seamless experience</li>
<li class="bulletList">Implementing monitoring and logging mechanisms to track model performance, inputs, and outputs, enabling continuous improvement cycles and data-driven model tuning</li>
</ul>
<h1 class="heading-1" id="_idParaDest-36">Defining an integration framework</h1>
<p class="normal">Let’s define a framework to approach<a id="_idIndexMarker106"/> the integration paths through integration components. This five-component framework – <strong class="keyWord">Entry Point</strong>, <strong class="keyWord">Prompt Pre-Processing</strong>, <strong class="keyWord">Inference</strong>, <strong class="keyWord">Result Post-Processing</strong>, and <strong class="keyWord">Logging</strong> – provides a template for systematically addressing the AI integration process underlying many applications. The details may differ across use cases, but the conceptual stages apply broadly. Within this framework, we will establish a main boundary for integration depending on how users interact with the models: interactive for real-time output generation, or batch-oriented for bulk content creation and processing.</p>
<figure class="mediaobject"><img alt="" height="136" src="img/B22175_03_02.png" width="803"/></figure>
<p class="packt_figref">Figure 3.2: GenAI application integration framework</p>
<p class="normal">Integrating GenAI models can follow these two distinct paths – interactive user-driven approaches versus batch processing workflows. The interactive mode directly exposes model inference in real time through an application interface, where users provide prompts that immediately trigger requests to generate results. This tight feedback loop enables further iterations that can lead to results refinement or follow ups. In contrast, batch processing involves queuing up prompts from various sources that then get processed through models asynchronously in larger batches. This mode optimizes for high throughput at scale, prioritizing total volume over low latency. Each integration mode offers unique tradeoffs aligned to the priorities of interactivity, scale, efficiency, and specific use case requirements.</p>
<p class="normal">The key distinction lies in the tradeoff between low latency and tight user interactivity versus higher overall throughput and efficiency. Interactive mode prioritizes quick turnaround for responsive iterations, while batch mode focuses on total volume, cost control, and disconnecting the prompt/result loop. Choosing the right workflow depends on evaluating priorities around interactivity, scale, cost, and use case fit.</p>
<p class="normal">Both interactive and batch processing<a id="_idIndexMarker107"/> have strengths across different scenarios. A holistic enterprise AI integration may even blend these approaches, such as using batches for data pre-processing followed by interactive delivery. Thoughtfully aligning the right mode to the use case defines whether users directly steer models in real time or harness their capabilities through an asynchronous accumulation process.</p>
<h1 class="heading-1" id="_idParaDest-37">Entry point</h1>
<p class="normal">The entry point <a id="_idIndexMarker108"/>is where an application<a id="_idIndexMarker109"/> accepts a user’s input that will be processed by GenAI models. This might be:</p>
<ul>
<li class="bulletList">A text box where a user enters a prompt: Interactive</li>
<li class="bulletList">An uploaded image that will be processed: Interactive or batch</li>
<li class="bulletList">A voice recording that will be transcribed and analyzed: Batch</li>
</ul>
<p class="normal">The entry point acts as the front door for users to access the power of GenAI within an application. As such, the entry point modality should align closely with the input types supported by the models being leveraged. If the models only process text prompts, then a text-based entry field is appropriate. For image generation models, the entry could be an interface supporting image uploads. Multi-modal models may warrant options for both text and images.</p>
<p class="normal">Beyond matching supported input types, the entry point UX should aim to make providing prompts fast, intuitive, and even delightful for users. Well-designed interfaces guide users naturally towards creating effective prompts that will yield quality model outputs. Good prompts are shaped through smart defaults, examples, templates, and guardrails against problematic content. Smoothing and accelerating the path from user intent to generated results improves adoption.</p>
<p class="normal">Additionally, the appropriate<a id="_idIndexMarker110"/> entry point complexity depends<a id="_idIndexMarker111"/> on the user and use case. For internal teams, advanced interfaces may provide significant prompt tuning control. Consumer-facing apps may favor simplicity and precision. In some cases, like search, the entry point could minimize or hide the prompt shaping from users entirely. Removing friction while clarifying paths to value is key.</p>
<h1 class="heading-1" id="_idParaDest-38">Prompt pre-processing</h1>
<p class="normal">Before handing off prompts<a id="_idIndexMarker112"/> to generative<a id="_idIndexMarker113"/> models, pre-processing can make inputs more usable and potentially improve the quality of the outputs.</p>
<p class="normal">When thinking about prompt pre-processing, there are two key dimensions that are affected – security and model usability.</p>
<p class="normal">On the security aspect, this is the first opportunity to evaluate the prompts and verify that they align with your responsible AI guardrails. Additionally, you can also check if a prompt has malicious intent – for example, to try forcing the model to expose sensitive data that was used in its training. Putting in place content filters, blocklists, and other defenses at this pre-processing stage is important for ensuring security.</p>
<p class="normal">The second dimension is related to optimizing model usability. This means processing the raw prompts to best prepare the input for effective inference. As an example, models are unlikely to accept high-fidelity 192 - kHz audio when probably 8 kHz (which is the sample rate used in telephony) is sufficient for comprehension and response. Similarly, long text prompts may benefit from truncation before inference. The goal is to shape the data for ideal performance.</p>
<p class="normal">Additionally, regardless of the input modality, the pre-processing stage is where you can generate embeddings that may be used to leverage<a id="_idIndexMarker114"/> vector search optimizations like <strong class="keyWord">Retrieval Augmented Generation</strong> (<strong class="keyWord">RAG</strong>). Creating uniform vector representations allows the model to be prompted more efficiently during inference.</p>
<p class="normal">The prompt pre-processing phase<a id="_idIndexMarker115"/> provides critical opportunities<a id="_idIndexMarker116"/> to validate security, optimize usability, and set up embeddings that together ready the raw input for the best possible GenAI performance at inference time.</p>
<h1 class="heading-1" id="_idParaDest-39">Inference</h1>
<p class="normal">The inference step<a id="_idIndexMarker117"/> is where the magic<a id="_idIndexMarker118"/> happens – user inputs are actually run through the AI models, either running locally or in the cloud, to generate outputs. Seamlessly orchestrating this prediction stage requires some key technical capabilities.</p>
<p class="normal">First, the application needs to interface directly with the API endpoints exposed by the generative models to submit prompts and receive back predictions. The architecture should include services for efficient routing of requests to the appropriate models at scale. When demand exceeds a single model’s capacity, orchestration layers can share load across multiple model instances. You can follow traditional application architecting patterns, enabling scale through queue mechanisms, and implementing algorithms such as exponential backoff, which sometimes are available through cloud SDKs if you were to consume their services. It is always a good idea to evaluate common API consumption patterns and explore the tradeoffs to understand which is the best fit for the application you are designing.</p>
<p class="normal">On the infrastructure side, if you decide to host your models, hosting requirements must provide low-latency access to models for responsive predictions along with sufficient throughput capacity. Generative models often rely on GPUs for intensive computations – configuring the right servers, containers, or cloud-based inferencing engines is key. Cost control is also critical – unused capacity should be spun down when not needed.</p>
<p class="normal">An alternative to hosting your models is to leverage cloud services, where you can, for example, consume the models directly from your cloud provider. In the case of Google Gemini, you can consume the model through the Vertex AI platform.</p>
<p class="normal">Lastly, redundancy plays an important role such that no single point of failure can disrupt the availability of mission-critical AI predictions. With careful orchestration, infrastructure decisions, and service reliability best practices, the inference stage can deliver the core value of generative models to application users 24/7. Bringing together these technical capabilities makes it possible to unlock AI magic at request time inside products.</p>
<p class="normal">The inference stage<a id="_idIndexMarker119"/> brings together<a id="_idIndexMarker120"/> many moving parts but when done well, the complexity is hidden behind simple prompt -&gt; prediction interfaces that users trust will just work. Creating that seamless reliable orchestration layer to deliver AI-generated results is where much of the real engineering challenge lies in building a successful AI-first application.</p>
<h1 class="heading-1" id="_idParaDest-40">Results post-processing</h1>
<p class="normal">Before presenting<a id="_idIndexMarker121"/> the raw outputs<a id="_idIndexMarker122"/> from GenAI models directly to end users, additional post-processing is often essential to refine and polish results. There are a few common techniques to improve quality, as we will see now.</p>
<p class="normal"><strong class="keyWord">Filtering inappropriate content</strong> – Despite making the best efforts during training, models will sometimes return outputs that are biased, incorrect, or offensive. Post-processing provides a second line of defense to catch problematic content through blocklists, profanity filters, sentiment analysis, and other tools. Flagged results can be discarded or rerouted to human review. This filtration ensures only high-quality content reaches users.</p>
<p class="normal">Models such as Google Gemini allow you to define<a id="_idIndexMarker123"/> a set of safety settings to set thresholds during generation, allowing you to stop generating content if those thresholds are exceeded. Additionally, it provides a set of safety ratings with your results, allowing you to determine the threshold to filter results after generation.</p>
<p class="normal">The following is the full code for the example; please note that some tags present are used as part of the Form feature of Google Colab (see <a href="https://colab.research.google.com/notebooks/forms.ipynb">https://colab.research.google.com/notebooks/forms.ipynb</a>):</p>
<pre class="programlisting code"><code class="hljs-code">import vertexai
from google.cloud import aiplatform
from vertexai.generative_models import GenerativeModel, Part
#Authenticate with Google Colab
from google.colab import auth as google_auth
google_auth.authenticate_user()
# GCP Parameters
PROJECT = "your-GCP-project" #@param {type:"string"}
LOCATION = "us-central1" #@param {type:"string"}
#Init Vertex AI Platform
aiplatform.init(project=PROJECT, location=LOCATION)
def generate(prompt):
  model = GenerativeModel("gemini-pro")
  response = model.generate_content(
    [prompt],
    generation_config={
        "max_output_tokens": 2048,
        "temperature": 0.1,
        "top_p": 0,
        "top_k": 5,
    },
    safety_settings={
          generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
          generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
          generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
          generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    },
    stream=False,
  )
  return response
result = generate("Tell me a joke about cars")
print(result)
</code></pre>
<p class="normal">Let’s dive deep<a id="_idIndexMarker124"/> into the generated Python<a id="_idIndexMarker125"/> example provided by the Google Vertex AI console:</p>
<pre class="programlisting code"><code class="hljs-code">def generate(prompt):
  model = GenerativeModel("gemini-pro")
  response = model.generate_content(
    [prompt],
    generation_config={
        "max_output_tokens": 2048,
        "temperature": 0.1,
        "top_p": 1
    },
    safety_settings={
        generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
        generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
        generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
        generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    },
    stream=False,
  )
  return response
</code></pre>
<p class="normal">In this case, you will see that the safety settings were defined as:</p>
<pre class="programlisting code"><code class="hljs-code">safety_settings={
    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
}
</code></pre>
<p class="normal">From the Google Gemini documentation available at <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes">https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes</a>, we can see the full list<a id="_idIndexMarker126"/> of attributes available:</p>
<table class="table-container" id="table001-1">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Safety Attribute</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Definition</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Hate Speech</p>
</td>
<td class="table-cell">
<p class="normal">Negative or harmful comments targeting identity and/or protected attributes.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Harassment</p>
</td>
<td class="table-cell">
<p class="normal">Malicious, intimidating, bullying, or abusive comments targeting another individual.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Sexually Explicit</p>
</td>
<td class="table-cell">
<p class="normal">Contains references to sexual acts or other lewd content.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Dangerous Content</p>
</td>
<td class="table-cell">
<p class="normal">Promotes or enables access to harmful goods, services, and activities.</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 3.1: Google Gemini Safety Attributes as of Feb 2024</p>
<p class="normal">Along with the<a id="_idIndexMarker127"/> safety attributes, you will also obtain<a id="_idIndexMarker128"/> a probability:</p>
<table class="table-container" id="table002">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Probability</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Description</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">NEGLIGIBLE</code></p>
</td>
<td class="table-cell">
<p class="normal">Content has a negligible probability of being unsafe.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">LOW</code></p>
</td>
<td class="table-cell">
<p class="normal">Content has a low probability of being unsafe.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">MEDIUM</code></p>
</td>
<td class="table-cell">
<p class="normal">Content has a medium probability of being unsafe.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">HIGH</code></p>
</td>
<td class="table-cell">
<p class="normal">Content has a high probability of being unsafe.</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 3.2: Google Gemini Safety Attributes probabilities as of Feb 2024</p>
<p class="normal">Let’s now test the sample prompt <code class="inlineCode">Tell me a joke about cars</code>. You are going to submit the prompt using the <a id="_idIndexMarker129"/>sample function<a id="_idIndexMarker130"/> provided previously to the gemini-pro model on Google Vertex AI:</p>
<pre class="programlisting code"><code class="hljs-code">candidates {
  content {
    role: "model"
    parts {
      text: "What do you call a car that\'s always breaking down?\n\nA lemon-aid stand!"
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 6
  candidates_token_count: 20
  total_token_count: 26
}
</code></pre>
<p class="normal">You can see that there is a property called <code class="inlineCode">finish_reason</code>, which is essentially the reason why the model stopped generating tokens. If this property is empty, the model has not yet stopped generating the tokens. The following is the full list of options per Gemini’s documentation as of February 2024:</p>
<table class="table-container" id="table003">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Finish Reason code</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Description</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">FINISH_REASON_UNSPECIFIED</code></p>
</td>
<td class="table-cell">
<p class="normal">The finish reason is unspecified.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">FINISH_REASON_STOP</code></p>
</td>
<td class="table-cell">
<p class="normal">Natural stop point of the model or provided stop sequence.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">FINISH_REASON_MAX_TOKENS</code></p>
</td>
<td class="table-cell">
<p class="normal">The maximum number of tokens as specified in the request was reached.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">FINISH_REASON_SAFETY</code></p>
</td>
<td class="table-cell">
<p class="normal">The token generation was stopped as the response was flagged for safety reasons. Note that <code class="inlineCode">Candidate.content</code> is empty if content filters block the output.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">FINISH_REASON_RECITATION</code></p>
</td>
<td class="table-cell">
<p class="normal">The token generation was stopped as the response was flagged for unauthorized citations.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">FINISH_REASON_OTHER</code></p>
</td>
<td class="table-cell">
<p class="normal">All other reasons that stopped the token.</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 3.3: Google Gemini finish reasons as of Feb 2024</p>
<p class="normal">After that<a id="_idIndexMarker131"/> section, you will find<a id="_idIndexMarker132"/> the <code class="inlineCode">safety_ratings</code> for what was generated. In your application, you can parse the result from the LLM and filter the results. A good application for leveraging <code class="inlineCode">safety_ratings</code> comes from analytics generation, where you can store the safety ratings from your prompts, and then analyze them to create insights.</p>
<pre class="programlisting code"><code class="hljs-code">safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
</code></pre>
<p class="normal">Let’s now experiment<a id="_idIndexMarker133"/> with a questionable prompt. Using the safety <a id="_idIndexMarker134"/>ratings, we will set the code to block anything with probabilities that are below or above the desired rating:</p>
<pre class="programlisting code"><code class="hljs-code">safety_settings={
    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
}
</code></pre>
<p class="normal">And we will run the prompt <code class="inlineCode">How do I rob a bank with a toy gun?</code>. After submitting the prompt to Google Gemini Pro, I received the following result:</p>
<pre class="programlisting code"><code class="hljs-code">candidates {
  content {
    role: "model"
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 10
  total_token_count: 10
}
</code></pre>
<p class="normal">As you can see, the generation<a id="_idIndexMarker135"/> was stopped due to <code class="inlineCode">SAFETY</code> reasons, and if <a id="_idIndexMarker136"/>you go to the <code class="inlineCode">safety_ratings</code> section, you will see that the <code class="inlineCode">HARM_CATEGORY_DANGEROUS_CONTENT</code> has a probability of <code class="inlineCode">LOW</code> and has the flag blocked as <code class="inlineCode">True</code>. In other words, the content was blocked due to the fact that it was classified as being in the Dangerous Content category.</p>
<h2 class="heading-2" id="_idParaDest-41">Selecting from amongst multiple outputs</h2>
<p class="normal">Models like LLMs often produce<a id="_idIndexMarker137"/> multiple candidate responses or images. Post-processing can analyze all options based on relevance, interest, diversity, and other attributes to automatically select the best single result for a given prompt. This avoids overloading users with extraneous outputs.</p>
<p class="normal">In the following example, you will use the Google PaLM 2 model for text (text-bison) to generate multiple responses:</p>
<pre class="programlisting code"><code class="hljs-code">import vertexai
from google.cloud import aiplatform
from vertexai.language_models import TextGenerationModel
from google.colab import auth as google_auth
google_auth.authenticate_user()
# GCP Parameters
PROJECT = "your-GCP-project" #@param {type:"string"}
LOCATION = "us-central1" #@param {type:"string"}
#Init Vertex AI Platform
aiplatform.init(project=PROJECT, location=LOCATION)
def generate(prompt):
  model = TextGenerationModel.from_pretrained("text-bison")
  parameters = {
    "candidate_count": 2,
    "max_output_tokens": 1024,
    "temperature": 0.9,
    "top_p": 1
    }
  response = model.predict(
    prompt,
    **parameters
    )
  return response
result = generate("Tell me a joke about cars")
for candidate in result.candidates:
  print(f"Response Candidate: {candidate}\n\n")
</code></pre>
<p class="normal">This is what the generate function looks like:</p>
<pre class="programlisting code"><code class="hljs-code">def generate(prompt):
  model = TextGenerationModel.from_pretrained("text-bison")
  parameters = {
    "candidate_count": 2,
    "max_output_tokens": 1024,
    "temperature": 0.5,
    "top_p": 1
    }
  response = model.predict(
    prompt,
    **parameters
    )
  return response
</code></pre>
<p class="normal">As you can see, we are setting the number of <code class="inlineCode">candidate_count</code> to <code class="inlineCode">2</code>.</p>
<p class="normal">When you get the response<a id="_idIndexMarker138"/> from the generate function, Google PaLM 2 will return a <code class="inlineCode">MultiCandidateTextGenerationResponse</code> object. In order to obtain all results, you will have to iterate over the candidates:</p>
<pre class="programlisting code"><code class="hljs-code">result = generate("Tell me a joke about cars")
for candidate in result.candidates:
  print(f"Response Candidate: {candidate}\n\n")
</code></pre>
<p class="normal">You will get a result<a id="_idIndexMarker139"/> in the following format:</p>
<pre class="programlisting con"><code class="hljs-con">Response Candidate:  Why did the car get a parking ticket?
Because it was parked illegally.
Response Candidate:  Why did the car get a speeding ticket?
Because it was going too fast!
</code></pre>
<h2 class="heading-2" id="_idParaDest-42">Refining generated outputs</h2>
<p class="normal">After a model generates<a id="_idIndexMarker140"/> an output, there are additional steps required to improve the output quality. The techniques will vary depending on what was generated as there are different methods for post-processing audio, images, and text. While automated post-processing techniques such as filtering and output selection provide a foundation, human interaction and refinement can take GenAI results to the next level of quality. There are additional methodologies to enable this collaborative improvement.</p>
<p class="normal">For example, users can take an initial model output, provide feedback on areas that need improving, and resubmit the prompt to refine the result further. Models can be prompted to expand on sections, fix errors, or adjust stylistic elements. Iterating in this loop surfaces the benefits of human and AI collaboration. Let’s see an example with a text generated by Google Gemini Pro using the following prompt:</p>
<pre class="programlisting con"><code class="hljs-con">Prompt: Tell me a joke about cars
Result:
Why don't cars like to go to the beach?
Because they just want to dune it.
</code></pre>
<p class="normal">Now let’s take that output, and request the model to write two paragraphs about it:</p>
<pre class="programlisting con"><code class="hljs-con">Prompt:
Expand the following text in two paragraphs:
Why don't cars like to go to the beach?
Because they just want to dune it.
Result: 
Cars, unlike humans and other creatures, harbor an aversion to the sandy shores of the beach. This peculiar reluctance stems from the woes they face when navigating the treacherous terrain. As tires sink into the soft sand, cars find themselves losing traction and struggling to maintain momentum. The endless dunes, often towering over them, pose another formidable obstacle. Attempts to conquer these sandy peaks can lead to wheels spinning futilely, leaving cars stranded amidst the unforgiving landscape.
Therefore, while the beach beckons with its allure of sun, sand, and surf, cars prefer to remain on paved roads, where they can glide effortlessly without the fear of getting bogged down. Their aversion to the beach is aptly summarized in the playful pun, "They just want to dune it," a clever play on words that highlights their desire to avoid the challenges and embrace the comfort of smooth roads.
</code></pre>
<p class="normal">Another helpful technique for improving GenAI outputs is leveraging multiple alternative responses. When users identify shortcomings or areas for improvement in a model’s initial result, applications can surface several alternative candidates for the user to choose from. This allows the human to select the option that comes closest to fulfilling their original intent and objective.</p>
<p class="normal">As an example, consider the example<a id="_idIndexMarker141"/> provided in the previous section with Google PaLM, which generates multiple candidate outputs for a given prompt. The application could display these alternatives and let the user pick the one that resonates most. The model acts as a powerful “brain-storming partner,” rapidly producing a diverse set of options. Then, human curation and selection refine the outputs iteratively, shaping them closer and closer to the ideal final result the user has in mind.</p>
<figure class="mediaobject"><img alt="" height="359" src="img/B22175_03_03.png" width="825"/></figure>
<p class="packt_figref">Figure 3.3: Multiple generation responses shown in the Google Cloud console</p>
<p class="normal">Now you will explore what the experience<a id="_idIndexMarker142"/> for generating multiple outputs would look like in the case of a Vertex AI API call to PaLM 2 with Python:</p>
<pre class="programlisting code"><code class="hljs-code">def generate(prompt):
  model = TextGenerationModel.from_pretrained("text-bison")
  parameters = {
    "candidate_count": 2,
    "max_output_tokens": 1024,
    "temperature": 0.9,
    "top_p": 1
  }
  response = model.predict(
    prompt,
    **parameters
  )
  return response
</code></pre>
<p class="normal">You will notice that we are using the text-bison model, and the <code class="inlineCode">candidate_count</code> parameter to specify how many results are going to be generated.</p>
<p class="normal">We will evaluate the results like this:</p>
<pre class="programlisting code"><code class="hljs-code">result = generate("Tell me a joke about cars")
for candidate in result.candidates:
  print(f"Response Candidate: {candidate}\n\n")
</code></pre>
<p class="normal">You will obtain a result similar to this:</p>
<pre class="programlisting con"><code class="hljs-con">Response Candidate:  What does a car with road rage suffer from?
Answer: A road rash.
Response Candidate:  What did one car say to the other car as it drove past? We should race and I bet I will win.
What did the other car say? Don't be silly there is no way you can beat a Volkswagen!
</code></pre>
<p class="normal">You can now iterate through multiple results, and select which result is more adequate for the task at hand.</p>
<p class="normal">Remember that you can refine<a id="_idIndexMarker143"/> an obtained result. For example, we can convert the response into, for example, a sonnet:</p>
<pre class="programlisting code"><code class="hljs-code">result = generate(f"Modify this joke to be a sonnet with no more than 2 verses: {result.candidates[0].text} ")
for candidate in result.candidates:
  print(f"Response Candidate: {candidate}\n\n")
</code></pre>
<pre class="programlisting con"><code class="hljs-con">Response Candidate:  A chariot of fire, its engine's beat,
A symphony of power, fierce and wild,
With burning wheels, it danced upon the street,
A tempest unleashed, untamed and beguiled.
Its metal frame, a vessel of desire,
A rebel's heart that yearned for freedom's call,
Through winding roads, it sparked electric fire,
A fearless spirit, soaring high and tall.
Response Candidate:  A car of wrath with engine's fiery might,
Its metal frame roars to the storm within,
Its wheels devour the roads in ravenous fight,
An elemental fury none can win.
Its tires are scorching marks upon the ground,
As it dances wildly under fate's harsh glance,
With bruised fenders and paintwork battle-bound,
This rage-filled vessel knows no calm expanse.
</code></pre>
<p class="normal">This collaborative generation workflow leverages both the strengths of GenAI and human creativity. Generative models contribute creativity, scalability, and the ability to explore a broad possibility space. Humans in turn provide intentionality, quality judgments, and context about what is desirable. Together, machines and humans waltz through an iterative process bringing the best out of each other.</p>
<p class="normal">The outputs start as raw ingredients<a id="_idIndexMarker144"/> from the AI, but progressively gets refined into increasingly useful, engaging, and delightful content through this hybrid collaboration. Leveraging both automated techniques like filtering and output selection, and direct human interaction to choose amongst alternatives, iteratively refining and editing results pushes the boundaries of GenAI quality.</p>
<h1 class="heading-1" id="_idParaDest-43">Results presentation</h1>
<p class="normal">At this point, we have<a id="_idIndexMarker145"/> covered techniques<a id="_idIndexMarker146"/> for selecting appropriate GenAI models, crafting effective prompts, and guiding the models to produce high-quality results. Now let’s explore considerations around presenting the outputs generated by large language models and other systems to application end users or downstream processes.</p>
<p class="normal">How LLM-produced content gets rendered and exposed is heavily dependent on the specific use case and application architecture. For example, in a chatbot scenario, results would be formatted into conversational textual or voice responses. On the other hand, for a search engine, text outputs could be incorporated into answer boxes and summaries. Document generation workflows may store LLM outputs directly into cloud content platforms. The possibilities span many formats.</p>
<p class="normal">Some technical aspects are common across different result presentation approaches. Text outputs often require post-processing such as Markdown tagging removal, JSON serialization, and more based on destination needs. Safety rating data may need to be persisted alongside generated text for governance. Multimodal outputs like images would integrate rendering frameworks to correctly display the media to users.</p>
<p class="normal">Certain applications may store the raw LLM outputs in databases or data lakes for later asynchronous consumption rather than immediate presentation. In these<a id="_idIndexMarker147"/> cases, additional <strong class="keyWord">extract</strong>, <strong class="keyWord">transform</strong>, <strong class="keyWord">load</strong> (<strong class="keyWord">ETL</strong>) workflows prepare and reshape unstructured AI-produced results into structured data repositories for downstream analytics, training, and governance. Appropriately tagging outputs ensures they can be found easily.</p>
<p class="normal">The end presentation format should focus first on usability – shaping contents, structure, resolution, and other attributes tailored to customer needs and the customer journey. Second, the focus<a id="_idIndexMarker148"/> should shift to ease of integration – efficiently slotting AI outputs into existing or custom UI codebases, content systems, and data pipelines based on accessibility requirements. Well-designed result-sharing unlocks the inherent value created by generative models.</p>
<h1 class="heading-1" id="_idParaDest-44">Logging</h1>
<p class="normal">Establishing comprehensive observability<a id="_idIndexMarker149"/> through logging<a id="_idIndexMarker150"/> is critical when integrating GenAI models into applications. Capturing detailed telemetry data across the entire workflow enables tracking metrics, monitoring issues, and identifying failure points – essential for ensuring reliable and responsible AI system behavior over time.</p>
<p class="normal">Detailed usage metrics and logging provide a wealth of observability benefits at the front-end integration points for GenAI. This telemetry data not only reveals how models are utilized in production environments but, more importantly, surfaces the interactive user experiences built around AI capabilities.</p>
<p class="normal">By tracking every user input, generation request, and context around those events, organizations gain an overview of emerging product usage patterns. This data can later be leveraged to expand on certain use cases to ultimately improve the user’s experience. </p>
<p class="normal">Questions like “Are certain demographics or geographies clustering around particular use cases?”, “Do input domains or content types reveal areas of user demand to double down on?”, and “What sequences of prompts characterize high-value user journeys?” can be answered to generate insights, which in turn will provide visibility on the interactions at scale.</p>
<p class="normal">Log analytics also facilitate cost monitoring and optimization. With visibility into volumes of inference requests by model, user cohort, feature area, and more, it becomes feasible to map operational expenditure and scaling needs directly to patterns of user activity over time. Load testing can measure incremental cost impacts before rolling out new AI-intensive features. Utilization metrics feed into autoscaling and provisioning processes. Ultimately, correlating business KPIs against infrastructure consumption allows aligning investments to maximize AI-driven value capture.</p>
<p class="normal">This AI usage intelligence effectively provides visibility on how customer experiences are evolving and financially impacts the business. It empowers use case prioritization, roadmap planning, and efficient resource allocation – all grounded in empirical data rather than gut instinct. Meticulous logging isn’t just about safety and compliance, but ensuring AI integrations sustainably deliver value and grow adoption and ROI.</p>
<p class="normal">The input prompts and data samples that seed generative models should get meticulously logged. This data is key for explainability by tying inputs to their corresponding outputs. Furthermore, it enables monitoring data quality issues by detecting drift away from expected distributions that models were trained on. Catching these shifts through differential logging proactively reveals when retraining may be needed.</p>
<p class="normal">Logging should extend<a id="_idIndexMarker151"/> to the output side by capturing<a id="_idIndexMarker152"/> rich metadata about the results produced by models including safety classifier scores, provenance details, and any intervening processing. Error cases such as rejections, safety violations, or failed inferences critically need logging to identify failure points that require remediation. Thorough output logging also supports auditing use cases for governance and compliance.</p>
<p class="normal">An important consideration when integrating GenAI models is that many platforms (should) treat inference requests in a stateless manner without inherent logging capabilities. For example, on Google Cloud’s Vertex AI, request details are not automatically logged by default when generating predictions from LLMs. The responsibility falls on the application itself to implement comprehensive logging.</p>
<p class="normal">While there is no single gold standard, best practices tend to encourage capturing several key pieces of information for each generative model interaction. At a bare minimum, logging payloads should include:</p>
<ul>
<li class="bulletList">The timestamp of the request</li>
<li class="bulletList">The raw user input </li>
<li class="bulletList">Any additional context data provided (chat history, retrieved information for RAG, etc.)</li>
<li class="bulletList">The prompt template used, if any pre-processing occurred</li>
<li class="bulletList">Identifiers for the specific model(s) invoked</li>
<li class="bulletList">The full model output or result</li>
<li class="bulletList">Any post-processing template used to shape the model output</li>
</ul>
<pre class="programlisting code"><code class="hljs-code">{
  "timestamp":,
  "user_input":, 
  "context": ,
  "pre_prosessing_prompt_template": ,
  "model": ,
  "result_payload":
  "post_prosessing_prompt_template":
}
</code></pre>
<p class="normal">Capturing this coherent payload allows the interaction history to be established and the results to be reproduced completely, explaining any given output. This supports analytic use cases like exploring user adoption, potential pain points, and shifts in usage trends, and at the same time, it enables continuous monitoring for potential issues or safety violations that require intervention.</p>
<p class="normal">Beyond these core fields, other metadata around the generation process can enrich observability. This may include latencies, resource consumption metrics, interim processing steps applied before or after inference, and data capturing lineage of reruns or iterations on a prompt. The log format should strike a balance between comprehensive detail while avoiding cumbersome bloat.</p>
<p class="normal">Implementing centralized structured logging<a id="_idIndexMarker153"/> conforming to established<a id="_idIndexMarker154"/> templates is a key building block for responsible AI model operationalization. It transforms opaque and stateless generation capabilities into transparent, reproducible, and monitorable production pipelines aligned with governance best practices. Robust logging regimes help GenAI earn trust at enterprise scale.</p>
<h1 class="heading-1" id="_idParaDest-45">Summary</h1>
<p class="normal">In this chapter, we discussed the integration of GenAI models into real-world applications that require a systematic approach. A five-component framework can guide this process: Entry Point, Prompt Pre-Processing, Inference, Result Post-Processing, and Logging. At the entry point, user inputs aligned with the AI model’s expected modalities are accepted, whether text prompts, images, audio, etc. Prompt pre-processing then cleans and formats these inputs for security checks and optimal model usability.</p>
<p class="normal">The core inference component then runs the prepared inputs through the integrated GenAI models to produce outputs. This stage requires integrating with model APIs, provisioning scalable model-hosting infrastructure, and managing availability alongside cost controls. Organizations can choose self-hosting models or leveraging cloud services for inference. After inference, result post-processing techniques filter inappropriate content, select ideal outputs from multiple candidates, and refine texts/images through automation or human-AI collaboration methods like iterative refinement.</p>
<p class="normal">How these AI-generated results get presented depends on the application’s use case – whether powering chatbots, search engines, or document workflows, among others. Regardless, common aspects include text output processing, handling safety ratings, and rendering multimodal outputs appropriately. Some applications may opt to store raw model outputs in data repositories for later asynchronous consumption via ETL pipelines rather than immediate presentation.</p>
<p class="normal">Comprehensive logging establishes critical observability across this entire workflow, tracking metrics, monitoring data quality issues that could indicate drift away from training sets, and identifying inference errors or failure points. Diligently structured logging should capture user inputs, context data, model outputs, safety ratings, and process metadata details. While some platforms treat inference requests on an ad hoc basis, without inherent logging, applications must implement centralized logging following best practices.</p>
<p class="normal">Key takeaways:</p>
<ul>
<li class="bulletList">Integrate GenAI through a systematic framework covering entry points, pre-processing, inference, post-processing, and logging.</li>
<li class="bulletList">Consider interactive vs. batch processing approaches based on latency, scale, and cost priorities.</li>
<li class="bulletList">Implement comprehensive logging for observability, monitoring, explainability, and governance.</li>
<li class="bulletList">Leverage human-AI collaboration to iteratively refine and enhance AI-generated outputs.</li>
<li class="bulletList">Design result presentation formats tailored to usability and integration needs.</li>
<li class="bulletList">Address security, responsible AI practices, and ethical considerations throughout the integration process.</li>
</ul>
<h1 class="heading-1">Join our community on Discord</h1>
<p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
<p class="normal"><a href="Chapter_03.xhtml">https://packt.link/genpat</a></p>
<p class="normal"><img alt="" height="177" src="img/QR_Code134841911667913109.png" width="177"/></p>
</div>
</div></body></html>