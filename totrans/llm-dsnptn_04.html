<html><head></head><body><div><div><div><h1 id="_idParaDest-62" class="chapter-number"><a id="_idTextAnchor072"/>4</h1>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor073"/>Handling Large Datasets for LLM Training</h1>
			<p>In this chapter, you’ll learn advanced techniques for managing and processing massive datasets essential for training state-of-the-art LLMs. We’ll explore the unique challenges posed by large-scale language datasets and provide you with practical solutions to overcome them.</p>
			<p>The aim of this chapter is to equip you with the knowledge and tools to efficiently handle data at scale, enabling you to train more powerful and effective LLMs.</p>
			<p>In this chapter, we’ll be covering the following topics:</p>
			<ul>
				<li>Challenges of large datasets</li>
				<li>Data sampling techniques</li>
				<li>Distributed data processing</li>
				<li>Data sharding and parallelization strategies</li>
				<li>Efficient data storage formats</li>
				<li>Streaming data processing for continuous LLM training</li>
				<li>Memory-efficient data loading techniques</li>
			</ul>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor074"/>Challenges of large datasets</h1>
			<p>Training LLMs requires enormous <a id="_idIndexMarker143"/>datasets, often in the terabytes or even petabytes range. This scale introduces several challenges:</p>
			<ul>
				<li><strong class="bold">Storage requirements</strong>: Datasets can exceed the capacity of single machines, necessitating distributed storage solutions.</li>
				<li><strong class="bold">Input/output (I/O) bottlenecks</strong>: Reading large volumes of data can become a significant bottleneck, limiting training speed.</li>
				<li><strong class="bold">Preprocessing overhead</strong>: Tokenization and other preprocessing steps can be time-consuming at scale due to the computational overhead of processing large volumes of text data through multiple sequential operations. The challenge arises from having <a id="_idIndexMarker144"/>to perform multiple steps on each piece of text – tokenization, normalization, cleaning, language detection, and other transformations – multiplied across millions or billions of text samples. This process is inherently sequential (each step depends on the previous one), requires CPU/memory<a id="_idIndexMarker145"/> resources, and can involve complex operations such as <strong class="bold">regular expressions</strong> (<strong class="bold">regexes</strong>), dictionary lookups, and language-specific rules. When dealing with multilingual or code-mixed data, the complexity increases further as different language rules need to be applied, and additional steps such as script normalization or language detection are required for each text segment, making the preprocessing pipeline a significant <a id="_idIndexMarker146"/>bottleneck in large-scale <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) systems.</li>
				<li><strong class="bold">Memory constraints</strong>: Loading entire datasets into memory is often infeasible, requiring streaming or batching approaches.</li>
				<li><strong class="bold">Data quality and diversity</strong>: Ensuring dataset quality and representativeness becomes more challenging as size increases.</li>
			</ul>
			<p>To address these challenges, we need to employ sophisticated data-handling techniques. Let’s explore a Python<a id="_idIndexMarker147"/> implementation using the <strong class="bold">Datasets</strong> library from Hugging Face, which is designed to handle large-scale datasets efficiently:</p>
			<pre class="source-code">
from datasets import load_dataset, Dataset
import psutil
def load_and_process_large_dataset(dataset_name, num_proc):
    # Load the dat<a id="_idTextAnchor075"/>aset
    dataset = load_dataset(dataset_name, streaming=True)
    # Define a preprocessing function
    def preprocess_function(examples):
        # Implement your preprocessing logic here
        return examples
    # Apply preprocessing in parallel
    processed_dataset = dataset.map(
        preprocess_function,
        batched=True,
        num_proc=num_proc,
        remove_columns=dataset["train"].column_names
    )
    return processed_dataset
#Determine the number of CPU cores for parallel processing
num_cores = psutil.cpu_count(logical=False)
#Load and process a large dataset (e.g., C4 dataset)
large_dataset = load_and_process_large_dataset("c4", 
    num_proc=num_cores)
#Print the first few examples
for example in large_dataset["train"].take(5):
    print(example)</pre>			<p>In this code, we use the Datasets library to efficiently load and process a large dataset (in this case, the <code>C4</code> dataset). The <code>num_proc</code> parameter specifies the number of processor cores to use for parallel processing in the dataset mapping operation. When preprocessing large datasets, using multiple CPU cores through parallel processing can significantly speed up the operation. For example, if <code>num_proc=4</code>, the preprocessing function will be executed on <a id="_idIndexMarker148"/>four processor cores simultaneously, processing different batches of data in parallel rather than sequentially.</p>
			<p>To better understand the context in which large datasets are used, it is helpful to explore a specific example. One such dataset used in<a id="_idIndexMarker149"/> the preceding code snippet is the <strong class="bold">Colossal Clean Crawled Corpus</strong> (<strong class="bold">C4</strong>) dataset, which plays a significant role in training modern LLMs.</p>
			<p>The <strong class="bold">C4</strong> dataset is a massive, cleaned web-crawled text corpus created by Google for training LLMs. Containing approximately <a id="_idIndexMarker150"/>750 GB of English-language text, C4 is derived from Common Crawl data and has undergone extensive filtering to remove duplicates, non-English content, and offensive material. It comes in several variants, including a standard cleaned version, an unfiltered version, and a subset focused on news-like content. While publicly available, accessing C4 requires some effort, typically through Google Cloud Storage or libraries such as Hugging Face datasets. Despite its cleaning process, C4 still has some limitations regarding content quality and potential biases, which researchers should consider when using it for model training. Nevertheless, it remains a valuable resource <a id="_idIndexMarker151"/>for NLP tasks and has been instrumental in training <a id="_idIndexMarker152"/>prominent models such as <strong class="bold">Text-to-Text Transfer Transformer</strong> (<strong class="bold">T5</strong>) and <strong class="bold">Language Model for Dialogue </strong><strong class="bold">Applications</strong> (<strong class="bold">LaMDA</strong>).</p>
			<p>We employ streaming to avoid loading the entire dataset into memory at once. The <code>num_proc</code> parameter is set to the number of physical CPU cores to maximize parallel processing efficiency.</p>
			<p>The <code>preprocess_function</code> function is<a id="_idIndexMarker153"/> where you implement dataset-specific preprocessing logic. This function is applied in parallel across the dataset, significantly speeding up preprocessing for large datasets.</p>
			<p>You can also use a GPU for the task. See the following code example (keep in mind that while GPU-based preprocessing is particularly beneficial for operations such as tokenization and embedding <a id="_idIndexMarker154"/>generation, it may not significantly accelerate simpler text manipulations):</p>
			<pre class="source-code">
import torch
from datasets import load_dataset
from torch.utils.data import DataLoader
from transformers import AutoTokenizer
def load_and_process_dataset(dataset_name, batch_size):
    dataset = load_dataset(dataset_name, streaming=True)
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    def preprocess(examples):
        return tokenizer(
            examples["text"], padding="max_length",
            truncation=True, return_tensors="pt"
        )
    def process_batch(batch):
        return {k: v.to(device) for k, v in preprocess(batch).items()}
    return DataLoader(
        dataset["train"].map(process_batch),
        batch_size=batch_size, num_workers=2,
        pin_memory=True
    )
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
dataloader = load_and_process_dataset("c4", batch_size=32)
for i, batch in enumerate(dataloader):
    if i &gt;= 5: break
    print(f"Batch {i}:", {k: v.shape for k, v in batch.items()})</pre>			<p>This code uses the PyTorch and Hugging Face libraries to process a dataset (for example, C4) with GPU acceleration. It employs a data loader for efficient batch processing, moves data to GPU memory, and uses a pre-trained tokenizer. The main GPU benefits come from parallel batch <a id="_idIndexMarker155"/>processing and GPU-accelerated tokenization. While this setup enables GPU usage, the most significant GPU advantages typically occur during model training or inference rather than preprocessing.</p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor076"/>Data sampling techniques</h1>
			<p>Data sampling is a practical approach to <a id="_idIndexMarker156"/>reducing the size of large datasets without sacrificing representativeness. Several techniques exist, each with specific use cases and trade-offs. <strong class="bold">Random sampling</strong> selects <a id="_idIndexMarker157"/>data points uniformly at random from the dataset. It is simple and effective when the data is independently and identically distributed, but it may miss important subgroups if the data is imbalanced. <strong class="bold">Systematic sampling</strong> selects every <em class="italic">k</em>th item from a list after a random starting point. It is <a id="_idIndexMarker158"/>more structured than random sampling and can be useful when the data is ordered in a meaningful way, though it risks introducing bias if the ordering aligns with hidden periodic patterns. <strong class="bold">Reservoir sampling</strong> is designed for streaming of unknown-size datasets. It<a id="_idIndexMarker159"/> maintains a fixed-size sample while iterating through the data sequentially and ensures that every item has an equal probability of being included. This is particularly useful in online or incremental learning scenarios where data arrives in continuous flows.</p>
			<p>Due to the length constraints of this chapter, we focus only on <strong class="bold">stratified sampling</strong>, a technique that preserves the proportional<a id="_idIndexMarker160"/> representation of subgroups within a dataset. It is especially suitable when certain attributes—such as label classes, sentence lengths, or metadata categories—are known to affect model performance and need to be maintained in the sampled subset. In NLP, text length is a common stratification variable, given its impact on model input dynamics.</p>
			<p>The following implementation demonstrates how to apply stratified sampling based on text length. It divides the<a id="_idIndexMarker161"/> dataset into percentile-based strata and samples proportionally from each stratum to create a subset that retains the length distribution of the full dataset:</p>
			<pre class="source-code">
import numpy as np
from datasets import Dataset
def stratified_length_sampling(
    dataset, num_samples, num_strata=10
):
    # Calculate text lengths
    lengths = [len(example['text']) for example in dataset]
    # Create strata based on text length
    strata_bounds = np.percentile(
        lengths, np.linspace(0, 100, num_strata + 1)
    )
    sampled_data = []
    for i in range(num_strata):
        stratum = [
            example for example in dataset
            if strata_bounds[i] &lt;= len(example['text']) &lt; \
                strata_bounds[i+1]
        ]
        stratum_samples = np.random.choice(
            stratum,
            size=num_samples // num_strata,
            replace=False
        )
        sampled_data.extend(stratum_samples)
    return Dataset.from_dict({
        key: [example[key] for example in sampled_data]
        for key in dataset[0].keys()
    })
#Usage
sampled_dataset = stratified_length_sampling(large_dataset, 
    num_samples=100000)</pre>			<p>This stratified sampling technique ensures that we maintain a representative distribution of text lengths in our sampled dataset. We use 10 strata (<code>num_strata=10</code>) to balance granularity and computational efficiency. Adjust this value based on your specific dataset characteristics and sampling requirements.</p>
			<p>As datasets grow in size and <a id="_idIndexMarker162"/>complexity, single-machine processing becomes a bottleneck in both speed and scalability. Techniques such as data sampling offer partial relief, but they do not resolve the computational limitations inherent in centralized architectures. To address these constraints, the next section introduces distributed data processing, where computation is spread across multiple machines or nodes to improve throughput, reduce latency, and support parallel workflows required for large-scale LLM training pipelines.</p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor077"/>Distributed data processing</h1>
			<p>For truly massive datasets, distributed processing becomes necessary. Here’s an example using <strong class="bold">Dask</strong>, a flexible library for <a id="_idIndexMarker163"/>parallel computing in Python (<a href="https://www.dask.org/">https://www.dask.org/</a>).</p>
			<p>Dask and Apache Spark <a id="_idIndexMarker164"/>are both distributed computing frameworks, but their main differences lie in their architecture and use cases. Spark is built around the concept of <strong class="bold">resilient distributed datasets</strong> (<strong class="bold">RDDs</strong>) and requires a cluster setup, making it ideal<a id="_idIndexMarker165"/> for large-scale production data processing. Dask, on the other hand, is designed to integrate seamlessly with the Python ecosystem and can scale from a single laptop to a cluster, using familiar APIs that mirror NumPy, pandas, and scikit-learn. While Spark excels at batch processing of massive datasets, Dask is more flexible for interactive computing and scientific workflows, particularly when working with Python-native libraries and when you need to scale up existing Python code with minimal modifications.</p>
			<p>Let’s get back to our code:</p>
			<pre class="source-code">
import dask.dataframe as dd
from dask.distributed import Client
def distributed_preprocessing(data_path, num_partitions):
    # Initialize Dask client
    client = Client()
    # Read the dataset into a Dask DataFrame
    df = dd.read_csv(data_path, blocksize="64MB")
    # Repartition the data for better distribution
    df = df.repartition(npartitions=num_partitions)
    # Define preprocessing function
    def preprocess(text):
        # Implement your preprocessing logic here
        return processed_text
    # Apply preprocessing in parallel
    df['processed_text'] = df['text'].map(preprocess)
    # Trigger computation and return results
    result = df.compute()
    client.close()
    return result
#Usage
processed_data = distributed_preprocessing(
    "path/to/large/dataset.csv", num_partitions=100
)</pre>			<p>In this example, we use Dask to distribute the preprocessing workload across multiple machines or cores. The <code>num_partitions</code> parameter (set to <code>100</code>) determines the level of parallelism and <a id="_idIndexMarker166"/>should be adjusted based on your available computational resources and dataset size.</p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor078"/>Data sharding and parallelization strategies</h1>
			<p><strong class="bold">Data sharding</strong> refers to the<a id="_idIndexMarker167"/> technique of breaking up a large dataset into smaller, more manageable <a id="_idIndexMarker168"/>pieces, known as “shards,” which are<a id="_idIndexMarker169"/> then distributed across multiple machines or storage systems. Each shard can be processed independently, making it easier to handle large datasets, especially those that don’t fit into the memory of a single machine. This approach is widely used in machine learning to distribute the processing of large datasets, thereby allowing for the training of larger <a id="_idIndexMarker170"/>models or faster computation.</p>
			<p>Data sharding enables more<a id="_idIndexMarker171"/> efficient use of computational resources as each shard can be processed independently, and the results can be aggregated later.</p>
			<p>However, careful consideration must be given to ensuring that the sharding strategy maintains the integrity and representativeness of the data distribution across all shards to avoid biases or inconsistencies in the trained model.</p>
			<p>Here’s an example of a sharding strategy:</p>
			<pre class="source-code">
import hashlib
def shard_data(dataset, num_shards):
    shards = [[] for _ in range(num_shards)]
    for item in dataset:
        # Use a hash function to determine the shard
        shard_index = int(
            hashlib.md5(
                item['id'].encode()
            ).hexdigest(), 16
        ) % num_shards
        shards[shard_index].append(item)
    return shards
#Usage
sharded_data = shard_data(large_dataset, num_shards=10)</pre>			<p>This sharding strategy uses a hash function to distribute data items across shards. The <code>num_shards</code> parameter (set to <code>10</code>) should be adjusted based on your infrastructure and parallelization needs.</p>
			<p>The <code>shard_data</code> function distributes<a id="_idIndexMarker172"/> items from a dataset into a specified number of shards by applying a consistent<a id="_idIndexMarker173"/> hashing scheme based on each item’s unique identifier. It initializes a list of empty lists, each representing a shard, and for every item in the input dataset, it calculates a shard index using the <code>'id'</code> field. The hash output is converted to an integer <a id="_idIndexMarker174"/>and taken modulo the number of shards to ensure uniform distribution across shards. This method guarantees that items with the same ID are consistently mapped to the same shard across executions, which is useful for tasks such as distributed storage or parallel processing where determinism and balance are important.</p>
			<p>Sharding strategies are chosen based on the nature of the data and expected query patterns, with each approach offering distinct trade-offs in scalability, performance, and complexity:</p>
			<ul>
				<li><strong class="bold">Hash sharding</strong>: Suitable for uniformly <a id="_idIndexMarker175"/>distributed data by mapping keys through a hash function to distribute load evenly across shards</li>
				<li><strong class="bold">Range sharding</strong>: Effective for ordered<a id="_idIndexMarker176"/> datasets such as time-series logs, where each shard holds a contiguous range of data values</li>
				<li><strong class="bold">Geographic sharding</strong>: Designed to <a id="_idIndexMarker177"/>optimize location-based queries by partitioning data according to geographical regions</li>
				<li><strong class="bold">Key-value sharding</strong>: Enables <a id="_idIndexMarker178"/>manual control of hotspots by assigning specific key ranges or values to defined shards</li>
				<li><strong class="bold">Directory-based sharding</strong>: Supports dynamic shard allocation using a lookup service to determine data placement, adapting to<a id="_idIndexMarker179"/> changes in data distribution</li>
				<li><strong class="bold">Consistent hashing</strong>: Minimizes data<a id="_idIndexMarker180"/> movement when the number of shards changes, maintaining stability and reducing rebalancing overhead</li>
				<li><strong class="bold">Round-robin sharding</strong>: Distributes <a id="_idIndexMarker181"/>data sequentially across shards, providing simplicity but poor performance for range-based queries</li>
				<li><strong class="bold">Workload-based sharding</strong>: Balances <a id="_idIndexMarker182"/>access load by assigning high-traffic data to separate shards based on observed query patterns</li>
				<li><strong class="bold">Composite sharding</strong>: Combines<a id="_idIndexMarker183"/> multiple strategies to support complex systems with diverse data types and query needs</li>
				<li><strong class="bold">Tag-based sharding</strong>: Categorizes <a id="_idIndexMarker184"/>data based on labels such as user roles or data <a id="_idIndexMarker185"/>categories, supporting domain-specific partitioning strategies</li>
			</ul>
			<p>For the preceding code <a id="_idIndexMarker186"/>block, we can also define the following function as the main orchestrator to process and aggregate shards:</p>
			<pre class="source-code">
def process_with_sharding(
    dataset: List[Dict], num_shards: int
) -&gt; List[Dict]:
    # Step 1: Shard the data
    shards = shard_data(dataset, num_shards)
    # Step 2: Process shards in parallel
    with ProcessPoolExecutor(max_workers=num_shards) as executor:
        processed_shards = list(executor.map(process_shard, shards))
    # Step 3: Aggregate results
    aggregated_results = []
    for shard_results in processed_shards:
        aggregated_results.extend(shard_results)</pre>			<p>The <code>process_with_sharding</code> function takes a dataset represented as a list of dictionaries and divides it into a specified number of shards using the <code>shard_data</code> function. It then uses <code>ProcessPoolExecutor</code> with as many workers as shards to process each shard concurrently by applying the <code>process_shard</code> function in parallel. After all shards have been <a id="_idIndexMarker187"/>processed, it aggregates the individual results from each shard into a single list by<a id="_idIndexMarker188"/> iterating over the processed shards and extending the final result list with their contents.</p>
			<p>Once data has been effectively partitioned and distributed for parallel processing, attention must then turn to how it is physically stored and accessed—bringing us to the choice of efficient storage formats.</p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor079"/>Efficient data storage formats</h1>
			<p>Choosing the right storage<a id="_idIndexMarker189"/> format can significantly impact data loading and processing speed.</p>
			<p>As an example, we can use <strong class="bold">Apache Parquet</strong> (<a href="https://parquet.apache.org/">https://parquet.apache.org/</a>), a columnar storage <a id="_idIndexMarker190"/>format that’s particularly efficient for large datasets.</p>
			<p>Here’s a table comparing different column formats and their characteristics for storing large language datasets:</p>
			<table id="table001-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Feature</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">CSV</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">JSON</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Apache Parquet</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Apache Arrow</strong></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Storage type</strong></p>
						</td>
						<td class="No-Table-Style">
							<p>Row-based</p>
						</td>
						<td class="No-Table-Style">
							<p>Row-based</p>
						</td>
						<td class="No-Table-Style">
							<p>Columnar</p>
						</td>
						<td class="No-Table-Style">
							<p>Columnar</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Compression</strong></p>
						</td>
						<td class="No-Table-Style">
							<p>Basic</p>
						</td>
						<td class="No-Table-Style">
							<p>Poor</p>
						</td>
						<td class="No-Table-Style">
							<p>Excellent</p>
						</td>
						<td class="No-Table-Style">
							<p>Excellent</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Query speed</strong></p>
						</td>
						<td class="No-Table-Style">
							<p>Slow</p>
						</td>
						<td class="No-Table-Style">
							<p>Slow</p>
						</td>
						<td class="No-Table-Style">
							<p>Fast</p>
						</td>
						<td class="No-Table-Style">
							<p>Very fast</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Nested structures</strong></p>
						</td>
						<td class="No-Table-Style">
							<p>No</p>
						</td>
						<td class="No-Table-Style">
							<p>Yes</p>
						</td>
						<td class="No-Table-Style">
							<p>Yes</p>
						</td>
						<td class="No-Table-Style">
							<p>Yes</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Schema support</strong></p>
						</td>
						<td class="No-Table-Style">
							<p>No</p>
						</td>
						<td class="No-Table-Style">
							<p>Limited</p>
						</td>
						<td class="No-Table-Style">
							<p>Yes</p>
						</td>
						<td class="No-Table-Style">
							<p>Yes</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Random access</strong></p>
						</td>
						<td class="No-Table-Style">
							<p>Poor</p>
						</td>
						<td class="No-Table-Style">
							<p>Poor</p>
						</td>
						<td class="No-Table-Style">
							<p>Good</p>
						</td>
						<td class="No-Table-Style">
							<p>Excellent</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Memory efficiency</strong></p>
						</td>
						<td class="No-Table-Style">
							<p>Poor</p>
						</td>
						<td class="No-Table-Style">
							<p>Poor</p>
						</td>
						<td class="No-Table-Style">
							<p>Good</p>
						</td>
						<td class="No-Table-Style">
							<p>Excellent</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Python integration</strong></p>
						</td>
						<td class="No-Table-Style">
							<p>Simple</p>
						</td>
						<td class="No-Table-Style">
							<p>Simple</p>
						</td>
						<td class="No-Table-Style">
							<p>Good (via PyArrow)</p>
						</td>
						<td class="No-Table-Style">
							<p>Native</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Typical </strong><strong class="bold">use case</strong></p>
						</td>
						<td class="No-Table-Style">
							<p>Small datasets</p>
						</td>
						<td class="No-Table-Style">
							<p>API responses</p>
						</td>
						<td class="No-Table-Style">
							<p>Large analytics</p>
						</td>
						<td class="No-Table-Style">
							<p>In-memory processing</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Loading speed</strong></p>
						</td>
						<td class="No-Table-Style">
							<p>Slow</p>
						</td>
						<td class="No-Table-Style">
							<p>Medium</p>
						</td>
						<td class="No-Table-Style">
							<p>Fast</p>
						</td>
						<td class="No-Table-Style">
							<p>Very fast</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">NLP </strong><strong class="bold">feature support</strong></p>
						</td>
						<td class="No-Table-Style">
							<p>Basic</p>
						</td>
						<td class="No-Table-Style">
							<p>Good</p>
						</td>
						<td class="No-Table-Style">
							<p>Excellent</p>
						</td>
						<td class="No-Table-Style">
							<p>Excellent</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Cross-platform</strong></p>
						</td>
						<td class="No-Table-Style">
							<p>Yes</p>
						</td>
						<td class="No-Table-Style">
							<p>Yes</p>
						</td>
						<td class="No-Table-Style">
							<p>Yes</p>
						</td>
						<td class="No-Table-Style">
							<p>Yes</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Metadata support</strong></p>
						</td>
						<td class="No-Table-Style">
							<p>No</p>
						</td>
						<td class="No-Table-Style">
							<p>Limited</p>
						</td>
						<td class="No-Table-Style">
							<p>Yes</p>
						</td>
						<td class="No-Table-Style">
							<p>Yes</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.1 – Characteristics of different column formats</p>
			<p>This table highlights why<a id="_idIndexMarker191"/> Parquet is often preferred for LLM datasets due to its columnar storage format, efficient compression, and strong support for complex data structures commonly found in NLP tasks.</p>
			<p>Here’s an example of how data is typically structured in Apache Parquet columns for an NLP dataset:</p>
			<table id="table002" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Column Name</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Data Type</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Example Values</strong></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>text_id</code></p>
						</td>
						<td class="No-Table-Style">
							<p>Integer</p>
						</td>
						<td class="No-Table-Style">
							<p><code>1, 2, </code><code>3, 4</code></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>Text</code></p>
						</td>
						<td class="No-Table-Style">
							<p>String</p>
						</td>
						<td class="No-Table-Style">
							<p><code>"This is sample text", "</code><code>Another example"</code></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>Tokens</code></p>
						</td>
						<td class="No-Table-Style">
							<p>List[String]</p>
						</td>
						<td class="No-Table-Style">
							<p><code>["This", "is", "sample", "text"], ["</code><code>Another", "example"]</code></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>Embeddings</code></p>
						</td>
						<td class="No-Table-Style">
							<p>List[Float]</p>
						</td>
						<td class="No-Table-Style">
							<p><code>[0.1, 0.2, 0.3], [0.4, </code><code>0.5, 0.6]</code></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>Metadata</code></p>
						</td>
						<td class="No-Table-Style">
							<p>Struct</p>
						</td>
						<td class="No-Table-Style">
							<p><code>{"lang": "en", "source": "web"}, {"lang": "fr", "</code><code>source": "news"}</code></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>Labels</code></p>
						</td>
						<td class="No-Table-Style">
							<p>Integer</p>
						</td>
						<td class="No-Table-Style">
							<p><code>1, 0, </code><code>1, 0</code></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>Timestamp</code></p>
						</td>
						<td class="No-Table-Style">
							<p>Timestamp</p>
						</td>
						<td class="No-Table-Style">
							<p><code>2024-01-01 10:30:00, </code><code>2024-01-01 10:31:00</code></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>language_score</code></p>
						</td>
						<td class="No-Table-Style">
							<p>Float</p>
						</td>
						<td class="No-Table-Style">
							<p><code>0.95, </code><code>0.87, 0.92</code></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>Entities</code></p>
						</td>
						<td class="No-Table-Style">
							<p>List[Struct]</p>
						</td>
						<td class="No-Table-Style">
							<p><code>[{"text": "Google", "type": "ORG"}, {"text": "New York", "</code><code>type": "LOC"}]</code></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>doc_stats</code></p>
						</td>
						<td class="No-Table-Style">
							<p>Struct</p>
						</td>
						<td class="No-Table-Style">
							<p><code>{"word_count": 150, "char_count": 750, "</code><code>sentence_count": 8}</code></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.2 – Structure of data in Apache Parquet columns</p>
			<p>Each column is stored separately and can be efficiently compressed and accessed independently, which is <a id="_idIndexMarker192"/>particularly useful for large-scale NLP processing.</p>
			<p>The following code snippet uses the PyArrow library to convert a dataset represented as a list of Python dictionaries into a Parquet file and read it back:</p>
			<pre class="source-code">
import pyarrow as pa
import pyarrow.parquet as pq
def convert_to_parquet(dataset, output_path):
    # Convert dataset to Arrow Table
    table = pa.Table.from_pydict(dataset[0])
    # Write to Parquet file
    pq.write_table(table, output_path)
def read_from_parquet(file_path):
    # Read Parquet file
    table = pq.read_table(file_path)
    # Convert back to dictionary
    return table.to_pydict()
#Usage
convert_to_parquet(large_dataset, "large_dataset.parquet")
loaded_dataset = read_from_parquet("large_dataset.parquet")</pre>			<p>In the preceding snippet, the <code>convert_to_parquet</code> function takes a dataset and an output file path, converts the first dictionary in the dataset to a PyArrow Table using <code>pa.Table.from_pydict</code>, and writes<a id="_idIndexMarker193"/> it to a Parquet file with <code>pq.write_table</code>. The <code>read_from_parquet</code> function reads a Parquet file from the specified path into a PyArrow Table using <code>pq.read_table</code> and then converts it back into a Python dictionary using <code>table.to_pydict</code>. In the usage example, a variable <code>large_dataset</code> is serialized to <code>"large_dataset.parquet"</code> and then deserialized back into <code>loaded_dataset</code>.</p>
			<p>Parquet offers several advantages for LLM datasets:</p>
			<ul>
				<li>Columnar storage for efficient querying</li>
				<li>Compression to reduce storage requirements</li>
				<li>Support for complex nested structures common in NLP data</li>
			</ul>
			<p>While previous sections have addressed methods for managing large-scale static datasets through sampling, distributed computation, and optimized storage strategies, these approaches assume a finite and well-defined corpus. However, training scenarios increasingly involve continuous inflow of data, such as user interactions, real-time telemetry, or evolving content streams. These dynamic contexts require a shift from traditional data pipelines to architectures capable of handling real-time ingestion and processing. The following section introduces streaming data processing as a necessary evolution for sustaining long-context, adaptive training regimes in LLMs.</p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor080"/>Streaming data processing for continuous LLM training</h1>
			<p>For scenarios where new data is <a id="_idIndexMarker194"/>constantly being generated, streaming processing allows for continuous model updates. Here’s an <a id="_idIndexMarker195"/>example <a id="_idIndexMarker196"/>using <strong class="bold">Apache Kafka </strong>(<a href="https://kafka.apache.org/">https://kafka.apache.org/</a>) and <strong class="bold">Faust</strong> (<a href="https://faust.readthedocs.io/en/latest/">https://faust.readthedocs.io/en/latest/</a>).</p>
			<p>Apache Kafka is a distributed streaming platform that serves as the backbone for building real-time data pipelines and streaming <a id="_idIndexMarker197"/>applications. It uses a <strong class="bold">publish-subscribe</strong> (<strong class="bold">pub-sub</strong>) model where data producers send messages to topics and consumers read from these topics, allowing for scalable, fault-tolerant data distribution across multiple brokers. When combined with async processing, these technologies enable systems to handle massive amounts of data in real time without blocking operations. Multiple brokers in Kafka provide redundancy and load balancing, ensuring high availability and throughput. This architecture is particularly useful in scenarios requiring real-time data processing, such as log aggregation, metrics collection, stream processing, and event sourcing.</p>
			<p>Faust, on the other hand, is a Python-based stream processing library designed to handle real-time data processing tasks by treating data as continuous streams of events. Similar to Kafka Streams but written in Python, Faust enables developers to build streaming applications that can process, transform, and analyze data in real time. It provides high-level abstractions <a id="_idIndexMarker198"/>for working with streams, making it easier to implement complex streaming workflows while maintaining the simplicity and expressiveness of Python. Faust internally uses modern Python features such as <code>async/await</code> and leverages the power of Python’s asyncio library for handling concurrent operations efficiently.</p>
			<p>The following code defines a simple real-time data processing application using Faust, a Python stream processing library built on top of Kafka. It demonstrates how to consume messages from a Kafka topic, apply preprocessing logic, and prepare data for downstream tasks such as training an LM:</p>
			<pre class="source-code">
import faust
class Text(faust.Record):
    content: str
app = faust.App('llm-training', broker='kafka://localhost:9092')
topic = app.topic('raw-text', value_type=Text)
@app.agent(topic)
async def process(stream):
    async for text in stream:
        processed_text = preprocess(text.content)
        # Here you would typically send the processed text to your LLM training pipeline
        print(f"Processed: {processed_text}")
if __name__ == '__main__':
    app.main()</pre>			<p>First, the code defines a <code>Text</code> class using <code>faust.Record</code> to represent incoming Kafka messages, which are expected to contain a single string field called <code>content</code>. The Faust application is then created with the <code>'llm-training'</code> identifier, and it connects to a local Kafka broker running at <code>kafka://localhost:9092</code>. The application subscribes to a topic named <code>'raw-text'</code>, with incoming messages deserialized into <code>Text</code> objects.</p>
			<p>The core processing logic is implemented in the <code>process</code> function, which is decorated with <code>@app.agent(topic)</code>, making it<a id="_idIndexMarker199"/> a Faust agent that processes events from the <code>raw-text</code> topic. The function asynchronously iterates over each message in the stream, applies a <code>preprocess</code> function to the <code>content</code> field, and prints the result. Although the code currently prints the processed text, in a real-world setup, this is where one would typically pass the output to a language model training pipeline or to further processing stages.</p>
			<p>Finally, the script includes<a id="_idIndexMarker200"/> a standard Python entry point to start the Faust application when the script is run directly. Note that the <code>preprocess</code> function is assumed to be defined elsewhere in the full implementation, as it is not included in the provided snippet.</p>
			<p>This setup allows you to continuously process incoming text data, which can then be used to update your LLM in real time or near real time. The <code>preprocess</code> function would contain your specific preprocessing logic.</p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor081"/>Memory-efficient data loading techniques</h1>
			<p>For datasets too large to fit in <a id="_idIndexMarker201"/>memory, we can use <strong class="bold">memory mapping</strong> or <strong class="bold">chunking</strong> techniques.</p>
			<p>Memory mapping<a id="_idIndexMarker202"/> leverages OS-level functionality to map large files directly into memory without loading the entire file. This <a id="_idIndexMarker203"/>enables random access to portions of the file, making it suitable for scenarios requiring frequent but <a id="_idIndexMarker204"/>non-sequential access. It is fast for large, structured datasets such as embeddings or tokenized text files but may have higher overhead for small, scattered reads.</p>
			<p>Chunking, on the other hand, divides data into smaller, sequentially processed chunks. This is effective for streaming large, sequentially accessed datasets (for example, text or logs) into memory-limited environments. While simpler and more portable, chunking may be slower for random access patterns compared to memory mapping.</p>
			<p>Here’s an example using NumPy’s <code>memmap</code> feature, which creates array-like objects that map to files on disk, permitting efficient read and write operations without loading the entire array into memory. The <code>memmap</code> feature leverages the operating system’s virtual memory capabilities to provide seamless array operations while minimizing memory usage:</p>
			<pre class="source-code">
import numpy as np
def create_memmap_dataset(dataset, output_file):
    # Determine the shape of the dataset
    num_samples = len(dataset)
    sample_shape = dataset[0]['input'].shape
    # Create a memory-mapped array
    mmap = np.memmap(
        output_file, dtype='float32', mode='w+',
        shape=(num_samples, *sample_shape)
    )
    # Write data to the memory-mapped array
    for i, sample in enumerate(dataset):
        mmap[i] = sample['input']
    # Flush to disk
    mmap.flush()
def load_memmap_dataset(file_path, shape):
    # Load the memory-mapped array
    return np.memmap(file_path, dtype='float32',
        mode='r', shape=shape)
#Usage
create_memmap_dataset(large_dataset, "large_dataset.mmap")
mmap_dataset = load_memmap_dataset(
    "large_dataset.mmap", shape=(len(large_dataset),
    *large_dataset[0]['input'].shape)
)</pre>			<p>This technique allows you to<a id="_idIndexMarker205"/> work with datasets larger than available <a id="_idIndexMarker206"/>RAM by keeping most of the data on disk and only loading the necessary portions into memory as needed.</p>
			<p>Here is an example of the chunking technique, which is particularly useful when working with large datasets that must be processed sequentially but do not fit into memory all at once. Unlike memory mapping, which allows random access, chunking explicitly loads and processes fixed-size blocks of data in sequence. This is a common pattern when dealing with large CSV files, text <a id="_idIndexMarker207"/>corpora, or streaming logs. In the <a id="_idIndexMarker208"/>following example, a large CSV file is processed in chunks using pandas, which internally reads blocks of rows into memory, minimizing the peak memory footprint:</p>
			<pre class="source-code">
import pandas as pd
def process_chunk(chunk):
    # Placeholder: process or transform the chunk here
    # For example, compute the mean of a column
    return chunk['value'].mean()
def process_large_csv(file_path, chunk_size=10000):
    results = []
    for chunk in pd.read_csv(file_path, chunksize=chunk_size):
        result = process_chunk(chunk)
        results.append(result)
    return results
# Usage
file_path = 'large_dataset.csv'
aggregated_results = process_large_csv(file_path)
print("Processed chunk-level results:", aggregated_results)</pre>			<p>In this example, the CSV file is read in blocks of 10,000 rows at a time. Each chunk is passed to a processing function, and intermediate results (in this case, the mean of a column named <code>'value'</code>) are stored for further<a id="_idIndexMarker209"/> aggregation or analysis. This approach is flexible and easily extended<a id="_idIndexMarker210"/> to tasks such as filtering, transformation, or writing chunked outputs to new files.</p>
			<p>Chunking is especially appropriate when data is accessed linearly and each chunk is independent. However, if random access to individual records or records across chunks is required, memory-mapping or indexed database solutions may <a id="_idTextAnchor082"/>be more efficient.</p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor083"/>Summary</h1>
			<p>In this section, we explored advanced techniques for managing and processing large datasets for LLM training. You learned about the challenges of large datasets, data sampling techniques, distributed processing, efficient storage formats, streaming processing, data sharding, and memory-efficient loading.</p>
			<p>These techniques are essential for scaling up LLM training to massive datasets while maintaining efficiency and data quality, each with its own contribution to processing large datasets for LLMs:</p>
			<ul>
				<li><strong class="bold">Data sampling techniques</strong>: They reduce the computational load by focusing on high-impact or representative data, enhancing efficiency and ensuring quality without processing the entire dataset</li>
				<li><strong class="bold">Distributed processing</strong>: Speeds up data preparation and training by parallelizing tasks across machines, enabling scalability for massive datasets</li>
				<li><strong class="bold">Efficient storage formats</strong>: They improve data retrieval speed and reduce storage size, streamlining access to large datasets and boosting I/O efficiency</li>
				<li><strong class="bold">Streaming processing</strong>: Minimizes memory usage by handling data incrementally, supporting real-time updates and efficient processing of continuous data streams</li>
				<li><strong class="bold">Data sharding</strong>: Balances workloads and reduces latency by splitting data into smaller chunks, enabling parallelism and seamless scaling</li>
				<li><strong class="bold">Memory-efficient loading</strong>: Limits memory usage by loading data in manageable portions, ensuring efficient processing of datasets that exceed memory capacity</li>
			</ul>
			<p>In the next chapter, we will introduce another pattern: data versioning for LLM development.</p>
		</div>
	</div></div></body></html>