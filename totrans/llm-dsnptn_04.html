<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer016">
			<h1 id="_idParaDest-62" class="chapter-number"><a id="_idTextAnchor072"/>4</h1>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor073"/>Handling Large Datasets for LLM Training</h1>
			<p>In this chapter, you’ll learn advanced techniques for managing and processing massive datasets essential for training state-of-the-art LLMs. We’ll explore the unique challenges posed by large-scale language datasets and provide you with practical solutions to <span class="No-Break">overcome them.</span></p>
			<p>The aim of this chapter is to equip you with the knowledge and tools to efficiently handle data at scale, enabling you to train more powerful and <span class="No-Break">effective LLMs.</span></p>
			<p>In this chapter, we’ll be covering the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Challenges of <span class="No-Break">large datasets</span></li>
				<li>Data <span class="No-Break">sampling techniques</span></li>
				<li>Distributed <span class="No-Break">data processing</span></li>
				<li>Data sharding and <span class="No-Break">parallelization strategies</span></li>
				<li>Efficient data <span class="No-Break">storage formats</span></li>
				<li>Streaming data processing for continuous <span class="No-Break">LLM training</span></li>
				<li>Memory-efficient data <span class="No-Break">loading techniques</span></li>
			</ul>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor074"/>Challenges of large datasets</h1>
			<p>Training LLMs requires enormous <a id="_idIndexMarker143"/>datasets, often in the terabytes or even petabytes range. This scale introduces <span class="No-Break">several challenges:</span></p>
			<ul>
				<li><strong class="bold">Storage requirements</strong>: Datasets can exceed the capacity of single machines, necessitating distributed <span class="No-Break">storage solutions.</span></li>
				<li><strong class="bold">Input/output (I/O) bottlenecks</strong>: Reading large volumes of data can become a significant bottleneck, limiting <span class="No-Break">training speed.</span></li>
				<li><strong class="bold">Preprocessing overhead</strong>: Tokenization and other preprocessing steps can be time-consuming at scale due to the computational overhead of processing large volumes of text data through multiple sequential operations. The challenge arises from having <a id="_idIndexMarker144"/>to perform multiple steps on each piece of text – tokenization, normalization, cleaning, language detection, and other transformations – multiplied across millions or billions of text samples. This process is inherently sequential (each step depends on the previous one), requires CPU/memory<a id="_idIndexMarker145"/> resources, and can involve complex operations such as <strong class="bold">regular expressions</strong> (<strong class="bold">regexes</strong>), dictionary lookups, and language-specific rules. When dealing with multilingual or code-mixed data, the complexity increases further as different language rules need to be applied, and additional steps such as script normalization or language detection are required for each text segment, making the preprocessing pipeline a significant <a id="_idIndexMarker146"/>bottleneck in large-scale <strong class="bold">natural language processing</strong> (<span class="No-Break"><strong class="bold">NLP</strong></span><span class="No-Break">) systems.</span></li>
				<li><strong class="bold">Memory constraints</strong>: Loading entire datasets into memory is often infeasible, requiring streaming or <span class="No-Break">batching approaches.</span></li>
				<li><strong class="bold">Data quality and diversity</strong>: Ensuring dataset quality and representativeness becomes more challenging as <span class="No-Break">size increases.</span></li>
			</ul>
			<p>To address these challenges, we need to employ sophisticated data-handling techniques. Let’s explore a Python<a id="_idIndexMarker147"/> implementation using the <strong class="bold">Datasets</strong> library from Hugging Face, which is designed to handle large-scale <span class="No-Break">datasets efficiently:</span></p>
			<pre class="source-code">
from datasets import load_dataset, Dataset
import psutil
def load_and_process_large_dataset(dataset_name, num_proc):
    # Load the dat<a id="_idTextAnchor075"/>aset
    dataset = load_dataset(dataset_name, streaming=True)
    # Define a preprocessing function
    def preprocess_function(examples):
        # Implement your preprocessing logic here
        return examples
    # Apply preprocessing in parallel
    processed_dataset = dataset.map(
        preprocess_function,
        batched=True,
        num_proc=num_proc,
        remove_columns=dataset["train"].column_names
    )
    return processed_dataset
#Determine the number of CPU cores for parallel processing
num_cores = psutil.cpu_count(logical=False)
#Load and process a large dataset (e.g., C4 dataset)
large_dataset = load_and_process_large_dataset("c4", 
    num_proc=num_cores)
#Print the first few examples
for example in large_dataset["train"].take(5):
    print(example)</pre>			<p>In this code, we use the Datasets library to efficiently load and process a large dataset (in this case, the <strong class="source-inline">C4</strong> dataset). The <strong class="source-inline">num_proc</strong> parameter specifies the number of processor cores to use for parallel processing in the dataset mapping operation. When preprocessing large datasets, using multiple CPU cores through parallel processing can significantly speed up the operation. For example, if <strong class="source-inline">num_proc=4</strong>, the preprocessing function will be executed on <a id="_idIndexMarker148"/>four processor cores simultaneously, processing different batches of data in parallel rather <span class="No-Break">than sequentially.</span></p>
			<p>To better understand the context in which large datasets are used, it is helpful to explore a specific example. One such dataset used in<a id="_idIndexMarker149"/> the preceding code snippet is the <strong class="bold">Colossal Clean Crawled Corpus</strong> (<strong class="bold">C4</strong>) dataset, which plays a significant role in training <span class="No-Break">modern LLMs.</span></p>
			<p>The <strong class="bold">C4</strong> dataset is a massive, cleaned web-crawled text corpus created by Google for training LLMs. Containing approximately <a id="_idIndexMarker150"/>750 GB of English-language text, C4 is derived from Common Crawl data and has undergone extensive filtering to remove duplicates, non-English content, and offensive material. It comes in several variants, including a standard cleaned version, an unfiltered version, and a subset focused on news-like content. While publicly available, accessing C4 requires some effort, typically through Google Cloud Storage or libraries such as Hugging Face datasets. Despite its cleaning process, C4 still has some limitations regarding content quality and potential biases, which researchers should consider when using it for model training. Nevertheless, it remains a valuable resource <a id="_idIndexMarker151"/>for NLP tasks and has been instrumental in training <a id="_idIndexMarker152"/>prominent models such as <strong class="bold">Text-to-Text Transfer Transformer</strong> (<strong class="bold">T5</strong>) and <strong class="bold">Language Model for Dialogue </strong><span class="No-Break"><strong class="bold">Applications</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">LaMDA</strong></span><span class="No-Break">).</span></p>
			<p>We employ streaming to avoid loading the entire dataset into memory at once. The <strong class="source-inline">num_proc</strong> parameter is set to the number of physical CPU cores to maximize parallel <span class="No-Break">processing efficiency.</span></p>
			<p>The <strong class="source-inline">preprocess_function</strong> function is<a id="_idIndexMarker153"/> where you implement dataset-specific preprocessing logic. This function is applied in parallel across the dataset, significantly speeding up preprocessing for <span class="No-Break">large datasets.</span></p>
			<p>You can also use a GPU for the task. See the following code example (keep in mind that while GPU-based preprocessing is particularly beneficial for operations such as tokenization and embedding <a id="_idIndexMarker154"/>generation, it may not significantly accelerate simpler <span class="No-Break">text manipulations):</span></p>
			<pre class="source-code">
import torch
from datasets import load_dataset
from torch.utils.data import DataLoader
from transformers import AutoTokenizer
def load_and_process_dataset(dataset_name, batch_size):
    dataset = load_dataset(dataset_name, streaming=True)
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    def preprocess(examples):
        return tokenizer(
            examples["text"], padding="max_length",
            truncation=True, return_tensors="pt"
        )
    def process_batch(batch):
        return {k: v.to(device) for k, v in preprocess(batch).items()}
    return DataLoader(
        dataset["train"].map(process_batch),
        batch_size=batch_size, num_workers=2,
        pin_memory=True
    )
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
dataloader = load_and_process_dataset("c4", batch_size=32)
for i, batch in enumerate(dataloader):
    if i &gt;= 5: break
    print(f"Batch {i}:", {k: v.shape for k, v in batch.items()})</pre>			<p>This code uses the PyTorch and Hugging Face libraries to process a dataset (for example, C4) with GPU acceleration. It employs a data loader for efficient batch processing, moves data to GPU memory, and uses a pre-trained tokenizer. The main GPU benefits come from parallel batch <a id="_idIndexMarker155"/>processing and GPU-accelerated tokenization. While this setup enables GPU usage, the most significant GPU advantages typically occur during model training or inference rather <span class="No-Break">than preprocessing.</span></p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor076"/>Data sampling techniques</h1>
			<p>Data sampling is a practical approach to <a id="_idIndexMarker156"/>reducing the size of large datasets without sacrificing representativeness. Several techniques exist, each with specific use cases and trade-offs. <strong class="bold">Random sampling</strong> selects <a id="_idIndexMarker157"/>data points uniformly at random from the dataset. It is simple and effective when the data is independently and identically distributed, but it may miss important subgroups if the data is imbalanced. <strong class="bold">Systematic sampling</strong> selects every <em class="italic">k</em>th item from a list after a random starting point. It is <a id="_idIndexMarker158"/>more structured than random sampling and can be useful when the data is ordered in a meaningful way, though it risks introducing bias if the ordering aligns with hidden periodic patterns. <strong class="bold">Reservoir sampling</strong> is designed for streaming of unknown-size datasets. It<a id="_idIndexMarker159"/> maintains a fixed-size sample while iterating through the data sequentially and ensures that every item has an equal probability of being included. This is particularly useful in online or incremental learning scenarios where data arrives in <span class="No-Break">continuous flows.</span></p>
			<p>Due to the length constraints of this chapter, we focus only on <strong class="bold">stratified sampling</strong>, a technique that preserves the proportional<a id="_idIndexMarker160"/> representation of subgroups within a dataset. It is especially suitable when certain attributes—such as label classes, sentence lengths, or metadata categories—are known to affect model performance and need to be maintained in the sampled subset. In NLP, text length is a common stratification variable, given its impact on model <span class="No-Break">input dynamics.</span></p>
			<p>The following implementation demonstrates how to apply stratified sampling based on text length. It divides the<a id="_idIndexMarker161"/> dataset into percentile-based strata and samples proportionally from each stratum to create a subset that retains the length distribution of the <span class="No-Break">full dataset:</span></p>
			<pre class="source-code">
import numpy as np
from datasets import Dataset
def stratified_length_sampling(
    dataset, num_samples, num_strata=10
):
    # Calculate text lengths
    lengths = [len(example['text']) for example in dataset]
    # Create strata based on text length
    strata_bounds = np.percentile(
        lengths, np.linspace(0, 100, num_strata + 1)
    )
    sampled_data = []
    for i in range(num_strata):
        stratum = [
            example for example in dataset
            if strata_bounds[i] &lt;= len(example['text']) &lt; \
                strata_bounds[i+1]
        ]
        stratum_samples = np.random.choice(
            stratum,
            size=num_samples // num_strata,
            replace=False
        )
        sampled_data.extend(stratum_samples)
    return Dataset.from_dict({
        key: [example[key] for example in sampled_data]
        for key in dataset[0].keys()
    })
#Usage
sampled_dataset = stratified_length_sampling(large_dataset, 
    num_samples=100000)</pre>			<p>This stratified sampling technique ensures that we maintain a representative distribution of text lengths in our sampled dataset. We use 10 strata (<strong class="source-inline">num_strata=10</strong>) to balance granularity and computational efficiency. Adjust this value based on your specific dataset characteristics and <span class="No-Break">sampling requirements.</span></p>
			<p>As datasets grow in size and <a id="_idIndexMarker162"/>complexity, single-machine processing becomes a bottleneck in both speed and scalability. Techniques such as data sampling offer partial relief, but they do not resolve the computational limitations inherent in centralized architectures. To address these constraints, the next section introduces distributed data processing, where computation is spread across multiple machines or nodes to improve throughput, reduce latency, and support parallel workflows required for large-scale LLM <span class="No-Break">training pipelines.</span></p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor077"/>Distributed data processing</h1>
			<p>For truly massive datasets, distributed processing becomes necessary. Here’s an example using <strong class="bold">Dask</strong>, a flexible library for <a id="_idIndexMarker163"/>parallel computing in <span class="No-Break">Python (</span><a href="https://www.dask.org/"><span class="No-Break">https://www.dask.org/</span></a><span class="No-Break">).</span></p>
			<p>Dask and Apache Spark <a id="_idIndexMarker164"/>are both distributed computing frameworks, but their main differences lie in their architecture and use cases. Spark is built around the concept of <strong class="bold">resilient distributed datasets</strong> (<strong class="bold">RDDs</strong>) and requires a cluster setup, making it ideal<a id="_idIndexMarker165"/> for large-scale production data processing. Dask, on the other hand, is designed to integrate seamlessly with the Python ecosystem and can scale from a single laptop to a cluster, using familiar APIs that mirror NumPy, pandas, and scikit-learn. While Spark excels at batch processing of massive datasets, Dask is more flexible for interactive computing and scientific workflows, particularly when working with Python-native libraries and when you need to scale up existing Python code with <span class="No-Break">minimal modifications.</span></p>
			<p>Let’s get back to <span class="No-Break">our code:</span></p>
			<pre class="source-code">
import dask.dataframe as dd
from dask.distributed import Client
def distributed_preprocessing(data_path, num_partitions):
    # Initialize Dask client
    client = Client()
    # Read the dataset into a Dask DataFrame
    df = dd.read_csv(data_path, blocksize="64MB")
    # Repartition the data for better distribution
    df = df.repartition(npartitions=num_partitions)
    # Define preprocessing function
    def preprocess(text):
        # Implement your preprocessing logic here
        return processed_text
    # Apply preprocessing in parallel
    df['processed_text'] = df['text'].map(preprocess)
    # Trigger computation and return results
    result = df.compute()
    client.close()
    return result
#Usage
processed_data = distributed_preprocessing(
    "path/to/large/dataset.csv", num_partitions=100
)</pre>			<p>In this example, we use Dask to distribute the preprocessing workload across multiple machines or cores. The <strong class="source-inline">num_partitions</strong> parameter (set to <strong class="source-inline">100</strong>) determines the level of parallelism and <a id="_idIndexMarker166"/>should be adjusted based on your available computational resources and <span class="No-Break">dataset size.</span></p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor078"/>Data sharding and parallelization strategies</h1>
			<p><strong class="bold">Data sharding</strong> refers to the<a id="_idIndexMarker167"/> technique of breaking up a large dataset into smaller, more manageable <a id="_idIndexMarker168"/>pieces, known as “shards,” which are<a id="_idIndexMarker169"/> then distributed across multiple machines or storage systems. Each shard can be processed independently, making it easier to handle large datasets, especially those that don’t fit into the memory of a single machine. This approach is widely used in machine learning to distribute the processing of large datasets, thereby allowing for the training of larger <a id="_idIndexMarker170"/>models or <span class="No-Break">faster computation.</span></p>
			<p>Data sharding enables more<a id="_idIndexMarker171"/> efficient use of computational resources as each shard can be processed independently, and the results can be <span class="No-Break">aggregated later.</span></p>
			<p>However, careful consideration must be given to ensuring that the sharding strategy maintains the integrity and representativeness of the data distribution across all shards to avoid biases or inconsistencies in the <span class="No-Break">trained model.</span></p>
			<p>Here’s an example of a <span class="No-Break">sharding strategy:</span></p>
			<pre class="source-code">
import hashlib
def shard_data(dataset, num_shards):
    shards = [[] for _ in range(num_shards)]
    for item in dataset:
        # Use a hash function to determine the shard
        shard_index = int(
            hashlib.md5(
                item['id'].encode()
            ).hexdigest(), 16
        ) % num_shards
        shards[shard_index].append(item)
    return shards
#Usage
sharded_data = shard_data(large_dataset, num_shards=10)</pre>			<p>This sharding strategy uses a hash function to distribute data items across shards. The <strong class="source-inline">num_shards</strong> parameter (set to <strong class="source-inline">10</strong>) should be adjusted based on your infrastructure and <span class="No-Break">parallelization needs.</span></p>
			<p>The <strong class="source-inline">shard_data</strong> function distributes<a id="_idIndexMarker172"/> items from a dataset into a specified number of shards by applying a consistent<a id="_idIndexMarker173"/> hashing scheme based on each item’s unique identifier. It initializes a list of empty lists, each representing a shard, and for every item in the input dataset, it calculates a shard index using the <strong class="bold">Message Digest Algorithm 5</strong> (<strong class="bold">MD5</strong>) hash of the item’s <strong class="source-inline">'id'</strong> field. The hash output is converted to an integer <a id="_idIndexMarker174"/>and taken modulo the number of shards to ensure uniform distribution across shards. This method guarantees that items with the same ID are consistently mapped to the same shard across executions, which is useful for tasks such as distributed storage or parallel processing where determinism and balance <span class="No-Break">are important.</span></p>
			<p>Sharding strategies are chosen based on the nature of the data and expected query patterns, with each approach offering distinct trade-offs in scalability, performance, <span class="No-Break">and complexity:</span></p>
			<ul>
				<li><strong class="bold">Hash sharding</strong>: Suitable for uniformly <a id="_idIndexMarker175"/>distributed data by mapping keys through a hash function to distribute load evenly <span class="No-Break">across shards</span></li>
				<li><strong class="bold">Range sharding</strong>: Effective for ordered<a id="_idIndexMarker176"/> datasets such as time-series logs, where each shard holds a contiguous range of <span class="No-Break">data values</span></li>
				<li><strong class="bold">Geographic sharding</strong>: Designed to <a id="_idIndexMarker177"/>optimize location-based queries by partitioning data according to <span class="No-Break">geographical regions</span></li>
				<li><strong class="bold">Key-value sharding</strong>: Enables <a id="_idIndexMarker178"/>manual control of hotspots by assigning specific key ranges or values to <span class="No-Break">defined shards</span></li>
				<li><strong class="bold">Directory-based sharding</strong>: Supports dynamic shard allocation using a lookup service to determine data placement, adapting to<a id="_idIndexMarker179"/> changes in <span class="No-Break">data distribution</span></li>
				<li><strong class="bold">Consistent hashing</strong>: Minimizes data<a id="_idIndexMarker180"/> movement when the number of shards changes, maintaining stability and reducing <span class="No-Break">rebalancing overhead</span></li>
				<li><strong class="bold">Round-robin sharding</strong>: Distributes <a id="_idIndexMarker181"/>data sequentially across shards, providing simplicity but poor performance for <span class="No-Break">range-based queries</span></li>
				<li><strong class="bold">Workload-based sharding</strong>: Balances <a id="_idIndexMarker182"/>access load by assigning high-traffic data to separate shards based on observed <span class="No-Break">query patterns</span></li>
				<li><strong class="bold">Composite sharding</strong>: Combines<a id="_idIndexMarker183"/> multiple strategies to support complex systems with diverse data types and <span class="No-Break">query needs</span></li>
				<li><strong class="bold">Tag-based sharding</strong>: Categorizes <a id="_idIndexMarker184"/>data based on labels such as user roles or data <a id="_idIndexMarker185"/>categories, supporting domain-specific <span class="No-Break">partitioning strategies</span></li>
			</ul>
			<p>For the preceding code <a id="_idIndexMarker186"/>block, we can also define the following function as the main orchestrator to process and <span class="No-Break">aggregate shards:</span></p>
			<pre class="source-code">
def process_with_sharding(
    dataset: List[Dict], num_shards: int
) -&gt; List[Dict]:
    # Step 1: Shard the data
    shards = shard_data(dataset, num_shards)
    # Step 2: Process shards in parallel
    with ProcessPoolExecutor(max_workers=num_shards) as executor:
        processed_shards = list(executor.map(process_shard, shards))
    # Step 3: Aggregate results
    aggregated_results = []
    for shard_results in processed_shards:
        aggregated_results.extend(shard_results)</pre>			<p>The <strong class="source-inline">process_with_sharding</strong> function takes a dataset represented as a list of dictionaries and divides it into a specified number of shards using the <strong class="source-inline">shard_data</strong> function. It then uses <strong class="source-inline">ProcessPoolExecutor</strong> with as many workers as shards to process each shard concurrently by applying the <strong class="source-inline">process_shard</strong> function in parallel. After all shards have been <a id="_idIndexMarker187"/>processed, it aggregates the individual results from each shard into a single list by<a id="_idIndexMarker188"/> iterating over the processed shards and extending the final result list with <span class="No-Break">their contents.</span></p>
			<p>Once data has been effectively partitioned and distributed for parallel processing, attention must then turn to how it is physically stored and accessed—bringing us to the choice of efficient <span class="No-Break">storage formats.</span></p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor079"/>Efficient data storage formats</h1>
			<p>Choosing the right storage<a id="_idIndexMarker189"/> format can significantly impact data loading and <span class="No-Break">processing speed.</span></p>
			<p>As an example, we can use <strong class="bold">Apache Parquet</strong> (<a href="https://parquet.apache.org/">https://parquet.apache.org/</a>), a columnar storage <a id="_idIndexMarker190"/>format that’s particularly efficient for <span class="No-Break">large datasets.</span></p>
			<p>Here’s a table comparing different column formats and their characteristics for storing large <span class="No-Break">language datasets:</span></p>
			<table id="table001-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Feature</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">CSV</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">JSON</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Apache Parquet</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Apache Arrow</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Storage type</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Row-based</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Row-based</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Columnar</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Columnar</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Compression</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Basic</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Poor</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Excellent</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Excellent</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Query speed</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Slow</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Slow</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Fast</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Very fast</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Nested structures</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">No</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Yes</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Yes</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Yes</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Schema support</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">No</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Limited</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Yes</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Yes</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Random access</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Poor</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Poor</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Good</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Excellent</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Memory efficiency</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Poor</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Poor</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Good</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Excellent</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Python integration</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Simple</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Simple</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Good (<span class="No-Break">via PyArrow)</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Native</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Typical </strong><span class="No-Break"><strong class="bold">use case</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Small datasets</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">API responses</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Large analytics</span></p>
						</td>
						<td class="No-Table-Style">
							<p>In-memory <span class="No-Break">processing</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Loading speed</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Slow</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Medium</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Fast</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Very fast</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">NLP </strong><span class="No-Break"><strong class="bold">feature support</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Basic</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Good</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Excellent</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Excellent</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Cross-platform</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Yes</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Yes</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Yes</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Yes</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Metadata support</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">No</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Limited</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Yes</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Yes</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.1 – Characteristics of different column formats</p>
			<p>This table highlights why<a id="_idIndexMarker191"/> Parquet is often preferred for LLM datasets due to its columnar storage format, efficient compression, and strong support for complex data structures commonly found in <span class="No-Break">NLP tasks.</span></p>
			<p>Here’s an example of how data is typically structured in Apache Parquet columns for an <span class="No-Break">NLP dataset:</span></p>
			<table id="table002" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Column Name</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Data Type</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Example Values</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">text_id</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Integer</span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1, 2, </strong><span class="No-Break"><strong class="source-inline">3, 4</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">Text</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">String</span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">"This is sample text", "</strong><span class="No-Break"><strong class="source-inline">Another example"</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">Tokens</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">List[String]</span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">["This", "is", "sample", "text"], ["</strong><span class="No-Break"><strong class="source-inline">Another", "example"]</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">Embeddings</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">List[Float]</span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">[0.1, 0.2, 0.3], [0.4, </strong><span class="No-Break"><strong class="source-inline">0.5, 0.6]</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">Metadata</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Struct</span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">{"lang": "en", "source": "web"}, {"lang": "fr", "</strong><span class="No-Break"><strong class="source-inline">source": "news"}</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">Labels</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Integer</span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1, 0, </strong><span class="No-Break"><strong class="source-inline">1, 0</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">Timestamp</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Timestamp</span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">2024-01-01 10:30:00, </strong><span class="No-Break"><strong class="source-inline">2024-01-01 10:31:00</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">language_score</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Float</span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">0.95, </strong><span class="No-Break"><strong class="source-inline">0.87, 0.92</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">Entities</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">List[Struct]</span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">[{"text": "Google", "type": "ORG"}, {"text": "New York", "</strong><span class="No-Break"><strong class="source-inline">type": "LOC"}]</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">doc_stats</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Struct</span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">{"word_count": 150, "char_count": 750, "</strong><span class="No-Break"><strong class="source-inline">sentence_count": 8}</strong></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.2 – Structure of data in Apache Parquet columns</p>
			<p>Each column is stored separately and can be efficiently compressed and accessed independently, which is <a id="_idIndexMarker192"/>particularly useful for large-scale <span class="No-Break">NLP processing.</span></p>
			<p>The following code snippet uses the PyArrow library to convert a dataset represented as a list of Python dictionaries into a Parquet file and read <span class="No-Break">it back:</span></p>
			<pre class="source-code">
import pyarrow as pa
import pyarrow.parquet as pq
def convert_to_parquet(dataset, output_path):
    # Convert dataset to Arrow Table
    table = pa.Table.from_pydict(dataset[0])
    # Write to Parquet file
    pq.write_table(table, output_path)
def read_from_parquet(file_path):
    # Read Parquet file
    table = pq.read_table(file_path)
    # Convert back to dictionary
    return table.to_pydict()
#Usage
convert_to_parquet(large_dataset, "large_dataset.parquet")
loaded_dataset = read_from_parquet("large_dataset.parquet")</pre>			<p>In the preceding snippet, the <strong class="source-inline">convert_to_parquet</strong> function takes a dataset and an output file path, converts the first dictionary in the dataset to a PyArrow Table using <strong class="source-inline">pa.Table.from_pydict</strong>, and writes<a id="_idIndexMarker193"/> it to a Parquet file with <strong class="source-inline">pq.write_table</strong>. The <strong class="source-inline">read_from_parquet</strong> function reads a Parquet file from the specified path into a PyArrow Table using <strong class="source-inline">pq.read_table</strong> and then converts it back into a Python dictionary using <strong class="source-inline">table.to_pydict</strong>. In the usage example, a variable <strong class="source-inline">large_dataset</strong> is serialized to <strong class="source-inline">"large_dataset.parquet"</strong> and then deserialized back <span class="No-Break">into </span><span class="No-Break"><strong class="source-inline">loaded_dataset</strong></span><span class="No-Break">.</span></p>
			<p>Parquet offers several advantages for <span class="No-Break">LLM datasets:</span></p>
			<ul>
				<li>Columnar storage for <span class="No-Break">efficient querying</span></li>
				<li>Compression to reduce <span class="No-Break">storage requirements</span></li>
				<li>Support for complex nested structures common in <span class="No-Break">NLP data</span></li>
			</ul>
			<p>While previous sections have addressed methods for managing large-scale static datasets through sampling, distributed computation, and optimized storage strategies, these approaches assume a finite and well-defined corpus. However, training scenarios increasingly involve continuous inflow of data, such as user interactions, real-time telemetry, or evolving content streams. These dynamic contexts require a shift from traditional data pipelines to architectures capable of handling real-time ingestion and processing. The following section introduces streaming data processing as a necessary evolution for sustaining long-context, adaptive training regimes <span class="No-Break">in LLMs.</span></p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor080"/>Streaming data processing for continuous LLM training</h1>
			<p>For scenarios where new data is <a id="_idIndexMarker194"/>constantly being generated, streaming processing allows for continuous model updates. Here’s an <a id="_idIndexMarker195"/>example <a id="_idIndexMarker196"/>using <strong class="bold">Apache Kafka </strong>(<a href="https://kafka.apache.org/">https://kafka.apache.org/</a>) and <span class="No-Break"><strong class="bold">Faust</strong></span><span class="No-Break"> (</span><a href="https://faust.readthedocs.io/en/latest/"><span class="No-Break">https://faust.readthedocs.io/en/latest/</span></a><span class="No-Break">).</span></p>
			<p>Apache Kafka is a distributed streaming platform that serves as the backbone for building real-time data pipelines and streaming <a id="_idIndexMarker197"/>applications. It uses a <strong class="bold">publish-subscribe</strong> (<strong class="bold">pub-sub</strong>) model where data producers send messages to topics and consumers read from these topics, allowing for scalable, fault-tolerant data distribution across multiple brokers. When combined with async processing, these technologies enable systems to handle massive amounts of data in real time without blocking operations. Multiple brokers in Kafka provide redundancy and load balancing, ensuring high availability and throughput. This architecture is particularly useful in scenarios requiring real-time data processing, such as log aggregation, metrics collection, stream processing, and <span class="No-Break">event sourcing.</span></p>
			<p>Faust, on the other hand, is a Python-based stream processing library designed to handle real-time data processing tasks by treating data as continuous streams of events. Similar to Kafka Streams but written in Python, Faust enables developers to build streaming applications that can process, transform, and analyze data in real time. It provides high-level abstractions <a id="_idIndexMarker198"/>for working with streams, making it easier to implement complex streaming workflows while maintaining the simplicity and expressiveness of Python. Faust internally uses modern Python features such as <strong class="source-inline">async/await</strong> and leverages the power of Python’s asyncio library for handling concurrent <span class="No-Break">operations efficiently.</span></p>
			<p>The following code defines a simple real-time data processing application using Faust, a Python stream processing library built on top of Kafka. It demonstrates how to consume messages from a Kafka topic, apply preprocessing logic, and prepare data for downstream tasks such as training <span class="No-Break">an LM:</span></p>
			<pre class="source-code">
import faust
class Text(faust.Record):
    content: str
app = faust.App('llm-training', broker='kafka://localhost:9092')
topic = app.topic('raw-text', value_type=Text)
@app.agent(topic)
async def process(stream):
    async for text in stream:
        processed_text = preprocess(text.content)
        # Here you would typically send the processed text to your LLM training pipeline
        print(f"Processed: {processed_text}")
if __name__ == '__main__':
    app.main()</pre>			<p>First, the code defines a <strong class="source-inline">Text</strong> class using <strong class="source-inline">faust.Record</strong> to represent incoming Kafka messages, which are expected to contain a single string field called <strong class="source-inline">content</strong>. The Faust application is then created with the <strong class="source-inline">'llm-training'</strong> identifier, and it connects to a local Kafka broker running at <strong class="source-inline">kafka://localhost:9092</strong>. The application subscribes to a topic named <strong class="source-inline">'raw-text'</strong>, with incoming messages deserialized into <span class="No-Break"><strong class="source-inline">Text</strong></span><span class="No-Break"> objects.</span></p>
			<p>The core processing logic is implemented in the <strong class="source-inline">process</strong> function, which is decorated with <strong class="source-inline">@app.agent(topic)</strong>, making it<a id="_idIndexMarker199"/> a Faust agent that processes events from the <strong class="source-inline">raw-text</strong> topic. The function asynchronously iterates over each message in the stream, applies a <strong class="source-inline">preprocess</strong> function to the <strong class="source-inline">content</strong> field, and prints the result. Although the code currently prints the processed text, in a real-world setup, this is where one would typically pass the output to a language model training pipeline or to further <span class="No-Break">processing stages.</span></p>
			<p>Finally, the script includes<a id="_idIndexMarker200"/> a standard Python entry point to start the Faust application when the script is run directly. Note that the <strong class="source-inline">preprocess</strong> function is assumed to be defined elsewhere in the full implementation, as it is not included in the <span class="No-Break">provided snippet.</span></p>
			<p>This setup allows you to continuously process incoming text data, which can then be used to update your LLM in real time or near real time. The <strong class="source-inline">preprocess</strong> function would contain your specific <span class="No-Break">preprocessing logic.</span></p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor081"/>Memory-efficient data loading techniques</h1>
			<p>For datasets too large to fit in <a id="_idIndexMarker201"/>memory, we can use <strong class="bold">memory mapping</strong> or <span class="No-Break"><strong class="bold">chunking</strong></span><span class="No-Break"> techniques.</span></p>
			<p>Memory mapping<a id="_idIndexMarker202"/> leverages OS-level functionality to map large files directly into memory without loading the entire file. This <a id="_idIndexMarker203"/>enables random access to portions of the file, making it suitable for scenarios requiring frequent but <a id="_idIndexMarker204"/>non-sequential access. It is fast for large, structured datasets such as embeddings or tokenized text files but may have higher overhead for small, <span class="No-Break">scattered reads.</span></p>
			<p>Chunking, on the other hand, divides data into smaller, sequentially processed chunks. This is effective for streaming large, sequentially accessed datasets (for example, text or logs) into memory-limited environments. While simpler and more portable, chunking may be slower for random access patterns compared to <span class="No-Break">memory mapping.</span></p>
			<p>Here’s an example using NumPy’s <strong class="source-inline">memmap</strong> feature, which creates array-like objects that map to files on disk, permitting efficient read and write operations without loading the entire array into memory. The <strong class="source-inline">memmap</strong> feature leverages the operating system’s virtual memory capabilities to provide seamless array operations while minimizing <span class="No-Break">memory usage:</span></p>
			<pre class="source-code">
import numpy as np
def create_memmap_dataset(dataset, output_file):
    # Determine the shape of the dataset
    num_samples = len(dataset)
    sample_shape = dataset[0]['input'].shape
    # Create a memory-mapped array
    mmap = np.memmap(
        output_file, dtype='float32', mode='w+',
        shape=(num_samples, *sample_shape)
    )
    # Write data to the memory-mapped array
    for i, sample in enumerate(dataset):
        mmap[i] = sample['input']
    # Flush to disk
    mmap.flush()
def load_memmap_dataset(file_path, shape):
    # Load the memory-mapped array
    return np.memmap(file_path, dtype='float32',
        mode='r', shape=shape)
#Usage
create_memmap_dataset(large_dataset, "large_dataset.mmap")
mmap_dataset = load_memmap_dataset(
    "large_dataset.mmap", shape=(len(large_dataset),
    *large_dataset[0]['input'].shape)
)</pre>			<p>This technique allows you to<a id="_idIndexMarker205"/> work with datasets larger than available <a id="_idIndexMarker206"/>RAM by keeping most of the data on disk and only loading the necessary portions into memory <span class="No-Break">as needed.</span></p>
			<p>Here is an example of the chunking technique, which is particularly useful when working with large datasets that must be processed sequentially but do not fit into memory all at once. Unlike memory mapping, which allows random access, chunking explicitly loads and processes fixed-size blocks of data in sequence. This is a common pattern when dealing with large CSV files, text <a id="_idIndexMarker207"/>corpora, or streaming logs. In the <a id="_idIndexMarker208"/>following example, a large CSV file is processed in chunks using pandas, which internally reads blocks of rows into memory, minimizing the peak <span class="No-Break">memory footprint:</span></p>
			<pre class="source-code">
import pandas as pd
def process_chunk(chunk):
    # Placeholder: process or transform the chunk here
    # For example, compute the mean of a column
    return chunk['value'].mean()
def process_large_csv(file_path, chunk_size=10000):
    results = []
    for chunk in pd.read_csv(file_path, chunksize=chunk_size):
        result = process_chunk(chunk)
        results.append(result)
    return results
# Usage
file_path = 'large_dataset.csv'
aggregated_results = process_large_csv(file_path)
print("Processed chunk-level results:", aggregated_results)</pre>			<p>In this example, the CSV file is read in blocks of 10,000 rows at a time. Each chunk is passed to a processing function, and intermediate results (in this case, the mean of a column named <strong class="source-inline">'value'</strong>) are stored for further<a id="_idIndexMarker209"/> aggregation or analysis. This approach is flexible and easily extended<a id="_idIndexMarker210"/> to tasks such as filtering, transformation, or writing chunked outputs to <span class="No-Break">new files.</span></p>
			<p>Chunking is especially appropriate when data is accessed linearly and each chunk is independent. However, if random access to individual records or records across chunks is required, memory-mapping or indexed database solutions may <a id="_idTextAnchor082"/>be <span class="No-Break">more efficient.</span></p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor083"/>Summary</h1>
			<p>In this section, we explored advanced techniques for managing and processing large datasets for LLM training. You learned about the challenges of large datasets, data sampling techniques, distributed processing, efficient storage formats, streaming processing, data sharding, and <span class="No-Break">memory-efficient loading.</span></p>
			<p>These techniques are essential for scaling up LLM training to massive datasets while maintaining efficiency and data quality, each with its own contribution to processing large datasets <span class="No-Break">for LLMs:</span></p>
			<ul>
				<li><strong class="bold">Data sampling techniques</strong>: They reduce the computational load by focusing on high-impact or representative data, enhancing efficiency and ensuring quality without processing the <span class="No-Break">entire dataset</span></li>
				<li><strong class="bold">Distributed processing</strong>: Speeds up data preparation and training by parallelizing tasks across machines, enabling scalability for <span class="No-Break">massive datasets</span></li>
				<li><strong class="bold">Efficient storage formats</strong>: They improve data retrieval speed and reduce storage size, streamlining access to large datasets and boosting <span class="No-Break">I/O efficiency</span></li>
				<li><strong class="bold">Streaming processing</strong>: Minimizes memory usage by handling data incrementally, supporting real-time updates and efficient processing of continuous <span class="No-Break">data streams</span></li>
				<li><strong class="bold">Data sharding</strong>: Balances workloads and reduces latency by splitting data into smaller chunks, enabling parallelism and <span class="No-Break">seamless scaling</span></li>
				<li><strong class="bold">Memory-efficient loading</strong>: Limits memory usage by loading data in manageable portions, ensuring efficient processing of datasets that exceed <span class="No-Break">memory capacity</span></li>
			</ul>
			<p>In the next chapter, we will introduce another pattern: data versioning for <span class="No-Break">LLM development.</span></p>
		</div>
	</div></div></body></html>