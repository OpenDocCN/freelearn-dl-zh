["```py\n %pip install sentence_transformers -q --user\n```", "```py\n import numpy as npfrom sentence_transformers import SentenceTransformer\n```", "```py\n model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n```", "```py\n sentence = ['This blanket has such a cozy temperature for me!', 'I am so much warmer and snug using this spread!', 'Taylor Swift was 34 years old in 2024.']\n```", "```py\n embedding = model.encode(sentence)\nprint(embedding)\nembedding.shape\n```", "```py\n [[-0.5129604   0.6386722   0.3116684  ... -0.5178649  -0.3977838  0.2960762 ][-0.07027415  0.23834501  0.44659805 ... -0.38965416  0.20492953  0.4301296 ][ 0.5601178  -0.96016043  0.48343912 ... -0.36059788  1.0021329  -0.5214774 ]]\n(3, 384)\n```", "```py\n def euclidean_distance(vec1, vec2):\n    return np.linalg.norm(vec1 - vec2)\n```", "```py\n print(\"Euclidean Distance: Review 1 vs Review 2:\",\n    euclidean_distance(embedding[0], embedding[1]))\nprint(\"Euclidean Distance: Review 1 vs Random Comment:\",\n    euclidean_distance(embedding[0], embedding[2]))\nprint(\"Euclidean Distance: Review 2 vs Random Comment:\",\n    euclidean_distance(embedding[1], embedding[2]))\nRunning this cell gives us this output:\nEuclidean Distance: Review 1 vs Review 2: 4.6202903\nEuclidean Distance: Review 1 vs Random Comment: 7.313547\nEuclidean Distance: Review 2 vs Random Comment: 6.3389034\n```", "```py\n print(\"Dot Product: Review 1 vs Review 2:\",\n    np.dot(embedding[0], embedding[1]))\nprint(\"Dot Product: Review 1 vs Random Comment:\",\n    np.dot(embedding[0], embedding[2]))\nprint(\"Dot Product: Review 2 vs Random Comment:\",\n    np.dot(embedding[1], embedding[2]))\n```", "```py\n Dot Product: Review 1 vs Review 2: 12.270497\nDot Product: Review 1 vs Random Comment: -0.7654616\nDot Product: Review 2 vs Random Comment: 0.95240986\n```", "```py\n def cosine_distance(vec1,vec2):\n    cosine = 1 - abs((np.dot(vec1,vec2)/(\n        np.linalg.norm(vec1)*np.linalg.norm(vec2))))\n    return cosine\n```", "```py\n print(\"Cosine Distance: Review 1 vs Review 2:\",\n    cosine_distance(embedding[0], embedding[1]))\nprint(\"Cosine Distance: Review 1 vs Random Comment:\",\n    cosine_distance(embedding[0], embedding[2]))\nprint(\"Cosine Distance: Review 2 vs Random Comment:\",\n    cosine_distance(embedding[1], embedding[2]))\n```", "```py\n Cosine Distance: Review 1 vs Review 2: 0.4523802399635315\nCosine Distance: Review 1 vs Random Comment: 0.970455639064312\nCosine Distance: Review 2 vs Random Comment: 0.9542623348534107\n```", "```py\n %pip install beautifulsoup4\n```", "```py\n %pip install PyPDF2 -q –user\n%pip install rank_bm25\n```", "```py\n from langchain_community.document_loaders import WebBaseLoader\nimport bs4\nfrom langchain_experimental.text_splitter import SemanticChunker\n```", "```py\n from PyPDF2 import PdfReader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.docstore.document import Document\nfrom langchain_community.retrievers import BM25Retriever\n```", "```py\n loader = WebBaseLoader(\n    web_paths=(\"https://kbourne.github.io/chapter1.html\",),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            class_=(\"post-content\", \"post-title\",\n                \"post-header\")\n        )\n    ),\n)\ndocs = loader.load()\n```", "```py\n pdf_path = \"google-2023-environmental-report.pdf\"\ncollection_name = \"google_environmental_report\"\nstr_output_parser = StrOutputParser()\n```", "```py\n pdf_reader = PdfReader(pdf_path)\ntext = \"\"\nfor page in pdf_reader.pages:\n    text += page.extract_text()\n```", "```py\n text_splitter = SemanticChunker(OpenAIEmbeddings())\nsplits = text_splitter.split_documents(docs)\n```", "```py\n character_splitter = RecursiveCharacterTextSplitter(\n    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n    chunk_size=1000,\n    chunk_overlap=200\n)\nsplits = character_splitter.split_text(text)\n```", "```py\n documents = [Document(page_content=text, metadata={\n    \"id\": str(i)}) for i, text in enumerate(splits)]\n```", "```py\n vectorstore = Chroma.from_documents(\n    documents=splits,embedding=OpenAIEmbeddings())\nretriever = vectorstore.as_retriever()\n```", "```py\n chroma_client = chromadb.Client()\nvectorstore = Chroma.from_documents(\n    documents=documents,\n    embedding=embedding_function,\n    collection_name=collection_name,\n    client=chroma_client\n)\ndense_retriever = vectorstore.as_retriever(\n    search_kwargs={\"k\": 10})\nsparse_retriever = BM25Retriever.from_documents(\n    documents, k=10)\n```", "```py\n def hybrid_search(query, k=10, dense_weight=0.5,\n    sparse_weight=0.5):\n```", "```py\n dense_docs = dense_retriever.get_relevant_documents(\n        query)[:k]\n    dense_doc_ids = [doc.metadata[\n        'id'] for doc in dense_docs]\n    print(\"\\nCompare IDs:\")\n    print(\"dense IDs: \", dense_doc_ids)\n    sparse_docs = sparse_retriever.get_relevant_documents(\n        query)[:k]\n    sparse_doc_ids = [doc.metadata[\n        'id'] for doc in sparse_docs]\n    print(\"sparse IDs: \", sparse_doc_ids)\n    all_doc_ids = list(set(dense_doc_ids + sparse_doc_ids))\n    dense_reciprocal_ranks = {\n        doc_id: 0.0 for doc_id in all_doc_ids}\n    sparse_reciprocal_ranks = {\n        doc_id: 0.0 for doc_id in all_doc_ids}\n```", "```py\n for i, doc_id in enumerate(dense_doc_ids):\n    dense_reciprocal_ranks[doc_id] = 1.0 / (i + 1)\nfor i, doc_id in enumerate(sparse_doc_ids):\n    sparse_reciprocal_ranks[doc_id] = 1.0 / (i + 1)\n```", "```py\n combined_reciprocal_ranks = {doc_id:\n    0.0 for doc_id in all_doc_ids}\nfor doc_id in all_doc_ids:\n   combined_reciprocal_ranks[doc_id] = dense_weight *\n       dense_reciprocal_ranks[doc_id] + sparse_weight *\n       sparse_reciprocal_ranks[doc_id]\n```", "```py\n sorted_doc_ids = sorted(all_doc_ids, key=lambda doc_id:\n    combined_reciprocal_ranks[doc_id], reverse=True)\n```", "```py\n sorted_docs = []\n    all_docs = dense_docs + sparse_docs\n    for doc_id in sorted_doc_ids:\n        matching_docs = [\n            doc for doc in all_docs if doc.metadata[\n                'id'] == doc_id]\n            if matching_docs:\n                doc = matching_docs[0]\n                doc.metadata['score'] =\n                    combined_reciprocal_ranks[doc_id]\n                doc.metadata['rank'] =\n                    sorted_doc_ids.index(doc_id) + 1\n                if len(matching_docs) > 1:\n                    doc.metadata['retriever'] = 'both'\n                elif doc in dense_docs:\n                    doc.metadata['retriever'] = 'dense'\n            else:\n                 doc.metadata['retriever'] = 'sparse'\n                 sorted_docs.append(doc)\n```", "```py\n return sorted_docs[:k]\n```", "```py\n rag_chain_with_source = RunnableParallel(\n    {\"context\": hybrid_search,\n     \"question\": RunnablePassthrough()}\n).assign(answer=rag_chain_from_docs)\n```", "```py\n user_query = \"What are Google's environmental initiatives?\" result = rag_chain_with_source.invoke(user_query)\nrelevance_score = result['answer']['relevance_score']\nfinal_answer = result['answer']['final_answer']\nretrieved_docs = result['context']\nprint(f\"\\nOriginal Question: {user_query}\\n\")\nprint(f\"Relevance Score: {relevance_score}\\n\")\nprint(f\"Final Answer:\\n{final_answer}\\n\\n\")\nprint(\"Retrieved Documents:\")\nfor i, doc in enumerate(retrieved_docs, start=1):\n    doc_id = doc.metadata['id']\n    doc_score = doc.metadata.get('score', 'N/A')\n    doc_rank = doc.metadata.get('rank', 'N/A')\n    doc_retriever = doc.metadata.get('retriever', 'N/A')\n    print(f\"Document {i}: Document ID: {doc_id}\n        Score: {doc_score} Rank: {doc_rank}\n        Retriever: {doc_retriever}\\n\")\n    print(f\"Content:\\n{doc.page_content}\\n\")\n```", "```py\n Compare IDs:\ndense IDs:  ['451', '12', '311', '344', '13', '115', '67', '346', '66', '262']\nsparse IDs:  ['150', '309', '298', '311', '328', '415', '139', '432', '91', '22']\nOriginal Question: What are Google's environmental initiatives? Relevance Score: 5\nFinal Answer:\nGoogle's environmental initiatives include partnering with suppliers to reduce energy consumption and GHG emissions, engaging with suppliers to report and manage emissions, empowering individuals to take action through sustainability features in products, working together with partners and customers to reduce carbon emissions, operating sustainably at their campuses, focusing on net-zero carbon energy, water stewardship, circular economy practices, and supporting various environmental projects and initiatives such as the iMasons Climate Accord, ReFED, and The Nature Conservancy. They also work on sustainable consumption of public goods and engage with coalitions and sustainability initiatives to promote environmental sustainability. Retrieved Documents: <st c=\"41555\">Document 1: Document ID: 150 Score: 0.5 Rank: 1 Retriever: sparse</st> Content: sustainability, and we're partnering with them… <st c=\"41678\">Document 2: Document ID: 451 Score: 0.5 Rank: 2 Retriever: dense</st> Content: Empowering individuals: A parking lot full of electric vehicles lined up outside a Google office… <st c=\"41850\">Document 3: Document ID: 311 Score: 0.29166666666666663 Rank: 3 Retriever: both</st> Content: In 2022, we audited a subset of our suppliers to verify compliance for the following environmental…\n```", "```py\n Google's environmental initiatives include empowering individuals to take action, working together with partners and customers, operating sustainably, achieving net-zero carbon emissions, focusing on water stewardship, and promoting a circular economy. They have reached a goal to help 1 billion people make more sustainable choices through their products and aim to collectively reduce 1 gigaton of carbon equivalent emissions annually by 2030\\. Google also audits suppliers for compliance with environmental criteria and is involved in public policy and advocacy efforts. Additionally, Google is a founding member of the iMasons Climate Accord, provided funding for the ReFED Catalytic Grant Fund to address food waste, and supported projects with The Nature Conservancy to promote reforestation and stop deforestation.\n```", "```py\n Google's environmental initiatives include partnering with suppliers to reduce energy consumption and GHG emissions, engaging with suppliers to report and manage emissions, empowering individuals to take action through sustainability features in products, working together with partners and customers to reduce carbon emissions, operating sustainably at their campuses, focusing on net-zero carbon energy, water stewardship, circular economy practices, and supporting various environmental projects and initiatives such as the iMasons Climate Accord, ReFED, and The Nature Conservancy.\n```", "```py\n Google's environmental initiatives include empowering individuals to take action, working together with partners and customers, operating sustainably, achieving net-zero carbon emissions, focusing on water stewardship, and promoting a circular economy.\n```", "```py\n They have reached a goal to help 1 billion people make more sustainable choices through their products and aim to collectively reduce 1 gigaton of carbon equivalent emissions annually by 2030.\n```", "```py\n They also work on sustainable consumption of public goods and engage with coalitions and sustainability initiatives to promote environmental sustainability.\n```", "```py\n from langchain.retrievers import EnsembleRetriever\n```", "```py\n dense_documents = [Document(page_content=text,\n    metadata={\"id\": str(i), \"source\": \"dense\"}) for i,\n    text in enumerate(splits)]\nsparse_documents = [Document(page_content=text,\n    metadata={\"id\": str(i), \"source\": \"sparse\"}) for i,\n    text in enumerate(splits)]\n```", "```py\n ensemble_retriever = EnsembleRetriever(retrievers=[\n    dense_retriever, sparse_retriever], weights=[0.5, 0.5],\n    c=0)\n```", "```py\n def hybrid_search(query, k=10, dense_weight=0.5,\n    sparse_weight=0.5):\n```", "```py\n rag_chain_with_source = RunnableParallel(\n    {\"context\": ensemble_retriever,\n     \"question\": RunnablePassthrough()}\n).assign(answer=rag_chain_from_docs)\n```", "```py\n user_query = \"What are Google's environmental initiatives?\" result = rag_chain_with_source.invoke(user_query)\nrelevance_score = result['answer']['relevance_score']\nfinal_answer = result['answer']['final_answer']\nretrieved_docs = result['context']\nprint(f\"Original Question: {user_query}\\n\")\nprint(f\"Relevance Score: {relevance_score}\\n\")\nprint(f\"Final Answer:\\n{final_answer}\\n\\n\")\nprint(\"Retrieved Documents:\")\nfor i, doc in enumerate(retrieved_docs, start=1):\n    print(f\"Document {i}: Document ID: {doc.metadata['id']}\n        source: {doc.metadata['source']}\")\n    print(f\"Content:\\n{doc.page_content}\\n\")\n```", "```py\n Original Question: What are Google's environmental initiatives? Relevance Score: 5\nFinal Answer:\nGoogle's environmental initiatives include being a founding member of the iMasons Climate Accord, providing funding for the ReFED Catalytic Grant Fund to address food waste, supporting projects with The Nature Conservancy for reforestation and deforestation prevention, engaging with suppliers to reduce energy consumption and emissions, auditing suppliers for environmental compliance, addressing climate-related risks, advocating for sustainable consumption of public goods, engaging with coalitions like the RE-Source Platform, and working on improving data center efficiency. Retrieved Documents: <st c=\"50220\">Document 1: Document ID: 344 source: dense</st> Content:\niMasons Climate AccordGoogle is a founding member and part… <st c=\"50332\">Document 2: Document ID: 150 source: sparse</st> Content:\nsustainability, and we're partnering with them to develop decarbonization roadmaps… <st c=\"50469\">Document 3: Document ID: 309 source: dense</st> Content:\nthat enable us to ensure that those we partner with are responsible environmental stewards…\n```"]