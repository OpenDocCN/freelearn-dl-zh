- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fine-Tuning – Building Domain-Specific LLM Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In developing ChatGPT-based applications, ensuring the model’s precision, relevance,
    and alignment to its intended purpose is paramount. As we navigate the intricacies
    of this technology, it becomes evident that a one-size-fits-all approach doesn’t
    suffice. Hence, customizing the model becomes necessary to adapt to certain specialized
    domains, such as medicine, biotechnology, legal, and others. This chapter delves
    deep into model customization for domain-specific applications via fine-tuning
    and **parameter-efficient fine-tuning** (**PEFT**). But how do we evaluate that
    our refinements truly hit the mark? How do we know that they align with human
    values? Through rigorous evaluation metrics and benchmarking. By understanding
    and applying these pivotal processes, we not only bring out the best in ChatGPT
    but also adhere closely to the vision of this book: generative AI for cloud solutions.
    We must ensure it’s not just smart but also context-aware, effective, honest,
    safe, and resonant with its user’s needs. Hallucinations in **large language models**
    (**LLMs**) refer to generating factually incorrect or nonsensical information
    as if it were true. To reduce problems such as hallucinations, which can have
    a detrimental impact on society, we will discuss three important techniques in
    this book: fine-tuning, **retrieval-augmented generation** (**RAG**), and prompt
    engineering. While this chapter focuses on fine-tuning, we will discuss RAG and
    prompt engineering in the subsequent chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following main topics in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What is fine-tuning and why does it matter?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques for fine-tuning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reinforcement learning from human feedback** (**RLHF**) – aligning models
    with human values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to evaluate fine-tuned model performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-life examples of fine-tuning success – InstructGPT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.1 – AI not fine-tuned for social interactions](img/B21443_03_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – AI not fine-tuned for social interactions
  prefs: []
  type: TYPE_NORMAL
- en: What is fine-tuning and why does it matter?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Issues inherent in general LLMs such as GPT-3 include their tendency to produce
    outputs that are false, toxic content, or negative sentiments. This is attributed
    to the training of LLMs, which focuses on predicting subsequent words from vast
    internet text, rather than securely accomplishing the user’s intended language
    task. In essence, these models lack alignment with their users’ objectives.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at three cases that I found in the first half of 2023 that demonstrate
    ChatGPT’s hallucination problems.
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 1 – an American law professor was falsely accused of being a sexual
    offender by ChatGPT**, with the generated response referencing a non-existent
    Washington News report. If this misinformation had gone unnoticed, it could have
    had severe and irreparable consequences for the professor’s reputation (source:
    [https://www.firstpost.com/world/chatgpt-makes-up-a-sexual-harassment-scandal-names-real-professor-as-accused-12418552.html](https://www.firstpost.com/world/chatgpt-makes-up-a-sexual-harassment-scandal-names-real-professor-as-accused-12418552.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 2 – a lawyer used ChatGPT in court and cited fake cases.** A lawyer
    used ChatGPT to help with an airline lawsuit. The AI suggested fake cases, which
    the lawyer unknowingly presented in court. This mistake led a judge to consider
    sanctions and has drawn attention to AI “hallucinations” in legal settings (source:
    [https://www.forbes.com/sites/mollybohannon/2023/06/08/lawyer-used-chatgpt-in-court-and-cited-fake-cases-a-judge-is-considering-sanctions/?sh=2f13a6c77c7f](https://www.forbes.com/sites/mollybohannon/2023/06/08/lawyer-used-chatgpt-in-court-and-cited-fake-cases-a-judge-is-considering-sanctions/?sh=2f13a6c77c7f)).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 3 – ChatGPT can fabricate information.** According to ChatGPT, The New
    York Times first reported on “artificial intelligence” on July 10, 1956, in an
    article titled *Machines Will Be Capable of Learning, Solving Problems, Scientists
    Predict*. However, it’s crucial to note that while the 1956 Dartmouth College
    conference mentioned in the response was real, the article itself did not exist;
    ChatGPT generated this information. This highlights how ChatGPT can not only provide
    incorrect information but also fabricate details, including names, dates, medical
    explanations, book plots, internet addresses, and even historical events that
    never occurred (source: [https://www.nytimes.com/2023/05/01/business/ai-chatbots-hallucination.html](https://www.nytimes.com/2023/05/01/business/ai-chatbots-hallucination.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The aforementioned hallucination problems occurred in the first half of 2023\.
    Since then, OpenAI has put strict measures and hallucination mitigation systems
    in place.
  prefs: []
  type: TYPE_NORMAL
- en: To curb hallucinations, fine-tuning is one of the potential options besides
    prompt engineering and RAG techniques, both of which we will discuss in later
    chapters. As highlighted previously, fine-tuning tailors LLMs for specific tasks
    or domains. In LLMs, weights refer to the parameters of the neural network, which
    are learned during the model’s training process and are used to calculate the
    output based on input data, allowing the model to make predictions and generate
    text. Essentially, fine-tuning improves a pretrained model by refining these parameters
    with data specific to a task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s consider the benefits of fine-tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reduced hallucinations**: Fine-tuning on trusted data reduces a model’s tendency
    to generate incorrect or fabricated outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Better task performance**: Since the model is tailored to your specific requirements,
    it can result in better responses that are required for your domain-specific use
    case. For instance, BioGPT, fine-tuned from GPT models using biomedical datasets,
    delivered enhanced answers to medical queries compared to non-fine-tuned GPT models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost-efficiency**: Although there are initial upfront costs when it comes
    to fine-tuning, once the model has been fine-tuned, you don’t need to provide
    as many few-shot samples to the prompt, leading to shorter prompts and lower costs.
    We will discuss the few-shot prompting technique further in [*Chapter 5*](B21443_05.xhtml#_idTextAnchor098).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improved latency**: Smaller prompts also mean lower latency requests as fewer
    resources are needed by the LLM to process your API call.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consistent results**: Fine-tuning an LLM with a domain-specific dataset enhances
    the consistency and accuracy of its responses within that domain. For example,
    training a general language model with a dataset of medical research papers not
    only enhances its response accuracy but also ensures consistent output in that
    field across multiple queries. For instance, when the model is asked to “Describe
    the typical symptoms of Type 2 Diabetes,” a fine-tuned model might accurately
    and consistently respond, “Typical symptoms of Type 2 Diabetes include increased
    thirst, frequent urination, hunger, fatigue, and blurred vision.” This specialized
    training ensures the model provides more reliable information for medical inquiries,
    maintaining this consistency across similar queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we explored the “What” and “Why” of fine tuning. Now let's
    understand some real-world use cases where fine-tuning can add value to your AI
    application.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Fine-tuning can be applied to a wide range of natural language processing tasks,
    including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text classification**: This involves classifying text into predefined categories
    by examining its content or context. For example, in sentiment analysis of customer
    reviews, we can classify text as positive, negative, or neutral.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Token classification**: This involves labeling words in a piece of text,
    often to spot names or specific entities. For example, when applying named entity
    recognition to text, we can identify people, cities, and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Question-answering**: This involves providing effective answers to questions
    in natural language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Summarization**: This involves providing concise summaries of long texts
    – for example, summarizing a news article.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language translation**: This involves converting text from one language into
    another. An example of this is translating a document from English into Spanish.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The aforementioned fine-tuning tasks are the most popular ones. This is a rapidly
    evolving field, and more tasks are emerging and can be found on Hugging Face (source:
    [https://huggingface.co/docs/transformers/training](https://huggingface.co/docs/transformers/training))
    and Azure’s Machine Learning Studio (Model Catalog) too.'
  prefs: []
  type: TYPE_NORMAL
- en: Each time, it refines the general-purpose language model into a task-specific
    expert. Models can also be customized without the need to update their weights.
    This process is called in-context learning or few-shot learning. We will cover
    this in [*Chapter 5*](B21443_05.xhtml#_idTextAnchor098), which focuses on prompt
    engineering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we delve into the different fine-tuning techniques, it is also crucial
    to understand the preceding step in fine-training LLM models: pre-training. This
    foundational training phase sets the stage for LLMs, preparing them for the tailored
    adjustments of fine-tuning. In the upcoming section, we’ll contrast pre-training
    with fine-tuning, emphasizing the unique benefits and improvements of the latter.'
  prefs: []
  type: TYPE_NORMAL
- en: Examining pre-training and fine-tuning processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pre-training and fine-tuning are two key stages when training LLMs such as
    GPT-3.5\. Pre-training is like a student’s general education in that it covers
    a broad range of subjects to provide foundational knowledge. Fine-tuning, on the
    other hand, is like a student later specializing in a specific subject in college,
    refining their skills for a particular field. In the context of LLMs, pre-training
    sets the broad base, and fine-tuning narrows the focus to excel in specific tasks.
    In this section, we’ll look at pre-training and fine-tuning to see how fine-tuning
    adds value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Two-step LLM training process](img/B21443_03_02(Ori).jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Two-step LLM training process
  prefs: []
  type: TYPE_NORMAL
- en: Let’s provide an overview of the two stages.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-training process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pre-training is the initial phase of training a language model. During this
    phase, the model learns from a massive amount of text data, often referred to
    as the “pre-training corpus.” The goal of pre-training is to help the model learn
    grammar, syntax, context, and even some world knowledge from the text. The model
    is trained to predict the next word in a sentence, given the previous words. The
    result of pre-training is a model that has learned a general understanding of
    language and can generate coherent text. However, it lacks specificity and the
    ability to generate targeted or domain-specific content.
  prefs: []
  type: TYPE_NORMAL
- en: 'The foundation for creating more advanced models lies in utilizing pristine
    and smart training data. The following figure shows the datasets that are used
    for pretraining OpenAI’s GPT-3 models. These datasets underwent data preparation
    to remove duplicates and ensure diversity and lack of bias before being used for
    pre-training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Datasets used for pretraining OpenAI’s GPT-3 models](img/B21443_03_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – Datasets used for pretraining OpenAI’s GPT-3 models
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, Llama models, by Meta, were developed using the following publicly
    available datasets, after thorough data purification and deduplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Llama model pre-training data](img/B21443_03_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Llama model pre-training data
  prefs: []
  type: TYPE_NORMAL
- en: This training dataset consisted of 1.4 trillion tokens after tokenization. We
    discussed the concept of tokens briefly in [*Chapter 2*](B21443_02.xhtml#_idTextAnchor036)
    and will discuss it in more detail in [*Chapter 5*](B21443_05.xhtml#_idTextAnchor098).
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fine-tuning is the second phase of training a language model and occurs after
    pre-training. During this phase, the model is trained on a more specific dataset
    that is carefully curated and customized for a particular task or domain. This
    dataset is often referred to as the “fine-tuning dataset.” The model is fed with
    data from the fine-tuning dataset, following which it predicts the next tokens
    and evaluates its predictions against the actual, or “ground truth,” values. In
    this process, it tries to minimize the loss. By doing this repetitively, the LLM
    becomes fine-tuned to the downstream task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – The process of fine-tuning](img/B21443_03_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – The process of fine-tuning
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram depicts a language model’s journey from pre-training to
    fine-tuning. Initially, it’s trained on a broad dataset sourced from diverse internet
    texts capturing a variety of language constructs, topics, and styles. Subsequently,
    it’s refined using a targeted, high-quality dataset with domain-specific prompts
    and completions. Ultimately, the data quality of this fine-tuning dataset dictates
    the model’s output precision. Finally, the fine-tuned model interacts with a user
    through queries and responses, catering to a particular downstream task. As discussed
    earlier, these downstream tasks could include text classification, token classification,
    question-answering, summarization, translation, and more.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve explored the overarching concept of fine-tuning, weighing its
    advantages and limitations. Now, let’s delve into some basic and advanced fine-tuning
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques for fine-tuning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we’ll discuss two fine-tuning methods: the traditional full
    fine-tuning approach and advanced techniques such as PEFT, which integrates optimizations
    to attain comparable results to full fine-tuning but with higher efficiency and
    reduced memory and computational expenses.'
  prefs: []
  type: TYPE_NORMAL
- en: Full fine-tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Full fine-tuning refers to the approach where all parameters/weights of a pretrained
    model are adjusted using a task-specific dataset. It’s a straightforward method
    and is generally effective, but it might require a considerable amount of data
    to avoid overfitting and compute, especially for large models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The challenges with generic full fine-tuning methods include updating all the
    model parameters of the LLMs for every downstream task. Here are some more issues
    to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '**High compute and memory requirements**: Full fine-tuning can increase the
    cost of compute exorbitantly, result in large memory requirements, and also result
    in having to update billions or trillions of parameters in the state-of-the-art
    models, which could become unwieldy and inefficient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Catastrophic forgetting**: Full fine-tuning is prone to forgetting old information
    once it’s fine-tuned on new information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiple copies of the LLM**: Fine-tuning requires building a full copy of
    the LLM for every task, such as sentiment analysis, machine translation, and question-answering
    tasks, thus increasing storage requirements. LLMs can be gigabytes in size sometimes
    and building multiple copies of them for different downstream tasks may require
    a lot of storage space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To tackle these challenges and make this process more efficient, a new fine-tuning
    technique has emerged called PEFT that trains a small set of parameters, which
    might be a subset of the existing model parameters or a set of newly added parameters,
    to achieve similar or better performance to the traditional fine-tuning methods
    under different scenarios. By doing this, it provides almost similar results with
    a lower cost in terms of compute and fewer parameter updates.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss different types of PEFT techniques and
    the trade-offs between them.
  prefs: []
  type: TYPE_NORMAL
- en: PEFT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PEFT addresses the challenges with full fine-tuning by training a smaller set
    of parameters. In this section, we will discuss various techniques on how such
    efficiency can be achieved by training a smaller set of parameters. These parameters
    could either be a subset of the current model’s parameters or a new set of added
    parameters. These techniques vary in terms of parameter efficiency, memory efficiency,
    and training speed, though model quality and any potential extra inference costs
    are also distinguishing factors among these methods. PEFT techniques can be broadly
    classified into three categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Selective
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reparameterization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure shows 30 PEFT methods that were discussed in 40 research
    papers published between February 2019 and February 2023:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – PEFT methods that were discussed in research papers published
    between 2019 and 2023](img/B21443_03_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – PEFT methods that were discussed in research papers published between
    2019 and 2023
  prefs: []
  type: TYPE_NORMAL
- en: 'This diagram was taken from a survey published in the paper *Scale Down to
    Scale Up: A Guide to* *Parameter-Efficient Tuning*.'
  prefs: []
  type: TYPE_NORMAL
- en: We will dive into each of these categories in this section but only cover the
    most important PEFT techniques that have shown promising results.
  prefs: []
  type: TYPE_NORMAL
- en: Additive
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The core concept of additive methods involves fine-tuning a model by adding
    extra parameters or layers, exclusively training these new parameters, and keeping
    the original model weights frozen. Although these techniques introduce new parameters
    to the network, they effectively reduce training times and increase memory efficiency
    by decreasing the size of gradients and the optimizer states. This is the most
    widely explored category of PEFT methods. A prominent method under this category
    is prompt tuning with soft prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt tuning with soft prompts
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This type of tuning involves freezing the model weights and updating the prompt
    parameters instead of model parameters like in model fine-tuning. When you freeze
    the weights of a model, you prevent them from being updated during training. These
    weights remain the same throughout the fine-tuning process. It is a very compute
    and energy-efficient technique compared to traditional fine-tuning. Prompt tuning
    should not be confused with prompt engineering, which we will discuss in [*Chapter
    5*](B21443_05.xhtml#_idTextAnchor098). To understand prompt tuning better, we
    need to understand the concept of soft prompts and embedding space.
  prefs: []
  type: TYPE_NORMAL
- en: Soft prompts and embedding space
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An embedding vector space is a high-dimensional space where words, phrases,
    or other types of data are represented as vectors such that semantically similar
    items are located close to each other in the space. In the context of natural
    language processing, these embeddings capture semantic meanings and relationships
    between words or sentences, allowing for operations that can infer similarities,
    analogies, and other linguistic patterns.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Soft prompts versus hard prompts](img/B21443_03_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – Soft prompts versus hard prompts
  prefs: []
  type: TYPE_NORMAL
- en: The above figure depicts a 3D embedding vector space along the *X*, *Y*, and
    *Z* axes. Representing natural language through tokens is considered to be challenging
    because each token is associated with a specific location in the embedding vector
    space. Hence, they are also referred to as hard prompts. On the other hand, soft
    prompts are not confined to fixed, discrete words in natural language and can
    assume any value in the multi-dimensional embedding vector space. In the following
    figure, words such as “jump,” “fox,” and others are hard prompts, whereas the
    unlabeled black-colored token is a soft prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt tuning process
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In prompt tuning, soft prompts, also known as virtual tokens, are concatenated
    with the prompts; it’s left to a supervised training process to determine the
    optimal values. As shown in the following figure, these trainable soft tokens
    are prepended to an embedding vector representation – in this case, “The student
    learns science:”
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Soft prompt concatenation](img/B21443_03_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – Soft prompt concatenation
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure provides a more detailed representation of the process.
    Vectors are attached to the beginning of each embedded input vector and fed into
    the model, the prediction is compared to the target to calculate a loss, and the
    error is backpropagated to calculate gradients, but only the new learnable vectors
    are updated, keeping the core model frozen. In other words, we are searching the
    embedding space for the best representation of the prompt that the LLMs should
    accept. Even though we can’t easily understand soft prompts learned this way,
    they can help us figure out how to do a task using the labeled dataset, doing
    the same job as text prompts written by hand but without being limited to specific
    words or phrases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Prompt tuning process (detailed)](img/B21443_03_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – Prompt tuning process (detailed)
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll compare three methods: model tuning (full fine-tuning), prompt
    tuning, and prompt design (prompt engineering). As shown in *Figure 3**.10*, research
    conducted by Google shows the difference between model tuning, prompt tuning,
    and prompt design (*Guiding Frozen Language Models with Learned Soft Prompts*,
    QUINTA-FEIRA, FEVEREIRO 10, 2022, posted by Brian Lester, AI Resident, and Noah
    Constant, Senior Staff Software Engineer, Google Research).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model tuning (full fine-tuning):'
  prefs: []
  type: TYPE_NORMAL
- en: This method starts with a pre-trained model that is then further trained (or
    “tuned”) on a specific task using additional input data. The model becomes more
    specialized in this process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This method represents “strong task performance” as the model gets more aligned
    with the particular task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prompt tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of tuning the entire model, only the prompt or input to the model is
    adjusted. The main model remains “frozen” or unchanged.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This introduces the concept of “tunable soft prompts,” which can be adjusted
    to get desired outputs from the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This method combines the general capabilities of the pre-trained model with
    a more task-specific approach, leading to “efficient multitask serving.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prompt design (prompt engineering):'
  prefs: []
  type: TYPE_NORMAL
- en: The focus is on designing a very specific input or prompt to guide the pre-trained
    model to produce the desired output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Like prompt tuning, the main model remains “frozen”.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This method is about exploiting the vast knowledge and capabilities of the pre-trained
    model by just crafting the right input. As mentioned earlier, we will cover prompt
    engineering in detail in [*Chapter 5*](B21443_05.xhtml#_idTextAnchor098).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In prompt tuning and prompt design, original model weights remain frozen, whereas
    in model tuning model parameters are updated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Model tuning, prompt tuning, and prompt design](img/B21443_03_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – Model tuning, prompt tuning, and prompt design
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure demonstrates model tuning (full fine-tuning) on the left
    and prompt tuning on the right. Tuning a model for a specific task necessitates
    creating a task-specific version of the entire pre-trained model for each downstream
    task, and separate batches of data must be used for inference. On the other hand,
    prompt tuning only necessitates storing a small, task-specific prompt for each
    task, allowing for mixed-task inference using the original pre-trained model.
    With a T5 “XXL” model, each tuned version of the model necessitates 11 billion
    parameters. In comparison, our tuned prompts only necessitate 20,480 parameters
    for each task, which is a reduction of over five orders of magnitude, assuming
    a prompt length of 5 tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – Model tuning versus prompt tuning](img/B21443_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 – Model tuning versus prompt tuning
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at the benefits of prompt tuning compared to prompt engineering
    and model fine-tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: Compared to model fine-tuning, prompt tuning does not require copies of the
    LLMs to be created for every task, thus resulting in a reduction in storage space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compared to few-shot prompt engineering, prompt tuning is not restricted to
    context length or a limited number of examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of crafting the best manual prompt to generate the desired output, you
    can use backpropagation to automatically learn a new model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resilient to domain shift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The research paper *The Power of Scale for Parameter-Efficient Prompt Tuning*
    from Google highlights the experiment (*Figure 3**.12*) that was conducted on
    the T5 Transformer model. As per the evaluation, prompt tuning on the T5 model
    matched the quality of model tuning (or fine-tuning) as size increases, while
    enabling the reuse of a single frozen model for all tasks. This approach significantly
    outperforms few-shot prompt designs using GPT-3\. SuperGLUE is a benchmark that’s
    designed to comprehensively evaluate the performance of various natural language
    understanding models across a range of challenging language tasks. We will learn
    more about SuperGLUE in the upcoming sections of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12 – Relationship between SuperGLUE Score and Model Parameters](img/B21443_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 – Relationship between SuperGLUE Score and Model Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3**.12 shows the relationship between SuperGLUE Score and Model Parameters
    for different fine-tuning techniques. As scale increases, prompt tuning matches
    model tuning, despite tuning 25,000 times* *fewer parameters.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following GitHub repository from Google Research provides a code implementation
    of this experiment for prompt tuning: [https://github.com/google-research/prompt-tuning](https://github.com/google-research/prompt-tuning).'
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the downsides of prompt tuning, interpreting soft prompts can be
    challenging as these tokens are not fixed hard prompts and do not represent natural
    language. To understand the nearest meaning, you must convert the embeddings back
    into tokens and determine the top-k closest neighbors by measuring the cosine
    similarity. This is because the closest neighbors form a semantic group with semantic
    similarities.
  prefs: []
  type: TYPE_NORMAL
- en: Reparameterization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Regular full fine-tuning, which involves retraining all parameters in a language
    model, is not feasible as the model size grows. This can become computationally
    very expensive. Hence, researchers have identified a new method called reparameterization,
    a technique that’s used in fine-tuning to reduce the number of trainable parameters
    in a model while maintaining its effectiveness. These methods use low-rank transformation
    to reparameterize the weights, thus reducing the number of trainable parameters
    while still allowing the method to work with high-dimensional matrices such as
    the pre-trained parameters of the networks. Let’s explore a very popular reparameterization
    method called **Low-Rank** **Adaptation** (**LoRa**).
  prefs: []
  type: TYPE_NORMAL
- en: LoRA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To enhance the efficiency of fine-tuning, LoRA utilizes a method where weight
    updates are depicted using two compact matrices via low-rank decomposition. This
    approach entails locking the pre-trained model weights and introducing trainable
    rank decomposition matrices into each layer of the Transformer architecture. Low-rank
    decomposition, often simply referred to as low-rank approximation, is a mathematical
    method that’s used to approximate a given matrix with the product of two lower-rank
    matrices. The primary goal of this technique is to capture the most important
    information contained in the original matrix while using fewer parameters or dimensions.
    The experimental results indicated that LoRa can reduce the number of trainable
    parameters by more than 96%.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the difference between regular fine-tuning and LoRA.
    As you can see, the weight update, W_delta, that was identified during backpropagation
    in full fine-tuning is decomposed into two low-rank matrices in LoRA. W_a and
    W_b provide the same information as the original W_delta but in a more efficient
    representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13 – Comparing regular full fine-tuning and LoRA](img/B21443_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 – Comparing regular full fine-tuning and LoRA
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the following table, researchers found that LoRa fine-tuning matches
    or outperforms full fine-tuning of GPT-3 by only updating 0.02 % (37.7M/175,255.8M)
    of the trainable parameters. With LoRA, the number of trainable parameters was
    reduced to 4.7M and 37.7M, from ~175B in full fine-tuning. The evaluation metrics
    were used for **ROUGE**, which we will discuss later in this chapter
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14 – Fine-tuning efficiency with LoRA](img/B21443_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 – Fine-tuning efficiency with LoRA
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s consider the benefits of LoRA:'
  prefs: []
  type: TYPE_NORMAL
- en: LoRA boosts fine-tuning efficiency by significantly cutting down trainable parameters
    and thus can be trained on a single GPU, avoiding the need for distributed cluster
    GPUs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The original pre-trained weights stay unchanged, allowing for various lightweight
    LoRA models to be used on top for different tasks. This eliminates the need to
    create a full copy of the fine-tuned model for every downstream task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LoRA can be combined with many other PEFT techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LoRA fine-tuned models match the performance of fully fine-tuned ones.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There’s no added serving latency with LoRA as adapter weights integrate with
    the base model and allow for quick task switching when deployed as a service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selective
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The selective approach is the simplest method of fine-tuning as it only involves
    the top layers of the network. However, researchers have mentioned that while
    they might excel in scenarios involving smaller-scale data with model parameters
    numbering less than a billion, they can demand significant computational resources
    and memory compared to conventional fine-tuning methods when applied to larger
    networks. Hence, these methods should not be the first choice when choosing a
    PEFT method.
  prefs: []
  type: TYPE_NORMAL
- en: '**BitFit** is one of the selective PEFT methods and fine-tunes only the biases
    of the network. BitFit updates a mere 0.05% of model parameters and initially
    showcased strong results comparable to or better than full fine-tuning in low-medium
    data scenarios for BERT models that consisted of under 1 billion parameters. When
    evaluated on larger networks, such as T0-3B or GPT-3, BitFit’s performance noticeably
    lags behind both full fine-tuning and other PEFT methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Other important selective PEFT techniques include DiffPruning, FishMask, Freeze,
    and Reconfigure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having understood fine-tuning, let’s explore a related method that augments
    the fine-tuning process: RLHF. This method leverages human insights to further
    tailor model behaviors and outputs, aligning them more closely with human values
    and expectations. Let’s delve into how RLHF works and its significance in the
    fine-tuning landscape.'
  prefs: []
  type: TYPE_NORMAL
- en: RLHF – aligning models with human values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Fine-tuning can be beneficial for achieving specific tasks, thus enhancing
    accuracy and improving model adaptability, but models can sometimes exhibit undesirable
    behavior. They might result in harmful language, displaying aggression, or even
    sharing detailed guidance on dangerous subjects such as weapons or explosive manufacturing.
    Such behaviors could be detrimental to society. This stems from the fact that
    models are trained on extensive internet data, which can contain malicious content.
    Both the pre-training phase and the fine-tuning process might yield outcomes that
    are counterproductive, hazardous, or misleading. Hence, it’s imperative to make
    sure that models resonate with human ethics and values. An added refinement step
    should integrate the three fundamental human principles: **helpfulness, harmlessness,
    and honesty** (**HHH**). RLHF is a method of training machine learning models,
    particularly in the context of **reinforcement learning** (**RL**), that uses
    feedback from humans. To understand RLHF, we must understand the concept of RL:'
  prefs: []
  type: TYPE_NORMAL
- en: '**RL**: This is a type of machine learning where an agent learns to make decisions
    by taking actions in an environment to maximize some notion of cumulative reward.
    The agent interacts with the environment, receives feedback in the form of rewards
    or penalties, and adjusts its actions accordingly. For example, a chess-playing
    AI improves its strategies by earning points for winning moves and losing points
    for blunders.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RLHF is a type of RL where the traditional reward signal, which usually comes
    from the environment, is replaced or augmented with feedback from humans. Initially,
    a model is trained to imitate human behavior. Then, instead of relying solely
    on environmental rewards, humans provide feedback by comparing different action
    sequences or trajectories. This human feedback is used to train a reward model,
    which then guides the agent’s learning process, helping it improve its decisions
    and actions in the environment. The core components of RLHF are the reward model
    and the RL algorithm.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Reward model:** In the context of RL, a reward model is a model that provides
    a numerical reward signal to an agent based on the actions it takes in a given
    state. Instead of manually designing a reward function, which can be challenging
    and error-prone, a reward model is learned from data, often incorporating human
    feedback.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Human feedback:** As shown in the following figure, the outputs from LLM
    models are ranked by humans with a scoring system and then fed into the reward
    model. After the learning process, the reward model is used to teach the agent
    what is helpful, harmless, and honest by showing examples or providing interactive
    feedback:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Reward model training process from Hugging Face (source: https://huggingface.co/blog/rlhf)](img/B21443_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.15 – Reward model training process from Hugging Face (source: [https://huggingface.co/blog/rlhf](https://huggingface.co/blog/rlhf))'
  prefs: []
  type: TYPE_NORMAL
- en: '**RL algorithm**: The RL algorithm utilizes inputs from reward models to refine
    the LLMs, enhancing the reward score progressively. A popular choice of RL algorithm
    is proximal policy optimization. As shown in the following figure, first, the
    LLM generates an output that is evaluated by the reward model quantitatively to
    provide a reward score of 1.79\. This reward is sent to the RL algorithm, which,
    in turn, updates the LLM weights. A very popular choice of RL algorithm that has
    emerged recently is the PPO. Understanding the inner details of PPO is beyond
    the scope of this book, but more information can be found in the research paper
    *Proximal Policy Optimization Algorithms*, from Open AI:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.16 – Updating LLMs with the RL algorithm](img/B21443_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 – Updating LLMs with the RL algorithm
  prefs: []
  type: TYPE_NORMAL
- en: '**Potential risks – reward hacking**: In RL, an agent seeks to maximize a reward
    model provided by the environment. However, sometimes, the agent finds unintended
    shortcuts or loopholes to get high rewards without actually solving the task as
    intended. This is known as “reward hacking.” This may lead to an RLuUpdated LLM
    that generates grammatically incorrect sentences, gibberish sentences, or exaggerated
    positive sentences to maximize rewards. To mitigate this, PPO establishes a boundary
    on the magnitude of policy modifications. This limitation is implemented through
    the use of **Kullback-Leibler** (**KL**)-divergence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kullback-Leibler (KL)**: Divergence measures how much one probability distribution
    differs from another reference distribution. Solomon Kullback and Richard A. Leibler
    introduced this concept to the world in 1951\. Within the context of PPO, KL-divergence
    is pivotal in steering optimization, ensuring that the refined policy remains
    closely aligned with its predecessor. In other words, it ensures the RL updates
    to LLMs are not drastic and stay within the threshold value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to evaluate fine-tuned model performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we’ve learned how to fine-tune LLMs to suit our needs, but how do we
    evaluate a model to make sure it’s performing well? But how do we know if a fine-tuned
    model made improvements over its predecessor model over a particular task? What
    are some industry-standard benchmarks that we can rely on to evaluate the models?
    In this section, we will see how LLMs such are GPT are evaluated and use the most
    popular benchmarks developed by researchers.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Bilingual Evaluation Understudy** (**BLEU**) and **Recall-Oriented Understudy
    for Gisting Evaluation** (**ROUGE**) are both widely used metrics for evaluating
    the quality of machine-generated text, especially in the context of machine translation
    and text summarization. They measure the quality of generated texts in different
    ways. Let’s take a closer look.'
  prefs: []
  type: TYPE_NORMAL
- en: ROUGE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'ROUGE is a set of metrics that’s used to evaluate the quality of summaries
    by comparing them to reference summaries. It’s mainly used to evaluate text summarization
    but can also be applied to other tasks, such as machine translation. ROUGE focuses
    on the overlap of n-grams – that is, word sequences of *n* items – between the
    generated summary and the reference summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Formula for ROUGE-N](img/B21443_03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 – Formula for ROUGE-N
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common variants of ROUGE are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ROUGE-N**: This variant measures the overlap of n-grams. For instance, ROUGE-1
    looks at the overlap of 1-gram (individual words), ROUGE-2 considers 2-grams (two
    consecutive words), and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ROUGE-L**: This variant considers the longest common subsequence between
    the generated summary and the reference summary. It focuses on the longest in-sequence
    set of words that both summaries share.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ROUGE-S**: This variant measures the overlap of skip-bigrams, which are pairs
    of words in a sentence, irrespective of their order, allowing for gaps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s look at an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use ROUGE-1, which focuses on individual word overlap, to illustrate
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reference summary: “The boy fell on the grass”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Generated summary: “The boy was on the grass.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, every word except “was” and “fell” match between the two summaries.
  prefs: []
  type: TYPE_NORMAL
- en: Total words in the reference = 6
  prefs: []
  type: TYPE_NORMAL
- en: Matching words = 5
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the ROUGE-1 recall (how many of the words in the reference summary are
    also in the generated summary) would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 5/6 = 0.83 or 83%
  prefs: []
  type: TYPE_NORMAL
- en: ROUGE can also compute precision (how many of the words in the generated summary
    are in the reference summary) and F1 score (the harmonic mean of precision and
    recall).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Precision: 5/6 = 0.83 or 83%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'F1 score: 2 * (Precision * Recall) / (Precision + Recall) = 83%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While ROUGE scores give a quantitative measure of the overlap between the generated
    and reference text, it’s essential to note that a high ROUGE score doesn’t always
    mean the generated summary is of high quality. Other factors, such as coherence
    and fluency, are not captured by ROUGE.
  prefs: []
  type: TYPE_NORMAL
- en: BLEU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: BLEU is a metric for evaluating the quality of text that has been machine-translated
    from one natural language into another. The core idea behind BLEU is that if a
    translation is good, the words and phrases in the translation should appear in
    the same sequence as in the reference translations made by humans.
  prefs: []
  type: TYPE_NORMAL
- en: 'BLEU considers the precision of n-grams (contiguous sequences of *n* items
    from a piece of text) in the machine-generated translation concerning the human
    reference translation(s). A typical BLEU score considers 1-gram (individual words),
    2-gram (pairs of consecutive words), 3-gram, and 4-gram precisions, then takes
    a weighted geometric mean to compute the final score. It also incorporates a penalty
    for translations that are shorter than their references, called the brevity penalty
    (source: BLEU, [https://aclanthology.org/P02-1040.pdf](https://aclanthology.org/P02-1040.pdf)).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Both ROUGE and BLEU are simple metrics and can be used for diagnostic purposes
    but shouldn’t be used for a full and final evaluation of the model. Hence, for
    a more comprehensive evaluation, we must consider benchmarking methods. These
    will be discussed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Benchmarks are critical for evaluation as well. This is a rapidly evolving research
    area, so in this section, we have focused on important benchmarks as of early
    2024\. Benchmarks are tests or tasks that are used to measure and compare the
    model’s performance in various areas, such as comprehension, generation, or accuracy.
    They help researchers and developers gauge how well the model understands and
    generates text and can be used to compare the performance of one LLM to another
    or track improvements over time. Evaluation metrics such as ROUGE and BLEU provide
    limited insights into the capabilities of LLM. Hence, to get a more comprehensive
    view of LLMs, we can leverage preexisting evaluation datasets and associated benchmarks
    that have been developed by LLM researchers.
  prefs: []
  type: TYPE_NORMAL
- en: GLUE and SuperGLUE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**General Language Understanding Evaluation** (**GLUE**) is a benchmark suite
    for evaluating the performance of **natural language understanding** (**NLU**)
    models on a variety of tasks. Introduced in 2018, GLUE comprises nine NLU tasks,
    including sentiment analysis, question-answering, and textual entailment, among
    others. It was developed to stimulate research in the field by providing a standard
    set of tasks for model comparison and competition and to push the boundaries of
    what NLU models can achieve.'
  prefs: []
  type: TYPE_NORMAL
- en: SuperGLUE ([https://super.gluebenchmark.com](https://super.gluebenchmark.com)/),
    building upon the foundation of GLUE, is a more challenging benchmark that was
    introduced later. It was designed in response to the rapid advancements in model
    performance on the original GLUE tasks. SuperGLUE consists of a set of tasks that
    are more diverse and difficult, aiming to further push the capabilities of state-of-the-art
    NLU models and to provide a rigorous evaluation framework for future models.
  prefs: []
  type: TYPE_NORMAL
- en: 'As of early 2024, SuperGLUE ([https://arxiv.org/pdf/1905.00537.pdf](https://arxiv.org/pdf/1905.00537.pdf))
    can evaluate models in 10 NLU tasks. This includes **Boolean Questions** (**BoolQ**),
    **CommitmentBank** (**CB**), **Choice of Plausible Alternatives** (**COPA**),
    **Multi-Sentence Reading Comprehension** (**MultiRC**), **Reading Comprehension
    with Commonsense Reasoning Dataset** (**ReCoRD**), **Recognizing Textual Entailment**
    (**RTE**), **Words in Context** (**WiC**), **Winograd Schema Challenge** (**WSC**),
    broad coverage diagnostics (AX-b), and Winogender Schema Diagnostics (gender parity/
    accuracy). Let’s take a closer look:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Task** | **Description** | **Example** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BoolQ | Answer yes/no questions based on a passage. | Passage: “Dolphins
    are known for their intelligence.” Question: “Are dolphins recognized for their
    intelligence?” Answer: Yes. |'
  prefs: []
  type: TYPE_TB
- en: '| CB | Predict the level of commitment in a statement. | Premise: “I think
    the cat might be in the garden.”Hypothesis: “The cat is in the garden.”Entailment:
    Unknown (the premise suggests a possibility, but it doesn’t firmly commit to the
    cat being in the garden.) |'
  prefs: []
  type: TYPE_TB
- en: '| COPA | Choose between two plausible alternatives as the cause or effect of
    a given premise. | Premise: “The ground was wet.” Question: What was the CAUSE
    of this?” Alternatives: (a) It rained. (b) It was sunny. Answer: (a) It rained.
    |'
  prefs: []
  type: TYPE_TB
- en: '| MultiRC | Answer questions about individual sentences in a passage. | Passage:
    “Jupiter is the largest planet. It’s primarily composed of hydrogen.” Question:
    “What is Jupiter primarily composed of?” Answer: Hydrogen. |'
  prefs: []
  type: TYPE_TB
- en: '| ReCoRD | Fill in the blanks in a passage using context. | Passage: “Lara
    loves reading. Her favorite genre is ____. She’s read every mystery novel.” Fill
    in the blank: mystery. |'
  prefs: []
  type: TYPE_TB
- en: '| RTE | Determine if a premise sentence entails a hypothesis sentence. | Premise:
    “Dogs are mammals.” Hypothesis: “Dogs give live birth.” Entailment: True. |'
  prefs: []
  type: TYPE_TB
- en: '| WiC | Determine if a word has the same meaning in two sentences. | Sentence
    1: “He used a key to open the door.” Sentence 2: “The answer is the key to this
    puzzle.” Word: “key” Answer: Different senses. |'
  prefs: []
  type: TYPE_TB
- en: '| WSC | Identify to which noun phrase a pronoun refers in a sentence. | Sentence:
    “The trophy doesn’t fit in the suitcase because it’s too large.” Question: What
    is too large? Answer: The trophy. |'
  prefs: []
  type: TYPE_TB
- en: Figure 3.18 – SuperGLUE benchmark
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the leaderboard for the SuperGLUE benchmark ([https://super.gluebenchmark.com/leaderboard](https://super.gluebenchmark.com/leaderboard)),
    with the LLM models leading across various NLU tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.19 – Snapshot of the SuperGLUE benchmark leaderboard as of February
    2024](img/B21443_03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.19 – Snapshot of the SuperGLUE benchmark leaderboard as of February
    2024
  prefs: []
  type: TYPE_NORMAL
- en: Massive Multitask Language Understanding (MMLU)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'MMLU was established in 2021\. This benchmark is quite suitable for modern
    massive LLMs. The goal is to evaluate and compare models regarding their world
    knowledge and problem-solving abilities. This benchmark encompasses 57 topics,
    spanning areas from STEM and the humanities to the social sciences and beyond.
    Its complexity varies from basic to expert levels, evaluating both general knowledge
    and analytical capabilities. The subjects touch upon both classic fields, such
    as mathematics and history, and more niche sectors, such as law and ethics. The
    detailed scope and variety of topics within the benchmark make it perfectly suited
    to pinpoint a model’s areas of weakness. These tasks go beyond basic language
    understanding, as evaluated by GLUE and SuperGLUE (source: MMLU, [https://arxiv.org/pdf/2009.03300.pdf](https://arxiv.org/pdf/2009.03300.pdf)).'
  prefs: []
  type: TYPE_NORMAL
- en: The leaderboard for the MMLU benchmark can be found at [https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu).
  prefs: []
  type: TYPE_NORMAL
- en: Beyond the Imitation Game Benchmark (BIG-bench)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'BIG-bench is a collaborative benchmark that was introduced in October 2022\.
    The goal of this benchmark is to build disruptive models and evaluate them on
    tasks that are beyond the capabilities of current language models. It consists
    of more than 204 diverse tasks ranging from linguistics, childhood development,
    math, common-sense reasoning, biology, physics, social bias, software development,
    and beyond (source: BIG-bench, [https://arxiv.org/abs/2206.04615](https://arxiv.org/abs/2206.04615)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following GitHub repository provides some code so that you can use BIG-bench
    to evaluate your models: [https://github.com/google/BIG-bench#submitting-a-model-evaluation](https://github.com/google/BIG-bench#submitting-a-model-evaluation).'
  prefs: []
  type: TYPE_NORMAL
- en: Holistic Evaluation of Language Model (HELM) (Classic, Lite, and Text-to-Image)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The HELM Classic benchmark, which was introduced in November 2022 by Stanford
    Research, evaluates models on seven key metrics: accuracy, calibration, robustness,
    fairness, bias, toxicity, and efficiency. The HELM framework aims to improve the
    transparency of models and offers insights into which models perform well on specific
    tasks. This benchmark measures these seven metrics across 51 scenarios and exposes
    the trade-offs between models and metrics. This benchmark is also continuously
    evolving, and more scenarios, metrics, and models are being added to this benchmark.
    Scenarios consist of a use case and a dataset of examples such as **Math Chain
    of Thought** (**MATH**), **Grade School Math** (**GSM8K**), HellaSwag (common-sense
    reasoning), MMLU, OpenBook QA (question-answering), and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a full list of scenarios, check out this page on HELM Classic scenarios:
    [https://crfm.stanford.edu/helm/classic/latest/#/scenarios](https://crfm.stanford.edu/helm/classic/latest/#/scenarios).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following link provides the latest results on the HELM leaderboard: [https://crfm.stanford.edu/helm/lite/latest/#/leaderboard](https://crfm.stanford.edu/helm/lite/latest/#/leaderboard).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This Python package can be used to evaluate your models against the HELM benchmarks
    and compare them against the most prominent models: [https://crfm-helm.readthedocs.io/en/latest/](https://crfm-helm.readthedocs.io/en/latest/)).'
  prefs: []
  type: TYPE_NORMAL
- en: Helm Classic was released before ChatGPT and its initial objective was to comprehensively
    assess every language model available across a variety of representative scenarios,
    such as linguistic capabilities, reasoning skills, knowledge, and more, as well
    as a range of metrics. However, it was quite heavyweight, hence a lighter version
    was released called HELM Lite. It is not only a subset of Classic but a more simplified
    version with fewer core scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the proliferation of multimodal LLMs, recently, Stanford published a new
    benchmark called **Holistic Evaluation of Image Models** (**HEIM**), which assesses
    text-to-image models on 12 different aspects required for real-world deployment
    ([https://arxiv.org/abs/2311.04287](https://arxiv.org/abs/2311.04287)):'
  prefs: []
  type: TYPE_NORMAL
- en: Image-text alignment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aesthetics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Originality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reasoning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Toxicity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fairness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robustness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-linguality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficiency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we delved into the key benchmarks and assessment metrics for
    LLMs. If you’re looking to construct an enterprise-level ChatGPT application,
    it’s crucial to measure GPT models against top benchmarks to ensure the application
    is effective, trustworthy, and safe. Such benchmarks serve as an excellent foundation
    for this endeavor.
  prefs: []
  type: TYPE_NORMAL
- en: Tools such as Azure AI Studio and Azure Prompt Flow provide qualitative and
    quantitative solutions to evaluate your models. It also provides benchmarking
    capabilities that help you assess different models using industry-leading benchmarks.
    Scores such as ROUGE-N and BLEU can be calculated using out-of-the-box functionalities
    on Azure Prompt Flow.
  prefs: []
  type: TYPE_NORMAL
- en: Real-life examples of fine-tuning success
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll explore a real-life example of a fine-tuning approach
    that OpenAI implemented, which yielded remarkable outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: InstructGPT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenAI’s InstructGPT is one of the most successful stories of fine-tuned models
    that laid the foundation of ChatGPT. ChatGPT is said to be a sibling model to
    InstructGPT. The methods that are used to fine-tune ChatGPT are similar to InstructGPT.
    InstructGPT was created by fine-tuning pre-trained GPT-3 models with RHLF. Supervised
    fine-tuning is the first step in RLHF for generating responses aligned to human
    preferences.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the beginning, GPT-3 models weren’t originally designed to adhere to user
    instructions. Their training focused on predicting the next word based on vast
    amounts of internet text data. Therefore, these models underwent fine-tuning using
    instructional datasets along with RLHF to enhance their ability to generate more
    useful and relevant responses aligned with human values when prompted with user
    instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.20 – The fine-tuning process with RLHF](img/B21443_03_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.20 – The fine-tuning process with RLHF
  prefs: []
  type: TYPE_NORMAL
- en: 'This figure depicts a schematic representation showcasing the InstructGPT fine-tuning
    process: (*1*) initial supervised fine-tuning, (*2*) training the reward model,
    and (*3*) executing RL through PPO using this established reward model. The utilization
    of this data to train respective models is indicated by the presence of blue arrows.
    In *step 2*, boxes A-D are samples from models that get ranked by labelers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure provides a comparison of the response quality of fine-tuned
    models with RLHF, supervised fine-tuned models, and general GPT models. The *Y*-axis
    consists of a Likert scale and shows quality ratings of model outputs on a 1–7
    scale (*Y*-axis), for various model sizes (*X*-axis), on prompts submitted to
    InstructGPT models via the OpenAI API. The results reveal that InstructGPT outputs
    receive significantly higher scores by labelers compared to outputs from GPT-3
    models with both few-shot prompts and those without, as well as models that underwent
    supervised learning fine-tuning. The labelers that were hired for this work were
    independent and were sourced from Scale AI and Upwork:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.21 – Evaluation of InstructGPT (image credits: Open AI)](img/B21443_03_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.21 – Evaluation of InstructGPT (image credits: Open AI)'
  prefs: []
  type: TYPE_NORMAL
- en: 'InstructGPT can be assessed across dimensions of toxicity, truthfulness, and
    appropriateness. Higher scores are desirable for TruthfulQA and appropriateness,
    whereas lower scores are preferred for toxicity and hallucinations. Measurement
    of hallucinations and appropriateness is conducted based on the distribution of
    prompts within our API. The outcomes are aggregated across various model sizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.22 – Evaluation of InstructGPT](img/B21443_03_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.22 – Evaluation of InstructGPT
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we introduced the concept of fine-tuning and discussed a success
    stories of fine-tuning with RLHF that led to the development of InstructGPT.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fine-tuning is a powerful technique for customizing models, but it may not always
    be necessary. As observed, it can be time-consuming and may have initial upfront
    costs. It’s advisable to start with easier and faster strategies, such as prompt
    engineering with few-shot examples, followed by data grounding using RAG. Only
    if the responses from the LLM remain suboptimal should you consider fine-tuning.
    We will discuss RAG and prompt engineering in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we delved into critical fine-tuning strategies tailored for
    specific tasks. Then, we explored an array of evaluation methods and benchmarks
    to assess your refined model. The RLHF process ensures your models align with
    human values, making them helpful, honest, and safe. In the upcoming chapter,
    we’ll tackle RAG methods paired with vector databases – an essential technique
    to ground your enterprise data and minimize hallucinations in LLM-driven applications.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://spotintelligence.com/2023/03/28/transfer-learning-large-language-models/](https://spotintelligence.com/2023/03/28/transfer-learning-large-language-models/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://platform.openai.com/docs/guides/fine-tuning](https://platform.openai.com/docs/guides/fine-tuning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PEFT Research Paper: [https://arxiv.org/abs/2303.15647](https://arxiv.org/abs/2303.15647)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BLEU: [https://aclanthology.org/P02-1040/](https://aclanthology.org/P02-1040/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Power of Scale for Parameter-Efficient Prompt Tuning: [https://aclanthology.org/2021.emnlp-main.243.pdf](https://aclanthology.org/2021.emnlp-main.243.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Low Rank Adaption of Large Language Models: [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLM (GPT) Fine Tuning — PEFT | LoRA | Adapters | Quantization | by Siddharth
    vij | Jul, 2023 | Medium: [https://tinyurl.com/2t8ntxy4](https://tinyurl.com/2t8ntxy4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'InstructGPT: [https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://towardsdatascience.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7](https://towardsdatascience.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.fuzzylabs.ai/blog-post/llm-fine-tuning-old-school-new-school-and-everything-in-between](https://www.fuzzylabs.ai/blog-post/llm-fine-tuning-old-school-new-school-and-everything-in-between)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Llama: [https://arxiv.org/abs/2302.13971](https://arxiv.org/abs/2302.13971)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.
    In Proceedings of ICLR: [1804.07461] [https://arxiv.org/abs/1804.07461](https://arxiv.org/abs/1804.07461)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding
    Systems: [1905.00537] [https://arxiv.org/abs/1905.00537](https://arxiv.org/abs/1905.00537)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MMLU Measuring Massive Multitask Language Understanding: [https://arxiv.org/pdf/2009.03300.pdf](https://arxiv.org/pdf/2009.03300.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BIG Bench: [https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/keywords_to_tasks.md#summary-table](https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/keywords_to_tasks.md#summary-table)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HELM: [https://arxiv.org/pdf/2211.09110.pdf](https://arxiv.org/pdf/2211.09110.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
