- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Fine-Tuning – Building Domain-Specific LLM Applications
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调 - 构建特定领域的大型语言模型（LLM）应用
- en: 'In developing ChatGPT-based applications, ensuring the model’s precision, relevance,
    and alignment to its intended purpose is paramount. As we navigate the intricacies
    of this technology, it becomes evident that a one-size-fits-all approach doesn’t
    suffice. Hence, customizing the model becomes necessary to adapt to certain specialized
    domains, such as medicine, biotechnology, legal, and others. This chapter delves
    deep into model customization for domain-specific applications via fine-tuning
    and **parameter-efficient fine-tuning** (**PEFT**). But how do we evaluate that
    our refinements truly hit the mark? How do we know that they align with human
    values? Through rigorous evaluation metrics and benchmarking. By understanding
    and applying these pivotal processes, we not only bring out the best in ChatGPT
    but also adhere closely to the vision of this book: generative AI for cloud solutions.
    We must ensure it’s not just smart but also context-aware, effective, honest,
    safe, and resonant with its user’s needs. Hallucinations in **large language models**
    (**LLMs**) refer to generating factually incorrect or nonsensical information
    as if it were true. To reduce problems such as hallucinations, which can have
    a detrimental impact on society, we will discuss three important techniques in
    this book: fine-tuning, **retrieval-augmented generation** (**RAG**), and prompt
    engineering. While this chapter focuses on fine-tuning, we will discuss RAG and
    prompt engineering in the subsequent chapters.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发基于 ChatGPT 的应用时，确保模型的精确性、相关性和与其预期目的的对齐至关重要。随着我们深入探索这项技术的复杂性，很明显，一刀切的方法是不够的。因此，定制模型以适应某些特定领域（如医学、生物技术、法律等）变得必要。本章深入探讨了通过微调和**参数高效微调**（**PEFT**）进行特定应用模型定制。但如何评估我们的改进真正击中目标？我们如何知道它们与人类价值观相一致？通过严格的评估指标和基准测试。通过理解和应用这些关键过程，我们不仅使
    ChatGPT 发挥最佳效果，而且紧密遵循本书的愿景：面向云解决方案的生成式 AI。我们必须确保它不仅聪明，而且具有情境意识、有效、诚实、安全，并且与用户的需求产生共鸣。**大型语言模型**（**LLMs**）中的幻觉是指生成事实错误或无意义的信息，仿佛它是真实的。为了减少如幻觉等问题，这些问题可能对社会产生有害影响，本书将讨论三种重要的技术：微调、**检索增强生成**（**RAG**）和提示工程。虽然本章重点讨论微调，但将在后续章节中讨论
    RAG 和提示工程。
- en: 'We will cover the following main topics in the chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要主题：
- en: What is fine-tuning and why does it matter?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是微调以及为什么它很重要？
- en: Techniques for fine-tuning models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调模型的技巧
- en: '**Reinforcement learning from human feedback** (**RLHF**) – aligning models
    with human values'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于人类反馈的强化学习**（**RLHF**） – 使模型与人类价值观对齐'
- en: How to evaluate fine-tuned model performance
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何评估微调模型的表现
- en: Real-life examples of fine-tuning success – InstructGPT
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调成功的生活实例 – InstructGPT
- en: '![Figure 3.1 – AI not fine-tuned for social interactions](img/B21443_03_1.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1 – 未针对社交互动微调的 AI](img/B21443_03_1.jpg)'
- en: Figure 3.1 – AI not fine-tuned for social interactions
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – 未针对社交互动微调的 AI
- en: What is fine-tuning and why does it matter?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是微调以及为什么它很重要？
- en: Issues inherent in general LLMs such as GPT-3 include their tendency to produce
    outputs that are false, toxic content, or negative sentiments. This is attributed
    to the training of LLMs, which focuses on predicting subsequent words from vast
    internet text, rather than securely accomplishing the user’s intended language
    task. In essence, these models lack alignment with their users’ objectives.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一般性 LLM（如 GPT-3）固有的问题包括其倾向于产生错误、有毒内容或负面情绪的输出。这归因于 LLM 的训练，其重点在于从庞大的互联网文本中预测后续单词，而不是安全地完成用户的语言任务。本质上，这些模型缺乏与用户目标的对齐。
- en: Let’s look at three cases that I found in the first half of 2023 that demonstrate
    ChatGPT’s hallucination problems.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 2023 年上半年我发现的三个案例，这些案例展示了 ChatGPT 的幻觉问题。
- en: '**Case 1 – an American law professor was falsely accused of being a sexual
    offender by ChatGPT**, with the generated response referencing a non-existent
    Washington News report. If this misinformation had gone unnoticed, it could have
    had severe and irreparable consequences for the professor’s reputation (source:
    [https://www.firstpost.com/world/chatgpt-makes-up-a-sexual-harassment-scandal-names-real-professor-as-accused-12418552.html](https://www.firstpost.com/world/chatgpt-makes-up-a-sexual-harassment-scandal-names-real-professor-as-accused-12418552.html)).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例1 – 一位美国法学教授被ChatGPT错误指控为性侵犯者**，生成的回复引用了一份不存在的华盛顿新闻报告。如果这种错误信息没有被注意到，可能会对教授的名誉造成严重且无法修复的后果（来源：[https://www.firstpost.com/world/chatgpt-makes-up-a-sexual-harassment-scandal-names-real-professor-as-accused-12418552.html](https://www.firstpost.com/world/chatgpt-makes-up-a-sexual-harassment-scandal-names-real-professor-as-accused-12418552.html))。'
- en: '**Case 2 – a lawyer used ChatGPT in court and cited fake cases.** A lawyer
    used ChatGPT to help with an airline lawsuit. The AI suggested fake cases, which
    the lawyer unknowingly presented in court. This mistake led a judge to consider
    sanctions and has drawn attention to AI “hallucinations” in legal settings (source:
    [https://www.forbes.com/sites/mollybohannon/2023/06/08/lawyer-used-chatgpt-in-court-and-cited-fake-cases-a-judge-is-considering-sanctions/?sh=2f13a6c77c7f](https://www.forbes.com/sites/mollybohannon/2023/06/08/lawyer-used-chatgpt-in-court-and-cited-fake-cases-a-judge-is-considering-sanctions/?sh=2f13a6c77c7f)).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例2 – 一位律师在法庭上使用ChatGPT并引用了虚假案例**。一位律师使用ChatGPT帮助处理一起航空诉讼案件。AI建议了虚假案例，律师不知情地在法庭上提出了这些案例。这个错误导致一位法官考虑采取制裁措施，并引起了人们对法律环境中AI“幻觉”的关注（来源：[https://www.forbes.com/sites/mollybohannon/2023/06/08/lawyer-used-chatgpt-in-court-and-cited-fake-cases-a-judge-is-considering-sanctions/?sh=2f13a6c77c7f](https://www.forbes.com/sites/mollybohannon/2023/06/08/lawyer-used-chatgpt-in-court-and-cited-fake-cases-a-judge-is-considering-sanctions/?sh=2f13a6c77c7f))。'
- en: '**Case 3 – ChatGPT can fabricate information.** According to ChatGPT, The New
    York Times first reported on “artificial intelligence” on July 10, 1956, in an
    article titled *Machines Will Be Capable of Learning, Solving Problems, Scientists
    Predict*. However, it’s crucial to note that while the 1956 Dartmouth College
    conference mentioned in the response was real, the article itself did not exist;
    ChatGPT generated this information. This highlights how ChatGPT can not only provide
    incorrect information but also fabricate details, including names, dates, medical
    explanations, book plots, internet addresses, and even historical events that
    never occurred (source: [https://www.nytimes.com/2023/05/01/business/ai-chatbots-hallucination.html](https://www.nytimes.com/2023/05/01/business/ai-chatbots-hallucination.html)).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例3 – ChatGPT可以编造信息**。根据ChatGPT的说法，1956年7月10日，《纽约时报》在题为《机器将能够学习，解决问题，科学家预测》的文章中首次报道了“人工智能”。然而，需要注意的是，虽然回复中提到的1956年达特茅斯学院会议是真实的，但该文章本身并不存在；ChatGPT生成了这些信息。这突显了ChatGPT不仅能够提供错误信息，还能编造包括姓名、日期、医学解释、书籍情节、互联网地址，甚至从未发生过的历史事件等细节（来源：[https://www.nytimes.com/2023/05/01/business/ai-chatbots-hallucination.html](https://www.nytimes.com/2023/05/01/business/ai-chatbots-hallucination.html))。'
- en: Note
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The aforementioned hallucination problems occurred in the first half of 2023\.
    Since then, OpenAI has put strict measures and hallucination mitigation systems
    in place.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 上述的幻觉问题发生在2023年上半年度。从那时起，OpenAI已经采取了严格的措施和幻觉缓解系统。
- en: To curb hallucinations, fine-tuning is one of the potential options besides
    prompt engineering and RAG techniques, both of which we will discuss in later
    chapters. As highlighted previously, fine-tuning tailors LLMs for specific tasks
    or domains. In LLMs, weights refer to the parameters of the neural network, which
    are learned during the model’s training process and are used to calculate the
    output based on input data, allowing the model to make predictions and generate
    text. Essentially, fine-tuning improves a pretrained model by refining these parameters
    with data specific to a task.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了遏制幻觉，除了提示工程和RAG技术之外，微调是潜在选项之一，这两者我们将在后续章节中讨论。如前所述，微调针对特定任务或领域定制LLM。在LLM中，权重指的是神经网络参数，这些参数在模型训练过程中学习，并用于根据输入数据计算输出，从而使模型能够做出预测和生成文本。本质上，微调通过使用特定于任务的特定数据来细化这些参数，从而改进预训练模型。
- en: 'Now, let’s consider the benefits of fine-tuning:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑微调的好处：
- en: '**Reduced hallucinations**: Fine-tuning on trusted data reduces a model’s tendency
    to generate incorrect or fabricated outputs.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少幻觉**：在可信数据上进行的微调减少了模型生成错误或虚构输出的倾向。'
- en: '**Better task performance**: Since the model is tailored to your specific requirements,
    it can result in better responses that are required for your domain-specific use
    case. For instance, BioGPT, fine-tuned from GPT models using biomedical datasets,
    delivered enhanced answers to medical queries compared to non-fine-tuned GPT models.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更好的任务性能**：由于模型是根据您的特定要求定制的，因此它可以产生更好的响应，这些响应对于您的特定用例是必需的。例如，BioGPT，使用生物医学数据集从GPT模型微调，与未微调的GPT模型相比，为医学查询提供了改进的答案。'
- en: '**Cost-efficiency**: Although there are initial upfront costs when it comes
    to fine-tuning, once the model has been fine-tuned, you don’t need to provide
    as many few-shot samples to the prompt, leading to shorter prompts and lower costs.
    We will discuss the few-shot prompting technique further in [*Chapter 5*](B21443_05.xhtml#_idTextAnchor098).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本效益**：虽然微调时会有初始的前期成本，但一旦模型被微调，您就不需要向提示提供那么多的少样本样本，这导致提示更短，成本更低。我们将在[*第五章*](B21443_05.xhtml#_idTextAnchor098)中进一步讨论少样本提示技术。'
- en: '**Improved latency**: Smaller prompts also mean lower latency requests as fewer
    resources are needed by the LLM to process your API call.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降低延迟**：较小的提示也意味着较低的延迟请求，因为LLM处理您的API调用所需的资源更少。'
- en: '**Consistent results**: Fine-tuning an LLM with a domain-specific dataset enhances
    the consistency and accuracy of its responses within that domain. For example,
    training a general language model with a dataset of medical research papers not
    only enhances its response accuracy but also ensures consistent output in that
    field across multiple queries. For instance, when the model is asked to “Describe
    the typical symptoms of Type 2 Diabetes,” a fine-tuned model might accurately
    and consistently respond, “Typical symptoms of Type 2 Diabetes include increased
    thirst, frequent urination, hunger, fatigue, and blurred vision.” This specialized
    training ensures the model provides more reliable information for medical inquiries,
    maintaining this consistency across similar queries.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致的结果**：使用特定领域的数据集微调LLM可以增强其在该领域内响应的一致性和准确性。例如，使用医学研究论文数据集训练通用语言模型不仅提高了其响应的准确性，还确保了在该领域内多个查询的一致输出。例如，当模型被要求“描述2型糖尿病的典型症状”时，微调模型可能会准确且一致地回答，“2型糖尿病的典型症状包括口渴增加、频繁排尿、饥饿、疲劳和视力模糊。”这种专业训练确保模型为医学查询提供更可靠的信息，并在类似查询中保持这种一致性。'
- en: In this section, we explored the “What” and “Why” of fine tuning. Now let's
    understand some real-world use cases where fine-tuning can add value to your AI
    application.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了微调的“是什么”和“为什么”。现在让我们了解一些现实世界的用例，在这些用例中，微调可以为您的AI应用增加价值。
- en: Fine-tuning applications
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调应用
- en: 'Fine-tuning can be applied to a wide range of natural language processing tasks,
    including the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 微调可以应用于广泛的自然语言处理任务，包括以下内容：
- en: '**Text classification**: This involves classifying text into predefined categories
    by examining its content or context. For example, in sentiment analysis of customer
    reviews, we can classify text as positive, negative, or neutral.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本分类**：这涉及到通过检查其内容或上下文将文本分类到预定义的类别中。例如，在客户评论的情感分析中，我们可以将文本分类为正面、负面或中性。'
- en: '**Token classification**: This involves labeling words in a piece of text,
    often to spot names or specific entities. For example, when applying named entity
    recognition to text, we can identify people, cities, and more.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记分类**：这涉及到对文本中的单词进行标记，通常是为了识别名称或特定实体。例如，当将命名实体识别应用于文本时，我们可以识别人名、城市等。'
- en: '**Question-answering**: This involves providing effective answers to questions
    in natural language.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问答**：这涉及到以自然语言提供有效的答案。'
- en: '**Summarization**: This involves providing concise summaries of long texts
    – for example, summarizing a news article.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**摘要**：这涉及到提供长文本的简洁摘要——例如，总结新闻文章。'
- en: '**Language translation**: This involves converting text from one language into
    another. An example of this is translating a document from English into Spanish.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言翻译**：这涉及到将文本从一种语言转换为另一种语言。例如，将文档从英语翻译成西班牙语。'
- en: 'The aforementioned fine-tuning tasks are the most popular ones. This is a rapidly
    evolving field, and more tasks are emerging and can be found on Hugging Face (source:
    [https://huggingface.co/docs/transformers/training](https://huggingface.co/docs/transformers/training))
    and Azure’s Machine Learning Studio (Model Catalog) too.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 上述微调任务是其中最受欢迎的。这是一个快速发展的领域，更多任务正在出现，可以在 Hugging Face ([https://huggingface.co/docs/transformers/training](https://huggingface.co/docs/transformers/training))
    和 Azure 的机器学习工作室（模型目录）上找到。
- en: Each time, it refines the general-purpose language model into a task-specific
    expert. Models can also be customized without the need to update their weights.
    This process is called in-context learning or few-shot learning. We will cover
    this in [*Chapter 5*](B21443_05.xhtml#_idTextAnchor098), which focuses on prompt
    engineering.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 每次都将其通用语言模型精炼为特定任务的专家。模型也可以在不更新其权重的需要下进行定制。这个过程被称为情境学习或少样本学习。我们将在[*第五章*](B21443_05.xhtml#_idTextAnchor098)中介绍这一内容，该章节专注于提示工程。
- en: 'Before we delve into the different fine-tuning techniques, it is also crucial
    to understand the preceding step in fine-training LLM models: pre-training. This
    foundational training phase sets the stage for LLMs, preparing them for the tailored
    adjustments of fine-tuning. In the upcoming section, we’ll contrast pre-training
    with fine-tuning, emphasizing the unique benefits and improvements of the latter.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨不同的微调技术之前，了解微调 LLM 模型的前一步骤——预训练——也是至关重要的。这个基础训练阶段为 LLM 奠定了基础，为微调的定制调整做好了准备。在接下来的部分，我们将对比预训练和微调，强调后者独特的优势和改进。
- en: Examining pre-training and fine-tuning processes
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查预训练和微调过程
- en: 'Pre-training and fine-tuning are two key stages when training LLMs such as
    GPT-3.5\. Pre-training is like a student’s general education in that it covers
    a broad range of subjects to provide foundational knowledge. Fine-tuning, on the
    other hand, is like a student later specializing in a specific subject in college,
    refining their skills for a particular field. In the context of LLMs, pre-training
    sets the broad base, and fine-tuning narrows the focus to excel in specific tasks.
    In this section, we’ll look at pre-training and fine-tuning to see how fine-tuning
    adds value:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练和微调是训练 GPT-3.5 等语言模型时的两个关键阶段。预训练类似于学生的通识教育，它涵盖广泛的学科以提供基础知识。另一方面，微调则类似于学生在大学后期专注于某一特定学科，为特定领域磨练技能。在
    LLM 的背景下，预训练设定了广泛的基础，而微调则将焦点缩小，以在特定任务中表现出色。在本节中，我们将探讨预训练和微调，看看微调是如何增加价值的：
- en: '![Figure 3.2 – Two-step LLM training process](img/B21443_03_02(Ori).jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2 – 两步 LLM 训练过程](img/B21443_03_02(Ori).jpg)'
- en: Figure 3.2 – Two-step LLM training process
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – 两步 LLM 训练过程
- en: Let’s provide an overview of the two stages.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们概述这两个阶段。
- en: Pre-training process
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练过程
- en: Pre-training is the initial phase of training a language model. During this
    phase, the model learns from a massive amount of text data, often referred to
    as the “pre-training corpus.” The goal of pre-training is to help the model learn
    grammar, syntax, context, and even some world knowledge from the text. The model
    is trained to predict the next word in a sentence, given the previous words. The
    result of pre-training is a model that has learned a general understanding of
    language and can generate coherent text. However, it lacks specificity and the
    ability to generate targeted or domain-specific content.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练是训练语言模型的初始阶段。在这个阶段，模型从大量的文本数据中学习，通常被称为“预训练语料库”。预训练的目标是帮助模型从文本中学习语法、句法、上下文甚至一些世界知识。模型被训练来预测句子中的下一个单词，给定前面的单词。预训练的结果是一个已经学习到语言的一般理解的模型，可以生成连贯的文本。然而，它缺乏特定性和生成目标或领域特定内容的能力。
- en: 'The foundation for creating more advanced models lies in utilizing pristine
    and smart training data. The following figure shows the datasets that are used
    for pretraining OpenAI’s GPT-3 models. These datasets underwent data preparation
    to remove duplicates and ensure diversity and lack of bias before being used for
    pre-training:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 创建更高级模型的基石在于利用原始且智能的训练数据。以下图显示了用于预训练 OpenAI 的 GPT-3 模型的数据集。这些数据集在用于预训练之前经过了数据准备，以去除重复项并确保多样性和无偏见：
- en: '![Figure 3.3 – Datasets used for pretraining OpenAI’s GPT-3 models](img/B21443_03_2.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3 – 用于预训练 OpenAI 的 GPT-3 模型的数据集](img/B21443_03_2.jpg)'
- en: Figure 3.3 – Datasets used for pretraining OpenAI’s GPT-3 models
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – 用于预训练 OpenAI 的 GPT-3 模型的数据集
- en: 'For instance, Llama models, by Meta, were developed using the following publicly
    available datasets, after thorough data purification and deduplication:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Meta开发的Llama模型使用以下公开可用的数据集开发，经过彻底的数据净化和去重：
- en: '![Figure 3.4 – Llama model pre-training data](img/B21443_03_3.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图3.4 – Llama模型预训练数据](img/B21443_03_3.jpg)'
- en: Figure 3.4 – Llama model pre-training data
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 – Llama模型预训练数据
- en: This training dataset consisted of 1.4 trillion tokens after tokenization. We
    discussed the concept of tokens briefly in [*Chapter 2*](B21443_02.xhtml#_idTextAnchor036)
    and will discuss it in more detail in [*Chapter 5*](B21443_05.xhtml#_idTextAnchor098).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这个训练数据集在标记化后由1.4万亿个标记组成。我们在[*第2章*](B21443_02.xhtml#_idTextAnchor036)中简要讨论了标记的概念，将在[*第5章*](B21443_05.xhtml#_idTextAnchor098)中更详细地讨论。
- en: Fine-tuning process
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调过程
- en: 'Fine-tuning is the second phase of training a language model and occurs after
    pre-training. During this phase, the model is trained on a more specific dataset
    that is carefully curated and customized for a particular task or domain. This
    dataset is often referred to as the “fine-tuning dataset.” The model is fed with
    data from the fine-tuning dataset, following which it predicts the next tokens
    and evaluates its predictions against the actual, or “ground truth,” values. In
    this process, it tries to minimize the loss. By doing this repetitively, the LLM
    becomes fine-tuned to the downstream task:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是训练语言模型的第二个阶段，发生在预训练之后。在这个阶段，模型在更具体的数据集上进行训练，这些数据集是经过精心策划和定制的，用于特定的任务或领域。这个数据集通常被称为“微调数据集”。模型被喂入微调数据集的数据，随后预测下一个标记，并将预测与实际或“真实”值进行比较。在这个过程中，它试图最小化损失。通过重复这样做，LLM
    就会针对下游任务进行微调：
- en: '![Figure 3.5 – The process of fine-tuning](img/B21443_03_4.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图3.5 – 微调过程](img/B21443_03_4.jpg)'
- en: Figure 3.5 – The process of fine-tuning
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 – 微调过程
- en: The preceding diagram depicts a language model’s journey from pre-training to
    fine-tuning. Initially, it’s trained on a broad dataset sourced from diverse internet
    texts capturing a variety of language constructs, topics, and styles. Subsequently,
    it’s refined using a targeted, high-quality dataset with domain-specific prompts
    and completions. Ultimately, the data quality of this fine-tuning dataset dictates
    the model’s output precision. Finally, the fine-tuned model interacts with a user
    through queries and responses, catering to a particular downstream task. As discussed
    earlier, these downstream tasks could include text classification, token classification,
    question-answering, summarization, translation, and more.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表描述了语言模型从预训练到微调的过程。最初，它在来自各种互联网文本的广泛数据集上进行训练，这些文本涵盖了各种语言结构、主题和风格。随后，它使用具有特定领域提示和补全的针对性、高质量数据集进行细化。最终，这个微调数据集的数据质量决定了模型的输出精度。最后，微调模型通过查询和响应与用户互动，满足特定的下游任务。如前所述，这些下游任务可能包括文本分类、标记分类、问答、摘要、翻译等等。
- en: So far, we’ve explored the overarching concept of fine-tuning, weighing its
    advantages and limitations. Now, let’s delve into some basic and advanced fine-tuning
    techniques.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了微调的总体概念，权衡了其优势和局限性。现在，让我们深入了解一些基本的和高级的微调技术。
- en: Techniques for fine-tuning models
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调模型的技巧
- en: 'In this section, we’ll discuss two fine-tuning methods: the traditional full
    fine-tuning approach and advanced techniques such as PEFT, which integrates optimizations
    to attain comparable results to full fine-tuning but with higher efficiency and
    reduced memory and computational expenses.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论两种微调方法：传统的完全微调方法和先进的技巧，如PEFT，它通过集成优化来实现与完全微调相当的结果，但具有更高的效率和更低的内存和计算成本。
- en: Full fine-tuning
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完全微调
- en: Full fine-tuning refers to the approach where all parameters/weights of a pretrained
    model are adjusted using a task-specific dataset. It’s a straightforward method
    and is generally effective, but it might require a considerable amount of data
    to avoid overfitting and compute, especially for large models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 完全微调是指使用特定任务的数据集调整预训练模型的所有参数/权重的方法。这是一个直接的方法，通常效果良好，但可能需要大量的数据来避免过拟合和计算，尤其是对于大型模型。
- en: 'The challenges with generic full fine-tuning methods include updating all the
    model parameters of the LLMs for every downstream task. Here are some more issues
    to consider:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通用完全微调方法的挑战包括为每个下游任务更新 LLM 的所有模型参数。以下是一些需要考虑的更多问题：
- en: '**High compute and memory requirements**: Full fine-tuning can increase the
    cost of compute exorbitantly, result in large memory requirements, and also result
    in having to update billions or trillions of parameters in the state-of-the-art
    models, which could become unwieldy and inefficient.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高计算和内存需求**：全量微调可能会极大地增加计算成本，导致大量内存需求，并需要在最先进的模型中更新数十亿或数万亿个参数，这可能会变得难以管理且效率低下。'
- en: '**Catastrophic forgetting**: Full fine-tuning is prone to forgetting old information
    once it’s fine-tuned on new information.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灾难性遗忘**：全量微调一旦在新信息上进行微调，就很容易忘记旧信息。'
- en: '**Multiple copies of the LLM**: Fine-tuning requires building a full copy of
    the LLM for every task, such as sentiment analysis, machine translation, and question-answering
    tasks, thus increasing storage requirements. LLMs can be gigabytes in size sometimes
    and building multiple copies of them for different downstream tasks may require
    a lot of storage space.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多个LLM副本**：微调需要为每个任务（如情感分析、机器翻译和问答任务）构建LLM的完整副本，从而增加存储需求。LLM有时可能达到几GB的大小，为不同的下游任务构建多个副本可能需要大量的存储空间。'
- en: To tackle these challenges and make this process more efficient, a new fine-tuning
    technique has emerged called PEFT that trains a small set of parameters, which
    might be a subset of the existing model parameters or a set of newly added parameters,
    to achieve similar or better performance to the traditional fine-tuning methods
    under different scenarios. By doing this, it provides almost similar results with
    a lower cost in terms of compute and fewer parameter updates.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战并使这个过程更加高效，出现了一种新的微调技术，称为PEFT，它通过训练一小组参数（可能是现有模型参数的子集或一组新添加的参数）来实现与传统微调方法在不同场景下相似或更好的性能。通过这种方式，它几乎提供了相同的结果，但在计算和参数更新方面的成本更低。
- en: In the next section, we will discuss different types of PEFT techniques and
    the trade-offs between them.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论不同类型的PEFT技术及其之间的权衡。
- en: PEFT
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PEFT
- en: 'PEFT addresses the challenges with full fine-tuning by training a smaller set
    of parameters. In this section, we will discuss various techniques on how such
    efficiency can be achieved by training a smaller set of parameters. These parameters
    could either be a subset of the current model’s parameters or a new set of added
    parameters. These techniques vary in terms of parameter efficiency, memory efficiency,
    and training speed, though model quality and any potential extra inference costs
    are also distinguishing factors among these methods. PEFT techniques can be broadly
    classified into three categories:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: PEFT通过训练一组较小的参数集来解决全量微调的挑战。在本节中，我们将讨论各种技术，这些技术通过训练较小的参数集来实现效率。这些参数可以是当前模型参数的子集，也可以是新增的一组参数。这些技术在参数效率、内存效率和训练速度方面有所不同，尽管模型质量和任何潜在额外的推理成本也是这些方法之间区分的因素。PEFT技术可以大致分为三类：
- en: Selective
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择性
- en: Additive
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加法
- en: Reparameterization
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新参数化
- en: 'The following figure shows 30 PEFT methods that were discussed in 40 research
    papers published between February 2019 and February 2023:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了2019年2月至2023年2月间发表的40篇研究论文中讨论的30种PEFT方法：
- en: '![Figure 3.6 – PEFT methods that were discussed in research papers published
    between 2019 and 2023](img/B21443_03_5.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图3.6 – 2019年至2023年间研究论文中讨论的PEFT方法](img/B21443_03_5.jpg)'
- en: Figure 3.6 – PEFT methods that were discussed in research papers published between
    2019 and 2023
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 – 2019年至2023年间研究论文中讨论的PEFT方法
- en: 'This diagram was taken from a survey published in the paper *Scale Down to
    Scale Up: A Guide to* *Parameter-Efficient Tuning*.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此图摘自发表在论文《缩小规模以扩大规模：参数高效调优指南》中的调查。
- en: We will dive into each of these categories in this section but only cover the
    most important PEFT techniques that have shown promising results.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨这些类别，但只涵盖那些已经显示出良好结果的最重要的PEFT技术。
- en: Additive
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加法
- en: The core concept of additive methods involves fine-tuning a model by adding
    extra parameters or layers, exclusively training these new parameters, and keeping
    the original model weights frozen. Although these techniques introduce new parameters
    to the network, they effectively reduce training times and increase memory efficiency
    by decreasing the size of gradients and the optimizer states. This is the most
    widely explored category of PEFT methods. A prominent method under this category
    is prompt tuning with soft prompts.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 加性方法的核心概念是通过添加额外的参数或层来微调模型，专门训练这些新参数，并保持原始模型权重冻结。尽管这些技术向网络引入了新的参数，但通过减少梯度和优化器状态的大小，它们有效地减少了训练时间并提高了内存效率。这是
    PEFT 方法中最广泛探索的类别。这个类别中的一个突出方法是带软提示的提示调整。
- en: Prompt tuning with soft prompts
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 带软提示的提示调整
- en: This type of tuning involves freezing the model weights and updating the prompt
    parameters instead of model parameters like in model fine-tuning. When you freeze
    the weights of a model, you prevent them from being updated during training. These
    weights remain the same throughout the fine-tuning process. It is a very compute
    and energy-efficient technique compared to traditional fine-tuning. Prompt tuning
    should not be confused with prompt engineering, which we will discuss in [*Chapter
    5*](B21443_05.xhtml#_idTextAnchor098). To understand prompt tuning better, we
    need to understand the concept of soft prompts and embedding space.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这种调整涉及冻结模型权重，而不是像模型微调那样更新模型参数，而是更新提示参数。当你冻结模型的权重时，你防止它们在训练过程中被更新。这些权重在整个微调过程中保持不变。与传统的微调相比，这是一种计算和能源效率非常高的技术。提示调整不应与提示工程混淆，我们将在[*第五章*](B21443_05.xhtml#_idTextAnchor098)中讨论。为了更好地理解提示调整，我们需要了解软提示和嵌入空间的概念。
- en: Soft prompts and embedding space
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 软提示和嵌入空间
- en: An embedding vector space is a high-dimensional space where words, phrases,
    or other types of data are represented as vectors such that semantically similar
    items are located close to each other in the space. In the context of natural
    language processing, these embeddings capture semantic meanings and relationships
    between words or sentences, allowing for operations that can infer similarities,
    analogies, and other linguistic patterns.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入向量空间是一个高维空间，其中单词、短语或其他类型的数据被表示为向量，使得语义上相似的项目在空间中彼此靠近。在自然语言处理的情况下，这些嵌入捕捉了单词或句子之间的语义含义和关系，允许进行可以推断相似性、类比和其他语言模式的操作。
- en: '![Figure 3.7 – Soft prompts versus hard prompts](img/B21443_03_6.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.7 – 软提示与硬提示](img/B21443_03_6.jpg)'
- en: Figure 3.7 – Soft prompts versus hard prompts
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – 软提示与硬提示
- en: The above figure depicts a 3D embedding vector space along the *X*, *Y*, and
    *Z* axes. Representing natural language through tokens is considered to be challenging
    because each token is associated with a specific location in the embedding vector
    space. Hence, they are also referred to as hard prompts. On the other hand, soft
    prompts are not confined to fixed, discrete words in natural language and can
    assume any value in the multi-dimensional embedding vector space. In the following
    figure, words such as “jump,” “fox,” and others are hard prompts, whereas the
    unlabeled black-colored token is a soft prompt.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了沿 *X*、*Y* 和 *Z* 轴的 3D 嵌入向量空间。将自然语言通过标记表示被认为是具有挑战性的，因为每个标记都与嵌入向量空间中的一个特定位置相关联。因此，它们也被称为硬提示。另一方面，软提示不受自然语言中固定、离散单词的限制，可以在多维嵌入向量空间中取任何值。在下面的图中，“跳跃”、“狐狸”等单词是硬提示，而未标记的黑色标记是软提示。
- en: Prompt tuning process
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提示调整过程
- en: In prompt tuning, soft prompts, also known as virtual tokens, are concatenated
    with the prompts; it’s left to a supervised training process to determine the
    optimal values. As shown in the following figure, these trainable soft tokens
    are prepended to an embedding vector representation – in this case, “The student
    learns science:”
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示调整中，软提示，也称为虚拟标记，与提示连接；最优值的确定留给监督训练过程。如图所示，这些可训练的软标记被添加到嵌入向量表示之前——在这种情况下，“学生学习科学：”
- en: '![Figure 3.8 – Soft prompt concatenation](img/B21443_03_7.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.8 – 软提示连接](img/B21443_03_7.jpg)'
- en: Figure 3.8 – Soft prompt concatenation
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 – 软提示连接
- en: 'The following figure provides a more detailed representation of the process.
    Vectors are attached to the beginning of each embedded input vector and fed into
    the model, the prediction is compared to the target to calculate a loss, and the
    error is backpropagated to calculate gradients, but only the new learnable vectors
    are updated, keeping the core model frozen. In other words, we are searching the
    embedding space for the best representation of the prompt that the LLMs should
    accept. Even though we can’t easily understand soft prompts learned this way,
    they can help us figure out how to do a task using the labeled dataset, doing
    the same job as text prompts written by hand but without being limited to specific
    words or phrases:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 下图提供了该过程的更详细表示。向量附加到每个嵌入输入向量的开头，并输入到模型中，预测与目标进行比较以计算损失，错误反向传播以计算梯度，但只更新新的可学习向量，保持核心模型冻结。换句话说，我们正在搜索嵌入空间中
    LLM 应接受的提示的最佳表示。尽管我们难以理解以这种方式学习的软提示，但它们可以帮助我们了解如何使用标记数据集完成任务，与手工编写的文本提示做相同的工作，但不受特定单词或短语的限制：
- en: '![Figure 3.9 – Prompt tuning process (detailed)](img/B21443_03_8.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.9 – 提示调整过程（详细）](img/B21443_03_8.jpg)'
- en: Figure 3.9 – Prompt tuning process (detailed)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 – 提示调整过程（详细）
- en: 'Next, we’ll compare three methods: model tuning (full fine-tuning), prompt
    tuning, and prompt design (prompt engineering). As shown in *Figure 3**.10*, research
    conducted by Google shows the difference between model tuning, prompt tuning,
    and prompt design (*Guiding Frozen Language Models with Learned Soft Prompts*,
    QUINTA-FEIRA, FEVEREIRO 10, 2022, posted by Brian Lester, AI Resident, and Noah
    Constant, Senior Staff Software Engineer, Google Research).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将比较三种方法：模型调整（全面微调）、提示调整和提示设计（提示工程）。如图 *图 3.10* 所示，Google 进行的研究显示了模型调整、提示调整和提示设计之间的差异（通过学习软提示引导冻结语言模型，QUINTA-FEIRA，2022
    年 2 月 10 日，由 Google 研究的 AI 居民 Brian Lester 和高级软件工程师 Noah Constant 发布）。
- en: 'Model tuning (full fine-tuning):'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 模型调整（全面微调）：
- en: This method starts with a pre-trained model that is then further trained (or
    “tuned”) on a specific task using additional input data. The model becomes more
    specialized in this process.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种方法从预训练模型开始，然后使用额外的输入数据在该特定任务上进行进一步训练（或“调整”）。在这个过程中，模型变得更加专业化。
- en: This method represents “strong task performance” as the model gets more aligned
    with the particular task.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种方法将“强任务性能”表示为模型与特定任务更加一致。
- en: 'Prompt tuning:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 提示调整：
- en: Instead of tuning the entire model, only the prompt or input to the model is
    adjusted. The main model remains “frozen” or unchanged.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与调整整个模型不同，只调整模型中的提示或输入。主要模型保持“冻结”或不变。
- en: This introduces the concept of “tunable soft prompts,” which can be adjusted
    to get desired outputs from the model.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这引入了“可调整软提示”的概念，可以通过调整来从模型中获得所需的输出。
- en: This method combines the general capabilities of the pre-trained model with
    a more task-specific approach, leading to “efficient multitask serving.”
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种方法结合了预训练模型的一般能力与更具体的任务方法，从而实现“高效的多任务服务”。
- en: 'Prompt design (prompt engineering):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 提示设计（提示工程）：
- en: The focus is on designing a very specific input or prompt to guide the pre-trained
    model to produce the desired output.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重点在于设计一个非常具体的输入或提示，以引导预训练模型产生所需的输出。
- en: Like prompt tuning, the main model remains “frozen”.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与提示调整类似，主要模型保持“冻结”状态。
- en: This method is about exploiting the vast knowledge and capabilities of the pre-trained
    model by just crafting the right input. As mentioned earlier, we will cover prompt
    engineering in detail in [*Chapter 5*](B21443_05.xhtml#_idTextAnchor098).
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种方法是通过精心构建正确的输入来利用预训练模型的广泛知识和能力。如前所述，我们将在[*第五章*](B21443_05.xhtml#_idTextAnchor098)中详细介绍提示工程。
- en: 'In prompt tuning and prompt design, original model weights remain frozen, whereas
    in model tuning model parameters are updated:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示调整和提示设计中，原始模型权重保持冻结，而在模型调整中，模型参数被更新：
- en: '![Figure 3.10 – Model tuning, prompt tuning, and prompt design](img/B21443_03_9.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.10 – 模型调整、提示调整和提示设计](img/B21443_03_9.jpg)'
- en: Figure 3.10 – Model tuning, prompt tuning, and prompt design
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 – 模型调整、提示调整和提示设计
- en: 'The following figure demonstrates model tuning (full fine-tuning) on the left
    and prompt tuning on the right. Tuning a model for a specific task necessitates
    creating a task-specific version of the entire pre-trained model for each downstream
    task, and separate batches of data must be used for inference. On the other hand,
    prompt tuning only necessitates storing a small, task-specific prompt for each
    task, allowing for mixed-task inference using the original pre-trained model.
    With a T5 “XXL” model, each tuned version of the model necessitates 11 billion
    parameters. In comparison, our tuned prompts only necessitate 20,480 parameters
    for each task, which is a reduction of over five orders of magnitude, assuming
    a prompt length of 5 tokens:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了左侧的模型调整（完全微调）和右侧的提示调整。为特定任务调整模型需要为每个下游任务创建整个预训练模型的特定版本，并且必须使用单独的数据批次进行推理。另一方面，提示调整只需要为每个任务存储一个小型的、特定于任务的提示，允许使用原始预训练模型进行混合任务推理。使用
    T5 “XXL” 模型，每个调整后的模型版本需要 110 亿个参数。相比之下，我们的调整提示只需要每个任务 20,480 个参数，假设提示长度为 5 个标记，这减少了超过五个数量级：
- en: '![Figure 3.11 – Model tuning versus prompt tuning](img/B21443_03_10.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.11 – 模型调整与提示调整](img/B21443_03_10.jpg)'
- en: Figure 3.11 – Model tuning versus prompt tuning
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11 – 模型调整与提示调整
- en: 'Now, let’s look at the benefits of prompt tuning compared to prompt engineering
    and model fine-tuning:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看与提示工程和模型微调相比，提示调整的优点：
- en: Compared to model fine-tuning, prompt tuning does not require copies of the
    LLMs to be created for every task, thus resulting in a reduction in storage space
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与模型微调相比，提示调整不需要为每个任务创建 LLM 的副本，从而减少了存储空间
- en: Compared to few-shot prompt engineering, prompt tuning is not restricted to
    context length or a limited number of examples
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与少样本提示工程相比，提示调整不受上下文长度或有限示例数量的限制
- en: Instead of crafting the best manual prompt to generate the desired output, you
    can use backpropagation to automatically learn a new model
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 而不是精心制作最佳的手动提示以生成所需输出，你可以使用反向传播自动学习一个新的模型
- en: Resilient to domain shift
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对领域变化具有弹性
- en: 'The research paper *The Power of Scale for Parameter-Efficient Prompt Tuning*
    from Google highlights the experiment (*Figure 3**.12*) that was conducted on
    the T5 Transformer model. As per the evaluation, prompt tuning on the T5 model
    matched the quality of model tuning (or fine-tuning) as size increases, while
    enabling the reuse of a single frozen model for all tasks. This approach significantly
    outperforms few-shot prompt designs using GPT-3\. SuperGLUE is a benchmark that’s
    designed to comprehensively evaluate the performance of various natural language
    understanding models across a range of challenging language tasks. We will learn
    more about SuperGLUE in the upcoming sections of this chapter:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 来自谷歌的论文《参数高效提示调整的规模力量》突出了在 T5 变换器模型上进行的实验（*图 3**.12*）。根据评估，随着规模的增加，T5 模型上的提示调整与模型调整（或微调）的质量相匹配，同时允许所有任务重用单个冻结模型。这种方法在
    GPT-3 的少样本提示设计中表现显著。SuperGLUE 是一个基准，旨在全面评估各种自然语言理解模型在一系列具有挑战性的语言任务中的性能。我们将在本章接下来的部分中了解更多关于
    SuperGLUE 的信息：
- en: '![Figure 3.12 – Relationship between SuperGLUE Score and Model Parameters](img/B21443_03_11.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.12 – SuperGLUE 分数与模型参数之间的关系](img/B21443_03_11.jpg)'
- en: Figure 3.12 – Relationship between SuperGLUE Score and Model Parameters
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.12 – SuperGLUE 分数与模型参数之间的关系
- en: '*Figure 3**.12 shows the relationship between SuperGLUE Score and Model Parameters
    for different fine-tuning techniques. As scale increases, prompt tuning matches
    model tuning, despite tuning 25,000 times* *fewer parameters.*'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3**.12 展示了不同微调技术下 SuperGLUE 分数与模型参数之间的关系。随着规模的增加，尽管调整的参数减少了 25,000 次，提示调整与模型调整相匹配*。'
- en: 'The following GitHub repository from Google Research provides a code implementation
    of this experiment for prompt tuning: [https://github.com/google-research/prompt-tuning](https://github.com/google-research/prompt-tuning).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下来自谷歌研究团队的 GitHub 仓库提供了提示调整实验的代码实现：[https://github.com/google-research/prompt-tuning](https://github.com/google-research/prompt-tuning)。
- en: In terms of the downsides of prompt tuning, interpreting soft prompts can be
    challenging as these tokens are not fixed hard prompts and do not represent natural
    language. To understand the nearest meaning, you must convert the embeddings back
    into tokens and determine the top-k closest neighbors by measuring the cosine
    similarity. This is because the closest neighbors form a semantic group with semantic
    similarities.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示微调的缺点方面，解释软提示可能具有挑战性，因为这些标记不是固定的硬提示，也不代表自然语言。为了理解最接近的含义，必须将嵌入转换回标记，并通过测量余弦相似度确定最接近的k个邻居。这是因为最接近的邻居形成一个具有语义相似性的语义组。
- en: Reparameterization
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重参数化
- en: Regular full fine-tuning, which involves retraining all parameters in a language
    model, is not feasible as the model size grows. This can become computationally
    very expensive. Hence, researchers have identified a new method called reparameterization,
    a technique that’s used in fine-tuning to reduce the number of trainable parameters
    in a model while maintaining its effectiveness. These methods use low-rank transformation
    to reparameterize the weights, thus reducing the number of trainable parameters
    while still allowing the method to work with high-dimensional matrices such as
    the pre-trained parameters of the networks. Let’s explore a very popular reparameterization
    method called **Low-Rank** **Adaptation** (**LoRa**).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 常规全微调，涉及重新训练语言模型中的所有参数，随着模型规模的增加变得不可行。这可能会变得计算成本非常高。因此，研究人员确定了一种新的方法，称为重参数化，这是一种在微调中使用的技巧，旨在减少模型中的可训练参数数量，同时保持其有效性。这些方法使用低秩变换来重新参数化权重，从而减少可训练参数的数量，同时仍然允许该方法与高维矩阵（如网络的预训练参数）一起工作。让我们探讨一个非常流行的重参数化方法，称为**低秩****适应**（**LoRa**）。
- en: LoRA
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LoRA
- en: To enhance the efficiency of fine-tuning, LoRA utilizes a method where weight
    updates are depicted using two compact matrices via low-rank decomposition. This
    approach entails locking the pre-trained model weights and introducing trainable
    rank decomposition matrices into each layer of the Transformer architecture. Low-rank
    decomposition, often simply referred to as low-rank approximation, is a mathematical
    method that’s used to approximate a given matrix with the product of two lower-rank
    matrices. The primary goal of this technique is to capture the most important
    information contained in the original matrix while using fewer parameters or dimensions.
    The experimental results indicated that LoRa can reduce the number of trainable
    parameters by more than 96%.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高微调的效率，LoRA利用了一种方法，通过低秩分解使用两个紧凑的矩阵来表示权重更新。这种方法包括锁定预训练模型权重，并在Transformer架构的每一层引入可训练的低秩分解矩阵。低秩分解，通常简称为低秩近似，是一种数学方法，用于用两个低秩矩阵的乘积来近似给定的矩阵。这种技术的首要目标是捕获原始矩阵中包含的最重要信息，同时使用更少的参数或维度。实验结果表明，LoRa可以将可训练参数的数量减少超过96%。
- en: 'The following figure shows the difference between regular fine-tuning and LoRA.
    As you can see, the weight update, W_delta, that was identified during backpropagation
    in full fine-tuning is decomposed into two low-rank matrices in LoRA. W_a and
    W_b provide the same information as the original W_delta but in a more efficient
    representation:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了常规微调和LoRA之间的差异。如您所见，在全微调中通过反向传播确定的权重更新W_delta在LoRA中被分解为两个低秩矩阵。W_a和W_b提供了与原始W_delta相同的信息，但以更有效的方式表示：
- en: '![Figure 3.13 – Comparing regular full fine-tuning and LoRA](img/B21443_03_12.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图3.13 – 比较常规全微调和LoRA](img/B21443_03_12.jpg)'
- en: Figure 3.13 – Comparing regular full fine-tuning and LoRA
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13 – 比较常规全微调和LoRA
- en: As shown in the following table, researchers found that LoRa fine-tuning matches
    or outperforms full fine-tuning of GPT-3 by only updating 0.02 % (37.7M/175,255.8M)
    of the trainable parameters. With LoRA, the number of trainable parameters was
    reduced to 4.7M and 37.7M, from ~175B in full fine-tuning. The evaluation metrics
    were used for **ROUGE**, which we will discuss later in this chapter
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如下表所示，研究人员发现，通过仅更新可训练参数的0.02%（37.7M/175,255.8M），LoRa微调与GPT-3的全微调相匹配或优于全微调。使用LoRA，可训练参数的数量减少到4.7M和37.7M，从全微调的约175B。评估指标使用了**ROUGE**，我们将在本章后面讨论。
- en: '![Figure 3.14 – Fine-tuning efficiency with LoRA](img/B21443_03_13.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图3.14 – 使用LoRA的微调效率](img/B21443_03_13.jpg)'
- en: Figure 3.14 – Fine-tuning efficiency with LoRA
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14 – 使用LoRA的微调效率
- en: 'Now, let’s consider the benefits of LoRA:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑LoRA的好处：
- en: LoRA boosts fine-tuning efficiency by significantly cutting down trainable parameters
    and thus can be trained on a single GPU, avoiding the need for distributed cluster
    GPUs.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LoRA通过显著减少可训练参数来提高微调效率，因此可以在单个GPU上训练，避免了分布式集群GPU的需求。
- en: The original pre-trained weights stay unchanged, allowing for various lightweight
    LoRA models to be used on top for different tasks. This eliminates the need to
    create a full copy of the fine-tuned model for every downstream task.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始预训练权重保持不变，允许使用各种轻量级的LoRA模型在顶部执行不同的任务。这消除了为每个下游任务创建完整微调模型副本的需要。
- en: LoRA can be combined with many other PEFT techniques.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LoRA可以与许多其他PEFT技术相结合。
- en: LoRA fine-tuned models match the performance of fully fine-tuned ones.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LoRA微调模型与完全微调模型的表现相当。
- en: There’s no added serving latency with LoRA as adapter weights integrate with
    the base model and allow for quick task switching when deployed as a service.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LoRA作为适配器权重与基础模型集成时，没有增加服务延迟，允许快速任务切换。
- en: Selective
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择性
- en: The selective approach is the simplest method of fine-tuning as it only involves
    the top layers of the network. However, researchers have mentioned that while
    they might excel in scenarios involving smaller-scale data with model parameters
    numbering less than a billion, they can demand significant computational resources
    and memory compared to conventional fine-tuning methods when applied to larger
    networks. Hence, these methods should not be the first choice when choosing a
    PEFT method.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 选择性方法是最简单的微调方法，因为它只涉及网络的顶层。然而，研究人员提到，虽然它们可能在涉及规模较小、模型参数少于10亿的数据场景中表现出色，但与传统的微调方法相比，它们可能需要大量的计算资源和内存。因此，在选择PEFT方法时，这些方法不应是首选。
- en: '**BitFit** is one of the selective PEFT methods and fine-tunes only the biases
    of the network. BitFit updates a mere 0.05% of model parameters and initially
    showcased strong results comparable to or better than full fine-tuning in low-medium
    data scenarios for BERT models that consisted of under 1 billion parameters. When
    evaluated on larger networks, such as T0-3B or GPT-3, BitFit’s performance noticeably
    lags behind both full fine-tuning and other PEFT methods.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**BitFit**是选择性PEFT方法之一，仅微调网络的偏差。BitFit仅更新了0.05%的模型参数，最初在低中数据场景下展示了与完整微调相当甚至更好的结果，这些BERT模型包含的参数少于10亿。当评估更大型的网络，如T0-3B或GPT-3时，BitFit的性能明显落后于完整微调和其他PEFT方法。'
- en: Other important selective PEFT techniques include DiffPruning, FishMask, Freeze,
    and Reconfigure.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 其他重要的选择性PEFT技术包括DiffPruning、FishMask、Freeze和Reconfigure。
- en: 'Having understood fine-tuning, let’s explore a related method that augments
    the fine-tuning process: RLHF. This method leverages human insights to further
    tailor model behaviors and outputs, aligning them more closely with human values
    and expectations. Let’s delve into how RLHF works and its significance in the
    fine-tuning landscape.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解了微调之后，让我们探索一种与微调过程相关的方法：RLHF。这种方法利用人类洞察力进一步调整模型行为和输出，使其更接近人类价值观和期望。让我们深入了解RLHF的工作原理及其在微调领域的重要性。
- en: RLHF – aligning models with human values
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RLHF – 将模型与人类价值观对齐
- en: 'Fine-tuning can be beneficial for achieving specific tasks, thus enhancing
    accuracy and improving model adaptability, but models can sometimes exhibit undesirable
    behavior. They might result in harmful language, displaying aggression, or even
    sharing detailed guidance on dangerous subjects such as weapons or explosive manufacturing.
    Such behaviors could be detrimental to society. This stems from the fact that
    models are trained on extensive internet data, which can contain malicious content.
    Both the pre-training phase and the fine-tuning process might yield outcomes that
    are counterproductive, hazardous, or misleading. Hence, it’s imperative to make
    sure that models resonate with human ethics and values. An added refinement step
    should integrate the three fundamental human principles: **helpfulness, harmlessness,
    and honesty** (**HHH**). RLHF is a method of training machine learning models,
    particularly in the context of **reinforcement learning** (**RL**), that uses
    feedback from humans. To understand RLHF, we must understand the concept of RL:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 微调对于实现特定任务是有益的，从而提高准确性和改进模型适应性，但模型有时可能会表现出不受欢迎的行为。它们可能会产生有害的语言，表现出攻击性，甚至分享有关危险主题（如武器或爆炸物制造）的详细指导。这些行为可能会对社会造成损害。这源于模型是在包含恶意内容的广泛互联网数据上训练的。预训练阶段和微调过程可能会产生反效果、危险或误导的结果。因此，确保模型与人类的伦理和价值观相一致至关重要。额外的细化步骤应整合三个基本的人类原则：**有用性、无害性和诚实性**（**HHH**）。RLHF是一种训练机器学习模型的方法，特别是在强化学习（RL）的背景下，它使用来自人类的反馈。为了理解RLHF，我们必须了解RL的概念：
- en: '**RL**: This is a type of machine learning where an agent learns to make decisions
    by taking actions in an environment to maximize some notion of cumulative reward.
    The agent interacts with the environment, receives feedback in the form of rewards
    or penalties, and adjusts its actions accordingly. For example, a chess-playing
    AI improves its strategies by earning points for winning moves and losing points
    for blunders.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RL**：这是一种机器学习类型，其中代理通过在环境中采取行动以最大化某种累积奖励的概念来学习做出决策。代理与环境互动，以奖励或惩罚的形式接收反馈，并相应地调整其行动。例如，一个下棋的AI通过赢得棋局获得积分来改进其策略，而犯错误则失去积分。'
- en: RLHF is a type of RL where the traditional reward signal, which usually comes
    from the environment, is replaced or augmented with feedback from humans. Initially,
    a model is trained to imitate human behavior. Then, instead of relying solely
    on environmental rewards, humans provide feedback by comparing different action
    sequences or trajectories. This human feedback is used to train a reward model,
    which then guides the agent’s learning process, helping it improve its decisions
    and actions in the environment. The core components of RLHF are the reward model
    and the RL algorithm.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RLHF是一种RL，其中传统的奖励信号，通常来自环境，被来自人类的反馈所取代或增强。最初，模型被训练来模仿人类行为。然后，而不是完全依赖环境奖励，人类通过比较不同的动作序列或轨迹来提供反馈。这种人类反馈用于训练奖励模型，然后指导代理的学习过程，帮助它在环境中改善其决策和行动。RLHF的核心组件是奖励模型和RL算法。
- en: '**Reward model:** In the context of RL, a reward model is a model that provides
    a numerical reward signal to an agent based on the actions it takes in a given
    state. Instead of manually designing a reward function, which can be challenging
    and error-prone, a reward model is learned from data, often incorporating human
    feedback.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励模型**：在强化学习的背景下，奖励模型是一种根据代理在给定状态下采取的行动提供数值奖励信号给代理的模型。而不是手动设计奖励函数，这可能会很具挑战性和容易出错，奖励模型通常从数据中学习，通常结合人类反馈。'
- en: '**Human feedback:** As shown in the following figure, the outputs from LLM
    models are ranked by humans with a scoring system and then fed into the reward
    model. After the learning process, the reward model is used to teach the agent
    what is helpful, harmless, and honest by showing examples or providing interactive
    feedback:'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人类反馈**：如图所示，LLM模型的输出由人类通过评分系统进行排序，然后输入到奖励模型中。在学习过程中，奖励模型通过展示示例或提供交互式反馈，教会代理什么是有帮助的、无害的和诚实的：'
- en: '![Figure 3.15 – Reward model training process from Hugging Face (source: https://huggingface.co/blog/rlhf)](img/B21443_03_14.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图3.15 – 来自Hugging Face的奖励模型训练过程（来源：https://huggingface.co/blog/rlhf）](img/B21443_03_14.jpg)'
- en: 'Figure 3.15 – Reward model training process from Hugging Face (source: [https://huggingface.co/blog/rlhf](https://huggingface.co/blog/rlhf))'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.15 – 来自Hugging Face的奖励模型训练过程（来源：[https://huggingface.co/blog/rlhf](https://huggingface.co/blog/rlhf)）
- en: '**RL algorithm**: The RL algorithm utilizes inputs from reward models to refine
    the LLMs, enhancing the reward score progressively. A popular choice of RL algorithm
    is proximal policy optimization. As shown in the following figure, first, the
    LLM generates an output that is evaluated by the reward model quantitatively to
    provide a reward score of 1.79\. This reward is sent to the RL algorithm, which,
    in turn, updates the LLM weights. A very popular choice of RL algorithm that has
    emerged recently is the PPO. Understanding the inner details of PPO is beyond
    the scope of this book, but more information can be found in the research paper
    *Proximal Policy Optimization Algorithms*, from Open AI:'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RL算法**：RL算法利用奖励模型的输入来改进LLM，逐步提高奖励分数。一种流行的RL算法是近端策略优化。如图所示，首先，LLM生成一个输出，该输出由奖励模型定量评估，以提供1.79的奖励分数。这个奖励被发送到RL算法，然后RL算法更新LLM的权重。最近出现的一种非常流行的RL算法是PPO。理解PPO的内部细节超出了本书的范围，但更多信息可以在Open
    AI的研究论文《近端策略优化算法》中找到：'
- en: '![Figure 3.16 – Updating LLMs with the RL algorithm](img/B21443_03_15.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图3.16 – 使用RL算法更新LLM](img/B21443_03_15.jpg)'
- en: Figure 3.16 – Updating LLMs with the RL algorithm
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16 – 使用RL算法更新LLM
- en: '**Potential risks – reward hacking**: In RL, an agent seeks to maximize a reward
    model provided by the environment. However, sometimes, the agent finds unintended
    shortcuts or loopholes to get high rewards without actually solving the task as
    intended. This is known as “reward hacking.” This may lead to an RLuUpdated LLM
    that generates grammatically incorrect sentences, gibberish sentences, or exaggerated
    positive sentences to maximize rewards. To mitigate this, PPO establishes a boundary
    on the magnitude of policy modifications. This limitation is implemented through
    the use of **Kullback-Leibler** (**KL**)-divergence.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在风险 - 奖励黑客攻击**：在强化学习中，智能体寻求最大化环境提供的奖励模型。然而，有时智能体会发现未经意料的捷径或漏洞，在不实际解决预期任务的情况下获得高奖励。这被称为“奖励黑客攻击”。这可能导致RLuUpdated
    LLM生成语法错误的句子、无意义的句子或夸张的积极句子以最大化奖励。为了减轻这一点，PPO通过使用**Kullback-Leibler** (**KL**)-距离来设定策略修改幅度的界限。这种限制是通过使用**Kullback-Leibler**（**KL**）-距离来实现的。'
- en: '**Kullback-Leibler (KL)**: Divergence measures how much one probability distribution
    differs from another reference distribution. Solomon Kullback and Richard A. Leibler
    introduced this concept to the world in 1951\. Within the context of PPO, KL-divergence
    is pivotal in steering optimization, ensuring that the refined policy remains
    closely aligned with its predecessor. In other words, it ensures the RL updates
    to LLMs are not drastic and stay within the threshold value.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kullback-Leibler (KL)**: 距离度量一个概率分布与另一个参考分布之间的差异程度。所罗门·库尔巴克和理查德·A·莱布勒于1951年将这一概念引入世界。在PPO的背景下，KL距离在引导优化中起着关键作用，确保改进后的策略与其前身保持紧密一致。换句话说，它确保了强化学习对LLM的更新不会过于剧烈，并保持在阈值值内。'
- en: How to evaluate fine-tuned model performance
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何评估微调模型性能
- en: So far, we’ve learned how to fine-tune LLMs to suit our needs, but how do we
    evaluate a model to make sure it’s performing well? But how do we know if a fine-tuned
    model made improvements over its predecessor model over a particular task? What
    are some industry-standard benchmarks that we can rely on to evaluate the models?
    In this section, we will see how LLMs such are GPT are evaluated and use the most
    popular benchmarks developed by researchers.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何微调LLM以满足我们的需求，但如何评估一个模型以确保其表现良好？但我们如何知道微调后的模型在特定任务上是否比其前身模型有所改进？有哪些行业标准基准我们可以依赖来评估模型？在本节中，我们将了解LLM（如GPT）是如何被评估的，并使用研究人员开发的最流行的基准。
- en: Evaluation metrics
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估指标
- en: '**Bilingual Evaluation Understudy** (**BLEU**) and **Recall-Oriented Understudy
    for Gisting Evaluation** (**ROUGE**) are both widely used metrics for evaluating
    the quality of machine-generated text, especially in the context of machine translation
    and text summarization. They measure the quality of generated texts in different
    ways. Let’s take a closer look.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**双语评估助手**（**BLEU**）和**基于理解评估的召回率助手**（**ROUGE**）都是广泛使用的用于评估机器生成文本质量的指标，尤其是在机器翻译和文本摘要的背景下。它们以不同的方式衡量生成文本的质量。让我们更深入地了解一下。'
- en: ROUGE
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ROUGE
- en: 'ROUGE is a set of metrics that’s used to evaluate the quality of summaries
    by comparing them to reference summaries. It’s mainly used to evaluate text summarization
    but can also be applied to other tasks, such as machine translation. ROUGE focuses
    on the overlap of n-grams – that is, word sequences of *n* items – between the
    generated summary and the reference summary:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE是一组用于通过将它们与参考摘要进行比较来评估摘要质量的指标。它主要用于评估文本摘要，但也可以应用于其他任务，如机器翻译。ROUGE关注生成摘要和参考摘要之间的n-gram重叠——即*n*个项目的单词序列。
- en: '![Figure 3.17 – Formula for ROUGE-N](img/B21443_03_17.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图3.17 – ROUGE-N的公式](img/B21443_03_17.jpg)'
- en: Figure 3.17 – Formula for ROUGE-N
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17 – ROUGE-N的公式
- en: 'The most common variants of ROUGE are as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE最常见的变体如下：
- en: '**ROUGE-N**: This variant measures the overlap of n-grams. For instance, ROUGE-1
    looks at the overlap of 1-gram (individual words), ROUGE-2 considers 2-grams (two
    consecutive words), and so on.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ROUGE-N**：此变体衡量n-gram的重叠。例如，ROUGE-1查看1-gram（单个单词）的重叠，ROUGE-2考虑2-gram（两个连续的单词），依此类推。'
- en: '**ROUGE-L**: This variant considers the longest common subsequence between
    the generated summary and the reference summary. It focuses on the longest in-sequence
    set of words that both summaries share.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ROUGE-L**：此变体考虑生成摘要和参考摘要之间的最长公共子序列。它关注两个摘要都共享的最长序列单词集。'
- en: '**ROUGE-S**: This variant measures the overlap of skip-bigrams, which are pairs
    of words in a sentence, irrespective of their order, allowing for gaps.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ROUGE-S**：此变体衡量跳过双词的重叠，即句子中不考虑顺序的单词对，允许存在间隔。'
- en: Now, let’s look at an example.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一个例子。
- en: 'Let’s use ROUGE-1, which focuses on individual word overlap, to illustrate
    this:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用专注于单个单词重叠的ROUGE-1来举例说明：
- en: 'Reference summary: “The boy fell on the grass”'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考摘要：“男孩跌倒在草地上”
- en: 'Generated summary: “The boy was on the grass.”'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成摘要：“男孩在草地上。”
- en: Here, every word except “was” and “fell” match between the two summaries.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，除了“was”和“fell”之外，其他每个单词在两个摘要之间都匹配。
- en: Total words in the reference = 6
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 参考中的总单词数 = 6
- en: Matching words = 5
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配单词 = 5
- en: 'So, the ROUGE-1 recall (how many of the words in the reference summary are
    also in the generated summary) would be as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，ROUGE-1的召回率（参考摘要中有多少单词也出现在生成摘要中）如下：
- en: 5/6 = 0.83 or 83%
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 5/6 = 0.83 或 83%
- en: ROUGE can also compute precision (how many of the words in the generated summary
    are in the reference summary) and F1 score (the harmonic mean of precision and
    recall).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE还可以计算精确度（生成摘要中有多少单词出现在参考摘要中）和F1分数（精确度和召回率的调和平均值）。
- en: 'In this example, we have the following:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们有以下内容：
- en: 'Precision: 5/6 = 0.83 or 83%'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精度：5/6 = 0.83 或 83%
- en: 'F1 score: 2 * (Precision * Recall) / (Precision + Recall) = 83%'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F1分数：2 * (Precision * Recall) / (Precision + Recall) = 83%
- en: While ROUGE scores give a quantitative measure of the overlap between the generated
    and reference text, it’s essential to note that a high ROUGE score doesn’t always
    mean the generated summary is of high quality. Other factors, such as coherence
    and fluency, are not captured by ROUGE.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然ROUGE分数给出了生成文本和参考文本之间重叠的定量度量，但需要注意的是，高ROUGE分数并不总是意味着生成的摘要质量高。其他因素，如连贯性和流畅性，并未被ROUGE所捕捉。
- en: BLEU
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BLEU
- en: BLEU is a metric for evaluating the quality of text that has been machine-translated
    from one natural language into another. The core idea behind BLEU is that if a
    translation is good, the words and phrases in the translation should appear in
    the same sequence as in the reference translations made by humans.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU是评估从一种自然语言翻译成另一种自然语言的文本质量的指标。BLEU背后的核心思想是，如果翻译质量好，翻译中的单词和短语应该与人类制作的参考翻译中的单词和短语以相同的顺序出现。
- en: 'BLEU considers the precision of n-grams (contiguous sequences of *n* items
    from a piece of text) in the machine-generated translation concerning the human
    reference translation(s). A typical BLEU score considers 1-gram (individual words),
    2-gram (pairs of consecutive words), 3-gram, and 4-gram precisions, then takes
    a weighted geometric mean to compute the final score. It also incorporates a penalty
    for translations that are shorter than their references, called the brevity penalty
    (source: BLEU, [https://aclanthology.org/P02-1040.pdf](https://aclanthology.org/P02-1040.pdf)).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU考虑了机器生成翻译中与人工参考翻译（s）相比的n-gram（文本中连续的*n*个项的序列）的精确度。典型的BLEU分数考虑了1-gram（单个单词）、2-gram（连续单词对）、3-gram和4-gram的精确度，然后通过加权几何平均计算最终分数。它还包含对翻译长度短于其参考的惩罚，称为简短惩罚（来源：BLEU，[https://aclanthology.org/P02-1040.pdf](https://aclanthology.org/P02-1040.pdf))。
- en: Note
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Both ROUGE and BLEU are simple metrics and can be used for diagnostic purposes
    but shouldn’t be used for a full and final evaluation of the model. Hence, for
    a more comprehensive evaluation, we must consider benchmarking methods. These
    will be discussed in the next section.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE和BLEU都是简单的指标，可用于诊断目的，但不应该用于模型的全面最终评估。因此，为了进行更全面的评估，我们必须考虑基准方法。这些将在下一节中讨论。
- en: Benchmarks
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准
- en: Benchmarks are critical for evaluation as well. This is a rapidly evolving research
    area, so in this section, we have focused on important benchmarks as of early
    2024\. Benchmarks are tests or tasks that are used to measure and compare the
    model’s performance in various areas, such as comprehension, generation, or accuracy.
    They help researchers and developers gauge how well the model understands and
    generates text and can be used to compare the performance of one LLM to another
    or track improvements over time. Evaluation metrics such as ROUGE and BLEU provide
    limited insights into the capabilities of LLM. Hence, to get a more comprehensive
    view of LLMs, we can leverage preexisting evaluation datasets and associated benchmarks
    that have been developed by LLM researchers.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 基准对于评估同样至关重要。这是一个快速发展的研究领域，因此在本节中，我们重点关注截至2024年初的重要基准。基准是用于衡量和比较模型在各个领域（如理解、生成或准确性）性能的测试或任务。它们帮助研究人员和开发者评估模型理解和生成文本的能力，并可用于比较一个大型语言模型（LLM）与另一个LLM的性能或跟踪随时间的变化。ROUGE和BLEU等评估指标对LLM的能力提供了有限的见解。因此，为了获得LLM的更全面视角，我们可以利用LLM研究人员开发的现有评估数据集和相关基准。
- en: GLUE and SuperGLUE
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GLUE和SuperGLUE
- en: '**General Language Understanding Evaluation** (**GLUE**) is a benchmark suite
    for evaluating the performance of **natural language understanding** (**NLU**)
    models on a variety of tasks. Introduced in 2018, GLUE comprises nine NLU tasks,
    including sentiment analysis, question-answering, and textual entailment, among
    others. It was developed to stimulate research in the field by providing a standard
    set of tasks for model comparison and competition and to push the boundaries of
    what NLU models can achieve.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '**通用语言理解评估**（**GLUE**）是一个用于评估各种任务上自然语言理解（**NLU**）模型性能的基准套件。GLUE于2018年推出，包括九个NLU任务，如情感分析、问答和文本蕴含等。它旨在通过提供一套标准任务以促进模型比较和竞赛，并推动NLU模型所能达到的边界。'
- en: SuperGLUE ([https://super.gluebenchmark.com](https://super.gluebenchmark.com)/),
    building upon the foundation of GLUE, is a more challenging benchmark that was
    introduced later. It was designed in response to the rapid advancements in model
    performance on the original GLUE tasks. SuperGLUE consists of a set of tasks that
    are more diverse and difficult, aiming to further push the capabilities of state-of-the-art
    NLU models and to provide a rigorous evaluation framework for future models.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在GLUE的基础上，**SuperGLUE**（[https://super.gluebenchmark.com](https://super.gluebenchmark.com)/）是一个后来推出的更具挑战性的基准。它是对原始GLUE任务上模型性能快速进步的回应。SuperGLUE包含一系列更加多样化和困难的任务，旨在进一步推动最先进NLU模型的能力，并为未来模型提供一个严格的评估框架。
- en: 'As of early 2024, SuperGLUE ([https://arxiv.org/pdf/1905.00537.pdf](https://arxiv.org/pdf/1905.00537.pdf))
    can evaluate models in 10 NLU tasks. This includes **Boolean Questions** (**BoolQ**),
    **CommitmentBank** (**CB**), **Choice of Plausible Alternatives** (**COPA**),
    **Multi-Sentence Reading Comprehension** (**MultiRC**), **Reading Comprehension
    with Commonsense Reasoning Dataset** (**ReCoRD**), **Recognizing Textual Entailment**
    (**RTE**), **Words in Context** (**WiC**), **Winograd Schema Challenge** (**WSC**),
    broad coverage diagnostics (AX-b), and Winogender Schema Diagnostics (gender parity/
    accuracy). Let’s take a closer look:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2024年初，SuperGLUE ([https://arxiv.org/pdf/1905.00537.pdf](https://arxiv.org/pdf/1905.00537.pdf))
    可以评估10个NLU任务中的模型。这包括**布尔问题**（**BoolQ**）、**承诺银行**（**CB**）、**合理替代选择**（**COPA**）、**多句阅读理解**（**MultiRC**）、**常识推理阅读理解数据集**（**ReCoRD**）、**识别文本蕴涵**（**RTE**）、**上下文中的词**（**WiC**）、**Winograd方案挑战**（**WSC**）、广泛覆盖诊断（AX-b）和Winogender方案诊断（性别平等/准确率）。让我们更详细地看看：
- en: '| **Task** | **Description** | **Example** |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| **任务** | **描述** | **示例** |'
- en: '| --- | --- | --- |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| BoolQ | Answer yes/no questions based on a passage. | Passage: “Dolphins
    are known for their intelligence.” Question: “Are dolphins recognized for their
    intelligence?” Answer: Yes. |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| BoolQ | 根据文章回答是/否问题。| 文章：“海豚以其智慧而闻名。”| 问题：“海豚因智慧而受到认可吗？”| 答案：是。|'
- en: '| CB | Predict the level of commitment in a statement. | Premise: “I think
    the cat might be in the garden.”Hypothesis: “The cat is in the garden.”Entailment:
    Unknown (the premise suggests a possibility, but it doesn’t firmly commit to the
    cat being in the garden.) |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| CB | 预测语句中的承诺程度。| 前提：“我认为猫可能在大花园里。”| 假设：“猫在大花园里。”| 推论：未知（前提暗示了一种可能性，但并没有坚定地承诺猫在大花园里。）|'
- en: '| COPA | Choose between two plausible alternatives as the cause or effect of
    a given premise. | Premise: “The ground was wet.” Question: What was the CAUSE
    of this?” Alternatives: (a) It rained. (b) It was sunny. Answer: (a) It rained.
    |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| COPA | 在给定的前提中，在两个合理的替代选项之间选择原因或结果。| 前提：“地面湿了。”| 问题：“是什么导致了这种情况？”| 选项：(a)
    下雨了。(b) 晴天。| 答案：(a) 下雨了。|'
- en: '| MultiRC | Answer questions about individual sentences in a passage. | Passage:
    “Jupiter is the largest planet. It’s primarily composed of hydrogen.” Question:
    “What is Jupiter primarily composed of?” Answer: Hydrogen. |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| MultiRC | 回答关于文章中单个句子的提问。| 文章：“木星是最大的行星。它主要由氢组成。”| 问题：“木星主要由什么组成？”| 答案：氢。|'
- en: '| ReCoRD | Fill in the blanks in a passage using context. | Passage: “Lara
    loves reading. Her favorite genre is ____. She’s read every mystery novel.” Fill
    in the blank: mystery. |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| ReCoRD | 使用上下文在文章中填空。| 文章：“Lara喜欢阅读。她最喜欢的类型是____。她已经读完了所有的侦探小说。”| 填空：神秘。|'
- en: '| RTE | Determine if a premise sentence entails a hypothesis sentence. | Premise:
    “Dogs are mammals.” Hypothesis: “Dogs give live birth.” Entailment: True. |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| RTE | 判断前提句子是否蕴涵假设句子。| 前提：“狗是哺乳动物。”| 假设：“狗是活产。”| 推论：真。|'
- en: '| WiC | Determine if a word has the same meaning in two sentences. | Sentence
    1: “He used a key to open the door.” Sentence 2: “The answer is the key to this
    puzzle.” Word: “key” Answer: Different senses. |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| WiC | 判断一个词在两个句子中是否有相同的意思。| 句子1：“他使用钥匙打开门。”| 句子2：“答案是这个谜题的关键。”| 词：“钥匙”| 答案：不同的含义。|'
- en: '| WSC | Identify to which noun phrase a pronoun refers in a sentence. | Sentence:
    “The trophy doesn’t fit in the suitcase because it’s too large.” Question: What
    is too large? Answer: The trophy. |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| WSC | 确定句子中的代词指代哪个名词短语。| 句子：“奖杯太大，放不进手提箱。”| 问题：“什么太大？”| 答案：奖杯。|'
- en: Figure 3.18 – SuperGLUE benchmark
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.18 – SuperGLUE基准
- en: 'The following figure shows the leaderboard for the SuperGLUE benchmark ([https://super.gluebenchmark.com/leaderboard](https://super.gluebenchmark.com/leaderboard)),
    with the LLM models leading across various NLU tasks:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了SuperGLUE基准排行榜（[https://super.gluebenchmark.com/leaderboard](https://super.gluebenchmark.com/leaderboard)），其中LLM模型在各种NLU任务中领先：
- en: '![Figure 3.19 – Snapshot of the SuperGLUE benchmark leaderboard as of February
    2024](img/B21443_03_18.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图3.19 – 2024年2月的SuperGLUE基准排行榜快照](img/B21443_03_18.jpg)'
- en: Figure 3.19 – Snapshot of the SuperGLUE benchmark leaderboard as of February
    2024
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.19 – 2024年2月的SuperGLUE基准排行榜快照
- en: Massive Multitask Language Understanding (MMLU)
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大规模多任务语言理解（MMLU）
- en: 'MMLU was established in 2021\. This benchmark is quite suitable for modern
    massive LLMs. The goal is to evaluate and compare models regarding their world
    knowledge and problem-solving abilities. This benchmark encompasses 57 topics,
    spanning areas from STEM and the humanities to the social sciences and beyond.
    Its complexity varies from basic to expert levels, evaluating both general knowledge
    and analytical capabilities. The subjects touch upon both classic fields, such
    as mathematics and history, and more niche sectors, such as law and ethics. The
    detailed scope and variety of topics within the benchmark make it perfectly suited
    to pinpoint a model’s areas of weakness. These tasks go beyond basic language
    understanding, as evaluated by GLUE and SuperGLUE (source: MMLU, [https://arxiv.org/pdf/2009.03300.pdf](https://arxiv.org/pdf/2009.03300.pdf)).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: MMLU 于 2021 年建立。这个基准非常适合现代大型语言模型。目标是评估和比较模型在他们的世界知识和问题解决能力方面的表现。该基准涵盖了 57 个主题，从
    STEM 和人文到社会科学等领域。其复杂性从基础到专家级别不等，评估了一般知识和分析能力。主题涉及经典领域，如数学和历史，以及更狭窄的领域，如法律和伦理。基准中详细的主题范围和多样性使其非常适合确定模型的弱点。这些任务超越了由
    GLUE 和 SuperGLUE 评估的基本语言理解，(来源：MMLU，[https://arxiv.org/pdf/2009.03300.pdf](https://arxiv.org/pdf/2009.03300.pdf)）。
- en: The leaderboard for the MMLU benchmark can be found at [https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: MMLU 基准的排行榜可以在 [https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu)
    找到。
- en: Beyond the Imitation Game Benchmark (BIG-bench)
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超越模仿游戏基准（BIG-bench）
- en: 'BIG-bench is a collaborative benchmark that was introduced in October 2022\.
    The goal of this benchmark is to build disruptive models and evaluate them on
    tasks that are beyond the capabilities of current language models. It consists
    of more than 204 diverse tasks ranging from linguistics, childhood development,
    math, common-sense reasoning, biology, physics, social bias, software development,
    and beyond (source: BIG-bench, [https://arxiv.org/abs/2206.04615](https://arxiv.org/abs/2206.04615)).'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: BIG-bench 是一个于 2022 年 10 月推出的协作基准。该基准的目标是构建颠覆性的模型，并在超出当前语言模型能力范围的任务上进行评估。它包含超过
    204 个多样化的任务，涵盖语言学、儿童发展、数学、常识推理、生物学、物理学、社会偏见、软件开发等领域（来源：BIG-bench，[https://arxiv.org/abs/2206.04615](https://arxiv.org/abs/2206.04615)）。
- en: 'The following GitHub repository provides some code so that you can use BIG-bench
    to evaluate your models: [https://github.com/google/BIG-bench#submitting-a-model-evaluation](https://github.com/google/BIG-bench#submitting-a-model-evaluation).'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 GitHub 仓库提供了一些代码，以便您可以使用 BIG-bench 评估您的模型：[https://github.com/google/BIG-bench#submitting-a-model-evaluation](https://github.com/google/BIG-bench#submitting-a-model-evaluation)。
- en: Holistic Evaluation of Language Model (HELM) (Classic, Lite, and Text-to-Image)
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 全面的语言模型评估（HELM）(经典、轻量级和文本到图像)
- en: 'The HELM Classic benchmark, which was introduced in November 2022 by Stanford
    Research, evaluates models on seven key metrics: accuracy, calibration, robustness,
    fairness, bias, toxicity, and efficiency. The HELM framework aims to improve the
    transparency of models and offers insights into which models perform well on specific
    tasks. This benchmark measures these seven metrics across 51 scenarios and exposes
    the trade-offs between models and metrics. This benchmark is also continuously
    evolving, and more scenarios, metrics, and models are being added to this benchmark.
    Scenarios consist of a use case and a dataset of examples such as **Math Chain
    of Thought** (**MATH**), **Grade School Math** (**GSM8K**), HellaSwag (common-sense
    reasoning), MMLU, OpenBook QA (question-answering), and so on.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 由斯坦福研究所在 2022 年 11 月推出的 HELM Classic 基准，评估模型在七个关键指标上的表现：准确性、校准、鲁棒性、公平性、偏差、毒性和效率。HELM
    框架旨在提高模型的可透明性，并提供了关于哪些模型在特定任务上表现良好的见解。该基准在 51 个场景中测量这七个指标，并揭示了模型与指标之间的权衡。该基准也在不断进化，更多场景、指标和模型正在被添加到该基准中。场景包括用例和示例数据集，例如
    **数学思维链**（**MATH**）、**小学数学**（**GSM8K**）、HellaSwag（常识推理）、MMLU、OpenBook QA（问答）等。
- en: 'For a full list of scenarios, check out this page on HELM Classic scenarios:
    [https://crfm.stanford.edu/helm/classic/latest/#/scenarios](https://crfm.stanford.edu/helm/classic/latest/#/scenarios).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看场景的完整列表，请查看 HELM Classic 场景页面：[https://crfm.stanford.edu/helm/classic/latest/#/scenarios](https://crfm.stanford.edu/helm/classic/latest/#/scenarios)。
- en: 'The following link provides the latest results on the HELM leaderboard: [https://crfm.stanford.edu/helm/lite/latest/#/leaderboard](https://crfm.stanford.edu/helm/lite/latest/#/leaderboard).'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接提供了 HELM 排行榜的最新结果：[https://crfm.stanford.edu/helm/lite/latest/#/leaderboard](https://crfm.stanford.edu/helm/lite/latest/#/leaderboard).
- en: 'This Python package can be used to evaluate your models against the HELM benchmarks
    and compare them against the most prominent models: [https://crfm-helm.readthedocs.io/en/latest/](https://crfm-helm.readthedocs.io/en/latest/)).'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Python 包可以用来评估你的模型与 HELM 基准的对比，并与其他最突出的模型进行比较：[https://crfm-helm.readthedocs.io/en/latest/](https://crfm-helm.readthedocs.io/en/latest/)).
- en: Helm Classic was released before ChatGPT and its initial objective was to comprehensively
    assess every language model available across a variety of representative scenarios,
    such as linguistic capabilities, reasoning skills, knowledge, and more, as well
    as a range of metrics. However, it was quite heavyweight, hence a lighter version
    was released called HELM Lite. It is not only a subset of Classic but a more simplified
    version with fewer core scenarios.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Helm Classic 在 ChatGPT 之前发布，其初始目标是全面评估在各种代表性场景中可用的各种语言模型，例如语言能力、推理技能、知识等，以及一系列指标。然而，它相当庞大，因此发布了一个更轻量级的版本，称为
    HELM Lite。它不仅是 Classic 的子集，而且是一个更简化的版本，具有更少的核心场景。
- en: 'With the proliferation of multimodal LLMs, recently, Stanford published a new
    benchmark called **Holistic Evaluation of Image Models** (**HEIM**), which assesses
    text-to-image models on 12 different aspects required for real-world deployment
    ([https://arxiv.org/abs/2311.04287](https://arxiv.org/abs/2311.04287)):'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 随着多模态大型语言模型（LLMs）的普及，最近，斯坦福大学发布了一个名为 **整体图像模型评估**（**HEIM**）的新基准，该基准从 12 个不同方面评估了适用于现实世界部署的文本到图像模型（[https://arxiv.org/abs/2311.04287](https://arxiv.org/abs/2311.04287)）：
- en: Image-text alignment
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像与文本对齐
- en: Image quality
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像质量
- en: Aesthetics
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 美学
- en: Originality
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原创性
- en: Reasoning
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解
- en: Knowledge
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知识
- en: Bias
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏见
- en: Toxicity
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 毒性
- en: Fairness
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公平性
- en: Robustness
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 健壮性
- en: Multi-linguality
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多语言性
- en: Efficiency
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 效率
- en: In this section, we delved into the key benchmarks and assessment metrics for
    LLMs. If you’re looking to construct an enterprise-level ChatGPT application,
    it’s crucial to measure GPT models against top benchmarks to ensure the application
    is effective, trustworthy, and safe. Such benchmarks serve as an excellent foundation
    for this endeavor.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们深入探讨了 LLMs 的关键基准和评估指标。如果你正在构建企业级的 ChatGPT 应用程序，将 GPT 模型与顶级基准进行比较至关重要，以确保应用程序的有效性、可靠性和安全性。此类基准为此努力提供了良好的基础。
- en: Tools such as Azure AI Studio and Azure Prompt Flow provide qualitative and
    quantitative solutions to evaluate your models. It also provides benchmarking
    capabilities that help you assess different models using industry-leading benchmarks.
    Scores such as ROUGE-N and BLEU can be calculated using out-of-the-box functionalities
    on Azure Prompt Flow.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 工具，如 Azure AI Studio 和 Azure Prompt Flow，提供了定性和定量解决方案来评估你的模型。它还提供了基准测试功能，帮助你使用行业领先的基准来评估不同的模型。ROUGE-N
    和 BLEU 等分数可以使用 Azure Prompt Flow 的开箱即用功能进行计算。
- en: Real-life examples of fine-tuning success
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调成功的实际案例
- en: In this section, we’ll explore a real-life example of a fine-tuning approach
    that OpenAI implemented, which yielded remarkable outcomes.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨 OpenAI 实施的一种微调方法的实际案例，该方法取得了显著成果。
- en: InstructGPT
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: InstructGPT
- en: OpenAI’s InstructGPT is one of the most successful stories of fine-tuned models
    that laid the foundation of ChatGPT. ChatGPT is said to be a sibling model to
    InstructGPT. The methods that are used to fine-tune ChatGPT are similar to InstructGPT.
    InstructGPT was created by fine-tuning pre-trained GPT-3 models with RHLF. Supervised
    fine-tuning is the first step in RLHF for generating responses aligned to human
    preferences.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 的 InstructGPT 是最成功的微调模型故事之一，为 ChatGPT 打下了基础。据说 ChatGPT 是 InstructGPT
    的一个兄弟模型。用于微调 ChatGPT 的方法与 InstructGPT 相似。InstructGPT 通过使用 RHLF 对预训练的 GPT-3 模型进行微调而创建。监督微调是
    RLHF 中生成符合人类偏好的响应的第一步。
- en: 'In the beginning, GPT-3 models weren’t originally designed to adhere to user
    instructions. Their training focused on predicting the next word based on vast
    amounts of internet text data. Therefore, these models underwent fine-tuning using
    instructional datasets along with RLHF to enhance their ability to generate more
    useful and relevant responses aligned with human values when prompted with user
    instructions:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在最初，GPT-3模型并非最初设计为遵循用户指令。它们的训练侧重于根据大量互联网文本数据预测下一个单词。因此，这些模型通过使用指导数据集和RLHF进行了微调，以增强它们在接收到用户指令时生成更符合人类价值观的有用和相关的响应的能力：
- en: '![Figure 3.20 – The fine-tuning process with RLHF](img/B21443_03_19.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图3.20 – 使用RLHF的微调过程](img/B21443_03_19.jpg)'
- en: Figure 3.20 – The fine-tuning process with RLHF
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.20 – 使用RLHF的微调过程
- en: 'This figure depicts a schematic representation showcasing the InstructGPT fine-tuning
    process: (*1*) initial supervised fine-tuning, (*2*) training the reward model,
    and (*3*) executing RL through PPO using this established reward model. The utilization
    of this data to train respective models is indicated by the presence of blue arrows.
    In *step 2*, boxes A-D are samples from models that get ranked by labelers.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 此图展示了InstructGPT微调过程的示意图：（*1*）初始的监督微调，（*2*）训练奖励模型，以及（*3*）使用此建立的奖励模型通过PPO执行强化学习。蓝色箭头表示使用这些数据来训练相应模型。在*步骤2*中，A-D框是经过标注者排名的模型样本。
- en: 'The following figure provides a comparison of the response quality of fine-tuned
    models with RLHF, supervised fine-tuned models, and general GPT models. The *Y*-axis
    consists of a Likert scale and shows quality ratings of model outputs on a 1–7
    scale (*Y*-axis), for various model sizes (*X*-axis), on prompts submitted to
    InstructGPT models via the OpenAI API. The results reveal that InstructGPT outputs
    receive significantly higher scores by labelers compared to outputs from GPT-3
    models with both few-shot prompts and those without, as well as models that underwent
    supervised learning fine-tuning. The labelers that were hired for this work were
    independent and were sourced from Scale AI and Upwork:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 下图提供了使用RLHF微调的模型、监督微调的模型和通用GPT模型响应质量的比较。*Y*轴由李克特量表组成，显示了模型输出在1-7尺度上的质量评分（*Y*轴），针对通过OpenAI
    API提交给InstructGPT模型的提示，以及各种模型大小（*X*轴）。结果显示，与GPT-3模型（无论是少量提示还是无提示）以及经过监督学习微调的模型相比，InstructGPT的输出获得了标注者显著更高的评分。为此工作雇佣的标注者是独立的，并来自Scale
    AI和Upwork：
- en: '![Figure 3.21 – Evaluation of InstructGPT (image credits: Open AI)](img/B21443_03_20.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图3.21 – InstructGPT的评估（图片版权：Open AI）](img/B21443_03_20.jpg)'
- en: 'Figure 3.21 – Evaluation of InstructGPT (image credits: Open AI)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.21 – InstructGPT的评估（图片版权：Open AI）
- en: 'InstructGPT can be assessed across dimensions of toxicity, truthfulness, and
    appropriateness. Higher scores are desirable for TruthfulQA and appropriateness,
    whereas lower scores are preferred for toxicity and hallucinations. Measurement
    of hallucinations and appropriateness is conducted based on the distribution of
    prompts within our API. The outcomes are aggregated across various model sizes:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: InstructGPT可以在毒性、真实性和适宜性等多个维度进行评估。对于真实问答和适宜性，较高的分数是可取的，而对于毒性和幻觉，则更倾向于较低的分数。幻觉和适宜性的测量是基于我们API中提示词的分布进行的。结果会根据不同的模型大小进行汇总：
- en: '![Figure 3.22 – Evaluation of InstructGPT](img/B21443_03_21.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图3.22 – InstructGPT的评估](img/B21443_03_21.jpg)'
- en: Figure 3.22 – Evaluation of InstructGPT
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.22 – InstructGPT的评估
- en: In this section, we introduced the concept of fine-tuning and discussed a success
    stories of fine-tuning with RLHF that led to the development of InstructGPT.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了微调的概念，并讨论了使用RLHF进行微调的成功案例，这导致了InstructGPT的开发。
- en: Summary
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Fine-tuning is a powerful technique for customizing models, but it may not always
    be necessary. As observed, it can be time-consuming and may have initial upfront
    costs. It’s advisable to start with easier and faster strategies, such as prompt
    engineering with few-shot examples, followed by data grounding using RAG. Only
    if the responses from the LLM remain suboptimal should you consider fine-tuning.
    We will discuss RAG and prompt engineering in the following chapters.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是定制模型的一种强大技术，但它可能并不总是必要的。观察发现，它可能耗时且可能存在初始的前期成本。建议从更容易、更快的策略开始，例如使用少量示例的提示工程，然后使用RAG进行数据接地。只有当LLM的响应仍然次优时，才应考虑微调。我们将在下一章讨论RAG和提示工程。
- en: In this chapter, we delved into critical fine-tuning strategies tailored for
    specific tasks. Then, we explored an array of evaluation methods and benchmarks
    to assess your refined model. The RLHF process ensures your models align with
    human values, making them helpful, honest, and safe. In the upcoming chapter,
    we’ll tackle RAG methods paired with vector databases – an essential technique
    to ground your enterprise data and minimize hallucinations in LLM-driven applications.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了针对特定任务的关键微调策略。然后，我们探索了一系列评估方法和基准，以评估您精炼后的模型。RLHF过程确保您的模型与人类价值观保持一致，使其变得有用、诚实和安全。在下一章中，我们将探讨与向量数据库配对的RAG方法——这是在LLM驱动的应用中定位企业数据并最小化幻觉的关键技术。
- en: References
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[https://spotintelligence.com/2023/03/28/transfer-learning-large-language-models/](https://spotintelligence.com/2023/03/28/transfer-learning-large-language-models/)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spotintelligence.com/2023/03/28/transfer-learning-large-language-models/](https://spotintelligence.com/2023/03/28/transfer-learning-large-language-models/)'
- en: '[https://platform.openai.com/docs/guides/fine-tuning](https://platform.openai.com/docs/guides/fine-tuning)'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://platform.openai.com/docs/guides/fine-tuning](https://platform.openai.com/docs/guides/fine-tuning)'
- en: 'PEFT Research Paper: [https://arxiv.org/abs/2303.15647](https://arxiv.org/abs/2303.15647)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PEFT 研究论文：[https://arxiv.org/abs/2303.15647](https://arxiv.org/abs/2303.15647)
- en: 'BLEU: [https://aclanthology.org/P02-1040/](https://aclanthology.org/P02-1040/)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BLEU：[https://aclanthology.org/P02-1040/](https://aclanthology.org/P02-1040/)
- en: 'The Power of Scale for Parameter-Efficient Prompt Tuning: [https://aclanthology.org/2021.emnlp-main.243.pdf](https://aclanthology.org/2021.emnlp-main.243.pdf)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数高效提示调优的规模力量：[https://aclanthology.org/2021.emnlp-main.243.pdf](https://aclanthology.org/2021.emnlp-main.243.pdf)
- en: 'Low Rank Adaption of Large Language Models: [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型的低秩自适应：[https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)
- en: 'LLM (GPT) Fine Tuning — PEFT | LoRA | Adapters | Quantization | by Siddharth
    vij | Jul, 2023 | Medium: [https://tinyurl.com/2t8ntxy4](https://tinyurl.com/2t8ntxy4)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM (GPT) 微调 — PEFT | LoRA | 适配器 | 量化 | by Siddharth vij | 七月，2023 | Medium：[https://tinyurl.com/2t8ntxy4](https://tinyurl.com/2t8ntxy4)
- en: 'InstructGPT: [https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: InstructGPT：[https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155)
- en: '[https://towardsdatascience.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7](https://towardsdatascience.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7](https://towardsdatascience.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7)'
- en: '[https://www.fuzzylabs.ai/blog-post/llm-fine-tuning-old-school-new-school-and-everything-in-between](https://www.fuzzylabs.ai/blog-post/llm-fine-tuning-old-school-new-school-and-everything-in-between)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.fuzzylabs.ai/blog-post/llm-fine-tuning-old-school-new-school-and-everything-in-between](https://www.fuzzylabs.ai/blog-post/llm-fine-tuning-old-school-new-school-and-everything-in-between)'
- en: 'Llama: [https://arxiv.org/abs/2302.13971](https://arxiv.org/abs/2302.13971)'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Llama：[https://arxiv.org/abs/2302.13971](https://arxiv.org/abs/2302.13971)
- en: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.
    In Proceedings of ICLR: [1804.07461] [https://arxiv.org/abs/1804.07461](https://arxiv.org/abs/1804.07461)'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GLUE：自然语言理解的多元任务基准和分析平台。在ICLR会议论文集：[1804.07461](https://arxiv.org/abs/1804.07461)
- en: 'SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding
    Systems: [1905.00537] [https://arxiv.org/abs/1905.00537](https://arxiv.org/abs/1905.00537)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SuperGLUE：通用语言理解系统的粘性基准：[1905.00537](https://arxiv.org/abs/1905.00537) [https://arxiv.org/abs/1905.00537](https://arxiv.org/abs/1905.00537)
- en: 'MMLU Measuring Massive Multitask Language Understanding: [https://arxiv.org/pdf/2009.03300.pdf](https://arxiv.org/pdf/2009.03300.pdf)'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MMLU 测量大规模多任务语言理解：[https://arxiv.org/pdf/2009.03300.pdf](https://arxiv.org/pdf/2009.03300.pdf)
- en: 'BIG Bench: [https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/keywords_to_tasks.md#summary-table](https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/keywords_to_tasks.md#summary-table)'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BIG Bench：[https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/keywords_to_tasks.md#summary-table](https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/keywords_to_tasks.md#summary-table)
- en: 'HELM: [https://arxiv.org/pdf/2211.09110.pdf](https://arxiv.org/pdf/2211.09110.pdf)'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HELM：[https://arxiv.org/pdf/2211.09110.pdf](https://arxiv.org/pdf/2211.09110.pdf)
- en: '[https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)'
