<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-34"><a id="_idTextAnchor034"/>2</h1>
<h1 id="_idParaDest-35"><a id="_idTextAnchor035"/>Accessing and Utilizing Models in Amazon Bedrock</h1>
<p>This chapter provides a practical guide to accessing Amazon Bedrock and uncovering its generative AI capabilities. We will start with an overview of the different interfaces for invoking Bedrock models, including the console playground, <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>), and <strong class="bold">software development kit</strong> (<strong class="bold">SDK</strong>). Then, we will unveil some of the core Bedrock APIs, along <a id="_idIndexMarker134"/>with the <a id="_idIndexMarker135"/>code snippets that you can run in your environment. Finally, we’ll demonstrate how to leverage Bedrock within the LangChain Python framework to build customized pipelines that chain multiple models and provide insight into PartyRock, a powerful playground for Amazon Bedrock.</p>
<p>By the end of this chapter, you will be able to run and execute applications by leveraging SOTA FMs available from Amazon Bedrock as you gain a deeper understanding of each of the FMs available and how to utilize them for your needs. You will also be able to accelerate your creative thinking regarding building new generative AI applications as we dive into building cool apps with PartyRock and learn how to integrate Amazon Bedrock into different use cases.</p>
<p>The following key topics will be covered in this chapter:</p>
<ul>
<li>Accessing Amazon Bedrock</li>
<li>Using Amazon Bedrock APIs</li>
<li>Amazon Bedrock integration points</li>
</ul>
<h1 id="_idParaDest-36"><a id="_idTextAnchor036"/>Technical requirements</h1>
<p>For this chapter, you’ll need to have access to an <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>) account. If you don’t have one already, you can go to <a href="https://aws.amazon.com/getting-started/">https://aws.amazon.com/getting-started/</a> and create one.</p>
<p>Once you’ve done this, you’ll need to install and configure the AWS CLI (<a href="https://aws.amazon.com/cli/">https://aws.amazon.com/cli/</a>) as you’ll need this to access Amazon Bedrock FMs from your local machine. Since the majority of the code blocks that we will execute are based on Python, setting up an AWS Python SDK (Boto3) (<a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html">https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html</a>) would be beneficial. You can set up Python by installing it on your local machine, or using AWS Cloud9, or utilizing AWS Lambda, or leveraging Amazon SageMaker.</p>
<p class="callout-heading">Note</p>
<p class="callout">There will be a charge associated with invocating and customizing the FMs of Amazon Bedrock. Please refer to <a href="https://aws.amazon.com/bedrock/pricing/">https://aws.amazon.com/bedrock/pricing/</a> to learn more.</p>
<h1 id="_idParaDest-37"><a id="_idTextAnchor037"/>Accessing Amazon Bedrock</h1>
<p>When building a generative <a id="_idIndexMarker136"/>AI application, you’re faced with a dizzying array of choices. Which FM should you use? How will you ensure security and privacy? Do you have the infrastructure to support large-scale deployment? Enter Amazon Bedrock.</p>
<p>As you know by now, Amazon Bedrock provides access to a selection of SOTA FMs from leading AI companies in the space, including AI21 Labs, Anthropic, Cohere, Meta, Stability AI, Amazon, and Mistral. With a single API, you can tap into cutting-edge generative AI across modalities such as text, embeddings, and images. You have the flexibility to mix and match models to find the perfect fit for your needs. Bedrock handles provisioning, scalability, and governance behind the scenes. Hence, you can choose the best model suited to your needs and simply invoke the Bedrock serverless API to plug those models into your application.</p>
<p>So, let’s jump onto the AWS console and see Amazon Bedrock in action.</p>
<p>When you open Amazon Bedrock in the AWS console by navigating to https://console.aws.amazon.com/ and choosing Bedrock from the search bar, you can explore different FMs, as well as a few learning tools, as depicted in <em class="italic">Figure 2</em><em class="italic">.1</em>:</p>
<div><div><img alt="Figure 2.1 – Amazon Bedrock – Overview" src="img/B22045_02_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – Amazon Bedrock – Overview</p>
<p>Amazon Bedrock provides users with the flexibility to experiment with various models through its playground interface. Users can access the Bedrock playground from the AWS console by <a id="_idIndexMarker137"/>navigating to the Amazon Bedrock landing page and clicking <strong class="bold">Examples</strong> to open the playground environment.</p>
<p class="callout-heading">Note</p>
<p class="callout">At the time of writing this book, users will have to initially enable access to the models by navigating to the <strong class="bold">Model access</strong> link in the left panel within the Bedrock console (as shown in <em class="italic">Figure 2</em><em class="italic">.2</em>). Once you’ve landed on the <strong class="bold">Model access</strong> page view, you can click on <strong class="bold">Manage model access</strong>, select the list of base models you want to leverage for your use cases, and click <strong class="bold">Save changes</strong>. Instantly, the users will be given access to those models. Users can also review the EULA agreement next to the respective base models to view their terms of service.</p>
<p>Within the playground, you can explore the different examples of generative AI models available in Bedrock. This allows you to test out and interact with the models without needing to configure resources or write any code. Overall, the playground provides a convenient way for users to try out the capabilities of Bedrock’s generative models. <em class="italic">Figure 2</em><em class="italic">.2</em> depicts some of the capabilities available within the Amazon Bedrock console:</p>
<div><div><img alt="Figure 2.2 – Amazon Bedrock’s capabilities" src="img/B22045_02_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – Amazon Bedrock’s capabilities</p>
<p>Within the playground, you are given the option to start exploring examples based on <strong class="bold">Text</strong>, <strong class="bold">Chat</strong>, and <strong class="bold">Image</strong>. This enables hands-on experimentation with the latest generative AI models in a convenient sandbox environment. The breadth of options, from conversational chatbots to text and image generation, gives you the flexibility to test diverse AI functions firsthand. By providing accessible entry points, emerging generative AI<a id="_idIndexMarker138"/> becomes more tangible and approachable for users to understand. Now, let’s learn about each of these in greater detail.</p>
<h2 id="_idParaDest-38"><a id="_idTextAnchor038"/>Chat playground</h2>
<p>Amazon <a id="_idIndexMarker139"/>Bedrock <a id="_idIndexMarker140"/>gives you access to chat models, which you can experiment with in the <strong class="bold">Chat playground</strong>.</p>
<p>The <strong class="bold">Chat playground</strong> is an experimental interface that allows you to test the conversational AI models available through Amazon Bedrock. You can enter sample prompts and view the responses that are generated by a selected model. Usage metrics are also displayed to evaluate the model’s performance. A compare mode is available to contrast the outputs of up to three different models side by side.</p>
<p>As shown in the<a id="_idIndexMarker141"/> following figures, users can select which model <a id="_idIndexMarker142"/>they want to use (<em class="italic">Figure 2</em><em class="italic">.3</em>):</p>
<div><div><img alt="Figure 2.3 – Selecting a model" src="img/B22045_02_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – Selecting a model</p>
<p>Thereafter, users can enter a query in the chat box (<em class="italic">Figure 2</em><em class="italic">.4</em>):</p>
<div><div><img alt="Figure 2.4 – Querying the chat model in the Chat playground" src="img/B22045_02_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – Querying the chat model in the Chat playground</p>
<p>Running a query fetches information from the chosen model. This allows you to evaluate factors such as accuracy, response length, latency, and suitability for your use case. Selecting the optimal model depends on weighing these factors against individual needs.</p>
<p>While invoking the FMs, you will see the option to<a id="_idIndexMarker143"/> modify the <strong class="bold">inference parameters</strong> so that you can influence the model’s response in a certain way. While some inference parameters are common among LLMs, image models have a separate set of parameters that can<a id="_idIndexMarker144"/> be tuned <a id="_idIndexMarker145"/>by users.</p>
<p>Let’s look at some of these common parameters.</p>
<h3>LLM inference parameters</h3>
<p><em class="italic">Temperature</em>, <em class="italic">Top P</em>, <em class="italic">Top K</em>, <em class="italic">Response length</em>, <em class="italic">Stop sequences</em>, and <em class="italic">Max tokens</em> are the inference<a id="_idIndexMarker146"/> parameters that we will learn about in detail in this section. <em class="italic">Figure 2</em><em class="italic">.5</em> shows them on the Amazon Bedrock <strong class="bold">Chat playground</strong> screen; they can be found in the <strong class="bold">Configurations</strong> window:</p>
<div><div><img alt="Figure 2.5 – Common LLM inference parameters" src="img/B22045_02_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5 – Common LLM inference parameters</p>
<p>Let’s take a closer look:</p>
<ul>
<li><strong class="bold">Temperature</strong>: This parameter controls the degree of randomness in the output. A lower temperature results in more deterministic output, favoring the most likely option. On the other hand, a higher temperature promotes randomness, leading to a wider range of diverse and creative outputs. For example, in QA tasks, a lower temperature ensures more factual and concise responses, whereas if your use case revolves around generating creative and diverse output, such as in creative writing or advertisement generation, it might be worthwhile to increase the temperature value.</li>
<li><strong class="bold">Top K</strong><strong class="bold"> and </strong><strong class="bold">Top P</strong>: Sampling techniques such as <strong class="bold">Top K</strong> and <strong class="bold">Top P</strong> can be employed to enhance the coherence and sense of the output. <strong class="bold">Top K</strong> limits the number of <a id="_idIndexMarker147"/>options to a specified number, ensuring a balance between randomness and coherence. <strong class="bold">Top P</strong>, on the other hand, restricts the predictions with combined probabilities below a specified threshold, preventing highly improbable options from being selected. These techniques help strike a balance between generating coherent text and maintaining a certain level of randomness, making the text generation process more natural and engaging for the reader.<p class="list-inset">Using these together balances novelty and fluency. For example, you could set <strong class="bold">Top K</strong> to 70 and <strong class="bold">Top P</strong> to 0.8. This allows some uncommon but still relevant words via the <strong class="bold">Top P</strong> setting, while <strong class="bold">Top K</strong> retains focus on more common words. The result is text that is fairly fluent with occasional novel words mixed in. You can experiment with different values for <strong class="bold">Top K</strong> and <strong class="bold">Top P</strong> to achieve the novelty versus fluency balance you want for a particular generative AI application. Start with a <strong class="bold">Top K</strong> value around 50 to 100 and a <strong class="bold">Top P</strong> value around 0.7 to 0.9 as reasonable initial settings. The optimal values depend on factors such as model size, dataset, and use case.</p></li>
<li><code>bedrock</code>, the model will stop generating output as soon as it encounters the word <em class="italic">bedrock</em> in the generated text.</li>
<li><code>Write a sentence in </code><code>100 words</code>.</li>
</ul>
<h3>Image model inference parameters</h3>
<p>When performing<a id="_idIndexMarker149"/> image generation with FMs, several key parameters affect the inference process. For instance, in the case of Stable Diffusion models, the model takes in a text prompt and random noise vector to produce an image. Several configuration settings for the model can influence the final generated image, as depicted in <em class="italic">Figure 2</em><em class="italic">.6</em>:</p>
<div><div><img alt="Figure 2.6 – Image model inference parameters" src="img/B22045_02_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6 – Image model inference parameters</p>
<p>Let’s take a closer<a id="_idIndexMarker150"/> look at these parameters:</p>
<ul>
<li><strong class="bold">Prompt strength</strong>: This controls the degree of randomness. Lowering the <strong class="bold">Prompt strength</strong> value generates a more random image while increasing it generates a more accurate representation of the prompt.</li>
<li><strong class="bold">Generation step</strong>: Similar to <strong class="bold">Prompt strength</strong>, increasing the <strong class="bold">Generation step</strong> value generates a more intricate and detailed image while decreasing it generates a simpler image.</li>
<li><strong class="bold">Seed</strong>: The <strong class="bold">Seed</strong> parameter controls the initial state of the random number generator, which affects the overall randomness of the generated image. It is important to note that the precise values of these parameters can vary depending on the specific use case and the desired trade-off between image fidelity and randomness.</li>
</ul>
<p>For a more detailed description of these parameters, take a look at the Stable Diffusion documentation: <a href="https://platform.stability.ai/docs/api-reference#tag/Image-to-Image">https://platform.stability.ai/docs/api-reference#tag/Image-to-Image</a>.</p>
<p>If you’re using Amazon <a id="_idIndexMarker151"/>Titan Image Generator, there are various parameters you can use. You can find a full list at <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-image.html">https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-image.html</a>.</p>
<h2 id="_idParaDest-39"><a id="_idTextAnchor039"/>Text playground</h2>
<p>The <strong class="bold">Text playground</strong> serves a<a id="_idIndexMarker152"/> similar function for evaluating <a id="_idIndexMarker153"/>generative text models from Amazon Bedrock. You may enter text prompts that the selected model will then expand upon or continue as a longer passage of generated text reflecting that prompt. The expanded text from the model is shown in the playground’s interface.</p>
<p>However, the <strong class="bold">Text playground </strong>doesn’t manage conversational context. Essentially, it generates a sequence of most likely tokens from the end of the text placed in the <strong class="bold">Text playground</strong> window. The behavior demonstrated in the <strong class="bold">Text playground</strong> is a fundamental building block of the chat behavior, and when chained together over multiple turns, it can create a chat experience.</p>
<p>Hence, similar to the <strong class="bold">Chat playground</strong>, users can also navigate to the text playground, select another model (for instance, Anthropic Claude 3 Sonnet, as shown in <em class="italic">Figure 2</em><em class="italic">.7</em>), update the inference configuration, and prompt the model to generate a response for their use case:</p>
<div><div><img alt="Figure 2.7 – Adding a prompt in the text playground" src="img/B22045_02_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.7 – Adding a prompt in the text playground</p>
<h2 id="_idParaDest-40"><a id="_idTextAnchor040"/>Image playground</h2>
<p>In <strong class="bold">Image playground</strong>, you <a id="_idIndexMarker154"/>can try out two different image<a id="_idIndexMarker155"/> models: Amazon Titan Image Generator and Stability AI’s Stable Diffusion. If these sound new to you, please refer to their eponymous sub-sections in <a href="B22045_01.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>. These models generate images through text or images and also perform in-painting, image editing, and more. Let's see an example:</p>
<div><div><img alt="Figure 2.8 – Adding a prompt in the image playground" src="img/B22045_02_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.8 – Adding a prompt in the image playground</p>
<p>As shown in <em class="italic">Figure 2</em><em class="italic">.8</em>, when we provide <code>High quality, intricate detailed, hyper-realistic cat photography, volumetric lighting, full character, 4k, in spacesuit</code> as a prompt, the model generates an image conditioned on the text that was provided. Within the configuration, you also have the option to provide a <strong class="bold">Negative prompt</strong> value, which tells the model what it shouldn’t generate. In addition, you can provide a <strong class="bold">Reference image</strong> value, which the model will use as a reference to generate the image. In <a href="B22045_09.xhtml#_idTextAnchor171"><em class="italic">Chapter 9</em></a>, we will <a id="_idIndexMarker156"/>explore how image generation and <a id="_idIndexMarker157"/>editing work with Amazon Bedrock.</p>
<h2 id="_idParaDest-41"><a id="_idTextAnchor041"/>API-based approach</h2>
<p>One of the major <a id="_idIndexMarker158"/>benefits of using a unified API for inference<a id="_idIndexMarker159"/> is that it allows you to easily experiment with different models from various providers using the same interface. Even as new model versions are released, you can swap them in with minimal code changes required on your end.</p>
<p>The single API abstraction acts as an insulation layer, shielding your application code from underlying model implementation details. This frees you from vendor lock-in and grants flexibility to adopt cutting-edge models as they become available. With a consistent API shielding this complexity, you can focus on product innovation rather than engineering logistics.</p>
<p>Amazon Bedrock provides a <a id="_idIndexMarker160"/>set of APIs that can be directly<a id="_idIndexMarker161"/> accessed and utilized via the <em class="italic">AWS CLI</em> or <em class="italic">AWS SDK</em>.</p>
<h3>AWS CLI</h3>
<p>The <code>list-foundation-models</code> API:</p>
<pre class="console">
$ aws bedrock list-foundation-models</pre>
<p>Similarly, to invoke a model (for example, the Mistral 7B Instruct model), you can call the <code>invoke-model</code> API of <code>bedrock-runtime</code>. At the time of writing, users have to request model access from the console. Once granted in the system, the following code can be used to invoke the respective model:</p>
<pre class="console">
$ aws bedrock-runtime invoke-model \
  --model-id mistral.mistral-7b-instruct-v0:2 \
  --body "{\"prompt\":\"&lt;s&gt;[INST]100 words tweet on MLOps with
Amazon SageMaker [/INST]\", \"max_tokens\":200, \"temperature\":0.5}" \
  --cli-binary-format raw-in-base64-out \
  output.txt</pre>
<p>In the <code>body</code> parameter of the <code>invoke-model</code> API call, you can see it is written in a particular format (<code>"{\"prompt\":\"&lt;s&gt;[INST]text [/INST]\"</code>). Different models may require a different structure of prompt while invoking the model. If you search for Amazon Bedrock in the AWS console, you can view the actual API request that’s sent to the model. Follow these steps to view the API requests:</p>
<ol>
<li>Open Amazon Bedrock in the AWS console by navigating to <a href="https://console.aws.amazon.com/">https://console.aws.amazon.com/</a> and choosing <strong class="bold">Bedrock</strong> from the search bar.</li>
<li>Select <strong class="bold">Providers</strong> under <strong class="bold">Getting started</strong>.</li>
<li>Choose any provider and model of your choice.</li>
<li>Scroll down to the <strong class="bold">Model</strong> section and expand <strong class="bold">API request</strong>.</li>
</ol>
<p>In <em class="italic">Figure 2</em><em class="italic">.9</em>, you can see the API request in JSON from the <em class="italic">Mistral 7B Instruct</em> model. In the <code>body</code> parameter of the API request, we can see the format of the prompt needed by the model, along with the inference parameters:</p>
<div><div><img alt="Figure 2.9 – Mistral 7B Instruct API request" src="img/B22045_02_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.9 – Mistral 7B Instruct API request</p>
<p>This enables transparency regarding how the user’s input gets formatted and passed to the underlying AI <a id="_idIndexMarker164"/>system. Overall, the playground allows <a id="_idIndexMarker165"/>users to not only test prompts but also inspect the API requests that are made to generate the AI responses.</p>
<h3>AWS SDK</h3>
<p>AWS provides SDKs <a id="_idIndexMarker166"/>for various programming <a id="_idIndexMarker167"/>languages, such as JavaScript, Python, Java, and more. These SDKs provide wrapper libraries that make it easy to integrate Bedrock API calls into your code. It is often beneficial to use an SDK tailored to your programming language of choice. Consulting the SDK documentation for your chosen language can provide helpful code samples, usage guidelines, and other resources to ensure the integration process goes smoothly (<a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html">https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html</a>).</p>
<p>You can call these Bedrock APIs through AWS SDKs from your local machines or use AWS services such as AWS Lambda, Amazon SageMaker Studio notebooks, AWS Cloud9, and others. Using the AWS SDK for Python (Boto3), you can call Bedrock APIs to build ML workflows. Let’s look at some of the APIs provided by Amazon Bedrock and examples of their usage in the <strong class="bold">AWS SDK</strong> for Python (Boto3).</p>
<p>Thus far, we have explored the array of FMs that are offered through Amazon Bedrock, experimenting with various prompts and tuning inference configurations to produce preferred outputs. We’ve tapped into models directly via the Amazon Bedrock playground and examined leveraging the AWS CLI and various SDKs to invoke FMs programmatically.</p>
<p>Having established this foundation of working knowledge, we’ll pivot to investigating Amazon Bedrock’s APIs more deeply. The next section will help us leverage these APIs in custom generative AI applications that harness the power of FMs while providing developers with more control and customization. We will map out an end-to-end workflow – from <a id="_idIndexMarker168"/>initializing a client to generating<a id="_idIndexMarker169"/> outputs – that will empower you to build robust, reliable generative apps powered by industrial-grade FMs.</p>
<h1 id="_idParaDest-42"><a id="_idTextAnchor042"/>Using Amazon Bedrock APIs</h1>
<p>Like other AWS services, Amazon <a id="_idIndexMarker170"/>Bedrock provides several APIs. These APIs can be placed under the Control Plane API for managing, training, and deploying FMs and the Runtime Plane API for making invocations or inference requests to the FMs. Some of the common Control Plane Bedrock APIs include <strong class="bold">ListFoundationModels</strong>, <strong class="bold">GetFoundationModels</strong>, and <strong class="bold">CreateModelCustomizationJob</strong>. On the other hand, the Runtime Plane API has two APIs: <strong class="bold">InvokeModel</strong> and <strong class="bold">InvokeModelWithResponseStream</strong>.</p>
<p>In addition, there are separate APIs associated with Agents for Amazon Bedrock, something we’ll cover in more detail in <a href="B22045_10.xhtml#_idTextAnchor192"><em class="italic">Chapter 10</em></a>.</p>
<p>You can find the complete list of API calls supported by Amazon Bedrock, including all the data types and actions you can perform, at <a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/">https://docs.aws.amazon.com/bedrock/latest/APIReference/</a>. Let’s look at some of the commonly used Bedrock API calls.</p>
<h2 id="_idParaDest-43"><a id="_idTextAnchor043"/>ListFoundationModels</h2>
<p>To utilize the <a id="_idIndexMarker171"/>generative capabilities of Bedrock, the first step <a id="_idIndexMarker172"/>is to discover which FMs are available via the service. The <strong class="bold">ListFoundationModels</strong> API retrieves metadata about the base models, including the unique model ID required to generate content using that model.</p>
<p>The following Python code sample demonstrates how to call the ListFoundationModels API to enumerate the available base models:</p>
<pre class="source-code">
import boto3
bedrock_client = boto3.client(service_name='bedrock')
bedrock_client.list_foundation_models()</pre>
<p>Let’s consider some of the currently available base models and their respective model IDs provided via Amazon Bedrock. You use a model ID as a means to indicate the base model when users intend to leverage any of the existing models using InvokeModel (<a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html">https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html</a>) or InvokeModelWithResponseStream (<a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html">https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html</a>). With this information, the desired model can be selected and its ID can be used to call other Bedrock operations, such as InvokeModel, to generate content tailored to your application’s needs.</p>
<h2 id="_idParaDest-44"><a id="_idTextAnchor044"/>GetFoundationModel</h2>
<p>Utilizing Amazon Bedrock, developers <a id="_idIndexMarker173"/>can<a id="_idIndexMarker174"/> access SOTA generative AI models through the <strong class="bold">GetFoundationModel</strong> API call. This operation retrieves comprehensive information on a specified base model. For example, to return details on Meta’s Llama 3 70B Instruct model in Python, you can run the following code:</p>
<pre class="source-code">
import boto3
bedrock_client = boto3.client(service_name='bedrock')
bedrock_client.get_foundation_model(modelIdentifier='meta.llama3-70b-instruct-v1:0')</pre>
<h2 id="_idParaDest-45"><a id="_idTextAnchor045"/>InvokeModel</h2>
<p>The <strong class="bold">InvokeModel</strong> API simplifies <a id="_idIndexMarker175"/>the deployment of ML <a id="_idIndexMarker176"/>models. With just a few API calls, you can deploy your trained models onto the AWS infrastructure securely. This eliminates the need for managing complex deployment processes, allowing you to focus on the core of your AI application.</p>
<p>You can invoke specified Bedrock models to perform inference using inputs provided in the request body. The InvokeModel API allows you to run inference for various model types, including text, embedding, and image models. This allows users to leverage pretrained models available via Amazon Bedrock to generate predictions and insights by passing data into the model and receiving the desired output.</p>
<p>Here’s an example of an API request for sending text to Meta’s Llama 3 70 B model. Inference parameters depend on the model that you are going to use.</p>
<pre class="source-code">
import boto3
import json
model_id = 'meta.llama3-70b-instruct-v1:0' # change this to use a different version from the model provider
prompt_data = "What is the significance of the number 42?"
# Following the request syntax of invoke_model, you can create request body with the below prompt and respective inference parameters.
payload = json.dumps({
    'prompt': prompt_data,
    'max_gen_len': 512,
    'top_p': 0.5,
    'temperature': 0.5,
})
bedrock_runtime = boto3.client(
    service_name='bedrock-runtime',
    region_name='us-east-1'
)
response = bedrock_runtime.invoke_model(
    body=payload,
    modelId=model_id,
    accept='application/json',
    contentType='application/json'
)
response_body = json.loads(response.get('body').read())
print(response_body.get('generation'))</pre>
<p>As shown in the preceding code block, the <code>InvokeModel</code> operation allows you to perform inference on models. The <code>modelId</code> field specifies the desired model to utilize. The process of obtaining <code>modelId</code> varies based on the model type. By leveraging the <code>InvokeModel</code> operation and specifying the appropriate <code>modelId</code> value, users can harness the power of a plethora of generative AI models to draw relevant insights.</p>
<p>If you are <a id="_idIndexMarker177"/>using <a id="_idIndexMarker178"/>Anthropic Claude models, you can use the Messages API to create conversational interfaces to manage the chat between the user and the model. Here’s an example of an API request that could be sent to the Anthropic Claude Sonnet 3 model:</p>
<pre class="source-code">
import boto3
import json
bedrock_client = boto3.client('bedrock-runtime',region_name='us-east-1')
prompt = """
Task: Compose an email to customer support team.
Output:
"""
messages = [{ "role":'user', "content":[{'type':'text','text': prompt}]}]
max_tokens=512
top_p=1
temp=0.5
system = "You are an AI Assistant"
body=json.dumps(
        {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": max_tokens,
            "messages": messages,
            "temperature": temp,
            "top_p": top_p,
            "system": system
        }
    )
modelId = "anthropic.claude-3-sonnet-20240229-v1:0"
accept = "application/json"
contentType = "application/json"
response = bedrock_client.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)
response_body = json.loads(response.get('body').read())
print(response_body)</pre>
<p>The API manages the back-and-forth flow of dialogue by accepting a series of messages with alternating <em class="italic">user</em> and <em class="italic">assistant</em> roles as input. To learn more about the Messages API, you can look at the documentation: <a href="https://docs.anthropic.com/claude/reference/messages_post">https://docs.anthropic.com/claude/reference/messages_post</a>.</p>
<p>Amazon Bedrock also allows you to precisely configure the throughput your models need to deliver responsive performance for your applications. With <strong class="bold">Provisioned Throughput</strong>, you <a id="_idIndexMarker179"/>can choose the compute capacity your models require to meet your workload demands and latency requirements. Hence, in the case of Amazon and third-party base models, and with customized models, users can purchase Provisioned Throughput before running inferences. This capability ensures that you get the guaranteed <a id="_idIndexMarker180"/>throughput <a id="_idIndexMarker181"/>your models require for optimal cost and performance. More details on Provisioned Throughput can be found here: <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html">https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html</a>.</p>
<h2 id="_idParaDest-46"><a id="_idTextAnchor046"/>InvokeModelWithResponseStream</h2>
<p>This streaming<a id="_idIndexMarker182"/> inference method<a id="_idIndexMarker183"/> that’s available via Amazon Bedrock allows FMs to produce long, coherent content on demand. Rather than waiting for generation to complete, applications can stream results. This allows you to send responses from the model in faster chunks rather than having to wait for a complete response.</p>
<p>To run inference with streaming, you can simply invoke the <code>InvokeModelWithResponseStream</code> operation provided by Amazon Bedrock. This runs inference on the model with the given input and returns the generated content progressively in a stream.</p>
<p>Let’s look at how<a id="_idIndexMarker184"/> the Claude V2 model <a id="_idIndexMarker185"/>can generate a 500-word blog on quantum computing.</p>
<p class="callout-heading">Note</p>
<p class="callout">The following code snippet works when run in a Jupyter Notebook environment. Jupyter Notebook provides additional functionality and initialization that allows this code to operate correctly. Attempting to run this snippet directly in a terminal without the Jupyter environment may result in errors. For the best results, run this code in Jupyter Notebook rather than directly in a terminal.</p>
<pre class="source-code">
from IPython.display import clear_output, display, display_markdown, Markdown
import boto3, json
brt = boto3.client(service_name='bedrock-runtime', region_name='us-east-1'
)
payload = json.dumps({
    'prompt': '\n\nHuman: write a blog on quantum computing in 500 words.\n\nAssistant:',
    'max_tokens_to_sample': 4096
})
response = brt.invoke_model_with_response_stream(
    modelId='anthropic.claude-v2',
    body=payload,
    accept='application/json',
    contentType='application/json'
)
streaming = response.get('body')
output = []
if streaming:
    for event in streaming:
        chunk = event.get('chunk')
        if chunk:
            chunk_object = json.loads(chunk.get('bytes').decode())
            text = chunk_object['completion']
            clear_output(wait=True)
            output.append(text)
            display_markdown(Markdown(''.join(output)))</pre>
<p>This will print out the generated blog text continuously as it is produced by the model. This stream-based approach allows the output to be displayed live while Claude V2 is <em class="italic">writing</em> the blog content. Hence, streaming inference unlocks new real-time and interactive use cases for large generative models.</p>
<p>In this section, we explored Amazon Bedrock’s key APIs, all of which allow us to build generative AI applications. We reviewed how to list the FMs that are available through Amazon Bedrock and detailed how to invoke these models to produce customized outputs. Next, we <a id="_idIndexMarker186"/>will uncover how <a id="_idIndexMarker187"/>Amazon Bedrock integrates with LangChain to choreograph and address intricate use cases. By leveraging Bedrock’s API and LangChain’s orchestration, developers can assemble sophisticated generative solutions.</p>
<h1 id="_idParaDest-47"><a id="_idTextAnchor047"/>Converse API</h1>
<p>The Amazon <a id="_idIndexMarker188"/>Bedrock <strong class="bold">Converse API</strong> offers a standardized method to interact with LLMs available via Amazon Bedrock. It facilitates turn-based communication between users and generative AI models and ensures consistent tool definitions for models that support functions (referred to<a id="_idIndexMarker189"/> as <strong class="bold">function calling</strong>).</p>
<p>The significance of the <code>Converse</code> API lies in its ability to streamline integration. Previously, using the <code>InvokeModel</code> API required adapting to varying JSON request and response structures from different model providers. With the <code>Converse</code> API, a uniform format for requests and responses is implemented across all LLMs on Amazon Bedrock, simplifying development and ensuring consistent interaction protocols.</p>
<p>Let us walk through an example of using <code>Converse</code> API for text generation scenario by leveraging Anthropic Claude 3 Sonnet model. Please ensure you have the required permission for you require permission for <code>bedrock:InvokeModel</code> operation in order to call <code>Converse</code> API.</p>
<pre class="source-code">
# Install the latest version for boto3 to leverage Converse API. We start with uninstalling the previous version
%pip install boto3==1.34.131
# Import the respective libraries
import boto3
import botocore
import os
import json
import sys
#Ensure you have the latest version of boto3 to invoke Converse API
print(boto3.__version__)
#Create client side Amazon Bedrock connection with Boto3 library
region = os.environ.get("AWS_REGION")
bedrock_client = boto3.client(service_name='bedrock-runtime',region_name=region)
model_id = "anthropic.claude-3-sonnet-20240229-v1:0"
# Inference parameters
top_k = 100
temp = 0.3
# inference model request fields
model_fields = {"top_k": top_k}
# Base inference parameters
inference_configuration = {"temperature": temp}
# Setup the system prompts and messages to send to the model.
system_prompts = [{"text": "You are an expert stylist that recommends different attire for the user based on the occasion."}]
message_1 = {
    "role": "user",
    "content": [{"text": "Give me top 3 trending style and attire recommendations for my son's graduation party"}]
  }
messages = []
# Start the conversation with the 1st message.
messages.append(message_1)
# Send the message.
response = bedrock_client.converse(
        modelId=model_id,
        messages=messages,
        system=system_prompts,
        inferenceConfig=inference_configuration,
        additionalModelRequestFields=model_fields
    )
# Add the response message to the conversation.
output_message = response['output']['message']
print(output_message['content'][0]['text'])</pre>
<p>Please note that switching the model ID to another text generation FM available on Amazon<a id="_idIndexMarker190"/> Bedrock allows it to run using the <code>Converse</code> API. The code example above, along with other <code>Converse</code> API examples, has been added to the GitHub repository for readers to experiment with in their own accounts.</p>
<p>The <code>Converse</code> API can also process documents and images. For instance, you can send an image or document in a message and use <code>Converse</code> API to have the model describe its contents. For more details on supported models and model features with <code>Converse</code> API, visit <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-call">https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-call</a></p>
<p>Similarly, the <strong class="bold">ConverseStream API</strong> makes <a id="_idIndexMarker191"/>it easy to send messages to specific Amazon Bedrock models and receive responses in a continuous stream. It provides a unified interface that works across all foundational models supported by Amazon Bedrock for messaging.</p>
<p>To use the <code>ConverseStream</code> API, you can invoke it with the <code>Converse</code> API. Note that you need the <code>bedrock:InvokeModelWithResponseStream</code> operation permission to use <code>ConverseStream.</code></p>
<pre class="source-code">
# Send the message.
model_response = bedrock_client.converse_stream(
        modelId=model_id,
        messages=messages,
        system=system_prompts,
        inferenceConfig=inference_config,
        additionalModelRequestFields=additional_model_fields
    )
# # Add the response message to the conversation.
stream = model_response.get('stream')
if stream:
    for event in stream:
        if 'contentBlockDelta' in event:
print(event['contentBlockDelta']['delta']['text'], end="")</pre>
<p>When you run the<a id="_idIndexMarker192"/> code sample above, it streams the response output. For more information on <code>ConverseStream,</code> you can refer to the documentation at <a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ConverseStream.html">https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ConverseStream.html</a>.</p>
<h1 id="_idParaDest-48"><a id="_idTextAnchor048"/>Amazon Bedrock integration points</h1>
<p>When building <a id="_idIndexMarker193"/>end-to-end generative AI applications, architects must follow best practices for security, performance, cost optimization, and latency reduction, as outlined in the AWS Well-Architected Framework pillars. They aid developers in weighing different choices and optimizations when creating end-to-end systems on AWS. More information on the AWS Well-Architected Framework can be found here: <a href="https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html">https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html</a>.</p>
<p>Many customers looking to build conversational interfaces such as chatbots, virtual assistants, or summarization systems integrate Amazon Bedrock’s serverless API with other services. Useful integration points include orchestration frameworks such as LangChain and AWS Step Functions for invoking Amazon Bedrock models via AWS Lambda.</p>
<p>As customers adopt LLMOps approaches to optimize building, scaling, and deploying LLMs for enterprise applications, these integration tools and frameworks are becoming more widely adopted. The serverless API, orchestration layer, and Lambda functions create a robust and scalable pipeline for delivering performant and cost-effective generative AI services.</p>
<h2 id="_idParaDest-49"><a id="_idTextAnchor049"/>Amazon Bedrock with LangChain</h2>
<p>Now, let’s take our <a id="_idIndexMarker194"/>understanding of Amazon Bedrock and generative <a id="_idIndexMarker195"/>AI applications to the next level by introducing LangChain integration with Amazon Bedrock!</p>
<p>LangChain is a revolutionary framework that empowers developers to build advanced language models and generate human-like text. By chaining together various components, you can create sophisticated use cases that were previously unimaginable. For example, if you work in the financial services industry, you can create an application that can provide insights, a simplified summary, and a Q&amp;A of complex financial documents, and by using the LangChain framework, you can abstract the API complexities. By bringing Bedrock and LangChain together, developers gain the best of both worlds. Need an AI assistant, search engine, or content generator? Spin up a capable model with Bedrock, then use LangChain’s templates and pipelines to craft the perfect prompt and handle the output. This modular approach allows for immense flexibility, adapting as your needs change. By creating a custom prompt template via LangChain, you can pass in different input variables on every run. This allows you to generate content that’s tailored to your specific use case, whether it’s responding to customer feedback or crafting personalized marketing messages.</p>
<p>And it’s easy to get started! LangChain’s Bedrock API component provides a simple way to invoke Bedrock APIs from within a LangChain pipeline. Just a few lines of code can kick off a request, feeding your input to a beefy model and returning the goods. From there, your app has a robust, scalable AI backend ready to go.</p>
<p>The following piece <a id="_idIndexMarker196"/>of code showcases the ease with which you <a id="_idIndexMarker197"/>can leverage Amazon Bedrock with LangChain.</p>
<p class="callout-heading">Note</p>
<p class="callout">Before running the following code, make sure you have the latest version of the LangChain package installed. If not, run the package installation cell provided next to install LangChain in your environment. Alternatively, you can download the package from <a href="https://pypi.org/project/langchain/">https://pypi.org/project/langchain/</a></p>
<pre class="source-code">
# Installing LangChain
!pip install langchain
#import the respective libraries and packages
import os
import sys
import json
import boto3
import botocore
# You need to specify LLM for LangChain Bedrock class, and can pass arguments for inference.
from langchain.llms.bedrock import Bedrock
#Create boto3 client for Amazon Bedrock-runtime
bedrock_client = boto3.client(service_name="bedrock-runtime", region_name='us-east-1')
#Provide the respective model ID of the FM you want to use
modelId="amazon.titan-tg1-large"
#Pass the Model ID and respective arguments to the LangChain Bedrock Class
llm = Bedrock(
    model_id=modelId,
    model_kwargs={
        "maxTokenCount": 4096,
        "stopSequences": [],
        "temperature": 0,
        "topP": 1,
    },
    client=bedrock_client,
)
#Provide Sample prompt data
prompt_data = "Tell me about LangChain"
#Invoke the LLM
response = llm(prompt_data)
print(response)</pre>
<p>As shown in the preceding code snippet, users can invoke a particular model using a simple prompt by easily leveraging the LLM for the LangChain Bedrock class and passing the <a id="_idIndexMarker198"/>respective<a id="_idIndexMarker199"/> FM’s arguments for inference.</p>
<h2 id="_idParaDest-50"><a id="_idTextAnchor050"/>Creating a LangChain custom prompt template</h2>
<p>By creating a <a id="_idIndexMarker200"/>template for a<a id="_idIndexMarker201"/> prompt, you can pass different input variables to it on every run. This is useful when you have to generate content with different input variables that you may be fetching from a database:</p>
<pre class="source-code">
#import the respective libraries and packages
import os
import sys
import boto3
import json
import botocore
# You need to specify LLM for LangChain Bedrock class, and can pass arguments for inference.
from langchain_aws import BedrockLLM
#Create boto3 client for Amazon Bedrock-runtime
bedrock_client = boto3.client(service_name="bedrock-runtime", region_name='us-east-1')
from langchain.prompts import PromptTemplate
# Create a prompt template that has multiple input variables
multi_var_prompt = PromptTemplate(
    input_variables=["leasingAgent", "tenantName", "feedbackFromTenant"],
    template="""
&lt;s&gt;[INST] Write an email from the Leasing Agent {leasingAgent} to {tenantName} in response to the following feedback that was received from the customer:
&lt;customer_feedback&gt;
{feedbackFromTenant}
&lt;/customer_feedback&gt; [/INST]\
"""
)
# Pass in values to the input variables
prompt_data = multi_var_prompt.format(leasingAgent="Jane",
                                 tenantName="Isabella",
                                 feedbackFromTenant="""Hi Jane,
     I have been living in this apartment for 2 years now, and I wanted to appreciate how lucky I am to be living here. I have hardly faced any issues in my apartment, but when any issue occurs, administration staff is always there to fix the problem, and are very polite. They also run multiple events throughout the year for all the tenants which helps us socialize. The best part of the apartment is it's location and it is very much affordable.
     """)
#Provide the respective model ID of the FM you want to use
modelId = 'mistral.mistral-large-2402-v1:0' # change this to use a different version from the model provider
#Pass the Model ID and respective parameters to the Langchain Bedrock Class
llm = BedrockLLM(
    model_id=modelId,
    model_kwargs={
        "max_tokens": 4096,
        "temperature": 0.5,
        "top_p": 0.5,
        "top_k":50,
    },
    client=bedrock_client,
)</pre>
<p>Now, we can invoke Bedrock using the prompt template to see a curated response:</p>
<pre class="source-code">
response = llm(prompt_data)
email = response[response.index('\n')+1:]
print(email)</pre>
<p>This integration exemplifies how LangChain’s framework facilitates the creation of complex language-based tasks. In this instance, the Bedrock API acts as a bridge between the LangChain components and the underlying language model.</p>
<p>Hence, by integrating LangChain and Amazon Bedrock, developers can leverage the advanced<a id="_idIndexMarker202"/> functionalities <a id="_idIndexMarker203"/>of LangChain, such as prompt templates, pipelines, and orchestration capabilities with other AI services, to create dynamic and adaptive applications.</p>
<h2 id="_idParaDest-51"><a id="_idTextAnchor051"/>PartyRock</h2>
<p>Now that we’ve<a id="_idIndexMarker204"/> discussed how to access and explore Amazon Bedrock for<a id="_idIndexMarker205"/> your applications using different techniques, let’s look at another interesting feature. Amazon also has a mechanism to quickly build and deploy a fun and intuitive application for experimentalists and hobbyists through <strong class="bold">PartyRock</strong>, a powerful playground for Amazon Bedrock. Within PartyRock, you can create multiple applications and experiment with Amazon Bedrock. For example, you can create an optimized party plan and budgeting tool for your 5-year-old.</p>
<p>In <em class="italic">Figure 2</em><em class="italic">.10</em>, we have created a sample application that can list different Grammy award winners based on the year(s) that users can input in the app. Users can simply click on the link provided next and enter a particular year (or years in each line) in the left pane. On entering a particular year or a few years, the system will generate the Grammy award<a id="_idIndexMarker206"/> winners in the right pane. You can check out the <a id="_idIndexMarker207"/>app at <a href="https://partyrock.aws/u/shikharkwtra/jAJQre8A0/Grammy-Celebrity-Namer">https://partyrock.aws/u/shikharkwtra/jAJQre8A0/Grammy-Celebrity-Namer</a>.</p>
<div><div><img alt="Figure 2.10 – PartyRock example – Grammy Celebrity Namer" src="img/B22045_02_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.10 – PartyRock example – Grammy Celebrity Namer</p>
<p>PartyRock provides builders with access to FMs from Amazon Bedrock to learn the fundamentals of prompt engineering and generative AI. Users are encouraged to build some cool apps with PartyRock and go a step further to understand Amazon Bedrock. Simply navigate to <a href="https://partyrock.aws/">https://partyrock.aws/</a>, click on <strong class="bold">Build your own app</strong>, and get started with your<a id="_idIndexMarker208"/> journey to becoming a generative AI application <a id="_idIndexMarker209"/>developer on PartyRock!</p>
<h1 id="_idParaDest-52"><a id="_idTextAnchor052"/>Summary</h1>
<p>Before moving on to the next chapter, let’s quickly recap what we covered in this chapter. First, we looked at how to access Amazon Bedrock through the AWS console. Utilizing the Bedrock console, we queried the <strong class="bold">Text</strong>, <strong class="bold">Chat</strong>, and <strong class="bold">Image playground</strong> APIs and experimented with various inference parameters to analyze their impact on model outputs. In addition to interacting with the models through the Bedrock console, we investigated accessing the FMs via the AWS CLI and AWS SDK.</p>
<p>By leveraging the CLI and SDK, we were able to uncover some of the underlying Bedrock APIs that can be used to list available FMs, retrieve detailed information about them, and invoke them. We concluded this chapter by looking at some integration points of Amazon Bedrock, including the popular LangChain framework, and provided a brief overview of PartyRock, a powerful playground in Amazon Bedrock for testing prompts and building fun applications.</p>
<p>Now that we have a good conceptual understanding of Amazon Bedrock and the ability to access various Bedrock models, in the next chapter, we will explore some effective prompt engineering techniques we can implement when we use Amazon Bedrock.</p>
</div>
</body></html>