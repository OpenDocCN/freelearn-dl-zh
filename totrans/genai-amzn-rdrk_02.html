<html><head></head><body>
<div id="_idContainer035">
<h1 class="chapter-number" id="_idParaDest-34"><a id="_idTextAnchor034"/><span class="koboSpan" id="kobo.1.1">2</span></h1>
<h1 id="_idParaDest-35"><a id="_idTextAnchor035"/><span class="koboSpan" id="kobo.2.1">Accessing and Utilizing Models in Amazon Bedrock</span></h1>
<p><span class="koboSpan" id="kobo.3.1">This chapter provides a practical guide to accessing Amazon Bedrock and uncovering its generative AI capabilities. </span><span class="koboSpan" id="kobo.3.2">We will start with an overview of the different interfaces for invoking Bedrock models, including the console playground, </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">command-line interface</span></strong><span class="koboSpan" id="kobo.5.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.6.1">CLI</span></strong><span class="koboSpan" id="kobo.7.1">), and </span><strong class="bold"><span class="koboSpan" id="kobo.8.1">software development kit</span></strong><span class="koboSpan" id="kobo.9.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.10.1">SDK</span></strong><span class="koboSpan" id="kobo.11.1">). </span><span class="koboSpan" id="kobo.11.2">Then, we will unveil some of the core Bedrock APIs, along </span><a id="_idIndexMarker134"/><span class="koboSpan" id="kobo.12.1">with the </span><a id="_idIndexMarker135"/><span class="koboSpan" id="kobo.13.1">code snippets that you can run in your environment. </span><span class="koboSpan" id="kobo.13.2">Finally, we’ll demonstrate how to leverage Bedrock within the LangChain Python framework to build customized pipelines that chain multiple models and provide insight into PartyRock, a powerful playground for </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">Amazon Bedrock.</span></span></p>
<p><span class="koboSpan" id="kobo.15.1">By the end of this chapter, you will be able to run and execute applications by leveraging SOTA FMs available from Amazon Bedrock as you gain a deeper understanding of each of the FMs available and how to utilize them for your needs. </span><span class="koboSpan" id="kobo.15.2">You will also be able to accelerate your creative thinking regarding building new generative AI applications as we dive into building cool apps with PartyRock and learn how to integrate Amazon Bedrock into different </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">use cases.</span></span></p>
<p><span class="koboSpan" id="kobo.17.1">The following key topics will be covered in </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">this chapter:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.19.1">Accessing </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">Amazon Bedrock</span></span></li>
<li><span class="koboSpan" id="kobo.21.1">Using Amazon </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">Bedrock APIs</span></span></li>
<li><span class="koboSpan" id="kobo.23.1">Amazon Bedrock </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">integration points</span></span></li>
</ul>
<h1 id="_idParaDest-36"><a id="_idTextAnchor036"/><span class="koboSpan" id="kobo.25.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.26.1">For this chapter, you’ll need to have access to an </span><strong class="bold"><span class="koboSpan" id="kobo.27.1">Amazon Web Services</span></strong><span class="koboSpan" id="kobo.28.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.29.1">AWS</span></strong><span class="koboSpan" id="kobo.30.1">) account. </span><span class="koboSpan" id="kobo.30.2">If you don’t have one already, you can go to </span><a href="https://aws.amazon.com/getting-started/"><span class="koboSpan" id="kobo.31.1">https://aws.amazon.com/getting-started/</span></a><span class="koboSpan" id="kobo.32.1"> and </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">create one.</span></span></p>
<p><span class="koboSpan" id="kobo.34.1">Once you’ve done this, you’ll need to install and configure the AWS CLI (</span><a href="https://aws.amazon.com/cli/"><span class="koboSpan" id="kobo.35.1">https://aws.amazon.com/cli/</span></a><span class="koboSpan" id="kobo.36.1">) as you’ll need this to access Amazon Bedrock FMs from your local machine. </span><span class="koboSpan" id="kobo.36.2">Since the majority of the code blocks that we will execute are based on Python, setting up an AWS Python SDK (Boto3) (</span><a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html"><span class="koboSpan" id="kobo.37.1">https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html</span></a><span class="koboSpan" id="kobo.38.1">) would be beneficial. </span><span class="koboSpan" id="kobo.38.2">You can set up Python by installing it on your local machine, or using AWS Cloud9, or utilizing AWS Lambda, or leveraging </span><span class="No-Break"><span class="koboSpan" id="kobo.39.1">Amazon SageMaker.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.40.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.41.1">There will be a charge associated with invocating and customizing the FMs of Amazon Bedrock. </span><span class="koboSpan" id="kobo.41.2">Please refer to </span><a href="https://aws.amazon.com/bedrock/pricing/"><span class="koboSpan" id="kobo.42.1">https://aws.amazon.com/bedrock/pricing/</span></a><span class="koboSpan" id="kobo.43.1"> to </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">learn more.</span></span></p>
<h1 id="_idParaDest-37"><a id="_idTextAnchor037"/><span class="koboSpan" id="kobo.45.1">Accessing Amazon Bedrock</span></h1>
<p><span class="koboSpan" id="kobo.46.1">When building a generative </span><a id="_idIndexMarker136"/><span class="koboSpan" id="kobo.47.1">AI application, you’re faced with a dizzying array of choices. </span><span class="koboSpan" id="kobo.47.2">Which FM should you use? </span><span class="koboSpan" id="kobo.47.3">How will you ensure security and privacy? </span><span class="koboSpan" id="kobo.47.4">Do you have the infrastructure to support large-scale deployment? </span><span class="koboSpan" id="kobo.47.5">Enter </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">Amazon Bedrock.</span></span></p>
<p><span class="koboSpan" id="kobo.49.1">As you know by now, Amazon Bedrock provides access to a selection of SOTA FMs from leading AI companies in the space, including AI21 Labs, Anthropic, Cohere, Meta, Stability AI, Amazon, and Mistral. </span><span class="koboSpan" id="kobo.49.2">With a single API, you can tap into cutting-edge generative AI across modalities such as text, embeddings, and images. </span><span class="koboSpan" id="kobo.49.3">You have the flexibility to mix and match models to find the perfect fit for your needs. </span><span class="koboSpan" id="kobo.49.4">Bedrock handles provisioning, scalability, and governance behind the scenes. </span><span class="koboSpan" id="kobo.49.5">Hence, you can choose the best model suited to your needs and simply invoke the Bedrock serverless API to plug those models into </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">your application.</span></span></p>
<p><span class="koboSpan" id="kobo.51.1">So, let’s jump onto the AWS console and see Amazon Bedrock </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">in action.</span></span></p>
<p><span class="koboSpan" id="kobo.53.1">When you open Amazon Bedrock in the AWS console by navigating to https://console.aws.amazon.com/ and choosing Bedrock from the search bar, you can explore different FMs, as well as a few learning tools, as depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.54.1">Figure 2</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.55.1">.1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer025">
<span class="koboSpan" id="kobo.57.1"><img alt="Figure 2.1 – Amazon Bedrock – Overview" src="image/B22045_02_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.58.1">Figure 2.1 – Amazon Bedrock – Overview</span></p>
<p><span class="koboSpan" id="kobo.59.1">Amazon Bedrock provides users with the flexibility to experiment with various models through its playground interface. </span><span class="koboSpan" id="kobo.59.2">Users can access the Bedrock playground from the AWS console by </span><a id="_idIndexMarker137"/><span class="koboSpan" id="kobo.60.1">navigating to the Amazon Bedrock landing page and clicking </span><strong class="bold"><span class="koboSpan" id="kobo.61.1">Examples</span></strong><span class="koboSpan" id="kobo.62.1"> to open the </span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">playground environment.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.64.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.65.1">At the time of writing this book, users will have to initially enable access to the models by navigating to the </span><strong class="bold"><span class="koboSpan" id="kobo.66.1">Model access</span></strong><span class="koboSpan" id="kobo.67.1"> link in the left panel within the Bedrock console (as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.68.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.69.1">.2</span></em><span class="koboSpan" id="kobo.70.1">). </span><span class="koboSpan" id="kobo.70.2">Once you’ve landed on the </span><strong class="bold"><span class="koboSpan" id="kobo.71.1">Model access</span></strong><span class="koboSpan" id="kobo.72.1"> page view, you can click on </span><strong class="bold"><span class="koboSpan" id="kobo.73.1">Manage model access</span></strong><span class="koboSpan" id="kobo.74.1">, select the list of base models you want to leverage for your use cases, and click </span><strong class="bold"><span class="koboSpan" id="kobo.75.1">Save changes</span></strong><span class="koboSpan" id="kobo.76.1">. </span><span class="koboSpan" id="kobo.76.2">Instantly, the users will be given access to those models. </span><span class="koboSpan" id="kobo.76.3">Users can also review the EULA agreement next to the respective base models to view their terms </span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">of service.</span></span></p>
<p><span class="koboSpan" id="kobo.78.1">Within the playground, you can explore the different examples of generative AI models available in Bedrock. </span><span class="koboSpan" id="kobo.78.2">This allows you to test out and interact with the models without needing to configure resources or write any code. </span><span class="koboSpan" id="kobo.78.3">Overall, the playground provides a convenient way for users to try out the capabilities of Bedrock’s generative models. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.79.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.80.1">.2</span></em><span class="koboSpan" id="kobo.81.1"> depicts some of the capabilities available within the Amazon </span><span class="No-Break"><span class="koboSpan" id="kobo.82.1">Bedrock console:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer026">
<span class="koboSpan" id="kobo.83.1"><img alt="Figure 2.2 – Amazon Bedrock’s capabilities" src="image/B22045_02_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.84.1">Figure 2.2 – Amazon Bedrock’s capabilities</span></p>
<p><span class="koboSpan" id="kobo.85.1">Within the playground, you are given the option to start exploring examples based on </span><strong class="bold"><span class="koboSpan" id="kobo.86.1">Text</span></strong><span class="koboSpan" id="kobo.87.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.88.1">Chat</span></strong><span class="koboSpan" id="kobo.89.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.90.1">Image</span></strong><span class="koboSpan" id="kobo.91.1">. </span><span class="koboSpan" id="kobo.91.2">This enables hands-on experimentation with the latest generative AI models in a convenient sandbox environment. </span><span class="koboSpan" id="kobo.91.3">The breadth of options, from conversational chatbots to text and image generation, gives you the flexibility to test diverse AI functions firsthand. </span><span class="koboSpan" id="kobo.91.4">By providing accessible entry points, emerging generative AI</span><a id="_idIndexMarker138"/><span class="koboSpan" id="kobo.92.1"> becomes more tangible and approachable for users to understand. </span><span class="koboSpan" id="kobo.92.2">Now, let’s learn about each of these in </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">greater detail.</span></span></p>
<h2 id="_idParaDest-38"><a id="_idTextAnchor038"/><span class="koboSpan" id="kobo.94.1">Chat playground</span></h2>
<p><span class="koboSpan" id="kobo.95.1">Amazon </span><a id="_idIndexMarker139"/><span class="koboSpan" id="kobo.96.1">Bedrock </span><a id="_idIndexMarker140"/><span class="koboSpan" id="kobo.97.1">gives you access to chat models, which you can experiment with in the </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.98.1">Chat playground</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.100.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.101.1">Chat playground</span></strong><span class="koboSpan" id="kobo.102.1"> is an experimental interface that allows you to test the conversational AI models available through Amazon Bedrock. </span><span class="koboSpan" id="kobo.102.2">You can enter sample prompts and view the responses that are generated by a selected model. </span><span class="koboSpan" id="kobo.102.3">Usage metrics are also displayed to evaluate the model’s performance. </span><span class="koboSpan" id="kobo.102.4">A compare mode is available to contrast the outputs of up to three different models side </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">by side.</span></span></p>
<p><span class="koboSpan" id="kobo.104.1">As shown in the</span><a id="_idIndexMarker141"/><span class="koboSpan" id="kobo.105.1"> following figures, users can select which model </span><a id="_idIndexMarker142"/><span class="koboSpan" id="kobo.106.1">they want to use (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.107.1">Figure 2</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.108.1">.3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer027">
<span class="koboSpan" id="kobo.110.1"><img alt="Figure 2.3 – Selecting a model" src="image/B22045_02_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.111.1">Figure 2.3 – Selecting a model</span></p>
<p><span class="koboSpan" id="kobo.112.1">Thereafter, users can enter a query in the chat box (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.113.1">Figure 2</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.114.1">.4</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer028">
<span class="koboSpan" id="kobo.116.1"><img alt="Figure 2.4 – Querying the chat model in the Chat playground" src="image/B22045_02_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.117.1">Figure 2.4 – Querying the chat model in the Chat playground</span></p>
<p><span class="koboSpan" id="kobo.118.1">Running a query fetches information from the chosen model. </span><span class="koboSpan" id="kobo.118.2">This allows you to evaluate factors such as accuracy, response length, latency, and suitability for your use case. </span><span class="koboSpan" id="kobo.118.3">Selecting the optimal model depends on weighing these factors against </span><span class="No-Break"><span class="koboSpan" id="kobo.119.1">individual needs.</span></span></p>
<p><span class="koboSpan" id="kobo.120.1">While invoking the FMs, you will see the option to</span><a id="_idIndexMarker143"/><span class="koboSpan" id="kobo.121.1"> modify the </span><strong class="bold"><span class="koboSpan" id="kobo.122.1">inference parameters</span></strong><span class="koboSpan" id="kobo.123.1"> so that you can influence the model’s response in a certain way. </span><span class="koboSpan" id="kobo.123.2">While some inference parameters are common among LLMs, image models have a separate set of parameters that can</span><a id="_idIndexMarker144"/><span class="koboSpan" id="kobo.124.1"> be tuned </span><a id="_idIndexMarker145"/><span class="No-Break"><span class="koboSpan" id="kobo.125.1">by users.</span></span></p>
<p><span class="koboSpan" id="kobo.126.1">Let’s look at some of these </span><span class="No-Break"><span class="koboSpan" id="kobo.127.1">common parameters.</span></span></p>
<h3><span class="koboSpan" id="kobo.128.1">LLM inference parameters</span></h3>
<p><em class="italic"><span class="koboSpan" id="kobo.129.1">Temperature</span></em><span class="koboSpan" id="kobo.130.1">, </span><em class="italic"><span class="koboSpan" id="kobo.131.1">Top P</span></em><span class="koboSpan" id="kobo.132.1">, </span><em class="italic"><span class="koboSpan" id="kobo.133.1">Top K</span></em><span class="koboSpan" id="kobo.134.1">, </span><em class="italic"><span class="koboSpan" id="kobo.135.1">Response length</span></em><span class="koboSpan" id="kobo.136.1">, </span><em class="italic"><span class="koboSpan" id="kobo.137.1">Stop sequences</span></em><span class="koboSpan" id="kobo.138.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.139.1">Max tokens</span></em><span class="koboSpan" id="kobo.140.1"> are the inference</span><a id="_idIndexMarker146"/><span class="koboSpan" id="kobo.141.1"> parameters that we will learn about in detail in this section. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.142.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.143.1">.5</span></em><span class="koboSpan" id="kobo.144.1"> shows them on the Amazon Bedrock </span><strong class="bold"><span class="koboSpan" id="kobo.145.1">Chat playground</span></strong><span class="koboSpan" id="kobo.146.1"> screen; they can be found in the </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.147.1">Configurations</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.148.1"> window:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer029">
<span class="koboSpan" id="kobo.149.1"><img alt="Figure 2.5 – Common LLM inference parameters" src="image/B22045_02_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.150.1">Figure 2.5 – Common LLM inference parameters</span></p>
<p><span class="koboSpan" id="kobo.151.1">Let’s take a </span><span class="No-Break"><span class="koboSpan" id="kobo.152.1">closer look:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.153.1">Temperature</span></strong><span class="koboSpan" id="kobo.154.1">: This parameter controls the degree of randomness in the output. </span><span class="koboSpan" id="kobo.154.2">A lower temperature results in more deterministic output, favoring the most likely option. </span><span class="koboSpan" id="kobo.154.3">On the other hand, a higher temperature promotes randomness, leading to a wider range of diverse and creative outputs. </span><span class="koboSpan" id="kobo.154.4">For example, in QA tasks, a lower temperature ensures more factual and concise responses, whereas if your use case revolves around generating creative and diverse output, such as in creative writing or advertisement generation, it might be worthwhile to increase the </span><span class="No-Break"><span class="koboSpan" id="kobo.155.1">temperature value.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.156.1">Top K</span></strong><strong class="bold"><span class="koboSpan" id="kobo.157.1"> and </span></strong><strong class="bold"><span class="koboSpan" id="kobo.158.1">Top P</span></strong><span class="koboSpan" id="kobo.159.1">: Sampling techniques such as </span><strong class="bold"><span class="koboSpan" id="kobo.160.1">Top K</span></strong><span class="koboSpan" id="kobo.161.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.162.1">Top P</span></strong><span class="koboSpan" id="kobo.163.1"> can be employed to enhance the coherence and sense of the output. </span><strong class="bold"><span class="koboSpan" id="kobo.164.1">Top K</span></strong><span class="koboSpan" id="kobo.165.1"> limits the number of </span><a id="_idIndexMarker147"/><span class="koboSpan" id="kobo.166.1">options to a specified number, ensuring a balance between randomness and coherence. </span><strong class="bold"><span class="koboSpan" id="kobo.167.1">Top P</span></strong><span class="koboSpan" id="kobo.168.1">, on the other hand, restricts the predictions with combined probabilities below a specified threshold, preventing highly improbable options from being selected. </span><span class="koboSpan" id="kobo.168.2">These techniques help strike a balance between generating coherent text and maintaining a certain level of randomness, making the text generation process more natural and engaging for </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">the reader.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.170.1">Using these together balances novelty and fluency. </span><span class="koboSpan" id="kobo.170.2">For example, you could set </span><strong class="bold"><span class="koboSpan" id="kobo.171.1">Top K</span></strong><span class="koboSpan" id="kobo.172.1"> to 70 and </span><strong class="bold"><span class="koboSpan" id="kobo.173.1">Top P</span></strong><span class="koboSpan" id="kobo.174.1"> to 0.8. </span><span class="koboSpan" id="kobo.174.2">This allows some uncommon but still relevant words via the </span><strong class="bold"><span class="koboSpan" id="kobo.175.1">Top P</span></strong><span class="koboSpan" id="kobo.176.1"> setting, while </span><strong class="bold"><span class="koboSpan" id="kobo.177.1">Top K</span></strong><span class="koboSpan" id="kobo.178.1"> retains focus on more common words. </span><span class="koboSpan" id="kobo.178.2">The result is text that is fairly fluent with occasional novel words mixed in. </span><span class="koboSpan" id="kobo.178.3">You can experiment with different values for </span><strong class="bold"><span class="koboSpan" id="kobo.179.1">Top K</span></strong><span class="koboSpan" id="kobo.180.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.181.1">Top P</span></strong><span class="koboSpan" id="kobo.182.1"> to achieve the novelty versus fluency balance you want for a particular generative AI application. </span><span class="koboSpan" id="kobo.182.2">Start with a </span><strong class="bold"><span class="koboSpan" id="kobo.183.1">Top K</span></strong><span class="koboSpan" id="kobo.184.1"> value around 50 to 100 and a </span><strong class="bold"><span class="koboSpan" id="kobo.185.1">Top P</span></strong><span class="koboSpan" id="kobo.186.1"> value around 0.7 to 0.9 as reasonable initial settings. </span><span class="koboSpan" id="kobo.186.2">The optimal values depend on factors such as model size, dataset, and </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">use case.</span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.188.1">Stop sequences</span></strong><span class="koboSpan" id="kobo.189.1">: This refers to a set of characters that the FM uses to determine when to stop generating output. </span><span class="koboSpan" id="kobo.189.2">The stop sequence can be any character or sequence of characters that you specify. </span><span class="koboSpan" id="kobo.189.3">For example, if you set the stop sequence to </span><strong class="source-inline"><span class="koboSpan" id="kobo.190.1">bedrock</span></strong><span class="koboSpan" id="kobo.191.1">, the model will stop generating output as soon as it encounters the word </span><em class="italic"><span class="koboSpan" id="kobo.192.1">bedrock</span></em><span class="koboSpan" id="kobo.193.1"> in the </span><span class="No-Break"><span class="koboSpan" id="kobo.194.1">generated text.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.195.1">Max tokens</span></strong><span class="koboSpan" id="kobo.196.1">: Also known as </span><strong class="bold"><span class="koboSpan" id="kobo.197.1">Length</span></strong><span class="koboSpan" id="kobo.198.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.199.1">Response Length</span></strong><span class="koboSpan" id="kobo.200.1"> in some models, this is the maximum length of the generated response in the form of tokens. </span><span class="koboSpan" id="kobo.200.2">Setting a lower </span><strong class="bold"><span class="koboSpan" id="kobo.201.1">Max tokens</span></strong><span class="koboSpan" id="kobo.202.1"> value results in shorter generated text, while higher values allow for longer, more detailed responses from the FM. </span><span class="koboSpan" id="kobo.202.2">Hence, this parameter provides a useful way to specify the desired text length. </span><span class="koboSpan" id="kobo.202.3">For instance, if you are generating sample text for a book description, you may set </span><strong class="bold"><span class="koboSpan" id="kobo.203.1">Max tokens</span></strong><span class="koboSpan" id="kobo.204.1"> to 100 so that the FM generates a concise blurb within a tight word limit. </span><span class="koboSpan" id="kobo.204.2">In </span><a id="_idIndexMarker148"/><span class="koboSpan" id="kobo.205.1">addition, you can also mention the word limit in the prompt by adding a simple instruction such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.206.1">Write a sentence in </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.207.1">100 words</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.208.1">.</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.209.1">Image model inference parameters</span></h3>
<p><span class="koboSpan" id="kobo.210.1">When performing</span><a id="_idIndexMarker149"/><span class="koboSpan" id="kobo.211.1"> image generation with FMs, several key parameters affect the inference process. </span><span class="koboSpan" id="kobo.211.2">For instance, in the case of Stable Diffusion models, the model takes in a text prompt and random noise vector to produce an image. </span><span class="koboSpan" id="kobo.211.3">Several configuration settings for the model can influence the final generated image, as depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.212.1">Figure 2</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.213.1">.6</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer030">
<span class="koboSpan" id="kobo.215.1"><img alt="Figure 2.6 – Image model inference parameters" src="image/B22045_02_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.216.1">Figure 2.6 – Image model inference parameters</span></p>
<p><span class="koboSpan" id="kobo.217.1">Let’s take a closer</span><a id="_idIndexMarker150"/><span class="koboSpan" id="kobo.218.1"> look at </span><span class="No-Break"><span class="koboSpan" id="kobo.219.1">these parameters:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.220.1">Prompt strength</span></strong><span class="koboSpan" id="kobo.221.1">: This controls the degree of randomness. </span><span class="koboSpan" id="kobo.221.2">Lowering the </span><strong class="bold"><span class="koboSpan" id="kobo.222.1">Prompt strength</span></strong><span class="koboSpan" id="kobo.223.1"> value generates a more random image while increasing it generates a more accurate representation of </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">the prompt.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.225.1">Generation step</span></strong><span class="koboSpan" id="kobo.226.1">: Similar to </span><strong class="bold"><span class="koboSpan" id="kobo.227.1">Prompt strength</span></strong><span class="koboSpan" id="kobo.228.1">, increasing the </span><strong class="bold"><span class="koboSpan" id="kobo.229.1">Generation step</span></strong><span class="koboSpan" id="kobo.230.1"> value generates a more intricate and detailed image while decreasing it generates a </span><span class="No-Break"><span class="koboSpan" id="kobo.231.1">simpler image.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.232.1">Seed</span></strong><span class="koboSpan" id="kobo.233.1">: The </span><strong class="bold"><span class="koboSpan" id="kobo.234.1">Seed</span></strong><span class="koboSpan" id="kobo.235.1"> parameter controls the initial state of the random number generator, which affects the overall randomness of the generated image. </span><span class="koboSpan" id="kobo.235.2">It is important to note that the precise values of these parameters can vary depending on the specific use case and the desired trade-off between image fidelity </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">and randomness.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.237.1">For a more detailed description of these parameters, take a look at the Stable Diffusion </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">documentation: </span></span><a href="https://platform.stability.ai/docs/api-reference#tag/Image-to-Image"><span class="No-Break"><span class="koboSpan" id="kobo.239.1">https://platform.stability.ai/docs/api-reference#tag/Image-to-Image</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.240.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.241.1">If you’re using Amazon </span><a id="_idIndexMarker151"/><span class="koboSpan" id="kobo.242.1">Titan Image Generator, there are various parameters you can use. </span><span class="koboSpan" id="kobo.242.2">You can find a full list </span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">at </span></span><a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-image.html"><span class="No-Break"><span class="koboSpan" id="kobo.244.1">https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-image.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.245.1">.</span></span></p>
<h2 id="_idParaDest-39"><a id="_idTextAnchor039"/><span class="koboSpan" id="kobo.246.1">Text playground</span></h2>
<p><span class="koboSpan" id="kobo.247.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.248.1">Text playground</span></strong><span class="koboSpan" id="kobo.249.1"> serves a</span><a id="_idIndexMarker152"/><span class="koboSpan" id="kobo.250.1"> similar function for evaluating </span><a id="_idIndexMarker153"/><span class="koboSpan" id="kobo.251.1">generative text models from Amazon Bedrock. </span><span class="koboSpan" id="kobo.251.2">You may enter text prompts that the selected model will then expand upon or continue as a longer passage of generated text reflecting that prompt. </span><span class="koboSpan" id="kobo.251.3">The expanded text from the model is shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">playground’s interface.</span></span></p>
<p><span class="koboSpan" id="kobo.253.1">However, the </span><strong class="bold"><span class="koboSpan" id="kobo.254.1">Text playground </span></strong><span class="koboSpan" id="kobo.255.1">doesn’t manage conversational context. </span><span class="koboSpan" id="kobo.255.2">Essentially, it generates a sequence of most likely tokens from the end of the text placed in the </span><strong class="bold"><span class="koboSpan" id="kobo.256.1">Text playground</span></strong><span class="koboSpan" id="kobo.257.1"> window. </span><span class="koboSpan" id="kobo.257.2">The behavior demonstrated in the </span><strong class="bold"><span class="koboSpan" id="kobo.258.1">Text playground</span></strong><span class="koboSpan" id="kobo.259.1"> is a fundamental building block of the chat behavior, and when chained together over multiple turns, it can create a </span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">chat experience.</span></span></p>
<p><span class="koboSpan" id="kobo.261.1">Hence, similar to the </span><strong class="bold"><span class="koboSpan" id="kobo.262.1">Chat playground</span></strong><span class="koboSpan" id="kobo.263.1">, users can also navigate to the text playground, select another model (for instance, Anthropic Claude 3 Sonnet, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.264.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.265.1">.7</span></em><span class="koboSpan" id="kobo.266.1">), update the inference configuration, and prompt the model to generate a response for their </span><span class="No-Break"><span class="koboSpan" id="kobo.267.1">use case:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer031">
<span class="koboSpan" id="kobo.268.1"><img alt="Figure 2.7 – Adding a prompt in the text playground" src="image/B22045_02_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.269.1">Figure 2.7 – Adding a prompt in the text playground</span></p>
<h2 id="_idParaDest-40"><a id="_idTextAnchor040"/><span class="koboSpan" id="kobo.270.1">Image playground</span></h2>
<p><span class="koboSpan" id="kobo.271.1">In </span><strong class="bold"><span class="koboSpan" id="kobo.272.1">Image playground</span></strong><span class="koboSpan" id="kobo.273.1">, you </span><a id="_idIndexMarker154"/><span class="koboSpan" id="kobo.274.1">can try out two different image</span><a id="_idIndexMarker155"/><span class="koboSpan" id="kobo.275.1"> models: Amazon Titan Image Generator and Stability AI’s Stable Diffusion. </span><span class="koboSpan" id="kobo.275.2">If these sound new to you, please refer to their eponymous sub-sections in </span><a href="B22045_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.276.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.277.1">. </span><span class="koboSpan" id="kobo.277.2">These models generate images through text or images and also perform in-painting, image editing, and more. </span><span class="koboSpan" id="kobo.277.3">Let's see </span><span class="No-Break"><span class="koboSpan" id="kobo.278.1">an example:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer032">
<span class="koboSpan" id="kobo.279.1"><img alt="Figure 2.8 – Adding a prompt in the image playground" src="image/B22045_02_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.280.1">Figure 2.8 – Adding a prompt in the image playground</span></p>
<p><span class="koboSpan" id="kobo.281.1">As shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.282.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.283.1">.8</span></em><span class="koboSpan" id="kobo.284.1">, when we provide </span><strong class="source-inline"><span class="koboSpan" id="kobo.285.1">High quality, intricate detailed, hyper-realistic cat photography, volumetric lighting, full character, 4k, in spacesuit</span></strong><span class="koboSpan" id="kobo.286.1"> as a prompt, the model generates an image conditioned on the text that was provided. </span><span class="koboSpan" id="kobo.286.2">Within the configuration, you also have the option to provide a </span><strong class="bold"><span class="koboSpan" id="kobo.287.1">Negative prompt</span></strong><span class="koboSpan" id="kobo.288.1"> value, which tells the model what it shouldn’t generate. </span><span class="koboSpan" id="kobo.288.2">In addition, you can provide a </span><strong class="bold"><span class="koboSpan" id="kobo.289.1">Reference image</span></strong><span class="koboSpan" id="kobo.290.1"> value, which the model will use as a reference to generate the image. </span><span class="koboSpan" id="kobo.290.2">In </span><a href="B22045_09.xhtml#_idTextAnchor171"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.291.1">Chapter 9</span></em></span></a><span class="koboSpan" id="kobo.292.1">, we will </span><a id="_idIndexMarker156"/><span class="koboSpan" id="kobo.293.1">explore how image generation and </span><a id="_idIndexMarker157"/><span class="koboSpan" id="kobo.294.1">editing work with </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">Amazon Bedrock.</span></span></p>
<h2 id="_idParaDest-41"><a id="_idTextAnchor041"/><span class="koboSpan" id="kobo.296.1">API-based approach</span></h2>
<p><span class="koboSpan" id="kobo.297.1">One of the major </span><a id="_idIndexMarker158"/><span class="koboSpan" id="kobo.298.1">benefits of using a unified API for inference</span><a id="_idIndexMarker159"/><span class="koboSpan" id="kobo.299.1"> is that it allows you to easily experiment with different models from various providers using the same interface. </span><span class="koboSpan" id="kobo.299.2">Even as new model versions are released, you can swap them in with minimal code changes required on </span><span class="No-Break"><span class="koboSpan" id="kobo.300.1">your end.</span></span></p>
<p><span class="koboSpan" id="kobo.301.1">The single API abstraction acts as an insulation layer, shielding your application code from underlying model implementation details. </span><span class="koboSpan" id="kobo.301.2">This frees you from vendor lock-in and grants flexibility to adopt cutting-edge models as they become available. </span><span class="koboSpan" id="kobo.301.3">With a consistent API shielding this complexity, you can focus on product innovation rather than </span><span class="No-Break"><span class="koboSpan" id="kobo.302.1">engineering logistics.</span></span></p>
<p><span class="koboSpan" id="kobo.303.1">Amazon Bedrock provides a </span><a id="_idIndexMarker160"/><span class="koboSpan" id="kobo.304.1">set of APIs that can be directly</span><a id="_idIndexMarker161"/><span class="koboSpan" id="kobo.305.1"> accessed and utilized via the </span><em class="italic"><span class="koboSpan" id="kobo.306.1">AWS CLI</span></em><span class="koboSpan" id="kobo.307.1"> or </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.308.1">AWS SDK</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">.</span></span></p>
<h3><span class="koboSpan" id="kobo.310.1">AWS CLI</span></h3>
<p><span class="koboSpan" id="kobo.311.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.312.1">AWS CLI</span></strong><span class="koboSpan" id="kobo.313.1"> allows you </span><a id="_idIndexMarker162"/><span class="koboSpan" id="kobo.314.1">to make Bedrock API </span><a id="_idIndexMarker163"/><span class="koboSpan" id="kobo.315.1">calls directly from the terminal of your machine, EC2 instance shell, and so on. </span><span class="koboSpan" id="kobo.315.2">Once you’ve configured the AWS CLI for your account, you can manage a range of AWS services, including Amazon Bedrock. </span><span class="koboSpan" id="kobo.315.3">For example, you can list the FMs available within Bedrock using the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.316.1">list-foundation-models</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.317.1"> API:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.318.1">
$ aws bedrock list-foundation-models</span></pre>
<p><span class="koboSpan" id="kobo.319.1">Similarly, to invoke a model (for example, the Mistral 7B Instruct model), you can call the </span><strong class="source-inline"><span class="koboSpan" id="kobo.320.1">invoke-model</span></strong><span class="koboSpan" id="kobo.321.1"> API of </span><strong class="source-inline"><span class="koboSpan" id="kobo.322.1">bedrock-runtime</span></strong><span class="koboSpan" id="kobo.323.1">. </span><span class="koboSpan" id="kobo.323.2">At the time of writing, users have to request model access from the console. </span><span class="koboSpan" id="kobo.323.3">Once granted in the system, the following code can be used to invoke the </span><span class="No-Break"><span class="koboSpan" id="kobo.324.1">respective model:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.325.1">
$ aws bedrock-runtime invoke-model \
  --model-id mistral.mistral-7b-instruct-v0:2 \
  --body "{\"prompt\":\"&lt;s&gt;[INST]100 words tweet on MLOps with
Amazon SageMaker [/INST]\", \"max_tokens\":200, \"temperature\":0.5}" \
  --cli-binary-format raw-in-base64-out \
  output.txt</span></pre>
<p><span class="koboSpan" id="kobo.326.1">In the </span><strong class="source-inline"><span class="koboSpan" id="kobo.327.1">body</span></strong><span class="koboSpan" id="kobo.328.1"> parameter of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.329.1">invoke-model</span></strong><span class="koboSpan" id="kobo.330.1"> API call, you can see it is written in a particular format (</span><strong class="source-inline"><span class="koboSpan" id="kobo.331.1">"{\"prompt\":\"&lt;s&gt;[INST]text [/INST]\"</span></strong><span class="koboSpan" id="kobo.332.1">). </span><span class="koboSpan" id="kobo.332.2">Different models may require a different structure of prompt while invoking the model. </span><span class="koboSpan" id="kobo.332.3">If you search for Amazon Bedrock in the AWS console, you can view the actual API request that’s sent to the model. </span><span class="koboSpan" id="kobo.332.4">Follow these steps to view the </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">API requests:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.334.1">Open Amazon Bedrock in the AWS console by navigating to </span><a href="https://console.aws.amazon.com/"><span class="koboSpan" id="kobo.335.1">https://console.aws.amazon.com/</span></a><span class="koboSpan" id="kobo.336.1"> and choosing </span><strong class="bold"><span class="koboSpan" id="kobo.337.1">Bedrock</span></strong><span class="koboSpan" id="kobo.338.1"> from the </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">search bar.</span></span></li>
<li><span class="koboSpan" id="kobo.340.1">Select </span><strong class="bold"><span class="koboSpan" id="kobo.341.1">Providers</span></strong><span class="koboSpan" id="kobo.342.1"> under </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.343.1">Getting started</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.345.1">Choose any provider and model of </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">your choice.</span></span></li>
<li><span class="koboSpan" id="kobo.347.1">Scroll down to the </span><strong class="bold"><span class="koboSpan" id="kobo.348.1">Model</span></strong><span class="koboSpan" id="kobo.349.1"> section and expand </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.350.1">API request</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.351.1">.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.352.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.353.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.354.1">.9</span></em><span class="koboSpan" id="kobo.355.1">, you can see the API request in JSON from the </span><em class="italic"><span class="koboSpan" id="kobo.356.1">Mistral 7B Instruct</span></em><span class="koboSpan" id="kobo.357.1"> model. </span><span class="koboSpan" id="kobo.357.2">In the </span><strong class="source-inline"><span class="koboSpan" id="kobo.358.1">body</span></strong><span class="koboSpan" id="kobo.359.1"> parameter of the API request, we can see the format of the prompt needed by the model, along with the </span><span class="No-Break"><span class="koboSpan" id="kobo.360.1">inference parameters:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer033">
<span class="koboSpan" id="kobo.361.1"><img alt="Figure 2.9 – Mistral 7B Instruct API request" src="image/B22045_02_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.362.1">Figure 2.9 – Mistral 7B Instruct API request</span></p>
<p><span class="koboSpan" id="kobo.363.1">This enables transparency regarding how the user’s input gets formatted and passed to the underlying AI </span><a id="_idIndexMarker164"/><span class="koboSpan" id="kobo.364.1">system. </span><span class="koboSpan" id="kobo.364.2">Overall, the playground allows </span><a id="_idIndexMarker165"/><span class="koboSpan" id="kobo.365.1">users to not only test prompts but also inspect the API requests that are made to generate the </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">AI responses.</span></span></p>
<h3><span class="koboSpan" id="kobo.367.1">AWS SDK</span></h3>
<p><span class="koboSpan" id="kobo.368.1">AWS provides SDKs </span><a id="_idIndexMarker166"/><span class="koboSpan" id="kobo.369.1">for various programming </span><a id="_idIndexMarker167"/><span class="koboSpan" id="kobo.370.1">languages, such as JavaScript, Python, Java, and more. </span><span class="koboSpan" id="kobo.370.2">These SDKs provide wrapper libraries that make it easy to integrate Bedrock API calls into your code. </span><span class="koboSpan" id="kobo.370.3">It is often beneficial to use an SDK tailored to your programming language of choice. </span><span class="koboSpan" id="kobo.370.4">Consulting the SDK documentation for your chosen language can provide helpful code samples, usage guidelines, and other resources to ensure the integration process goes </span><span class="No-Break"><span class="koboSpan" id="kobo.371.1">smoothly (</span></span><a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html"><span class="No-Break"><span class="koboSpan" id="kobo.372.1">https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.373.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.374.1">You can call these Bedrock APIs through AWS SDKs from your local machines or use AWS services such as AWS Lambda, Amazon SageMaker Studio notebooks, AWS Cloud9, and others. </span><span class="koboSpan" id="kobo.374.2">Using the AWS SDK for Python (Boto3), you can call Bedrock APIs to build ML workflows. </span><span class="koboSpan" id="kobo.374.3">Let’s look at some of the APIs provided by Amazon Bedrock and examples of their usage in the </span><strong class="bold"><span class="koboSpan" id="kobo.375.1">AWS SDK</span></strong><span class="koboSpan" id="kobo.376.1"> for </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">Python (Boto3).</span></span></p>
<p><span class="koboSpan" id="kobo.378.1">Thus far, we have explored the array of FMs that are offered through Amazon Bedrock, experimenting with various prompts and tuning inference configurations to produce preferred outputs. </span><span class="koboSpan" id="kobo.378.2">We’ve tapped into models directly via the Amazon Bedrock playground and examined leveraging the AWS CLI and various SDKs to invoke </span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">FMs programmatically.</span></span></p>
<p><span class="koboSpan" id="kobo.380.1">Having established this foundation of working knowledge, we’ll pivot to investigating Amazon Bedrock’s APIs more deeply. </span><span class="koboSpan" id="kobo.380.2">The next section will help us leverage these APIs in custom generative AI applications that harness the power of FMs while providing developers with more control and customization. </span><span class="koboSpan" id="kobo.380.3">We will map out an end-to-end workflow – from </span><a id="_idIndexMarker168"/><span class="koboSpan" id="kobo.381.1">initializing a client to generating</span><a id="_idIndexMarker169"/><span class="koboSpan" id="kobo.382.1"> outputs – that will empower you to build robust, reliable generative apps powered by </span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">industrial-grade FMs.</span></span></p>
<h1 id="_idParaDest-42"><a id="_idTextAnchor042"/><span class="koboSpan" id="kobo.384.1">Using Amazon Bedrock APIs</span></h1>
<p><span class="koboSpan" id="kobo.385.1">Like other AWS services, Amazon </span><a id="_idIndexMarker170"/><span class="koboSpan" id="kobo.386.1">Bedrock provides several APIs. </span><span class="koboSpan" id="kobo.386.2">These APIs can be placed under the Control Plane API for managing, training, and deploying FMs and the Runtime Plane API for making invocations or inference requests to the FMs. </span><span class="koboSpan" id="kobo.386.3">Some of the common Control Plane Bedrock APIs include </span><strong class="bold"><span class="koboSpan" id="kobo.387.1">ListFoundationModels</span></strong><span class="koboSpan" id="kobo.388.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.389.1">GetFoundationModels</span></strong><span class="koboSpan" id="kobo.390.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.391.1">CreateModelCustomizationJob</span></strong><span class="koboSpan" id="kobo.392.1">. </span><span class="koboSpan" id="kobo.392.2">On the other hand, the Runtime Plane API has two APIs: </span><strong class="bold"><span class="koboSpan" id="kobo.393.1">InvokeModel</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.394.1">and </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.395.1">InvokeModelWithResponseStream</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.396.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.397.1">In addition, there are separate APIs associated with Agents for Amazon Bedrock, something we’ll cover in more detail in </span><a href="B22045_10.xhtml#_idTextAnchor192"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.398.1">Chapter 10</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.399.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.400.1">You can find the complete list of API calls supported by Amazon Bedrock, including all the data types and actions you can perform, at </span><a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/"><span class="koboSpan" id="kobo.401.1">https://docs.aws.amazon.com/bedrock/latest/APIReference/</span></a><span class="koboSpan" id="kobo.402.1">. </span><span class="koboSpan" id="kobo.402.2">Let’s look at some of the commonly used Bedrock </span><span class="No-Break"><span class="koboSpan" id="kobo.403.1">API calls.</span></span></p>
<h2 id="_idParaDest-43"><a id="_idTextAnchor043"/><span class="koboSpan" id="kobo.404.1">ListFoundationModels</span></h2>
<p><span class="koboSpan" id="kobo.405.1">To utilize the </span><a id="_idIndexMarker171"/><span class="koboSpan" id="kobo.406.1">generative capabilities of Bedrock, the first step </span><a id="_idIndexMarker172"/><span class="koboSpan" id="kobo.407.1">is to discover which FMs are available via the service. </span><span class="koboSpan" id="kobo.407.2">The </span><strong class="bold"><span class="koboSpan" id="kobo.408.1">ListFoundationModels</span></strong><span class="koboSpan" id="kobo.409.1"> API retrieves metadata about the base models, including the unique model ID required to generate content using </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">that model.</span></span></p>
<p><span class="koboSpan" id="kobo.411.1">The following Python code sample demonstrates how to call the ListFoundationModels API to enumerate the available </span><span class="No-Break"><span class="koboSpan" id="kobo.412.1">base models:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.413.1">
import boto3
bedrock_client = boto3.client(service_name='bedrock')
bedrock_client.list_foundation_models()</span></pre>
<p><span class="koboSpan" id="kobo.414.1">Let’s consider some of the currently available base models and their respective model IDs provided via Amazon Bedrock. </span><span class="koboSpan" id="kobo.414.2">You use a model ID as a means to indicate the base model when users intend to leverage any of the existing models using InvokeModel (</span><a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html"><span class="koboSpan" id="kobo.415.1">https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html</span></a><span class="koboSpan" id="kobo.416.1">) or InvokeModelWithResponseStream (</span><a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html"><span class="koboSpan" id="kobo.417.1">https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html</span></a><span class="koboSpan" id="kobo.418.1">). </span><span class="koboSpan" id="kobo.418.2">With this information, the desired model can be selected and its ID can be used to call other Bedrock operations, such as InvokeModel, to generate content tailored to your </span><span class="No-Break"><span class="koboSpan" id="kobo.419.1">application’s needs.</span></span></p>
<h2 id="_idParaDest-44"><a id="_idTextAnchor044"/><span class="koboSpan" id="kobo.420.1">GetFoundationModel</span></h2>
<p><span class="koboSpan" id="kobo.421.1">Utilizing Amazon Bedrock, developers </span><a id="_idIndexMarker173"/><span class="koboSpan" id="kobo.422.1">can</span><a id="_idIndexMarker174"/><span class="koboSpan" id="kobo.423.1"> access SOTA generative AI models through the </span><strong class="bold"><span class="koboSpan" id="kobo.424.1">GetFoundationModel</span></strong><span class="koboSpan" id="kobo.425.1"> API call. </span><span class="koboSpan" id="kobo.425.2">This operation retrieves comprehensive information on a specified base model. </span><span class="koboSpan" id="kobo.425.3">For example, to return details on Meta’s Llama 3 70B Instruct model in Python, you can run the </span><span class="No-Break"><span class="koboSpan" id="kobo.426.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.427.1">
import boto3
bedrock_client = boto3.client(service_name='bedrock')
bedrock_client.get_foundation_model(modelIdentifier='meta.llama3-70b-instruct-v1:0')</span></pre>
<h2 id="_idParaDest-45"><a id="_idTextAnchor045"/><span class="koboSpan" id="kobo.428.1">InvokeModel</span></h2>
<p><span class="koboSpan" id="kobo.429.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.430.1">InvokeModel</span></strong><span class="koboSpan" id="kobo.431.1"> API simplifies </span><a id="_idIndexMarker175"/><span class="koboSpan" id="kobo.432.1">the deployment of ML </span><a id="_idIndexMarker176"/><span class="koboSpan" id="kobo.433.1">models. </span><span class="koboSpan" id="kobo.433.2">With just a few API calls, you can deploy your trained models onto the AWS infrastructure securely. </span><span class="koboSpan" id="kobo.433.3">This eliminates the need for managing complex deployment processes, allowing you to focus on the core of your </span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">AI application.</span></span></p>
<p><span class="koboSpan" id="kobo.435.1">You can invoke specified Bedrock models to perform inference using inputs provided in the request body. </span><span class="koboSpan" id="kobo.435.2">The InvokeModel API allows you to run inference for various model types, including text, embedding, and image models. </span><span class="koboSpan" id="kobo.435.3">This allows users to leverage pretrained models available via Amazon Bedrock to generate predictions and insights by passing data into the model and receiving the </span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">desired output.</span></span></p>
<p><span class="koboSpan" id="kobo.437.1">Here’s an example of an API request for sending text to Meta’s Llama 3 70 B model. </span><span class="koboSpan" id="kobo.437.2">Inference parameters depend on the model that you are going </span><span class="No-Break"><span class="koboSpan" id="kobo.438.1">to use.</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.439.1">
import boto3
import json
model_id = 'meta.llama3-70b-instruct-v1:0' # change this to use a different version from the model provider
prompt_data = "What is the significance of the number 42?"
</span><span class="koboSpan" id="kobo.439.2"># Following the request syntax of invoke_model, you can create request body with the below prompt and respective inference parameters.
</span><span class="koboSpan" id="kobo.439.3">payload = json.dumps({
    'prompt': prompt_data,
    'max_gen_len': 512,
    'top_p': 0.5,
    'temperature': 0.5,
})
bedrock_runtime = boto3.client(
    service_name='bedrock-runtime',
    region_name='us-east-1'
)
response = bedrock_runtime.invoke_model(
    body=payload,
    modelId=model_id,
    accept='application/json',
    contentType='application/json'
)
response_body = json.loads(response.get('body').read())
print(response_body.get('generation'))</span></pre>
<p><span class="koboSpan" id="kobo.440.1">As shown in the preceding code block, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.441.1">InvokeModel</span></strong><span class="koboSpan" id="kobo.442.1"> operation allows you to perform inference on models. </span><span class="koboSpan" id="kobo.442.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.443.1">modelId</span></strong><span class="koboSpan" id="kobo.444.1"> field specifies the desired model to utilize. </span><span class="koboSpan" id="kobo.444.2">The process of obtaining </span><strong class="source-inline"><span class="koboSpan" id="kobo.445.1">modelId</span></strong><span class="koboSpan" id="kobo.446.1"> varies based on the model type. </span><span class="koboSpan" id="kobo.446.2">By leveraging the </span><strong class="source-inline"><span class="koboSpan" id="kobo.447.1">InvokeModel</span></strong><span class="koboSpan" id="kobo.448.1"> operation and specifying the appropriate </span><strong class="source-inline"><span class="koboSpan" id="kobo.449.1">modelId</span></strong><span class="koboSpan" id="kobo.450.1"> value, users can harness the power of a plethora of generative AI models to draw </span><span class="No-Break"><span class="koboSpan" id="kobo.451.1">relevant insights.</span></span></p>
<p><span class="koboSpan" id="kobo.452.1">If you are </span><a id="_idIndexMarker177"/><span class="koboSpan" id="kobo.453.1">using </span><a id="_idIndexMarker178"/><span class="koboSpan" id="kobo.454.1">Anthropic Claude models, you can use the Messages API to create conversational interfaces to manage the chat between the user and the model. </span><span class="koboSpan" id="kobo.454.2">Here’s an example of an API request that could be sent to the Anthropic Claude Sonnet </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">3 model:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.456.1">
import boto3
import json
bedrock_client = boto3.client('bedrock-runtime',region_name='us-east-1')
prompt = """
Task: Compose an email to customer support team.
</span><span class="koboSpan" id="kobo.456.2">Output:
"""
messages = [{ "role":'user', "content":[{'type':'text','text': prompt}]}]
max_tokens=512
top_p=1
temp=0.5
system = "You are an AI Assistant"
body=json.dumps(
        {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": max_tokens,
            "messages": messages,
            "temperature": temp,
            "top_p": top_p,
            "system": system
        }
    )
modelId = "anthropic.claude-3-sonnet-20240229-v1:0"
accept = "application/json"
contentType = "application/json"
response = bedrock_client.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)
response_body = json.loads(response.get('body').read())
print(response_body)</span></pre>
<p><span class="koboSpan" id="kobo.457.1">The API manages the back-and-forth flow of dialogue by accepting a series of messages with alternating </span><em class="italic"><span class="koboSpan" id="kobo.458.1">user</span></em><span class="koboSpan" id="kobo.459.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.460.1">assistant</span></em><span class="koboSpan" id="kobo.461.1"> roles as input. </span><span class="koboSpan" id="kobo.461.2">To learn more about the Messages API, you can look at the </span><span class="No-Break"><span class="koboSpan" id="kobo.462.1">documentation: </span></span><a href="https://docs.anthropic.com/claude/reference/messages_post"><span class="No-Break"><span class="koboSpan" id="kobo.463.1">https://docs.anthropic.com/claude/reference/messages_post</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.464.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.465.1">Amazon Bedrock also allows you to precisely configure the throughput your models need to deliver responsive performance for your applications. </span><span class="koboSpan" id="kobo.465.2">With </span><strong class="bold"><span class="koboSpan" id="kobo.466.1">Provisioned Throughput</span></strong><span class="koboSpan" id="kobo.467.1">, you </span><a id="_idIndexMarker179"/><span class="koboSpan" id="kobo.468.1">can choose the compute capacity your models require to meet your workload demands and latency requirements. </span><span class="koboSpan" id="kobo.468.2">Hence, in the case of Amazon and third-party base models, and with customized models, users can purchase Provisioned Throughput before running inferences. </span><span class="koboSpan" id="kobo.468.3">This capability ensures that you get the guaranteed </span><a id="_idIndexMarker180"/><span class="koboSpan" id="kobo.469.1">throughput </span><a id="_idIndexMarker181"/><span class="koboSpan" id="kobo.470.1">your models require for optimal cost and performance. </span><span class="koboSpan" id="kobo.470.2">More details on Provisioned Throughput can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.471.1">here: </span></span><a href="https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html"><span class="No-Break"><span class="koboSpan" id="kobo.472.1">https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.473.1">.</span></span></p>
<h2 id="_idParaDest-46"><a id="_idTextAnchor046"/><span class="koboSpan" id="kobo.474.1">InvokeModelWithResponseStream</span></h2>
<p><span class="koboSpan" id="kobo.475.1">This streaming</span><a id="_idIndexMarker182"/><span class="koboSpan" id="kobo.476.1"> inference method</span><a id="_idIndexMarker183"/><span class="koboSpan" id="kobo.477.1"> that’s available via Amazon Bedrock allows FMs to produce long, coherent content on demand. </span><span class="koboSpan" id="kobo.477.2">Rather than waiting for generation to complete, applications can stream results. </span><span class="koboSpan" id="kobo.477.3">This allows you to send responses from the model in faster chunks rather than having to wait for a </span><span class="No-Break"><span class="koboSpan" id="kobo.478.1">complete response.</span></span></p>
<p><span class="koboSpan" id="kobo.479.1">To run inference with streaming, you can simply invoke the </span><strong class="bold"><span class="koboSpan" id="kobo.480.1">InvokeModelWithResponseStream</span></strong><span class="koboSpan" id="kobo.481.1"> operation. </span><span class="koboSpan" id="kobo.481.2">For instance, to leverage Claude V2’s streaming capabilities, you would use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.482.1">InvokeModelWithResponseStream</span></strong><span class="koboSpan" id="kobo.483.1"> operation provided by Amazon Bedrock. </span><span class="koboSpan" id="kobo.483.2">This runs inference on the model with the given input and returns the generated content progressively in </span><span class="No-Break"><span class="koboSpan" id="kobo.484.1">a stream.</span></span></p>
<p><span class="koboSpan" id="kobo.485.1">Let’s look at how</span><a id="_idIndexMarker184"/><span class="koboSpan" id="kobo.486.1"> the Claude V2 model </span><a id="_idIndexMarker185"/><span class="koboSpan" id="kobo.487.1">can generate a 500-word blog on </span><span class="No-Break"><span class="koboSpan" id="kobo.488.1">quantum computing.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.489.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.490.1">The following code snippet works when run in a Jupyter Notebook environment. </span><span class="koboSpan" id="kobo.490.2">Jupyter Notebook provides additional functionality and initialization that allows this code to operate correctly. </span><span class="koboSpan" id="kobo.490.3">Attempting to run this snippet directly in a terminal without the Jupyter environment may result in errors. </span><span class="koboSpan" id="kobo.490.4">For the best results, run this code in Jupyter Notebook rather than directly in </span><span class="No-Break"><span class="koboSpan" id="kobo.491.1">a terminal.</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.492.1">
from IPython.display import clear_output, display, display_markdown, Markdown
import boto3, json
brt = boto3.client(service_name='bedrock-runtime', region_name='us-east-1'
)
payload = json.dumps({
    'prompt': '\n\nHuman: write a blog on quantum computing in 500 words.\n\nAssistant:',
    'max_tokens_to_sample': 4096
})
response = brt.invoke_model_with_response_stream(
    modelId='anthropic.claude-v2',
    body=payload,
    accept='application/json',
    contentType='application/json'
)
streaming = response.get('body')
output = []
if streaming:
    for event in streaming:
        chunk = event.get('chunk')
        if chunk:
            chunk_object = json.loads(chunk.get('bytes').decode())
            text = chunk_object['completion']
            clear_output(wait=True)
            output.append(text)
            display_markdown(Markdown(''.join(output)))</span></pre>
<p><span class="koboSpan" id="kobo.493.1">This will print out the generated blog text continuously as it is produced by the model. </span><span class="koboSpan" id="kobo.493.2">This stream-based approach allows the output to be displayed live while Claude V2 is </span><em class="italic"><span class="koboSpan" id="kobo.494.1">writing</span></em><span class="koboSpan" id="kobo.495.1"> the blog content. </span><span class="koboSpan" id="kobo.495.2">Hence, streaming inference unlocks new real-time and interactive use cases for large </span><span class="No-Break"><span class="koboSpan" id="kobo.496.1">generative models.</span></span></p>
<p><span class="koboSpan" id="kobo.497.1">In this section, we explored Amazon Bedrock’s key APIs, all of which allow us to build generative AI applications. </span><span class="koboSpan" id="kobo.497.2">We reviewed how to list the FMs that are available through Amazon Bedrock and detailed how to invoke these models to produce customized outputs. </span><span class="koboSpan" id="kobo.497.3">Next, we </span><a id="_idIndexMarker186"/><span class="koboSpan" id="kobo.498.1">will uncover how </span><a id="_idIndexMarker187"/><span class="koboSpan" id="kobo.499.1">Amazon Bedrock integrates with LangChain to choreograph and address intricate use cases. </span><span class="koboSpan" id="kobo.499.2">By leveraging Bedrock’s API and LangChain’s orchestration, developers can assemble sophisticated </span><span class="No-Break"><span class="koboSpan" id="kobo.500.1">generative solutions.</span></span></p>
<h1 id="_idParaDest-47"><a id="_idTextAnchor047"/><span class="koboSpan" id="kobo.501.1">Converse API</span></h1>
<p><span class="koboSpan" id="kobo.502.1">The Amazon </span><a id="_idIndexMarker188"/><span class="koboSpan" id="kobo.503.1">Bedrock </span><strong class="bold"><span class="koboSpan" id="kobo.504.1">Converse API</span></strong><span class="koboSpan" id="kobo.505.1"> offers a standardized method to interact with LLMs available via Amazon Bedrock. </span><span class="koboSpan" id="kobo.505.2">It facilitates turn-based communication between users and generative AI models and ensures consistent tool definitions for models that support functions (referred to</span><a id="_idIndexMarker189"/><span class="koboSpan" id="kobo.506.1"> as </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.507.1">function calling</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.508.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.509.1">The significance of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.510.1">Converse</span></strong><span class="koboSpan" id="kobo.511.1"> API lies in its ability to streamline integration. </span><span class="koboSpan" id="kobo.511.2">Previously, using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.512.1">InvokeModel</span></strong><span class="koboSpan" id="kobo.513.1"> API required adapting to varying JSON request and response structures from different model providers. </span><span class="koboSpan" id="kobo.513.2">With the </span><strong class="source-inline"><span class="koboSpan" id="kobo.514.1">Converse</span></strong><span class="koboSpan" id="kobo.515.1"> API, a uniform format for requests and responses is implemented across all LLMs on Amazon Bedrock, simplifying development and ensuring consistent </span><span class="No-Break"><span class="koboSpan" id="kobo.516.1">interaction protocols.</span></span></p>
<p><span class="koboSpan" id="kobo.517.1">Let us walk through an example of using </span><strong class="source-inline"><span class="koboSpan" id="kobo.518.1">Converse</span></strong><span class="koboSpan" id="kobo.519.1"> API for text generation scenario by leveraging Anthropic Claude 3 Sonnet model. </span><span class="koboSpan" id="kobo.519.2">Please ensure you have the required permission for you require permission for </span><strong class="source-inline"><span class="koboSpan" id="kobo.520.1">bedrock:InvokeModel</span></strong><span class="koboSpan" id="kobo.521.1"> operation in order to call </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.522.1">Converse</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.523.1"> API.</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.524.1">
# Install the latest version for boto3 to leverage Converse API. </span><span class="koboSpan" id="kobo.524.2">We start with uninstalling the previous version
%pip install boto3==1.34.131
# Import the respective libraries
import boto3
import botocore
import os
import json
import sys
#Ensure you have the latest version of boto3 to invoke Converse API
print(boto3.__version__)
#Create client side Amazon Bedrock connection with Boto3 library
region = os.environ.get("AWS_REGION")
bedrock_client = boto3.client(service_name='bedrock-runtime',region_name=region)
model_id = "anthropic.claude-3-sonnet-20240229-v1:0"
# Inference parameters
top_k = 100
temp = 0.3
# inference model request fields
model_fields = {"top_k": top_k}
# Base inference parameters
inference_configuration = {"temperature": temp}
# Setup the system prompts and messages to send to the model.
</span><span class="koboSpan" id="kobo.524.3">system_prompts = [{"text": "You are an expert stylist that recommends different attire for the user based on the occasion."}]
message_1 = {
    "role": "user",
    "content": [{"text": "Give me top 3 trending style and attire recommendations for my son's graduation party"}]
  }
messages = []
# Start the conversation with the 1st message.
</span><span class="koboSpan" id="kobo.524.4">messages.append(message_1)
# Send the message.
</span><span class="koboSpan" id="kobo.524.5">response = bedrock_client.converse(
        modelId=model_id,
        messages=messages,
        system=system_prompts,
        inferenceConfig=inference_configuration,
        additionalModelRequestFields=model_fields
    )
# Add the response message to the conversation.
</span><span class="koboSpan" id="kobo.524.6">output_message = response['output']['message']
print(output_message['content'][0]['text'])</span></pre>
<p><span class="koboSpan" id="kobo.525.1">Please note that switching the model ID to another text generation FM available on Amazon</span><a id="_idIndexMarker190"/><span class="koboSpan" id="kobo.526.1"> Bedrock allows it to run using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.527.1">Converse</span></strong><span class="koboSpan" id="kobo.528.1"> API. </span><span class="koboSpan" id="kobo.528.2">The code example above, along with other </span><strong class="source-inline"><span class="koboSpan" id="kobo.529.1">Converse</span></strong><span class="koboSpan" id="kobo.530.1"> API examples, has been added to the GitHub repository for readers to experiment with in their </span><span class="No-Break"><span class="koboSpan" id="kobo.531.1">own accounts.</span></span></p>
<p><span class="koboSpan" id="kobo.532.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.533.1">Converse</span></strong><span class="koboSpan" id="kobo.534.1"> API can also process documents and images. </span><span class="koboSpan" id="kobo.534.2">For instance, you can send an image or document in a message and use </span><strong class="source-inline"><span class="koboSpan" id="kobo.535.1">Converse</span></strong><span class="koboSpan" id="kobo.536.1"> API to have the model describe its contents. </span><span class="koboSpan" id="kobo.536.2">For more details on supported models and model features with </span><strong class="source-inline"><span class="koboSpan" id="kobo.537.1">Converse</span></strong><span class="koboSpan" id="kobo.538.1"> API, </span><span class="No-Break"><span class="koboSpan" id="kobo.539.1">visit </span></span><a href="https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-call"><span class="No-Break"><span class="koboSpan" id="kobo.540.1">https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-call</span></span></a></p>
<p><span class="koboSpan" id="kobo.541.1">Similarly, the </span><strong class="bold"><span class="koboSpan" id="kobo.542.1">ConverseStream API</span></strong><span class="koboSpan" id="kobo.543.1"> makes </span><a id="_idIndexMarker191"/><span class="koboSpan" id="kobo.544.1">it easy to send messages to specific Amazon Bedrock models and receive responses in a continuous stream. </span><span class="koboSpan" id="kobo.544.2">It provides a unified interface that works across all foundational models supported by Amazon Bedrock </span><span class="No-Break"><span class="koboSpan" id="kobo.545.1">for messaging.</span></span></p>
<p><span class="koboSpan" id="kobo.546.1">To use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.547.1">ConverseStream</span></strong><span class="koboSpan" id="kobo.548.1"> API, you can invoke it with the </span><strong class="bold"><span class="koboSpan" id="kobo.549.1">converse_stream</span></strong><span class="koboSpan" id="kobo.550.1"> call, replacing the same code as used in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.551.1">Converse</span></strong><span class="koboSpan" id="kobo.552.1"> API. </span><span class="koboSpan" id="kobo.552.2">Note that you need the </span><strong class="source-inline"><span class="koboSpan" id="kobo.553.1">bedrock:InvokeModelWithResponseStream</span></strong><span class="koboSpan" id="kobo.554.1"> operation permission to </span><span class="No-Break"><span class="koboSpan" id="kobo.555.1">use </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.556.1">ConverseStream.</span></strong></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.557.1">
# Send the message.
</span><span class="koboSpan" id="kobo.557.2">model_response = bedrock_client.converse_stream(
        modelId=model_id,
        messages=messages,
        system=system_prompts,
        inferenceConfig=inference_config,
        additionalModelRequestFields=additional_model_fields
    )
# # Add the response message to the conversation.
</span><span class="koboSpan" id="kobo.557.3">stream = model_response.get('stream')
if stream:
    for event in stream:
        if 'contentBlockDelta' in event:
print(event['contentBlockDelta']['delta']['text'], end="")</span></pre>
<p><span class="koboSpan" id="kobo.558.1">When you run the</span><a id="_idIndexMarker192"/><span class="koboSpan" id="kobo.559.1"> code sample above, it streams the response output. </span><span class="koboSpan" id="kobo.559.2">For more information on </span><strong class="source-inline"><span class="koboSpan" id="kobo.560.1">ConverseStream,</span></strong><span class="koboSpan" id="kobo.561.1"> you can refer to the documentation </span><span class="No-Break"><span class="koboSpan" id="kobo.562.1">at </span></span><a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ConverseStream.html"><span class="No-Break"><span class="koboSpan" id="kobo.563.1">https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ConverseStream.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.564.1">.</span></span></p>
<h1 id="_idParaDest-48"><a id="_idTextAnchor048"/><span class="koboSpan" id="kobo.565.1">Amazon Bedrock integration points</span></h1>
<p><span class="koboSpan" id="kobo.566.1">When building </span><a id="_idIndexMarker193"/><span class="koboSpan" id="kobo.567.1">end-to-end generative AI applications, architects must follow best practices for security, performance, cost optimization, and latency reduction, as outlined in the AWS Well-Architected Framework pillars. </span><span class="koboSpan" id="kobo.567.2">They aid developers in weighing different choices and optimizations when creating end-to-end systems on AWS. </span><span class="koboSpan" id="kobo.567.3">More information on the AWS Well-Architected Framework can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.568.1">here: </span></span><a href="https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html"><span class="No-Break"><span class="koboSpan" id="kobo.569.1">https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.570.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.571.1">Many customers looking to build conversational interfaces such as chatbots, virtual assistants, or summarization systems integrate Amazon Bedrock’s serverless API with other services. </span><span class="koboSpan" id="kobo.571.2">Useful integration points include orchestration frameworks such as LangChain and AWS Step Functions for invoking Amazon Bedrock models via </span><span class="No-Break"><span class="koboSpan" id="kobo.572.1">AWS Lambda.</span></span></p>
<p><span class="koboSpan" id="kobo.573.1">As customers adopt LLMOps approaches to optimize building, scaling, and deploying LLMs for enterprise applications, these integration tools and frameworks are becoming more widely adopted. </span><span class="koboSpan" id="kobo.573.2">The serverless API, orchestration layer, and Lambda functions create a robust and scalable pipeline for delivering performant and cost-effective generative </span><span class="No-Break"><span class="koboSpan" id="kobo.574.1">AI services.</span></span></p>
<h2 id="_idParaDest-49"><a id="_idTextAnchor049"/><span class="koboSpan" id="kobo.575.1">Amazon Bedrock with LangChain</span></h2>
<p><span class="koboSpan" id="kobo.576.1">Now, let’s take our </span><a id="_idIndexMarker194"/><span class="koboSpan" id="kobo.577.1">understanding of Amazon Bedrock and generative </span><a id="_idIndexMarker195"/><span class="koboSpan" id="kobo.578.1">AI applications to the next level by introducing LangChain integration with </span><span class="No-Break"><span class="koboSpan" id="kobo.579.1">Amazon Bedrock!</span></span></p>
<p><span class="koboSpan" id="kobo.580.1">LangChain is a revolutionary framework that empowers developers to build advanced language models and generate human-like text. </span><span class="koboSpan" id="kobo.580.2">By chaining together various components, you can create sophisticated use cases that were previously unimaginable. </span><span class="koboSpan" id="kobo.580.3">For example, if you work in the financial services industry, you can create an application that can provide insights, a simplified summary, and a Q&amp;A of complex financial documents, and by using the LangChain framework, you can abstract the API complexities. </span><span class="koboSpan" id="kobo.580.4">By bringing Bedrock and LangChain together, developers gain the best of both worlds. </span><span class="koboSpan" id="kobo.580.5">Need an AI assistant, search engine, or content generator? </span><span class="koboSpan" id="kobo.580.6">Spin up a capable model with Bedrock, then use LangChain’s templates and pipelines to craft the perfect prompt and handle the output. </span><span class="koboSpan" id="kobo.580.7">This modular approach allows for immense flexibility, adapting as your needs change. </span><span class="koboSpan" id="kobo.580.8">By creating a custom prompt template via LangChain, you can pass in different input variables on every run. </span><span class="koboSpan" id="kobo.580.9">This allows you to generate content that’s tailored to your specific use case, whether it’s responding to customer feedback or crafting personalized </span><span class="No-Break"><span class="koboSpan" id="kobo.581.1">marketing messages.</span></span></p>
<p><span class="koboSpan" id="kobo.582.1">And it’s easy to get started! </span><span class="koboSpan" id="kobo.582.2">LangChain’s Bedrock API component provides a simple way to invoke Bedrock APIs from within a LangChain pipeline. </span><span class="koboSpan" id="kobo.582.3">Just a few lines of code can kick off a request, feeding your input to a beefy model and returning the goods. </span><span class="koboSpan" id="kobo.582.4">From there, your app has a robust, scalable AI backend ready </span><span class="No-Break"><span class="koboSpan" id="kobo.583.1">to go.</span></span></p>
<p><span class="koboSpan" id="kobo.584.1">The following piece </span><a id="_idIndexMarker196"/><span class="koboSpan" id="kobo.585.1">of code showcases the ease with which you </span><a id="_idIndexMarker197"/><span class="koboSpan" id="kobo.586.1">can leverage Amazon Bedrock </span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">with LangChain.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.588.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.589.1">Before running the following code, make sure you have the latest version of the LangChain package installed. </span><span class="koboSpan" id="kobo.589.2">If not, run the package installation cell provided next to install LangChain in your environment. </span><span class="koboSpan" id="kobo.589.3">Alternatively, you can download the package </span><span class="No-Break"><span class="koboSpan" id="kobo.590.1">from </span></span><a href="https://pypi.org/project/langchain/"><span class="No-Break"><span class="koboSpan" id="kobo.591.1">https://pypi.org/project/langchain/</span></span></a></p>
<pre class="source-code"><span class="koboSpan" id="kobo.592.1">
# Installing LangChain
!pip install langchain
#import the respective libraries and packages
import os
import sys
import json
import boto3
import botocore
# You need to specify LLM for LangChain Bedrock class, and can pass arguments for inference.
</span><span class="koboSpan" id="kobo.592.2">from langchain.llms.bedrock import Bedrock
#Create boto3 client for Amazon Bedrock-runtime
bedrock_client = boto3.client(service_name="bedrock-runtime", region_name='us-east-1')
#Provide the respective model ID of the FM you want to use
modelId="amazon.titan-tg1-large"
#Pass the Model ID and respective arguments to the LangChain Bedrock Class
llm = Bedrock(
    model_id=modelId,
    model_kwargs={
        "maxTokenCount": 4096,
        "stopSequences": [],
        "temperature": 0,
        "topP": 1,
    },
    client=bedrock_client,
)
#Provide Sample prompt data
prompt_data = "Tell me about LangChain"
#Invoke the LLM
response = llm(prompt_data)
print(response)</span></pre>
<p><span class="koboSpan" id="kobo.593.1">As shown in the preceding code snippet, users can invoke a particular model using a simple prompt by easily leveraging the LLM for the LangChain Bedrock class and passing the </span><a id="_idIndexMarker198"/><span class="koboSpan" id="kobo.594.1">respective</span><a id="_idIndexMarker199"/><span class="koboSpan" id="kobo.595.1"> FM’s arguments </span><span class="No-Break"><span class="koboSpan" id="kobo.596.1">for inference.</span></span></p>
<h2 id="_idParaDest-50"><a id="_idTextAnchor050"/><span class="koboSpan" id="kobo.597.1">Creating a LangChain custom prompt template</span></h2>
<p><span class="koboSpan" id="kobo.598.1">By creating a </span><a id="_idIndexMarker200"/><span class="koboSpan" id="kobo.599.1">template for a</span><a id="_idIndexMarker201"/><span class="koboSpan" id="kobo.600.1"> prompt, you can pass different input variables to it on every run. </span><span class="koboSpan" id="kobo.600.2">This is useful when you have to generate content with different input variables that you may be fetching from </span><span class="No-Break"><span class="koboSpan" id="kobo.601.1">a database:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.602.1">
#import the respective libraries and packages
import os
import sys
import boto3
import json
import botocore
# You need to specify LLM for LangChain Bedrock class, and can pass arguments for inference.
</span><span class="koboSpan" id="kobo.602.2">from langchain_aws import BedrockLLM
#Create boto3 client for Amazon Bedrock-runtime
bedrock_client = boto3.client(service_name="bedrock-runtime", region_name='us-east-1')
from langchain.prompts import PromptTemplate
# Create a prompt template that has multiple input variables
multi_var_prompt = PromptTemplate(
    input_variables=["leasingAgent", "tenantName", "feedbackFromTenant"],
    template="""
&lt;s&gt;[INST] Write an email from the Leasing Agent {leasingAgent} to {tenantName} in response to the following feedback that was received from the customer:
&lt;customer_feedback&gt;
{feedbackFromTenant}
&lt;/customer_feedback&gt; [/INST]\
"""
)
# Pass in values to the input variables
prompt_data = multi_var_prompt.format(leasingAgent="Jane",
                                 tenantName="Isabella",
                                 feedbackFromTenant="""Hi Jane,
     I have been living in this apartment for 2 years now, and I wanted to appreciate how lucky I am to be living here. </span><span class="koboSpan" id="kobo.602.3">I have hardly faced any issues in my apartment, but when any issue occurs, administration staff is always there to fix the problem, and are very polite. </span><span class="koboSpan" id="kobo.602.4">They also run multiple events throughout the year for all the tenants which helps us socialize. </span><span class="koboSpan" id="kobo.602.5">The best part of the apartment is it's location and it is very much affordable.
</span><span class="koboSpan" id="kobo.602.6">     """)
#Provide the respective model ID of the FM you want to use
modelId = 'mistral.mistral-large-2402-v1:0' # change this to use a different version from the model provider
#Pass the Model ID and respective parameters to the Langchain Bedrock Class
llm = BedrockLLM(
    model_id=modelId,
    model_kwargs={
        "max_tokens": 4096,
        "temperature": 0.5,
        "top_p": 0.5,
        "top_k":50,
    },
    client=bedrock_client,
)</span></pre>
<p><span class="koboSpan" id="kobo.603.1">Now, we can invoke Bedrock using the prompt template to see a </span><span class="No-Break"><span class="koboSpan" id="kobo.604.1">curated response:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.605.1">
response = llm(prompt_data)
email = response[response.index('\n')+1:]
print(email)</span></pre>
<p><span class="koboSpan" id="kobo.606.1">This integration exemplifies how LangChain’s framework facilitates the creation of complex language-based tasks. </span><span class="koboSpan" id="kobo.606.2">In this instance, the Bedrock API acts as a bridge between the LangChain components and the underlying </span><span class="No-Break"><span class="koboSpan" id="kobo.607.1">language model.</span></span></p>
<p><span class="koboSpan" id="kobo.608.1">Hence, by integrating LangChain and Amazon Bedrock, developers can leverage the advanced</span><a id="_idIndexMarker202"/><span class="koboSpan" id="kobo.609.1"> functionalities </span><a id="_idIndexMarker203"/><span class="koboSpan" id="kobo.610.1">of LangChain, such as prompt templates, pipelines, and orchestration capabilities with other AI services, to create dynamic and </span><span class="No-Break"><span class="koboSpan" id="kobo.611.1">adaptive applications.</span></span></p>
<h2 id="_idParaDest-51"><a id="_idTextAnchor051"/><span class="koboSpan" id="kobo.612.1">PartyRock</span></h2>
<p><span class="koboSpan" id="kobo.613.1">Now that we’ve</span><a id="_idIndexMarker204"/><span class="koboSpan" id="kobo.614.1"> discussed how to access and explore Amazon Bedrock for</span><a id="_idIndexMarker205"/><span class="koboSpan" id="kobo.615.1"> your applications using different techniques, let’s look at another interesting feature. </span><span class="koboSpan" id="kobo.615.2">Amazon also has a mechanism to quickly build and deploy a fun and intuitive application for experimentalists and hobbyists through </span><strong class="bold"><span class="koboSpan" id="kobo.616.1">PartyRock</span></strong><span class="koboSpan" id="kobo.617.1">, a powerful playground for Amazon Bedrock. </span><span class="koboSpan" id="kobo.617.2">Within PartyRock, you can create multiple applications and experiment with Amazon Bedrock. </span><span class="koboSpan" id="kobo.617.3">For example, you can create an optimized party plan and budgeting tool for </span><span class="No-Break"><span class="koboSpan" id="kobo.618.1">your 5-year-old.</span></span></p>
<p><span class="koboSpan" id="kobo.619.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.620.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.621.1">.10</span></em><span class="koboSpan" id="kobo.622.1">, we have created a sample application that can list different Grammy award winners based on the year(s) that users can input in the app. </span><span class="koboSpan" id="kobo.622.2">Users can simply click on the link provided next and enter a particular year (or years in each line) in the left pane. </span><span class="koboSpan" id="kobo.622.3">On entering a particular year or a few years, the system will generate the Grammy award</span><a id="_idIndexMarker206"/><span class="koboSpan" id="kobo.623.1"> winners in the right pane. </span><span class="koboSpan" id="kobo.623.2">You can check out the </span><a id="_idIndexMarker207"/><span class="koboSpan" id="kobo.624.1">app </span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">at </span></span><a href="https://partyrock.aws/u/shikharkwtra/jAJQre8A0/Grammy-Celebrity-Namer"><span class="No-Break"><span class="koboSpan" id="kobo.626.1">https://partyrock.aws/u/shikharkwtra/jAJQre8A0/Grammy-Celebrity-Namer</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.627.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer034">
<span class="koboSpan" id="kobo.628.1"><img alt="Figure 2.10 – PartyRock example – Grammy Celebrity Namer" src="image/B22045_02_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.629.1">Figure 2.10 – PartyRock example – Grammy Celebrity Namer</span></p>
<p><span class="koboSpan" id="kobo.630.1">PartyRock provides builders with access to FMs from Amazon Bedrock to learn the fundamentals of prompt engineering and generative AI. </span><span class="koboSpan" id="kobo.630.2">Users are encouraged to build some cool apps with PartyRock and go a step further to understand Amazon Bedrock. </span><span class="koboSpan" id="kobo.630.3">Simply navigate to </span><a href="https://partyrock.aws/"><span class="koboSpan" id="kobo.631.1">https://partyrock.aws/</span></a><span class="koboSpan" id="kobo.632.1">, click on </span><strong class="bold"><span class="koboSpan" id="kobo.633.1">Build your own app</span></strong><span class="koboSpan" id="kobo.634.1">, and get started with your</span><a id="_idIndexMarker208"/><span class="koboSpan" id="kobo.635.1"> journey to becoming a generative AI application </span><a id="_idIndexMarker209"/><span class="koboSpan" id="kobo.636.1">developer </span><span class="No-Break"><span class="koboSpan" id="kobo.637.1">on PartyRock!</span></span></p>
<h1 id="_idParaDest-52"><a id="_idTextAnchor052"/><span class="koboSpan" id="kobo.638.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.639.1">Before moving on to the next chapter, let’s quickly recap what we covered in this chapter. </span><span class="koboSpan" id="kobo.639.2">First, we looked at how to access Amazon Bedrock through the AWS console. </span><span class="koboSpan" id="kobo.639.3">Utilizing the Bedrock console, we queried the </span><strong class="bold"><span class="koboSpan" id="kobo.640.1">Text</span></strong><span class="koboSpan" id="kobo.641.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.642.1">Chat</span></strong><span class="koboSpan" id="kobo.643.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.644.1">Image playground</span></strong><span class="koboSpan" id="kobo.645.1"> APIs and experimented with various inference parameters to analyze their impact on model outputs. </span><span class="koboSpan" id="kobo.645.2">In addition to interacting with the models through the Bedrock console, we investigated accessing the FMs via the AWS CLI and </span><span class="No-Break"><span class="koboSpan" id="kobo.646.1">AWS SDK.</span></span></p>
<p><span class="koboSpan" id="kobo.647.1">By leveraging the CLI and SDK, we were able to uncover some of the underlying Bedrock APIs that can be used to list available FMs, retrieve detailed information about them, and invoke them. </span><span class="koboSpan" id="kobo.647.2">We concluded this chapter by looking at some integration points of Amazon Bedrock, including the popular LangChain framework, and provided a brief overview of PartyRock, a powerful playground in Amazon Bedrock for testing prompts and building </span><span class="No-Break"><span class="koboSpan" id="kobo.648.1">fun applications.</span></span></p>
<p><span class="koboSpan" id="kobo.649.1">Now that we have a good conceptual understanding of Amazon Bedrock and the ability to access various Bedrock models, in the next chapter, we will explore some effective prompt engineering techniques we can implement when we use </span><span class="No-Break"><span class="koboSpan" id="kobo.650.1">Amazon Bedrock.</span></span></p>
</div>
</body></html>