["```py\npip install learn2learn  # after installing new environment with torch\n\npip install tqdm # used for displaying progress\n```", "```py\nimport learn2learn as l2l\nclass Net(nn.Module):\n    def __init__(self, ways=3):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n        self.fc2 = nn.Linear(500, ways)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4 * 4 * 50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n```", "```py\ndef main(lr=0.005, maml_lr=0.01, iterations=1000, ways=5, shots=1, tps=32, fas=5, device=torch.device(\"cpu\"),\n         download_location=\"/tmp/mnist\"):\n    transformations = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,)),\n        lambda x: x.view(1, 1, 28, 28),\n    ])\n\n    mnist_train = l2l.data.MetaDataset(MNIST(download_location, train=True, download=True, transform=transformations))\n    # mnist_test = MNIST(file_location, train=False, download=True, transform=transformations)\n\n    train_gen = l2l.data.TaskGenerator(mnist_train, ways=ways, tasks=10000)\n    # test_gen = l2l.data.TaskGenerator(mnist_test, ways=ways)\n\n    model = Net(ways)\n    model.to(device)\n    meta_model = l2l.algorithms.MAML(model, lr=maml_lr)\n    opt = optim.Adam(meta_model.parameters(), lr=lr)\n    loss_func = nn.NLLLoss(reduction=\"sum\")\n```", "```py\niteration_error = 0.0\niteration_acc = 0.0\nfor _ in range(tps):\n    learner = meta_model.clone()\n    train_task = train_gen.sample()\n    valid_task = train_gen.sample(task=train_task.sampled_task)\n```", "```py\nfor step in range(fas):\n    train_error, _ = compute_loss(train_task, device, learner, loss_func, batch=shots * ways)\n    learner.adapt(train_error)\n```", "```py\ndef compute_loss(task, device, learner, loss_func, batch=5):\n    loss = 0.0\n    acc = 0.0\n    dataloader = DataLoader(task, batch_size=batch, shuffle=False, num_workers=0)\n    for i, (x, y) in enumerate(dataloader):\n        x, y = x.squeeze(dim=1).to(device), y.view(-1).to(device)\n        output = learner(x)\n        curr_loss = loss_func(output, y)\n        acc += accuracy(output, y)\n        loss += curr_loss / x.size(0)\n    loss /= len(dataloader)\n    return loss, acc\n```", "```py\ntrain_error, _ = compute_loss(train_task, device, learner, loss_func, batch=shots * ways)\nlearner.adapt(train_error)\n```", "```py\nvalid_error, valid_acc = compute_loss(valid_task, device, learner, loss_func, batch=shots * ways)\niteration_error += valid_error\niteration_acc += valid_acc\n```", "```py\niteration_error /= tps\niteration_acc /= tps\ntqdm_bar.set_description(\"Loss : {:.3f} Acc : {:.3f}\".format(iteration_error.item(), iteration_acc))\n\n# Take the meta-learning step\nopt.zero_grad()\niteration_error.backward()\nopt.step()\n```", "```py\npip install cherry-rl\n```", "```py\ndef main(\n        experiment='dev',\n        env_name='Particles2D-v1',\n        adapt_lr=0.1,\n        meta_lr=0.01,\n        adapt_steps=1,\n        num_iterations=200,\n        meta_bsz=20,\n        adapt_bsz=20,\n        tau=1.00,\n        gamma=0.99,\n        num_workers=2,\n        seed=42,\n):\n    random.seed(seed)\n    np.random.seed(seed)\n    th.manual_seed(seed)\n\n    def make_env():\n        return gym.make(env_name)\n```", "```py\nenv = l2l.gym.AsyncVectorEnv([make_env for _ in range(num_workers)])\nenv.seed(seed)\nenv = ch.envs.Torch(env)\npolicy = DiagNormalPolicy(env.state_size, env.action_size)\nmeta_learner = l2l.algorithms.MetaSGD(policy, lr=meta_lr)\nbaseline = LinearValue(env.state_size, env.action_size)\nopt = optim.Adam(policy.parameters(), lr=meta_lr)\nall_rewards = []\n```", "```py\nfor iteration in range(num_iterations):\n    iteration_loss = 0.0\n    iteration_reward = 0.0\n    for task_config in tqdm(env.sample_tasks(meta_bsz)): \n        learner = meta_learner.clone()\n        env.set_task(task_config)\n        env.reset()\n        task = ch.envs.Runner(env)\n\n        # Fast Adapt\n        for step in range(adapt_steps):\n            train_episodes = task.run(learner, episodes=adapt_bsz)\n            loss = maml_a2c_loss(train_episodes, learner, baseline, gamma, tau)\n            learner.adapt(loss)\n```", "```py\nvalid_episodes = task.run(learner, episodes=adapt_bsz)\nloss = maml_a2c_loss(valid_episodes, learner, baseline, gamma, tau)\niteration_loss += loss\niteration_reward += valid_episodes.reward().sum().item() / adapt_bsz\n```", "```py\nadaptation_loss = iteration_loss / meta_bsz\nprint('adaptation_loss', adaptation_loss.item())\n\nopt.zero_grad()\nadaptation_loss.backward()\nopt.step()\n```", "```py\ndef compute_advantages(baseline, tau, gamma, rewards, dones, states, next_states):\n    # Update baseline\n    returns = ch.td.discount(gamma, rewards, dones)\n    baseline.fit(states, returns)\n    values = baseline(states)\n    next_values = baseline(next_states)\n    bootstraps = values * (1.0 - dones) + next_values * dones\n    next_value = th.zeros(1, device=values.device)\n    return ch.pg.generalized_advantage(tau=tau,\n                                       gamma=gamma,\n                                       rewards=rewards,\n                                       dones=dones,\n                                       values=bootstraps,\n                                       next_value=next_value)\n\ndef maml_a2c_loss(train_episodes, learner, baseline, gamma, tau):    \n    states = train_episodes.state()\n    actions = train_episodes.action()\n    rewards = train_episodes.reward()\n    dones = train_episodes.done()\n    next_states = train_episodes.next_state()\n    log_probs = learner.log_prob(states, actions)\n    advantages = compute_advantages(baseline, tau, gamma, rewards,\n                                    dones, states, next_states)\n    advantages = ch.normalize(advantages).detach()\n    return a2c.policy_loss(log_probs, advantages)\n```", "```py\nclass Env(object):\n    def __init__(self, num_bits):\n        self.num_bits = num_bits\n\n    def reset(self):\n        self.done = False\n        self.num_steps = 0\n        self.state = np.random.randint(2, size=self.num_bits)\n        self.target = np.random.randint(2, size=self.num_bits)\n        return self.state, self.target\n\n    def step(self, action):\n        if self.done:\n            raise RESET        \n        self.state[action] = 1 - self.state[action]        \n        if self.num_steps > self.num_bits + 1:\n            self.done = True\n        self.num_steps += 1        \n        if np.sum(self.state == self.target) == self.num_bits:\n            self.done = True\n            return np.copy(self.state), 0, self.done, {}\n        else:\n            return np.copy(self.state), -1, self.done, {}\n```", "```py\nclass Model(nn.Module):\n    def __init__(self, num_inputs, num_outputs, hidden_size=256):\n        super(Model, self).__init__()\n\n        self.linear1 = nn.Linear(num_inputs, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, num_outputs)\n\n    def forward(self, state, goal):\n        x = torch.cat([state, goal], 1)\n        x = F.relu(self.linear1(x))\n        x = self.linear2(x)\n        return x\n```", "```py\nnew_episode = []\n  for state, reward, done, next_state, goal in episode:\n    for t in np.random.choice(num_bits, new_goals):\n      try:\n        episode[t]\n      except:\n        continue\n      new_goal = episode[t][-2]\n      if np.sum(next_state == new_goal) == num_bits:\n        reward = 0\n      else:\n        reward = -1\n      replay_buffer.push(state, action, reward, next_state, done, new_goal)\n      new_episode.append((state, reward, done, next_state, new_goal)) \n```", "```py\ndef get_action(model, state, goal, epsilon=0.1):\n    if random.random() < 0.1:\n        return random.randrange(env.num_bits)\n\n    state = torch.FloatTensor(state).unsqueeze(0).to(device)\n    goal = torch.FloatTensor(goal).unsqueeze(0).to(device)\n    q_value = model(state, goal)\n    return q_value.max(1)[1].item()\n```", "```py\ndef compute_td_error(batch_size):\n    if batch_size > len(replay_buffer):\n        return None\n\n    state, action, reward, next_state, done, goal = replay_buffer.sample(batch_size)\n\n    state = torch.FloatTensor(state).to(device)\n    reward = torch.FloatTensor(reward).unsqueeze(1).to(device)\n    action = torch.LongTensor(action).unsqueeze(1).to(device)\n    next_state = torch.FloatTensor(next_state).to(device)\n    goal = torch.FloatTensor(goal).to(device)\n    mask = torch.FloatTensor(1 - np.float32(done)).unsqueeze(1).to(device)\n\n    q_values = model(state, goal)\n    q_value = q_values.gather(1, action)\n\n    next_q_values = target_model(next_state, goal)\n    target_action = next_q_values.max(1)[1].unsqueeze(1)\n    next_q_value = target_model(next_state, goal).gather(1, target_action)\n\n    expected_q_value = reward + 0.99 * next_q_value * mask\n\n    loss = (q_value - expected_q_value.detach()).pow(2).mean()\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    return loss\n```", "```py\nclass ActorCritic(OnPolicy):\n    def __init__(self, in_shape, num_actions):\n        super(ActorCritic, self).__init__()\n\n        self.in_shape = in_shape\n\n        self.features = nn.Sequential(\n            nn.Conv2d(in_shape[0], 16, kernel_size=3, stride=1),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=2),\n            nn.ReLU(),\n        )\n\n        self.fc = nn.Sequential(\n            nn.Linear(self.feature_size(), 256),\n            nn.ReLU(),\n        )\n\n        self.critic = nn.Linear(256, 1)\n        self.actor = nn.Linear(256, num_actions)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        logit = self.actor(x)\n        value = self.critic(x)\n        return logit, value\n\n    def feature_size(self):\n        return self.features(autograd.Variable(torch.zeros(1, *self.in_shape))).view(1, -1).size(1)\n```", "```py\nclass RolloutStorage(object):\n    def __init__(self, num_steps, num_envs, state_shape):\n        self.num_steps = num_steps\n        self.num_envs = num_envs\n        self.states = torch.zeros(num_steps + 1, num_envs, *state_shape)\n        self.rewards = torch.zeros(num_steps, num_envs, 1)\n        self.masks = torch.ones(num_steps + 1, num_envs, 1)\n        self.actions = torch.zeros(num_steps, num_envs, 1).long()\n        #self.use_cuda = False\n\n    def cuda(self):\n        #self.use_cuda = True\n        self.states = self.states.cuda()\n        self.rewards = self.rewards.cuda()\n        self.masks = self.masks.cuda()\n        self.actions = self.actions.cuda()\n\n    def insert(self, step, state, action, reward, mask):\n        self.states[step + 1].copy_(state)\n        self.actions[step].copy_(action)\n        self.rewards[step].copy_(reward)\n        self.masks[step + 1].copy_(mask)\n\n    def after_update(self):\n        self.states[0].copy_(self.states[-1])\n        self.masks[0].copy_(self.masks[-1])\n\n    def compute_returns(self, next_value, gamma):\n        returns = torch.zeros(self.num_steps + 1, self.num_envs, 1)\n        #if self.use_cuda:\n        # returns = returns.cuda()\n        returns[-1] = next_value\n        for step in reversed(range(self.num_steps)):\n            returns[step] = returns[step + 1] * gamma * self.masks[step + 1] + self.rewards[step]\n        return returns[:-1]\n```", "```py\ndef main():\n    mode = \"regular\"\n    num_envs = 16\n\n    def make_env():\n        def _thunk():\n            env = MiniPacman(mode, 1000)\n            return env\n\n        return _thunk\n\n    envs = [make_env() for i in range(num_envs)]\n    envs = SubprocVecEnv(envs)\n\n    state_shape = envs.observation_space.shape\n```", "```py\nfor i_update in range(num_frames):\n        for step in range(num_steps):\n            action = actor_critic.act(autograd.Variable(state))\n```", "```py\noptimizer.zero_grad()\nloss = value_loss * value_loss_coef + action_loss - entropy * entropy_coef\nloss.backward()\nnn.utils.clip_grad_norm(actor_critic.parameters(), max_grad_norm)\noptimizer.step()\n```", "```py\nactor_critic.load_state_dict(torch.load(\"actor_critic_\" + mode))\n```", "```py\nfor frame_idx, states, actions, rewards, next_states, dones in play_games(envs, num_updates):\n    states = torch.FloatTensor(states)\n    actions = torch.LongTensor(actions)\n    batch_size = states.size(0)\n\n    onehot_actions = torch.zeros(batch_size, num_actions, *state_shape[1:])\n    onehot_actions[range(batch_size), actions] = 1\n    inputs = autograd.Variable(torch.cat([states, onehot_actions], 1))\n```", "```py\nimagined_state, imagined_reward = env_model(inputs)\n\ntarget_state = pix_to_target(next_states)\ntarget_state = autograd.Variable(torch.LongTensor(target_state))\ntarget_reward = rewards_to_target(mode, rewards)\ntarget_reward = autograd.Variable(torch.LongTensor(target_reward))\n\noptimizer.zero_grad()\nimage_loss = criterion(imagined_state, target_state)\nreward_loss = criterion(imagined_reward, target_reward)\nloss = image_loss + reward_coef * reward_loss\nloss.backward()\noptimizer.step()\n\nlosses.append(loss.item())\nall_rewards.append(np.mean(rewards))\n```", "```py\nwhile not done:\n    steps += 1\n    actions = get_action(state)\n    onehot_actions = torch.zeros(batch_size, num_actions, *state_shape[1:])\n    onehot_actions[range(batch_size), actions] = 1\n    state = torch.FloatTensor(state).unsqueeze(0)\n\n    inputs = autograd.Variable(torch.cat([state, onehot_actions], 1))      \n    imagined_state, imagined_reward = env_model(inputs)\n    imagined_state = F.softmax(imagined_state)\n    iss.append(imagined_state)\n\n    next_state, reward, done, _ = env.step(actions[0])\n    ss.append(state)\n    state = next_state\n\n    imagined_image = target_to_pix(imagined_state.view(batch_size, -1, len(pixels))[0].max(1)[1].data.cpu().numpy())\n    imagined_image = imagined_image.reshape(15, 19, 3)\n    state_image = torch.FloatTensor(next_state).permute(1, 2, 0).cpu().numpy()\n\n    plt.figure(figsize=(10,3))\n    plt.subplot(131)\n    plt.title(\"Imagined\")\n    plt.imshow(imagined_image)\n    plt.subplot(132)\n    plt.title(\"Actual\")\n    plt.imshow(state_image)\n    plt.show()\n    time.sleep(0.3)\n\n    if steps > 30:\n       break\n```", "```py\nclass BasicBlock(nn.Module):\n    def __init__(self, in_shape, n1, n2, n3):\n        super(BasicBlock, self).__init__()\n\n        self.in_shape = in_shape\n        self.n1 = n1\n        self.n2 = n2\n        self.n3 = n3\n\n        self.maxpool = nn.MaxPool2d(kernel_size=in_shape[1:])\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_shape[0] * 2, n1, kernel_size=1, stride=2, padding=6),\n            nn.ReLU(),\n            nn.Conv2d(n1, n1, kernel_size=10, stride=1, padding=(5, 6)),\n            nn.ReLU(),\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_shape[0] * 2, n2, kernel_size=1),\n            nn.ReLU(),\n            nn.Conv2d(n2, n2, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(n1 + n2, n3, kernel_size=1),\n            nn.ReLU()\n        )\n\n    def forward(self, inputs):\n        x = self.pool_and_inject(inputs)\n        x = torch.cat([self.conv1(x), self.conv2(x)], 1)\n        x = self.conv3(x)\n        x = torch.cat([x, inputs], 1)\n        return x\n\n    def pool_and_inject(self, x):\n        pooled = self.maxpool(x)\n        tiled = pooled.expand((x.size(0),) + self.in_shape)\n        out = torch.cat([tiled, x], 1)\n        return out\n```", "```py\nclass I2A(OnPolicy):\n    def __init__(self, in_shape, num_actions, num_rewards, hidden_size, imagination, full_rollout=True):\n        super(I2A, self).__init__()\n\n        self.in_shape = in_shape\n        self.num_actions = num_actions\n        self.num_rewards = num_rewards\n\n        self.imagination = imagination\n\n        self.features = nn.Sequential(\n            nn.Conv2d(in_shape[0], 16, kernel_size=3, stride=1),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=2),\n            nn.ReLU(),\n        )\n\n        self.encoder = RolloutEncoder(in_shape, num_rewards, hidden_size)\n\n        if full_rollout:\n            self.fc = nn.Sequential(\n                nn.Linear(self.feature_size() + num_actions * hidden_size, 256),\n                nn.ReLU(),\n            )\n        else:\n            self.fc = nn.Sequential(\n                nn.Linear(self.feature_size() + hidden_size, 256),\n                nn.ReLU(),\n            )\n\n        self.critic = nn.Linear(256, 1)\n        self.actor = nn.Linear(256, num_actions)\n\n    def forward(self, state):\n        batch_size = state.size(0)\n\n        imagined_state, imagined_reward = self.imagination(state.data)\n        hidden = self.encoder(autograd.Variable(imagined_state), autograd.Variable(imagined_reward))\n        hidden = hidden.view(batch_size, -1)\n\n        state = self.features(state)\n        state = state.view(state.size(0), -1)\n\n        x = torch.cat([state, hidden], 1)\n        x = self.fc(x)\n\n        logit = self.actor(x)\n        value = self.critic(x)\n\n        return logit, value\n\n    def feature_size(self):\n        return self.features(autograd.Variable(torch.zeros(1, *self.in_shape))).view(1, -1).size(1)\n```", "```py\nclass ImaginationCore(object):\n    def __init__(self, num_rolouts, in_shape, num_actions, num_rewards, env_model, distil_policy, full_rollout=True):\n        self.num_rolouts = num_rolouts\n        self.in_shape = in_shape\n        self.num_actions = num_actions\n        self.num_rewards = num_rewards\n        self.env_model = env_model\n        self.distil_policy = distil_policy\n        self.full_rollout = full_rollout\n\n    def __call__(self, state):\n        state = state.cpu()\n        batch_size = state.size(0)\n\n        rollout_states = []\n        rollout_rewards = []\n\n        if self.full_rollout:\n            state = state.unsqueeze(0).repeat(self.num_actions, 1, 1, 1, 1).view(-1, *self.in_shape)\n            action = torch.LongTensor([[i] for i in range(self.num_actions)]*batch_size)\n            action = action.view(-1)\n            rollout_batch_size = batch_size * self.num_actions\n        else:\n            action = self.distil_policy.act(autograd.Variable(state, volatile=True))\n            action = action.data.cpu()\n            rollout_batch_size = batch_size\n\n        for step in range(self.num_rolouts):\n            onehot_action = torch.zeros(rollout_batch_size, self.num_actions, *self.in_shape[1:])\n            onehot_action[range(rollout_batch_size), action] = 1\n            inputs = torch.cat([state, onehot_action], 1)\n\n            imagined_state, imagined_reward = self.env_model(autograd.Variable(inputs, volatile=True))\n\n            imagined_state = F.softmax(imagined_state).max(1)[1].data.cpu()\n            imagined_reward = F.softmax(imagined_reward).max(1)[1].data.cpu()\n\n            imagined_state = target_to_pix(imagined_state.numpy())\n            imagined_state = torch.FloatTensor(imagined_state).view(rollout_batch_size, *self.in_shape)\n\n            onehot_reward = torch.zeros(rollout_batch_size, self.num_rewards)\n            onehot_reward[range(rollout_batch_size), imagined_reward] = 1\n\n            rollout_states.append(imagined_state.unsqueeze(0))\n            rollout_rewards.append(onehot_reward.unsqueeze(0))\n\n            state = imagined_state\n            action = self.distil_policy.act(autograd.Variable(state, volatile=True))\n            action = action.data.cpu()\n\n        return torch.cat(rollout_states), torch.cat(rollout_rewards)\n```", "```py\nenvs = [make_env() for i in range(num_envs)]\nenvs = SubprocVecEnv(envs)\nstate_shape = envs.observation_space.shape\nnum_actions = envs.action_space.n\nnum_rewards = len(task_rewards[mode])\n\nfull_rollout = True\n\nenv_model = EnvModel(envs.observation_space.shape, num_pixels, num_rewards)\nenv_model.load_state_dict(torch.load(\"env_model_\" + mode))\ndistil_policy = ActorCritic(envs.observation_space.shape, envs.action_space.n)\ndistil_optimizer = optim.Adam(distil_policy.parameters())\n\nimagination = ImaginationCore(1, state_shape, num_actions, num_rewards, env_model, distil_policy, full_rollout=full_rollout)\n\nactor_critic = I2A(state_shape, num_actions, num_rewards, 256, imagination, full_rollout=full_rollout)\n```", "```py\nfor i_update in tqdm(range(num_frames)):\n    for step in range(num_steps):\n        action = actor_critic.act(autograd.Variable(current_state))\n        next_state, reward, done, _ = envs.step(action.squeeze(1).cpu().data.numpy())\n        reward = torch.FloatTensor(reward).unsqueeze(1)\n        episode_rewards += reward\n        masks = torch.FloatTensor(1-np.array(done)).unsqueeze(1)\n        final_rewards *= masks\n        final_rewards += (1-masks) * episode_rewards\n        episode_rewards *= masks\n```", "```py\n_, next_value = actor_critic(autograd.Variable(rollout.states[-1], volatile=True))\nnext_value = next_value.data\n\nreturns = rollout.compute_returns(next_value, gamma)\nlogit, action_log_probs, values, entropy = actor_critic.evaluate_actions(\nautograd.Variable(rollout.states[:-1]).view(-1, *state_shape),\n            autograd.Variable(rollout.actions).view(-1, 1)\n        )\n\ndistil_logit, _, _, _ = distil_policy.evaluate_actions(\n            autograd.Variable(rollout.states[:-1]).view(-1, *state_shape),\n            autograd.Variable(rollout.actions).view(-1, 1)\n        )\n\ndistil_loss = 0.01 * (F.softmax(logit).detach() * F.log_softmax(distil_logit)).sum(1).mean()\n\nvalues = values.view(num_steps, num_envs, 1)\naction_log_probs = action_log_probs.view(num_steps, num_envs, 1)\nadvantages = autograd.Variable(returns) - values\n\nvalue_loss = advantages.pow(2).mean()\naction_loss = -(autograd.Variable(advantages.data) * action_log_probs).mean()\n\noptimizer.zero_grad()\nloss = value_loss * value_loss_coef + action_loss - entropy * entropy_coef\nloss.backward()\nnn.utils.clip_grad_norm(actor_critic.parameters(), max_grad_norm)\noptimizer.step()\ndistil_optimizer.zero_grad()\ndistil_loss.backward()\noptimizer.step()\n```"]