["```py\n     List five key benefits of Neo4j for knowledge graphs \n    ```", "```py\nTell me about Neo4j \n```", "```py\n    Explain the onboarding process for new hires \n    ```", "```py\nWhat are the benefits of using XYZ software? \n```", "```py\nXYZ software improves productivity, enhances collaboration, and reduces costs. \n```", "```py\nToo generic; lacks specifics about XYZ software. \n```", "```py\nXYZ software offers real-time data synchronization, customizable workflows, and advanced security features, making it ideal for enterprise resource planning. \n```", "```py\ncontext encoder model and *tokenizer* from Hugging Face’s Transformers library:\n```", "```py\n    documents = [\n        \"The IPL 2024 was a thrilling season with unexpected results.\",\n    .....\n        \"Dense Passage Retrieval (') is a state-of-the-art technique for information retrieval.\"\n    ] \n    ```", "```py\n    def encode_documents(documents):\n        inputs = tokenizer(\n            documents, return_tensors='pt', \n            padding=True, truncation=True)\n        with torch.no_grad():\n            outputs = model(**inputs)\n        return outputs.pooler_output.numpy()\n\n    document_embeddings = encode_documents(documents) \n    ```", "```py\n    def retrieve_documents(query, num_results=3):\n        inputs = tokenizer(query, return_tensors='pt', \n            padding=True, truncation=True)\n        with torch.no_grad():\n            query_embedding = model(**inputs).pooler_output\n                                             .numpy()\n        similarity_scores = cosine_similarity(\n            query_embedding, document_embeddings).flatten()\n        top_indices = similarity_scores.argsort()[-num_results:]\n            [::-1]\n        top_docs = [\n            (documents[i], similarity_scores[i]) \n            for i in top_indices]\n        return top_doc \n    ```", "```py\nQuery: What is Dense Passage Retrieval? \n```", "```py\nTop Results:\nScore: 0.7777, Document: Dense Passage Retrieval (') is a state-of-the-art technique for information retrieval.\n... \n```", "```py\n    tokenizer = T5Tokenizer.from_pretrained('t5-small', \n        legacy=False)\n    model = T5ForConditionalGeneration.from_pretrained(\n        't5-small') \n    ```", "```py\n    query = \"What are the benefits of solar energy?\"\n    retrieved_passages = \"\"\"\n    Solar energy is a renewable resource and reduces electricity bills.\n    ......\n    \"\"\" \n    ```", "```py\n    def generate_response(query, retrieved_passages):\n            input_text = f\"Answer this question based on the provided context: {query} Context: {retrieved_passages}\" \n        inputs = tokenizer(input_text, return_tensors='pt', \n            padding=True, \n            truncation=True, max_length=512\n        ).to(device)\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_length=300,  # Allow longer responses\n                num_beams=3,     # Use beam search for better results\n                early_stopping=True\n            )\n        return tokenizer.decode(outputs[0], \n            skip_special_tokens=True) \n    ```", "```py\n    response = generate_response(query, retrieved_passages) \n    print(\"Query:\", query) \n    print(\"Retrieved Passages:\", retrieved_passages) \n    print(\"Generated Response:\", response) \n    ```", "```py\nQuery: What are the benefits of solar energy? \n```", "```py\nSolar energy is a renewable resource and reduces electricity bills.\n...... \n```", "```py\nGenerated Response: it is environmentally friendly and helps combat climate change \n```", "```py\n    def rag_pipeline(query):\n        retrieved_docs = retrieve_documents(query)\n        response = generate_response(query, retrieved_docs)\n        return response\n\n    query = \"How does climate change affect biodiversity?\"\n    generated_text = rag_pipeline(query)\n    print(\"Final Generated Text:\", generated_text) \n    ```", "```py\n    query_inputs = question_tokenizer(query, return_tensors=\"pt\")\n    with torch.no_grad():\n      query_embeddings = question_encoder(\n            **query_inputs\n        ).pooler_output \n    ```", "```py\n    for doc in documents:\n        doc_inputs = context_tokenizer(doc, return_tensors=\"pt\")\n        with torch.no_grad():\n            doc_embeddings.append(\n                context_encoder(**doc_inputs).pooler_output)\n    doc_embeddings = torch.cat(doc_embeddings) \n    ```", "```py\n    scores = torch.matmul(query_embeddings, doc_embeddings.T).squeeze() \n    ```", "```py\n    ranked_docs = sorted(\n        zip(documents, scores), key=lambda x: x[1], reverse=True) \n    ```", "```py\nWhat are the benefits of solar energy? \n```", "```py\nDocument: Solar energy is a renewable source of power., Score: 80.8264\n....\nDocument: Graph databases like Neo4j are used to model complex relationships., Score: 52.8945 \n```", "```py\n    tokenized_corpus = [doc.split() for doc in corpus]\n    # Initialize BM25 with the tokenized corpus\n    bm25 = BM25Okapi(tokenized_corpus, k1=1.5, b=0.75) \n    ```", "```py\n    tokenized_query = query.split() \n    ```", "```py\n    scores = bm25.get_scores(tokenized_query) \n    ```", "```py\n    ranked_docs = sorted(zip(corpus, scores), key=lambda x: x[1], \n        reverse=True) \n    ```", "```py\nquick fox \n```", "```py\nRanked Documents:\nDocument: The quick brown fox jumps over the lazy dog., Score: 0.6049\n.....\nDocument: Artificial intelligence is transforming the world., Score: 0.0000 \n```", "```py\n# Extract passages for the reader\npassages = [doc for doc, score in ranked_docs]\n\n# Prepare inputs for the reader\ninputs = reader_tokenizer(\n    questions=query,\n    titles=[\"Passage\"] * len(passages),\n    texts=passages,\n    return_tensors=\"pt\",\n    padding=True,\n    truncation=True\n)\n# Use the reader to extract the most relevant passage\nwith torch.no_grad():\n    outputs = reader(**inputs)\n# Extract the passage with the highest score\nmax_score_index = torch.argmax(outputs.relevance_logits)\nmost_relevant_passage = passages[max_score_index] \n```", "```py\nWhat are the benefits of solar energy? \n```", "```py\nRanked Documents:\nDocument: Solar energy is a renewable source of power., Score: 80.8264\n.....\nDocument: It has low maintenance costs., Score: 57.9905\n\nMost Relevant Passage: Solar panels help combat climate change and reduce carbon footprint. \n```", "```py\ndef integrate_and_generate(query, retrieved_docs):\n    # Combine query and retrieved documents into a single input\n    input_text = f\"Answer this question based on the following context: {query} Context: {' '.join(retrieved_docs)}\"\n\n    # Tokenize input for T5\n    inputs = t5_tokenizer(input_text, return_tensors=\"pt\", \n        padding=True, truncation=True, max_length=512)\n\n    # Generate a response\n    with torch.no_grad():\n        outputs = t5_model.generate(**inputs, max_length=100)\n\n    # Decode and return the generated response\n    return t5_tokenizer.decode(outputs[0], skip_special_tokens=True) \n```", "```py\nWhat are the benefits of solar energy? \n```", "```py\nRanked Documents:\nDocument: Solar energy is a renewable source of power., Score: 80.8264\n....\nDocument: It has low maintenance costs., Score: 57.9905\n\nMost Relevant Passage: Solar panels help combat climate change and reduce carbon footprint. \nsized response. The generate() function processes the combined input (query and passages) through the encoder to produce contextual embeddings, *ℎ*. These embeddings are then used by the decoder, which generates each token sequentially based on probabilities:\n```", "```py\n    # Load the GitHub issues dataset\n    issues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n\n    # Filter out pull requests and keep only issues with comments\n    issues_dataset = issues_dataset.filter(\n        lambda x: not x[\"is_pull_request\"] and len(x[\"comments\"]) > 0) \n    ```", "```py\n    # Define columns to keep\n    columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n    columns_to_remove = set(issues_dataset.column_names) - \\ \n                        set(columns_to_keep)\n    # Remove unnecessary columns\n    issues_dataset = issues_dataset.remove_columns(columns_to_remove) \n    ```", "```py\n    # Set format to pandas and convert the dataset\n    issues_dataset.set_format(\"pandas\")\n    df = issues_dataset[:] \n    ```", "```py\n    # Explode comments into separate rows\n    comments_df = df.explode(\"comments\", ignore_index=True) \n    # Convert the DataFrame back to a Dataset\n    comments_dataset = Dataset.from_pandas(comments_df)\n\n    # Compute the length of each comment\n    comments_dataset = comments_dataset.map(\n        lambda x: {\"comment_length\": len(x[\"comments\"].split())}, \n        num_proc=1)\n    # Filter out short comments\n    comments_dataset = comments_dataset.filter(\n        lambda x: x[\"comment_length\"] > 15) \n    ```", "```py\n    # Function to concatenate text fields\n    def concatenate_text(examples):\n        return {\n           \"text\": examples[\"title\"] + \" \\n \" + \n                   examples[\"body\"] + \" \\n \" + \n                   examples[\"comments\"]\n        }\n    # Apply the function to create a text field\n    comments_dataset = comments_dataset.map(concatenate_text, \n        num_proc=1) \n    ```", "```py\n    # Load pre-trained model and tokenizer\n    model_ckpt = \"sentence-transformers/all-MiniLM-L6-v2\"\n    tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n    model = AutoModel.from_pretrained(model_ckpt).to(\"cpu\") \n    ```", "```py\n    # Function to get embeddings for a list of texts\n    def get_embeddings(text_list):\n        encoded_input = tokenizer(text_list, padding=True, \n            truncation=True, return_tensors=\"pt\").to(\"cpu\")\n        with torch.no_grad():\n            model_output = model(**encoded_input)\n        return cls_pooling(model_output).numpy() \n    ```", "```py\n    # Compute embeddings for the dataset\n    comments_dataset = comments_dataset.map(\n        lambda batch: {\"embeddings\": [get_embeddings([text])[0] \n            for text in batch[\"text\"]]},\n        batched=True,\n        batch_size=100,\n        num_proc=1\n    ) \n    ```", "```py\n    # Define a query\n    question = \"How can I load a dataset offline?\"\n    # Compute the embedding for the query\n    query_embedding = get_embeddings([question]).reshape(1, -1) \n    # Find the nearest examples\n    embeddings = np.vstack(comments_dataset[\"embeddings\"])\n    similarities = cosine_similarity(\n        query_embedding, embeddings\n    ).flatten()\n    # Display the results\n    top_indices = np.argsort(similarities)[::-1][:5]\n    for idx in top_indices:\n        result = comments_dataset[int(idx)]  # Convert NumPy integer to native Python integer\n        print(f\"COMMENT: {result['comments']}\")\n        print(f\"SCORE: {similarities[idx]}\")\n        print(f\"TITLE: {result['title']}\")\n        print(f\"URL: {result['html_url']}\")\n        print(\"=\" * 50) \n    ```", "```py\nHow can I load a dataset offline?. \n```", "```py\nCOMMENT: Yes currently you need an internet connection because the lib tries to check for the etag of the dataset script ...\nSCORE: 0.9054292969045314\nTITLE: Downloaded datasets are not usable offline\nURL: https://github.com/huggingface/datasets/issues/761\n==================================================\nCOMMENT: Requiring online connection is a deal breaker in some cases ...\nSCORE: 0.9052456782359709\nTITLE: Discussion using datasets in offline mode\nURL: https://github.com/huggingface/datasets/issues/824\n================================================== \n```"]