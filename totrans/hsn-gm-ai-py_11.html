<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Optimizing for Continuous Control</h1>
                </header>
            
            <article>
                
<p>Up until now, we have considered most of the training/challenge environments we've looked at as being episodic; that is, the game or environment has a beginning and an end. This is good since most games have a beginning and an end <span>–</span> it is, after all, a game. However, in the real world, or for some games, an episode could last days, weeks, months, or even years. For these types of environment, we no longer think of an episode; rather we work with the concept of an environment that requires continuous control. So far, we have looked at a subset of algorithms that can solve this type of problem but they don't do so very well. So, like most things in RL, we have a special class of algorithms devoted to those types of environment, and we'll explore them in this chapter.</p>
<p>In this chapter, we'll look at improving the policy methods we looked at previously for performing continuous control of advanced environments. We'll start off by setting up and installing the Mujoco environment, a specialized area we can use to test these new algorithms, the first of which will be the proximal policy optimization or PPO method. After that, we'll look at a novel improvement called recurrent networks for capturing context and learn how that is applied on top of PPO. Then, we'll get back into actor-critic and this time look at asynchronous actor-critic in a couple of different configurations. Finally, we'll look at ACER and actor-critic with experience replay.</p>
<p>Here is a summary of the main topics we will cover in this chapter:</p>
<ul>
<li>Understanding continuous control with Mujoco</li>
<li>Introducing proximal policy optimization</li>
<li>Using PPO with recurrent networks</li>
<li>Deciding on synchronous and asynchronous actors</li>
<li>Building actor-critic with experience replay</li>
</ul>
<p class="mce-root"/>
<p>In this chapter, we'll look at a class of RL methods that attempts to deal specifically with real-world problems of robotics or other control systems. Of course, this doesn't mean these same algorithms couldn't be used in gaming <span>– </span>they are. In the next section, we'll begin by looking at the specialized Mujoco environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding continuous control with Mujoco</h1>
                </header>
            
            <article>
                
<p>The standard environment for building continuous control agents is the Mujoco environment. <strong>Mujoco</strong> stands for <strong>Multi-Joint dynamics with Contract</strong> and it is a full physics environment for training robotic or simulation agents. This environment provides a number of simulations that challenge some form of robotic control agent to perform a task, such as walking, crawling, and implementing several other physics control-based tasks. An example of the diversity of these environments is summarized well in the following image, which has been extracted from the Mujoco home page:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-626 image-border" src="assets/48271473-8094-4df7-b3fd-fccd2982a72d.png" style="width:152.25em;height:46.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Extract of example environments from the Mujoco home page</div>
<p>Obviously, we will want to use this cool environment. However, this package is not free and requires a license, but note that a 30-day trial is provided. Now for the bad news. The package is extremely difficult to set up, install, and train, especially if you are using Windows. In fact, it is so difficult that, although we strongly suggest using Mujoco as an environment, we won't be using it for the remaining exercises in this chapter. Why? Again, it is extremely difficult and we don't want to exclude people who are unable to install the Mujoco environment.  </p>
<p class="mce-root"/>
<div class="packt_tip">There are plenty of blog posts or Stack Overflow articles available that walk through various installations of the various versions of Mujoco for Windows. Mujoco's support for Windows was stopped after version 1.5. While it is still possible to install Mujoco on Windows, it is not trivial and is likely to change often. As such, if you are inclined to use Windows with Mujoco, your best bet is to look to the most recent blogs or forum posts discussing this for help.</div>
<p><span>In this exercise, we'll walk through the basic installation of Mujoco (not for Windows):</span></p>
<ol>
<li>The first thing we need is a license. Open your browser, go to <a href="http://mujoco.org">mujoco.org</a>, and locate the <span class="packt_screen">License</span> button at the top of the page. Then, click it.</li>
<li>On the page, you will see an entry for <strong><span class="packt_screen">Computer id</span></strong>. This will require you to download a key generator from the blue links shown to the right. Click one of the links to download the key generator.</li>
<li>Run the key generator on your system and enter the key in the <span class="packt_screen"><strong>Computer i</strong><strong>d</strong></span> field.</li>
<li>Fill in the rest of the license information with your name and email and click <strong>Submit</strong>, as shown in the following screenshot:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-627 image-border" src="assets/674b587e-a965-42ca-ad66-7f09e85416fa.png" style="width:65.92em;height:20.50em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span>Submitting for a Mujoco license</span></div>
<ol start="5">
<li>You should get a key emailed to you in a few minutes with directions as to where to put the key. Then, you need to download the binaries for your platform. Click on the <span class="packt_screen">Products</span> link at the top of the page to be taken to the downloads. Download the version you need for your OS.</li>
</ol>
<ol start="6">
<li>Unzip the files into your root user folder, <kbd>~/.mujoco/mujoco%version%</kbd>, where <kbd>%version%</kbd> denotes the version of the software. On Windows, your user folder is <kbd>C:\Users\%username%</kbd>, where <kbd>%username%</kbd> denotes the logged in user's name.</li>
<li>Now, you need to build the Mujoco package and set up the <kbd>mujoco-py</kbd> scripts. This varies widely by installation. Use the following commands to build and install Mujoco:</li>
</ol>
<pre style="padding-left: 60px"><strong>pip3 install -U 'mujoco-py&lt;2.1,&gt;=2.0'  #use the version appropriate for you</strong><br/><strong>cd path/to/mujoco-py/folder</strong><br/><span><strong>python -c "import mujoco_py" #force compile mujoco_py</strong><br/><strong>python setup.py install</strong> </span></pre>
<ol start="8">
<li>To test the installation and check for dependencies, run the following command to reinstall the entire Gym again:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>pip install gym[all]</span></strong></pre>
<div class="packt_infobox">If you run this command and still see errors, you likely need more help. Consult online resources for the most current search on <kbd>mujoco install</kbd> and try those instructions. Again, at the time of writing, Windows is no longer supported and you may be better off using another platform. Fortunately, setting up a VM or Cloud service for this can now be quite easy and you may have more luck there.</div>
<ol start="9">
<li>You can test the Mujoco installation and ensure that the license is all set up by running <kbd>Chapter_9_Mujoco.py</kbd> as you normally would. The listing is shown here:</li>
</ol>
<pre style="padding-left: 60px">import gym <br/>from gym import envs<br/><br/>env = gym.make('FetchReach-v1')<br/><br/>env.reset()<br/>for _ in range(1000):<br/> env.render()<br/> env.step(env.action_space.sample()) # take a random action<br/>env.close()</pre>
<p>If you have everything installed correctly, then you should see something similar to the following image, which has been taken from the Mujoco environment:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-865 image-border" src="assets/ab253b9c-0b6d-47d0-8e1d-4f1a38b3ea18.png" style="width:50.25em;height:38.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The fetch reach Mujoco environment</div>
<p>If you are able to install the Mujoco environments, then great <span>–</span> have fun exploring a whole new world of environments. For those of you who are not able to install Mujoco, don't fret. We will learn how to create our own physics-based environments when we start using Unity in <a href="ab9a7f4f-60d8-4643-8627-199cf95bcf55.xhtml">Chapter 10</a>, <em>Exploiting ML-Agents</em>. Rest assured that, while Mujoco is indeed cool, much like the Atari games we have seen before, it is also not trivial to train. Not unlike Atari, Mujoco environments can take millions of training iterations. Therefore, to keep things simple and also remain energy-conscious, we will use the regular old Gym environments. The additional plus now is that we have a better comparison between various algorithms across a single environment.  </p>
<p class="mce-root"/>
<div class="packt_infobox"><strong>Deep reinforcement learning</strong> (<strong>DRL</strong>) and <strong>machine learning</strong> (<strong>ML</strong>) in general are getting a bit of a bad name due to the additional energy they consume. Many state-of-the-art DRL models can be measured in terms of energy consumption and, in most cases, the amount of energy is quite high. In one case, DeepMind has admitted that the amount of processing/energy it used to train a single model would run a single desktop computer for 45 years. That is an incredible amount of energy in a world that needs to be cautious about energy consumption. Therefore, wherever applicable, in this book we will favor cheaper training environments. </div>
<p class="mce-root"> In<span> </span><span>the next section, we look at advancing these policy methods with gradient optimization.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing proximal policy optimization</h1>
                </header>
            
            <article>
                
<p>We are now entering areas where we will start looking at state-of-the-art algorithms, at least at the time of writing. Of course, that will likely change and things will advance. For now, though, the <strong>proximal policy optimization</strong> algorithm (<strong>PPO</strong>), was introduced by OpenAI, is considered a state-of-the-art deep reinforcement learning algorithm. As such, the sky is the limit as to what environments we can throw at this problem. However, in order to quantify our progress and for a variety of other reasons, we will continue to baseline against the Lunar Lander environment. </p>
<p>The PPO algorithm is just an extension and simplification of the <strong>trust region policy optimization</strong> (<strong>TRPO</strong>) algorithm we covered in <a href="42626cbd-87b8-428c-8f2a-ecc06f5e387c.xhtml">Chapter 8</a>, <em>Policy Gradient Methods</em>, but with a few key differences. PPO is also much simpler to understand and follow. For these reasons, we will review each feature that makes policy optimization with trust regions in the case of TRPO and clipping with PPO so powerful.</p>
<div class="packt_infobox">The code for this chapter was originally sourced from the following repository: <a href="https://github.com/seungeunrho/minimalRL">https://github.com/seungeunrho/minimalRL</a>. A number of modifications have been made to the code so that it fits the examples in this book.</div>
<p><span>Since we have already gone over the major intuition behind this improvement, let's jump into the next coding exercise by opening </span><kbd>Chapter_9_PPO.py</kbd><span>. Perform the following steps:</span></p>
<ol>
<li>The code for this listing is quite similar to many of the other listings we have reviewed. As such, we will limit our review to critical sections:</li>
</ol>
<pre style="padding-left: 60px">for iteration in range(iterations):<br/> s = env.reset()<br/> done = False<br/> while not done:<br/>   <strong>for t in range(T_horizon):</strong><br/>     prob = model.pi(torch.from_numpy(s).float()) <br/>     m = Categorical(prob)<br/>     a = m.sample().item()<br/>     s_prime, r, done, info = env.step(a)<br/><br/>     model.put_data((s, a, r/100.0, s_prime, prob[a].item(),done))<br/>     s = s_prime<br/><br/>     score += r<br/>     if done:<br/>       if score/print_interval &gt; min_play_reward:<br/>         play_game()<br/>       break<br/><br/>   model.train_net()<br/> if iteration%print_interval==0 and iteration!=0:<br/>   print("# of episode :{}, avg score : {:.1f}".format(iteration,<br/>     score/print_interval))<br/>   score = 0.0<br/><br/>env.close()</pre>
<ol start="2">
<li>Scrolling right to the bottom, we can see that the training code is almost identical to our most recent examples in the previous chapters. One key thing to notice is the inclusion of a new hyperparameter, <kbd>T_horizon</kbd>, which we will define shortly:</li>
</ol>
<pre style="padding-left: 60px">learning_rate = 0.0005<br/>gamma = 0.98<br/>lmbda = 0.95<br/>eps_clip = 0.1<br/>K_epoch = 3<br/>T_horizon = 20</pre>
<ol start="3">
<li>If we scroll back to the top, you will see the definition of new hyperparameters for <kbd>T_horizon</kbd>, <kbd>K_epoch</kbd>, <kbd>eps_clip</kbd>, and <kbd>lambda</kbd>. Just note these new variables for now <span>–</span> we will get to their purpose shortly.</li>
<li>Let's jump to some of the other important differences, such as the network definition, which can be seen in the <kbd>init</kbd> method of the <kbd>PPO</kbd> class as follows:</li>
</ol>
<pre style="padding-left: 60px">def __init__(self, input_shape, num_actions):<br/> super(PPO, self).__init__()<br/> self.data = []<br/><br/> self.fc1 = nn.Linear(input_shape,256)<br/> self.fc_pi = nn.Linear(256,num_actions)<br/> self.fc_v = nn.Linear(256,1)<br/> self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)</pre>
<ol start="5">
<li>What we can see is that the network comprises a first input state <kbd>Linear</kbd> layer called <kbd>fc1</kbd> that is composed of 256 neurons. Then, we can see that the <kbd>fc_pi</kbd> or policy network is defined as <kbd>Linear</kbd> with 256 neurons and outputs the <kbd>num_actions</kbd> or number of actions. Following that is the definition of <kbd>fc_v</kbd>, which is the value layer. Again, this has 256 neurons and one output, that is, the expected value.</li>
<li>The rest of the code for the PPO class is almost the same as in the previous examples and we won't need to cover it here.  </li>
<li>Run the code as normal. This example will take a while to run but not as long as previous versions. We'll leave it up to you whether you want to wait for the example to complete before continuing.</li>
</ol>
<p>One thing you should quickly notice is how much faster the algorithm trains. Indeed, the agent gets good quite fast and could actually solve the environment in fewer than 10,000 iterations, which is quite impressive. Now that we have seen how impressive policy optimization can be, we will look at how this is possible in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The hows of policy optimization</h1>
                </header>
            
            <article>
                
<p>In <a href="42626cbd-87b8-428c-8f2a-ecc06f5e387c.xhtml"/><a href="42626cbd-87b8-428c-8f2a-ecc06f5e387c.xhtml">Chapter 8</a>, <em>Policy Gradient Methods</em>, we covered how policy gradient methods can fail and then introduced the TRPO method. Here, we talked about the general strategies TRPO uses to address the failings in PG methods. However, as we have seen, TRPO is quite complex and seeing how it works in code was not much help either. This is the main reason we minimized our discussion of the details when we introduced TRPO and instead waited until we got to this section to tell</p>
<p>the full story in a concise manner. </p>
<p>That said, let's review how policy optimization with TRPO or PPO can do what it does:</p>
<ul>
<li>
<p class="hh hi ar by bx hj hk hl hm hn ho hp hq hr hs ht hu"><strong>Minorize-Maximization MM algorithm</strong>: Recall that this is where we find the minimum of an upper bound function by finding the maximum of a lower bound function that is constrained to be within the upper bound function.  </p>
</li>
<li>
<p class="hh hi ar by bx hj hk hl hm hn ho hp hq hr hs ht hu"><span><strong>Line search</strong>: We have se</span><span>en this being used to define in which direction and by what amount we could optimize our function (deep learning network). This allows our algorithm to avoid overshooting the target of optimization.</span></p>
</li>
</ul>
<ul>
<li><strong>Trust region</strong>: Along with MM and Line Search, we also want the policy function to have a stable base or platform to move along on. You can think of this stable base as a region of trust or safety. In PPO, this is defined differently, as we will see.</li>
</ul>
<p>PPO and TRPO share all these improvements as a way of finding a better policy. PPO improves on this by also understanding how much we want to change the policy's distribution over each iteration. This understanding also allows us to limit the amount of change during each iteration. We have seen how TRPO does this to a certain extent with KL divergence, but PPO takes this one step further by adjusting or adapting to the amount of change. In the next section, we'll look at how this adaptation works.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">PPO and clipped objectives</h1>
                </header>
            
            <article>
                
<p>Before we get into the finer details of how PPO works, we need to step back and understand how we equate the difference in distributed data distributions or just distributions. Remember that PG methods look to understand the returns-based sampling distribution and then use that to find the optimum action or the probability of the optimum action. Due to this, we can use a method called <strong>KL Divergence</strong> to determine how different the two distributions are. By understanding this, we can determine how much room or area of trust we can allow our optimization algorithm to explore with. PPO improves on this by clipping the objective function by using two policy networks.  </p>
<div class="packt_tip">Jonathan Hui has a number of insightful blog posts on the mathematics behind various RL and PG methods. In particular, his post on PPO (<a href="https://medium.com/@jonathan_hui/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12is">https://medium.com/@jonathan_hui/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12</a>) is quite good. Be warned that they do assume a very sophisticated level of mathematics knowledge. If you are serious about RL, you will want to be able to read and understand this content at some point. However, you can get quite far in DRL by intuitively understanding most algorithms, like we're doing here.  </div>
<p class="mce-root"/>
<p><span>Let's learn how this works in code by opening up </span><kbd>Chapter_9_PPO.py</kbd><span> and performing the following steps:</span></p>
<ol>
<li>Having looked through the bulk of the main code, we only want to focus on the training code here and, in particular, the <kbd>train_net</kbd> function from the <kbd>PPO</kbd> class, as shown here:</li>
</ol>
<pre style="padding-left: 60px">def train_net(self):<br/> s, a, r, s_prime, done_mask, prob_a = self.make_batch()<br/><br/>for i in range(K_epoch):<br/> td_target = r + gamma * self.v(s_prime) * done_mask<br/> delta = td_target - self.v(s)<br/> delta = delta.detach().numpy()<br/><br/> advantage_lst = []<br/> advantage = 0.0<br/> for delta_t in delta[::-1]:<br/>   advantage = gamma * lmbda * advantage + delta_t[0]<br/>   advantage_lst.append([advantage])<br/> advantage_lst.reverse()<br/> advantage = torch.tensor(advantage_lst, dtype=torch.float)<br/><br/> pi = self.pi(s, softmax_dim=1)<br/> pi_a = pi.gather(1,a)<br/> ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a)) <br/><br/> surr1 = ratio * advantage<br/> surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage<br/> loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s) , <br/>   td_target.detach())<br/><br/> self.optimizer.zero_grad()<br/> loss.mean().backward()<br/> self.optimizer.step()</pre>
<ol start="2">
<li>After the initial <kbd>make_batch</kbd> function call, in order to build the lists, we come to the iteration loop controlled by <kbd>K_epoch</kbd>. <kbd>K_epoch</kbd> is a new hyperparameter that controls the number of iterations we use to optimize the advantage convergence:</li>
</ol>
<pre style="padding-left: 60px">td_target = r + gamma * self.v(s_prime) * done_mask<br/>delta = td_target - self.v(s)<br/>delta = delta.detach().numpy()</pre>
<ol start="3">
<li>The first block of code inside the <kbd>K_epoch</kbd> iteration is the calculation of <kbd>td_target</kbd> using the reward <kbd>r</kbd>, plus the discount factor <kbd>gamma</kbd> times the output of the v or value network and <kbd>done_mask</kbd>. Then, we take the <kbd>delta</kbd> or TD change and convert it into a <kbd>numpy</kbd> tensor:</li>
</ol>
<pre style="padding-left: 60px">for delta_t in delta[::-1]:<br/>  <strong>advantage = gamma * lmbda * advantage + delta_t[0]</strong><br/>  advantage_lst.append([advantage])</pre>
<ol start="4">
<li>Next, using the delta, we build a list of advantages using the <kbd>advantage</kbd> function, as follows:</li>
</ol>
<pre style="padding-left: 60px">pi = self.pi(s, softmax_dim=1)<br/>pi_a = pi.gather(1,a)<br/>ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a)) </pre>
<ol start="5">
<li>Then, we push the state <strong>s</strong> into the policy network, <kbd>pi</kbd>. Next, we gather the axis along the first dimension and then take the ratio using the equation <img class="fm-editor-equation" src="assets/4c9bd681-0b83-473f-939e-9f9ec792ad3c.png" style="width:11.00em;height:1.75em;"/>, which is used to calculate a possible ratio for the clipping region or area we want to use for trust:</li>
</ol>
<pre style="padding-left: 60px">surr1 = ratio * advantage<br/>surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage<br/>loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s) , <br/>   td_target.detach())</pre>
<ol start="6">
<li>We use the <kbd>ratio</kbd> value to calculate the <kbd>surr1</kbd> value, which defines the surface or clipping region. The next line calculates a second version of the surface, <kbd>surr2</kbd>, by clamping this ratio and using the bounds of our clipping region set by <kbd>eps_clip</kbd> to define the area. Then, it takes the minimum area of either surface and uses that to calculate the loss.</li>
</ol>
<div class="packt_infobox">We use the term <kbd>surface</kbd> here to understand that the calculation of loss is over a multi-dimensional array of values. Our optimizer works across this surface to find the best global minimum or lowest area on the surface.</div>
<ol start="7">
<li>The last section of code is our typical gradient descent optimization and is shown next for completeness:</li>
</ol>
<pre style="padding-left: 60px">self.optimizer.zero_grad()<br/>loss.mean().backward()<br/>self.optimizer.step()</pre>
<ol start="8">
<li>There's nothing new here. Go ahead and run the sample again or review the output from the previous exercise. An example of the training output is shown here:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-629 image-border" src="assets/801102d6-d763-47f2-a7c2-2562ab1d850d.png" style="width:43.83em;height:42.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Example output from Chapter_9_PPO.py</div>
<p>A couple of things to note is that we are stilling using discrete action spaces and not continuous ones. Again, our primary reason for doing this is to continue using a baseline-consistent environment, Lunar Lander v2. Lunar Lander does have a continuous action environment that you can try, but you will need to convert the sample so that you can use continuous actions. On the second item of note, PPO and other PG methods actually perform better on continuous action environments, which means you aren't really seeing their full potential. So, why are we continuing to use discrete action spaces? Well, in almost all cases, games and interactive environments will use discrete spaces. Since this is a book on games and AI and not robotics and AI, we will stick to discrete spaces.</p>
<p class="mce-root">There are various research initiatives on other PG methods but you should consider PPO to be a milestone in DRL, not unlike DQN. For those of you who are curious, PPO made its name by beating human players in the DOTA2 strategy game. In the next section, we'll look at other methods that have been layered on top of PG and other methods to improve DRL.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using PPO with recurrent networks</h1>
                </header>
            
            <article>
                
<p>In <a href="42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml">Chapter 7</a>, <em>Going Deeper with DDQN</em>, we saw how we could interpret visual state using a concept called <strong>convolutional neural networks</strong> (<strong>CNNs</strong>). CNN networks are used to detect features in visual environments such as Atari games. While this technique allowed us to play any of a number of games with the same agent, the added CNN layers took much more time to train. In the end, the extra training time wasn't worth the cool factor of playing Atari games. However, there are other network structures we can put on top of our networks in order to make better interpretations of state. One such network structure is called recurrent networks. Recurrent network layers allow us to add the concept of context or time in our model's interpretation of state. This can work very well in any problem where context or memory is important.</p>
<p>Recurrent network layers are a form of deep learning perceptron that essentially feeds back its state to previous neurons. This, in effect, gives the network the ability to understand time or context. It does this so well that recurrent networks are now at the heart of all text and language processing networks. Language is especially contextual and recurrent layers, in various configurations, make short work of understanding the context. There are various configurations of recurrent network layers but the only one we'll focus on here is called <strong>long short term memory</strong> (<strong>LSTM</strong>). </p>
<p class="mce-root"/>
<div class="packt_infobox">Recurrent networks and LSTM layers are worthy of in-depth study on their own. These powerful network layers have been responsible for some very interesting discoveries in the last few years. While recurrent layers have turned out to have limited use in DRL, it is believed they should have more. After all, understanding context in a trajectory must be important.</div>
<p>LSTM layers for the purposes of DRL are quite simple to put in place. Open <kbd>Chapter_9_PPO_LSTM.py</kbd> and follow these steps to see how this works:</p>
<ol>
<li>This sample is virtually identical to <kbd>Chapter_9_PPO.py</kbd> but with the few key differences, all of which we will look at here.</li>
<li>Skip to the <kbd>PPO</kbd> class definition, as shown here:</li>
</ol>
<pre style="padding-left: 60px">class PPO(nn.Module):<br/>    def __init__(self, input_shape, num_actions):<br/>        super(PPO, self).__init__()<br/>        self.data = []<br/>        <br/>        self.fc1 = nn.Linear(input_shape,64)<br/>        <strong>self.lstm = nn.LSTM(64,32)</strong><br/>        self.fc_pi = nn.Linear(32,num_actions)<br/>        self.fc_v = nn.Linear(32,1)<br/>        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)</pre>
<ol start="3">
<li>The only new part here is the definition of a new layer, <kbd>lstm</kbd>, which is of the <kbd>LSTM(64.32)</kbd> type. The LSTM layer that's injected at the top of the state encoding allows the network to learn the context in actions or memory. Now, instead of understanding which state-actions provide the best trajectory, our agent is learning which state-action sets are providing the best outcome. In gaming, this may be analogous to learning that a special move unlocks a sequence to get some special reward.</li>
<li>Next, we will move down to the policy pi function and value v function network definitions and look at how it has been modified:</li>
</ol>
<pre style="padding-left: 60px">def pi(self, x, hidden):<br/>        x = F.relu(self.fc1(x))<br/>        x = x.view(-1, 1, 64)<br/>       <strong> x, lstm_hidden = self.lstm(x, hidden)</strong><br/>        x = self.fc_pi(x)<br/>        prob = F.softmax(x, dim=2)<br/>        return prob, lstm_hidden<br/><br/>def v(self, x, hidden):<br/>        x = F.relu(self.fc1(x))<br/>        x = x.view(-1, 1, 64)<br/>        <strong>x, lstm_hidden = self.lstm(x, hidden)</strong><br/>        v = self.fc_v(x)<br/>        return v</pre>
<ol start="5">
<li>The <kbd>pi</kbd> and <kbd>v</kbd> functions take a hidden layer but only <kbd>pi</kbd>, or the policy function, is used as the output of a hidden layer. We will see how these hidden LSTM layers work shortly.</li>
<li>Then, at the top of the <kbd>train_net</kbd> function, we can see where the layers are extracted from the batching process, <kbd>make_batch</kbd>:</li>
</ol>
<pre style="padding-left: 60px">def train_net(self):<br/>        s,a,r,s_prime,done_mask, prob_a, (h1_in, h2_in), (h1_out, h2_out) = self.make_batch()<br/>        first_hidden = (h1_in.detach(), h2_in.detach())<br/>        second_hidden = (h1_out.detach(), h2_out.detach())</pre>
<ol start="7">
<li>We use two hidden or middle LSTM layers between our actor-critics, where <kbd>second_hidden</kbd> denotes the output and <kbd>first_hidden</kbd> denotes the input. Below that, in the <kbd>for</kbd> loop, we can see the calculation of delta using the LSTM input and output:</li>
</ol>
<pre style="padding-left: 60px">v_prime = self.v(s_prime, second_hidden).squeeze(1)<br/>td_target = r + gamma * v_prime * done_mask<br/>v_s = self.v(s, first_hidden).squeeze(1)<br/>delta = td_target - v_s<br/>delta = delta.detach().numpy()</pre>
<ol start="8">
<li>The calculation of delta here is done by applying the difference between the before and after the LSTM layer was applied, which allows the delta to encapsulate the effect the LSTM has on the calculation of value <kbd>v</kbd>.</li>
</ol>
<ol start="9">
<li>Run the sample as you normally would and observe the output, as shown here:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-630 image-border" src="assets/87ea8d62-2ae5-4df4-9978-d56fcbbfc35c.png" style="width:39.67em;height:37.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Example output of Chapter_9_PPO_LSTM.py</div>
<p>Notice how this slight improvement increased training performance significantly from the vanilla PPO example we looked at in the previous exercise. In the next section, we'll improve PPO further by applying parallel environments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deciding on synchronous and asynchronous actors</h1>
                </header>
            
            <article>
                
<p>We started off this book with a simple discussion of what <strong>artificial general intelligence</strong> (<strong>AGI</strong>) is. In short, AGI is our attempt at generalizing an intelligent system to solve multiple tasks. RL is often thought of as a stepping stool to AGI primarily because it tries to generalize state-based learning. While both RL and AGI take deep inspiration from how we think, be it rewards or possibly consciousness itself, the former tends to incorporate direct analogies. The actor-critic concept in RL is an excellent example of how we use an interpretation of human psychology to create a form of learning. For instance, we humans often consider the consequences of our actions and determine the advantages they may or may not give us. This example is perfectly analogous to actor-critic and advantage methods we use in RL. Take this further and we can consider another human thought process: asynchronous and synchronous thought.</p>
<p>A direct example of asynchronous/synchronous thought is when an answer to a problem <strong>pops</strong> into your head after being asked a question several hours earlier. Perhaps you didn't have the answer then but then it came to you a few hours later. Were you thinking about the question all that time? Not likely, and more than likely the answer just came to you <span>–</span> it popped into your head. But were you thinking of it all the time in some background process or did some process just fire up and provide the answer? The animal brain thinks like this all the time and it is what we would call in computerese parallel processing. So, could our agents not also benefit from this same thought process? As it turns out, yes.</p>
<p>This inspiration likely came in part from the preceding analogy but also has a mathematical background in how we can evaluate advantage. The direct evaluation of how our brains think is still a big answer but we can assume our thoughts to be synchronous or asynchronous. So, instead of just considering one thought process, what if we consider several? We can take this analogy a step further and apply this back to DRL <span>–</span> in particular, actor-critic. Here, we have a single thought process or global network that is fed the output of several worker thought processes. An example of this is shown here:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-687 image-border" src="assets/eeb6bb78-7555-424c-8ed4-da6407ecc1ab.png" style="width:31.67em;height:30.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Example of asynchronous AC by Dr. Arthur Juliani</div>
<p>What we can see here is the basic intuition behind the advantage of the actor-critic architecture, A2C, and the asynchronous advantage actor-critic architecture, A3C. Notice how each worker brain/agent has its own copy of a separate environment. All of these worker agents feed their learning into a master brain. Each worker brain is then updated iteratively to coincide with the master, not unlike our earlier advantage calculations. In the next section, we will see how this is put into practice by implementing A2C.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using A2C</h1>
                </header>
            
            <article>
                
<p>To avoid any confusion, it is important to understand that A2C and A3C both use AC, but it is the fashion in which they update their models that differ. In A2C, the method is synchronous, so each brain is feeding thoughts into the main brain. </p>
<p>Let's see how this looks in code by opening the <kbd>Chapter_9_A2C.py</kbd> file and reviewing the hyperparameters inside it:</p>
<pre>n_train_processes = 3<br/>learning_rate = 0.0002<br/>update_interval = 5<br/>gamma = 0.98<br/>max_train_steps = 60000<br/>PRINT_INTERVAL = update_interval * 100<br/>environment = "LunarLander-v2"</pre>
<p>Keep the sample open and follow these steps to continue with this exercise:</p>
<ol>
<li>This is a large code example, so we will limit the sections we show here. The main thing of note here is the hyperparameters that are listed at the top of the file. The only thing new to note is <kbd>n_train_processes</kbd>, which sets the number of worker processes:</li>
</ol>
<pre style="padding-left: 60px">class ActorCritic(nn.Module):<br/>    def __init__(self, input_shape, num_actions):<br/>        super(ActorCritic, self).__init__()<br/>        self.fc1 = nn.Linear(input_shape, 256)<br/>        self.fc_pi = nn.Linear(256, num_actions)<br/>        self.fc_v = nn.Linear(256, 1)<br/><br/>    def pi(self, x, softmax_dim=1):<br/>        x = F.relu(self.fc1(x))<br/>        x = self.fc_pi(x)<br/>        prob = F.softmax(x, dim=softmax_dim)<br/>        return prob<br/><br/>    def v(self, x):<br/>        x = F.relu(self.fc1(x))<br/>        v = self.fc_v(x)<br/>        return v</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li>Next comes the <kbd>ActorCritic</kbd> class, which is the same class that we used previously:</li>
</ol>
<pre style="padding-left: 60px">def worker(worker_id, master_end, worker_end):<br/>    master_end.close() <br/>    env = gym.make(environment)<br/>    env.seed(worker_id)<br/><br/>    while True:<br/>        cmd, data = worker_end.recv()<br/>        if cmd == 'step':<br/>            ob, reward, done, info = env.step(data)<br/>            if done:<br/>                ob = env.reset()<br/>            worker_end.send((ob, reward, done, info))<br/>        elif cmd == 'reset':<br/>            ob = env.reset()<br/>            worker_end.send(ob)<br/>        elif cmd == 'reset_task':<br/>            ob = env.reset_task()<br/>            worker_end.send(ob)<br/>        elif cmd == 'close':<br/>            worker_end.close()<br/>            break<br/>        elif cmd == 'get_spaces':<br/>            worker_end.send((env.observation_space, env.action_space))<br/>        else:<br/>            raise NotImplementedError</pre>
<ol start="3">
<li>Then comes the definition of the <kbd>worker</kbd> function. This function is where the worker's brain sends messages between the worker and master brains:</li>
</ol>
<pre style="padding-left: 60px">class ParallelEnv:<br/>    def __init__(self, n_train_processes):<br/>        self.nenvs = n_train_processes<br/>        self.waiting = False<br/>        self.closed = False<br/>        self.workers = list()<br/><br/>        master_ends, worker_ends = zip(*[mp.Pipe() for _ in range(self.nenvs)])<br/>        self.master_ends, self.worker_ends = master_ends, worker_ends<br/><br/>        for worker_id, (master_end, worker_end) in enumerate(zip(master_ends, worker_ends)):<br/>          p = mp.Process(target=worker,<br/>                           args=(worker_id, master_end, worker_end))<br/>            p.daemon = True<br/>            p.start()<br/>            self.workers.append(p)<br/><br/>        # Forbid master to use the worker end for messaging<br/>        for worker_end in worker_ends:<br/>            worker_end.close()</pre>
<ol start="4">
<li>After those functions is the big <kbd>ParallelEnv</kbd> class. The preceding code just shows the <kbd>init</kbd> function from that class since it's quite large. This class merely coordinates activities between the masters and workers:</li>
</ol>
<pre style="padding-left: 60px">def compute_target(v_final, r_lst, mask_lst):<br/>    G = v_final.reshape(-1)<br/>    td_target = list()<br/><br/>    for r, mask in zip(r_lst[::-1], mask_lst[::-1]):<br/>        G = r + gamma * G * mask<br/>        td_target.append(G)<br/><br/>    return torch.tensor(td_target[::-1]).float()</pre>
<ol start="5">
<li>Scrolling down past the <kbd>test</kbd> function, or the <kbd>play_game</kbd> function in our other examples, we can see the <kbd>compute_target</kbd> function. This is the calculation of the TD loss and the difference here is the use of the <kbd>mask</kbd> variable. <kbd>mask</kbd> is just a flag or filter that removes any calculation of discounted G on 0 returns:</li>
</ol>
<pre style="padding-left: 60px">if __name__ == '__main__':<br/>    envs = ParallelEnv(n_train_processes)<br/>    env = gym.make(environment)<br/>    state_size = env.observation_space.shape[0]<br/>    action_size = env.action_space.n<br/>    model = ActorCritic(state_size, action_size)<br/>    optimizer = optim.Adam(model.parameters(), lr=learning_rate)</pre>
<ol start="6">
<li>After that, we get into an <kbd>if</kbd> function, which determines whether the current process is <kbd>'__main__'</kbd>. We do this to avoid having additional worker processes trying to run this same block of code. After that, we can see the typical environment and model setup being completed:</li>
</ol>
<pre style="padding-left: 60px">for _ in range(update_interval):<br/>    prob = model.pi(torch.from_numpy(s).float())<br/>    a = Categorical(prob).sample().numpy()<br/>    s_prime, r, done, info = envs.step(a)<br/><br/>    s_lst.append(s)<br/>    a_lst.append(a)<br/>    r_lst.append(r/100.0)<br/>    mask_lst.append(1 - done)<br/><br/>    s = s_prime<br/>    step_idx += 1</pre>
<ol start="7">
<li>The interval training loop code is virtually identical to those from the previous examples and should be self-explanatory at this point, for the most part. Something to note is the <kbd>env.steps</kbd> function call. This represents a step in all the worker environments, synchronously. Remember that the worker agents are running synchronously in A2C:</li>
</ol>
<pre style="padding-left: 60px">s_final = torch.from_numpy(s_prime).float()<br/>v_final = model.v(s_final).detach().clone().numpy()<br/>td_target = compute_target(v_final, r_lst, mask_lst)<br/><br/>td_target_vec = td_target.reshape(-1)<br/>s_vec = torch.tensor(s_lst).float().reshape(-1, state_size) <br/>a_vec = torch.tensor(a_lst).reshape(-1).unsqueeze(1)<br/>mod = model.v(s_vec)<br/>advantage = td_target_vec - mod.reshape(-1)<br/><br/>pi = model.pi(s_vec, softmax_dim=1)<br/>pi_a = pi.gather(1, a_vec).reshape(-1)<br/>loss = -(torch.log(pi_a) * advantage.detach()).mean() +\<br/>            F.smooth_l1_loss(model.v(s_vec).reshape(-1), td_target_vec)<br/><br/>optimizer.zero_grad()<br/>loss.backward()<br/>optimizer.step()</pre>
<ol start="8">
<li>Then, we come to the outer training loop. In this example, we can see how the training targets are pulled from the lists constructed by the workers, where <kbd>s_lst</kbd> is for the state, <kbd>a_lst</kbd> is for actions, <kbd>r_lst</kbd> is for rewards, and <kbd>mask_lst</kbd> is for done. Aside from torch tensor manipulation, the calculations are the same as in PPO.  </li>
</ol>
<ol start="9">
<li>Run the code as you normally would and visualize the output, an example of which is as follows:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-866 image-border" src="assets/5d2c9c6d-7807-41c4-8045-167b37dff337.png" style="width:37.25em;height:43.42em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Example output from Chapter_9_A2C.py</div>
<p>You will need to tune the hyperparameters to get this example to run perfectly. Now, we will move on and look at the asynchronous version of A2C—A3C.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using A3C</h1>
                </header>
            
            <article>
                
<p>Synchronous actor-critic workers provide a training advantage by essentially providing more sampling variations that should, in turn, reduce the amount of expected error and thus improve training performance. Mathematically, all we are doing is providing a larger sampling space which, as any statistician will tell you, just reduces the sampling error. However, if we assume that each worker is asynchronous, meaning it updates the global model in its own time, this also provides us with more statistical variability in our sampling across the entire trajectory space. This can also happen along the sampling space at the same time. In essence, we could have workers sampling the trajectory at many different points, as shown in the following diagram:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-632 image-border" src="assets/e93cbeb7-e14c-4ab5-8adc-9d04b8215ff7.png" style="width:100.58em;height:49.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Multiple workers sampling across a trajectory space</div>
<p>With A3C and asynchronous actor-critic workers, we can get a much better picture of the entire trajectory space sooner, which allows our agent to make clearer and better decisions. It does this by sampling across the trajectory space asynchronously with multiple workers. Let's see how this works by opening up <kbd>Chapter_9_A3C.py</kbd> and performing the following steps:</p>
<ol>
<li>We will start by looking at typical hyperparameters and setup code, as follows:</li>
</ol>
<pre style="padding-left: 60px">n_train_processes = 6<br/>learning_rate = 0.0002<br/>update_interval = 6<br/>gamma = 0.98<br/>max_train_ep = 3000<br/>max_test_ep = 400<br/>environment = "LunarLander-v2"<br/><br/>env = gym.make(environment)<br/>state_size = env.observation_space.shape[0]<br/>action_size = env.action_space.n</pre>
<ol start="2">
<li>Here, we can see the inclusion of two new hyperparameters, <kbd>max_train_ep</kbd> and <kbd>max_test_ep</kbd>. The first variable, <kbd>max_train_ep</kbd>, sets the maximum number of training episodes, while the second variable, <kbd>max_text_ep</kbd>, is used to evaluate performance.</li>
<li>The next section is the <kbd>ActorCritic</kbd> class and is identical to our previous couple of examples, so we won't need to review it here. After that is the <kbd>train</kbd> function, as shown here:</li>
</ol>
<pre style="padding-left: 60px">def train(global_model, rank):<br/>    local_model = ActorCritic(state_size, action_size)<br/>    local_model.load_state_dict(global_model.state_dict())<br/><br/>    optimizer = optim.Adam(global_model.parameters(), lr=learning_rate)<br/><br/>    env = gym.make(environment)<br/><br/>    for n_epi in range(max_train_ep):<br/>        done = False<br/>        s = env.reset()<br/>        while not done:<br/>            s_lst, a_lst, r_lst = [], [], []<br/>            for t in range(update_interval):<br/>                prob = local_model.pi(torch.from_numpy(s).float())<br/>                m = Categorical(prob)<br/>                a = m.sample().item()<br/>                s_prime, r, done, info = env.step(a)<br/><br/>                s_lst.append(s)<br/>                a_lst.append([a])<br/>                r_lst.append(r/100.0)<br/><br/>                s = s_prime<br/>                if done:<br/>                    break<br/><br/>            s_final = torch.tensor(s_prime, dtype=torch.float)<br/>            R = 0.0 if done else local_model.v(s_final).item()<br/>            td_target_lst = []<br/>            for reward in r_lst[::-1]:<br/>                R = gamma * R + reward<br/>                td_target_lst.append([R])<br/>            td_target_lst.reverse()<br/><br/>            s_batch, a_batch, td_target = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \<br/>                torch.tensor(td_target_lst)<br/>            advantage = td_target - local_model.v(s_batch)<br/><br/>            pi = local_model.pi(s_batch, softmax_dim=1)<br/>            pi_a = pi.gather(1, a_batch)<br/>            loss = -torch.log(pi_a) * advantage.detach() + \<br/>                F.smooth_l1_loss(local_model.v(s_batch), td_target.detach())<br/><br/>            optimizer.zero_grad()<br/>            loss.mean().backward()<br/>            for global_param, local_param in zip(global_model.parameters(), local_model.parameters()):<br/>                global_param._grad = local_param.grad<br/>            optimizer.step()<br/>            local_model.load_state_dict(global_model.state_dict())<br/><br/>    env.close()<br/>    print("Training process {} reached maximum episode.".format(rank))</pre>
<ol start="4">
<li>The <kbd>train</kbd> function is quite similar to our previous training code. However, notice how we are passing in a <kbd>global_model</kbd> input. This global model is used as the clone for the local model, which we then train on experiences learned by the worker agent. One of the keys things to observe about this code is the last section, which is where we update the global model using the local model we have been training independently.</li>
<li>Next comes the test function. This is where <kbd>global_model</kbd> is evaluated using the following code:</li>
</ol>
<pre style="padding-left: 60px">def test(global_model):<br/>    env = gym.make(environment)<br/>    score = 0.0<br/>    print_interval = 20<br/><br/>    for n_epi in range(max_test_ep):<br/>        done = False<br/>        s = env.reset()<br/>        while not done:<br/>            prob = global_model.pi(torch.from_numpy(s).float())<br/>            a = Categorical(prob).sample().item()<br/>            s_prime, r, done, info = env.step(a)<br/>            s = s_prime<br/>            score += r<br/><br/>        if n_epi % print_interval == 0 and n_epi != 0:<br/>            print("# of episode :{}, avg score : {:.1f}".format(<br/>                n_epi, score/print_interval))<br/>            score = 0.0<br/>            time.sleep(1)<br/>    env.close()</pre>
<ol start="6">
<li>All this code does is evaluate the model by using it to play the game and evaluate the score. This would certainly be a great place to render the environment for visuals while training.</li>
<li>Finally, we have the main block of processing code. This block of code is identified with the <kbd>name</kbd> if statement, as follows:</li>
</ol>
<pre style="padding-left: 60px">if __name__ == '__main__': <br/>    global_model = ActorCritic(state_size, action_size)<br/>    global_model.share_memory()<br/><br/>    processes = []<br/>    for rank in range(n_train_processes + 1): # + 1 for test process<br/>        if rank == 0:<br/>            p = mp.Process(target=test, args=(global_model,))<br/>        else:<br/>            p = mp.Process(target=train, args=(global_model, rank,))<br/>        p.start()<br/>        processes.append(p)<br/>    for p in processes:<br/>        p.join()</pre>
<ol start="8">
<li>As we can see, this is where the <kbd>global_model</kbd> model is constructed with shared memory. Then, we start up the subprocesses using the first or rank 0 process as the test or evaluation process. Finally, we can see that the code ends when we join back up all the processes with <kbd>p.join</kbd>.</li>
</ol>
<ol start="9">
<li>Run the code as you normally would and take a look at the results, an example of which is as follows:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-867 image-border" src="assets/29d7c553-5d1d-4ee9-9d29-f4b95612abaf.png" style="width:76.33em;height:29.58em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Example output from Chapter_9_A3C.py</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building actor-critic with experience replay</h1>
                </header>
            
            <article>
                
<p class="mce-root">We have come to a point in this book where we have learned about all the major concepts of DRL. There will be more tools we will throw at you in later chapters, such as the one we showed in this section, but if you have made it this far, you should consider yourself knowledgeable of DRL. As such, consider building your own tools or enhancements to DRL, not unlike the one we'll show in this section. If you are wondering if you need to have the math worked out first, then the answer is no. It can often be more intuitive to build these models in code first and then understand the math later.</p>
<p class="mce-root">Actor-critic with experience replay (ACER) provides another advantage by adjusting sampling based on past experiences. This concept was originally introduced by DeepMind in a paper titled <em>Sample Efficient Actor-Critic with Experience Replay</em> and developed the concept for ACER. The intuition behind ACER is that we develop dual dueling stochastic networks in order to reduce the bias and variance and update the trust regions we select in PPO. <span>In the next exercise, we'll explore actor-critic combined with experience replay.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<div class="packt_tip">The math behind ACER is best understood by reviewing the aforementioned paper or searching for blog posts. It is strongly suggested that you understand the math behind TRPO and PPO first, before tackling the paper.</div>
<p class="mce-root">Open <kbd>Chapter_9_ACER.py</kbd> and follow these steps to complete this exercise:</p>
<ol>
<li>The first thing you will notice in this example is the <kbd>ReplayBuffer</kbd> class, as shown here:</li>
</ol>
<pre style="padding-left: 60px">class ReplayBuffer():<br/>    def __init__(self):<br/>        self.buffer = collections.deque(maxlen=buffer_limit)<br/><br/>    def put(self, seq_data):<br/>        self.buffer.append(seq_data)<br/>    <br/>    def sample(self, on_policy=False):<br/>        if on_policy:<br/>            mini_batch = [self.buffer[-1]]<br/>        else:<br/>            mini_batch = random.sample(self.buffer, batch_size)<br/><br/>        s_lst, a_lst, r_lst, prob_lst, done_lst, is_first_lst = [], [], [], [], [], []<br/>        for seq in mini_batch:<br/>            is_first = True <br/>            for transition in seq:<br/>                s, a, r, prob, done = transition<br/><br/>                s_lst.append(s)<br/>                a_lst.append([a])<br/>                r_lst.append(r)<br/>                prob_lst.append(prob)<br/>                done_mask = 0.0 if done else 1.0<br/>                done_lst.append(done_mask)<br/>                is_first_lst.append(is_first)<br/>                is_first = False<br/><br/>        s,a,r,prob,done_mask,is_first = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \<br/>                                        r_lst, torch.tensor(prob_lst, dtype=torch.float), done_lst, \<br/>                                        is_first_lst<br/>        return s,a,r,prob,done_mask,is_first<br/>    <br/>    def size(self):<br/>        return len(self.buffer)</pre>
<p class="mce-root"/>
<ol start="2">
<li>This is an updated version of the <kbd>ReplayBuffer</kbd> class we looked at in previous chapters.</li>
<li>The bulk of the code should be self-explanatory, aside from new sections in the <kbd>train</kbd> function, starting with the first few blocks of code:</li>
</ol>
<pre style="padding-left: 60px">q = model.q(s)<br/>q_a = q.gather(1,a)<br/>pi = model.pi(s, softmax_dim = 1)<br/>pi_a = pi.gather(1,a)<br/>v = (q * pi).sum(1).unsqueeze(1).detach()<br/>    <br/>rho = pi.detach()/prob<br/>rho_a = rho.gather(1,a)<br/>rho_bar = rho_a.clamp(max=c)<br/>correction_coeff = (1-c/rho).clamp(min=0)</pre>
<ol start="4">
<li>The new code is the calculation of <kbd>rho</kbd> from taking the ratio of <kbd>pi</kbd> over the action probability, <kbd>prob</kbd>. Then, the code gathers the tensor to 1, clamps it, and calculates a correction coefficient called <kbd>correction_coeff.</kbd></li>
<li>Scrolling past some of the other familiar code, we come to a new section where the calculation of loss has been updated with the values <kbd>rho_bar</kbd> and <kbd>correction_coeff</kbd>, as follows:</li>
</ol>
<pre style="padding-left: 60px">loss1 = -rho_bar * torch.log(pi_a) * (q_ret - v) <br/>loss2 = -correction_coeff * pi.detach() * torch.log(pi) * (q.detach()-v) loss = loss1 + loss2.sum(1) + F.smooth_l1_loss(q_a, q_ret)</pre>
<ol start="6">
<li>Here, we can see that the inverse of <kbd>rho_bar</kbd> and <kbd>correction_coeff</kbd> are both used to skew the calculations of loss. <kbd>rho</kbd>, the original value we used to calculate these coefficients from, is based on the ratio between previous actions and predicted actions. The effect that's produced by applying this bias is narrowing the search along the trajectory path. This is a very good thing when it's applied to continuous control tasks.</li>
<li>Finally, let's skip to the training loop code and see where the data is appended to <kbd>ReplayBuffer</kbd>:</li>
</ol>
<pre style="padding-left: 60px">seq_data.append((s, a, r/100.0, prob.detach().numpy(), done))</pre>
<ol start="8">
<li>What we can see here is that the action probability, <kbd>prob</kbd>, is entered by detaching from the PyTorch tensor using <kbd>detach()</kbd> and then converting it into a <kbd>numpy</kbd> tensor. This value is what we use to calculate <kbd>rho</kbd> later in the <kbd>train_net</kbd> function.</li>
</ol>
<ol start="9">
<li>Run the code as you normally would and observe the output, an example of which is as follows:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-634 image-border" src="assets/acbb32ad-94ac-4bf2-ab02-ee54cc8b4600.png" style="width:52.42em;height:42.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Example output from Chapter_9_ACER.py</div>
<p>Here, we can see how the buffer size increases and that the agent becomes smarter. This is because we are using those experiences in the replay buffer to adjust the understanding of bias and variance from the policy distribution, which, in turn, reduces the size or clipping area of the trust regions we use. As we can see from this exercise, which is the most impressive one in this chapter, it does indeed learn the environment in a more convergent manner than our previous attempts in this chapter.</p>
<p>That's all for optimizing PG methods. In the next section, we'll look at some exercises that you can carry out on your own.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<p>The exercises in this section are for you to explore on your own. Substantially advancing any of the techniques we cover from this point forward is an accomplishment, so the work you do here could morph into something beyond just learning. Indeed, the environments and examples you work on now will likely indicate your working preference going forward. As always, try to complete two to three of the following exercises:</p>
<ol>
<li>Tune the hyperparameters for <kbd>Chapter_9_PPO.py</kbd> and/or <kbd><span>Chapter_9_PPO_LSTM.py</span></kbd>.</li>
<li>Tune the hyperparameters for <kbd>Chapter_9_A2C.py</kbd> and/or <kbd><span>Chapter_9_A3C.py</span></kbd>.</li>
<li>Tune the hyperparameters for <kbd>Chapter_9_ACER.py</kbd>.</li>
<li>Apply LSTM layers to the A2C and/or A3C examples.</li>
<li>Apply LSTM layers to the ACER example.</li>
<li>Add a <kbd>play_game</kbd> function to the A2C and/or A3C examples.</li>
<li>Add a <kbd>play_game</kbd> function to the ACER example.</li>
<li>Adjust the buffer size in the ACER example and see how that improves training.</li>
<li>Add synchronous and/or asynchronous workers to the ACER example.</li>
<li>Add the ability to output results to matplot or TensorBoard. This is quite advanced but is something we will cover in later chapters.</li>
</ol>
<p>These exercises are intended to reinforce what we learned in this chapter. In the next section, we will summarize this chapter.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned how PG methods are not without their own faults and looked at ways to fix or correct them. This led us to explore more implementation methods that improved sampling efficiency and optimized the objective or clipped gradient function. We did this by looking at the PPO method, which uses clipped objective functions to optimize the region of trust we use to calculate the gradient. After that, we looked at adding a new network layer configuration to understand the context in state.</p>
<p>Then, we used the new layer type, an LSTM layer, on top of PPO to see the improvements it generated. Then, we looked at improving sampling using parallel environments and synchronous or asynchronous workers. We did this by implementing synchronous workers by building an A2C example, followed by looking at an example of using asynchronous workers on A3C. We finished this chapter by looking at another improvement we can make to sampling efficiency through the use of ACER, or actor-critic with experience replay.</p>
<p>In the next chapter, we'll improve upon our knowledge by looking at different methods that are more applicable to gaming. PG methods have been shown to be very successful in gaming tasks, but in the next chapter we'll go back to DQN and see how it can be made state-of-the-art with varying improvements.</p>


            </article>

            
        </section>
    </body></html>