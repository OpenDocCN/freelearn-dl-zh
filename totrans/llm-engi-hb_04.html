<html><head></head><body>
  <div id="_idContainer088" class="Basic-Text-Frame">
    <h1 class="chapterNumber">4</h1>
    <h1 id="_idParaDest-86" class="chapterTitle">RAG Feature Pipeline</h1>
    <p class="normal"><strong class="keyWord">Retrieval-augmented generation</strong> (<strong class="keyWord">RAG</strong>) is<a id="_idIndexMarker255"/> fundamental in most generative AI applications. RAG’s core responsibility is to inject custom data into the <strong class="keyWord">large language model</strong> (<strong class="keyWord">LLM</strong>) to perform a given action (e.g., summarize, reformulate, and extract <a id="_idIndexMarker256"/>the injected data). You often want to use the LLM on data it wasn’t trained on (e.g., private or new data). As fine-tuning an LLM is a highly costly operation, RAG is a compelling strategy that bypasses the need for constant fine-tuning to access that new data.</p>
    <p class="normal">We will start this chapter with a theoretical part that focuses on the fundamentals of RAG and how it works. We will then walk you through all the components of a naïve RAG system: chunking, embedding, and vector DBs. Ultimately, we will present various optimizations used for an advanced RAG system. Then, we will continue exploring LLM Twin’s RAG feature pipeline architecture. At this step, we will apply all the theoretical aspects we discussed at the beginning of the chapter. Finally, we will go through a practical example by implementing the LLM Twin’s RAG feature pipeline based on the system design described throughout the book.</p>
    <p class="normal">The main sections of this chapter are:</p>
    <ul>
      <li class="bulletList">Understanding RAG</li>
      <li class="bulletList">An overview of advanced RAG</li>
      <li class="bulletList">Exploring the LLM Twin’s RAG feature pipeline architecture</li>
      <li class="bulletList">Implementing the LLM Twin’s RAG feature pipeline</li>
    </ul>
    <p class="normal">By the end of this chapter, you will have a clear and comprehensive understanding of what RAG is and how it is applied to our LLM Twin use case.</p>
    <h1 id="_idParaDest-87" class="heading-1">Understanding RAG</h1>
    <p class="normal">RAG enhances the accuracy and reliability of generative AI models with information fetched from external <a id="_idIndexMarker257"/>sources. It is a technique complementary to the internal knowledge of the LLMs. Before going into the details, let’s understand what RAG stands for:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Retrieval</strong>: Search for relevant data</li>
      <li class="bulletList"><strong class="keyWord">Augmented</strong>: Add the data as context to the prompt</li>
      <li class="bulletList"><strong class="keyWord">Generation</strong>: Use the augmented prompt with an LLM for generation</li>
    </ul>
    <p class="normal">Any LLM is bound to understand the data it was trained on, sometimes called parameterized knowledge. Thus, even if the LLM can perfectly answer what happened in the past, it won’t have access to the newest data or any other external sources on which it wasn’t trained.</p>
    <p class="normal">Let’s take the most powerful model from OpenAI as an example, which, in the summer of 2024, is GPT-4o. The model is trained on data up to October 2023. Thus, if we ask what happened during the 2020 pandemic, it can be answered perfectly due to its parametrized knowledge. However, it will not know the answer if we ask about the 2024 European Football Championship results due to its bounded parametrized knowledge. Another scenario is that it will start confidently hallucinating and provide a faulty answer.</p>
    <p class="normal">RAG overcomes these two limitations of LLMs. It provides access to external or latest data and prevents hallucinations, enhancing generative AI models’ accuracy and reliability.</p>
    <h2 id="_idParaDest-88" class="heading-2">Why use RAG?</h2>
    <p class="normal">We briefly explained the importance of using RAG in generative AI applications earlier. Now, we will dig deeper into the “why,” following which we will focus on what a naïve RAG framework looks like.</p>
    <p class="normal">For now, to get an intuition about RAG, you have to know that when using RAG, we inject the necessary information into the prompt to answer the initial user question. After that, we pass the augmented prompt to the LLM for the final answer. Now, the LLM will use the additional context to answer the user question.</p>
    <p class="normal">There are two fundamental problems that RAG solves:</p>
    <ul>
      <li class="bulletList">Hallucinations</li>
      <li class="bulletList">Old or private information</li>
    </ul>
    <h3 id="_idParaDest-89" class="heading-3">Hallucinations</h3>
    <p class="normal">If a chatbot without RAG is asked a question about something it wasn’t trained on, there is a high chance that it will give <a id="_idIndexMarker258"/>you a confident answer about something that isn’t true. Let’s take the 2024 European Football Championship as an example. If the model is trained up to October 2023 and we ask it something about the tournament, it will most likely come up with a random answer that is hard to differentiate between reality and truth. Even if the LLM doesn’t hallucinate all the time, it raises concerns about the trustworthiness of its answers. Thus, we must ask ourselves: “When can we trust the LLM’s answers?” and “How can we evaluate if the answers are correct?”. </p>
    <p class="normal">By introducing RAG, we enforce the LLM to always answer solely based on the introduced context. The LLM will act as the reasoning engine, while the additional information added through RAG will act as the single source of truth for the generated answer. By doing so, we can quickly evaluate if the LLM’s answer is based on the external data or not.</p>
    <h3 id="_idParaDest-90" class="heading-3">Old information</h3>
    <p class="normal">Any LLM is trained or fine-tuned on a subset of the total world knowledge dataset. This is due to three main issues:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Private data</strong>: You cannot train your model on data you don’t own or have the right to use.</li>
      <li class="bulletList"><strong class="keyWord">New data</strong>: New data is generated every second. Thus, you would have to constantly train your LLM to keep up.</li>
      <li class="bulletList"><strong class="keyWord">Costs</strong>: Training or fine-tuning an LLM is an extremely costly operation. Hence, it is not feasible to do it on an hourly or daily basis.</li>
    </ul>
    <p class="normal">RAG solves these issues, as <a id="_idIndexMarker259"/>you no longer have to constantly fine-tune your LLM on new data (or even private data). Directly injecting the necessary data to respond to user questions into the prompts that are fed to the LLM is enough to generate correct and valuable answers.</p>
    <p class="normal">To conclude, RAG is key for a robust and flexible generative AI system. But how do we inject the right data into the prompt based on the user’s questions? We will dig into the technical aspects<a id="_idIndexMarker260"/> of RAG in the next sections.</p>
    <h2 id="_idParaDest-91" class="heading-2">The vanilla RAG framework</h2>
    <p class="normal">Every RAG system is similar at its roots. We will first focus on understanding RAG in its simplest form. Later, we<a id="_idIndexMarker261"/> will gradually introduce more advanced RAG techniques to improve the system’s accuracy. Note that we will use vanilla and naive RAG interchangeably to avoid repetition. </p>
    <p class="normal">A RAG system is composed of three main modules independent of each other:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Ingestion pipeline</strong>: A batch or streaming pipeline used to populate the vector DB</li>
      <li class="bulletList"><strong class="keyWord">Retrieval pipeline</strong>: A module that queries the vector DB and retrieves relevant entries to the user’s input</li>
      <li class="bulletList"><strong class="keyWord">Generation pipeline</strong>: The layer that uses the retrieved data to augment the prompt and an LLM to generate answers</li>
    </ul>
    <p class="normal">As these three components are classes or services of their own, we will dig into each separately. But for now, let’s try to answer the question “How are these three modules connected?”. Here is a very simplistic overview:</p>
    <ol>
      <li class="numberedList" value="1">On the backend side, the ingestion pipeline runs either on a schedule or constantly to populate the vector DB with external data.</li>
      <li class="numberedList">On the client side, the user asks a question.</li>
      <li class="numberedList">The question is passed to the retrieval module, which preprocesses the user’s input and queries the vector DB.</li>
      <li class="numberedList">The generation pipelines use a prompt template, user input, and retrieved context to create the prompt.</li>
      <li class="numberedList">The prompt is passed to an LLM to generate the answer.</li>
      <li class="numberedList">The answer is shown to the user.</li>
    </ol>
    <figure class="mediaobject"><img src="../Images/B31105_04_01.png" alt=""/></figure>
    <figure class="mediaobject">Figure 4.1: Vanilla RAG architecture</figure>
    <p class="normal">You must implement RAG in your generative AI application when you need access to any type of external information. For example, when implementing a financial assistant, you most likely need access to the latest news, reports, and prices before providing valuable answers. Or, if you build a traveling recommender, you must retrieve and parse a list of potential attractions, restaurants, and activities. At training time, LLMs don’t have access to your specific data, so you will often have to implement a RAG strategy in your generative AI project. Now, let’s dig into the ingestion, retrieval, and generation pipelines.</p>
    <h3 id="_idParaDest-92" class="heading-3">Ingestion pipeline</h3>
    <p class="normal">The RAG ingestion pipeline extracts raw documents from various data sources (e.g., data warehouse, data lake, web pages, etc.). Then, it cleans, chunks (splits into smaller sections), and embeds the documents. Ultimately, it loads the embedded chunks into a vector DB (or other similar vector storage).</p>
    <p class="normal">Thus, the RAG ingestion pipeline is split into the following:</p>
    <ul>
      <li class="bulletList">The <strong class="keyWord">data extraction module</strong> gathers all the necessary data from various sources such as DBs, APIs, or web pages. This module is highly dependent on your data. It can be as easy as querying your data warehouse or something more complex such as crawling Wikipedia.</li>
      <li class="bulletList">A <strong class="keyWord">cleaning layer</strong> standardizes and removes unwanted characters from the extracted data. For example, you must remove all invalid characters from your input text, such as non-ASCII and bold and italic characters. Another popular cleaning strategy is to replace URLs with placeholders. However, your cleaning strategy will vary depending on your data source and embedding model.</li>
      <li class="bulletList">The <strong class="keyWord">chunking module</strong> splits the cleaned documents into smaller ones. As we want to pass the document’s content to an embedding model, this is necessary to ensure it doesn’t exceed the model’s input maximum size. Also, chunking is required to separate specific regions that are semantically related. For example, when chunking a book’s chapter, the most optimal way is to group similar paragraphs into the same section or chunk. By doing so, at the retrieval time, you will add only the essential data to the prompt.</li>
      <li class="bulletList">The <strong class="keyWord">embedding component</strong> uses an<strong class="keyWord"> </strong>embedding model to take the chunk’s content (text, images, audio, etc.) and project it into a dense vector packed with semantic value—more on embeddings in the <em class="italic">What are embeddings?</em> section below.</li>
      <li class="bulletList">The <strong class="keyWord">loading module</strong> takes the embedded chunks along with a metadata document. The metadata will contain essential information such as the embedded content, the URL to the source of the chunk, and when the content was published on the web. The embedding is used as an index to query similar chunks, while the metadata is used to access the information added to augment the prompt.</li>
    </ul>
    <p class="normal">At this point, we have a RAG ingestion pipeline that takes raw documents as input, processes them, and populates a vector DB. The next step is to retrieve relevant data from the vector store correctly.</p>
    <h3 id="_idParaDest-93" class="heading-3">Retrieval pipeline</h3>
    <p class="normal">The retrieval components take the user’s input (text, image, audio, etc.), embed it, and query the vector DB for similar vectors to the user’s input.</p>
    <p class="normal">The primary function of the retrieval step is to project the user’s input into the same vector space as the embeddings used as an index in the vector DB. This allows us to find the top K’s most similar entries by comparing the embeddings from the vector storage with the user’s input vector. These entries then serve as content to augment the prompt that is passed to the LLM to generate the answer.</p>
    <p class="normal">You must use a distance metric to compare two vectors, such as the Euclidean or Manhattan distance. But the most popular one is the cosine distance, which is equal to 1 minus the cosine of the angle between two vectors, as follows:</p>
    <p class="center"><img src="../Images/B31105_04_001.png" alt=""/></p>
    <p class="normal">It ranges from <code class="inlineCode">-1</code> to <code class="inlineCode">1</code>, with a value of <code class="inlineCode">-1</code> when vectors <strong class="keyWord">A</strong> and <strong class="keyWord">B</strong> are in opposite directions, <code class="inlineCode">0</code> if they are orthogonal, and <code class="inlineCode">1</code> if they point in the same direction.</p>
    <p class="normal">Most of the time, the cosine distance works well in non-linear complex vector spaces. However, it is essential to notice that choosing the proper distance between two vectors depends on your data and the embedding model you use.</p>
    <p class="normal">One critical factor to highlight is that the user’s input and embeddings must be in the same vector space. Otherwise, you cannot compute the distance between them. To do so, it is essential to preprocess the user input in the same way you processed the raw documents in the RAG ingestion pipeline. This means you must clean, chunk (if necessary), and embed the user’s input using the same functions, models, and hyperparameters. This is similar to how you have to preprocess the data into features in the same way between training and inference; otherwise, the inference will yield inaccurate results—a phenomenon also known as the training-serving skew.</p>
    <h3 id="_idParaDest-94" class="heading-3">Generation pipeline</h3>
    <p class="normal">The last step of the RAG system is to take the user’s input, retrieve data, pass it to an LLM, and generate a valuable answer.</p>
    <p class="normal">The final prompt results from a system and prompt template populated with the user’s query and retrieved context. You might have a single prompt template or multiple prompt templates, depending on your application. Usually, all the prompt engineering is done at the prompt template level.</p>
    <p class="normal">Below, you can see a dummy example of what a generic system and prompt template look like and how they are used together with the retrieval logic and the LLM to generate the final answer:</p>
    <pre class="programlisting code"><code class="hljs-code">system_template = <span class="hljs-string">"""</span>
<span class="hljs-string">You are a helpful assistant who answers all the user's questions politely.</span>
<span class="hljs-string">"""</span>
prompt_template = <span class="hljs-string">"""</span>
<span class="hljs-string">Answer the user's question using only the provided context. If you cannot answer using the context, respond with "I don't know."</span>
<span class="hljs-string">Context: {context}</span>
<span class="hljs-string">User question: {user_question}</span>
<span class="hljs-string">"""</span>
user_question = <span class="hljs-string">"&lt;your_question&gt;"</span>
retrieved_context = retrieve(user_question)
prompt = <span class="hljs-string">f"</span><span class="hljs-subst">{system_template}</span><span class="hljs-string">\n"</span>
prompt += prompt_template.<span class="hljs-built_in">format</span>(context=retrieved_context, user_question=user_question)
answer  = llm(prompt)
</code></pre>
    <p class="normal">As the prompt templates evolve, each change should be tracked and versioned using <strong class="keyWord">machine learning operations</strong> (<strong class="keyWord">MLOps</strong>) best practices. Thus, during training or inference time, you always know that a given answer was generated by a specific version of the LLM and prompt template(s). You can do this through Git, store the prompt templates in a DB, or use specific prompt management tools such as LangFuse.</p>
    <p class="normal">As we’ve seen in the retrieval pipeline, some critical aspects that directly impact the accuracy of your RAG system are the embeddings of the external data, usually stored in vector DBs, the embedding of the user’s query, and how we can find similarities between the two using functions such as the cosine distance. To better understand this part of the RAG algorithm, let’s zoom in on what embeddings are and how they are computed.</p>
    <h2 id="_idParaDest-95" class="heading-2">What are embeddings?</h2>
    <p class="normal">Imagine you’re trying to teach a computer to understand the world. Embeddings are like a particular translator that turns these things into a numerical code. This code isn’t random, though, because<a id="_idIndexMarker262"/> similar words or items end up with codes that are close to each other. It’s like a map where words with similar meanings are clustered together.</p>
    <p class="normal">With that in mind, a more theoretical definition is that embeddings are dense numerical representations of objects encoded as vectors in a continuous vector space, such as words, images, or items in a recommendation system. This transformation helps capture the semantic meaning and relationships between the objects. For instance, in <strong class="keyWord">natural language processing</strong> (<strong class="keyWord">NLP</strong>), embeddings translate words into vectors where semantically similar words are positioned closely together in the vector space.</p>
    <figure class="mediaobject"><img src="../Images/B31105_04_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.2: What are embeddings?</p>
    <p class="normal">A popular method is visualizing the embeddings to understand and evaluate their geometrical relationship. As the embeddings often have more than 2 or 3 dimensions, usually between 64 and 2048, you must project them again to 2D or 3D.</p>
    <p class="normal">For example, you can use UMAP (<a href="https://umap-learn.readthedocs.io/en/latest/index.html"><span class="url">https://umap-learn.readthedocs.io/en/latest/index.html</span></a>), a dimensionality reduction method well known for keeping<a id="_idIndexMarker263"/> the geometrical properties between the points when projecting the embeddings to 2D or 3D. Another popular algorithm for dimensionality reduction when visualizing vectors is t-SNE (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html"><span class="url">https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html</span></a>). However, compared to UMAP, it is more stochastic and doesn’t preserve the topological relationships between the points.</p>
    <div class="note">
      <p class="normal"> A dimensionality reduction algorithm, such as PCA, UMAP, and t-SNE, is a mathematical technique used to reduce the number of input variables or features in a dataset while preserving the data’s essential patterns, structure, and relationships. The goal is to transform high-dimensional data into a lower-dimensional form, making it easier to visualize, interpret, and process while minimizing the loss of important information. These methods help to address the “curse of dimensionality,” improve computational efficiency, and often enhance the performance of ML algorithms.</p>
    </div>
    <figure class="mediaobject"><img src="../Images/B31105_04_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.3: Visualize embeddings using UMAP (Source: UMAP’s documentation)</p>
    <h3 id="_idParaDest-96" class="heading-3">Why embeddings are so powerful</h3>
    <p class="normal">Firstly, ML models work <a id="_idIndexMarker264"/>only with numerical values. This is not a problem when working with tabular data, as the data is often in numerical form or can easily be processed into numbers. Embeddings come in handy when we want to feed words, images, or audio data into models.</p>
    <p class="normal">For instance, when working with transformer models, you tokenize all your text input, where each token has an embedding associated with it. The beauty of this process lies in its simplicity; the input to the transformer is a sequence of embeddings, which can be easily and confidently interpreted by the dense layers of the neural network.</p>
    <p class="normal">Based on this example, you can use embeddings to encode any categorical variable and feed it to an ML model. But why not use other simple methods, such as <strong class="keyWord">one-hot encoding?</strong> When working with categorical variables with high cardinality, such as language vocabularies, you will suffer from the curse of dimensionality when using other classical methods. For example, if your vocabulary has 10,000 tokens, then only one token will have a length of 10,000 after applying one-hot encoding. If the input sequence has N tokens, that will become N * 10,000 input parameters. If N &gt;= 100, often, when inputting text, the input is too large to be usable. Another issue with other classical methods<a id="_idIndexMarker265"/> that don’t suffer from the curse of dimensionality, such as <strong class="keyWord">hashing</strong>, is that you lose the semantic relationships between the vectors.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">One-hot encoding</strong> is a technique that converts categorical variables into a binary matrix representation. Each category is represented as a unique binary vector. For each categorical variable, a binary vector is created with a length equal to the number of unique categories, where all values are zero except for the index corresponding to the specific category, which is set to one. The method preserves all information about the categories. It is simple and interpretable. However, a significant disadvantage is that it can lead to a high-dimensional feature space if the categorical variable has many unique values, making the method impractical.</p>
      <p class="normal"><strong class="keyWord">Feature hashing</strong>, also known as hashing encoding or the “hash trick,” is a technique used to convert categorical variables into numerical features by applying a hash function to the category values. Compared to one-hot encoding, the method is not bound to the number of unique categories, but it reduces the dimensionality of the feature space by mapping categories into a fixed number of bins or buckets. Thus, it reduces the dimensionality of the feature space, which is particularly useful when dealing with high-cardinality categorical variables. This makes it efficient in terms of memory usage and computational time. However, there is a risk of collisions, where different categories might map to the same bin, leading to a loss of information. The mapping makes the method uninterpretable. Also, it is difficult to understand the relationship between the original categories and the hashed features.</p>
      <p class="normal">Embeddings help us encode categorical variables while controlling the output vector’s dimension. They also use ingenious ways to condense information into a lower dimension space than naive hashing tricks.</p>
    </div>
    <p class="normal">Secondly, embedding your input reduces the size of its dimension and condenses all of its semantic meaning into a dense vector. This is an extremely popular technique when working with images, where a CNN encoder module maps the high-dimensional meaning into an embedding, which is later processed by a CNN decoder that performs the classification or regression steps.</p>
    <p class="normal">The following image shows a typical CNN layout. Imagine tiny squares within each layer. Those are the “receptive fields.” Each square feeds information to a single neuron in the previous layer. As you move<a id="_idIndexMarker266"/> through the network, two key things are happening:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Shrinking the picture</strong>: Special “subsampling” operations make the layers smaller, focusing on essential details.</li>
      <li class="bulletList"><strong class="keyWord">Learning features</strong>: “Convolution” operations, on the other hand, actually increase the layer size as the network learns more complex features from the image.</li>
    </ul>
    <p class="normal">Finally, a fully connected layer at the end takes all this processed information and transforms it into the final vector embedding, a numerical image representation.</p>
    <figure class="mediaobject"><img src="../Images/B31105_04_04.png" alt=""/></figure>
    <figure class="mediaobject">Figure 4.4: Creating embeddings from an image using a CNN (Image source)</figure>
    <div class="note">
      <p class="normal">The preceding image is sourced from <em class="italic">Wikimedia Commons</em> (<a href="https://commons.wikimedia.org/wiki/File:Typical_cnn.png"><span class="url">https://commons.wikimedia.org/wiki/File:Typical_cnn.png</span></a>) and licensed under the Creative Commons Attribution-ShareAlike 4.0 International License (CC BY-SA 4.0: <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.en"><span class="url">https://creativecommons.org/licenses/by-sa/4.0/deed.en</span></a>).</p>
    </div>
    <h3 id="_idParaDest-97" class="heading-3">How are embeddings created?</h3>
    <p class="normal">Embeddings are created by deep learning models that understand the context and semantics of your input and project it into a continuous vector space.</p>
    <p class="normal">Various deep learning<a id="_idIndexMarker267"/> models can be used to create embeddings, varying by the data input type. Thus, it is fundamental to understand your data and what you need from it before picking an embedding model.</p>
    <p class="normal">For example, when working with text data, one of the early methods used to create embeddings for your vocabulary is Word2Vec and GloVe. These are still popular methods used today for simpler applications.</p>
    <p class="normal">Another popular method is to use encoder-only transformers, such as BERT, or other methods from its family, such as RoBERTa. These models leverage the encoder of the transformer architecture to smartly project your input into a dense vector space that can later be used as embeddings.</p>
    <p class="normal">To quickly compute the embeddings in Python, you can conveniently leverage the Sentence Transformers Python package (also available in Hugging Face’s transformer package). This tool provides a user-friendly interface, making the embedding process straightforward and efficient.</p>
    <p class="normal">In the code snippet below, you can see how we loaded a model from SentenceTransformer, computed the embeddings for three sentences, and, ultimately, computed the cosine similarity between them. The similarity between one sentence and itself is always 1. Also, the similarity between the first and second sentences is approximately 0, as the sentences have nothing in common. In contrast, the value between the first and third one is higher as there is some overlapping context:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sentence_transformers <span class="hljs-keyword">import</span> SentenceTransformer
model = SentenceTransformer(<span class="hljs-string">"all-MiniLM-L6-v2"</span>)
sentences = [
<span class="hljs-string">"The dog sits outside waiting for a treat."</span>,
<span class="hljs-string">"I am going swimming."</span>,
<span class="hljs-string">"The dog is swimming."</span>
]
embeddings = model.encode(sentences)
<span class="hljs-built_in">print</span>(embeddings.shape)
<span class="hljs-comment"># Output: [3, 384]</span>
similarities = model.similarity(embeddings, embeddings)
<span class="hljs-built_in">print</span>(similarities)
<span class="hljs-comment"># Output:</span>
<span class="hljs-comment"># tensor([[ 1.0000, -0.0389, 0.2692],</span>
<span class="hljs-comment"># [-0.0389, 1.0000, 0.3837],</span>
<span class="hljs-comment"># [ 0.2692, 0.3837, 1.0000]])</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># similarities[0, 0] = The similarity between the first sentence and itself.</span>
<span class="hljs-comment"># similarities[0, 1] = The similarity between the first and second sentence.</span>
<span class="hljs-comment"># similarities[2, 1] = The similarity between the third and second sentence.</span>
</code></pre>
    <p class="normal">The source code for the <a id="_idIndexMarker268"/>preceding snippet can be found at <a href="https://github.com/PacktPublishing/LLM-Engineering/blob/main/code_snippets/08_text_embeddings.py"><span class="url">https://github.com/PacktPublishing/LLM-Engineering/blob/main/code_snippets/08_text_embeddings.py</span></a>.</p>
    <div class="note">
      <p class="normal">The examples in the embeddings section can be run within the virtual environment used across the book, as it contains all the required dependencies.</p>
    </div>
    <p class="normal">The best-performing embedding model can change with time and your specific use case. You can find particular models on the <strong class="keyWord">Massive Text Embedding Benchmark</strong> (<strong class="keyWord">MTEB</strong>) on Hugging Face. Depending on your needs, you can consider the best-performing model, the one with the best accuracy, or the one with the smallest memory footprint. This decision is solely based on your requirements (e.g., accuracy and hardware). However, Hugging Face and SentenceTransformer make switching between different models straightforward. Thus, you can always experiment with various options.</p>
    <p class="normal">When working with images, you can embed them using <strong class="keyWord">convolutional neural networks</strong> (<strong class="keyWord">CNNs</strong>). Popular CNN networks are based on the ResNet architecture. However, we can’t directly use image embedding techniques for audio recordings. Instead, we can create a visual representation of the audio, such as a spectrogram, and then apply image embedding models to those visuals. This allows us to capture the essence of images and sounds in a way computers can understand.</p>
    <p class="normal">By leveraging models like CLIP, you can practically embed a piece of text and an image in the same vector space. This allows you to find similar images using a sentence as input, or the other way around, demonstrating the practicality of CLIP.</p>
    <p class="normal">In the following code <a id="_idIndexMarker269"/>snippet, we use CLIP to encode a crazy cat image and three sentences. Ultimately, we use cosine similarity to compute the resemblance between the picture and the sentences:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> io <span class="hljs-keyword">import</span> BytesIO
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">from</span> sentence_transformers <span class="hljs-keyword">import</span> SentenceTransformer
response = requests.get(
<span class="hljs-string">"https://github.com/PacktPublishing/LLM-Engineering/blob/main/images/crazy_cat.jpg?raw=true"</span>
)
image = Image.<span class="hljs-built_in">open</span>(BytesIO(response.content))
model = SentenceTransformer(<span class="hljs-string">"</span><span class="hljs-string">clip-ViT-B-32"</span>)
img_emb = model.encode(image)
text_emb = model.encode(
[<span class="hljs-string">"A crazy cat smiling."</span>,
<span class="hljs-string">"A white and brown cat with a yellow bandana."</span>,
<span class="hljs-string">"A man eating in the garden."</span>]
)
<span class="hljs-built_in">print</span>(text_emb.shape) <span class="hljs-comment"># noqa</span>
<span class="hljs-comment"># Output: (3, 512)</span>
similarity_scores = model.similarity(img_emb, text_emb)
<span class="hljs-built_in">print</span>(similarity_scores) <span class="hljs-comment"># noqa</span>
<span class="hljs-comment"># Output: tensor([[0.3068, 0.3300, 0.1719]])</span>
</code></pre>
    <p class="normal">The source code can be found at <a href="https://github.com/PacktPublishing/LLM-Engineering/blob/main/code_snippets/08_text_image_embeddings.py"><span class="url">https://github.com/PacktPublishing/LLM-Engineering/blob/main/code_snippets/08_text_image_embeddings.py</span></a>.</p>
    <p class="normal">Here, we provided a small introduction to how embeddings can be computed. The realm of specific implementations is vast, but what is important to know is that embeddings can be computed for most digital data categories, such as words, sentences, documents, images, videos, and graphs.</p>
    <p class="normal">It’s crucial to grasp that you must use specialized models when you need to compute the distance between two different data categories, such as the distance between the vector of a sentence <a id="_idIndexMarker270"/>and of an image. These models are designed to project both data types into the same vector space, such as CLIP, ensuring accurate distance computation.</p>
    <h3 id="_idParaDest-98" class="heading-3">Applications of embeddings</h3>
    <p class="normal">Due to the generative AI <a id="_idIndexMarker271"/>revolution, which uses RAG, embeddings have become extremely popular in information retrieval tasks, such as semantic search for text, code, images, and audio, and long-term memory of agents. But before generative AI, embeddings were already heavily used in:</p>
    <ul>
      <li class="bulletList">Representing categorical variables (e.g., vocabulary tokens) that are fed to an ML model</li>
      <li class="bulletList">Recommender systems by encoding the users and items and finding their relationship</li>
      <li class="bulletList">Clustering and outlier detection</li>
      <li class="bulletList">Data visualization by using algorithms such as UMAP</li>
      <li class="bulletList">Classification by using the embeddings as features</li>
      <li class="bulletList">Zero-shot classification by comparing the embedding of each class and picking the most similar one</li>
    </ul>
    <p class="normal">The last step to fully understanding how RAG works is to examine vector DBs and how they leverage embeddings to retrieve data.</p>
    <h2 id="_idParaDest-99" class="heading-2">More on vector DBs</h2>
    <p class="normal">Vector DBs are specialized<a id="_idIndexMarker272"/> DBs designed to efficiently store, index, and retrieve vector embeddings. Traditional scalar-based DBs struggle with<a id="_idIndexMarker273"/> the complexity of vector data, making vector DBs crucial for tasks like real-time semantic search.</p>
    <p class="normal">While standalone vector indices like FAISS are effective for similarity search, they lack vector DBs’ comprehensive data management capabilities. Vector DBs support CRUD operations, metadata filtering, scalability, real-time updates, backups, ecosystem integration, and robust data security, making them more suited for production environments than standalone indices.</p>
    <h3 id="_idParaDest-100" class="heading-3">How does a vector DB work?</h3>
    <p class="normal">Think of how you usually search a DB. You type in something specific, and the system spits out the exact match. That’s<a id="_idIndexMarker274"/> how traditional DBs work. Vector DBs are different. Instead of perfect matches, we look for the closest neighbors of the query vector. Under the hood, a vector DB uses <strong class="keyWord">approximate nearest neighbor</strong> (<strong class="keyWord">ANN</strong>) algorithms to find these close neighbors.</p>
    <p class="normal">While ANN algorithms don’t return the top matches for a given search, standard nearest neighbor algorithms are too slow to work in practice. Also, it is shown empirically that using only approximations of the top matches for a given input query works well enough. Thus, the trade-off between accuracy and latency ultimately favors ANN algorithms.</p>
    <p class="normal">This is a typical workflow of a vector DB:</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">Indexing vectors</strong>: Vectors are indexed using data structures optimized for high-dimensional data. Common indexing techniques include <strong class="keyWord">hierarchical navigable small world</strong> (<strong class="keyWord">HNSW</strong>), random projection, <strong class="keyWord">product quantization</strong> (<strong class="keyWord">PQ</strong>), and <strong class="keyWord">locality-sensitive hashing</strong> (<strong class="keyWord">LSH</strong>).</li>
      <li class="numberedList"><strong class="keyWord">Querying for similarity</strong>: During a search, the DB queries the indexed vectors to find those most similar to the input vector. This process involves comparing vectors based on similarity measures such as cosine similarity, Euclidean distance, or dot product. Each has unique advantages and is suitable for different use cases.</li>
      <li class="numberedList"><strong class="keyWord">Post-processing results</strong>: After identifying potential matches, the results undergo post-processing to refine accuracy. This step ensures that the most relevant vectors are returned to the user.</li>
    </ol>
    <p class="normal">Vector DBs can filter results based on metadata before or after the vector search. Both approaches have trade-offs in terms of performance and accuracy. The query also depends on the metadata (along with the vector index), so it contains a metadata index user for filtering operations.</p>
    <h3 id="_idParaDest-101" class="heading-3">Algorithms for creating the vector index</h3>
    <p class="normal">Vector DBs use<a id="_idIndexMarker275"/> various algorithms to create the vector index and manage searching data efficiently:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Random projection</strong>: Random projection reduces the dimensionality of vectors by projecting them into a lower-dimensional space using a random matrix. This technique preserves the relative distances between vectors, facilitating faster searches.</li>
      <li class="bulletList"><strong class="keyWord">PQ</strong>: PQ compresses vectors by dividing them into smaller sub-vectors and then quantizing these sub-vectors into representative codes. This reduces memory usage and speeds up similarity searches.</li>
      <li class="bulletList"><strong class="keyWord">LSH</strong>: LSH maps similar vectors into buckets. This method enables fast approximate nearest neighbor searches by focusing on a subset of the data, reducing the <a id="_idIndexMarker276"/>computational complexity.</li>
      <li class="bulletList"><strong class="keyWord">HNSW</strong>: HNSW constructs a multi-layer graph where each node represents a set of vectors. Similar nodes are connected, allowing the algorithm to navigate the graph and find the nearest neighbors efficiently.</li>
    </ul>
    <p class="normal">These algorithms enable vector DBs to efficiently handle complex and large-scale data, making them a perfect fit for a variety of AI and ML applications.</p>
    <h3 id="_idParaDest-102" class="heading-3">DB operations</h3>
    <p class="normal">Vector DBs also share<a id="_idIndexMarker277"/> common characteristics with standard DBs to ensure high performance, fault tolerance, and ease of management in production environments. Key operations include:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Sharding and replication</strong>: Data is partitioned (sharded) across multiple nodes to ensure scalability and high availability. Data replication across nodes helps maintain data integrity and availability in case of node failures.</li>
      <li class="bulletList"><strong class="keyWord">Monitoring</strong>: Continuous monitoring of DB performance, including query latency and resource usage (RAM, CPU, disk), helps maintain optimal operations and identify potential issues before they impact the system.</li>
      <li class="bulletList"><strong class="keyWord">Access control</strong>: Implementing robust access control mechanisms ensures that only authorized users can access and modify data. This includes role-based access controls and other security protocols to protect sensitive information.</li>
      <li class="bulletList"><strong class="keyWord">Backups</strong>: Regular DB backups are critical for disaster recovery. Automated backup processes ensure that data can be restored to a previous state in case of corruption or loss.</li>
    </ul>
    <h1 id="_idParaDest-103" class="heading-1">An overview of advanced RAG</h1>
    <p class="normal">The vanilla RAG framework <a id="_idIndexMarker278"/>we just presented doesn’t address many fundamental aspects that impact the quality of the retrieval and answer generation, such as:</p>
    <ul>
      <li class="bulletList">Are the retrieved documents relevant to the user’s question?</li>
      <li class="bulletList">Is the retrieved context enough to answer the user’s question?</li>
      <li class="bulletList">Is there any redundant information that only adds noise to the augmented prompt?</li>
      <li class="bulletList">Does the latency of the retrieval step match our requirements?</li>
      <li class="bulletList">What do we do if we can’t generate a valid answer using the retrieved information?</li>
    </ul>
    <p class="normal">From the questions above, we can draw two conclusions. The first one is that we need a robust evaluation module for our RAG system that can quantify and measure the quality of the retrieved data and generate answers relative to the user’s question. We will discuss this topic in more detail in <em class="italic">Chapter 9</em>. The second conclusion is that we must improve our RAG framework to <a id="_idIndexMarker279"/>address the retrieval limitations directly in the algorithm. These improvements are known as advanced RAG.</p>
    <p class="normal">The vanilla RAG design can be optimized at three different stages:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Pre-retrieval</strong>: This stage focuses on how to structure and preprocess your data for data indexing optimizations as well as query optimizations.</li>
      <li class="bulletList"><strong class="keyWord">Retrieval</strong>: This stage revolves around improving the embedding models and metadata filtering to improve the vector search step.</li>
      <li class="bulletList"><strong class="keyWord">Post-retrieval</strong>: This stage mainly targets different ways to filter out noise from the retrieved documents and compress the prompt before feeding it to an LLM for answer generation.</li>
    </ul>
    <figure class="mediaobject"><img src="../Images/B31105_04_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.5: The three stages of advanced RAG</p>
    <p class="normal">This section is not meant to be an exhaustive list of all the advanced RAG methods available. The goal is to<a id="_idIndexMarker280"/> build an intuition about what can be optimized. We will use only examples based on text data, but the principles of advanced RAG remain the same regardless of the data category. Now, let’s zoom in on all three components.</p>
    <h2 id="_idParaDest-104" class="heading-2">Pre-retrieval</h2>
    <p class="normal">The pre-retrieval steps are performed in two different ways:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Data indexing</strong>: It is part of the <a id="_idIndexMarker281"/>RAG ingestion pipeline. It is mainly implemented within the<a id="_idIndexMarker282"/> cleaning or chunking modules to preprocess the data for better indexing.</li>
      <li class="bulletList"><strong class="keyWord">Query optimization</strong>: The algorithm is <a id="_idIndexMarker283"/>performed directly on the user’s query before embedding it and retrieving the chunks from the vector DB.</li>
    </ul>
    <p class="normal">As we index our data using embeddings that semantically represent the content of a chunked document, most of the <strong class="keyWord">data indexing</strong> techniques<a id="_idIndexMarker284"/> focus on better preprocessing and structuring the data to improve retrieval efficiency, such as:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Sliding window</strong>: The sliding window technique introduces overlap between text chunks, ensuring that important context near chunk boundaries is retained, which enhances retrieval accuracy. This is particularly beneficial in domains like legal documents, scientific papers, customer support logs, and medical records, where critical information often spans multiple sections. The embedding is computed on the chunk along with the overlapping portion. Hence, the sliding window improves the system’s ability to retrieve relevant and coherent information by maintaining context across boundaries.</li>
      <li class="bulletList"><strong class="keyWord">Enhancing data granularity</strong>: This involves data cleaning techniques like removing irrelevant details, verifying factual accuracy, and updating outdated information. A clean and accurate dataset allows for sharper retrieval.</li>
      <li class="bulletList"><strong class="keyWord">Metadata</strong>: Adding metadata tags like dates, URLs, external IDs, or chapter markers helps filter results efficiently during retrieval.</li>
      <li class="bulletList"><strong class="keyWord">Optimizing index structures</strong>: It is based on different data index methods, such as various chunk sizes and multi-indexing strategies.</li>
      <li class="bulletList"><strong class="keyWord">Small-to-big</strong>: The algorithm decouples the chunks used for retrieval and the context used in the prompt for the final answer generation. The algorithm uses a small sequence of text to compute the embedding while preserving the sequence itself and a wider window around it in the metadata. Thus, using smaller chunks enhances the retrieval’s accuracy, while the larger context adds more contextual information to the LLM. </li>
    </ul>
    <p class="bulletList">The intuition behind this is that if we use the whole text for computing the embedding, we might introduce too much noise, or the text could contain multiple topics, which results in a poor overall semantic representation of the embedding.</p>
    <figure class="mediaobject"><img src="../Images/B31105_04_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.6: Query routing</p>
    <p class="normal">On the <strong class="keyWord">query optimization</strong> side, we <a id="_idIndexMarker285"/>can leverage techniques such as query<a id="_idIndexMarker286"/> routing, query rewriting, and query expansion to refine the retrieved information for the LLM further:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Query routing</strong>: Based on the user’s input, we might have to interact with different categories of data <a id="_idIndexMarker287"/>and query each category differently. Query rooting is used to decide what action to take based on the user’s input, similar to if/else statements. Still, the decisions are made solely using natural language instead of logical statements. </li>
    </ul>
    <p class="bulletList">As illustrated in Figure 4.6, let’s assume that, based on the user’s input, to do RAG, we can retrieve additional context from a vector DB using vector search queries, a standard SQL DB by translating the user query to an SQL command, or the internet by leveraging REST API calls. The query router can also detect whether a context is required, helping us avoid making redundant calls to external data storage. Also, a query router can be used to pick the best prompt template for a given input. For <a id="_idIndexMarker288"/>example, in the LLM Twin use case, depending on whether the user wants an article paragraph, a post, or a code snippet, you need different prompt templates to optimize the creation process. The routing usually uses an LLM to decide what route to take or embeddings by picking the path with the most similar vectors. To summarize, query routing is identical to an if/else statement but much more versatile as it works directly with natural language.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Query rewriting</strong>: Sometimes, the <a id="_idIndexMarker289"/>user’s initial query might not perfectly align with the way your data is structured. Query rewriting tackles this by reformulating the question to match the indexed information better. This can involve techniques like:<ul>
          <li class="bulletList level-2"><strong class="keyWord">Paraphrasing</strong>: Rephrasing the user’s query while preserving its meaning (e.g., “What are the causes of climate change?” could be rewritten as “Factors contributing to global warming”).</li>
          <li class="bulletList level-2"><strong class="keyWord">Synonym substitution</strong>: Replacing less common words with synonyms to broaden the search scope (e.g., “ joyful” could be rewritten as “happy”).</li>
          <li class="bulletList level-2"><strong class="keyWord">Sub-queries</strong>: For longer queries, we can break them down into multiple shorter and more focused sub-queries. This can help the retrieval stage identify relevant documents more precisely.</li>
        </ul>
      </li>
      <li class="bulletList"><strong class="keyWord">Hypothetical document embeddings</strong> (<strong class="keyWord">HyDE</strong>): This technique involves having an LLM create a<a id="_idIndexMarker290"/> hypothetical response to the query. Then, both the original query and the LLM’s response are fed into the retrieval stage.</li>
      <li class="bulletList"><strong class="keyWord">Query expansion</strong>: This approach aims to enrich the user’s question by adding additional terms or concepts, resulting in different perspectives of the same initial question. For example, when searching for “disease,” you can leverage synonyms and related terms associated with the original query words and also include “illnesses” or “ailments.”</li>
      <li class="bulletList"><strong class="keyWord">Self-query</strong>: The core idea is to map unstructured queries into structured ones. An LLM identifies key entities, events, and relationships within the input text. These identities are used as filtering parameters to reduce the vector search space (e.g., identify cities within the query, for example, “Paris,” and add it to your filter to reduce your vector search space).</li>
    </ul>
    <p class="normal">Both data indexing and query optimization pre-retrieval optimization techniques depend highly on your data type, structure, and source. Thus, as with any data processing pipeline, no<a id="_idIndexMarker291"/> method always works, as every use case has its own particularities and gotchas. Optimizing your pre-retrieval RAG layer is experimental. Thus, what is essential is to try multiple methods (such as the ones enumerated in this section), reiterate, and observe what works best.</p>
    <h2 id="_idParaDest-105" class="heading-2">Retrieval</h2>
    <p class="normal">The retrieval step can be<a id="_idIndexMarker292"/> optimized in two fundamental ways:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Improving the embedding models</strong> used in the RAG ingestion pipeline to encode the chunked documents and, at inference time, transform the user’s input.</li>
      <li class="bulletList"><strong class="keyWord">Leveraging the DB’s filter and search features. </strong>This<strong class="keyWord"> </strong>step will be used solely at inference time when you have to retrieve the most similar chunks based on user input.</li>
    </ul>
    <p class="normal">Both strategies are aligned with our ultimate goal: to enhance the vector search step by leveraging the semantic similarity between the query and the indexed data.</p>
    <p class="normal">When improving the embedding models, you usually have to fine-tune the pre-trained embedding models to tailor them to specific jargon and nuances of your domain, especially for areas with evolving terminology or rare terms.</p>
    <p class="normal">Instead of fine-tuning the embedding model, you can leverage instructor models (<a href="https://huggingface.co/hkunlp/instructor-xl"><span class="url">https://huggingface.co/hkunlp/instructor-xl</span></a>) to guide the embedding generation process with an instruction/prompt aimed at your domain. Tailoring your embedding network to your data using such a model can be a good option, as fine-tuning a model consumes more computing and human resources.</p>
    <p class="normal">In the code snippet below, you can see an example of an Instructor model that embeds article titles about AI:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> InstructorEmbedding <span class="hljs-keyword">import</span> INSTRUCTOR
model = INSTRUCTOR(<span class="hljs-string">"hkunlp/instructor-base"</span>)
sentence = <span class="hljs-string">"</span><span class="hljs-string">RAG Fundamentals First"</span>
instruction = <span class="hljs-string">"Represent the title of an article about AI:"</span>
embeddings = model.encode([[instruction, sentence]])
<span class="hljs-built_in">print</span>(embeddings.shape) <span class="hljs-comment"># noqa</span>
<span class="hljs-comment"># Output: (1, 768)</span>
</code></pre>
    <p class="normal">The source code can be<a id="_idIndexMarker293"/> found at <a href="https://github.com/PacktPublishing/LLM-Engineering/blob/main/code_snippets/08_instructor_embeddings.py"><span class="url">https://github.com/PacktPublishing/LLM-Engineering/blob/main/code_snippets/08_instructor_embeddings.py</span></a>.</p>
    <div class="note">
      <p class="normal">To run the instructor code, you have to create a different virtual environment and activate it:</p>
    </div>
    <pre class="programlisting con"><code class="hljs-con">python3 -m venv instructor_venv &amp;&amp; source instructor_venv/bin/activate
</code></pre>
    <div class="note">
      <p class="normal">And install the required Python dependencies:</p>
    </div>
    <pre class="programlisting con"><code class="hljs-con">pip install sentence-transformers==2.2.2 InstructorEmbedding==1.0.1
</code></pre>
    <p class="normal">On the other side of the spectrum, here is how you can improve your retrieval by leveraging classic filter and search DB features:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Hybrid search</strong>: This is a vector and keyword-based search blend. Keyword-based search excels at<a id="_idIndexMarker294"/> identifying documents containing specific keywords. When your task demands pinpoint accuracy and the retrieved information must include exact keyword matches, hybrid search shines. Vector search, while powerful, can sometimes struggle with finding exact matches, but it excels at finding more general semantic similarities. You leverage both keyword matching and semantic similarities by combining the two methods. You have a parameter, usually called alpha, that controls the weight between the two methods. The algorithm has two independent searches, which are later normalized and unified.</li>
      <li class="bulletList"><strong class="keyWord">Filtered vector search</strong>: This type of search leverages the metadata index to filter for specific keywords <a id="_idIndexMarker295"/>within the metadata. It differs from a hybrid search in that you retrieve the data once using only the vector index and perform the filtering step before or after the vector search to reduce your search space.</li>
    </ul>
    <p class="normal">In practice, on the retrieval side, you usually start with filtered vector search or hybrid search, as they are fairly quick to implement. This approach gives you the flexibility to adjust your strategy <a id="_idIndexMarker296"/>based on performance. If the results are not as expected, you can always fine-tune your embedding model.</p>
    <h2 id="_idParaDest-106" class="heading-2">Post-retrieval</h2>
    <p class="normal">The post-retrieval optimizations are solely performed on the retrieved data to ensure that the LLM’s performance<a id="_idIndexMarker297"/> is not compromised by issues such as limited context windows or noisy data. This is because the retrieved context can sometimes be too large or contain irrelevant information, both of which can distract the LLM.</p>
    <p class="normal">Two popular methods performed at the post-retrieval step are:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Prompt compression</strong>: Eliminate <a id="_idIndexMarker298"/>unnecessary details while keeping the essence of the data.</li>
      <li class="bulletList"><strong class="keyWord">Re-ranking</strong>: Use a cross-encoder <a id="_idIndexMarker299"/>ML model to give a matching score between the user’s input and every retrieved chunk. The retrieved items are sorted based on this score. Only the top N results are kept as the most relevant. As you can see in <em class="italic">Figure 4.7</em>, this works because the re-ranking model can find more complex relationships between the user input and some content than a simple similarity search. However, we can’t apply this model at the initial retrieval step because it is costly. That is why a popular strategy is to retrieve the data using a similarity<a id="_idIndexMarker300"/> distance between the embeddings and refine the retrieved information using a re-raking model, as illustrated in Figure 4.8.</li>
    </ul>
    <figure class="mediaobject"><img src="../Images/B31105_04_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.7: Bi-encoder (the standard embedding model) versus cross-encoder</p>
    <p class="normal">The abovementioned techniques are far from an exhaustive list of all potential solutions. We used them <a id="_idIndexMarker301"/>as examples to get an intuition on what you can (and should) optimize at each step in your RAG workflow. The truth is that these techniques can vary tremendously by the type of data you work with. </p>
    <p class="normal">For example, if you work with multi-modal data such as text and images, most of the techniques from earlier won’t work as they are designed for text only.</p>
    <figure class="mediaobject"><img src="../Images/B31105_04_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.8: The re-ranking algorithm</p>
    <p class="normal">To summarize, the <a id="_idIndexMarker302"/>primary goal of these optimizations is to enhance the RAG algorithm at three key stages: pre-retrieval, retrieval, and post-retrieval. This involves preprocessing data for improved vector indexing, adjusting user queries for more accurate searches, enhancing the embedding model, utilizing classic filtering DB operations, and removing noisy data. By keeping these goals in mind, you can effectively optimize your RAG workflow for data processing and retrieval</p>
    <h1 id="_idParaDest-107" class="heading-1">Exploring the LLM Twin’s RAG feature pipeline architecture</h1>
    <p class="normal">Now that you have a strong intuition <a id="_idIndexMarker303"/>and understanding of RAG and its workings, we will continue exploring our particular LLM Twin use case. The goal is to provide a hands-on end-to-end example to solidify the theory presented in this chapter.</p>
    <p class="normal">Any RAG system is split into two independent components:</p>
    <ul>
      <li class="bulletList">The <strong class="keyWord">ingestion pipeline</strong> takes in <a id="_idIndexMarker304"/>raw data, cleans, chunks, embeds, and loads it into a vector DB.</li>
      <li class="bulletList">The <strong class="keyWord">inference pipeline</strong> queries the vector DB for relevant context and ultimately generates <a id="_idIndexMarker305"/>an answer by levering an LLM.</li>
    </ul>
    <p class="normal">In this chapter, we will focus on implementing the RAG ingestion pipeline, and in <em class="italic">Chapter 9</em>, we will continue developing the inference pipeline.</p>
    <p class="normal">With that in mind, let’s have a quick refresher on the problem we are trying to solve and where we get our raw data. Remember that we are building an end-to-end ML system. Thus, all the components talk to each other through an interface (or a contract), and each pipeline has a single responsibility. In our case, we ingest raw documents, preprocess them, and load them into a vector DB.</p>
    <h2 id="_idParaDest-108" class="heading-2">The problem we are solving</h2>
    <p class="normal">As presented in the previous chapter, this book aims to show you how to build a production-ready LLM Twin backed by an end-to-end ML system. In this chapter specifically, we want to design a RAG feature pipeline that takes raw social media data (e.g., articles, code<a id="_idIndexMarker306"/> repositories, and posts) from our MongoDB data warehouse. The text of the raw documents will be cleaned, chunked, embedded, and ultimately loaded to a feature store. As discussed in <em class="italic">Chapter 1</em>, we will implement a logical feature store using ZenML artifacts and a Qdrant vector DB.</p>
    <p class="normal">As we want to build a fully automated feature pipeline, we want to sync the data warehouse and logical feature store. Remember that, at inference time, the context used to generate the answer is retrieved from the vector DB. Thus, the speed of synchronization between the data warehouse and the feature store will directly impact the accuracy of our RAG algorithm.</p>
    <p class="normal">Another key consideration is how to automate the feature pipeline and integrate it with the rest of our ML system. Our goal is to minimize any desynchronization between the two data storages, as this could potentially compromise the integrity of our system.</p>
    <p class="normal">To conclude, we must design a feature pipeline that constantly syncs the data warehouse and logical feature store while processing the data accordingly. Having the data in a feature store is<a id="_idIndexMarker307"/> critical for a production-ready ML system. The LLM Twin inference pipeline will query it for RAG, while the training pipeline will consume tracked and versioned fine-tuning datasets from it.</p>
    <h2 id="_idParaDest-109" class="heading-2">The feature store</h2>
    <p class="normal">The <strong class="keyWord">feature store</strong> will be the <strong class="keyWord">central access point</strong> for all the features used within the training and inference pipelines. The training <a id="_idIndexMarker308"/>pipeline will use the cleaned data from the feature store (stored as artifacts) to fine-tune LLMs. The inference pipeline will query the vector DB for chunked<a id="_idIndexMarker309"/> documents for RAG. That is why we are designing a feature pipeline and not only a RAG ingestion pipeline. In practice, the feature pipeline contains multiple subcomponents, one of which is the RAG logic.</p>
    <p class="normal">Remember that the feature pipeline is mainly used as a mind map to navigate the complexity of ML systems. It clearly states that it takes raw data as input and then outputs features and optional labels, which are stored in the feature store. Thus, a good intuition is to consider that all the logic between the data warehouse and the feature store goes into the feature pipeline namespace, consisting of one or more sub-pipelines. For example, we will implement another pipeline that takes in cleaned data, processes it into instruct datasets, and stores it in artifacts; this also sits under the feature pipeline umbrella as the artifacts are part of the logical feature store. Another example would be implementing a data validation pipeline on top of the raw data or computed features.</p>
    <p class="normal">Another important observation to make is that text data stored as strings are not considered features if you follow the standard conventions. A feature is something that is fed directly into the model. For example, we would have to tokenize the instruct datasets or chunked documents to be considered features. Why? Because the tokens are fed directly to the model and not the sentences as strings. Unfortunately, this makes the system more complex and unflexible. Thus, we will do the tokenization at runtime. But this observation is important to understand as it’s a clear example that you don’t have to be too rigid <a id="_idIndexMarker310"/>about the feature/training/inference (FTI) architecture. You have to take it and adapt it to your own use case.</p>
    <h2 id="_idParaDest-110" class="heading-2">Where does the raw data come from?</h2>
    <p class="normal">As a quick reminder, all the raw documents are stored in a MongoDB data warehouse. The data warehouse is populated by the data collection ETL pipeline presented in <em class="italic">Chapter 3</em>. The ETL pipeline<a id="_idIndexMarker311"/> crawls various platforms such as Medium and Substack, standardizes the data, and loads it into MongoDB. Check out <em class="italic">Chapter 3</em> for more details on this topic.</p>
    <h2 id="_idParaDest-111" class="heading-2">Designing the architecture of the RAG feature pipeline</h2>
    <p class="normal">The last step is to <a id="_idIndexMarker312"/>architect and go through the design of the RAG feature pipeline of the LLM Twin application. We will use a batch design scheduled to poll data from the MongoDB data warehouse, process it, and load it to a Qdrant vector DB. The first question to ask ourselves is, “Why a batch pipeline?”</p>
    <p class="normal">But before answering that, let’s quickly understand how a batch architecture works and behaves relative to a streaming design.</p>
    <figure class="mediaobject"><img src="../Images/B31105_04_09.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.9: The architecture of the LLM Twin’s RAG feature pipeline</p>
    <h3 id="_idParaDest-112" class="heading-3">Batch pipelines</h3>
    <p class="normal">A batch pipeline in <a id="_idIndexMarker313"/>data systems refers to a data processing method where data is collected, processed, and stored in predefined intervals and larger volumes, also known as “batches”. This approach differs from real-time or streaming data processing, where data is processed continuously as it arrives. This is what happens in a batch pipeline:</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">Data collection</strong>: Data is collected from various sources and stored until sufficient amounts are accumulated for processing. This can include data from DBs, logs, files, and other sources.</li>
      <li class="numberedList"><strong class="keyWord">Scheduled processing</strong>: Data processing is scheduled at regular intervals, for example, hourly or daily. During this time, the collected data is processed in bulk. This can involve data cleansing, transformation, aggregation, and other operations.</li>
      <li class="numberedList"><strong class="keyWord">Data loading</strong>: After processing, the data is loaded into the target system, such as a DB, data warehouse, data lake, or feature store. This processed data is then available for analysis, querying, or further processing.</li>
    </ol>
    <p class="normal">Batch pipelines are particularly <a id="_idIndexMarker314"/>useful when dealing with large volumes of data that do not require immediate processing. They offer several advantages, including:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Efficiency</strong>: Batch processing can handle large volumes of data more efficiently than real-time processing, allowing for optimized resource allocation and parallel processing.</li>
      <li class="bulletList"><strong class="keyWord">Complex processing</strong>: Batch pipelines can perform complex data transformations and aggregations that might be too resource-intensive for real-time processing.</li>
      <li class="bulletList"><strong class="keyWord">Simplicity</strong>: Batch processing systems’ architectures are often simpler than those of real-time systems, making them easier to implement and maintain.</li>
    </ul>
    <h3 id="_idParaDest-113" class="heading-3">Batch versus streaming pipelines</h3>
    <p class="normal">When implementing feature pipelines, you have two main design choices: batch and streaming. Thus, it is worthwhile to see the difference between the two and understand why we chose a batch architecture over a streaming one for our LLM Twin use case.</p>
    <p class="normal">You can effortlessly<a id="_idIndexMarker315"/> write a dedicated chapter on streaming pipelines, which suggests its complexity over a batch design. However, as streaming architectures become increasingly popular, one must have an intuition of how they work to choose the best option for your application.</p>
    <p class="normal">The core elements of streaming applications are a distributed event streaming platform such as Apache Kafka or Redpanda to store events from multiple clients and a streaming engine such as Apache Flink or Bytewax to process the events. To simplify your architecture, you can swap your event streaming platform with queues, such as RabbitMQ, to store the events until processed. <em class="italic">Table 4.1</em> compares batch and streaming pipelines based on multiple<a id="_idIndexMarker316"/> criteria such as processing schedule and complexity:</p>
    <table id="table001" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Aspect</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Batch pipeline</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Streaming pipeline</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Processing schedule</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="url">Processes data at regular intervals (e.g., every minute, hourly, daily).</span></p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="url">Processes data continuously, with minimal latency.</span></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Efficiency</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="url">Handles large volumes of data more efficiently, optimizing resource allocation and parallel processing.</span></p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="url">Handles single data points, providing immediate insights and updates, allowing for rapid response to changes.</span></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Processing complexity</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="url">Capable of performing complex data transformations and aggregations.</span></p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="url">Designed to handle high-velocity data streams with low latency.</span></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Use cases</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="url">Suitable for scenarios where immediate data processing is not critical. Commonly used in data warehousing, reporting, ETL processes, and feature pipelines.</span></p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="url">Ideal for applications requiring real-time analytics, features, monitoring, and event-driven architectures.</span></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">System complexity</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="url">Compared to streaming pipelines, systems are generally simpler to implement and maintain.</span></p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="url">More complex to implement and maintain due to the need for low-latency processing, fault tolerance, and scalability. The tooling is also more advanced and complicated.</span></p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 4.1: Batch versus streaming pipelines</p>
    <p class="normal">For example, streaming pipelines are extremely powerful in social media recommender systems like TikTok. When using social media, user behavior changes frequently. A typical scenario is that you want to relax at a certain point in time and mostly look at videos of puppies. Still, after 15 minutes, you get bored and want something more serious, such as educative content or news. This means the recommender system has to capture these behavior changes without delay to keep you engaged. As the transition between interests is cyclical and not predictable, you can’t use a batch pipeline that runs every 30 minutes or every hour to generate more content. You can run it every minute to create new content, but, at the same time, it will result in unnecessary costs, as most predictions will not be<a id="_idIndexMarker317"/> consumed. By implementing a streaming pipeline, you update the features of specific users in real time, which are then passed to a chain of models that predict the new recommendations.</p>
    <p class="normal">Streaming architectures are also the backbone of real-time fraud detection algorithms, such as those used at Stripe or PayPal. In this context, it’s critical to identify potentially fraudulent transactions as they occur, not after a few minutes or hours as a batch pipeline would process them. The same urgency applies to high-frequency trading platforms that make stock predictions based on the constant influx of market data, enabling traders to make decisions within milliseconds.</p>
    <p class="normal">On the other hand, you can use a batch architecture for an offline recommender system. For example, when implementing one for an e-commerce or streaming platform, you don’t need the system to be so reactive, as the user’s behavior rarely changes. Thus, updating the recommendations periodically, such as every night, based on historical user behavior data using a batch pipeline is easier to implement and cheaper.</p>
    <p class="normal">Another popular example of batch pipelines is the ETL design used to extract, transform, and load data for different use cases. The ETL design is widespread in data pipelines used to move data from one DB to another. Some practical use cases include aggregating data for analytics, where you have to extract data from multiple sources, aggregate it, and load it to a data warehouse connected to a dashboard. The analytics domains can be widespread, from e-commerce and marketing to finance and research.</p>
    <p class="normal">The data collection pipeline used in the LLM Twin use case is another example of an ETL pipeline that extracts data from the internet, structures it, and loads it into a data warehouse for future processing.</p>
    <p class="normal">Along with prediction or feature freshness, another disadvantage of batch pipelines over streaming ones is that you usually make redundant predictions. Let’s take the example of a recommender system for a streaming platform like Netflix. Every night, you make the predictions for all users. There is a significant chance that a large chunk of users won’t log in that day. Also, users <a id="_idIndexMarker318"/>usually don’t browse all the recommendations but stick to the first ones. Thus, only a portion of predictions are used, wasting computing power on all the others.</p>
    <p class="normal">That’s why a popular strategy is to start with a batch architecture, as it’s faster and easier to implement. After the product is in place, you gradually move to a streaming design to reduce costs and improve the user experience.</p>
    <p class="normal">To conclude, we have used a batch architecture (and not a streaming one) to implement the LLM Twin’s feature pipeline for the following reasons:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Does not require immediate data processing</strong>: Even if syncing the data warehouse and feature store is critical for an accurate RAG system, a delay of a few minutes is acceptable. Thus, we can schedule the batch pipeline to run every minute, constantly syncing the two data storages. This technique works because the data volume is small. The whole data warehouse will have only thousands of records, not millions or billions. Hence, we can quickly iterate through them and sync the two DBs.</li>
      <li class="bulletList"><strong class="keyWord">Simplicity</strong>: As stated earlier, implementing a streaming pipeline is two times more complex. In the real world, you want to keep your system as simple as possible, making it easier to understand, debug, and maintain. Also, simplicity usually translates to lower infrastructure and development costs.</li>
    </ul>
    <p class="normal">In <em class="italic">Figure 8.10</em>, we compare what tools you can use based on your architecture (streaming versus batch) and the quantity of data you have to process (small versus big data). In our use case, we are in the smaller data and batch quadrant, where we picked a combination of vanilla Python and generative AI tools such as LangChain, Sentence Transformers, and Unstructured.</p>
    <figure class="mediaobject"><img src="../Images/B31105_04_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.10: Tools on the streaming versus batch and smaller versus bigger data spectrum</p>
    <p class="normal">In the <em class="italic">Change data capture: syncing the data warehouse and feature store</em><code class="inlineCode"> </code>section later in this chapter, we will discuss <a id="_idIndexMarker319"/>when switching from a batch architecture to a streaming one makes sense.</p>
    <h3 id="_idParaDest-114" class="heading-3">Core steps</h3>
    <p class="normal">Most of the RAG feature<a id="_idIndexMarker320"/> pipelines are composed of five core steps. The one implemented in the LLM Twin architecture makes no exception. Thus, you can quickly adapt this pattern for other RAG applications, but here is what the LLM Twin’s RAG feature pipeline looks like:</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">Data extraction</strong>: Extract the<a id="_idIndexMarker321"/> latest articles, code repositories, and posts from the MongoDB data warehouse. At the extraction step, you usually aggregate all the data you need for processing.</li>
      <li class="numberedList"><strong class="keyWord">Cleaning</strong>: The data from the data warehouse is standardized and partially clean, but we have to ensure that the text contains only useful information, is not duplicated, and can be interpreted by the embedding model. For example, we must <a id="_idIndexMarker322"/>clean and normalize all non-ASCII characters before passing the text to the embedding model. Also, to keep the information semantically dense, we decided to replace all the URLs with placeholders and remove all emojis. The cleaning step is more art than <strong class="keyWord">science</strong>. Hence, after you have the first iteration with an evaluation mechanism in place, you will probably reiterate and improve it.</li>
      <li class="numberedList"><strong class="keyWord">Chunking</strong>: You must adopt various chunking strategies based on each data category and embedding model. For example, when working with code repositories, you<a id="_idIndexMarker323"/> want the chunks broader, whereas when working with articles, you want them narrower or scoped at the paragraph level. Depending on your data, you must decide if you split your document based on the chapter, section, paragraph, sentence, or just a fixed window size. Also, you have to ensure that the chunk size doesn’t exceed the maximum input size of the embedding model. That is why you usually chunk a document based on your data structure and the maximum input size of the model.</li>
      <li class="numberedList"><strong class="keyWord">Embedding</strong>: You pass each <a id="_idIndexMarker324"/>chunk individually to an embedding model of your choice. Implementation-wise, this step is usually the simplest, as tools such as SentenceTransformer and Hugging Face provide high-level interfaces for most embedding models. As explained in the <em class="italic">What are embeddings?</em> section of this chapter, at this step, the most critical decisions are to decide what model to use and whether to fine-tune it or not. For example, we used an <code class="inlineCode">"all-mpnet-base-v2"</code> embedding model from <em class="italic">SentenceTransformer</em>, which is relatively tiny and runs on most machines. However, we provide a configuration file where you can quickly configure the embedding model with something more powerful based on the state of the art when reading this book. You can quickly find other options on the MTEB on Hugging Face (<a href="https://huggingface.co/spaces/mteb/leaderboard"><span class="url">https://huggingface.co/spaces/mteb/leaderboard</span></a>).</li>
      <li class="numberedList"><strong class="keyWord">Data loading</strong>: The final step <a id="_idIndexMarker325"/>combines the embedding of a chunked document and its metadata, such as the author and the document ID, content, URL, platform, and creation date. Ultimately, we wrap the vector and the metadata into a structure compatible with Qdrant and push it to the vector DB. As we want to use Qdrant as the single source of truth for the features, we also push the cleaned documents (before chunking) to Qdrant. We can push data without vectors, as the metadata <a id="_idIndexMarker326"/>index of Qdrant behaves like a NoSQL DB. Thus, pushing metadata without a vector attached to it is like using a standard NoSQL engine.</li>
    </ol>
    <h3 id="_idParaDest-115" class="heading-3">Change data capture: syncing the data warehouse and feature store</h3>
    <p class="normal">As highlighted a few times in this chapter, data is constantly changing, which can result in DBs, data lakes, data warehouses, and feature<a id="_idIndexMarker327"/> stores getting out of sync. <strong class="keyWord">Change data capture</strong> (<strong class="keyWord">CDC</strong>) is a strategy that allows <a id="_idIndexMarker328"/>you to optimally keep two or more data storage types in sync without computing and I/O overhead. It captures any CRUD operation done on the source DB and replicates it on a target DB. Optionally, you can add preprocessing steps in between the replication.</p>
    <p class="normal">The syncing issues also apply when building a feature pipeline. One key design choice concerns how to sync the data warehouse with the feature store to have data fresh enough for your particular use case.</p>
    <p class="normal">In our LLM Twin use case, we chose a naïve approach out of simplicity. We implemented a batch pipeline that is triggered periodically or manually. It reads all the raw data from the data warehouse, processes it in batches, and inserts new records or updates old ones from the Qdrant vector DB. This works fine when you are working with a small number of records, at the order of thousands or tens of thousands. But our naïve approach raises the following questions:</p>
    <ul>
      <li class="bulletList">What happens if the data suddenly grows to millions of records (or higher)?</li>
      <li class="bulletList">What happens if a record is deleted from the data warehouse? How is this reflected in the feature store?</li>
      <li class="bulletList">What if we want to process only the new or updated items from the data warehouse and not all of them?</li>
    </ul>
    <p class="normal">Fortunately, the CDC pattern can solve all of these issues. When implementing CDC, you can take multiple approaches, but all of them use either a push or pull strategy:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Push:</strong> The source DB is the primary driver in the push approach. It actively identifies and transmits data modifications to target systems for processing. This method ensures near-instantaneous<a id="_idIndexMarker329"/> updates at the target, but data loss can occur if target systems are inaccessible. To mitigate this, a messaging system is typically employed as a buffer.</li>
      <li class="bulletList"><strong class="keyWord">Pull:</strong> The pull method <a id="_idIndexMarker330"/>assigns a more passive role to the source DB, which only records data changes. Target systems periodically request these changes and handle updates accordingly. While this approach lightens the load on the source, it introduces a delay in data propagation. A messaging system is again essential to prevent data loss during periods of target system unavailability.</li>
    </ul>
    <p class="normal">In summary, the push method is ideal for <a id="_idIndexMarker331"/>applications demanding immediate data access, whereas the pull method is better suited for large-scale data transfers where real-time updates aren’t critical. With that in mind, there are different methods to detect changes in data. Thus, let’s list the main CDC patterns that are used in the industry:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Timestamp-based</strong>: The approach involves adding a modification time column to DB tables, usually called <code class="inlineCode">LAST_MODIFIED</code> or <code class="inlineCode">LAST_UPDATED</code>. Downstream systems can query this column to identify records that have been updated since their <a id="_idIndexMarker332"/>last check. While simple to implement, this method is limited to tracking changes, not deletions, and imposes performance overhead due to the need to scan entire tables.</li>
      <li class="bulletList"><strong class="keyWord">Trigger-based</strong>: The trigger-based approach utilizes DB triggers to automatically record data<a id="_idIndexMarker333"/> modifications in a separate table upon INSERT, UPDATE, or DELETE operations, often known as the event table. This method provides comprehensive change tracking but can impact the DB performance due to the additional write operations involved for each event.</li>
      <li class="bulletList"><strong class="keyWord">Log-based</strong>: DBs maintain transaction logs to record all data modifications, including timestamps. Primarily <a id="_idIndexMarker334"/>used for recovery, these logs can also be leveraged to propagate changes to target systems in real time. This approach minimizes the performance impact on the source DB. As a huge advantage, it avoids additional processing overhead on the source DB, captures all data changes, and requires no schema modification. But on the opposite side, it lacks standardized log formats, leading to vendor-specific implementations.</li>
    </ul>
    <div class="note">
      <p class="normal">For more details on CDC, I recommend <em class="italic">What is Change Data Capture?</em> from Confluent’s blog: <a href="https://www.confluent.io/en-gb/learn/change-data-capture/"><span class="url">https://www.confluent.io/en-gb/learn/change-data-capture/</span></a>.</p>
    </div>
    <p class="normal">With these CDC techniques in mind, we could quickly implement a pull timestamp-based strategy in our RAG feature pipeline to sync the data warehouse and feature store more optimally when the data grows. Our implementation is still pull-based but doesn’t check any last updated field in the source DB; it just pulls everything from the data warehouse.</p>
    <p class="normal">However, the most popular <a id="_idIndexMarker335"/>and optimal technique in the industry is the log-based one. It doesn’t add any I/O overhead to the source DB, has low latency, and supports all CRUD operations. The biggest downside is its development complexity, which requires a queue to capture all the CRUD events and a streaming pipeline to process them.</p>
    <p class="normal">As this is an LLM book and not a data engineering one, we wanted to keep things simple, but it’s important to know that these techniques exist, and you can always upgrade your current implementation when it doesn’t fit your application requirements anymore.</p>
    <h3 id="_idParaDest-116" class="heading-3">Why is the data stored in two snapshots?</h3>
    <p class="normal">We store two snapshots of our <a id="_idIndexMarker336"/>data in the logical feature store:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">After the data is cleaned</strong>: For fine-tuning LLMs</li>
      <li class="bulletList"><strong class="keyWord">After the documents are chunked and embedded</strong>: For RAG</li>
    </ul>
    <p class="normal"><em class="italic">Why did we design it this way?</em> Remember that the features should be accessed solely from the feature store for training and inference. Thus, this adds consistency to our design and makes it cleaner.</p>
    <p class="normal">Also, storing the data cleaned specifically for our fine-tuning and embedding use case in the MongoDB data warehouse would have been an antipattern. The data from the warehouse is shared all across the company. Thus, processing it for a specific use case is not good practice. Imagine another summarization use case where we must clean and preprocess the data differently. We must create a new “Cleaned Data” table prefixed with the use case name. We have to repeat that for every new use case. Therefore, to avoid having a spaghetti data warehouse, the data from the data warehouse is generic and is modeled to specific applications only in downstream components, which, in our case, is the feature store.</p>
    <p class="normal">Ultimately, as we mentioned in the <em class="italic">Core steps</em> section, you can leverage the metadata index of a vector DB as a NoSQL DB. Based on these factors, we decided to keep the cleaned data in Qdrant, along with the chunked and embedded versions of the documents.</p>
    <p class="normal">As a quick reminder, when operationalizing our LLM Twin system, the create instruct dataset pipeline, explained in <em class="italic">Chapter 5</em>, will read the cleaned documents from Qdrant, process them, and save them under a versioned ZenML artifact. The training pipeline requires a dataset and <a id="_idIndexMarker337"/>not plain documents. This is a reminder that our logical feature store comprises the Qdrant vector DB for online serving and ZenML artifacts for offline training.</p>
    <h3 id="_idParaDest-117" class="heading-3">Orchestration</h3>
    <p class="normal">ZenML will orchestrate the batch RAG feature pipeline. Using ZenML, we can schedule it to run on a schedule, for example, every hour, or quickly manually trigger it. Another option is to trigger it after the ETL data collection pipeline finishes.</p>
    <p class="normal">By orchestrating the feature <a id="_idIndexMarker338"/>pipeline and integrating it into ZenML (or any other orchestration tool), we can operationalize the feature pipeline with the end<a id="_idIndexMarker339"/> goal of <strong class="keyWord">continuous training</strong> (<strong class="keyWord">CT</strong>).</p>
    <p class="normal">We will go into all the details of orchestration, scheduling, and CT in <em class="italic">Chapter 11</em>.</p>
    <h1 id="_idParaDest-118" class="heading-1">Implementing the LLM Twin’s RAG feature pipeline</h1>
    <p class="normal">The last step is to review the LLM Twin’s RAG feature pipeline code to see how we applied everything we <a id="_idIndexMarker340"/>discussed in this chapter. We will walk you through the following:</p>
    <ul>
      <li class="bulletList">ZenML code</li>
      <li class="bulletList">Pydantic domain objects</li>
      <li class="bulletList">A custom <strong class="keyWord">object-vector mapping</strong> (<strong class="keyWord">OVM</strong>) implementation</li>
      <li class="bulletList">The <a id="_idIndexMarker341"/>cleaning, chunking, and <a id="_idIndexMarker342"/>embedding logic for all our data categories</li>
    </ul>
    <p class="normal">We will take a top-down approach. Thus, let’s start with the Settings class and ZenML pipeline.</p>
    <h2 id="_idParaDest-119" class="heading-2">Settings</h2>
    <p class="normal">We use Pydantic Settings (<a href="https://docs.pydantic.dev/latest/concepts/pydantic_settings/"><span class="url">https://docs.pydantic.dev/latest/concepts/pydantic_settings/</span></a>) to define a global Settings class<a id="_idIndexMarker343"/> that loads sensitive or non-sensitive <a id="_idIndexMarker344"/>variables from a <code class="inlineCode">.env</code> file. This approach also gives us all the benefits of Pydantic, such as type validation. For example, if we provide a string for the <code class="inlineCode">QDRANT_DATABASE_PORT</code> variable instead <a id="_idIndexMarker345"/>of an integer, the program will crash. This behavior makes the whole application more deterministic and reliable.</p>
    <p class="normal">Here is what the <code class="inlineCode">Settings</code> class looks like with all the variables necessary to build the RAG feature pipeline:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> pydantic <span class="hljs-keyword">import</span> BaseSettings
<span class="hljs-keyword">class</span> <span class="hljs-title">Settings</span>(<span class="hljs-title">BaseSettings</span>):
    <span class="hljs-keyword">class</span> <span class="hljs-title">Config</span>:
        env_file = <span class="hljs-string">".env"</span>
        env_file_encoding = <span class="hljs-string">"utf-8"</span>
    … <span class="hljs-comment"># Some other settings…</span>
    <span class="hljs-comment"># RAG</span>
    TEXT_EMBEDDING_MODEL_ID: <span class="hljs-built_in">str</span> = <span class="hljs-string">"sentence-transformers/all-MiniLM-L6-v2"</span>
    RERANKING_CROSS_ENCODER_MODEL_ID: <span class="hljs-built_in">str</span> = <span class="hljs-string">"cross-encoder/ms-marco-MiniLM-L-4-v2"</span>
    RAG_MODEL_DEVICE: <span class="hljs-built_in">str</span> = <span class="hljs-string">"cpu"</span>
    <span class="hljs-comment"># QdrantDB Vector DB</span>
    USE_QDRANT_CLOUD: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>
    QDRANT_DATABASE_HOST: <span class="hljs-built_in">str</span> = <span class="hljs-string">"localhost"</span>
    QDRANT_DATABASE_PORT: <span class="hljs-built_in">int</span> = <span class="hljs-number">6333</span>
    QDRANT_CLOUD_URL: <span class="hljs-built_in">str</span> = <span class="hljs-string">"str"</span>
    QDRANT_APIKEY: <span class="hljs-built_in">str</span> | <span class="hljs-literal">None</span> = <span class="hljs-literal">None</span>
    … <span class="hljs-comment"># More settings…</span>
settings = Settings()
</code></pre>
    <p class="normal">As stated in the internal Config class, all the variables have default values or can be overridden by <a id="_idIndexMarker346"/>providing a <code class="inlineCode">.env</code> file.</p>
    <h2 id="_idParaDest-120" class="heading-2">ZenML pipeline and steps</h2>
    <p class="normal">The ZenML pipeline is the entry point for the RAG feature engineering pipeline. It reflects the five core phases of RAG <a id="_idIndexMarker347"/>ingestion code: extracting raw documents, cleaning, chunking, embedding, and loading them to the logical <a id="_idIndexMarker348"/>feature store. The calls within the <code class="inlineCode">feature_engineering()</code> function are ZenML steps, representing a single execution unit performing the five phases of RAG. The code is available in the GitHub repository at <a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/pipelines/feature_engineering.py"><span class="url">https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/pipelines/feature_engineering.py</span></a>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> zenml <span class="hljs-keyword">import</span> pipeline
<span class="hljs-keyword">from</span> llm_engineering.interfaces.orchestrator.steps <span class="hljs-keyword">import</span> feature_engineering <span class="hljs-keyword">as</span> fe_steps
<span class="hljs-meta">@pipeline</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">feature_engineering</span>(<span class="hljs-params">author_full_names: </span><span class="hljs-built_in">list</span><span class="hljs-params">[</span><span class="hljs-built_in">str</span><span class="hljs-params">]</span>) -&gt; <span class="hljs-literal">None</span>:
    raw_documents = fe_steps.query_data_warehouse(author_full_names)
    cleaned_documents = fe_steps.clean_documents(raw_documents)
     last_step_1 = fe_steps.load_to_vector_db(cleaned_documents)
    embedded_documents = fe_steps.chunk_and_embed(cleaned_documents)
    last_step_2 = fe_steps.load_to_vector_db(embedded_documents)
    <span class="hljs-keyword">return</span> [last_step_1.invocation_id, last_step_2.invocation_id]
</code></pre>
    <p class="normal"><em class="italic">Figure 4.11</em> shows how multiple feature engineering pipeline runs look in ZenML’s dashboard.</p>
    <figure class="mediaobject"><img src="../Images/B31105_04_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.11: Feature pipeline runs in the ZenML dashboard</p>
    <p class="normal"><em class="italic">Figure 8.12</em> shows the DAG of the RAG feature pipeline, where you can follow all the pipeline steps and their <a id="_idIndexMarker349"/>output artifacts. Remember <a id="_idIndexMarker350"/>that whatever is returned from a ZenML step is automatically saved as an artifact, stored in ZenML’s artifact registry, versioned, and shareable across the application.</p>
    <figure class="mediaobject"><img src="../Images/B31105_04_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.12: Feature pipeline DAG in the ZenML dashboard</p>
    <p class="normal">The final puzzle piece is understanding how to configure the RAG feature pipeline dynamically. All its available settings are exposed as function parameters. Here, we need only a list of author’s names, as seen in the function’s signature: <code class="inlineCode">feature_engineering(author_full_names: list[str])</code>. We inject a YAML configuration file at<a id="_idIndexMarker351"/> runtime that contains all the necessary values based on different use cases. For example, the following configuration includes a list of all the authors of this book as we want to populate the feature store with data from all of us (available in the GitHub repository at <code class="inlineCode">configs/feature_engineering.yaml</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">parameters:
  author_full_names:
    - Alex Vesa
    - Maxime Labonne
    - Paul Iusztin
</code></pre>
    <p class="normal">The beauty of this approach is that you don’t have to modify the code to configure the feature pipeline with different input values. You have to provide a different configuration file when running it, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">feature_engineering.with_options(config_path="…/feature_engineering.yaml")()
</code></pre>
    <p class="normal">You can either hardcode<a id="_idIndexMarker352"/> the path to the config file or provide the <code class="inlineCode">config_path</code> from the CLI, which allows you to modify the pipeline’s configuration between different runs. Out of simplicity, we hard-coded the configuration file. Thus, we can call the feature engineering pipeline calling the <code class="inlineCode">run.py</code> script as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">python -m tools.run --no-cache --run-feature-engineering
</code></pre>
    <p class="normal">However, you can easily add another CLI argument to pass the <code class="inlineCode">config_path</code> variable. Also, you can run the feature pipeline using the following <code class="inlineCode">poe</code> command:</p>
    <pre class="programlisting con"><code class="hljs-con">poetry poe run-feature-engineering-pipeline
</code></pre>
    <p class="normal">Let’s move forward to the ZenML steps and sequentially zoom in on all of them. The source code for all the feature engineering pipeline steps is available on GitHub at <code class="inlineCode">"steps/feature_engineering"</code>. We will begin with the first step, which involves querying the data warehouse for new content to process into features.</p>
    <h3 id="_idParaDest-121" class="heading-3">Querying the data warehouse</h3>
    <p class="normal">The first thing to notice is that<a id="_idIndexMarker353"/> a step is a Python function decorated with <code class="inlineCode">@step</code>, similar to how a ZenML pipeline works. The function below takes as input a list of authors’ full names and performs the following core steps:</p>
    <ul>
      <li class="bulletList">It attempts to get or create a <code class="inlineCode">UserDocument</code> instance using the first and last names, appending this instance to the <span class="url">authors</span> list. If the user doesn’t exist, it throws an error.</li>
      <li class="bulletList">It fetches all the raw data for the user from the data warehouse and extends the <code class="inlineCode">documents</code> list to include these user documents.</li>
      <li class="bulletList">Ultimately, it computes a descriptive metadata dictionary logged and tracked in ZenML.</li>
    </ul>
    <pre class="programlisting code"><code class="hljs-code">… <span class="hljs-comment"># other imports</span>
<span class="hljs-keyword">from</span> zenml <span class="hljs-keyword">import</span> get_step_context, step
<span class="hljs-meta">@step</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">query_data_warehouse</span>(
<span class="hljs-params">    author_full_names: </span><span class="hljs-built_in">list</span><span class="hljs-params">[</span><span class="hljs-built_in">str</span><span class="hljs-params">],</span>
) -&gt; Annotated[<span class="hljs-built_in">list</span>, <span class="hljs-string">"raw_documents"</span>]:
    documents = []
    authors = []
    <span class="hljs-keyword">for</span> author_full_name <span class="hljs-keyword">in</span> author_full_names:
        logger.info(<span class="hljs-string">f"Querying data warehouse for user: </span><span class="hljs-subst">{author_full_name}</span><span class="hljs-string">"</span>)
        first_name, last_name = utils.split_user_full_name(author_full_name)
        logger.info(<span class="hljs-string">f"First name: </span><span class="hljs-subst">{first_name}</span><span class="hljs-string">, Last name: </span><span class="hljs-subst">{last_name}</span><span class="hljs-string">"</span>)
        user = UserDocument.get_or_create(first_name=first_name, last_name=last_name)
        authors.append(user)
        results = fetch_all_data(user)
        user_documents = [doc <span class="hljs-keyword">for</span> query_result <span class="hljs-keyword">in</span> results.values() <span class="hljs-keyword">for</span> doc <span class="hljs-keyword">in</span> query_result]
        documents.extend(user_documents)
    step_context = get_step_context()
    step_context.add_output_metadata(output_name=<span class="hljs-string">"raw_documents"</span>, metadata=_get_metadata(documents))
    <span class="hljs-keyword">return</span> documents
</code></pre>
    <p class="normal">The fetch function leverages a thread pool that runs each query on a different thread. As we have multiple data categories, we have to make a different query for the articles, posts, and repositories, as<a id="_idIndexMarker354"/> they are stored in different collections. Each query calls the data warehouse, which is bounded by the network I/O and data warehouse latency, not by the machine’s CPU. Thus, by moving each query to a different thread, we can parallelize them. Ultimately, instead of adding the latency of each query as the total timing, the time to run this fetch function will be the max between all the calls.</p>
    <p class="normal">Using threads to parallelize I/O-bounded calls is good practice in Python, as they are not locked by the Python <strong class="keyWord">Global Interpreter Lock</strong> (<strong class="keyWord">GIL</strong>). In contrast, adding each call to a different process would add too much <a id="_idIndexMarker355"/>overhead, as a process takes longer to spin off than a thread.</p>
    <p class="normal">In Python, you want to parallelize things with processes only when the operations are CPU or memory-bound<a id="_idIndexMarker356"/> because the GIL affects them. Each process has a different GIL. Thus, parallelizing your computing logic, such as processing a batch of documents or images already loaded in memory, isn’t affected by Python’s GIL limitations.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">fetch_all_data</span>(<span class="hljs-params">user: UserDocument</span>) -&gt; <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">list</span>[NoSQLBaseDocument]]:
    user_id = <span class="hljs-built_in">str</span>(user.<span class="hljs-built_in">id</span>)
    <span class="hljs-keyword">with</span> ThreadPoolExecutor() <span class="hljs-keyword">as</span> executor:
        future_to_query = {
            executor.submit(__fetch_articles, user_id): <span class="hljs-string">"articles"</span>,
            executor.submit(__fetch_posts, user_id): <span class="hljs-string">"posts"</span>,
            executor.submit(__fetch_repositories, user_id): <span class="hljs-string">"repositories"</span>,
        }
        results = {}
        <span class="hljs-keyword">for</span> future <span class="hljs-keyword">in</span> as_completed(future_to_query):
            query_name = future_to_query[future]
            <span class="hljs-keyword">try</span>:
                results[query_name] = future.result()
            <span class="hljs-keyword">except</span> Exception:
                logger.exception(<span class="hljs-string">f"'</span><span class="hljs-subst">{query_name}</span><span class="hljs-string">' request failed."</span>)
                results[query_name] = []
    <span class="hljs-keyword">return</span> results
</code></pre>
    <p class="normal">The <code class="inlineCode">_get_metadata()</code> function takes the list of queried documents and authors and counts the number of them relative to each data category:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">_get_metadata</span>(<span class="hljs-params">documents: </span><span class="hljs-built_in">list</span><span class="hljs-params">[Document]</span>) -&gt; <span class="hljs-built_in">dict</span>:
    metadata = {
        <span class="hljs-string">"num_documents"</span>: <span class="hljs-built_in">len</span>(documents),
    }
    <span class="hljs-keyword">for</span> document <span class="hljs-keyword">in</span> documents:
        collection = document.get_collection_name()
        <span class="hljs-keyword">if</span> collection <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> metadata:
            metadata[collection] = {}
        <span class="hljs-keyword">if</span> <span class="hljs-string">"authors"</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> metadata[collection]:
            metadata[collection][<span class="hljs-string">"authors"</span>] = <span class="hljs-built_in">list</span>()
        metadata[collection][<span class="hljs-string">"</span><span class="hljs-string">num_documents"</span>] = metadata[collection].get(<span class="hljs-string">"num_documents"</span>, <span class="hljs-number">0</span>) + <span class="hljs-number">1</span>
        metadata[collection][<span class="hljs-string">"authors"</span>].append(document.author_full_name)
    <span class="hljs-keyword">for</span> value <span class="hljs-keyword">in</span> metadata.values():
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(value, <span class="hljs-built_in">dict</span>) <span class="hljs-keyword">and</span> <span class="hljs-string">"authors"</span> <span class="hljs-keyword">in</span> value:
            value[<span class="hljs-string">"authors"</span>] = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(value[<span class="hljs-string">"authors"</span>]))
    <span class="hljs-keyword">return</span> metadata
</code></pre>
    <p class="normal">We will expose this metadata<a id="_idIndexMarker357"/> in the ZenML dashboard to quickly see some statistics on the loaded data. For example, in <em class="italic">Figure 4.13</em>, we accessed the metadata tab of the <code class="inlineCode">query_data_warehouse()</code> step, where you can see that, within that particular run of the feature pipeline, we loaded 76 documents from three authors. This is extremely powerful for monitoring and debugging batch pipelines. </p>
    <p class="normal">You can always extend it with anything that makes sense for your use case.</p>
    <figure class="mediaobject"><img src="../Images/B31105_04_13.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.13: Metadata of the “query the data warehouse” ZenML step</p>
    <h3 id="_idParaDest-122" class="heading-3">Cleaning the documents</h3>
    <p class="normal">In the cleaning step, we iterate through all <a id="_idIndexMarker358"/>the documents and delegate all the logic to a <code class="inlineCode">CleaningDispatcher</code> who knows what cleaning logic to apply based on the data category. Remember that we want to apply, or have the ability to apply in the future, different cleaning techniques on articles, posts, and code repositories.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@step</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">clean_documents</span>(
<span class="hljs-params">    documents: Annotated[</span><span class="hljs-built_in">list</span><span class="hljs-params">, </span><span class="hljs-string">"</span><span class="hljs-string">raw_documents"</span><span class="hljs-params">],</span>
) -&gt; Annotated[<span class="hljs-built_in">list</span>, <span class="hljs-string">"cleaned_documents"</span>]:
    cleaned_documents = []
    <span class="hljs-keyword">for</span> document <span class="hljs-keyword">in</span> documents:
        cleaned_document = CleaningDispatcher.dispatch(document)
        cleaned_documents.append(cleaned_document)
    step_context = get_step_context()
    step_context.add_output_metadata(output_name=<span class="hljs-string">"cleaned_documents"</span>, metadata=_get_metadata(cleaned_documents))
    <span class="hljs-keyword">return</span> cleaned_documents
</code></pre>
    <p class="normal">The computed metadata is similar to what we logged in the <code class="inlineCode">query_data_warehouse()</code> step. Thus, let’s move <a id="_idIndexMarker359"/>on to chunking and embedding.</p>
    <h3 id="_idParaDest-123" class="heading-3">Chunk and embed the cleaned documents</h3>
    <p class="normal">Similar to how we cleaned the documents, we delegate the chunking and embedding logic to a dispatcher who knows how to handle each data category. Note that the chunking dispatcher returns a list<a id="_idIndexMarker360"/> instead of a single object, which <a id="_idIndexMarker361"/>makes sense as the document is split into multiple chunks. We will dig into the dispatcher in the “The dispatcher layer” section of this chapter.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@step</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">chunk_and_embed</span>(
<span class="hljs-params">    cleaned_documents: Annotated[</span><span class="hljs-built_in">list</span><span class="hljs-params">, </span><span class="hljs-string">"cleaned_documents"</span><span class="hljs-params">],</span>
) -&gt; Annotated[<span class="hljs-built_in">list</span>, <span class="hljs-string">"embedded_documents"</span>]:
    metadata = {<span class="hljs-string">"chunking"</span>: {}, <span class="hljs-string">"embedding"</span>: {}, <span class="hljs-string">"num_documents"</span>: <span class="hljs-built_in">len</span>(cleaned_documents)}
    embedded_chunks = []
    <span class="hljs-keyword">for</span> document <span class="hljs-keyword">in</span> cleaned_documents:
        chunks = ChunkingDispatcher.dispatch(document)
        metadata[<span class="hljs-string">"chunking"</span>] = _add_chunks_metadata(chunks, metadata[<span class="hljs-string">"chunking"</span>])
        <span class="hljs-keyword">for</span> batched_chunks <span class="hljs-keyword">in</span> utils.misc.batch(chunks, <span class="hljs-number">10</span>):
            batched_embedded_chunks = EmbeddingDispatcher.dispatch(batched_chunks)
            embedded_chunks.extend(batched_embedded_chunks)
    metadata[<span class="hljs-string">"embedding"</span>] = _add_embeddings_metadata(embedded_chunks, metadata[<span class="hljs-string">"embedding"</span>])
    metadata[<span class="hljs-string">"num_chunks"</span>] = <span class="hljs-built_in">len</span>(embedded_chunks)
    metadata[<span class="hljs-string">"num_embedded_chunks"</span>] = <span class="hljs-built_in">len</span>(embedded_chunks)
    step_context = get_step_context()
    step_context.add_output_metadata(output_name=<span class="hljs-string">"embedded_documents"</span>, metadata=metadata)
    <span class="hljs-keyword">return</span> embedded_chunks
</code></pre>
    <p class="normal">In <em class="italic">Figure 4.14</em>, you can see the <a id="_idIndexMarker362"/>metadata of the chunking and embedding ZenML step. For example, you can quickly <a id="_idIndexMarker363"/>understand that we transformed 76 documents into 2,373 chunks, or the properties we used for chunking articles, such as a <code class="inlineCode">chunk_size</code> of 500 and a <code class="inlineCode">chunk_overlap</code> of <strong class="keyWord">50</strong>.</p>
    <figure class="mediaobject"><img src="../Images/B31105_04_14.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.14: Metadata of the embedding and chunking ZenML step, detailing the uncategorized and chunking dropdowns</p>
    <p class="normal">In Figure 4.15, the rest<a id="_idIndexMarker364"/> of the ZenML metadata from the<a id="_idIndexMarker365"/> embedding and chunking step details the embedding model and its properties used to compute the vectors.</p>
    <figure class="mediaobject"><img src="../Images/B31105_04_15.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.15: Metadata of the embedding and chunking ZenML step, detailing the embedding dropdown</p>
    <p class="normal">As ML systems can break <a id="_idIndexMarker366"/>at any time while in <a id="_idIndexMarker367"/>production due to drifts or untreated use cases, leveraging the metadata section to monitor the ingested data can be a powerful tool that will save debugging days, translating to tens of thousands of dollars or more for your business.</p>
    <h3 id="_idParaDest-124" class="heading-3">Loading the documents to the vector DB</h3>
    <p class="normal">As each article, post, or <a id="_idIndexMarker368"/>code repository sits in a different collection inside the vector DB, we have to group all the documents based on their data category. Then, we load each group in bulk in the Qdrant vector DB:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@step</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">load_to_vector_db</span>(
<span class="hljs-params">    documents: Annotated[</span><span class="hljs-built_in">list</span><span class="hljs-params">, </span><span class="hljs-string">"documents"</span><span class="hljs-params">],</span>
) -&gt; <span class="hljs-literal">None</span>:
    logger.info(<span class="hljs-string">f"Loading </span><span class="hljs-subst">{</span><span class="hljs-built_in">len</span><span class="hljs-subst">(documents)}</span><span class="hljs-string"> documents into the vector database."</span>)
    grouped_documents = VectorBaseDocument.group_by_class(documents)
    <span class="hljs-keyword">for</span> document_class, documents <span class="hljs-keyword">in</span> grouped_documents.items():
        logger.info(<span class="hljs-string">f"Loading documents into </span><span class="hljs-subst">{document_class.get_collection_name()}</span><span class="hljs-string">"</span>)
        <span class="hljs-keyword">for</span> documents_batch <span class="hljs-keyword">in</span> utils.misc.batch(documents, size=<span class="hljs-number">4</span>):
            <span class="hljs-keyword">try</span>:
                document_class.bulk_insert(documents_batch)
            <span class="hljs-keyword">except</span> Exception:
                <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>
    <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span>
</code></pre>
    <h2 id="_idParaDest-125" class="heading-2">Pydantic domain entities</h2>
    <p class="normal">Before investigating the dispatchers, we <a id="_idIndexMarker369"/>must understand the domain objects we work with. To some extent, in implementing the LLM Twin, we are<a id="_idIndexMarker370"/> following the <strong class="keyWord">domain-driven design</strong> (<strong class="keyWord">DDD</strong>) principles, which <a id="_idIndexMarker371"/>state that domain entities are the core of your application. Thus, before proceeding, it’s important to understand the hierarchy of the domain classes we are working with.</p>
    <div class="note">
      <p class="normal">The code for the domain entities is available on GitHub at <a href="https://github.com/PacktPublishing/LLM-Engineering/tree/main/llm_engineering/domain"><span class="url">https://github.com/PacktPublishing/LLM-Engineering/tree/main/llm_engineering/domain</span></a>.</p>
    </div>
    <p class="normal">We used Pydantic to model all our domain entities. When we wrote the book, choosing Pydantic was a no-brainer, as it is the go-to Python package for writing data structures with out-of-the-box type validation. As Python is a dynamically typed language, using Pydantic for type validation at runtime makes your system order of times more robust, as you can be sure that you are <a id="_idIndexMarker372"/>always working with the right type of data.</p>
    <p class="normal">The domain of our LLM Twin application is split<a id="_idIndexMarker373"/> into two dimensions:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">The data category</strong>: Post, article, and repository</li>
      <li class="bulletList"><strong class="keyWord">The state of the data</strong>: Cleaned, chunked, and embedded</li>
    </ul>
    <p class="normal">We decided to create a base class<a id="_idIndexMarker374"/> for each state of the document, resulting in having the following base abstract classes:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">class CleanedDocument(VectorBaseDocument, ABC)</code></li>
      <li class="bulletList"><code class="inlineCode">class Chunk(VectorBaseDocument, ABC)</code></li>
      <li class="bulletList"><code class="inlineCode">class EmbeddedChunk(VectorBaseDocument, ABC)</code></li>
    </ul>
    <p class="normal">Note that all of them inherit the <code class="inlineCode">VectorBaseDocument</code> class, which is our custom <strong class="keyWord">OVM</strong> implementation, which we will explain in the next section of this chapter. Also, it inherits from ABC, which makes the class abstract. Thus, you cannot initialize an object out of these classes; you may only inherit from them. That is why base classes are always marked as abstract.</p>
    <p class="normal">Each base abstract class from above (which models the state) will have a subclass that adds the data category dimension. For example, the <code class="inlineCode">CleanedDocument</code> class will have the following subclasses:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">class CleanedPostDocument(CleanedDocument)</code></li>
      <li class="bulletList"><code class="inlineCode">class CleanedArticleDocument(CleanedDocument)</code></li>
      <li class="bulletList"><code class="inlineCode">class CleanedRepositoryDocument(CleanedDocument)</code></li>
    </ul>
    <p class="normal">As we can see in <em class="italic">Figure 8.16</em>, we will repeat the same logic for the <code class="inlineCode">Chunk</code> and <code class="inlineCode">EmbeddedChunk</code> base abstract classes. We will implement a specific document class for each data category and state combination, resulting in nine types of domain entities. For example, when ingesting a raw document, the cleaning step will yield a <code class="inlineCode">CleanedArticleDocument</code> instance, the chunking step will return a list of <code class="inlineCode">ArticleChunk</code> objects, and the embedding operation will return <code class="inlineCode">EmbeddedArticleChunk</code> instances that encapsulate the embedding and all the necessary metadata to ingest in the vector DB. </p>
    <p class="normal">The same will happen for the posts and repositories.</p>
    <figure class="mediaobject"><img src="../Images/B31105_04_16.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.16: Domain entities class hierarchy and their interaction</p>
    <p class="normal">We chose this design because the list of states will rarely change, and we want to extend the list of data<a id="_idIndexMarker375"/> categories. Thus, structuring the classes after the state allows us to plug another data category by inheriting these base abstract classes.</p>
    <p class="normal">Let’s see the complete code for the hierarchy of the cleaned document. All the attributes of a cleaned document will be saved within the metadata of the vector DB. For example, the metadata of a cleaned article document will always contain the content, platform, author ID, author full name, and link of the article.</p>
    <p class="normal">Another fundamental aspect is the <code class="inlineCode">Config</code> internal class, which defines the name of the collection within the <a id="_idIndexMarker376"/>vector DB, the data category of the entity, and whether to leverage the vector index when creating the collection:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">CleanedDocument</span>(VectorBaseDocument, ABC):
    content: <span class="hljs-built_in">str</span>
    platform: <span class="hljs-built_in">str</span>
    author_id: UUID4
    author_full_name: <span class="hljs-built_in">str</span>
<span class="hljs-keyword">class</span> <span class="hljs-title">CleanedPostDocument</span>(<span class="hljs-title">CleanedDocument</span>):
    image: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span>
    <span class="hljs-keyword">class</span> <span class="hljs-title">Config</span>:
        name = <span class="hljs-string">"cleaned_posts"</span>
        category = DataCategory.POSTS
        use_vector_index = <span class="hljs-literal">False</span>
<span class="hljs-keyword">class</span> <span class="hljs-title">CleanedArticleDocument</span>(<span class="hljs-title">CleanedDocument</span>):
    link: <span class="hljs-built_in">str</span>
    <span class="hljs-keyword">class</span> <span class="hljs-title">Config</span>:
        name = <span class="hljs-string">"cleaned_articles"</span>
        category = DataCategory.ARTICLES
        use_vector_index = <span class="hljs-literal">False</span>
<span class="hljs-keyword">class</span> <span class="hljs-title">CleanedRepositoryDocument</span>(<span class="hljs-title">CleanedDocument</span>):
    name: <span class="hljs-built_in">str</span>
    link: <span class="hljs-built_in">str</span>
    <span class="hljs-keyword">class</span> <span class="hljs-title">Config</span>:
        name = <span class="hljs-string">"cleaned_repositories"</span>
        category = DataCategory.REPOSITORIES
        use_vector_index = <span class="hljs-literal">False</span>
</code></pre>
    <p class="normal">To conclude this section, let’s also take a look at the base abstract class of the chunk and embedded chunk:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">Chunk</span>(VectorBaseDocument, ABC):
    content: <span class="hljs-built_in">str</span>
    platform: <span class="hljs-built_in">str</span>
    document_id: UUID4
    author_id: UUID4
    author_full_name: <span class="hljs-built_in">str</span>
    metadata: <span class="hljs-built_in">dict</span> = Field(default_factory=<span class="hljs-built_in">dict</span>)
… <span class="hljs-comment"># PostChunk, ArticleChunk, RepositoryChunk</span>
<span class="hljs-keyword">class</span> <span class="hljs-title">EmbeddedChunk</span>(VectorBaseDocument, ABC):
    content: <span class="hljs-built_in">str</span>
    embedding: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">float</span>] | <span class="hljs-literal">None</span>
    platform: <span class="hljs-built_in">str</span>
    document_id: UUID4
    author_id: UUID4
    author_full_name: <span class="hljs-built_in">str</span>
    metadata: <span class="hljs-built_in">dict</span> = Field(default_factory=<span class="hljs-built_in">dict</span>)
… <span class="hljs-comment"># EmbeddedPostChunk, EmbeddedArticleChunk, EmbeddedRepositoryChunk</span>
</code></pre>
    <p class="normal">We also defined an enum<a id="_idIndexMarker377"/> that aggregates all our data categories in a single structure of constants:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">DataCategory</span>(<span class="hljs-title">StrEnum</span>):
    POSTS = <span class="hljs-string">"posts"</span>
    ARTICLES = <span class="hljs-string">"articles"</span>
    REPOSITORIES = <span class="hljs-string">"repositories"</span>
</code></pre>
    <p class="normal">The last step to fully understand how the domain objects work is to zoom into the <code class="inlineCode">VectorBaseDocument</code> OVM class.</p>
    <h3 id="_idParaDest-126" class="heading-3">OVM</h3>
    <p class="normal">The term OVM is inspired by the <strong class="keyWord">object-relational mapping</strong> (<strong class="keyWord">ORM</strong>) pattern we discussed in <em class="italic">Chapter 3</em>. We called it <a id="_idIndexMarker378"/>OVM because we work with embedding and vector DBs<a id="_idIndexMarker379"/> instead of structured data and SQL tables. Otherwise, it follows the same principles as an ORM pattern.</p>
    <p class="normal">Similar to what we did in <em class="italic">Chapter 3</em>, we will implement our own OVM version. Even if our custom example is simple, it’s a powerful example of how to write modular and extendable classes by leveraging OOP best practices and principles.</p>
    <div class="note">
      <p class="normal">The full implementation of the <code class="inlineCode">VectorBaseDocument</code> class is available on GitHub at <a href="https://github.com/PacktPublishing/LLM-Engineering/blob/main/llm_engineering/domain/base/vector.py"><span class="url">https://github.com/PacktPublishing/LLM-Engineering/blob/main/llm_engineering/domain/base/vector.py</span></a>.</p>
    </div>
    <p class="normal">Our OVM base class is called <code class="inlineCode">VectorBaseDocument</code>. It will support CRUD operations on top of Qdrant. Based on our application’s demands, we limited it only to create and read operations, but it can<a id="_idIndexMarker380"/> easily be extended to update and delete functions. </p>
    <p class="normal">Let’s take a look at the definition of the <code class="inlineCode">VectorBaseDocument</code> class:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> pydantic <span class="hljs-keyword">import</span> UUID4, BaseModel
<span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Generic</span>
<span class="hljs-keyword">from</span> llm_engineering.infrastructure.db.qdrant <span class="hljs-keyword">import</span> connection
T = TypeVar(<span class="hljs-string">"T"</span>, bound=<span class="hljs-string">"VectorBaseDocument"</span>)
<span class="hljs-keyword">class</span> <span class="hljs-title">VectorBaseDocument</span>(BaseModel, <span class="hljs-type">Generic</span>[T], ABC):
    <span class="hljs-built_in">id</span>: UUID4 = Field(default_factory=uuid.uuid4)
<span class="hljs-meta">    @classmethod</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">from_record</span>(<span class="hljs-params">cls: </span><span class="hljs-type">Type</span><span class="hljs-params">[T], point: Record</span>) -&gt; T:
        _<span class="hljs-built_in">id</span> = UUID(point.<span class="hljs-built_in">id</span>, version=<span class="hljs-number">4</span>)
        payload = point.payload <span class="hljs-keyword">or</span> {}
        attributes = {
            <span class="hljs-string">"id"</span>: _<span class="hljs-built_in">id</span>,
            **payload,
        }
        <span class="hljs-keyword">if</span> cls._has_class_attribute(<span class="hljs-string">"embedding"</span>):
            payload[<span class="hljs-string">"embedding"</span>] = point.vector <span class="hljs-keyword">or</span> <span class="hljs-literal">None</span>
        <span class="hljs-keyword">return</span> cls(**attributes)
    <span class="hljs-keyword">def</span> <span class="hljs-title">to_point</span>(<span class="hljs-params">self: T, **kwargs</span>) -&gt; PointStruct:
        exclude_unset = kwargs.pop(<span class="hljs-string">"exclude_unset"</span>, <span class="hljs-literal">False</span>)
        by_alias = kwargs.pop(<span class="hljs-string">"</span><span class="hljs-string">by_alias"</span>, <span class="hljs-literal">True</span>)
        payload = <span class="hljs-variable">self</span>.<span class="hljs-built_in">dict</span>(exclude_unset=exclude_unset, by_alias=by_alias, **kwargs)
        _<span class="hljs-built_in">id</span> = <span class="hljs-built_in">str</span>(payload.pop(<span class="hljs-string">"id"</span>))
        vector = payload.pop(<span class="hljs-string">"embedding"</span>, {})
        <span class="hljs-keyword">if</span> vector <span class="hljs-keyword">and</span> <span class="hljs-built_in">isinstance</span>(vector, np.ndarray):
            vector = vector.tolist()
        <span class="hljs-keyword">return</span> PointStruct(<span class="hljs-built_in">id</span>=_<span class="hljs-built_in">id</span>, vector=vector, payload=payload)
</code></pre>
    <ul>
      <li class="bulletList">The <code class="inlineCode">VectorBaseDocument</code> class inherits from Pydantic’s <code class="inlineCode">BaseModel</code> and helps us structure a single record’s attributes from the vector DB. Every OVM will be initialized by default with UUID4 as its unique identifier. Using generics—more precisely, by inheriting from <code class="inlineCode">Generic[T]</code>—the signatures of all the subclasses of the <code class="inlineCode">VectorBaseDocument</code> class will adapt to that given class. For example, the <code class="inlineCode">from_record()</code> method of the <code class="inlineCode">Chunk()</code> class, which inherits <code class="inlineCode">VectorBaseDocument</code>, will return the Chunk type, which drastically helps the static analyzer and type checkers such as <code class="inlineCode">mypy</code> (<a href="https://mypy.readthedocs.io/en/stable/"><span class="url">https://mypy.readthedocs.io/en/stable/</span></a>).</li>
    </ul>
    <p class="normal">The <code class="inlineCode">from_record()</code> method adapts a data point from Qdrant’s format to our internal structure based on Pydantic. On the other hand, the <code class="inlineCode">to_point()</code> method takes the attributes of the current instance and adapts them to Qdrant’s <code class="inlineCode">PointStruct()</code> format. We will leverage these two methods for our create and read operations.</p>
    <p class="normal">Ultimately, all operations made <a id="_idIndexMarker381"/>to Qdrant will be done through the <code class="inlineCode">connection</code> instance, which is instantiated in the application’s infrastructure layer.</p>
    <p class="normal">The <code class="inlineCode">bulk_insert()</code> method maps each document to a point. Then, it uses the Qdrant <code class="inlineCode">connection</code> instance to load all the points to a given collection in Qdrant. If the insertion fails once, it tries to create the collection and do the insertion again. Often, it is good practice to split your logic into two functions. One private function contains the logic, in our case <code class="inlineCode">_bulk_insert()</code>, and one public function handles all the errors and failure scenarios.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">VectorBaseDocument</span>(BaseModel, <span class="hljs-type">Generic</span>[T], ABC):
    … <span class="hljs-comment"># Rest of the class</span>
<span class="hljs-meta">    @classmethod</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">bulk_insert</span>(<span class="hljs-params">cls: </span><span class="hljs-type">Type</span><span class="hljs-params">[T], documents: </span><span class="hljs-built_in">list</span><span class="hljs-params">[</span><span class="hljs-string">"VectorBaseDocument"</span><span class="hljs-params">]</span>) -&gt; <span class="hljs-built_in">bool</span>:
        <span class="hljs-keyword">try</span>:
            cls._bulk_insert(documents)
        <span class="hljs-keyword">except</span> exceptions.UnexpectedResponse:
            logger.info(
                <span class="hljs-string">f"Collection '</span><span class="hljs-subst">{cls.get_collection_name()}</span><span class="hljs-string">' does not exist. Trying to create the collection and reinsert the documents."</span>
            )
            cls.create_collection()
            <span class="hljs-keyword">try</span>:
                cls._bulk_insert(documents)
            <span class="hljs-keyword">except</span> exceptions.UnexpectedResponse:
                logger.error(<span class="hljs-string">f"Failed to insert documents in '</span><span class="hljs-subst">{cls.get_collection_name()}</span><span class="hljs-string">'."</span>)
                <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>
        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span>
<span class="hljs-meta">    @classmethod</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">_bulk_insert</span>(<span class="hljs-params">cls: </span><span class="hljs-type">Type</span><span class="hljs-params">[T], documents: </span><span class="hljs-built_in">list</span><span class="hljs-params">[</span><span class="hljs-string">"VectorBaseDocument"</span><span class="hljs-params">]</span>) -&gt; <span class="hljs-literal">None</span>:
        points = [doc.to_point() <span class="hljs-keyword">for</span> doc <span class="hljs-keyword">in</span> documents]
        connection.upsert(collection_name=cls.get_collection_name(), points=points)
</code></pre>
    <p class="normal">The collection name is<a id="_idIndexMarker382"/> inferred from the <code class="inlineCode">Config</code> class defined in the subclasses inheriting the OVM:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">VectorBaseDocument</span>(BaseModel, <span class="hljs-type">Generic</span>[T], ABC):
    … <span class="hljs-comment"># Rest of the class</span>
<span class="hljs-meta">    @classmethod</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">get_collection_name</span>(<span class="hljs-params">cls: </span><span class="hljs-type">Type</span><span class="hljs-params">[T]</span>) -&gt; <span class="hljs-built_in">str</span>:
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">hasattr</span>(cls, <span class="hljs-string">"Config"</span>) <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">hasattr</span>(cls.Config, <span class="hljs-string">"name"</span>):
            <span class="hljs-keyword">raise</span> ImproperlyConfigured(
                <span class="hljs-string">"The class should define a Config class with"</span> <span class="hljs-string">"the 'name' property that reflects the collection's name."</span>
            )
        <span class="hljs-keyword">return</span> cls.Config.name
</code></pre>
    <p class="normal">Now, we must define a method that lets us read all the records from the vector DB (without using vector similarity search logic). The <code class="inlineCode">bulk_find()</code> method enables us to scroll (or list) all the records from a collection. The function below scrolls the Qdrant vector DB, which returns a list of data points, which are ultimately mapped to our internal structure using the <code class="inlineCode">from_record()</code> method.</p>
    <p class="normal">The limit parameters control how<a id="_idIndexMarker383"/> many items we return at once, and the offset signals the ID of the point from which Qdrant starts returning records.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">VectorBaseDocument</span>(BaseModel, <span class="hljs-type">Generic</span>[T], ABC):
    … <span class="hljs-comment"># Rest of the class</span>
<span class="hljs-meta">    @classmethod</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">bulk_find</span>(<span class="hljs-params">cls: </span><span class="hljs-type">Type</span><span class="hljs-params">[T], limit: </span><span class="hljs-built_in">int</span><span class="hljs-params"> = </span><span class="hljs-number">10</span><span class="hljs-params">, **kwargs</span>) -&gt; <span class="hljs-built_in">tuple</span>[<span class="hljs-built_in">list</span>[T], UUID | <span class="hljs-literal">None</span>]:
        <span class="hljs-keyword">try</span>:
            documents, next_offset = cls._bulk_find(limit=limit, **kwargs)
        <span class="hljs-keyword">except</span> exceptions.UnexpectedResponse:
            logger.error(<span class="hljs-string">f"Failed to search documents in '</span><span class="hljs-subst">{cls.get_collection_name()}</span><span class="hljs-string">'."</span>)
            documents, next_offset = [], <span class="hljs-literal">None</span>
        <span class="hljs-keyword">return</span> documents, next_offset
<span class="hljs-meta">    @classmethod</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">_bulk_find</span>(<span class="hljs-params">cls: </span><span class="hljs-type">Type</span><span class="hljs-params">[T], limit: </span><span class="hljs-built_in">int</span><span class="hljs-params"> = </span><span class="hljs-number">10</span><span class="hljs-params">, **kwargs</span>) -&gt; <span class="hljs-built_in">tuple</span>[<span class="hljs-built_in">list</span>[T], UUID | <span class="hljs-literal">None</span>]:
        collection_name = cls.get_collection_name()
        offset = kwargs.pop(<span class="hljs-string">"offset"</span>, <span class="hljs-literal">None</span>)
        offset = <span class="hljs-built_in">str</span>(offset) <span class="hljs-keyword">if</span> offset <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>
        records, next_offset = connection.scroll(
            collection_name=collection_name,
            limit=limit,
            with_payload=kwargs.pop(<span class="hljs-string">"</span><span class="hljs-string">with_payload"</span>, <span class="hljs-literal">True</span>),
            with_vectors=kwargs.pop(<span class="hljs-string">"with_vectors"</span>, <span class="hljs-literal">False</span>),
            offset=offset,
            **kwargs,
        )
        documents = [cls.from_record(record) <span class="hljs-keyword">for</span> record <span class="hljs-keyword">in</span> records]
        <span class="hljs-keyword">if</span> next_offset <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
            next_offset = UUID(next_offset, version=<span class="hljs-number">4</span>)
        <span class="hljs-keyword">return</span> documents, next_offset
</code></pre>
    <p class="normal">The last piece of the puzzle is to define a method that performs a vector similarity search on a provided query embedding. Like before, we defined a public <code class="inlineCode">search()</code> and private <code class="inlineCode">_search()</code> method. The<a id="_idIndexMarker384"/> search is performed by Qdrant when calling the <code class="inlineCode">connection.search()</code> function.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">VectorBaseDocument</span>(BaseModel, <span class="hljs-type">Generic</span>[T], ABC):
    … <span class="hljs-comment"># Rest of the class</span>
<span class="hljs-meta">    @classmethod</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">search</span>(<span class="hljs-params">cls: </span><span class="hljs-type">Type</span><span class="hljs-params">[T], query_vector: </span><span class="hljs-built_in">list</span><span class="hljs-params">, limit: </span><span class="hljs-built_in">int</span><span class="hljs-params"> = </span><span class="hljs-number">10</span><span class="hljs-params">, **kwargs</span>) -&gt; <span class="hljs-built_in">list</span>[T]:
        <span class="hljs-keyword">try</span>:
            documents = cls._search(query_vector=query_vector, limit=limit, **kwargs)
        <span class="hljs-keyword">except</span> exceptions.UnexpectedResponse:
            logger.error(<span class="hljs-string">f"Failed to search documents in '</span><span class="hljs-subst">{cls.get_collection_name()}</span><span class="hljs-string">'."</span>)
            documents = []
        <span class="hljs-keyword">return</span> documents
<span class="hljs-meta">    @classmethod</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">_search</span>(<span class="hljs-params">cls: </span><span class="hljs-type">Type</span><span class="hljs-params">[T], query_vector: </span><span class="hljs-built_in">list</span><span class="hljs-params">, limit: </span><span class="hljs-built_in">int</span><span class="hljs-params"> = </span><span class="hljs-number">10</span><span class="hljs-params">, **kwargs</span>) -&gt; <span class="hljs-built_in">list</span>[T]:
        collection_name = cls.get_collection_name()
        records = connection.search(
            collection_name=collection_name,
            query_vector=query_vector,
            limit=limit,
            with_payload=kwargs.pop(<span class="hljs-string">"with_payload"</span>, <span class="hljs-literal">True</span>),
            with_vectors=kwargs.pop(<span class="hljs-string">"with_vectors"</span>, <span class="hljs-literal">False</span>),
            **kwargs,
        )
        documents = [cls.from_record(record) <span class="hljs-keyword">for</span> record <span class="hljs-keyword">in</span> records]
        <span class="hljs-keyword">return</span> documents
</code></pre>
    <p class="normal">Now that we understand what our domain entities look like and how the OVM works, let’s move on to the dispatchers who clean, chunk, and embed the documents.</p>
    <h2 id="_idParaDest-127" class="heading-2">The dispatcher layer</h2>
    <p class="normal">A dispatcher inputs a document and applies <a id="_idIndexMarker385"/>dedicated handlers based on its data category (article, post, or repository). A handler can either <a id="_idIndexMarker386"/>clean, chunk, or embed a document.</p>
    <p class="normal">Let’s start by zooming in on the <code class="inlineCode">CleaningDispatcher</code>. It mainly implements a <code class="inlineCode">dispatch()</code> method that inputs a raw document. Based on its data category, it instantiates and calls a handler that applies the cleaning logic specific to that data point:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">CleaningDispatcher</span>:
    cleaning_factory = CleaningHandlerFactory()
<span class="hljs-meta">    @classmethod</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">dispatch</span>(<span class="hljs-params">cls, data_model: NoSQLBaseDocument</span>) -&gt; VectorBaseDocument:
        data_category = DataCategory(data_model.get_collection_name())
        handler = cls.cleaning_factory.create_handler(data_category)
        clean_model = handler.clean(data_model)
        logger.info(
            <span class="hljs-string">"Data cleaned successfully."</span>,
            data_category=data_category,
            cleaned_content_len=<span class="hljs-built_in">len</span>(clean_model.content),
        )
        <span class="hljs-keyword">return</span> clean_model
</code></pre>
    <p class="normal">The key in the dispatcher logic is the <code class="inlineCode">CleaningHandlerFactory()</code>, which instantiates a different cleaning handler based on the document’s data category:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">CleaningHandlerFactory</span>:
<span class="hljs-meta">    @staticmethod</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">create_handler</span>(<span class="hljs-params">data_category: DataCategory</span>) -&gt; CleaningDataHandler:
        <span class="hljs-keyword">if</span> data_category == DataCategory.POSTS:
            <span class="hljs-keyword">return</span> PostCleaningHandler()
        <span class="hljs-keyword">elif</span> data_category == DataCategory.ARTICLES:
            <span class="hljs-keyword">return</span> ArticleCleaningHandler()
        <span class="hljs-keyword">elif</span> data_category == DataCategory.REPOSITORIES:
            <span class="hljs-keyword">return</span> RepositoryCleaningHandler()
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">"Unsupported data type"</span>)
</code></pre>
    <p class="normal">The Dispatcher or Factory classes are nothing fancy, but they offer an intuitive and simple interface for applying various operations to your documents. When manipulating documents, instead of worrying about their data category and polluting your business logic with if-else statements, you have a class dedicated to handling that. You have a single class that cleans any document, which respects the DRY (don’t repeat yourself) principles from software engineering. By respecting DRY, you have a single point of failure, and the code can easily be extended. For example, if we add an extra type, we must extend only the Factory class instead of multiple occurrences in the code.</p>
    <p class="normal">The <code class="inlineCode">ChunkingDispatcher</code> and <code class="inlineCode">EmbeddingDispatcher</code> follow the same pattern. They use a <code class="inlineCode">ChunkingHandlerFactory</code> and, respectively, an <code class="inlineCode">EmbeddingHandlerFactory</code> that initializes the correct handler <a id="_idIndexMarker387"/>based on the data category of the input document. Afterward, they call the handler and return the result.</p>
    <div class="note">
      <p class="normal">The source code of all the dispatchers and factories can be found on GitHub at <a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/preprocessing/dispatchers.py"><span class="url">https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/preprocessing/dispatchers.py</span></a></p>
    </div>
    <p class="normal">The Factory class leverages theabstract factory creational pattern (<a href="https://refactoring.guru/design-patterns/abstract-factory"><span class="url">https://refactoring.guru/design-patterns/abstract-factory</span></a>), which instantiates a family of classes implementing the same interface. In our case, these handlers implement the <code class="inlineCode">clean()</code> method regardless of the handler type. </p>
    <p class="normal">Also, the Handler class family leverages the strategy behavioral pattern (<a href="https://refactoring.guru/design-patterns/strategy"><span class="url">https://refactoring.guru/design-patterns/strategy</span></a>) used to instantiate when you want to use different variants of an algorithm within an object and be able to switch from one algorithm to another during runtime.</p>
    <p class="normal">Intuitively, in our dispatcher layer, the combination of the factory and strategy patterns works as follows:</p>
    <ol>
      <li class="numberedList" value="1">Initially, we knew we wanted to clean the data, but as we knew the data category only at runtime, we couldn’t decide on what strategy to apply.</li>
      <li class="numberedList">We can write the whole code around the cleaning code and abstract away the logic under a <code class="inlineCode">Handler()</code> interface, which will represent our strategy.</li>
      <li class="numberedList">When we get a data point, we apply the abstract factory pattern and create the correct cleaning handler for its data type.</li>
      <li class="numberedList">Ultimately, the dispatcher layer uses the handler and executes the right strategy.</li>
    </ol>
    <p class="normal">By doing so, we:</p>
    <ul>
      <li class="bulletList">Isolate the logic for a given data category.</li>
      <li class="bulletList">Leverage polymorphism to avoid filling up the code with hundreds of <code class="inlineCode">if-else</code> statements.</li>
      <li class="bulletList">Make the code modular and extendable. When a new data category arrives, we must implement a new handler and modify the Factory class without touching any other part of the code.</li>
    </ul>
    <div class="note">
      <p class="normal">Until now, we have just <a id="_idIndexMarker388"/>modeled our entities and how the data flows in our application. We haven’t written a single piece of cleaning, chunking, or embedding code. That is one big difference between a quick demo and a production-ready application. In a demo, you don’t care about software engineering best practices and structuring your code to make it future-proof. However, writing clean, modular, and scalable code is critical for its longevity when building a real-world application.</p>
    </div>
    <p class="normal">The last component of the RAG feature pipeline is the implementation of the cleaning, chunking, and embedding handlers.</p>
    <h2 id="_idParaDest-128" class="heading-2">The handlers</h2>
    <p class="normal">The handler has a one-on-one <a id="_idIndexMarker389"/>structure with our domain, meaning<a id="_idIndexMarker390"/> that every entity has its own handler, as shown in Figure 8.17. In total, we will have nine Handler classes that follow the next base interfaces:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">class CleaningDataHandler()</code></li>
      <li class="bulletList"><code class="inlineCode">class ChunkingDataHandler()</code></li>
      <li class="bulletList"><code class="inlineCode">class EmbeddingDataHandler()</code></li>
    </ul>
    <figure class="mediaobject"><img src="../Images/B31105_04_17.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.17: Handler class hierarchy and their interaction</p>
    <div class="note">
      <p class="normal">The code for all the handlers is available on GitHub at <a href="https://github.com/PacktPublishing/LLM-Engineering/tree/main/llm_engineering/application/preprocessing"><span class="url">https://github.com/PacktPublishing/LLM-Engineering/tree/main/llm_engineering/application/preprocessing</span></a>.</p>
    </div>
    <p class="normal">Let’s examine <a id="_idIndexMarker391"/>each handler family and see how it is implemented.</p>
    <h3 id="_idParaDest-129" class="heading-3">The cleaning handlers</h3>
    <p class="normal">The <code class="inlineCode">CleaningDataHandler()</code> strategy<a id="_idIndexMarker392"/> interface looks as<a id="_idIndexMarker393"/> follows:</p>
    <pre class="programlisting code"><code class="hljs-code">… <span class="hljs-comment"># Other imports.</span>
<span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Generic</span>, TypeVar
DocumentT = TypeVar(<span class="hljs-string">"DocumentT"</span>, bound=Document)
CleanedDocumentT = TypeVar(<span class="hljs-string">"CleanedDocumentT"</span>, bound=CleanedDocument)
<span class="hljs-keyword">class</span> <span class="hljs-title">CleaningDataHandler</span>(ABC, <span class="hljs-type">Generic</span>[DocumentT, CleanedDocumentT]):
<span class="hljs-meta">    @abstractmethod</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">clean</span>(<span class="hljs-params">self, data_model: DocumentT</span>) -&gt; CleanedDocumentT:
        <span class="hljs-keyword">pass</span>
</code></pre>
    <p class="normal">Now, for every post, article <a id="_idIndexMarker394"/>and repository, we have to implement a <a id="_idIndexMarker395"/>different handler, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">PostCleaningHandler</span>(<span class="hljs-title">CleaningDataHandler</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title">clean</span>(<span class="hljs-params">self, data_model: PostDocument</span>) -&gt; CleanedPostDocument:
        <span class="hljs-keyword">return</span> CleanedPostDocument(
            <span class="hljs-built_in">id</span>=data_model.<span class="hljs-built_in">id</span>,
            content=clean_text(<span class="hljs-string">" #### "</span>.join(data_model.content.values())),
            … <span class="hljs-comment"># Copy the rest of the parameters from the data_model object.</span>
        )
<span class="hljs-keyword">class</span> <span class="hljs-title">ArticleCleaningHandler</span>(<span class="hljs-title">CleaningDataHandler</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title">clean</span>(<span class="hljs-params">self, data_model: ArticleDocument</span>) -&gt; CleanedArticleDocument:
        valid_content = [content <span class="hljs-keyword">for</span> content <span class="hljs-keyword">in</span> data_model.content.values() <span class="hljs-keyword">if</span> content]
        <span class="hljs-keyword">return</span> CleanedArticleDocument(
            <span class="hljs-built_in">id</span>=data_model.<span class="hljs-built_in">id</span>,
            content=clean_text(<span class="hljs-string">" #### "</span>.join(valid_content)),
            platform=data_model.platform,
            link=data_model.link,
            author_id=data_model.author_id,
            author_full_name=data_model.author_full_name,
        )
<span class="hljs-keyword">class</span> <span class="hljs-title">RepositoryCleaningHandler</span>(<span class="hljs-title">CleaningDataHandler</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title">clean</span>(<span class="hljs-params">self, data_model: RepositoryDocument</span>) -&gt; CleanedRepositoryDocument:
        <span class="hljs-keyword">return</span> CleanedRepositoryDocument(
            <span class="hljs-built_in">id</span>=data_model.<span class="hljs-built_in">id</span>,
            content=clean_text(<span class="hljs-string">" #### "</span>.join(data_model.content.values())),
            … <span class="hljs-comment"># Copy the rest of the parameters from the data_model object.</span>
        )
</code></pre>
    <p class="normal">The handlers input a raw document domain entity, clean the content, and return a cleaned document. All the handlers use the <code class="inlineCode">clean_text()</code> function to clean the text. Out of simplicity, we used the same cleaning technique for all the data categories. Still, in a real-world setup, we would have to further optimize and create a different cleaning function for each data category. The strategy pattern makes this a breeze, as we swap the cleaning function in<a id="_idIndexMarker396"/> the handlers, and that’s it.</p>
    <p class="normal">The cleaning steps<a id="_idIndexMarker397"/> applied in the <code class="inlineCode">clean_text()</code> function are the same ones discussed in <em class="chapterRef">Chapter </em><em class="chapterRef">5</em> in the <em class="italic">Creating an instruction dataset</em> section. We don’t want to repeat ourselves. Thus, for a refresher, check out that chapter. At this point, we mostly care about automating and integrating the whole logic into the RAG feature pipeline. Thus, after operationalizing the ML system, all the cleaned data used for fine-tuning will be accessed from the logical feature store, making it the single source of truth for accessing data.</p>
    <h3 id="_idParaDest-130" class="heading-3">The chunking handlers</h3>
    <p class="normal">First, let’s examine the <code class="inlineCode">ChunkingDataHandler()</code> strategy handler. We exposed the <code class="inlineCode">metadata</code> dictionary as a property <a id="_idIndexMarker398"/>to aggregate all the necessary properties <a id="_idIndexMarker399"/>required for chunking in a single structure. By structuring it like this, we can easily log everything to ZenML to track and debug our chunking logic. The handler takes cleaned documents as input and returns chunk entities. All the handlers can be found on GitHub at <a href="https://github.com/PacktPublishing/LLM-Engineering/tree/main/llm_engineering/application/preprocessing"><span class="url">https://github.com/PacktPublishing/LLM-Engineering/tree/main/llm_engineering/application/preprocessing</span></a>.</p>
    <pre class="programlisting code"><code class="hljs-code">… <span class="hljs-comment"># Other imports.</span>
<span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Generic</span>, TypeVar
CleanedDocumentT = TypeVar(<span class="hljs-string">"CleanedDocumentT"</span>, bound=CleanedDocument)
ChunkT = TypeVar(<span class="hljs-string">"ChunkT"</span>, bound=Chunk)
 <span class="hljs-keyword">class</span> <span class="hljs-title">ChunkingDataHandler</span>(ABC, <span class="hljs-type">Generic</span>[CleanedDocumentT, ChunkT]):
<span class="hljs-meta">    @property</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">metadata</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">dict</span>:
        <span class="hljs-keyword">return</span> {
            <span class="hljs-string">"chunk_size"</span>: <span class="hljs-number">500</span>,
            <span class="hljs-string">"chunk_overlap"</span>: <span class="hljs-number">50</span>,
        }
<span class="hljs-meta">    @abstractmethod</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">chunk</span>(<span class="hljs-params">self, data_model: CleanedDocumentT</span>) -&gt; <span class="hljs-built_in">list</span>[ChunkT]:
        <span class="hljs-keyword">pass</span>
</code></pre>
    <p class="normal">Let’s understand how the <code class="inlineCode">ArticleChunkingHandler()</code> class is implemented. The first step is to override the metadata property and customize the type of properties the chunking logic requires. For example, when working with articles, we are interested in the chunk’s minimum and maximum length.</p>
    <p class="normal">The handler’s <code class="inlineCode">chunk()</code> method inputs cleaned article documents and returns a list of article chunk entities. It uses the <code class="inlineCode">chunk_text()</code> function to split the cleaned content into chunks. The chunking function <a id="_idIndexMarker400"/>is customized based on the <code class="inlineCode">min_length</code> and <code class="inlineCode">max_length</code> metadata fields. The chunk_id is computed as the MD5 hash of the chunk’s content. Thus, if the two chunks<a id="_idIndexMarker401"/> have precisely the same content, they will have the same ID, and we can easily deduplicate them. Lastly, we create a list of chunk entities and return them.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">ArticleChunkingHandler</span>(<span class="hljs-title">ChunkingDataHandler</span>):
<span class="hljs-meta">    @property</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">metadata</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">dict</span>:
        <span class="hljs-keyword">return</span> {
            <span class="hljs-string">"min_length"</span>: <span class="hljs-number">1000</span>,
            <span class="hljs-string">"max_length"</span>: <span class="hljs-number">1000</span>,
        }
    <span class="hljs-keyword">def</span> <span class="hljs-title">chunk</span>(<span class="hljs-params">self, data_model: CleanedArticleDocument</span>) -&gt; <span class="hljs-built_in">list</span>[ArticleChunk]:
        data_models_list = []
        cleaned_content = data_model.content
        chunks = chunk_article(
            cleaned_content, min_length=<span class="hljs-variable">self</span>.metadata[<span class="hljs-string">"min_length"</span>], max_length=<span class="hljs-variable">self</span>.metadata[<span class="hljs-string">"max_length"</span>]
        )
        <span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> chunks:
            chunk_id = hashlib.md5(chunk.encode()).hexdigest()
            model = ArticleChunk(
                <span class="hljs-built_in">id</span>=UUID(chunk_id, version=<span class="hljs-number">4</span>),
                content=chunk,
                platform=data_model.platform,
                link=data_model.link,
                document_id=data_model.<span class="hljs-built_in">id</span>,
                author_id=data_model.author_id,
                author_full_name=data_model.author_full_name,
                metadata=<span class="hljs-variable">self</span>.metadata,
            )
            data_models_list.append(model)
        <span class="hljs-keyword">return</span> data_models_list
</code></pre>
    <p class="normal">The last step is to dig into the <code class="inlineCode">chunk_article()</code> function, which mainly does two things:</p>
    <ul>
      <li class="bulletList">It uses a regex to find all the sentences within the given text by looking for periods, question<a id="_idIndexMarker402"/> marks, or exclamation points followed by a space. However, it avoids splitting into cases where the punctuation is part of <a id="_idIndexMarker403"/>an abbreviation or initialism (like “<code class="inlineCode">e.g.</code>" or “<code class="inlineCode">Dr.</code>")</li>
      <li class="bulletList">It groups sentences into a single chunk until the <code class="inlineCode">max_length</code> limit is reached. When the maximum size is reached, and the chunk size is bigger than the minimum allowed value, it is added to the final list the function returns.</li>
    </ul>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">chunk_article</span>(<span class="hljs-params">text: </span><span class="hljs-built_in">str</span><span class="hljs-params">, min_length: </span><span class="hljs-built_in">int</span><span class="hljs-params">, max_length: </span><span class="hljs-built_in">int</span>) -&gt; <span class="hljs-built_in">list</span>[<span class="hljs-built_in">str</span>]:
    sentences = re.split(<span class="hljs-string">r"(?&lt;!\w\.\w.)(?&lt;![A-Z][a-z]\.)(?&lt;=\.|\?|\!)\s"</span>, text)
    extracts = []
    current_chunk = <span class="hljs-string">""</span>
    <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> sentences:
        sentence = sentence.strip()
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> sentence:
            <span class="hljs-keyword">continue</span>
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(current_chunk) + <span class="hljs-built_in">len</span>(sentence) &lt;= max_length:
            current_chunk += sentence + <span class="hljs-string">" "</span>
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(current_chunk) &gt;= min_length:
                extracts.append(current_chunk.strip())
            current_chunk = sentence + <span class="hljs-string">" "</span>
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(current_chunk) &gt;= min_length:
        extracts.append(current_chunk.strip())
    <span class="hljs-keyword">return</span> extracts
</code></pre>
    <p class="normal">The <code class="inlineCode">PostChunkingHandler</code> and <code class="inlineCode">RepositoryChunkingHandler</code>, available on GitHub at <code class="inlineCode">llm_engineering/application/preprocessing/chunking_data_handlers.py</code>, have a similar structure to the ArticleChunkingHandler. However, they use a more generic chunking function called <code class="inlineCode">chunk_text()</code>, worth looking into. The <code class="inlineCode">chunk_text()</code> function is a two-step process<a id="_idIndexMarker404"/> that has the following logic:</p>
    <ol>
      <li class="numberedList" value="1">It uses a <code class="inlineCode">RecursiveCharacterTextSplitter()</code> from LangChain to split the text based on a given separator or chunk size. Using the separator, we first try to find paragraphs in the given<a id="_idIndexMarker405"/> text, but if there are no paragraphs or they are too long, we cut it at a given chunk size.</li>
      <li class="numberedList">Notice that we want to ensure that the chunk doesn’t exceed the maximum input length of the embedding model. Thus, we pass all the chunks created above into a <code class="inlineCode">SenteceTransformersTokenTextSplitter()</code>, which considers the maximum input length of the model. At this point, we also apply the <code class="inlineCode">chunk_overlap</code> logic, as we want to do it only after we validate that the chunk is small enough.
        <pre class="programlisting code-one"><code class="hljs-code">… <span class="hljs-comment"># Other imports.</span>
<span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter
<span class="hljs-keyword">from</span> llm_engineering.application.networks <span class="hljs-keyword">import</span> EmbeddingModelSingleton
<span class="hljs-keyword">def</span> <span class="hljs-title">chunk_text</span>(<span class="hljs-params">text: </span><span class="hljs-built_in">str</span><span class="hljs-params">, chunk_size: </span><span class="hljs-built_in">int</span><span class="hljs-params"> = </span><span class="hljs-number">500</span><span class="hljs-params">, chunk_overlap: </span><span class="hljs-built_in">int</span><span class="hljs-params"> = </span><span class="hljs-number">50</span>) -&gt; <span class="hljs-built_in">list</span>[<span class="hljs-built_in">str</span>]:
    character_splitter = RecursiveCharacterTextSplitter(separators=[<span class="hljs-string">"\n\n"</span>], chunk_size=chunk_size, chunk_overlap=<span class="hljs-number">0</span>)
    text_split_by_characters = character_splitter.split_text(text)
    token_splitter = SentenceTransformersTokenTextSplitter(
        chunk_overlap=chunk_overlap,
        tokens_per_chunk=embedding_model.max_input_length,
        model_name=embedding_model.model_id,
    )
    chunks_by_tokens = []
    <span class="hljs-keyword">for</span> section <span class="hljs-keyword">in</span> text_split_by_characters:
        chunks_by_tokens.extend(token_splitter.split_text(section))
    <span class="hljs-keyword">return</span> chunks_by_tokens
</code></pre>
      </li>
    </ol>
    <p class="normal">To conclude, the function above <a id="_idIndexMarker406"/>returns a list of chunks that respect both<a id="_idIndexMarker407"/> the provided chunk parameters and the embedding model’s max input length.</p>
    <h3 id="_idParaDest-131" class="heading-3">The embedding handlers</h3>
    <p class="normal">The embedding handlers differ slightly<a id="_idIndexMarker408"/> from the others as the <code class="inlineCode">EmbeddingDataHandler()</code> interface contains most of the logic. We took this approach because, when calling the embedding model, we want to batch as many samples as possible to optimize the inference process. When running the model on a GPU, the batched samples <a id="_idIndexMarker409"/>are processed independently and in parallel. Thus, by batching the chunks, we can optimize the inference process by 10x or more, depending on the batch size and hardware we use.</p>
    <p class="normal">We implemented an <code class="inlineCode">embed()</code> method, in case you want to run the inference on a single data point, and an <code class="inlineCode">embed_batch()</code> method. The <code class="inlineCode">embed_batch()</code> method takes chunked documents as input, gathers their content into a list, passes them to the embedding model, and maps the results to an embedded chunk domain entity. The mapping is done through the <code class="inlineCode">map_model()</code> abstract method, which has to be customized for every data category.</p>
    <pre class="programlisting code"><code class="hljs-code">… <span class="hljs-comment"># Other imports.</span>
<span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Generic</span>, TypeVar, cast
<span class="hljs-keyword">from</span> llm_engineering.application.networks <span class="hljs-keyword">import</span> EmbeddingModelSingleton
ChunkT = TypeVar(<span class="hljs-string">"ChunkT"</span>, bound=Chunk)
EmbeddedChunkT = TypeVar(<span class="hljs-string">"EmbeddedChunkT"</span>, bound=EmbeddedChunk)
embedding_model = EmbeddingModelSingleton()
<span class="hljs-keyword">class</span> <span class="hljs-title">EmbeddingDataHandler</span>(ABC, <span class="hljs-type">Generic</span>[ChunkT, EmbeddedChunkT]):
    <span class="hljs-string">"""</span>
<span class="hljs-string">    Abstract class for all embedding data handlers.</span>
<span class="hljs-string">    All data transformations logic for the embedding step is done here</span>
<span class="hljs-string">    """</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">embed</span>(<span class="hljs-params">self, data_model: ChunkT</span>) -&gt; EmbeddedChunkT:
        <span class="hljs-keyword">return</span> <span class="hljs-variable">self</span>.embed_batch([data_model])[<span class="hljs-number">0</span>]
    <span class="hljs-keyword">def</span> <span class="hljs-title">embed_batch</span>(<span class="hljs-params">self, data_model: </span><span class="hljs-built_in">list</span><span class="hljs-params">[ChunkT]</span>) -&gt; <span class="hljs-built_in">list</span>[EmbeddedChunkT]:
        embedding_model_input = [data_model.content <span class="hljs-keyword">for</span> data_model <span class="hljs-keyword">in</span> data_model]
        embeddings = embedding_model(embedding_model_input, to_list=<span class="hljs-literal">True</span>)
        embedded_chunk = [
            <span class="hljs-variable">self</span>.map_model(data_model, cast(<span class="hljs-built_in">list</span>[<span class="hljs-built_in">float</span>], embedding))
            <span class="hljs-keyword">for</span> data_model, embedding <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(data_model, embeddings, strict=<span class="hljs-literal">False</span>)
        ]
        <span class="hljs-keyword">return</span> embedded_chunk
<span class="hljs-meta">    @abstractmethod</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">map_model</span>(<span class="hljs-params">self, data_model: ChunkT, embedding: </span><span class="hljs-built_in">list</span><span class="hljs-params">[</span><span class="hljs-built_in">float</span><span class="hljs-params">]</span>) -&gt; EmbeddedChunkT:
        <span class="hljs-keyword">pass</span>
</code></pre>
    <p class="normal">Let’s look only at the implementation of the <code class="inlineCode">ArticleEmbeddingHandler()</code>, as the other handlers are highly similar. As <a id="_idIndexMarker410"/>you can see, we only have to implement the <code class="inlineCode">map_model()</code> method, which takes a chunk of input and computes the<a id="_idIndexMarker411"/> embeddings in batch mode. Its scope is to map this information to an <code class="inlineCode">EmbeddedArticleChunk</code> Pydantic entity.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">ArticleEmbeddingHandler</span>(<span class="hljs-title">EmbeddingDataHandler</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title">map_model</span>(<span class="hljs-params">self, data_model: ArticleChunk, embedding: </span><span class="hljs-built_in">list</span><span class="hljs-params">[</span><span class="hljs-built_in">float</span><span class="hljs-params">]</span>) -&gt; EmbeddedArticleChunk:
        <span class="hljs-keyword">return</span> EmbeddedArticleChunk(
            <span class="hljs-built_in">id</span>=data_model.<span class="hljs-built_in">id</span>,
            content=data_model.content,
            embedding=embedding,
            platform=data_model.platform,
            link=data_model.link,
            document_id=data_model.document_id,
            author_id=data_model.author_id,
            author_full_name=data_model.author_full_name,
            metadata={
                <span class="hljs-string">"embedding_model_id"</span>: embedding_model.model_id,
                <span class="hljs-string">"embedding_size"</span>: embedding_model.embedding_size,
                <span class="hljs-string">"max_input_length"</span>: embedding_model.max_input_length,
            },
        )
</code></pre>
    <p class="normal">The last step is to understand how the <code class="inlineCode">EmbeddingModelSingleton()</code> works. It is a wrapper over the <code class="inlineCode">SentenceTransformer()</code> class from Sentence Transformers that initializes the embedding model. Writing a wrapper over external packages is often good practice. Thus, when you<a id="_idIndexMarker412"/> want to change the third-party tool, you have to modify only the internal logic of the wrapper instead of the whole code base.</p>
    <p class="normal">The <code class="inlineCode">SentenceTransformer()</code> class is initialized with the <code class="inlineCode">model_id</code> defined in the <code class="inlineCode">Settings</code> class, allowing us to<a id="_idIndexMarker413"/> quickly test multiple embedding models just by changing the configuration file and not the code. That is why I am not insisting at all on what embedding model to use. This differs constantly based on your use case, data, hardware, and latency. But by writing a generic class, which can quickly be configured, you can experiment with multiple embedding models until you find the best one for you.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sentence_transformers.SentenceTransformer <span class="hljs-keyword">import</span> SentenceTransformer
<span class="hljs-keyword">from</span> llm_engineering.settings <span class="hljs-keyword">import</span> settings
<span class="hljs-keyword">from</span> .base <span class="hljs-keyword">import</span> SingletonMeta
<span class="hljs-keyword">class</span> <span class="hljs-title">EmbeddingModelSingleton</span>(metaclass=SingletonMeta):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(
<span class="hljs-params">        self,</span>
<span class="hljs-params">        model_id: </span><span class="hljs-built_in">str</span><span class="hljs-params"> = settings.TEXT_EMBEDDING_MODEL_ID,</span>
<span class="hljs-params">        device: </span><span class="hljs-built_in">str</span><span class="hljs-params"> = settings.RAG_MODEL_DEVICE,</span>
<span class="hljs-params">        cache_dir: </span><span class="hljs-type">Optional</span><span class="hljs-params">[Path] = </span><span class="hljs-literal">None</span><span class="hljs-params">,</span>
<span class="hljs-params">    </span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-variable">self</span>._model_id = model_id
        <span class="hljs-variable">self</span>._device = device
        <span class="hljs-variable">self</span>._model = SentenceTransformer(
            <span class="hljs-variable">self</span>._model_id,
            device=<span class="hljs-variable">self</span>._device,
            cache_folder=<span class="hljs-built_in">str</span>(cache_dir) <span class="hljs-keyword">if</span> cache_dir <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,
        )
        <span class="hljs-variable">self</span>._model.<span class="hljs-built_in">eval</span>()
<span class="hljs-meta">    @property</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">model_id</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">str</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-variable">self</span>._model_id
<span class="hljs-meta">    @cached_property</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">embedding_size</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">int</span>:
        dummy_embedding = <span class="hljs-variable">self</span>._model.encode(<span class="hljs-string">""</span>)
        <span class="hljs-keyword">return</span> dummy_embedding.shape[<span class="hljs-number">0</span>]
<span class="hljs-meta">    @property</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">max_input_length</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">int</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-variable">self</span>._model.max_seq_length
<span class="hljs-meta">    @property</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">tokenizer</span>(<span class="hljs-params">self</span>) -&gt; AutoTokenizer:
        <span class="hljs-keyword">return</span> <span class="hljs-variable">self</span>._model.tokenizer
    <span class="hljs-keyword">def</span> <span class="hljs-title">__call__</span>(
<span class="hljs-params">        self, input_text: </span><span class="hljs-built_in">str</span><span class="hljs-params"> | </span><span class="hljs-built_in">list</span><span class="hljs-params">[</span><span class="hljs-built_in">str</span><span class="hljs-params">], to_list: </span><span class="hljs-built_in">bool</span><span class="hljs-params"> = </span><span class="hljs-literal">True</span>
<span class="hljs-params">    </span>) -&gt; NDArray[np.float32] | <span class="hljs-built_in">list</span>[<span class="hljs-built_in">float</span>] | <span class="hljs-built_in">list</span>[<span class="hljs-built_in">list</span>[<span class="hljs-built_in">float</span>]]:
        <span class="hljs-keyword">try</span>:
            embeddings = <span class="hljs-variable">self</span>._model.encode(input_text)
        <span class="hljs-keyword">except</span> Exception:
            logger.error(<span class="hljs-string">f"Error generating embeddings for </span><span class="hljs-subst">{self._model_id=}</span><span class="hljs-string"> and </span><span class="hljs-subst">{input_text=}</span><span class="hljs-string">"</span>)
            <span class="hljs-keyword">return</span> [] <span class="hljs-keyword">if</span> to_list <span class="hljs-keyword">else</span> np.array([])
        <span class="hljs-keyword">if</span> to_list:
            embeddings = embeddings.tolist()
        <span class="hljs-keyword">return</span> embeddings
</code></pre>
    <p class="normal">The embedding model <a id="_idIndexMarker414"/>class implements the singleton pattern (<a href="https://refactoring.guru/design-patterns/singleton"><span class="url">https://refactoring.guru/design-patterns/singleton</span></a>), a creational design pattern<a id="_idIndexMarker415"/> that ensures a class has only one instance while providing a global access point to this instance. The <code class="inlineCode">EmbeddingModelSingleton()</code> class inherits from the <code class="inlineCode">SingletonMeta</code> class, which ensures that whenever an <code class="inlineCode">EmbeddingModelSingleton()</code> is instantiated, it returns the same instance. This works well with ML models, as you load them once in memory through the singleton pattern, and afterward, you can use them anywhere in the code base. Otherwise, you risk loading the model in memory every time you use it or loading it multiple times, resulting in <a id="_idIndexMarker416"/>memory issues. Also, this makes it very convenient to access properties such as <code class="inlineCode">embedding_size</code>, where you have to make a dummy forward pass into the embedding model to find the size of its output. As a singleton, you <a id="_idIndexMarker417"/>do this forward pass only once, and then you have it accessible all the time during the program’s execution.</p>
    <h1 id="_idParaDest-132" class="heading-1">Summary</h1>
    <p class="normal">This chapter began with a soft introduction to RAG and why and when you should use it. We also understood how embeddings and vector DBs work, representing the cornerstone of any RAG system. Then, we looked into advanced RAG and why we need it in the first place. We built a strong understanding of what parts of the RAG can be optimized and proposed some popular advanced RAG techniques for working with textual data. Next, we applied everything we learned about RAG to designing the architecture of LLM Twin’s RAG feature pipeline. We also understood the difference between a batch and streaming pipeline and presented a short introduction to the CDC pattern, which helps sync two DBs.</p>
    <p class="normal">Ultimately, we went step-by-step into the implementation of the LLM Twin’s RAG feature pipeline, where we saw how to integrate ZenML as an orchestrator, how to design the domain entities of the application, and how to implement an OVM module. Also, we understood how to apply some software engineering best practices, such as the abstract factory and strategy software patterns, to implement a modular and extendable layer that applies different cleaning, chunking, and embedding techniques based on the data category of each document.</p>
    <p class="normal">This chapter focused only on implementing the ingestion pipeline, which is just one component of a standard RAG application. In <em class="chapterRef">Chapter 9</em>, we will conclude the RAG system by implementing the retrieval and generation components and integrating them into the inference pipeline. But first, in the next chapter, we will explore how to generate a custom dataset using the data we collected and fine-tune an LLM with it. </p>
    <h1 id="_idParaDest-133" class="heading-1">References</h1>
    <ul>
      <li class="bulletList">Kenton, J.D.M.W.C. and Toutanova, L.K., 2019, June. Bert: Pre-training of deep bidirectional transformers for language understanding. In <em class="italic">Proceedings of naacL-HLT</em> (Vol. 1, p. 2).</li>
      <li class="bulletList">Liu, Y., 2019. Roberta: A robustly optimized bert pretraining approach. <em class="italic">arXiv preprint arXiv:1907.11692</em>.</li>
      <li class="bulletList">Mikolov, T., 2013. Efficient estimation of word representations in vector space. <em class="italic">arXiv preprint arXiv:1301.3781</em>.</li>
      <li class="bulletList">Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. <em class="italic">GloVe: Global Vectors for Word Representation</em>. In <em class="italic">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</em> (<em class="italic">EMNLP</em>), pages 1532–1543, Doha, Qatar. Association for Computational Linguistics.</li>
      <li class="bulletList">He, K., Zhang, X., Ren, S. and Sun, J., 2016. Deep residual learning for image recognition. In <em class="italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em> (pp. 770-778).</li>
      <li class="bulletList">Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J. and Krueger, G., 2021, July. Learning transferable visual models from natural language supervision. In <em class="italic">International conference on machine learning</em> (pp. 8748-8763). PMLR.</li>
      <li class="bulletList"><em class="italic">What is Change Data Capture (CDC)? | Confluent</em>. (n.d.). Confluent. <a href="https://www.confluent.io/en-gb/learn/change-data-capture/ "><span class="url">https://www.confluent.io/en-gb/learn/change-data-capture/</span></a></li>
      <li class="bulletList">Refactoring.Guru. (2024, January 1). <em class="italic">Singleton</em>. <a href="https://refactoring.guru/design-patterns/singleton "><span class="url">https://refactoring.guru/design-patterns/singleton</span></a></li>
      <li class="bulletList">Refactoring.Guru. (2024b, January 1). <em class="italic">Strategy</em>. <a href="https://refactoring.guru/design-patterns/strategy "><span class="url">https://refactoring.guru/design-patterns/strategy</span></a></li>
      <li class="bulletList">Refactoring.Guru. (2024a, January 1). <em class="italic">Abstract Factory</em>. <a href="https://refactoring.guru/design-patterns/abstract-factory "><span class="url">https://refactoring.guru/design-patterns/abstract-factory</span></a></li>
      <li class="bulletList">Schwaber-Cohen, R. (n.d.). <em class="italic">What is a Vector Database &amp; How Does it Work? Use Cases + Examples</em>. Pinecone. <a href="https://www.pinecone.io/learn/vector-database/ "><span class="url">https://www.pinecone.io/learn/vector-database/</span></a></li>
      <li class="bulletList">Monigatti, L. (2024, February 19). <em class="italic">Advanced Retrieval-Augmented Generation: From Theory to LlaMaIndex Implementatio</em>n. <em class="italic">Medium</em>. <a href="https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930 "><span class="url">https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930</span></a></li>
      <li class="bulletList">Monigatti, L. (2023, December 6). A guide on 12 tuning Strategies for Production-Ready RAG applications. <em class="italic">Medium</em>. <a href="https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439 "><span class="url">https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439</span></a></li>
      <li class="bulletList">Monigatti, L. (2024b, February 19). <em class="italic">Advanced Retrieval-Augmented Generation: From Theory to LlaMaIndex Implementation</em>. <em class="italic">Medium</em>. <a href="https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930 "><span class="url">https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930</span></a></li>
      <li class="bulletList">Maameri, S. (2024, May 10). Routing in RAG-Driven applications - towards data science. <em class="italic">Medium</em>. <a href="https://towardsdatascience.com/routing-in-rag-driven-applications-a685460a7220  "><span class="url">https://towardsdatascience.com/routing-in-rag-driven-applications-a685460a7220</span></a></li>
    </ul>
    <p class="normal"><a href="https://towardsdatascience.com/routing-in-rag-driven-applications-a685460a7220  "/></p>
    <h1 id="_idParaDest-134" class="heading-1">Join our book’s Discord space</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/llmeng"><span class="url">https://packt.link/llmeng</span></a></p>
    <p class="normal"><span class="url"><img src="../Images/QR_Code79969828252392890.png" alt=""/></span></p>
  </div>
</body></html>