- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformers and Their Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn about transformers and how to apply them to perform
    various NLP tasks. Typical tasks in the NLP domain involve loading and processing
    data so that it can be used downstream seamlessly. Once the data is read, another
    task is that of transforming the data into a form that the various models can
    use. Once the data is transformed into the requisite format, we use it to perform
    the actual tasks, such as classification, text generation, and language translation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a list of the recipes in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading a dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenizing the text in your dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the tokenized text to perform classification with Transformer models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using different Transformer models based on different requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating text by taking a cue from an initial starting sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translating text between different languages using pre-trained Transformer models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this chapter is in the `Chapter08` folder in the GitHub repository
    of the book ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter08](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter08)).
  prefs: []
  type: TYPE_NORMAL
- en: As in previous chapters, the packages required for the chapter are part of the
    `poetry` environment. Alternatively, you can install all the packages using the
    `requirements.txt` file.
  prefs: []
  type: TYPE_NORMAL
- en: Loading a dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to load a public dataset and work with it.
    We will use the `RottenTomatoes` dataset for this recipe as an example. This dataset
    contains ratings and reviews for movies. Please refer to the link at [https://www.kaggle.com/datasets/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset](https://www.kaggle.com/datasets/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset)
    for more information about the dataset
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As part of this chapter, we will use the libraries from the `HuggingFace` site
    ([huggingface.co](http://huggingface.co)). For this recipe, we will use the dataset
    package. You can use the `8.1_Transformers_dataset.ipynb` notebook from the code
    site if you need to work from an existing notebook.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, you will load the `RottenTomatoes` dataset from the `HuggingFace`
    site using the dataset package. This package will download the dataset for you
    if it does not exist. For any subsequent runs, it will use the downloaded dataset
    from the cache if it was downloaded previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'The recipe does the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: Reads in the **RottenTomatoes** dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describes the features of the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loads the data from the training split of the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Samples a few sentences from the dataset and prints them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The steps for the recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the necessary imports to import the necessary types and functions from the
    datasets package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load **"rotten tomatoes"** via the **load_dataset** function and print the
    internal dataset splits. This dataset contains the train, validation, and test
    splits:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding command would be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset and print the attributes of the train split. The **training_data.description**
    describes the dataset details and the **training_data.features** describes the
    features of the dataset. In the output, we can see that the **training_data**
    split contains the features **text**, which is of the type string, and **label**,
    which is of type categorical, with the values **neg** and **pos**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the command is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have loaded the dataset, we will print the first five sentences
    from it. This is just to confirm that we are indeed able to read from the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the command is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Tokenizing the text in your dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The components contained within the transformer do not have any intrinsic knowledge
    of the words that it processes. Instead, the tokenizer only uses the token identifiers
    for the words that it processes. In this recipe, we will learn how to transform
    the text in your dataset into a representation that can be used by the models
    for downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As part of this recipe, we will use the `AutoTokenizer` module from the transformers
    package. You can use the `8.2_Basic_Tokenization.ipynb` notebook from the code
    site if you need to work from an existing notebook.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, you will continue from the previous example of using the `RottenTomatoes`
    dataset and sampling a few sentences from it. We will then encode the sampled
    sentences into tokens and their respective representations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The recipe does the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: Loads a few sentences into memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instantiates a tokenizer and tokenizes the sentences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converts the token IDs generated from the previous step back into the tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The steps for the recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the necessary imports to import the necessary **AutoTokenizer** module from
    the **transformers** library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We initialize a sentence array consisting of three sentences that we will use
    for this example. These sentences are of different lengths and have a good combination
    of the same and different words. This will allow us to understand how the tokenized
    representation varies for each of them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a tokenizer of the **bert-base-cased** type. This tokenizer is
    case-sensitive. This means that the words star and STAR will have different tokenized
    representations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we tokenize all the sentences in the **sentences** array. We
    call the tokenizer constructor and pass it the **sentences** array as an argument
    followed by printing the **tokenized_output** instance returned by the constructor
    function. This object is a dictionary of three items:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**input_ids**: These are the numerical token identifiers that are assigned
    to each token.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_type_ids**: These IDs define the type of tokens that are contained
    in the sentences.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_mask**: These define the attention values for each token in the
    input. This mask determines what tokens are paid attention to when downstream
    tasks are performed. These values are floats and can vary from 0 (no attention)
    to 1 (full attention).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we take the input IDs of the first sentence and convert them
    back into tokens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Converting them into tokens returns the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In addition to the original tokens, the tokenizer adds the `[CLS]` and `[SEP]`.
    These tokens were added for the training tasks that were performed to train BERT.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we have learned about the internal representation of the text used
    by the transformer internally, let us learn how we can classify a piece of text
    into different categories.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Classifying text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the `RottenTomatoes` dataset and classify the review
    texts for sentiment. We will classify the test split of the dataset and evaluate
    the results of the classifier against the true labels in the test split of the
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As part of this recipe, we will use the pipeline module from the transformers
    package. You can use the `8.3_Classification_And_Evaluation.ipynb` notebook from
    the code site if you need to work from an existing notebook.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, you will continue from the previous example using the `RottenTomatoes`
    dataset and sample a few sentences from it. We will then classify a small subset
    of five sentences for sentiment classification and demonstrate the results on
    this smaller subset. We will then perform inference on the whole test split of
    the dataset and evaluate the results of the classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'The recipe does the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: Loads the **RottenTomatoes** dataset and prints the first five sentences from
    it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instantiates a pipeline with a pre-trained Roberta model that has been trained
    on the same dataset for sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performs inference (or sentiment prediction) on the whole test split of the
    dataset using the pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluates the results of the inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The steps for the recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the necessary imports to import the required packages and modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we probe for the presence of a **Compute Unified Device Architecture**
    (**CUDA**) compatible device (or **Graphics Processing Unit** (**GPU**)) present
    in the system. If such a device is present, our model will be loaded on it. This
    accelerates the training and inference performance of a model if it is supported.
    However, if such a device is not present, the **Central Processing Unit** (**CPU**)
    will be used. We also load the **RottenTomatoes** dataset and select the first
    five sentences from it. This is to ensure we are indeed able to read the data
    present in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the pipeline for sentiment analysis via a pipeline. A pipeline is
    an abstraction that allows us to easily use models or inference tasks without
    having to write the code to piece them together. We load the **roberta-base-rotten-tomatoes**
    model from **textattack**, which has been trained on this dataset. In the following
    segment, we use the pipeline for the sentiment analysis task and set a specific
    model to be used for this task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we generate the predictions for the small subset of sentences
    that we selected in step 2\. Using the pipeline object to generate predictions
    is as easy as just passing it a series of sentences. If you are running this example
    on a machine without a compatible CUDA device, this step might take a little time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we iterate through our sentences and check the predictions for
    our sentences. We print the actual and generated predictions along with the sentence
    text for the five sentences. The actual labels are read from the dataset, whereas
    the predictions are generated via the pipeline object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have validated the pipeline and its results, let’s generate the
    inference for the whole test set and generate the evaluation measures of this
    particular model. Load the complete test split for the **RottenTomatoes** dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize an evaluator object that can be used to perform
    the inference along with evaluating the results of the classification. It can
    also be used to present an easy-to-read summary of the evaluation results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we call the **compute** method on the **evaluator** instance.
    This triggers the inference and the evaluation using the same pipeline instance
    that we initialized in step 4\. It returns the evaluation metrics of **accuracy**,
    **precision**, **recall**, and **f1**, along with some performance metrics related
    to inference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we print the results of the evaluation. Of note are the **precision**,
    **recall**, and **f1** values. An **f1** of **0.88**, observed in this case, is
    an indicator of the very good efficacy of the classifier, though it could always
    be improved further:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this recipe, we used a pre-trained classifier to classify the data on a dataset.
    The dataset and model were both for sentiment analysis. There are cases where
    we can use classifiers that are trained on a different class of data but can still
    be used as is. This saves us from having to train a classifier of our own and
    repurpose a model that already exists. We will learn about this use case in the
    next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Using a zero-shot classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will classify a sentence using a zero-shot classifier. There
    are instances where we do not have the luxury of training a classifier from scratch
    or using a model that has been trained as per the labels of our data. **Zero-shot
    classification** can be used in such scenarios for any team to get up and running
    quickly. The zero in the terminology means that the classifier has not seen any
    data (zero samples precisely) from the target dataset that will be used for inference.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As part of this recipe, we will use the pipeline module from the transformers
    package. You can use the `8.4_Zero_shot_classification.ipynb` notebook from the
    code site if you need to work from an existing notebook.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will use a couple of sentences and classify them. We will
    use our own set of labels for these sentences. We will use the `facebook/bart-large-mnli`
    model for this recipe. This model is suitable for the task of zero-shot classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'The recipe does the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: Initializes a pipeline based on a zero-shot classification model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uses the pipeline to classify a sentence into a custom set of user-defined labels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prints the results of the classification with the classes and their associated
    probabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The steps for the recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the necessary imports and identify the compute device, as described in the
    previous classification recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize a pipeline instance with the **facebook/bart-large-mnli**
    model. We have chosen this particular model for our example, but other models
    can also be used – available on the **HuggingFace** site:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the pipeline instance to classify a sentence into a given set of candidate
    labels. The labels provided in the example are completely novel and have been
    defined by us. The model was not trained on examples with these labels. The classification
    output is stored in the **result** variable, which is a dictionary. This dictionary
    has the **''sequence''**, **''labels''**, and **''scores''** keys. The **''sequence''**
    element stores the original sentence passed to the classifier. The **''labels''**
    element stores the labels for the classes, but the ordering is different than
    what we passed in the arguments. The **''scores''** element stores the probabilities
    of the classes and corresponds to the same ordering in the **''labels''** element.
    The last argument in this call is **device**. If there is a CUDA-compatible device
    present in the system, it will be used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We print the sequence, followed by printing each label and its associated probability.
    Note that the order of the labels has changed from the initial input that we specified
    in the previous step. The function call reorders the labels based on the descending
    order of the label probability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We run a zero-shot classification on a different sentence and print the results
    for it. This time, we emit a result that picks the class with the highest probability
    and prints the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: So far, we have used the transformer and some pre-trained models to generate
    token IDs and classifications. These recipes have used the encoder part of the
    transformer. The encoder generates a representation of the text, which is then
    used by a classifier head in front of it to generate classification labels. However,
    the transformer has another component, called the decoder. A decoder uses a given
    representation of text and generates subsequent text. In the next recipe, we will
    learn more about the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Generating text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use a **generative transformer model** to generate text
    from a given seed sentence. One such model to generate text is the GPT-2 model,
    which is an improved version of the original **General Purpose Transformer** (**GPT**)
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As part of this recipe, we will use the pipeline module from the transformers
    package. You can use the `8.5_Transformer_text_generation.ipynb` notebook from
    the code site if you need to work from an existing notebook.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will start with an initial seed sentence and use the GPT-2
    model to generate text based on the given seed sentence. We will also tinker with
    certain parameters to improve the quality of the generated text.
  prefs: []
  type: TYPE_NORMAL
- en: 'The recipe does the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: It initializes a starting sentence from which a continuing sentence will be
    generated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It initializes a GPT-2 model as part of a pipeline and uses it to generate five
    sentences as part of the parameters passed to the generator method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It prints the results of the generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The steps for the recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the necessary imports and identify the compute device, as described in the
    previous classification recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize a seed input sentence based on which the subsequent text will be
    generated. Our goal here is to use the GPT-2 decoder to hypothetically generate
    the text that follows it based on the generation parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize a text-generation pipeline with the **''gpt-2''**
    model. This model is based on a **large language model** (**LLM**) that was trained
    using a large text corpus. The last argument in this call is **device**. If there
    is a CUDA-compatible device present in the system, it will be used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate the continuing sequence for the seed sentence and store the results.
    The parameters of note used in the call other than the seed text are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**max_length**: The maximum length of the generated sentence, including the
    length of the seed sentence.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_return_sequences**: The number of generated sequences to return.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_beams**: This parameter controls the quality of the generated sequence.
    A higher number generally results in improved quality of the generated sequence
    but also slows down the generation. We encourage you to try out different values
    for this parameter based on the quality requirements of the generated sequence.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the generated sentences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we can see in the preceding example, the generated output was rudimentary,
    repetitive, grammatically incorrect, or perhaps incoherent. There are different
    techniques that we can use to improve the generated output.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the `no_repeat_ngram_size` parameter this time to generate the text.
    We will set the value of this parameter to `2`. This instructs the generator to
    not repeat bi-grams.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will change the line in *step 4* to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see in the following output, the sentences have reduced repetition,
    but some of them are still incoherent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: To improve the coherency, we can use another technique to include the next word
    from a set of words that have the highest likelihood of being the next word. We
    use the `top_k` parameter and set its value to `50`. This instructs the generator
    to sample the next word from the top 50 words, arranged according to their probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'We change the line in *step 4* to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also combine the `top_k` parameter with the `top_p` parameter. This
    instructs the generator to select the next word from the set of words that have
    a probability higher than this defined value. Adding this parameter with a value
    of `0.8` yields the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the addition of additional parameters to the generator continues
    to improve the generated output.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a final example, let us generate a longer output sequence by changing the
    line in *step 4* to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the generated output, however fictitious, is more coherent and
    readable. We encourage you to experiment with different mixes of parameters and
    their respective values to improve the generated output based on their use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the output returned by the model might differ a bit from what
    this example has shown. This happens because the internal language model is probabilistic
    in nature. The next word is sampled from a distribution that contains words that
    have a probability larger than what we defined in our parameters for generation.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we used the decoder module of the transformer to generate text,
    given a seed sentence. There are use cases where an encoder and decoder are used
    together to generate text. We will learn about this in the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Language translation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use transformers for language translation. We will use
    the **Google Text-To-Text Transfer Transformer** (**T5**) model. This model is
    an end-to-end model that uses both the encoder and decoder components of the transformer
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As part of this recipe, we will use the pipeline module from the transformers
    package. You can use the `8.6_Language_Translation_with_transformers.ipynb` notebook
    from the code site if you need to work from an existing notebook.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, you will initialize a seed sentence in English and translate
    it to French. The T5 model expects the input format to encode the information
    about the language translation task along with the seed sentence. In this case,
    the encoder uses the input in the source language and generates a representation
    of the text. The decoder uses this representation and generates text for the target
    language. The T5 model is trained specifically for this task, in addition to many
    others. If you are running on a machine that does not have a CUDA-compatible device,
    it might take some time for the recipe steps to be executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The recipe does the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: It initializes the **Google t5-base** model and tokenizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It initializes a seed sentence in English that will be translated into French
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It tokenizes the seed sentence along with the task specification to translate
    the seed sentence into French
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It generates the translated tokens, decodes them into the target language (French),
    and prints them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The steps for the recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the necessary imports and identify the compute device, as described in the
    previous classification recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize a tokenizer and model instance with the **t5-base** model from Google.
    We use the **model_max_length** parameter of **200**. Feel free to experiment
    with higher values if your seed sentence is longer than 200 words. We also load
    the model onto the device that was identified for computation in step 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize a seed sequence that you want to translate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Tokenize the input sequence. The tokenizer specifies the source and the target
    language as part of its input encoding. This is done by appending the “**translate
    English to French:**” text to the input seed sequence. We load these token IDs
    into the device that is used for computation. It is a requirement for both the
    model and the token IDs to be on the same device:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Translate the source language token IDs to the target language token IDs via
    the model. The model uses the encoder-decoder architecture to convert the input
    token IDs to the output token IDs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Decode the text from the token IDs to the target language tokens. We use the
    tokenizer to convert the output token IDs to the target language tokens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the translated output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In conclusion, this chapter introduced the concept of transformers, along with
    some of its basic applications. The next chapter will focus on how we can use
    the different NLP techniques to understand text better.
  prefs: []
  type: TYPE_NORMAL
