- en: Training Autoencoders Using RNNSharp
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be discussing **autoencoders** and their usage. We
    will talk about what an autoencoder is, the different types of autoencoder, and
    present different samples to help you better understand how to use this technology
    in your applications. By the end of this chapter, you will know how to design
    your own autoencoder, load and save it from disk, and train and test it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics are covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What is an autoencoder?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different types of autoencoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating your own autoencoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will require Microsoft Visual Studio.
  prefs: []
  type: TYPE_NORMAL
- en: What is an autoencoder?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An autoencoder is an unsupervised learning algorithm that applies back propagation
    and sets target values equal to the inputs. An autoencoder learns to compress
    data from the input layer into shorter code, and then it uncompresses that code
    into something that closely matches the original data. This is better known as
    **dimensionality reduction**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a depiction of an autoencoder. The original images are encoded,
    and then decoded to reconstruct the original:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b65f99cf-4805-402c-9e69-518888711abb.png)'
  prefs: []
  type: TYPE_IMG
- en: Different types of autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are different types of autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d624ffb-80b9-4c7a-a52b-138d50241b5f.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's briefly discuss autoencoders and the variants we have just seen. Please
    note that there are other variants out there; these are just probably the most
    common that I thought you should at least be familiar with.
  prefs: []
  type: TYPE_NORMAL
- en: Standard autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An autoencoder learns to compress data from the input layer into smaller code,
    and then uncompress that code into something that (hopefully) matches the original
    data. The basic idea behind a standard autoencoder is to encode information automatically,
    hence the name. The entire network always resembles an hourglass, in terms of
    its shape, with fewer hidden layers than input and output layers. Everything up
    to the middle layer is called the encoding part, everything after the middle layer
    is called the decoding part, and the middle layer itself is called, as you have
    probably guessed, the code. You can train autoencoders by feeding input data and
    setting the error status as the difference between the input and what came out.
    Autoencoders can be built so that encoding weights are the same as decoding weights.
  prefs: []
  type: TYPE_NORMAL
- en: Variational autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Variational autoencoders** have the same architecture as autoencoders but
    are taught something else: an approximated probability distribution of the input
    samples. This is because they are a bit more closely related to Boltzmann and
    Restricted Boltzmann Machines. They do however rely on Bayesian mathematics as
    well as a re-parametrization trick to achieve this different representation. The
    basics come down to this: taking influence into account. If one thing happens
    in one place, and something else happens somewhere else, they are not necessarily
    related. If they are not related, then error propagation should consider that.
    This is a useful approach, because neural networks are large graphs (in a way),
    so it helps to be able to rule out the influence some nodes have on other nodes
    as you dive into deeper layers.'
  prefs: []
  type: TYPE_NORMAL
- en: De-noising autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**De-noising autoencoders** reconstruct the input from a corrupted version
    of themselves. This does two things. First, it tries to encode the input while
    preserving as much information as possible. Second, it tries to undo the effect
    of the corruption process. The input is reconstructed after a percentage of the
    data has been randomly removed, which forces the network to learn robust features
    that tend to generalize better. De-noising autoencoders are autoencoders where
    we feed the input data with noise (such as making an image grainier). We compute
    the error the same way, so the output of the network is compared to the original
    input without noise. This encourages the network to learn not details but broader
    features, which are usually more accurate, as they are not affected by constantly
    changing noise.'
  prefs: []
  type: TYPE_NORMAL
- en: Sparse autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Sparse autoencoders** are, in a way, the opposite of autoencoders. Instead
    of teaching a network to represent information in less space or fewer nodes, we
    try to encode information in more space. Instead of the network converging in
    the middle and then expanding back to the input size, we blow up the middle. These
    types of network can be used to extract many small features from a dataset. If
    you were to train a sparse autoencoder the same way as an autoencoder, you would
    in almost all cases end up with a pretty useless identity network (as in, what
    comes in is what comes out, without any transformation or decomposition). To prevent
    this, we feed back the input, plus what is known as a **sparsity driver**. This
    can take the form of a threshold filter, where only a certain error is passed
    back and trained. The other error will be irrelevant for that pass and will be
    set to zero. In a way, this resembles spiking neural networks, where not all neurons
    fire all the time.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating your own autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you are an expert on autoencoders, let's move on to less theory and
    more practice. Let's take a bit of a different route on this one. Instead of using
    an open-source package and showing you how to use it, let's write our own autoencoder
    framework that you can enhance to make your own. We'll discuss and implement the
    basic pieces needed, and then write some sample code showing how to use it. We
    will make this chapter unique in that we won't finish the usage sample; we'll
    do just enough to get you started along your own path to autoencoder creation.
    With that in mind, let's begin.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start off by thinking about what an autoencoder is and what things we
    would want to include. First off, we're going to need to keep track of the number
    of layers that we have. These layers will be Restricted Boltzmann Machines for
    sure. Just so you know, we'll also refer to **Restricted Boltzmann Machines** as **RBMs** from
    time to time, where brevity is required.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we know we''ll need to track the number of layers that our autoencoder
    has. We''re also going to need to monitor the weights we''ll need to use: learning
    rate, recognition weights, and generative weights. Training data is important,
    of course, as are errors. I think for now that should be it. Let''s block out
    a class to do just this.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with an `interface`, which we will use to calculate errors. We
    will only need one method, which will calculate the error for us. The RBM will
    be responsible for doing this, but we''ll get to that later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we define our RBM class, we''ll need to look at the layers that it will
    use. To best represent this, we''ll create an `abstract` class. We''ll need to
    track the state of the layer, the bias used, the amount of bias change, the activity
    itself, and how many neurons it will have. Rather than distinguish between mirror
    and canonical neurons, we''ll simply represent all neuron types as one single
    object. We also will need to have multiple types of RBM layer. Gaussian and binary
    are two that come to mind, so the following will be the base class for those layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We must keep in mind that our RBM will need to track its weights. Since weights
    are applied through layers with a thing called a **synapse**, we''ll create a
    class to represent all we want to do with weights. Since we''ll need to track
    the weights, their changes, and the pre- and post-size, let''s just create a class
    that encapsulates all of that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, as our learning rate encompasses features such as weights, biases, and
    momentum, we will be best served if we create a separate class to represent all
    of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s create a class that encompasses our training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'With all of this now defined, let''s go ahead and work on our `RestrictedBoltzmannMachine`
    class. For this class, we''ll need to keep track of how many visible and hidden
    layers we have, the weights and learning rate we will use, and our training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And, finally, with everything else in place, let''s create our `Autoencoder`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Even though we know there will be a lot more required for some of these classes,
    this is the basic framework that we need to get started framing in the rest of
    the code. To do that, we should think about some things.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since weights are a prominent aspect of our autoencoder, we are going to have
    to use and initialize weights quite often. But how should we initialize our weights,
    and with what values? We will provide two distinct choices. We will either initialize
    all weights to zero, or use a Gaussian. We will also have to initialize the biases
    as well. Let''s go ahead and create an interface from which to do this so it will
    make it easier later to select the type of initialization we want (zero or Gaussian):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We mentioned earlier that we needed to have multiple types of RBM layer to
    use. Gaussian and binary were two that came to mind. We have already created the
    interface for that, so let''s go ahead and put our base classes into the form,
    as we will need them shortly. To do this, we will need to expand our RBM layer
    class and add two abstract methods so that they can be cloned, and so that we
    can set the state of the layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `RestrictedBoltzmannMachineLayer` class now looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'With our very basic autoencoder in place, we should now turn our attention
    to how we will build our autoencoder. Let''s try to keep things as modular as
    possible and, with that in mind, let''s create an `AutoEncoderBuilder` class that
    we can have encapsulate things such as weight initialization, adding layers, and
    so forth. It will look something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have this class blocked in, let''s begin to add some meat to it
    in the form of functions. We know that when we build an autoencoder we are going
    to need to add layers. We can do that with this function. We will pass it the
    layer, and then update our internal learning-rate layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have this base function, we can then add some higher-level functions,
    which will make it easier for us to add layers to our autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s add a `Build()` method to our autoencoder builder to make it
    easy to build:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s turn our attention to our autoencoder itself. We are going to need
    a function to help us initialize our biases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we are going to need to initialize our training data. This will basically
    involve creating all the arrays that we need and setting their initial values
    to zero as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'With that behind us, we''re off to a good start. Let''s start to use the software
    and see what we''re missing. Let''s create our `builder` object, add some binary
    and Gaussian layers, and see how it looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Not bad, right? So, what''s next? Well, we''ve got our autoencoder created
    and have added layers. We now lack functions to allow us to fine tune and train
    learning rates and momentum. Let''s see how they would look if we were to add
    them here as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'That looks about right. At this point, we should add these functions into our `autoencoderbuilder`
    object so we can use them. Let''s see how that would look. Remember that with
    our builder object we automatically created our learning rate object, so now we
    just have to use it to populate things such as our weights and biases, along with
    the momentum weights and biases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Well, let''s now stop and take a look at what our sample program is turning
    out to look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Not bad. All we should need to do now is to call our `Build()` method on our
    `builder` and we should have the first version of our framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'With all this now complete,and looking back at the preceding code, I think
    at some point we are going to need to be able to gain access to our individual
    layers; what do you think? Just in case, we''d better provide a function to do
    that. Let''s see how that would look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Since our internal layers are `RestrictedBoltzmannMachine` layers, that is
    the type that we should be returning, as you can see from the previous code. The
    `GetLayer()` function needs to reside inside our autoencoder object, though, not
    the builder. So, let''s go ahead and add it now. We''ll need to be good developers
    and make sure that we have a bounds check to ensure that we are passing a valid
    layer index before we try to use it. We''ll store all those neat little utility
    functions in a class of their own, and we might as well call it `Utility`, since
    the name makes sense. I won''t go into how we can code that function, as I am
    fairly confident that every reader already knows how to do bounds checks, so you
    can either make up your own or look at the accompanying source code to see how
    it''s done in this instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: OK, so we can now create our autoencoders, set weights and biases, and gain
    access to individual layers. I think the next thing we need to start thinking
    about is training and testing. We'll need to take each separately, of course,
    so why don't we start with training?
  prefs: []
  type: TYPE_NORMAL
- en: 'We will need to be able to train our RBM, so why don''t we create an object
    dedicated to doing this. We''ll call it, no surprise here, `RestrictedBoltzmannMachineTrainer`. Again,
    we are going to need to deal with our `LearningRate`, object, and weight sets,
    so let''s make sure we add them as variables right away:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, what functions do you think we will need for our trainer? Obviously, a
    `Train()` method is required; otherwise, we named our object incorrectly. We''ll
    also need to train our weights and layer biases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Last, but not least, we should probably have a `helper` function that lets
    us know the training amount, which for us will involve taking the positive visible
    amount times the positive hidden amount and subtracting that from the negative
    visible amount times the negative hidden amount:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'OK, let''s see where our program stands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Nice. Can you see how it's all starting to come together? Now it's time to consider
    how we are going to add data to our network. Before we do any kind of training
    on the network, we will need to load data. How will we do this? Let's consider
    the notion of pre-training. This is the act of loading data into the network manually
    before we train it. What would this function look like in the context of our program?
    How about something such as this?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We would just need to tell our autoencoder which layer we want to populate
    with data, and then supply the data. That should work for us. If we did this,
    then the following is how our program would evolve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: What do you think so far? With this code, we would be able to populate three
    layers with data. I threw in an extra function, `PreTrainingComplete`, as a nice
    way to let our program know that we have finished pre-training. Now, let's figure
    out how those functions come together.
  prefs: []
  type: TYPE_NORMAL
- en: For pretraining, we will do this in batches. We can have from 1 to *n* number
    of batches. In many cases, the number of batches will be just 1\. Once we determine
    the number of batches we want to use, we will iterate through each batch of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each batch of data, we will process the data and determine whether our
    neurons were activated. We then set the layer state based upon that. We will move
    both forward and backward through the network, setting our states. Using the following
    diagram, we will move forward through layers like this *Y -> V -> W -> (Z)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f4bef37-b92f-4a94-ab1b-1bbb8035a003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once activations are set, we must perform the actual pre-training. We do this
    in the pre-synaptic layer, starting at layer 0\. When we pre-train, we call our
    trainer object''s `Train` method, which we created earlier and then pass the layer(s)
    and the training data, our recognition weights, and learning rate. To do this,
    we will need to create our actual function, which we will call `PerformPreTraining()`.
    The following is what this code would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Once pre-training is complete, we now will need to calculate the error rate
    based upon the positive and negative visible data properties. That will complete
    our `pretraining` function, and our sample program will now look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'With all this code behind us, all we need to do now is to save the autoencoder,
    and we should be all set. We will do this by creating a `Save()` function in the
    autoencoder, and call it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: To implement this function, let's look at what we need to do. First, we need
    a filename to use for the autoencoder name. Once we open a **.NET TextWriter**
    object, we then save the learning rates, the recognition weights, and generative
    weights. Next, we iterate through all the layers, write out the layer type, and
    then save the data. If you decide to implement more types of RBM layers than we
    created, make sure that you in turn update the `Save()` and `Load()` methods so
    that your new layer data is saved and re-loaded correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at our `Save` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: With our autoencoder saved to disk, we now should really deal with the ability
    to reload that data into memory and create an autoencoder from it. So, we'll now
    need a `Load()` function. We'll need to basically follow the steps we did to write
    our autoencoder to disk but, this time, we'll read them in, instead of writing
    them out. Our weights, learning rate, and layers will have also a `Load()` function,
    just like each of the preceding items had a `Save()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `Load()` function will be a bit different in its declaration. Since we
    are loading in a saved autoencoder, we have to assume that, at the time this call
    is made, an autoencoder object has not yet been created. Therefore, we will make
    this function `static()` on the autoencoder object itself, as it will return a
    newly created autoencoder for us. Here''s how our function will look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'With that done, let''s see how we would call our `Load()` function. It should
    be like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'So, let''s stop here and take a look at all we''ve accomplished. Let''s see
    what our program can do, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well, folks, I think it's time to wrap this chapter up and move on. You should
    commend yourself, as you've written a complete autoencoder from start to (almost)
    finish. In the accompanying source code, I have added even more functions to make
    this more complete, and for you to have a better starting point from which to
    make this a powerful framework for you to use. As you are enhancing this, think
    about the things you need your autoencoder to do, block in those functions, and
    then complete them as we have done. Rather than learn to use an open-source framework,
    you've built your own—congratulations!
  prefs: []
  type: TYPE_NORMAL
- en: I have taken the liberty of developing a bit more of our autoencoder framework
    with the supplied source code. You can feel free to use it, discard it, or modify
    it to suit your needs. It's useful, but, as I mentioned, please feel free to embellish
    this and make it your own, even if it's just for educational purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s briefly recap what we have learned in this chapter: we learned about
    autoencoders and different variants, and we wrote our own autoencoder and created
    some powerful functionality. In the next chapter, we are going to move on to perhaps
    my most intense passion, and I hope it will soon be yours, **swarm intelligence**.
    There''s going to be some theory, of course, but, once we''re discussed that,
    I think you''re going to be impressed with what particle swarm optimization algorithms
    can do!'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vincent P, La Rochelle H, Bengio Y, and Manzagol P A (2008), *Extracting and
    Composing Robust Features with Denoising Autoencoders*, proceedings of the 25th
    international conference on machine learning (ICML, 2008), pages 1,096 - 1,103,
    ACM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vincent, Pascal, et al, *Extracting and Composing Robust Features with De-noising
    Autoencoders.,* proceedings of the 25th international conference on machine learning.
    ACM, 2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma, Diederik P and Max Welling*, Auto-encoding variational bayes,* arXiv
    pre-print arXiv:1312.6114 (2013).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Marc'Aurelio Ranzato, Christopher Poultney, Sumit Chopra, and Yann LeCun, *Efficient
    Learning of Sparse Representations with an Energy-Based Model,* proceedings of
    NIPS, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bourlard, Hervé, and Yves Kamp, *Auto Association by Multilayer Perceptrons
    and Singular Value Decomposition, Biological Cybernetics 59.4–5* (1988): 291-294.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
