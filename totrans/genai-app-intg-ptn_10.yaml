- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Embedding Responsible AI into Your GenAI Applications
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将负责任的人工智能嵌入您的生成式人工智能应用
- en: In the previous chapters, we explored various integration patterns and operational
    considerations for leveraging **Generative AI** (**GenAI**) models like Google
    Gemini on Vertex AI. As we implement these powerful technologies, it’s crucial
    to address the ethical implications and responsibilities that come with building
    and deploying AI models that will be added to your applications. This chapter
    will focus on best practices for responsible AI, ensuring that our GenAI applications
    are fair, interpretable, private, and safe.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们探讨了在Vertex AI上利用像谷歌Gemini这样的生成式人工智能（**GenAI**）模型的各种集成模式和操作考虑因素。随着我们实施这些强大的技术，解决与构建和部署将被添加到您的应用程序中的AI模型相关的伦理影响和责任至关重要。本章将重点介绍负责任人工智能的最佳实践，确保我们的生成式人工智能应用是公平的、可解释的、私密的和安全的。
- en: 'In this chapter, we’ll cover:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖：
- en: Introduction to responsible AI
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负责任的人工智能简介
- en: Fairness in GenAI applications
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成式人工智能应用中的公平性
- en: Interpretability and explainability
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释性和可解释性
- en: Privacy and data protection
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐私和数据保护
- en: Safety and security in GenAI systems
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成式人工智能系统中的安全和安保
- en: Google’s approach to responsible AI
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌对负责任人工智能的方法
- en: Anthropic’s approach to responsible AI
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic对负责任人工智能的方法
- en: Introduction to responsible AI
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负责任的人工智能简介
- en: Responsible AI is an approach to developing and deploying AI systems that prioritizes
    ethical considerations, transparency, and accountability.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的人工智能是一种开发和应用人工智能系统的方法，它优先考虑伦理考量、透明度和问责制。
- en: 'As GenAI models and applications such as Google’s Gemini, OpenAI’s GPT, and
    Anthropic’s Claude become increasingly powerful and widely used, it’s essential
    to ensure that these systems are designed and implemented in ways that benefit
    society while minimizing potential harm. Let’s explore, at a high level, the key
    aspects of implementing responsible AI in your systems. We will also talk about
    how over-indexing on the following topics can have a negative effect on innovation:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着生成式人工智能模型以及像谷歌的Gemini、OpenAI的GPT和Anthropic的Claude这样的应用变得越来越强大和广泛使用，确保这些系统以造福社会并最大限度地减少潜在危害的方式进行设计和实施变得至关重要。让我们从高层次上探讨在您的系统中实施负责任人工智能的关键方面。我们还将讨论过度关注以下主题可能对创新产生的负面影响：
- en: '**Fairness**: Achieving fairness in AI systems is a crucial goal that requires
    thoughtful design and implementation throughout the entire AI lifecycle. Several
    key factors contribute to making AI systems fair. First and foremost is the quality
    and diversity of the training data. Ensuring that the dataset represents a wide
    range of demographics, experiences, and perspectives helps to minimize bias and
    promote equitable outcomes. This involves not just collecting diverse data, but
    also carefully curating and balancing it to avoid over- or under-representation
    of certain groups, ensuring AI systems don’t perpetuate or amplify biases. While
    aiming for fairness is crucial, it’s complex to implement. Different definitions
    of fairness can conflict with each other. For example, achieving demographic parity
    might not always align with equal opportunity. Overcompensating for historical
    biases could potentially create new forms of discrimination. For instance, an
    AI hiring system can be so focused on demographic parity that it overlooks genuine
    qualifications, potentially leading to more capable candidates being overlooked.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公平性**：在人工智能系统中实现公平是一个至关重要的目标，它需要在人工智能生命周期的整个过程中进行深思熟虑的设计和实施。几个关键因素有助于使人工智能系统公平。首先和最重要的是训练数据的质量和多样性。确保数据集代表广泛的群体、经验和观点有助于最小化偏见并促进公平的结果。这不仅仅涉及收集多样化的数据，还包括精心策划和平衡数据，以避免某些群体的过度或不足代表，确保人工智能系统不会持续或放大偏见。虽然追求公平至关重要，但实施起来很复杂。不同的公平定义可能相互冲突。例如，实现人口统计学上的平等可能并不总是与平等机会相一致。过度补偿历史偏见可能会产生新的歧视形式。例如，一个人工智能招聘系统可能会过于关注人口统计学上的平等，以至于忽视了真正的资格，这可能导致更有能力的候选人被忽视。'
- en: '**Interpretability**: This is about understanding how AI systems make decisions.
    Highly interpretable models might sacrifice performance for explainability. This
    trade-off could be particularly problematic in fields like medical diagnosis or
    financial forecasting, where complex patterns might be crucial for accurate predictions.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释性**：这是关于理解人工智能系统如何做出决策。高度可解释的模型可能会为了可解释性而牺牲性能。这种权衡在医疗诊断或金融预测等需要复杂模式进行准确预测的领域可能特别有问题。'
- en: '**Privacy**: This involves protecting user data and respecting privacy rights.
    Strict privacy measures can hinder beneficial data sharing and collaborative research.
    They might also increase costs for businesses, potentially stifling innovation
    in smaller companies that can’t afford robust privacy protection measures. For
    example, overly strict data protection might prevent the creation of the large,
    diverse datasets needed for developing treatments for rare diseases.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私**：这涉及到保护用户数据和尊重隐私权利。严格的隐私措施可能会阻碍有益的数据共享和协作研究。它们也可能增加企业的成本，可能抑制无法承担强大隐私保护措施的小公司的创新。例如，过于严格的数据保护可能会阻止创建用于开发罕见病治疗所需的大型、多样化的数据集。'
- en: '**Safety**: This is where you ensure that AI systems behave as intended and
    don’t cause harm. Rigorous safety testing can significantly slow down the development
    and deployment of AI systems. This could delay the introduction of potentially
    life-saving technologies. Additionally, overly cautious approaches might lead
    to missed opportunities for learning from controlled, real-world testing.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性**：这是确保人工智能系统按预期行为并不会造成伤害的地方。严格的安全测试可能会显著减缓人工智能系统的发展和部署。这可能会延迟具有潜在救命技术的引入。此外，过于谨慎的方法可能会导致错过从受控的、现实世界的测试中学习的机会。'
- en: '**Accountability**: This is about taking responsibility for AI system outcomes.
    Clear lines of accountability are necessary; overly punitive measures could discourage
    innovation. It might also lead to a culture of blame rather than learning and
    improvement when issues do arise.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问责制**：这是关于对人工智能系统结果承担责任。清晰的问责制界限是必要的；过于严厉的措施可能会阻碍创新。它还可能导致当问题出现时，形成一种归咎于而不是学习和改进的文化。'
- en: 'There is no secret recipe for getting all these aspects correct. Companies
    need to find the balance and address these concerns in practice. Let’s consider
    the following approaches to minimize concerns about over-indexing in rigid approaches:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 没有获得所有这些方面正确的秘密配方。公司需要找到平衡，并在实践中解决这些关注。让我们考虑以下方法来最小化对僵化方法过度依赖的担忧：
- en: '**Contextual application**: Recognize that the importance of each aspect may
    vary depending on the specific AI application and its potential impact. For example,
    interpretability might be more crucial in medical diagnosis AI than in a movie
    recommendation system.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情境应用**：认识到每个方面的重要性可能因具体的人工智能应用及其潜在影响而异。例如，可解释性在医疗诊断人工智能中可能比在电影推荐系统中更为关键。'
- en: '**Stakeholder engagement**: Involve diverse stakeholders in the development
    and deployment process. This can help identify potential issues and trade-offs
    early on. Adopt a risk-based approach where the level of scrutiny and safeguards
    is proportional to the potential harm or impact of the AI system.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利益相关者参与**：在开发和部署过程中涉及多样化的利益相关者。这有助于早期识别潜在的问题和权衡。采用基于风险的策略，其中审查和保障措施的水平与人工智能系统的潜在危害或影响成比例。'
- en: '**Transparency and monitoring**: Be open about the limitations and trade-offs
    of your AI system. This can help manage expectations and build trust. Regularly
    assess the performance and impact of AI systems in real-world use and be prepared
    to make adjustments. Implement these principles progressively, starting with minimum
    viable standards and improving over time based on real-world feedback and outcomes.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**透明度和监控**：公开你的人工智能系统的局限性和权衡。这有助于管理期望并建立信任。定期评估人工智能系统在实际应用中的性能和影响，并准备好进行调整。逐步实施这些原则，从最低可行标准开始，并根据现实世界的反馈和结果逐步改进。'
- en: '**Regulatory collaboration**: Work with policymakers to develop regulations
    that encourage responsible innovation rather than stifling it. Invest in educating
    both developers and users about these principles and their implications.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监管合作**：与政策制定者合作，制定鼓励负责任创新而不是抑制创新的法规。投资于教育开发者和用户了解这些原则及其影响。'
- en: By taking a nuanced, context-specific approach and continuously reassessing
    the balance between these principles, we can work towards maximizing the benefits
    of AI while minimizing potential harm.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采取细致入微、具体情境的方法，并持续重新评估这些原则之间的平衡，我们可以努力最大化人工智能的益处，同时最小化潜在的危害。
- en: Fairness in GenAI applications
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通用人工智能应用中的公平性
- en: Fairness in AI systems is a critical concern as these technologies become increasingly
    integrated into decision-making processes across various sectors of society. Ensuring
    that AI systems do not perpetuate or amplify existing biases is essential for
    building trust, promoting equality, and maximizing the benefits of AI for all.
    However, achieving fairness in AI is a complex and multifaceted challenge that
    requires ongoing effort and vigilance.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这些技术越来越多地融入社会各个领域的决策过程，人工智能系统中的公平性是一个关键问题。确保人工智能系统不会持续或放大现有偏差对于建立信任、促进平等和最大化人工智能对所有人的利益至关重要。然而，实现人工智能的公平性是一个复杂且多方面的挑战，需要持续的努力和警惕。
- en: 'It involves considerations at every stage of the AI lifecycle, from data collection
    and model development to deployment and monitoring. The following points outline
    key strategies and considerations for promoting fairness in AI systems, with a
    focus on practical approaches and real-world examples. By implementing these practices,
    organizations can work towards creating AI systems that are more equitable, transparent,
    and beneficial to society as a whole:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及到人工智能生命周期的每个阶段，从数据收集和模型开发到部署和监控。以下要点概述了促进人工智能系统公平性的关键策略和考虑因素，重点关注实际方法和现实世界案例。通过实施这些做法，组织可以朝着创建更公平、更透明且对整个社会有益的人工智能系统迈进。
- en: '**Diverse and representative data**: Ensuring diverse and representative data
    is fundamental to creating fair AI systems. This involves not just including data
    from various demographic groups but also considering intersectionality and less
    visible forms of diversity. For example, in developing a speech recognition system,
    ensure the training data includes speakers with various accents and dialects from
    a range of age groups and genders. For instance, Apple faced criticism when early
    versions of Siri struggled with Scottish accents, highlighting the importance
    of linguistic diversity in training data.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样性和代表性数据**：确保多样性和代表性数据对于创建公平的人工智能系统至关重要。这不仅包括包括来自各个人口群体的数据，还要考虑交叉性和不太明显的多样性形式。例如，在开发语音识别系统时，确保训练数据包括来自不同年龄组和性别的各种口音和方言的说话者。例如，当苹果公司的早期版本Siri在处理苏格兰口音时遇到困难时，受到了批评，这突显了在训练数据中语言多样性的重要性。'
- en: '**Bias detection and mitigation**: This involves both proactive and reactive
    measures. *Proactively*, use statistical techniques to identify potential biases
    in your data and model outputs. *Reactively*, implement feedback mechanisms to
    catch and correct biases that emerge in real-world use. For example, consider
    an AI recruitment tool showing bias against specific demographics or genders in
    the tech industry. This could be the result of the system being trained on historical
    hiring data that reflected past gender or demographic biases in the tech industry.
    This case highlights the importance of scrutinizing training data and model outputs
    for unfair patterns.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差检测和缓解**：这包括主动和被动措施。**主动**地，使用统计技术来识别数据和中模型输出中的潜在偏差。**被动**地，实施反馈机制来捕捉和纠正现实世界使用中出现的偏差。例如，考虑一个在技术行业中针对特定人口或性别表现出偏差的人工智能招聘工具。这可能是因为系统在历史招聘数据上进行了训练，这些数据反映了技术行业过去在性别或人口结构上的偏差。这个案例突显了仔细审查训练数据和模型输出以发现不公平模式的重要性。'
- en: '**Regular audits**: Fairness audits should be comprehensive, examining not
    just the model’s outputs but also its impact in the real world. This may involve
    both quantitative metrics and qualitative assessments. For example, credit-scoring
    AI might pass initial fairness tests, but regular audits could reveal that it’s
    inadvertently disadvantaged certain groups over time due to changing economic
    conditions. Regular audits would catch this drift and allow for timely corrections.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定期审计**：公平性审计应该是全面的，不仅要检查模型的输出，还要检查其在现实世界中的影响。这可能涉及定量指标和定性评估。例如，信用评分人工智能可能通过初步的公平性测试，但定期的审计可能会揭示，由于经济条件的变化，它无意中在一段时间内对某些群体造成了不利影响。定期的审计将捕捉到这种漂移，并允许及时纠正。'
- en: '**Inclusive design**: Inclusive design goes beyond just consulting diverse
    stakeholders. It involves empowering them to actively shape the development process
    and giving their input real weight in decision-making. For example, when Microsoft
    developed the Xbox Adaptive Controller for gamers with limited mobility, they
    involved gamers with disabilities throughout the design process. This inclusive
    approach led to innovations that might have been overlooked by designers without
    lived experience of disability.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**包容性设计**：包容性设计不仅限于咨询多样化的利益相关者。它涉及赋予他们积极塑造发展过程的能力，并在决策中给予他们的意见实际权重。例如，当微软为有限活动能力的游戏玩家开发Xbox自适应控制器时，他们让有残疾的游戏玩家参与了整个设计过程。这种包容性方法导致了可能被没有残疾生活经验的开发者忽视的创新。'
- en: '**Contextual fairness**: Fairness can mean different things in different contexts.
    What’s fair in one situation might not be in another. AI systems need to be flexible
    enough to adapt to these nuances. For example, an AI system for college admissions
    might need to balance multiple fairness criteria – equal opportunity, demographic
    parity, and individual merit. The appropriate balance might differ between a public
    university with a mandate for diverse representation and a specialized technical
    institute.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情境公平性**：在不同的情境中，公平可能意味着不同的事情。在一个情况下公平的事情可能在另一个情况下就不公平。人工智能系统需要足够灵活，以适应这些细微差别。例如，用于大学录取的人工智能系统可能需要平衡多个公平标准——平等机会、人口比例和个体优势。适当的平衡可能在有多样化代表性使命的公立大学和专门的技术学院之间有所不同。'
- en: '**Transparency and explainability**: For AI systems to be truly fair, users
    should understand how decisions are made. This involves both technical explainability
    and clear communication with non-technical stakeholders. In healthcare, an AI
    system making treatment recommendations should be able to explain its reasoning
    in terms that both doctors and patients can understand. This allows informed consent
    and gives opportunities for patients to provide additional context that the AI
    might have missed.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**透明度和可说明性**：为了使人工智能系统真正公平，用户应该了解决策是如何做出的。这既涉及技术可说明性，也涉及与非技术利益相关者的清晰沟通。在医疗保健领域，做出治疗建议的人工智能系统应该能够用医生和患者都能理解的方式解释其推理。这允许知情同意，并为患者提供机会，让他们提供人工智能可能错过的额外背景信息。'
- en: '**Ongoing monitoring and adaptation**: Fairness isn’t a one-time achievement
    but an ongoing process. Societal norms and understanding of fairness evolve, and
    AI systems need to adapt accordingly. For example, an AI content moderation system
    for a social media platform might need to continuously update its understanding
    of hate speech and discriminatory language as social norms evolve and new forms
    of coded language emerge.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续监控和调整**：公平不是一次性的成就，而是一个持续的过程。社会规范和对公平的理解不断发展，人工智能系统需要相应地进行调整。例如，用于社交媒体平台的内容审核的人工智能系统可能需要持续更新其对仇恨言论和歧视性语言的理解，因为社会规范在演变，新的编码语言形式出现。'
- en: '**Legal and ethical compliance**: Ensure that fairness measures comply with
    relevant laws (like anti-discrimination legislation) and align with ethical standards.
    This may vary by jurisdiction and application area. In the EU, the GDPR and the
    proposed AI Act set specific requirements for fairness and non-discrimination
    in AI systems. Companies operating in or selling to the EU market need to ensure
    their AI systems comply with these regulations.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**法律和伦理合规性**：确保公平措施符合相关法律（如反歧视立法）并符合伦理标准。这可能因司法管辖区和应用领域而异。在欧盟，GDPR和拟议中的AI法案对人工智能系统中的公平性和非歧视性设定了具体要求。在欧盟市场运营或向欧盟市场销售的公司需要确保其人工智能系统符合这些规定。'
- en: These expanded points highlight the complexity and nuance involved in ensuring
    fairness in AI systems. It’s an ongoing challenge that requires vigilance, adaptability,
    and a commitment to ethical principles.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这些扩展点突出了确保人工智能系统公平性所涉及的复杂性和细微差别。这是一个持续性的挑战，需要警觉性、适应性和对伦理原则的承诺。
- en: Interpretability and explainability
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释性和可说明性
- en: Interpretability and explainability in AI systems, particularly in **large language
    models** (**LLMs**) and GenAI, are crucial for fostering trust, enabling effective
    oversight, and ensuring responsible deployment. As these systems become more complex
    and their decision-making processes more opaque, the need for methods to understand
    and explain their outputs grows increasingly important. Interpretability allows
    stakeholders to peek inside the “black box” of AI, while explainability focuses
    on communicating how decisions are made in a way that humans can understand.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能系统中，尤其是在**大型语言模型**（LLMs）和GenAI中，可解释性和可解释性对于建立信任、实现有效监督和确保负责任部署至关重要。随着这些系统变得更加复杂，其决策过程更加不透明，理解和解释其输出的方法的需求日益增长。可解释性允许利益相关者窥视人工智能的“黑箱”，而可解释性则侧重于以人类可以理解的方式传达决策是如何做出的。
- en: The following points outline key strategies for enhancing interpretability and
    explainability in AI systems, with a focus on practical approaches and real-world
    examples. By implementing these practices, organizations can create more transparent
    AI systems, facilitating better decision-making, regulatory compliance, and user
    trust.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下要点概述了增强人工智能系统中可解释性和可解释性的关键策略，重点关注实用方法和现实世界案例。通过实施这些实践，组织可以创建更透明的AI系统，促进更好的决策、合规性和用户信任。
- en: '**Model cards**: Model cards provide a standardized way to document AI models,
    including their performance characteristics, intended uses, and limitations. They
    serve as a crucial tool for transparency and responsible AI deployment. For example,
    Google’s BERT language model comes with a detailed model card that outlines its
    training data, evaluation results across different tasks and demographics, and
    ethical considerations. This allows users to make informed decisions about whether
    BERT is appropriate for their specific use case and helps them understand potential
    biases or limitations.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型卡**：模型卡提供了一种标准化的方式来记录人工智能模型，包括其性能特征、预期用途和限制。它们是透明度和负责任人工智能部署的关键工具。例如，谷歌的BERT语言模型附带了一份详细的模型卡，概述了其训练数据、不同任务和人口统计学的评估结果，以及伦理考量。这使用户能够做出是否BERT适合其特定用例的知情决策，并帮助他们了解潜在的偏见或限制。'
- en: '**Explainable AI (XAI) techniques**: XAI techniques aim to make the decision-making
    process of AI models more understandable to humans. These methods can provide
    insights into which features are most important for a particular decision or prediction.
    For example, in a medical diagnosis AI system, **SHapley Additive exPlanations**
    (**SHAP**) values could be used to show which symptoms or test results contributed
    most to a particular diagnosis. This allows doctors to understand the AI’s reasoning
    and compare it with their own clinical judgment.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释人工智能（XAI）技术**：XAI技术旨在使人工智能模型的决策过程更易于人类理解。这些方法可以提供关于哪些特征对特定决策或预测最重要的见解。例如，在医疗诊断人工智能系统中，**SHapley
    Additive exPlanations**（SHAP）值可以用来显示哪些症状或测试结果对特定诊断的贡献最大。这允许医生理解人工智能的推理，并将其与自己的临床判断进行比较。'
- en: '**User-friendly explanations**: Technical explanations of AI decisions are
    often not useful for end-users. Developing clear, non-technical explanations tailored
    to the user’s level of expertise is crucial for practical explainability. Credit-scoring
    AI might explain its decision to deny a loan not just with a numerical score,
    but with a simple explanation like “Your application was declined primarily due
    to your high debt-to-income ratio and recent missed payments on your credit card.”
    This gives the user actionable information without requiring them to understand
    the underlying AI model.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户友好的解释**：人工智能决策的技术性解释通常对最终用户来说并不有用。开发清晰、非技术性的解释，针对用户的技能水平进行定制，对于实现实用性解释至关重要。信用评分人工智能可能不仅仅用数值分数来解释拒绝贷款的决定，而是用一个简单的解释，例如“你的申请主要由于高债务收入比和信用卡近期逾期还款而被拒绝。”这为用户提供可操作的信息，而无需他们理解底层人工智能模型。'
- en: '**Traceability**: Maintaining detailed logs of an AI system’s inputs, outputs,
    and key decision points allows for auditing and helps in understanding the system’s
    behavior over time. This is particularly important for regulatory compliance and
    debugging. In an autonomous vehicle system, maintaining a detailed log of sensor
    inputs, decision points, and actions taken would be crucial. If an accident occurs,
    this log could be analyzed to understand why the AI made certain decisions and
    how similar incidents could be prevented in the future.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可追溯性**：维护人工智能系统输入、输出和关键决策点的详细日志，可以用于审计并有助于理解系统随时间的行为。这对于法规遵从和调试尤其重要。在一个自动驾驶车辆系统中，维护详细的传感器输入、决策点和采取的行动的日志至关重要。如果发生事故，此日志可以用于分析AI为何做出某些决策，以及如何在未来防止类似事件的发生。'
- en: '**Interpretable model architectures**: While not using inherently more interpretable
    model architectures can be a powerful approach, this might involve using simpler
    models where possible or developing new architectures designed for interpretability.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释的模型架构**：虽然不使用本质上更可解释的模型架构可能是一种强大的方法，但这可能涉及尽可能使用更简单的模型，或者开发旨在提高可解释性的新架构。'
- en: In a financial fraud detection system, a decision tree model might be used instead
    of a more complex neural network for certain components. The decision tree’s logic
    can be easily visualized and understood, allowing for clear explanations of why
    a transaction was flagged as potentially fraudulent.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融欺诈检测系统中，对于某些组件，可能会使用决策树模型而不是更复杂的神经网络。决策树的逻辑可以轻松可视化并理解，从而可以清楚地解释为什么一笔交易被标记为可能存在欺诈。
- en: '**Interactive explanations**: Providing users with interactive tools to explore
    model behavior can greatly enhance understanding. This allows users to ask “what
    if” questions and see how changes in inputs can affect outputs. For example, a
    recommendation system for an e-commerce platform could include an interactive
    feature allowing users to adjust the importance of different factors (for example,
    price, brand, ratings) and see in real time how this affects product recommendations.
    This helps users understand the system’s logic and tailor it to their preferences.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交互式解释**：向用户提供交互式工具以探索模型行为可以极大地增强理解。这使用户能够提出“如果...会怎样”的问题，并看到输入的变化如何影响输出。例如，一个电子商务平台的推荐系统可以包括一个交互式功能，允许用户调整不同因素（例如，价格、品牌、评分）的重要性，并实时看到这如何影响产品推荐。这有助于用户理解系统的逻辑，并根据他们的偏好进行调整。'
- en: By implementing these strategies, organizations can significantly enhance the
    interpretability and explainability of their AI systems, leading to more transparent,
    trustworthy, and effective AI applications.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实施这些策略，组织可以显著提高其人工智能系统的可解释性和可解释性，从而实现更透明、更值得信赖和更有效的人工智能应用。
- en: Privacy and data protection
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐私和数据保护
- en: 'Privacy and data protection in AI systems, especially in the context of powerful
    GenAI models, is a critical concern that impacts user trust, legal compliance,
    and ethical use of this technology. As AI systems process increasingly large amounts
    of personal and potentially sensitive data, ensuring robust privacy safeguards
    becomes a make-or-break point for organizations. Effective privacy protection
    involves not only technical measures but also organizational policies and user
    empowerment. The following points outline key strategies for enhancing privacy
    and data protection in AI systems, with a focus on practical approaches and real-world
    examples. By implementing these practices, organizations can create AI systems
    that respect user privacy, comply with regulations, and maintain the trust of
    their users and stakeholders:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能系统中，尤其是在强大的通用人工智能模型背景下，隐私和数据保护是一个关键问题，它影响着用户的信任、法律合规性和技术的道德使用。随着人工智能系统处理越来越多的个人和可能敏感的数据，确保强大的隐私保障成为组织成败的关键点。有效的隐私保护不仅涉及技术措施，还包括组织政策和用户赋权。以下要点概述了增强人工智能系统中隐私和数据保护的关键策略，重点关注实际方法和现实世界案例。通过实施这些实践，组织可以创建尊重用户隐私、遵守法规并维护用户和利益相关者信任的人工智能系统：
- en: '**Data minimization**: This involves collecting and using only the data that
    is absolutely necessary for the AI system’s intended function. This principle
    reduces privacy risks and aligns with many data protection regulations. For instance,
    a smart home AI assistant could be designed to process voice commands locally
    on the device whenever possible, rather than sending all audio data to cloud servers.
    This minimizes the amount of potentially sensitive data that leaves the user’s
    control and increases the trust of users in their data not being at risk of a
    leak or used to further improve models.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据最小化**：这涉及仅收集和使用AI系统预期功能绝对必要的数据。这一原则降低了隐私风险，并与许多数据保护法规相一致。例如，一个智能家居AI助手可以设计为在设备上尽可能本地处理语音命令，而不是将所有音频数据发送到云端服务器。这减少了可能敏感数据离开用户控制的情况，并增加了用户对其数据不会泄露或用于进一步改进模型的信任。'
- en: '**Anonymization and encryption**: These techniques help protect individual
    privacy by removing or obscuring personally identifiable information in datasets
    used for training and inference. Properly implemented, they can allow useful data
    analysis while preserving privacy. In developing an AI system for analyzing hospital
    patient outcomes, researchers could use differential privacy techniques. For example,
    differential privacy adds carefully calibrated noise to the data or to the model’s
    outputs, allowing accurate overall analysis while making it mathematically impossible
    to identify individual patients.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**匿名化和加密**：这些技术通过在用于训练和推理的数据集中移除或模糊个人可识别信息来帮助保护个人隐私。如果正确实施，它们可以在保护隐私的同时允许有用的数据分析。在开发用于分析医院患者结果的AI系统时，研究人员可以使用差分隐私技术。例如，差分隐私会在数据或模型输出中添加精心校准的噪声，允许进行准确的整体分析，同时从数学上使其不可能识别个别患者。'
- en: '**User control**: Empowering users with control over their data is not only
    a legal requirement in many jurisdictions but also builds trust. This includes
    providing clear, easily accessible options for data management. For example, a
    social media platform using AI for content recommendations could provide a detailed
    *Privacy Dashboard* where users can see what data is being collected, how it’s
    being used, and easily opt out of specific data collection or processing activities.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户控制**：赋予用户对其数据的控制权不仅是许多司法管辖区的一项法律要求，而且有助于建立信任。这包括提供清晰、易于访问的数据管理选项。例如，一个使用AI进行内容推荐的社交媒体平台可以提供一个详细的*隐私仪表板*，用户可以在其中查看正在收集哪些数据，如何使用这些数据，以及轻松地退出特定的数据收集或处理活动。'
- en: '**Compliance**: Adherence to relevant data protection regulations is crucial.
    This often involves implementing privacy by design principles, conducting impact
    assessments, and maintaining detailed documentation of data processing activities.
    A multinational company developing an AI-powered HR tool would need to ensure
    compliance with GDPR for EU employees, CCPA for California residents, and other
    applicable local regulations. This might involve creating region-specific data
    handling processes and providing different privacy notices and controls based
    on user location.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合规性**：遵守相关的数据保护法规至关重要。这通常涉及实施隐私设计原则、进行影响评估以及维护数据处理活动的详细记录。一家开发AI人力资源工具的跨国公司需要确保遵守GDPR以符合欧盟员工的要求，CCPA以符合加州居民的要求，以及其他适用的当地法规。这可能包括创建特定区域的数据处理流程，并根据用户位置提供不同的隐私通知和控制措施。'
- en: '**Avoid logging PII and other sensitive information**: Logs are often overlooked
    as a potential source of privacy breaches. Implementing practices to avoid capturing
    personally identifiable information in logs is crucial for maintaining privacy.
    Even when logs don’t contain PII, they can still contain sensitive information
    about system operations. Implementing strong security measures for log data is
    essential. A financial institution using AI for fraud detection could implement
    a secure, encrypted log storage system with strict access controls. Only authorized
    personnel would be able to access logs, and all access would be logged for auditing
    purposes.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**避免记录PII和其他敏感信息**：日志经常被忽视作为潜在的隐私泄露来源。在日志中避免捕获个人可识别信息是维护隐私的关键。即使日志不包含PII，它们也可能包含有关系统操作的敏感信息。对日志数据进行强大的安全措施是至关重要的。一家使用AI进行欺诈检测的金融机构可以实施一个安全、加密的日志存储系统，并具有严格的访问控制。只有授权人员才能访问日志，所有访问都会被记录以供审计目的。'
- en: '**Retention policies**: Establishing and enforcing data retention policies
    helps minimize privacy risks over time and often aligns with legal requirements
    for data minimization. An AI-powered fitness app could implement a policy to automatically
    delete user activity data after a certain period (for example, 6 months) unless
    the user explicitly opts in to keep it longer. This reduces the risk of old data
    being compromised while still allowing users to maintain long-term records if
    desired.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保留政策**：建立和执行数据保留政策有助于随着时间的推移降低隐私风险，并且通常符合数据最小化方面的法律要求。一个由人工智能（AI）驱动的健身应用可以实施一项政策，在用户明确选择保留更长时间之前，自动删除用户活动数据（例如，6个月）。这降低了旧数据被泄露的风险，同时仍然允许用户在需要时保持长期记录。'
- en: '**Privacy-preserving AI techniques**: Emerging techniques like federated learning
    and homomorphic encryption allow AI models to learn from data without directly
    accessing it, providing powerful new tools for privacy protection. A keyboard
    prediction AI on smartphones could use federated learning to improve its model.
    The model would be updated on individual devices using local data, and only the
    model updates (not the raw data) would be sent back to the central server, preserving
    user privacy.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私保护的人工智能技术**：如联邦学习和同态加密等新兴技术允许人工智能（AI）模型在不直接访问数据的情况下从数据中学习，为隐私保护提供了强大的新工具。智能手机上的键盘预测人工智能（AI）可以使用联邦学习来改进其模型。模型将使用本地数据在单个设备上更新，并且只有模型更新（而不是原始数据）会被发送回中央服务器，从而保护用户隐私。'
- en: By implementing these strategies, organizations can significantly enhance the
    privacy and data protection aspects of their AI systems, leading to more trustworthy
    and legally compliant AI applications that respect user privacy.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实施这些策略，组织可以显著提高其人工智能（AI）系统的隐私和数据保护方面，从而产生更值得信赖且符合法律规定的、尊重用户隐私的人工智能（AI）应用。
- en: Safety and security in GenAI systems
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通用人工智能（GenAI）系统中的安全性和安全性
- en: 'Ensuring that GenAI systems operate safely, securely, and as intended is crucial
    for protecting users and preventing potential misuse of information generated
    and from training, as well as unintended consequences. This involves a multi-faceted
    approach that encompasses both proactive measures and reactive capabilities. The
    following points outline key strategies for enhancing safety and security in GenAI
    systems, with a focus on practical approaches and real-world examples:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 确保通用人工智能（GenAI）系统安全、安全地按预期运行对于保护用户和防止信息生成和训练中的潜在滥用以及意外后果至关重要。这需要一种多方面的方法，包括主动措施和反应能力。以下要点概述了增强通用人工智能（GenAI）系统安全性和安全性的关键策略，重点关注实际方法和现实世界案例：
- en: '**Content filtering**: Implementing filters to prevent the generation of harmful
    or inappropriate content is a crucial safety measure. For instance, a GenAI-powered
    chatbot for a children’s educational platform could use advanced content filtering
    algorithms to detect and block any attempts to generate age-inappropriate content,
    ensuring a safe learning environment. This might involve maintaining and regularly
    updating a comprehensive list of prohibited terms and topics, as well as using
    more sophisticated natural language processing techniques to identify potentially
    harmful content even when it’s expressed in novel ways.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内容过滤**：实施过滤器以防止生成有害或不适当的内容是一项至关重要的安全措施。例如，为儿童教育平台提供的由通用人工智能（GenAI）驱动的聊天机器人可以使用高级内容过滤算法来检测和阻止生成任何不适宜年龄的内容尝试，确保一个安全的学习环境。这可能涉及维护和定期更新一个全面的禁止词汇和主题列表，以及使用更复杂的自然语言处理技术来识别即使以新颖方式表达的可能有害内容。'
- en: '**Rate limiting**: Applying reasonable limits on API calls is essential to
    prevent abuse and ensure fair usage. A GenAI service providing image generation
    capabilities could implement tiered rate limiting, where free users are limited
    to a certain number of generations per day, while paid users have higher limits.
    This not only prevents potential denial-of-service attacks but also helps manage
    computational resources effectively. The system could also implement more nuanced
    rate limiting based on the complexity of the requests, allowing more frequent
    simple generations while limiting resource-intensive ones.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速率限制**：对API调用应用合理的限制对于防止滥用和确保公平使用至关重要。一个提供图像生成能力的通用人工智能（GenAI）服务可以实施分层速率限制，其中免费用户每天的限制数量有限，而付费用户有更高的限制。这不仅防止了潜在的拒绝服务攻击，还有助于有效管理计算资源。系统还可以根据请求的复杂性实施更细致的速率限制，允许更频繁的简单生成，同时限制资源密集型生成。'
- en: '**Adversarial testing**: Regularly testing the system with adversarial inputs
    is crucial for identifying and addressing vulnerabilities. These tests need to
    be added to the development cycle as well as the continuous monitoring of the
    system.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对抗性测试**：定期使用对抗性输入测试系统对于识别和解决漏洞至关重要。这些测试需要添加到开发周期以及系统的持续监控中。'
- en: For a GenAI system designed to assist in medical diagnosis, researchers could
    create a suite of adversarial test cases designed to trick the system into making
    incorrect diagnoses. This might include subtle alterations to medical images or
    carefully crafted textual descriptions of symptoms. By continuously running and
    expanding these tests, developers can identify weak points in the system’s decision-making
    process and implement targeted improvements to enhance robustness.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于旨在协助医疗诊断的通用人工智能（GenAI）系统，研究人员可以创建一系列对抗性测试案例，旨在欺骗系统做出错误的诊断。这可能包括对医学图像进行细微的修改或精心制作的症状文本描述。通过持续运行和扩展这些测试，开发者可以识别系统决策过程中的弱点，并实施有针对性的改进以增强鲁棒性。
- en: '**Monitoring and alerting**: Setting up systems to detect and respond to anomalous
    behavior is critical for maintaining ongoing safety and security. A financial
    institution using GenAI for fraud detection could implement a real-time monitoring
    system that tracks unusual patterns in the AI’s behavior. For example, if the
    system suddenly starts flagging an unusually high number of transactions as fraudulent,
    or if its confidence scores show unexpected fluctuations, automated alerts could
    be triggered for human review. This allows quick intervention in case of potential
    system malfunctions or targeted attacks.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控和警报**：建立系统以检测和响应异常行为对于维持持续的安全和保障至关重要。使用通用人工智能（GenAI）进行欺诈检测的金融机构可以实施实时监控系统，以跟踪AI行为中的异常模式。例如，如果系统突然开始将异常高数量的交易标记为欺诈，或者其置信度分数显示出意外的波动，则可以自动触发警报供人工审查。这允许在系统出现潜在故障或遭受针对性攻击时进行快速干预。'
- en: '**Improved robustness**: Enhanced stability and consistency in outputs, as
    demonstrated by models like Gemini 1.5, is crucial for safety. A GenAI system
    used for automated customer service could leverage these improvements to provide
    more reliable and consistent responses across a wide range of customer inquiries.
    This reduces the risk of the system providing contradictory or nonsensical information,
    which could lead to customer confusion or dissatisfaction. Regular evaluations
    comparing the system’s outputs over time and across different inputs can help
    verify and maintain this robustness.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强的鲁棒性**：如Gemini 1.5等模型所展示的输出稳定性和一致性对于安全性至关重要。用于自动化客户服务的通用人工智能（GenAI）系统可以利用这些改进，在广泛的客户咨询中提供更可靠和一致的反应。这降低了系统提供矛盾或无意义信息的风险，这可能导致客户困惑或不满。定期评估系统随时间和不同输入的输出，可以帮助验证和维护这种鲁棒性。'
- en: '**Content safety**: Built-in mechanisms to avoid generating harmful or explicit
    content are essential safeguards. A GenAI-powered creative writing assistant could
    incorporate multiple layers of content safety checks. This might include analyzing
    generated text for potentially offensive language, checking for age-appropriate
    content based on the user’s profile, and providing warnings or alternatives when
    the system detects that it might be veering into sensitive territory. These mechanisms
    should be regularly updated to reflect evolving societal norms and newly identified
    risks.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内容安全**：内置机制以避免生成有害或露骨的内容是基本的安全保障。由通用人工智能（GenAI）驱动的创意写作助手可以整合多层内容安全检查。这可能包括分析生成的文本中可能冒犯性的语言，根据用户的个人资料检查适合年龄的内容，并在系统检测到可能进入敏感领域时提供警告或替代方案。这些机制应定期更新，以反映不断变化的社会规范和新的识别风险。'
- en: '**Multimodal safety**: Extending safety considerations across text, image,
    and other modalities is increasingly important as GenAI systems become more versatile.
    A multimodal GenAI system used in social media content moderation could apply
    safety checks across text, images, and videos simultaneously. For instance, it
    could analyze both the text of a post and any accompanying images to detect potential
    policy violations, ensuring that harmful content isn’t slipping through by exploiting
    gaps between different modalities. This requires developing and maintaining comprehensive
    safety models that can understand context across different types of data.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多模态安全**: 随着GenAI系统变得更加多功能，将安全考虑扩展到文本、图像和其他模态变得越来越重要。一个用于社交媒体内容审查的多模态GenAI系统可以在文本、图像和视频上同时进行安全检查。例如，它可以分析帖子的文本和任何伴随的图像，以检测潜在的政策违规行为，确保有害内容不会通过利用不同模态之间的差距而溜走。这需要开发和维护能够理解不同类型数据上下文的全面安全模型。'
- en: By implementing these strategies, organizations can significantly enhance the
    safety and security of their GenAI systems, creating more reliable, trustworthy,
    and beneficial AI applications. Regular reviews and updates of these measures
    are essential to keep pace with the rapidly evolving capabilities and potential
    risks associated with GenAI technologies.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实施这些策略，组织可以显著提高其GenAI系统的安全性和安全性，创造更可靠、值得信赖和有益的AI应用。定期审查和更新这些措施对于跟上GenAI技术的快速发展和潜在风险至关重要。
- en: Google’s approach to responsible AI
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谷歌对负责任的AI的方法
- en: 'Google’s approach to responsible AI is a comprehensive framework that serves
    as a model for organizations implementing GenAI systems. At its core, the approach
    prioritizes:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌对负责任的AI的方法是一个全面的框架，为实施GenAI系统的组织提供了一个模型。其核心，该方法优先考虑：
- en: '*Human-Centered Design*, emphasizing the importance of user needs and societal
    impact in AI development. This ensures that AI systems are created with a deep
    understanding of their real-world implications and potential benefits for users
    across diverse backgrounds.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*以人为本的设计* 强调了在AI开发中用户需求和 societal impact 的重要性。这确保了AI系统是在对其现实世界影响和为不同背景的用户带来的潜在益处的深刻理解下创建的。'
- en: '*Fairness* is another crucial aspect of Google’s framework. The company has
    developed sophisticated tools and practices to detect and mitigate unfair bias
    in AI systems. This commitment to fairness involves rigorous testing, diverse
    dataset creation, and ongoing monitoring to ensure AI applications remain equitable
    as they evolve and are deployed in various contexts.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*公平性* 是谷歌框架的另一个关键方面。公司开发了复杂的工具和实践来检测和减轻AI系统中的不公平偏见。对公平性的承诺包括严格的测试、创建多样化的数据集以及持续的监控，以确保AI应用在演变和部署到各种环境中时保持公平。'
- en: '*Interpretability* is a key focus, with Google striving to create explainable
    AI systems that promote transparency. This involves developing methods to make
    AI decision-making processes more understandable to both developers and end-users,
    fostering trust and accountability in AI applications.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可解释性* 是一个关键焦点，谷歌致力于创建可解释的AI系统，以促进透明度。这涉及到开发方法，使AI决策过程对开发者和最终用户都更加可理解，从而在AI应用中培养信任和问责制。'
- en: '*Privacy* is a fundamental concern addressed through advanced techniques such
    as federated learning and differential privacy. These approaches allow for the
    development of powerful AI models while protecting individual user data, striking
    a balance between innovation and privacy protection.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*隐私* 是一个基本关注点，通过诸如联邦学习和差分隐私等高级技术得到解决。这些方法允许开发强大的AI模型，同时保护个人用户数据，在创新和隐私保护之间取得平衡。'
- en: '*Safety and Security* are paramount in Google’s responsible AI approach. The
    company conducts rigorous testing and focuses on developing robust AI systems
    that can withstand potential misuse or manipulation. This proactive stance on
    security helps ensure that AI technologies are not only powerful but also trustworthy
    and resilient.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*安全和安全* 在谷歌负责任的AI方法中至关重要。公司进行严格的测试，并专注于开发能够抵御潜在滥用或操纵的强大AI系统。这种主动的安全立场有助于确保AI技术不仅强大，而且值得信赖和有弹性。'
- en: Google’s approach emphasizes the importance of cross-functional collaboration,
    bringing together experts from various fields to address the multifaceted challenges
    of AI development. This is complemented by a strong commitment to ongoing research,
    pushing the boundaries of AI capabilities while simultaneously exploring its ethical
    implications.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌的方法强调跨职能协作的重要性，将来自不同领域的专家聚集在一起，以应对人工智能发展的多方面挑战。这得到了对持续研究的坚定承诺的补充，推动人工智能能力的边界，同时探索其伦理影响。
- en: Google’s Secure AI Framework (SAIF)
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 谷歌的安全人工智能框架（SAIF）
- en: 'This is a conceptual framework designed to manage risks associated with rapidly
    advancing AI technologies. It consists of six core elements:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个概念框架，旨在管理与快速发展的人工智能技术相关的风险。它包括六个核心要素：
- en: '**Expand strong security foundations**: Adapt existing cybersecurity measures
    to protect AI systems and develop expertise to keep pace with AI advancements.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩展强大的安全基础**：调整现有的网络安全措施以保护人工智能系统，并培养与人工智能进步同步的专业知识。'
- en: '**Extend detection and response**: Incorporate AI-related threats into existing
    cybersecurity practices, including monitoring AI system inputs and outputs for
    anomalies.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩展检测和响应**：将人工智能相关威胁纳入现有的网络安全实践，包括监控人工智能系统输入和输出中的异常。'
- en: '**Automate defenses**: Utilize AI innovations to improve the scale and speed
    of responses to security incidents, countering potential AI-enhanced threats.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化防御**：利用人工智能创新来提高对安全事件的响应规模和速度，对抗潜在的增强人工智能威胁。'
- en: '**Harmonize platform-level controls**: Ensure consistent security across different
    AI platforms and tools within an organization, leveraging secure-by-default protections.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协调平台级控制**：确保组织内不同人工智能平台和工具之间的安全一致性，利用默认保护措施。'
- en: '**Adapt controls**: Continuously test and evolve detection and protection mechanisms
    through techniques like reinforcement learning and regular Red Team exercises.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调整控制**：通过强化学习、定期红队演习等技术持续测试和改进检测和保护机制。'
- en: '**Contextualize AI system risks**: Conduct comprehensive risk assessments for
    AI deployments, considering end-to-end business risks and implementing automated
    checks for AI performance validation.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情境化人工智能系统风险**：对人工智能部署进行全面风险评估，考虑端到端业务风险，并实施自动化检查以验证人工智能性能。'
- en: This framework aims to help organizations evolve their risk management strategies
    alongside AI advancements, ensuring more secure and responsible AI implementation.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架旨在帮助组织随着人工智能的进步而发展其风险管理策略，确保更安全、更负责任的人工智能实施。
- en: 'Building upon the core elements of Google’s **Secure AI Framework** (**SAIF**),
    the approach to implementation ([https://services.google.com/fh/files/blogs/google_secure_ai_framework_approach.pdf](https://services.google.com/fh/files/blogs/google_secure_ai_framework_approach.pdf))
    provides a practical guide for organizations looking to integrate AI securely
    into their operations. The approach breaks down the process into four key steps
    and elaborates on how to apply the six core elements of SAIF effectively:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在谷歌**安全人工智能框架**（**SAIF**）的核心要素基础上，实施方法（[https://services.google.com/fh/files/blogs/google_secure_ai_framework_approach.pdf](https://services.google.com/fh/files/blogs/google_secure_ai_framework_approach.pdf)）为希望将人工智能安全地整合到其运营中的组织提供了一个实用的指南。该方法将过程分解为四个关键步骤，并详细阐述了如何有效地应用SAIF的六个核心要素：
- en: '**Understand the use**: Clearly define the specific AI use case and its context
    within the organization.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**理解用途**：明确定义特定的AI用例及其在组织中的上下文。'
- en: '**Assemble the team**: Create a cross-functional team including stakeholders
    from various departments such as security, legal, data science, and ethics.'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**组建团队**：创建一个跨职能团队，包括来自安全、法律、数据科学和伦理等各个部门的利益相关者。'
- en: '**Level set with an AI primer**: Ensure all team members have a basic understanding
    of AI concepts and terminology.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**通过人工智能入门课程进行水平设定**：确保所有团队成员对人工智能概念和术语有基本了解。'
- en: '**Apply the six core elements of SAIF**:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**应用SAIF的六个核心要素**：'
- en: 'Expand strong security foundations:'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扩展强大的安全基础：
- en: Review existing security controls and their applicability to AI systems.
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 审查现有的安全控制措施及其对人工智能系统的适用性。
- en: Evaluate traditional controls against AI-specific threats.
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估传统控制措施对人工智能特定威胁的适用性。
- en: Prepare for supply chain management and data governance.
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备供应链管理和数据治理。
- en: 'Extend detection and response:'
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扩展检测和响应：
- en: Develop an understanding of AI-specific threats.
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理解人工智能特定威胁。
- en: Prepare to respond to attacks against AI and issues raised by AI output.
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备应对针对人工智能的攻击和人工智能输出提出的问题。
- en: Adjust abuse policies and incident response processes for AI-specific incidents.
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整针对人工智能特定事件的滥用政策和事件响应流程。
- en: 'Automate defenses:'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自动化防御措施：
- en: Identify AI security capabilities for protecting systems and data pipelines.
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别用于保护系统和数据管道的人工智能安全能力。
- en: Use AI defenses to counter AI threats while keeping humans in the loop.
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在保持人工参与的同时，使用人工智能防御措施来对抗人工智能威胁。
- en: Leverage AI to automate time-consuming tasks and speed up defensive mechanisms.
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利用人工智能自动化耗时任务并加快防御机制。
- en: 'Harmonize platform-level controls:'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 协调平台级控制措施：
- en: Review AI usage and the lifecycle of AI-based applications.
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 审查人工智能的使用和基于人工智能的应用程序的生命周期。
- en: Standardize tooling and frameworks to prevent control fragmentation.
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标准化工具和框架以防止控制碎片化。
- en: 'Adapt controls:'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 适应控制措施：
- en: Conduct Red Team exercises for AI-powered products.
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对人工智能驱动的产品进行红队演习。
- en: Stay informed about novel AI attacks.
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 了解新型人工智能攻击。
- en: Apply machine learning to improve detection accuracy and speed.
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用机器学习来提高检测准确性和速度。
- en: Create feedback loops for continuous improvement.
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建反馈循环以实现持续改进。
- en: 'Contextualize AI system risks:'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 语境化人工智能系统风险：
- en: Establish a model risk management framework.
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立模型风险管理框架。
- en: Build an inventory of AI models and their risk profiles.
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立人工智能模型及其风险概况的清单。
- en: Implement policies and controls throughout the ML model lifecycle.
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在机器学习模型的生命周期中实施政策和控制措施。
- en: Perform comprehensive risk assessments for AI use.
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对人工智能的使用进行全面的风险评估。
- en: Consider shared responsibility in AI security.
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑人工智能安全中的共同责任。
- en: Match AI use cases to organizational risk tolerances.
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将人工智能用例与组织风险承受能力相匹配。
- en: This approach provides a structured way for organizations to implement SAIF,
    ensuring that AI integration is secure, responsible, and aligned with business
    objectives.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法为组织提供了一个结构化的方式来实施SAIF，确保人工智能的集成是安全的、负责任的，并与业务目标保持一致。
- en: Google’s Red Teaming approach
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 谷歌的“红队”方法
- en: This approach to AI systems is a comprehensive strategy to identify and mitigate
    potential security risks in AI deployments. The company has established a dedicated
    AI Red Team that combines traditional red teaming expertise with specialized AI
    subject matter knowledge. This team simulates threat actors targeting AI deployments,
    with the goal of assessing the impact of simulated attacks on users and products,
    analyzing the resilience of new AI detection and prevention capabilities, improving
    detection capabilities for early attack recognition, and raising awareness among
    stakeholders about key risks and necessary security controls. For further reading,
    please refer to [https://services.google.com/fh/files/blogs/google_ai_red_team_digital_final.pdf](https://services.google.com/fh/files/blogs/google_ai_red_team_digital_final.pdf).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这种人工智能系统的方法是一种全面的策略，旨在识别和减轻人工智能部署中潜在的安全风险。公司已成立一个专门的AI红队，该队结合了传统的红队专业知识和专门的人工智能专业知识。该团队模拟针对人工智能部署的威胁行为者，目的是评估模拟攻击对用户和产品的影响，分析新的人工智能检测和预防能力的弹性，提高早期攻击识别的检测能力，并提高利益相关者对关键风险和必要安全控制的意识。欲了解更多信息，请参阅[https://services.google.com/fh/files/blogs/google_ai_red_team_digital_final.pdf](https://services.google.com/fh/files/blogs/google_ai_red_team_digital_final.pdf)。
- en: The AI Red Team works closely with Google’s threat intelligence teams like Mandiant
    and the **Threat Analysis Group** (**TAG**) to ensure their simulations reflect
    realistic adversary activities. This collaboration allows the team to prioritize
    different exercises and shape engagements that closely resemble real-world threats.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能红队与谷歌的威胁情报团队（如Mandiant和**威胁分析小组**（**TAG**））紧密合作，确保他们的模拟反映现实世界的对手活动。这种合作使团队能够优先考虑不同的练习，并形成与真实世界威胁非常相似的互动。
- en: 'Google’s approach to red teaming for AI systems focuses on several common types
    of attacks:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌对人工智能系统进行红队攻击的方法侧重于几种常见的攻击类型：
- en: '*Prompt attacks* are a significant concern, especially for LLMs powering GenAI
    products. These attacks involve crafting inputs that manipulate the model’s behavior
    in unintended ways. For example, an attacker might inject hidden instructions
    into an email to bypass AI-based phishing detection systems.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提示攻击*是一个重大问题，特别是对于为生成人工智能产品提供动力的LLMs。这些攻击涉及构建输入，以未预期的方式操纵模型的行为。例如，攻击者可能会在电子邮件中注入隐藏指令，以绕过基于人工智能的钓鱼检测系统。'
- en: '*Training data extraction attacks* aim to reconstruct verbatim training examples,
    potentially exposing sensitive information like **personally identifiable information**
    (**PII**) or passwords. These attacks are particularly dangerous for personalized
    models or those trained on data containing PII.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练数据提取攻击*旨在重建字面训练示例，可能暴露敏感信息，如**个人身份信息**（**PII**）或密码。这些攻击对个性化模型或训练在包含PII的数据上的模型尤其危险。'
- en: '*Backdooring the model* is another critical threat where attackers attempt
    to covertly change the model’s behavior to produce incorrect outputs with specific
    “trigger” words or features. This can be achieved through direct manipulation
    of the model’s weights, fine-tuning for adversarial purposes, or modifying the
    file representation of the model.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型后门攻击*是另一种关键威胁，攻击者试图秘密改变模型的行为，以产生带有特定“触发”词或特征的错误输出。这可以通过直接操纵模型的权重、针对对抗目的的微调或修改模型的文件表示来实现。'
- en: '*Adversarial examples* involve inputs that result in unexpected outputs from
    the model. For instance, an image that appears as a dog to humans but is classified
    as a cat by the model. The impact of these attacks can vary widely depending on
    the AI system’s use case.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对抗性示例*涉及导致模型产生意外输出的输入。例如，人类看起来像狗的图像，但模型将其分类为猫。这些攻击的影响可能因人工智能系统的用例而异。'
- en: '*Data poisoning attacks* involve manipulating the training data to influence
    the model’s output according to the attacker’s preferences. This highlights the
    importance of securing the data supply chain in AI development.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据中毒攻击*涉及操纵训练数据，以根据攻击者的偏好影响模型的输出。这突出了在人工智能开发中确保数据供应链安全的重要性。'
- en: '*Exfiltration attacks* focus on stealing the model itself, which often includes
    sensitive intellectual property. These can range from simple file copying to more
    complex attacks involving repeated querying of the model to determine its capabilities
    and recreate it.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据泄露攻击*专注于窃取模型本身，这通常包括敏感的知识产权。这些攻击可能从简单的文件复制到更复杂的攻击，涉及重复查询模型以确定其功能并重新创建它。'
- en: Google emphasizes that while these AI-specific attacks are crucial to address,
    they should be considered in addition to, not instead of, traditional security
    threats. The company advocates for a comprehensive approach that combines AI-specific
    red teaming with traditional security practices like penetration testing, vulnerability
    management, and the secure development lifecycle.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Google 强调，虽然这些针对人工智能的特定攻击至关重要，但它们应被视为传统安全威胁的补充，而不是替代。公司主张采用综合方法，结合人工智能特定的红队测试和传统的安全实践，如渗透测试、漏洞管理和安全开发生命周期。
- en: By sharing their approach and lessons learned, Google aims to establish clear
    industry security standards for AI and help advance the field of AI security.
    Their experience underscores the importance of cross-functional collaboration,
    continuous learning, and the integration of AI security with broader cybersecurity
    efforts to create more robust and secure AI deployments.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分享他们的方法和经验教训，Google 旨在建立人工智能的明确行业安全标准，并帮助推进人工智能安全领域。他们的经验强调了跨职能协作、持续学习和将人工智能安全与更广泛的网络安全努力相结合以创建更强大和安全的AI部署的重要性。
- en: Anthropic’s approach to responsible AI
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Anthropic 对负责任的人工智能开发的方法
- en: 'Anthropic’s approach to responsible AI development is comprehensive and multifaceted,
    integrating ethical considerations, safety measures, and empirical research throughout
    their work. Key aspects of their approach include:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic 对负责任的人工智能开发的方法是全面且多方面的，将伦理考量、安全措施和实证研究贯穿于他们的工作之中。他们方法的关键方面包括：
- en: '**Prioritizing Safety Research**: Anthropic views AI safety research as urgently
    important and worthy of broad support. They employ a portfolio approach, preparing
    for a range of scenarios, from optimistic to pessimistic, regarding the difficulty
    of creating safe AI systems.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优先考虑安全研究**：Anthropic 将人工智能安全研究视为迫切重要且值得广泛支持。他们采用投资组合方法，为从乐观到悲观的各种场景做准备，以应对创建安全人工智能系统的难度。'
- en: '**Empirical Focus**: The company emphasizes empirically driven safety research,
    believing that close contact with frontier AI systems is crucial for identifying
    and addressing potential risks before they become critical issues.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实证重点**：公司强调以实证为基础的安全研究，认为与前沿人工智能系统的密切接触对于在风险成为关键问题之前识别和解决潜在风险至关重要。'
- en: '**Balancing Progress and Caution**: Anthropic carefully navigates the trade-off
    between advancing necessary safety research and potentially accelerating the deployment
    of dangerous technologies. They aim to integrate safety research into real systems
    quickly while maintaining responsible development practices.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平衡进步与谨慎**：Anthropic在推进必要的安全研究和可能加速危险技术部署之间谨慎权衡。他们旨在快速将安全研究整合到实际系统中，同时保持负责任的发展实践。'
- en: '**Transparency and Collaboration**: While not publishing capabilities research,
    Anthropic shares safety-oriented research to contribute to the broader AI community’s
    understanding of these issues.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**透明度和合作**：尽管不发布能力研究，Anthropic仍分享以安全为导向的研究，以贡献于更广泛的AI社区对这些问题的理解。'
- en: '**Ethical Deployment**: The company is thoughtful about demonstrating frontier
    capabilities, prioritizing safety research over public deployments when appropriate.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负责任的部署**：公司在展示前沿能力时持谨慎态度，在适当的情况下优先考虑安全研究而非公共部署。'
- en: '**Commitment to Safety Standards**: Anthropic plans to make externally verifiable
    commitments to only develop advanced models if specific safety standards are met,
    allowing independent evaluation of their models’ capabilities and safety.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对安全标准的承诺**：Anthropic计划对外做出可验证的承诺，只有在满足特定安全标准的情况下才会开发高级模型，允许对他们的模型能力和安全性进行独立评估。'
- en: '**Societal Impact Assessment**: They conduct research to evaluate the potential
    societal impacts of their AI systems, informing both internal policies and broader
    AI governance discussions.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社会影响评估**：他们进行研究以评估其AI系统可能产生的潜在社会影响，既为内部政策提供信息，也参与更广泛的AI治理讨论。'
- en: '**Responsible Scaling**: Anthropic recognizes the risks of rapidly scaling
    AI capabilities and aims to develop techniques like scalable oversight to ensure
    powerful systems remain aligned with human values.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负责任的扩展**：Anthropic认识到快速扩展AI能力的风险，并旨在开发可扩展监督等技术，以确保强大的系统与人类价值观保持一致。'
- en: '**Interdisciplinary Approach**: Their strategy spans technical research, policy
    considerations, and ethical deliberations, reflecting the complex nature of AI
    safety challenges.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跨学科方法**：他们的策略涵盖了技术研究、政策考虑和伦理审议，反映了AI安全挑战的复杂性质。'
- en: '**Adaptability**: Anthropic maintains flexibility in their approach, ready
    to adjust strategies as they learn more about AI development and its associated
    risks.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适应性**：Anthropic在其方法上保持灵活性，随时准备根据对AI发展和相关风险的了解调整策略。'
- en: By integrating these principles, Anthropic aims to contribute to the development
    of AI systems that are not only powerful but also safe, ethical, and beneficial
    to humanity. Their approach reflects a deep commitment to responsible innovation
    in the face of potentially transformative AI technologies. Anthropic’s research
    areas are diverse and comprehensive, focusing on various aspects of AI safety
    and responsible development. Their approach spans from immediate practical concerns
    to long-term, speculative risks, demonstrating a commitment to addressing the
    multifaceted challenges of AI development.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 通过整合这些原则，Anthropic旨在为开发既强大又安全、符合伦理且对人类有益的AI系统做出贡献。他们的方法反映了面对可能变革性的AI技术时对负责任创新的深厚承诺。Anthropic的研究领域多样且全面，专注于AI安全和负责任发展的各个方面。他们的方法从即时的实际关注到长期的、推测性的风险，展示了应对AI发展多方面挑战的承诺。
- en: At the core of Anthropic’s research is *Mechanistic Interpretability*, which
    aims to “reverse engineer” neural networks into human-understandable algorithms.
    This ambitious project seeks to enable something akin to “code reviews” for AI
    models, potentially allowing thorough audits to identify unsafe aspects or provide
    strong safety guarantees. The team has made progress in extending this approach
    from vision models to small language models and has discovered mechanisms driving
    in-context learning.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic研究的核心是*机制可解释性*，旨在“逆向工程”神经网络成为人类可理解算法。这个雄心勃勃的项目旨在使AI模型能够进行类似“代码审查”的操作，可能允许进行彻底的审计以识别不安全方面或提供强大的安全保证。该团队在将这种方法从视觉模型扩展到小型语言模型方面取得了进展，并发现了驱动情境学习的机制。
- en: '*Scalable Oversight* is another crucial area of focus for Anthropic. This research
    aims to develop methods for AI systems to partially supervise themselves or assist
    humans in supervision. It’s a response to the challenge of providing adequate
    high-quality feedback to steer AI behaviors as systems become increasingly complex.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**可扩展的监督**是Anthropic关注的另一个关键领域。这项研究旨在开发使人工智能系统部分自我监督或协助人类进行监督的方法。这是对提供足够高质量反馈以引导人工智能行为的挑战的回应，随着系统变得越来越复杂，这一挑战变得更加严峻。'
- en: The team explores various approaches, including extensions of Constitutional
    AI, variants of human-assisted supervision, AI-AI debate, and red teaming via
    multi-agent reinforcement learning.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 团队探索了各种方法，包括宪法人工智能的扩展、人类辅助监督的变体、人工智能-人工智能辩论和多智能体强化学习中的红队攻击。
- en: '*Process-Oriented Learning* represents a novel approach to AI training. Instead
    of rewarding AI systems for achieving specific outcomes, this method focuses on
    training systems to follow safe processes. The goal is to address concerns about
    AI safety by ensuring systems follow comprehensible, approved processes rather
    than pursuing goals through potentially harmful or inscrutable means.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**以过程为导向的学习**代表了一种新颖的人工智能训练方法。这种方法不是通过奖励人工智能系统实现特定结果来训练系统，而是专注于训练系统遵循安全的过程。目标是确保系统遵循可理解的、经过批准的过程，而不是通过可能有害或难以理解的手段追求目标，从而解决关于人工智能安全性的担忧。'
- en: Testing for *Dangerous Failure Modes* is a proactive approach to identifying
    potential risks in AI systems. This involves deliberately training problematic
    properties into small-scale models to isolate and study them before they become
    direct threats in more capable systems. A particular area of interest is studying
    how AI systems behave when they are “situationally aware” and how this impacts
    their behavior during training.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 测试**危险故障模式**是识别人工智能系统中潜在风险的一种主动方法。这涉及到故意将问题属性训练到小型模型中，以便在它们成为更强大系统中的直接威胁之前进行隔离和研究。一个特别感兴趣的研究领域是研究人工智能系统在“情境感知”时的行为以及这如何影响它们在训练过程中的行为。
- en: Lastly, Anthropic places significant emphasis on researching *Societal Impacts
    and Evaluations*. This involves building tools and measurements to assess the
    capabilities, limitations, and potential societal impact of AI systems. Studies
    in this area have included research on predictability and surprise in LLMs, methods
    for red teaming language models, and investigations into reducing bias and stereotyping
    in AI outputs. This research not only informs Anthropic’s internal practices but
    also contributes to broader discussions on AI governance and policy.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Anthropic高度重视研究**社会影响和评估**。这涉及到构建工具和测量方法来评估人工智能系统的能力、局限性和潜在的社会影响。该领域的研究包括对LLMs中可预测性和意外的研究、针对语言模型的红队攻击方法，以及减少人工智能输出中的偏见和刻板印象的研究。这项研究不仅为Anthropic的内部实践提供了信息，也为更广泛的人工智能治理和政策讨论做出了贡献。
- en: These interconnected research areas collectively aim to address various aspects
    of AI safety and ethics. Anthropic’s approach is notable for its breadth and depth,
    reflecting the complex nature of AI development challenges. The company maintains
    flexibility in its research focus, ready to adapt as new information and challenges
    emerge in the rapidly evolving field of AI, demonstrating a commitment to responsible
    innovation in the face of potentially transformative AI technologies.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这些相互关联的研究领域共同旨在解决人工智能安全和伦理的各个方面。Anthropic的方法因其广度和深度而引人注目，反映了人工智能发展挑战的复杂性质。该公司在其研究重点上保持灵活性，随时准备适应人工智能快速发展的领域中出现的新信息和挑战，展示了面对可能具有变革性的人工智能技术时的负责任创新承诺。
- en: 'Anthropic, a leading AI research company, has developed a comprehensive approach
    to addressing the challenges of AI safety and ethics. Their method, called **Constitutional
    AI**, represents a significant advancement in creating AI systems that are both
    powerful and aligned with human values, while reducing the need for extensive
    human oversight in the training process. Their approach is highlighted in the
    paper *Constitutional AI: Harmlessness from AI Feedback*, ([https://arxiv.org/pdf/2212.08073](https://arxiv.org/pdf/2212.08073))
    which presents Anthropic’s approach to developing safe and responsible AI systems
    without relying on human feedback for harmlessness. The research introduces a
    method called **Constitutional AI** (**CAI**), which aims to create AI assistants
    that are both helpful and harmless while avoiding evasive responses.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic，一家领先的AI研究公司，已经开发了一套全面的方法来应对AI安全和伦理方面的挑战。他们的方法被称为**宪法AI**，在创建既强大又符合人类价值观的AI系统方面取得了重大进步，同时减少了在训练过程中对大量人类监督的需求。他们的方法在论文《宪法AI：来自AI反馈的无害性》中被突出展示，([https://arxiv.org/pdf/2212.08073](https://arxiv.org/pdf/2212.08073))，该论文介绍了Anthropic开发安全且负责任的AI系统的方法，而不依赖于人类反馈来评估无害性。研究引入了一种称为**宪法AI**（**CAI**）的方法，旨在创建既有帮助又无害的AI助手，同时避免规避回答。
- en: 'The core of Anthropic’s approach involves two main stages: a supervised learning
    phase and a reinforcement learning phase. In the supervised phase, the AI model
    is trained to critique and revise its own responses based on a set of principles
    or “constitution.” This process helps to reduce harmful content in the AI’s outputs.
    The reinforcement learning phase then further refines the model’s behavior using
    AI-generated feedback, rather than human labels, to evaluate harmlessness.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic的方法核心包括两个主要阶段：监督学习阶段和强化学习阶段。在监督阶段，AI模型被训练根据一套原则或“宪法”来批评和修改自己的响应。这个过程有助于减少AI输出中的有害内容。随后，强化学习阶段通过使用AI生成的反馈而不是人类标签来评估无害性，进一步细化模型的行为。
- en: Anthropic’s motivation for developing this technique stems from several factors.
    They aim to scale supervision by leveraging AI to help oversee other AI systems,
    reduce the tension between helpfulness and harmlessness in AI assistants, increase
    transparency in AI decision-making, and decrease reliance on extensive human feedback.
    The researchers emphasize the importance of empirical, data-driven approaches
    to AI safety, as well as the need for flexibility in addressing various potential
    scenarios, from optimistic to pessimistic, regarding the difficulty of creating
    safe AI systems.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic开发这种技术的动机源于几个因素。他们旨在通过利用AI来帮助监督其他AI系统，减少AI助手中帮助性和无害性之间的紧张关系，提高AI决策的透明度，并减少对大量人类反馈的依赖。研究人员强调，实证、数据驱动的方法对AI安全的重要性，以及应对从乐观到悲观的各种潜在场景时所需的灵活性，这些场景涉及创建安全AI系统的难度。
- en: The paper presents experimental results showing that the Constitutional AI method
    can produce models that are both less harmful and more helpful than those trained
    with traditional **reinforcement learning from human feedback** (**RLHF**). Notably,
    the CAI models demonstrate the ability to engage with sensitive topics in a thoughtful,
    non-evasive manner while still maintaining safety.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 论文展示了实验结果，表明宪法AI方法可以产生比使用传统**从人类反馈中强化学习**（**RLHF**）训练的模型更少有害且更有帮助的模型。值得注意的是，CAI模型展示了以深思熟虑、非规避的方式处理敏感话题的能力，同时仍保持安全性。
- en: Anthropic’s approach also incorporates techniques such as chain-of-thought reasoning
    to improve the transparency and interpretability of AI decision-making. This allows
    better understanding of how the AI system arrives at its conclusions and behaviors.
    The researchers stress the importance of continuous adaptation and improvement
    in their safety techniques as AI capabilities advance.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic的方法还结合了诸如思维链推理等技术，以提高AI决策的透明度和可解释性。这有助于更好地理解AI系统如何得出结论和采取行为。研究人员强调，随着AI能力的提升，他们对其安全技术的持续适应和改进的重要性。
- en: The paper concludes by discussing potential future directions for this research,
    including applying constitutional methods to steer AI behavior in various ways,
    improving robustness against adversarial attacks, and further refining the balance
    between helpfulness and harmlessness. Anthropic acknowledges the dual-use potential
    of these techniques and emphasizes the need for responsible development and deployment
    of AI systems.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 论文最后讨论了该研究的潜在未来方向，包括将宪法方法应用于以各种方式引导人工智能行为，提高对对抗性攻击的鲁棒性，以及进一步细化有益性和无害性之间的平衡。Anthropic承认这些技术的双重用途潜力，并强调人工智能系统负责任开发和部署的必要性。
- en: Overall, this research represents a significant step towards creating AI systems
    that can be both powerful and aligned with human values, while reducing the need
    for extensive human oversight in the training process.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这项研究代表了朝着创建既强大又符合人类价值观的人工智能系统迈出的重要一步，同时减少了在训练过程中对大量人工监督的需求。
- en: Summary
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'When designing AI applications, developers can incorporate elements from both
    Google’s and Anthropic’s approaches to create more responsible, safer systems.
    This comprehensive strategy involves several key areas of focus:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当设计人工智能应用时，开发者可以结合谷歌和Anthropic的方法，创建更负责任、更安全的系统。这种综合策略涉及几个关键关注领域：
- en: '*Comprehensive safety and impact assessment* is crucial for responsible AI
    development. This process should include scenario planning for both optimistic
    and pessimistic outcomes, allowing developers to prepare for a wide range of possibilities.
    Empirical testing of safety measures on small-scale systems is essential, as it
    provides valuable insights without the risks associated with large-scale deployment.
    Engaging diverse stakeholders helps identify potential issues early in the development
    process, ensuring a broad perspective on the AI system’s potential impacts. Establishing
    clear safety thresholds before scaling or deploying more advanced capabilities
    helps maintain control over the AI’s development and ensures that safety remains
    a priority throughout the process.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*全面的安全和影响评估*对于负责任的人工智能开发至关重要。这个过程应包括对乐观和悲观结果的情景规划，使开发者能够为各种可能性做好准备。在小型系统上对安全措施进行实证测试是必不可少的，因为它提供了宝贵的见解，而没有大规模部署的风险。让多元化的利益相关者参与有助于在开发早期识别潜在问题，确保对人工智能系统潜在影响有广泛的视角。在扩展或部署更高级功能之前建立明确的安全阈值，有助于保持对人工智能发展的控制，并确保在整个过程中安全始终是首要任务。'
- en: '*A transparent and adaptive development process* is vital for building trust
    and maintaining the safety of AI systems. Implementing explainable AI techniques
    makes the system’s decision-making process more interpretable, allowing both developers
    and users to understand how the AI arrives at its conclusions. Regular external
    audits of the system’s performance, fairness, and safety provide objective assessments
    and help identify areas for improvement. Clear documentation of the model’s capabilities,
    limitations, and intended use cases helps manage expectations and prevents misuse.
    Developing a process for rapidly integrating new safety research findings ensures
    that the AI system remains up to date with the latest advancements in AI safety.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*透明和适应性强的开发过程*对于建立信任并保持人工智能系统的安全性至关重要。实施可解释的人工智能技术使系统的决策过程更具可解释性，使开发者和用户都能理解人工智能如何得出结论。定期对系统的性能、公平性和安全性进行外部审计提供客观评估，并有助于确定改进领域。对模型的性能、局限性和预期用例进行清晰的文档记录有助于管理期望并防止滥用。开发一个快速整合新的安全研究成果的过程确保人工智能系统始终与人工智能安全领域的最新进展保持同步。'
- en: '*Fairness and bias mitigation* are critical aspects of responsible AI development.
    Ensuring AI systems don’t perpetuate or amplify biases requires using diverse
    and representative training data, implementing proactive and reactive bias detection
    and mitigation measures, and conducting regular fairness audits across different
    contexts and user groups. Applying contextual fairness appropriate to the specific
    application helps ensure that the AI system’s decisions are equitable in various
    scenarios.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*公平性和偏见缓解*是负责任的人工智能开发的关键方面。确保人工智能系统不会持续或放大偏见，需要使用多样化和代表性的训练数据，实施主动和反应性的偏见检测和缓解措施，并在不同的背景和用户群体中进行定期的公平性审计。应用适合特定应用的情境公平性有助于确保人工智能系统在各种场景中的决策是公平的。'
- en: '*Privacy and data protection* are key to safe AI development. Implementing
    strong safeguards for user privacy involves applying data minimization principles,
    using anonymization, encryption, and privacy-preserving AI techniques, providing
    users with clear control over their data, and establishing and enforcing data
    retention policies. These measures help build user trust and ensure compliance
    with data protection regulations.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*隐私和数据保护* 对于安全的AI开发至关重要。实施强大的用户隐私保护措施包括应用数据最小化原则，使用匿名化、加密和隐私保护AI技术，为用户提供对其数据的明确控制权，并建立和执行数据保留政策。这些措施有助于建立用户信任并确保符合数据保护法规。'
- en: '*Safety and security measures* are essential for ensuring AI systems behave
    as intended and resist potential attacks. This includes implementing content filtering
    and rate limiting to prevent misuse, conducting regular adversarial testing to
    identify vulnerabilities, setting up monitoring and alerting systems for anomalous
    behavior, and applying multimodal safety considerations for versatile AI systems.
    These measures help protect the AI system from external threats and prevent unintended
    harmful behaviors.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '*安全和安全措施* 对于确保AI系统按预期行为并抵抗潜在攻击至关重要。这包括实施内容过滤和速率限制以防止滥用，定期进行对抗性测试以识别漏洞，建立监控和警报系统以检测异常行为，以及为多模态AI系统应用多模式安全考虑。这些措施有助于保护AI系统免受外部威胁并防止意外有害行为。'
- en: Responsible AI is not just an ethical imperative; it’s a crucial component of
    building sustainable, trustworthy, and effective GenAI applications. By integrating
    fairness, interpretability, privacy, and safety considerations throughout the
    development lifecycle, we can harness the power of models like Google Gemini while
    mitigating potential risks and negative impacts.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的AI不仅是一个道德上的要求，而且是构建可持续、值得信赖和有效的通用人工智能应用的关键组成部分。通过在整个开发周期中整合公平性、可解释性、隐私和安全考虑因素，我们可以利用像Google
    Gemini这样的模型的力量，同时减轻潜在的风险和负面影响。
- en: As we’ve explored in this chapter, implementing responsible AI practices requires
    a multifaceted approach. This includes diverse and representative data, bias detection
    and mitigation, explainable AI techniques, robust privacy protections, and comprehensive
    safety measures. By learning from industry leaders like Google and Anthropic,
    and adapting their frameworks to our specific use cases, we can create GenAI applications
    that not only push the boundaries of what’s possible but do so in a way that benefits
    society as a whole.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章中探讨的那样，实施负责任的AI实践需要多方面的方法。这包括多样化的代表性数据、偏见检测和缓解、可解释AI技术、强大的隐私保护和全面的安全措施。通过学习像Google和Anthropic这样的行业领导者，并将他们的框架适应到我们的特定用例中，我们可以创建通用人工智能应用，不仅推动可能性的边界，而且以造福整个社会的方式进行。
- en: Remember that responsible AI is an ongoing process, not a one-time checkbox.
    As GenAI technologies continue to evolve rapidly, it’s essential to stay informed
    about the latest developments in AI ethics, regularly reassess our applications,
    and be prepared to adapt our practices accordingly.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，负责任的AI是一个持续的过程，而不是一次性的勾选框。随着通用人工智能技术持续快速地发展，了解AI伦理的最新进展至关重要，定期重新评估我们的应用，并准备好相应地调整我们的实践。
- en: Throughout this book, we’ve embarked on a comprehensive journey through the
    world of GenAI, focusing on leveraging powerful models like Google Gemini on Vertex
    AI. We began by exploring the process of use case ideation and selection, learning
    how to identify opportunities where GenAI can provide significant value and transform
    business processes.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们开始了一段穿越通用人工智能世界的全面旅程，重点关注利用像Vertex AI上的Google Gemini这样的强大模型。我们首先探索了用例构思和选择的过程，学习如何识别通用人工智能可以提供重大价值并转型业务流程的机会。
- en: From there, we delved into the crucial phase of **proof of concept** (**POC**)
    development with four practical examples contextualized into a framework to simplify
    scale and refine our approach. We then explored the critical aspects of instrumentation,
    learning how to integrate GenAI models effectively into our applications and systems,
    and how to monitor and optimize their performance.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里，我们深入到至关重要的**概念验证**（**POC**）开发阶段，通过四个实际示例，将其置于一个框架中以简化规模并细化我们的方法。然后，我们探讨了关键的仪器化方面，学习如何有效地将通用人工智能模型集成到我们的应用和系统中，以及如何监控和优化其性能。
- en: Finally, we addressed the vital topic of responsible AI, ensuring that as we
    push the boundaries of what’s possible with GenAI, we do so in a way that is ethical,
    fair, and beneficial to society.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们讨论了至关重要的负责任AI主题，确保我们在推动生成式AI可能性的边界时，以符合道德、公平且对社会有益的方式进行。
- en: This journey from ideation to responsible implementation represents a blueprint
    for successfully leveraging GenAI in real-world applications. As these technologies
    continue to advance at a rapid pace, the principles and practices we’ve discussed
    will become increasingly important.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 从构思到负责任实施的过程代表了在现实世界中成功利用生成式AI的蓝图。随着这些技术以快速的速度发展，我们讨论的原则和实践将变得越来越重要。
- en: The future of AI is not just about building more powerful models; it’s about
    building smarter, more responsible systems that augment human capabilities and
    contribute positively to our world. By following the approaches outlined in this
    book – from careful use case selection to rigorous POC development, thoughtful
    instrumentation, and unwavering commitment to responsible AI – you are well-equipped
    to lead the way in this exciting and transformative field.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的未来不仅仅是构建更强大的模型；它关于构建更智能、更负责任的系统，这些系统能够增强人类能力，并为我们的世界做出积极贡献。通过遵循本书中概述的方法——从谨慎选择用例到严格的POC开发、深思熟虑的仪表化和对负责任AI的坚定不移的承诺——您将充分准备好在这个激动人心且变革性的领域中引领潮流。
- en: As you move forward with your own GenAI projects, remember that success lies
    not just in technical implementation, but in the thoughtful consideration of how
    these powerful tools can be used to create genuine value while upholding the highest
    standards of ethics and responsibility. The journey of AI development is ongoing,
    and your role in shaping its future begins now.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在您推进自己的生成式AI项目时，请记住，成功不仅在于技术实现，还在于深思熟虑地考虑如何使用这些强大的工具在创造真正价值的同时，维护最高的道德和责任标准。AI发展的旅程是持续的，您在塑造其未来的角色现在就开始了。
- en: Join our community on Discord
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的社区Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/genpat](Chapter_10.xhtml)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/genpat](Chapter_10.xhtml)'
- en: '![](img/QR_Code134841911667913109.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code134841911667913109.png)'
- en: '![](img/New_Packt_Logo.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![新Packt标志](img/New_Packt_Logo.png)'
- en: '[packt.com](https://packt.com)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[packt.com](https://packt.com)'
- en: Subscribe to our online digital library for full access to over 7,000 books
    and videos, as well as industry leading tools to help you plan your personal development
    and advance your career. For more information, please visit our website.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 订阅我们的在线数字图书馆，全面访问超过7,000本书籍和视频，以及行业领先的工具，帮助您规划个人发展并推进您的职业生涯。更多信息，请访问我们的网站。
- en: Why subscribe?
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么订阅？
- en: Spend less time learning and more time coding with practical eBooks and Videos
    from over 4,000 industry professionals
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用来自超过4,000位行业专业人士的实用电子书和视频，花更少的时间学习，更多的时间编码
- en: Improve your learning with Skill Plans built especially for you
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过为您量身定制的技能计划提高您的学习效果
- en: Get a free eBook or video every month
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每月免费获得一本电子书或视频
- en: Fully searchable for easy access to vital information
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全可搜索，便于轻松访问关键信息
- en: Copy and paste, print, and bookmark content
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制粘贴、打印和收藏内容
- en: At [www.packt.com](https://www.packt.com), you can also read a collection of
    free technical articles, sign up for a range of free newsletters, and receive
    exclusive discounts and offers on Packt books and eBooks.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在[www.packt.com](https://www.packt.com)，您还可以阅读一系列免费的技术文章，订阅各种免费通讯，并享受Packt书籍和电子书的独家折扣和优惠。
- en: Other Books You May Enjoy
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 您可能还会喜欢的其他书籍
- en: 'If you enjoyed this book, you may be interested in these other books by Packt:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您喜欢这本书，您可能对Packt出版的以下其他书籍也感兴趣：
- en: '[![](img/9781835083468.png)](https://www.packtpub.com/en-in/product/generative-ai-with-langchain-9781835083468)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '![封面](img/9781835083468.png)(https://www.packtpub.com/en-in/product/generative-ai-with-langchain-9781835083468)'
- en: '**Generative AI with LangChain**'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用LangChain的生成式AI**'
- en: Ben Auffarth
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 本·奥法特
- en: 'ISBN: 978-1-83508-346-8'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 'ISBN: 978-1-83508-346-8'
- en: Understand LLMs, their strengths and limitations
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解LLM，它们的优点和局限性
- en: Grasp generative AI fundamentals and industry trends
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 掌握生成式AI基础和行业趋势
- en: Create LLM apps with LangChain like question-answering systems and chatbots
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LangChain创建类似问答系统和聊天机器人的LLM应用
- en: Understand transformer models and attention mechanisms
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解Transformer模型和注意力机制
- en: Automate data analysis and visualization using pandas and Python
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用pandas和Python自动化数据分析可视化
- en: Grasp prompt engineering to improve performance
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 掌握提示工程以提高性能
- en: Fine-tune LLMs and get to know the tools to unleash their power
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调LLMs并了解释放其力量的工具
- en: Deploy LLMs as a service with LangChain and apply evaluation strategies
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LangChain将LLMs作为服务部署并应用评估策略
- en: Privately interact with documents using open-source LLMs to prevent data leaks
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用开源LLMs私下与文档互动，以防止数据泄露
- en: '[![](img/9781835462317_cov.png)](https://www.packtpub.com/en-in/product/building-llm-powered-applications-9781835462317)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/9781835462317_cov.png)](https://www.packtpub.com/en-in/product/building-llm-powered-applications-9781835462317)'
- en: '**Building LLM Powered Applications**'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '**构建由LLM驱动的应用程序**'
- en: Valentina Alto
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Valentina Alto
- en: 'ISBN: 978-1-83546-231-7'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 'ISBN: 978-1-83546-231-7'
- en: Explore the core components of LLM architecture, including encoder-decoder blocks
    and embeddings
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索LLM架构的核心组件，包括编码器-解码器块和嵌入
- en: Understand the unique features of LLMs like GPT-3.5/4, Llama 2, and Falcon LLM
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解LLMs（如GPT-3.5/4、Llama 2和Falcon LLM）的独特功能
- en: Use AI orchestrators like LangChain, with Streamlit for the frontend
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LangChain等AI编排器，配合Streamlit进行前端开发
- en: Get familiar with LLM components such as memory, prompts, and tools
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉LLM组件，如内存、提示和工具
- en: Learn how to use non-parametric knowledge and vector databases
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何使用非参数知识和向量数据库
- en: Understand the implications of LFMs for AI research and industry applications
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解LFMs对人工智能研究和行业应用的影响
- en: Customize your LLMs with fine tuning
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用微调自定义你的LLMs
- en: Learn about the ethical implications of LLM-powered applications
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解由LLM驱动的应用程序的伦理影响
- en: Packt is searching for authors like you
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Packt正在寻找像你这样的作者
- en: If you’re interested in becoming an author for Packt, please visit [authors.packtpub.com](https://authors.packtpub.com)
    and apply today. We have worked with thousands of developers and tech professionals,
    just like you, to help them share their insight with the global tech community.
    You can make a general application, apply for a specific hot topic that we are
    recruiting an author for, or submit your own idea.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣成为Packt的作者，请访问[authors.packtpub.com](https://authors.packtpub.com)并今天申请。我们已与成千上万的开发者和科技专业人士合作，就像你一样，帮助他们将见解分享给全球科技社区。你可以提交一般申请，申请我们正在招募作者的特定热门话题，或者提交你自己的想法。
- en: Share your thoughts
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分享你的想法
- en: Now you’ve finished *Generative AI Application Integration Patterns*, we’d love
    to hear your thoughts! If you purchased the book from Amazon, please [click here
    to go straight to the Amazon review page](https://packt.link/r/1835887619) for
    this book and share your feedback or leave a review on the site that you purchased
    it from.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经完成了《生成式AI应用集成模式》，我们非常想听听你的想法！如果你在亚马逊购买了这本书，请[点击此处直接转到该书的亚马逊评论页面](https://packt.link/r/1835887619)并分享你的反馈或在该购买网站上留下评论。
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 你的评论对我们和科技社区非常重要，并将帮助我们确保我们提供高质量的内容。
