- en: <st c="0">5</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2">Managing Security in RAG Applications</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="39">Depending on the environment in which you are building your</st>
    **<st c="100">retrieval-augmented generation</st>** <st c="130">(</st>**<st c="132">RAG</st>**<st
    c="135">) application, security failures can lead to legal liability, reputation
    damage, and costly</st> <st c="228">service disruptions.</st> <st c="249">RAG
    systems present unique security risks, primarily due to their reliance on external
    data sources for enhancing content generation.</st> <st c="383">To address these
    risks, we will dive deep into the world of RAG application security, exploring
    both the security-related advantages and potential risks associated with</st>
    <st c="552">this technology.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="568">In this chapter, the topics that we will cover include</st> <st
    c="624">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="638">How RAG can be leveraged as a</st> <st c="669">security solution</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="686">RAG</st> <st c="691">security challenges</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="710">Red teaming</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="722">Common areas to target with</st> <st c="751">red teaming</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="762">Code lab 5.1 – Securing</st> <st c="787">your code</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="796">Code lab 5.2 – Red</st> <st c="816">team attack!</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="828">Code lab 5.3 – Blue</st> <st c="849">team defend!</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="861">By the end of the chapter, you will have a comprehensive understanding
    of the security landscape surrounding RAG applications, equipped with practical
    strategies and techniques to safeguard your systems and data.</st> <st c="1075">As
    we embark on this journey, remember that security is an ongoing process that requires
    constant vigilance and adaptation in the face of ever-evolving threats.</st> <st
    c="1236">Let’s dive in and explore how to build secure, trustworthy, and robust
    RAG applications that harness the power of generative artificial intelligence
    (AI) while prioritizing the safety and privacy of users and</st> <st c="1445">businesses
    alike.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1462">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1467">As with any other technical application that has users and technical
    infrastructure, there are numerous general security concerns that you must address.</st>
    <st c="1621">Given the scope of this chapter and book, our focus is on security
    aspects that are specific to</st> <st c="1717">RAG applications.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1734">Technical requirements</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1757">The code for this chapter is placed in the following GitHub</st>
    <st c="1818">repository:</st> [<st c="1830">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_05</st>](https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_05)
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1927">How RAG can be leveraged as a security solution</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1975">Let’s start</st> <st c="1987">with the most positive security aspect
    of RAG.</st> <st c="2035">RAG can actually be considered a solution to mitigate
    security concerns, rather than cause them.</st> <st c="2132">If done right, you
    can limit data access via user, ensure more reliable responses, and provide more
    transparency</st> <st c="2245">of sources.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2256">Limiting data</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="2270">RAG applications</st> <st c="2288">may be a relatively new concept,
    but you can still apply the same authentication and database-based access approaches
    you can with web and similar types of applications.</st> <st c="2458">This provides
    the same level of security you can apply in these other types of applications.</st>
    <st c="2551">By implementing user-based access controls, you can restrict the
    data that each user or user group can retrieve through the RAG system.</st> <st
    c="2687">This ensures that sensitive information is only accessible to authorized
    individuals.</st> <st c="2773">Additionally, by leveraging secure database connections
    and encryption techniques, you can safeguard the data at rest and in transit,
    preventing unauthorized access or</st> <st c="2941">data breaches.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2955">Ensuring the reliability of generated content</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="3001">One of the</st> <st c="3013">key benefits of RAG is its ability
    to mitigate inaccuracies in generated content.</st> <st c="3095">By allowing applications
    to retrieve proprietary data at the point of generation, the risk of producing
    misleading or incorrect responses is substantially reduced.</st> <st c="3259">Feeding
    the most current data available through your RAG system helps to mitigate inaccuracies
    that might</st> <st c="3365">otherwise occur.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3381">With RAG, you</st> <st c="3395">have control over the data sources
    used for retrieval.</st> <st c="3451">By carefully curating and maintaining high-quality,
    up-to-date datasets, you can ensure that the information used to generate responses
    is accurate and reliable.</st> <st c="3613">This is particularly important in
    domains where precision and correctness are critical, such as healthcare, finance,
    or</st> <st c="3733">legal applications.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3752">Maintaining transparency</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="3777">RAG makes it easier to</st> <st c="3800">provide transparency in
    the generated content.</st> <st c="3848">By incorporating data such as citations
    and references to the retrieved data sources, you can increase the credibility
    and trustworthiness of the</st> <st c="3994">generated responses.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4014">When a RAG system generates a response, it can include links or
    references to the specific data points or documents used in the generation process.</st>
    <st c="4163">This allows users to verify the information and trace it back to
    its original sources.</st> <st c="4250">By providing this level of transparency,
    you can build trust with your users and demonstrate the reliability of the</st>
    <st c="4366">generated content.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4384">Transparency in RAG can also help with accountability and auditing.</st>
    <st c="4453">If there are any concerns or disputes regarding the generated content,
    having clear citations and references makes it easier to investigate and resolve
    any issues.</st> <st c="4617">This transparency also facilitates compliance with
    regulatory requirements or industry standards that may require traceability</st>
    <st c="4744">of information.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4759">That covers many of the security-related benefits you can achieve
    with RAG.</st> <st c="4836">However, there are some security challenges associated
    with RAG as well.</st> <st c="4909">Let’s discuss these</st> <st c="4929">challenges
    next.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4945">RAG security challenges</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="4969">RAG applications face</st> <st c="4991">unique security challenges
    due to their reliance on</st> **<st c="5044">large language models</st>** <st
    c="5065">(</st>**<st c="5067">LLMs</st>**<st c="5071">) and</st> <st c="5077">external
    data sources.</st> <st c="5101">Let’s start with</st> <st c="5117">the</st> **<st
    c="5122">black box challenge</st>**<st c="5141">, highlighting the relative difficulty
    in understanding how an LLM determines</st> <st c="5219">its response.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5232">LLMs as black boxes</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="5252">When</st> <st c="5257">something is in a dark, black box with the
    lid closed, you cannot see what is going on in there!</st> <st c="5355">That is
    the idea behind the black box when discussing LLMs, meaning there is a lack of
    transparency and interpretability in how these complex AI models process input
    and generate output.</st> <st c="5542">The most popular LLMs are also some of
    the largest, meaning they can have more than 100 billion parameters.</st> <st
    c="5650">The intricate interconnections and weights of these parameters make it
    difficult to understand how the model arrives at a</st> <st c="5772">particular
    output.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5790">While the black box aspects of LLMs do not directly create a security
    problem, it does make it more difficult to identify solutions to problems when
    they occur.</st> <st c="5952">This makes it difficult to trust LLM outputs, which
    is a critical factor in most of the applications for LLMs, including RAG applications.</st>
    <st c="6091">This lack of transparency makes it more difficult to debug issues
    you might have in building an RAG application, which increases the risk of having
    more</st> <st c="6244">security issues.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6260">There is a lot of research and effort in the academic field to
    build models that are more transparent and interpretable, called</st> **<st c="6389">explainable
    AI</st>**<st c="6403">. Explainable AI aims at making the operations of</st> <st
    c="6453">AI systems transparent and understandable.</st> <st c="6496">It can involve
    tools, frameworks, and anything else that, when applied to RAG, helps us understand
    how the language models that we use produce the content they are generating.</st>
    <st c="6672">This is a big movement in the field, but this technology may not
    be immediately available as you read this.</st> <st c="6780">It will hopefully
    play a larger role in the future to help mitigate black box risk, but right now,
    none of the most popular LLMs are using explainable models.</st> <st c="6939">So,
    in the meantime, we will talk about other ways to address</st> <st c="7001">this
    issue.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7012">You can</st> <st c="7021">use</st> **<st c="7025">human-in-the-loop</st>**<st
    c="7042">, where you involve</st> *<st c="7062">humans</st>* <st c="7068">at different
    stages of the process to provide an added line of defense against unexpected outputs.</st>
    <st c="7168">This can often help to reduce the impact of the black box aspect
    of LLMs.</st> <st c="7242">If your response time is not as critical, you</st>
    <st c="7288">can also use an additional LLM to perform a review of the response
    before it is returned to the user, looking for issues.</st> <st c="7410">We will
    review how to add a second LLM call in</st> *<st c="7457">code lab 5.3,</st>*
    <st c="7470">but with a focus on preventing prompt attacks.</st> <st c="7518">But
    this concept is similar, in that you can add additional LLMs to do a number of
    extra tasks and improve the security of</st> <st c="7641">your application.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="7658">Black box</st>* <st c="7668">isn’t the only security issue you
    face when using RAG applications though; another very important topic is</st>
    <st c="7776">privacy protection.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7795">Privacy concerns and protecting user data</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**<st c="7837">Personally identifiable information</st>** <st c="7873">(</st>**<st
    c="7875">PII</st>**<st c="7878">) is a key topic in the generative AI space, with
    governments</st> <st c="7941">around the world trying to</st> <st c="7967">determine
    the best path to balance user privacy with the data-hungry needs of these LLMs.</st>
    <st c="8058">As this</st> <st c="8066">gets worked out, it is important to pay
    attention to the laws and regulations that are taking shape where your company
    is doing business and make sure all of the technologies you are integrating into
    your RAG applications adhere.</st> <st c="8296">Many companies, such as Google
    and Microsoft, are taking these efforts into their own hands, establishing their
    own standards of protection for their user data and emphasizing them in training
    literature for</st> <st c="8504">their platforms.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8520">At the corporate level, there is another challenge related to PII
    and sensitive information.</st> <st c="8614">As we have said many times, the nature
    of the RAG application is to give it access to the company data and combine that
    with the power of the LLM.</st> <st c="8761">For example, for financial institutions,
    RAG represents a way to give their customers unprecedented access to their own
    data in ways that allow them to speak naturally with technologies such as chatbots
    and get near-instant access to hard-to-find answers buried deep in their</st>
    <st c="9037">customer data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9051">In many ways, this can be a huge benefit if implemented properly.</st>
    <st c="9118">But given that this is a security discussion, you</st> <st c="9167">may
    already see where I am going with this.</st> <st c="9212">We are giving unprecedented
    access to customer data using a technology that has artificial intelligence, and
    as we said previously in the black box discussion, we don’t completely understand
    how</st> <st c="9406">it works!</st> <st c="9417">If not implemented properly,
    this could be a recipe for disaster with massive negative repercussions for companies
    that get it wrong.</st> <st c="9551">Of course, it could be argued that the databases
    that contain the data are also a potential security risk.</st> <st c="9658">Having
    the data anywhere is a risk!</st> <st c="9694">But without taking on this risk,
    we also cannot provide the significant benefits</st> <st c="9775">they represent.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9790">As with other IT applications that contain sensitive data, you
    can forge forward, but you need to have a healthy fear of what can happen to data
    and proactively take measures to protect that data.</st> <st c="9988">The more
    you understand how RAG works, the better job you can do in preventing a potentially
    disastrous data leak.</st> <st c="10103">These steps can help you protect your
    company as well as the people who trusted your company with</st> <st c="10201">their
    data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10212">This section was about protecting data that exists.</st> <st c="10265">However,
    a new risk that has risen with LLMs has been the generation of data that isn’t</st>
    *<st c="10353">real</st>*<st c="10357">, called hallucinations.</st> <st c="10382">Let’s
    discuss how this presents a new risk not common in the</st> <st c="10443">IT world.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10452">Hallucinations</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="10467">We have discussed this</st> <st c="10491">in previous chapters,
    but LLMs can, at times, generate responses that sound coherent and factual but
    can be very wrong.</st> <st c="10611">These are called</st> **<st c="10628">hallucinations</st>**
    <st c="10642">and there have been many shocking examples</st> <st c="10685">provided
    in the news, especially in late 2022 and 2023, when LLMs became everyday tools
    for</st> <st c="10778">many users.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10789">Some are just funny with little consequence other than a good
    laugh, such as when ChatGPT was asked by a writer for</st> *<st c="10906">The
    Economist</st>*<st c="10919">, “</st>`<st c="10922">When was the Golden Gate Bridge
    transported for the second time across Egypt?</st>`<st c="11000">” ChatGPT responded,
    “</st>`<st c="11023">The Golden Gate Bridge was transported for the second time
    across Egypt in October of</st>` `<st c="11110">2016</st>`<st c="11114">” (</st>[<st
    c="11118">https://www.economist.com/by-invitation/2022/09/02/artificial-neural-networks-today-are-not-conscious-according-to-douglas-hofstadter</st>](https://www.economist.com/by-invitation/2022/09/02/artificial-neural-networks-today-are-not-conscious-according-to-douglas-hofstadter)<st
    c="11252">).</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11255">Other hallucinations are more nefarious, such as when a New York
    lawyer used ChatGPT for legal research in a client’s personal injury case against
    Avianca Airlines, where he submitted six cases that had been completely made up
    by the chatbot, leading to court sanctions (</st>[<st c="11527">https://www.courthousenews.com/sanctions-ordered-for-lawyers-who-relied-on-chatgpt-artificial-intelligence-to-prepare-court-brief/</st>](https://www.courthousenews.com/sanctions-ordered-for-lawyers-who-relied-on-chatgpt-artificial-intelligence-to-prepare-court-brief/)<st
    c="11658">).</st> <st c="11662">Even worse, generative AI has been known to give
    biased, racist, and bigoted perspectives, particularly when prompted in a</st>
    <st c="11785">manipulative way.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11802">When combined</st> <st c="11816">with the black box nature of
    these LLMs, where we are not always certain how and why a response is generated,
    this can be a genuine issue for companies wanting to use these LLMs in their</st>
    <st c="12004">RAG applications.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12021">From what we know though, hallucinations are primarily a result
    of the probabilistic nature of LLMs.</st> <st c="12123">For all responses that
    an LLM generates, it typically uses a probability distribution to determine what
    token it is going to provide next.</st> <st c="12262">In situations where it has
    a strong knowledge base of a certain subject, these probabilities for the next
    word/token can be 99% or higher.</st> <st c="12401">But in situations where the
    knowledge base is not as strong, the highest probability could be low, such as
    20% or even lower.</st> <st c="12527">In these cases, it is still the highest
    probability and, therefore, that is the token that has the highest probability
    to be selected.</st> <st c="12662">The LLM has been trained on stringing tokens
    together in a very natural language way while using this probabilistic approach
    to select which tokens to display.</st> <st c="12822">As it strings together words
    with low probability, it forms sentences, and then paragraphs that sound natural
    and factual but are not based on high probability data.</st> <st c="12988">Ultimately,
    this results in a response that sounds very plausible but is, in fact, based on
    very loose facts that</st> <st c="13102">are incorrect.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13116">For a company, this poses a risk that goes beyond the embarrassment
    of your chatbot saying something wrong.</st> <st c="13225">What is said wrong
    could ruin your relationship(s) with your customer(s), or it could lead to the
    LLM offering your customer something that you did not intend to offer, or worse,
    cannot afford to offer.</st> <st c="13428">For example, when Microsoft released
    a chatbot named Tay on Twitter in 2016 with the intention of</st> *<st c="13526">learning</st>*
    <st c="13534">from interactions with Twitter users, users manipulated this spongy
    personality trait to get</st> *<st c="13628">it</st>* <st c="13631">to say numerous
    racist and bigoted remarks.</st> <st c="13675">This reflected poorly on Microsoft,
    which was promoting its expertise in the AI area with Tay, causing significant
    damage to its reputation at the</st> <st c="13822">time (</st>[<st c="13828">https://www.theguardian.com/technology/2016/mar/26/microsoft-deeply-sorry-for-offensive-tweets-by-ai-chatbot</st>](https://www.theguardian.com/technology/2016/mar/26/microsoft-deeply-sorry-for-offensive-tweets-by-ai-chatbot)<st
    c="13937">).</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13940">Hallucinations, threats</st> <st c="13965">related to black box
    aspects, and protecting user data can all be addressed through red teaming.</st>
    <st c="14062">Let’s dive into this well-established security approach and learn
    how to apply it to RAG</st> <st c="14151">applications directly.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14173">Red teaming</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**<st c="14185">Red teaming</st>** <st c="14197">is a</st> <st c="14202">security
    testing methodology that involves simulating adversarial attacks to proactively
    identify and mitigate vulnerabilities in RAG applications.</st> <st c="14351">With
    the red team approach, an individual or team takes the role of the</st> *<st c="14423">red
    team</st>* <st c="14432">and has the goal of attacking and finding vulnerabilities
    in a system.</st> <st c="14503">The opposing team is the</st> *<st c="14528">blue
    team</st>*<st c="14537">, who does their best to thwart this attack.</st> <st
    c="14582">It is very common in the IT security space, particularly in cyber security.</st>
    <st c="14658">The concept of red teaming originated in the military, where it
    has been used for decades to improve strategies, tactics, and decision-making.</st>
    <st c="14801">But much like in the military, your RAG application has the potential
    to be the target of adversaries that have ill intentions for the company, particularly
    the user data you are trusted to protect.</st> <st c="15000">When applied to RAG,
    red teaming can help improve security by proactively identifying and mitigating</st>
    <st c="15101">potential risks.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15117">While red teaming is a widely accepted practice in general IT
    security, RAG applications have introduced a whole new set of threats for us to
    use red teaming to find and address.</st> <st c="15297">In the context of RAG
    applications, the main task of the red team is to bypass the safeguards of a given
    application, with the objective of finding ways to make the application misbehave,
    such as returning an inappropriate or</st> <st c="15523">incorrect answer.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15540">It is important to note that the evaluation of your RAG application
    from a security standpoint is different from other types of evaluation.</st> <st
    c="15681">You will often hear about benchmarks</st> <st c="15717">on LLMs in general,
    such as ARC (AI2 reasoning challenge), HellaSwag, and MMLU (massive multitask
    language understanding).</st> <st c="15841">These benchmarks test performance
    based on question-answering tasks.</st> <st c="15910">However, these benchmarks
    do not adequately test safety and security aspects, such as the model’s potential
    to generate offensive content, propagate stereotypes, or be used for nefarious
    purposes.</st> <st c="16107">Because RAG applications use LLMs, they share the
    same risks LLMs have, including toxicity, criminal activities, bias, and privacy
    concerns.</st> <st c="16248">Red teaming is an approach that is focused on identifying
    and defending against these types</st> <st c="16340">of risks.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16349">Developing a red team plan takes careful planning and a deep understanding
    of the vulnerabilities of these RAG systems.</st> <st c="16470">Let’s review the
    common areas you would want to attack as part of</st> <st c="16536">your plan.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16546">Common areas to target with red teaming</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="16586">Consider these categories</st> <st c="16612">for your red team
    RAG</st> <st c="16635">attack strategy:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="16651">Bias and stereotypes</st>**<st c="16672">: The chatbot may
    be manipulated to give biased answers, which can harm the company’s reputation
    if shared on</st> <st c="16783">social media.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="16796">Sensitive information disclosure</st>**<st c="16829">: Competitors
    or cybercriminals may attempt to obtain sensitive information, such as prompts
    or private data, through</st> <st c="16948">the chatbot.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="16960">Service disruption</st>**<st c="16979">: Ill-intentioned individuals
    may send long or crafted requests to disrupt the chatbot’s availability for</st>
    <st c="17086">legitimate users.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="17103">Hallucinations</st>**<st c="17118">: The chatbot may provide
    incorrect information due to suboptimal retrieval mechanisms, low-quality documents,
    or the LLM’s tendency to agree with</st> <st c="17266">the user.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="17275">Techniques you can employ to make</st> <st c="17309">these attacks
    include</st> <st c="17332">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="17346">Bypassing safeguards</st>**<st c="17367">:</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="17369">Text completion</st>**<st c="17384">: Red teaming</st> <st
    c="17398">techniques for bypassing safeguards in LLM applications include exploiting</st>
    <st c="17473">text completion by taking advantage of the LLM’s tendency to predict
    the next token in</st> <st c="17561">a sequence.</st>'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="17572">Biased prompts</st>**<st c="17587">: This technique</st> <st
    c="17604">involves using biased prompts that contain implicit bias to manipulate
    the model’s response and bypass content filters or other</st> <st c="17733">protective
    measures.</st>'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="17753">Prompt injection/jailbreaking</st>**<st c="17783">: Another
    approach is direct prompt injection, also</st> <st c="17835">known as jailbreaking,
    which involves injecting new instructions to overwrite the initial prompt and
    change the model’s behavior, effectively bypassing any restrictions or guidelines
    set in the</st> <st c="18030">original prompt.</st>'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="18046">Gray box prompt attacks</st>**<st c="18070">: Gray box prompt
    attacks</st> <st c="18096">can also be employed to bypass safeguards by injecting
    incorrect data within the prompt, assuming knowledge of the system prompt.</st>
    <st c="18227">This allows the attacker to manipulate the context and make the
    model generate unintended or harmful responses.</st> <st c="18339">How do you
    gain knowledge of the system prompt?</st> <st c="18387">Use the next approach,</st>
    <st c="18410">prompt probing.</st>'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="18425">Prompt probing</st>**<st c="18440">: Prompt probing</st> <st
    c="18458">can be used to discover the system prompt itself, enabling more efficient
    versions of the other attacks mentioned, by revealing the underlying structure
    and content of the prompts used to guide the</st> <st c="18656">LLM’s behavior.</st>'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="18671">Automating red teaming</st>** <st c="18694">To scale and repeat
    the red teaming process for all LLM applications, automation is</st> <st c="18779">crucial.</st>
    <st c="18788">This can be achieved through</st> <st c="18817">several approaches:</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="18836">Manually defined</st>**<st c="18853">: One method involves
    using a list of manually defined injection techniques and automating the detection
    of successful injections.</st> <st c="18985">By adding prompt injecting strings
    to a list and looping through each one, the automation tool can detect whether
    the injection bypasses</st> <st c="19122">the safeguards.</st>'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="19137">Prompt library</st>**<st c="19152">: Another approach is utilizing
    a library of prompts and automating the detection of injections.</st> <st c="19250">This
    method is similar to the previous one but relies on a list of known prompts.</st>
    <st c="19332">However, it requires maintaining an up-to-date library of prompt
    injection techniques to</st> <st c="19421">remain effective.</st>'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="19438">Open source tools that are continually updated</st>**<st c="19485">:
    A more advanced option is employing an automated tool, such as Giskard’s open
    source Python library</st> **<st c="19588">LLM scan</st>**<st c="19596">, which</st>
    <st c="19603">is regularly updated with the latest techniques by a team of machine
    learning (ML) researchers.</st> <st c="19700">Such a tool can run specialized
    tests on LLM-based applications, including those for prompt injections, and analyze
    the output to determine when a failure occurs.</st> <st c="19863">This approach
    saves time and effort in keeping up with the evolving landscape of injection techniques.</st>
    <st c="19966">These automated red teaming tools typically generate a thorough
    report outlining all the discovered attack vectors, providing valuable insights</st>
    <st c="20109">for improving the security and robustness of</st> <st c="20155">LLM
    applications.</st>'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="20172">Red teaming is a powerful approach to identifying vulnerabilities
    and improving the safety and security of LLM applications.</st> <st c="20298">By
    simulating adversarial attacks, organizations can proactively mitigate risks and
    ensure the robustness and reliability of their AI-powered applications.</st> <st
    c="20454">As the field of generative AI and RAG applications continues to evolve,
    red teaming will play an increasingly important role in addressing the novel and
    complex concepts of risk associated with</st> <st c="20648">these systems.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20662">It can be a daunting task to know where to start when designing
    your red team plan.</st> <st c="20747">While every situation is going to be relatively
    unique, you can gain some inspiration from publicly available resources that seek
    to catalog the numerous potential threats in the field.</st> <st c="20933">Next,
    let’s review some of these resources that you can use to inspire your red</st>
    <st c="21013">team plan.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21023">Resources for building your red team plan</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="21065">When</st> <st c="21071">evaluating RAG application safety, it
    is crucial to identify scenarios to protect against and ask,</st> *<st c="21170">What
    could go wrong?</st>* <st c="21190">These three resources provide a good starting
    place for making your</st> <st c="21259">own list:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="21268">Open Web Application Security Project</st>** <st c="21306">(</st>**<st
    c="21308">OWASP</st>**<st c="21313">)</st> **<st c="21316">Foundation Top 10 for
    LLM applications</st>**<st c="21354">: The</st> <st c="21361">OWASP Top 10 for
    LLM applications is a project by OWASP that aims to identify and raise awareness
    about the most critical security risks associated with LLM applications.</st>
    <st c="21532">It provides a standardized list of the top ten vulnerabilities and
    risks specific to LLM applications, helping developers, security professionals,
    and organizations prioritize their efforts in securing these</st> <st c="21740">systems
    (</st>[<st c="21749">https://owasp.org/www-project-top-10-for-large-language-model-applications/</st>](https://owasp.org/www-project-top-10-for-large-language-model-applications/)<st
    c="21825">).</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="21828">AI Incident Database</st>**<st c="21849">: The</st> <st c="21856">AI
    Incident Database is a publicly accessible collection of real-world incidents
    involving AI systems, including LLMs.</st> <st c="21975">It serves as a valuable
    resource for researchers, developers, and policymakers to learn from past incidents
    and understand the potential risks and consequences associated with AI systems.</st>
    <st c="22163">The database contains a wide range of incidents, such as system
    failures, unintended consequences, biases, privacy breaches, and</st> <st c="22292">more
    (</st>[<st c="22298">https://incidentdatabase.ai/</st>](https://incidentdatabase.ai/)<st
    c="22327">).</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="22330">AI Vulnerability Database</st>** <st c="22356">(</st>**<st
    c="22358">AVID</st>**<st c="22362">): The AVID is a centralized repository that
    collects</st> <st c="22417">and organizes information about vulnerabilities found
    in AI systems, including LLMs.</st> <st c="22502">The AVID aims to provide a comprehensive
    resource for AI researchers, developers, and security professionals to stay informed
    about known vulnerabilities and their potential impact on AI systems.</st> <st
    c="22698">The AVID collects vulnerability information from various sources, such
    as academic research, industry reports, and real-world</st> <st c="22824">incidents
    (</st>[<st c="22835">https://avidml.org/</st>](https://avidml.org/)<st c="22855">).</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="22858">As you develop your</st> <st c="22878">red team strategy, these
    resources will give you many ideas for ways to attack your system.</st> <st c="22971">In
    the next section, we are going to add a fundamental security coding practice to
    our code, and then we will dive into launching a full red team attack on our RAG
    pipeline.</st> <st c="23145">But don’t worry, we are also going to show how to
    use LLM power to defend against attacks</st> <st c="23235">as well!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23243">Code lab 5.1 – Securing your keys</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="23277">This code can be found in the</st> `<st c="23308">CHAPTER5-1_SECURING_YOUR_KEYS.ipynb</st>`
    <st c="23343">file in the</st> `<st c="23356">CHAPTER_05</st>` <st c="23366">directory
    of the</st> <st c="23384">GitHub repository.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23402">In</st> [*<st c="23406">Chapter 2</st>*](B22475_02.xhtml#_idTextAnchor035)<st
    c="23415">, we provided a</st> <st c="23431">coding step right after adding imports
    where we added your OpenAI API key.</st> <st c="23506">In that section, we indicated
    that it was a very simple demonstration of how the API key is ingested into the
    system, but this is not a secure way to use an API key.</st> <st c="23672">Typically,
    as your RAG application expands, you will have multiple API keys as well.</st>
    <st c="23757">But even if you only have the OpenAI API key, this is enough to
    institute further security measures to protect your key.</st> <st c="23878">This
    key can be used to run up expensive bills on your OpenAI account, exposing you
    to potential</st> <st c="23975">financial risk.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23990">We are going to start this code lab with a very common security-driven
    practice of hiding your sensitive API code (and any other</st> *<st c="24120">secret</st>*
    <st c="24126">code) in a separate file that can be hidden from your versioning
    system.</st> <st c="24200">The most typical reason to implement this is when you
    are using a versioning system and you want to set up a file with your</st> *<st
    c="24324">secrets</st>* <st c="24331">separately that you list in the</st> `<st
    c="24364">ignore</st>` <st c="24370">file to prevent them from getting exposed,
    while still being able to use the secrets in the code for proper</st> <st c="24479">code
    execution.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24494">This is the code provided previously for accessing your OpenAI</st>
    <st c="24558">API key:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: <st c="24683">As mentioned, you</st> <st c="24702">are going to need to replace</st>
    `<st c="24731">sk-###################</st>` <st c="24753">with your actual OpenAI
    API key for the rest of your code to work.</st> <st c="24821">But wait, this is
    not a very secure way to do this!</st> <st c="24873">Let’s</st> <st c="24879">fix
    that!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24888">First, let’s create the new file you will use to save your secrets.</st>
    <st c="24957">With the</st> `<st c="24966">dotenv</st>` <st c="24972">Python package,
    you can use</st> `<st c="25001">.env</st>` <st c="25005">out of the box.</st>
    <st c="25022">However, in some environments, you may run into system restrictions
    that prevent you from using a file starting with a dot (</st>`<st c="25146">.</st>`<st
    c="25147">).</st> <st c="25151">In those cases, you can still use</st> `<st c="25185">dotenv</st>`<st
    c="25191">, but you have to create a file, name it, and then point</st> `<st c="25248">dotenv</st>`
    <st c="25254">to it.</st> <st c="25262">For example, if I cannot use</st> `<st
    c="25291">.env</st>`<st c="25295">, I use</st> `<st c="25303">env.txt</st>`<st
    c="25310">, and that is the file where I store the OpenAI API key.</st> <st c="25367">Add
    the</st> `<st c="25375">.env</st>` <st c="25379">file you want to use to your
    environment and add the API key to the</st> `<st c="25448">.env</st>` <st c="25452">file</st>
    <st c="25458">like this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: <st c="25508">This will essentially just be a text file with that one line of
    code in it.</st> <st c="25585">It may not seem like much, but handling it this
    way protects that API key from getting spread across your versioning system, which
    makes it significantly less secure.</st> <st c="25752">As I mentioned in</st>
    [*<st c="25770">Chapter 2</st>*](B22475_02.xhtml#_idTextAnchor035)<st c="25779">,
    you have to fill in your actual API key to replace the</st> `<st c="25836">sk-###################</st>`
    <st c="25858">part of</st> <st c="25867">the code.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25876">If you are using Git for version control, add whatever the name
    of your file is to your</st> `<st c="25965">gitignore</st>` <st c="25974">file
    so that, when you commit it to Git, you do not push the file with all your secrets
    in it!</st> <st c="26070">In fact, this is a good time to generate a new OpenAI
    API key and delete the one you were just using, especially if you think it could
    show up in the history of your code prior to making the changes we are implementing
    in this chapter.</st> <st c="26306">Delete the old key and start fresh with a
    new key in your</st> `<st c="26364">.env</st>` <st c="26368">file, preventing
    any key from ever being exposed in your Git</st> <st c="26430">versioning system.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26448">You can use this file for all keys and similar information you
    want to keep secret.</st> <st c="26533">So, for example, you could have multiple
    keys in your</st> `<st c="26587">.env</st>` <st c="26591">file, such as what you</st>
    <st c="26615">see here:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: <st c="26759">This is an</st> <st c="26771">example that shows multiple keys
    that we want to keep secret and out of the hands of untrusted users.</st> <st
    c="26873">If there is still a security breach, you can cancel the API key in your
    OpenAI API account, as well as the others that you may have there.</st> <st c="27012">But
    in general, by not allowing these keys to be copied into your versioning system,
    you are significantly reducing the likelihood that there will be a</st> <st c="27164">security
    breach.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27180">Next, you will install</st> `<st c="27204">python-dotdev</st>`
    <st c="27217">at the top of your code, like this (the last line is new compared
    to your code from</st> [*<st c="27302">Chapter 2</st>*](B22475_02.xhtml#_idTextAnchor035)<st
    c="27311">):</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: <st c="27341">You always want to restart your kernel after installing new packages,
    as you do in the preceding code.</st> <st c="27445">You can review how to do this
    in</st> [*<st c="27478">Chapter 2</st>*](B22475_02.xhtml#_idTextAnchor035)<st
    c="27487">. But in this case, this always refreshes your code to be able to pull
    in and recognize the</st> `<st c="27579">.env</st>` <st c="27583">file.</st> <st
    c="27590">If you make any changes to the</st> `<st c="27621">.env</st>` <st c="27625">file,
    be sure to restart your kernel so that those changes are pulled into your environment.</st>
    <st c="27719">Without restarting the kernel, your system will likely not be able
    to find the file and will return an empty string for</st> `<st c="27839">OPEN_API_KEY</st>`<st
    c="27851">, which will cause your LLM calls</st> <st c="27885">to fail.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27893">Next, you will need to import that same library for use in</st>
    <st c="27953">your code:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: <st c="28007">At this point, you have installed and imported the Python package
    that will allow you to hide information in your code in a more secure way.</st>
    <st c="28149">Next, we want to use the</st> `<st c="28174">load_dotenv</st>` <st
    c="28185">function you just imported to retrieve the secret and be able to use
    it in the code.</st> <st c="28271">We mentioned earlier, though, that in some
    environments, you may not be able to use a file starting with a dot (</st>`<st
    c="28383">.</st>`<st c="28384">).</st> <st c="28388">If you found yourself in
    this situation, then you would have set up the</st> `<st c="28460">env.txt</st>`
    <st c="28467">file, rather than the</st> `<st c="28490">.env</st>` <st c="28494">file.</st>
    <st c="28501">Based on your situation, choose the appropriate approach from</st>
    <st c="28563">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28577">If you are using a</st> `<st c="28597">.env</st>` <st c="28601">file,</st>
    <st c="28608">use this:</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="28648">If you are using an</st> `<st c="28669">env.txt</st>` <st c="28676">file,</st>
    <st c="28683">use this:</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="28731">The</st> `<st c="28736">.env</st>` <st c="28740">approach</st>
    <st c="28750">is the most common approach, so I wanted to make sure you were familiar
    with it.</st> <st c="28831">But in theory, you could always use the</st> `<st
    c="28871">env.txt</st>` <st c="28878">approach, making it more universal.</st>
    <st c="28915">For this reason, I recommend using the</st> `<st c="28954">env.txt</st>`
    <st c="28961">approach so that your code works in more environments.</st> <st
    c="29017">Just make sure you have restarted the kernel after adding the</st> `<st
    c="29079">.env</st>` <st c="29083">or</st> `<st c="29087">env.txt</st>` <st c="29094">file
    so that your code can find the file and use it.</st> <st c="29148">You only need
    to select one of these options in your code.</st> <st c="29207">We will use the</st>
    `<st c="29223">env.txt</st>` <st c="29230">approach from now on in this book,
    as we like to practice good security measures</st> <st c="29312">whenever possible!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29330">But wait.</st> <st c="29341">What is that?</st> <st c="29355">Over
    the horizon, a new security threat is approaching, it’s the dreaded</st> <st c="29428">red
    team!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29437">Code lab 5.2 – Red team attack!</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="29469">This code can be found in the</st> `<st c="29500">CHAPTER5-2_SECURING_YOUR_KEYS.ipynb</st>`
    <st c="29535">file in the</st> `<st c="29548">CHAPTER_05</st>` <st c="29558">directory
    of the</st> <st c="29576">GitHub repository.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29594">Through</st> <st c="29603">our hands-on code lab, we will engage
    in an exciting red team versus blue team exercise, showcasing how LLMs can be
    both a vulnerability and a defense mechanism in the battle for RAG</st> <st c="29786">application
    security.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29807">We will first take the role of red team and orchestrate a prompt
    probe on our RAG pipeline code.</st> <st c="29905">As mentioned earlier in this
    chapter, prompt probing is the initial step to gain insight into the internal
    prompts a RAG system is using to discover the system prompt(s) of a RAG application.</st>
    <st c="30097">The system prompt is the initial set of instructions or context
    provided to the LLM to guide its behavior and responses.</st> <st c="30218">By
    uncovering the system prompt, attackers can gain valuable insights into the inner
    workings of the application and this sets the foundation for designing more targeted
    and efficient attacks using the other techniques described previously.</st> <st
    c="30459">For example, prompt probing can reveal the information you need to launch
    a more effective gray box prompt attack.</st> <st c="30574">As we mentioned, a
    gray box prompt attack can also be employed to bypass safeguards by injecting
    incorrect data within the prompt, but you need knowledge of the system prompt
    in order to launch this kind of attack.</st> <st c="30789">Prompt probing is an
    effective way to get the system prompt information you need to conduct the gray
    box</st> <st c="30894">prompt attack.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30908">Are smarter LLMs more difficult to hack?</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30949">We are using Gpt-4o, one of the top LLMs in the marketplace.</st>
    <st c="31011">It is newer, smarter, and more sophisticated than just about any
    other option.</st> <st c="31090">In theory, that makes it more difficult for us
    to pull off our red team attack, right?</st> <st c="31177">Well, actually, we
    are going to use the fact that GPT-4o is smarter against it!</st> <st c="31257">This
    attack was unsuccessful with GPT-3.5, as it was not able to follow the thorough
    instructions we used to implement the attack.</st> <st c="31388">But GPT-4 is
    smart enough to follow these instructions, allowing us to take advantage of its
    increased intelligence and turn it on itself.</st> <st c="31527">Crazy, right?</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31540">We will pick</st> <st c="31553">up from where we left off in</st>
    *<st c="31583">Code lab 5.1</st>*<st c="31595">. Let’s start at the end of our
    code, where we expand it to show just</st> <st c="31665">the answer:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: <st c="31772">The output of the final line here, if you run through all of the
    other code, should be the same response we’ve seen in previous chapters, which
    is something similar</st> <st c="31938">to this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: <st c="32343">As we’ve discussed in previous chapters, the prompt we pass to
    the LLM includes the question the user has passed to the RAG pipeline but also
    includes additional instructions</st> <st c="32518">for the LLM to follow.</st>
    <st c="32542">In this case, you can find this out by printing out the template
    with the</st> <st c="32616">following code:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: <st c="32666">The output of this code is</st> <st c="32694">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: <st c="32941">As the red team, though, we do not know what that system prompt
    looks like.</st> <st c="33018">Our goal is to figure that out, as well as what
    kind of information is getting pulled into</st> `<st c="33109">{context}</st>`<st
    c="33118">, which, in other applications, could be sensitive customer data, possibly
    even from customers other than the</st> <st c="33228">current user!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33241">In the military tradition of red teaming, we are going to set
    up our</st> <st c="33311">mission parameters.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33330">Mission parameters</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33349">You are playing the red team and the goal of your attack is to
    design a prompt probe to identify the system prompt for this RAG application,
    allowing us to infiltrate our opponent’s system through the design of more sophisticated
    prompt</st> <st c="33587">injection attacks.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33605">This is our attack prompt, which you can add at the bottom of</st>
    <st c="33668">the notebook:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: <st c="34665">Run this</st> <st c="34674">cell so that</st> `<st c="34688">prompt_probe</st>`
    <st c="34700">is added as a variable.</st> <st c="34725">In this prompt, we are
    using prompt injection (jailbreaking) to inject new instructions to overwrite
    the initial prompt and change the model’s behavior.</st> <st c="34878">In this
    case, we tell the LLM to now take the role of helping to write</st> <st c="34949">the
    instructions.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34966">Another technique used here is asking the LLM to take the previous
    instructions and make a small change to them.</st> <st c="35080">This is a common
    technique that takes advantage of the LLM’s inclination to perform a task, which
    gives it more motivation to override other instructions.</st> <st c="35235">While
    results may vary, when I tried this prompt attack without the</st> `<st c="35303">REPLACE
    COMMAS WITH EXCLAMATION POINTS</st>` <st c="35341">portion, this prompt injection
    was not working.</st> <st c="35390">Try it yourself!</st> <st c="35407">But that
    shows you how strong this LLM inclination is to do this task.</st> <st c="35478">There
    is often a very fine line between what works and what doesn’t, so you have to
    try a lot of different approaches to figure out what will work</st> <st c="35625">for
    you.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35633">We also used</st> <st c="35646">common techniques for prompting
    in general, such as using several hashtags to denote important areas, several
    dashes to note other important areas, and all caps to emphasize your instructions
    over</st> <st c="35844">non-capitalized text.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35865">We need to send this prompt into the pipeline to administer the</st>
    <st c="35930">prompt attack:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: <st c="36032">The output of this code should look similar</st> <st c="36077">to
    this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: <st c="36599">We have successfully prompted the LLM to provide a significant
    part of the prompt instructions that are hidden within the code.</st> <st c="36728">This
    not only reveals the instructions at the top of the system prompt but also all
    of the data the system internally retrieves to respond to the user’s question.</st>
    <st c="36891">This is a major breach!</st> <st c="36915">A huge win for the</st>
    <st c="36934">red team!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36943">We now</st> <st c="36950">have a much better understanding of
    the LLM application and how to exploit the prompts it is fed in a way that may
    allow us to compromise the entire RAG pipeline and the data it has access to.</st>
    <st c="37144">If these prompts were valuable intellectual property, we could now
    steal them.</st> <st c="37223">If they access private or valuable data, we could
    exploit our new knowledge of the prompts to try to access that data.</st> <st
    c="37342">This sets the foundation for a more advanced attack on</st> <st c="37397">the
    system.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37408">Let’s next play the role of blue team, coming up with a solution
    for our code to prevent</st> <st c="37498">this attack.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37510">Code lab 5.3 – Blue team defend!</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="37543">This code can be found in the</st> `<st c="37574">CHAPTER5-3_SECURING_YOUR_KEYS.ipynb</st>`
    <st c="37609">file in the</st> `<st c="37622">CHAPTER5</st>` <st c="37631">directory
    of the</st> <st c="37648">GitHub repository.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37666">There are a</st> <st c="37679">number of solutions we can implement
    to prevent this attack from revealing our prompt.</st> <st c="37766">We are going
    to address this with a second LLM that acts as the guardian of the response.</st>
    <st c="37856">Using a second LLM to check the original response or to format and
    understand the input is a common solution for many RAG-related applications.</st>
    <st c="38000">We will show how to use it to better secure</st> <st c="38044">the
    code.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38053">It is important to note up front, though, that this is just one
    example of a solution.</st> <st c="38141">The great security battle against potential
    adversaries is always shifting and changing.</st> <st c="38230">You must continuously
    stay diligent and come up with new and better solutions to prevent</st> <st c="38319">security
    breaches.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38337">Add this line to</st> <st c="38355">your imports:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: <st c="38418">This imports the</st> `<st c="38436">PromptTemplate</st>` <st
    c="38450">class from the</st> `<st c="38466">langchain_core.prompts</st>` <st
    c="38488">module, which allows us to define and create our own custom</st> <st
    c="38549">prompt templates.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38566">The new prompt we are going to create will be a relevance prompt
    designated for our hidden guardian LLM that will be looking out for attacks just
    like the one we just encountered.</st> <st c="38747">Add this prompt in a cell
    after the original prompt cell, keeping</st> <st c="38813">both prompts:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: <st c="39236">For simplicity</st> <st c="39252">purposes, we are going to use
    the same LLM instance that we already set up, but we will call the LLM a separate
    time to act as</st> <st c="39379">the guardian.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39392">Next, we are going to apply a significant update to our RAG chain,
    including the addition of</st> <st c="39486">two functions:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: <st c="39611">The</st> `<st c="39616">extract_score</st>` <st c="39629">function
    takes</st> `<st c="39645">llm_output</st>` <st c="39655">as input.</st> <st c="39666">It
    attempts to convert</st> `<st c="39689">llm_output</st>` <st c="39699">to a float
    by first stripping any leading/trailing whitespace using</st> `<st c="39768">strip</st>`
    <st c="39773">and then converting it to a float using</st> `<st c="39814">float</st>`<st
    c="39819">. If the conversion is successful, it returns the score as a float.</st>
    <st c="39887">If the conversion raises a</st> `<st c="39914">ValueError</st>`
    <st c="39924">message (indicating that</st> `<st c="39950">llm_output</st>` <st
    c="39960">cannot be converted to a float), it catches the exception and returns</st>
    `<st c="40031">0</st>` <st c="40032">as the</st> <st c="40040">default score.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40054">Next, let’s set up a function to apply the logic for what happens
    when a query is</st> <st c="40137">not relevant:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: <st c="40303">The</st> `<st c="40308">conditional_answer</st>` <st c="40326">function
    takes a dictionary,</st> `<st c="40356">x</st>`<st c="40357">, as input and extracts
    the</st> `<st c="40385">'relevance_score'</st>` <st c="40402">variable from the
    dictionary,</st> `<st c="40433">x</st>`<st c="40434">, passing it to the</st>
    `<st c="40454">extract_score</st>` <st c="40467">function to get the</st> `<st
    c="40488">relevance_score</st>` <st c="40503">value.</st> <st c="40511">If the</st>
    `<st c="40518">relevance_score</st>` <st c="40533">is less than</st> `<st c="40547">4</st>`<st
    c="40548">, it returns</st> <st c="40561">the string</st> `<st c="40572">I don't
    know</st>`<st c="40584">. Otherwise, it returns the value associated with the
    key,</st> `<st c="40643">'answer'</st>`<st c="40651">, from the</st> <st c="40662">dictionary,</st>
    `<st c="40674">x</st>`<st c="40675">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40676">Last, let’s set up an expanded</st> `<st c="40708">rag_chain_from_docs</st>`
    <st c="40727">chain with the new security</st> <st c="40756">features embedded:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: <st c="41211">The</st> `<st c="41216">rag_chain_from_docs</st>` <st c="41235">chain
    was present in the previous code, but it saw some updates to account for the new
    LLM’s job and the relevant functions listed previously.</st> <st c="41379">The</st>
    <st c="41383">first step is the same as previous iterations, where we assign a
    function to the context key that formats the</st> `<st c="41493">context data</st>`
    <st c="41505">from the input dictionary using the</st> `<st c="41542">format_docs</st>`
    <st c="41553">function.</st> <st c="41564">The next step is a</st> `<st c="41583">RunnableParallel</st>`
    <st c="41600">instance that runs two parallel operations, saving</st> <st c="41651">processing
    time:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41667">The first operation generates a</st> `<st c="41700">relevance_score</st>`
    <st c="41715">by passing the</st> `<st c="41731">question</st>` <st c="41739">and</st>
    `<st c="41744">context</st>` <st c="41751">variables through the</st> `<st c="41774">relevance_prompt_template</st>`
    <st c="41799">template, then through an LLM, and finally, parsing the output using</st>
    `<st c="41869">StrOutputParser</st>` <st c="41884">function</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="41893">The second operation generates an answer by passing the input
    through a prompt, then through an LLM, and parsing the output using</st> `<st
    c="42024">StrOutputParser</st>` <st c="42039">function</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="42048">The final step is to assign the</st> `<st c="42081">conditional_answer</st>`
    <st c="42099">function to the</st> `<st c="42116">final_answer</st>` <st c="42128">key,
    which determines the final answer based on</st> <st c="42177">the</st> `<st c="42181">relevance_score</st>`<st
    c="42196">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42197">In general, what we added to this code was a second LLM that looks
    at the question submitted by the user and the context that is pulled in by the
    retriever and tells you on a scale of</st> `<st c="42382">1</st>` <st c="42383">to</st>
    `<st c="42387">5</st>` <st c="42388">whether they are relevant to each other.</st>
    `<st c="42430">1</st>` <st c="42431">would indicate not at all relevant and</st>
    `<st c="42471">5</st>` <st c="42472">is highly relevant.</st> <st c="42493">This
    follows the instructions in the relevance prompt we added earlier.</st> <st c="42565">If
    the LLM scores the relevance less than</st> `<st c="42607">4</st>`<st c="42608">,
    then the response is automatically converted to</st> `<st c="42658">I don't know.</st>`
    <st c="42671">rather than sharing the secret system prompts of the</st> <st c="42725">RAG
    pipeline.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42738">We are</st> <st c="42745">going to update the chain invoking code
    as well so that we can print out the relevant information.</st> <st c="42845">For
    the original question invocation, update it</st> <st c="42893">to this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: <st c="43198">The output will be similar to before with the proper response
    we’ve seen in the past, but near the top of the output, we see a new element,</st>
    `<st c="43339">Relevance Score</st>`<st c="43354">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: <st c="43459">Our new guardian LLM deemed this question the top score of</st>
    `<st c="43519">5</st>` <st c="43520">for relevance to the content in the RAG pipeline.</st>
    <st c="43571">Next, let’s update the code for the prompt probe to reflect changes
    in the</st> <st c="43646">code and see what the final answer</st> <st c="43681">will
    be:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: <st c="44001">Your resulting output from this red team prompt probe should look</st>
    <st c="44068">like this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: <st c="44112">Our blue team's effort is successful in thwarting the prompt probe
    attack!</st> <st c="44188">Justice has been served and our code is now significantly
    more secure than before!</st> <st c="44271">Does this mean we are done with security?</st>
    <st c="44313">Of course not, hackers are always coming up with new ways to penetrate
    our organizations.</st> <st c="44403">We need to remain diligent.</st> <st c="44431">The
    next step in a real-world application would be to go back to being the red team
    and try to come up with other ways to get around the new fixes.</st> <st c="44579">But
    at least it will be more difficult.</st> <st c="44619">Try some other prompt approaches
    to see whether you can still access the system prompt.</st> <st c="44707">It is
    definitely more</st> <st c="44729">difficult now!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44743">You now have a more secure code base, and with the additions made
    in</st> [*<st c="44813">Chapter 3</st>*](B22475_03.xhtml#_idTextAnchor056)<st
    c="44822">, it is more transparent</st> <st c="44847">as well!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44855">Summary</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="44863">In this chapter, we explored the critical aspect of security in
    RAG applications.</st> <st c="44946">We began by discussing how RAG can be leveraged
    as a security solution, enabling organizations to limit data access, ensure more
    reliable responses, and provide greater transparency of sources.</st> <st c="45140">However,
    we also acknowledged the challenges posed by the black box nature of LLMs and
    the importance of protecting user data</st> <st c="45266">and privacy.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="45278">We introduced the concept of red teaming a security testing methodology
    that involves simulating adversarial attacks to proactively identify and mitigate
    vulnerabilities in RAG applications.</st> <st c="45470">We explored common areas
    targeted by red teams, such as bias and stereotypes, sensitive information disclosure,
    service disruption,</st> <st c="45602">and hallucinations.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="45621">Through a hands-on code lab, we demonstrated how to implement
    security best practices in RAG pipelines, including techniques for securely storing
    API keys and defending against prompt injection attacks.</st> <st c="45825">We
    engaged in an exciting red team versus blue team exercise, showcasing how LLMs
    can be both a vulnerability and a defense mechanism in the battle for RAG</st>
    <st c="45981">application security.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46002">Throughout the chapter, we emphasized the importance of ongoing
    vigilance and adaptation in the face of ever-evolving security threats.</st> <st
    c="46139">By understanding the security landscape surrounding RAG applications
    and implementing practical strategies and techniques, you can build secure, trustworthy,
    and robust systems that harness the power of generative AI while prioritizing
    the safety and privacy of users</st> <st c="46407">and businesses.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46422">Generally, we do not claim that this list of security issues or
    solutions is exhaustive.</st> <st c="46512">Our primary goal here was to alert
    you to some of the key security threats you might encounter, but most importantly,
    to always be on guard and diligent at defending your system.</st> <st c="46691">Continually
    think through how your system might be vulnerable, using techniques such as red
    teaming, and use this approach to build much stronger defenses against any</st>
    <st c="46858">potential threats.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46876">Looking ahead, in the next chapter, we will dive into the practical
    aspects of interfacing with RAG applications using Gradio.</st> <st c="47004">This
    next chapter will provide a hands-on guide to building interactive applications
    with RAG, leveraging Gradio as a user-friendly interface.</st> <st c="47147">You
    will learn how to quickly prototype and deploy RAG-powered applications, enabling
    end users to interact with AI models in</st> <st c="47273">real time.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="0">Part 2 – Components of RAG</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="27">In this part, you will learn about key components of a RAG system
    and how to implement them using LangChain.</st> <st c="137">You’ll explore interfacing
    with RAG using Gradio to create interactive user interfaces, the crucial role
    of vectors and vector stores in enhancing RAG performance, and techniques for
    evaluating RAG quantitatively and with visualizations.</st> <st c="375">Additionally,
    you’ll dive into using LangChain components such as document loaders, text splitters,
    and output parsers to further optimize your</st> <st c="519">RAG pipeline.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="532">This part contains the</st> <st c="556">following chapters:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[*<st c="575">Chapter 6</st>*](B22475_06.xhtml#_idTextAnchor114)<st c="585">,</st>
    *<st c="587">Interfacing with RAG and Gradio</st>*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*<st c="618">Chapter 7</st>*](B22475_07.xhtml#_idTextAnchor122)<st c="628">,</st>
    *<st c="630">The Key Role Vectors and Vector Stores Play in RAG</st>*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*<st c="680">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st c="690">,</st>
    *<st c="692">Similarity Searching with Vectors</st>*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*<st c="725">Chapter 9</st>*](B22475_09.xhtml#_idTextAnchor184)<st c="735">,</st>
    *<st c="737">Evaluating RAG Quantitatively and with Visualizations</st>*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*<st c="790">Chapter 10</st>*](B22475_10.xhtml#_idTextAnchor218)<st c="801">,</st>
    *<st c="803">Key RAG Components in LangChain</st>*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*<st c="834">Chapter 11</st>*](B22475_11.xhtml#_idTextAnchor229)<st c="845">,</st>
    *<st c="847">Using LangChain to Get More from RAG</st>*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
