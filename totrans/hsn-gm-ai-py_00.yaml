- en: Preface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book is your one-stop shop for learning how various **reinforcement learning**
    (**RL**) techniques and algorithms play an important role in game development
    using Python.
  prefs: []
  type: TYPE_NORMAL
- en: The book will start with the basics to provide you with the necessary foundation
    to understand how RL is playing a major role in game development. Each chapter
    will help you implement various RL techniques, such as Markov decision processes,
    Q-learning, the actor-critic method, **state-action-reward-state-action** (**SARSA**),
    and the deterministic policy gradients algorithm, to build logical self-learning
    agents. You will use these techniques to enhance your game development skills
    and add various features to improve your overall productivity. Later in the book,
    you will learn how deep RL techniques can be used to devise strategies that enable
    agents to learn from their own actions so that you can build fun and engaging
    games.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the book, you will be able to use RL techniques to build various
    projects and contribute to open source applications.
  prefs: []
  type: TYPE_NORMAL
- en: Who this book is for
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book is for game developers who are looking to add to their knowledge by
    implementing RL techniques to build games from scratch. This book will also appeal
    to machine learning and deep learning practitioners, and RL researchers who want
    to understand how self-learning agents can be used in the game domain. Prior knowledge
    of game development and a working knowledge of Python programming are expected.
  prefs: []
  type: TYPE_NORMAL
- en: What this book covers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Chapter 1](5553d896-c079-4404-a41b-c25293c745bb.xhtml), *Understanding Rewards-Based
    Learning*, explores the basics of learning, what it is to learn, and how RL differs
    from other, more classic learning methods. From there, we explore how the Markov
    decision process works in code and how it relates to learning. This leads us to
    the classic multi-armed and contextual bandit problems. Finally, we will learn
    about Q-learning and quality-based model learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 2](8237fd36-1edf-4da0-b271-9a50c5b8deb3.xhtml), *Dynamic Programming
    and the Bellman Equation*, digs deeper into dynamic programming and explores how
    the Bellman equation can be intertwined into RL. Here, you will learn how the
    Bellman equation is used to update a policy. We then go further into detail about
    policy iteration or value iteration methods using our understanding of Q-learning,
    by training an agent on a new grid-style environment.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 3](5f6ea967-ae50-426e-ad18-c8dda835a950.xhtml), *Monte Carlo Methods*, explores
    model-based methods and how they can be used to train agents on more classic board
    games.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 4](bb05e528-e21b-4753-9e4c-372b8ed11e96.xhtml), *Temporal Difference
    Learning*, explores the heart of RL and how it solves the temporal credit assignment
    problem often discussed in academia. We apply **temporal difference learning**
    (**TDL**) to Q-learning and use it to solve a grid world environment (such as
    FrozenLake).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 5](3e0c16c5-2145-498c-8ba1-b917745e0ef0.xhtml), *Exploring SARSA*, goes
    deeper into the fundamentals of on-policy methods such as SARSA. We will explore
    policy-based learning through understanding the partially observable Markov decision
    process. Then, we''ll look at how we can implement SARSA with Q-learning. This
    will set the stage for the more advanced policy methods that we will explore in
    later chapters, called PPO and TRPO.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 6](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml), *Going Deep with DQN*,
    takes the Q-learning model and integrates it with deep learning to create advanced
    agents known as **deep Q-learning networks** (**DQNs**). From this, we explain
    how basic deep learning models work for regression or, in this case, to solve
    the Q equation. We will use DQNs in the CartPole environment.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 7](42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml), *Going Deeper with
    DDQNs*, looks at how extensions to **deep learning** (**DL**) called **convolutional
    neural networks** (**CNNs**) can be used to observe a visual state. We will then
    use that knowledge to play Atari games and look at further enhancements.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 8](42626cbd-87b8-428c-8f2a-ecc06f5e387c.xhtml), *Policy Gradient Methods*, delves
    into more advanced policy methods and how they integrate into deep RL agents.
    This is an advanced chapter as it covers higher-level calculus and probability
    concepts. You will get to experience the MuJoCo animation RL environment in this
    chapter as a reward for your hard work.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 9](2f6812c0-fd1f-4eda-9df2-6c67c8077aec.xhtml), *Optimizing for Continuous
    Control*, looks at improving the policy methods we looked at previously for continuously
    controlling advanced environments. We start off by setting up and installing the
    MuJoCo environment. After that, we look at a novel improvement called recurrent
    networks for capturing context and see how recurrent networks are applied on top
    of PPO. Then we get back into the actor-critic method and this time look at asynchronous
    actor-critic in a couple of different configurations, before finally progressing
    to actor-critic with experience replay.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 10](1fbfb255-7fd9-44ea-8d02-f385e95d88d2.xhtml), *All Together Rainbow
    DQN*, tells us all about Rainbow. Google DeepMind recently explored the combination
    of a number of RL enhancements all together in an algorithm called Rainbow. Rainbow
    is another advanced toolkit that you can explore and either borrow from or use
    to work with more advanced RL environments.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 11](ab9a7f4f-60d8-4643-8627-199cf95bcf55.xhtml), *Exploiting ML-Agents*, looks
    at how we can either use elements from the ML-Agents toolkit in our own agents
    or use the toolkit to get a fully developed agent.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 12](6d061d35-176a-421a-9b62-aed35f48a6b7.xhtml), *DRL Frameworks*,
    opens up the possibilities of playing with solo agents in various environments.
    We will explore various multi-agent environments as well.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 13](e54c6adf-d238-4f1e-8e32-7ba3c5da0f46.xhtml), *3D Worlds*, trains
    us to use RL agents effectively to tackle a variety of 3D environmental challenges.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 14](a171ddfa-e639-4b4e-9652-4279b5ac872a.xhtml), *From DRL to AGI*, looks
    beyond DRL and into the realm of AGI, or at least where we hope we are going with
    AGI. We will also looks at various DRL algorithms that can be applied in the real
    world.'
  prefs: []
  type: TYPE_NORMAL
- en: To get the most out of this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A working knowledge of Python and game development is essential. A good PC with
    a GPU would be beneficial.
  prefs: []
  type: TYPE_NORMAL
- en: Download the example code files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can download the example code files for this book from your account at [www.packt.com](http://www.packt.com).
    If you purchased this book elsewhere, you can visit [www.packtpub.com/support](https://www.packtpub.com/support)
    and register to have the files emailed directly to you.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the code files by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in or register at [www.packt.com](http://www.packt.com).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the Support tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on Code Downloads.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the name of the book in the Search box and follow the onscreen instructions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the file is downloaded, please make sure that you unzip or extract the
    folder using the latest version of:'
  prefs: []
  type: TYPE_NORMAL
- en: WinRAR/7-Zip for Windows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zipeg/iZip/UnRarX for Mac
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 7-Zip/PeaZip for Linux
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code bundle for the book is also hosted on GitHub at [https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-for-Games](https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-for-Games). In
    case there's an update to the code, it will be updated on the existing GitHub
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: We also have other code bundles from our rich catalog of books and videos available
    at **[https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)**.
    Check them out!
  prefs: []
  type: TYPE_NORMAL
- en: Download the color images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We also provide a PDF file that has color images of the screenshots/diagrams
    used in this book. You can download it here: [http://www.packtpub.com/sites/default/files/downloads/9781839214936_ColorImages.pdf](http://www.packtpub.com/sites/default/files/downloads/9781839214936_ColorImages.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Conventions used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of text conventions used throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: '`CodeInText`: Indicates code words in text, database table names, folder names,
    filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles.
    Here is an example: "The three functions `make_atari`, `wrap_deepmind`, and `wrap_pytorch` are
    all located in the new `wrappers.py` file we imported earlier."'
  prefs: []
  type: TYPE_NORMAL
- en: 'A block of code is set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When we wish to draw your attention to a particular part of a code block, the
    relevant lines or items are set in bold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Any command-line input or output is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Bold**: Indicates a new term, an important word, or words that you see onscreen.
    For example, words in menus or dialog boxes appear in the text like this. Here
    is an example: "Building on that, we''ll look at a variant of the DQN called the **DDQN**,
    or **double (dueling) DQN**."'
  prefs: []
  type: TYPE_NORMAL
- en: Warnings or important notes appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Tips and tricks appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Get in touch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feedback from our readers is always welcome.
  prefs: []
  type: TYPE_NORMAL
- en: '**General feedback**: If you have questions about any aspect of this book,
    mention the book title in the subject of your message and email us at `customercare@packtpub.com`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you would report this to us. Please visit [www.packtpub.com/support/errata](https://www.packtpub.com/support/errata),
    selecting your book, clicking on the Errata Submission Form link, and entering
    the details.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Piracy**: If you come across any illegal copies of our works in any form
    on the Internet, we would be grateful if you would provide us with the location
    address or website name. Please contact us at `copyright@packt.com` with a link
    to the material.'
  prefs: []
  type: TYPE_NORMAL
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit [authors.packtpub.com](http://authors.packtpub.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: Reviews
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Please leave a review. Once you have read and used this book, why not leave
    a review on the site that you purchased it from? Potential readers can then see
    and use your unbiased opinion to make purchase decisions, we at Packt can understand
    what you think about our products, and our authors can see your feedback on their
    book. Thank you!
  prefs: []
  type: TYPE_NORMAL
- en: For more information about Packt, please visit [packt.com](http://www.packt.com/).
  prefs: []
  type: TYPE_NORMAL
