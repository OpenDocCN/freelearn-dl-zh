- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Harnessing the Power of RAG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, we know the FMs are trained using large datasets. However, the data
    used to train FMs might not be recent, and this can cause the models to hallucinate.
    In this chapter, we will harness the power of RAG by augmenting the model with
    external data sources to overcome the challenge of hallucination.
  prefs: []
  type: TYPE_NORMAL
- en: We will explore the importance of RAG in generative AI scenarios, how RAG works,
    and its components. We will then delve into the integration of RAG with Amazon
    Bedrock, including a fully managed RAG experience by Amazon Bedrock called Knowledge
    Bases. The chapter will then take a hands-on approach to the implementation of
    Knowledge Bases and using APIs.
  prefs: []
  type: TYPE_NORMAL
- en: We will explore some real-world scenarios of RAG and discuss a few solution
    architectures for implementing RAG. You will also be introduced to implementing
    a RAG framework using Amazon Bedrock, LangChain orchestration, and other generative
    AI systems. We will end by examining current limitations and future research directions
    with Amazon Bedrock in the context of RAG.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to understand the importance of
    RAG and will be able to implement it with Amazon Bedrock. Learning these methods
    will empower you to apply the concept of RAG in your own enterprise use cases
    and build production-level applications, such as conversational interfaces, question
    answering systems, or module summarization workflows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the key topics that will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Decoding RAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing RAG with Amazon Bedrock
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing RAG with other methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced RAG techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limitations and future directions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter requires you to have access to an AWS account. If you don’t have
    one already, you can go to [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    and create an AWS account.
  prefs: []
  type: TYPE_NORMAL
- en: 'Secondly, you will need to install and configure AWS CLI ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/))
    after you create an account, which will be needed to access Amazon Bedrock FMs
    from your local machine. Since the majority chunk of code cells we will be executing
    is based in Python, setting up an AWS Python SDK (Boto3) ([https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html](https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html))
    would be beneficial at this point. You can carry out the Python setup in these
    ways: install it on your local machine, or use AWS Cloud9, or AWS Lambda, or leverage
    Amazon SageMaker.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: There will be a charge associated with the invocation and customization of FMs
    of Amazon Bedrock. Please refer to [https://aws.amazon.com/bedrock/pricing/](https://aws.amazon.com/bedrock/pricing/)
    to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: Decoding RAG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RAG is an approach in NLP that combines large-scale retrieval with neural generative
    models. The key idea is to retrieve relevant knowledge from large corpora and
    incorporate that knowledge into the text-generation process. This allows generative
    models such as Amazon Titan Text, Anthropic Claude, and **Generative Pre-trained
    Transformer 3** (**GPT-3**) to produce more factual, specific, and coherent text
    by grounding generations in external knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: RAG has emerged as a promising technique to make neural generative models more
    knowledgeable and controllable. In this section, we will provide an overview of
    RAG, explain how it works, and discuss key applications.
  prefs: []
  type: TYPE_NORMAL
- en: What is RAG?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditional generative models, such as BART, T5 or GPT-4 are trained on vast
    amounts of text data in a self-supervised fashion. While this allows them to generate
    fluent and human-like text, a major limitation is that they lack world knowledge
    beyond what is contained in their training data. This can lead to factual inconsistencies,
    repetitions, and hallucinations in the generated text.
  prefs: []
  type: TYPE_NORMAL
- en: RAG aims to ground generations in knowledge by retrieving relevant context from
    large external corpora. For example, if the model is generating text about Paris,
    it could retrieve *Wikipedia* passages about Paris to inform the generation. This
    retrieved context is encoded and integrated into the model to guide the text generation.
  prefs: []
  type: TYPE_NORMAL
- en: Augmenting generative models with retrieved knowledge has been shown to produce
    more factual, specific, and coherent text across a variety of domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key components of RAG systems are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A GenAI model – specifically, an FM or LLM – that can generate fluent text (or
    multi-modal) outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A corpus of data to retrieve relevant information from (for example, *Wikipedia*,
    web pages, documents).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retriever module, which encodes the input query and retrieves relevant passages
    from the knowledge corpus based on relevance to the query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Re-ranker to select the optimal contextual information by re-scoring and ranking
    the retrieved passages based on relevance to the query. (This step is optional
    in building basic RAG systems but becomes crucial when building enterprise-scale
    systems with advanced RAG techniques).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fusion module to integrate retrieval into the language model. This can involve
    techniques such as concatenation or allowing the language model to condition on
    relevant external knowledge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other components may also include query reformulation, hybrid search techniques,
    and multi-stage retrieval, which will be covered later in this chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to gain a better understanding of RAG approaches, let us walk through
    a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '`What are` `the key events in the life of` `Marie Curie?`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`What are the key events in the life of Marie Curie?` into a dense vector representation.
    It then searches through the knowledge corpus (for example, *Wikipedia*, web pages)
    to find relevant passages. For example, it may retrieve the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Marie Curie was a Polish physicist and chemist who conducted pioneering research`
    `on radioactivity...`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`In 1903, Curie became the first woman to win a Nobel Prize for her study of`
    `spontaneous radiation...`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Curie won a second Nobel Prize in 1911, this time in chemistry, for her discovery
    of the elements radium` `and polonium...`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Re-ranker**: The re-ranker scores and re-ranks the retrieved passages based
    on their relevance to the original query using cross-attention. It may determine
    that passages *II* and *III* are more relevant than *I*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fusion module**: The top re-ranked passages (for example, *II* and *III*)
    are then integrated into the generative language model, either by concatenating
    them, summarizing them, or allowing the model to attend over them; that is, focus
    on different parts of the retrieved passages as needed while generating the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the goal of the fusion step is to provide the generative language
    model with the most pertinent external knowledge in a manner that allows effective
    conditioning of the generated output on that knowledge, leading to more accurate,
    informative, and grounded responses.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`The key events in the life of Marie` `Curie include:`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`In 1903, she became the first woman to win a Nobel Prize for her study of
    spontaneous` `radiation (radioactivity).`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`In 1911, she won a second Nobel Prize in chemistry for her discovery of the
    elements radium` `and polonium.`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: By retrieving relevant knowledge from an external corpus and integrating it
    into the language model, the RAG system can generate a more informative and accurate
    response, overcoming the limitations of relying solely on the model’s training
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Dense vector representations, also known as dense embeddings or dense vectors,
    are a way of encoding meaning and semantic relationships in a numerical format
    that can be effectively processed by machines. This allows techniques such as
    cosine similarity to identify semantically related words/texts even without exact
    keyword matches. Dense vectors power many modern NLP applications, such as semantic
    search, text generation, translation, and so on, by providing effective semantic
    representations as inputs to **deep** **learning** models.
  prefs: []
  type: TYPE_NORMAL
- en: We will further dive deep into these components in the *Components of RAG* section.
    Since you now have a brief understanding of RAG, it’s time to realize the importance
    of RAG in the context of the GenAI universe.
  prefs: []
  type: TYPE_NORMAL
- en: Importance of RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we dive into how RAG works and its components, it’s important to understand
    why RAG is needed. As LLMs become more capable of generating fluent and coherent
    text, it also becomes more important to ground them in factual knowledge and guard
    against potential hallucinations. If you ask an LLM questions pertaining to recent
    events, you might notice the model to be hallucinating. With RAG, you can augment
    the latest knowledge as context to the model to improve content quality by reducing
    the chance of factual errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another major advantage of RAG is overcoming the limited context length (input
    token limit) of the model. When providing pieces of text as a context that fits
    within the token limit of the model, you may not need to use RAG and leverage
    in-context prompting. However, if you want to provide a large corpus of documents
    as a context to the model, using RAG would be a better approach. However, RAG
    is beneficial even when the corpus can fit in the context due to needle-in-a-haystack
    problems, which can affect retrieval accuracy. To summarize, RAG becomes specifically
    useful in two primary use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: When the corpus size exceeds that of the context length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we want to dynamically provide context to the model instead of feeding
    it the entire corpus in context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG has many potential applications for improving GenAI. It can help build contextual
    chatbots that rely on real enterprise data. It can enable personalized search
    and recommendations based on user history and preferences. A RAG approach can
    also aid real-time summarization of large documents by retrieving and condensing
    key facts. For example, applying RAG to summarize extensive legal texts or academic
    papers allows for the extraction and condensation of important information, providing
    succinct summaries that capture the core points. Overall, RAG is an important
    technique for overcoming some limitations of current generative models and grounding
    them in factual knowledge. This helps make the generated content more useful,
    reliable, and personalized.
  prefs: []
  type: TYPE_NORMAL
- en: Key applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Compared to other LLM customization techniques, such as prompt engineering
    or fine-tuning, RAG offers several advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Flexibility of knowledge source**: The knowledge base can be customized for
    each use case without changing the underlying LLM. Knowledge can be easily added,
    removed, or updated without costly model retraining. This is especially useful
    for organizations whose knowledge is rapidly evolving.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost-effective**: RAG allows a single-hosted LLM to be shared across many
    use cases through swappable knowledge sources. There is no need to train bespoke
    models for each use case, which means greater cost efficiency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Natural language queries**: RAG relies on natural language for context retrieval
    from the knowledge source, unlike prompt engineering, which uses rigid prompt
    templates. This enables users to be more flexible when working with the models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For most organizations with a custom knowledge pool of information, RAG strikes
    a balance between cost, flexibility, and usability. Prompt engineering is sufficient
    for small amounts of context, while full model fine-tuning entails high training
    costs and rigid knowledge. RAG allows easy knowledge base updates and sharing
    of LLMs across use cases.
  prefs: []
  type: TYPE_NORMAL
- en: For example, RAG is well suited for **business-to-business Software-as-a-Service**
    (**B2B SaaS**) companies that manage evolving document bases across many customers.
    A single-hosted LLM can handle queries across clients by swapping their context
    documents, eliminating the need for per-client models.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the importance and potential applications of RAG in different
    scenarios, let us jump into exploring the working of RAG.
  prefs: []
  type: TYPE_NORMAL
- en: How does RAG work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Figure 5**.1* provides a high-level overview of how RAG works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Simplified RAG](img/B22045_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Simplified RAG
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us now understand these steps in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Given a prompt from the user, the retriever module is invoked to encode the
    input query in a dense vector representation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The retriever module then finds relevant context (passages or documents) from
    the knowledge corpus, based on maximum inner product similarity (semantic similarity)
    between the query vector and pre-computed dense vector representations of the
    corpus contents.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An optional re-ranker module can then re-score and re-rank the initially retrieved
    results and select the best context passages to augment the generation. The re-ranker
    helps surface the most relevant passages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The top-ranked retrieved contexts are fused with the input query to form an
    augmented prompt (query and context).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The generative model, i.e. the FM or LLM then produces the output text conditioned
    on both the original query prompt and the retrieved relevant knowledge contexts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In some RAG systems, the retrieval and re-ranking process can be repeated during
    the generation step to dynamically retrieve more relevant knowledge as the output
    is being generated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The key benefits of RAG are ensuring that the generated outputs are grounded
    in accurate and up-to-date information from trusted external sources, providing
    source citations for transparency, and reducing hallucinations or inaccuracies
    from the language model’s training data alone.
  prefs: []
  type: TYPE_NORMAL
- en: RAG systems meet enterprise requirements for GenAI, such as being comprehensive,
    trustworthy, transparent, and credible by properly sourcing, vetting, and customizing
    the underlying data sources and models for specific use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Components of RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As explained earlier, once a query is received, relevant context is retrieved
    from the knowledge source and condensed into a context document. This context
    is then concatenated with the original query and fed into the LLM to generate
    a final response. The knowledge source acts as a dynamic long-term memory, in
    a way that can be frequently updated, while the LLM contributes its strong language
    generation capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The knowledge base
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A key component of RAG models is the knowledge base, which contains the external
    knowledge used for retrieval. The knowledge base stores information in a format
    optimized for fast retrieval, such as dense vectors or indexes.
  prefs: []
  type: TYPE_NORMAL
- en: Popular knowledge sources used in RAG include *Wikipedia*, news archives, books,
    scientific papers, and proprietary knowledge bases created specifically for RAG
    models. The knowledge can consist of both structured (for example, tables and
    lists) and unstructured (for example, free text) data.
  prefs: []
  type: TYPE_NORMAL
- en: In a typical RAG scenario, the textual contents of the documents (or web pages)
    that make up the knowledge corpus and need to be converted into dense vector representations
    or embeddings are encoded data into smaller chunks. To preserve this structure
    of tables or lists while encoding, more advanced encoding techniques are used
    that can embed entire tables/lists as a single vector while retaining their row/column
    relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Long unstructured text passages are typically chunked or split into smaller
    text segments or passages of a maximum length (for example, 200 tokens). Each
    of these chunks or passages is then encoded independently into a dense vector
    representation.
  prefs: []
  type: TYPE_NORMAL
- en: This embedding process typically happens asynchronously or as a batch process,
    separate from and ahead of time before any user queries are received by the RAG
    system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The embeddings for the entire document corpus are pre-computed and stored before
    the system is deployed or used for any query answering. This pre-computation step
    is necessary for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The document corpus can typically be very large (for example, *Wikipedia* has
    millions of articles)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embedding the full corpus at query time would be extremely slow and inefficient
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pre-computed embeddings allow fast maximum inner product search at query time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By embedding the sources asynchronously ahead of time, the RAG system can quickly
    retrieve relevant documents by comparing the query embedding against the pre-computed
    document embeddings using efficient vector similarity search methods such as cosine
    similarity, Euclidean distance, **Microprocessor without Interlocked Pipelined
    Stages** (**MIPS**), or **Facebook AI Similarity Search** (**FAISS**). Readers
    are encouraged to review the paper *A Survey on Efficient Processing of Similarity
    Queries over Neural Embeddings* ([https://arxiv.org/abs/2204.07922](https://arxiv.org/abs/2204.07922)),
    which debriefs methods on efficient processing of similarity queries.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the sizes and scope of the knowledge base has a major influence on
    the capabilities of the RAG system. Larger knowledge bases with more diverse,
    high-quality knowledge provide more contextual information for the model to draw
    from, based on the user’s questions.
  prefs: []
  type: TYPE_NORMAL
- en: Retriever module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The retriever module is responsible for finding and retrieving the most relevant
    knowledge from the knowledge base for each specific context. The input to the
    retrieval model is typically the prompt or context from the user.
  prefs: []
  type: TYPE_NORMAL
- en: The embedding model encodes the prompt into a vector representation and matches
    it against encoded representations of the knowledge base to find the closest matching
    entries.
  prefs: []
  type: TYPE_NORMAL
- en: Common retrieval methods include sparse methods such as **Term Frequency-Inverse
    Document Frequency** (**TF-IDF**) or **Best Match 25** (**BM25**), as well as
    dense methods such as semantic search over embedded representations from a dual-encoder
    model. The retrieval model ranks the knowledge and returns the top *k* most relevant
    pieces back to the generative model.
  prefs: []
  type: TYPE_NORMAL
- en: The tighter the integration between the retrieval model and the generative model,
    the better the retrieval results.
  prefs: []
  type: TYPE_NORMAL
- en: Conditioning the generative model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The key aspect that makes the RAG process generative is the conditional generative
    model. This model takes the retrieved knowledge along with the original prompt
    and generates the output text.
  prefs: []
  type: TYPE_NORMAL
- en: 'The knowledge can be provided in different ways to condition the generation:'
  prefs: []
  type: TYPE_NORMAL
- en: Concatenating the retrieved text to the prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoding the retrieved text into dense vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inserting the retrieved text into the input at particular positions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, in a typical scenario, the retrieved knowledge is augmented with
    the input prompt and fed to the LLM to provide a succinct response to the end
    user. This allows the LLM to directly condition the text generation on the relevant
    facts and context. Users are encouraged to check out the paper *Leveraging Passage
    Retrieval with Generative Models for Open Domain Question Answering* ([https://arxiv.org/pdf/2007.01282.pdf](https://arxiv.org/pdf/2007.01282.pdf))
    in order to gain a deeper understanding of the complexity of RAG in the realm
    of question answering frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: The generative model is usually a large pre-trained language model such as GPT-4,
    Anthropic Claude 3, Amazon Titan Text G1, and so on. The model can be further
    fine-tuned end to end on downstream RAG tasks, if needed, in order to optimize
    the integration of the retrieved knowledge for domain-specific use cases. Now,
    let us dive into exploring RAG with Amazon Bedrock.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing RAG with Amazon Bedrock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Prior to responding to user queries, the system must ingest and index the provided
    documents. This process can be considered as *step 0*, and consists of these sub-steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Ingest the raw text documents into the knowledge base.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess the documents by splitting them into smaller chunks to enable more
    granular retrieval.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate dense vector representations for each passage using an embedding model
    such as Amazon Bedrock’s Titan Text Embeddings model. This encodes the semantic
    meaning of each passage into a high-dimensional vector space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Index the passages and their corresponding vector embeddings into a specialized
    search index optimized for efficient **nearest neighbor** (**NN**) search. These
    are also referred to as **vector databases**, which store numerical representations
    of text in the form of vectors. This index powers fast retrieval of the most relevant
    passages in response to user queries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By completing this workflow, the system constructs an indexed corpus ready to
    serve relevant results for natural language queries over the ingested document
    collection. The passage splitting, embedding, and indexing steps enable robust
    ranking and retrieval capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'The flow diagram depicted in *Figure 5**.2* exemplifies the overall flow of
    the RAG process as described previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – RAG with Amazon Bedrock](img/B22045_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – RAG with Amazon Bedrock
  prefs: []
  type: TYPE_NORMAL
- en: 'When documents have been properly indexed, the system can provide contextual
    answers to natural language questions through the following pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Encode the input question into a dense vector representation (embedding)
    using an embedding model, such as the Amazon Titan Text Embeddings model or Cohere’s
    embedding model, both of which can be accessed via Amazon Bedrock. This captures
    the semantic meaning of the question.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 2**: Compare the question embedding to indexed document embeddings using
    cosine similarity or other distance metrics. This retrieves the most relevant
    document chunks. Append the top-ranking document chunks to the prompt as contextual
    information. This provides relevant background knowledge for the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 3**: Pass the prompt with context to an LLM available on Amazon Bedrock
    such as Anthropic Claude 3, Meta Llama 3, or Amazon Titan Text G1 - Express. This
    leverages the model’s capabilities to generate an answer conditioned on the retrieved
    documentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, return the model-generated answer, which should show an understanding
    of the question in relation to the contextual documents.
  prefs: []
  type: TYPE_NORMAL
- en: The system thus leverages Amazon Bedrock FMs to provide natural language question
    answering grounded in relevant documentation and context. Careful indexing and
    encoding of documents enable seamless integration of retrieval with generative
    models for more informed and accurate answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of RAG implementation with Amazon Bedrock and Amazon OpenSearch
    Serverless as a vector engine: [https://aws.amazon.com/blogs/big-data/build-scalable-and-serverless-rag-workflows-with-a-vector-engine-for-amazon-opensearch-serverless-and-amazon-bedrock-claude-models/](https://aws.amazon.com/blogs/big-data/build-scalable-and-serverless-rag-workflows-with-a-vector-engine-for-amazon-opensearch-serverless-and-amazon-bedrock-claude-models/).'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have discussed some details around implementing RAG with Amazon
    Bedrock, let us dive deep into tackling use cases using RAG through Knowledge
    Bases on Amazon Bedrock.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Bedrock Knowledge Bases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon Bedrock provides a fully managed RAG experience with Knowledge Bases,
    handling the complexity behind the scenes while giving you control over your data.
    Bedrock’s Knowledge Base capability enables the aggregation of diverse data sources
    into a centralized repository of machine-readable information. Knowledge Bases
    automate the creation of vector embeddings from your data, store them in a managed
    vector index, and handle embedding, querying, source attribution, and short-term
    memory for production RAG.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key benefits of Knowledge Bases in Amazon Bedrock include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Seamless RAG workflow**: There’s no need to set up and manage the components
    yourself. You can just provide your data and let Amazon Bedrock handle ingestion,
    embedding, storage, and querying.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom vector embeddings**: Your data is ingested and converted into vector
    representations tailored to your use case with a choice of embedding models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RetrieveAndGenerate` API within Amazon Bedrock provides attribution back to
    source documents and manages conversation history for contextual responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexible integration**: Incorporate RAG into your workflows with API access
    and integration support for other GenAI tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Bedrock Knowledge Base setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Objectively speaking, the following steps facilitate Knowledge Base creation
    and integration:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify and prepare data sources for ingestion
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upload data to **Amazon Simple Storage Service** (**Amazon S3**) for centralized
    access
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate embeddings for data via FMs and persist in a vector store
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connect applications and agents to query and incorporate Knowledge Base into
    workflows
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To create ingestion jobs, follow the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Set up your Knowledge Base**: Before you can ingest data, you need to create
    a knowledge base. This involves defining the structure and schema of the knowledge
    base to ensure it can store and manage the data effectively.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prepare your** **data source**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensure your data is stored in Amazon S3\. The data can be in various formats,
    including structured (for example, CSV, JSON) and unstructured (for example, text
    files, PDFs).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Organize your data in a way that makes it easy to manage and retrieve.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create an** **ingestion job**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the AWS Bedrock console and go to the **Knowledge** **base** section.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Select the option to create a new ingestion job.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide the necessary details, such as the name of the job, the S3 bucket location,
    and the data format.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Configure the job to specify how the data should be processed and ingested into
    the knowledge base.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Configure** **sync settings**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the sync settings to ensure the knowledge base is updated with the most
    recent data from your S3 location.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can configure the sync to run at regular intervals (for example, daily or
    weekly) or trigger it manually as needed.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that the sync settings are optimized to handle large volumes of data
    efficiently.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Run the** **ingestion job**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the job is configured, you can start the ingestion process.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor the job’s progress through the AWS Bedrock console. You can view logs
    and status updates to ensure the job is running smoothly.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of the ingestion process, let us walk
    through these details thoroughly.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to initiate this pipeline within the AWS console, one can navigate
    to the **Orchestration** section within the Amazon Bedrock page, as shown in *Figure
    5**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Knowledge base](img/B22045_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Knowledge base
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at these steps in greater depth:'
  prefs: []
  type: TYPE_NORMAL
- en: Click on **Knowledge base** and enter the details pertaining to the knowledge
    base you intend to create. You can provide a custom knowledge base name, description,
    and the respective **Identity and Access Management** (**IAM**) permissions for
    creating either a new service role or leveraging an existing service role for
    the knowledge base. You can also provide tags to this resource for easy searching
    and filtering of your resource or tracking AWS costs associated with the service
    in the knowledge base details section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the next step, you will set up the data source by specifying the S3 location
    where the data to be indexed resides. You can specify a particular data source
    name (or leverage the default pre-filled name) and provide the S3 URI (Uniform
    Resource Identifier) of the bucket containing the source data, as depicted in
    *Figure 5**.4*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Knowledge base: Set up data source](img/B22045_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4 – Knowledge base: Set up data source'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also provide a customer-managed **Key Management Service** (**KMS**)
    key you used for encrypting your S3 data, in order to allow the Bedrock service
    to decrypt it when ingesting the given data into the vector database. Under **Advanced
    settings** (as shown in *Figure 5**.5*), users have the option to choose the default
    KMS key or customize encryption settings by choosing a different key of their
    choice by entering the **Amazon Resource Name** (**ARN**) or searching for their
    stored customized key (or creating a new AWS KMS key on the fly). Providing a
    customer-managed KMS key for encrypting S3 data sources ingested by Amazon Bedrock
    is desired for enhanced data security, compliance, and control. It allows data
    sovereignty, key rotation/revocation, **separation of duties** (**SoD**), auditing/logging
    capabilities, and integration with existing key management infrastructure. By
    managing your own encryption keys, you gain greater control over data protection,
    meeting regulatory requirements and aligning with organizational security policies
    for sensitive or regulated data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Knowledge base: Advanced settings](img/B22045_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5 – Knowledge base: Advanced settings'
  prefs: []
  type: TYPE_NORMAL
- en: 'Under **Chunking strategy** (as shown in *Figure 5**.6*), users have the option
    to select how to break down text in the source location into smaller segments
    before creating the embedding. By default, the knowledge base will automatically
    split your data into tiny chunks each containing, at most, 300 tokens. If a document
    or, in other words, source data contains fewer than 300 tokens, it is not split
    any further in that case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Knowledge base: Chunking strategy](img/B22045_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6 – Knowledge base: Chunking strategy'
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you have the option to customize the chunk size using **Fixed
    size chunking** or simply opt for **No chunking** in case you have already preprocessed
    your source documents into separate files of smaller chunks and don’t intend to
    chunk your documents any further using Bedrock.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next stage, users will select an embedding model to convert their selected
    data into an embedding. Currently, there are four embeddings models that are supported,
    as shown in *Figure 5**.7*. Further, under **Vector database**, users have the
    option to go with the recommended route – that is, select the quick create option,
    which will create an Amazon OpenSearch Serverless vector store in the background
    automatically in the respective account of their choice:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Vector embeddings are numeric representations of text data that encode semantic
    or contextual meaning. In NLP pipelines, text documents are passed through an
    embedding model to convert the chunks, including discrete tokens such as words
    into dense vectors in a continuous vector space. Good vector representations allow
    **machine learning** (**ML**) models to understand similarities, analogies, and
    other patterns between words and concepts. In other words, if the vector representations
    (embeddings) are trained well on a large dataset, they will capture meaningful
    relationships in the data. This allows ML models that use those embeddings to
    recognize things such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Which words are similar in meaning (for example, *king* and *queen*)'
  prefs: []
  type: TYPE_NORMAL
- en: '- Which concepts follow an analogical pattern (for example, *man* is to *king*
    as *woman* is to *queen*)'
  prefs: []
  type: TYPE_NORMAL
- en: '- Other patterns in how the concepts are represented in the embedding space'
  prefs: []
  type: TYPE_NORMAL
- en: The well-trained embeddings essentially provide the ML models with a numeric
    map of the relationships and patterns inherent in the data. This makes it easier
    for the models to then learn and make inferences about those patterns during training
    on downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, simply put, good embeddings help ML models understand similarities and
    relationships between words and concepts rather than just treating them as isolated
    data points.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – Knowledge base: Configure vector store](img/B22045_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7 – Knowledge base: Configure vector store'
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you have the option to choose your own vector store (as shown
    in *Figure 5**.8*). At the time of writing this book, you have the option to select
    **Vector engine for Amazon OpenSearch Serverless**, **Amazon Aurora**, **MongoDB
    Atlas**, **Pinecone**, or **Redis Enterprise Cloud**. Once selected, you can provide
    the field mapping to proceed with the knowledge base creation final setup. Depending
    on the use case, developers or teams may opt for one vector database over another.
    You can read more about the role of vector datastores in GenAI applications at
    [https://aws.amazon.com/blogs/database/the-role-of-vector-datastores-in-generative-ai-applications/](https://aws.amazon.com/blogs/database/the-role-of-vector-datastores-in-generative-ai-applications/):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Knowledge base: Vector database](img/B22045_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.8 – Knowledge base: Vector database'
  prefs: []
  type: TYPE_NORMAL
- en: You can check out [https://aws.amazon.com/blogs/aws/preview-connect-foundation-models-to-your-company-data-sources-with-agents-for-amazon-bedrock/](https://aws.amazon.com/blogs/aws/preview-connect-foundation-models-to-your-company-data-sources-with-agents-for-amazon-bedrock/)
    to learn more about how you can set up your own vector store with Pinecone, OpenSearch
    Serverless, or Redis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming that you opt for the default route, involving the creation of a new
    Amazon OpenSearch Serverless vector store, you can proceed and click on **Create
    knowledge base** post reviewing all the provided details as depicted in *Figure
    5**.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Knowledge base: Review and create](img/B22045_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9 – Knowledge base: Review and create'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once created, you can sync the information to ensure the knowledge base is
    ingesting and operating on the most recent data stored in your Amazon S3 location.
    After syncing is completed for the knowledge base, users can test said knowledge
    base by selecting the appropriate model suitable for their use case by clicking
    on **Select Model**, as shown in *Figure 5**.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Test knowledge base: Select Model](img/B22045_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10 – Test knowledge base: Select Model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the appropriate model has been selected, you can test it by entering a
    particular query in the textbox and receiving a particular response generated
    by the model, depicted in *Figure 5**.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11 – Test Knowledge base](img/B22045_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – Test Knowledge base
  prefs: []
  type: TYPE_NORMAL
- en: At its core, Amazon Bedrock transforms the user’s query into vector representations
    of meaning; that is, embeddings. It then searches the knowledge base for relevant
    information using these embeddings as the search criteria. Any knowledge retrieved
    is combined with the prompt engineered for the FM, providing essential context.
    The FM integrates this contextual knowledge into its response generation to answer
    the user’s question. For conversations spanning multiple turns, Amazon Bedrock
    leverages its knowledge base to maintain conversation context and history, delivering
    increasingly relevant results.
  prefs: []
  type: TYPE_NORMAL
- en: Additional information for testing the knowledge base and inspecting source
    chunks can be found at [https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-test.html](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-test.html).
  prefs: []
  type: TYPE_NORMAL
- en: To ensure your knowledge base is always up to date, it is essential to automate
    the syncing process. This can be achieved by using AWS Lambda functions or AWS
    Step Functions to trigger ingestion jobs based on specific events or schedules.
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS Lambda** is a serverless compute service that allows you to run code
    without provisioning or managing servers. You can create Lambda functions to automate
    tasks such as triggering data ingestion jobs, processing data, or sending notifications.
    Lambda functions can be triggered by various events, including file uploads to
    Amazon S3, changes to DynamoDB tables, or scheduled events using Amazon CloudWatch
    Events.'
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS Step Functions** is a serverless function orchestrator that allows you
    to coordinate multiple AWS services into business workflows. You can create state
    machines that define a series of steps, including Lambda functions, data processing
    tasks, and error-handling logic. Step Functions can be particularly useful for
    orchestrating complex data ingestion pipelines or ML workflows.'
  prefs: []
  type: TYPE_NORMAL
- en: Regularly monitoring and managing data sources is crucial to maintain their
    relevance and accuracy. **Amazon CloudWatch** is a monitoring and observability
    service that provides data and actionable insights across your AWS resources.
    You can utilize CloudWatch to set up alarms and notifications for any issues or
    anomalies in the data syncing process. CloudWatch can monitor metrics such as
    Lambda function invocations, Step Functions executions, and Amazon S3 bucket activity,
    allowing you to proactively identify and address potential issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Adhering to best practices for data management, such as organizing data logically,
    maintaining data quality, and ensuring data security, is vital. AWS provides various
    services and tools to support data management best practices:'
  prefs: []
  type: TYPE_NORMAL
- en: You can organize your data in Amazon S3 buckets and leverage features such as
    versioning, lifecycle policies, and access controls to maintain data quality and
    security.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Glue** is a fully managed **extract, transform, and load** (**ETL**)
    service that can help you prepare and move data reliably between different data
    stores. Glue can be used to clean, transform, and enrich your data before ingesting
    it into your knowledge base.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Lake Formation** is a service that helps you build, secure, and manage
    data lakes on Amazon S3\. It provides features such as data cataloging, access
    control, and auditing, which can help ensure data security and governance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regular reviews and updates of the knowledge base should be conducted to remove
    outdated information and incorporate new, relevant data. AWS provides services
    such as **Amazon Kendra** and **Amazon Comprehend** that can help you analyze
    and understand your knowledge base content, identify outdated or irrelevant information,
    and suggest updates or improvements.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking actionable metrics, such as search success rate, user engagement, and
    data freshness, is also important. These metrics can help continuously improve
    the knowledge base, ensuring it meets the needs of its users effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon CloudWatch can be used to collect and analyze metrics from various AWS
    services, including your knowledge base application. You can create custom metrics,
    dashboards, and alarms to monitor the performance and usage of your knowledge
    base.
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging AWS services such as Lambda, Step Functions, CloudWatch, S3, Glue,
    Lake Formation, Kendra, and Comprehend, you can automate the syncing process,
    monitor and manage data sources, adhere to data management best practices, and
    track actionable metrics to ensure your knowledge base remains up to date, relevant,
    and effective in meeting the needs of your users.
  prefs: []
  type: TYPE_NORMAL
- en: Readers are encouraged to visit the Amazon Bedrock RAG GitHub repository ([https://github.com/aws-samples/amazon-bedrock-rag](https://github.com/aws-samples/amazon-bedrock-rag))
    to explore and implement a fully managed RAG solution using Knowledge Bases for
    Amazon Bedrock.
  prefs: []
  type: TYPE_NORMAL
- en: API calls
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For users who wish to invoke Bedrock outside of the console, the `RetrieveAndGenerate`
    API provides programmatic access to execute this same workflow. This allows Bedrock’s
    capabilities to be tightly integrated into custom applications via API calls rather
    than console interaction. The `RetrieveAndGenerate` API gives developers the flexibility
    to build Amazon Bedrock-powered solutions tailored to their specific needs. *Figure
    5**.12* illustrates the RAG workflow using Amazon Bedrock’s `RetrieveAndGenerate`
    API:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12 – RetrieveAndGenerate API](img/B22045_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – RetrieveAndGenerate API
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `RetrieveAndGenerate` API, the generated response output contains three
    components: the text of the model-generated response itself, source attribution
    indicating where the FM retrieved information from, and the specific text excerpts
    that were retrieved from those sources as part of generating the response. The
    API provides full transparency by returning not just the final output text but
    also the underlying source materials and attributions that informed the FM’s response
    generation process. This allows users to inspect both the final output as well
    as the intermediate retrieved texts that were used by the system during response
    generation.'
  prefs: []
  type: TYPE_NORMAL
- en: The following is a code sample for running the same operation as showcased in
    the console using the API.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensure you have the latest version of the `boto3` and `botocore` packages prior
    to running the code shown next. In case the packages are not installed, run the
    following command in your Jupyter notebook. Note that `!` will not be needed if
    you’re running Python code from a Python terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '`!pip install` `boto3 botocore`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This script assumes that readers have already created a knowledge base and ingested
    the relevant documents, following the procedures outlined in the preceding section.
    With this prerequisite fulfilled, invoking the `RetrieveAndGenerate` API will
    enable the system to fetch the associated documents using the provided code sample.
  prefs: []
  type: TYPE_NORMAL
- en: The code provided will print the extracted text output to display the relevant
    information from the data source in context to the input query, formatted as desired.
    The response is generated by contextualizing pertinent details from the data source
    with respect to the specifics of the input query. The output is then formatted
    and presented in the requested structure. This allows customized extraction and
    formatting of relevant data from the source to provide responses tailored to the
    input query in a suitable structure.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Please ensure you have the right permissions to invoke Amazon Bedrock APIs by
    navigating to IAM roles and permissions, searching for the respective role (if
    you are running the notebook in Amazon SageMaker, search for the execution role
    that was assigned when you created the Amazon SageMaker domain), and attaching
    Amazon Bedrock policies for invoking Bedrock models and Bedrock agent runtime
    APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yet another resourceful Amazon Bedrock API, the `Retrieve` API, enables more
    advanced processing and utilization of the retrieved text segments. This API transforms
    user queries into vector representations, performs similarity searches against
    the knowledge base, and returns the most relevant results along with relevance
    scores. The `Retrieve` API provides users with more fine-grained control to build
    custom pipelines leveraging semantic search capabilities. Through the `Retrieve`
    API, developers can orchestrate subsequent stages of text generation based on
    the search results, implement additional relevance filtering, or derive other
    workflow optimizations. *Figure 5**.13* exemplifies the usage of the `Retrieve`
    API in Amazon Bedrock in a RAG pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13 – Retrieve API](img/B22045_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – Retrieve API
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the Amazon Bedrock console, you can toggle the switch to disable the
    `What is Quantum Computing?` again. *Figure 5**.14* showcases the generated responses
    retrieved from the knowledge base pertaining to the question on quantum computing.
    Note that Amazon Bedrock cites the references along with the generated responses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.14 – Test knowledge base](img/B22045_05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 – Test knowledge base
  prefs: []
  type: TYPE_NORMAL
- en: This time, instead of a fluid natural language response, notice that the output
    displays the retrieved text chunks alongside links to the original source documents
    from which they were extracted. This approach provides transparency by explicitly
    showing the relevant information retrieved from the knowledge base and its provenance.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensure you have the latest version of the `boto3` and `botocore` packages prior
    to running the code shown next. In case the packages are not installed, run the
    following command in your Jupyter notebook. Note that `!` will not be needed if
    you’re running Python code from a Python terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '`!pip install` `boto3 botocore`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Leveraging the `Retrieve` API using `boto3` looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `Retrieve` API returns a response containing the retrieved text excerpts,
    as well as metadata about the source of each excerpt. Specifically, the response
    includes the location type and URI of the source data from which each text chunk
    was retrieved. Additionally, each retrieved text chunk is accompanied by a relevancy
    score. This score provides an indication of how closely the semantic content of
    the retrieved chunk matches the user’s input query. Text chunks with higher scores
    are more relevant matches to the query compared to chunks with lower scores. By
    examining the scores of the retrieved chunks, the user can focus on the most relevant
    excerpts returned by the `Retrieve` API. Therefore, the `Retrieve` API provides
    not only the retrieved text but also insightful metadata to enable productive
    utilization of the API response.
  prefs: []
  type: TYPE_NORMAL
- en: 'By tapping into the custom chunking and vector store capabilities within the
    RAG framework, you gain more fine-grained control over how your NLP workflows
    operate under the hood. Expertly applying these customizations helps ensure RAG
    is tailored to your specific needs and use cases. Note that at the time of writing
    this book, when creating a data source for your knowledge base, you can specify
    the chunking strategy in the `ChunkingConfiguration` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at this in a bit more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '`FIXED_SIZE` allows you to set a fixed chunk size in tokens for splitting your
    data sources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NONE` treats each file as a single chunk, giving you full control over pre-chunking
    your data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further information on using the API with the AWS Python SDK can be found at
    [https://aws.amazon.com/blogs/aws/knowledge-bases-now-delivers-fully-managed-rag-experience-in-amazon-bedrock/](https://aws.amazon.com/blogs/aws/knowledge-bases-now-delivers-fully-managed-rag-experience-in-amazon-bedrock/).
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge Bases for Amazon Bedrock reduces the complexity of RAG, allowing you
    to enhance language generation with your own grounded knowledge. The capabilities
    open new possibilities for building contextual chatbots, question answering applications,
    and other AI systems that need to generate informed specific responses.
  prefs: []
  type: TYPE_NORMAL
- en: Let us further explore how the RAG approach can be implemented using the LangChain
    orchestrator and other GenAI systems.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing RAG with other methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon Bedrock is not the only way to implement RAG, and in this section, we
    will learn about the other ways. Starting with LangChain, we will also look at
    some other GenAI systems.
  prefs: []
  type: TYPE_NORMAL
- en: Using LangChain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LangChain provides an excellent framework for building RAG models by integrating
    retrieval tools and LLMs. In this section, we will look at how to implement RAG
    with LangChain using the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LLMs**: LangChain integrates with Amazon Bedrock’s powerful LLMs using Bedrock’s
    available FM invocation APIs. Amazon Bedrock can be used to generate fluent NL
    responses after reviewing the retrieved documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embedding model**: Text embedding models available via Amazon Bedrock, such
    as Amazon Titan Text Embeddings, generate vector representations of text passages.
    This allows comparing textual similarity in order to retrieve relevant contextual
    information to augment the input prompt for composing a final response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document loader**: LangChain provides a PDF loader to ingest documents from
    local storage. This can be replaced by a loader to retrieve enterprise documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pgvector`, can be leveraged based on the use case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Index**: The vector index matches input embeddings with stored document embeddings
    to find the most relevant contexts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wrapper**: LangChain provides a wrapper class that abstracts away the underlying
    logic, handling retrieval, embeddings, indexing, and generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The RAG workflow via LangChain orchestration is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Ingest a collection of documents into the document loader
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate embeddings for all documents using the embedding model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Index all document embeddings in the vector store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For an input question, generate its embedding using the embedding model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the index to retrieve the most similar document embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass the relevant documents to the LLM to generate a natural language answer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By orchestrating retrieval and generation in this way, LangChain provides an
    easy yet powerful framework for developing RAG models. The modular architecture
    allows flexibility, extensibility, and scalability. For more details on RAG implementation
    with LangChain, follow the steps in the Amazon Bedrock workshop at [https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/01_Langchain_KnowledgeBases_and_RAG_examples/01_qa_w_rag_claude.ipynb](https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/01_Langchain_KnowledgeBases_and_RAG_examples/01_qa_w_rag_claude.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Other GenAI systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RAG models can be integrated with other GenAI tools and applications to create
    more powerful and versatile AI systems. For instance, RAG’s knowledge retrieval
    capabilities can be combined with conversational agents built on Amazon Bedrock.
    This allows the agents to perform multi-step tasks and leverage external knowledge
    bases to generate responses that are more contextually relevant.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the RAG knowledge base retrieval enables seamless integration
    of RAG into custom GenAI pipelines. Developers can retrieve knowledge from RAG
    indexes and fuse it with LangChain’s generative capabilities. This unlocks new
    use cases such as building AI assistants that can provide expert domain knowledge
    alongside general conversational abilities.
  prefs: []
  type: TYPE_NORMAL
- en: Further information on LangChain retrievers can be found at [https://python.langchain.com/docs/integrations/retrievers](https://python.langchain.com/docs/integrations/retrievers).
  prefs: []
  type: TYPE_NORMAL
- en: We will cover more details about agents for Amazon Bedrock in [*Chapter 10*](B22045_10.xhtml#_idTextAnchor192)
    where we will uncover more RAG-based integration with Amazon Bedrock agents.
  prefs: []
  type: TYPE_NORMAL
- en: With Amazon Bedrock’s managed approach, incorporating real-world knowledge into
    FMs has become more accessible than ever. Now, let us uncover some advanced RAG
    techniques that are rapidly growing as a mechanism to improve upon current RAG
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced RAG techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While basic RAG pipelines involve retrieving relevant documents and directly
    providing them as context to the LLM, advanced RAG techniques employ various methods
    to enhance the quality, relevance, and factual accuracy of generated responses.
    These advanced techniques go beyond the naive approach of simple document retrieval
    and context augmentation, aiming to optimize various stages of the RAG pipeline
    for improved performance.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now look at some key areas where advanced RAG techniques focus.
  prefs: []
  type: TYPE_NORMAL
- en: Query handler – query reformulation and expansion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One key area of advancement is query reformulation and expansion. Instead of
    relying solely on the user’s initial query, advanced RAG systems employ NLP techniques
    to generate additional related queries. This increases the chances of retrieving
    a more comprehensive set of relevant information from the knowledge base. Query
    reformulation can involve techniques such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"``Hurricane formation"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Expanded query*: `"Hurricane formation" OR "Tropical cyclone genesis" OR "Tropical`
    `storm development"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`What` `causes hurricanes?`*   *Rewritten query*: `Explain the meteorological
    conditions and processes that lead to the formation of hurricanes or` `tropical
    cyclones.`*   `When was the first` `iPhone released?`*   *Extracted* *entities*:
    `iPhone`*   *Expanded query*: `iPhone AND ("product launch" OR "release date"`
    `OR "history")`*   `Causes of the American` `Civil War`”*   *Generated queries*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`What were the key political and economic factors that led to the American`
    `Civil War?`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`How did the issue of slavery contribute to starting the American` `Civil War?`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`What were the major events and incidents that precipitated the outbreak of
    the Civil War` `in America?`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 5**.15* illustrates an overview of a query handler with rewriting and
    re-ranking mechanisms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15 – Query handler with rewriting and re-ranking mechanisms](img/B22045_05_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 – Query handler with rewriting and re-ranking mechanisms
  prefs: []
  type: TYPE_NORMAL
- en: By retrieving information for multiple reformulated queries, the system can
    gather a richer context to better understand the user’s intent and provide more
    complete and accurate responses.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid search and retrieval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Advanced RAG systems often employ hybrid retrieval strategies that combine
    different retrieval methods to leverage their respective strengths. For example,
    a system might use sparse vector search for initial filtering, followed by dense
    vector search for re-ranking and surfacing the most relevant documents. Other
    hybrid approaches include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Combining keyword matching with vector similarity search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using different retrieval methods for different types of data (for example,
    structured versus unstructured)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical retrieval, where coarse-grained retrieval is followed by fine-grained
    re-ranking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s a simple example to illustrate hybrid search and retrieval:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.16 – Hybrid search and retrieval approach](img/B22045_05_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.16 – Hybrid search and retrieval approach
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say you are searching for information about `apple products` on a website
    that sells electronics and grocery items.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hybrid search approach combines two retrieval methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`apple products`, it will retrieve documents/pages that contain the words `apple`
    and `products`, such as the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Buy the latest Apple iPhone` `models here`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Apple MacBook Pro laptops` `on sale`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Apple cider and apple juice in the` `grocery section`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`apple products`, it may retrieve documents such as the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Top tech gadgets and accessories for students` (semantically related to electronics/products)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Healthy fruits and snacks for kids'' lunchboxes` (semantically related to
    apple as a fruit)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The hybrid search can then combine and re-rank the results from both retrieval
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Buy the latest Apple iPhone` `models here`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Apple MacBook Pro laptops` `on sale`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Top tech gadgets and accessories` `for students`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Apple cider and apple juice in the` `grocery section`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Healthy fruits and snacks for` `kids'' lunchboxes`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By combining keyword matching (for brand/product names) and semantic understanding
    (for broader context), the hybrid approach can provide more comprehensive and
    relevant search results compared to using just one method.
  prefs: []
  type: TYPE_NORMAL
- en: The key benefit is retrieving documents that are relevant both lexically (containing
    the exact query keywords) and semantically (related conceptually to the query
    intent), improving overall search quality.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding and index optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The quality of the vector embeddings and indexes used for retrieval can significantly
    impact the performance of RAG systems. Advanced techniques in this area include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Embedding fine-tuning**: Instead of using a general pre-trained embedding
    model, the embedding model can be fine-tuned on domain-specific data to better
    capture the semantics and nuances of that domain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, if building a RAG system for a medical question answering task,
    the embedding model can be further fine-tuned on a large corpus of medical literature,
    such as research papers, clinical notes, and so on. This allows the model to better
    understand domain-specific terminology, abbreviations, and contextual relationships.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Index structuring and partitioning**: Instead of storing all document embeddings
    in a single flat index, the index can be structured or partitioned in ways that
    improve retrieval efficiency; for example, clustering, hierarchical indexing,
    and metadata filtering:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering**: Documents can be clustered based on their embeddings, and separate
    indices created for each cluster. At query time, the query embedding is compared
    against cluster centroids to identify the relevant cluster(s) to search within.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchical indexing**: A coarse-level index can first retrieve relevant
    high-level topics/categories, and then finer-grained indices are searched within
    those topics.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metadata filtering**: If document metadata such as type, source, date, and
    so on is available, the index can be partitioned based on that metadata to allow
    filtering before vector search.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 5**.17* depicts an advanced retrieval mechanism enriched with metadata:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.17 – Retrieval mechanism enriched with metadata](img/B22045_05_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.17 – Retrieval mechanism enriched with metadata
  prefs: []
  type: TYPE_NORMAL
- en: '**Approximate NN (ANN) indexing**: For very large vector indices, techniques
    such as **Hierarchical Navigable Small World** (**HNSW**), FAISS, or **Approximate
    Nearest Neighbors Oh Yeah** (**Annoy**) can be used to create ANN indices. This
    allows trading off some accuracy for massive computational speedups in retrieval
    time over brute-force search. Interested readers can read more details about indexing
    for NN search in the paper *Learning to Index for Nearest Neighbor* *Search* ([https://arxiv.org/pdf/1807.02962](https://arxiv.org/pdf/1807.02962)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Index compression and quantization**: The size of vector indices can be reduced
    through compression and quantization techniques without significantly impacting
    retrieval accuracy. This includes methods such as product quantization, scalar
    quantization, residual quantization, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper *Vector Quantization for Recommender Systems: A Review and Outlook*
    ([https://arxiv.org/html/2405.03110v1](https://arxiv.org/html/2405.03110v1)) provides
    a detailed overview of vector quantization for recommender systems. By optimizing
    the embeddings and indexes, advanced RAG systems can improve the relevance and
    comprehensiveness of the retrieved information, leading to better overall performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval re-ranking and filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Even after initial retrieval, advanced RAG systems often employ additional
    re-ranking and filtering techniques to surface the most relevant information.
    These techniques include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cross-attention re-ranking**: Using more expensive cross-attention models
    can be leveraged in order to re-score and re-rank initially retrieved documents
    based on their relevance to the query. The paper *Multi-Vector Attention Models
    for Deep Re-ranking* ([https://aclanthology.org/2021.emnlp-main.443.pdf](https://aclanthology.org/2021.emnlp-main.443.pdf))
    provides a mechanism for deep re-ranking using multi-vector attention models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learned re-rankers**: Training neural networks or other ML models specifically
    for the task of re-ranking retrieved documents can assist in improving the search
    results augmented with the input query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Filtering and pruning**: Removing less relevant or redundant documents from
    the initial retrieval set based on various heuristics or models can provide contextual
    optimization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, the user may ask a query: `What were the causes of the American`
    `Civil War?`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of initial retrieval via vector search:'
  prefs: []
  type: TYPE_NORMAL
- en: '`The issue of slavery was a primary cause of the` `Civil War...`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Economic differences between North and South led` `to tensions...`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`The election of Abraham Lincoln in 1860` `triggered secession...`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`The Missouri Compromise failed to resolve` `slavery expansion...`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`The Underground Railroad helped enslaved` `people escape...`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some examples of re-ranking:'
  prefs: []
  type: TYPE_NORMAL
- en: '`The election of Abraham Lincoln in 1860` `triggered secession...`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`The issue of slavery was a primary cause of the` `Civil War...`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`The Missouri Compromise failed to resolve` `slavery expansion...`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Economic differences between North and South led` `to tensions...`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`The Underground Railroad helped enslaved` `people escape...`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are the top 3 re-ranked and filtered results:'
  prefs: []
  type: TYPE_NORMAL
- en: '`The election of Abraham Lincoln in 1860` `triggered secession...`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`The issue of slavery was a primary cause of the` `Civil War...`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`The Missouri Compromise failed to resolve` `slavery expansion...`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The top 3 re-ranked and filtered results are then provided as context to the
    language model for generating a final response about the causes of the Civil War,
    focused on the most relevant information.
  prefs: []
  type: TYPE_NORMAL
- en: By re-ranking and filtering the retrieved information, advanced RAG systems
    can provide the LLM with a more focused and relevant context, improving the quality
    and factual accuracy of the generated responses.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5**.18* demonstrates a complete architectural flow using Amazon Bedrock
    and some of the advanced RAG techniques (re-ranking with hybrid search mechanism)
    in order to further enhance the output response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.18 – Advanced RAG approach with Amazon Bedrock](img/B22045_05_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.18 – Advanced RAG approach with Amazon Bedrock
  prefs: []
  type: TYPE_NORMAL
- en: As depicted in *Figure 5**.18*, employing hybrid search (natively available)
    within Knowledge Bases for Amazon Bedrock can greatly enhance contextual search
    quality. Additionally, instead of parsing the data chunks directly to the LLM,
    feeding the retrieved data chunks to a re-ranker model in order to rank the contextual
    results can further improve the quality of the output. Cohere Rerank, Meta’s Dense
    Passage Retrieval, BERT for re-ranking, or open source models in Hugging Face
    (`cross-encoder`/`ms-marco-MiniLM-L6-v2`) are a few examples of re-ranking models
    that can be utilized for such ranking optimization tasks. Finally, once the augmented
    prompt is created with an enhanced query and optimized context, wherein the prompt
    is parsed to the Amazon Bedrock LLM for outputting a desirable response.
  prefs: []
  type: TYPE_NORMAL
- en: In such a manner, advanced RAG techniques can aim to enhance the quality, relevance,
    and factual accuracy of language model outputs by improving various stages of
    the RAG pipeline by incorporating these techniques. Readers are encouraged to
    visit [https://aws.amazon.com/blogs/machine-learning/create-a-multimodal-assistant-with-advanced-rag-and-amazon-bedrock/](https://aws.amazon.com/blogs/machine-learning/create-a-multimodal-assistant-with-advanced-rag-and-amazon-bedrock/)
    to learn how to implement **multimodal RAG** (**mmRAG**) with Amazon Bedrock using
    advanced RAG techniques. This solution also uncovers comprehensive solutioning
    by leveraging advanced LangChain capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'The paper *RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation*
    ([https://arxiv.org/html/2404.00610v1](https://arxiv.org/html/2404.00610v1)) walks
    through yet another advanced RAG approach – **Refine Query for RAG** (**RQ-RAG**),
    which can aid in further optimization of queries by equipping it with capabilities
    for explicit rewriting, decomposition, and disambiguation.'
  prefs: []
  type: TYPE_NORMAL
- en: Since we have uncovered deeper details into RAG functionality, training, and
    its implementation with Bedrock and other GenAI systems, one should also keep
    in mind some limitations and research problems. This provides an opportunity for
    us to evolve with further enhancements with RAG and lead GenAI pathways with more
    insightful thought processes. Some of the limitations and future directions are
    discussed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations and future directions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While promising, RAG models also come with challenges and open research problems,
    including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Knowledge selection**:One of the critical challenges in RAG is determining
    the most relevant and salient knowledge to retrieve from the knowledge base. With
    vast amounts of information available, it becomes crucial to identify and prioritize
    the most pertinent knowledge for the given context. Existing retrieval methods
    may struggle to capture the nuances and subtleties of the query, leading to the
    retrieval of irrelevant or tangential information. Developing more sophisticated
    query understanding and knowledge selection mechanisms is a key area of research.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Knowledge grounding**: Seamlessly integrating retrieved knowledge into the
    generation process is a non-trivial task. RAG models need to understand the retrieved
    knowledge, reason over it, and coherently weave it into the generated text. This
    process requires advanced **NL understanding** (**NLU**) and **NL generation**
    (**NLG**) capabilities, as well as a deep understanding of the context and discourse
    structure. Failure to ground the retrieved knowledge properly can lead to inconsistencies,
    incoherence, or factual errors in the generated output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training objectives**: One of the major limitations of RAG is the lack of
    large-scale supervised datasets for end-to-end training. Creating such datasets
    requires extensive human annotation, which is time-consuming and costly. Additionally,
    defining suitable training objectives that balance the retrieval and generation
    components is challenging. Existing training objectives may not adequately capture
    the complexity of the task, leading to sub-optimal performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Knowledge base construction**: The quality and coverage of the knowledge
    base play a crucial role in the effectiveness of RAG models. Creating broad-coverage
    knowledge bases that span diverse domains and topics is a daunting task. Existing
    knowledge bases may be incomplete, biased, or outdated, limiting the model’s ability
    to retrieve relevant information. Furthermore, ensuring the accuracy and factual
    correctness of the knowledge base is essential but challenging, especially for
    rapidly evolving or controversial topics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-step reasoning**: RAG systems often struggle with combining retrieved
    knowledge across multiple steps to perform complex reasoning or inference tasks.
    Technical domains frequently require multi-step reasoning, such as deriving conclusions
    from multiple premises, following intricate logical chains, or synthesizing information
    from diverse sources. Current RAG systems may lack the capability to effectively
    integrate and reason over retrieved knowledge in a coherent and logical manner,
    limiting their applicability in scenarios involving intricate reasoning processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation**: Evaluating the performance of RAG models is challenging due
    to the complexity of the task. Traditional metrics for text generation, such as
    perplexity or **BiLingual Evaluation Understudy** (**BLEU**) scores, may not adequately
    capture the factual correctness, coherence, and consistency of the generated output.
    Developing robust evaluation methodologies that consider these aspects, as well
    as the quality of the retrieved knowledge, is an open research problem. Readers
    are encouraged to check out Ragas ([https://docs.ragas.io/en/v0.1.6/index.html](https://docs.ragas.io/en/v0.1.6/index.html)),
    which is essentially a framework to assist you evaluate your RAG pipelines at
    scale.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite these limitations, RAG holds significant promise for enhancing the capabilities
    of GenAI models by leveraging external knowledge sources. Addressing these challenges
    will be crucial for the widespread adoption and success of RAG in various applications,
    such as question answering, dialogue systems, and content generation.
  prefs: []
  type: TYPE_NORMAL
- en: Key research priorities going forward include improving retrieval precision,
    developing more sophisticated fusion methods, exploring efficient large-scale
    training techniques, and creating better evaluation benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Future directions**'
  prefs: []
  type: TYPE_NORMAL
- en: Researchers are exploring advanced techniques such as dense passage retrieval,
    learned sparse representations, and hybrid approaches that combine symbolic and
    neural methods. Additionally, incorporating external knowledge sources beyond
    traditional corpora, such as structured databases or knowledge graphs, could significantly
    improve retrieval precision and context understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Developing more sophisticated fusion methods is another critical area of research.
    While current approaches such as retrieval-augmented language models have shown
    promising results, they often rely on simple concatenation or attention mechanisms
    to fuse retrieved information with the language model’s generation. Researchers
    are investigating more advanced fusion techniques that can better capture the
    complex relationships between retrieved knowledge and the generation context,
    potentially leveraging techniques from areas such as multi-modal learning, **graph
    neural networks** (**GNNs**), and neuro-symbolic reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring efficient large-scale training techniques is essential for scaling
    RAG to massive knowledge sources and complex domains. Current systems are often
    trained on relatively small datasets due to computational constraints, limiting
    their ability to effectively leverage vast knowledge repositories. Researchers
    are investigating techniques such as distributed training, knowledge distillation,
    and efficient retrieval indexing to enable training on large-scale knowledge sources
    while maintaining computational feasibility.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, creating better evaluation benchmarks is crucial for accurately assessing
    the performance of RAG systems and driving progress in the field. Existing benchmarks
    often focus on specific tasks or domains, making it challenging to evaluate the
    generalization capabilities of these systems. Researchers are working on developing
    more comprehensive and challenging benchmarks that cover a wider range of knowledge
    sources, domains, and generation tasks, as well as incorporating more sophisticated
    evaluation metrics that go beyond traditional measures such as perplexity or BLEU
    scores.
  prefs: []
  type: TYPE_NORMAL
- en: By addressing these key research priorities, the field of RAG can continue to
    advance, enabling the development of more powerful and versatile language generation
    systems that can effectively leverage vast knowledge repositories to produce high-quality,
    informative, and context-relevant text.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RAG is a rapidly evolving technique that overcomes knowledge limitations in
    neural generative models by conditioning them on relevant external contexts. We
    uncovered how training LLMs with a RAG approach works and how to implement RAG
    with Amazon Bedrock, the LangChain orchestrator, and other GenAI systems. We further
    explored the importance and limitations of RAG approaches in the GenAI realm.
    As indicated, early results across a variety of domains are promising and demonstrate
    the potential of grounding text generation in real-world knowledge. As research
    addresses current limitations, retrieval augmentation could enable GenAI systems
    that are factual, informative, and safe.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will delve into practical applications by employing
    various approaches on Amazon Bedrock. We will commence with a text summarization
    use case, and then explore insights into the methodologies and techniques in depth.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Amazon Bedrock Architecture Patterns'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we will explore various architectural patterns and use cases for
    leveraging the powerful capabilities of Amazon Bedrock. These include text generation,
    building question answering systems, entity extraction, code generation, image
    creation, and developing intelligent agents. In addition, we will dive deep into
    the real-world applications, equipping you with the knowledge and skills to maximize
    Amazon Bedrock’s capabilities in your own projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part contains the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B22045_06.xhtml#_idTextAnchor117), *Generating and Summarizing
    Text with Amazon Bedrock*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B22045_07.xhtml#_idTextAnchor132), *Building Question Answering
    Systems and Conversational Interfaces*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B22045_08.xhtml#_idTextAnchor151), *Extracting Entities and Generating
    Code with Amazon Bedrock*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B22045_09.xhtml#_idTextAnchor171), *Generating and Transforming*
    *Images Using Amazon Bedrock*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B22045_10.xhtml#_idTextAnchor192), *Developing Intelligent Agents
    with Amazon Bedrock*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
