["```py\n    pi@myrobot:~ $ sudo raspi-config\n    ```", "```py\n    pi@myrobot:~ $ sudo reboot\n    ```", "```py\n    raspistill command takes a still image, and the -o parameter tells it to store that image in test.jpg. This command may take a while; taking a still can be slow if light conditions are poor.\n    ```", "```py\n    pi@myrobot:~ $ sudo apt install -y libavcodec58 libilmbase23 libgtk-3-0 libatk1.0-0 libpango-1.0-0 libavutil56 libavformat58 libjasper1 libopenexr23 libswscale5 libpangocairo-1.0-0 libtiff5 libcairo2 libwebp6 libgdk-pixbuf2.0-0 libcairo-gobject2 libhdf5-dev\n    pi@myrobot:~ $ sudo pip3 install \"opencv_python_headless<4.5\" \"opencv_contrib_python_headless<4.5\"\n    ```", "```py\n    pi@myrobot:~ $ echo export LD_PRELOAD=/usr/lib/arm-linux-gnueabihf/libatomic.so.1 >>.bashrc\n    pi@myrobot:~ $ source .bashrc\n    ```", "```py\n    pi@myrobot:~ $ sudo pip3 install flask\n    ```", "```py\n    pi@myrobot:~ $ sudo apt install -y libgfortran5 libatlas3-base\n    pi@myrobot:~ $ sudo pip3 install numpy\n    ```", "```py\n    pi@myrobot:~ $ sudo pip3 install picamera[array]\n    ```", "```py\n    PiCamera code needed to access our camera. cv2 is OpenCV, the computer vision library used to process the images. Here, NumPy is *aliased*, or nicknamed, np. \n    ```", "```py\n    encode parameter. \n    ```", "```py\n    def setup_camera():\n        camera = PiCamera()\n        camera.resolution = size\n        camera.rotation = 180\n        return camera\n    ```", "```py\n    PiRGBArray instance, a type for storing RGB images. We then set up the stream of data with capture_continuous, a picamera method to take photos repeatedly. We pass it to the image store and tell it to format the output data as bgr (blue, green, red), which is how OpenCV stores color data. The last parameter to this is use_video_port, which, when set to true, results in a reduction in image quality in exchange for faster production of frames. \n    ```", "```py\n    def get_encoded_bytes_for_frame(frame):\n        result, encoded_image = cv2.imencode('.jpg', frame, encode_param)\n        return encoded_image.tostring()\n    ```", "```py\n    Flask app object, which handles routing; a way to render templates into output; and a way to make our web app response. We import the camera_stream library we've just made, and we import time so we can limit the frame rate to something sensible. After the imports, we create a Flask app object for us to register everything with.\n    ```", "```py\n    '/' route will be the index page, what you get by default if you just land on the robot's app server. Our function renders a template, which we'll write in the next section. \n    ```", "```py\n    time.sleep is here because we need to let the camera warm up after turning it on. Otherwise, we may not get usable frames from it. \n    ```", "```py\n    start_stream, encoding each frame to JPG. \n    ```", "```py\n    b in front of this string to tell Python to treat this as raw bytes and not perform further encoding on the information. The \\r and \\n items are raw line-ending characters. That completes the frame_generator function. \n    ```", "```py\n    display route generates a response from frame_generator. As that is a generator, Flask will keep consuming items from that generator and sending those parts to the browser. The response also specifies a content type with a boundary between items. This boundary must be a string of characters. We have used `frame`. The boundary must match in `mimetype` and the boundary (`--frame`) in the content (*step 5*).\n    ```", "```py\n    app.run(host=\"0.0.0.0\", debug=True, port=5001)\n    ```", "```py\n    <html>\n        <head>\n            <title>Robot Image Server</title>\n        </head>\n        <body>\n            <h1>Robot Image Server</h1>\n    ```", "```py\n    url_for here. Flask can use a template renderer, Jinja, to insert the URL from a route in Flask by its function name.\n    ```", "```py\n        </body>\n    </html>\n    ```", "```py\n    Queue and Process to create the process and communicate with it. We then use the same imports for Flask that we used previously. Note—we are *not* importing any of the camera parts in this module.\n    ```", "```py\n    app = Flask(__name__)\n    control_queue = Queue()\n    display_queue = Queue(maxsize=2)\n    ```", "```py\n    display_template = 'image_server.html'\n    ```", "```py\n    @app.route('/')\n    def index():\n        return render_template(display_template)\n    ```", "```py\n    def frame_generator():\n        while True:\n            time.sleep(0.05)\n    ```", "```py\n            encoded_bytes = display_queue.get()\n            yield (b'--frame\\r\\n'\n                    b'Content-Type: image/jpeg\\r\\n\\r\\n' + encoded_bytes + b'\\r\\n')\n    ```", "```py\n    @app.route('/display')\n    def display():\n        return Response(frame_generator(),\n            mimetype='multipart/x-mixed-replace; boundary=frame')\n    ```", "```py\n    @app.route('/control', methods=['POST'])\n    def control():\n        control_queue.put(request.form)\n        return Response('queued')\n    ```", "```py\n    template_name in the global display_template. The preceding index route uses the template. Instead of calling app.run, we create a Process object. The Process parameter target is a function to run (app.run), and some parameters need to be given to that function (the host and port settings). We then start the server process and return the process handle so our code can stop it later.\n    ```", "```py\n    def put_output_image(encoded_bytes):\n        if display_queue.empty():\n            display_queue.put(encoded_bytes)\n    ```", "```py\n    def get_control_instruction():\n        if control_queue.empty():\n            return None\n        else:\n            return control_queue.get()\n    ```", "```py\n    import time\n    from image_app_core import start_server_process, get_control_instruction, put_output_image\n    import camera_stream\n    ```", "```py\n    def controlled_image_server_behavior():\n        camera = camera_stream.setup_camera()\n        time.sleep(0.1)\n    ```", "```py\n        for frame in camera_stream.start_stream(camera):\n            encoded_bytes = camera_stream.get_encoded_bytes_for_frame(frame)\n            put_output_image(encoded_bytes)\n    ```", "```py\n    return to stop the behavior when it receives the exit instruction from the control queue. \n    ```", "```py\n    process = start_server_process('control_image_behavior.html')\n    try:\n        controlled_image_server_behavior()\n    finally:\n        process.terminate()\n    ```", "```py\n<html>\n    <head>\n        <script src=\"img/jquery-3.3.1.min.js\"></script>\n        <title>Robot Image Server</title>\n    </head>\n    <body>\n        <h1>Robot Image Server</h1>\n        <img src=\"img/{{ url_for('display') }}\"><br>\n        <a href=\"#\" onclick=\"$.post('/control', {'command': 'exit'}); \">Exit</a>\n    </body>\n</html>\n```", "```py\n    class PIController(object):\n        def __init__(self, proportional_constant=0, integral_constant=0, windup_limit=None):\n            self.proportional_constant = proportional_constant\n            self.integral_constant = integral_constant\n            self.windup_limit = windup_limit\n            self.integral_sum = 0\n    ```", "```py\n        def handle_integral(self, error):\n            if self.windup_limit is None or \\\n                    (abs(self.integral_sum) < self.windup_limit) or \\\n                    ((error > 0) != (self.integral_sum > 0)):\n                self.integral_sum += error\n            return self.integral_constant * self.integral_sum\n    ```", "```py\n        def reset(self):\n            self.integral_sum = 0\n    ```", "```py\n            <img src=\"img/{{ url_for('display') }}\"><br>\n            <a href=\"#\" onclick=\"$.post('/control', {'command': 'start'});\">Start</a>\n            <a href=\"#\" onclick=\"$.post('/control', {'command': 'stop'})\">Stop</a><br>\n            <a href=\"#\" onclick=\"$.post('/control', {'command': 'exit'});\">Exit</a>\n    ```", "```py\n    import time\n    from image_app_core import start_server_process, get_control_instruction, put_output_image\n    import cv2\n    import numpy as np\n    import camera_stream\n    from pid_controller import PIController\n    from robot import Robot\n    ```", "```py\n    class ColorTrackingBehavior:\n        def __init__(self, robot):\n            self.robot = robot\n    ```", "```py\n    low_range and high_range values for the color filter (as seen in *Figure 13.13*). Colors that lie between these HSV ranges would be white in the masked image. Our hue is 25 to 80, which correspond to 50 to 160 degrees on a hue wheel. Saturation is 70 to 255—any lower and we'd start to detect washed out or gray colors. Light is 25 (very dark) to 255 (fully lit).The `correct_radius` value sets the size we intend to keep the object at and behaves as a distance setting. `center` should be half the horizontal resolution of the pictures we capture.\n    ```", "```py\n            self.running = False\n    ```", "```py\n    start, stop, and exit buttons. It uses the running variable to start or stop the robot moving. \n    ```", "```py\n        def find_object(self, original_frame):\n            \"\"\"Find the largest enclosing circle for all contours in a masked image.\n            Returns: the masked image, the object coordinates, the object radius\"\"\"\n    ```", "```py\n            frame_hsv = cv2.cvtColor(original_frame, cv2.COLOR_BGR2HSV)\n            masked = cv2.inRange(frame_hsv, self.low_range, self.high_range)\n    ```", "```py\n    RETR_LIST. OpenCV is capable of more detailed types, but they take more time to process. The last parameter is the method used to find the contours. We use the `CHAIN_APPROX_SIMPLE` method to simplify the outline to an approximate chain of points, such as four points for a rectangle. Note the `_` in the return values; there is optionally a hierarchy returned here, but we neither want nor use it. The `_` means ignore the hierarchy return value.\n    ```", "```py\n    cv2 returns each circle as a radius and coordinates—exactly what we want. \n    ```", "```py\n    largest value of 0, and then we loop through the circles. If the circle has a radius larger than the circle we last stored, we replace the stored circle with the current circle. We also convert the values to int here, as minEnclosingCircle produces non-integer floating-point numbers.\n    ```", "```py\n            return masked, largest[0], largest[1]\n    ```", "```py\n    np.concatenate function to join the two images, which are equivalent to NumPy arrays. You could change the axis parameter to 0 if you wanted screens stacked vertically instead of horizontally.\n    ```", "```py\n    cvtColor to change the masked image to a three-channel image—the original frame and processed frame must use the same color system to join them into a display. We use cv2.circle to draw a circle around the tracked object on the original frame so we can see what our robot has tracked on the web app, too.\n    ```", "```py\n        def run(self):\n            self.robot.set_pan(0)\n            self.robot.set_tilt(0)\n            camera = camera_stream.setup_camera()\n    ```", "```py\n    speed_pid = PIController(proportional_constant=0.8, \n                integral_constant=0.1, windup_limit=100)\n    direction_pid = PIController(proportional_constant=0.25, \n                integral_constant=0.05, windup_limit=400)\n    ```", "```py\n            time.sleep(0.1)\n            self.robot.servos.stop_all()\n    ```", "```py\n            print(\"Setup Complete\")\n            print('Radius, Radius error, speed value, direction error, direction value')\n    ```", "```py\n            for frame in camera_stream.start_stream(camera):\n                (x, y), radius = self.process_frame(frame)\n    ```", "```py\n                self.process_control()\n                if self.running and radius > 20:\n    ```", "```py\n                   radius_error = self.correct_radius - radius\n                    speed_value = speed_pid.get_value(radius_error)\n    ```", "```py\n                    direction_error = self.center - x\n                    direction_value = direction_pid.get_value(direction_error)\n    ```", "```py\n                    print(f\"{radius}, {radius_error}, {speed_value:.2f}, {direction_error}, {direction_value:.2f}\")\n    ```", "```py\n                    self.robot.set_left(speed_value - direction_value)\n                    self.robot.set_right(speed_value + direction_value)\n    ```", "```py\n                else:\n                    self.robot.stop_motors()\n                    if not self.running:\n                        speed_pid.reset()\n                        direction_pid.reset()\n    ```", "```py\n    print(\"Setting up\")\n    behavior = ColorTrackingBehavior(Robot())\n    process = start_server_process('color_track_behavior.html')\n    try:\n        behavior.run()\n    finally:\n        process.terminate()\n    ```", "```py\n    import time\n    from image_app_core import start_server_process, get_control_instruction, put_output_image\n    import cv2\n    import os\n    import camera_stream\n    from pid_controller import PIController\n    from robot import Robot\n    ```", "```py\n    class FaceTrackBehavior:\n        def __init__(self, robot):\n            self.robot = robot\n            cascade_path = \"/usr/local/lib/python3.7/dist-packages/cv2/data/haarcascade_frontalface_default.xml\"\n            assert os.path.exists(cascade_path), f\"File {cascade_path} not found\"\n            self.cascade = cv2.CascadeClassifier(cascade_path)\n    ```", "```py\n            self.center_x = 160\n            self.center_y = 120\n            self.min_size = 20\n            self.pan_pid = PIController(proportional_constant=0.1, integral_constant=0.03)\n            self.tilt_pid = PIController(proportional_constant=-0.1, integral_constant=-0.03)\n    ```", "```py\n            self.running = False\n    ```", "```py\n        def process_control(self):\n            instruction = get_control_instruction()\n            if instruction:\n                command = instruction['command']\n                if command == \"start\":\n                    self.running = True\n                elif command == \"stop\":\n                    self.running = False\n                    self.pan_pid.reset()\n                    self.tilt_pid.reset()\n                    self.robot.servos.stop_all()\n                elif command == \"exit\":\n                    print(\"Stopping\")\n                    exit()\n    ```", "```py\n        def find_object(self, original_frame):\n            gray_img = cv2.cvtColor(original_frame, cv2.COLOR_BGR2GRAY)\n    ```", "```py\n    detectMultiScale method creates the integral image and applies the Haar cascade algorithm. It will return several objects as rectangles, with x, y, width, and height values. \n    ```", "```py\n    largest = 0, (0, 0, 0, 0) \n            for (x, y, w, h) in objects:\n                item_area = w * h\n                if item_area > largest[0]:\n                    largest = item_area, (x, y, w, h)\n    ```", "```py\n            return largest[1]\n    ```", "```py\n        def make_display(self, display_frame):\n            encoded_bytes = camera_stream.get_encoded_bytes_for_frame(display_frame)\n            put_output_image(encoded_bytes)\n    ```", "```py\n        def process_frame(self, frame):\n            (x, y, w, h) = self.find_object(frame)\n            cv2.rectangle(frame, (x, y), (x + w, y + w), [255, 0, 0])\n            self.make_display(frame)\n            return x, y, w, h\n    ```", "```py\n        def run(self):\n            camera = camera_stream.setup_camera()\n            time.sleep(0.1)\n            print(\"Setup Complete\")\n    ```", "```py\n            for frame in camera_stream.start_stream(camera):\n                (x, y, w, h) = self.process_frame(frame)\n                self.process_control()\n    ```", "```py\n                if self.running and h > self.min_size:\n    ```", "```py\n                    pan_error = self.center_x - (x + (w / 2))\n                    pan_value = self.pan_pid.get_value(pan_error)\n                    self.robot.set_pan(int(pan_value))\n                    tilt_error = self.center_y - (y + (h /2))\n                    tilt_value = self.tilt_pid.get_value(tilt_error)\n                    self.robot.set_tilt(int(tilt_value))\n    ```", "```py\n                    print(f\"x: {x}, y: {y}, pan_error: {pan_error}, tilt_error: {tilt_error}, pan_value: {pan_value:.2f}, tilt_value: {tilt_value:.2f}\")\n    ```", "```py\n    print(\"Setting up\")\n    behavior = FaceTrackBehavior(Robot())\n    process = start_server_process('color_track_behavior.html')\n    try:\n        behavior.run()\n    finally:\n        process.terminate()\n    ```", "```py\n    $ find /usr/ -iname \"haarcas*\"\n    ```"]