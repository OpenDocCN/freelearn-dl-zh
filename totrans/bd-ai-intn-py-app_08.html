<html><head></head><body>
		<div><h1 class="chapter-number" id="_idParaDest-141"><a id="_idTextAnchor180"/>8</h1>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor181"/>Implementing Vector Search in AI Applications</h1>
			<p>Vector search is revolutionizing the way people interact with data in AI applications. MongoDB Atlas Vector Search allows developers to implement sophisticated search capabilities that understand the nuances of discovery and retrieval. It works by converting text, video, image, or audio files into numerical vector representations, which can then be stored and searched efficiently. MongoDB Atlas can perform similarity searches alongside your operational data, making it an essential tool for enhancing user experience in applications ranging from e-commerce to content discovery. With MongoDB Atlas, setting up vector search is streamlined, enabling developers to focus on creating dynamic, responsive, and intelligent applications.</p>
			<p>In this chapter, you will learn how to use the Vector Search feature of MongoDB Atlas to build intelligent applications. You will learn how to build <strong class="bold">retrieval-augmented generation</strong> (<strong class="bold">RAG</strong>) architecture systems and delve deeper into the understanding and development of various patterns of complex RAG architectures with MongoDB Atlas, unraveling the synergies that underpin their joint value and potential. Through real-world use cases and practical demonstrations, you will learn how this dynamic duo can seamlessly transform businesses across industries, driving efficiency, accuracy, and operational excellence.</p>
			<p>This chapter covers the following topics:</p>
			<ul>
				<li>Leverage vector search and full-text search with MongoDB Atlas, which will later help you build a robust retriever for RAG</li>
				<li>Understand the various components involved in the development of a RAG system</li>
				<li>Learn about the process and steps involved in the development of simple RAG and advanced RAG systems.</li>
			</ul>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor182"/>Technical requirements</h1>
			<p>This chapter assumes that you have at least beginner-level expertise in Python coding. To follow along with the demos, you’ll need to set up your development environment by completing the following steps:</p>
			<ol>
				<li>Install either <code>python@3.9</code> or <code>python@3.11</code> on the operating system of your choice.</li>
				<li>Set up a Python virtual environment and activate it:<pre class="source-code">
$ python3 -m venv venv
$ source venv/bin/activate</pre></li>				<li>You will be using the following packages to develop the demo described in this chapter:<ul><li><code>pandas</code>: Helps with data preprocessing and handling</li><li><code>numpy</code>: Handles numerical data</li><li><code>openai</code>: For the embedding model and invoking the LLM</li><li><code>pymongo</code>: For the MongoDB Atlas vector store and full-text search</li><li><code>s3fs</code>: Allows loading data directly from an S3 bucket</li><li><code>langchain_mongodb</code>: Enables vector search in MongoDB Atlas using a LangChain wrapper</li><li><code>langchain</code>: Used to build a RAG application</li><li><code>langchain-openai</code>: Enables you to interact with OpenAI chat models</li><li><code>boto3</code>: Enables you to interact with AWS s3 buckets</li><li><code>python-dotenv:</code> Enables you to load environment variables from a <code>.</code><code>env</code> file</li></ul><p class="list-inset">To install the mentioned packages in your Python virtual environment, run the following command:</p><pre class="source-code">
pip3 install langchain==0.2.14 langchain-community==0.2.12 langchain-core==0.2.33 langchain-mongodb==0.1.8 langchain-openai==0.1.22 langchain-text-splitters==0.2.2 numpy==1.26.4 openai==1.41.1 s3fs==2024.6.1 pymongo==4.8.0 pandas==2.2.2 boto3==1.35.2 python-dotenv==1.0.1</pre><p class="list-inset">You will also need to know how to set up and run JupyterLab or Jupyter Notebook.</p></li>			</ol>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor183"/>Information retrieval with MongoDB Atlas Vector Search</h1>
			<p>Information retrieval is a critical component of RAG systems. It enhances the accuracy and relevance of the generated text by sourcing information from extensive knowledge bases. This process allows the RAG system to produce responses that are not only precise but also deeply rooted in factual content, making it a powerful tool for various <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) tasks. By effectively combining retrieval with generation, RAG addresses challenges related to bias and misinformation, contributing to the advancement of AI-related applications and tasks.</p>
			<p>In the context of information retrieval, it’s essential to distinguish between <em class="italic">relevance</em> and <em class="italic">similarity</em>. While <strong class="bold">similarity</strong> focuses on word matching, <strong class="bold">relevance</strong> is about the interconnectedness of ideas. While a vector database query can help identify semantically related content, more advanced tools are needed to accurately retrieve relevant information.</p>
			<p>In <em class="italic">Chapter 5</em>, <em class="italic">Vector Databases</em>, you learned about MongoDB Atlas Vector Search and how it enhances the retrieval of relevant information by allowing the creation and indexing of vector embeddings, which can be generated using machine learning models, such as embedding models. This facilitates semantic search capabilities, enabling the identification of content that is contextually similar rather than just being keyword based. Full-text search complements this by providing robust text search capabilities that can handle typos, synonyms, and other variations in text, ensuring that searches return the most pertinent results. Together, these tools provide a comprehensive search solution that can discern and retrieve information based on both the similarity of terms and the relevance of the content.</p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor184"/>Vector search tutorial in Python</h2>
			<p>With the help of an example, let’s see how to load a small dataset in MongoDB to perform a vector search along with full-text search to perform information retrieval. For this demonstration, you will load a sample movie dataset from an S3 bucket:</p>
			<ol>
				<li>Write a simple Python function to accept search terms or phrases and pass it through the embeddings API again to get a query vector.</li>
				<li>Take the resultant query vector embeddings and perform a vector search query using the <a href="https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/"><code>$vectorsearch</code></a> operator in the MongoDB aggregation pipeline.</li>
				<li>Pre-filter the documents using meta information to narrow the search across your dataset, thereby speeding up the performance of the vector search results while retaining accuracy.</li>
				<li>Further, post-filter the retrieved documents that are semantically similar (based on relevancy score), if you want to demonstrate a higher degree of control over the semantic search behavior.</li>
				<li>Initialize the OpenAI API key and MongoDB connection string:<pre class="source-code">
import os
import getpass
# set openai api key
try:
    openai_api_key = os.environ["OPENAI_API_KEY"]
except KeyError:
    openai_api_key = getpass.getpass("Please enter your OPENAI API KEY (hit enter): ")
# Set MongoDB Atlas connection string
try:
    MONGO_CONN_STR = os.environ["MONGODB_CONNECTION_STR"]
except KeyError:
    MONGO_CONN = getpass.getpass("Please enter your MongoDB Atlas Connection String (hit enter): ")</pre></li>				<li>Now, load the dataset from the S3 bucket. Run the following lines of code in Jupyter Notebook to read data from an AWS S3 bucket directly to a <code>pandas</code> DataFrame:<pre class="source-code">
import pandas as pd
import s3fs
df = pd.read_json("https://ashwin-partner-bucket.s3.eu-west-1.amazonaws.com/movies_sample_dataset.jsonl", orient="records", lines=True)
df.to_json("./movies_sample_dataset.jsonl", orient="records", lines=True)
df[:3]</pre><p class="list-inset">On executing the preceding snippet of code, you should see the following result in your Jupyter Notebook cell.</p></li>			</ol>
			<div><div><img alt="" role="presentation" src="img/B22495_08_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1: Sample movies data view</p>
			<ol>
				<li value="7">Initialize and run an embedding job to embed the <code>sample_movies</code> dataset. In the following code example, you create a <code>final</code> field, which is a field derived from the <code>text</code> and <code>overview</code> fields that are already available in the dataset.</li>
				<li>Next, run this <code>final</code> field against the embedding API from OpenAI, as shown here:<pre class="source-code">
import numpy as np
from tqdm import tqdm
import openai
df['final'] = df['text'] + "    Overview: " + df['overview']
df['final'][:5]
step = int(np.ceil(df['final'].shape[0]/100))
embeddings_t = []
lines = []
# Note that we must split the dataset into smaller batches to not exceed the rate limits imposed by OpenAI API's.
for x, y in list(map(lambda x: (x, x+step), list(range(0, df.shape[0], step)))):
    lines += [df.final.values[x:y].tolist()]
for i in tqdm(lines):
    embeddings_t += openai.embeddings.create(
        model='text-embedding-ada-002', input=i).data
out = []
for ele in embeddings_t:
    out += [ele.embedding]
df['embedding'] = out
df[:5]</pre><p class="list-inset">You should see that the <code>sample_movies</code> dataset is enriched with the OpenAI embeddings in the <code>embedding</code> field, as shown in <em class="italic">Figure 8</em><em class="italic">.2</em>.</p></li>			</ol>
			<div><div><img alt="" role="presentation" src="img/B22495_08_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2: Sample movies dataset view with OpenAI embeddings</p>
			<ol>
				<li value="9">Next, initialize MongoDB Atlas and insert data into a MongoDB collection.</li>
				<li>Now that you have created the vector embeddings for your <code>sample_movies</code> dataset, you can initialize the MongoDB client and insert the documents into your collection of choice by running the following lines of code:<pre class="source-code">
from pymongo import MongoClient
import osmongo_client = MongoClient(os.environ["MONGODB_CONNECTION_STR"])
# Upload documents along with vector embeddings to MongoDB Atlas Collection
output_collection = mongo_client["sample_movies"]["embed_movies"]
if output_collection.count_documents({})&gt;0:
    output_collection.delete_many({})
_ = output_collection.insert_many(df.to_dict("records"))</pre><p class="list-inset">You have ingested the test data to build a vector search capability. Now, let’s proceed to build a vector search index in the following steps.</p></li>				<li>Let’s first create vector index definitions. You can create a vector search index in the MongoDB Atlas Vector Search UI by following the steps explained in <em class="italic">Chapter 5</em>, <em class="italic">Vector Databases</em>. The vector index required for this demo tutorial is provided here:<pre class="source-code">
{
    "fields": [
      {
        "type": "vector",
        "numDimensions": 1536,
        "path": "embedding",
        "similarity": "cosine"
      },
      {
        "type": "filter",
        "path": "year"
      },
    ]
}</pre><p class="list-inset">Once the vector index definitions are added under the Vector Search index JSON editor in the MongoDB Atlas UI, the process for creating a vector search index is triggered and the vector search index is created at the specified <code>path</code> field mentioned in the vector index definition. Now, you are ready to perform vector search queries on the <code>sample_movies.embed_movies</code> collection in MongoDB Atlas where all the data is stored, and create vector indexes.</p><p class="list-inset">Let’s equip the vector search or the retriever API to use in your RAG framework.</p></li>				<li>You can query a MongoDB vector index using <code>$vectorSearch</code>. MongoDB Atlas brings the flexibility of using vector search alongside search filters. Additionally, you can apply range, string, and numeric filters using the aggregation pipeline. This allows the end user to control the behavior of the semantic search response from the search engine.<p class="list-inset">The following code example demonstrates how you can perform vector search along with pre-filtering on the <code>year</code> field to get movies released post <code>1990</code>. To have better control over the relevance of returned results, you can perform post-filtering on the response using the MongoDB Query API.</p><p class="list-inset">The following code demonstrates how you can perform these steps:</p><ol><li class="upper-roman">Represent a raw text query as a vector embedding. There are multiple embedding models currently available with OpenAI, such as <code>text-embedding-3-small</code>, <code>text-embedding-3-large</code> with variable dimensions, and the <code>text-embedding-ada-002</code> model.</li><li class="upper-roman">Build and perform a vector search query to MongoDB Atlas.</li><li class="upper-roman">Perform pre-filtering before performing a vector search on the <code>year</code> field.</li><li class="upper-roman">Perform post-filtering using the <code>score</code> field to better control the relevancy and accuracy of the returned results.</li></ol><p class="list-inset">Run the following code to initialize a function that can help you achieve vector search, pre-filter, and post-filter:</p><pre class="source-code">
def query_vector_search(q, prefilter = {}, postfilter = {},path="embedding",topK=2):
    ele = openai.embeddings.create(model='text-embedding-ada-002', input=q).data
    query_embedding = ele[0].embedding
    vs_query = {
                "index": "default",
                "path": path,
                "queryVector": query_embedding,
                "numCandidates": 10,
                "limit": topK,
            }
    if len(prefilter)&gt;0:
        vs_query["filter"] = prefilter
    new_search_query = {"$vectorSearch": vs_query}
    project = {"$project": {"score": {"$meta": "vectorSearchScore"},"_id": 0,"title": 1, "release_date": 1, "overview": 1,"year": 1}}
    if len(postfilter.keys())&gt;0:
        postFilter = {"$match":postfilter}
        res = list(output_collection.aggregate([new_search_query, project, postFilter]))
    else:
        res = list(output_collection.aggregate([new_search_query, project]))
    return res
query_vector_search("I like Christmas movies, any recommendations for movies release after 1990?", prefilter={"year": {"$gt": 1990}}, topK=5)</pre><p class="list-inset">You should get the following result:</pre></li>			</ol>
			<div><div><img alt="" role="presentation" src="img/B22495_08_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3: Sample result from running the vector search query with pre-filters</p>
			<p class="list-inset">This is a sample query with <code>year</code> as a pre-filter and a <code>score</code>-based post-filter to retain only the relevant results:</p>
			<pre class="source-code">
query_vector_search("I like Christmas movies, any recommendations for movies release after 1990?", prefilter={"year":{"$gt": 1990}}, postfilter= {"score": {"$gt":0.905}},topK=5)</pre>			<p class="list-inset">You should get the following result:</p>
			<div><div><img alt="" role="presentation" src="img/B22495_08_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4: Sample result from running the vector search query with a pre-filter and post-filter</p>
			<p>With this Python method, you were able to filter on the <code>score</code> field and the <code>year</code> field to generate results as well as results for vector similarity. Using a heuristic, you were able to control the accuracy of the results to retain only the most relevant documents and were also able to apply a range filter query (on the <code>year</code> field).</p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor185"/>Vector Search tutorial with LangChain</h2>
			<p>Utilizing <strong class="bold">LangChain</strong> with MongoDB Atlas Vector Search for building a semantic similarity retriever offers several advantages. The following example demonstrates how to carry out a vector similarity search using LangChain wrapper classes:</p>
			<pre class="source-code">
from langchain_mongodb.vectorstores import MongoDBAtlasVectorSearch
from langchain_openai import OpenAIEmbeddings
import json
embedding_model = OpenAIEmbeddings(model="text-embedding-ada-002")
vector_search = MongoDBAtlasVectorSearch(output_collection, embedding_model, text_key='final')
fquery = {"year": {"$gt": 1990}}
search_kwargs = {
    "k": 5,
    'filter': fquery,
}
retriever = vector_search.as_retriever(search_kwargs=search_kwargs)
docs = retriever.invoke("I like Christmas movies, any recommendations for movies release after 1990?")
for doc in docs:
    foo = {}
    foo['title'] = doc.metadata['title']
    foo['year'] = doc.metadata['year']
    foo['final'] = doc.metadata['text']
    print(json.dumps(foo,indent=1))</pre>			<p>Here’s the result:</p>
			<div><div><img alt="" role="presentation" src="img/B22495_08_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5: Sample result vector search query using the LangChain module for MongoDB</p>
			<p>This demonstrates a more sophisticated yet simple approach that is particularly beneficial for developers creating RAG applications. The LangChain framework offers a suite of APIs and wrapper classes that can be used to integrate with various serverless LLM providers, such as OpenAI, and talk to MongoDB Atlas Vector Search to build RAG frameworks with very few lines of code. It is also easy to maintain and scale.</p>
			<p>In this section, you were able to build and perform vector similarity search using MongoDB Atlas. You developed reusable wrapper classes and functions that will be useful in developing a more sophisticated application, such as a chatbot.</p>
			<p>Now, let’s delve deep into understanding what RAG architectures are and how to develop one using the resources that you’ve created so far.</p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor186"/>Building RAG architecture systems</h1>
			<p>In the dynamic landscape of modern business, the relentless pursuit of efficiency and accuracy urges organizations to adopt cutting-edge technologies. Among these, automation stands as a cornerstone, particularly in processing and automating workflows. However, traditional methods suffer when they’re subjected to large volumes of data with intricate tasks, and human-led processes often fall short due to error-prone manual interventions.</p>
			<p>This section explores the transformative landscape of automation, discussing the pivotal role RAG plays in revolutionizing business operations. MongoDB, known for its prowess in data management and flexible schemas, offers a compelling synergy with RAG through its vector search and full-text search capabilities. Delving into the architectural details of RAG, this section dissects its constituent building blocks, offering practical insights into constructing automated document-processing workflows that harness the full potential of LLMs and MongoDB Vector Search.</p>
			<div><div><img alt="" role="presentation" src="img/B22495_08_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6: Building blocks of RAG architecture</p>
			<p>Let’s go over the key components of the RAG architecture in detail:</p>
			<ol>
				<li><strong class="bold">Document loading</strong>: Initially, documents are loaded from data storage. This involves text extraction, parsing, formatting, and cleaning to prepare the data for document splitting.</li>
				<li><strong class="bold">Document splitting</strong>: The next step is to break down the documents into smaller, manageable segments or chunks. Strategies for splitting can vary, from fixed-size chunking to content-aware chunking that considers the content structure.</li>
				<li><strong class="bold">Text embedding</strong>: These document chunks are then transformed into vector representations (embeddings) using techniques such as <strong class="bold">OpenAIEmbeddings</strong>, <strong class="bold">Sentence e-BERT</strong>, and <strong class="bold">Instructor Embeddings</strong>. This step is crucial for understanding the semantic content of the chunks.</li>
				<li><strong class="bold">Vector store</strong>: The generated vectors, each associated with unique document chunks, are stored in a vector store alongside the document chunks and other metadata extracted from the MongoDB Atlas collection. Atlas Vector Search indexes and Apache Lucene search can be built through the<strong class="bold"> MongoDB Atlas UI</strong> for easy and fast retrieval.</li>
				<li><strong class="bold">Query processing</strong>: When a user submits a query, it is also converted into a vector representation using the same embedding technique as mentioned in <em class="italic">Step 3</em>.</li>
				<li><strong class="bold">Document retrieval</strong>: The retriever component locates and fetches document chunks that are semantically like the query. This retrieval process employs vector similarity search techniques and MongoDB Atlas using the <strong class="bold">Hierarchical Navigable Small Worlds</strong> (<strong class="bold">HNSW)</strong> algorithm to perform a fast nearest neighbor search to retrieve relevant documents without compromising the accuracy of the retrieved search results.</li>
				<li><strong class="bold">Document chunk post-filtering</strong>: The relevant document chunks are retrieved from the MongoDB collection with the help of the <strong class="bold">Unified Query API</strong> and can be post-filtered easily to transform the output document chunks into the required format.</li>
				<li><strong class="bold">LLM prompt creation</strong>: The retrieved document chunks and the query are combined to create a context and prompt for the LLM.</li>
				<li><strong class="bold">Answer generation</strong>: Finally, the LLM generates a response based on the prompt, completing the RAG process.</li>
			</ol>
			<p>In the context of RAG systems, there are two primary types: <strong class="bold">simple</strong> (<strong class="bold">or naive</strong>) <strong class="bold">RAG</strong> and <strong class="bold">advanced</strong> <strong class="bold">RAG</strong>. In practical scenarios, this classification helps address different types of personas and questions the applications are handling, and it’s common to encounter both simple and complex RAG queries within the same workflow and from the same persona. As a developer, it is important to reason out the functionalities that the application is expected to serve before deciding on the building blocks involved in the RAG architecture.</p>
			<p>When building your RAG architecture system, consider the following points to help with programming and planning:</p>
			<ul>
				<li><strong class="bold">Workflow specificity</strong>: Define the specific workflow you intend to automate with RAG; it may be related to <strong class="bold">question answering</strong> (<strong class="bold">QA</strong>), data augmentation, summarization, reasoning, or assertion. Maybe your customers frequently ask a specific set of three or four types of queries.</li>
				<li><strong class="bold">User experience</strong>: Collaborate with your target user group to understand the types of queries they are likely to ask to identify the user group journey, which might be a simple single-state response or a multi-state chat flow.</li>
				<li><strong class="bold">Data sources</strong>: First, identify the nature of your data source—whether it’s unstructured or structured. Next, map the locations of these data sources. Once you’ve done that, classify the data based on whether it serves operational or analytical purposes. Finally, observe the data patterns to determine whether answers are readily available in one location or if you’ll need to gather information from multiple sources.</li>
			</ul>
			<p>These pointers will help you determine whether you need to go for a simple RAG system or an advanced RAG system and also help you to determine the essential building blocks to consider while constructing your RAG architecture.</p>
			<p>Now, let’s delve deeper into the building blocks of this architecture with some code examples to better explain the nuances. However, before you develop RAG-powered applications, let’s look at the fundamentals of how to process source documents to maximize the accuracy of the rated responses from the RAG application. The following strategies will come in handy while processing documents before storing them in a MongoDB Atlas collection.</p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor187"/>Chunking or document-splitting strategies</h2>
			<p><strong class="bold">Chunking</strong> or <strong class="bold">document splitting</strong> is a critical step in handling extensive texts within RAG systems. When dealing with large documents, the token limits imposed by language models (such as <strong class="bold">gpt-3.5-turbo</strong>) necessitate breaking them into manageable chunks. However, a naive fixed-chunk-size approach can lead to fragmented sentences across chunks, affecting subsequent tasks such as QA.</p>
			<p>To address this, consider semantics when dividing documents. Most segmentation algorithms use chunk size and overlap principles. <strong class="bold">Chunk size</strong> (measured by characters, words, or tokens) determines segment length, while <strong class="bold">overlaps</strong> ensure continuity by sharing context between adjacent chunks. This approach preserves semantic context and enhances RAG system performance.</p>
			<p>Now, let’s delve into the intricacies of document-splitting techniques, particularly focusing on content-aware chunking. While fixed-size chunking with overlap is straightforward and computationally efficient, more sophisticated methods enhance the quality of text segmentation. The following are the various document-splitting techniques:</p>
			<ul>
				<li><strong class="bold">Recursive chunking</strong>: This technique includes the following approaches:<ul><li><strong class="bold">Hierarchical approach</strong>: Recursive chunking breaks down input text into smaller chunks iteratively. It operates hierarchically, using different separators or criteria at each level.</li><li><strong class="bold">Customizable structure</strong>: By adjusting the criteria, you can achieve the desired chunk size or structure. Recursive chunking adapts well to varying document lengths.</li></ul></li>
				<li><strong class="bold">Sentence splitting</strong>: Sentence splitting involves various strategies, such as the ones listed here:<ul><li><strong class="bold">Naive splitting</strong>: This method relies on basic punctuation marks (such as periods and new lines) to divide text into sentences. While simple, it might not handle complex sentence structures well.</li><li><strong class="bold">spaCy</strong>: Another robust NLP library, spaCy, offers accurate sentence segmentation. It uses statistical models and linguistic rules.</li><li><strong class="bold">Natural Language Toolkit (NLTK)</strong>: NLTK, a powerful Python library for NLP, provides efficient sentence tokenization. It considers context and punctuation patterns.</li><li><strong class="bold">Advanced tools</strong>: Some tools employ smaller models to predict sentence boundaries, ensuring precise divisions.</li></ul></li>
				<li><strong class="bold">Specialized techniques</strong>: Specialized techniques include the following:<ul><li><strong class="bold">Structured content</strong>: For documents with specific formats (e.g., Markdown, LaTeX), specialized techniques come into play.</li><li><strong class="bold">Intelligent division</strong>: These methods analyze the content’s structure and hierarchy. They create semantically coherent chunks by understanding headings, lists, and other formatting cues.</li></ul></li>
			</ul>
			<p>In summary, while fixed-size chunking serves as a baseline, content-aware techniques consider semantics, context, and formatting intricacies. Choosing the right method depends on your data’s unique characteristics and the requirements of your RAG system. While choosing the retriever for storing and retrieving these chunks, you may want to consider solutions such as document hierarchies and knowledge graphs. MongoDB Atlas has a flexible schema and a simple unified query API to query data from it.</p>
			<p>Now let’s use the recursive document-splitting strategy to build a simple RAG application.</p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor188"/>Simple RAG</h2>
			<p>A simple RAG architecture implements a naive approach where the model retrieves a predetermined number of documents from the knowledge base based on their similarity to the user’s query. These retrieved documents are then combined with the query and input into the language model for generation, as shown in <em class="italic">Figure 8</em><em class="italic">.7</em>.</p>
			<div><div><img alt="" role="presentation" src="img/B22495_08_07.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7: Naive RAG</p>
			<p>To build a simple RAG application, you will use the dataset you loaded to the MongoDB Atlas collection in the <em class="italic">Information retrieval with MongoDB Vector Search</em> section of this chapter. With this application, you’ll be able perform queries on the available movies and create a recommender system.</p>
			<h3>LLM</h3>
			<p>This example will use the OpenAI APIs and <code>gpt-3.5-turbo</code>, but there are other variations of LLM models made available from OpenAI, such as <code>gpt-4o</code> and <code>gpt-4o-mini</code>. The same prompting technique can be used with other LLMs, such as <code>claude-v2</code> or <code>mistral8x-7B</code>, to achieve similar results.</p>
			<p>The following is the sample code to invoke the OpenAI LLM using LangChain:</p>
			<pre class="source-code">
from openai import OpenAI
client = OpenAI()
def invoke_llm(prompt, model_name='gpt-3.5-turbo-0125'):
    """
    Queries with input prompt to OpenAI API using the chat completion api gets the model's response.
    """
    response = client.chat.completions.create(
      model=model_name,
      messages=[
        {
          «role»: «user»,
          «content»: prompt
        }
      ],
      temperature=0.2,
      max_tokens=256,
      top_p=1,
      frequency_penalty=0,
      presence_penalty=0
    )
    chatbot_response = response.choices[0].message.content.strip()
    return chatbot_response
invoke_llm("This is a test")</pre>			<p>Here is the result:</p>
			<pre class="source-code">
'Great! What do you need help with?'</pre>			<p>Now that you have the APIs to call MongoDB Atlas Vector Search for retrieval and an API for invoking an LLM, you can combine these two tools to create a RAG system.</p>
			<h3>Prompt</h3>
			<p>A prompt to an LLM is a user-provided instruction or input that guides the model’s response. It can be a question, a statement, or a command, and is designed to drive the LLM to respond with a specific output. The effectiveness of a prompt can greatly influence the quality of the results generated by a RAG-based system, making prompt engineering a crucial aspect for interacting with these models. Good prompts are clear, specific, and structured to communicate the user’s intent to the LLM, enabling it to generate the most accurate and helpful responses possible.</p>
			<p>The following is an example of a prompt to perform QA on a private knowledge base:</p>
			<pre class="source-code">
def get_prompt(question, context):
    prompt = f"""Question: {question}
            System: Let's think step by step.
            Context: {context}
            """
    return prompt
def get_recommendation_prompt(query, context):
    prompt = f"""
        From the given movie listing data, choose a few great movie recommendations.
        User query: {query}
        Context: {context}
        Movie Recommendations:
        1. Movie_name: Movie_overview
        """
    return prompt</pre>			<p>To demonstrate the benefits of RAG over a foundational LLM, let's first ask the LLM a question without vector search context and then with it included. This will demonstrate how you can improve the accuracy of the results and reduce hallucinations while utilizing a foundational LLM, such as <code>gpt-3.5-turbo</code>, that was not trained on a private knowledge base.</p>
			<p>Here is the query response without vector search:</p>
			<pre class="source-code">
print(invoke_llm("In which movie does a footballer go completely blind?"))</pre>			<p>This is the result:</p>
			<pre class="source-code">
The Game of Their Lives" (2005), where the character Davey Herold, a footballer, goes completely blind after being hit in the head during a game</pre>			<p>Although the LLM’s response shows it struggles with factual accuracy, there is still promise in using it alongside human oversight for enterprise applications. Together, these systems can work effectively to power applications for businesses. To help overcome this issue, you need to add context to the prompt through vector search results.</p>
			<p>Let's see how you can use the <code>invoke_llm</code> function with the <code>query_vector_search</code> method to provide the relevant context alongside the user query to generate a response with a factually correct answer:</p>
			<pre class="source-code">
idea = "In which movie does a footballer go completely blind?"
search_response = query_vector_search(idea, prefilter={"year":{"$gt": 1990}}, postfilter={"score": {"$gt":0.8}},topK=10)
premise = "\n".join(list(map(lambda x:x['final'], search_response)))
print(invoke_llm(get_prompt(idea, premise)))</pre>			<p>Here is the result:</p>
			<pre class="source-code">
The movie in which a footballer goes completely blind is "23 Blast."</pre>			<p>Similarly, you can use the <code>get_recommendation_prompt</code> method to generate some movie recommendations using a simple RAG framework:</p>
			<pre class="source-code">
question = "I like Christmas movies, any recommendations for movies release after 1990?"
search_response = query_vector_search(question,topK=10)
context = "\n".join(list(map(lambda x:x['final'], search_response)))
print(invoke_llm(get_recommendation_prompt("I like Christmas movies, any recommendations for movies release after 1990?", context)))</pre>			<p>Here is the result:</p>
			<div><div><img alt="" role="presentation" src="img/B22495_08_08.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8: Sample output from the simple RAG application</p>
			<p>The simple RAG system you just built can handle straightforward queries that need answers to the point. Some examples are a customer service chatbot responding to a basic question such as “<code>Where is the customer support center in Bangalore?</code>” or helping you find all the restaurants where your favorite delicacy is served in Koramangala. The chatbot can retrieve the contextual piece of information in its retrieval step and generate an answer to this question with the help of the LLM.</p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor189"/>Advanced RAG</h2>
			<p>An advanced RAG framework incorporates more complex retrieval techniques, better integration of retrieved information, and often, the ability to iteratively refine both the retrieval and generation processes. In this section, you will learn how to build an intelligent recommendation engine on fashion data that can identify the interest of the user and then generate relevant fashion product or accessory recommendations only when there is intent to purchase a product in the user’s utterance. You will be building an intelligent conversation chatbot that leverages the power of LangChain, MongoDB Atlas Vector Search, and OpenAI in this section.</p>
			<p>The advanced RAG system in the current example will demonstrate the following features:</p>
			<ul>
				<li>Utilize an LLM to generate multiple searchable fashion queries given a user’s chat utterance</li>
				<li>Classify the user’s chat utterance as to whether there is an intent to purchase</li>
				<li>Develop a fusion stage that will also fetch vector similarity search results from multiple search queries to fuse them as a single recommendation set that is reranked with the help of an LLM</li>
			</ul>
			<p>The flow of steps when a user queries the RAG system is depicted in <em class="italic">Figure 8</em><em class="italic">.9</em>:</p>
			<div><div><img alt="" role="presentation" src="img/B22495_08_09.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9: Sample advanced RAG, flowchart for query processing and recommendation</p>
			<p>Let’s walk through the code to load the sample dataset and build the advanced RAG system with all the features that were listed at the beginning of this section.</p>
			<h3>Loading the dataset</h3>
			<p>For this example, you will utilize fashion data from a popular e-commerce company. The following code shows you how to load a dataset from an S3 bucket to a <code>pandas</code> DataFrame and then insert these documents into a MongoDB Atlas collection, <code>search.catalog_final_myn</code>:</p>
			<pre class="source-code">
import pandas as pd
import s3fs
import os
import boto3
s3_uri= "https://ashwin-partner-bucket.s3.eu-west-1.amazonaws.com/fashion_dataset.jsonl"
df = pd.read_json(s3_uri, orient="records", lines=True)
print(df[:3])
from pymongo import MongoClient
mongo_client = MongoClient(os.environ["MONGODB_CONNECTION_STR"])
# Upload documents along with vector embeddings to MongoDB Atlas Collection
col = mongo_client["search"]["catalog_final_myn"]
col.insert_many(df.to_dict(orient="records"))</pre>			<p>Here is the result:</p>
			<div><div><img alt="" role="presentation" src="img/B22495_08_10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10: Sample view of the fashion dataset with OpenAI embeddings</p>
			<h3>Creating a vector search index</h3>
			<p>As you can see in <em class="italic">Figure 8</em><em class="italic">.10</em>, the vector embeddings are already provided as part of the dataset. Therefore, the next step is to create a vector search index. You can create the vector search index by following the steps detailed in <em class="italic">Chapter 5</em>, <em class="italic">Vector Databases</em>, using the following index mapping:</p>
			<pre class="source-code">
{
    "fields": [
      {
        "type": "vector",
        "numDimensions": 1536,
        "path": "openAIVec",
        "similarity": "cosine"
      }
    ]
}</pre>			<h3>Fashion recommendations using advanced RAG</h3>
			<p>You have successfully loaded the new fashion dataset into the MongoDB Atlas collection and also created a vector search index with all the building blocks in place. You can now use the following code to set up an advanced RAG system and build a recommender system with the features mentioned earlier:</p>
			<pre class="source-code">
from langchain_core.output_parsers import JsonOutputParser # type: ignore
from langchain_core.prompts import PromptTemplate # type: ignore
from langchain_core.pydantic_v1 import BaseModel, Field # type: ignore
from langchain_openai import ChatOpenAI # type: ignore
from langchain_community.embeddings import OpenAIEmbeddings # type: ignore
from langchain_mongodb.vectorstores import MongoDBAtlasVectorSearch # type: ignore
from pymongo import MongoClient # type: ignore
from typing import List
from itertools import chain
import certifi # type: ignore
import os
from dotenv import load_dotenv # type: ignore
load_dotenv()
from functools import lru_cache
@lru_cache
def get_openai_emb_transformers():
    """
    Returns an instance of OpenAIEmbeddings for OpenAI transformer models.
    This function creates and returns an instance of the OpenAIEmbeddings class,
    which provides access to OpenAI transformer models for natural language processing.
    The instance is cached using the lru_cache decorator for efficient reuse.
    Returns:
        embeddings (OpenAIEmbeddings): An instance of the OpenAIEmbeddings class.
    """
    embeddings = OpenAIEmbeddings()
    return embeddings
@lru_cache
def get_vector_store():
    """
    Retrieves the vector store for MongoDB Atlas.
    Returns:
        MongoDBAtlasVectorSearch: The vector store object.
    """
    vs = MongoDBAtlasVectorSearch(collection=col, embedding=get_openai_emb_transformers(), index_name="vector_index_openAi_cosine", embedding_key="openAIVec", text_key="title")
    return vs
@lru_cache(10)
def get_conversation_chain_conv():
    """
    Retrieves a conversation chain model for chat conversations.
    Returns:
        ChatOpenAI: The conversation chain model for chat conversations.
    """
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.2, max_tokens=2048)
    # chain = ConversationChain(llm=llm, memory=ConversationBufferWindowMemory(k=5))
    return llm
# Define your desired data structure.
class ProductRecoStatus(BaseModel):
    """
    Represents the status of product recommendations.
    Attributes:
        relevancy_status (bool): Product recommendation status conditioned on the context of the input query.
                                 True if the query is related to purchasing fashion clothing and/or accessories.
                                 False otherwise.
        recommendations (List[str]): List of recommended product titles based on the input query context and
                                     if the relevancy_status is True.
    """
    relevancy_status: bool = Field(description="Product recommendation status is conditioned on the fact if the context of input query is to purchase a fashion clothing and or fashion accessories.")
    recommendations: List[str] = Field(description="list of recommended product titles based on the input query context and if recommendation_status is true.")
class Product(BaseModel):
    """
    Represents a product.
    Attributes:
        title (str): Title of the product.
        baseColour (List[str]): List of base colours of the product.
        gender (List[str]): List of genders the product is targeted for.
        articleType (str): Type of the article.
        mfg_brand_name (str): Manufacturer or brand name of the product.
    """
    title: str = Field(description="Title of the product.")
    baseColour: List[str] = Field(description="List of base colours of the product.")
    gender: List[str] = Field(description="List of genders the product is targeted for.")
    articleType: str = Field(description="Type of the article.")    mfg_brand_name: str = Field(description="Manufacturer or brand name of the product.")
class Recommendations(BaseModel):
    """
    Represents a set of recommendations for products and a message to the user.
    Attributes:
        products (List[Product]): List of recommended products.
        message (str): Message to the user and context of the chat history summary.
    """
    products: List[Product] = Field(description="List of recommended products.")
    message: str = Field(description="Message to the user and context of the chat history summary.")
reco_status_parser = JsonOutputParser(pydantic_object=ProductRecoStatus)
reco_status_prompt = PromptTemplate(    template="You are AI assistant tasked at identifying if there is a product purchase intent in the query and providing suitable fashion recommendations.\n{format_instructions}\n{query}\n\
        #Chat History Summary: {chat_history}\n\nBased on the context of the query, please provide the relevancy status and list of recommended products.",
    input_variables=["query", "chat_history"],
    partial_variables={"format_instructions": reco_status_parser.get_format_instructions()},
)
reco_parser = JsonOutputParser(pydantic_object=Recommendations)
reco_prompt = PromptTemplate(
    input_variables=["question", "recommendations", "chat_history"],
    partial_variables={"format_instructions": reco_parser.get_format_instructions()},
    template="\n User query:{question} \n Chat Summary: {chat_history} \n Rank and suggest me suitable products for creating grouped product recommendations given all product recommendations below feature atleast one product for each articleType \n {recommendations} \n show output in {format_instructions} for top 10 products"
)
def get_product_reco_status(query: str, chat_history: List[str] = []):
    """
    Retrieves the recommendation status for a product based on the given query and chat history.
    Args:
        query (str): The query to be used for retrieving the recommendation status.
        chat_history (List[str]): The chat history containing previous conversations.
    Returns:
        The response containing the recommendation status.
    """
    llm = get_conversation_chain_conv()
    chain = reco_status_prompt | llm | reco_status_parser
    resp = chain.invoke({"query": query, "chat_history": chat_history})
    return resp
def get_sorted_results(product_recommendations):
    all_titles = [rec['title'] for rec in product_recommendations['products']]
    results = list(col.find({"title": {"$in":all_titles}}, {"_id": 0 , "id":1, "title": 1, "price": 1, "baseColour": 1, "articleType": 1, "gender": 1, "link" : 1, "mfg_brand_name": 1}))
    sorted_results = []
    for title in all_titles:
        for result in results:
            if result['title'] == title:
                sorted_results.append(result)
                break
    return sorted_results
def get_product_recommendations(query: str, reco_queries: List[str], chat_history: List[str]=[]):
    """
    Retrieves product recommendations based on the given query and chat history.
    Args:
        query (str): The query string for the recommendation.
        chat_history (List[str]): The list of previous chat messages.
        filter_query (dict): The filter query to apply during the recommendation retrieval.
        reco_queries (List[str]): The list of recommendation queries.
    Returns:
        dict: The response containing the recommendations.
    """
    vectorstore = get_vector_store()
    retr = vectorstore.as_retriever(search_kwargs={"k": 10})
    all_recommendations = list(chain(*retr.batch(reco_queries)))
    llm = get_conversation_chain_conv()
    llm_chain = reco_prompt | llm | reco_parser
    resp = llm_chain.invoke({"question": query, "chat_history": chat_history, "recommendations": [v.page_content for v in all_recommendations]})
    resp = get_sorted_results(resp)
    return resp</pre>			<p>The preceding code carries out the following tasks:</p>
			<ol>
				<li>Importing the necessary modules and functions from various libraries. These include <code>JsonOutputParser</code> for parsing JSON output, <code>PromptTemplate</code> for creating prompts, <code>BaseModel</code> and <code>Field</code> for defining data models, and <code>MongoDBAtlasVectorSearch</code> for interacting with a MongoDB Atlas vector store. It also imports <code>MongoClient</code> for connecting to MongoDB, <code>load_dotenv</code> for loading environment variables, and <code>lru_cache</code> for caching function results.</li>
				<li>It defines three functions, each decorated with <code>lru_cache</code> to cache their results for efficiency. <code>get_openai_emb_transformers</code> returns an instance of <code>OpenAIEmbeddings</code>, which provides access to OpenAI transformer models for NLP. <code>get_vector_store</code> retrieves the vector store for MongoDB Atlas. <code>get_conversation_chain_conv</code> retrieves a conversation chain model for chat conversations.</li>
				<li>It defines three classes using Pydantic’s <code>BaseModel</code> and <code>Field</code>. These classes represent the status of product recommendations (<code>ProductRecoStatus</code>), a product (<code>Product</code>), and a set of recommendations for products and a message to the user (<code>Recommendations</code>).</li>
				<li>Creating instances of <code>JsonOutputParser</code> and <code>PromptTemplate</code> for parsing JSON output and creating prompts, respectively. These instances are used to create conversation chains in the next section.</li>
				<li>It defines two functions for retrieving the recommendation status for a product and retrieving product recommendations based on a given query and chat history. <code>get_product_reco_status</code> uses a conversation chain to determine the recommendation status for a product based on a given query and chat history. <code>get_product_recommendations</code> retrieves product recommendations based on a given query and chat history, a filter query, and a list of recommendation queries. It uses a vector store retriever to get relevant documents for each recommendation query, and then uses a conversation chain to generate the final recommendations.</li>
			</ol>
			<p>Let’s now use these methods to create a product recommendations example. Enter the following code and then examine its output:</p>
			<pre class="source-code">
query = "Can you suggest me some casual dresses for date occasion with my boyfriend"
status = get_product_reco_status(query)
print(status)
print(get_product_recommendations(query, reco_queries=status["recommendations"], chat_history=[])</pre>			<p>This is the status output:</p>
			<pre class="source-code">
{'relevancy_status': True,
 'recommendations': ['Floral Print Wrap Dress',
  'Off-Shoulder Ruffle Dress',
  'Lace Fit and Flare Dress',
  'Midi Slip Dress',
  'Denim Shirt Dress']}</pre>			<p>You can see from the preceding example output that the LLM is able to classify the product intent purchase as positive and recommend suitable queries by performing vector similarity search on the MongoDB Atlas collection.</p>
			<p>This is the product recommendations output:</p>
			<div><div><img alt="" role="presentation" src="img/B22495_08_11.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.11: Sample output from the advanced RAG chatbot with recommendations for the user's search intent</p>
			<p>Conversely, you can test the same methods to find a suitable place for a date instead of ideas for gifts or what to wear. In this case, the model will classify the query as having negative product purchase intent and not provide any search term suggestions:</p>
			<pre class="source-code">
query = "Where should I take my boy friend for date"
status = get_product_reco_status(query)
print(status)
print(get_conversation_chain_conv().invoke(query).content)</pre>			<p>Here is the status output:</p>
			<pre class="source-code">
{'relevancy_status': False, 'recommendations': []}</pre>			<p>Here is the output from the LLM:</p>
			<div><div><img alt="" role="presentation" src="img/B22495_08_12.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.12: Sample output from the advanced RAG system when there is no purchase intent in the query</p>
			<p>Advanced RAG introduces the concept of modularity when building RAG architecture systems. The above example focuses on developing a user flow-based approach for the sample advanced RAG system. It also explores how to leverage LLMs for conditional decision making, recommendation generation, and re-ranking the recommendations retrieved from the retriever system. The goal is to enhance the user experience during interactions with the application.</p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor190"/>Summary</h1>
			<p>In this chapter, you explored the pivotal role of vector search in enhancing AI-powered systems. The key takeaway is that vector search plays a vital role in AI applications, addressing the challenge of efficient search as unstructured and multimodal datasets expand. It benefits image recognition, NLP, and recommendation systems.</p>
			<p>MongoDB Atlas is used to demonstrate vector search implementation using its flexible schema and vector indexing capabilities. You were able to build a RAG framework for solving QA use cases that combines retrieval and generation models, with a simple RAG system utilizing pre-trained language models and embedding models from OpenAI. You also learned how to build an advanced RAG system that employs iterative refinement and sophisticated retrieval algorithms with the help of LLMs for building a recommendation system for the fashion industry. With these insights, you can now build efficient AI applications for any domain or industry.</p>
			<p>In the next chapter, you will delve into the critical aspects of evaluating LLM outputs in such RAG applications and explore various evaluation methods, metrics, and user feedback. You will also learn about the implementation of guardrails to ensure responsible AI deployment and how to better control the behavior of LLM-generated responses.</p>
		</div>
	

		<div><div></div>
		</div>
		<div><h1 id="_idParaDest-152"><a id="_idTextAnchor191"/>Part 3</h1>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor192"/>Optimizing AI Applications: Scaling, Fine-Tuning, Troubleshooting, Monitoring, and Analytics</h1>
			<p>This set of chapters shares techniques and practices for evaluating your AI application as well as strategies and expert insights for improving your application, avoiding pitfalls, and ensuring that your application continues to function optimally despite rapid technological changes.</p>
			<p>This part of the book includes the following chapters:</p>
			<ul>
				<li><a href="B22495_09.xhtml#_idTextAnchor193"><em class="italic">Chapter 9</em></a>, <em class="italic">LLM Output Evaluation</em></li>
				<li><a href="B22495_10.xhtml#_idTextAnchor214"><em class="italic">Chapter 10</em></a>, <em class="italic">Refining the Semantic Data Model to Improve Accuracy</em></li>
				<li><a href="B22495_11.xhtml#_idTextAnchor232"><em class="italic">Chapter 11</em></a>, <em class="italic">Common Failures of Generative AI</em></li>
				<li><a href="B22495_12.xhtml#_idTextAnchor253"><em class="italic">Chapter 12</em></a>, <em class="italic">Correcting and Optimizing Your Generative AI Application</em></li>
			</ul>
		</div>
	</body></html>