<html><head></head><body>
		<div id="_idContainer085">
			<h1 class="chapter-number" id="_idParaDest-141"><a id="_idTextAnchor180"/>8</h1>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor181"/>Implementing Vector Search in AI Applications</h1>
			<p>Vector search is revolutionizing the way people interact with data in AI applications. MongoDB Atlas Vector Search allows developers to implement sophisticated search capabilities that understand the nuances of discovery and retrieval. It works by converting text, video, image, or audio files into numerical vector representations, which can then be stored and searched efficiently. MongoDB Atlas can perform similarity searches alongside your operational data, making it an essential tool for enhancing user experience in applications ranging from e-commerce to content discovery. With MongoDB Atlas, setting up vector search is streamlined, enabling developers to focus on creating dynamic, responsive, and <span class="No-Break">intelligent applications.</span></p>
			<p>In this chapter, you will learn how to use the Vector Search feature of MongoDB Atlas to build intelligent applications. You will learn how to build <strong class="bold">retrieval-augmented generation</strong> (<strong class="bold">RAG</strong>) architecture systems and delve deeper into the understanding and development of various patterns of complex RAG architectures with MongoDB Atlas, unraveling the synergies that underpin their joint value and potential. Through real-world use cases and practical demonstrations, you will learn how this dynamic duo can seamlessly transform businesses across industries, driving efficiency, accuracy, and <span class="No-Break">operational excellence.</span></p>
			<p>This chapter covers the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Leverage vector search and full-text search with MongoDB Atlas, which will later help you build a robust retriever <span class="No-Break">for RAG</span></li>
				<li>Understand the various components involved in the development of a <span class="No-Break">RAG system</span></li>
				<li>Learn about the process and steps involved in the development of simple RAG and advanced <span class="No-Break">RAG systems.</span></li>
			</ul>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor182"/>Technical requirements</h1>
			<p>This chapter assumes that you have at least beginner-level expertise in Python coding. To follow along with the demos, you’ll need to set up your development environment by completing the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Install either <strong class="source-inline">python@3.9</strong> or <strong class="source-inline">python@3.11</strong> on the operating system of <span class="No-Break">your choice.</span></li>
				<li>Set up a Python virtual environment and <span class="No-Break">activate it:</span><pre class="source-code">
$ python3 -m venv venv
$ source venv/bin/activate</pre></li>				<li>You will be using the following packages to develop the demo described in <span class="No-Break">this chapter:</span><ul><li><strong class="source-inline">pandas</strong>: Helps with data preprocessing <span class="No-Break">and handling</span></li><li><strong class="source-inline">numpy</strong>: Handles <span class="No-Break">numerical data</span></li><li><strong class="source-inline">openai</strong>: For the embedding model and invoking <span class="No-Break">the LLM</span></li><li><strong class="source-inline">pymongo</strong>: For the MongoDB Atlas vector store and <span class="No-Break">full-text search</span></li><li><strong class="source-inline">s3fs</strong>: Allows loading data directly from an <span class="No-Break">S3 bucket</span></li><li><strong class="source-inline">langchain_mongodb</strong>: Enables vector search in MongoDB Atlas using a <span class="No-Break">LangChain wrapper</span></li><li><strong class="source-inline">langchain</strong>: Used to build a <span class="No-Break">RAG application</span></li><li><strong class="source-inline">langchain-openai</strong>: Enables you to interact with OpenAI <span class="No-Break">chat models</span></li><li><strong class="source-inline">boto3</strong>: Enables you to interact with AWS <span class="No-Break">s3 buckets</span></li><li><strong class="source-inline">python-dotenv:</strong> Enables you to load environment variables from a <strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">env</strong></span><span class="No-Break"> file</span></li></ul><p class="list-inset">To install the mentioned packages in your Python virtual environment, run the <span class="No-Break">following command:</span></p><pre class="source-code">
pip3 install langchain==0.2.14 langchain-community==0.2.12 langchain-core==0.2.33 langchain-mongodb==0.1.8 langchain-openai==0.1.22 langchain-text-splitters==0.2.2 numpy==1.26.4 openai==1.41.1 s3fs==2024.6.1 pymongo==4.8.0 pandas==2.2.2 boto3==1.35.2 python-dotenv==1.0.1</pre><p class="list-inset">You will also need to know how to set up and run JupyterLab or <span class="No-Break">Jupyter Notebook.</span></p></li>			</ol>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor183"/>Information retrieval with MongoDB Atlas Vector Search</h1>
			<p>Information retrieval is a critical component of RAG systems. It enhances the accuracy and relevance of the generated text by sourcing information from extensive knowledge bases. This process allows the RAG system to produce responses that are not only precise but also deeply rooted in factual content, making it a powerful tool for various <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) tasks. By effectively combining retrieval with generation, RAG addresses challenges related to bias and misinformation, contributing to the advancement of AI-related applications <span class="No-Break">and tasks.</span></p>
			<p>In the context of information retrieval, it’s essential to distinguish between <em class="italic">relevance</em> and <em class="italic">similarity</em>. While <strong class="bold">similarity</strong> focuses on word matching, <strong class="bold">relevance</strong> is about the interconnectedness of ideas. While a vector database query can help identify semantically related content, more advanced tools are needed to accurately retrieve <span class="No-Break">relevant information.</span></p>
			<p>In <span class="No-Break"><em class="italic">Chapter 5</em></span>, <em class="italic">Vector Databases</em>, you learned about MongoDB Atlas Vector Search and how it enhances the retrieval of relevant information by allowing the creation and indexing of vector embeddings, which can be generated using machine learning models, such as embedding models. This facilitates semantic search capabilities, enabling the identification of content that is contextually similar rather than just being keyword based. Full-text search complements this by providing robust text search capabilities that can handle typos, synonyms, and other variations in text, ensuring that searches return the most pertinent results. Together, these tools provide a comprehensive search solution that can discern and retrieve information based on both the similarity of terms and the relevance of <span class="No-Break">the content.</span></p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor184"/>Vector search tutorial in Python</h2>
			<p>With the help of an example, let’s see how to load a small dataset in MongoDB to perform a vector search along with full-text search to perform information retrieval. For this demonstration, you will load a sample movie dataset from an <span class="No-Break">S3 bucket:</span></p>
			<ol>
				<li>Write a simple Python function to accept search terms or phrases and pass it through the embeddings API again to get a <span class="No-Break">query vector.</span></li>
				<li>Take the resultant query vector embeddings and perform a vector search query using the <a href="https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/"><strong class="source-inline">$vectorsearch</strong></a> operator in the MongoDB <span class="No-Break">aggregation pipeline.</span></li>
				<li>Pre-filter the documents using meta information to narrow the search across your dataset, thereby speeding up the performance of the vector search results while <span class="No-Break">retaining accuracy.</span></li>
				<li>Further, post-filter the retrieved documents that are semantically similar (based on relevancy score), if you want to demonstrate a higher degree of control over the semantic <span class="No-Break">search behavior.</span></li>
				<li>Initialize the OpenAI API key and MongoDB <span class="No-Break">connection string:</span><pre class="source-code">
import os
import getpass
# set openai api key
try:
    openai_api_key = os.environ["OPENAI_API_KEY"]
except KeyError:
    openai_api_key = getpass.getpass("Please enter your OPENAI API KEY (hit enter): ")
# Set MongoDB Atlas connection string
try:
    MONGO_CONN_STR = os.environ["MONGODB_CONNECTION_STR"]
except KeyError:
    MONGO_CONN = getpass.getpass("Please enter your MongoDB Atlas Connection String (hit enter): ")</pre></li>				<li>Now, load the dataset from the S3 bucket. Run the following lines of code in Jupyter Notebook to read data from an AWS S3 bucket directly to a <span class="No-Break"><strong class="source-inline">pandas</strong></span><span class="No-Break"> DataFrame:</span><pre class="source-code">
import pandas as pd
import s3fs
df = pd.read_json("https://ashwin-partner-bucket.s3.eu-west-1.amazonaws.com/movies_sample_dataset.jsonl", orient="records", lines=True)
df.to_json("./movies_sample_dataset.jsonl", orient="records", lines=True)
df[:3]</pre><p class="list-inset">On executing the preceding snippet of code, you should see the following result in your Jupyter <span class="No-Break">Notebook cell.</span></p></li>			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer073">
					<img alt="" role="presentation" src="image/B22495_08_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1: Sample movies data view</p>
			<ol>
				<li value="7">Initialize and run an embedding job to embed the <strong class="source-inline">sample_movies</strong> dataset. In the following code example, you create a <strong class="source-inline">final</strong> field, which is a field derived from the <strong class="source-inline">text</strong> and <strong class="source-inline">overview</strong> fields that are already available in <span class="No-Break">the dataset.</span></li>
				<li>Next, run this <strong class="source-inline">final</strong> field against the embedding API from OpenAI, as <span class="No-Break">shown here:</span><pre class="source-code">
import numpy as np
from tqdm import tqdm
import openai
df['final'] = df['text'] + "    Overview: " + df['overview']
df['final'][:5]
step = int(np.ceil(df['final'].shape[0]/100))
embeddings_t = []
lines = []
# Note that we must split the dataset into smaller batches to not exceed the rate limits imposed by OpenAI API's.
for x, y in list(map(lambda x: (x, x+step), list(range(0, df.shape[0], step)))):
    lines += [df.final.values[x:y].tolist()]
for i in tqdm(lines):
    embeddings_t += openai.embeddings.create(
        model='text-embedding-ada-002', input=i).data
out = []
for ele in embeddings_t:
    out += [ele.embedding]
df['embedding'] = out
df[:5]</pre><p class="list-inset">You should see that the <strong class="source-inline">sample_movies</strong> dataset is enriched with the OpenAI embeddings in the <strong class="source-inline">embedding</strong> field, as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">.</span></p></li>			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer074">
					<img alt="" role="presentation" src="image/B22495_08_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2: Sample movies dataset view with OpenAI embeddings</p>
			<ol>
				<li value="9">Next, initialize MongoDB Atlas and insert data into a <span class="No-Break">MongoDB collection.</span></li>
				<li>Now that you have created the vector embeddings for your <strong class="source-inline">sample_movies</strong> dataset, you can initialize the MongoDB client and insert the documents into your collection of choice by running the following lines <span class="No-Break">of code:</span><pre class="source-code">
from pymongo import MongoClient
import osmongo_client = MongoClient(os.environ["MONGODB_CONNECTION_STR"])
# Upload documents along with vector embeddings to MongoDB Atlas Collection
output_collection = mongo_client["sample_movies"]["embed_movies"]
if output_collection.count_documents({})&gt;0:
    output_collection.delete_many({})
_ = output_collection.insert_many(df.to_dict("records"))</pre><p class="list-inset">You have ingested the test data to build a vector search capability. Now, let’s proceed to build a vector search index in the <span class="No-Break">following steps.</span></p></li>				<li>Let’s first create vector index definitions. You can create a vector search index in the MongoDB Atlas Vector Search UI by following the steps explained in <span class="No-Break"><em class="italic">Chapter 5</em></span>, <em class="italic">Vector Databases</em>. The vector index required for this demo tutorial is <span class="No-Break">provided here:</span><pre class="source-code">
{
    "fields": [
      {
        "type": "vector",
        "numDimensions": 1536,
        "path": "embedding",
        "similarity": "cosine"
      },
      {
        "type": "filter",
        "path": "year"
      },
    ]
}</pre><p class="list-inset">Once the vector index definitions are added under the Vector Search index JSON editor in the MongoDB Atlas UI, the process for creating a vector search index is triggered and the vector search index is created at the specified <strong class="source-inline">path</strong> field mentioned in the vector index definition. Now, you are ready to perform vector search queries on the <strong class="source-inline">sample_movies.embed_movies</strong> collection in MongoDB Atlas where all the data is stored, and create <span class="No-Break">vector indexes.</span></p><p class="list-inset">Let’s equip the vector search or the retriever API to use in your <span class="No-Break">RAG framework.</span></p></li>				<li>You can query a MongoDB vector index using <strong class="source-inline">$vectorSearch</strong>. MongoDB Atlas brings the flexibility of using vector search alongside search filters. Additionally, you can apply range, string, and numeric filters using the aggregation pipeline. This allows the end user to control the behavior of the semantic search response from the <span class="No-Break">search engine.</span><p class="list-inset">The following code example demonstrates how you can perform vector search along with pre-filtering on the <strong class="source-inline">year</strong> field to get movies released post <strong class="source-inline">1990</strong>. To have better control over the relevance of returned results, you can perform post-filtering on the response using the MongoDB <span class="No-Break">Query API.</span></p><p class="list-inset">The following code demonstrates how you can perform <span class="No-Break">these steps:</span></p><ol><li class="upper-roman">Represent a raw text query as a vector embedding. There are multiple embedding models currently available with OpenAI, such as <strong class="source-inline">text-embedding-3-small</strong>, <strong class="source-inline">text-embedding-3-large</strong> with variable dimensions, and the <span class="No-Break"><strong class="source-inline">text-embedding-ada-002</strong></span><span class="No-Break"> model.</span></li><li class="upper-roman">Build and perform a vector search query to <span class="No-Break">MongoDB Atlas.</span></li><li class="upper-roman">Perform pre-filtering before performing a vector search on the <span class="No-Break"><strong class="source-inline">year</strong></span><span class="No-Break"> field.</span></li><li class="upper-roman">Perform post-filtering using the <strong class="source-inline">score</strong> field to better control the relevancy and accuracy of the <span class="No-Break">returned results.</span></li></ol><p class="list-inset">Run the following code to initialize a function that can help you achieve vector search, pre-filter, <span class="No-Break">and post-filter:</span></p><pre class="source-code">
def query_vector_search(q, prefilter = {}, postfilter = {},path="embedding",topK=2):
    ele = openai.embeddings.create(model='text-embedding-ada-002', input=q).data
    query_embedding = ele[0].embedding
    vs_query = {
                "index": "default",
                "path": path,
                "queryVector": query_embedding,
                "numCandidates": 10,
                "limit": topK,
            }
    if len(prefilter)&gt;0:
        vs_query["filter"] = prefilter
    new_search_query = {"$vectorSearch": vs_query}
    project = {"$project": {"score": {"$meta": "vectorSearchScore"},"_id": 0,"title": 1, "release_date": 1, "overview": 1,"year": 1}}
    if len(postfilter.keys())&gt;0:
        postFilter = {"$match":postfilter}
        res = list(output_collection.aggregate([new_search_query, project, postFilter]))
    else:
        res = list(output_collection.aggregate([new_search_query, project]))
    return res</pre><p class="list-inset">Here’s a sample query with <strong class="source-inline">year</strong> as <span class="No-Break">a pre-filter:</span></p><pre class="source-code">query_vector_search("I like Christmas movies, any recommendations for movies release after 1990?", prefilter={"year": {"$gt": 1990}}, topK=5)</pre><p class="list-inset">You should get the <span class="No-Break">following result:</span></p></li>			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer075">
					<img alt="" role="presentation" src="image/B22495_08_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3: Sample result from running the vector search query with pre-filters</p>
			<p class="list-inset">This is a sample query with <strong class="source-inline">year</strong> as a pre-filter and a <strong class="source-inline">score</strong>-based post-filter to retain only the <span class="No-Break">relevant results:</span></p>
			<pre class="source-code">
query_vector_search("I like Christmas movies, any recommendations for movies release after 1990?", prefilter={"year":{"$gt": 1990}}, postfilter= {"score": {"$gt":0.905}},topK=5)</pre>			<p class="list-inset">You should get the <span class="No-Break">following result:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer076">
					<img alt="" role="presentation" src="image/B22495_08_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4: Sample result from running the vector search query with a pre-filter and post-filter</p>
			<p>With this Python method, you were able to filter on the <strong class="source-inline">score</strong> field and the <strong class="source-inline">year</strong> field to generate results as well as results for vector similarity. Using a heuristic, you were able to control the accuracy of the results to retain only the most relevant documents and were also able to apply a range filter query (on the <span class="No-Break"><strong class="source-inline">year</strong></span><span class="No-Break"> field).</span></p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor185"/>Vector Search tutorial with LangChain</h2>
			<p>Utilizing <strong class="bold">LangChain</strong> with MongoDB Atlas Vector Search for building a semantic similarity retriever offers several advantages. The following example demonstrates how to carry out a vector similarity search using LangChain <span class="No-Break">wrapper classes:</span></p>
			<pre class="source-code">
from langchain_mongodb.vectorstores import MongoDBAtlasVectorSearch
from langchain_openai import OpenAIEmbeddings
import json
embedding_model = OpenAIEmbeddings(model="text-embedding-ada-002")
vector_search = MongoDBAtlasVectorSearch(output_collection, embedding_model, text_key='final')
fquery = {"year": {"$gt": 1990}}
search_kwargs = {
    "k": 5,
    'filter': fquery,
}
retriever = vector_search.as_retriever(search_kwargs=search_kwargs)
docs = retriever.invoke("I like Christmas movies, any recommendations for movies release after 1990?")
for doc in docs:
    foo = {}
    foo['title'] = doc.metadata['title']
    foo['year'] = doc.metadata['year']
    foo['final'] = doc.metadata['text']
    print(json.dumps(foo,indent=1))</pre>			<p>Here’s <span class="No-Break">the result:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer077">
					<img alt="" role="presentation" src="image/B22495_08_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5: Sample result vector search query using the LangChain module for MongoDB</p>
			<p>This demonstrates a more sophisticated yet simple approach that is particularly beneficial for developers creating RAG applications. The LangChain framework offers a suite of APIs and wrapper classes that can be used to integrate with various serverless LLM providers, such as OpenAI, and talk to MongoDB Atlas Vector Search to build RAG frameworks with very few lines of code. It is also easy to maintain <span class="No-Break">and scale.</span></p>
			<p>In this section, you were able to build and perform vector similarity search using MongoDB Atlas. You developed reusable wrapper classes and functions that will be useful in developing a more sophisticated application, such as <span class="No-Break">a chatbot.</span></p>
			<p>Now, let’s delve deep into understanding what RAG architectures are and how to develop one using the resources that you’ve created <span class="No-Break">so far.</span></p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor186"/>Building RAG architecture systems</h1>
			<p>In the dynamic landscape of modern business, the relentless pursuit of efficiency and accuracy urges organizations to adopt cutting-edge technologies. Among these, automation stands as a cornerstone, particularly in processing and automating workflows. However, traditional methods suffer when they’re subjected to large volumes of data with intricate tasks, and human-led processes often fall short due to error-prone <span class="No-Break">manual interventions.</span></p>
			<p>This section explores the transformative landscape of automation, discussing the pivotal role RAG plays in revolutionizing business operations. MongoDB, known for its prowess in data management and flexible schemas, offers a compelling synergy with RAG through its vector search and full-text search capabilities. Delving into the architectural details of RAG, this section dissects its constituent building blocks, offering practical insights into constructing automated document-processing workflows that harness the full potential of LLMs and MongoDB <span class="No-Break">Vector Search.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer078">
					<img alt="" role="presentation" src="image/B22495_08_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6: Building blocks of RAG architecture</p>
			<p>Let’s go over the key components of the RAG architecture <span class="No-Break">in detail:</span></p>
			<ol>
				<li><strong class="bold">Document loading</strong>: Initially, documents are loaded from data storage. This involves text extraction, parsing, formatting, and cleaning to prepare the data for <span class="No-Break">document splitting.</span></li>
				<li><strong class="bold">Document splitting</strong>: The next step is to break down the documents into smaller, manageable segments or chunks. Strategies for splitting can vary, from fixed-size chunking to content-aware chunking that considers the <span class="No-Break">content structure.</span></li>
				<li><strong class="bold">Text embedding</strong>: These document chunks are then transformed into vector representations (embeddings) using techniques such as <strong class="bold">OpenAIEmbeddings</strong>, <strong class="bold">Sentence e-BERT</strong>, and <strong class="bold">Instructor Embeddings</strong>. This step is crucial for understanding the semantic content of <span class="No-Break">the chunks.</span></li>
				<li><strong class="bold">Vector store</strong>: The generated vectors, each associated with unique document chunks, are stored in a vector store alongside the document chunks and other metadata extracted from the MongoDB Atlas collection. Atlas Vector Search indexes and Apache Lucene search can be built through the<strong class="bold"> MongoDB Atlas UI</strong> for easy and <span class="No-Break">fast retrieval.</span></li>
				<li><strong class="bold">Query processing</strong>: When a user submits a query, it is also converted into a vector representation using the same embedding technique as mentioned in <span class="No-Break"><em class="italic">Step 3</em></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Document retrieval</strong>: The retriever component locates and fetches document chunks that are semantically like the query. This retrieval process employs vector similarity search techniques and MongoDB Atlas using the <strong class="bold">Hierarchical Navigable Small Worlds</strong> (<strong class="bold">HNSW)</strong> algorithm to perform a fast nearest neighbor search to retrieve relevant documents without compromising the accuracy of the retrieved <span class="No-Break">search results.</span></li>
				<li><strong class="bold">Document chunk post-filtering</strong>: The relevant document chunks are retrieved from the MongoDB collection with the help of the <strong class="bold">Unified Query API</strong> and can be post-filtered easily to transform the output document chunks into the <span class="No-Break">required format.</span></li>
				<li><strong class="bold">LLM prompt creation</strong>: The retrieved document chunks and the query are combined to create a context and prompt for <span class="No-Break">the LLM.</span></li>
				<li><strong class="bold">Answer generation</strong>: Finally, the LLM generates a response based on the prompt, completing the <span class="No-Break">RAG process.</span></li>
			</ol>
			<p>In the context of RAG systems, there are two primary types: <strong class="bold">simple</strong> (<strong class="bold">or naive</strong>) <strong class="bold">RAG</strong> and <strong class="bold">advanced</strong> <strong class="bold">RAG</strong>. In practical scenarios, this classification helps address different types of personas and questions the applications are handling, and it’s common to encounter both simple and complex RAG queries within the same workflow and from the same persona. As a developer, it is important to reason out the functionalities that the application is expected to serve before deciding on the building blocks involved in the <span class="No-Break">RAG architecture.</span></p>
			<p>When building your RAG architecture system, consider the following points to help with programming <span class="No-Break">and planning:</span></p>
			<ul>
				<li><strong class="bold">Workflow specificity</strong>: Define the specific workflow you intend to automate with RAG; it may be related to <strong class="bold">question answering</strong> (<strong class="bold">QA</strong>), data augmentation, summarization, reasoning, or assertion. Maybe your customers frequently ask a specific set of three or four types <span class="No-Break">of queries.</span></li>
				<li><strong class="bold">User experience</strong>: Collaborate with your target user group to understand the types of queries they are likely to ask to identify the user group journey, which might be a simple single-state response or a multi-state <span class="No-Break">chat flow.</span></li>
				<li><strong class="bold">Data sources</strong>: First, identify the nature of your data source—whether it’s unstructured or structured. Next, map the locations of these data sources. Once you’ve done that, classify the data based on whether it serves operational or analytical purposes. Finally, observe the data patterns to determine whether answers are readily available in one location or if you’ll need to gather information from <span class="No-Break">multiple sources.</span></li>
			</ul>
			<p>These pointers will help you determine whether you need to go for a simple RAG system or an advanced RAG system and also help you to determine the essential building blocks to consider while constructing your <span class="No-Break">RAG architecture.</span></p>
			<p>Now, let’s delve deeper into the building blocks of this architecture with some code examples to better explain the nuances. However, before you develop RAG-powered applications, let’s look at the fundamentals of how to process source documents to maximize the accuracy of the rated responses from the RAG application. The following strategies will come in handy while processing documents before storing them in a MongoDB <span class="No-Break">Atlas collection.</span></p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor187"/>Chunking or document-splitting strategies</h2>
			<p><strong class="bold">Chunking</strong> or <strong class="bold">document splitting</strong> is a critical step in handling extensive texts within RAG systems. When dealing with large documents, the token limits imposed by language models (such as <strong class="bold">gpt-3.5-turbo</strong>) necessitate breaking them into manageable chunks. However, a naive fixed-chunk-size approach can lead to fragmented sentences across chunks, affecting subsequent tasks such <span class="No-Break">as QA.</span></p>
			<p>To address this, consider semantics when dividing documents. Most segmentation algorithms use chunk size and overlap principles. <strong class="bold">Chunk size</strong> (measured by characters, words, or tokens) determines segment length, while <strong class="bold">overlaps</strong> ensure continuity by sharing context between adjacent chunks. This approach preserves semantic context and enhances RAG <span class="No-Break">system performance.</span></p>
			<p>Now, let’s delve into the intricacies of document-splitting techniques, particularly focusing on content-aware chunking. While fixed-size chunking with overlap is straightforward and computationally efficient, more sophisticated methods enhance the quality of text segmentation. The following are the various <span class="No-Break">document-splitting techniques:</span></p>
			<ul>
				<li><strong class="bold">Recursive chunking</strong>: This technique includes the <span class="No-Break">following approaches:</span><ul><li><strong class="bold">Hierarchical approach</strong>: Recursive chunking breaks down input text into smaller chunks iteratively. It operates hierarchically, using different separators or criteria at <span class="No-Break">each level.</span></li><li><strong class="bold">Customizable structure</strong>: By adjusting the criteria, you can achieve the desired chunk size or structure. Recursive chunking adapts well to varying <span class="No-Break">document lengths.</span></li></ul></li>
				<li><strong class="bold">Sentence splitting</strong>: Sentence splitting involves various strategies, such as the ones <span class="No-Break">listed here:</span><ul><li><strong class="bold">Naive splitting</strong>: This method relies on basic punctuation marks (such as periods and new lines) to divide text into sentences. While simple, it might not handle complex sentence <span class="No-Break">structures well.</span></li><li><strong class="bold">spaCy</strong>: Another robust NLP library, spaCy, offers accurate sentence segmentation. It uses statistical models and <span class="No-Break">linguistic rules.</span></li><li><strong class="bold">Natural Language Toolkit (NLTK)</strong>: NLTK, a powerful Python library for NLP, provides efficient sentence tokenization. It considers context and <span class="No-Break">punctuation patterns.</span></li><li><strong class="bold">Advanced tools</strong>: Some tools employ smaller models to predict sentence boundaries, ensuring <span class="No-Break">precise divisions.</span></li></ul></li>
				<li><strong class="bold">Specialized techniques</strong>: Specialized techniques include <span class="No-Break">the following:</span><ul><li><strong class="bold">Structured content</strong>: For documents with specific formats (e.g., Markdown, LaTeX), specialized techniques come <span class="No-Break">into play.</span></li><li><strong class="bold">Intelligent division</strong>: These methods analyze the content’s structure and hierarchy. They create semantically coherent chunks by understanding headings, lists, and other <span class="No-Break">formatting cues.</span></li></ul></li>
			</ul>
			<p>In summary, while fixed-size chunking serves as a baseline, content-aware techniques consider semantics, context, and formatting intricacies. Choosing the right method depends on your data’s unique characteristics and the requirements of your RAG system. While choosing the retriever for storing and retrieving these chunks, you may want to consider solutions such as document hierarchies and knowledge graphs. MongoDB Atlas has a flexible schema and a simple unified query API to query data <span class="No-Break">from it.</span></p>
			<p>Now let’s use the recursive document-splitting strategy to build a simple <span class="No-Break">RAG application.</span></p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor188"/>Simple RAG</h2>
			<p>A simple RAG architecture implements a naive approach where the model retrieves a predetermined number of documents from the knowledge base based on their similarity to the user’s query. These retrieved documents are then combined with the query and input into the language model for generation, as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer079">
					<img alt="" role="presentation" src="image/B22495_08_07.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7: Naive RAG</p>
			<p>To build a simple RAG application, you will use the dataset you loaded to the MongoDB Atlas collection in the <em class="italic">Information retrieval with MongoDB Vector Search</em> section of this chapter. With this application, you’ll be able perform queries on the available movies and create a <span class="No-Break">recommender system.</span></p>
			<h3>LLM</h3>
			<p>This example will use the OpenAI APIs and <strong class="source-inline">gpt-3.5-turbo</strong>, but there are other variations of LLM models made available from OpenAI, such as <strong class="source-inline">gpt-4o</strong> and <strong class="source-inline">gpt-4o-mini</strong>. The same prompting technique can be used with other LLMs, such as <strong class="source-inline">claude-v2</strong> or <strong class="source-inline">mistral8x-7B</strong>, to achieve <span class="No-Break">similar results.</span></p>
			<p>The following is the sample code to invoke the OpenAI LLM <span class="No-Break">using LangChain:</span></p>
			<pre class="source-code">
from openai import OpenAI
client = OpenAI()
def invoke_llm(prompt, model_name='gpt-3.5-turbo-0125'):
    """
    Queries with input prompt to OpenAI API using the chat completion api gets the model's response.
    """
    response = client.chat.completions.create(
      model=model_name,
      messages=[
        {
          «role»: «user»,
          «content»: prompt
        }
      ],
      temperature=0.2,
      max_tokens=256,
      top_p=1,
      frequency_penalty=0,
      presence_penalty=0
    )
    chatbot_response = response.choices[0].message.content.strip()
    return chatbot_response
invoke_llm("This is a test")</pre>			<p>Here is <span class="No-Break">the result:</span></p>
			<pre class="source-code">
'Great! What do you need help with?'</pre>			<p>Now that you have the APIs to call MongoDB Atlas Vector Search for retrieval and an API for invoking an LLM, you can combine these two tools to create a <span class="No-Break">RAG system.</span></p>
			<h3>Prompt</h3>
			<p>A prompt to an LLM is a user-provided instruction or input that guides the model’s response. It can be a question, a statement, or a command, and is designed to drive the LLM to respond with a specific output. The effectiveness of a prompt can greatly influence the quality of the results generated by a RAG-based system, making prompt engineering a crucial aspect for interacting with these models. Good prompts are clear, specific, and structured to communicate the user’s intent to the LLM, enabling it to generate the most accurate and helpful <span class="No-Break">responses possible.</span></p>
			<p>The following is an example of a prompt to perform QA on a private <span class="No-Break">knowledge base:</span></p>
			<pre class="source-code">
def get_prompt(question, context):
    prompt = f"""Question: {question}
            System: Let's think step by step.
            Context: {context}
            """
    return prompt
def get_recommendation_prompt(query, context):
    prompt = f"""
        From the given movie listing data, choose a few great movie recommendations.
        User query: {query}
        Context: {context}
        Movie Recommendations:
        1. Movie_name: Movie_overview
        """
    return prompt</pre>			<p>To demonstrate the benefits of RAG over a foundational LLM, let's first ask the LLM a question without vector search context and then with it included. This will demonstrate how you can improve the accuracy of the results and reduce hallucinations while utilizing a foundational LLM, such as <strong class="source-inline">gpt-3.5-turbo</strong>, that was not trained on a private <span class="No-Break">knowledge base.</span></p>
			<p>Here is the query response without <span class="No-Break">vector search:</span></p>
			<pre class="source-code">
print(invoke_llm("In which movie does a footballer go completely blind?"))</pre>			<p>This is <span class="No-Break">the result:</span></p>
			<pre class="source-code">
The Game of Their Lives" (2005), where the character Davey Herold, a footballer, goes completely blind after being hit in the head during a game</pre>			<p>Although the LLM’s response shows it struggles with factual accuracy, there is still promise in using it alongside human oversight for enterprise applications. Together, these systems can work effectively to power applications for businesses. To help overcome this issue, you need to add context to the prompt through vector <span class="No-Break">search results.</span></p>
			<p>Let's see how you can use the <strong class="source-inline">invoke_llm</strong> function with the <strong class="source-inline">query_vector_search</strong> method to provide the relevant context alongside the user query to generate a response with a factually <span class="No-Break">correct answer:</span></p>
			<pre class="source-code">
idea = "In which movie does a footballer go completely blind?"
search_response = query_vector_search(idea, prefilter={"year":{"$gt": 1990}}, postfilter={"score": {"$gt":0.8}},topK=10)
premise = "\n".join(list(map(lambda x:x['final'], search_response)))
print(invoke_llm(get_prompt(idea, premise)))</pre>			<p>Here is <span class="No-Break">the result:</span></p>
			<pre class="source-code">
The movie in which a footballer goes completely blind is "23 Blast."</pre>			<p>Similarly, you can use the <strong class="source-inline">get_recommendation_prompt</strong> method to generate some movie recommendations using a simple <span class="No-Break">RAG framework:</span></p>
			<pre class="source-code">
question = "I like Christmas movies, any recommendations for movies release after 1990?"
search_response = query_vector_search(question,topK=10)
context = "\n".join(list(map(lambda x:x['final'], search_response)))
print(invoke_llm(get_recommendation_prompt("I like Christmas movies, any recommendations for movies release after 1990?", context)))</pre>			<p>Here is <span class="No-Break">the result:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer080">
					<img alt="" role="presentation" src="image/B22495_08_08.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8: Sample output from the simple RAG application</p>
			<p>The simple RAG system you just built can handle straightforward queries that need answers to the point. Some examples are a customer service chatbot responding to a basic question such as “<strong class="source-inline">Where is the customer support center in Bangalore?</strong>” or helping you find all the restaurants where your favorite delicacy is served in Koramangala. The chatbot can retrieve the contextual piece of information in its retrieval step and generate an answer to this question with the help of <span class="No-Break">the LLM.</span></p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor189"/>Advanced RAG</h2>
			<p>An advanced RAG framework incorporates more complex retrieval techniques, better integration of retrieved information, and often, the ability to iteratively refine both the retrieval and generation processes. In this section, you will learn how to build an intelligent recommendation engine on fashion data that can identify the interest of the user and then generate relevant fashion product or accessory recommendations only when there is intent to purchase a product in the user’s utterance. You will be building an intelligent conversation chatbot that leverages the power of LangChain, MongoDB Atlas Vector Search, and OpenAI in <span class="No-Break">this section.</span></p>
			<p>The advanced RAG system in the current example will demonstrate the <span class="No-Break">following features:</span></p>
			<ul>
				<li>Utilize an LLM to generate multiple searchable fashion queries given a user’s <span class="No-Break">chat utterance</span></li>
				<li>Classify the user’s chat utterance as to whether there is an intent <span class="No-Break">to purchase</span></li>
				<li>Develop a fusion stage that will also fetch vector similarity search results from multiple search queries to fuse them as a single recommendation set that is reranked with the help of <span class="No-Break">an LLM</span></li>
			</ul>
			<p>The flow of steps when a user queries the RAG system is depicted in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.9</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer081">
					<img alt="" role="presentation" src="image/B22495_08_09.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9: Sample advanced RAG, flowchart for query processing and recommendation</p>
			<p>Let’s walk through the code to load the sample dataset and build the advanced RAG system with all the features that were listed at the beginning of <span class="No-Break">this section.</span></p>
			<h3>Loading the dataset</h3>
			<p>For this example, you will utilize fashion data from a popular e-commerce company. The following code shows you how to load a dataset from an S3 bucket to a <strong class="source-inline">pandas</strong> DataFrame and then insert these documents into a MongoDB Atlas <span class="No-Break">collection, </span><span class="No-Break"><strong class="source-inline">search.catalog_final_myn</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
import pandas as pd
import s3fs
import os
import boto3
s3_uri= "https://ashwin-partner-bucket.s3.eu-west-1.amazonaws.com/fashion_dataset.jsonl"
df = pd.read_json(s3_uri, orient="records", lines=True)
print(df[:3])
from pymongo import MongoClient
mongo_client = MongoClient(os.environ["MONGODB_CONNECTION_STR"])
# Upload documents along with vector embeddings to MongoDB Atlas Collection
col = mongo_client["search"]["catalog_final_myn"]
col.insert_many(df.to_dict(orient="records"))</pre>			<p>Here is <span class="No-Break">the result:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer082">
					<img alt="" role="presentation" src="image/B22495_08_10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10: Sample view of the fashion dataset with OpenAI embeddings</p>
			<h3>Creating a vector search index</h3>
			<p>As you can see in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.10</em>, the vector embeddings are already provided as part of the dataset. Therefore, the next step is to create a vector search index. You can create the vector search index by following the steps detailed in <span class="No-Break"><em class="italic">Chapter 5</em></span>, <em class="italic">Vector Databases</em>, using the following <span class="No-Break">index mapping:</span></p>
			<pre class="source-code">
{
    "fields": [
      {
        "type": "vector",
        "numDimensions": 1536,
        "path": "openAIVec",
        "similarity": "cosine"
      }
    ]
}</pre>			<h3>Fashion recommendations using advanced RAG</h3>
			<p>You have successfully loaded the new fashion dataset into the MongoDB Atlas collection and also created a vector search index with all the building blocks in place. You can now use the following code to set up an advanced RAG system and build a recommender system with the features <span class="No-Break">mentioned earlier:</span></p>
			<pre class="source-code">
from langchain_core.output_parsers import JsonOutputParser # type: ignore
from langchain_core.prompts import PromptTemplate # type: ignore
from langchain_core.pydantic_v1 import BaseModel, Field # type: ignore
from langchain_openai import ChatOpenAI # type: ignore
from langchain_community.embeddings import OpenAIEmbeddings # type: ignore
from langchain_mongodb.vectorstores import MongoDBAtlasVectorSearch # type: ignore
from pymongo import MongoClient # type: ignore
from typing import List
from itertools import chain
import certifi # type: ignore
import os
from dotenv import load_dotenv # type: ignore
load_dotenv()
from functools import lru_cache
@lru_cache
def get_openai_emb_transformers():
    """
    Returns an instance of OpenAIEmbeddings for OpenAI transformer models.
    This function creates and returns an instance of the OpenAIEmbeddings class,
    which provides access to OpenAI transformer models for natural language processing.
    The instance is cached using the lru_cache decorator for efficient reuse.
    Returns:
        embeddings (OpenAIEmbeddings): An instance of the OpenAIEmbeddings class.
    """
    embeddings = OpenAIEmbeddings()
    return embeddings
@lru_cache
def get_vector_store():
    """
    Retrieves the vector store for MongoDB Atlas.
    Returns:
        MongoDBAtlasVectorSearch: The vector store object.
    """
    vs = MongoDBAtlasVectorSearch(collection=col, embedding=get_openai_emb_transformers(), index_name="vector_index_openAi_cosine", embedding_key="openAIVec", text_key="title")
    return vs
@lru_cache(10)
def get_conversation_chain_conv():
    """
    Retrieves a conversation chain model for chat conversations.
    Returns:
        ChatOpenAI: The conversation chain model for chat conversations.
    """
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.2, max_tokens=2048)
    # chain = ConversationChain(llm=llm, memory=ConversationBufferWindowMemory(k=5))
    return llm
# Define your desired data structure.
class ProductRecoStatus(BaseModel):
    """
    Represents the status of product recommendations.
    Attributes:
        relevancy_status (bool): Product recommendation status conditioned on the context of the input query.
                                 True if the query is related to purchasing fashion clothing and/or accessories.
                                 False otherwise.
        recommendations (List[str]): List of recommended product titles based on the input query context and
                                     if the relevancy_status is True.
    """
    relevancy_status: bool = Field(description="Product recommendation status is conditioned on the fact if the context of input query is to purchase a fashion clothing and or fashion accessories.")
    recommendations: List[str] = Field(description="list of recommended product titles based on the input query context and if recommendation_status is true.")
class Product(BaseModel):
    """
    Represents a product.
    Attributes:
        title (str): Title of the product.
        baseColour (List[str]): List of base colours of the product.
        gender (List[str]): List of genders the product is targeted for.
        articleType (str): Type of the article.
        mfg_brand_name (str): Manufacturer or brand name of the product.
    """
    title: str = Field(description="Title of the product.")
    baseColour: List[str] = Field(description="List of base colours of the product.")
    gender: List[str] = Field(description="List of genders the product is targeted for.")
    articleType: str = Field(description="Type of the article.")    mfg_brand_name: str = Field(description="Manufacturer or brand name of the product.")
class Recommendations(BaseModel):
    """
    Represents a set of recommendations for products and a message to the user.
    Attributes:
        products (List[Product]): List of recommended products.
        message (str): Message to the user and context of the chat history summary.
    """
    products: List[Product] = Field(description="List of recommended products.")
    message: str = Field(description="Message to the user and context of the chat history summary.")
reco_status_parser = JsonOutputParser(pydantic_object=ProductRecoStatus)
reco_status_prompt = PromptTemplate(    template="You are AI assistant tasked at identifying if there is a product purchase intent in the query and providing suitable fashion recommendations.\n{format_instructions}\n{query}\n\
        #Chat History Summary: {chat_history}\n\nBased on the context of the query, please provide the relevancy status and list of recommended products.",
    input_variables=["query", "chat_history"],
    partial_variables={"format_instructions": reco_status_parser.get_format_instructions()},
)
reco_parser = JsonOutputParser(pydantic_object=Recommendations)
reco_prompt = PromptTemplate(
    input_variables=["question", "recommendations", "chat_history"],
    partial_variables={"format_instructions": reco_parser.get_format_instructions()},
    template="\n User query:{question} \n Chat Summary: {chat_history} \n Rank and suggest me suitable products for creating grouped product recommendations given all product recommendations below feature atleast one product for each articleType \n {recommendations} \n show output in {format_instructions} for top 10 products"
)
def get_product_reco_status(query: str, chat_history: List[str] = []):
    """
    Retrieves the recommendation status for a product based on the given query and chat history.
    Args:
        query (str): The query to be used for retrieving the recommendation status.
        chat_history (List[str]): The chat history containing previous conversations.
    Returns:
        The response containing the recommendation status.
    """
    llm = get_conversation_chain_conv()
    chain = reco_status_prompt | llm | reco_status_parser
    resp = chain.invoke({"query": query, "chat_history": chat_history})
    return resp
def get_sorted_results(product_recommendations):
    all_titles = [rec['title'] for rec in product_recommendations['products']]
    results = list(col.find({"title": {"$in":all_titles}}, {"_id": 0 , "id":1, "title": 1, "price": 1, "baseColour": 1, "articleType": 1, "gender": 1, "link" : 1, "mfg_brand_name": 1}))
    sorted_results = []
    for title in all_titles:
        for result in results:
            if result['title'] == title:
                sorted_results.append(result)
                break
    return sorted_results
def get_product_recommendations(query: str, reco_queries: List[str], chat_history: List[str]=[]):
    """
    Retrieves product recommendations based on the given query and chat history.
    Args:
        query (str): The query string for the recommendation.
        chat_history (List[str]): The list of previous chat messages.
        filter_query (dict): The filter query to apply during the recommendation retrieval.
        reco_queries (List[str]): The list of recommendation queries.
    Returns:
        dict: The response containing the recommendations.
    """
    vectorstore = get_vector_store()
    retr = vectorstore.as_retriever(search_kwargs={"k": 10})
    all_recommendations = list(chain(*retr.batch(reco_queries)))
    llm = get_conversation_chain_conv()
    llm_chain = reco_prompt | llm | reco_parser
    resp = llm_chain.invoke({"question": query, "chat_history": chat_history, "recommendations": [v.page_content for v in all_recommendations]})
    resp = get_sorted_results(resp)
    return resp</pre>			<p>The preceding code carries out the <span class="No-Break">following tasks:</span></p>
			<ol>
				<li>Importing the necessary modules and functions from various libraries. These include <strong class="source-inline">JsonOutputParser</strong> for parsing JSON output, <strong class="source-inline">PromptTemplate</strong> for creating prompts, <strong class="source-inline">BaseModel</strong> and <strong class="source-inline">Field</strong> for defining data models, and <strong class="source-inline">MongoDBAtlasVectorSearch</strong> for interacting with a MongoDB Atlas vector store. It also imports <strong class="source-inline">MongoClient</strong> for connecting to MongoDB, <strong class="source-inline">load_dotenv</strong> for loading environment variables, and <strong class="source-inline">lru_cache</strong> for caching <span class="No-Break">function results.</span></li>
				<li>It defines three functions, each decorated with <strong class="source-inline">lru_cache</strong> to cache their results for efficiency. <strong class="source-inline">get_openai_emb_transformers</strong> returns an instance of <strong class="source-inline">OpenAIEmbeddings</strong>, which provides access to OpenAI transformer models for NLP. <strong class="source-inline">get_vector_store</strong> retrieves the vector store for MongoDB Atlas. <strong class="source-inline">get_conversation_chain_conv</strong> retrieves a conversation chain model for <span class="No-Break">chat conversations.</span></li>
				<li>It defines three classes using Pydantic’s <strong class="source-inline">BaseModel</strong> and <strong class="source-inline">Field</strong>. These classes represent the status of product recommendations (<strong class="source-inline">ProductRecoStatus</strong>), a product (<strong class="source-inline">Product</strong>), and a set of recommendations for products and a message to the <span class="No-Break">user (</span><span class="No-Break"><strong class="source-inline">Recommendations</strong></span><span class="No-Break">).</span></li>
				<li>Creating instances of <strong class="source-inline">JsonOutputParser</strong> and <strong class="source-inline">PromptTemplate</strong> for parsing JSON output and creating prompts, respectively. These instances are used to create conversation chains in the <span class="No-Break">next section.</span></li>
				<li>It defines two functions for retrieving the recommendation status for a product and retrieving product recommendations based on a given query and chat history. <strong class="source-inline">get_product_reco_status</strong> uses a conversation chain to determine the recommendation status for a product based on a given query and chat history. <strong class="source-inline">get_product_recommendations</strong> retrieves product recommendations based on a given query and chat history, a filter query, and a list of recommendation queries. It uses a vector store retriever to get relevant documents for each recommendation query, and then uses a conversation chain to generate the <span class="No-Break">final recommendations.</span></li>
			</ol>
			<p>Let’s now use these methods to create a product recommendations example. Enter the following code and then examine <span class="No-Break">its output:</span></p>
			<pre class="source-code">
query = "Can you suggest me some casual dresses for date occasion with my boyfriend"
status = get_product_reco_status(query)
print(status)
print(get_product_recommendations(query, reco_queries=status["recommendations"], chat_history=[])</pre>			<p>This is the <span class="No-Break">status output:</span></p>
			<pre class="source-code">
{'relevancy_status': True,
 'recommendations': ['Floral Print Wrap Dress',
  'Off-Shoulder Ruffle Dress',
  'Lace Fit and Flare Dress',
  'Midi Slip Dress',
  'Denim Shirt Dress']}</pre>			<p>You can see from the preceding example output that the LLM is able to classify the product intent purchase as positive and recommend suitable queries by performing vector similarity search on the MongoDB <span class="No-Break">Atlas collection.</span></p>
			<p>This is the product <span class="No-Break">recommendations output:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer083">
					<img alt="" role="presentation" src="image/B22495_08_11.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.11: Sample output from the advanced RAG chatbot with recommendations for the user's search intent</p>
			<p>Conversely, you can test the same methods to find a suitable place for a date instead of ideas for gifts or what to wear. In this case, the model will classify the query as having negative product purchase intent and not provide any search <span class="No-Break">term suggestions:</span></p>
			<pre class="source-code">
query = "Where should I take my boy friend for date"
status = get_product_reco_status(query)
print(status)
print(get_conversation_chain_conv().invoke(query).content)</pre>			<p>Here is the <span class="No-Break">status output:</span></p>
			<pre class="source-code">
{'relevancy_status': False, 'recommendations': []}</pre>			<p>Here is the output from <span class="No-Break">the LLM:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer084">
					<img alt="" role="presentation" src="image/B22495_08_12.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.12: Sample output from the advanced RAG system when there is no purchase intent in the query</p>
			<p>Advanced RAG introduces the concept of modularity when building RAG architecture systems. The above example focuses on developing a user flow-based approach for the sample advanced RAG system. It also explores how to leverage LLMs for conditional decision making, recommendation generation, and re-ranking the recommendations retrieved from the retriever system. The goal is to enhance the user experience during interactions with <span class="No-Break">the application.</span></p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor190"/>Summary</h1>
			<p>In this chapter, you explored the pivotal role of vector search in enhancing AI-powered systems. The key takeaway is that vector search plays a vital role in AI applications, addressing the challenge of efficient search as unstructured and multimodal datasets expand. It benefits image recognition, NLP, and <span class="No-Break">recommendation systems.</span></p>
			<p>MongoDB Atlas is used to demonstrate vector search implementation using its flexible schema and vector indexing capabilities. You were able to build a RAG framework for solving QA use cases that combines retrieval and generation models, with a simple RAG system utilizing pre-trained language models and embedding models from OpenAI. You also learned how to build an advanced RAG system that employs iterative refinement and sophisticated retrieval algorithms with the help of LLMs for building a recommendation system for the fashion industry. With these insights, you can now build efficient AI applications for any domain <span class="No-Break">or industry.</span></p>
			<p>In the next chapter, you will delve into the critical aspects of evaluating LLM outputs in such RAG applications and explore various evaluation methods, metrics, and user feedback. You will also learn about the implementation of guardrails to ensure responsible AI deployment and how to better control the behavior of <span class="No-Break">LLM-generated responses.</span></p>
		</div>
	

		<div>
			<div id="_idContainer086">
			</div>
		</div>
		<div id="_idContainer087">
			<h1 id="_idParaDest-152"><a id="_idTextAnchor191"/>Part 3</h1>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor192"/>Optimizing AI Applications: Scaling, Fine-Tuning, Troubleshooting, Monitoring, and Analytics</h1>
			<p>This set of chapters shares techniques and practices for evaluating your AI application as well as strategies and expert insights for improving your application, avoiding pitfalls, and ensuring that your application continues to function optimally despite rapid <span class="No-Break">technological changes.</span></p>
			<p>This part of the book includes the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B22495_09.xhtml#_idTextAnchor193"><em class="italic">Chapter 9</em></a>, <em class="italic">LLM Output Evaluation</em></li>
				<li><a href="B22495_10.xhtml#_idTextAnchor214"><em class="italic">Chapter 10</em></a>, <em class="italic">Refining the Semantic Data Model to Improve Accuracy</em></li>
				<li><a href="B22495_11.xhtml#_idTextAnchor232"><em class="italic">Chapter 11</em></a>, <em class="italic">Common Failures of Generative AI</em></li>
				<li><a href="B22495_12.xhtml#_idTextAnchor253"><em class="italic">Chapter 12</em></a>, <em class="italic">Correcting and Optimizing Your Generative AI Application</em></li>
			</ul>
		</div>
	</body></html>