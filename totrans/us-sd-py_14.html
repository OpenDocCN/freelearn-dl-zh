<html><head></head><body>
		<div><h1 id="_idParaDest-168" class="chapter-number"><a id="_idTextAnchor277"/>14</h1>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor278"/>Generating Video Using Stable Diffusion</h1>
			<p>Harnessing the power of the Stable Diffusion model, we can generate high-quality images using techniques such as LoRA, text embedding, and ControlNet. A natural progression from static images is toward dynamic content, that is, videos. Can we generate consistent videos using the Stable Diffusion model?</p>
			<p>The Stable Diffusion model‚Äôs UNet architecture, while effective for single-image processing, lacks contextual awareness when dealing with multiple images. Consequently, generating identical or consistently related images with the same prompt and parameters but different seeds is challenging. The resulting images may vary significantly in color, shape, or style due to the randomness introduced by the model‚Äôs nature.</p>
			<p>One might consider an image-to-image pipeline or a ControlNet approach, where a video clip is segmented into individual images, and each image is processed sequentially. However, maintaining consistency across the entire sequence, especially when applying significant changes (such as transforming a realistic video into a cartoon), remains a challenge. Even with pose alignment, the output video may still exhibit noticeable flickering.</p>
			<p>A breakthrough came with the publication of <em class="italic">AnimateDiff: Animating Your Personalized Text-to-Image Diffusion Models without Specific Tuning</em> [1] by Yuwei Gao and colleagues. This work paved the way for generating consistent images from text, thereby enabling the creation of short videos.</p>
			<p>In this chapter, we will explore the following:</p>
			<ul>
				<li>The principles of text-to-video generation</li>
				<li>Practical applications of AnimateDiff</li>
				<li>Utilizing Motion LoRA to control animation motion</li>
			</ul>
			<p>By the end of this chapter, you will understand the theoretical aspects of video generation, the inner workings of AnimateDiff, and why this methodology is effective in creating consistent and coherent images. With the provided sample code, you will be able to generate a 16-frame video. You can then apply Motion LoRA to manipulate the animation‚Äôs motion.</p>
			<p>Please note that the results of this chapter cannot be fully appreciated in a static format like paper or PDF. For the best experience, we encourage you to engage with the associated sample code, run it, and observe the generated video.</p>
			<h1 id="_idParaDest-170"><a id="_idTextAnchor279"/>Technical requirements</h1>
			<p>In this chapter, we will employ <code>AnimateDiffPipeline</code>, available in the <code>Diffusers</code> library, to generate videos. You won‚Äôt need to install any extra tools or packages, as Diffusers (after version 0.23.0) offers all the required components and classes. Throughout the chapter, I will guide you through the usage of these features.</p>
			<p>To export the result in MP4 video format, you will also need to install the <code>opencv-python</code> package:</p>
			<pre class="source-code">
pip install opencv-python</pre>
			<p>Also, note that the <code>AnimateDiffPipeline</code> will require at least 8 GB of VRAM to generate a 16-frame 256x256 video clip.</p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor280"/>The principles of text-to-video generation</h1>
			<p>The<a id="_idIndexMarker440"/> Stable Diffusion UNet, while effective for generating single images, falls short when it comes to generating consistent images due to its lack of contextual awareness. Researchers have proposed solutions to overcome this limitation, such as incorporating temporal information from the preceding one or two frames. However, this approach still fails to ensure pixel-level consistency, leading to noticeable differences between consecutive images and flickering in the generated video.</p>
			<p>To address this inconsistency problem, the authors of AnimateDiff trained a separated motion model ‚Äì a zero-initialized convolution side model ‚Äì similar to the ControlNet model. Further, rather than controlling an image, the motion model is applied to a series of continuous frames, as shown in <em class="italic">Figure 14</em><em class="italic">.1</em>:</p>
			<div><div><img src="img/B21263_14_01.jpg" alt="Figure 14.1: The architecture of AnimatedDiff"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.1: The architecture of AnimatedDiff</p>
			<p>The <a id="_idIndexMarker441"/>process involves training a motion modeling module on video datasets to extract motion priors while keeping the base Stable Diffusion model frozen. Motion priors are the prior knowledge about motion in order to guide the generation or customization of videos. During the training stage, a <strong class="bold">motion module</strong> (also called <strong class="bold">Motion UNet</strong>) is <a id="_idIndexMarker442"/>added to the Stable Diffusion UNet. Similar to normal Stable Diffusion V1.5 UNet training, this Motion UNet will work on all frames simultaneously. We can treat them as images in one batch from the same video clip.</p>
			<p>For instance, if we feed in a video with 16 frames, the motion module with attention headers will be trained to consider all 16 frames. If we look into the implementation source code, <code>TransformerTemporalModel</code> [4] is the core component of <code>MotionModules</code> [3].</p>
			<p>During the inference, the time when we want to generate videos, the motion module will be loaded and its weights will be merged into Stable Diffusion UNet. When we want to generate a video with 16 frames, the pipeline will first initialize 16 random latents with Gaussian noise ‚Äì  ùí©(0,1). Without the motion module, the Stable Diffusion UNet will remove noise and generate 16 independent images. However, with the help of the motion module with the Transformer attention header built inside, the motion UNet attempts to create 16 correlated frames. You may ask, why are the images correlated? That is because the frames in the training video are correlated. After the denoising stage, the decoder D from the VAE will convert the 16 latents into pixel images.</p>
			<p>The Motion UNet is responsible for introducing correlations between successive frames in the generated video. It is similar to the correlation of different areas in one image. This is because the attention header pays attention to different parts of the image, and the model learned this knowledge during the training stage. Likewise, during the video generation, the model learned the correlation between frames during the training stage.</p>
			<p>At its core, this approach involves designing an attention mechanism that operates on a sequence of continuous images. By learning the relationships between frames, AnimateDiff can generate more consistent and coherent images from text. Furthermore, since the base Stable Diffusion model remains locked, various Stable Diffusion extension techniques, such as LoRA, textual embedding, ControlNet, and image-to-image generation, can be applied to AnimateDiff as well.</p>
			<p>Anything that works for standard Stable Diffusion, in theory, should also work for AnimateDiff to AnimateDiff as well!</p>
			<p>Before moving on to the next section, be aware that the AnimateDiff model requires a minimum of 12 GB of VRAM to generate a 16-frame, 256x256 video clip. To truly grasp the concept, writing code to utilize AnimateDiff is highly recommended. Now, let‚Äôs proceed to generate a short video (in GIF and MP4 format) using AnimateDif<a id="_idTextAnchor281"/>f.</p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor282"/>Practical applications of AnimateDiff</h1>
			<p>The original AnimateDiff <a id="_idIndexMarker443"/>code and model were released as a standalone GitHub repository [2]. While the author provided sample code and Google Colab to demonstrate the results, users still needed to manually pull the code and download the model file to use it, being cautious about package versions.</p>
			<p>In November 2023, Dhruv Nair [9] merged an AnimateDiff Pipeline for Diffusers, allowing users to generate video clips using the AnimateDiff pertained model without leaving the <code>Diffusers </code>package. Here‚Äôs how to use the AnimatedDiff pipeline from Diffusers:</p>
			<ol>
				<li>Install this specific version of Diffusers with the integrated AnimateDiff code:<pre class="source-code">
pip install diffusers==0.23.0</pre><p class="list-inset">At the time of writing this chapter, the version of Diffusers with the latest AnimateDiff code is 0.23.0. By specifying this version number, you can ensure that the sample code runs smoothly and error-free, as it was tested against this particular version.</p><p class="list-inset">You can<a id="_idIndexMarker444"/> also try installing the latest version of Diffusers, as it may have added more features to the pipeline by the time you read this:</p><pre class="source-code">
pip install -U diffusers</pre></li>
				<li>Load up the motion adapter. We will use the pre-trained motion adapter model from the author of the original paper:<pre class="source-code">
from diffusers import MotionAdapter</pre><pre class="source-code">
adapter = MotionAdapter.from_pretrained(</pre><pre class="source-code">
¬†¬†¬†¬†"guoyww/animatediff-motion-adapter-v1-5-2"</pre><pre class="source-code">
)</pre></li>
				<li>Load up an AnimateDiff pipeline from a Stable Diffusion v1.5-based checkpoint model:<pre class="source-code">
from diffusers import AnimateDiffPipeline</pre><pre class="source-code">
pipe = AnimateDiffPipeline.from_pretrained(</pre><pre class="source-code">
¬†¬†¬†¬†"digiplay/majicMIX_realistic_v6",</pre><pre class="source-code">
¬†¬†¬†¬†motion_adapter¬†¬†¬†¬†= adapterm,</pre><pre class="source-code">
¬†¬†¬†¬†safety_checker¬†¬†¬†¬†= None</pre><pre class="source-code">
)</pre></li>
				<li>Use a proper scheduler. The scheduler plays an important role in the process of generating coherent images. An comparative study conducted by the author of the paper shows different schedulers can lead to different results. Experimentation shows that the <code>EulerAncestralDiscreteScheduler</code> scheduler with the following setting can generate relatively good results:<pre class="source-code">
from diffusers import EulerAncestralDiscreteScheduler</pre><pre class="source-code">
scheduler = EulerAncestralDiscreteScheduler.from_pretrained(</pre><pre class="source-code">
¬†¬†¬†¬†model_path,</pre><pre class="source-code">
¬†¬†¬†¬†subfolder¬†¬†¬†¬†¬†¬†¬†¬†¬†= "scheduler",</pre><pre class="source-code">
¬†¬†¬†¬†clip_sample¬†¬†¬†¬†¬†¬†¬†= False,</pre><pre class="source-code">
¬†¬†¬†¬†timestep_spacing¬†¬†= "linspace",</pre><pre class="source-code">
¬†¬†¬†¬†steps_offset¬†¬†¬†¬†¬†¬†= 1</pre><pre class="source-code">
)</pre><pre class="source-code">
pipe.scheduler = scheduler</pre><pre class="source-code">
pipe.enable_vae_slicing()</pre><pre class="source-code">
pipe.enable_model_cpu_offload()</pre><p class="list-inset">To optimize VRAM usage, you can employ two strategies. First, use <code>pipe.enable_vae_slicing()</code> to configure the <a id="_idIndexMarker445"/>VAE to decode one frame at a time, thereby reducing memory consumption. Additionally, utilize <code>pipe.enable_model_cpu_offload()</code> to offload idle sub-models to the CPU, further decreasing VRAM usage.</p></li>
				<li>Generate coherent images:<pre class="source-code">
import torch</pre><pre class="source-code">
from diffusers.utils import export_to_gif, export_to_video</pre><pre class="source-code">
prompt = """photorealistic, 1girl, dramatic lighting"""</pre><pre class="source-code">
neg_prompt = """worst quality, low quality, normal quality, lowres, bad anatomy, bad hands, monochrome, grayscale watermark, moles"""</pre><pre class="source-code">
#pipe.to("cuda:0")</pre><pre class="source-code">
output = pipe(</pre><pre class="source-code">
¬†¬†¬†¬†prompt = prompt,</pre><pre class="source-code">
¬†¬†¬†¬†negative_prompt = neg_prompt,</pre><pre class="source-code">
¬†¬†¬†¬†height = 256,</pre><pre class="source-code">
¬†¬†¬†¬†width = 256,</pre><pre class="source-code">
¬†¬†¬†¬†num_frames = 16,</pre><pre class="source-code">
¬†¬†¬†¬†num_inference_steps = 30,</pre><pre class="source-code">
¬†¬†¬†¬†guidance_scale= 8.5,</pre><pre class="source-code">
¬†¬†¬†¬†generator = torch.Generator("cuda").manual_seed(7)</pre><pre class="source-code">
)</pre><pre class="source-code">
frames = output.frames[0]</pre><pre class="source-code">
torch.cuda.empty_cache()</pre><pre class="source-code">
export_to_gif(frames, "animation_origin_256_wo_lora.gif")</pre><pre class="source-code">
export_to_video(frames, "animation_origin_256_wo_lora.mp4")</pre></li>
			</ol>
			<p>Now, you<a id="_idIndexMarker446"/> should be able to see a GIF file generated using the 16 frames produced by AnimateDiff. This GIF uses 16 256x256 images. You can apply the image super-resolution techniques introduced in <a href="B21263_11.xhtml#_idTextAnchor214"><em class="italic">Chapter 11</em></a> to upscale the image and create a 512x512 GIF. I will not duplicate the code in this chapter. It is highly recommended to leverage the skills learned in <a href="B21263_11.xhtml#_idTextAnchor214"><em class="italic">Chapter 11</em></a> to further enhance the quality of video gener<a id="_idTextAnchor283"/>ation.</p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor284"/>Utilizing Motion LoRA to control animation motion</h1>
			<p>Besides the<a id="_idIndexMarker447"/> motion adapter model, the author <a id="_idIndexMarker448"/>of the paper also introduced Motion LoRA to control the motion style. Motion LoRA is the same LoRA adapter we introduced in <a href="B21263_08.xhtml#_idTextAnchor153"><em class="italic">Chapter 8</em></a>. As mentioned before, the AnimateDiff pipeline supports all other community-shared LoRAs. You can find these Motion LoRAs on the author‚Äôs Hugging Face repository [8].</p>
			<p>These Motion LoRAs can be used to control the camera view. Here, we will use zoom-in LoRA ‚Äì <code>guoyww/animatediff-motion-lora-zoom-in</code> ‚Äì as an example. The zoom-in will guide the model to generate a video with zoom-in motion.</p>
			<p>The usage is simply one additional line of code:</p>
			<pre class="source-code">
pipe.load_lora_weights("guoyww/animatediff-motion-lora-zoom-in", 
¬†¬†¬†¬†adapter_name="zoom-in")</pre>
			<p>Here is the complete generation code. We are mostly reusing the code from the previous section:</p>
			<pre class="source-code">
import torch
from diffusers.utils import export_to_gif, export_to_video
prompt = """
photorealistic, 1girl, dramatic lighting
"""
neg_prompt = """
worst quality, low quality, normal quality, lowres, bad anatomy, bad hands
, monochrome, grayscale watermark, moles
"""
pipe.to("cuda:0")
pipe.load_lora_weights("guoyww/animatediff-motion-lora-zoom-in", adapter_name="zoom-in")
output = pipe(
¬†¬†¬†¬†prompt = prompt,
¬†¬†¬†¬†negative_prompt = neg_prompt,
¬†¬†¬†¬†height = 256,
¬†¬†¬†¬†width = 256,
¬†¬†¬†¬†num_frames = 16,
¬†¬†¬†¬†num_inference_steps = 40,
¬†¬†¬†¬†guidance_scale = 8.5,
¬†¬†¬†¬†generator = torch.Generator("cuda").manual_seed(123)
)
frames = output.frames[0]
pipe.to("cpu")
torch.cuda.empty_cache()
export_to_gif(frames, "animation_origin_256_w_lora_zoom_in.gif")
export_to_video(frames, "animation_origin_256_w_lora_zoom_in.mp4")</pre>
			<p>You <a id="_idIndexMarker449"/>should see a zoom-in GIF clip is generated <a id="_idIndexMarker450"/>under the same folder, named <code>animation_origin_256_w_lora_zoom_in.gif</code> and an MP4 video clip is generated named <code>animation_origin_256_w_lora_zo<a id="_idTextAnchor285"/>om_in.mp4</code>.</p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor286"/>Summary</h1>
			<p>Every day, the quality and duration of text-to-video samples circulating on social networks are improving. It‚Äôs likely that by the time you read this chapter, the function of the technologies metioned in this chapter will have surpassed what was described here. However, one constant is the concept of training a model to apply an attention mechanism to a sequence of images.</p>
			<p>At the time of writing, OpenAI‚Äôs Sora [9] has just been released. This technology can generate high-quality videos based on the Transformer Diffusion architecture. This is a similar methodology to that used in AnimatedDiff, which combines the Transformer and diffusion models.</p>
			<p>What sets AnimatedDiff apart is its openness and adaptability. It can be applied to any community model with the same base checkpoint version, a feature not currently offered by any other solution. Furthermore, the authors of the paper have completely open-sourced the code and model.</p>
			<p>This chapter primarily discussed the challenges of text-to-image generation, then introduced AnimatedDiff, explaining how and why it works. We also provided a sample code to use the AnimatedDiff pipeline from the Diffusers package to generate a GIF clip from 16 coherent images on your own GPU.</p>
			<p>In the next chapter, we will explore the solutions for generating text descriptions from<a id="_idTextAnchor287"/> an image.</p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor288"/>References</h1>
			<ol>
				<li>Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai, <em class="italic">AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific </em><em class="italic">Tuning</em>: <a href="https://arxiv.org/abs/2307.04725">https://arxiv.org/abs/2307.04725</a></li>
				<li>Original AnimateDiff code repository: <a href="https://github.com/guoyww/AnimateDiff">https://github.com/guoyww/AnimateDiff</a> </li>
				<li>Diffusers Motion modules implementation: <a href="https://github.com/huggingface/diffusers/blob/3dd4168d4c96c429d2b74c2baaee0678c57578da/src/diffusers/models/unets/unet_motion_model.py#L50&#13;">https://github.com/huggingface/diffusers/blob/3dd4168d4c96c429d2b74c2baaee0678c57578da/src/diffusers/models/unets/unet_motion_model.py#L50</a></li>
				<li>Hugging Face Diffusers TransformerTemporalModel implementation: <a href="https://github.com/huggingface/diffusers/blob/3dd4168d4c96c429d2b74c2baaee0678c57578da/src/diffusers/models/transformers/transformer_temporal.py#L41">https://github.com/huggingface/diffusers/blob/3dd4168d4c96c429d2b74c2baaee0678c57578da/src/diffusers/models/transformers/transformer_temporal.py#L41</a> </li>
				<li>[4] Dhruv Nair, <a href="https://github.com/DN6">https://github.com/DN6</a></li>
				<li>AnimateDiff proposal pull request: <a href="https://github.com/huggingface/diffusers/pull/5413">https://github.com/huggingface/diffusers/pull/5413</a></li>
				<li>animatediff-motion-adapter-v1-5-2: <a href="https://huggingface.co/guoyww/animatediff-motion-adapter-v1-5-2">https://huggingface.co/guoyww/animatediff-motion-adapter-v1-5-2</a></li>
				<li>Yuwei Guo‚Äôs Hugging Face repository: <a href="https://huggingface.co/guoyww">https://huggingface.co/guoyww</a></li>
				<li>Video generation models as world simulators: <a href="https://openai.com/research/video-generation-models-as-world-simulators">https://openai.com/research/video-generation-models-as-world-simulators</a></li>
			</ol>
		</div>
	</body></html>