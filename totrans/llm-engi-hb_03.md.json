["```py\n below, we can see the implementation of the ZenML digital_data_etl pipeline, which inputs the user’s full name and a list of links that will be crawled under that user (considered the author of the content extracted from those links). Within the function, we call two steps. In the first one, we look up the user in the database based on its full name. Then, we loop through all the links and crawl each independently. The pipeline’s implementation is available in our repository at pipelines/digital_data_etl.py.\n```", "```py\nfrom zenml import pipeline\nfrom steps.etl import crawl_links, get_or_create_user\n@pipeline\ndef digital_data_etl(user_full_name: str, links: list[str]) -> str:\n    user = get_or_create_user(user_full_name)\n    last_step = crawl_links(user=user, links=links)\n    return last_step.invocation_id \n```", "```py\nfrom loguru import logger\nfrom typing_extensions import Annotated\nfrom zenml import get_step_context, step\nfrom llm_engineering.application import utils\nfrom llm_engineering.domain.documents import UserDocument \n```", "```py\n@step\ndef get_or_create_user(user_full_name: str) -> Annotated[UserDocument, \"user\"]: \n```", "```py\n logger.info(f\"Getting or creating user: {user_full_name}\")\n    first_name, last_name = utils.split_user_full_name(user_full_name)\n    user = UserDocument.get_or_create(first_name=first_name, last_name=last_name)\n    step_context = get_step_context()\n    step_context.add_output_metadata(output_name=\"user\", metadata=_get_metadata(user_full_name, user))\n    return user \n```", "```py\ndef _get_metadata(user_full_name: str, user: UserDocument) -> dict:\n    return {\n        \"query\": {\n            \"user_full_name\": user_full_name,\n        },\n        \"retrieved\": {\n            \"user_id\": str(user.id),\n            \"first_name\": user.first_name,\n            \"last_name\": user.last_name,\n        },\n    } \n```", "```py\nfrom urllib.parse import urlparse\nfrom loguru import logger\nfrom tqdm import tqdm\nfrom typing_extensions import Annotated\nfrom zenml import get_step_context, step\nfrom llm_engineering.application.crawlers.dispatcher import CrawlerDispatcher\nfrom llm_engineering.domain.documents import UserDocument \n```", "```py\n@step\ndef crawl_links(user: UserDocument, links: list[str]) -> Annotated[list[str], \"crawled_links\"]:\n    dispatcher = CrawlerDispatcher.build().register_linkedin().register_medium().register_github()\n    logger.info(f\"Starting to crawl {len(links)} link(s).\") \n```", "```py\n metadata = {}\n    successfull_crawls = 0\n    for link in tqdm(links):\n        successfull_crawl, crawled_domain = _crawl_link(dispatcher, link, user)\n        successfull_crawls += successfull_crawl\n        metadata = _add_to_metadata(metadata, crawled_domain, successfull_crawl) \n```", "```py\n step_context = get_step_context()\n    step_context.add_output_metadata(output_name=\"crawled_links\", metadata=metadata)\n    logger.info(f\"Successfully crawled {successfull_crawls} / {len(links)} \nlinks.\")\n    return links \n```", "```py\ndef _crawl_link(dispatcher: CrawlerDispatcher, link: str, user: UserDocument) -> tuple[bool, str]:\n    crawler = dispatcher.get_crawler(link)\n    crawler_domain = urlparse(link).netloc\n    try:\n        crawler.extract(link=link, user=user)\n        return (True, crawler_domain)\n    except Exception as e:\n        logger.error(f\"An error occurred while crawling: {e!s}\")\n        return (False, crawler_domain) \n```", "```py\ndef _add_to_metadata(metadata: dict, domain: str, successfull_crawl: bool) -> dict:\n    if domain not in metadata:\n        metadata[domain] = {}\n    metadata[domain][\"successful\"] = metadata.get(domain, {}).get(\"successful\", 0) + successfull_crawl\n    metadata[domain][\"total\"] = metadata.get(domain, {}).get(\"total\", 0) + 1\n    return metadata \n```", "```py\nimport re\nfrom urllib.parse import urlparse\nfrom loguru import logger\nfrom .base import BaseCrawler\nfrom .custom_article import CustomArticleCrawler\nfrom .github import GithubCrawler\nfrom .linkedin import LinkedInCrawler\nfrom .medium import MediumCrawler \n```", "```py\nclass CrawlerDispatcher:\n    def __init__(self) -> None:\n        self._crawlers = {} \n```", "```py\n @classmethod\n    def build(cls) -> \"CrawlerDispatcher\":\n        dispatcher = cls()\n        return dispatcher \n```", "```py\n def register_medium(self) -> \"CrawlerDispatcher\":\n        self.register(\"https://medium.com\", MediumCrawler)\n        return self\n    def register_linkedin(self) -> \"CrawlerDispatcher\":\n        self.register(\"https://linkedin.com\", LinkedInCrawler)\n        return self\n    def register_github(self) -> \"CrawlerDispatcher\":\n        self.register(\"https://github.com\", GithubCrawler)\n        return self \n```", "```py\n def register(self, domain: str, crawler: type[BaseCrawler]) -> None:\n        parsed_domain = urlparse(domain)\n        domain = parsed_domain.netloc\n        self._crawlers[r\"https://(www\\.)?{}/*\".format(re.escape(domain))] = crawler \n```", "```py\n def get_crawler(self, url: str) -> BaseCrawler:\n        for pattern, crawler in self._crawlers.items():\n            if re.match(pattern, url):\n                return crawler()\n        else:\n            logger.warning(f\"No crawler found for {url}. Defaulting to CustomArticleCrawler.\")\n            return CustomArticleCrawler() \n```", "```py\ncrawler = dispatcher.get_crawler(link)\ncrawler.extract(link=link, user=user) \n```", "```py\nfrom abc import ABC, abstractmethod\nclass BaseCrawler(ABC):\n    model: type[NoSQLBaseDocument]\n    @abstractmethod\n    def extract(self, link: str, **kwargs) -> None: ... \n```", "```py\nimport time\nfrom tempfile import mkdtemp\nimport chromedriver_autoinstaller\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom llm_engineering.domain.documents import NoSQLBaseDocument\n# Check if the current version of chromedriver exists\n# and if it doesn't exist, download it automatically,\n# then add chromedriver to path\nchromedriver_autoinstaller.install() \n```", "```py\nclass BaseSeleniumCrawler(BaseCrawler, ABC):\n    def __init__(self, scroll_limit: int = 5) -> None:\n        options = webdriver.ChromeOptions()\n\n        options.add_argument(\"--no-sandbox\")\n        options.add_argument(\"--headless=new\")\n        options.add_argument(\"--disable-dev-shm-usage\")\n        options.add_argument(\"--log-level=3\")\n        options.add_argument(\"--disable-popup-blocking\")\n        options.add_argument(\"--disable-notifications\")\n        options.add_argument(\"--disable-extensions\")\n        options.add_argument(\"--disable-background-networking\")\n        options.add_argument(\"--ignore-certificate-errors\")\n        options.add_argument(f\"--user-data-dir={mkdtemp()}\")\n        options.add_argument(f\"--data-path={mkdtemp()}\")\n        options.add_argument(f\"--disk-cache-dir={mkdtemp()}\")\n        options.add_argument(\"--remote-debugging-port=9226\") \n```", "```py\n self.set_extra_driver_options(options)\n        self.scroll_limit = scroll_limit\n        self.driver = webdriver.Chrome(\n            options=options,\n        ) \n```", "```py\n def set_extra_driver_options(self, options: Options) -> None:\n        pass\n    def login(self) -> None:\n        pass \n```", "```py\n def scroll_page(self) -> None:\n        \"\"\"Scroll through the LinkedIn page based on the scroll limit.\"\"\"\n        current_scroll = 0\n        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n        while True:\n            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n            time.sleep(5)\n            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n            if new_height == last_height or (self.scroll_limit and current_scroll >= self.scroll_limit):\n                break\n            last_height = new_height\n            current_scroll += 1 \n```", "```py\nclass GithubCrawler(BaseCrawler):\n    model = RepositoryDocument\n    def __init__(self, ignore=(\".git\", \".toml\", \".lock\", \".png\")) -> None:\n        super().__init__()\n        self._ignore = ignore \n```", "```py\ndef extract(self, link: str, **kwargs) -> None:\n    old_model = self.model.find(link=link)\n    if old_model is not None:\n        logger.info(f\"Repository already exists in the database: {link}\")\n        return \n```", "```py\n logger.info(f\"Starting scrapping GitHub repository: {link}\")\n    repo_name = link.rstrip(\"/\").split(\"/\")[-1]\n    local_temp = tempfile.mkdtemp() \n```", "```py\n try:\n        os.chdir(local_temp)\n        subprocess.run([\"git\", \"clone\", link]) \n```", "```py\n repo_path = os.path.join(local_temp, os.listdir(local_temp)[0])  # \n        tree = {}\n        for root, _, files in os.walk(repo_path):\n            dir = root.replace(repo_path, \"\").lstrip(\"/\")\n            if dir.startswith(self._ignore):\n                continue\n            for file in files:\n                if file.endswith(self._ignore):\n                    continue\n                file_path = os.path.join(dir, file)\n                with open(os.path.join(root, file), \"r\", errors=\"ignore\") as f:\n                    tree[file_path] = f.read().replace(\" \", \"\") \n```", "```py\n user = kwargs[\"user\"]\n        instance = self.model(\n            content=tree,\n            name=repo_name,\n            link=link,\n            platform=\"github\",\n            author_id=user.id,\n            author_full_name=user.full_name,\n        )\n        instance.save() \n```", "```py\n except Exception:\n        raise\n    finally:\n        shutil.rmtree(local_temp)\n    logger.info(f\"Finished scrapping GitHub repository: {link}\") \n```", "```py\nfrom urllib.parse import urlparse\nfrom langchain_community.document_loaders import AsyncHtmlLoader\nfrom langchain_community.document_transformers.html2text import Html2TextTransformer\nfrom loguru import logger\nfrom llm_engineering.domain.documents import ArticleDocument\nfrom .base import BaseCrawler \n```", "```py\nclass CustomArticleCrawler(BaseCrawler):\n    model = ArticleDocument\n    def extract(self, link: str, **kwargs) -> None:\n        old_model = self.model.find(link=link)\n        if old_model is not None:\n            logger.info(f\"Article already exists in the database: {link}\")\n            return \n```", "```py\n logger.info(f\"Starting scrapping article: {link}\")\n        loader = AsyncHtmlLoader([link])\n        docs = loader.load()\n        html2text = Html2TextTransformer()\n        docs_transformed = html2text.transform_documents(docs)\n        doc_transformed = docs_transformed[0] \n```", "```py\n content = {\n            \"Title\": doc_transformed.metadata.get(\"title\"),\n            \"Subtitle\": doc_transformed.metadata.get(\"description\"),\n            \"Content\": doc_transformed.page_content,\n            \"language\": doc_transformed.metadata.get(\"language\"),\n        } \n```", "```py\n parsed_url = urlparse(link)\n        platform = parsed_url.netloc \n```", "```py\n user = kwargs[\"user\"]\n        instance = self.model(\n            content=content,\n            link=link,\n            platform=platform,\n            author_id=user.id,\n            author_full_name=user.full_name,\n        )\n        instance.save()\n        logger.info(f\"Finished scrapping custom article: {link}\") \n```", "```py\nfrom bs4 import BeautifulSoup\nfrom loguru import logger\nfrom llm_engineering.domain.documents import ArticleDocument\nfrom .base import BaseSeleniumCrawler\nclass MediumCrawler(BaseSeleniumCrawler):\n    model = ArticleDocument \n```", "```py\n def set_extra_driver_options(self, options) -> None:\n        options.add_argument(r\"--profile-directory=Profile 2\") \n```", "```py\n def extract(self, link: str, **kwargs) -> None:\n        old_model = self.model.find(link=link)\n        if old_model is not None:\n            logger.info(f\"Article already exists in the database: {link}\")\n            return\n        logger.info(f\"Starting scrapping Medium article: {link}\")\n        self.driver.get(link)\n        self.scroll_page() \n```", "```py\n soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n        title = soup.find_all(\"h1\", class_=\"pw-post-title\")\n        subtitle = soup.find_all(\"h2\", class_=\"pw-subtitle-paragraph\")\n        data = {\n            \"Title\": title[0].string if title else None,\n            \"Subtitle\": subtitle[0].string if subtitle else None,\n            \"Content\": soup.get_text(),\n        } \n```", "```py\n self.driver.close()\n        user = kwargs[\"user\"]\n        instance = self.model(\n            platform=\"medium\",\n            content=data,\n            link=link,\n            author_id=user.id,\n            author_full_name=user.full_name,\n        )\n        instance.save()\n        logger.info(f\"Successfully scraped and saved article: {link}\") \n```", "```py\nfrom sqlalchemy import Column, Integer, String, create_engine\nfrom sqlalchemy.orm import declarative_base, sessionmaker\n   Base = declarative_base()\n# Define a class that maps to the users table.\n   class User(Base):\n    __tablename__ = \"users\"\n    id = Column(Integer, primary_key=True)\n   name = Column(String) \n```", "```py\nengine = create_engine(\"sqlite:///:memory:\")\nBase.metadata.create_all(engine)\n# Create a session used to interact with the database.\nSession = sessionmaker(bind=engine)\nsession = Session()\n# Add a new user.\nnew_user = User(name=\"Alice\")\nsession.add(new_user)\nsession.commit() \n```", "```py\nuser = session.query(User).first()\nif user:\nprint(f\"User ID: {user.id}\")\nprint(f\"User name: {user.name}\") \n```", "```py\nimport uuid\nfrom abc import ABC\nfrom typing import Generic, Type, TypeVar\nfrom loguru import logger\nfrom pydantic import UUID4, BaseModel, Field\nfrom pymongo import errors\nfrom llm_engineering.domain.exceptions import ImproperlyConfigured\nfrom llm_engineering.infrastructure.db.mongo import connection\nfrom llm_engineering.settings import settings\n_database = connection.get_database(settings.DATABASE_NAME) \n```", "```py\nT = TypeVar(\"T\", bound=\"NoSQLBaseDocument\")\nclass NoSQLBaseDocument(BaseModel, Generic[T], ABC): \n```", "```py\nid: UUID4 = Field(default_factory=uuid.uuid4)\ndef __eq__(self, value: object) -> bool:\n    if not isinstance(value, self.__class__):\n        return False\n    return self.id == value.id\ndef __hash__(self) -> int:\n    return hash(self.id) \n```", "```py\n@classmethod\ndef from_mongo(cls: Type[T], data: dict) -> T:\n    if not data:\n        raise ValueError(\"Data is empty.\")\n    id = data.pop(\"_id\")\n    return cls(**dict(data, id=id))\ndef to_mongo(self: T, **kwargs) -> dict:\n    exclude_unset = kwargs.pop(\"exclude_unset\", False)\n    by_alias = kwargs.pop(\"by_alias\", True)\n    parsed = self.model_dump(exclude_unset=exclude_unset, by_alias=by_alias, **kwargs)\n    if \"_id\" not in parsed and \"id\" in parsed:\n        parsed[\"_id\"] = str(parsed.pop(\"id\"))\n    for key, value in parsed.items():\n        if isinstance(value, uuid.UUID):\n            parsed[key] = str(value)\n    return parsed \n```", "```py\ndef save(self: T, **kwargs) -> T | None:\n    collection = _database[self.get_collection_name()]\n    try:\n        collection.insert_one(self.to_mongo(**kwargs))\n        return self\n    except errors.WriteError:\n        logger.exception(\"Failed to insert document.\")\n        return None \n```", "```py\n@classmethod\ndef get_or_create(cls: Type[T], **filter_options) -> T:\n    collection = _database[cls.get_collection_name()]\n    try:\n        instance = collection.find_one(filter_options)\n        if instance:\n            return cls.from_mongo(instance)\n        new_instance = cls(**filter_options)\n        new_instance = new_instance.save()\n        return new_instance\n    except errors.OperationFailure:\n        logger.exception(f\"Failed to retrieve document with filter options: {filter_options}\")\n        raise \n```", "```py\n@classmethod\ndef bulk_insert(cls: Type[T], documents: list[T], **kwargs) -> bool:\n    collection = _database[cls.get_collection_name()]\n    try:\n        collection.insert_many([doc.to_mongo(**kwargs) for doc in documents])\n        return True\n    except (errors.WriteError, errors.BulkWriteError):\nlogger.error(f\"Failed to insert documents of type {cls.__name__}\")\n        return False \n```", "```py\n@classmethod\ndef find(cls: Type[T], **filter_options) -> T | None:\n    collection = _database[cls.get_collection_name()]\n    try:\n        instance = collection.find_one(filter_options)\n        if instance:\n            return cls.from_mongo(instance)\n        return None\n    except errors.OperationFailure:\n        logger.error(\"Failed to retrieve document.\")\n        return None \n```", "```py\n@classmethod\ndef bulk_find(cls: Type[T], **filter_options) -> list[T]:\n    collection = _database[cls.get_collection_name()]\n    try:\n        instances = collection.find(filter_options)\n        return [document for instance in instances if (document := cls.from_mongo(instance)) is not None]\n    except errors.OperationFailure:\n        logger.error(\"Failed to retrieve document.\")\n        return [] \n```", "```py\n@classmethod\ndef get_collection_name(cls: Type[T]) -> str:\n    if not hasattr(cls, \"Settings\") or not hasattr(cls.Settings, \"name\"):\n        raise ImproperlyConfigured(\n            \"Document should define an Settings configuration class with the name of the collection.\"\n        )\n    return cls.Settings.name \n```", "```py\nfrom abc import ABC\nfrom typing import Optional\nfrom pydantic import UUID4, Field\nfrom .base import NoSQLBaseDocument\nfrom .types import DataCategory \n```", "```py\nfrom enum import StrEnum\nclass DataCategory(StrEnum):\n    PROMPT = \"prompt\"\n    QUERIES = \"queries\"\n    INSTRUCT_DATASET_SAMPLES = \"instruct_dataset_samples\"\n    INSTRUCT_DATASET = \"instruct_dataset\"\n    PREFERENCE_DATASET_SAMPLES = \"preference_dataset_samples\"\n    PREFERENCE_DATASET = \"preference_dataset\"\n    POSTS = \"posts\"\n    ARTICLES = \"articles\"\n     REPOSITORIES = \"repositories\" \n```", "```py\nclass Document(NoSQLBaseDocument, ABC):\n    content: dict\n    platform: str\n    author_id: UUID4 = Field(alias=\"author_id\")\n    author_full_name: str = Field(alias=\"author_full_name\") \n```", "```py\nclass RepositoryDocument(Document):\n    name: str\n    link: str\n    class Settings:\n        name = DataCategory.REPOSITORIES\nclass PostDocument(Document):\n    image: Optional[str] = None\n    link: str | None = None\n    class Settings:\n        name = DataCategory.POSTS\nclass ArticleDocument(Document):\n    link: str\n    class Settings:\n        name = DataCategory.ARTICLES \n```", "```py\nclass UserDocument(NoSQLBaseDocument):\n    first_name: str\n    last_name: str\n    class Settings:\n        name = \"users\"\n    @property\n    def full_name(self):\n        return f\"{self.first_name} {self.last_name}\" \n```", "```py\npoetry poe run-digital-data-etl-maxime \n```", "```py\nparameters:\n  user_full_name: Maxime Labonne # [First Name(s)] [Last Name]\n  links:\n    # Personal Blog\n    - https://mlabonne.github.io/blog/posts/2024-07-29_Finetune_Llama31.html\n    - https://mlabonne.github.io/blog/posts/2024-07-15_The_Rise_of_Agentic_Data_Generation.html\n    # Substack\n    - https://maximelabonne.substack.com/p/uncensor-any-llm-with-abliteration-d30148b7d43e\n    - https://maximelabonne.substack.com/p/create-mixtures-of-experts-with-mergekit-11b318c99562\n    - https://maximelabonne.substack.com/p/merge-large-language-models-with-mergekit-2118fb392b54\n    … # More Substack links \n```", "```py\nfrom zenml.client import Client\nartifact = Client().get_artifact_version('8349ce09-0693-4e28-8fa2-20f82c76ddec')\nloaded_artifact = artifact.load() \n```", "```py\nparameters:\n  user_full_name: Paul Iusztin # [First Name(s)] [Last Name]\n  links:\n    # Medium\n    - https://medium.com/decodingml/an-end-to-end-framework-for-production-ready-llm-systems-by-building-your-llm-twin-2cc6bb01141f\n    - https://medium.com/decodingml/a-real-time-retrieval-system-for-rag-on-social-media-data-9cc01d50a2a0\n    - https://medium.com/decodingml/sota-python-streaming-pipelines-for-fine-tuning-llms-and-rag-in-real-time-82eb07795b87\n    … # More Medium links\n    # Substack\n    - https://decodingml.substack.com/p/real-time-feature-pipelines-with?r=1ttoeh\n    - https://decodingml.substack.com/p/building-ml-systems-the-right-way?r=1ttoeh\n    - https://decodingml.substack.com/p/reduce-your-pytorchs-code-latency?r=1ttoeh\n    … # More Substack links \n```", "```py\npoetry poe run-digital-data-etl-paul \n```", "```py\npoetry run python -m tools.run --run-etl --no-cache --etl-config-filename digital_data_etl_paul_iusztin.yaml \n```", "```py\npoetry poe run-digital-data-etl \n```", "```py\nfrom llm_engineering.domain.documents import ArticleDocument, UserDocument\nuser = UserDocument.get_or_create(first_name=\"Paul\", last_name=\"Iusztin\")\narticles = ArticleDocument.bulk_find(author_id=str(user.id))\nprint(f\"User ID: {user.id}\")\nprint(f\"User name: {user.first_name} {user.last_name}\")\nprint(f\"Number of articles: {len(articles)}\")\nprint(\"First article link:\", articles[0].link) \n```", "```py\nUser ID: 900fec95-d621-4315-84c6-52e5229e0b96\nUser name: Paul Iusztin\nNumber of articles: 50\nFirst article link: https://medium.com/decodingml/an-end-to-end-framework-for-production-ready-llm-systems-by-building-your-llm-twin-2cc6bb01141f \n```", "```py\npoetry poe run-import-data-warehouse-from-json \n```"]