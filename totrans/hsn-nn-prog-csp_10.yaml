- en: Training CNNs Using ConvNetSharp
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to use the phenomenal open source package **ConvNetSharp**,
    by Cédric Bovar, to demonstrate how to train our **Convolutional Neural Networks** (**CNNs**).
    In this chapter, we will look at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Common neural network modules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The various terms and concepts related to CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional networks that process images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need Microsoft Visual Studio and ConvNetSharp framework for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Getting acquainted
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we begin diving into code, let's cover some basic terminology so that
    we are all on the same page when referring to things. This terminology applies
    to CNNs as well as the **ConvNetSharp **framework.
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolution**: In mathematics, a *convolution* is an operation performed
    on two functions. This operation produces a third function, which is an expression
    of how the shape of one is modified by the other. This is represented visually
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3fdf9f1-90bd-4897-9781-ccf6e645341b.png)'
  prefs: []
  type: TYPE_IMG
- en: It is important to note that the convolutional layer itself is the building
    block of a CNN. This layer's parameters consist of a set of learnable filters
    (sometimes called **kernels**). These kernels have a small receptive field, which
    is a smaller view into the total image, and this view extends through the full
    depth of the input volume. During the forward propagation phase, each filter is
    **convolved** across the width and the height of the entire input volume. It is
    this convolution that computes the dot product between the filter and the input.
    This then produces a two-dimensional map (sometimes called an **activation map**)
    of the filter. This helps the network learn which filters should activate when
    they detect a feature at that respective input position.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dot product computation**: The following diagram is a visualization of what
    we mean when we say dot product computation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3831e519-e3b9-458d-95fb-c4f32f93a446.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Vol class**: In ConvNetSharp, the `Vol` class is simply a wrapper around
    a one-dimensional list of numbers, their gradients, and dimensions (that is, width,
    depth, and height).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Net class**: In ConvNetSharp, `Net` is a very simple class that contains
    a list of layers. When a `Vol` is passed through the `Net` class, `Net` iterates
    through all its layers, forward-propagates each one by calling the `forward()`
    function, and returns the result of the last layer. During back propagation, `Net`
    calls the `backward()` function of each layer to compute the gradient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layers**: As we know, every neural network is just a linear list of layers,
    and ours is no different. For a neural network, the first layer must be an input
    layer, and our last layer must be an output layer. Every layer takes an input
    `Vol` and produces a new output `Vol`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fully-connected layer**: The fully-connected layer is perhaps the most important
    layer and is definitely the most interesting in terms of what it does. It houses
    a layer of neurons that perform weighted addition of all the inputs. These are
    then passed through a non-linear activation function such as a ReLU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss layers** and **classifier layers**: These layers are helpful when we
    need to predict a set of discrete classes for our data. You can use softmax, SVM,
    and many other types of layers. As always, you should experiment with your particular
    problem to see which one works best.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss layers** and the **L2 regression layer**: This layer takes a list of
    targets and backward-propagates the L2 loss through them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Convolution layer**: This layer is almost a mirror of the fully-connected
    layer. The difference here is that neurons are only connected locally to a few
    neurons in the layer rather than being connected to all of them. They also share
    parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trainers**: The `Trainer` class takes a network and a set of parameters.
    It passes this through the network, sees the predictions, and adjusts the network
    weights to make the provided labels more accurate for that particular input. Over
    time, the process will transform the network and map all the inputs to the correct
    outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With that behind us, let's now talk a bit about CNNs themselves. A CNN consists
    of an input and an output layer; there's no big surprise there. There will be
    one or more hidden layers which consist of convolutional layers, pooling layers,
    fully-connected layers, or normalization layers. It is in these hidden layers
    that the magic happens. Convolutional layers apply a **convolution** operation
    to the input and pass the result to the next layer. We'll talk more about that
    in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: As we progress, the activation maps will be stacked for all of the filters that
    run along the depth dimension. This, in turn, will form the full output volume
    of the layer itself. Each neuron on that layer processes data only for its own
    receptive field (the data view it can see). This information is shared with other
    neurons.
  prefs: []
  type: TYPE_NORMAL
- en: The thing that we have to always keep in mind with a CNN is the input size,
    which can require an extremely high number of neurons to process, depending on
    the resolution of the image. This could become architecturally inconvenient, and
    even intractable, because each pixel is a variable that needs to be processed.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at an example. If we have an image of 100 x 100 pixels, we
    would all agree that this is a small image. However, this image has 10,000 pixels
    in total (100 x 100), all of which are weights for each neuron in the second layer.
    Convolution is key to addressing this issue, as it reduces the number of parameters
    and allows the network to go deeper with fewer parameters. With 10,000 learnable
    parameters, the solution may be totally intractable; however, if we reduce that
    image to a 5 x 5 area, for example, we now have 25 different neurons to handle
    instead of 10,000, which is much more feasible. This will also help us to eliminate,
    or at least greatly reduce, the vanishing or exploding gradient problem we sometimes
    encounter when we train multi-layer networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now take a quick look at how this works visually. As shown in the following
    diagram, we will use the number 6 and run it through a CNN to see if our network
    can detect the number we are trying to draw. The image at the bottom of the following
    screenshot is what we will draw. By the time we convolve things all the way up
    to the top, we should be able to light up the single neuron that denotes the number
    6, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6739312a-092e-4a5c-9d00-86e2217de804.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding screenshot, we can see an input layer (our single number 6),
    convolutional layers, down-sampling layers, and an output layer. Our progression
    is as follows: we start with a 32 x 32 image, which leaves us with 1,024 neurons.
    We then go down to 120 neurons, then to 100 neurons, and finally to 10 neurons
    in our output layer – that''s one neuron for each of the 10 numerical digits.
    You can see that as we progress towards our output layer, the dimension of the
    image decreases. As we can see, we have 32 x 32 in our first convolutional layer,
    10 x 10 in our second, and 5 x 5 in our second pooling layer.'
  prefs: []
  type: TYPE_NORMAL
- en: It's also worth noting that each neuron in the output layer is fully connected
    to all 100 nodes in the fully-connected layer preceding it; hence, the term fully-connected
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we make a three-dimensional drawing of this network and flip it around,
    we can better see how convolution occurs. The following diagram depicts just that,
    as the activated neurons are brighter in color. The layers continue to convolve until
    a decision is made as to which digit we have drawn, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57d82dfd-42e3-4e84-8732-4bea9d17e9cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Filters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the other unique features of a CNN is that many neurons can share the
    same vector of weights and biases, or more formally, the same **filter**. Why
    is that important? Because each neuron computes an output value by applying a
    function to the input values of the previous layer. Incremental adjustments to
    these weights and biases are what helps the network to learn. If the same filter
    can be re-used, then the required memory footprint will be greatly reduced. This
    becomes very important, especially as the image or receptive field gets larger.
  prefs: []
  type: TYPE_NORMAL
- en: 'CNNs have the following distinguishing features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Three-dimensional volumes of neurons**: The layers of a CNN have neurons
    arranged in three dimensions: width, height, and depth. The neurons inside each
    layer are connected to a small region of the layer before it called their receptive
    field. Different types of connected layers are stacked to form the actual convolutional
    architecture, as shown in the following diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/bb61aafc-a0ad-4dc4-88b6-bc5dd911e80a.png)**Convolving**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Shared weights**: In a convolutional neural network, each receptive field
    (filter) is replicated across the entire visual field, as the preceding image
    shows. These filters share the same weight vector and bias parameters, and form
    what is commonly referred to as a **feature map**. This means that all the neurons
    in a given convolutional layer respond to the same feature within their specific
    field. Replicating units in this way allows for features to be detected regardless
    of their position in the visual field. The following diagram is a simple example
    of what this means:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**This is a sample**'
  prefs: []
  type: TYPE_NORMAL
- en: This is a sample
  prefs: []
  type: TYPE_NORMAL
- en: This is a sample
  prefs: []
  type: TYPE_NORMAL
- en: This is a sample
  prefs: []
  type: TYPE_NORMAL
- en: Creating a network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using the ConvNetSharp framework, there are three ways in which to create a
    neural network. First, we can use the `Core.Layers` or `Flow.Layers` objects to
    create a convolutional network (with or without a computational graph), as shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43f2c91a-b159-41aa-96d6-8ad561d05816.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Alternatively, we can create a computational graph like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c529c15-1cf6-4a33-85e6-9e377fac4259.png)'
  prefs: []
  type: TYPE_IMG
- en: Example 1 – a simple example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at our first example. This is a minimal example in which
    we will define a **two**-**layer neural network** and train it on a single data
    point. We are intentionally making this example verbose so that we can walk through
    each step together to improve our understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `InputLayer` variable declares size of input. As shown in the preceding
    code, we use two-dimensional data. Three-dimensional volumes (width, height, and
    depth) are required, but if you''re not dealing with images then we can leave
    the first two dimensions (width and height) at a size of 1, as we have done in
    the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Declare a fully-connected layer comprising `20` neurons, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to declare a Rectified Linear Unit non-linearity (`ReLU`) layer,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, declare a fully-connected layer that will be used by the `SoftmaxLayer`
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Declare the linear classifier on top of the previous hidden layer, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We then need to move forward with a random data point through the network,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`prob` is a volume. Volumes have property weights that store the raw data,
    and weight gradients that store gradients. The following code prints approximately
    0.50101, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to train the network, specifying that `x` is class zero and using
    a stochastic gradient descent trainer, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The output should now be 0.50374, which is slightly higher than the previous
    value of 0.50101\. This is because the network weights have been adjusted by the
    `trainer` to give a higher probability to the class we trained the network with
    (which was zero).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2 – another simple example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As in the previous section, the following example also solves a simple problem,
    while demonstrating how to save and load a graph as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a graph, input the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, to compute the dCost/dW at every node of the graph, we use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To display the graph, input the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Example 3 – our final simple example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following example does a simple calculation and displays the resultant
    computational graph. The code needed is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a graph, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, to compute the dCost/dW at every node of the graph, we use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, to display the graph, input the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Using the Fluent API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For those of you who have the bug for Fluent APIs, ConvNetSharp has done a job
    of providing one for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just look at the following example to see how easy it is to use the Fluent
    DSL when adding layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: GPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to use GPU capability in your software using ConvNetSharp, you must
    have CUDA Version 8 and Cudnn Version 6.0 (April 27, 2017) installed. The `Cudnn
    bin path` should also be referenced in the **PATH** environment variable.
  prefs: []
  type: TYPE_NORMAL
- en: Fluent training with the MNIST database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following example, we will train our CNN against the `MNIST` database
    of images.
  prefs: []
  type: TYPE_NORMAL
- en: 'To declare a function, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, download the training and testing `datasets` with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Load `100` validation sets with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it''s time to create the neural network using the Fluent API, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the stochastic gradient descent trainer from the network with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, get the `NextBatch` of data with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '`Train` the data received with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s now time to get the `NextBatch` of data; to do so, use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The code can be tested with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'To report the `accuracy`, input the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Training the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To train the convolutional network, we must perform both forward- and backward-propagation,
    as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot illustrates our training in progress:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a081492-e7e9-4de1-90cf-575f5f8f5f83.png)'
  prefs: []
  type: TYPE_IMG
- en: Testing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section details the `Test` function, which will show us how to test the
    data we have trained. We get the network prediction and track the accuracy for
    each label that we have with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '`Forward` momentum can be found with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'To track the `accuracy`, input the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Predicting data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Predicting data in this instance means predicting the `argmax` value. To do
    this, we assume that the last layer of the network is a `SoftmaxLayer`. Prediction
    occurs when we call the `GetPrediction()`function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Computational graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following screenshots two computational graphs that we created based on
    our example applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7d55cd18-0124-4aef-aab3-00679b65dea2.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/255f4619-e2a1-41bc-9e33-712f0eac428d.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we used the open source package ConvNetSharp to explain CNNs.
    We looked at how to test and train these networks and also learned why they are
    convolutional. We worked with several example applications to explain how ConvNetSharp
    functions and operates. In the next chapter, we will look at autoencoders and
    RNNSharp, exposing you further to recurrent neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ConvNetSharp Copyright (c) 2018 Cédric Bovar. Used with permission granted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mostly complete chart of neural networks, explained, Asimov Institute, used
    with permission granted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://scs.ryerson.ca/~aharley/vis/conv/flat.html](http://scs.ryerson.ca/~aharley/vis/conv/flat.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
