["```py\n    builder.add_node(\"pay\", payments_agent)\n    ```", "```py\n    def _run_payment(state):\n      result = payments_agent.invoke({\"client_id\"; state[\"client_id\"]})\n     return {\"payment status\": ...}\n    ...\n    builder.add_node(\"pay\", _run_payment)\n    ```", "```py\nfrom datasets import load_dataset\nds = load_dataset(\"cais/mmlu\", \"high_school_geography\")\nds_dict = ds[\"test\"].take(2).to_dict()\nprint(ds_dict[\"question\"][0])\n>> The main factor preventing subsistence economies from advancing economically is the lack of\n```", "```py\nprint(ds_dict[\"choices\"][0])\n>> ['a currency.', 'a well-connected transportation infrastructure.', 'government activity.', 'a banking service.']\n```", "```py\nfrom langchain.agents import load_tools\nfrom langgraph.prebuilt import create_react_agent\nresearch_tools = load_tools(\n  tool_names=[\"ddg-search\", \"arxiv\", \"wikipedia\"],\n  llm=llm)\nsystem_prompt = (\n \"You're a hard-working, curious and creative student. \"\n \"You're preparing an answer to an exam quesion. \"\n \"Work hard, think step by step.\"\n \"Always provide an argumentation for your answer. \"\n \"Do not assume anything, use available tools to search \"\n \"for evidence and supporting statements.\"\n)\n```", "```py\nfrom langchain_core.prompts import ChatPromptTemplate, PromptTemplate\nfrom langgraph.graph import MessagesState\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\nraw_prompt_template = (\n \"Answer the following multiple-choice question. \"\n \"\\nQUESTION:\\n{question}\\n\\nANSWER OPTIONS:\\n{option}\\n\"\n)\nprompt = ChatPromptTemplate.from_messages(\n   [(\"system\", system_prompt),\n    (\"user\", raw_prompt_template),\n    (\"placeholder\", \"{messages}\")\n    ]\n)\nclass MyAgentState(AgentState):\n question: str\n options: str\nresearch_agent = create_react_agent(\n  model=llm_small, tools=research_tools, state_schema=MyAgentState,\n  prompt=prompt)\n```", "```py\nreflection_prompt = (\n \"You are a university professor and you're supervising a student who is \"\n \"working on multiple-choice exam question. \"\n \"nQUESTION: {question}.\\nANSWER OPTIONS:\\n{options}\\n.\"\n \"STUDENT'S ANSWER:\\n{answer}\\n\"\n```", "```py\n \"Reflect on the answer and provide a feedback whether the answer \"\n \"is right or wrong. If you think the final answer is correct, reply with \"\n \"the final answer. Only provide critique if you think the answer might \"\n \"be incorrect or there are reasoning flaws. Do not assume anything, \"\n \"evaluate only the reasoning the student provided and whether there is \"\n \"enough evidence for their answer.\"\n)\nclass Response(BaseModel):\n \"\"\"A final response to the user.\"\"\"\n   answer: Optional[str] = Field(\n       description=\"The final answer. It should be empty if critique has been provided.\",\n       default=None,\n   )\n   critique: Optional[str] = Field(\n       description=\"A critique of the initial answer. If you think it might be incorrect, provide an actionable feedback\",\n       default=None,\n   )\nreflection_chain = PromptTemplate.from_template(reflection_prompt) | llm.with_structured_output(Response)\n```", "```py\nraw_prompt_template_with_critique = (\n \"You tried to answer the exam question and you get feedback from your \"\n \"professor. Work on improving your answer and incorporating the feedback. \"\n \"\\nQUESTION:\\n{question}\\n\\nANSWER OPTIONS:\\n{options}\\n\\n\"\n \"INITIAL ANSWER:\\n{answer}\\n\\nFEEDBACK:\\n{feedback}\"\n)\nprompt = ChatPromptTemplate.from_messages(\n   [(\"system\", system_prompt),\n    (\"user\", raw_prompt_template_with_critique),\n    (\"placeholder\", \"{messages}\")\n    ]\n)\nclass ReflectionState(ResearchState):\n answer: str\n feedback: str\nresearch_agent_with_critique = create_react_agent(model=llm_small, tools=research_tools, state_schema=ReflectionState, prompt=prompt)\n```", "```py\nfrom typing import Annotated, Literal, TypedDict\nfrom langchain_core.runnables.config import RunnableConfig\nfrom operator import add\nfrom langchain_core.output_parsers import StrOutputParser\nclass ReflectionAgentState(TypedDict):\n   question: str\n```", "```py\n   options: str\n   answer: str\n   steps: Annotated[int, add]\n   response: Response\ndef _should_end(state: AgentState, config: RunnableConfig) -> Literal[\"research\", END]:\n   max_reasoning_steps = config[\"configurable\"].get(\"max_reasoning_steps\", 10)\n if state.get(\"response\") and state[\"response\"].answer:\n return END\n if state.get(\"steps\", 1) > max_reasoning_steps:\n return END\n return \"research\"\nreflection_chain = PromptTemplate.from_template(reflection_prompt) | llm.with_structured_output(Response)\ndef _reflection_step(state):\n   result = reflection_chain.invoke(state)\n return {\"response\": result, \"steps\": 1}\ndef _research_start(state):\n answer = research_agent.invoke(state)\n return {\"answer\": answer[\"messages\"][-1].content}\ndef _research(state):\n agent_state = {\n \"answer\": state[\"answer\"],\n \"question\": state[\"question\"],\n \"options\": state[\"options\"],\n \"feedback\": state[\"response\"].critique\n }\n answer = research_agent_with_critique.invoke(agent_state)\n return {\"answer\": answer[\"messages\"][-1].content}\n```", "```py\nbuilder = StateGraph(ReflectionAgentState)\nbuilder.add_node(\"research_start\", _research_start)\nbuilder.add_node(\"research\", _research)\nbuilder.add_node(\"reflect\", _reflection_step)\nbuilder.add_edge(START, \"research_start\")\nbuilder.add_edge(\"research_start\", \"reflect\")\nbuilder.add_edge(\"research\", \"reflect\")\nbuilder.add_conditional_edges(\"reflect\", _should_end)\ngraph = builder.compile()\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```", "```py\nquestion = ds_dict[\"question\"][0]\noptions = \"\\n\".join(\n  [f\"{i}. {a}\" for i, a in enumerate(ds_dict[\"choices\"][0])])\nasync for _, event in graph.astream({\"question\": question, \"options\": options}, stream_mode=[\"updates\"]):\n print(event)\n```", "```py\nBased on the DuckDuckGo search results, none of the provided statements are entirely true.  The searches reveal that while there has been significant progress in women's labor force participation globally,  it hasn't reached a point where most women work in agriculture, nor has there been a worldwide decline in participation.  Furthermore, the information about working hours suggests that it's not universally true that women work longer hours than men in most regions. Therefore, there is no correct answer among the options provided.\n```", "```py\nThe student's reasoning relies on outside search results which are not provided, making it difficult to assess the accuracy of their claims. The student states that none of the answers are entirely true, but multiple-choice questions often have one best answer even if it requires nuance. To properly evaluate the answer, the search results need to be provided, and each option should be evaluated against those results to identify the most accurate choice, rather than dismissing them all. It is possible one of the options is more correct than the others, even if not perfectly true. Without the search results, it's impossible to determine if the student's conclusion that no answer is correct is valid. Additionally, the student should explicitly state what the search results were.\n```", "```py\nasync for _, event in research_agent.astream({\"question\": question, \"options\": options}, stream_mode=[\"values\"]):\n print(len(event[\"messages\"]))\n>> 0\n1\n3\n4\n```", "```py\nasync for _, event in research_agent.astream({\"question\": question, \"options\": options}, stream_mode=[\"updates\"]):\n node = list(event.keys())[0]\n print(node, len(event[node].get(\"messages\", [])))\n>> agent 1\ntools 2\nagent 1\n```", "```py\nseen_events = set([])\nasync for event in research_agent.astream_events({\"question\": question, \"options\": options}, version=\"v1\"):\n if event[\"event\"] not in seen_events:\n   seen_events.add(event[\"event\"])\nprint(seen_events)\n>> {'on_chat_model_end', 'on_chat_model_stream', 'on_chain_end', 'on_prompt_end', 'on_tool_start', 'on_chain_stream', 'on_chain_start', 'on_prompt_start', 'on_chat_model_start', 'on_tool_end'}\n```", "```py\nfrom langgraph.types import Command\ndef _make_payment(state):\n  ...\n if ...:\n return Command(\n     update={\"payment_id\": payment_id},\n     goto=\"refresh_balance\"\n  )\n  ...\n```", "```py\nfrom langgraph.types import Send\ndef _make_payment(state):\n  ...\n if ...:\n return Command(\n     update={\"payment_id\": payment_id},\n     goto=[Send(\"refresh_balance\", {\"payment_id\": payment_id}, ...],\n     graph=Command.PARENT\n  )\n  ...\n```", "```py\nsystem_prompt = (\n \"You're a hard-working, curious and creative student. \"\n \"You're working on exam quesion. Think step by step.\"\n \"Always provide an argumentation for your answer. \"\n \"Do not assume anything, use available tools to search \"\n \"for evidence and supporting statements.\"\n)\nresearch_agent = create_react_agent(\n  model=llm_small, tools=research_tools, prompt=system_prompt)\n```", "```py\nreflection_prompt = (\n \"You are a university professor and you're supervising a student who is \"\n \"working on multiple-choice exam question. Given the dialogue above, \"\n \"reflect on the answer provided and give a feedback \"\n \" if needed. If you think the final answer is correct, reply with \"\n \"an empty message. Only provide critique if you think the last answer \"\n \"might be incorrect or there are reasoning flaws. Do not assume anything, \"\n \"evaluate only the reasoning the student provided and whether there is \"\n \"enough evidence for their answer.\"\n)\n```", "```py\nfrom langgraph.types import Command\nquestion_template = PromptTemplate.from_template(\n \"QUESTION:\\n{question}\\n\\nANSWER OPTIONS:\\n{options}\\n\\n\"\n)\ndef _ask_question(state):\n return {\"messages\": [(\"human\", question_template.invoke(state).text)]}\ndef _give_feedback(state, config: RunnableConfig):\n messages = event[\"messages\"] + [(\"human\", reflection_prompt)]\n max_messages = config[\"configurable\"].get(\"max_messages\", 20)\n if len(messages) > max_messages:\n return Command(update={}, goto=END)\n result = llm.invoke(messages)\n if result.content:\n return Command(\n     update={\"messages\": [(\"assistant\", result.content)]},\n     goto=\"research\"\n )\n return Command(update={}, goto=END)\n```", "```py\nclass ReflectionAgentState(MessagesState):\n question: str\n options: str\nbuilder = StateGraph(ReflectionAgentState)\nbuilder.add_node(\"ask_question\", _ask_question)\nbuilder.add_node(\"research\", research_agent)\nbuilder.add_node(\"reflect\", _give_feedback)\nbuilder.add_edge(START, \"ask_question\")\n```", "```py\nbuilder.add_edge(\"ask_question\", \"research\")\nbuilder.add_edge(\"research\", \"reflect\")\ngraph = builder.compile()\n```", "```py\nfrom langgraph.types import interrupt, Command\nclass State(MessagesState):\n   home_address: Optional[str]\ndef _human_input(state: State):\n   address = interrupt(\"What is your address?\")\n return {\"home_address\": address}\nbuilder = StateGraph(State)\nbuilder.add_node(\"human_input\", _human_input)\nbuilder.add_edge(START, \"human_input\")\ncheckpointer = MemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\nfor chunk in graph.stream({\"messages\": [(\"human\", \"What is weather today?\")]}, config):\n print(chunk)\n>> {'__interrupt__': (Interrupt(value='What is your address?', resumable=True, ns=['human_input:b7e8a744-b404-0a60-7967-ddb8d30b11e3'], when='during'),)}\n```", "```py\nfor chunk in graph.stream(Command(resume=\"Munich\"), config):\n print(chunk)\n>> {'human_input': {'home_address': 'Munich'}}\n```", "```py\nexecution_agent = prompt_template | create_react_agent(model=llm, tools=tools)\n```", "```py\nfrom langchain_core.prompts import ChatPromptTemplate\nclass ReplanStep(BaseModel):\n \"\"\"Replanned next step in the plan.\"\"\"\n   steps: list[str] = Field(\n       description=\"different options of the proposed next step\"\n   )\nllm_replanner = llm.with_structured_output(ReplanStep)\nreplanner_prompt_template = (\n \"Suggest next action in the plan. Do not add any superfluous steps.\\n\"\n \"If you think no actions are needed, just return an empty list of steps. \"\n \"TASK: {task}\\n PREVIOUS STEPS WITH OUTPUTS: {current_plan}\"\n)\nreplanner_prompt = ChatPromptTemplate.from_messages(\n   [(\"system\", \"You're a helpful assistant. You goal is to help with planning actions to solve the task. Do not solve the task itself.\"),\n    (\"user\", replanner_prompt_template)\n   ]\n)\nreplanner = replanner_prompt | llm_replanner\n```", "```py\nclass TreeNode:\n def __init__(\n```", "```py\n       self,\n       node_id: int,\n       step: str,\n       step_output: Optional[str] = None,\n       parent: Optional[\"TreeNode\"] = None,\n ):\n self.node_id = node_id\n self.step = step\n self.step_output = step_output\n self.parent = parent\n self.children = []\n self.final_response = None\n def __repr__(self):\n   parent_id = self.parent.node_id if self.parent else \"None\"\n return f\"Node_id: {self.node_id}, parent: {parent_id}, {len(self.children)} children.\"\n def get_full_plan(self) -> str:\n \"\"\"Returns formatted plan with step numbers and past results.\"\"\"\n   steps = []\n   node = self\n while node.parent:\n     steps.append((node.step, node.step_output))\n     node = node.parent\n   full_plan = []\n for i, (step, result) in enumerate(steps[::-1]):\n if result:\n       full_plan.append(f\"# {i+1}. Planned step: {step}\\nResult: {result}\\n\")\n return \"\\n\".join(full_plan)\n```", "```py\nasync def _run_node(state: PlanState, config: RunnableConfig):\n node = state.get(\"next_node\")\n visited_ids = state.get(\"visited_ids\", set())\n queue = state[\"queue\"]\n if node is None:\n while queue and not node:\n     node = state[\"queue\"].popleft()\n if node.node_id in visited_ids:\n       node = None\n if not node:\n return Command(goto=\"vote\", update={})\n step = await execution_agent.ainvoke({\n \"previous_steps\": node.get_full_plan(),\n \"step\": node.step,\n \"task\": state[\"task\"]})\n node.step_output = step[\"messages\"][-1].content\n visited_ids.add(node.node_id)\n return {\"current_node\": node, \"queue\": queue, \"visited_ids\": visited_ids, \"next_node\": None}\nasync def _plan_next(state: PlanState, config: RunnableConfig) -> PlanState:\n max_candidates = config[\"configurable\"].get(\"max_candidates\", 1)\n node = state[\"current_node\"]\n next_step = await replanner.ainvoke({\"task\": state[\"task\"], \"current_plan\": node.get_full_plan()})\n if not next_step.steps:\n return {\"is_current_node_final\": True}\n max_id = state[\"max_id\"]\n for step in next_step.steps[:max_candidates]:\n   child = TreeNode(node_id=max_id+1, step=step, parent=node)\n   max_id += 1\n   node.children.append(child)\n   state[\"queue\"].append(child)\n```", "```py\n return {\"is_current_node_final\": False, \"next_node\": child, \"max_id\": max_id}\nasync def _get_final_response(state: PlanState) -> PlanState:\n node = state[\"current_node\"]\n final_response = await responder.ainvoke({\"task\": state[\"task\"], \"plan\": node.get_full_plan()})\n node.final_response = final_response\n return {\"paths_explored\": 1, \"candidates\": [final_response]}\n```", "```py\nimport operator\nfrom collections import deque\nfrom typing import Annotated\nclass PlanState(TypedDict):\n   task: str\n   root: TreeNode\n   queue: deque[TreeNode]\n   current_node: TreeNode\n   next_node: TreeNode\n   is_current_node_final: bool\n   paths_explored: Annotated[int, operator.add]\n   visited_ids: set[int]\n   max_id: int\n   candidates: Annotated[list[str], operator.add]\n   best_candidate: str\n```", "```py\ndef my_node(state):\n  queue = state[\"queue\"]\n  node = queue.pop()\n  ...\n  queue.append(another_node)\n return {\"key\": \"value\"}\n```", "```py\nprompt_voting = PromptTemplate.from_template(\n \"Pick the best solution for a given task. \"\n \"\\nTASK:{task}\\n\\nSOLUTIONS:\\n{candidates}\\n\"\n)\ndef _vote_for_the_best_option(state):\n candidates = state.get(\"candidates\", [])\n if not candidates:\n return {\"best_response\": None}\n all_candidates = []\n for i, candidate in enumerate(candidates):\n   all_candidates.append(f\"OPTION {i+1}: {candidate}\")\n response_schema = {\n \"type\": \"STRING\",\n \"enum\": [str(i+1) for i in range(len(all_candidates))]}\n llm_enum = ChatVertexAI(\n     model_name=\"gemini-2.0-flash-001\", response_mime_type=\"text/x.enum\",\n     response_schema=response_schema)\n result = (prompt_voting | llm_enum | StrOutputParser()).invoke(\n     {\"candidates\": \"\\n\".join(all_candidates), \"task\": state[\"task\"]}\n```", "```py\n )\n return {\"best_candidate\": candidates[int(result)-1]}\n```", "```py\nfrom typing import Literal\nfrom langgraph.graph import StateGraph, START, END\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langgraph.types import Command\nfinal_prompt = PromptTemplate.from_template(\n \"You're a helpful assistant that has executed on a plan.\"\n \"Given the results of the execution, prepare the final response.\\n\"\n \"Don't assume anything\\nTASK:\\n{task}\\n\\nPLAN WITH RESUlTS:\\n{plan}\\n\"\n \"FINAL RESPONSE:\\n\"\n)\nresponder = final_prompt | llm | StrOutputParser()\nasync def _build_initial_plan(state: PlanState) -> PlanState:\n plan = await planner.ainvoke(state[\"task\"])\n queue = deque()\n root = TreeNode(step=plan.steps[0], node_id=1)\n queue.append(root)\n current_root = root\n for i, step in enumerate(plan.steps[1:]):\n   child = TreeNode(node_id=i+2, step=step, parent=current_root)\n   current_root.children.append(child)\n   queue.append(child)\n   current_root = child\n return {\"root\": root, \"queue\": queue, \"max_id\": i+2}\n```", "```py\nasync def _get_final_response(state: PlanState) -> PlanState:\n node = state[\"current_node\"]\n final_response = await responder.ainvoke({\"task\": state[\"task\"], \"plan\": node.get_full_plan()})\n node.final_response = final_response\n return {\"paths_explored\": 1, \"candidates\": [final_response]}\ndef _should_create_final_response(state: PlanState) -> Literal[\"run\", \"generate_response\"]:\n return \"generate_response\" if state[\"is_current_node_final\"] else \"run\"\ndef _should_continue(state: PlanState, config: RunnableConfig) -> Literal[\"run\", \"vote\"]:\n max_paths = config[\"configurable\"].get(\"max_paths\", 30)\n if state.get(\"paths_explored\", 1) > max_paths:\n return \"vote\"\n if state[\"queue\"] or state.get(\"next_node\"):\n return \"run\"\n return \"vote\"\n```", "```py\nbuilder = StateGraph(PlanState)\nbuilder.add_node(\"initial_plan\", _build_initial_plan)\nbuilder.add_node(\"run\", _run_node)\nbuilder.add_node(\"plan_next\", _plan_next)\nbuilder.add_node(\"generate_response\", _get_final_response)\nbuilder.add_node(\"vote\", _vote_for_the_best_option)\nbuilder.add_edge(START, \"initial_plan\")\nbuilder.add_edge(\"initial_plan\", \"run\")\nbuilder.add_edge(\"run\", \"plan_next\")\nbuilder.add_conditional_edges(\"plan_next\", _should_create_final_response)\nbuilder.add_conditional_edges(\"generate_response\", _should_continue)\nbuilder.add_edge(\"vote\", END)\n```", "```py\ngraph = builder.compile()\nfrom IPython.display import Image, display\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```", "```py\ntask = \"Write a strategic one-pager of building an AI startup\"\nresult = await graph.ainvoke({\"task\": task}, config={\"recursion_limit\": 10000, \"configurable\": {\"max_paths\": 10}})\nprint(len(result[\"candidates\"]))\nprint(result[\"best_candidate\"])\n```", "```py\nfrom langchain_core.caches import InMemoryCache\nfrom langchain_core.globals import set_llm_cache\ncache = InMemoryCache()\nset_llm_cache(cache)\nllm = ChatVertexAI(model=\"gemini-2.0-flash-001\", temperature=0.5)\nllm.invoke(\"What is the capital of UK?\")\n```", "```py\nimport langchain\nprint(langchain.llm_cache._cache)\n```", "```py\nfrom langgraph.store.memory import InMemoryStore\nin_memory_store = InMemoryStore()\nin_memory_store.put(namespace=(\"users\", \"user1\"), key=\"fact1\", value={\"message1\": \"My name is John.\"})\nin_memory_store.put(namespace=(\"users\", \"user1\", \"conv1\"), key=\"address\", value={\"message\": \"I live in Berlin.\"})\n```", "```py\nin_memory_store.get(namespace=(\"users\", \"user1\", \"conv1\"), key=\"address\")\n>>  Item(namespace=['users', 'user1'], key='fact1', value={'message1': 'My name is John.'}, created_at='2025-03-18T14:25:23.305405+00:00', updated_at='2025-03-18T14:25:23.305408+00:00')\n```", "```py\nin_memory_store.get(namespace=(\"users\", \"user1\"), key=\"conv1\")\n```", "```py\nprint(len(in_memory_store.search((\"users\", \"user1\", \"conv1\"), query=\"name\")))\nprint(len(in_memory_store.search((\"users\", \"user1\"), query=\"name\")))\n>> 1\n2\n```"]