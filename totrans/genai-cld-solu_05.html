<html><head></head><body>
		<div id="_idContainer071">
			<h1 id="_idParaDest-98" class="chapter-number"><a id="_idTextAnchor098"/>5</h1>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor099"/>Effective Prompt Engineering Techniques: Unlocking Wisdom Through AI</h1>
			<p>Prompt engineering emerged as a standout profession in 2023, captivating the tech industry with its profound impact on AI interactions and applications. But what sparked this surge in popularity? The answer lies in the nuanced and intricate nature of the discipline. Understanding the essentials of prompt engineering is crucial; it’s not just about communicating with the model; it’s about crafting prompts that guide the AI to understand the context and nuances of the task at hand. In the previous chapter, we learned about how we can add relevant context through RAG by searching through vector DB. Finally, a prompt needs to be crafted and sent to the LLMs. This leads to more accurate and relevant responses, turning a simple interaction into a robust tool for a variety of cloud-based applications. Whether it’s automating customer support, generating content, or analyzing data, the ability to fine-tune prompts is a game-changer, ensuring that the AI’s capabilities are fully leveraged in a controlled and <span class="No-Break">purposeful manner.</span></p>
			<p>This chapter delves into the techniques for effective prompt engineering, offering strategies to refine interactions for optimal outcomes. This goes hand in hand with the ethical dimensions of prompt engineering, a topic of paramount importance in today’s tech landscape. It addresses the responsibility of developers to ensure that AI interactions are not only efficient and goal-oriented but also ethically sound and bias-free. Finally, the integration of prompt engineering with cloud solutions opens up a new realm of possibilities. It allows for scalable, efficient, and flexible AI solutions that can be seamlessly integrated into existing cloud infrastructure, revolutionizing how businesses and individuals interact with AI. In essence, this chapter is not just an instructional guide but is a cornerstone for building responsible and effective cloud-based <span class="No-Break">GenAI applications.</span></p>
			<p>In this chapter, we will talk about <span class="No-Break">the following:</span></p>
			<ul>
				<li>The essentials of prompt engineering <span class="No-Break">with ChatGPT</span></li>
				<li>What is <span class="No-Break">prompt engineering?</span></li>
				<li>Techniques for effective <span class="No-Break">prompt engineering</span></li>
				<li>The ethical dimensions of <span class="No-Break">prompt engineering</span></li>
			</ul>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/chapter_5-joke.jpg" alt="Figure 5.1 – Comic depiction of Prompt Engineer"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Comic depiction of Prompt Engineer</p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor100"/>The essentials of prompt engineering</h1>
			<p>Before discussing prompt engineering, it is important to first understand the foundational components of a <a id="_idIndexMarker473"/>prompt. In this section, we’ll delve into the key components of a prompt, such as ChatGPT prompts, completions, and tokens. Additionally, grasping what tokens are is pivotal to understanding the model’s constraints and <span class="No-Break">managing costs.</span></p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor101"/>ChatGPT prompts and completions</h2>
			<p>A prompt is an input provided to LLMs, whereas <a id="_idIndexMarker474"/>completions refer to the output of LLMs. The structure and content of a prompt can vary based on the <a id="_idIndexMarker475"/>type of LLM (e.g., the text or image generation model), specific use cases, and the desired output of the <span class="No-Break">language model.</span></p>
			<p>Completions refer to the <a id="_idIndexMarker476"/>response generated by ChatGPT prompts; basically, it is an answer to your<a id="_idIndexMarker477"/> questions. Check out the following example to understand the difference between prompts and<a id="_idIndexMarker478"/> completions when we prompt ChatGPT with, “What is the capital <span class="No-Break">of India?”</span></p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/B21443_05_2.jpg" alt="Figure 5.2 – An image showing a sample LLM prompt and completion"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – An image showing a sample LLM prompt and completion</p>
			<p>Based on the use case, we can leverage one of the two ChatGPT API calls, named <strong class="bold">Completions</strong> or <strong class="bold">ChatCompletions</strong>, to interact with the model. However, OpenAI recommends using the ChatCompletions API in the majority <span class="No-Break">of scenarios.</span></p>
			<h3>Completions API</h3>
			<p>The Completions API is designed to<a id="_idIndexMarker479"/> generate creative, free-form text. You provide a prompt, and the API generates text that continues from it. This is often used for tasks where you want the model to answer a question or generate creative text, such as for writing an article or <span class="No-Break">a poem.</span></p>
			<h3>ChatCompletions API</h3>
			<p>The ChatCompletions API is<a id="_idIndexMarker480"/> designed for multi-turn conversations. You send a series of messages instead of a single prompt, and the model generates a message as a response. The <a id="_idIndexMarker481"/>messages sent to the model include a role (which <a id="_idIndexMarker482"/>can be a <strong class="bold">system</strong>, <strong class="bold">user</strong>, or <strong class="bold">assistant</strong>) and the content of the message. The system role is <a id="_idIndexMarker483"/>used to set the behavior of the assistant, the user role is used to instruct the assistant, and the model’s responses are under the <span class="No-Break">assistant role.</span></p>
			<p>The following is an example of a sample ChatCompletions <span class="No-Break">API call:</span></p>
			<pre class="source-code">
import openai
openai.api_key = 'your-api-key'
response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "You are a helpful sports \
            assistant."},
        {"role": "user", "content": "Who won the cricket world cup \
            in 2011?"},
        {"role": "assistant", "content": "India won the cricket \
            world cup in 2011"},
        {"role": "assistant", "content": "Where was it played"}
    ]
)
print(response['choices'][0]['message']['content'])</pre>			<p>The main difference between the <a id="_idIndexMarker484"/>Completions API and ChatCompletions API is that the Completions API is designed for single-turn tasks, while the ChatCompletions API is designed to handle multiple turns in a conversation, making it more suitable for building conversational agents. However, the ChatCompletions API format can be modified to behave as a Completions API by using a single <span class="No-Break">user message.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The CompletionsAPI, launched in June 2020, initially offered a freeform text interface for Open AI’s language models. However, experience has shown that structured prompts often yield better outcomes. The chat-based approach, especially through the ChatCompletions API, excels in addressing a wide array of needs, offering enhanced flexibility and specificity and reducing prompt injection risks. Its design supports multi-turn conversations and a variety of tasks, enabling developers to create advanced conversational experiences. Hence, Open AI announced that they would be deprecating some of the older models using Completions API and, in moving forward, they would be investing in the ChatCompletions <a id="_idIndexMarker485"/>API to optimize their efforts to use compute capacity. While the Completions API will remain accessible, it shall be labeled as “legacy” in the Open AI <span class="No-Break">developer documentation.</span></p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor102"/>Tokens</h2>
			<p>Understanding the concepts <a id="_idIndexMarker486"/>of tokens is essential, as it helps us better comprehend the restrictions, such as model limitations, and the aspect of cost management when <span class="No-Break">utilizing ChatGPT.</span></p>
			<p>A ChatGPT token is a unit of text that ChatGPT’s language model uses to understand and generate language. In ChatGPT, a token is a sequence of characters that the model uses to generate new sequences of tokens and form a coherent response to a given prompt. The models use tokens to represent words, phrases, and other language elements. The tokens are not cut where the word starts or ends but can consist of trailing spaces, sub words and <span class="No-Break">punctuations, too.</span></p>
			<p>As stated on the OpenAI website, tokens can be thought of as pieces of words. Before the API processes the prompts, the input is broken down <span class="No-Break">into tokens.</span></p>
			<p>To understand tokens in terms of lengths, the following is used as a rule <span class="No-Break">of thumb:</span></p>
			<ul>
				<li>1 token ~= 4 chars <span class="No-Break">in English</span></li>
				<li>1 token ~= ¾ <span class="No-Break">words</span></li>
				<li>100 tokens ~= <span class="No-Break">75 words</span></li>
				<li>1–2 sentences ~= <span class="No-Break">30 tokens</span></li>
				<li>1 paragraph ~= <span class="No-Break">100 tokens</span></li>
				<li>1,500 words ~= <span class="No-Break">2048 tokens</span></li>
				<li>1 US page (8 ½” x 11”) ~= 450 tokens (assuming ~1800 characters <span class="No-Break">per page)</span></li>
			</ul>
			<p>For example, this famous quote from Thomas Edison (“Genius is one percent inspiration and ninety-nine percent perspiration.”) has <span class="No-Break"><strong class="bold">14</strong></span><span class="No-Break"> tokens:</span></p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/B21443_05_3.jpg" alt="Figure 5.3 – Tokenization of sentence"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Tokenization of sentence</p>
			<p>We used the OpenAI <strong class="bold">Tokenizer</strong> tool to <a id="_idIndexMarker487"/>calculate the tokens; the tool can be found at <a href="https://platform.openai.com/tokenizer">https://platform.openai.com/tokenizer</a>. An alternative way to<a id="_idIndexMarker488"/> tokenize text (programmatically) is to <a id="_idIndexMarker489"/>use the <strong class="bold">Tiktoken library</strong> on Github; this can be found <span class="No-Break">at </span><a href="https://github.com/openai/tiktoken"><span class="No-Break">https://github.com/openai/tiktoken</span></a><span class="No-Break">.</span></p>
			<h3>Token limits in ChatGPT models</h3>
			<p>Depending on the model, the<a id="_idIndexMarker490"/> token limits on the model will vary. As of Feb 2024, the token limit for the family of GPT-4 models ranges from 8,192 to 128,000 tokens. This means the sum of prompt and completion tokens for an API call cannot exceed 32,768 tokens for the GPT-4-32K model. If the prompt is 30,000 tokens, the response cannot be more than 2,768 tokens. The GPT4-Turbo 128K is<a id="_idIndexMarker491"/> the most recent model as of Feb 2024, with 128,000 tokens, which is close to 300 pages of text in a single prompt and completion. This is a massive context prompt compared to its <span class="No-Break">predecessor models.</span></p>
			<p>Though this can be a technical limitation, there are creative ways to address the problem of limitation, such as using chunking and condensing your prompts. We discussed chunking strategies in <a href="B21443_04.xhtml#_idTextAnchor070"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, which can help you address <span class="No-Break">token limitations.</span></p>
			<p>The following figure shows various models and <span class="No-Break">token limits:</span></p>
			<table id="table001-3" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Model</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Token Limit</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT-3.5-turbo</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">4,096</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT-3.5-turbo-16k</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">16,384</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT-3.5-turbo-0613</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">4,096</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT-3.5-turbo-16k-0613</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">16,384</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT-4</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">8,192</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT-4-0613</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">32,768</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT-4-32K</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">32,768</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT-4-32-0613</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">32,768</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT-4-Turbo 128K</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">128,000</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – Models and associated Token Limits</p>
			<p>For the latest updates on model<a id="_idIndexMarker492"/> limits for newer versions of models, please check <a id="_idIndexMarker493"/>the <span class="No-Break">OpenAI website.</span></p>
			<h3>Tokens and cost considerations</h3>
			<p>The cost of using ChatGPT or<a id="_idIndexMarker494"/> similar models via an API is often tied to the number of tokens processed, encompassing both the input prompts and the model’s <span class="No-Break">generated responses.</span></p>
			<p>In terms of pricing, providers typically have a per-token charge, leading to a direct correlation between conversation length and cost; the more tokens processed, the higher the cost. The latest cost updates can be found on the <span class="No-Break">OpenAI website.</span></p>
			<p>From an optimization perspective, understanding this cost-token relationship can guide more efficient API usage. For instance, creating more succinct prompts and configuring the model for brief yet effective responses can help control token count and, consequently, <span class="No-Break">manage expenses.</span></p>
			<p>We hope you now have a good understanding of the key components of a prompt. Now, you are ready to learn about prompt engineering. In the next section, we will explore the details of prompt engineering and effective strategies, enabling you to maximize the potential of your prompt contents through the one-shot and few-shot <span class="No-Break">learning approaches.</span></p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor103"/>What is prompt engineering?</h1>
			<p>Prompt engineering is the art of crafting or designing prompts to unlock desired outcomes from large <a id="_idIndexMarker495"/>language models or AI systems. The concept of prompt engineering revolves around the fundamental idea that the quality of your response is intricately tied to the quality of the question you pose. By strategically engineering prompts, one can influence the generated outputs and improve the overall performance and usefulness of the system. In this section, we will learn about the necessary elements of effective prompt design, prompt engineering techniques, best practices, bonus tips, <span class="No-Break">and tricks.</span></p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor104"/>Elements of a good prompt design</h2>
			<p>Designing a good prompt is important because it significantly influences the output of a language model such as GPT. The prompt provides the initial context, sets the task, guides the style and structure <a id="_idIndexMarker496"/>of the response, reduces ambiguities and hallucinations, and supports the optimization of resources, thereby reducing costs and energy use. In this section, let’s understand the elements of good <span class="No-Break">prompt design.</span></p>
			<p>The foundational elements of a good prompt include instructions, questions, input data, <span class="No-Break">and examples:</span></p>
			<ul>
				<li><strong class="bold">Instructions</strong>: The instructions in a<a id="_idIndexMarker497"/> prompt refer to the specific guidelines or directions given to a language model within the input text to guide the kind of response it <span class="No-Break">should produce.</span></li>
				<li><strong class="bold">Questions</strong>: Questions in a prompt <a id="_idIndexMarker498"/>refer to queries or interrogative statements that are included in the input text. The purpose of these questions is to instruct the language model to provide a response or an answer to the query. In order to obtain the results, either the question or instruction <span class="No-Break">is </span><span class="No-Break"><a id="_idIndexMarker499"/></span><span class="No-Break">mandatory.</span></li>
				<li><strong class="bold">Input data</strong>: The purpose of input data is to provide any additional supporting context when prompting the LLM. It could be used to provide new information the model has not previously been trained on for more <span class="No-Break">personalized experiences.</span></li>
				<li><strong class="bold">Examples</strong>: The purpose of examples in a prompt is to provide specific instances or scenarios that illustrate the desired behavior or response from ChatGPT. You can input a prompt that includes one or more examples, typically in the form of <span class="No-Break">input-output </span><span class="No-Break"><a id="_idIndexMarker500"/></span><span class="No-Break">pairs.</span></li>
			</ul>
			<p>The following table shows how to build effective prompts using the aforementioned <span class="No-Break">prompt elements:</span></p>
			<table id="table002" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Sample </strong><span class="No-Break"><strong class="bold">Prompt Formula</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Example</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Questions + <span class="No-Break">Instructions</span></p>
						</td>
						<td class="No-Table-Style">
							<p>How should I create a healthy meal plan for <span class="No-Break">a week?</span></p>
							<p>Include a variety of nutrients and food groups, and explain the benefits of each <span class="No-Break">meal choice.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Instructions + <span class="No-Break">Input Data</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Provide a punchy title in less than 5 words for the <span class="No-Break">paragraph below.</span></p>
							<p>{Jake finally took his brand-new Tesla for a spin on the coastal highway, the smooth hum of the electric motor filling the air as the scenic ocean views <span class="No-Break">passed by.}</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Examples + <span class="No-Break">Question</span></p>
						</td>
						<td class="No-Table-Style">
							<p>I enjoy movies such as Star Wars, Matrix, <span class="No-Break">and Transformers.</span></p>
							<p>What other movies would <span class="No-Break">you recommend?</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – Sample Prompt formula consisting of prompt elements with examples</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor105"/>Prompt parameters</h2>
			<p>ChatGPT prompt <a id="_idIndexMarker501"/>parameters are variables that you can set in the API calls. They allow users to influence the model’s output, customizing the behavior of the model to better fit specific applications or contexts. The following table shows some of the most important parameters of a ChatGPT <span class="No-Break">API call:</span></p>
			<table id="table003" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Parameter</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Description</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Effect </strong><span class="No-Break"><strong class="bold">and Usage</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Model</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Determines the model to be used in the API. Larger models have higher costs <span class="No-Break">and latency.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Select based on the task complexity, cost considerations, and acceptable latency. Always try to use the latest <span class="No-Break">model version.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Temperature</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Controls the randomness of the model’s responses. It can be set between 0 (more focused responses) and 2 (more <span class="No-Break">diverse responses).</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Lower values yield more deterministic responses, which is ideal for more formal or exact responses, such as in legal use cases. Higher values may result in more creative output but can also lead <span class="No-Break">to hallucinations.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Top_P (<span class="No-Break">Nucleus Sampling)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Sets a cumulative probability threshold for the model’s responses. A value of 0.1 implies only the top 10% of probable tokens <span class="No-Break">are considered.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Lower values yield more predictable and focused responses. OpenAI recommends using either Temperature or Top_p, <span class="No-Break">not both.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Max Tokens</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Sets the maximum length of the generated response. This is useful for controlling the length of output and <span class="No-Break">the cost.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Lower values lead to shorter responses, reduced latency, and potentially lower costs, while higher values allow for longer, more <span class="No-Break">detailed responses.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6 – Essential Prompt Parameters</p>
			<p>In this section, only the <a id="_idIndexMarker502"/>top parameters for building an effective prompt are highlighted. For a full list of parameters, refer to the OpenAI API <span class="No-Break">reference (</span><a href="https://platform.openai.com/docs/api-reference"><span class="No-Break">https://platform.openai.com/docs/api-reference</span></a><span class="No-Break">).</span></p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor106"/>ChatGPT roles</h2>
			<h3>System message</h3>
			<p>This is the part where you <a id="_idIndexMarker503"/>design your metaprompts. Metaprompts help to set the initial context, theme, and behavior of the ChatGPT API to guide the model’s<a id="_idIndexMarker504"/> interactions with the user, thus setting roles<a id="_idIndexMarker505"/> or response styles for <span class="No-Break">the assistant.</span></p>
			<p>Metaprompts are structured instructions or guidelines that dictate how the system should interpret and respond to user requests. These metaprompts are designed to ensure that the system’s outputs adhere to specific policies, ethical guidelines, or operational rules. They’re essentially “prompts about how to handle prompts,” guiding the system in generating responses, handling data, or interacting with users in a way that aligns with <span class="No-Break">predefined standards.</span></p>
			<p>The following table is a metaprompt framework that you can follow to design the ChatGPT <span class="No-Break">system message:</span></p>
			<table id="table004" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Elements of </strong><span class="No-Break"><strong class="bold">a Metaprompt</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Description</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Task <span class="No-Break">and Audience</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Explain the intended audience for the application and expectations from <span class="No-Break">the model.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Helper Tools</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Clarify how the model should address user queries and whether there are external tools, such as plugins, APIs, or code, that the model might need to <span class="No-Break">consider utilizing.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Scope of <span class="No-Break">the Task</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Clarify on how the model should respond if a question is out of scope, and set those <span class="No-Break">guard rails.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Posture <span class="No-Break">and Tone</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Setting postures and tones, such as professional, friendly, respectful, and motivational, help improve user experiences with <span class="No-Break">chat application.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Format <span class="No-Break">of Responses</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Based on the requirements of the application, you can set the output format to be of a certain format. It could be a table of contents, a certain programming language, JSON, <span class="No-Break">or XML.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Few-Shot Examples</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Outline the challenging scenarios where the prompts are unclear or complex, providing the model with more insight into how to handle <span class="No-Break">such situations.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Chain-of-Thought Reasoning</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Demonstrate the reasoning process to guide the model in taking the necessary steps to produce the <span class="No-Break">desired results.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Guardrails to Address <span class="No-Break">Specific Harm</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Establish clear boundaries to address and prevent any potential harm that has been recognized and deemed important for the <span class="No-Break">given scenario.</span></p>
							<p>For example, if jailbreaking attempts are detected, you must have clear guardrails to address those jailbreaking attempts in the system message. We will learn more about jailbreaking in <a href="B21443_08.xhtml#_idTextAnchor163"><span class="No-Break"><em class="italic">Chapter 8</em></span></a><span class="No-Break">.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.7 – Elements of a Metaprompt</p>
			<h3>User</h3>
			<p>The messages from the user <a id="_idIndexMarker506"/>serve as prompts or remarks that the assistant is expected to <a id="_idIndexMarker507"/>react to or engage with. what is it establishes the anticipated scope of queries that may come from <span class="No-Break">the user.</span></p>
			<h3>Assistant</h3>
			<p>While not mandatory, assistant <a id="_idIndexMarker508"/>messages can be included to illustrate the <a id="_idIndexMarker509"/>preferred conduct or response patterns of <span class="No-Break">the chatbot.</span></p>
			<p>Now let’s look at an example; if you are developing an application for a fitness AI assistant, a very well-rounded system<a id="_idIndexMarker510"/> message might look <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "system",
            "content":
            "You are a fitness AI assistant. Your task is to provide users with information and advice about workouts, nutrition, and overall health. You will help users by answering their questions to the best of your abilities and providing suggestions based on their input. Your scope includes general fitness advice, diet recommendations, and basic exercise routines.
            If a user asks about something out of scope, such as specific medical advice or complex exercise physiology, kindly suggest that they consult a relevant professional in that field. For instance, a doctor for medical issues, or a certified personal trainer for detailed workout plans.
            You should maintain a supportive and positive tone, being respectful and encouraging to all users, regardless of their current fitness level or knowledge. Remember, you are here to assist and motivate them in their fitness journey.
            Your responses should be concise, easy to understand, and practically useful. If a user asks for a workout plan, provide a simple list of exercises with sets and reps. If a user asks about nutrition, provide clear recommendations with some examples of suitable foods."
        },
        {"role": "user", "content": "I'm new to fitness, can you suggest a simple workout plan to get started?"},
        {
            "role": "assistant",
            "content":
            "Absolutely, I'm here to help you get started on your fitness journey. Here's a simple workout plan you can begin with:
            1. Warm-up: 5-10 minutes of brisk walking or light jogging
            2. Squats: 2 sets of 10 reps
            3. Push-ups: 2 sets of 10 reps (you can start with knee push-ups if full push-ups are too challenging)
            4. Lunges: 2 sets of 10 reps per leg
            5. Plank: Hold for 30 seconds, repeat 2 times
            6. Cool down: 5-10 minutes of stretching
            Remember, the key to a successful workout routine is consistency. Start with what you can manage and gradually increase the intensity as you get stronger. It's always important to listen to your body and adjust the workouts according to your comfort level. Lastly, don't forget to hydrate and eat a balanced diet to fuel your workouts.
            Please consult a fitness professional if you're unsure about the correct form for any of these exercises to avoid injury. Enjoy your workout!"
        }
    ]
)</pre>			<p>The following link provides <a id="_idIndexMarker511"/>great examples of “Act As” prompts for you to leverage in your system<a id="_idIndexMarker512"/> message: <em class="italic">Awesome ChatGPT </em><span class="No-Break"><em class="italic">Prompts</em></span><span class="No-Break"> (</span><a href="https://github.com/f/awesome-chatgpt-prompts/"><span class="No-Break">https://github.com/f/awesome-chatgpt-prompts/</span></a><span class="No-Break">).</span></p>
			<h1 id="_idParaDest-107"><a id="_idTextAnchor107"/>Techniques for effective prompt engineering</h1>
			<p>In the past two years, a wide <a id="_idIndexMarker513"/>array of prompt-engineering techniques have been developed. This section focuses on the essential ones, offering key strategies <a id="_idIndexMarker514"/>that you might find indispensable for daily interactions with ChatGPT and other <span class="No-Break">LLM-based applications.</span></p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor108"/>N-shot prompting</h2>
			<p>N-shot prompting is a<a id="_idIndexMarker515"/> term used in the context of training large language<a id="_idIndexMarker516"/> models, particularly for <strong class="bold">zero-shot</strong> or <strong class="bold">few-shot</strong> learning tasks. It is also <a id="_idIndexMarker517"/>called in-context learning and refers to the technique of providing the model with example prompts along with corresponding responses during training to steer the model’s behavior to provide more <span class="No-Break">accurate responses.</span></p>
			<p>The “N” in “N-shot” refers to the number of example prompts provided to the model. For instance, in a one-shot learning scenario, only one example prompt and its response are given to the model. In an N-shot learning scenario, multiple example prompts and responses <span class="No-Break">are provided.</span></p>
			<p>While ChatGPT works great with zero-shot prompting, it may sometimes be useful to provide examples for a more accurate response. Let’s see some examples of zero-shot and <span class="No-Break">few-shot prompting:</span></p>
			<table id="table005" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Techniques</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Prompt Example</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Zero-shot prompting</strong>: No additional examples are provided in line with <span class="No-Break">the prompt.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>System<a id="_idIndexMarker518"/> message: You are an AI assistant who determines the sentiment of the customer <span class="No-Break">review provided.</span></p>
							<p>(No examples responses provided in the form of assistant response, hence it is called <span class="No-Break">zero-shot prompting.)</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Few-Shot Prompting</strong>: A few examples are provided in line with <span class="No-Break">the prompt.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>System message: You are <a id="_idIndexMarker519"/>an AI assistant <a id="_idIndexMarker520"/>who determines the sentiment of the customer <span class="No-Break">review provided.</span></p>
							<p><span class="No-Break">Example 1:</span></p>
							<p>User: The product <span class="No-Break">is miserable.</span></p>
							<p><span class="No-Break">Assistant: Negative</span></p>
							<p><span class="No-Break">Example 2:</span></p>
							<p>User: This shirt is made from very <span class="No-Break">good material.</span></p>
							<p><span class="No-Break">Assistant: Positive</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.8 – N-shot prompting examples</p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor109"/>Chain-of-thought (CoT) prompting</h2>
			<p>Chain-of-thought <a id="_idIndexMarker521"/>prompting refers to a <a id="_idIndexMarker522"/>sequence of intermediate reasoning steps, significantly boosting the capability of large language models to tackle complex reasoning tasks. By presenting a few chain-of-thought demonstrations as examples in the prompts, the models proficiently handle intricate <span class="No-Break">reasoning tasks:</span></p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/B21443_05_4.jpg" alt="Figure 5.9 – Chain-of-Thought Prompting Examples"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.9 – Chain-of-Thought Prompting Examples</p>
			<p>Figure<a id="_idIndexMarker523"/> sourced <span class="No-Break">from </span><a href="https://arxiv.org/pdf/2201.11903.pdf"><span class="No-Break">https://arxiv.org/pdf/2201.11903.pdf</span></a><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor110"/>Program-aided language (PAL) models</h2>
			<p><strong class="bold">Program-aided language</strong> (<strong class="bold">PAL</strong>) models, also called<a id="_idIndexMarker524"/> <strong class="bold">program-of-thought prompting</strong> (<strong class="bold">PoT</strong>), is a technique that<a id="_idIndexMarker525"/> incorporates additional<a id="_idIndexMarker526"/> task-specific instructions, pseudo-code, rules, or programs alongside the free-form text to guide the behavior of a <span class="No-Break">language model:</span></p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/B21443_05_5.jpg" alt="Figure 5.10 – Program-aided language prompting examples"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.10 – Program-aided language prompting examples</p>
			<p>Figure sourced <span class="No-Break">from </span><a href="https://arxiv.org/abs/2211.10435"><span class="No-Break">https://arxiv.org/abs/2211.10435</span></a><span class="No-Break">.</span></p>
			<p>In this section, although <a id="_idIndexMarker527"/>we have not explored all prompt engineering techniques (only the most important ones), we want to <a id="_idIndexMarker528"/>convey to our readers that there are numerous variants of these techniques, as illustrated in the following figure from the research paper <em class="italic">A Systematic Survey of prompt engineering in Large Language Models: Techniques and Applications</em> (<a href="https://arxiv.org/pdf/2402.07927.pdf">https://arxiv.org/pdf/2402.07927.pdf</a>). This paper provides an extensive inventory of prompt engineering strategies across various application areas, showcasing the evolution and breadth of this field over the last <span class="No-Break">four years:</span></p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/B21443_05_6.jpg" alt="Figure 5.11 – Taxonomy of prompt engineering techniques across multiple application domains"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.11 – Taxonomy of prompt engineering techniques across multiple application domains</p>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor111"/>Prompt engineering best practices</h2>
			<p>In the following list, we<a id="_idIndexMarker529"/> outline additional best practices to optimize and enhance your experience with <span class="No-Break">prompt creation:</span></p>
			<ul>
				<li><strong class="bold">Clarity and precision for accurate responses</strong>: Ensure that prompts are clear, concise, and specific, avoiding ambiguity or <span class="No-Break">multiple interpretations:</span></li>
			</ul>
			<table id="table006" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Bad Prompt</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Good Prompt</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Tell me about World <span class="No-Break">War 1</span></p>
						</td>
						<td class="No-Table-Style">
							<p>How did World War 1 start, and who <span class="No-Break">won it?</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.12 – Best practice: clarity and precision</p>
			<ul>
				<li><strong class="bold">Descriptive</strong>: Be descriptive so that ChatGPT can understand <span class="No-Break">your intent:</span></li>
			</ul>
			<table id="table007" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Bad Prompt</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Good Prompt</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Write a poem <span class="No-Break">about India.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Write a poem about India focusing on its cultural diversity, deciduous cuisine, beautiful wildlife, nature, technology innovation, and <span class="No-Break">film industry.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.13 – Best practice: be descriptive</p>
			<ul>
				<li><strong class="bold">Format the output</strong>: Mention the format of the output, which can be bullet points, paragraphs, sentences, tables, and languages, such as XML, HTML, and JSON. Use examples to <a id="_idIndexMarker530"/>articulate the <span class="No-Break">desired output.</span></li>
				<li><strong class="bold">Adjust the Temperature and Top_p parameters for creativity</strong>: As indicated in the parameters section, modifying the <strong class="bold">Temperatures</strong> and <strong class="bold">Top_p</strong> can significantly influence the variability of the model’s output. In scenarios that call for creativity and imagination, raising the temperature proves beneficial. On the other hand, when dealing with legal applications that demand a reduction in hallucinations, a lower temperature <span class="No-Break">becomes advantageous.</span></li>
				<li><strong class="bold">Use syntax as separators in prompts</strong>: In this example, for a more effective output, use “”” or ### to separate instruction and <span class="No-Break">input data:</span></li>
			</ul>
			<p class="callout-heading">Example:</p>
			<p class="callout">Convert the text below <span class="No-Break">to Spanish</span></p>
			<p class="callout"><span class="No-Break">Text: “””</span></p>
			<p class="callout">{text <span class="No-Break">input here}</span></p>
			<p class="callout">“””</p>
			<ul>
				<li><strong class="bold">Order of the prompt elements matter</strong>: It has been found, in certain instances, that giving an instruction before an example can improve the quality of your outputs. Additionally, the order of examples can affect the output <span class="No-Break">of prompts.</span></li>
				<li><strong class="bold">Use guiding words</strong>: This<a id="_idIndexMarker531"/> helps steer the model toward a specific structure, such as the text highlighted in <span class="No-Break">the following:</span></li>
			</ul>
			<p class="callout-heading">Example:</p>
			<p class="callout"># Create a basic Python <span class="No-Break">function that</span></p>
			<p class="callout"># 1. Requests the user to enter a temperature <span class="No-Break">in Celsius</span></p>
			<p class="callout"># 2. Converts the Celsius temperature <span class="No-Break">to Fahrenheit</span></p>
			<p class="callout"><span class="No-Break">def ctf():</span></p>
			<ul>
				<li><strong class="bold">Instead of saying what not to provide, give alternative recommendations</strong>: Provide an alternative path if ChatGPT is unable to perform a task, such as in the following <span class="No-Break">highlighted message:</span></li>
			</ul>
			<p class="callout-heading">Example:</p>
			<p class="callout">System Message: You are an AI nutrition consultant that provides nutrition consultation based on health and wellness goals of the customer Please note that any questions or inquiries beyond the scope of nutrition consultation will NOT be answered and instead will receive the response: “Sorry! This question falls outside my domain <span class="No-Break">of expertise!”</span></p>
			<p class="callout">Customer: How do I invest <span class="No-Break">in 401K?</span></p>
			<p class="callout">Nutrition AI Assistant: “Sorry! This question falls outside my domain <span class="No-Break">of expertise!”</span></p>
			<ul>
				<li><strong class="bold">Provide example-based prompts</strong>: This helps the language model learn from specific instances <a id="_idIndexMarker532"/>and patterns. Start with a zero-shot, then a few-shot, and if neither of them works, then fine-tune <span class="No-Break">the model.</span></li>
				<li><strong class="bold">Ask ChatGPT to provide citations/sources</strong>: When asking ChatGPT to provide information, you can ask it to answer only using reliable sources and to cite <span class="No-Break">the sources:</span></li>
			</ul>
			<table id="table008" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Prompts</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">ChatGPT Completion</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>When was the steam <span class="No-Break">engine invented?</span></p>
							<p>Answer using only reliable sources and cite <span class="No-Break">the sources</span></p>
						</td>
						<td class="No-Table-Style">
							<p>The steam engine was invented during the late 17th century. The first practical steam engine is credited to Thomas Savery, an English military engineer and inventor, who patented his design in 1698. Savery’s steam engine was primarily used for pumping water out <span class="No-Break">of mines.</span></p>
							<p>Source: Encyclopædia Britannica. (2021, September 2). Thomas Savery. In Encyclopædia <span class="No-Break">Britannica. </span><a href="https://www.britannica.com/biography/Thomas-Savery"><span class="No-Break">https://www.britannica.com/biography/Thomas-Savery</span></a><span class="No-Break">.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.14 – Best practice: provide citations</p>
			<ul>
				<li><strong class="bold">Break down a complex task into simpler tasks</strong>: See the <span class="No-Break">following example:</span></li>
			</ul>
			<table id="table009" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Bad Prompt</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Effective Prompt</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Provide a summary of the following and provide key points from <span class="No-Break">the summary</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Provide a summary of the following text: [Insert long <span class="No-Break">text here]</span></p>
							<p>Summarize the key points from the following passage: [Insert long <span class="No-Break">text here]</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.15 – Best practice: break down a complex task</p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor112"/>Bonus tips and tricks</h2>
			<p>The following list provides some<a id="_idIndexMarker533"/> helpful bonus tips <span class="No-Break">and tricks:</span></p>
			<ul>
				<li><strong class="bold">Use of tags</strong>: Tags, such as <strong class="source-inline">&lt;begin&gt;</strong>, <strong class="source-inline">&lt;end&gt;</strong>, and <strong class="source-inline">&lt;|endofprompt|&gt;</strong>, that determine the beginning and end of prompts can help separate the different elements of a prompt. This can help generate <span class="No-Break">high-quality output.</span></li>
				<li><strong class="bold">Use of languages</strong>: Though ChatGPT performs best with English, it can be used to generate responses in several <span class="No-Break">other languages.</span></li>
				<li><strong class="bold">Obtaining the most accurate, up-to-date information</strong>: This can be achieved by using the grounding process with a <strong class="bold">retrieval augmented generation</strong> (<strong class="bold">RAG</strong>) architecture and plugins, as discussed in <a href="B21443_04.xhtml#_idTextAnchor070"><span class="No-Break"><em class="italic">Chapter 4</em></span></a> already. This helps in addressing the knowledge cutoff limitation <span class="No-Break">of LLMs.</span></li>
			</ul>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor113"/>Ethical guidelines for prompt engineering</h1>
			<p>Prompt engineering is a <a id="_idIndexMarker534"/>critical stage where AI behavior is molded, and incorporating ethics at this level helps ensure that AI language models are developed and deployed responsibly. It promotes fairness, transparency, and user trust while avoiding potential risks and negative <span class="No-Break">societal impact.</span></p>
			<p>While <a href="B21443_04.xhtml#_idTextAnchor070"><span class="No-Break"><em class="italic">Chapter 4</em></span></a> delved further into constructing ethical generative AI solutions, in this section, our focus will be on briefly discussing the integration of ethical approaches at the prompt <span class="No-Break">engineering level:</span></p>
			<ul>
				<li><strong class="bold">Diverse and </strong><span class="No-Break"><strong class="bold">representative data</strong></span><ul><li>When fine-tuning the model with few-shot examples, use training data that represent diverse perspectives <span class="No-Break">and demographics.</span></li><li>If the AI language model is intended for healthcare, the training data should cover medical cases from different demographics <span class="No-Break">and regions.</span></li><li>For instance, if a user poses a question to the LLM, such as, “Can you describe some global traditional festivals?” the response should offer a comprehensive view that encompasses a multitude of countries rather than focusing on just one. This can be ensured by including diverse few-shot examples in <span class="No-Break">the </span><span class="No-Break"><a id="_idIndexMarker535"/></span><span class="No-Break">prompts.</span></li></ul></li>
				<li><strong class="bold">Bias detection </strong><span class="No-Break"><strong class="bold">and mitigation</strong></span><ul><li>Identify and address biases in the model’s outputs to <span class="No-Break">ensure fairness.</span></li><li>Implementing debiasing techniques to reduce gender or <span class="No-Break">racial biases.</span></li><li>Ensuring that generated content related to sensitive topics is neutral <span class="No-Break">and unbiased.</span></li><li>For instance, if a user asks the LLM, “What is the gender of a nurse?” improperly trained models might default to “female” due to biases in their training data. To address this, it’s vital to incorporate few-shot examples that emphasize nurses can be of any gender, be it male <span class="No-Break">or female.</span></li></ul></li>
				<li><strong class="bold">Reduce misinformation </strong><span class="No-Break"><strong class="bold">and disinformation</strong></span><ul><li>As AI language models can inadvertently generate false or misleading information due to model “hallucinations,” implement measures to minimize the spread of misinformation and disinformation through carefully crafted prompts <span class="No-Break">and responses.</span></li><li>For example, based on the guidelines from the prompt engineering section and <a href="B21443_03.xhtml#_idTextAnchor052"><em class="italic">Chapter 3</em></a>’s grounding techniques, system prompts should clearly state their scope, such as, “Your scope is XYZ.” If a user asks about something outside this, such as ABC, the system should have a <span class="No-Break">set response.</span></li></ul></li>
				<li><strong class="bold">Privacy and </strong><span class="No-Break"><strong class="bold">data security</strong></span><ul><li>When engineering prompts, one must prioritize user privacy and <span class="No-Break">data security.</span></li><li>Prompt engineers should be transparent about data usage, gain user consent, and implement safeguards to protect <span class="No-Break">sensitive information.</span></li><li>For example, when crafting prompts, system messages, or providing few-shot examples, it is essential to exclude personal user data such as social security numbers, credit card details, <span class="No-Break">and passwords.</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Content moderation</strong></span><ul><li>Implement mechanisms to filter out harmful or <span class="No-Break">inappropriate content.</span></li><li>Use profanity filters to prevent offensive language. Apply keyword filters to avoid<a id="_idIndexMarker536"/> generating content that promotes violence <span class="No-Break">or discrimination.</span></li><li>For example, if someone asks, “How to create a bomb?”, the LLM should not answer. Set clear rules around the scope in the system message to prevent this (as discussed in the P<em class="italic">rompt engineering best </em><span class="No-Break"><em class="italic">practices</em></span><span class="No-Break"> section).</span></li></ul></li>
				<li><strong class="bold">User consent </strong><span class="No-Break"><strong class="bold">and control</strong></span><ul><li>Ensure users are aware of AI interactions and have control <span class="No-Break">over them.</span></li><li>Clearly inform users that they are interacting with an AI <span class="No-Break">language model.</span></li><li>For example, whenever a user initiates a chat with an LLM, they should receive a notification that says, “You are now conversing with an LLM,” or a <span class="No-Break">similar message.</span></li></ul></li>
				<li><strong class="bold">Regular audits </strong><span class="No-Break"><strong class="bold">and testing</strong></span><ul><li>Conduct routine audits and tests regarding prompts to identify and address <span class="No-Break">ethical issues.</span></li><li>For instance, users should try various versions of a prompt to verify diverse responses, protect user privacy, and follow content moderation guidelines. This is an essential aspect of <a id="_idIndexMarker537"/>operationalizing LLM models, also known <span class="No-Break">as LLMOps.</span></li></ul></li>
				<li><strong class="bold">Education </strong><span class="No-Break"><strong class="bold">and training</strong></span><ul><li>Train prompt engineers and developers about ethical AI practices on an <span class="No-Break">ongoing basis</span></li></ul></li>
				<li><strong class="bold">Ethics guidelines </strong><span class="No-Break"><strong class="bold">and policies</strong></span><ul><li>Develop clear guidelines and policies for <span class="No-Break">prompt engineering</span></li><li>Establish an ethics<a id="_idIndexMarker538"/> charter that outlines the principles followed in <span class="No-Break">prompt engineering</span></li><li>Defining a content safety policy that prohibits harmful or <span class="No-Break">offensive outputs</span></li></ul></li>
			</ul>
			<p>Microsoft’s <strong class="bold">Responsible AI</strong> team has been a trailblazer in terms of steering the AI revolution with ethical practices. The following figure<a id="_idIndexMarker539"/> published by Microsoft can serve as a guide to structuring safety metaprompts, focusing on four core elements: <strong class="bold">response</strong> <strong class="bold">grounding</strong>, <strong class="bold">tone</strong>, <strong class="bold">safety</strong>, and <strong class="bold">jailbreaks</strong>. This approach is instrumental in implementing a robust safety system within the application layer. However, in <a href="B21443_09.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, we will delve into<a id="_idIndexMarker540"/> more detail regarding the best practices of responsible AI for generative <span class="No-Break">AI applications:</span></p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/B21443_05_7.jpg" alt="Figure 5.16 – Metaprompt best practices from Microsoft"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.16 – Metaprompt best practices from Microsoft</p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor114"/>Summary</h1>
			<p>In summary, in this chapter, we have outlined the fundamentals of prompt engineering, offering insights into how to formulate effective prompts that maximize the potential of LLMs. Additionally, we have examined prompt engineering from an ethical perspective. Thus far, in this book, we have explored the essential elements and methodologies necessary for constructing a solid generative AI framework. In the next chapter, we will integrate these concepts with application development strategies for generative AI involving agents. We will also discuss methods for operationalizing these strategies through LLMOps, which stands as a critical component in the <span class="No-Break">automation process.</span></p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor115"/>References</h1>
			<ul>
				<li><em class="italic">Introduction to Prompt </em><span class="No-Break"><em class="italic">Engineering</em></span><span class="No-Break">: </span><a href="https://tinyurl.com/azu5ubma"><span class="No-Break">https://tinyurl.com/azu5ubma</span></a></li>
				<li><em class="italic">Prompt Engineering </em><span class="No-Break"><em class="italic">Guide</em></span><span class="No-Break">: </span><a href="https://www.promptingguide.ai/"><span class="No-Break">https://www.promptingguide.ai/</span></a></li>
				<li><em class="italic">Awesome ChatGPT </em><span class="No-Break"><em class="italic">prompts</em></span><span class="No-Break">: </span><a href="https://github.com/f/awesome-chatgpt-prompts"><span class="No-Break">https://github.com/f/awesome-chatgpt-prompts</span></a></li>
				<li><em class="italic">Best practices for prompt engineering with Open </em><span class="No-Break"><em class="italic">AI:</em></span><span class="No-Break"> </span><a href="https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api"><span class="No-Break">https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api</span></a></li>
				<li><em class="italic">Azure OpenAI Service: Azure OpenAI | Microsoft </em><span class="No-Break"><em class="italic">Learn:</em></span><span class="No-Break"> </span><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering"><span class="No-Break">https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering</span></a></li>
				<li><em class="italic">System Message </em><span class="No-Break"><em class="italic">Framework:</em></span><span class="No-Break"> </span><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/system-message"><span class="No-Break">https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/system-message</span></a></li>
				<li><em class="italic">Ethics in Prompt </em><span class="No-Break"><em class="italic">Engineering</em></span><span class="No-Break">: </span><a href="https://promptengineeringai.in/ethics-of-prompt-engineering/#:~:text=Prompt%20engineering%20should%20respect%20user,their%20data%20is%20being%20used"><span class="No-Break">https://promptengineeringai.in/ethics-of-prompt-engineering/#:~:text=Prompt%20engineering%20should%20respect%20user,their%20data%20is%20being%20used</span></a><span class="No-Break">.</span></li>
				<li><em class="italic">Ethics of Prompt Engineeering and its potential </em><span class="No-Break"><em class="italic">Implications:</em></span><span class="No-Break"> </span><a href="https://promptengineering.guide/article/The_ethics_of_prompt_engineering_and_its_potential_implications.html"><span class="No-Break">https://promptengineering.guide/article/The_ethics_of_prompt_engineering_and_its_potential_implications.html</span></a></li>
				<li><em class="italic">A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and </em><span class="No-Break"><em class="italic">Applications</em></span><span class="No-Break">: (</span><a href="https://arxiv.org/pdf/2402.07927.pdf"><span class="No-Break">https://arxiv.org/pdf/2402.07927.pdf</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">Chain of Thought </em><span class="No-Break"><em class="italic">Prompting</em></span><span class="No-Break">: </span><a href="https://arxiv.org/abs/2201.11903"><span class="No-Break">https://arxiv.org/abs/2201.11903</span></a></li>
			</ul>
		</div>
	

		<div id="_idContainer072" class="Content">
			<h1 id="_idParaDest-116" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor116"/>Part 3: Developing, Operationalizing, and Scaling Generative  AI Applications</h1>
		</div>
		<div id="_idContainer073">
			<p>In this section, we will explore important concepts such as agents, copilots, and autonomous agents, alongside discussing prominent application development frameworks such as Semantic Kernel and LangChain, as well as the agent collaboration framework AutoGen, which are currently very popular. This discussion aims to guide you in creating strong autonomous generative AI applications. We will also concentrate on strategies for deploying these generative AI applications in a live production environment and scaling them efficiently for a large enterprise-wide scenario, considering the existing rate limits of <strong class="bold">Large Language Model</strong> (<span class="No-Break"><strong class="bold">LLM</strong></span><span class="No-Break">) APIs.</span></p>
			<p>This part contains the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B21443_06.xhtml#_idTextAnchor117"><em class="italic">Chapter 6</em></a>, <em class="italic">Developing and Operationalizing LLM-Based Cloud Applications: Exploring Dev Frameworks and LLMOps</em><ul><li><a href="B21443_07.xhtml#_idTextAnchor143"><em class="italic">Chapter 7</em></a>, <em class="italic">Deploying ChatGPT in the Cloud: Architecture Design and Scaling Strategies</em></li></ul></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer074">
			</div>
		</div>
		<div>
			<div id="_idContainer075" class="Basic-Graphics-Frame">
			</div>
		</div>
	</body></html>