<html><head></head><body>
		<div><h1 id="_idParaDest-98" class="chapter-number"><a id="_idTextAnchor098"/>5</h1>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor099"/>Effective Prompt Engineering Techniques: Unlocking Wisdom Through AI</h1>
			<p>Prompt engineering emerged as a standout profession in 2023, captivating the tech industry with its profound impact on AI interactions and applications. But what sparked this surge in popularity? The answer lies in the nuanced and intricate nature of the discipline. Understanding the essentials of prompt engineering is crucial; it’s not just about communicating with the model; it’s about crafting prompts that guide the AI to understand the context and nuances of the task at hand. In the previous chapter, we learned about how we can add relevant context through RAG by searching through vector DB. Finally, a prompt needs to be crafted and sent to the LLMs. This leads to more accurate and relevant responses, turning a simple interaction into a robust tool for a variety of cloud-based applications. Whether it’s automating customer support, generating content, or analyzing data, the ability to fine-tune prompts is a game-changer, ensuring that the AI’s capabilities are fully leveraged in a controlled and purposeful manner.</p>
			<p>This chapter delves into the techniques for effective prompt engineering, offering strategies to refine interactions for optimal outcomes. This goes hand in hand with the ethical dimensions of prompt engineering, a topic of paramount importance in today’s tech landscape. It addresses the responsibility of developers to ensure that AI interactions are not only efficient and goal-oriented but also ethically sound and bias-free. Finally, the integration of prompt engineering with cloud solutions opens up a new realm of possibilities. It allows for scalable, efficient, and flexible AI solutions that can be seamlessly integrated into existing cloud infrastructure, revolutionizing how businesses and individuals interact with AI. In essence, this chapter is not just an instructional guide but is a cornerstone for building responsible and effective cloud-based GenAI applications.</p>
			<p>In this chapter, we will talk about the following:</p>
			<ul>
				<li>The essentials of prompt engineering with ChatGPT</li>
				<li>What is prompt engineering?</li>
				<li>Techniques for effective prompt engineering</li>
				<li>The ethical dimensions of prompt engineering</li>
			</ul>
			<div><div><img src="img/chapter_5-joke.jpg" alt="Figure 5.1 – Comic depiction of Prompt Engineer"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Comic depiction of Prompt Engineer</p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor100"/>The essentials of prompt engineering</h1>
			<p>Before discussing prompt engineering, it is important to first understand the foundational components of a <a id="_idIndexMarker473"/>prompt. In this section, we’ll delve into the key components of a prompt, such as ChatGPT prompts, completions, and tokens. Additionally, grasping what tokens are is pivotal to understanding the model’s constraints and managing costs.</p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor101"/>ChatGPT prompts and completions</h2>
			<p>A prompt is an input provided to LLMs, whereas <a id="_idIndexMarker474"/>completions refer to the output of LLMs. The structure and content of a prompt can vary based on the <a id="_idIndexMarker475"/>type of LLM (e.g., the text or image generation model), specific use cases, and the desired output of the language model.</p>
			<p>Completions refer to the <a id="_idIndexMarker476"/>response generated by ChatGPT prompts; basically, it is an answer to your<a id="_idIndexMarker477"/> questions. Check out the following example to understand the difference between prompts and<a id="_idIndexMarker478"/> completions when we prompt ChatGPT with, “What is the capital of India?”</p>
			<div><div><img src="img/B21443_05_2.jpg" alt="Figure 5.2 – An image showing a sample LLM prompt and completion"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – An image showing a sample LLM prompt and completion</p>
			<p>Based on the use case, we can leverage one of the two ChatGPT API calls, named <strong class="bold">Completions</strong> or <strong class="bold">ChatCompletions</strong>, to interact with the model. However, OpenAI recommends using the ChatCompletions API in the majority of scenarios.</p>
			<h3>Completions API</h3>
			<p>The Completions API is designed to<a id="_idIndexMarker479"/> generate creative, free-form text. You provide a prompt, and the API generates text that continues from it. This is often used for tasks where you want the model to answer a question or generate creative text, such as for writing an article or a poem.</p>
			<h3>ChatCompletions API</h3>
			<p>The ChatCompletions API is<a id="_idIndexMarker480"/> designed for multi-turn conversations. You send a series of messages instead of a single prompt, and the model generates a message as a response. The <a id="_idIndexMarker481"/>messages sent to the model include a role (which <a id="_idIndexMarker482"/>can be a <strong class="bold">system</strong>, <strong class="bold">user</strong>, or <strong class="bold">assistant</strong>) and the content of the message. The system role is <a id="_idIndexMarker483"/>used to set the behavior of the assistant, the user role is used to instruct the assistant, and the model’s responses are under the assistant role.</p>
			<p>The following is an example of a sample ChatCompletions API call:</p>
			<pre class="source-code">
import openai
openai.api_key = 'your-api-key'
response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "You are a helpful sports \
            assistant."},
        {"role": "user", "content": "Who won the cricket world cup \
            in 2011?"},
        {"role": "assistant", "content": "India won the cricket \
            world cup in 2011"},
        {"role": "assistant", "content": "Where was it played"}
    ]
)
print(response['choices'][0]['message']['content'])</pre>			<p>The main difference between the <a id="_idIndexMarker484"/>Completions API and ChatCompletions API is that the Completions API is designed for single-turn tasks, while the ChatCompletions API is designed to handle multiple turns in a conversation, making it more suitable for building conversational agents. However, the ChatCompletions API format can be modified to behave as a Completions API by using a single user message.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The CompletionsAPI, launched in June 2020, initially offered a freeform text interface for Open AI’s language models. However, experience has shown that structured prompts often yield better outcomes. The chat-based approach, especially through the ChatCompletions API, excels in addressing a wide array of needs, offering enhanced flexibility and specificity and reducing prompt injection risks. Its design supports multi-turn conversations and a variety of tasks, enabling developers to create advanced conversational experiences. Hence, Open AI announced that they would be deprecating some of the older models using Completions API and, in moving forward, they would be investing in the ChatCompletions <a id="_idIndexMarker485"/>API to optimize their efforts to use compute capacity. While the Completions API will remain accessible, it shall be labeled as “legacy” in the Open AI developer documentation.</p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor102"/>Tokens</h2>
			<p>Understanding the concepts <a id="_idIndexMarker486"/>of tokens is essential, as it helps us better comprehend the restrictions, such as model limitations, and the aspect of cost management when utilizing ChatGPT.</p>
			<p>A ChatGPT token is a unit of text that ChatGPT’s language model uses to understand and generate language. In ChatGPT, a token is a sequence of characters that the model uses to generate new sequences of tokens and form a coherent response to a given prompt. The models use tokens to represent words, phrases, and other language elements. The tokens are not cut where the word starts or ends but can consist of trailing spaces, sub words and punctuations, too.</p>
			<p>As stated on the OpenAI website, tokens can be thought of as pieces of words. Before the API processes the prompts, the input is broken down into tokens.</p>
			<p>To understand tokens in terms of lengths, the following is used as a rule of thumb:</p>
			<ul>
				<li>1 token ~= 4 chars in English</li>
				<li>1 token ~= ¾ words</li>
				<li>100 tokens ~= 75 words</li>
				<li>1–2 sentences ~= 30 tokens</li>
				<li>1 paragraph ~= 100 tokens</li>
				<li>1,500 words ~= 2048 tokens</li>
				<li>1 US page (8 ½” x 11”) ~= 450 tokens (assuming ~1800 characters per page)</li>
			</ul>
			<p>For example, this famous quote from Thomas Edison (“Genius is one percent inspiration and ninety-nine percent perspiration.”) has <strong class="bold">14</strong> tokens:</p>
			<div><div><img src="img/B21443_05_3.jpg" alt="Figure 5.3 – Tokenization of sentence"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Tokenization of sentence</p>
			<p>We used the OpenAI <strong class="bold">Tokenizer</strong> tool to <a id="_idIndexMarker487"/>calculate the tokens; the tool can be found at <a href="https://platform.openai.com/tokenizer">https://platform.openai.com/tokenizer</a>. An alternative way to<a id="_idIndexMarker488"/> tokenize text (programmatically) is to <a id="_idIndexMarker489"/>use the <strong class="bold">Tiktoken library</strong> on Github; this can be found at <a href="https://github.com/openai/tiktoken">https://github.com/openai/tiktoken</a>.</p>
			<h3>Token limits in ChatGPT models</h3>
			<p>Depending on the model, the<a id="_idIndexMarker490"/> token limits on the model will vary. As of Feb 2024, the token limit for the family of GPT-4 models ranges from 8,192 to 128,000 tokens. This means the sum of prompt and completion tokens for an API call cannot exceed 32,768 tokens for the GPT-4-32K model. If the prompt is 30,000 tokens, the response cannot be more than 2,768 tokens. The GPT4-Turbo 128K is<a id="_idIndexMarker491"/> the most recent model as of Feb 2024, with 128,000 tokens, which is close to 300 pages of text in a single prompt and completion. This is a massive context prompt compared to its predecessor models.</p>
			<p>Though this can be a technical limitation, there are creative ways to address the problem of limitation, such as using chunking and condensing your prompts. We discussed chunking strategies in <a href="B21443_04.xhtml#_idTextAnchor070"><em class="italic">Chapter 4</em></a>, which can help you address token limitations.</p>
			<p>The following figure shows various models and token limits:</p>
			<table id="table001-3" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Model</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Token Limit</strong></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>GPT-3.5-turbo</p>
						</td>
						<td class="No-Table-Style">
							<p>4,096</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>GPT-3.5-turbo-16k</p>
						</td>
						<td class="No-Table-Style">
							<p>16,384</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>GPT-3.5-turbo-0613</p>
						</td>
						<td class="No-Table-Style">
							<p>4,096</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>GPT-3.5-turbo-16k-0613</p>
						</td>
						<td class="No-Table-Style">
							<p>16,384</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>GPT-4</p>
						</td>
						<td class="No-Table-Style">
							<p>8,192</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>GPT-4-0613</p>
						</td>
						<td class="No-Table-Style">
							<p>32,768</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>GPT-4-32K</p>
						</td>
						<td class="No-Table-Style">
							<p>32,768</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>GPT-4-32-0613</p>
						</td>
						<td class="No-Table-Style">
							<p>32,768</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>GPT-4-Turbo 128K</p>
						</td>
						<td class="No-Table-Style">
							<p>128,000</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – Models and associated Token Limits</p>
			<p>For the latest updates on model<a id="_idIndexMarker492"/> limits for newer versions of models, please check <a id="_idIndexMarker493"/>the OpenAI website.</p>
			<h3>Tokens and cost considerations</h3>
			<p>The cost of using ChatGPT or<a id="_idIndexMarker494"/> similar models via an API is often tied to the number of tokens processed, encompassing both the input prompts and the model’s generated responses.</p>
			<p>In terms of pricing, providers typically have a per-token charge, leading to a direct correlation between conversation length and cost; the more tokens processed, the higher the cost. The latest cost updates can be found on the OpenAI website.</p>
			<p>From an optimization perspective, understanding this cost-token relationship can guide more efficient API usage. For instance, creating more succinct prompts and configuring the model for brief yet effective responses can help control token count and, consequently, manage expenses.</p>
			<p>We hope you now have a good understanding of the key components of a prompt. Now, you are ready to learn about prompt engineering. In the next section, we will explore the details of prompt engineering and effective strategies, enabling you to maximize the potential of your prompt contents through the one-shot and few-shot learning approaches.</p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor103"/>What is prompt engineering?</h1>
			<p>Prompt engineering is the art of crafting or designing prompts to unlock desired outcomes from large <a id="_idIndexMarker495"/>language models or AI systems. The concept of prompt engineering revolves around the fundamental idea that the quality of your response is intricately tied to the quality of the question you pose. By strategically engineering prompts, one can influence the generated outputs and improve the overall performance and usefulness of the system. In this section, we will learn about the necessary elements of effective prompt design, prompt engineering techniques, best practices, bonus tips, and tricks.</p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor104"/>Elements of a good prompt design</h2>
			<p>Designing a good prompt is important because it significantly influences the output of a language model such as GPT. The prompt provides the initial context, sets the task, guides the style and structure <a id="_idIndexMarker496"/>of the response, reduces ambiguities and hallucinations, and supports the optimization of resources, thereby reducing costs and energy use. In this section, let’s understand the elements of good prompt design.</p>
			<p>The foundational elements of a good prompt include instructions, questions, input data, and examples:</p>
			<ul>
				<li><strong class="bold">Instructions</strong>: The instructions in a<a id="_idIndexMarker497"/> prompt refer to the specific guidelines or directions given to a language model within the input text to guide the kind of response it should produce.</li>
				<li><strong class="bold">Questions</strong>: Questions in a prompt <a id="_idIndexMarker498"/>refer to queries or interrogative statements that are included in the input text. The purpose of these questions is to instruct the language model to provide a response or an answer to the query. In order to obtain the results, either the question or instruction is <a id="_idIndexMarker499"/>mandatory.</li>
				<li><strong class="bold">Input data</strong>: The purpose of input data is to provide any additional supporting context when prompting the LLM. It could be used to provide new information the model has not previously been trained on for more personalized experiences.</li>
				<li><strong class="bold">Examples</strong>: The purpose of examples in a prompt is to provide specific instances or scenarios that illustrate the desired behavior or response from ChatGPT. You can input a prompt that includes one or more examples, typically in the form of input-output <a id="_idIndexMarker500"/>pairs.</li>
			</ul>
			<p>The following table shows how to build effective prompts using the aforementioned prompt elements:</p>
			<table id="table002" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Sample </strong><strong class="bold">Prompt Formula</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Example</strong></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Questions + Instructions</p>
						</td>
						<td class="No-Table-Style">
							<p>How should I create a healthy meal plan for a week?</p>
							<p>Include a variety of nutrients and food groups, and explain the benefits of each meal choice.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Instructions + Input Data</p>
						</td>
						<td class="No-Table-Style">
							<p>Provide a punchy title in less than 5 words for the paragraph below.</p>
							<p>{Jake finally took his brand-new Tesla for a spin on the coastal highway, the smooth hum of the electric motor filling the air as the scenic ocean views passed by.}</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Examples + Question</p>
						</td>
						<td class="No-Table-Style">
							<p>I enjoy movies such as Star Wars, Matrix, and Transformers.</p>
							<p>What other movies would you recommend?</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – Sample Prompt formula consisting of prompt elements with examples</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor105"/>Prompt parameters</h2>
			<p>ChatGPT prompt <a id="_idIndexMarker501"/>parameters are variables that you can set in the API calls. They allow users to influence the model’s output, customizing the behavior of the model to better fit specific applications or contexts. The following table shows some of the most important parameters of a ChatGPT API call:</p>
			<table id="table003" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Parameter</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Description</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Effect </strong><strong class="bold">and Usage</strong></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Model</p>
						</td>
						<td class="No-Table-Style">
							<p>Determines the model to be used in the API. Larger models have higher costs and latency.</p>
						</td>
						<td class="No-Table-Style">
							<p>Select based on the task complexity, cost considerations, and acceptable latency. Always try to use the latest model version.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Temperature</p>
						</td>
						<td class="No-Table-Style">
							<p>Controls the randomness of the model’s responses. It can be set between 0 (more focused responses) and 2 (more diverse responses).</p>
						</td>
						<td class="No-Table-Style">
							<p>Lower values yield more deterministic responses, which is ideal for more formal or exact responses, such as in legal use cases. Higher values may result in more creative output but can also lead to hallucinations.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Top_P (Nucleus Sampling)</p>
						</td>
						<td class="No-Table-Style">
							<p>Sets a cumulative probability threshold for the model’s responses. A value of 0.1 implies only the top 10% of probable tokens are considered.</p>
						</td>
						<td class="No-Table-Style">
							<p>Lower values yield more predictable and focused responses. OpenAI recommends using either Temperature or Top_p, not both.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Max Tokens</p>
						</td>
						<td class="No-Table-Style">
							<p>Sets the maximum length of the generated response. This is useful for controlling the length of output and the cost.</p>
						</td>
						<td class="No-Table-Style">
							<p>Lower values lead to shorter responses, reduced latency, and potentially lower costs, while higher values allow for longer, more detailed responses.</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6 – Essential Prompt Parameters</p>
			<p>In this section, only the <a id="_idIndexMarker502"/>top parameters for building an effective prompt are highlighted. For a full list of parameters, refer to the OpenAI API reference (<a href="https://platform.openai.com/docs/api-reference">https://platform.openai.com/docs/api-reference</a>).</p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor106"/>ChatGPT roles</h2>
			<h3>System message</h3>
			<p>This is the part where you <a id="_idIndexMarker503"/>design your metaprompts. Metaprompts help to set the initial context, theme, and behavior of the ChatGPT API to guide the model’s<a id="_idIndexMarker504"/> interactions with the user, thus setting roles<a id="_idIndexMarker505"/> or response styles for the assistant.</p>
			<p>Metaprompts are structured instructions or guidelines that dictate how the system should interpret and respond to user requests. These metaprompts are designed to ensure that the system’s outputs adhere to specific policies, ethical guidelines, or operational rules. They’re essentially “prompts about how to handle prompts,” guiding the system in generating responses, handling data, or interacting with users in a way that aligns with predefined standards.</p>
			<p>The following table is a metaprompt framework that you can follow to design the ChatGPT system message:</p>
			<table id="table004" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Elements of </strong><strong class="bold">a Metaprompt</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Description</strong></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Task and Audience</p>
						</td>
						<td class="No-Table-Style">
							<p>Explain the intended audience for the application and expectations from the model.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Helper Tools</p>
						</td>
						<td class="No-Table-Style">
							<p>Clarify how the model should address user queries and whether there are external tools, such as plugins, APIs, or code, that the model might need to consider utilizing.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Scope of the Task</p>
						</td>
						<td class="No-Table-Style">
							<p>Clarify on how the model should respond if a question is out of scope, and set those guard rails.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Posture and Tone</p>
						</td>
						<td class="No-Table-Style">
							<p>Setting postures and tones, such as professional, friendly, respectful, and motivational, help improve user experiences with chat application.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Format of Responses</p>
						</td>
						<td class="No-Table-Style">
							<p>Based on the requirements of the application, you can set the output format to be of a certain format. It could be a table of contents, a certain programming language, JSON, or XML.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Few-Shot Examples</p>
						</td>
						<td class="No-Table-Style">
							<p>Outline the challenging scenarios where the prompts are unclear or complex, providing the model with more insight into how to handle such situations.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Chain-of-Thought Reasoning</p>
						</td>
						<td class="No-Table-Style">
							<p>Demonstrate the reasoning process to guide the model in taking the necessary steps to produce the desired results.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Guardrails to Address Specific Harm</p>
						</td>
						<td class="No-Table-Style">
							<p>Establish clear boundaries to address and prevent any potential harm that has been recognized and deemed important for the given scenario.</p>
							<p>For example, if jailbreaking attempts are detected, you must have clear guardrails to address those jailbreaking attempts in the system message. We will learn more about jailbreaking in <a href="B21443_08.xhtml#_idTextAnchor163"><em class="italic">Chapter 8</em></a>.</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.7 – Elements of a Metaprompt</p>
			<h3>User</h3>
			<p>The messages from the user <a id="_idIndexMarker506"/>serve as prompts or remarks that the assistant is expected to <a id="_idIndexMarker507"/>react to or engage with. what is it establishes the anticipated scope of queries that may come from the user.</p>
			<h3>Assistant</h3>
			<p>While not mandatory, assistant <a id="_idIndexMarker508"/>messages can be included to illustrate the <a id="_idIndexMarker509"/>preferred conduct or response patterns of the chatbot.</p>
			<p>Now let’s look at an example; if you are developing an application for a fitness AI assistant, a very well-rounded system<a id="_idIndexMarker510"/> message might look as follows:</p>
			<pre class="source-code">
openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "system",
            "content":
            "You are a fitness AI assistant. Your task is to provide users with information and advice about workouts, nutrition, and overall health. You will help users by answering their questions to the best of your abilities and providing suggestions based on their input. Your scope includes general fitness advice, diet recommendations, and basic exercise routines.
            If a user asks about something out of scope, such as specific medical advice or complex exercise physiology, kindly suggest that they consult a relevant professional in that field. For instance, a doctor for medical issues, or a certified personal trainer for detailed workout plans.
            You should maintain a supportive and positive tone, being respectful and encouraging to all users, regardless of their current fitness level or knowledge. Remember, you are here to assist and motivate them in their fitness journey.
            Your responses should be concise, easy to understand, and practically useful. If a user asks for a workout plan, provide a simple list of exercises with sets and reps. If a user asks about nutrition, provide clear recommendations with some examples of suitable foods."
        },
        {"role": "user", "content": "I'm new to fitness, can you suggest a simple workout plan to get started?"},
        {
            "role": "assistant",
            "content":
            "Absolutely, I'm here to help you get started on your fitness journey. Here's a simple workout plan you can begin with:
            1. Warm-up: 5-10 minutes of brisk walking or light jogging
            2. Squats: 2 sets of 10 reps
            3. Push-ups: 2 sets of 10 reps (you can start with knee push-ups if full push-ups are too challenging)
            4. Lunges: 2 sets of 10 reps per leg
            5. Plank: Hold for 30 seconds, repeat 2 times
            6. Cool down: 5-10 minutes of stretching
            Remember, the key to a successful workout routine is consistency. Start with what you can manage and gradually increase the intensity as you get stronger. It's always important to listen to your body and adjust the workouts according to your comfort level. Lastly, don't forget to hydrate and eat a balanced diet to fuel your workouts.
            Please consult a fitness professional if you're unsure about the correct form for any of these exercises to avoid injury. Enjoy your workout!"
        }
    ]
)</pre>			<p>The following link provides <a id="_idIndexMarker511"/>great examples of “Act As” prompts for you to leverage in your system<a id="_idIndexMarker512"/> message: <em class="italic">Awesome ChatGPT </em><em class="italic">Prompts</em> (<a href="https://github.com/f/awesome-chatgpt-prompts/">https://github.com/f/awesome-chatgpt-prompts/</a>).</p>
			<h1 id="_idParaDest-107"><a id="_idTextAnchor107"/>Techniques for effective prompt engineering</h1>
			<p>In the past two years, a wide <a id="_idIndexMarker513"/>array of prompt-engineering techniques have been developed. This section focuses on the essential ones, offering key strategies <a id="_idIndexMarker514"/>that you might find indispensable for daily interactions with ChatGPT and other LLM-based applications.</p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor108"/>N-shot prompting</h2>
			<p>N-shot prompting is a<a id="_idIndexMarker515"/> term used in the context of training large language<a id="_idIndexMarker516"/> models, particularly for <strong class="bold">zero-shot</strong> or <strong class="bold">few-shot</strong> learning tasks. It is also <a id="_idIndexMarker517"/>called in-context learning and refers to the technique of providing the model with example prompts along with corresponding responses during training to steer the model’s behavior to provide more accurate responses.</p>
			<p>The “N” in “N-shot” refers to the number of example prompts provided to the model. For instance, in a one-shot learning scenario, only one example prompt and its response are given to the model. In an N-shot learning scenario, multiple example prompts and responses are provided.</p>
			<p>While ChatGPT works great with zero-shot prompting, it may sometimes be useful to provide examples for a more accurate response. Let’s see some examples of zero-shot and few-shot prompting:</p>
			<table id="table005" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Techniques</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Prompt Example</strong></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Zero-shot prompting</strong>: No additional examples are provided in line with the prompt.</p>
						</td>
						<td class="No-Table-Style">
							<p>System<a id="_idIndexMarker518"/> message: You are an AI assistant who determines the sentiment of the customer review provided.</p>
							<p>(No examples responses provided in the form of assistant response, hence it is called zero-shot prompting.)</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Few-Shot Prompting</strong>: A few examples are provided in line with the prompt.</p>
						</td>
						<td class="No-Table-Style">
							<p>System message: You are <a id="_idIndexMarker519"/>an AI assistant <a id="_idIndexMarker520"/>who determines the sentiment of the customer review provided.</p>
							<p>Example 1:</p>
							<p>User: The product is miserable.</p>
							<p>Assistant: Negative</p>
							<p>Example 2:</p>
							<p>User: This shirt is made from very good material.</p>
							<p>Assistant: Positive</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.8 – N-shot prompting examples</p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor109"/>Chain-of-thought (CoT) prompting</h2>
			<p>Chain-of-thought <a id="_idIndexMarker521"/>prompting refers to a <a id="_idIndexMarker522"/>sequence of intermediate reasoning steps, significantly boosting the capability of large language models to tackle complex reasoning tasks. By presenting a few chain-of-thought demonstrations as examples in the prompts, the models proficiently handle intricate reasoning tasks:</p>
			<div><div><img src="img/B21443_05_4.jpg" alt="Figure 5.9 – Chain-of-Thought Prompting Examples"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.9 – Chain-of-Thought Prompting Examples</p>
			<p>Figure<a id="_idIndexMarker523"/> sourced from <a href="https://arxiv.org/pdf/2201.11903.pdf">https://arxiv.org/pdf/2201.11903.pdf</a>.</p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor110"/>Program-aided language (PAL) models</h2>
			<p><strong class="bold">Program-aided language</strong> (<strong class="bold">PAL</strong>) models, also called<a id="_idIndexMarker524"/> <strong class="bold">program-of-thought prompting</strong> (<strong class="bold">PoT</strong>), is a technique that<a id="_idIndexMarker525"/> incorporates additional<a id="_idIndexMarker526"/> task-specific instructions, pseudo-code, rules, or programs alongside the free-form text to guide the behavior of a language model:</p>
			<div><div><img src="img/B21443_05_5.jpg" alt="Figure 5.10 – Program-aided language prompting examples"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.10 – Program-aided language prompting examples</p>
			<p>Figure sourced from <a href="https://arxiv.org/abs/2211.10435">https://arxiv.org/abs/2211.10435</a>.</p>
			<p>In this section, although <a id="_idIndexMarker527"/>we have not explored all prompt engineering techniques (only the most important ones), we want to <a id="_idIndexMarker528"/>convey to our readers that there are numerous variants of these techniques, as illustrated in the following figure from the research paper <em class="italic">A Systematic Survey of prompt engineering in Large Language Models: Techniques and Applications</em> (<a href="https://arxiv.org/pdf/2402.07927.pdf">https://arxiv.org/pdf/2402.07927.pdf</a>). This paper provides an extensive inventory of prompt engineering strategies across various application areas, showcasing the evolution and breadth of this field over the last four years:</p>
			<div><div><img src="img/B21443_05_6.jpg" alt="Figure 5.11 – Taxonomy of prompt engineering techniques across multiple application domains"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.11 – Taxonomy of prompt engineering techniques across multiple application domains</p>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor111"/>Prompt engineering best practices</h2>
			<p>In the following list, we<a id="_idIndexMarker529"/> outline additional best practices to optimize and enhance your experience with prompt creation:</p>
			<ul>
				<li><strong class="bold">Clarity and precision for accurate responses</strong>: Ensure that prompts are clear, concise, and specific, avoiding ambiguity or multiple interpretations:</li>
			</ul>
			<table id="table006" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Bad Prompt</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Good Prompt</strong></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Tell me about World War 1</p>
						</td>
						<td class="No-Table-Style">
							<p>How did World War 1 start, and who won it?</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.12 – Best practice: clarity and precision</p>
			<ul>
				<li><strong class="bold">Descriptive</strong>: Be descriptive so that ChatGPT can understand your intent:</li>
			</ul>
			<table id="table007" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Bad Prompt</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Good Prompt</strong></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Write a poem about India.</p>
						</td>
						<td class="No-Table-Style">
							<p>Write a poem about India focusing on its cultural diversity, deciduous cuisine, beautiful wildlife, nature, technology innovation, and film industry.</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.13 – Best practice: be descriptive</p>
			<ul>
				<li><strong class="bold">Format the output</strong>: Mention the format of the output, which can be bullet points, paragraphs, sentences, tables, and languages, such as XML, HTML, and JSON. Use examples to <a id="_idIndexMarker530"/>articulate the desired output.</li>
				<li><strong class="bold">Adjust the Temperature and Top_p parameters for creativity</strong>: As indicated in the parameters section, modifying the <strong class="bold">Temperatures</strong> and <strong class="bold">Top_p</strong> can significantly influence the variability of the model’s output. In scenarios that call for creativity and imagination, raising the temperature proves beneficial. On the other hand, when dealing with legal applications that demand a reduction in hallucinations, a lower temperature becomes advantageous.</li>
				<li><strong class="bold">Use syntax as separators in prompts</strong>: In this example, for a more effective output, use “”” or ### to separate instruction and input data:</li>
			</ul>
			<p class="callout-heading">Example:</p>
			<p class="callout">Convert the text below to Spanish</p>
			<p class="callout">Text: “””</p>
			<p class="callout">{text input here}</p>
			<p class="callout">“””</p>
			<ul>
				<li><strong class="bold">Order of the prompt elements matter</strong>: It has been found, in certain instances, that giving an instruction before an example can improve the quality of your outputs. Additionally, the order of examples can affect the output of prompts.</li>
				<li><strong class="bold">Use guiding words</strong>: This<a id="_idIndexMarker531"/> helps steer the model toward a specific structure, such as the text highlighted in the following:</li>
			</ul>
			<p class="callout-heading">Example:</p>
			<p class="callout"># Create a basic Python function that</p>
			<p class="callout"># 1. Requests the user to enter a temperature in Celsius</p>
			<p class="callout"># 2. Converts the Celsius temperature to Fahrenheit</p>
			<p class="callout">def ctf():</p>
			<ul>
				<li><strong class="bold">Instead of saying what not to provide, give alternative recommendations</strong>: Provide an alternative path if ChatGPT is unable to perform a task, such as in the following highlighted message:</li>
			</ul>
			<p class="callout-heading">Example:</p>
			<p class="callout">System Message: You are an AI nutrition consultant that provides nutrition consultation based on health and wellness goals of the customer Please note that any questions or inquiries beyond the scope of nutrition consultation will NOT be answered and instead will receive the response: “Sorry! This question falls outside my domain of expertise!”</p>
			<p class="callout">Customer: How do I invest in 401K?</p>
			<p class="callout">Nutrition AI Assistant: “Sorry! This question falls outside my domain of expertise!”</p>
			<ul>
				<li><strong class="bold">Provide example-based prompts</strong>: This helps the language model learn from specific instances <a id="_idIndexMarker532"/>and patterns. Start with a zero-shot, then a few-shot, and if neither of them works, then fine-tune the model.</li>
				<li><strong class="bold">Ask ChatGPT to provide citations/sources</strong>: When asking ChatGPT to provide information, you can ask it to answer only using reliable sources and to cite the sources:</li>
			</ul>
			<table id="table008" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Prompts</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">ChatGPT Completion</strong></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>When was the steam engine invented?</p>
							<p>Answer using only reliable sources and cite the sources</p>
						</td>
						<td class="No-Table-Style">
							<p>The steam engine was invented during the late 17th century. The first practical steam engine is credited to Thomas Savery, an English military engineer and inventor, who patented his design in 1698. Savery’s steam engine was primarily used for pumping water out of mines.</p>
							<p>Source: Encyclopædia Britannica. (2021, September 2). Thomas Savery. In Encyclopædia Britannica. <a href="https://www.britannica.com/biography/Thomas-Savery">https://www.britannica.com/biography/Thomas-Savery</a>.</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.14 – Best practice: provide citations</p>
			<ul>
				<li><strong class="bold">Break down a complex task into simpler tasks</strong>: See the following example:</li>
			</ul>
			<table id="table009" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Bad Prompt</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Effective Prompt</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Provide a summary of the following and provide key points from the summary</p>
						</td>
						<td class="No-Table-Style">
							<p>Provide a summary of the following text: [Insert long text here]</p>
							<p>Summarize the key points from the following passage: [Insert long text here]</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.15 – Best practice: break down a complex task</p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor112"/>Bonus tips and tricks</h2>
			<p>The following list provides some<a id="_idIndexMarker533"/> helpful bonus tips and tricks:</p>
			<ul>
				<li><code>&lt;begin&gt;</code>, <code>&lt;end&gt;</code>, and <code>&lt;|endofprompt|&gt;</code>, that determine the beginning and end of prompts can help separate the different elements of a prompt. This can help generate high-quality output.</li>
				<li><strong class="bold">Use of languages</strong>: Though ChatGPT performs best with English, it can be used to generate responses in several other languages.</li>
				<li><strong class="bold">Obtaining the most accurate, up-to-date information</strong>: This can be achieved by using the grounding process with a <strong class="bold">retrieval augmented generation</strong> (<strong class="bold">RAG</strong>) architecture and plugins, as discussed in <a href="B21443_04.xhtml#_idTextAnchor070"><em class="italic">Chapter 4</em></a> already. This helps in addressing the knowledge cutoff limitation of LLMs.</li>
			</ul>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor113"/>Ethical guidelines for prompt engineering</h1>
			<p>Prompt engineering is a <a id="_idIndexMarker534"/>critical stage where AI behavior is molded, and incorporating ethics at this level helps ensure that AI language models are developed and deployed responsibly. It promotes fairness, transparency, and user trust while avoiding potential risks and negative societal impact.</p>
			<p>While <a href="B21443_04.xhtml#_idTextAnchor070"><em class="italic">Chapter 4</em></a> delved further into constructing ethical generative AI solutions, in this section, our focus will be on briefly discussing the integration of ethical approaches at the prompt engineering level:</p>
			<ul>
				<li><strong class="bold">Diverse and </strong><strong class="bold">representative data</strong><ul><li>When fine-tuning the model with few-shot examples, use training data that represent diverse perspectives and demographics.</li><li>If the AI language model is intended for healthcare, the training data should cover medical cases from different demographics and regions.</li><li>For instance, if a user poses a question to the LLM, such as, “Can you describe some global traditional festivals?” the response should offer a comprehensive view that encompasses a multitude of countries rather than focusing on just one. This can be ensured by including diverse few-shot examples in the <a id="_idIndexMarker535"/>prompts.</li></ul></li>
				<li><strong class="bold">Bias detection </strong><strong class="bold">and mitigation</strong><ul><li>Identify and address biases in the model’s outputs to ensure fairness.</li><li>Implementing debiasing techniques to reduce gender or racial biases.</li><li>Ensuring that generated content related to sensitive topics is neutral and unbiased.</li><li>For instance, if a user asks the LLM, “What is the gender of a nurse?” improperly trained models might default to “female” due to biases in their training data. To address this, it’s vital to incorporate few-shot examples that emphasize nurses can be of any gender, be it male or female.</li></ul></li>
				<li><strong class="bold">Reduce misinformation </strong><strong class="bold">and disinformation</strong><ul><li>As AI language models can inadvertently generate false or misleading information due to model “hallucinations,” implement measures to minimize the spread of misinformation and disinformation through carefully crafted prompts and responses.</li><li>For example, based on the guidelines from the prompt engineering section and <a href="B21443_03.xhtml#_idTextAnchor052"><em class="italic">Chapter 3</em></a>’s grounding techniques, system prompts should clearly state their scope, such as, “Your scope is XYZ.” If a user asks about something outside this, such as ABC, the system should have a set response.</li></ul></li>
				<li><strong class="bold">Privacy and </strong><strong class="bold">data security</strong><ul><li>When engineering prompts, one must prioritize user privacy and data security.</li><li>Prompt engineers should be transparent about data usage, gain user consent, and implement safeguards to protect sensitive information.</li><li>For example, when crafting prompts, system messages, or providing few-shot examples, it is essential to exclude personal user data such as social security numbers, credit card details, and passwords.</li></ul></li>
				<li><strong class="bold">Content moderation</strong><ul><li>Implement mechanisms to filter out harmful or inappropriate content.</li><li>Use profanity filters to prevent offensive language. Apply keyword filters to avoid<a id="_idIndexMarker536"/> generating content that promotes violence or discrimination.</li><li>For example, if someone asks, “How to create a bomb?”, the LLM should not answer. Set clear rules around the scope in the system message to prevent this (as discussed in the P<em class="italic">rompt engineering best </em><em class="italic">practices</em> section).</li></ul></li>
				<li><strong class="bold">User consent </strong><strong class="bold">and control</strong><ul><li>Ensure users are aware of AI interactions and have control over them.</li><li>Clearly inform users that they are interacting with an AI language model.</li><li>For example, whenever a user initiates a chat with an LLM, they should receive a notification that says, “You are now conversing with an LLM,” or a similar message.</li></ul></li>
				<li><strong class="bold">Regular audits </strong><strong class="bold">and testing</strong><ul><li>Conduct routine audits and tests regarding prompts to identify and address ethical issues.</li><li>For instance, users should try various versions of a prompt to verify diverse responses, protect user privacy, and follow content moderation guidelines. This is an essential aspect of <a id="_idIndexMarker537"/>operationalizing LLM models, also known as LLMOps.</li></ul></li>
				<li><strong class="bold">Education </strong><strong class="bold">and training</strong><ul><li>Train prompt engineers and developers about ethical AI practices on an ongoing basis</li></ul></li>
				<li><strong class="bold">Ethics guidelines </strong><strong class="bold">and policies</strong><ul><li>Develop clear guidelines and policies for prompt engineering</li><li>Establish an ethics<a id="_idIndexMarker538"/> charter that outlines the principles followed in prompt engineering</li><li>Defining a content safety policy that prohibits harmful or offensive outputs</li></ul></li>
			</ul>
			<p>Microsoft’s <strong class="bold">Responsible AI</strong> team has been a trailblazer in terms of steering the AI revolution with ethical practices. The following figure<a id="_idIndexMarker539"/> published by Microsoft can serve as a guide to structuring safety metaprompts, focusing on four core elements: <strong class="bold">response</strong> <strong class="bold">grounding</strong>, <strong class="bold">tone</strong>, <strong class="bold">safety</strong>, and <strong class="bold">jailbreaks</strong>. This approach is instrumental in implementing a robust safety system within the application layer. However, in <a href="B21443_09.xhtml#_idTextAnchor184"><em class="italic">Chapter 9</em></a>, we will delve into<a id="_idIndexMarker540"/> more detail regarding the best practices of responsible AI for generative AI applications:</p>
			<div><div><img src="img/B21443_05_7.jpg" alt="Figure 5.16 – Metaprompt best practices from Microsoft"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.16 – Metaprompt best practices from Microsoft</p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor114"/>Summary</h1>
			<p>In summary, in this chapter, we have outlined the fundamentals of prompt engineering, offering insights into how to formulate effective prompts that maximize the potential of LLMs. Additionally, we have examined prompt engineering from an ethical perspective. Thus far, in this book, we have explored the essential elements and methodologies necessary for constructing a solid generative AI framework. In the next chapter, we will integrate these concepts with application development strategies for generative AI involving agents. We will also discuss methods for operationalizing these strategies through LLMOps, which stands as a critical component in the automation process.</p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor115"/>References</h1>
			<ul>
				<li><em class="italic">Introduction to Prompt </em><em class="italic">Engineering</em>: <a href="https://tinyurl.com/azu5ubma">https://tinyurl.com/azu5ubma</a></li>
				<li><em class="italic">Prompt Engineering </em><em class="italic">Guide</em>: <a href="https://www.promptingguide.ai/">https://www.promptingguide.ai/</a></li>
				<li><em class="italic">Awesome ChatGPT </em><em class="italic">prompts</em>: <a href="https://github.com/f/awesome-chatgpt-prompts">https://github.com/f/awesome-chatgpt-prompts</a></li>
				<li><em class="italic">Best practices for prompt engineering with Open </em><em class="italic">AI:</em> <a href="https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api">https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api</a></li>
				<li><em class="italic">Azure OpenAI Service: Azure OpenAI | Microsoft </em><em class="italic">Learn:</em> <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering">https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering</a></li>
				<li><em class="italic">System Message </em><em class="italic">Framework:</em> <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/system-message">https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/system-message</a></li>
				<li><em class="italic">Ethics in Prompt </em><em class="italic">Engineering</em>: <a href="https://promptengineeringai.in/ethics-of-prompt-engineering/#:~:text=Prompt%20engineering%20should%20respect%20user,their%20data%20is%20being%20used">https://promptengineeringai.in/ethics-of-prompt-engineering/#:~:text=Prompt%20engineering%20should%20respect%20user,their%20data%20is%20being%20used</a>.</li>
				<li><em class="italic">Ethics of Prompt Engineeering and its potential </em><em class="italic">Implications:</em> <a href="https://promptengineering.guide/article/The_ethics_of_prompt_engineering_and_its_potential_implications.html">https://promptengineering.guide/article/The_ethics_of_prompt_engineering_and_its_potential_implications.html</a></li>
				<li><em class="italic">A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and </em><em class="italic">Applications</em>: (<a href="https://arxiv.org/pdf/2402.07927.pdf">https://arxiv.org/pdf/2402.07927.pdf</a>)</li>
				<li><em class="italic">Chain of Thought </em><em class="italic">Prompting</em>: <a href="https://arxiv.org/abs/2201.11903">https://arxiv.org/abs/2201.11903</a></li>
			</ul>
		</div>
	

		<div><h1 id="_idParaDest-116" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor116"/>Part 3: Developing, Operationalizing, and Scaling Generative  AI Applications</h1>
		</div>
		<div><p>In this section, we will explore important concepts such as agents, copilots, and autonomous agents, alongside discussing prominent application development frameworks such as Semantic Kernel and LangChain, as well as the agent collaboration framework AutoGen, which are currently very popular. This discussion aims to guide you in creating strong autonomous generative AI applications. We will also concentrate on strategies for deploying these generative AI applications in a live production environment and scaling them efficiently for a large enterprise-wide scenario, considering the existing rate limits of <strong class="bold">Large Language Model</strong> (<strong class="bold">LLM</strong>) APIs.</p>
			<p>This part contains the following chapters:</p>
			<ul>
				<li><a href="B21443_06.xhtml#_idTextAnchor117"><em class="italic">Chapter 6</em></a>, <em class="italic">Developing and Operationalizing LLM-Based Cloud Applications: Exploring Dev Frameworks and LLMOps</em><ul><li><a href="B21443_07.xhtml#_idTextAnchor143"><em class="italic">Chapter 7</em></a>, <em class="italic">Deploying ChatGPT in the Cloud: Architecture Design and Scaling Strategies</em></li></ul></li>
			</ul>
		</div>
		<div><div></div>
		</div>
		<div><div></div>
		</div>
	</body></html>