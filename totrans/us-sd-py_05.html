<html><head></head><body>
		<div><h1 id="_idParaDest-62" class="chapter-number"><a id="_idTextAnchor097"/>5</h1>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor098"/>Understanding How Stable Diffusion Works</h1>
			<p>In <a href="B21263_04.xhtml#_idTextAnchor081"><em class="italic">Chapter 4</em></a>, we dove into the internal workings of the diffusion model with some math formulas. If you are not used to reading the formulas every day, it can be scary, but once you get familiar with those symbols and Greek letters, the benefit of fully understanding those formulas is huge. Math formulas and equations not only help us understand the core of the process in a precise and concise form, but they also enable us to read more papers and works from others.</p>
			<p>While the original diffusion model is more like a proof of a concept, it shows the huge potential of the multi-step diffusion model compared with a one-pass neural network. However, some <a id="_idIndexMarker130"/>drawbacks come with the original diffusion model, <strong class="bold">denoising diffusion probabilistic models</strong> (<strong class="bold">DDPM</strong>) [1], and later Classifier Guidance denoising. Let me list two:</p>
			<ul>
				<li>To train a diffusion model with Classifier Guidance requires training a new classifier, and we can’t reuse a pre-trained classifier. Also, in diffusion model training, training a classifier with 1,000 categories is already not easy.</li>
				<li>Pre-trained model inferences in pixel space are computationally expensive, not to mention training a model. Using a pre-trained model to generate 512x512 images in pixel space on a home computer with 8 GB of VRAM, without memory optimization, is not possible.</li>
			</ul>
			<p>In 2022, researchers <a id="_idIndexMarker131"/>proposed <strong class="bold">Latent Diffusion models</strong>, Robin et al [2]. The model nicely solved both the classification problem and the performance problem. The Latent Diffusion model was later known as Stable Diffusion.</p>
			<p>In this chapter, we will take a look at how Stable Diffusion solved the preceding problems and led to state-of-the-art developments in the field of image generation. We will specifically cover the following topics:</p>
			<ul>
				<li>Stable Diffusion in latent space</li>
				<li>Generating latent vectors using Diffusers</li>
				<li>Generating text embeddings using CLIP</li>
				<li>Generating time step embeddings</li>
				<li>Initializing Stable Diffusion UNet</li>
				<li>Implementing a text-to-image Stable Diffusion inference pipeline</li>
				<li>Implementing a text-guided image-to-image Stable Diffusion inference pipeline</li>
				<li>Putting all the code together</li>
			</ul>
			<p>Let’s dive into the core of Stable Diffusion<a id="_idTextAnchor099"/>.</p>
			<p>The sample code for this chapter is tested using version 0.20.0 of the Diffusers package. To ensure the code runs smoothly, please use Diffusers v0.20.0. You can install it using the following command:</p>
			<pre class="source-code">
<code>pip install diffusers==0.20.0</code></pre>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor100"/>Stable Diffusion in latent space</h1>
			<p>Instead of processing diffusion in pixel space, Stable Diffusion uses latent space to represent an image. What is latent space? In short, latent space<a id="_idIndexMarker132"/> is the vector representation of an object. To use an analogy, before you go on a blind date, a matchmaker could provide you with your counterpart’s height, weight, age, hobbies, and so on in the form of a vector:</p>
			<pre class="source-code">
[height, weight, age, hobbies,...]</pre>
			<p>You can take this vector as the latent space of your blind date counterpart. A real person’s true property dimension is almost unlimited (you could write a biography for one). The latent space can be used to represent a real person with only a limited number of features, such as height, weight, and age.</p>
			<p>In the case of the Stable Diffusion training stage, a trained encoder model, usually denoted as ℇ <em class="italic">(E)</em>, is used to encode an input image in a latent vector representation. After the reverse diffusion process, the latent space is decoded by a decoder in pixel space. The decoder is usually denoted as D <em class="italic">(D)</em>.</p>
			<p>Both <a id="_idIndexMarker133"/>training and sampling work take place in the latent space. The training process is shown in <em class="italic">Figure 5</em><em class="italic">.1</em>:</p>
			<div><div><img src="img/B21263_05_01.jpg" alt="Figure 5.1: Training Stable Diffusion model in latent space"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1: Training Stable Diffusion model in latent space</p>
			<p><em class="italic">Figure 5</em><em class="italic">.1</em> illustrates the training process of the Stable Diffusion model. It shows a high-level overview of how the model is trained.</p>
			<p>Here is a step-by-step breakdown of the process:</p>
			<ol>
				<li><strong class="bold">Inputs</strong>: The<a id="_idIndexMarker134"/> model is trained using images, caption text, and time step embeddings (specifying at which step the denoising happens).</li>
				<li><strong class="bold">Image encoder</strong>: The input<a id="_idIndexMarker135"/> image is passed through an encoder. The encoder<a id="_idIndexMarker136"/> is a neural network that processes the input image and converts it into a more abstract and compressed representation. This <a id="_idIndexMarker137"/>representation is often referred to as a “latent space” because it captures the image’s underlying characteristics, but not the pixel-level details.</li>
				<li><strong class="bold">Latent space</strong>: The <a id="_idIndexMarker138"/>encoder outputs a vector that represents the input image in the latent space. The<a id="_idIndexMarker139"/> latent space is typically a lower-dimensional space than the input space (the pixel space of the image), which allows for faster processing and more efficient representation of the input data. The whole training happens in the latent space.</li>
				<li><strong class="bold">Iterate N steps</strong>: The<a id="_idIndexMarker140"/> training process involves iterating through the latent space multiple times (<em class="italic">N</em> steps). This iterative process is where the model learns to refine the latent space representation and make small adjustments to match the desired output image.</li>
				<li><strong class="bold">UNet</strong>: After each<a id="_idIndexMarker141"/> iteration, the model uses UNet<a id="_idIndexMarker142"/> to generate an output image based on the current latent space vector. UNet generates the predicted noise and incorporates the input text embedding, step information, and potentially other <a id="_idIndexMarker143"/>embeddings.</li>
				<li><strong class="bold">The loss function</strong>: The <a id="_idIndexMarker144"/>model’s training process<a id="_idIndexMarker145"/> also involves a loss function. This measures the difference between the output image and the desired output image. As the model iterates, the loss is continually calculated, and the model makes adjustments to its weights to minimize this loss. This is how the model learns from its mistakes and improves over time.</li>
			</ol>
			<p>Refer to <a href="B21263_21.xhtml#_idTextAnchor405"><em class="italic">Chapter 21</em></a> for more detailed steps on model training.</p>
			<p>The process of inferencing from UNet is shown in <em class="italic">Figure 5</em><em class="italic">.2</em>:</p>
			<div><div><img src="img/B21263_05_02.jpg" alt="Figure 5.2: Stable Diffusion inferencing in latent space"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2: Stable Diffusion inferencing in latent space</p>
			<p>Stable Diffusion <a id="_idIndexMarker146"/>not only supports text-guided image generation; it also supports image-guided generation.</p>
			<p>In <em class="italic">Figure 5</em><em class="italic">.2</em>, starting from the left side, we can see that both text and an image are used to guide the image generation.</p>
			<p>When we provide a text input, Stable Diffusion uses CLIP [3] to generate an embedding vector, which will be fed into UNet, using the attention mechanism.</p>
			<p>When we provide an image as the guiding signal, the input image will be encoded to latent space and then concatenate with the randomly generated Gaussian noise.</p>
			<p>It is all up to us to provide guidance; we can provide either text, an image, or both. We can even generate images without providing any images; in this “empty” guidance case, the UNet model will decide what to generate based on the randomly initialized noise.</p>
			<p>With the two essential inputs provided text embeddings and the initial image latent noise (with or without the initial image’s encoded vectors in latent space), UNet kicks off to remove noise from the initial image in the latent space. After several denoising steps, with the help of a decoder, Stable Diffusion can output a vivid image in pixel space.</p>
			<p>The process is similar to the training process but without sending the loss value back to update the <a id="_idIndexMarker147"/>weights. Instead, after a number of denoising steps (<em class="italic">N</em> steps), the latent decoder (<strong class="bold">Variational Autoencoder</strong> (<strong class="bold">VAE</strong>) [4]) converts the image from latent space to visible pixel space.</p>
			<p>Next, let’s take a look at what those components (the text encoder, image Encoder, UNet, and image decoder) look like, and then we’ll build one of our own Stable Diffusion pipeline<a id="_idTextAnchor101"/>s step by step.</p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor102"/>Generating latent vectors using diffusers</h1>
			<p>In this<a id="_idIndexMarker148"/> section, we are going to use a pre-trained Stable Diffusion <a id="_idIndexMarker149"/>model to encode an image into latent space so that we have a concrete impression of what a latent vector looks and feels like. Then, we will decode the latent vector back into an image. This operation will also establish the foundation for building the image-to-image custom pipeline:</p>
			<ol>
				<li><code>load_image</code> function from <code>diffusers</code> to load an image from local storage or a URL. In the following code, we load an image named <code>dog.png</code> from the same directory of the current program:<pre class="source-code">
from diffusers.utils import load_image</pre><pre class="source-code">
image = load_image("dog.png")</pre><pre class="source-code">
display(image)</pre></li>
				<li><strong class="bold">Pre-process the image</strong>: Each pixel of the loaded image is represented by a number ranging from 0 to 255. The image encoder from the Stable Diffusion process handles image data ranging from -1.0 to 1.0. So, we first need to make the data range conversion:<pre class="source-code">
import numpy as np</pre><pre class="source-code">
# convert image object to array and </pre><pre class="source-code">
# convert pixel data from 0 ~ 255 to 0 ~ 1</pre><pre class="source-code">
image_array = np.array(image).astype(np.float32)/255.0</pre><pre class="source-code">
# convert the number from 0 ~ 1 to -1 ~ 1</pre><pre class="source-code">
image_array = image_array * 2.0 - 1.0</pre><p class="list-inset">Now, if we use Python code, <code>image_array.shape</code>, to check the <code>image_array</code> data shape, we will see the shape of the image data as – <code>(512,512,3)</code>, arranged as <code>(width, height, channel)</code>, instead of the commonly used <code>(channel, width, height).</code> Here, we need to convert the image data shape to <code>(channel, width, height)</code> or <code>(3,512,512)</code>, using the <code>transpose()</code> function:</p><pre class="source-code">
# transform the image array from width,height,</pre><pre class="source-code">
# channel to channel,width,height</pre><pre class="source-code">
image_array_cwh = image_array.transpose(2,0,1)</pre><p class="list-inset">The <code>2</code> is in the first position of <code>2, 0, 1</code>, which means moving the original third dimension (indexed as <code>2</code>) to the first dimension. The same logic applies to <code>0</code> and <code>1</code>. The original <code>0</code> dimension is now converted to the second position, and the original <code>1</code> is now in the third dimension.</p><p class="list-inset">With this transpose operation, the NumPy array, <code>image_array_cwh</code>, is now in the <code>(</code><code>3,512,512)</code> shape.</p><p class="list-inset">The Stable <a id="_idIndexMarker150"/>Diffusion image encoder handles image <a id="_idIndexMarker151"/>data in batches, which, in this instance is four-dimensional data with the batch dimension in the first position; we need to add the batch dimension here:</p><pre class="source-code">
# add batch dimension</pre><pre class="source-code">
image_array_cwh = np.expand_dims(image_array_cwh, axis = 0)</pre></li>
				<li><code>torch</code><strong class="bold"> and move to CUDA</strong>: We will convert the image data to latent space using CUDA. To achieve this, we will need to load the data into the CUDA VRAM before handing it off to the next step model:<pre class="source-code">
# load image with torch</pre><pre class="source-code">
import torch</pre><pre class="source-code">
image_array_cwh = torch.from_numpy(image_array_cwh)</pre><pre class="source-code">
image_array_cwh_cuda = image_array_cwh.to(</pre><pre class="source-code">
    "cuda",</pre><pre class="source-code">
    dtype=torch.float16</pre><pre class="source-code">
)</pre></li>
				<li><strong class="bold">Load the Stable Diffusion image encoder VAE</strong>: This VAE model is used to convert <a id="_idIndexMarker152"/>the image from pixel space to<a id="_idIndexMarker153"/> latent space:<pre class="source-code">
# Initialize VAE model</pre><pre class="source-code">
import torch</pre><pre class="source-code">
from diffusers import AutoencoderKL</pre><pre class="source-code">
vae_model = AutoencoderKL.from_pretrained(</pre><pre class="source-code">
    "runwayml/stable-diffusion-v1-5",</pre><pre class="source-code">
    subfolder = "vae",</pre><pre class="source-code">
    torch_dtype=torch.float16</pre><pre class="source-code">
).to("cuda")</pre></li>
				<li><strong class="bold">Encode the image into a latent vector</strong>: Now, everything is ready, and we can encode any image into a latent vector as PyTorch tensor:<pre class="source-code">
latents = vae_model.encode(</pre><pre class="source-code">
    image_array_cwh_cuda).latent_dist.sample()</pre><p class="list-inset">Check the data and shape of the latent data:</p><pre class="source-code">
print(latents[0])</pre><pre class="source-code">
print(latents[0].shape)</pre><p class="list-inset">We can see that the latent is in the <code>(4, 64, 64)</code> shape, with each element in the range of <code>-1.0</code> to <code>1.0</code>.</p><p class="list-inset">Stable Diffusion processes all the denoising steps on a 64x64 tensor with 4-channel for a 512x512 image generation. The data size is way less than its original image size, 512x512 with three color channels.</p></li>
				<li><strong class="bold">Decode latent to image (optional)</strong>: You may be wondering, can I convert the latent data back<a id="_idIndexMarker154"/> to the pixel image? Yes, we <a id="_idIndexMarker155"/>can do this with lines of code:<pre class="source-code">
import numpy as np</pre><pre class="source-code">
from PIL import Image</pre><pre class="source-code">
def latent_to_img(latents_input, scale_rate = 1):</pre><pre class="source-code">
    latents_2 = (1 / scale_rate) * latents_input</pre><pre class="source-code">
    # decode image</pre><pre class="source-code">
    with torch.no_grad():</pre><pre class="source-code">
        decode_image = vae_model.decode(</pre><pre class="source-code">
        latents_input, </pre><pre class="source-code">
        return_dict = False</pre><pre class="source-code">
        )[0][0]</pre><pre class="source-code">
    decode_image =  (decode_image / 2 + 0.5).clamp(0, 1)</pre><pre class="source-code">
    # move latent data from cuda to cpu</pre><pre class="source-code">
    decode_image = decode_image.to("cpu")</pre><pre class="source-code">
    # convert torch tensor to numpy array</pre><pre class="source-code">
    numpy_img = decode_image.detach().numpy()</pre><pre class="source-code">
    # covert image array from (width, height, channel) </pre><pre class="source-code">
    # to (channel, width, height)</pre><pre class="source-code">
    numpy_img_t = numpy_img.transpose(1,2,0)</pre><pre class="source-code">
    # map image data to 0, 255, and convert to int number</pre><pre class="source-code">
    numpy_img_t_01_255 = \ </pre><pre class="source-code">
        (numpy_img_t*255).round().astype("uint8")</pre><pre class="source-code">
    # shape the pillow image object from the numpy array</pre><pre class="source-code">
    return Image.fromarray(numpy_img_t_01_255)</pre><pre class="source-code">
pil_img = latent_to_img(latents_input)</pre><pre class="source-code">
pil_img</pre></li>
			</ol>
			<p>The <code>diffusers</code> Stable <a id="_idIndexMarker156"/>Diffusion pipeline will finally generate<a id="_idIndexMarker157"/> a latent tensor. We will follow similar steps to recover a denoised latent for an image in the lat<a id="_idTextAnchor103"/>ter part of this chapter.</p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor104"/>Generating text embeddings using CLIP</h1>
			<p>To<a id="_idIndexMarker158"/> generate the<a id="_idIndexMarker159"/> text embeddings (the embeddings contain the image features), we need first to tokenize the input text or prompt and then encode the token IDs into embeddings. Here are steps to achieve this:</p>
			<ol>
				<li><strong class="bold">Get the prompt </strong><strong class="bold">token IDs</strong>:<pre class="source-code">
input_prompt = "a running dog"</pre><pre class="source-code">
# input tokenizer and clip embedding model</pre><pre class="source-code">
import torch</pre><pre class="source-code">
from transformers import CLIPTokenizer,CLIPTextModel</pre><pre class="source-code">
# initialize tokenizer</pre><pre class="source-code">
clip_tokenizer = CLIPTokenizer.from_pretrained(</pre><pre class="source-code">
    "runwayml/stable-diffusion-v1-5",</pre><pre class="source-code">
    subfolder = "tokenizer",</pre><pre class="source-code">
    dtype     = torch.float16</pre><pre class="source-code">
)</pre><pre class="source-code">
input_tokens = clip_tokenizer(</pre><pre class="source-code">
    input_prompt,</pre><pre class="source-code">
    return_tensors = "pt"</pre><pre class="source-code">
)["input_ids"]</pre><pre class="source-code">
input_tokens</pre><p class="list-inset">The <a id="_idIndexMarker160"/>preceding code will convert<a id="_idIndexMarker161"/> the <code>a running dog</code> text prompt to a token ID list as a <code>torch</code> tensor object – <code>tensor([[49406,   320,</code><code>  2761,  </code><code>1929, 49407]])</code>.</p></li>
				<li><strong class="bold">Encode the token IDs </strong><strong class="bold">into embeddings</strong>:<pre class="source-code">
# initialize CLIP text encoder model</pre><pre class="source-code">
clip_text_encoder = CLIPTextModel.from_pretrained(</pre><pre class="source-code">
    "runwayml/stable-diffusion-v1-5",</pre><pre class="source-code">
    subfolder="text_encoder",</pre><pre class="source-code">
    # dtype=torch.float16</pre><pre class="source-code">
).to("cuda")</pre><pre class="source-code">
# encode token ids to embeddings</pre><pre class="source-code">
prompt_embeds = clip_text_encoder(</pre><pre class="source-code">
    input_tokens.to("cuda")</pre><pre class="source-code">
)[0]</pre></li>
				<li><strong class="bold">Check the </strong><strong class="bold">embedding data</strong>:<pre class="source-code">
print(prompt_embeds)</pre><pre class="source-code">
print(prompt_embeds.shape)</pre><p class="list-inset">Now, we<a id="_idIndexMarker162"/> can see the data of <code>prompt_embeds</code> as <a id="_idIndexMarker163"/>follows:</p><pre class="source-code">
tensor([[[-0.3884,0.0229, -0.0522,..., -0.4899, -0.3066,0.0675],</pre><pre class="source-code">
    [ 0.0290, -1.3258,  0.3085,..., -0.5257,0.9768,0.6652],</pre><pre class="source-code">
    [ 1.4642,0.2696,0.7703,..., -1.7454, -0.3677,0.5046],</pre><pre class="source-code">
    [-1.2369,0.4149,1.6844,..., -2.8617, -1.3217,0.3220],</pre><pre class="source-code">
    [-1.0182,0.7156,0.4969,..., -1.4992, -1.1128, -0.2895]]],</pre><pre class="source-code">
    device='cuda:0', grad_fn=&lt;NativeLayerNormBackward0&gt;)</pre><p class="list-inset">Its shape is <code>torch.Size([1, 5, 768])</code>. Each token ID is encoded into a 768-dimension vector.</p></li>
				<li><code>prompt</code> and <code>prompt</code>/<code>negative </code><code>prompt</code> cases:<pre class="source-code">
# prepare neg prompt embeddings</pre><pre class="source-code">
uncond_tokens = "blur"</pre><pre class="source-code">
# get the prompt embedding length</pre><pre class="source-code">
max_length = prompt_embeds.shape[1]</pre><pre class="source-code">
# generate negative prompt tokens with the same length of prompt</pre><pre class="source-code">
uncond_input_tokens = clip_tokenizer(</pre><pre class="source-code">
    uncond_tokens,</pre><pre class="source-code">
    padding = "max_length",</pre><pre class="source-code">
    max_length = max_length,</pre><pre class="source-code">
    truncation = True,</pre><pre class="source-code">
    return_tensors = "pt"</pre><pre class="source-code">
)["input_ids"]</pre><pre class="source-code">
# generate the negative embeddings</pre><pre class="source-code">
with torch.no_grad():</pre><pre class="source-code">
    negative_prompt_embeds = clip_text_encoder(</pre><pre class="source-code">
        uncond_input_tokens.to("cuda")</pre><pre class="source-code">
    )[0]</pre></li>
				<li><code>torch</code> vector:<pre class="source-code">
prompt_embeds = torch.cat([negative_prompt_embeds, </pre><pre class="source-code">
    prompt_embeds])</pre></li>
			</ol>
			<p>Next, we <a id="_idTextAnchor105"/>will initialize the time step data.</p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor106"/>Initializing time step embeddings</h1>
			<p>We introduced the <a id="_idIndexMarker168"/>scheduler in <a href="B21263_03.xhtml#_idTextAnchor064"><em class="italic">Chapter 3</em></a>. By using the scheduler, we can sample key steps for image generation. Instead of denoising 1,000 steps to generate an image in the original diffusion model (DDPM), by using a scheduler, we can generate an image in a mere 20 steps.</p>
			<p>In this section, we are going to use the Euler scheduler to generate time step embeddings, and then we’ll take a look at what the time step embeddings look like. No matter how good the diagram that tries to plot the process is, we can only understand how it works by reading the actual data and code:</p>
			<ol>
				<li><strong class="bold">Initialize a scheduler from the scheduler configuration for </strong><strong class="bold">the model</strong>:<pre class="source-code">
from diffusers import EulerDiscreteScheduler as Euler</pre><pre class="source-code">
# initialize scheduler from a pretrained checkpoint</pre><pre class="source-code">
scheduler = Euler.from_pretrained(</pre><pre class="source-code">
    "runwayml/stable-diffusion-v1-5",</pre><pre class="source-code">
    subfolder = "scheduler"</pre><pre class="source-code">
)</pre><p class="list-inset">The preceding code will initialize a scheduler from the checkpoint’s scheduler config file. Note that you can also create a scheduler, as we discussed in <a href="B21263_03.xhtml#_idTextAnchor064"><em class="italic">Chapter 3</em></a>, like this:</p><pre class="source-code">
import torch</pre><pre class="source-code">
from diffusers import StableDiffusionPipeline</pre><pre class="source-code">
from diffusers import EulerDiscreteScheduler as Euler</pre><pre class="source-code">
text2img_pipe = StableDiffusionPipeline.from_pretrained(</pre><pre class="source-code">
    "runwayml/stable-diffusion-v1-5",</pre><pre class="source-code">
    torch_dtype = torch.float16</pre><pre class="source-code">
).to("cuda:0")</pre><pre class="source-code">
scheduler = Euler.from_config(text2img_pipe.scheduler.config)</pre><p class="list-inset">However, this will<a id="_idIndexMarker169"/> require you to load a model first, which is not only slow but also unnecessary; the only thing we need is the model’s scheduler.</p></li>
				<li><strong class="bold">Sample the steps for the image </strong><strong class="bold">diffusion process</strong>:<pre class="source-code">
inference_steps = 20</pre><pre class="source-code">
scheduler.set_timesteps(inference_steps, device = "cuda")</pre><pre class="source-code">
timesteps = scheduler.timesteps</pre><pre class="source-code">
for t in timesteps:</pre><pre class="source-code">
    print(t)</pre><p class="list-inset">We will see the 20-step value as follows:</p><pre class="source-code">
...</pre><pre class="source-code">
tensor(999., device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(946.4211, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(893.8421, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(841.2632, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(788.6842, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(736.1053, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(683.5263, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(630.9474, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(578.3684, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(525.7895, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(473.2105, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(420.6316, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(368.0526, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(315.4737, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(262.8947, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(210.3158, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(157.7368, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(105.1579, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(52.5789, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(0., device='cuda:0', dtype=torch.float64)</pre></li>
			</ol>
			<p>Here, the scheduler takes <a id="_idIndexMarker170"/>20 steps out of the 1,000 steps, and those 20 steps may be enough to denoise a complete Gaussian distribution for image generation. This step sampling technique also contributes to <a id="_idTextAnchor107"/>Stable Diffusion performance boosting.</p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor108"/>Initializing the Stable Diffusion UNet</h1>
			<p>The UNet architecture [5] was <a id="_idIndexMarker171"/>introduced by Ronneberger et al. for biomedical image segmentation purposes. Before the UNet architecture, a convolution network was commonly used for image classification tasks. When using a convolution network, the output is a single class label. However, in many visual tasks, the desired output should include localization too, and the UNet model solved this problem.</p>
			<p>The U-shaped architecture of UNet enables efficient learning of features at different scales. UNet’s skip connections directly combine feature maps from different stages, allowing a model to effectively propagate information across various scales. This is crucial for denoising, as it ensures the model retains both fine-grained details and global context during noise removal. These features make UNet a good candidate for the denoising model.</p>
			<p>In the <code>Diffuser</code> library, there is a class named <code>UNet2DconditionalModel</code>; this is a conditional 2D UNet model for image generation and related tasks. It is a key component of diffusion models and plays a crucial role in the image generation process. We can load a UNet model in just several lines of code, like this:</p>
			<pre class="source-code">
import torch
from diffusers import UNet2DConditionModel
unet = UNet2DConditionModel.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    subfolder ="unet",
    torch_dtype = torch.float16
).to("cuda")</pre>
			<p>Together with the UNet model we have just loaded up, we have all the components required by Stable Diffusion. Not that hard, right? Next, we are going to use those building blocks to build two Stable Diffusion pipelines – one text-to-image and another image-to-image.</p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor109"/>Implementing a text-to-image Stable Diffusion inference pipeline</h1>
			<p>So far, we have all the <a id="_idIndexMarker172"/>text encoder, image VAE, and denoising UNet model initialized and loaded into the CUDA VRAM. The following steps will chain them together to form the simplest and working Stable Diffusion text-to-image pipeline:</p>
			<ol>
				<li><strong class="bold">Initialize a latent noise</strong>: In <em class="italic">Figure 5</em><em class="italic">.2</em>, the starting point of inference is randomly initialized Gaussian latent noise. We can create one of the latent noise with this code:<pre class="source-code">
# prepare noise latents</pre><pre class="source-code">
shape = torch.Size([1, 4, 64, 64])</pre><pre class="source-code">
device = "cuda"</pre><pre class="source-code">
noise_tensor = torch.randn(</pre><pre class="source-code">
    shape,</pre><pre class="source-code">
    generator = None,</pre><pre class="source-code">
    dtype     = torch.float16</pre><pre class="source-code">
).to("cuda")</pre><p class="list-inset">During the training stage, an initial noise sigma is used to help prevent the diffusion process from becoming stuck in local minima. When the diffusion process starts, it is very likely to be in a state where it is very close to a local minimum. <code>init_noise_sigma = 14.6146</code> is used to help avoid this. So, during the inference, we will also use <code>init_noise_sigma</code> to shape the initial latent.</p><pre class="source-code">
# scale the initial noise by the standard deviation required by </pre><pre class="source-code">
# the scheduler</pre><pre class="source-code">
latents = noise_tensor * scheduler.init_noise_sigma</pre></li>
				<li><strong class="bold">Loop through UNet</strong>: With <a id="_idIndexMarker173"/>all those components prepared, we are finally at the stage of feeding the<a id="_idIndexMarker174"/> initial latents to UNet to generate the target latent we want:<pre class="source-code">
guidance_scale = 7.5</pre><pre class="source-code">
latents_sd = torch.clone(latents)</pre><pre class="source-code">
for i,t in enumerate(timesteps):</pre><pre class="source-code">
    # expand the latents if we are doing classifier free guidance</pre><pre class="source-code">
    latent_model_input = torch.cat([latents_sd] * 2)</pre><pre class="source-code">
    latent_model_input = scheduler.scale_model_input(</pre><pre class="source-code">
        latent_model_input, t)</pre><pre class="source-code">
    # predict the noise residual</pre><pre class="source-code">
    with torch.no_grad():</pre><pre class="source-code">
        noise_pred = unet(</pre><pre class="source-code">
            latent_model_input,</pre><pre class="source-code">
            t,</pre><pre class="source-code">
            encoder_hidden_states=prompt_embeds,</pre><pre class="source-code">
            return_dict = False,</pre><pre class="source-code">
        )[0]</pre><pre class="source-code">
    # perform guidance</pre><pre class="source-code">
    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)</pre><pre class="source-code">
    noise_pred = noise_pred_uncond + guidance_scale *</pre><pre class="source-code">
        (noise_pred_text - noise_pred_uncond)</pre><pre class="source-code">
    # compute the previous noisy sample x_t -&gt; x_t-1</pre><pre class="source-code">
    latents_sd = scheduler.step(noise_pred, t,</pre><pre class="source-code">
        latents_sd, return_dict=False)[0]</pre><p class="list-inset">The <a id="_idIndexMarker175"/>preceding code is a simplified denoising loop of <code>DiffusionPipeline</code> from the <code>diffusers</code> package, removing all those edging cases and only keeping the core of the inferencing.</p><p class="list-inset">The algorithm works by iteratively adding noise to a latent representation of an image. In each iteration, the noise is guided by a text prompt, which helps the model generate images that are more similar to the prompt.</p><p class="list-inset">The preceding code first defines a few variables:</p><ul><li>The <code>guidance_scale</code> variable controls the amount of guidance that is applied to the noise.</li><li>The <code>latents_sd</code> variable stores the latent representation of the image that is generated. The time steps variable stores a list of time steps at which the noise will be added.</li></ul><p class="list-inset">The main loop of the code iterates over the time steps. In each iteration, the code first expands the latent representation to include two copies of itself. This is done because the Stable Diffusion algorithm uses a classifier-free guidance mechanism, which requires two copies of the latent representation.</p><p class="list-inset">The code then calls the <code>unet</code> function to predict the noise residual for the current time step.</p><p class="list-inset">The code then performs guidance on the noise residual. This involves adding a scaled version of the text-conditioned noise residual to the unconditional noise residual. The amount of guidance that is applied is controlled by the <code>guidance_scale</code> variable.</p><p class="list-inset">Finally, the code calls the <code>scheduler</code> function to update the latent representation of the image. The <code>scheduler</code> function is a function that controls the amount of noise that is added to the latent representation at each time step.</p><p class="list-inset">As <a id="_idIndexMarker176"/>mentioned previously, the preceding code is a simplified version of the Stable Diffusion algorithm. In practice, the algorithm is much more complex, and it incorporates a number of other techniques to improve the quality of the generated images.</p></li>
				<li><code>latent_to_img</code> function to recover the image from the latent space:<pre class="source-code">
import numpy as np</pre><pre class="source-code">
from PIL import Image</pre><pre class="source-code">
def latent_to_img(latents_input):</pre><pre class="source-code">
    # decode image</pre><pre class="source-code">
    with torch.no_grad():</pre><pre class="source-code">
        decode_image = vae_model.decode(</pre><pre class="source-code">
            latents_input,</pre><pre class="source-code">
            return_dict = False</pre><pre class="source-code">
        )[0][0]</pre><pre class="source-code">
    decode_image =  (decode_image / 2 + 0.5).clamp(0, 1)</pre><pre class="source-code">
    # move latent data from cuda to cpu</pre><pre class="source-code">
    decode_image = decode_image.to("cpu")</pre><pre class="source-code">
    # convert torch tensor to numpy array</pre><pre class="source-code">
    numpy_img = decode_image.detach().numpy()</pre><pre class="source-code">
    # covert image array from (channel, width, height) </pre><pre class="source-code">
    # to (width, height, channel)</pre><pre class="source-code">
    numpy_img_t = numpy_img.transpose(1,2,0)</pre><pre class="source-code">
    # map image data to 0, 255, and convert to int number</pre><pre class="source-code">
    numpy_img_t_01_255 = \ </pre><pre class="source-code">
        (numpy_img_t*255).round().astype("uint8")</pre><pre class="source-code">
    # shape the pillow image object from the numpy array</pre><pre class="source-code">
    return Image.fromarray(numpy_img_t_01_255)</pre><pre class="source-code">
latents_2 = (1 / 0.18215) * latents_sd</pre><pre class="source-code">
pil_img = latent_to_img(latents_2)</pre><p class="list-inset">The <code>latent_to_img</code> function<a id="_idIndexMarker177"/> performs actions in the following sequence:</p><ol><li class="upper-roman">It calls the <code>vae_model.decode</code> function to decode the latent vector into an image. The <code>vae_model.decode</code> function is a function that is trained on a dataset of images. It can be used to generate new images that are similar to the images in the dataset.</li><li class="upper-roman">Normalizes the image data to a range of <code>0</code> to <code>1</code>. This is done because the <code>Image.fromarray</code> function expects image data to be in this range.</li><li class="upper-roman">Moves the image data from the GPU to the CPU. Then, it converts the image data from a torch tensor to a NumPy array. This is done because the <code>Image.fromarray</code> function only accepts NumPy arrays as input.</li><li class="upper-roman">Flips the dimensions of the image array so that it is in the (width, height, channel) format, the format that the <code>Image.fromarray</code> function expects.</li><li class="upper-roman">Maps the image data to a range from <code>0</code> to <code>255</code> and converts it to an integer type.</li><li class="upper-roman">Calls the <code>Image.fromarray</code> function to create a Python imaging library (PIL) image object from the image data.</li></ol><p class="list-inset">The <code>latents_2 = (1 / 0.18215) * latents_sd</code> line of code is needed when decoding the latent to image because the latents are scaled by a factor of <code>0.18215</code> during<a id="_idIndexMarker178"/> training. This scaling is done to ensure that latent space has a unit variance. When decoding, the latents need to be scaled back to their original scale to reconstruct the original image.</p><p class="list-inset">Then, you should be able to see something like this if everything is going well:</p></li>
			</ol>
			<div><div><img src="img/B21263_05_03.jpg" alt="Figure 5.3: A running dog, generated by a custom Stable Diffusion pipeline"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3: A running dog, generated by a custom Stable Diffusion pipeline</p>
			<p>In the next section, we are going to implement<a id="_idTextAnchor110"/> an image-to-image Stable Diffusion pipeline.</p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor111"/>Implementing a text-guided image-to-image Stable Diffusion inference pipeline</h1>
			<p>The <a id="_idIndexMarker179"/>only thing we need to do now is concatenate the starting image with the starting latent noise. The <code>latents_input</code> Torch tensor is the latent we encoded from a dog image earlier in this chapter:</p>
			<pre class="source-code">
strength = 0.7
# scale the initial noise by the standard deviation required by the 
# scheduler
latents = latents_input*(1-strength) + 
    noise_tensor*scheduler.init_noise_sigma</pre>
			<p>That is all that is necessary; use the same code from the text-to-image pipeline, and you should generate something like <em class="italic">Figure 5</em><em class="italic">.4</em>:</p>
			<div><div><img src="img/B21263_05_04.jpg" alt="Figure 5.4: A running dog, generated by a custom image-to-image Stable Diffusion pipeline"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4: A running dog, generated by a custom image-to-image Stable Diffusion pipeline</p>
			<p>Note that the preceding code uses <code>strength = 0.7</code>; the strength denotes the weight of the original latent noise. If you want an image more similar to the initial image (the image you provided to the image-to-image pipeline), use a l<a id="_idTextAnchor112"/>ower strength number; otherwise, increase it.</p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor113"/>Summary</h1>
			<p>In this chapter, we moved on from the original diffusion model, DDPM, and explained what Stable Diffusion is and why it is faster and better than the DDPM model.</p>
			<p>As suggested by the paper <em class="italic">High-Resolution Image Synthesis with Latent Diffusion Models</em> [6] that introduced Stable Diffusion, the biggest feature that differentiates Stable Diffusion from its predecessor is the “<em class="italic">Latent</em>.” This chapter explained what latent space is and how Stable Diffusion training and inference work internally.</p>
			<p>For a comprehensive understanding, we created components using methods such as encoding the initial image into latent data, converting input prompts to token IDs and embedding them to text embeddings using the CLIP text model, using the Stable Diffusion scheduler to sample detailed steps for inference, creating the initial noise latent, concatenating initial noise latent with the initial image latent, putting all the components together to build a custom text-to-image Stable Diffusion pipeline, and extending the pipeline to enable a text-guided image-to-image Stable Diffusion pipeline. We created these components one by one, and finally, we built two Stable Diffusion pipelines – one text-to-image pipeline and an extended text-guided image-to-image pipeline.</p>
			<p>By completing this chapter, you should not only have a general understanding of Stable Diffusion but also be able to freely build your own pipelines to meet specific requirements.</p>
			<p>In the next chapter, we are going to introdu<a id="_idTextAnchor114"/>ce solutions to load Stable Diffusion models.</p>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor115"/>References</h1>
			<ol>
				<li>Jonathan Ho, Ajay Jain, Pieter Abbeel, Denoising Diffusion Probabilistic Models: <a href="https://arxiv.org/abs/2006.11239">https://arxiv.org/abs/2006.11239</a></li>
				<li>Robin et al, High-Resolution Image Synthesis with Latent Diffusion Models: <a href="https://arxiv.org/abs/2112.10752">https://arxiv.org/abs/2112.10752</a></li>
				<li>Alec et al, Learning Transferable Visual Models From Natural Language Supervision: <a href="https://arxiv.org/abs/2103.00020">https://arxiv.org/abs/2103.00020</a></li>
				<li>VAEs: <a href="https://en.wikipedia.org/wiki/Variational_autoencoder">https://en.wikipedia.org/wiki/Variational_autoencoder</a></li>
				<li>UNet2DConditionModel document from Hugging Face: <a href="https://huggingface.co/docs/diffusers/api/models/unet2d-cond">https://huggingface.co/docs/diffusers/api/models/unet2d-cond</a></li>
				<li>Robin et al, High-Resolution Image Synthesis with Latent Diffusion Models: <a href="https://arxiv.org/abs/2112.10752">https://arxiv.org/abs/2112.10752</a></li>
			</ol>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor116"/>Additional reading</h1>
			<p>Jonathan Ho, Tim Salimans, Classifier-Free Diffusion Guidance: <a href="https://arxiv.org/abs/2207.12598">https://arxiv.org/abs/2207.12598</a></p>
			<p>Stable Diffusion with Diffusers: <a href="https://huggingface.co/blog/stable_diffusion">https://huggingface.co/blog/stable_diffusion</a></p>
			<p>Olaf Ronneberger, Philipp Fischer, Thomas Brox, UNet: Convolutional Networks for Biomedical Image Segmentation: <a href="https://arxiv.org/abs/1505.04597">https://arxiv.org/abs/1505.04597</a></p>
		</div>
	</body></html>