<html><head></head><body>
		<div id="_idContainer038">
			<h1 id="_idParaDest-62" class="chapter-number"><a id="_idTextAnchor097"/>5</h1>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor098"/>Understanding How Stable Diffusion Works</h1>
			<p>In <a href="B21263_04.xhtml#_idTextAnchor081"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, we dove into the internal workings of the diffusion model with some math formulas. If you are not used to reading the formulas every day, it can be scary, but once you get familiar with those symbols and Greek letters, the benefit of fully understanding those formulas is huge. Math formulas and equations not only help us understand the core of the process in a precise and concise form, but they also enable us to read more papers and works <span class="No-Break">from others.</span></p>
			<p>While the original diffusion model is more like a proof of a concept, it shows the huge potential of the multi-step diffusion model compared with a one-pass neural network. However, some <a id="_idIndexMarker130"/>drawbacks come with the original diffusion model, <strong class="bold">denoising diffusion probabilistic models</strong> (<strong class="bold">DDPM</strong>) [1], and later Classifier Guidance denoising. Let me <span class="No-Break">list two:</span></p>
			<ul>
				<li>To train a diffusion model with Classifier Guidance requires training a new classifier, and we can’t reuse a pre-trained classifier. Also, in diffusion model training, training a classifier with 1,000 categories is already <span class="No-Break">not easy.</span></li>
				<li>Pre-trained model inferences in pixel space are computationally expensive, not to mention training a model. Using a pre-trained model to generate 512x512 images in pixel space on a home computer with 8 GB of VRAM, without memory optimization, is <span class="No-Break">not possible.</span></li>
			</ul>
			<p>In 2022, researchers <a id="_idIndexMarker131"/>proposed <strong class="bold">Latent Diffusion models</strong>, Robin et al [2]. The model nicely solved both the classification problem and the performance problem. The Latent Diffusion model was later known as <span class="No-Break">Stable Diffusion.</span></p>
			<p>In this chapter, we will take a look at how Stable Diffusion solved the preceding problems and led to state-of-the-art developments in the field of image generation. We will specifically cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Stable Diffusion in <span class="No-Break">latent space</span></li>
				<li>Generating latent vectors <span class="No-Break">using Diffusers</span></li>
				<li>Generating text embeddings <span class="No-Break">using CLIP</span></li>
				<li>Generating time <span class="No-Break">step embeddings</span></li>
				<li>Initializing Stable <span class="No-Break">Diffusion UNet</span></li>
				<li>Implementing a text-to-image Stable Diffusion <span class="No-Break">inference pipeline</span></li>
				<li>Implementing a text-guided image-to-image Stable Diffusion <span class="No-Break">inference pipeline</span></li>
				<li>Putting all the <span class="No-Break">code together</span></li>
			</ul>
			<p>Let’s dive into the core of <span class="No-Break">Stable Diffusion<a id="_idTextAnchor099"/>.</span></p>
			<p>The sample code for this chapter is tested using version 0.20.0 of the Diffusers package. To ensure the code runs smoothly, please use Diffusers v0.20.0. You can install it using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="source-inline">pip install diffusers==0.20.0</strong></pre>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor100"/>Stable Diffusion in latent space</h1>
			<p>Instead of processing diffusion in pixel space, Stable Diffusion uses latent space to represent an image. What is latent space? In short, latent space<a id="_idIndexMarker132"/> is the vector representation of an object. To use an analogy, before you go on a blind date, a matchmaker could provide you with your counterpart’s height, weight, age, hobbies, and so on in the form of <span class="No-Break">a vector:</span></p>
			<pre class="source-code">
[height, weight, age, hobbies,...]</pre>
			<p>You can take this vector as the latent space of your blind date counterpart. A real person’s true property dimension is almost unlimited (you could write a biography for one). The latent space can be used to represent a real person with only a limited number of features, such as height, weight, <span class="No-Break">and age.</span></p>
			<p>In the case of the Stable Diffusion training stage, a trained encoder model, usually denoted as ℇ <em class="italic">(E)</em>, is used to encode an input image in a latent vector representation. After the reverse diffusion process, the latent space is decoded by a decoder in pixel space. The decoder is usually denoted as <span class="No-Break">D</span><span class="No-Break"> </span><span class="No-Break"><em class="italic">(D)</em></span><span class="No-Break">.</span></p>
			<p>Both <a id="_idIndexMarker133"/>training and sampling work take place in the latent space. The training process is shown in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/B21263_05_01.jpg" alt="Figure 5.1: Training Stable Diffusion model in latent space"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1: Training Stable Diffusion model in latent space</p>
			<p><span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.1</em> illustrates the training process of the Stable Diffusion model. It shows a high-level overview of how the model <span class="No-Break">is trained.</span></p>
			<p>Here is a step-by-step breakdown of <span class="No-Break">the process:</span></p>
			<ol>
				<li><strong class="bold">Inputs</strong>: The<a id="_idIndexMarker134"/> model is trained using images, caption text, and time step embeddings (specifying at which step the <span class="No-Break">denoising happens).</span></li>
				<li><strong class="bold">Image encoder</strong>: The input<a id="_idIndexMarker135"/> image is passed through an encoder. The encoder<a id="_idIndexMarker136"/> is a neural network that processes the input image and converts it into a more abstract and compressed representation. This <a id="_idIndexMarker137"/>representation is often referred to as a “latent space” because it captures the image’s underlying characteristics, but not the <span class="No-Break">pixel-level details.</span></li>
				<li><strong class="bold">Latent space</strong>: The <a id="_idIndexMarker138"/>encoder outputs a vector that represents the input image in the latent space. The<a id="_idIndexMarker139"/> latent space is typically a lower-dimensional space than the input space (the pixel space of the image), which allows for faster processing and more efficient representation of the input data. The whole training happens in the <span class="No-Break">latent space.</span></li>
				<li><strong class="bold">Iterate N steps</strong>: The<a id="_idIndexMarker140"/> training process involves iterating through the latent space multiple times (<em class="italic">N</em> steps). This iterative process is where the model learns to refine the latent space representation and make small adjustments to match the desired <span class="No-Break">output image.</span></li>
				<li><strong class="bold">UNet</strong>: After each<a id="_idIndexMarker141"/> iteration, the model uses UNet<a id="_idIndexMarker142"/> to generate an output image based on the current latent space vector. UNet generates the predicted noise and incorporates the input text embedding, step information, and potentially <span class="No-Break">other </span><span class="No-Break"><a id="_idIndexMarker143"/></span><span class="No-Break">embeddings.</span></li>
				<li><strong class="bold">The loss function</strong>: The <a id="_idIndexMarker144"/>model’s training process<a id="_idIndexMarker145"/> also involves a loss function. This measures the difference between the output image and the desired output image. As the model iterates, the loss is continually calculated, and the model makes adjustments to its weights to minimize this loss. This is how the model learns from its mistakes and improves <span class="No-Break">over time.</span></li>
			</ol>
			<p>Refer to <a href="B21263_21.xhtml#_idTextAnchor405"><span class="No-Break"><em class="italic">Chapter 21</em></span></a> for more detailed steps on <span class="No-Break">model training.</span></p>
			<p>The process of inferencing from UNet is shown in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/B21263_05_02.jpg" alt="Figure 5.2: Stable Diffusion inferencing in latent space"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2: Stable Diffusion inferencing in latent space</p>
			<p>Stable Diffusion <a id="_idIndexMarker146"/>not only supports text-guided image generation; it also supports <span class="No-Break">image-guided generation.</span></p>
			<p>In <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.2</em>, starting from the left side, we can see that both text and an image are used to guide the <span class="No-Break">image generation.</span></p>
			<p>When we provide a text input, Stable Diffusion uses CLIP [3] to generate an embedding vector, which will be fed into UNet, using the <span class="No-Break">attention mechanism.</span></p>
			<p>When we provide an image as the guiding signal, the input image will be encoded to latent space and then concatenate with the randomly generated <span class="No-Break">Gaussian noise.</span></p>
			<p>It is all up to us to provide guidance; we can provide either text, an image, or both. We can even generate images without providing any images; in this “empty” guidance case, the UNet model will decide what to generate based on the randomly <span class="No-Break">initialized noise.</span></p>
			<p>With the two essential inputs provided text embeddings and the initial image latent noise (with or without the initial image’s encoded vectors in latent space), UNet kicks off to remove noise from the initial image in the latent space. After several denoising steps, with the help of a decoder, Stable Diffusion can output a vivid image in <span class="No-Break">pixel space.</span></p>
			<p>The process is similar to the training process but without sending the loss value back to update the <a id="_idIndexMarker147"/>weights. Instead, after a number of denoising steps (<em class="italic">N</em> steps), the latent decoder (<strong class="bold">Variational Autoencoder</strong> (<strong class="bold">VAE</strong>) [4]) converts the image from latent space to visible <span class="No-Break">pixel space.</span></p>
			<p>Next, let’s take a look at what those components (the text encoder, image Encoder, UNet, and image decoder) look like, and then we’ll build one of our own Stable Diffusion pipeline<a id="_idTextAnchor101"/>s step <span class="No-Break">by step.</span></p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor102"/>Generating latent vectors using diffusers</h1>
			<p>In this<a id="_idIndexMarker148"/> section, we are going to use a pre-trained Stable Diffusion <a id="_idIndexMarker149"/>model to encode an image into latent space so that we have a concrete impression of what a latent vector looks and feels like. Then, we will decode the latent vector back into an image. This operation will also establish the foundation for building the image-to-image <span class="No-Break">custom pipeline:</span></p>
			<ol>
				<li><strong class="bold">Load an image</strong>: We can use the <strong class="source-inline">load_image</strong> function from <strong class="source-inline">diffusers</strong> to load an image from local storage or a URL. In the following code, we load an image named <strong class="source-inline">dog.png</strong> from the same directory of the <span class="No-Break">current program:</span><pre class="source-code">
from diffusers.utils import load_image</pre><pre class="source-code">
image = load_image("dog.png")</pre><pre class="source-code">
display(image)</pre></li>
				<li><strong class="bold">Pre-process the image</strong>: Each pixel of the loaded image is represented by a number ranging from 0 to 255. The image encoder from the Stable Diffusion process handles image data ranging from -1.0 to 1.0. So, we first need to make the data <span class="No-Break">range conversion:</span><pre class="source-code">
import numpy as np</pre><pre class="source-code">
# convert image object to array and </pre><pre class="source-code">
# convert pixel data from 0 ~ 255 to 0 ~ 1</pre><pre class="source-code">
image_array = np.array(image).astype(np.float32)/255.0</pre><pre class="source-code">
# convert the number from 0 ~ 1 to -1 ~ 1</pre><pre class="source-code">
image_array = image_array * 2.0 - 1.0</pre><p class="list-inset">Now, if we use Python code, <strong class="source-inline">image_array.shape</strong>, to check the <strong class="source-inline">image_array</strong> data shape, we will see the shape of the image data as – <strong class="source-inline">(512,512,3)</strong>, arranged as <strong class="source-inline">(width, height, channel)</strong>, instead of the commonly used <strong class="source-inline">(channel, width, height).</strong> Here, we need to convert the image data shape to <strong class="source-inline">(channel, width, height)</strong> or <strong class="source-inline">(3,512,512)</strong>, using the <span class="No-Break"><strong class="source-inline">transpose()</strong></span><span class="No-Break"> function:</span></p><pre class="source-code">
# transform the image array from width,height,</pre><pre class="source-code">
# channel to channel,width,height</pre><pre class="source-code">
image_array_cwh = image_array.transpose(2,0,1)</pre><p class="list-inset">The <strong class="source-inline">2</strong> is in the first position of <strong class="source-inline">2, 0, 1</strong>, which means moving the original third dimension (indexed as <strong class="source-inline">2</strong>) to the first dimension. The same logic applies to <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. The original <strong class="source-inline">0</strong> dimension is now converted to the second position, and the original <strong class="source-inline">1</strong> is now in the <span class="No-Break">third dimension.</span></p><p class="list-inset">With this transpose operation, the NumPy array, <strong class="source-inline">image_array_cwh</strong>, is now in the <strong class="source-inline">(</strong><span class="No-Break"><strong class="source-inline">3,512,512)</strong></span><span class="No-Break"> shape.</span></p><p class="list-inset">The Stable <a id="_idIndexMarker150"/>Diffusion image encoder handles image <a id="_idIndexMarker151"/>data in batches, which, in this instance is four-dimensional data with the batch dimension in the first position; we need to add the batch <span class="No-Break">dimension here:</span></p><pre class="source-code">
# add batch dimension</pre><pre class="source-code">
image_array_cwh = np.expand_dims(image_array_cwh, axis = 0)</pre></li>
				<li><strong class="bold">Load image data with </strong><strong class="source-inline">torch</strong><strong class="bold"> and move to CUDA</strong>: We will convert the image data to latent space using CUDA. To achieve this, we will need to load the data into the CUDA VRAM before handing it off to the next <span class="No-Break">step model:</span><pre class="source-code">
# load image with torch</pre><pre class="source-code">
import torch</pre><pre class="source-code">
image_array_cwh = torch.from_numpy(image_array_cwh)</pre><pre class="source-code">
image_array_cwh_cuda = image_array_cwh.to(</pre><pre class="source-code">
    "cuda",</pre><pre class="source-code">
    dtype=torch.float16</pre><pre class="source-code">
)</pre></li>
				<li><strong class="bold">Load the Stable Diffusion image encoder VAE</strong>: This VAE model is used to convert <a id="_idIndexMarker152"/>the image from pixel space to<a id="_idIndexMarker153"/> <span class="No-Break">latent space:</span><pre class="source-code">
# Initialize VAE model</pre><pre class="source-code">
import torch</pre><pre class="source-code">
from diffusers import AutoencoderKL</pre><pre class="source-code">
vae_model = AutoencoderKL.from_pretrained(</pre><pre class="source-code">
    "runwayml/stable-diffusion-v1-5",</pre><pre class="source-code">
    subfolder = "vae",</pre><pre class="source-code">
    torch_dtype=torch.float16</pre><pre class="source-code">
).to("cuda")</pre></li>
				<li><strong class="bold">Encode the image into a latent vector</strong>: Now, everything is ready, and we can encode any image into a latent vector as <span class="No-Break">PyTorch tensor:</span><pre class="source-code">
latents = vae_model.encode(</pre><pre class="source-code">
    image_array_cwh_cuda).latent_dist.sample()</pre><p class="list-inset">Check the data and shape of the <span class="No-Break">latent data:</span></p><pre class="source-code">
print(latents[0])</pre><pre class="source-code">
print(latents[0].shape)</pre><p class="list-inset">We can see that the latent is in the <strong class="source-inline">(4, 64, 64)</strong> shape, with each element in the range of <strong class="source-inline">-1.0</strong> <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">1.0</strong></span><span class="No-Break">.</span></p><p class="list-inset">Stable Diffusion processes all the denoising steps on a 64x64 tensor with 4-channel for a 512x512 image generation. The data size is way less than its original image size, 512x512 with three <span class="No-Break">color channels.</span></p></li>
				<li><strong class="bold">Decode latent to image (optional)</strong>: You may be wondering, can I convert the latent data back<a id="_idIndexMarker154"/> to the pixel image? Yes, we <a id="_idIndexMarker155"/>can do this with lines <span class="No-Break">of code:</span><pre class="source-code">
import numpy as np</pre><pre class="source-code">
from PIL import Image</pre><pre class="source-code">
def latent_to_img(latents_input, scale_rate = 1):</pre><pre class="source-code">
    latents_2 = (1 / scale_rate) * latents_input</pre><pre class="source-code">
    # decode image</pre><pre class="source-code">
    with torch.no_grad():</pre><pre class="source-code">
        decode_image = vae_model.decode(</pre><pre class="source-code">
        latents_input, </pre><pre class="source-code">
        return_dict = False</pre><pre class="source-code">
        )[0][0]</pre><pre class="source-code">
    decode_image =  (decode_image / 2 + 0.5).clamp(0, 1)</pre><pre class="source-code">
    # move latent data from cuda to cpu</pre><pre class="source-code">
    decode_image = decode_image.to("cpu")</pre><pre class="source-code">
    # convert torch tensor to numpy array</pre><pre class="source-code">
    numpy_img = decode_image.detach().numpy()</pre><pre class="source-code">
    # covert image array from (width, height, channel) </pre><pre class="source-code">
    # to (channel, width, height)</pre><pre class="source-code">
    numpy_img_t = numpy_img.transpose(1,2,0)</pre><pre class="source-code">
    # map image data to 0, 255, and convert to int number</pre><pre class="source-code">
    numpy_img_t_01_255 = \ </pre><pre class="source-code">
        (numpy_img_t*255).round().astype("uint8")</pre><pre class="source-code">
    # shape the pillow image object from the numpy array</pre><pre class="source-code">
    return Image.fromarray(numpy_img_t_01_255)</pre><pre class="source-code">
pil_img = latent_to_img(latents_input)</pre><pre class="source-code">
pil_img</pre></li>
			</ol>
			<p>The <strong class="source-inline">diffusers</strong> Stable <a id="_idIndexMarker156"/>Diffusion pipeline will finally generate<a id="_idIndexMarker157"/> a latent tensor. We will follow similar steps to recover a denoised latent for an image in the lat<a id="_idTextAnchor103"/>ter part of <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor104"/>Generating text embeddings using CLIP</h1>
			<p>To<a id="_idIndexMarker158"/> generate the<a id="_idIndexMarker159"/> text embeddings (the embeddings contain the image features), we need first to tokenize the input text or prompt and then encode the token IDs into embeddings. Here are steps to <span class="No-Break">achieve this:</span></p>
			<ol>
				<li><strong class="bold">Get the prompt </strong><span class="No-Break"><strong class="bold">token IDs</strong></span><span class="No-Break">:</span><pre class="source-code">
input_prompt = "a running dog"</pre><pre class="source-code">
# input tokenizer and clip embedding model</pre><pre class="source-code">
import torch</pre><pre class="source-code">
from transformers import CLIPTokenizer,CLIPTextModel</pre><pre class="source-code">
# initialize tokenizer</pre><pre class="source-code">
clip_tokenizer = CLIPTokenizer.from_pretrained(</pre><pre class="source-code">
    "runwayml/stable-diffusion-v1-5",</pre><pre class="source-code">
    subfolder = "tokenizer",</pre><pre class="source-code">
    dtype     = torch.float16</pre><pre class="source-code">
)</pre><pre class="source-code">
input_tokens = clip_tokenizer(</pre><pre class="source-code">
    input_prompt,</pre><pre class="source-code">
    return_tensors = "pt"</pre><pre class="source-code">
)["input_ids"]</pre><pre class="source-code">
input_tokens</pre><p class="list-inset">The <a id="_idIndexMarker160"/>preceding code will convert<a id="_idIndexMarker161"/> the <strong class="source-inline">a running dog</strong> text prompt to a token ID list as a <strong class="source-inline">torch</strong> tensor object – <strong class="source-inline">tensor([[49406,   320,</strong><strong class="source-inline">  2761,  </strong><span class="No-Break"><strong class="source-inline">1929, 49407]])</strong></span><span class="No-Break">.</span></p></li>
				<li><strong class="bold">Encode the token IDs </strong><span class="No-Break"><strong class="bold">into embeddings</strong></span><span class="No-Break">:</span><pre class="source-code">
# initialize CLIP text encoder model</pre><pre class="source-code">
clip_text_encoder = CLIPTextModel.from_pretrained(</pre><pre class="source-code">
    "runwayml/stable-diffusion-v1-5",</pre><pre class="source-code">
    subfolder="text_encoder",</pre><pre class="source-code">
    # dtype=torch.float16</pre><pre class="source-code">
).to("cuda")</pre><pre class="source-code">
# encode token ids to embeddings</pre><pre class="source-code">
prompt_embeds = clip_text_encoder(</pre><pre class="source-code">
    input_tokens.to("cuda")</pre><pre class="source-code">
)[0]</pre></li>
				<li><strong class="bold">Check the </strong><span class="No-Break"><strong class="bold">embedding data</strong></span><span class="No-Break">:</span><pre class="source-code">
print(prompt_embeds)</pre><pre class="source-code">
print(prompt_embeds.shape)</pre><p class="list-inset">Now, we<a id="_idIndexMarker162"/> can see the data of <strong class="source-inline">prompt_embeds</strong> <span class="No-Break">as </span><span class="No-Break"><a id="_idIndexMarker163"/></span><span class="No-Break">follows:</span></p><pre class="source-code">
tensor([[[-0.3884,0.0229, -0.0522,..., -0.4899, -0.3066,0.0675],</pre><pre class="source-code">
    [ 0.0290, -1.3258,  0.3085,..., -0.5257,0.9768,0.6652],</pre><pre class="source-code">
    [ 1.4642,0.2696,0.7703,..., -1.7454, -0.3677,0.5046],</pre><pre class="source-code">
    [-1.2369,0.4149,1.6844,..., -2.8617, -1.3217,0.3220],</pre><pre class="source-code">
    [-1.0182,0.7156,0.4969,..., -1.4992, -1.1128, -0.2895]]],</pre><pre class="source-code">
    device='cuda:0', grad_fn=&lt;NativeLayerNormBackward0&gt;)</pre><p class="list-inset">Its shape is <strong class="source-inline">torch.Size([1, 5, 768])</strong>. Each token ID is encoded into a <span class="No-Break">768-dimension vector.</span></p></li>
				<li><strong class="bold">Generate embedding for negative prompt embeddings</strong>: Even though we don’t <a id="_idIndexMarker164"/>have the negative prompt, we’ll <a id="_idIndexMarker165"/>also prepare an embedding vector with the same size as the input prompt. This will ensure that our code will support both only <strong class="source-inline">prompt</strong> and <strong class="source-inline">prompt</strong>/<strong class="source-inline">negative </strong><span class="No-Break"><strong class="source-inline">prompt</strong></span><span class="No-Break"> cases:</span><pre class="source-code">
# prepare neg prompt embeddings</pre><pre class="source-code">
uncond_tokens = "blur"</pre><pre class="source-code">
# get the prompt embedding length</pre><pre class="source-code">
max_length = prompt_embeds.shape[1]</pre><pre class="source-code">
# generate negative prompt tokens with the same length of prompt</pre><pre class="source-code">
uncond_input_tokens = clip_tokenizer(</pre><pre class="source-code">
    uncond_tokens,</pre><pre class="source-code">
    padding = "max_length",</pre><pre class="source-code">
    max_length = max_length,</pre><pre class="source-code">
    truncation = True,</pre><pre class="source-code">
    return_tensors = "pt"</pre><pre class="source-code">
)["input_ids"]</pre><pre class="source-code">
# generate the negative embeddings</pre><pre class="source-code">
with torch.no_grad():</pre><pre class="source-code">
    negative_prompt_embeds = clip_text_encoder(</pre><pre class="source-code">
        uncond_input_tokens.to("cuda")</pre><pre class="source-code">
    )[0]</pre></li>
				<li><strong class="bold">Concatenate prompt and negative prompt embedding into one vector</strong>: Because we<a id="_idIndexMarker166"/> will feed the whole prompt into UNet at <a id="_idIndexMarker167"/>once, and then handle the positive and negative signals at the UNet inference stage, we will concatenate the prompt and negative prompt embeddings into one <span class="No-Break"><strong class="source-inline">torch</strong></span><span class="No-Break"> vector:</span><pre class="source-code">
prompt_embeds = torch.cat([negative_prompt_embeds, </pre><pre class="source-code">
    prompt_embeds])</pre></li>
			</ol>
			<p>Next, we <a id="_idTextAnchor105"/>will initialize the time <span class="No-Break">step data.</span></p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor106"/>Initializing time step embeddings</h1>
			<p>We introduced the <a id="_idIndexMarker168"/>scheduler in <a href="B21263_03.xhtml#_idTextAnchor064"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>. By using the scheduler, we can sample key steps for image generation. Instead of denoising 1,000 steps to generate an image in the original diffusion model (DDPM), by using a scheduler, we can generate an image in a mere <span class="No-Break">20 steps.</span></p>
			<p>In this section, we are going to use the Euler scheduler to generate time step embeddings, and then we’ll take a look at what the time step embeddings look like. No matter how good the diagram that tries to plot the process is, we can only understand how it works by reading the actual data <span class="No-Break">and code:</span></p>
			<ol>
				<li><strong class="bold">Initialize a scheduler from the scheduler configuration for </strong><span class="No-Break"><strong class="bold">the model</strong></span><span class="No-Break">:</span><pre class="source-code">
from diffusers import EulerDiscreteScheduler as Euler</pre><pre class="source-code">
# initialize scheduler from a pretrained checkpoint</pre><pre class="source-code">
scheduler = Euler.from_pretrained(</pre><pre class="source-code">
    "runwayml/stable-diffusion-v1-5",</pre><pre class="source-code">
    subfolder = "scheduler"</pre><pre class="source-code">
)</pre><p class="list-inset">The preceding code will initialize a scheduler from the checkpoint’s scheduler config file. Note that you can also create a scheduler, as we discussed in <a href="B21263_03.xhtml#_idTextAnchor064"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <span class="No-Break">like this:</span></p><pre class="source-code">
import torch</pre><pre class="source-code">
from diffusers import StableDiffusionPipeline</pre><pre class="source-code">
from diffusers import EulerDiscreteScheduler as Euler</pre><pre class="source-code">
text2img_pipe = StableDiffusionPipeline.from_pretrained(</pre><pre class="source-code">
    "runwayml/stable-diffusion-v1-5",</pre><pre class="source-code">
    torch_dtype = torch.float16</pre><pre class="source-code">
).to("cuda:0")</pre><pre class="source-code">
scheduler = Euler.from_config(text2img_pipe.scheduler.config)</pre><p class="list-inset">However, this will<a id="_idIndexMarker169"/> require you to load a model first, which is not only slow but also unnecessary; the only thing we need is the <span class="No-Break">model’s scheduler.</span></p></li>
				<li><strong class="bold">Sample the steps for the image </strong><span class="No-Break"><strong class="bold">diffusion process</strong></span><span class="No-Break">:</span><pre class="source-code">
inference_steps = 20</pre><pre class="source-code">
scheduler.set_timesteps(inference_steps, device = "cuda")</pre><pre class="source-code">
timesteps = scheduler.timesteps</pre><pre class="source-code">
for t in timesteps:</pre><pre class="source-code">
    print(t)</pre><p class="list-inset">We will see the 20-step value <span class="No-Break">as follows:</span></p><pre class="source-code">
...</pre><pre class="source-code">
tensor(999., device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(946.4211, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(893.8421, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(841.2632, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(788.6842, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(736.1053, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(683.5263, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(630.9474, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(578.3684, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(525.7895, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(473.2105, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(420.6316, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(368.0526, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(315.4737, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(262.8947, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(210.3158, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(157.7368, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(105.1579, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(52.5789, device='cuda:0', dtype=torch.float64)</pre><pre class="source-code">
tensor(0., device='cuda:0', dtype=torch.float64)</pre></li>
			</ol>
			<p>Here, the scheduler takes <a id="_idIndexMarker170"/>20 steps out of the 1,000 steps, and those 20 steps may be enough to denoise a complete Gaussian distribution for image generation. This step sampling technique also contributes to <a id="_idTextAnchor107"/>Stable Diffusion <span class="No-Break">performance boosting.</span></p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor108"/>Initializing the Stable Diffusion UNet</h1>
			<p>The UNet architecture [5] was <a id="_idIndexMarker171"/>introduced by Ronneberger et al. for biomedical image segmentation purposes. Before the UNet architecture, a convolution network was commonly used for image classification tasks. When using a convolution network, the output is a single class label. However, in many visual tasks, the desired output should include localization too, and the UNet model solved <span class="No-Break">this problem.</span></p>
			<p>The U-shaped architecture of UNet enables efficient learning of features at different scales. UNet’s skip connections directly combine feature maps from different stages, allowing a model to effectively propagate information across various scales. This is crucial for denoising, as it ensures the model retains both fine-grained details and global context during noise removal. These features make UNet a good candidate for the <span class="No-Break">denoising model.</span></p>
			<p>In the <strong class="source-inline">Diffuser</strong> library, there is a class named <strong class="source-inline">UNet2DconditionalModel</strong>; this is a conditional 2D UNet model for image generation and related tasks. It is a key component of diffusion models and plays a crucial role in the image generation process. We can load a UNet model in just several lines of code, <span class="No-Break">like this:</span></p>
			<pre class="source-code">
import torch
from diffusers import UNet2DConditionModel
unet = UNet2DConditionModel.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    subfolder ="unet",
    torch_dtype = torch.float16
).to("cuda")</pre>
			<p>Together with the UNet model we have just loaded up, we have all the components required by Stable Diffusion. Not that hard, right? Next, we are going to use those building blocks to build two Stable Diffusion pipelines – one text-to-image and <span class="No-Break">another image-to-image</span><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor109"/>Implementing a text-to-image Stable Diffusion inference pipeline</h1>
			<p>So far, we have all the <a id="_idIndexMarker172"/>text encoder, image VAE, and denoising UNet model initialized and loaded into the CUDA VRAM. The following steps will chain them together to form the simplest and working Stable Diffusion <span class="No-Break">text-to-image pipeline:</span></p>
			<ol>
				<li><strong class="bold">Initialize a latent noise</strong>: In <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.2</em>, the starting point of inference is randomly initialized Gaussian latent noise. We can create one of the latent noise with <span class="No-Break">this code:</span><pre class="source-code">
# prepare noise latents</pre><pre class="source-code">
shape = torch.Size([1, 4, 64, 64])</pre><pre class="source-code">
device = "cuda"</pre><pre class="source-code">
noise_tensor = torch.randn(</pre><pre class="source-code">
    shape,</pre><pre class="source-code">
    generator = None,</pre><pre class="source-code">
    dtype     = torch.float16</pre><pre class="source-code">
).to("cuda")</pre><p class="list-inset">During the training stage, an initial noise sigma is used to help prevent the diffusion process from becoming stuck in local minima. When the diffusion process starts, it is very likely to be in a state where it is very close to a local minimum. <strong class="source-inline">init_noise_sigma = 14.6146</strong> is used to help avoid this. So, during the inference, we will also use <strong class="source-inline">init_noise_sigma</strong> to shape the <span class="No-Break">initial latent.</span></p><pre class="source-code">
# scale the initial noise by the standard deviation required by </pre><pre class="source-code">
# the scheduler</pre><pre class="source-code">
latents = noise_tensor * scheduler.init_noise_sigma</pre></li>
				<li><strong class="bold">Loop through UNet</strong>: With <a id="_idIndexMarker173"/>all those components prepared, we are finally at the stage of feeding the<a id="_idIndexMarker174"/> initial latents to UNet to generate the target latent <span class="No-Break">we want:</span><pre class="source-code">
guidance_scale = 7.5</pre><pre class="source-code">
latents_sd = torch.clone(latents)</pre><pre class="source-code">
for i,t in enumerate(timesteps):</pre><pre class="source-code">
    # expand the latents if we are doing classifier free guidance</pre><pre class="source-code">
    latent_model_input = torch.cat([latents_sd] * 2)</pre><pre class="source-code">
    latent_model_input = scheduler.scale_model_input(</pre><pre class="source-code">
        latent_model_input, t)</pre><pre class="source-code">
    # predict the noise residual</pre><pre class="source-code">
    with torch.no_grad():</pre><pre class="source-code">
        noise_pred = unet(</pre><pre class="source-code">
            latent_model_input,</pre><pre class="source-code">
            t,</pre><pre class="source-code">
            encoder_hidden_states=prompt_embeds,</pre><pre class="source-code">
            return_dict = False,</pre><pre class="source-code">
        )[0]</pre><pre class="source-code">
    # perform guidance</pre><pre class="source-code">
    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)</pre><pre class="source-code">
    noise_pred = noise_pred_uncond + guidance_scale *</pre><pre class="source-code">
        (noise_pred_text - noise_pred_uncond)</pre><pre class="source-code">
    # compute the previous noisy sample x_t -&gt; x_t-1</pre><pre class="source-code">
    latents_sd = scheduler.step(noise_pred, t,</pre><pre class="source-code">
        latents_sd, return_dict=False)[0]</pre><p class="list-inset">The <a id="_idIndexMarker175"/>preceding code is a simplified denoising loop of <strong class="source-inline">DiffusionPipeline</strong> from the <strong class="source-inline">diffusers</strong> package, removing all those edging cases and only keeping the core of <span class="No-Break">the inferencing.</span></p><p class="list-inset">The algorithm works by iteratively adding noise to a latent representation of an image. In each iteration, the noise is guided by a text prompt, which helps the model generate images that are more similar to <span class="No-Break">the prompt.</span></p><p class="list-inset">The preceding code first defines a <span class="No-Break">few variables:</span></p><ul><li>The <strong class="source-inline">guidance_scale</strong> variable controls the amount of guidance that is applied to <span class="No-Break">the noise.</span></li><li>The <strong class="source-inline">latents_sd</strong> variable stores the latent representation of the image that is generated. The time steps variable stores a list of time steps at which the noise will <span class="No-Break">be added.</span></li></ul><p class="list-inset">The main loop of the code iterates over the time steps. In each iteration, the code first expands the latent representation to include two copies of itself. This is done because the Stable Diffusion algorithm uses a classifier-free guidance mechanism, which requires two copies of the <span class="No-Break">latent representation.</span></p><p class="list-inset">The code then calls the <strong class="source-inline">unet</strong> function to predict the noise residual for the current <span class="No-Break">time step.</span></p><p class="list-inset">The code then performs guidance on the noise residual. This involves adding a scaled version of the text-conditioned noise residual to the unconditional noise residual. The amount of guidance that is applied is controlled by the <span class="No-Break"><strong class="source-inline">guidance_scale</strong></span><span class="No-Break"> variable.</span></p><p class="list-inset">Finally, the code calls the <strong class="source-inline">scheduler</strong> function to update the latent representation of the image. The <strong class="source-inline">scheduler</strong> function is a function that controls the amount of noise that is added to the latent representation at each <span class="No-Break">time step.</span></p><p class="list-inset">As <a id="_idIndexMarker176"/>mentioned previously, the preceding code is a simplified version of the Stable Diffusion algorithm. In practice, the algorithm is much more complex, and it incorporates a number of other techniques to improve the quality of the <span class="No-Break">generated images.</span></p></li>
				<li><strong class="bold">Recover the image from the latent</strong>: We can reuse the <strong class="source-inline">latent_to_img</strong> function to recover the image from the <span class="No-Break">latent space:</span><pre class="source-code">
import numpy as np</pre><pre class="source-code">
from PIL import Image</pre><pre class="source-code">
def latent_to_img(latents_input):</pre><pre class="source-code">
    # decode image</pre><pre class="source-code">
    with torch.no_grad():</pre><pre class="source-code">
        decode_image = vae_model.decode(</pre><pre class="source-code">
            latents_input,</pre><pre class="source-code">
            return_dict = False</pre><pre class="source-code">
        )[0][0]</pre><pre class="source-code">
    decode_image =  (decode_image / 2 + 0.5).clamp(0, 1)</pre><pre class="source-code">
    # move latent data from cuda to cpu</pre><pre class="source-code">
    decode_image = decode_image.to("cpu")</pre><pre class="source-code">
    # convert torch tensor to numpy array</pre><pre class="source-code">
    numpy_img = decode_image.detach().numpy()</pre><pre class="source-code">
    # covert image array from (channel, width, height) </pre><pre class="source-code">
    # to (width, height, channel)</pre><pre class="source-code">
    numpy_img_t = numpy_img.transpose(1,2,0)</pre><pre class="source-code">
    # map image data to 0, 255, and convert to int number</pre><pre class="source-code">
    numpy_img_t_01_255 = \ </pre><pre class="source-code">
        (numpy_img_t*255).round().astype("uint8")</pre><pre class="source-code">
    # shape the pillow image object from the numpy array</pre><pre class="source-code">
    return Image.fromarray(numpy_img_t_01_255)</pre><pre class="source-code">
latents_2 = (1 / 0.18215) * latents_sd</pre><pre class="source-code">
pil_img = latent_to_img(latents_2)</pre><p class="list-inset">The <strong class="source-inline">latent_to_img</strong> function<a id="_idIndexMarker177"/> performs actions in the <span class="No-Break">following sequence:</span></p><ol><li class="upper-roman">It calls the <strong class="source-inline">vae_model.decode</strong> function to decode the latent vector into an image. The <strong class="source-inline">vae_model.decode</strong> function is a function that is trained on a dataset of images. It can be used to generate new images that are similar to the images in <span class="No-Break">the dataset.</span></li><li class="upper-roman">Normalizes the image data to a range of <strong class="source-inline">0</strong> to <strong class="source-inline">1</strong>. This is done because the <strong class="source-inline">Image.fromarray</strong> function expects image data to be in <span class="No-Break">this range.</span></li><li class="upper-roman">Moves the image data from the GPU to the CPU. Then, it converts the image data from a torch tensor to a NumPy array. This is done because the <strong class="source-inline">Image.fromarray</strong> function only accepts NumPy arrays <span class="No-Break">as input.</span></li><li class="upper-roman">Flips the dimensions of the image array so that it is in the (width, height, channel) format, the format that the <strong class="source-inline">Image.fromarray</strong> <span class="No-Break">function expects.</span></li><li class="upper-roman">Maps the image data to a range from <strong class="source-inline">0</strong> to <strong class="source-inline">255</strong> and converts it to an <span class="No-Break">integer type.</span></li><li class="upper-roman">Calls the <strong class="source-inline">Image.fromarray</strong> function to create a Python imaging library (PIL) image object from the <span class="No-Break">image data.</span></li></ol><p class="list-inset">The <strong class="source-inline">latents_2 = (1 / 0.18215) * latents_sd</strong> line of code is needed when decoding the latent to image because the latents are scaled by a factor of <strong class="source-inline">0.18215</strong> during<a id="_idIndexMarker178"/> training. This scaling is done to ensure that latent space has a unit variance. When decoding, the latents need to be scaled back to their original scale to reconstruct the <span class="No-Break">original image.</span></p><p class="list-inset">Then, you should be able to see something like this if everything is <span class="No-Break">going well</span><span class="No-Break">:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/B21263_05_03.jpg" alt="Figure 5.3: A running dog, generated by a custom Stable Diffusion pipeline"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3: A running dog, generated by a custom Stable Diffusion pipeline</p>
			<p>In the next section, we are going to implement<a id="_idTextAnchor110"/> an image-to-image Stable <span class="No-Break">Diffusion pipeline.</span></p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor111"/>Implementing a text-guided image-to-image Stable Diffusion inference pipeline</h1>
			<p>The <a id="_idIndexMarker179"/>only thing we need to do now is concatenate the starting image with the starting latent noise. The <strong class="source-inline">latents_input</strong> Torch tensor is the latent we encoded from a dog image earlier in <span class="No-Break">this chapter:</span></p>
			<pre class="source-code">
strength = 0.7
# scale the initial noise by the standard deviation required by the 
# scheduler
latents = latents_input*(1-strength) + 
    noise_tensor*scheduler.init_noise_sigma</pre>
			<p>That is all that is necessary; use the same code from the text-to-image pipeline, and you should generate something like <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B21263_05_04.jpg" alt="Figure 5.4: A running dog, generated by a custom image-to-image Stable Diffusion pipeline"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4: A running dog, generated by a custom image-to-image Stable Diffusion pipeline</p>
			<p>Note that the preceding code uses <strong class="source-inline">strength = 0.7</strong>; the strength denotes the weight of the original latent noise. If you want an image more similar to the initial image (the image you provided to the image-to-image pipeline), use a l<a id="_idTextAnchor112"/>ower strength number; otherwise, <span class="No-Break">increase it.</span></p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor113"/>Summary</h1>
			<p>In this chapter, we moved on from the original diffusion model, DDPM, and explained what Stable Diffusion is and why it is faster and better than the <span class="No-Break">DDPM model.</span></p>
			<p>As suggested by the paper <em class="italic">High-Resolution Image Synthesis with Latent Diffusion Models</em> [6] that introduced Stable Diffusion, the biggest feature that differentiates Stable Diffusion from its predecessor is the “<em class="italic">Latent</em>.” This chapter explained what latent space is and how Stable Diffusion training and inference <span class="No-Break">work internally.</span></p>
			<p>For a comprehensive understanding, we created components using methods such as encoding the initial image into latent data, converting input prompts to token IDs and embedding them to text embeddings using the CLIP text model, using the Stable Diffusion scheduler to sample detailed steps for inference, creating the initial noise latent, concatenating initial noise latent with the initial image latent, putting all the components together to build a custom text-to-image Stable Diffusion pipeline, and extending the pipeline to enable a text-guided image-to-image Stable Diffusion pipeline. We created these components one by one, and finally, we built two Stable Diffusion pipelines – one text-to-image pipeline and an extended text-guided <span class="No-Break">image-to-image pipeline.</span></p>
			<p>By completing this chapter, you should not only have a general understanding of Stable Diffusion but also be able to freely build your own pipelines to meet <span class="No-Break">specific requirements.</span></p>
			<p>In the next chapter, we are going to introdu<a id="_idTextAnchor114"/>ce solutions to load Stable <span class="No-Break">Diffusion models.</span></p>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor115"/>References</h1>
			<ol>
				<li>Jonathan Ho, Ajay Jain, Pieter Abbeel, Denoising Diffusion Probabilistic <span class="No-Break">Models: </span><a href="https://arxiv.org/abs/2006.11239"><span class="No-Break">https://arxiv.org/abs/2006.11239</span></a></li>
				<li>Robin et al, High-Resolution Image Synthesis with Latent Diffusion <span class="No-Break">Models: </span><a href="https://arxiv.org/abs/2112.10752"><span class="No-Break">https://arxiv.org/abs/2112.10752</span></a></li>
				<li>Alec et al, Learning Transferable Visual Models From Natural Language <span class="No-Break">Supervision: </span><a href="https://arxiv.org/abs/2103.00020"><span class="No-Break">https://arxiv.org/abs/2103.00020</span></a></li>
				<li><span class="No-Break">VAEs: </span><a href="https://en.wikipedia.org/wiki/Variational_autoencoder"><span class="No-Break">https://en.wikipedia.org/wiki/Variational_autoencoder</span></a></li>
				<li>UNet2DConditionModel document from Hugging <span class="No-Break">Face: </span><a href="https://huggingface.co/docs/diffusers/api/models/unet2d-cond"><span class="No-Break">https://huggingface.co/docs/diffusers/api/models/unet2d-cond</span></a></li>
				<li>Robin et al, High-Resolution Image Synthesis with Latent Diffusion <span class="No-Break">Models: </span><a href="https://arxiv.org/abs/2112.10752"><span class="No-Break">https://arxiv.org/abs/2112.10752</span></a></li>
			</ol>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor116"/>Additional reading</h1>
			<p>Jonathan Ho, Tim Salimans, Classifier-Free Diffusion <span class="No-Break">Guidance: </span><a href="https://arxiv.org/abs/2207.12598"><span class="No-Break">https://arxiv.org/abs/2207.12598</span></a></p>
			<p>Stable Diffusion with <span class="No-Break">Diffusers: </span><a href="https://huggingface.co/blog/stable_diffusion"><span class="No-Break">https://huggingface.co/blog/stable_diffusion</span></a></p>
			<p>Olaf Ronneberger, Philipp Fischer, Thomas Brox, UNet: Convolutional Networks for Biomedical Image <span class="No-Break">Segmentation: </span><a href="https://arxiv.org/abs/1505.04597"><span class="No-Break">https://arxiv.org/abs/1505.04597</span></a></p>
		</div>
	</body></html>