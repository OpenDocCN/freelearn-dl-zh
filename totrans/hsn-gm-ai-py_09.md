# 深入学习 DDQN

深度学习是原始计算学习的演变，它正在迅速发展并开始主导数据科学、**机器学习**（**ML**）和**人工智能**（**AI**）等所有领域。反过来，这些增强带来了在**深度强化学习**（**DRL**）方面的惊人创新，使得它能够玩以前被认为不可能的游戏。现在，DRL 能够处理像经典 Atari 2600 系列这样的游戏环境，并且比人类玩得更好。在本章中，我们将探讨深度学习中的哪些新特性使得 DRL 能够玩视觉状态游戏，例如 Atari 游戏。首先，我们将探讨如何将游戏屏幕用作视觉状态。然后，我们将了解深度学习如何通过一个名为**卷积神经网络**（**CNNs**）的新组件来消费视觉状态。之后，我们将利用这些知识来构建一个修改后的 DQN 代理来处理 Atari 环境。在此基础上，我们将探讨 DQN 的一个增强版本，称为**DDQN**，或**双重（对抗）DQN**。最后，我们将通过玩其他视觉环境来结束本章。

总结来说，在本章中，我们将探讨深度学习（DL）的扩展，称为卷积神经网络（CNNs），如何被用来观察视觉状态。然后，我们将利用这些知识来玩 Atari 游戏，并在过程中实现进一步的增强。以下是本章我们将涵盖的内容：

+   理解视觉状态

+   介绍 CNNs

+   在 Atari 上使用 DQN

+   介绍 DDQN

+   扩展回放与优先经验回放

我们将继续在本章中使用我们在[第 6 章](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml)“深入学习 DQN”中构建的相同虚拟环境。为了使用本章中的示例，您需要正确设置和配置该环境。

# 理解视觉状态

迄今为止，我们观察状态作为编码值或值。这些值可能是网格中的单元格编号或区域中的 x,y 位置。无论如何，这些值都是相对于某个参考进行编码的。在网格环境的情况下，我们可能使用一个数字来表示方块或一对数字。对于 x,y 坐标，我们仍然需要表示一个原点，以下是一些三种类型编码机制的示例：

![图片](img/b90d1fe2-5c07-428d-a721-5ba6484829e8.png)

代理的编码状态的三种类型

在前面的图中，有三个用于环境状态编码的例子。对于第一个例子，位于左侧，我们只是用一个数字来表示该状态。向右移动到下一个网格，状态现在表示为一对数字，按行和列排列。在右侧，我们可以看到我们熟悉的好朋友月球着陆器，以及它的部分状态，即位置，相对于着陆点，即原点。在这些所有情况下，状态总是以某种编码形式表示，无论是单个数字还是像着陆器环境中的八个数字。通过编码，我们是指我们使用一个值，即一个数字，来表示环境的这种状态。在[第五章](3e0c16c5-2145-498c-8ba1-b917745e0ef0.xhtml)《探索SARSA》中，我们学习了状态离散化是这种编码转换成更简单形式的一种类型，但转换这种编码需要调整或学习，我们意识到需要有一种更好的方法来做这件事。幸运的是，我们确实设计了一种更好的方法，但在我们到达那里之前，让我们考虑一下状态本身是什么。

状态只是我们策略的一个数值表示或索引，它让我们的智能体确定其下一步行动的选择。在这里需要记住的重要一点是，状态需要是策略或，在DRL的情况下，模型的索引。因此，我们的智能体始终需要将那种状态转换成模型中的数值索引。正如我们已经看到的，使用深度学习（DL）可以使这个过程大大简化。理想的情况是智能体能够像我们人类一样直观地消费相同的可见状态——游戏区域——并学会自行编码状态。10年前，这样的说法听起来像是科幻小说。今天，这已经成为科学事实，我们将在下一节中学习它是如何实现的。

# 编码视觉状态

幸运的是，对于DRL来说，从图像中学习这一概念已经成为了深度学习（DL）持续研究30多年的中心。深度学习已经将这一概念从识别手写数字扩展到检测物体位置和旋转，再到理解人类姿态。所有这些都是在将原始像素输入深度学习网络并教会（或教会自己）如何将这些图像编码成某种答案的过程中完成的。我们将在本章中使用这些相同的工具，但在我们这样做之前，让我们了解将图像输入网络的基本原理。以下图示展示了你可能如何进行这一操作：

![示例图](img/0a2d2581-3f16-4722-86ac-39237e1cdbd0.png)

将图像剖析为深度学习输入

在前面的图中，图像被分割成四个部分，每个部分作为一块被输入到网络中。需要注意的是，每个部分是如何被输入到输入层上的每个神经元的。现在，我们可以使用四个部分，就像前面的图一样，或者100个部分，可能是将图像分解成像素。无论如何，我们仍然是在盲目地离散化空间，即图像，并试图理解它。有趣的是，我们在强化学习（RL）中识别出的离散化问题在深度学习（DL）中也会遇到。在DL中，这个问题可能更加复杂，因为我们通常会简单地将图像，一个二维的数据矩阵，展平成一个一维的数字向量。例如，在前面的例子中，我们可以看到两个眼睛被输入到网络中，但没有表明它们之间的关系，例如间距和方向。当我们展平图像时，这些信息完全丢失，而且随着我们对输入图像的离散化程度越高，这些信息就越重要。我们需要的是，深度学习（DL）发现的是，从一组数据，如图像，中提取特定特征，并保留这些特征，以便以某种方式对整个图像进行分类。实际上，深度学习（DL）很好地解决了这个问题，我们将在下一节中了解到这一点。

# 介绍CNN

2012年9月，多伦多大学的杰弗里·辛顿博士领导的团队，被认为是深度学习的教父，竞争构建AlexNet。AlexNet是在对抗一个名为ImageNet的巨大图像测试集时进行训练的。ImageNet包含超过1400万张图片，分布在20000多个不同的类别中。AlexNet那年以超过10分的优势击败了其竞争对手，一个非深度学习解决方案，并实现了许多人认为不可能的事情——即图像中对象的识别做得和人类一样好，甚至可能更好。从那时起，使这一切成为可能的组件——CNN——在某些情况下已经超过了人类在图像识别方面的认知水平。

使这一切成为可能的组件，CNN（卷积神经网络），通过将图像分解成特征来工作——这些特征是通过学习检测这些特征来学习的。这听起来有点递归，确实如此，但这也是它之所以能如此有效的原因。所以，让我们再重复一遍。CNN通过检测图像中的特征来工作，但我们没有指定这些特征——我们指定的是答案是对还是错。通过使用那个答案，我们可以使用反向传播将任何错误推回网络并通过纠正网络检测这些特征的方式来纠正。

为了检测特征，我们使用滤波器，这与你在Photoshop中使用滤波器的方式类似。这些滤波器是我们现在训练的部件，我们通过在新的层类型CNN（卷积或CONV）中引入它们来进行训练。我们发现，我们还可以将这些层堆叠起来以提取更多特征。这些概念可能仍然很抽象。幸运的是，有许多优秀的工具可以帮助我们在下一个练习中探索这些概念。让我们看看其中一个：

1.  打开并使用网络浏览器访问[tensorspace.org](https://tensorspace.org)。

1.  找到**Playground**的链接并点击它。

1.  在**TensorSpace Playground**页面，注意左侧的各种模型名称。点击以下截图所示的**AlexNet**示例：

![图片](img/6a62226d-3d38-4446-98b7-f1c66579887a.png)

TensorSpace Playground – AlexNet

Playground允许你交互式地探索各种深度学习模型，例如AlexNet，直到层级别。

1.  在图中移动并点击各个层。你可以放大和缩小，并在3D中探索模型。你将能够查看网络模型中的所有层。每种层类型都有不同的颜色编码。这包括CNN层（黄色），以及特殊的池化层（蓝色）。

池化层，这些层从CNN层收集学习到的特征，允许网络更快地学习，因为层实际上减少了学习空间的大小。然而，这种减少消除了特征之间的空间关系。因此，我们通常在DRL和游戏中避免使用池化层。

1.  如果放大，你可以查看图像是如何被每个颜色通道（红色、绿色和蓝色）分割，然后输入到网络中的。以下截图显示了这一点：

![图片](img/60a88ff3-b244-4d61-918f-a9ccda3f98c9.png)

检查图像分割和滤波器提取

1.  从图像分离的方式中，我们可以看到CNN的第一层，即滤波器，是如何提取特征的。通过这种方式，可以识别整个狗，但随着通过层的深入，特征会越来越小。

1.  最后，有一个蓝色的最终池化层，接着是一个绿色的层，这是一个单行层。这个单行层表示输入数据被展平，以便可以输入到典型的深度学习网络的后续层中。

当然，你也可以自由探索Playground中的许多其他模型。理解层如何提取特征对于理解CNN的工作原理非常重要。在下一节中，我们将看看如何升级我们的DQN代理，使其能够使用CNN玩Atari游戏。

# 在Atari上使用DQN

现在我们已经看到了CNN在滤波器方面的输出，了解这一工作原理的最好方式是查看构建它们的代码。在我们到达那里之前，让我们开始一个新的练习，使用新的DQN形式来解决Atari：

1.  打开本章的示例代码，该代码位于`Chapter_7_DQN_CNN.py`文件中。代码与`Chapter_6_lunar.py`非常相似，但有一些关键的不同之处。我们将只关注这个练习中的差异。如果你需要更好的代码解释，请回顾[第6章](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml)，*深入DQN*：

[PRE0]

1.  从顶部开始，唯一的改变是从一个名为`wrappers.py`的本地文件中导入一个新的模块。我们将通过创建环境来检查这个模块的作用：

[PRE1]

1.  由于几个原因，我们在这里以相当不同的方式创建环境。三个函数`make_atari`、`wrap_deepmind`和`wrap_pytorch`都位于我们之前导入的新`wrappers.py`文件中。这些包装器基于OpenAI为创建Gym环境包装器而制定的规范。我们稍后会更多地讨论包装器，但现在，这三个函数执行以下操作：

    +   `make_atari`: 这项操作准备环境，以便我们可以以CNN可编码的形式捕获视觉输入。我们这样设置是为了能够以设定的时间间隔对环境进行截图。

    +   `wrap_deepmind`: 这又是一个包装器，允许使用一些辅助工具。我们稍后会查看这个包装器。

    +   `wrap_pytorch`: 这是一个辅助库，它将我们加载到CNN网络中的视觉输入图像转换为PyTorch的特殊格式。不同的深度学习框架为CNN层有不同的输入风格，因此在所有DL框架标准化之前，你必须注意你的输入图像中通道的排列方式。在PyTorch中，图像通道需要放在第一位。对于其他框架，如Keras，则正好相反。

1.  之后，我们需要修改一些设置超参数的其他代码，如下所示：

[PRE2]

1.  突出的行显示了我们所做的更改。我们主要改变的是增加值——很多。Pong Atari环境是最简单的，但仍可能需要一百万次迭代才能解决。在某些系统上，这可能需要几天时间：

[PRE3]

1.  在前面的代码块中，我们可以看到我们正在构建一个新的类`CnnDQN`。我们很快就会接触到它。之后，代码基本上相同，除了有一个新的变量`replay_start`以及现在设置的回放缓冲区的大小。我们的缓冲区大小增加了100倍，从1,000条记录增加到100,000条记录。然而，我们希望在缓冲区完全填满之前就能训练智能体。毕竟，那是一个很大的数字。因此，我们使用`replay_start`来表示当缓冲区用于训练智能体时的训练起点：

[PRE4]

1.  接下来，我们将剧集计数更新到一个更高的数值。这是因为我们可以预期这个环境至少需要一百万个剧集来训练一个智能体：

[PRE5]

1.  除了训练循环的最后部分之外，所有其他代码都保持不变，这部分代码可以在前面的代码中看到。这段代码显示我们每200,000个回合绘制一次迭代。以前，我们每2,000个回合就做一次。当然，你可以增加这个值，或者完全删除它，如果长时间训练感到烦恼的话。

这个环境和我们将要查看的许多其他环境现在可能需要数小时或数天才能训练。实际上，DeepMind最近估计，一个普通的桌面系统大约需要45年才能训练其最顶尖的强化学习算法。如果你想知道的话，大多数其他环境需要4000万次迭代才能收敛。Pong是最简单的，只需要100万次迭代。

1.  按照常规方式运行示例。等待一段时间，也许可以继续阅读这本书的其余部分。这个样本需要数小时才能训练，所以我们在它运行的同时将继续探索代码的其他部分。不过，为了确认样本正在正确运行，只需确认环境正在渲染，如下面的图片所示：

![图片](img/715cbba5-c487-4daa-9775-2fff2a6632a4.png)

运行代码示例

保持样本运行。在下一节中，我们将探讨CNN层是如何构建到新模型中的。

# 添加CNN层

现在我们已经了解了CNN层背后的基本原理，是时候深入探讨它们是如何工作的了。打开代码示例，可以在`Chapter_7_DQN_CNN.py`文件中找到，并按照以下步骤进行：

1.  到目前为止，我们只需要关注一个名为`CnnDQN`的新类的代码，如下所示：

[PRE6]

1.  上述类替换了我们的先前vanilla DQN版本。两者之间存在一些关键差异，所以让我们从网络设置和构建第一个卷积层开始，如下所示：

[PRE7]

1.  首先要注意的是，我们正在构建一个新的模型并将其放入`self.features`。`features`将是我们的模型，用于执行卷积和分离特征。第一层是通过传递`input_shape`、过滤器数量（32）、`kernel_size`（8）和`stride`（4）来构建的。所有这些输入的详细信息都可以在这里找到：

    +   `input_shape[0]`：输入形状指的是观察空间。通过我们之前查看的包装器，我们将输入空间转换为（1，84，84）。记住，我们需要首先对通道进行排序。1个通道意味着我们可以看到我们的图像是灰度的（没有RGB）。1个通道也是我们输入到`Conv2d`的第一个值。

    +   **数量**（`32`）：下一个输入表示我们想要在这个层中构建的过滤器补丁的数量。每个过滤器都应用于图像，并由窗口大小（核大小）和移动（步长）确定。我们在使用TensorSpace Playground查看CNN模型细节时观察了这些补丁的结果。

    +   `kernel_size` (`8`): 这代表窗口大小。在这种情况下，由于我们使用的是2D卷积，Conv2d，这个大小实际上代表了一个8x8的值。将窗口或内核在图像上移动并应用学习到的滤波器是卷积操作。

    +   `stride` (`4`): 步长表示窗口或内核在操作之间移动的距离。步长为4意味着窗口移动了4个像素或单位，这实际上是一半的窗口大小8。

1.  卷积如何工作的一个例子可以在以下图像中看到。上方区域是一个单独的输出块。内核中的每个元素，即以下图像中的3x3块，是正在学习的部分：

![图片](img/ad2c12b1-37c5-42e7-ae9e-86b1633ea357.png)

步长卷积过程的解释

1.  在图像上应用内核的过程是通过简单地乘以块中的值与图像中的每个值来完成的。所有这些值相加，然后输出为输出滤波器操作的结果中的单个元素：

[PRE8]

1.  使用构建卷积层模型的代码，我们构建另一个线性模型，就像我们在之前的例子中所构建的那样。这个模型将卷积层的输出展平，并使用这个展平的模型来预测动作。在这种情况下，我们最终有两个网络模型，但请注意，我们将从第一个模型传递输出到第二个模型，以及从第一个模型反向传播错误到第二个模型。`feature_size`函数只是一个辅助函数，以便我们可以计算CNN模型到`Linear`模型的输入：

[PRE9]

1.  在`forward`函数内部，我们可以看到我们模型的预测已经改变。现在，我们将通过将其传递到`self.features`或我们模型的CNN部分来分解预测。然后，我们需要展平数据，并通过`self.fc`将其馈入线性部分。

1.  `action`函数与我们的先前DQN实现相同。

如果智能体仍在运行，看看你是否可以等待它完成。这可能需要一段时间，但看到最终结果可能会很有奖励和趣味。像RL中的几乎所有事情一样，DQN模型已经经历了各种改进，我们将在下一节中探讨那些改进。

# 介绍DDQN

**DDQN**代表**对战的DQN**，与双DQN不同，尽管人们经常混淆它们。这两种变体都假设某种形式的对偶性，但在第一种情况下，模型被假设在基础部分被分割，而在第二种情况，双DQN中，模型被假设分割成两个完全不同的DQN模型。

以下图表展示了DDQN和DQN之间的区别，不要与对战的DQN混淆：

![图片](img/ab231724-a4a6-41ea-ac48-ed0003d71459.png)

DQN和DDQN之间的区别

在前面的图表中，两个模型都在使用CNN层，但在即将到来的练习中，我们将只使用线性全连接层，只是为了简化问题。

注意DDQN网络如何分为两部分，然后又回到一个答案。这就是DDQN模型中我们将很快讨论的对抗部分。在此之前，让我们先探索双重DQN模型。

# 双重DQN或固定Q目标

为了理解为什么我们可能会组合使用两个网络，或者说是“对抗”，我们首先需要明白为什么我们需要这样做。让我们回顾一下我们是如何计算TD损失并使用它作为我们估计动作的方法的。如您所回忆的那样，我们是基于目标估计来计算损失的。然而，在我们的DQN模型中，那个目标现在是持续变化的。我们可以用的类比是，我们的智能体有时可能会追逐自己的尾巴，试图找到目标。那些非常细心的你们可能在前面的训练中已经看到了这一点，通过看到波动的奖励。我们在这里可以做的就是创建另一个目标网络，我们将朝着它前进并在过程中更新它。这听起来比实际上要复杂得多，所以让我们看看一个例子：

1.  打开`Chapter_7_DoubleDQN.py`文件中的代码示例。这个例子是从我们之前看过的`Chapter_6_DQN_lunar.py`文件构建的。这里有一些细微的变化，所以我们将详细审查每一个，从模型构建开始：

[PRE10]

1.  如其名称所示，我们现在构建了两个DQN模型：一个用于在线使用，一个作为目标。我们训练`current_model`的值，然后每*x*次迭代后使用以下代码切换回目标模型：

[PRE11]

1.  `update_target`函数通过使用`current_model`更新`target_model`，确保目标Q值总是足够地提前或落后，因为我们使用跳过跟踪并回顾过去。

1.  随后是`compute_td_loss`函数，需要按照以下方式更新：

[PRE12]

1.  函数中突出显示的行显示了被更改的行。注意新的模型`current_model`和`target_model`是如何现在用来预测损失，而不仅仅是单个模型本身。最后，在训练或试错循环中，我们可以看到一些最终的变化：

[PRE13]

1.  第一个变化是我们现在从`current_model`模型中获取动作：

[PRE14]

1.  第二个变化是使用`update_target`更新`target_model`，使用`current_model`的权重：

[PRE15]

1.  我们还需要更新`play_game`函数，以便我们可以从`current_model`中获取动作。如果你将其更改为目标模型，可能会很有趣地看到会发生什么。

1.  在这一点上，像平常一样运行代码，并观察结果。

现在我们已经理解了为什么我们可能想要使用不同的模型，我们将继续学习如何使用对抗DQN或DDQN来解决相同的环境。

# 对抗DQN或真正的DDQN

对抗DQN或DDQN扩展了固定目标或固定Q目标的概念，并将其扩展到包括一个称为优势的新概念。优势是一个概念，其中我们确定通过采取其他动作可能获得的额外价值或优势。理想情况下，我们希望计算优势，使其包括所有其他动作。我们可以通过计算图来实现这一点，通过将层分离为状态值的计算和从状态和动作的所有排列中计算优势的另一个计算。

这种结构可以在以下图中看到：

![图片](img/e94bf46b-2e18-484a-95d1-ac81d8b8330c.png)

DDQN的详细可视化

上述图再次显示了CNN层，但我们的示例将仅从线性展平模型开始。我们可以看到模型在展平后分为两部分。第一部分计算状态值或价值，第二部分计算优势或动作值。然后，这些值被聚合以输出Q值。这种设置之所以有效，是因为我们可以使用优化（也称为反向传播）将损失推回整个网络。因此，网络学习如何计算每个动作的优势。让我们看看如何在新的代码示例中实现这一点。在`Chapter_7_DDQN.py`文件中打开它，并按照以下步骤操作：

1.  这个例子使用之前的例子作为源，但在许多重要细节上有所不同：

[PRE16]

1.  除了`act`函数外，DDQN类几乎完全是新构建的。在`init`函数中，我们可以看到三个子模型的构建：`self.feature`、`self.value`和`self.advantage`。然后，在`forward`函数中，我们可以看到输入**x**是如何被第一个**feature**子模型转换的，然后输入到优势和价值子模型中。然后，输出`advantage`和`value`被用来计算预测值，如下所示：

[PRE17]

1.  我们可以看到预测值是表示为价值的州价值。这被添加到优势或组合状态-动作值中，并从平均值或平均中减去。结果是最佳优势的预测或智能体可能学习到的优势：

[PRE18]

1.  下一个变化是我们现在在之前的double DQN示例中构建两个DDQN模型实例，而不是一个DQN。这意味着我们也继续使用两个模型来评估我们的目标。毕竟，我们不想退步。

1.  下一个主要变化发生在`compute_td_loss`函数中。更新的行如下：

[PRE19]

1.  这实际上简化了前面的代码。现在，我们可以清楚地看到我们的下一个_q_values是从`target_model`中获取的。

1.  按照您通常的方式运行代码示例，并观察智能体玩Lander。确保您让智能体训练直到它达到一定数量的正奖励。这可能需要您增加训练迭代次数或剧集数。

作为提醒，我们使用术语“episode”来表示一次训练观察或迭代一个时间步。许多例子将使用“frame”和“frames”来表示相同的东西。虽然在某些情况下“frame”可能是合适的，但在其他情况下则不太合适，尤其是在我们开始堆叠帧或输入观察时。如果你觉得这个名字令人困惑，一个替代方案可能是使用“训练迭代”。

你会发现这个算法确实收敛得更快，但正如你所预期的，我们还可以对这个算法进行改进。我们将在下一节中探讨如何改进。

# 使用优先级经验回放扩展重放

到目前为止，我们已经看到了如何使用重放缓冲区或经验回放机制，使我们能够在稍后的时间以批量的方式拉回值，以便训练网络图。这些数据批次由随机样本组成，效果不错，但当然，我们可以做得更好。因此，我们不仅存储所有数据，还可以做出两个决定：存储哪些数据和哪些数据是优先使用的。为了简化问题，我们只需关注从经验回放中提取数据的优先级。通过优先提取数据，我们希望这能显著提高我们提供给网络用于学习的信息，从而提高整个智能体的性能。

不幸的是，优先级回放背后的想法虽然简单易懂，但在实践中推导和估计则困难得多。我们可以做的是，通过TD误差或预测和实际期望目标的损失来优先级返回事件。因此，我们优先考虑智能体预测中误差最大或智能体错误最多的值。另一种思考方式是，我们优先考虑最令智能体惊讶的事件。回放缓冲区结构化得如此之好，以至于它通过惊讶程度优先级来优先考虑这些事件，然后返回这些事件的样本，但它并不一定按惊讶程度对事件进行排序。在这里，最好是从按惊讶程度排序的桶或分布中随机采样事件。这意味着智能体更有可能选择来自更平均的惊讶事件的样本。

在本节中，我们将使用优先级经验回放机制，该机制首次在以下论文中介绍：[https://arxiv.org/pdf/1511.05952.pdf](https://arxiv.org/pdf/1511.05952.pdf)。然后它被编码在PyTorch中，来自这个仓库：[https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb](https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb)。我们的实现已被修改，以便在笔记本外运行，并适用于Python 3.6 ([https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb](https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb))).

我们将使用一个全新的样本。打开 `Chapter_7_DDQN_wprority.py` 并按照以下步骤操作：

1.  这个样本的第一个重大变化是将 `ReplayBuffer` 类升级到 `NaivePrioritizedBuffer`，如下所示：

[PRE20]

1.  这段代码天真地根据观察到的错误预测分配优先级。然后，它根据优先级顺序对这些值进行排序。接着，它随机抽取这些事件。再次，由于抽样是随机的，但样本是按优先级对齐的，因此随机抽样通常会抽取平均误差的样本。

1.  发生的事情是通过重新排序样本，我们重新排序到预期的实际数据分布。因此，为了解决这个问题，我们引入了一个新的因子，称为 **beta**，或 **重要性抽样**。**Beta** 允许我们控制事件的分布，并基本上将它们重置到原始位置：

[PRE21]

1.  现在，我们将定义一个函数，使用前面的代码返回随剧集增加的 beta。然后，代码像我们绘制 epsilon 一样绘制 beta，如下所示：

![](img/ae38a330-3d0d-49bf-aeab-886984d7adda.png)

beta 和 epsilon 绘图示例

1.  在修改了重放缓冲区中的样本函数之后，我们还需要更新 `compute_td_loss` 函数，如下所示：

[PRE22]

1.  只有前面突出显示的行与我们已经看到的有所不同。第一个区别是返回了两个新的值：`indices` 和 `weights`。然后，我们可以看到 `replay_buffer` 根据之前返回的 `indices` 调用 `update_priorities`：

[PRE23]

1.  接下来，在训练循环内部，我们更新了 `play_game` 的调用，并引入了一个新的 `min_play_reward` 阈值。这允许我们在渲染游戏之前设置一些最低奖励阈值。渲染游戏可能相当耗时，这将也会加快训练速度：

[PRE24]

1.  继续在训练循环内部，我们可以看到我们如何提取 `beta` 并在 `td_compute_loss` 函数中使用它。

1.  再次运行样本。这次，你可能需要等待看到智能体驾驶 Lander，但一旦它这样做，它将表现得相当不错，如下所示：

![](img/da8e4389-45ac-40c0-8f9d-04de5d01e475.png)

智能体成功着陆 Lander

通常，在合理短的时间内，智能体将能够持续地将 Lander 安全着陆。算法应该在 75,000 次迭代内收敛到着陆。当然，你可以继续调整和玩转超参数，但这正是我们下一节要讨论的内容。

# 练习

随着我们在这本书中的进步，这些练习的价值和成本将越来越大。这里的“昂贵”是指你需要投入每个练习的时间将增加。这可能意味着你倾向于做更少的练习，但请继续尝试自己完成两到三个练习：

1.  回到 TensorSpace 游戏场，看看你是否能理解池化在这些模型中产生的差异。记住，我们避免使用池化是为了避免丢失空间完整性。

1.  打开`Chapter_7_DQN_CNN.py`并修改一些卷积层的输入，例如内核或步长大小。看看这会对训练产生什么影响。

1.  调整`Chapter_7_DoubleDQN.py`的超参数或创建新的超参数。

1.  调整`Chapter_7_DDQN.py`的超参数或创建新的超参数。

1.  调整`Chapter_7_DoubleDQN_wprority.py`的超参数或创建新的超参数。

1.  将`Chapter_7_DoubleDQN.py`转换为使用卷积层，然后升级示例，使其能够与Pong等Atari环境一起工作。

1.  将`Chapter_7_DDQN.py`转换为使用卷积层，然后升级示例，使其能够与Pong等Atari环境一起工作。

1.  将`Chapter_7_DDQN_wprority.py`转换为使用卷积层，然后升级示例，使其能够与Pong等Atari环境一起工作。

1.  在一个示例中的卷积层之间添加一个池化层。你可能需要查阅PyTorch文档来学习如何做到这一点。

1.  你还能如何改进前面示例中的经验回放缓冲区？你还能使用其他形式的回放缓冲区吗？

像往常一样，享受通过示例工作的乐趣。毕竟，如果你不高兴看到你的代码在玩月球探险者或Atari游戏，你什么时候才会开心呢？

在下一节中，我们将总结本章内容，并看看我们将学习什么。

# 摘要

从我们之前中断的地方继续使用DQN，我们探讨了如何通过添加CNN来扩展这个模型，并添加额外的网络来创建双DQN和对抗DQN，或DDQN。在探索CNN之前，我们简要介绍了视觉观察编码及其必要性。然后，我们简要介绍了CNN，并使用TensorSpace Playground探索了一些知名的最先进模型。接下来，我们将CNN添加到DQN模型中，并使用它来玩Atari游戏环境Pong。之后，我们更详细地研究了如何通过添加另一个网络作为目标，并添加另一个网络来对抗或反驳其他网络来扩展DQN，这也被称为对抗DQN或DDQN。这引入了选择动作时的优势概念。最后，我们探讨了如何扩展经验回放缓冲区，以便我们可以优先处理那里捕获的事件。使用这个框架，我们能够仅通过少量代理训练就轻松地将Lander着陆。

在下一章中，我们将探讨新的策略选择方法，而不再关注全局平均值。相反，我们将使用策略梯度方法来采样分布。
