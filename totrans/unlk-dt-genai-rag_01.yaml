- en: <st c="0">1</st>
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1<st c="0"></st>
- en: <st c="2">What Is Retrieval-Augmented Generation (RAG)</st>
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是检索增强生成（RAG）<st c="2"></st>
- en: <st c="46">The field of</st> **<st c="60">artificial intelligence</st>** <st
    c="83">(</st>**<st c="85">AI</st>**<st c="87">) is</st> <st c="93">rapidly evolving.</st>
    <st c="111">At the center of it all is</st> **<st c="138">generative AI</st>**<st
    c="151">. At the</st> <st c="160">center of generative AI is</st> **<st c="187">retrieval-augmented
    generation</st>** <st c="217">(</st>**<st c="219">RAG</st>**<st c="222">).</st>
    <st c="226">RAG</st> <st c="230">is emerging as a significant addition to the
    generative AI toolkit, harnessing the intelligence and text generation capabilities
    of</st> **<st c="362">large language models</st>** <st c="383">(</st>**<st c="385">LLMs</st>**<st
    c="389">) and</st> <st c="395">integrating them with a company’s internal data.</st>
    <st c="445">This offers a method to enhance organizational operations significantly.</st>
    <st c="518">This book focuses on numerous aspects of RAG, examining its role in
    augmenting the capabilities of LLMs and leveraging internal corporate data for</st>
    <st c="665">strategic advantage.</st>
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工智能**（**AI**）领域正在迅速发展。<st c="46"></st>**<st c="60">人工智能</st>**（**<st c="85">AI</st>**<st
    c="87">**</st>）的核心是**<st c="138">生成式AI</st>**<st c="151">。生成式AI的核心是**<st c="187">检索增强生成</st>**（**<st
    c="219">RAG</st>**<st c="222">**</st>）。RAG正在成为生成式AI工具箱中的一个重要补充，利用大型语言模型（**<st
    c="362">LLMs</st>**<st c="383">**</st>）的智能和文本生成能力，并将它们与公司的内部数据相结合。<st c="226">RAG</st>
    <st c="230">正在成为增强组织运营效率的重要方法。</st> <st c="445">这本书重点介绍了RAG的多个方面，探讨了其在增强LLMs能力以及利用内部企业数据获得战略优势中的作用。</st>
    <st c="518"></st>'
- en: <st c="685">As this book progresses, we will outline the potential of RAG in
    the enterprise, suggesting how it can make AI applications more responsive and
    smarter, aligning them with your organizational objectives.</st> <st c="890">RAG
    is well-positioned to become a key facilitator of customized, efficient, and insightful
    AI solutions, bridging the gap between generative AI’s potential and your specific
    business needs.</st> <st c="1081">Our exploration of RAG will encourage you to
    unlock the full potential of your corporate data, paving the way for you to enter
    the era of</st> <st c="1219">AI-driven innovation.</st>
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 随着本书的深入，我们将概述RAG在企业中的潜力，建议如何使其使AI应用更加响应和智能，与您的组织目标保持一致。<st c="685"></st>RAG非常适合成为定制化、高效和有洞察力的AI解决方案的关键促进者，弥合生成式AI的潜力与您的具体业务需求之间的差距。<st
    c="890"></st>我们对RAG的探索将鼓励您挖掘企业数据的全部潜力，为您进入**<st c="1219">AI驱动创新时代</st>**铺平道路。<st
    c="1081"></st>
- en: <st c="1240">In this chapter, we will cover the</st> <st c="1276">following
    topics:</st>
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：<st c="1240"></st><st c="1276"></st>
- en: <st c="1293">The basics of RAG and how it combines LLMs with a company’s</st>
    <st c="1354">private data</st>
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG的基本原理以及它如何将大型语言模型（LLMs）与公司的私有数据相结合<st c="1293"></st><st c="1354"></st>
- en: <st c="1366">The key advantages of RAG, such as improved accuracy, customization,</st>
    <st c="1436">and flexibility</st>
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG的关键优势，如提高准确性、定制化和灵活性<st c="1366"></st>
- en: <st c="1451">The challenges and limitations of RAG, including data quality and</st>
    <st c="1518">computational complexity</st>
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG的挑战和限制，包括数据质量和计算复杂性<st c="1451"></st><st c="1518"></st>
- en: <st c="1542">Important RAG vocabulary terms, with an emphasis on vectors</st>
    <st c="1603">and embeddings</st>
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重要的RAG词汇术语，重点介绍向量和嵌入<st c="1542"></st>
- en: <st c="1617">Real-world examples of RAG applications across</st> <st c="1665">various
    industries</st>
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG在各个行业中的应用实例<st c="1617"></st><st c="1665"></st>
- en: <st c="1683">How RAG differs from conventional generative AI and</st> <st c="1736">model
    fine-tuning</st>
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG与传统的生成式AI和模型微调有何不同<st c="1683"></st><st c="1736"></st>
- en: <st c="1753">The overall architecture and stages of a RAG system from user and</st>
    <st c="1820">technical perspectives</st>
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从用户和技术角度出发，RAG系统的整体架构和阶段<st c="1753"></st><st c="1820"></st>
- en: <st c="1842">By the end of this chapter, you will have a solid foundation in
    the core RAG concepts and understand the immense potential it offers organizations
    so that they can extract more value from their data and empower their LLMs.</st>
    <st c="2066">Let’s</st> <st c="2072">get started!</st>
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将对RAG的核心概念有坚实的基础，并了解它为组织提供的巨大潜力，以便他们能够从数据中提取更多价值并赋予他们的LLMs更多能力。<st c="1842"></st>让我们开始吧！<st
    c="2066"></st>
- en: <st c="2084">Understanding RAG – Basics and principles</st>
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="2084">理解RAG - 基本原理和原则</st>
- en: '<st c="2126">Modern-day LLMs are</st> <st c="2147">impressive, but they have
    never seen your company’s private data (hopefully!).</st> <st c="2226">This means
    the ability of an LLM to help your</st> <st c="2272">company fully utilize its
    data is very limited.</st> <st c="2320">This very large barrier has given rise
    to the concept of RAG, where you are using the power and capabilities of the LLM
    but combining it with the knowledge and data contained within your company’s internal
    data repositories.</st> <st c="2545">This is the primary motivation for using
    RAG: to make new data available to the LLM and significantly increase the value
    you can extract from</st> <st c="2687">that data.</st>'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="2126">现代的大型语言模型（LLM）令人印象深刻，但它们从未见过贵公司的私有数据（希望如此！）。</st> <st c="2147">这意味着LLM帮助贵公司充分利用其数据的能力非常有限。</st>
    <st c="2226">这一个非常大的障碍催生了RAG的概念，即您正在使用LLM的强大功能和能力，但将其与公司内部数据存储库中包含的知识和数据相结合。</st>
    <st c="2320">这是使用RAG的主要动机：使新数据对LLM可用，并显著增加您可以从这些数据中提取的价值。</st> <st c="2545">这使您可以从这些数据中提取的价值。</st>
- en: <st c="2697">Beyond internal data, RAG is</st> <st c="2726">also useful in cases
    where the LLM has not been trained on the data, even if it is public, such as
    the most recent research papers or articles about a topic that is strategic to
    your company.</st> <st c="2919">In both cases, we are talking about data that
    was not present during the training of the LLM.</st> <st c="3013">You can have
    the latest LLM trained on the most tokens ever, but if that data was not present
    for training, then the LLM will be at a disadvantage in helping you reach your</st>
    <st c="3186">full productivity.</st>
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="2697">除了内部数据之外，RAG在LLM未在数据上训练的情况下也很有用，即使这些数据是公开的，例如关于对公司战略至关重要的主题的最新研究论文或文章。</st>
    <st c="2726">在这两种情况下，我们谈论的是在LLM训练期间不存在的数据。</st> <st c="2919">您可以让最新的LLM在最多的标记上训练，但如果这些数据在训练期间不存在，那么LLM在帮助您达到完全生产力方面将处于不利地位。</st>
    <st c="3013">您可以让最新的LLM在最多的标记上训练，但如果这些数据在训练期间不存在，那么LLM在帮助您达到完全生产力方面将处于不利地位。</st>
    <st c="3186">您达到完全生产力。</st>
- en: <st c="3204">Ultimately, this highlights the fact that, for most organizations,
    it is a central need to connect new data to an LLM.</st> <st c="3324">RAG is the
    most popular paradigm for doing this.</st> <st c="3373">This book focuses on showing
    you how to set up a RAG application with your data, as well as how to get the
    most out of it in various situations.</st> <st c="3518">We intend to give you
    an in-depth understanding of RAG and its importance in leveraging an LLM within
    the context of a company’s private or specific</st> <st c="3667">data needs.</st>
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="3204">最终，这突出了这样一个事实，对于大多数组织来说，将新数据连接到LLM是一个基本需求。</st> <st c="3324">RAG是实现这一目标的最受欢迎的方法。</st>
    <st c="3373">本书的重点是向您展示如何使用您的数据设置RAG应用程序，以及如何在各种情况下最大限度地发挥其作用。</st> <st c="3518">我们旨在让您深入了解RAG及其在满足公司私有或特定数据需求时利用LLM的重要性。</st>
- en: <st c="3678">Now that you understand the basic motivations behind implementing
    RAG, let’s review some of the advantages of</st> <st c="3789">using it.</st>
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="3678">现在您已经了解了实施RAG背后的基本动机，让我们回顾一下使用RAG的一些优势。</st> <st c="3789">RAG的优势</st>
- en: <st c="3798">Advantages of RAG</st>
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="3798">RAG的优势</st>
- en: <st c="3816">Some of the potential</st> <st c="3839">advantages of using RAG
    include improved accuracy and relevance, customization, flexibility, and expanding
    the model’s knowledge beyond the training data.</st> <st c="3994">Let’s take a</st>
    <st c="4007">closer look:</st>
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="3816">使用RAG的一些潜在优势包括提高准确性和相关性、定制、灵活性和扩展模型知识，使其超越训练数据。</st> <st c="3994">让我们更深入地了解一下：</st>
- en: '**<st c="4019">Improved accuracy and relevance</st>**<st c="4051">: RAG can
    significantly enhance the accuracy and relevance of responses that are generated
    by LLMs.</st> <st c="4152">RAG fetches and incorporates specific information from
    a database or dataset, typically in real time, and ensures that the output is
    based on both the model’s pre-existing knowledge and the most current and relevant
    data that you are</st> <st c="4386">providing directly.</st>'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="4019">提高准确性和相关性</st>**<st c="4051">：RAG可以显著提高LLM生成的响应的准确性和相关性。</st>
    <st c="4152">RAG从数据库或数据集中获取并整合特定信息，通常是在实时进行的，并确保输出基于模型预先存在的知识和您直接提供的最新和最相关数据。</st>
    <st c="4386">您直接提供的最新和最相关数据。</st>'
- en: '**<st c="4405">Customization</st>**<st c="4419">: RAG allows you to customize
    and adapt the model’s knowledge to your specific domain or use case.</st> <st
    c="4519">By pointing RAG to databases or datasets directly relevant to your application,
    you can tailor the model’s outputs so that they align closely with the information
    and style that matters most for your specific needs.</st> <st c="4735">This customization
    enables the model to provide more targeted and</st> <st c="4801">useful responses.</st>'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="4405">定制化</st>**<st c="4419">：RAG允许您根据特定的领域或用例定制和调整模型的知识。</st> <st
    c="4519">通过将RAG指向与您的应用程序直接相关的数据库或数据集，您可以定制模型的输出，使其与您特定需求最相关的信息和风格紧密一致。</st> <st
    c="4735">这种定制化使得模型能够提供更精准和有用的响应。</st>'
- en: '**<st c="4818">Flexibility</st>**<st c="4830">: RAG provides flexibility in
    terms of the data sources that the model can access.</st> <st c="4914">You can
    apply RAG to various structured and unstructured data, including databases, web
    pages, documents, and more.</st> <st c="5030">This flexibility allows you to leverage
    diverse information sources and combine them in novel ways to enhance the model’s
    capabilities.</st> <st c="5166">Additionally, you can update or swap out the data
    sources as needed, enabling the model to adapt to changing</st> <st c="5275">information
    landscapes.</st>'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="4818">灵活性</st>**<st c="4830">：RAG在模型可以访问的数据源方面提供了灵活性。</st> <st c="4914">您可以将RAG应用于各种结构化和非结构化数据，包括数据库、网页、文档等。</st>
    <st c="5030">这种灵活性使您能够利用多样化的信息来源，并以新颖的方式将它们结合起来，以增强模型的能力。</st> <st c="5166">此外，您可以根据需要更新或更换数据源，使模型能够适应不断变化的信息环境。</st>'
- en: '**<st c="5298">Expanding model knowledge beyond training data</st>**<st c="5345">:
    LLMs are limited by the scope of their training data.</st> <st c="5402">RAG overcomes
    this limitation by enabling models to access and utilize information that was
    not included in their initial training sets.</st> <st c="5539">This effectively
    expands the knowledge base of the model without the need for retraining, making
    LLMs more versatile and adaptable to new domains or rapidly</st> <st c="5696">evolving
    topics.</st>'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="5298">扩展模型知识超越训练数据</st>**<st c="5345">：LLM受限于其训练数据的范围。</st> <st c="5402">RAG通过使模型能够访问和利用其初始训练集未包含的信息来克服这一限制。</st>
    <st c="5539">这实际上扩展了模型的知识库，而无需重新训练，使LLM更加灵活，并能适应新的领域或快速发展的主题。</st>'
- en: '**<st c="5712">Removing hallucinations</st>**<st c="5736">: The LLM is a key
    component within the RAG system.</st> <st c="5789">LLMs have the potential to
    provide wrong information, also known as hallucinations.</st> <st c="5873">These</st>
    <st c="5878">hallucinations can manifest in several ways, such as made-up facts,
    incorrect facts, or even nonsensical verbiage.</st> <st c="5994">Often, the hallucination
    is worded in a way that can be very convincing, causing it to be difficult to
    identify.</st> <st c="6107">A well-designed RAG application can remove hallucinations
    much more easily than when directly using</st> <st c="6207">an LLM.</st>'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="5712">消除幻觉</st>**<st c="5736">：LLM（大型语言模型）是RAG系统中的关键组件。</st> <st c="5789">LLM有可能提供错误信息，也称为幻觉。</st>
    <st c="5873">这些幻觉可以以多种方式表现出来，如虚构的事实、错误的事实，甚至无意义的措辞。</st> <st c="5994">通常，幻觉的措辞可能非常令人信服，导致难以识别。</st>
    <st c="6107">一个设计良好的RAG应用程序可以比直接使用LLM更容易地消除幻觉。</st>'
- en: <st c="6214">With that, we’ve covered the key advantages of implementing RAG
    in your organization.</st> <st c="6301">Next, let’s discuss some of the challenges
    you</st> <st c="6348">might face.</st>
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="6214">至此，我们已经介绍了在您的组织中实施RAG的关键优势。</st> <st c="6301">接下来，让我们讨论一些您可能面临的挑战。</st>
- en: <st c="6359">Challenges of RAG</st>
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="6359">RAG的挑战</st>
- en: <st c="6377">There are</st> <st c="6388">some challenges to using RAG as well,
    which include dependency on the quality of the internal data, the need for data
    manipulation and cleaning, computational overhead, more complex integrations,
    and the potential for information overload.</st> <st c="6628">Let’s review these
    challenges and gain a better understanding of how they impact RAG pipelines and
    what can be done</st> <st c="6744">about them:</st>
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="6377">使用RAG（检索增强生成）也存在一些挑战，包括对内部数据质量的依赖、数据操作和清洗的需求、计算开销、更复杂的集成以及信息过载的潜在风险。</st>
    <st c="6628">让我们回顾这些挑战，并更好地理解它们如何影响RAG管道以及可以采取哪些措施来应对：</st>
- en: '**<st c="6755">Dependency on data quality</st>**<st c="6782">: When talking
    about how data can impact an AI model, the saying in data science circles is</st>
    *<st c="6875">garbage in, garbage out.</st>* <st c="6899">This means that if you
    give a model bad data, it will give you bad results.</st> <st c="6976">RAG is
    no different.</st> <st c="6997">The effectiveness of RAG is directly tied to the
    quality of the data it retrieves.</st> <st c="7080">If the underlying database
    or dataset contains outdated, biased, or inaccurate information, the outputs generated
    by RAG will likely suffer from the</st> <st c="7229">same issues.</st>'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="6755">对数据质量的依赖</st>**<st c="6782">：当谈论数据如何影响AI模型时，数据科学领域的说法是 *<st
    c="6875">垃圾进，垃圾出。</st>* <st c="6899">这意味着如果你给模型提供糟糕的数据，它将给出糟糕的结果。</st> <st c="6976">RAG也不例外。</st>
    <st c="6997">RAG的有效性直接与其检索到的数据质量相关。</st> <st c="7080">如果底层数据库或数据集包含过时、有偏见或不准确的信息，RAG生成的输出可能会出现同样的问题。</st>
    <st c="7229">相同的问题。</st>'
- en: '**<st c="7241">Need for data manipulation and cleaning</st>**<st c="7281">:
    Data in the recesses of the company often has a lot of value to it, but it is
    not often in good, accessible shape.</st> <st c="7399">For example, data from
    PDF-based customer statements needs a lot of massaging so that it can be put into
    a format that can be useful to a</st> <st c="7537">RAG pipeline.</st>'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="7241">数据操作和清洗的必要性</st>**<st c="7281">：公司深处的数据往往具有很高的价值，但它通常并不处于良好、易于访问的状态。</st>
    <st c="7399">例如，基于PDF的客户声明数据需要大量的处理才能被放入一个对RAG管道有用的格式。</st> <st c="7537">RAG管道。</st>'
- en: '**<st c="7550">Computational overhead</st>**<st c="7573">: A RAG pipeline introduces
    a host of new computational steps into the response generation process, including
    data retrieval, processing, and integration.</st> <st c="7729">LLMs are getting
    faster every day, but even the fastest</st> <st c="7785">response can be more
    than a second, and some can take several seconds.</st> <st c="7856">If you combine
    that with other data processing steps, and possibly multiple LLM calls, the result
    can be a very significant increase in the time it takes to receive a response.</st>
    <st c="8033">This all leads to increased computational overhead, affecting the
    efficiency and scalability of the entire system.</st> <st c="8148">As with any
    other IT initiative, an organization must balance the benefits of enhanced accuracy
    and customization against the resource requirements and potential latency introduced
    by these</st> <st c="8338">additional processes.</st>'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="7550">计算开销</st>**<st c="7573">：RAG管道将一系列新的计算步骤引入到响应生成过程中，包括数据检索、处理和集成。</st>
    <st c="7729">LLMs（大型语言模型）每天都在变快，但即使是最快的响应也可能超过一秒，有些可能需要几秒钟。</st> <st c="7785">响应。</st>
    <st c="7856">如果你将这一点与其他数据处理步骤结合起来，以及可能的多个LLM调用，结果可能是接收响应所需时间的显著增加。</st> <st c="8033">这所有的一切都导致了计算开销的增加，影响了整个系统的效率和可扩展性。</st>
    <st c="8148">与其他任何IT项目一样，组织必须平衡这些额外流程带来的增强准确性和定制化的好处与资源需求和潜在延迟。</st> <st c="8338">这些额外流程。</st>'
- en: '**<st c="8359">Data storage explosion; complexity in integration and maintenance</st>**<st
    c="8425">: Traditionally, your data resides in a data source that’s queried in
    various ways to be made available to your internal and external systems.</st>
    <st c="8569">But with RAG, your data resides in multiple forms and locations,
    such as vectors in a vector database, that represent the same data, but in a different
    format.</st> <st c="8729">Add in the complexity of connecting these various data
    sources to LLMs and relevant technical mechanisms such as vector searches and
    you have a significant increase in complexity.</st> <st c="8909">This increased
    complexity can be resource-intensive.</st> <st c="8962">Maintaining this integration
    over time, especially as data sources evolve or expand, adds even more complexity
    and cost.</st> <st c="9083">Organizations need to invest in technical expertise
    and infrastructure to leverage RAG capabilities effectively while accounting for
    the rapid increase in complexities these systems bring</st> <st c="9271">with
    them.</st>'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="8359">数据存储爆炸；集成和维护的复杂性</st>**<st c="8425">：传统上，您的数据存储在数据源中，通过各种方式查询以供您的内部和外部系统使用。</st>
    <st c="8569">但使用RAG，您的数据以多种形式和位置存在，例如向量数据库中的向量，代表相同的数据，但格式不同。</st> <st c="8729">再加上将这些各种数据源连接到LLM和相关技术机制（如向量搜索）的复杂性，您将面临显著的增加复杂性。</st>
    <st c="8909">这种增加的复杂性可能会非常消耗资源。</st> <st c="8962">随着时间的推移维护这种集成，尤其是在数据源演变或扩展时，还会增加更多的复杂性和成本。</st>
    <st c="9083">组织需要投资于技术专长和基础设施，以有效地利用RAG功能，同时考虑到这些系统带来的复杂性的快速增加</st> <st c="9271">及其带来的挑战。</st>'
- en: '**<st c="9281">Potential for information overload</st>**<st c="9316">: RAG-based
    systems can pull in too much information.</st> <st c="9371">It is just as important
    to implement mechanisms to address this issue as it is to handle times when not
    enough relevant information is found.</st> <st c="9513">Determining the relevance
    and importance of retrieved information to be included in the final output requires
    sophisticated filtering and ranking mechanisms.</st> <st c="9671">Without these,
    the quality of the generated content could be compromised by an excess of unnecessary
    or marginally</st> <st c="9786">relevant details.</st>'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="9281">信息过载的潜在风险</st>**<st c="9316">：基于RAG的系统可能会引入过多的信息。</st> <st c="9371">实施机制来解决这个问题与处理找不到足够相关信息的情况一样重要。</st>
    <st c="9513">确定要包含在最终输出中的检索信息的关联性和重要性需要复杂的过滤和排序机制。</st> <st c="9671">没有这些机制，生成内容的质量可能会因过多的不必要或边际相关的细节而受损。</st>'
- en: '**<st c="9803">Hallucinations</st>**<st c="9818">: While we listed removing
    hallucinations as an advantage of using RAG, hallucinations do pose one of the
    biggest challenges to RAG pipelines if they’re not dealt with properly.</st> <st
    c="9997">A well-designed RAG application must take measures to</st> <st c="10050">identify
    and remove hallucinations and undergo significant testing before the final output
    text is provided to the</st> <st c="10166">end user.</st>'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="9803">幻觉</st>**<st c="9818">：虽然我们将消除幻觉列为使用RAG的优势之一，但如果处理不当，幻觉确实是对RAG管道的最大挑战之一。</st>
    <st c="9997">一个设计良好的RAG应用必须采取措施来</st> <st c="10050">识别和消除幻觉，并在向最终用户提供最终输出文本之前进行重大测试。</st>'
- en: '**<st c="10175">High levels of complexity within RAG components</st>**<st c="10223">:
    A typical RAG application tends to have a high level of complexity, with many
    components that need to be optimized for the overall application to function properly.</st>
    <st c="10391">The components can interact with each other in several ways, often
    with many more steps than the basic RAG pipeline you start with.</st> <st c="10523">Every
    component within the pipeline needs significant amounts of trials and testing,
    including your prompt design and engineering, the LLMs you use and how you use
    them, the various algorithms and their parameters for retrieval, the interface
    you use to access your RAG application, and numerous other aspects that you will
    need to add over the course of</st> <st c="10878">your development.</st>'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="10175">RAG组件中的高度复杂性</st>**<st c="10223">：典型的RAG应用往往具有高度复杂性，需要许多组件进行优化，以确保整体应用正常工作。</st>
    <st c="10391">这些组件可以以多种方式相互交互，通常比您开始的基本RAG管道有更多的步骤。</st> <st c="10523">管道中的每个组件都需要大量的试验和测试，包括您的提示设计和工程、您使用的LLM以及您如何使用它们、检索的各种算法及其参数、您用于访问RAG应用的界面，以及您在开发过程中需要添加的众多其他方面。</st>'
- en: <st c="10895">In this section, we explored the key advantages of implementing
    RAG in your organization, including improved accuracy and relevance, customization,
    flexibility, and the ability to expand the model’s knowledge beyond its initial
    training data.</st> <st c="11139">We also discussed some of the challenges you
    might face when deploying RAG, such as dependency on data quality, the need for
    data manipulation and cleaning, increased computational overhead, complexity in
    integration and maintenance, and the potential for information overload.</st>
    <st c="11417">Understanding these benefits and challenges provides a foundation
    for diving deeper into the core concepts and vocabulary used in</st> <st c="11547">RAG
    systems.</st>
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="10895">在本节中，我们探讨了在组织中实施 RAG 的关键优势，包括提高准确性和相关性、定制化、灵活性和将模型的知识扩展到其初始训练数据之外的能力。</st>
    <st c="11139">我们还讨论了在部署 RAG 时可能会遇到的挑战，例如对数据质量的依赖、数据操作和清洗的需求、计算开销的增加、集成和维护的复杂性以及信息过载的可能性。</st>
    <st c="11417">理解这些优势和挑战为深入探讨 RAG 系统中使用的核心概念和词汇奠定了基础。</st>
- en: <st c="11559">To understand the approaches we will introduce, you will need
    a good understanding of the vocabulary used to discuss these approaches.</st>
    <st c="11695">In the following section, we will familiarize ourselves with some
    of the foundational concepts so that you can better understand the various components
    and techniques involved in building effective</st> <st c="11893">RAG pipelines.</st>
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="11559">为了理解我们将要介绍的方法，你需要对讨论这些方法所使用的词汇有很好的理解。</st> <st c="11695">在接下来的部分，我们将熟悉一些基础概念，以便你更好地理解构建有效的</st>
    <st c="11893">RAG 管道所涉及的各个组件和技术。</st>
- en: <st c="11907">RAG vocabulary</st>
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="11907">RAG 词汇</st>
- en: <st c="11922">Now is as</st> <st c="11933">good a time as any to review some
    vocabulary that should help you become familiar with the various concepts in RAG.</st>
    <st c="12049">In the following subsections, we will familiarize ourselves with
    some of this vocabulary, including LLMs, prompting concepts, inference, context
    windows, fine-tuning approaches, vector databases, and vectors/embeddings.</st>
    <st c="12269">This is not an exhaustive list, but understanding these core concepts
    should help you understand everything else we will teach you about RAG in a more</st>
    <st c="12420">effective way.</st>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="11922">现在是</st> <st c="11933">审查一些有助于你熟悉 RAG 中各种概念的词汇的好时机。</st> <st c="12049">在接下来的子节中，我们将熟悉一些这些词汇，包括
    LLM、提示概念、推理、上下文窗口、微调方法、向量数据库和向量/嵌入。</st> <st c="12269">这不是一个详尽的列表，但理解这些核心概念应该有助于你更有效地理解我们将教授你关于
    RAG 的所有其他内容。</st>
- en: <st c="12434">LLM</st>
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="12434">LLM</st>
- en: <st c="12438">Most of this book</st> <st c="12456">will deal with LLMs.</st>
    <st c="12478">LLMs are generative AI technologies that focus on generating text.</st>
    <st c="12545">We will keep things simple by concentrating on the type of model
    that most RAG pipelines use, the LLM.</st> <st c="12648">However, we would like
    to clarify that while we will focus primarily on LLMs, RAG can also be applied
    to other types of generative models, such as those for images, audio, and videos.</st>
    <st c="12832">We will focus on these other types of models and how they are used
    in RAG in</st> [*<st c="12909">Chapter 14</st>*](B22475_14.xhtml#_idTextAnchor283)<st
    c="12919">.</st>
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="12438">本书的大部分内容</st> <st c="12456">将涉及 LLM。</st> <st c="12478">LLM 是专注于生成文本的生成式
    AI 技术。</st> <st c="12545">我们将通过专注于大多数 RAG 管道使用的模型类型，即 LLM，来简化问题。</st> <st c="12648">然而，我们想澄清的是，虽然我们将主要关注
    LLM，但 RAG 也可以应用于其他类型的生成模型，例如图像、音频和视频的生成模型。</st> <st c="12832">我们将在</st> [*<st
    c="12909">第 14 章</st>*](B22475_14.xhtml#_idTextAnchor283)<st c="12919">中关注这些其他类型的模型以及它们在
    RAG 中的应用。</st>
- en: <st c="12920">Some popular examples of LLMs are the OpenAI ChatGPT models, the
    Meta Llama models, Google’s Gemini models, and Anthropic’s</st> <st c="13045">Claude
    models.</st>
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="12920">一些流行的 LLM 示例包括 OpenAI 的 ChatGPT 模型、Meta 的 Llama 模型、Google 的 Gemini
    模型以及 Anthropic 的</st> <st c="13045">Claude 模型。</st>
- en: <st c="13059">Prompting, prompt design, and prompt engineering</st>
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="13059">提示、提示设计、提示工程</st>
- en: <st c="13108">These terms are sometimes used interchangeably, but technically,
    while they all have to do with prompting, they do have</st> <st c="13229">different
    meanings:</st>
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="13108">这些术语有时可以互换使用，但从技术上讲，虽然它们都与提示有关，但它们确实有不同的含义：</st> <st c="13229">提示、提示设计、提示工程</st>
- en: '**<st c="13248">Prompting</st>** <st c="13258">is the</st> <st c="13265">act</st>
    <st c="13270">of sending a query or</st> *<st c="13292">prompt</st>* <st c="13298">to</st>
    <st c="13302">an LLM.</st>'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="13248">提示**</st> <st c="13258">是</st> <st c="13265">发送</st> <st c="13270">查询或</st>
    *<st c="13292">提示</st> <st c="13298">到</st> <st c="13302">一个LLM。</st>'
- en: '**<st c="13309">Prompt design</st>** <st c="13323">refers</st> <st c="13330">to
    the strategy you implement to</st> *<st c="13364">design</st>* <st c="13370">the</st>
    <st c="13374">prompt you will send to the LLM.</st> <st c="13408">Many different
    prompt design strategies work in different scenarios.</st> <st c="13477">We will
    review many of these in</st> [*<st c="13509">Chapter 13</st>*](B22475_13.xhtml#_idTextAnchor256)<st
    c="13519">.</st>'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="13309">提示设计**</st> <st c="13323">指的是</st> <st c="13330">您实施的策略，用于</st>
    *<st c="13364">设计</st> <st c="13370">您将发送到LLM的</st> <st c="13374">提示。</st> <st
    c="13408">许多不同的提示设计策略在不同的场景中有效。</st> <st c="13477">我们将在</st> [*<st c="13509">第13章</st>](B22475_13.xhtml#_idTextAnchor256)<st
    c="13519">中回顾许多这些策略。</st>'
- en: '**<st c="13520">Prompt engineering</st>** <st c="13539">focuses</st> <st c="13548">more
    on the technical aspects</st> <st c="13577">surrounding the prompt that you use
    to improve the outputs from the LLM.</st> <st c="13651">For example, you may break
    up a complex query into two or three different LLM interactions,</st> *<st c="13743">engineering</st>*
    <st c="13754">it better to achieve superior results.</st> <st c="13794">We will
    also review prompt engineering in</st> [*<st c="13836">Chapter 13</st>*](B22475_13.xhtml#_idTextAnchor256)<st
    c="13846">.</st>'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="13520">提示工程**</st> <st c="13539">更侧重于</st> <st c="13548">围绕您用于改进LLM输出的提示的技术方面。</st>
    <st c="13651">例如，您可以将一个复杂的查询拆分为两个或三个不同的LLM交互，</st> *<st c="13743">工程化</st> <st
    c="13754">它以实现更优的结果。</st> <st c="13794">我们还将回顾</st> [*<st c="13836">第13章</st>](B22475_13.xhtml#_idTextAnchor256)<st
    c="13846">中的提示工程。</st>'
- en: <st c="13847">LangChain and LlamaIndex</st>
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="13847">LangChain和LlamaIndex</st>
- en: <st c="13872">This book</st> <st c="13883">will focus on using LangChain as
    the framework for building our RAG pipelines.</st> <st c="13962">LangChain is
    an open source framework</st> <st c="13999">that supports not just RAG but any
    development that wants to use LLMs within a pipeline approach.</st> <st c="14098">With
    over 15 million monthly downloads, LangChain is the most popular generative AI
    development framework.</st> <st c="14205">It supports RAG particularly well, providing
    a modular and flexible set of tools that make RAG development significantly more
    efficient than not using</st> <st c="14356">a framework.</st>
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="13872">本书</st> <st c="13883">将专注于使用LangChain作为构建我们的RAG管道的框架。</st> <st
    c="13962">LangChain是一个开源框架</st> <st c="13999">，不仅支持RAG，还支持任何希望在使用管道方法时使用LLM的开发。</st>
    <st c="14098">拥有超过1500万次的月下载量，LangChain是最受欢迎的生成式AI开发框架。</st> <st c="14205">它特别支持RAG，提供了一套模块化和灵活的工具，使得RAG开发比不使用</st>
    <st c="14356">框架</st> 的开发效率显著提高。
- en: <st c="14368">While LangChain is currently the most popular framework for developing
    RAG pipelines, LlamaIndex</st> <st c="14465">is a</st> <st c="14471">leading alternative
    to LangChain, with similar capabilities in general.</st> <st c="14543">LlamaIndex
    is known for its focus on search and retrieval tasks and may be a good option
    if you require advanced search or need to handle</st> <st c="14681">large datasets.</st>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="14368">虽然LangChain目前是开发RAG管道最流行的框架，但LlamaIndex</st> <st c="14465">是LangChain的一个领先替代品，在总体上具有相似的功能。</st>
    <st c="14543">LlamaIndex以其对搜索和检索任务的关注而闻名，如果您需要高级搜索或需要处理</st> <st c="14681">大量数据集。</st>
- en: <st c="14696">Many other options focus on various niches.</st> <st c="14741">Once
    you have gotten familiar with building RAG pipelines, be sure to look at some
    of the other options to see if there are frameworks that work for your particular</st>
    <st c="14906">project better.</st>
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="14696">许多其他选项专注于各种利基市场。</st> <st c="14741">一旦您熟悉了构建RAG管道，请务必查看一些其他选项，看看是否有适合您特定</st>
    <st c="14906">项目</st> 的框架。
- en: <st c="14921">Inference</st>
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="14921">推理</st>
- en: <st c="14931">We will use</st> <st c="14943">the term</st> **<st c="14953">inference</st>**
    <st c="14962">from time to time.</st> <st c="14982">Generally, this refers to
    the process of the LLM generating outputs or predictions based on given inputs
    using a pre-trained language model.</st> <st c="15123">For example, when you ask
    ChatGPT a question, the steps it takes to provide you with a response is</st>
    <st c="15222">called inference.</st>
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="14931">我们将不时使用</st> <st c="14943">术语</st> **<st c="14953">推理</st>** <st
    c="14962">。</st> <st c="14982">通常，这指的是LLM根据给定的输入使用预训练的语言模型生成输出或预测的过程。</st> <st
    c="15123">例如，当你向ChatGPT提问时，它提供响应所采取的步骤被称为推理。</st>
- en: <st c="15239">Context window</st>
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="15239">上下文窗口</st>
- en: <st c="15254">A</st> <st c="15256">context window, in</st> <st c="15275">the
    context of LLMs, refers to the maximum number of tokens (words, sub-words, or
    characters) that the model can process in a single pass.</st> <st c="15415">It
    determines the amount of text the model can</st> *<st c="15462">see</st>* <st
    c="15465">or</st> *<st c="15469">attend to</st>* <st c="15478">at once when making
    predictions or</st> <st c="15514">generating responses.</st>
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15254">在LLM的上下文中，上下文窗口是指模型在一次遍历中可以处理的标记（单词、子词或字符）的最大数量。</st> <st c="15415">它决定了模型在做出预测或</st>
    *<st c="15462">看到</st> * <st c="15465">或</st> *<st c="15469">关注</st> * <st c="15478">时可以一次性处理或</st>
    <st c="15514">生成响应的文本量。</st>
- en: <st c="15535">The context window size is a key parameter of the model architecture
    and is typically fixed during model training.</st> <st c="15651">It directly relates
    to the input size of the model as it sets an upper limit on the number of tokens
    that can be fed into the model at</st> <st c="15786">a time.</st>
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15535">上下文窗口大小是模型架构的关键参数，通常在模型训练期间固定。</st> <st c="15651">它直接关系到模型的输入大小，因为它为一次可以输入模型中的标记数量设定了一个上限。</st>
- en: <st c="15793">For example, if a model</st> <st c="15818">has a context window
    size of 4,096 tokens, it means that the model can process and generate sequences
    of up to 4,096 tokens.</st> <st c="15943">When processing longer texts, such as
    documents or conversations, the input needs to be divided into smaller segments
    that fit within the context window.</st> <st c="16097">This is often done using
    techniques such as sliding windows</st> <st c="16157">or truncation.</st>
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15793">例如，如果一个模型的上下文窗口大小为4,096个标记，这意味着该模型可以处理和生成最多4,096个标记的序列。</st> <st
    c="15943">在处理较长的文本，如文档或对话时，输入需要被分成适合上下文窗口的小段。</st> <st c="16097">这通常使用滑动窗口</st>
    <st c="16157">或截断等技术来完成。</st>
- en: <st c="16171">The size of the</st> <st c="16188">context window has implications
    for the model’s ability to understand and maintain long-range dependencies and
    context.</st> <st c="16308">Models with larger context windows can capture and
    utilize more contextual information when generating responses, which can lead
    to more coherent and contextually relevant outputs.</st> <st c="16489">However,
    increasing the context window size also increases the computational resources
    required to train and run</st> <st c="16602">the model.</st>
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="16171">上下文窗口的大小对模型理解并维持长距离依赖和上下文的能力有影响。</st> <st c="16308">具有更大上下文窗口的模型在生成响应时可以捕获和利用更多的上下文信息，这可能导致更连贯和上下文相关的输出。</st>
    <st c="16489">然而，增加上下文窗口大小也会增加训练和运行</st> <st c="16602">模型所需的计算资源。</st>
- en: <st c="16612">In the context of RAG, the context window size is essential because
    it determines how much information from the retrieved documents can be effectively
    utilized by the model when generating the final response.</st> <st c="16822">Recent
    advancements in language models have led to the development of models with significantly
    larger context windows, enabling them to process and retain more information from
    the retrieved sources.</st> <st c="17023">See</st> *<st c="17027">Table 1.1</st>*
    <st c="17036">to see the context windows of many popular LLMs, both closed and</st>
    <st c="17102">open sourced:</st>
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="16612">在RAG的上下文中，上下文窗口大小至关重要，因为它决定了检索到的文档中的多少信息可以被模型在生成最终响应时有效地利用。</st>
    <st c="16822">语言模型在最近的发展中导致了具有显著更大的上下文窗口的模型的发展，这使得它们能够处理和保留更多来自检索源的信息。</st> <st
    c="17023">见</st> *<st c="17027">表1.1</st>* <st c="17036">以查看许多流行的LLM的上下文窗口，包括封闭和</st>
    <st c="17102">开源的：</st>
- en: '| **<st c="17115">LLM</st>** | **<st c="17119">Context</st>** **<st c="17128">Window
    (Tokens)</st>** |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| **<st c="17115">LLM</st>** | **<st c="17119">上下文</st>** **<st c="17128">窗口（标记）</st>**
    |'
- en: '| <st c="17143">ChatGPT-3.5 Turbo</st> <st c="17162">0613 (OpenAI)</st> | <st
    c="17175">4,096</st> |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| <st c="17143">ChatGPT-3.5 Turbo</st> <st c="17162">0613 (OpenAI)</st> | <st
    c="17175">4,096</st> |'
- en: '| <st c="17181">Llama</st> <st c="17188">2 (Meta)</st> | <st c="17196">4,096</st>
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| <st c="17181">Llama</st> <st c="17188">2 (Meta)</st> | <st c="17196">4,096</st>
    |'
- en: '| <st c="17202">Llama</st> <st c="17209">3 (Meta)</st> | <st c="17217">8,000</st>
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| <st c="17202">Llama</st> <st c="17209">3 (Meta)</st> | <st c="17217">8,000</st>
    |'
- en: '| <st c="17223">ChatGPT-4 (OpenAI)</st> | <st c="17242">8,192</st> |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| <st c="17223">ChatGPT-4 (OpenAI)</st> | <st c="17242">8,192</st> |'
- en: '| <st c="17248">ChatGPT-3.5 Turbo</st> <st c="17267">0125 (OpenAI)</st> | <st
    c="17280">16,385</st> |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| <st c="17248">ChatGPT-3.5 Turbo</st> <st c="17267">0125 (OpenAI)</st> | <st
    c="17280">16,385</st> |'
- en: '| <st c="17287">ChatGPT-4.0-32k (OpenAI)</st> | <st c="17312">32,000</st> |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| <st c="17287">ChatGPT-4.0-32k (OpenAI)</st> | <st c="17312">32,000</st> |'
- en: '| <st c="17319">Mistral (</st><st c="17329">Mistral AI)</st> | <st c="17341">32,000</st>
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| <st c="17319">Mistral (</st><st c="17329">Mistral AI)</st> | <st c="17341">32,000</st>
    |'
- en: '| <st c="17348">Mixtral (</st><st c="17358">Mistral AI)</st> | <st c="17370">32,000</st>
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| <st c="17348">Mixtral (</st><st c="17358">Mistral AI)</st> | <st c="17370">32,000</st>
    |'
- en: '| <st c="17377">DBRX (Databricks)</st> | <st c="17395">32,000</st> |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| <st c="17377">DBRX (Databricks)</st> | <st c="17395">32,000</st> |'
- en: '| <st c="17402">Gemini 1.0</st> <st c="17414">Pro (Google)</st> | <st c="17426">32,000</st>
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| <st c="17402">Gemini 1.0</st> <st c="17414">Pro (Google)</st> | <st c="17426">32,000</st>
    |'
- en: '| <st c="17433">ChatGPT-4.0</st> <st c="17446">Turbo (OpenAI)</st> | <st c="17460">128,000</st>
    |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| <st c="17433">ChatGPT-4.0</st> <st c="17446">Turbo (OpenAI)</st> | <st c="17460">128,000</st>
    |'
- en: '| <st c="17468">ChatGPT-4o (OpenAI)</st> | <st c="17488">128,000</st> |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| <st c="17468">ChatGPT-4o (OpenAI)</st> | <st c="17488">128,000</st> |'
- en: '| <st c="17496">Claude</st> <st c="17504">2.1 (Anthropic)</st> | <st c="17519">200,000</st>
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| <st c="17496">Claude</st> <st c="17504">2.1 (Anthropic)</st> | <st c="17519">200,000</st>
    |'
- en: '| <st c="17527">Claude</st> <st c="17535">3 (Anthropic)</st> | <st c="17548">200,000</st>
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| <st c="17527">Claude</st> <st c="17535">3 (Anthropic)</st> | <st c="17548">200,000</st>
    |'
- en: '| <st c="17556">Gemini 1.5</st> <st c="17568">Pro (Google)</st> | <st c="17580">1,000,000</st>
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| <st c="17556">Gemini 1.5</st> <st c="17568">Pro (Google)</st> | <st c="17580">1,000,000</st>
    |'
- en: <st c="17590">Table 1.1 – Different context windows for LLMs</st>
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="17590">表1.1 – LLMs的不同上下文窗口</st>
- en: '*<st c="17637">Figure 1</st>**<st c="17646">.1</st>*<st c="17648">, which is</st>
    <st c="17658">based on</st> *<st c="17668">Table 1.1</st>*<st c="17677">, shows
    that Gemini 1.5 Pro is far</st> <st c="17711">larger than</st> <st c="17724">the
    others.</st>'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*<st c="17637">图1</st>**<st c="17646">.1</st>*<st c="17648">，基于</st> *<st c="17668">表1.1</st>*<st
    c="17677">，显示Gemini 1.5 Pro远大于其他模型。</st>'
- en: '![Figure 1.1 – Different context windows for LLMs](img/B22475_01_01.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图1.1 – LLMs的不同上下文窗口](img/B22475_01_01.jpg)'
- en: <st c="17828">Figure 1.1 – Different context windows for LLMs</st>
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="17828">图1.1 – LLMs的不同上下文窗口</st>
- en: <st c="17875">Note that</st> *<st c="17886">Figure 1</st>**<st c="17894">.1</st>*
    <st c="17896">shows models that have generally aged from right to left, meaning
    the older models tended to have smaller context windows, with the newest models
    having larger context windows.</st> <st c="18074">This trend is likely to continue,
    pushing the typical context window larger as</st> <st c="18153">time progresses.</st>
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="17875">请注意</st> *<st c="17886">图1</st>**<st c="17894">.1</st>* <st c="17896">显示了从右到左普遍老龄化的模型，这意味着较老的模型倾向于具有较小的上下文窗口，而最新的模型具有较大的上下文窗口。</st>
    <st c="18074">这种趋势很可能会继续，随着时间的推移，典型的上下文窗口会越来越大。</st>
- en: <st c="18169">Fine-tuning – full-model fine-tuning (FMFT) and parameter-efficient
    fine-tuning (PEFT)</st>
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="18169">微调 – 全模型微调（FMFT）和参数高效微调（PEFT）</st>
- en: <st c="18256">FMFT is</st> <st c="18265">where you take a</st> <st c="18281">foundation
    model and train it further to gain new capabilities.</st> <st c="18346">You could
    simply give it new knowledge for a specific domain, or you could give it a skill,
    such as being a conversational chatbot.</st> <st c="18478">FMFT updates all the
    parameters and biases in</st> <st c="18524">the model.</st>
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="18256">FMFT是</st> <st c="18265">您将基础模型进一步训练以获得新能力的地方。</st> <st c="18281">您可以简单地为其提供特定领域的知识，或者您可以给它一项技能，例如成为一个会话聊天机器人。</st>
    <st c="18346">FMFT更新模型中的所有参数和偏差。</st>
- en: <st c="18534">PEFT, on</st> <st c="18544">the</st> <st c="18548">other hand,
    is a type of fine-tuning where you focus only on specific parts of the parameters
    or biases when you fine-tune the model, but with a similar goal as general fine-tuning.</st>
    <st c="18730">The latest research in this area shows that you can achieve similar
    results to FMFT with far less cost, time commitment,</st> <st c="18851">and data.</st>
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="18534">PEFT，另一方面，是一种微调类型，在微调模型时您只关注参数或偏差的特定部分，但与通用微调有相似的目标。</st> <st
    c="18730">该领域的最新研究显示，您可以用更少的成本、时间和数据实现与FMFT相似的结果。</st> <st c="18851">和数据。</st>
- en: <st c="18860">While this book does not focus on fine-tuning, it is a very valid
    strategy to try to use a model fine-tuned with your data to give it more knowledge
    from your domain or to give it more of a</st> *<st c="19051">voice</st>* <st c="19056">from
    your domain.</st> <st c="19075">For example, you could train it to talk more like
    a scientist than a generic foundation model, if you’re using this in a scientific
    field.</st> <st c="19214">Alternatively, if you are developing in a legal field,
    you may want it to sound more like</st> <st c="19304">a lawyer.</st>
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="18860">虽然这本书不专注于微调，但尝试使用您自己的数据微调的模型来赋予它更多来自您领域的知识，或者给它更多来自您领域的</st> *<st
    c="19051">声音</st>* <st c="19056">，是一个非常有效的策略。</st> <st c="19075">例如，如果您在科学领域使用它，您可以训练它说话更像科学家而不是通用基础模型。</st>
    <st c="19214">或者，如果您在法律领域开发，您可能希望它听起来更像</st> <st c="19304">律师。</st>
- en: <st c="19313">Fine-tuning also helps the LLM to understand your company’s data
    better, making it better at generating an effective response during the RAG process.</st>
    <st c="19464">For example, if you have a scientific company, you might fine-tune
    a model with scientific information and use it for a RAG application that summarizes
    your research.</st> <st c="19631">This may improve your RAG application’s output
    (the summaries of your research) because your fine-tuned model understands your
    data better and can provide a more</st> <st c="19793">effective summary.</st>
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="19313">微调还有助于大型语言模型更好地理解贵公司的数据，使其在RAG过程中生成有效响应的能力更强。</st> <st c="19464">例如，如果您是一家科技公司，您可能会微调一个包含科学信息的模型，并将其用于总结您的研究的RAG应用。</st>
    <st c="19631">这可能提高了您的RAG应用输出（您的研究总结）的质量，因为您的微调模型对您的数据理解得更好，并能提供更有效的总结。</st> <st
    c="19793">总结。</st>
- en: <st c="19811">Vector store or vector database?</st>
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="19811">向量存储还是向量数据库？</st>
- en: <st c="19844">Both!</st> <st c="19851">All</st> <st c="19855">vector databases</st>
    <st c="19871">are vector stores, but not all vector</st> <st c="19910">stores
    are vector databases.</st> <st c="19939">OK, while you get out your chalkboard
    to draw a Vinn diagram, I will continue to explain</st> <st c="20028">this statement.</st>
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="19844">两者都是！</st> <st c="19851">所有</st> <st c="19855">向量数据库</st> <st
    c="19871">都是向量存储，但并非所有向量</st> <st c="19910">存储都是向量数据库。</st> <st c="19939">好的，当您拿出粉笔在黑板上画Venn图时，我会继续解释</st>
    <st c="20028">这个陈述。</st>
- en: <st c="20043">There are ways to store vectors that are not full databases.</st>
    <st c="20105">They are simply storage devices for vectors.</st> <st c="20150">So,
    to</st> <st c="20156">encompass all possible ways to store vectors, LangChain
    calls them all</st> **<st c="20228">vector stores</st>**<st c="20241">. Let’s
    do</st> <st c="20251">the same!</st> <st c="20262">Just know that not all the</st>
    *<st c="20289">vector stores</st>* <st c="20302">that LangChain connects with
    are officially considered vector databases, but in general, most of them are and
    many people refer to all of them as vector databases, even when they are not technically
    full databases from a functionality standpoint.</st> <st c="20550">Phew – glad
    we cleared</st> <st c="20573">that up!</st>
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="20043">有存储向量的方法，并不一定是完整的数据库。</st> <st c="20105">它们只是向量的存储设备。</st> <st
    c="20150">因此，为了</st> <st c="20156">涵盖所有可能的向量存储方式，LangChain将它们都称为</st> **<st c="20228">向量存储</st>**<st
    c="20241">。让我们做</st> <st c="20251">同样的事情！</st> <st c="20262">只需知道，并非所有LangChain连接的</st>
    *<st c="20289">向量存储</st>* <st c="20302">都被官方视为向量数据库，但一般来说，大多数都是，许多人将它们都称为向量数据库，即使它们在功能上并不是完整的数据库。</st>
    <st c="20550">好了——很高兴我们澄清了这一点！</st>
- en: <st c="20581">Vectors, vectors, vectors!</st>
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="20581">向量，向量，向量！</st>
- en: <st c="20608">A vector is</st> <st c="20620">a mathematical representation of
    your data.</st> <st c="20665">They are often referred to as embeddings when talking
    specifically about</st> **<st c="20738">natural language processing</st>** <st
    c="20765">(</st>**<st c="20767">NLP</st>**<st c="20770">) and</st> <st c="20777">LLMs.</st>
    <st c="20783">Vectors are one of the most important concepts to understand and
    there are many different parts of a RAG pipeline that</st> <st c="20902">utilize
    vectors.</st>
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="20608">向量是</st> <st c="20620">你数据的数学表示。</st> <st c="20665">当具体谈到**<st
    c="20738">自然语言处理</st>** <st c="20765">(**<st c="20767">NLP</st>**<st c="20770">**)和**<st
    c="20777">LLMs</st>**时，它们通常被称为嵌入。</st> <st c="20783">向量是理解最重要的概念之一，RAG管道的许多部分都利用了向量。</st>
- en: <st c="20918">We just covered many key vocabulary terms that will be important
    for you to understand the rest of this book.</st> <st c="21029">Many of these
    concepts will be expanded upon in future chapters.</st> <st c="21094">In the next
    section, we will continue to discuss vectors in further depth.</st> <st c="21169">And
    beyond that, we will spend</st> *<st c="21200">Chapters 7</st>* <st c="21210">and</st>
    *<st c="21215">8</st>* <st c="21216">going over vectors and how they are used
    to find</st> <st c="21266">similar content.</st>
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="20918">我们刚刚介绍了许多关键词汇，这些词汇对于理解本书的其余部分非常重要。</st> <st c="21029">这些概念中的许多将在未来的章节中进一步阐述。</st>
    <st c="21094">在下一节中，我们将进一步深入讨论向量。</st> <st c="21169">而且，我们将用</st> *<st c="21200">第7章</st>
    <st c="21210">和</st> *<st c="21215">第8章</st> <st c="21216">来介绍向量以及它们如何用于查找**<st
    c="21266">相似内容</st>**。</st>
- en: <st c="21282">Vectors</st>
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="21282">向量</st>
- en: <st c="21290">It could be argued</st> <st c="21309">that understanding vectors
    and all the ways they are used in RAG is the most important part of this entire
    book.</st> <st c="21423">As mentioned previously, vectors are simply the mathematical
    representations of your external data, and they are often referred to as embeddings.</st>
    <st c="21569">These representations capture semantic information in a format that
    can be processed by algorithms, facilitating tasks such as similarity search,
    which is a crucial step in the</st> <st c="21746">RAG process.</st>
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21290">可以认为</st> <st c="21309">理解向量和它们在RAG中的所有应用方式是这本书最重要的部分。</st> <st
    c="21423">如前所述，向量只是你外部数据的数学表示，它们通常被称为嵌入。</st> <st c="21569">这些表示以算法可以处理的形式捕获语义信息，从而促进诸如相似度搜索等任务，这是RAG过程中的关键步骤。</st>
- en: <st c="21758">Vectors typically have a specific dimension based on how many
    numbers are represented by them.</st> <st c="21854">For example, this is a</st>
    <st c="21877">four-dimensional vector:</st>
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21758">向量的维度通常是特定的，这取决于它们表示了多少个数字。</st> <st c="21854">例如，这是一个</st> <st
    c="21877">四维向量：</st>
- en: '[PRE0]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: <st c="21930">If you didn’t know</st> <st c="21949">we were talking about vectors
    and you saw this in Python code, you might recognize this as a list of four floating
    points, and you aren’t too far off.</st> <st c="22101">However, when working with
    vectors in Python, you want to recognize them as a NumPy array, rather than lists.</st>
    <st c="22211">NumPy arrays are generally more machine-learning-friendly because
    they are optimized to be processed much faster and more efficiently than Python
    lists, and they are more broadly recognized as the de facto representation of
    embeddings across machine learning packages such as SciPy, pandas, scikit-learn,
    TensorFlow, Keras, Pytorch, and many others.</st> <st c="22561">NumPy also enables
    you to perform vectorized math directly on the NumPy array, such as performing
    element-wise operations, without having to code in loops and other approaches
    you might have to use if you were using a different type</st> <st c="22794">of
    sequence.</st>
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21930">如果你不知道</st> <st c="21949">我们正在谈论向量，并且你在Python代码中看到了这个，你可能会认出这是一个包含四个浮点数的列表，而且你并不离谱。</st>
    <st c="22101">然而，当你在Python中使用向量时，你希望将它们识别为NumPy数组，而不是列表。</st> <st c="22211">NumPy数组通常更受机器学习友好，因为它们被优化得可以比Python列表更快、更有效地处理，并且在机器学习包（如SciPy、pandas、scikit-learn、TensorFlow、Keras、Pytorch等）中被更广泛地认可为嵌入的默认表示。</st>
    <st c="22561">NumPy还允许你直接在NumPy数组上执行向量数学，例如执行元素级操作，而无需使用循环和其他你可能需要使用不同类型序列的方法。</st>
- en: <st c="22806">When working with vectors for vectorization, there are often hundreds
    or thousands of dimensions, which refers to the number of floating points present
    in the vector.</st> <st c="22974">Higher dimensionality can capture more detailed
    semantic information, which is crucial for accurately matching query inputs with
    relevant documents or data in</st> <st c="23133">RAG applications.</st>
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="22806">在处理向量进行向量化的工作时，通常会有数百或数千个维度，这指的是向量中存在的浮点数数量。</st> <st c="22974">更高的维度可以捕捉更详细的语义信息，这对于在RAG应用中准确匹配查询输入与相关文档或数据至关重要。</st>
- en: <st c="23150">In</st> [*<st c="23154">Chapter 7</st>*](B22475_07.xhtml#_idTextAnchor122)<st
    c="23163">, we will cover the key role vectors and vector databases play in RAG
    implementation.</st> <st c="23249">Then, in</st> [*<st c="23258">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="23267">, we will dive more into the concept of similarity searches, which utilize
    vectors to search much faster and more efficiently.</st> <st c="23394">These are
    key concepts that will help you gain a much deeper understanding of how to better
    implement a</st> <st c="23498">RAG pipeline.</st>
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23150">在第<st c="23154">7章</st>](B22475_07.xhtml#_idTextAnchor122)<st
    c="23163">中，我们将介绍向量和向量数据库在RAG实现中的关键作用。</st> <st c="23249">然后，在第<st c="23258">8章</st>](B22475_08.xhtml#_idTextAnchor152)<st
    c="23267">中，我们将更深入地探讨相似性搜索的概念，它利用向量以更快、更高效的方式搜索。</st> <st c="23394">这些是帮助你更深入理解如何更好地实现RAG管道的关键概念。</st>
- en: <st c="23511">Understanding vectors can be a crucial underlying concept to understand
    how to implement RAG, but how is RAG used in practical applications in the enterprise?</st>
    <st c="23671">We will discuss these practical AI applications of RAG in the</st>
    <st c="23733">next section.</st>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23511">理解向量可以是一个关键的基础概念，以了解如何实现RAG，但在企业中RAG是如何在实际应用中被使用的呢？</st> <st c="23671">我们将在下一节讨论RAG的这些实际AI应用。</st>
- en: <st c="23746">Implementing RAG in AI applications</st>
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="23746">在AI应用中实现RAG</st>
- en: <st c="23782">RAG is</st> <st c="23789">rapidly becoming a cornerstone of generative
    AI platforms in the corporate world.</st> <st c="23872">RAG combines the power
    of retrieving internal or</st> *<st c="23921">new</st>* <st c="23924">data with
    generative language models to enhance the quality and relevance of the generated
    text.</st> <st c="24022">This technique can be particularly useful for companies
    across various industries to improve their products, services, and operational
    efficiencies.</st> <st c="24171">The following are some examples of how RAG can</st>
    <st c="24218">be used:</st>
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23782">RAG正在迅速成为企业界生成式AI平台的基础。</st> <st c="23789">RAG结合了检索内部或</st> **<st
    c="23921">新</st>** <st c="23924">数据与生成式语言模型的力量，以增强生成文本的质量和相关性。</st> <st c="24022">这项技术对于各个行业的公司来说特别有用，可以帮助它们改进产品、服务和运营效率。</st>
    <st c="24171">以下是一些RAG可以</st> <st c="24218">被使用的例子：</st>
- en: '**<st c="24226">Customer support and chatbots</st>**<st c="24256">: These can
    exist without RAG, but when integrated with</st> <st c="24313">RAG, it can connect
    those chatbots with past customer interactions, FAQs, support documents, and anything
    else that was specific to</st> <st c="24445">that customer.</st>'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="24226">客户支持和聊天机器人</st>**<st c="24256">：这些可以在没有RAG的情况下存在，但当与RAG集成时，它可以连接那些聊天机器人与过去的客户互动、常见问题解答、支持文档以及与该客户相关的任何其他内容。</st>'
- en: '**<st c="24459">Technical support</st>**<st c="24477">: With better access
    to customer history and information, RAG-enhanced chatbots can provide a significant
    improvement to current technical</st> <st c="24617">support chatbots.</st>'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="24459">技术支持</st>**<st c="24477">：通过更好地访问客户历史和相关信息，增强RAG的聊天机器人可以显著提高当前技术支持聊天机器人的性能。</st>'
- en: '**<st c="24634">Automated reporting</st>**<st c="24654">: RAG can assist in
    creating initial drafts or summarizing existing articles, research papers, and
    other types of unstructured data into more</st> <st c="24797">digestible formats.</st>'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="24634">自动报告</st>**<st c="24654">：RAG可以帮助创建初始草案或总结现有的文章、研究论文和其他类型的非结构化数据，使其更易于消化。</st>'
- en: '**<st c="24816">E-commerce support</st>**<st c="24835">: For e-commerce companies,
    RAG can help generate dynamic product descriptions and user content, as well as
    make better</st> <st c="24956">product recommendations.</st>'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="24816">电子商务支持</st>**<st c="24835">：对于电子商务公司，RAG可以帮助生成动态的产品描述和用户内容，以及做出更好的</st>
    <st c="24956">产品推荐。</st>'
- en: '**<st c="24980">Utilizing knowledge bases</st>**<st c="25006">: RAG improves
    the searchability and utility of both internal and general knowledge bases by
    generating summaries, providing direct answers to queries, and retrieving relevant
    information across various domains such as legal, compliance, research, medical,
    academia, patents, and</st> <st c="25287">technical documents.</st>'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="24980">利用知识库</st>**<st c="25006">：RAG通过生成摘要、提供直接答案以及对法律、合规、研究、医疗、学术界、专利和技术文档等各个领域的相关信息进行检索，从而提高了内部和通用知识库的可搜索性和实用性。</st>
    <st c="25287">技术文档。</st>'
- en: '**<st c="25307">Innovation scouting</st>**<st c="25327">: This is like searching
    general knowledge bases but with a focus on innovation.</st> <st c="25409">With
    this, companies can use RAG to scan and summarize information from quality sources
    to identify trends and potential areas for innovations that are relevant to that</st>
    <st c="25578">company’s specialization.</st>'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="25307">创新侦察</st>**<st c="25327">：这就像在搜索通用知识库，但重点是创新。</st> <st c="25409">利用这一点，公司可以使用RAG扫描和总结来自优质来源的信息，以识别与公司专业化相关的趋势和潜在的创新领域。</st>
    <st c="25578">公司。</st>'
- en: '**<st c="25603">Training and education</st>**<st c="25626">: RAG can be used
    by education organizations and corporate training programs to generate or customize
    learning materials based on specific needs and knowledge levels of the learners.</st>
    <st c="25810">With RAG, a much deeper level of internal knowledge from the organization
    can be incorporated into the educational curriculum in very customized ways to
    the individual</st> <st c="25978">or role.</st>'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="25603">培训和教育工作</st>**<st c="25626">：教育组织和企业培训计划可以使用RAG根据学习者的具体需求和知识水平生成或定制学习材料。</st>
    <st c="25810">利用RAG，可以将组织内部的知识以非常定制化的方式融入教育课程中，针对个人或角色。</st>'
- en: <st c="25986">These are just a</st> <st c="26003">few of the ways organizations
    are using RAG right now to improve their operations.</st> <st c="26087">We will
    dive into each of these areas in more depth in</st> [*<st c="26142">Chapter 3</st>*](B22475_03.xhtml#_idTextAnchor056)<st
    c="26151">, helping you understand how you can implement all these game-changing
    initiatives in multiple places in</st> <st c="26256">your company.</st>
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25986">这些只是组织目前使用RAG来改进其运营的一些方法。</st> <st c="26003">我们将在</st> [*<st c="26142">第3章</st>*](B22475_03.xhtml#_idTextAnchor056)<st
    c="26151">中更深入地探讨这些领域，帮助你了解如何在公司的多个地方实施所有这些变革性举措。</st> <st c="26256">公司。</st>
- en: <st c="26269">You might be wondering, “</st>*<st c="26295">If I am using an
    LLM such as ChatGPT to answer my questions in my company, does that mean my company
    is using</st>* *<st c="26406">RAG already?</st>*<st c="26418">”</st>
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26269">你可能想知道，“</st>*<st c="26295">如果我使用LLM如ChatGPT来回答公司的问题，这意味着我的公司已经使用</st>*
    *<st c="26406">RAG了吗？</st>*<st c="26418">”</st>
- en: <st c="26420">The answer</st> <st c="26431">is “</st>*<st c="26435">No.</st>*<st
    c="26439">”</st>
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26420">答案是“</st>*<st c="26435">不。</st>*<st c="26439">”</st>
- en: <st c="26441">If you just log in to ChatGPT and ask questions, that is not the
    same as implementing RAG.</st> <st c="26532">Both ChatGPT and RAG are forms of
    generative AI, and they are sometimes used together, but they are two different
    concepts.</st> <st c="26656">In the next section, we will discuss the differences
    between generative AI</st> <st c="26731">and RAG.</st>
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26441">如果你只是登录ChatGPT并提问，这并不等同于实施RAG。</st> <st c="26532">ChatGPT和RAG都是生成式AI的形式，它们有时会一起使用，但它们是两个不同的概念。</st>
    <st c="26656">在下一节中，我们将讨论生成式AI与RAG之间的区别。</st> <st c="26731">和RAG。</st>
- en: <st c="26739">Comparing RAG with conventional generative AI</st>
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="26739">比较RAG与传统生成式AI</st>
- en: <st c="26785">Conventional generative AI has</st> <st c="26817">already</st>
    <st c="26824">shown to be a revolutionary change for companies, helping their
    employees reach new levels of productivity.</st> <st c="26933">LLMs such as ChatGPT
    are assisting users with a rapidly growing list of applications that include writing
    business plans, writing and improving code, writing marketing copy, and even providing
    healthier recipes for a specific diet.</st> <st c="27165">Ultimately, much of
    what users are doing is getting</st> <st c="27217">done faster.</st>
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26785">传统的生成式AI已经</st> <st c="26817">证明</st> <st c="26824">是公司的一次革命性变革，帮助员工达到新的生产力水平。</st>
    <st c="26933">例如ChatGPT这样的LLM正在帮助用户使用快速增长的列表中的应用程序，包括撰写商业计划、编写和改进代码、撰写营销文案，甚至为特定饮食提供更健康的食谱。</st>
    <st c="27165">最终，用户所做的许多事情都变得</st> <st c="27217">更快。</st>
- en: <st c="27229">However, conventional generative AI</st> <st c="27265">does not
    know what it does not know.</st> <st c="27303">And that includes most of the internal
    data in your company.</st> <st c="27364">Can you imagine what you could do with
    all the benefits mentioned previously, but combined with all the data within your
    company – about everything your company has ever done, about your customers and
    all their interactions, or about all your products and services combined with
    a knowledge of what a specific customer’s needs are?</st> <st c="27696">You do
    not have to imagine it – that is what</st> <st c="27741">RAG does!</st>
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27229">然而，传统的生成式 AI</st> <st c="27265">不知道它不知道的东西。</st> <st c="27303">这包括你公司的大部分内部数据。</st>
    <st c="27364">你能想象，如果将之前提到的所有好处结合起来，再加上你公司内部的所有数据——关于你公司所做的一切，关于你的客户及其所有互动，或者关于所有产品和服务，再加上对特定客户需求的认识，你能做什么吗？</st>
    <st c="27696">你不必想象——这就是 RAG 所做的！</st>
- en: <st c="27750">Before RAG, most of the services you saw that connected customers
    or employees with the data resources of the company were just scratching the surface
    of what is possible compared to if they could access</st> *<st c="27955">all</st>*
    <st c="27958">the data in the company.</st> <st c="27984">With the advent of RAG
    and generative AI in general, corporations are on the precipice of something really,</st>
    <st c="28092">really big.</st>
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27750">在 RAG 之前，你看到的大多数将客户或员工与公司数据资源连接的服务，与它们能够访问</st> *<st c="27955">所有</st>*
    <st c="27958">公司数据相比，只是触及了可能性的皮毛。</st> <st c="27984">随着 RAG 和通用生成式 AI 的出现，企业正站在一个真正的、</st>
    <st c="28092">真正重大的转折点上。</st>
- en: <st c="28103">Another area you might confuse RAG with is the concept of fine-tuning
    a model.</st> <st c="28183">Let’s discuss what the differences are between these
    types</st> <st c="28242">of approaches.</st>
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28103">你可能将 RAG 与调整模型的概念混淆。</st> <st c="28183">让我们讨论一下这些方法之间的区别。</st>
- en: <st c="28256">Comparing RAG with model fine-tuning</st>
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="28256">比较 RAG 与模型微调</st>
- en: <st c="28293">LLMs can be</st> <st c="28305">adapted to your</st> <st c="28322">data
    in</st> <st c="28330">two ways:</st>
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28293">LLM 可以通过两种方式来</st> <st c="28305">适应你的</st> <st c="28322">数据：</st>
- en: '**<st c="28339">Fine-tuning</st>**<st c="28351">: With fine-tuning, you are
    adjusting the weights and/or biases that define the model’s intelligence based
    on new training data.</st> <st c="28481">This directly impacts the model, permanently
    changing how it will interact with</st> <st c="28561">new inputs.</st>'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="28339">微调</st>**<st c="28351">：通过微调，你根据新的训练数据调整定义模型智能的权重和/或偏差。</st>
    <st c="28481">这直接影响模型，永久地改变它将如何与</st> <st c="28561">新输入交互。</st>'
- en: '**<st c="28572">Input/prompts</st>**<st c="28586">: This is where you</st>
    *<st c="28607">use</st>* <st c="28610">the model, using the prompt/input to introduce
    new knowledge that the LLM can</st> <st c="28689">act upon.</st>'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="28572">输入/提示</st>**<st c="28586">：这是你使用模型的地方，使用提示/输入来引入 LLM 可以</st>
    *<st c="28607">使用</st>* <st c="28610">的新知识。</st>'
- en: <st c="28698">Why not use fine-tuning in all situations?</st> <st c="28742">Once
    you have introduced the new knowledge, the LLM will always have it!</st> <st c="28815">It
    is also how the model was created – by being trained with data, right?</st> <st
    c="28889">That sounds right in theory, but in practice, fine-tuning has been more
    reliable in teaching a model specialized tasks (such as teaching a model how to
    converse in a certain way), and less reliable for</st> <st c="29091">factual recall.</st>
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28698">为什么不在所有情况下都使用微调呢？</st> <st c="28742">一旦你引入了新的知识，LLM 总是会保留它！</st>
    <st c="28815">这也是模型是如何被创建的——通过用数据训练，对吧？</st> <st c="28889">在理论上听起来是正确的，但在实践中，微调在教授模型专门任务（例如教授模型如何以某种方式交谈）方面更为可靠，而在</st>
    <st c="29091">事实回忆方面则不太可靠。</st>
- en: <st c="29106">The reason is complicated, but in general, a model’s knowledge
    of facts is like a human’s long-term memory.</st> <st c="29215">If you memorize
    a long passage from a speech or book and then try to recall it a few months later,
    you will likely still understand the context of the information, but you may forget
    specific details.</st> <st c="29416">On the other hand, adding knowledge through
    the input of the model is like our short-term memory, where the facts, details,
    and even the order of wording are all very fresh and available for recall.</st>
    <st c="29615">It is this latter scenario that lends itself better in a situation
    where you want successful factual recall.</st> <st c="29724">And given how much
    more expensive fine-tuning can be, this makes it that much more important to</st>
    <st c="29820">consider RAG.</st>
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="29106">原因很复杂，但总的来说，模型对事实的了解就像人类的长时记忆。</st> <st c="29215">如果你记住了演讲或书籍中的长篇大论，然后几个月后尝试回忆，你可能会仍然理解信息的上下文，但你可能会忘记具体的细节。</st>
    <st c="29416">另一方面，通过模型输入添加知识就像我们的短期记忆，其中事实、细节，甚至措辞的顺序都非常新鲜且可供回忆。</st> <st c="29615">在需要成功回忆事实的情况下，这种后一种情况更适合。</st>
    <st c="29724">鉴于微调可能更加昂贵，这使得考虑RAG变得尤为重要。</st>
- en: <st c="29833">There is a trade-off, though.</st> <st c="29864">While there are
    generally ways to feed all data you have to a model for fine-tuning, inputs are
    limited by the context window of the model.</st> <st c="30004">This is an area
    that is actively being addressed.</st> <st c="30054">For example, early versions
    of ChatGPT 3.5 had a 4,096 token context window, which is the equivalent of about
    five pages of text.</st> <st c="30184">When ChatGPT 4 was released, they expanded
    the context window to 8,192 tokens (10 pages) and there was a Chat 4-32k version
    that had a context window of 32,768 tokens (40 pages).</st> <st c="30363">This
    issue is so important that they included the context window size in the name of
    the model.</st> <st c="30459">That is a strong indicator of how important the
    context</st> <st c="30515">window is!</st>
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，<st c="29833">也存在权衡。</st> <st c="29864">虽然通常有方法将所有数据输入模型进行微调，但输入受限于模型的上下文窗口。</st>
    <st c="30004">这是一个正在积极解决的问题。</st> <st c="30054">例如，ChatGPT 3.5的早期版本有一个4,096个标记的上下文窗口，相当于大约五页文本。</st>
    <st c="30184">当ChatGPT 4发布时，他们将其上下文窗口扩展到8,192个标记（10页），还有一个Chat 4-32k版本，其上下文窗口为32,768个标记（40页）。</st>
    <st c="30363">这个问题非常重要，以至于他们将其上下文窗口大小包含在模型名称中。</st> <st c="30459">这是上下文窗口重要性的一个强烈指标！</st>
- en: <st c="30525">Interesting fact!</st>
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="30525">有趣的事实！</st>
- en: <st c="30543">What about the latest Gemini 1.5 model?</st> <st c="30584">It
    has a 1 million token context window or over</st> <st c="30632">1,000 pages!</st>
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 关于最新的Gemini 1.5型号呢？<st c="30543">它拥有100万个标记的上下文窗口，或者说超过</st> <st c="30584">1,000页！</st>
- en: <st c="30644">As the</st> <st c="30652">context windows expand, another issue
    is created.</st> <st c="30702">Early models with expanded context windows were</st>
    <st c="30749">shown to lose a lot of the details, especially in the</st> *<st
    c="30804">middle</st>* <st c="30810">of the text.</st> <st c="30824">This issue
    is also being addressed.</st> <st c="30860">The Gemini 1.5 model with its 1 million
    token context window has performed well in tests called</st> *<st c="30956">needle
    in a haystack</st>* <st c="30976">tests for</st> *<st c="30987">remembering</st>*
    <st c="30998">all details well throughout the text it can take as input.</st>
    <st c="31058">Unfortunately, the model did not perform as well in the</st> *<st
    c="31114">multiple needles in a haystack</st>* <st c="31144">tests.</st> <st c="31152">Expect
    more effort in this area as these context windows become larger.</st> <st c="31224">Keep
    this in mind if you need to work with large amounts of text at</st> <st c="31292">a
    time.</st>
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 随着<st c="30644">上下文窗口的扩大，另一个问题也随之产生。</st> <st c="30652">早期具有扩展上下文窗口的模型在测试中显示，细节丢失很多，尤其是在</st>
    *<st c="30804">文本的中间</st>* <st c="30810">部分。</st> <st c="30824">这个问题也在被解决。</st>
    <st c="30860">具有100万个标记上下文窗口的Gemini 1.5模型在所谓的*<st c="30956">大海捞针</st>* <st c="30976">测试中表现良好，这些测试用于测试*<st
    c="30987">记住</st> *<st c="30998">整个文本中它所能接受的输入的所有细节。</st> <st c="31058">不幸的是，该模型在*<st
    c="31114">多针大海捞针</st>* <st c="31144">测试中的表现并不理想。</st> <st c="31152">随着这些上下文窗口变得更大，预计在这个领域将会有更多的努力。</st>
    <st c="31224">如果你需要一次性处理大量文本，请记住这一点。</st>
- en: <st c="31299">Note</st>
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="31299">注意</st>
- en: <st c="31304">It is important to note that token count differs from word count
    as tokens include punctuation, symbols, numbers, and other text representations.</st>
    <st c="31451">How a compound word such as</st> *<st c="31479">ice cream</st>*
    <st c="31488">is treated token-wise depends on the tokenization scheme and it
    can vary across LLMs.</st> <st c="31575">But most well-known LLMs (such as ChatGPT
    and Gemini) would consider</st> *<st c="31644">ice cream</st>* <st c="31653">as
    two tokens.</st> <st c="31669">Under certain circumstances in NLP, you may argue
    that it should be one token based on the concept that a token should represent
    a useful semantic unit for processing, but that is not the case for</st> <st c="31866">these
    models.</st>
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="31304">需要注意的是，标记数与词数不同，因为标记包括标点符号、符号、数字和其他文本表示。</st> <st c="31451">复合词，例如</st>
    *<st c="31479">ice cream</st>* <st c="31488">在标记方面的处理取决于标记化方案，并且可能在不同的大型语言模型（LLM）中有所不同。</st>
    <st c="31575">但大多数知名的大型语言模型（如ChatGPT和Gemini）会将</st> *<st c="31644">ice cream</st>*
    <st c="31653">视为两个标记。</st> <st c="31669">在自然语言处理（NLP）的某些情况下，你可能认为它应该是一个标记，基于标记应该代表一个有用的语义处理单元的概念，但对于这些模型来说并非如此。</st>
    <st c="31866">这些模型。</st>
- en: <st c="31879">Fine-tuning can also be quite expensive, depending on the environment
    and resources you have available.</st> <st c="31984">In recent years, the costs
    for fine-tuning have come down substantially due to new techniques such as representative
    fine-tuning, LoRA-related techniques, and quantization.</st> <st c="32157">But
    in many RAG development efforts, fine-tuning is considered an additional cost
    to already expensive RAG efforts, so it is considered a more expensive addition
    to</st> <st c="32322">the efforts.</st>
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="31879">微调也可能相当昂贵，这取决于你拥有的环境和资源。</st> <st c="31984">近年来，由于代表性的微调、LoRA相关技术和量化等新技术，微调的成本大幅下降。</st>
    <st c="32157">但在许多RAG开发工作中，微调被视为对已经昂贵的RAG努力的额外成本，因此它被视为对努力的更昂贵补充。</st> <st c="32322">这些努力。</st>
- en: <st c="32334">Ultimately, when deciding between RAG and fine-tuning, consider
    your specific use case and requirements.</st> <st c="32440">RAG is</st> <st c="32446">generally
    superior for retrieving factual information that is not present in the LLM’s training
    data or is private.</st> <st c="32563">It allows you to dynamically integrate
    external knowledge</st> <st c="32621">without modifying the model’s weights.</st>
    <st c="32660">Fine-tuning, on the other hand, is more suitable for teaching the
    model specialized tasks or adapting it to a specific domain.</st> <st c="32787">Keep
    the limitations of context window sizes and the potential for overfitting in mind
    when fine-tuning a</st> <st c="32893">specific dataset.</st>
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="32334">最终，在决定选择RAG还是微调时，请考虑您的具体用例和需求。</st> <st c="32440">RAG通常在检索LLM训练数据中不存在或私有的事实信息方面更优越。</st>
    <st c="32563">它允许您在不修改模型权重的情况下动态集成外部知识。</st> <st c="32621">另一方面，微调更适合教授模型专业任务或将其适应特定领域。</st>
    <st c="32787">在微调特定数据集时，请记住上下文窗口大小的限制和过拟合的潜在可能性。</st>
- en: <st c="32910">Now that we have defined what RAG is, particularly when compared
    to other approaches that use generative AI, let’s review the general architecture
    of</st> <st c="33061">RAG systems.</st>
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="32910">既然我们已经定义了什么是RAG，尤其是与其他使用生成式AI的方法相比，让我们回顾一下</st> <st c="33061">RAG系统的通用架构。</st>
- en: <st c="33073">The architecture of RAG systems</st>
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="33073">RAG系统的架构</st>
- en: <st c="33105">The following are</st> <st c="33124">the stages of a RAG process
    from a</st> <st c="33159">user’s perspective:</st>
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="33105">以下是从</st> <st c="33124">用户的角度看RAG过程的</st> <st c="33159">各个阶段：</st>
- en: <st c="33178">A user enters</st> <st c="33193">a query/question.</st>
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="33178">用户输入</st> <st c="33193">一个查询/问题。</st>
- en: <st c="33210">The application thinks for a little while before checking the
    data it has access to so that it can see what is the</st> <st c="33326">most relevant.</st>
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="33210">应用程序会稍作思考，然后检查它所能访问的数据，以便确定什么是最相关的。</st>
- en: <st c="33340">The application provides a response that focuses on answering
    the user’s question, but using data that has been provided to it through the</st>
    <st c="33480">RAG pipeline.</st>
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="33340">应用程序提供响应，专注于回答用户的问题，但使用通过</st> <st c="33480">RAG管道</st> <st c="33527">提供给它</st>
    <st c="33536">的数据。</st>
- en: '<st c="33493">From a technical standpoint, this</st> <st c="33527">captures
    two of the stages you will code: the</st> **<st c="33574">retrieval</st>** <st
    c="33583">and</st> **<st c="33588">generation</st>** <st c="33598">stages.</st>
    <st c="33607">But</st> <st c="33610">there is one other stage, known</st> <st
    c="33643">as</st> **<st c="33646">indexing</st>**<st c="33654">, which can be
    and is often executed before the user enters the query.</st> <st c="33725">With
    indexing, you are turning supporting data into vectors, storing them in a vector
    database, and likely optimizing the search functionality so that the retrieval
    step is as fast and effective</st> <st c="33920">as possible.</st>'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="33493">从技术角度来看，这</st> <st c="33527">捕捉了您将编码的两个阶段：</st> **<st c="33574">检索</st>**
    <st c="33583">和</st> **<st c="33588">生成</st>** <st c="33598">阶段。</st> <st c="33607">但是</st>
    <st c="33610">还有一个其他阶段，被称为</st> <st c="33643">索引</st>**<st c="33654">，这可以在用户输入查询之前执行，并且通常是这样做的。</st>
    <st c="33725">通过索引，您将辅助数据转换为向量，将它们存储在向量数据库中，并可能优化搜索功能，以便检索步骤尽可能快且有效。</st> <st
    c="33920">尽可能快且有效。</st>
- en: <st c="33932">Once the user passes their query into the system, the following</st>
    <st c="33997">steps occur:</st>
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="33932">一旦用户将他们的查询传递到系统中，以下</st> <st c="33997">步骤就会发生：</st>
- en: <st c="34009">The user query</st> <st c="34025">is vectorized.</st>
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="34009">用户查询</st> <st c="34025">被向量化。</st>
- en: <st c="34039">The vectorized query is passed to a vector search to retrieve
    the most relevant data in a vector database representing your</st> <st c="34164">external
    data.</st>
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="34039">将向量查询传递到向量搜索中，以检索表示您</st> <st c="34164">外部数据</st> <st c="34178">的向量数据库中最相关的数据。</st>
- en: <st c="34178">The vector search returns the most relevant results and unique
    keys referencing the original content those</st> <st c="34286">vectors represent.</st>
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="34178">向量搜索返回最相关的结果和引用这些向量所代表原始内容的唯一键。</st>
- en: <st c="34304">The unique keys are used to pull out the original data associated
    with those vectors, often in a batch of</st> <st c="34411">multiple documents.</st>
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="34304">唯一的键用于提取与这些向量相关的原始数据，通常是在</st> <st c="34411">多个文档的批次中。</st>
- en: <st c="34430">The original data might be filtered or post-processed but will
    typically then be passed to an LLM based on what you expect the RAG process</st>
    <st c="34570">to do.</st>
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="34430">原始数据可能被过滤或后处理，但通常会传递给基于您期望RAG过程</st> <st c="34570">执行的操作的LLM。</st>
- en: <st c="34576">The LLM is provided with a prompt that generally says something
    like “</st>`<st c="34647">You are a helpful assistant for question-answering tasks.</st>
    <st c="34706">Take the following question (the user query) and use this helpful
    information (the data retrieved in the similarity search) to answer it.</st> <st
    c="34844">If you don't know the answer based on the information provided, just
    say you</st>` `<st c="34921">don't know.</st>`<st c="34932">”</st>
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="34576">LLM被提供了一个提示，通常会说类似“</st>`<st c="34647">您是一个问答任务的助手。</st> <st c="34706">使用以下问题（用户查询）并使用此有用的信息（相似性搜索中检索到的数据）来回答它。</st>
    <st c="34844">如果您根据提供的信息不知道答案，只需说您</st>` `<st c="34921">不知道。</st>`<st c="34932">”</st>
- en: <st c="34934">The LLM processes that prompt and provides a response based on
    the external data</st> <st c="35015">you provided.</st>
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="34934">LLM处理这些提示并根据您提供的</st> <st c="35015">外部数据</st> <st c="35028">提供响应。</st>
- en: <st c="35028">Depending on the scope of the RAG system, these steps can be done
    in real time, or steps such as</st> <st c="35126">indexing can be done before
    the query so that it is ready to be searched when the</st> <st c="35208">time
    comes.</st>
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="35028">根据RAG系统的范围，这些步骤可以实时完成，或者像</st> <st c="35126">索引</st> <st c="35134">这样的步骤可以在查询之前完成，以便在需要时可以立即搜索。</st>
    <st c="35208">需要时可以立即搜索。</st>
- en: <st c="35219">As mentioned previously, we can break these aspects down into
    three main stages (see</st> *<st c="35305">Figure 1</st>**<st c="35313">.2</st>*<st
    c="35315">):</st>
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="35219">如前所述，我们可以将这些方面分解为三个主要阶段（见</st> *<st c="35305">图1</st>**<st c="35313">.2</st>*<st
    c="35315">）：</st>
- en: <st c="35318">Indexing</st>
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="35318">索引</st>
- en: <st c="35327">Retrieval</st>
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="35327">检索</st>
- en: <st c="35337">Generation</st>
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="35337">生成</st>
- en: '![Figure 1.2 – The three stages of RAG](img/B22475_01_02.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图1.2 – RAG的三个阶段](img/B22475_01_02.jpg)'
- en: <st c="35430">Figure 1.2 – The three stages of RAG</st>
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="35430">图1.2 – RAG的三个阶段</st>
- en: <st c="35466">As described</st> <st c="35479">previously, these three stages
    make up the overall user pattern and design of a general RAG system.</st> <st
    c="35580">In</st> [*<st c="35583">Chapter 4</st>*](B22475_04.xhtml#_idTextAnchor078)<st
    c="35592">, we will dive much deeper into understanding these stages.</st> <st
    c="35652">This will help you tie the concepts of this coding paradigm with their</st>
    <st c="35723">actual implementation.</st>
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="35466">正如之前所描述的</st> <st c="35479">，这三个阶段构成了通用RAG系统的整体用户模式和设计。</st> <st
    c="35580">在</st> [*<st c="35583">第四章</st>*](B22475_04.xhtml#_idTextAnchor078)<st
    c="35592">中，我们将更深入地探讨这些阶段。</st> <st c="35652">这将帮助您将这一编码范式中的概念与它们的</st> <st c="35723">实际实现联系起来。</st>
- en: <st c="35745">Summary</st>
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="35745">总结</st>
- en: <st c="35753">In this chapter, we explored RAG and its ability to enhance the
    capabilities of LLMs by integrating them with an organization’s internal data.</st>
    <st c="35897">We learned how RAG combines the power of LLMs with a company’s private
    data, enabling the model to utilize information it was not originally trained
    on, making the LLM’s outputs more relevant and valuable for the specific organization.</st>
    <st c="36133">We also discussed the advantages of RAG, such as improved accuracy
    and relevance, customization to a company’s domain, flexibility in data sources
    used, and expansion of the model’s knowledge beyond its original training data.</st>
    <st c="36360">Additionally, we examined the challenges and limitations of RAG,
    including dependency on data quality, the need for data cleaning, added computational
    overhead and complexity, and the potential for information overload if not</st>
    <st c="36586">properly filtered.</st>
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="35753">在本章中，我们探讨了RAG及其通过整合组织内部数据来增强大型语言模型（LLM）能力的能力。</st> <st c="35897">我们学习了RAG如何结合LLM的力量和公司的私有数据，使模型能够利用其最初训练时未使用的信息，从而使LLM的输出对特定组织更加相关和有价值。</st>
    <st c="36133">我们还讨论了RAG的优势，例如提高准确性和相关性、针对公司领域的定制化、数据源使用的灵活性以及将模型的知识扩展到其原始训练数据之外。</st>
    <st c="36360">此外，我们还考察了RAG的挑战和局限性，包括对数据质量的依赖、数据清洗的需求、增加的计算开销和复杂性，以及如果未</st> <st
    c="36586">适当过滤，可能导致的信息过载。</st>
- en: <st c="36604">Midway through this chapter, we defined key vocabulary terms and
    emphasized the critical importance of understanding vectors.</st> <st c="36731">We
    explored examples of how RAG is being implemented across industries for various
    applications and compared RAG to conventional generative AI and</st> <st c="36878">model
    fine-tuning.</st>
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="36604">在本章的中间部分，我们定义了关键词汇，并强调了理解向量的重要性。</st> <st c="36731">我们探讨了RAG在各个行业中的应用实例，并将其与传统生成式AI和</st>
    <st c="36878">模型微调进行了比较。</st>
- en: <st c="36896">Finally, we outlined the architecture and stages of a typical
    RAG pipeline from both the user’s perspective and a technical standpoint while
    covering the indexing, retrieval, and generation stages of the RAG pipeline.</st>
    <st c="37115">In the next chapter, we will walk through these stages using an
    actual</st> <st c="37186">coding example.</st>
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="36896">最后，我们从用户视角和技术角度概述了典型RAG管道的架构和阶段，同时涵盖了RAG管道的索引、检索和生成阶段。</st> <st
    c="37115">在下一章中，我们将通过实际的</st> <st c="37186">编码示例来逐步讲解这些阶段。</st>
