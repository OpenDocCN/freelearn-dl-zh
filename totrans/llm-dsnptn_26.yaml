- en: '26'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Retrieval-Augmented Generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Retrieval-augmented generation** (**RAG**) is a technique that enhances the
    performance of AI models, particularly in tasks that require knowledge or data
    not contained within the model’s pre-trained parameters. It combines the strengths
    of both retrieval-based models and generative models. The retrieval component
    fetches relevant information from external sources, such as databases, documents,
    or web content, and the generative component uses this information to produce
    more accurate, contextually enriched responses.'
  prefs: []
  type: TYPE_NORMAL
- en: RAG is implemented by integrating a retrieval mechanism with a language model.
    The process begins by querying a knowledge base or external resource for relevant
    documents or snippets. These retrieved pieces of information are then fed into
    the language model, which generates a response by incorporating both the prompt
    and the retrieved data. This approach improves the model’s ability to answer questions
    or solve problems with up-to-date or domain-specific information that it would
    otherwise lack.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll introduce you to RAG. You’ll learn how to implement a
    simple RAG system that can enhance LLM outputs with relevant external information.
  prefs: []
  type: TYPE_NORMAL
- en: The key benefits of RAG include enhanced factual accuracy, access to current
    information, improved domain-specific knowledge, and reduced hallucination in
    LLM outputs.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll cover embedding and indexing techniques of vector databases
    for efficient retrieval, query formulation strategies, and methods for integrating
    retrieved information with LLM generation. By the end of this chapter, you’ll
    be able to implement basic RAG systems to augment your LLMs with external knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Building a simple RAG system for LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embedding and indexing techniques for LLM retrieval
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Query formulation strategies in LLM-based RAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating retrieved information with LLM generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges and opportunities in RAG for LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a simple RAG system for LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section provides a practical illustration of a simple RAG system, leveraging
    the robust search capabilities of **SerpApi**, the semantic understanding of sentence
    embeddings, and the generative prowess of OpenAI’s GPT-4o model. SerpApi is a
    web scraping API that provides real-time access to search engine results, offering
    structured data for Google, Bing, and other platforms without the need for manual
    scraping.
  prefs: []
  type: TYPE_NORMAL
- en: Through this example, we will explore the fundamental components of a RAG system,
    including query-based web searching, snippet extraction and ranking, and, ultimately,
    the generation of a comprehensive answer using a state-of-the-art LLM, highlighting
    the interplay between these elements in a step-by-step manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for the simple RAG system we’ll be building contains the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SerpApi**: To find relevant web pages based on the user’s query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence embeddings**: To extract the most relevant snippets from the search
    results using sentence embeddings and cosine similarity. Sentence embeddings are
    dense numerical representations of text that capture semantic meaning by mapping
    words, phrases, or entire sentences into high-dimensional vector space, where
    similar meanings are positioned closer together. Cosine similarity measures the
    angle between these embedding vectors (ranging from -1 to 1), rather than their
    magnitude, making it an effective way to evaluate semantic similarity regardless
    of text length; when two embeddings have a cosine similarity close to 1, they’re
    highly similar in meaning, while values closer to 0 indicate unrelated content
    and negative values suggest opposing meanings. This combination of techniques
    powers many modern **natural language processing** (**NLP**) applications, from
    search engines and recommendation systems to language translation and content
    clustering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenAI’s GPT-4o**: To generate a comprehensive and coherent answer based
    on the retrieved snippets (context) and the original query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, let’s install the following dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding command, we install `serpapi` for searching, `sentence_transformers`
    for embedding, and `openai` for accessing GPT-4o.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let us see how a complete RAG system is implemented using search APIs,
    embeddings, and an LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first import the installed libraries along with `torch` for tensor operations.
    The code snippet also sets up API keys for SerpApi and OpenAI. Remember to replace
    the placeholders with your actual API keys:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then initialize the search engine and Sentence Transformer. The following
    code defines the search function to perform a Google search using SerpApi and
    initializes the Sentence Transformer model (`all-mpnet-base-v2`) for creating
    sentence embeddings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we retrieve relevant snippets. We define the `retrieve_snippets` function,
    which takes the search results, extracts snippets, computes their embeddings,
    and calculates the cosine similarity between the query embedding and each snippet
    embedding. It then returns the top *k* snippets that are most similar to the query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then define the `generate_answer` function to generate an answer using GPT-4o.
    This is the core of the generation part of our RAG system:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This function constructs a structured prompt for an LLM to generate an answer
    constrained strictly to a given context. It formats the conversation as a system-user
    message pair, instructing the model to act as a subject matter expert and restrict
    its answer to the supplied information, explicitly avoiding speculation. If the
    information isn’t present, the system is directed to return a fallback message
    indicating that the answer couldn’t be found. The query and context are embedded
    directly into the user message, and the LLM (in this case, `gpt-4o`) is queried
    with a moderate creativity level via `temperature=0.7` and a response length cap
    of `256` tokens. This design makes the function reliable for context-grounded
    Q&A tasks, particularly in RAG pipelines or constrained-answering settings such
    as document QA or compliance tools.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here’s the main RAG function and example usage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This code defines the `rag_system` function, which orchestrates the entire
    process: searching, retrieving snippets, and generating an answer. It then demonstrates
    how to use `rag_system` with an example query, printing the generated answer to
    the console'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `rag_system` function answers a query by first searching for relevant information
    using `search(query)` and then extracting relevant snippets through the API called
    `retrieve_snippets(query, search_results)`. If no snippets are found, it returns
    a message indicating no information was found. If snippets are available, they
    are combined into a single context string and used to generate an answer through
    `generate_answer(query, context)`. Finally, the function returns the generated
    answer based on the context. In the example usage, the function is called with
    the query `"What are the latest advancements in quantum computing?"` and will
    return a generated response based on the relevant search results. In real production
    systems, we should implement retries and error handling around `retrieve_snippets`
    API calls.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Before we move on to the next section, here are some things to remember:'
  prefs: []
  type: TYPE_NORMAL
- en: '**API keys**: Make sure you have valid API keys for both SerpApi and OpenAI
    and have replaced the placeholders in the code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenAI costs**: Be mindful of OpenAI API usage costs. GPT-4o can be more
    expensive than other models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt engineering**: The quality of the generated answer heavily depends
    on the prompt you provide to GPT-4o. You might need to experiment with different
    prompts to get the best results. Consider adding instructions about the desired
    answer format, length, or style.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`try-except` blocks) to handle potential issues such as network problems, API
    errors, or invalid inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advanced techniques**: This is a basic RAG system. You can improve it further
    by doing the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Better snippet selection**: Consider factors such as source diversity, factuality,
    and snippet length'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterative retrieval**: Retrieve more context if the initial answer is not
    satisfactory'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-tuning**: Fine-tune a smaller, more specialized language model on your
    specific domain for potentially better performance and lower costs'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We’ve successfully built a simple RAG system, covering the core components
    of retrieval and generation. Now that we have a functional RAG system, let’s dive
    deeper into the crucial techniques that enable efficient retrieval from large
    datasets: embedding and indexing. We’ll explore different methods for representing
    text semantically and organizing these representations for fast similarity search.'
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings and indexing for retrieval in LLM applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Embedding and indexing techniques provide efficient and effective retrieval
    in RAG-based LLM applications. They allow LLMs to quickly find and utilize relevant
    information from vast amounts of data. The following subsections provide a breakdown
    of common techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Embeddings are numerical vector representations of data, such as text, images,
    or audio, that map complex, high-dimensional data into a continuous vector space
    where similar items are positioned close to each other. These vectors capture
    the underlying patterns, relationships, and semantic properties of the data, making
    it easier for machine learning models to understand and process. For text, for
    example, word embeddings transform words or phrases into dense vectors that represent
    their meaning in a way that reflects semantic relationships, such as synonyms
    being closer together in the vector space. Embeddings are typically learned from
    large datasets through techniques such as neural networks, and they serve as a
    foundation for tasks such as information retrieval, classification, clustering,
    and recommendation systems. By reducing the dimensionality of data while preserving
    important features, embeddings enable models to generalize better and make sense
    of varied input data efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: For LLMs, text embeddings are most relevant. They are generated by passing text
    through a neural network (like the Sentence Transformer models we used in the
    previous section).
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need embeddings?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Embeddings are important for RAG applications for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Semantic search**: Embeddings enable semantic search, where you find information
    based on meaning rather than just keyword matching'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contextual understanding**: LLMs can use embeddings to understand the relationships
    between different pieces of information, improving their ability to reason and
    generate relevant responses'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficient retrieval**: When combined with appropriate indexing, embeddings
    allow for the fast retrieval of relevant information from large datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common embedding technologies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Several embedding technologies are commonly used in RAG systems, and these
    vary in their underlying models, methods, and suitability for different applications.
    Here are some prominent embedding technologies for RAG:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-trained transformer-based embeddings (e.g., BERT, RoBERTa, and T5)**:
    Transformer models such as **Bidirectional Encoder Representations from Transformers**
    (**BERT**) and its variants, such as RoBERTa and T5, have been widely used to
    generate dense, contextual embeddings for text. These models are fine-tuned on
    large corpora and capture a rich understanding of language semantics. In a RAG
    system, these embeddings can be used to retrieve relevant passages from a document
    store based on semantic similarity. The embeddings are typically high-dimensional
    and are generated by feeding text through the transformer model to produce a fixed-size
    vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence-BERT (SBERT)**: A variation of BERT designed for sentence-level
    embeddings, SBERT focuses on optimizing the model for tasks such as semantic textual
    similarity and clustering. It uses a Siamese network architecture to map sentences
    into a dense vector space where semantically similar sentences are closer together.
    This makes it particularly effective for tasks such as information retrieval in
    RAG, where retrieving semantically relevant passages from a large corpus is essential.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Facebook AI Similarity Search (Faiss)**: Faiss is a library developed by
    Facebook AI Research that provides efficient similarity search through **approximate
    nearest neighbor** (**ANN**) search. Faiss is not an embedding technology by itself
    but works in conjunction with various embedding models to index and search over
    large collections of vectors. When used in RAG, Faiss enables the fast retrieval
    of relevant documents or passages by comparing the similarity of their embeddings
    against a query embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dense retriever models (e.g., DPR and ColBERT)**: **Dense Passage Retrieval**
    (**DPR**) is an approach to information retrieval that uses two separate encoders
    (usually BERT-based models) to encode both queries and passages into dense vectors.
    DPR outperforms traditional sparse retrieval methods by leveraging the contextual
    knowledge encoded in dense embeddings. ColBERT, on the other hand, is another
    dense retrieval model that balances the efficiency of dense retrieval and the
    effectiveness of traditional methods. These models are especially useful for RAG
    when retrieving high-quality passages that are semantically related to a query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contrastive Language-Image Pre-Training (CLIP)**: While originally designed
    for multimodal applications (text and image), CLIP has been adapted for text-only
    tasks as well. It learns embeddings by aligning text and image data in a shared
    vector space. Although CLIP is primarily used for multimodal tasks, its ability
    to represent language in a common space with images provides a flexible embedding
    framework that can be used in RAG, especially when working with multimodal data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep semantic similarity models (e.g., USE and InferSent)**: Models such
    as the **Universal Sentence Encoder** (**USE**) and InferSent generate sentence
    embeddings by capturing deeper semantic meaning, which can be used for various
    NLP tasks, including document retrieval. These models produce fixed-size vector
    representations that can be compared for similarity, making them useful for RAG
    when paired with retrieval systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Doc2Vec**: An extension of Word2Vec, Doc2Vec generates embeddings for entire
    documents rather than individual words. It maps variable-length text into a fixed-size
    vector, which can be used to retrieve semantically similar documents or passages.
    Though not as powerful as transformer-based models in terms of semantic richness,
    Doc2Vec is still an effective tool for more lightweight retrieval tasks in RAG
    applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embedding-based search engines (e.g., Elasticsearch with dense vectors)**:
    Some modern search engines, such as Elasticsearch, have integrated support for
    dense vectors alongside traditional keyword-based indexing. Elasticsearch can
    store and retrieve text embeddings, allowing for more flexible and semantically
    aware searches. When used with RAG, these embeddings can be used to rank documents
    by their relevance to a query, improving retrieval performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenAI embeddings (e.g., GPT-based models)**: OpenAI’s embeddings, derived
    from models such as GPT-3, are also used for RAG tasks. These embeddings are based
    on the language model’s ability to generate high-quality text representations,
    which can be indexed and searched over large corpora. While they are not as specifically
    tuned for retrieval as some other models (such as DPR), they are highly flexible
    and can be used in general-purpose RAG applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These embedding technologies provide various advantages depending on the specific
    requirements of a RAG system, such as retrieval speed, model accuracy, and the
    scale of the data being processed. Each of them can be fine-tuned and optimized
    for specific use cases, and the choice of embedding technology will depend on
    factors such as the nature of the documents being retrieved, computational resources,
    and latency requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Indexing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Indexing is the process of organizing embeddings in a data structure that allows
    for fast similarity search. Think of it like the index of a book, but for vectors
    instead of words.
  prefs: []
  type: TYPE_NORMAL
- en: Using a more detailed description using LLM terminology, vector indexing technologies
    optimize embedding storage and retrieval by creating specialized data structures
    that organize high-dimensional vectors according to their similarity relationships,
    rather than sequential order. These structures—whether graph-based (connecting
    similar vectors through navigable pathways), tree-based (recursively partitioning
    the vector space), or quantization-based (compressing vectors while preserving
    similarity)—all serve the fundamental purpose of transforming an otherwise prohibitively
    expensive exhaustive search into a manageable process by strategically limiting
    the search space, enabling vector databases to handle billions of embeddings with
    sub-second query times while maintaining an acceptable trade-off between speed,
    memory efficiency, and result accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Why is indexing important?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Indexing is important for LLMs for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Speed**: Without indexing, you would have to compare a query embedding to
    every single embedding in your dataset, which is computationally expensive and
    slow'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Indexing allows LLM applications to scale to handle massive
    datasets containing millions or even billions of data points'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common indexing techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s look at some of the common indexing techniques for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a visual diagram of these index techniques, I recommend you check out the
    following website: [https://kdb.ai/learning-hub/articles/indexing-basics/](https://kdb.ai/learning-hub/articles/indexing-basics/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Flat index (****brute force)**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How it works**: Stores all embeddings in a simple list or array. During a
    search, it calculates the distance (e.g., cosine similarity) between the query
    embedding and every embedding in the index.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pros**: Simple to implement and perfect accuracy (finds the true nearest
    neighbors).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**: Slow and computationally expensive for large datasets, as it requires
    an exhaustive search.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Suitable for**: Very small datasets or when perfect accuracy is an absolute
    requirement.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inverted file** **index (IVF)**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How** **it works**:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering**: Divides the embedding space into clusters using algorithms
    such as *k*-means'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inverted index**: Creates an inverted index that maps each cluster centroid
    to a list of the embeddings belonging to that cluster'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Search**:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finds the nearest cluster centroid(s) to the query embedding
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Only searches within those clusters, significantly reducing the search space
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pros**: Faster than a flat index; relatively simple to implement'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**: Approximate (might not always find the true nearest neighbors); accuracy
    depends on the number of clusters'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Suitable for**: Medium-sized datasets where a good balance between speed
    and accuracy is needed'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchical navigable small** **world (HNSW)**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How** **it works**:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graph-based**: Constructs a hierarchical graph where each node represents
    an embedding.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layers**: The graph has multiple layers, with the top layer having long-range
    connections (for faster traversal) and the bottom layer having short-range connections
    (for accurate search).'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Search**: Starts at a random node in the top layer and greedily moves towards
    the query embedding by exploring connections. The search progresses down the layers,
    refining the results.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pros**: Very fast and accurate; often considered the state of the art for
    ANN search'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**: More complex to implement than IVF, and higher memory overhead due
    to the graph structure'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Suitable for**: Large datasets where both speed and accuracy are crucial'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Product** **quantization (PQ)**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How** **it works**:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Subvectors**: Divides each embedding into multiple subvectors.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Codebooks**: Creates separate codebooks for each subvector using clustering.
    Each codebook contains a set of representative subvectors (centroids).'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encoding**: Encodes each embedding by replacing its subvectors with the closest
    centroids from the corresponding codebooks. This creates a compressed representation
    of the embedding.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Search**: Calculates the approximate distance between the query and the encoded
    embeddings using pre-computed distances between the query’s subvectors and the
    codebook centroids.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pros**: Significantly reduces memory usage by compressing embeddings; fast
    search.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**: Approximate, and accuracy depends on the number of subvectors and
    the size of the codebooks.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Suitable for**: Very large datasets where memory efficiency is a primary
    concern.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Locality sensitive** **hashing (LSH)**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How it works**: Uses hash functions to map similar embeddings to the same
    “bucket” with high probability'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pros**: Relatively simple; can be distributed across multiple machines'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**: Approximate, and performance depends on the choice of hash functions
    and the number of buckets'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Suitable for**: Very large, high-dimensional datasets'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve covered different indexing methods, let’s introduce some popular
    libraries and tools that implement these indexing techniques, making them easier
    to use in practice. This will provide a practical perspective on how to leverage
    these technologies in your RAG applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some libraries and tools for implementing indexing:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Faiss**: A highly optimized library developed by Facebook AI for efficient
    similarity search and clustering of dense vectors. It implements many of the indexing
    techniques mentioned previously (flat, IVF, HNSW, and PQ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Approximate Nearest Neighbors Oh Yeah (Annoy)**: Another popular library
    for ANN search, known for its ease of use and good performance. It uses a tree-based
    approach.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalable Nearest Neighbors (ScaNN)**: A library developed by Google, designed
    for large-scale, high-dimensional datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vespa.ai**: Provide tools to query, organize, and make inferences in vectors,
    tensors, text, and structured data. It is used by [https://www.perplexity.ai/](https://www.perplexity.ai/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pinecone, Weaviate, Milvus, Qdrant**: Vector databases designed specifically
    for storing and searching embeddings. They handle indexing, scaling, and other
    infrastructure concerns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The best embedding and indexing techniques for your LLM application will depend
    on several factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset size**: For small datasets, a flat index might be sufficient. For
    large datasets, consider HNSW, IVF, or PQ.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speed requirements**: If low latency is critical, HNSW is generally the fastest
    option.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accuracy requirements**: If perfect accuracy is required, a flat index is
    the only choice, but it’s not scalable. HNSW often provides the best accuracy
    among approximate methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory constraints**: If memory is limited, PQ can significantly reduce storage
    requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Development effort**: Faiss and Annoy offer a good balance between performance
    and ease of implementation. Vector databases simplify infrastructure management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By carefully considering these factors and understanding the strengths and weaknesses
    of each technique and library, you can choose the most appropriate embedding and
    indexing methods to build efficient and effective LLM applications.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll now demonstrate an example involving embedding, indexing, and searching
    using Faiss, a powerful library for efficient similarity search. I’ll use the
    `all-mpnet-base-v2` Sentence Transformer model to generate embeddings. Since the
    code will be more than 20 lines, I’ll break it down into blocks with explanations
    preceding each block.
  prefs: []
  type: TYPE_NORMAL
- en: Example code demonstrating embedding, indexing, and searching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we’ll be showing the code for a typical workflow for using
    embeddings and indexing to enable fast similarity search within a collection of
    text documents: Here’s what it does:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Loads a Sentence Transformer model**: Initializes a pre-trained model for
    generating sentence embeddings.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Creates sample data**: Defines a list of example sentences (you would replace
    this with your actual data).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`SentenceTransformer` to create embeddings for each sentence.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`IndexFlatL2` for a flat L2 distance index in this example) to store the embeddings.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Adds embeddings to the index**: Adds the generated embeddings to the Faiss
    index.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Defines a search query**: Sets a sample query for which we want to find similar
    sentences.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Encodes the query**: Creates an embedding for the search query using the
    same Sentence Transformer model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Performs a search**: Uses the Faiss index to search for the *k* most similar
    embeddings to the query embedding.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prints the results**: Displays the indices and distances of the *k* nearest
    neighbors found in the index.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Before we check out the code, let us install the following dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us now see the code example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the necessary libraries—`sentence_transformers` for creating
    embeddings and `faiss` for indexing and searching—and load the `all-mpnet-base-v2`
    Sentence Transformer model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then prepare the data by defining some sample sentences (you can replace
    these with your actual data) and then use the Sentence Transformer model to generate
    embeddings for each sentence (the embeddings are converted to float32, which is
    required by Faiss):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then create a Faiss index and add the embeddings to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we’re using IndexFlatL2, which is a flat index that uses L2 distance (Euclidean
    distance) for similarity comparisons. This type of index provides accurate results
    but can be slow for very large datasets. The index is created with the correct
    dimensionality (`768` for this Sentence Transformer model).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we define a sample search query and encode it into an embedding using
    the same Sentence Transformer model. The query embedding is also converted to
    float32:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we perform similarity search using the `index.search()` method. We
    search for the two most similar sentences (*k*=2). The method returns the distances
    and the indices of the nearest neighbors. We then print the indices and distances
    of the nearest neighbors found:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is sample output you might get from running the preceding code
    blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This demonstrates how semantic similarity search works using Sentence Transformers
    and Faiss. Note that actual numbers will vary depending on the hardware, model
    versions, and runtime conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s what’s happening.
  prefs: []
  type: TYPE_NORMAL
- en: The query `"What is the dog doing?"` is embedded and compared against all embedded
    sentences in the list. Faiss retrieves the two most semantically similar sentences
    based on Euclidean (L2) distance in the embedding space. The smallest distance
    indicates the highest similarity. In this example, the sentence about the man
    walking his dog is closest to the query, which makes sense semantically.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re running this on your machine, your values may look different due to
    the non-determinism in model initialization and floating-point precision, but
    the closest sentence should consistently be the one most semantically related
    to the query.
  prefs: []
  type: TYPE_NORMAL
- en: Important
  prefs: []
  type: TYPE_NORMAL
- en: '`IndexIVFFlat` or `IndexHNSWFlat` from Faiss, to improve search speed.'
  prefs: []
  type: TYPE_NORMAL
- en: '`faiss-gpu` to significantly speed up indexing and searching.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data preprocessing**: For real-world applications, you might need to perform
    additional data preprocessing steps, such as lowercasing, removing punctuation,
    or stemming/lemmatization, depending on your specific needs and the nature of
    your data.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distance metrics**: Faiss supports different distance metrics. We used L2
    distance here, but you could also use the inner product (IndexFlatIP) or other
    metrics depending on how your embeddings are generated and what kind of similarity
    you want to measure.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vector databases**: For production-level systems, consider using a dedicated
    vector database such as Pinecone, Weaviate, or Milvus to manage your embeddings
    and indexes more efficiently. They often provide features such as automatic indexing,
    scaling, and data management, which simplify the deployment of similarity search
    applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve covered the fundamentals of embeddings, indexing, and searching with
    Faiss, along with important considerations for real-world implementation. Now,
    let’s turn our attention to another crucial aspect of RAG: query formulation.
    We’ll explore various strategies to refine and expand user queries, ultimately
    leading to more effective information retrieval from the knowledge base.'
  prefs: []
  type: TYPE_NORMAL
- en: Query formulation strategies in LLM-based RAG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Query formulation strategies in LLM-based RAG systems aim to enhance retrieval
    by improving the expressiveness and coverage of user queries. Common expansion
    strategies include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Synonym and paraphrase expansion**: This involves generating semantically
    equivalent alternatives using LLMs or lexical resources. For example, expanding
    “climate change impact” to include “effects of global warming” or “environmental
    consequences of climate change” can help match a broader range of documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contextual reformulation**: LLMs can reinterpret queries by inferring their
    intent based on conversational or document context. This helps in tailoring the
    query to better align with how the information might be expressed in the knowledge
    base.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pseudo-relevance feedback**: Also known as blind relevance feedback, this
    strategy involves running an initial query, analyzing the top-ranked documents
    for salient terms, and using these terms to expand the query. While effective,
    it requires safeguards against topic drift.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Template-based augmentation**: Useful in structured domains, this method
    uses domain-specific templates or patterns to systematically generate variants.
    For example, a medical query about “treatment for hypertension” might also include
    “hypertension therapy” or “managing high blood pressure.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Entity and concept linking**: Named entities and domain concepts in the query
    are identified and replaced or augmented with their aliases, definitions, or hierarchical
    relations. This is often guided by ontologies or knowledge graphs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt-based query rewriting**: With LLMs, prompts can be crafted to explicitly
    instruct the model to generate reformulated queries. This is particularly useful
    in multilingual or multi-domain RAG systems, where queries need to be adapted
    to match the style and vocabulary of the target corpus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each strategy contributes differently to recall and precision. Choosing or combining
    them depends on the structure and variability of the underlying knowledge base.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, the `QueryExpansionRAG` implementation uses a prompt-based
    query rewriting strategy powered by a pre-trained sequence-to-sequence language
    model (specifically, T5-small). This approach instructs the model to generate
    alternative phrasings of the input query by prefixing the prompt with `"expand
    query:"`. The generated expansions reflect paraphrastic reformulation, where the
    model synthesizes semantically related variations to increase retrieval coverage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This code defines a `QueryExpansionRAG` class that extends a RAG framework by
    incorporating query expansion using a pre-trained T5 model. When a user submits
    a query, the `expand_query` method uses the T5 model through a text-to-text generation
    pipeline to produce multiple alternative phrasings of the query, which are then
    combined with the original query. The `retrieve` method iterates over these expanded
    queries, retrieving documents for each one and aggregating the results while removing
    duplicates. This approach increases the chances of retrieving relevant content
    by broadening the lexical and semantic scope of the original query, making it
    especially effective when the knowledge base expresses information in varied ways.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that poorly expanded queries can introduce noise and reduce retrieval
    precision. In this implementation, expansions generated by the T5 model are combined
    with the original query, increasing coverage. However, to maintain a balance,
    consider reranking results using similarity scores or assigning lower weights
    to generated expansions during retrieval. This helps ensure that expansions improve
    recall without compromising the alignment with the original intent.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve seen how query expansion can enhance retrieval in RAG systems, but it’s
    essential to manage the trade-off between recall and precision. Now, let’s shift
    our focus to the other side of the RAG pipeline: integrating the retrieved information
    with the LLM to generate the final answer. We’ll explore how to craft prompts
    that effectively leverage the retrieved context.'
  prefs: []
  type: TYPE_NORMAL
- en: Integrating retrieved information with LLM generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To integrate retrieved information with LLM generation, we can create a prompt
    that incorporates the retrieved documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, the `GenerativeRAG` class extends a RAG pipeline
    by integrating a causal language model for answer generation. It inherits from
    `QueryExpansionRAG`, which already provides retrieval functionality, and adds
    a generator component using Hugging Face’s `AutoModelForCausalLM`. In the constructor,
    it initializes the generator model and tokenizer based on the given model name.
    The `generate_response` method first retrieves relevant documents for a given
    query, concatenates them into a single context string, and constructs a prompt
    that combines this context with the question. This prompt is then tokenized and
    passed into the language model, which generates a text continuation as the answer.
    The final output is obtained by decoding the generated tokens into a string. This
    modular structure separates the retrieval and generation steps, making it easy
    to scale or replace individual components depending on the task or model performance
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Having covered the basics of RAG systems, we will now focus on real-world challenges,
    such as scalability, dynamic updates, and multilingual retrieval. Specifically,
    we will discuss how a sharded indexing architecture can improve retrieval efficiency
    at scale, highlighting its impact on performance in data-heavy environments.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges and opportunities in RAG for LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some key challenges and opportunities in RAG include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability**: Efficiently handling very large knowledge bases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic knowledge updating**: Keeping the knowledge base current.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross-lingual RAG**: Retrieving and generating in multiple languages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-modal RAG**: Incorporating non-text information in retrieval and generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep in mind that cross-lingual and multi-modal RAG will need specialized retrieval
    pipelines or adapters because standard retrieval approaches often struggle with
    semantic matching across languages or modalities, requiring dedicated components
    that can properly encode, align, and retrieve relevant information regardless
    of the source language or format while maintaining contextual understanding and
    relevance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Explainable RAG**: Providing transparency in the retrieval and generation
    process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To keep this chapter from becoming too long, in this section, we will only
    show an example on how to address the scalability challenge by implementing a
    sharded index. A sharded index refers to a distributed data structure that partitions
    the index into multiple smaller, manageable segments called shards, each stored
    and maintained independently across different nodes or storage units. This approach
    enables parallel processing, reduces lookup time, and mitigates bottlenecks associated
    with centralized indexing, making it suitable for handling large-scale datasets
    or high query volumes commonly encountered in AI applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the scalability is handled by dividing the knowledge
    base into multiple smaller indexes, or shards, each containing a portion of the
    overall data. This approach reduces the computational and memory burden on any
    single index and allows retrieval operations to remain efficient even as the dataset
    grows. During a query, the system embeds the query once, searches across all shards
    independently, and then merges the results. This design avoids bottlenecks that
    would arise from searching a single large index and makes it feasible to scale
    to much larger knowledge bases. It also lays the groundwork for further optimizations,
    such as parallelizing shard queries or distributing them across multiple machines.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RAG is a powerful technique for enhancing LLMs with external knowledge. By implementing
    the strategies and techniques discussed in this chapter, you can create more informed
    and accurate language models capable of accessing and utilizing vast amounts of
    information.
  prefs: []
  type: TYPE_NORMAL
- en: As we move forward, the next chapter will explore graph-based RAG for LLMs,
    which extends the RAG concept to leverage structured knowledge representations.
    This will further enhance the ability of LLMs to reason over complex relationships
    and generate more contextually appropriate responses.
  prefs: []
  type: TYPE_NORMAL
