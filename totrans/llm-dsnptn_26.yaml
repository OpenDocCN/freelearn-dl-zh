- en: '26'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '26'
- en: Retrieval-Augmented Generation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检索增强生成
- en: '**Retrieval-augmented generation** (**RAG**) is a technique that enhances the
    performance of AI models, particularly in tasks that require knowledge or data
    not contained within the model’s pre-trained parameters. It combines the strengths
    of both retrieval-based models and generative models. The retrieval component
    fetches relevant information from external sources, such as databases, documents,
    or web content, and the generative component uses this information to produce
    more accurate, contextually enriched responses.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**检索增强生成**（**RAG**）是一种增强AI模型性能的技术，尤其是在需要模型预训练参数中不包含的知识或数据的任务中。它结合了基于检索的模型和生成模型的优点。检索组件从外部来源，如数据库、文档或网络内容中检索相关信息，而生成组件则使用这些信息生成更准确、更具情境丰富性的响应。'
- en: RAG is implemented by integrating a retrieval mechanism with a language model.
    The process begins by querying a knowledge base or external resource for relevant
    documents or snippets. These retrieved pieces of information are then fed into
    the language model, which generates a response by incorporating both the prompt
    and the retrieved data. This approach improves the model’s ability to answer questions
    or solve problems with up-to-date or domain-specific information that it would
    otherwise lack.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: RAG是通过将检索机制与语言模型集成来实现的。这个过程从查询知识库或外部资源以获取相关文档或片段开始。然后，这些检索到的信息被输入到语言模型中，通过结合提示和检索到的数据生成响应。这种方法提高了模型回答问题或解决问题的能力，特别是对于它原本缺乏的更新或特定领域的信息。
- en: In this chapter, we’ll introduce you to RAG. You’ll learn how to implement a
    simple RAG system that can enhance LLM outputs with relevant external information.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将向您介绍RAG。您将学习如何实现一个简单的RAG系统，该系统可以使用相关的外部信息增强LLM的输出。
- en: The key benefits of RAG include enhanced factual accuracy, access to current
    information, improved domain-specific knowledge, and reduced hallucination in
    LLM outputs.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: RAG的关键优势包括增强的事实准确性、获取最新信息、改进的特定领域知识，以及减少LLM输出中的幻觉。
- en: In this chapter, we’ll cover embedding and indexing techniques of vector databases
    for efficient retrieval, query formulation strategies, and methods for integrating
    retrieved information with LLM generation. By the end of this chapter, you’ll
    be able to implement basic RAG systems to augment your LLMs with external knowledge.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍嵌入和索引技术，用于高效检索向量数据库，查询制定策略，以及将检索信息与LLM生成集成的方法。到本章结束时，你将能够实现基本的RAG系统，以增强你的LLM外部知识。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Building a simple RAG system for LLMs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为LLM构建简单的RAG系统
- en: Embedding and indexing techniques for LLM retrieval
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM检索的嵌入和索引技术
- en: Query formulation strategies in LLM-based RAG
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于LLM的RAG查询制定策略
- en: Integrating retrieved information with LLM generation
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将检索信息与LLM生成集成
- en: Challenges and opportunities in RAG for LLMs
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM中RAG的挑战和机遇
- en: Building a simple RAG system for LLMs
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为LLM构建简单的RAG系统
- en: This section provides a practical illustration of a simple RAG system, leveraging
    the robust search capabilities of **SerpApi**, the semantic understanding of sentence
    embeddings, and the generative prowess of OpenAI’s GPT-4o model. SerpApi is a
    web scraping API that provides real-time access to search engine results, offering
    structured data for Google, Bing, and other platforms without the need for manual
    scraping.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了一个简单RAG系统的实际示例，利用**SerpApi**强大的搜索能力、句子嵌入的语义理解以及OpenAI的GPT-4o模型的生成能力。SerpApi是一个网络抓取API，提供对搜索引擎结果的实时访问，为Google、Bing和其他平台提供结构化数据，无需手动抓取。
- en: Through this example, we will explore the fundamental components of a RAG system,
    including query-based web searching, snippet extraction and ranking, and, ultimately,
    the generation of a comprehensive answer using a state-of-the-art LLM, highlighting
    the interplay between these elements in a step-by-step manner.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个例子，我们将探讨RAG系统的基本组件，包括基于查询的网页搜索、片段提取和排名，以及最终使用最先进的LLM生成全面答案的过程，以逐步方式突出这些元素之间的相互作用。
- en: 'The code for the simple RAG system we’ll be building contains the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要构建的简单RAG系统的代码包含以下内容：
- en: '**SerpApi**: To find relevant web pages based on the user’s query.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SerpApi**：根据用户的查询找到相关的网页。'
- en: '**Sentence embeddings**: To extract the most relevant snippets from the search
    results using sentence embeddings and cosine similarity. Sentence embeddings are
    dense numerical representations of text that capture semantic meaning by mapping
    words, phrases, or entire sentences into high-dimensional vector space, where
    similar meanings are positioned closer together. Cosine similarity measures the
    angle between these embedding vectors (ranging from -1 to 1), rather than their
    magnitude, making it an effective way to evaluate semantic similarity regardless
    of text length; when two embeddings have a cosine similarity close to 1, they’re
    highly similar in meaning, while values closer to 0 indicate unrelated content
    and negative values suggest opposing meanings. This combination of techniques
    powers many modern **natural language processing** (**NLP**) applications, from
    search engines and recommendation systems to language translation and content
    clustering.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**句子嵌入**: 通过使用句子嵌入和余弦相似度来从搜索结果中提取最相关的片段。句子嵌入是文本的密集数值表示，通过将单词、短语或整个句子映射到高维向量空间来捕获语义意义，其中相似的意义被放置得更近。余弦相似度衡量这些嵌入向量之间的角度（范围从-1到1），而不是它们的幅度，这使得它成为评估语义相似性的有效方法，无论文本长度如何；当两个嵌入的余弦相似度接近1时，它们在意义上高度相似，而接近0的值表示无关内容，负值则表示相反的意义。这种技术的组合为许多现代**自然语言处理**（**NLP**）应用提供了动力，从搜索引擎和推荐系统到语言翻译和内容聚类。'
- en: '**OpenAI’s GPT-4o**: To generate a comprehensive and coherent answer based
    on the retrieved snippets (context) and the original query.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI的GPT-4o**: 基于检索到的片段（上下文）和原始查询生成全面且连贯的答案。'
- en: 'First, let’s install the following dependencies:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们安装以下依赖项：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding command, we install `serpapi` for searching, `sentence_transformers`
    for embedding, and `openai` for accessing GPT-4o.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的命令中，我们安装了`serpapi`用于搜索，`sentence_transformers`用于嵌入，以及`openai`用于访问GPT-4o。
- en: 'Next, let us see how a complete RAG system is implemented using search APIs,
    embeddings, and an LLM:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何使用搜索API、嵌入和LLM实现一个完整的RAG系统：
- en: 'We first import the installed libraries along with `torch` for tensor operations.
    The code snippet also sets up API keys for SerpApi and OpenAI. Remember to replace
    the placeholders with your actual API keys:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入安装的库以及`torch`进行张量操作。代码片段还设置了SerpApi和OpenAI的API密钥。请记住用你实际的API密钥替换占位符：
- en: '[PRE1]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We then initialize the search engine and Sentence Transformer. The following
    code defines the search function to perform a Google search using SerpApi and
    initializes the Sentence Transformer model (`all-mpnet-base-v2`) for creating
    sentence embeddings:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们初始化搜索引擎和句子转换器。以下代码定义了搜索函数，使用SerpApi执行Google搜索，并初始化句子转换器模型（`all-mpnet-base-v2`）以创建句子嵌入：
- en: '[PRE2]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we retrieve relevant snippets. We define the `retrieve_snippets` function,
    which takes the search results, extracts snippets, computes their embeddings,
    and calculates the cosine similarity between the query embedding and each snippet
    embedding. It then returns the top *k* snippets that are most similar to the query:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们检索相关片段。我们定义了`retrieve_snippets`函数，它接受搜索结果，提取片段，计算它们的嵌入，并计算查询嵌入与每个片段嵌入之间的余弦相似度。然后，它返回与查询最相似的顶部*k*个片段：
- en: '[PRE3]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We then define the `generate_answer` function to generate an answer using GPT-4o.
    This is the core of the generation part of our RAG system:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义了`generate_answer`函数，用于使用GPT-4o生成答案。这是我们的RAG系统生成部分的核心：
- en: '[PRE4]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This function constructs a structured prompt for an LLM to generate an answer
    constrained strictly to a given context. It formats the conversation as a system-user
    message pair, instructing the model to act as a subject matter expert and restrict
    its answer to the supplied information, explicitly avoiding speculation. If the
    information isn’t present, the system is directed to return a fallback message
    indicating that the answer couldn’t be found. The query and context are embedded
    directly into the user message, and the LLM (in this case, `gpt-4o`) is queried
    with a moderate creativity level via `temperature=0.7` and a response length cap
    of `256` tokens. This design makes the function reliable for context-grounded
    Q&A tasks, particularly in RAG pipelines or constrained-answering settings such
    as document QA or compliance tools.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此函数构建一个结构化的提示，用于 LLM 生成一个严格受给定上下文约束的答案。它将对话格式化为系统-用户消息对，指示模型充当主题专家，并将答案限制在提供的信息内，明确避免推测。如果信息不存在，系统将指导返回一个回退消息，表明答案无法找到。查询和上下文直接嵌入到用户消息中，LLM（在本例中为
    `gpt-4o`）通过 `temperature=0.7` 和 `256` 令牌的响应长度上限进行适度创造性的查询。这种设计使得该函数在基于上下文的问答任务中可靠，尤其是在
    RAG 管道或文档问答或合规工具等约束回答环境中。
- en: 'Here’s the main RAG function and example usage:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里是主要的 RAG 函数及其示例用法：
- en: '[PRE5]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This code defines the `rag_system` function, which orchestrates the entire
    process: searching, retrieving snippets, and generating an answer. It then demonstrates
    how to use `rag_system` with an example query, printing the generated answer to
    the console'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此代码定义了 `rag_system` 函数，该函数协调整个流程：搜索、检索片段和生成答案。然后它演示了如何使用示例查询 `rag_system`，并将生成的答案打印到控制台。
- en: The `rag_system` function answers a query by first searching for relevant information
    using `search(query)` and then extracting relevant snippets through the API called
    `retrieve_snippets(query, search_results)`. If no snippets are found, it returns
    a message indicating no information was found. If snippets are available, they
    are combined into a single context string and used to generate an answer through
    `generate_answer(query, context)`. Finally, the function returns the generated
    answer based on the context. In the example usage, the function is called with
    the query `"What are the latest advancements in quantum computing?"` and will
    return a generated response based on the relevant search results. In real production
    systems, we should implement retries and error handling around `retrieve_snippets`
    API calls.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`rag_system` 函数通过首先使用 `search(query)` 搜索相关信息，然后通过调用 `retrieve_snippets(query,
    search_results)` API 提取相关片段来回答查询。如果没有找到片段，它将返回一条消息，表明没有找到信息。如果可用片段，它们将被组合成一个单一上下文字符串，并通过
    `generate_answer(query, context)` 生成答案。最后，该函数根据上下文返回生成的答案。在示例用法中，该函数使用查询 `"What
    are the latest advancements in quantum computing?"` 被调用，并将根据相关搜索结果返回一个生成的响应。在实际生产系统中，我们应该在
    `retrieve_snippets` API 调用周围实现重试和错误处理。'
- en: 'Before we move on to the next section, here are some things to remember:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入下一节之前，这里有一些事情需要记住：
- en: '**API keys**: Make sure you have valid API keys for both SerpApi and OpenAI
    and have replaced the placeholders in the code.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API 密钥**：确保你有有效的 SerpApi 和 OpenAI API 密钥，并在代码中替换了占位符。'
- en: '**OpenAI costs**: Be mindful of OpenAI API usage costs. GPT-4o can be more
    expensive than other models.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI 成本**：注意 OpenAI API 使用成本。GPT-4o 可能比其他模型更昂贵。'
- en: '**Prompt engineering**: The quality of the generated answer heavily depends
    on the prompt you provide to GPT-4o. You might need to experiment with different
    prompts to get the best results. Consider adding instructions about the desired
    answer format, length, or style.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示工程**：生成的答案质量很大程度上取决于提供给 GPT-4o 的提示。你可能需要尝试不同的提示以获得最佳结果。考虑添加有关所需答案格式、长度或样式的说明。'
- en: '`try-except` blocks) to handle potential issues such as network problems, API
    errors, or invalid inputs.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `try-except` 块来处理潜在问题，例如网络问题、API 错误或无效输入。
- en: '**Advanced techniques**: This is a basic RAG system. You can improve it further
    by doing the following:'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级技术**：这是一个基本的 RAG 系统。你可以通过以下方式进一步改进它：'
- en: '**Better snippet selection**: Consider factors such as source diversity, factuality,
    and snippet length'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更好的片段选择**：考虑因素包括来源多样性、事实性和片段长度'
- en: '**Iterative retrieval**: Retrieve more context if the initial answer is not
    satisfactory'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代检索**：如果初始答案不满意，则检索更多上下文'
- en: '**Fine-tuning**: Fine-tune a smaller, more specialized language model on your
    specific domain for potentially better performance and lower costs'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调**：在您的特定领域上对较小的、更专业的语言模型进行微调，以实现更好的性能和更低的成本'
- en: 'We’ve successfully built a simple RAG system, covering the core components
    of retrieval and generation. Now that we have a functional RAG system, let’s dive
    deeper into the crucial techniques that enable efficient retrieval from large
    datasets: embedding and indexing. We’ll explore different methods for representing
    text semantically and organizing these representations for fast similarity search.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功构建了一个简单的RAG系统，涵盖了检索和生成的核心组件。现在我们有了功能性的RAG系统，让我们更深入地探讨那些能够从大量数据集中高效检索的关键技术：嵌入和索引。我们将探讨不同的方法来表示文本的语义，并组织这些表示以实现快速相似性搜索。
- en: Embeddings and indexing for retrieval in LLM applications
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM应用中的检索嵌入和索引
- en: Embedding and indexing techniques provide efficient and effective retrieval
    in RAG-based LLM applications. They allow LLMs to quickly find and utilize relevant
    information from vast amounts of data. The following subsections provide a breakdown
    of common techniques.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入和索引技术为基于RAG的LLM应用提供了高效和有效的检索。它们允许LLM快速找到并利用大量数据中的相关信息。以下小节提供了常见技术的分解。
- en: Embeddings
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入
- en: Embeddings are numerical vector representations of data, such as text, images,
    or audio, that map complex, high-dimensional data into a continuous vector space
    where similar items are positioned close to each other. These vectors capture
    the underlying patterns, relationships, and semantic properties of the data, making
    it easier for machine learning models to understand and process. For text, for
    example, word embeddings transform words or phrases into dense vectors that represent
    their meaning in a way that reflects semantic relationships, such as synonyms
    being closer together in the vector space. Embeddings are typically learned from
    large datasets through techniques such as neural networks, and they serve as a
    foundation for tasks such as information retrieval, classification, clustering,
    and recommendation systems. By reducing the dimensionality of data while preserving
    important features, embeddings enable models to generalize better and make sense
    of varied input data efficiently.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是数据的数值向量表示，例如文本、图像或音频，它将复杂的高维数据映射到一个连续的向量空间中，其中相似的项目彼此靠近。这些向量捕捉数据的潜在模式、关系和语义属性，使得机器学习模型更容易理解和处理。例如，对于文本，词嵌入将单词或短语转换成密集的向量，以反映它们在向量空间中的语义关系，如同义词彼此更接近。嵌入通常通过神经网络等技术从大型数据集中学习，它们是信息检索、分类、聚类和推荐系统等任务的基础。通过降低数据的维度同时保留重要特征，嵌入使得模型能够更好地泛化并有效地处理各种输入数据。
- en: For LLMs, text embeddings are most relevant. They are generated by passing text
    through a neural network (like the Sentence Transformer models we used in the
    previous section).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LLM，文本嵌入最为相关。它们是通过将文本传递到神经网络（如我们在上一节中使用的Sentence Transformer模型）来生成的。
- en: Why do we need embeddings?
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们为什么需要嵌入？
- en: 'Embeddings are important for RAG applications for the following reasons:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入对于RAG应用的重要性如下：
- en: '**Semantic search**: Embeddings enable semantic search, where you find information
    based on meaning rather than just keyword matching'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义搜索**：嵌入使得语义搜索成为可能，您可以根据意义而不是仅仅根据关键词匹配来查找信息'
- en: '**Contextual understanding**: LLMs can use embeddings to understand the relationships
    between different pieces of information, improving their ability to reason and
    generate relevant responses'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文理解**：LLM可以使用嵌入来理解不同信息片段之间的关系，提高其推理和生成相关响应的能力'
- en: '**Efficient retrieval**: When combined with appropriate indexing, embeddings
    allow for the fast retrieval of relevant information from large datasets'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高效检索**：当与适当的索引结合时，嵌入允许从大型数据集中快速检索相关信息'
- en: Common embedding technologies
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 常见的嵌入技术
- en: 'Several embedding technologies are commonly used in RAG systems, and these
    vary in their underlying models, methods, and suitability for different applications.
    Here are some prominent embedding technologies for RAG:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在RAG系统中，常用的嵌入技术多种多样，它们在底层模型、方法和适用于不同应用方面有所不同。以下是RAG中一些突出的嵌入技术：
- en: '**Pre-trained transformer-based embeddings (e.g., BERT, RoBERTa, and T5)**:
    Transformer models such as **Bidirectional Encoder Representations from Transformers**
    (**BERT**) and its variants, such as RoBERTa and T5, have been widely used to
    generate dense, contextual embeddings for text. These models are fine-tuned on
    large corpora and capture a rich understanding of language semantics. In a RAG
    system, these embeddings can be used to retrieve relevant passages from a document
    store based on semantic similarity. The embeddings are typically high-dimensional
    and are generated by feeding text through the transformer model to produce a fixed-size
    vector.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预训练的基于transformer的嵌入（例如，BERT、RoBERTa和T5）**：像**双向编码器表示（BERT）**及其变体（如RoBERTa和T5）这样的Transformer模型已被广泛用于生成文本的密集、上下文嵌入。这些模型在大语料库上进行微调，并捕捉到丰富的语言语义理解。在RAG系统中，这些嵌入可用于根据语义相似性从文档存储中检索相关段落。这些嵌入通常是高维的，并且通过将文本输入到Transformer模型中生成固定大小的向量。'
- en: '**Sentence-BERT (SBERT)**: A variation of BERT designed for sentence-level
    embeddings, SBERT focuses on optimizing the model for tasks such as semantic textual
    similarity and clustering. It uses a Siamese network architecture to map sentences
    into a dense vector space where semantically similar sentences are closer together.
    This makes it particularly effective for tasks such as information retrieval in
    RAG, where retrieving semantically relevant passages from a large corpus is essential.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sentence-BERT（SBERT）**：SBERT是BERT的一种变体，专为句子级嵌入设计，它专注于优化模型以执行诸如语义文本相似性和聚类等任务。它使用Siamese网络架构将句子映射到密集向量空间，其中语义相似的句子彼此更接近。这使得它在RAG中的信息检索任务中特别有效，在这些任务中，从大型语料库中检索与语义相关的段落至关重要。'
- en: '**Facebook AI Similarity Search (Faiss)**: Faiss is a library developed by
    Facebook AI Research that provides efficient similarity search through **approximate
    nearest neighbor** (**ANN**) search. Faiss is not an embedding technology by itself
    but works in conjunction with various embedding models to index and search over
    large collections of vectors. When used in RAG, Faiss enables the fast retrieval
    of relevant documents or passages by comparing the similarity of their embeddings
    against a query embedding.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Facebook AI相似性搜索（Faiss）**：Faiss是由Facebook AI Research开发的库，它通过**近似最近邻（ANN）搜索**提供高效的相似性搜索。Faiss本身不是一种嵌入技术，但它与各种嵌入模型协同工作，以索引和搜索大量向量集合。在RAG中使用时，Faiss通过比较其嵌入与查询嵌入之间的相似性，实现了快速检索相关文档或段落。'
- en: '**Dense retriever models (e.g., DPR and ColBERT)**: **Dense Passage Retrieval**
    (**DPR**) is an approach to information retrieval that uses two separate encoders
    (usually BERT-based models) to encode both queries and passages into dense vectors.
    DPR outperforms traditional sparse retrieval methods by leveraging the contextual
    knowledge encoded in dense embeddings. ColBERT, on the other hand, is another
    dense retrieval model that balances the efficiency of dense retrieval and the
    effectiveness of traditional methods. These models are especially useful for RAG
    when retrieving high-quality passages that are semantically related to a query.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**密集检索模型（例如，DPR和ColBERT）**：**密集段落检索（DPR）**是一种信息检索方法，它使用两个独立的编码器（通常是基于BERT的模型）将查询和段落编码为密集向量。DPR通过利用密集嵌入中编码的上下文知识，优于传统的稀疏检索方法。另一方面，ColBERT是另一种平衡密集检索效率和传统方法有效性的密集检索模型。这些模型在RAG中检索与查询语义相关的优质段落时特别有用。'
- en: '**Contrastive Language-Image Pre-Training (CLIP)**: While originally designed
    for multimodal applications (text and image), CLIP has been adapted for text-only
    tasks as well. It learns embeddings by aligning text and image data in a shared
    vector space. Although CLIP is primarily used for multimodal tasks, its ability
    to represent language in a common space with images provides a flexible embedding
    framework that can be used in RAG, especially when working with multimodal data.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对比语言-图像预训练（CLIP）**：虽然最初是为多模态应用（文本和图像）设计的，但CLIP也被适应用于仅文本的任务。它通过在共享向量空间中对齐文本和图像数据来学习嵌入。尽管CLIP主要用于多模态任务，但它在图像空间中共同表示语言的能力提供了一个灵活的嵌入框架，该框架可用于RAG，尤其是在处理多模态数据时。'
- en: '**Deep semantic similarity models (e.g., USE and InferSent)**: Models such
    as the **Universal Sentence Encoder** (**USE**) and InferSent generate sentence
    embeddings by capturing deeper semantic meaning, which can be used for various
    NLP tasks, including document retrieval. These models produce fixed-size vector
    representations that can be compared for similarity, making them useful for RAG
    when paired with retrieval systems.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度语义相似性模型（例如，USE和InferSent）**：如**通用句子编码器（USE**）和InferSent之类的模型通过捕捉更深层的语义意义来生成句子嵌入，可用于各种NLP任务，包括文档检索。这些模型产生固定大小的向量表示，可以用于比较相似性，当与检索系统结合使用时，它们对RAG非常有用。'
- en: '**Doc2Vec**: An extension of Word2Vec, Doc2Vec generates embeddings for entire
    documents rather than individual words. It maps variable-length text into a fixed-size
    vector, which can be used to retrieve semantically similar documents or passages.
    Though not as powerful as transformer-based models in terms of semantic richness,
    Doc2Vec is still an effective tool for more lightweight retrieval tasks in RAG
    applications.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Doc2Vec**：Word2Vec的扩展，Doc2Vec为整个文档生成嵌入，而不是单个单词。它将可变长度的文本映射到固定大小的向量，可用于检索语义相似的文档或段落。虽然在语义丰富性方面不如基于transformer的模型强大，但Doc2Vec仍然是RAG应用中用于更轻量级检索任务的有效工具。'
- en: '**Embedding-based search engines (e.g., Elasticsearch with dense vectors)**:
    Some modern search engines, such as Elasticsearch, have integrated support for
    dense vectors alongside traditional keyword-based indexing. Elasticsearch can
    store and retrieve text embeddings, allowing for more flexible and semantically
    aware searches. When used with RAG, these embeddings can be used to rank documents
    by their relevance to a query, improving retrieval performance.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于嵌入的搜索引擎（例如，使用密集向量的Elasticsearch）**：一些现代搜索引擎，如Elasticsearch，已经集成了对密集向量的支持，同时保留了传统的基于关键词的索引。Elasticsearch可以存储和检索文本嵌入，允许进行更灵活和语义感知的搜索。当与RAG结合使用时，这些嵌入可以用于根据查询的相关性对文档进行排序，从而提高检索性能。'
- en: '**OpenAI embeddings (e.g., GPT-based models)**: OpenAI’s embeddings, derived
    from models such as GPT-3, are also used for RAG tasks. These embeddings are based
    on the language model’s ability to generate high-quality text representations,
    which can be indexed and searched over large corpora. While they are not as specifically
    tuned for retrieval as some other models (such as DPR), they are highly flexible
    and can be used in general-purpose RAG applications.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI嵌入（例如，基于GPT的模型）**：OpenAI的嵌入，如GPT-3模型，也用于RAG任务。这些嵌入基于语言模型生成高质量文本表示的能力，可以在大型语料库中进行索引和搜索。虽然它们在检索方面的特定调整不如某些其他模型（如DPR），但它们非常灵活，可以用于通用RAG应用。'
- en: These embedding technologies provide various advantages depending on the specific
    requirements of a RAG system, such as retrieval speed, model accuracy, and the
    scale of the data being processed. Each of them can be fine-tuned and optimized
    for specific use cases, and the choice of embedding technology will depend on
    factors such as the nature of the documents being retrieved, computational resources,
    and latency requirements.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这些嵌入技术根据RAG系统的具体需求提供各种优势，例如检索速度、模型准确性和处理数据的规模。每个技术都可以针对特定用例进行微调和优化，嵌入技术的选择将取决于诸如检索文档的性质、计算资源和延迟要求等因素。
- en: Indexing
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 索引
- en: Indexing is the process of organizing embeddings in a data structure that allows
    for fast similarity search. Think of it like the index of a book, but for vectors
    instead of words.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 索引是将嵌入组织到一种数据结构中的过程，该数据结构允许进行快速相似性搜索。想象一下书的索引，但针对的是向量而不是单词。
- en: Using a more detailed description using LLM terminology, vector indexing technologies
    optimize embedding storage and retrieval by creating specialized data structures
    that organize high-dimensional vectors according to their similarity relationships,
    rather than sequential order. These structures—whether graph-based (connecting
    similar vectors through navigable pathways), tree-based (recursively partitioning
    the vector space), or quantization-based (compressing vectors while preserving
    similarity)—all serve the fundamental purpose of transforming an otherwise prohibitively
    expensive exhaustive search into a manageable process by strategically limiting
    the search space, enabling vector databases to handle billions of embeddings with
    sub-second query times while maintaining an acceptable trade-off between speed,
    memory efficiency, and result accuracy.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更详细的LLM术语描述，向量索引技术通过创建专门的数据结构来优化嵌入的存储和检索，这些数据结构根据其相似性关系组织高维向量，而不是按顺序排列。这些结构——无论是基于图（通过可导航路径连接相似向量）、基于树（递归划分向量空间）还是基于量化（压缩向量同时保留相似性）——都服务于将原本代价高昂的全面搜索转化为可管理过程的根本目的，通过策略性地限制搜索空间，使向量数据库能够以亚秒级的查询时间处理数十亿个嵌入，同时保持速度、内存效率和结果精度之间的可接受权衡。
- en: Why is indexing important?
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么索引很重要？
- en: 'Indexing is important for LLMs for the following reasons:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 索引对于LLM之所以重要，有以下原因：
- en: '**Speed**: Without indexing, you would have to compare a query embedding to
    every single embedding in your dataset, which is computationally expensive and
    slow'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度**：没有索引，您必须将查询嵌入与数据集中的每个嵌入进行比较，这计算成本高且速度慢。'
- en: '**Scalability**: Indexing allows LLM applications to scale to handle massive
    datasets containing millions or even billions of data points'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：索引允许LLM应用扩展以处理包含数百万甚至数十亿数据点的庞大数据集。'
- en: Common indexing techniques
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 常见的索引技术
- en: Let’s look at some of the common indexing techniques for LLMs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看LLM的一些常见索引技术。
- en: 'For a visual diagram of these index techniques, I recommend you check out the
    following website: [https://kdb.ai/learning-hub/articles/indexing-basics/](https://kdb.ai/learning-hub/articles/indexing-basics/)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些索引技术的可视化图表，我建议您查看以下网站：[https://kdb.ai/learning-hub/articles/indexing-basics/](https://kdb.ai/learning-hub/articles/indexing-basics/)
- en: '**Flat index (****brute force)**:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平面索引（暴力法）**：'
- en: '**How it works**: Stores all embeddings in a simple list or array. During a
    search, it calculates the distance (e.g., cosine similarity) between the query
    embedding and every embedding in the index.'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作原理**：将所有嵌入存储在简单的列表或数组中。在搜索过程中，它计算查询嵌入与索引中每个嵌入之间的距离（例如，余弦相似度）。'
- en: '**Pros**: Simple to implement and perfect accuracy (finds the true nearest
    neighbors).'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优点**：简单易实现且完美精度（找到真正的最近邻）。'
- en: '**Cons**: Slow and computationally expensive for large datasets, as it requires
    an exhaustive search.'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺点**：对于大数据集来说，速度慢且计算成本高，因为它需要进行全面搜索。'
- en: '**Suitable for**: Very small datasets or when perfect accuracy is an absolute
    requirement.'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适用场景**：非常小的数据集或当完美精度是绝对要求时。'
- en: '**Inverted file** **index (IVF)**:'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**倒排文件索引（IVF）**：'
- en: '**How** **it works**:'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作原理**：'
- en: '**Clustering**: Divides the embedding space into clusters using algorithms
    such as *k*-means'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：使用如k-means等算法将嵌入空间划分为簇。'
- en: '**Inverted index**: Creates an inverted index that maps each cluster centroid
    to a list of the embeddings belonging to that cluster'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**倒排索引**：创建一个倒排索引，将每个簇中心映射到属于该簇的嵌入列表。'
- en: '**Search**:'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索**：'
- en: Finds the nearest cluster centroid(s) to the query embedding
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到与查询嵌入最近的簇中心。
- en: Only searches within those clusters, significantly reducing the search space
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只在那些簇内进行搜索，显著减少了搜索空间。
- en: '**Pros**: Faster than a flat index; relatively simple to implement'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优点**：比平面索引快；相对简单易实现。'
- en: '**Cons**: Approximate (might not always find the true nearest neighbors); accuracy
    depends on the number of clusters'
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺点**：近似（可能无法始终找到真正的最近邻）；精度取决于簇的数量。'
- en: '**Suitable for**: Medium-sized datasets where a good balance between speed
    and accuracy is needed'
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适用场景**：需要速度和精度之间良好平衡的中等大小数据集。'
- en: '**Hierarchical navigable small** **world (HNSW)**:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分层可导航小世界（HNSW）**：'
- en: '**How** **it works**:'
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作原理**：'
- en: '**Graph-based**: Constructs a hierarchical graph where each node represents
    an embedding.'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于图**：构建一个分层图，其中每个节点代表一个嵌入。'
- en: '**Layers**: The graph has multiple layers, with the top layer having long-range
    connections (for faster traversal) and the bottom layer having short-range connections
    (for accurate search).'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层**：图有多个层，顶层有长距离连接（用于更快地遍历），底层有短距离连接（用于准确搜索）。'
- en: '**Search**: Starts at a random node in the top layer and greedily moves towards
    the query embedding by exploring connections. The search progresses down the layers,
    refining the results.'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索**：从顶层随机节点开始，通过探索连接贪婪地向查询嵌入移动。搜索在层中向下进行，逐步细化结果。'
- en: '**Pros**: Very fast and accurate; often considered the state of the art for
    ANN search'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优点**：非常快且准确；通常被认为是近似最近邻搜索的当前最佳水平'
- en: '**Cons**: More complex to implement than IVF, and higher memory overhead due
    to the graph structure'
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺点**：比IVF实现更复杂，并且由于图结构，内存开销更大'
- en: '**Suitable for**: Large datasets where both speed and accuracy are crucial'
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适用范围**：对于速度和准确度都至关重要的大型数据集'
- en: '**Product** **quantization (PQ)**:'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**产品量化 (PQ)**：'
- en: '**How** **it works**:'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作原理**：'
- en: '**Subvectors**: Divides each embedding into multiple subvectors.'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**子向量**：将每个嵌入分割成多个子向量。'
- en: '**Codebooks**: Creates separate codebooks for each subvector using clustering.
    Each codebook contains a set of representative subvectors (centroids).'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**码本**：使用聚类为每个子向量创建单独的码本。每个码本包含一组代表性的子向量（中心点）。'
- en: '**Encoding**: Encodes each embedding by replacing its subvectors with the closest
    centroids from the corresponding codebooks. This creates a compressed representation
    of the embedding.'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码**：通过用对应码本中最接近的中心点替换其子向量来编码每个嵌入。这创建了一个嵌入的压缩表示。'
- en: '**Search**: Calculates the approximate distance between the query and the encoded
    embeddings using pre-computed distances between the query’s subvectors and the
    codebook centroids.'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索**：通过使用查询子向量和码本中心之间的预计算距离，计算查询与编码嵌入之间的近似距离。'
- en: '**Pros**: Significantly reduces memory usage by compressing embeddings; fast
    search.'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优点**：通过压缩嵌入显著减少内存使用；快速搜索。'
- en: '**Cons**: Approximate, and accuracy depends on the number of subvectors and
    the size of the codebooks.'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺点**：近似，准确度取决于子向量的数量和码本的大小。'
- en: '**Suitable for**: Very large datasets where memory efficiency is a primary
    concern.'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适用范围**：非常适合内存效率是主要关注点的非常大的数据集。'
- en: '**Locality sensitive** **hashing (LSH)**:'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部敏感哈希 (LSH)**：'
- en: '**How it works**: Uses hash functions to map similar embeddings to the same
    “bucket” with high probability'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作原理**：使用哈希函数以高概率将相似的嵌入映射到同一个“桶”中'
- en: '**Pros**: Relatively simple; can be distributed across multiple machines'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优点**：相对简单；可以跨多台机器分布式部署'
- en: '**Cons**: Approximate, and performance depends on the choice of hash functions
    and the number of buckets'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺点**：近似，性能取决于哈希函数的选择和桶的数量'
- en: '**Suitable for**: Very large, high-dimensional datasets'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适用范围**：非常适合非常大的、高维数据集'
- en: Now that we’ve covered different indexing methods, let’s introduce some popular
    libraries and tools that implement these indexing techniques, making them easier
    to use in practice. This will provide a practical perspective on how to leverage
    these technologies in your RAG applications.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了不同的索引方法，让我们介绍一些流行的库和工具，它们实现了这些索引技术，使得在实际中使用它们变得更加容易。这将提供一个实际的角度，了解如何在你的RAG应用中利用这些技术。
- en: 'The following are some libraries and tools for implementing indexing:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些实现索引的库和工具：
- en: '**Faiss**: A highly optimized library developed by Facebook AI for efficient
    similarity search and clustering of dense vectors. It implements many of the indexing
    techniques mentioned previously (flat, IVF, HNSW, and PQ).'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Faiss**：Facebook AI开发的高度优化的库，用于高效地搜索和聚类密集向量。它实现了之前提到的许多索引技术（平面、IVF、HNSW和PQ）。'
- en: '**Approximate Nearest Neighbors Oh Yeah (Annoy)**: Another popular library
    for ANN search, known for its ease of use and good performance. It uses a tree-based
    approach.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**近似最近邻 Oh Yeah (Annoy)**：另一个流行的近似最近邻搜索库，以其易用性和良好的性能而闻名。它采用基于树的方法。'
- en: '**Scalable Nearest Neighbors (ScaNN)**: A library developed by Google, designed
    for large-scale, high-dimensional datasets.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Scalable Nearest Neighbors (ScaNN)**：由谷歌开发的库，旨在用于大规模、高维数据集。'
- en: '**Vespa.ai**: Provide tools to query, organize, and make inferences in vectors,
    tensors, text, and structured data. It is used by [https://www.perplexity.ai/](https://www.perplexity.ai/).'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Vespa.ai**：提供查询、组织和在向量、张量、文本和结构化数据中进行推理的工具。它被[https://www.perplexity.ai/](https://www.perplexity.ai/)使用。'
- en: '**Pinecone, Weaviate, Milvus, Qdrant**: Vector databases designed specifically
    for storing and searching embeddings. They handle indexing, scaling, and other
    infrastructure concerns.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pinecone, Weaviate, Milvus, Qdrant**：专门设计用于存储和搜索嵌入的向量数据库。它们处理索引、扩展和其他基础设施问题。'
- en: 'The best embedding and indexing techniques for your LLM application will depend
    on several factors:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于你的LLM应用的最佳嵌入和索引技术将取决于几个因素：
- en: '**Dataset size**: For small datasets, a flat index might be sufficient. For
    large datasets, consider HNSW, IVF, or PQ.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集大小**：对于小型数据集，平面索引可能足够。对于大型数据集，考虑HNSW、IVF或PQ。'
- en: '**Speed requirements**: If low latency is critical, HNSW is generally the fastest
    option.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度要求**：如果低延迟至关重要，HNSW通常是速度最快的选项。'
- en: '**Accuracy requirements**: If perfect accuracy is required, a flat index is
    the only choice, but it’s not scalable. HNSW often provides the best accuracy
    among approximate methods.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精度要求**：如果需要完美的精度，平面索引是唯一的选择，但它不可扩展。HNSW在近似方法中通常提供最佳的精度。'
- en: '**Memory constraints**: If memory is limited, PQ can significantly reduce storage
    requirements.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存限制**：如果内存有限，PQ可以显著减少存储需求。'
- en: '**Development effort**: Faiss and Annoy offer a good balance between performance
    and ease of implementation. Vector databases simplify infrastructure management.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发工作量**：Faiss和Annoy在性能和易于实现之间提供了良好的平衡。向量数据库简化了基础设施管理。'
- en: By carefully considering these factors and understanding the strengths and weaknesses
    of each technique and library, you can choose the most appropriate embedding and
    indexing methods to build efficient and effective LLM applications.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仔细考虑这些因素，并理解每种技术和库的优缺点，你可以选择最合适的嵌入和索引方法来构建高效且有效的LLM应用。
- en: We’ll now demonstrate an example involving embedding, indexing, and searching
    using Faiss, a powerful library for efficient similarity search. I’ll use the
    `all-mpnet-base-v2` Sentence Transformer model to generate embeddings. Since the
    code will be more than 20 lines, I’ll break it down into blocks with explanations
    preceding each block.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将演示一个使用Faiss的嵌入、索引和搜索示例，Faiss是一个用于高效相似性搜索的强大库。我将使用`all-mpnet-base-v2` Sentence
    Transformer模型来生成嵌入。由于代码将超过20行，我将将其分解为带有每个块前解释的代码块。
- en: Example code demonstrating embedding, indexing, and searching
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例代码演示嵌入、索引和搜索
- en: 'In this section, we’ll be showing the code for a typical workflow for using
    embeddings and indexing to enable fast similarity search within a collection of
    text documents: Here’s what it does:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示使用嵌入和索引实现快速文本文档集合中相似性搜索的典型工作流程代码：以下是它的功能：
- en: '**Loads a Sentence Transformer model**: Initializes a pre-trained model for
    generating sentence embeddings.'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**加载Sentence Transformer模型**：初始化用于生成句子嵌入的预训练模型。'
- en: '**Creates sample data**: Defines a list of example sentences (you would replace
    this with your actual data).'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建样本数据**：定义一个示例句子列表（你将用实际数据替换这部分）。'
- en: '`SentenceTransformer` to create embeddings for each sentence.'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`SentenceTransformer`为每个句子创建嵌入。
- en: '`IndexFlatL2` for a flat L2 distance index in this example) to store the embeddings.'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此示例中，使用`IndexFlatL2`为平面L2距离索引存储嵌入。
- en: '**Adds embeddings to the index**: Adds the generated embeddings to the Faiss
    index.'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将嵌入添加到索引中**：将生成的嵌入添加到Faiss索引中。'
- en: '**Defines a search query**: Sets a sample query for which we want to find similar
    sentences.'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义搜索查询**：设置一个我们想要找到相似句子的示例查询。'
- en: '**Encodes the query**: Creates an embedding for the search query using the
    same Sentence Transformer model.'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**编码查询**：使用相同的Sentence Transformer模型为搜索查询创建嵌入。'
- en: '**Performs a search**: Uses the Faiss index to search for the *k* most similar
    embeddings to the query embedding.'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**执行搜索**：使用Faiss索引搜索与查询嵌入最相似的*k*个嵌入。'
- en: '**Prints the results**: Displays the indices and distances of the *k* nearest
    neighbors found in the index.'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**打印结果**：显示在索引中找到的最近的*k*个邻居的索引和距离。'
- en: 'Before we check out the code, let us install the following dependencies:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看代码之前，让我们安装以下依赖项：
- en: '[PRE6]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let us now see the code example:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看代码示例：
- en: 'First, we import the necessary libraries—`sentence_transformers` for creating
    embeddings and `faiss` for indexing and searching—and load the `all-mpnet-base-v2`
    Sentence Transformer model:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入必要的库——`sentence_transformers`用于创建嵌入，`faiss`用于索引和搜索——并加载`all-mpnet-base-v2`
    Sentence Transformer模型：
- en: '[PRE7]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We then prepare the data by defining some sample sentences (you can replace
    these with your actual data) and then use the Sentence Transformer model to generate
    embeddings for each sentence (the embeddings are converted to float32, which is
    required by Faiss):'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们通过定义一些示例句子（你可以用你的实际数据替换这些句子）来准备数据，然后使用 Sentence Transformer 模型为每个句子生成嵌入（嵌入被转换为
    float32，这是 Faiss 所需要的）：
- en: '[PRE8]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We then create a Faiss index and add the embeddings to it:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建一个 Faiss 索引并将嵌入添加到其中：
- en: '[PRE9]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, we’re using IndexFlatL2, which is a flat index that uses L2 distance (Euclidean
    distance) for similarity comparisons. This type of index provides accurate results
    but can be slow for very large datasets. The index is created with the correct
    dimensionality (`768` for this Sentence Transformer model).
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，我们使用 IndexFlatL2，这是一个使用 L2 距离（欧几里得距离）进行相似度比较的平面索引。这种类型的索引可以提供准确的结果，但可能对于非常大的数据集来说比较慢。索引是根据正确的维度性创建的（对于这个
    Sentence Transformer 模型是 `768`）。
- en: 'Next, we define a sample search query and encode it into an embedding using
    the same Sentence Transformer model. The query embedding is also converted to
    float32:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个示例搜索查询，并使用相同的 Sentence Transformer 模型将其编码成嵌入。查询嵌入也被转换为 float32：
- en: '[PRE10]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, we perform similarity search using the `index.search()` method. We
    search for the two most similar sentences (*k*=2). The method returns the distances
    and the indices of the nearest neighbors. We then print the indices and distances
    of the nearest neighbors found:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用 `index.search()` 方法执行相似度搜索。我们搜索两个最相似的句子（*k*=2）。该方法返回最近邻的距离和索引。然后我们打印出找到的最近邻的索引和距离：
- en: '[PRE11]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following is sample output you might get from running the preceding code
    blocks:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从运行前面的代码块可能得到的示例输出：
- en: '[PRE12]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This demonstrates how semantic similarity search works using Sentence Transformers
    and Faiss. Note that actual numbers will vary depending on the hardware, model
    versions, and runtime conditions.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了如何使用 Sentence Transformers 和 Faiss 进行语义相似度搜索。请注意，实际数字将根据硬件、模型版本和运行条件而变化。
- en: Here’s what’s happening.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是发生的事情。
- en: The query `"What is the dog doing?"` is embedded and compared against all embedded
    sentences in the list. Faiss retrieves the two most semantically similar sentences
    based on Euclidean (L2) distance in the embedding space. The smallest distance
    indicates the highest similarity. In this example, the sentence about the man
    walking his dog is closest to the query, which makes sense semantically.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 查询 `"What is the dog doing?"` 被嵌入并与列表中的所有嵌入句子进行比较。Faiss 根据嵌入空间中的欧几里得（L2）距离检索出两个最语义相似的句子。最小的距离表示最高的相似度。在这个例子中，关于男人遛狗的句子与查询在语义上最接近，这是有意义的。
- en: If you’re running this on your machine, your values may look different due to
    the non-determinism in model initialization and floating-point precision, but
    the closest sentence should consistently be the one most semantically related
    to the query.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在自己的机器上运行这个程序，由于模型初始化的非确定性和浮点精度，你的值可能会有所不同，但最接近的句子应该始终是与查询最语义相关的句子。
- en: Important
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 重要
- en: '`IndexIVFFlat` or `IndexHNSWFlat` from Faiss, to improve search speed.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Faiss 的 `IndexIVFFlat` 或 `IndexHNSWFlat` 可以提高搜索速度。
- en: '`faiss-gpu` to significantly speed up indexing and searching.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `faiss-gpu` 可以显著加快索引和搜索的速度。
- en: '**Data preprocessing**: For real-world applications, you might need to perform
    additional data preprocessing steps, such as lowercasing, removing punctuation,
    or stemming/lemmatization, depending on your specific needs and the nature of
    your data.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据预处理**: 对于实际应用，你可能需要执行额外的数据预处理步骤，例如小写化、去除标点符号或词干提取/词形还原，具体取决于你的具体需求和数据性质。'
- en: '**Distance metrics**: Faiss supports different distance metrics. We used L2
    distance here, but you could also use the inner product (IndexFlatIP) or other
    metrics depending on how your embeddings are generated and what kind of similarity
    you want to measure.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**距离度量**: Faiss 支持不同的距离度量。这里我们使用了 L2 距离，但你也可以使用内积（IndexFlatIP）或其他度量，具体取决于你的嵌入是如何生成的以及你想要测量哪种相似度。'
- en: '**Vector databases**: For production-level systems, consider using a dedicated
    vector database such as Pinecone, Weaviate, or Milvus to manage your embeddings
    and indexes more efficiently. They often provide features such as automatic indexing,
    scaling, and data management, which simplify the deployment of similarity search
    applications.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**向量数据库**：对于生产级系统，考虑使用专门的向量数据库，如Pinecone、Weaviate或Milvus，以更有效地管理您的嵌入和索引。它们通常提供自动索引、扩展和数据管理等功能，这些功能简化了相似性搜索应用的部署。'
- en: 'We’ve covered the fundamentals of embeddings, indexing, and searching with
    Faiss, along with important considerations for real-world implementation. Now,
    let’s turn our attention to another crucial aspect of RAG: query formulation.
    We’ll explore various strategies to refine and expand user queries, ultimately
    leading to more effective information retrieval from the knowledge base.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了使用Faiss进行嵌入、索引和搜索的基础知识，以及现实世界实施的重要考虑因素。现在，让我们将注意力转向RAG的另一个关键方面：查询公式。我们将探讨各种策略来精炼和扩展用户查询，最终导致从知识库中检索更有效的信息。
- en: Query formulation strategies in LLM-based RAG
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于LLM的RAG查询公式策略
- en: 'Query formulation strategies in LLM-based RAG systems aim to enhance retrieval
    by improving the expressiveness and coverage of user queries. Common expansion
    strategies include the following:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的RAG系统中的查询公式策略旨在通过提高用户查询的表达性和覆盖范围来增强检索。常见的扩展策略包括以下内容：
- en: '**Synonym and paraphrase expansion**: This involves generating semantically
    equivalent alternatives using LLMs or lexical resources. For example, expanding
    “climate change impact” to include “effects of global warming” or “environmental
    consequences of climate change” can help match a broader range of documents.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同义词和释义扩展**：这涉及到使用LLM或词汇资源生成语义等效的替代方案。例如，将“气候变化影响”扩展到包括“全球变暖的影响”或“气候变化的环境后果”可以帮助匹配更广泛的文档。'
- en: '**Contextual reformulation**: LLMs can reinterpret queries by inferring their
    intent based on conversational or document context. This helps in tailoring the
    query to better align with how the information might be expressed in the knowledge
    base.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文重构**：LLM可以通过推断对话或文档上下文中的意图来重新解释查询。这有助于调整查询以更好地与知识库中可能表达的信息相匹配。'
- en: '**Pseudo-relevance feedback**: Also known as blind relevance feedback, this
    strategy involves running an initial query, analyzing the top-ranked documents
    for salient terms, and using these terms to expand the query. While effective,
    it requires safeguards against topic drift.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**伪相关性反馈**：也称为盲相关性反馈，这种策略涉及运行初始查询，分析排名靠前的文档中的显著术语，并使用这些术语来扩展查询。虽然有效，但需要防止主题漂移。'
- en: '**Template-based augmentation**: Useful in structured domains, this method
    uses domain-specific templates or patterns to systematically generate variants.
    For example, a medical query about “treatment for hypertension” might also include
    “hypertension therapy” or “managing high blood pressure.”'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于模板的增强**：在结构化领域中很有用，这种方法使用特定领域的模板或模式来系统地生成变体。例如，关于“高血压治疗”的医疗查询也可能包括“高血压疗法”或“管理高血压”。'
- en: '**Entity and concept linking**: Named entities and domain concepts in the query
    are identified and replaced or augmented with their aliases, definitions, or hierarchical
    relations. This is often guided by ontologies or knowledge graphs.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实体和概念链接**：在查询中识别命名实体和领域概念，并用它们的别名、定义或层次关系替换或增强。这通常由本体或知识图指导。'
- en: '**Prompt-based query rewriting**: With LLMs, prompts can be crafted to explicitly
    instruct the model to generate reformulated queries. This is particularly useful
    in multilingual or multi-domain RAG systems, where queries need to be adapted
    to match the style and vocabulary of the target corpus.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于提示的查询重写**：使用LLM，可以精心制作提示来明确指示模型生成重构后的查询。这在多语言或多领域RAG系统中特别有用，其中查询需要适应目标语料库的风格和词汇。'
- en: Each strategy contributes differently to recall and precision. Choosing or combining
    them depends on the structure and variability of the underlying knowledge base.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 每种策略都对召回率和精确度有不同的贡献。选择或组合它们取决于底层知识库的结构和可变性。
- en: 'In the following code, the `QueryExpansionRAG` implementation uses a prompt-based
    query rewriting strategy powered by a pre-trained sequence-to-sequence language
    model (specifically, T5-small). This approach instructs the model to generate
    alternative phrasings of the input query by prefixing the prompt with `"expand
    query:"`. The generated expansions reflect paraphrastic reformulation, where the
    model synthesizes semantically related variations to increase retrieval coverage:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，`QueryExpansionRAG`实现使用了一个由预训练的序列到序列语言模型（具体来说，是T5-small）驱动的基于提示的查询重写策略。这种方法指示模型通过在提示前加上“expand
    query:”来生成输入查询的替代表述。生成的扩展反映了释义性改写，其中模型综合了语义相关的变体以增加检索覆盖范围：
- en: '[PRE13]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This code defines a `QueryExpansionRAG` class that extends a RAG framework by
    incorporating query expansion using a pre-trained T5 model. When a user submits
    a query, the `expand_query` method uses the T5 model through a text-to-text generation
    pipeline to produce multiple alternative phrasings of the query, which are then
    combined with the original query. The `retrieve` method iterates over these expanded
    queries, retrieving documents for each one and aggregating the results while removing
    duplicates. This approach increases the chances of retrieving relevant content
    by broadening the lexical and semantic scope of the original query, making it
    especially effective when the knowledge base expresses information in varied ways.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码定义了一个`QueryExpansionRAG`类，它通过结合使用预训练的T5模型来扩展RAG框架，实现了查询扩展。当用户提交查询时，`expand_query`方法通过文本到文本生成管道使用T5模型，生成查询的多个替代表述，然后将这些表述与原始查询结合。`retrieve`方法遍历这些扩展查询，为每个查询检索文档，并聚合结果同时去除重复项。这种方法通过扩大原始查询的词汇和语义范围，增加了检索相关内容的机会，当知识库以多种方式表达信息时，这种方法尤其有效。
- en: Keep in mind that poorly expanded queries can introduce noise and reduce retrieval
    precision. In this implementation, expansions generated by the T5 model are combined
    with the original query, increasing coverage. However, to maintain a balance,
    consider reranking results using similarity scores or assigning lower weights
    to generated expansions during retrieval. This helps ensure that expansions improve
    recall without compromising the alignment with the original intent.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，扩展不当的查询可能会引入噪声并降低检索精确率。在此实现中，T5模型生成的扩展与原始查询结合，增加了覆盖范围。然而，为了保持平衡，考虑使用相似度分数重新排序结果，或者在检索期间为生成的扩展分配较低的权重。这有助于确保扩展提高了召回率，同时不会损害与原始意图的对齐。
- en: 'We’ve seen how query expansion can enhance retrieval in RAG systems, but it’s
    essential to manage the trade-off between recall and precision. Now, let’s shift
    our focus to the other side of the RAG pipeline: integrating the retrieved information
    with the LLM to generate the final answer. We’ll explore how to craft prompts
    that effectively leverage the retrieved context.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了查询扩展如何增强RAG系统中的检索，但管理召回率和精确率之间的权衡是至关重要的。现在，让我们将我们的重点转向RAG管道的另一端：将检索到的信息与LLM整合以生成最终答案。我们将探讨如何构建能够有效利用检索到的上下文的提示。
- en: Integrating retrieved information with LLM generation
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将检索到的信息与LLM生成整合
- en: 'To integrate retrieved information with LLM generation, we can create a prompt
    that incorporates the retrieved documents:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 要将检索到的信息与LLM生成整合，我们可以创建一个包含检索到的文档的提示：
- en: '[PRE14]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the preceding code snippet, the `GenerativeRAG` class extends a RAG pipeline
    by integrating a causal language model for answer generation. It inherits from
    `QueryExpansionRAG`, which already provides retrieval functionality, and adds
    a generator component using Hugging Face’s `AutoModelForCausalLM`. In the constructor,
    it initializes the generator model and tokenizer based on the given model name.
    The `generate_response` method first retrieves relevant documents for a given
    query, concatenates them into a single context string, and constructs a prompt
    that combines this context with the question. This prompt is then tokenized and
    passed into the language model, which generates a text continuation as the answer.
    The final output is obtained by decoding the generated tokens into a string. This
    modular structure separates the retrieval and generation steps, making it easy
    to scale or replace individual components depending on the task or model performance
    requirements.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，`GenerativeRAG`类通过集成用于答案生成的因果语言模型扩展了RAG管道。它继承自`QueryExpansionRAG`，后者已经提供了检索功能，并添加了一个使用Hugging
    Face的`AutoModelForCausalLM`的生成器组件。在构造函数中，它根据给定的模型名称初始化生成器模型和分词器。`generate_response`方法首先检索给定查询的相关文档，将它们连接成一个单一上下文字符串，并构建一个将此上下文与问题结合的提示。然后，该提示被分词并传递给语言模型，该模型生成作为答案的文本续写。最终输出是通过将生成的标记解码成字符串获得的。这种模块化结构将检索和生成步骤分开，使得根据任务或模型性能要求轻松扩展或替换单个组件。
- en: Having covered the basics of RAG systems, we will now focus on real-world challenges,
    such as scalability, dynamic updates, and multilingual retrieval. Specifically,
    we will discuss how a sharded indexing architecture can improve retrieval efficiency
    at scale, highlighting its impact on performance in data-heavy environments.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍了RAG系统的基本知识后，我们将现在关注现实世界的挑战，例如可扩展性、动态更新和多语言检索。具体来说，我们将讨论如何通过分片索引架构提高大规模检索效率，突出其在数据密集型环境中的性能影响。
- en: Challenges and opportunities in RAG for LLMs
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs在RAG中的挑战和机遇
- en: 'Some key challenges and opportunities in RAG include the following:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: RAG中的一些关键挑战和机遇包括以下内容：
- en: '**Scalability**: Efficiently handling very large knowledge bases.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**: 高效地处理非常大的知识库。'
- en: '**Dynamic knowledge updating**: Keeping the knowledge base current.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态知识更新**: 保持知识库的时效性。'
- en: '**Cross-lingual RAG**: Retrieving and generating in multiple languages.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跨语言RAG**: 在多种语言中进行检索和生成。'
- en: '**Multi-modal RAG**: Incorporating non-text information in retrieval and generation.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多模态RAG**: 在检索和生成过程中结合非文本信息。'
- en: Keep in mind that cross-lingual and multi-modal RAG will need specialized retrieval
    pipelines or adapters because standard retrieval approaches often struggle with
    semantic matching across languages or modalities, requiring dedicated components
    that can properly encode, align, and retrieve relevant information regardless
    of the source language or format while maintaining contextual understanding and
    relevance.
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请记住，跨语言和多模态RAG需要专门的检索管道或适配器，因为标准的检索方法通常在跨语言或模态的语义匹配上遇到困难，需要专门的组件来正确编码、对齐和检索相关信息，无论源语言或格式如何，同时保持上下文理解和相关性。
- en: '**Explainable RAG**: Providing transparency in the retrieval and generation
    process.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释RAG**: 在检索和生成过程中提供透明度。'
- en: 'To keep this chapter from becoming too long, in this section, we will only
    show an example on how to address the scalability challenge by implementing a
    sharded index. A sharded index refers to a distributed data structure that partitions
    the index into multiple smaller, manageable segments called shards, each stored
    and maintained independently across different nodes or storage units. This approach
    enables parallel processing, reduces lookup time, and mitigates bottlenecks associated
    with centralized indexing, making it suitable for handling large-scale datasets
    or high query volumes commonly encountered in AI applications:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使本章内容不过于冗长，在本节中，我们将仅展示一个示例，说明如何通过实现分片索引来解决可扩展性挑战。分片索引指的是将索引分割成多个较小的、可管理的片段，称为分片，每个分片独立存储和维护在不同的节点或存储单元上。这种方法可以实现并行处理，减少查找时间，并缓解与集中式索引相关的瓶颈，使其适用于处理在AI应用中常见的大规模数据集或高查询量：
- en: '[PRE15]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the preceding code, the scalability is handled by dividing the knowledge
    base into multiple smaller indexes, or shards, each containing a portion of the
    overall data. This approach reduces the computational and memory burden on any
    single index and allows retrieval operations to remain efficient even as the dataset
    grows. During a query, the system embeds the query once, searches across all shards
    independently, and then merges the results. This design avoids bottlenecks that
    would arise from searching a single large index and makes it feasible to scale
    to much larger knowledge bases. It also lays the groundwork for further optimizations,
    such as parallelizing shard queries or distributing them across multiple machines.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，可扩展性是通过将知识库划分为多个较小的索引或分片来处理的，每个分片包含整体数据的一部分。这种方法减轻了单个索引的计算和内存负担，并允许检索操作在数据集增长的情况下保持高效。在查询过程中，系统将查询嵌入一次，独立地对所有分片进行搜索，然后合并结果。这种设计避免了搜索单个大型索引时可能出现的瓶颈，并使得扩展到更大的知识库成为可能。它还为进一步的优化奠定了基础，例如并行化分片查询或将它们分布到多台机器上。
- en: Summary
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: RAG is a powerful technique for enhancing LLMs with external knowledge. By implementing
    the strategies and techniques discussed in this chapter, you can create more informed
    and accurate language models capable of accessing and utilizing vast amounts of
    information.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 是一种强大的技术，用于通过外部知识增强大型语言模型（LLMs）。通过实施本章讨论的策略和技术，你可以创建更明智、更准确的语言模型，这些模型能够访问和利用大量信息。
- en: As we move forward, the next chapter will explore graph-based RAG for LLMs,
    which extends the RAG concept to leverage structured knowledge representations.
    This will further enhance the ability of LLMs to reason over complex relationships
    and generate more contextually appropriate responses.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们继续前进，下一章将探讨基于图的大型语言模型（LLMs）的 RAG，这扩展了 RAG 概念以利用结构化知识表示。这将进一步增强 LLMs 在复杂关系上进行推理和生成更符合上下文响应的能力。
