- en: '26'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Retrieval-Augmented Generation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Retrieval-augmented generation** (**RAG**) is a technique that enhances the
    performance of AI models, particularly in tasks that require knowledge or data
    not contained within the model’s pre-trained parameters. It combines the strengths
    of both retrieval-based models and generative models. The retrieval component
    fetches relevant information from external sources, such as databases, documents,
    or web content, and the generative component uses this information to produce
    more accurate, contextually enriched responses.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: RAG is implemented by integrating a retrieval mechanism with a language model.
    The process begins by querying a knowledge base or external resource for relevant
    documents or snippets. These retrieved pieces of information are then fed into
    the language model, which generates a response by incorporating both the prompt
    and the retrieved data. This approach improves the model’s ability to answer questions
    or solve problems with up-to-date or domain-specific information that it would
    otherwise lack.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll introduce you to RAG. You’ll learn how to implement a
    simple RAG system that can enhance LLM outputs with relevant external information.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: The key benefits of RAG include enhanced factual accuracy, access to current
    information, improved domain-specific knowledge, and reduced hallucination in
    LLM outputs.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll cover embedding and indexing techniques of vector databases
    for efficient retrieval, query formulation strategies, and methods for integrating
    retrieved information with LLM generation. By the end of this chapter, you’ll
    be able to implement basic RAG systems to augment your LLMs with external knowledge.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Building a simple RAG system for LLMs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embedding and indexing techniques for LLM retrieval
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Query formulation strategies in LLM-based RAG
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating retrieved information with LLM generation
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges and opportunities in RAG for LLMs
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a simple RAG system for LLMs
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section provides a practical illustration of a simple RAG system, leveraging
    the robust search capabilities of **SerpApi**, the semantic understanding of sentence
    embeddings, and the generative prowess of OpenAI’s GPT-4o model. SerpApi is a
    web scraping API that provides real-time access to search engine results, offering
    structured data for Google, Bing, and other platforms without the need for manual
    scraping.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Through this example, we will explore the fundamental components of a RAG system,
    including query-based web searching, snippet extraction and ranking, and, ultimately,
    the generation of a comprehensive answer using a state-of-the-art LLM, highlighting
    the interplay between these elements in a step-by-step manner.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for the simple RAG system we’ll be building contains the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '**SerpApi**: To find relevant web pages based on the user’s query.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence embeddings**: To extract the most relevant snippets from the search
    results using sentence embeddings and cosine similarity. Sentence embeddings are
    dense numerical representations of text that capture semantic meaning by mapping
    words, phrases, or entire sentences into high-dimensional vector space, where
    similar meanings are positioned closer together. Cosine similarity measures the
    angle between these embedding vectors (ranging from -1 to 1), rather than their
    magnitude, making it an effective way to evaluate semantic similarity regardless
    of text length; when two embeddings have a cosine similarity close to 1, they’re
    highly similar in meaning, while values closer to 0 indicate unrelated content
    and negative values suggest opposing meanings. This combination of techniques
    powers many modern **natural language processing** (**NLP**) applications, from
    search engines and recommendation systems to language translation and content
    clustering.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenAI’s GPT-4o**: To generate a comprehensive and coherent answer based
    on the retrieved snippets (context) and the original query.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, let’s install the following dependencies:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding command, we install `serpapi` for searching, `sentence_transformers`
    for embedding, and `openai` for accessing GPT-4o.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let us see how a complete RAG system is implemented using search APIs,
    embeddings, and an LLM:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'We first import the installed libraries along with `torch` for tensor operations.
    The code snippet also sets up API keys for SerpApi and OpenAI. Remember to replace
    the placeholders with your actual API keys:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We then initialize the search engine and Sentence Transformer. The following
    code defines the search function to perform a Google search using SerpApi and
    initializes the Sentence Transformer model (`all-mpnet-base-v2`) for creating
    sentence embeddings:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we retrieve relevant snippets. We define the `retrieve_snippets` function,
    which takes the search results, extracts snippets, computes their embeddings,
    and calculates the cosine similarity between the query embedding and each snippet
    embedding. It then returns the top *k* snippets that are most similar to the query:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We then define the `generate_answer` function to generate an answer using GPT-4o.
    This is the core of the generation part of our RAG system:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This function constructs a structured prompt for an LLM to generate an answer
    constrained strictly to a given context. It formats the conversation as a system-user
    message pair, instructing the model to act as a subject matter expert and restrict
    its answer to the supplied information, explicitly avoiding speculation. If the
    information isn’t present, the system is directed to return a fallback message
    indicating that the answer couldn’t be found. The query and context are embedded
    directly into the user message, and the LLM (in this case, `gpt-4o`) is queried
    with a moderate creativity level via `temperature=0.7` and a response length cap
    of `256` tokens. This design makes the function reliable for context-grounded
    Q&A tasks, particularly in RAG pipelines or constrained-answering settings such
    as document QA or compliance tools.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here’s the main RAG function and example usage:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This code defines the `rag_system` function, which orchestrates the entire
    process: searching, retrieving snippets, and generating an answer. It then demonstrates
    how to use `rag_system` with an example query, printing the generated answer to
    the console'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `rag_system` function answers a query by first searching for relevant information
    using `search(query)` and then extracting relevant snippets through the API called
    `retrieve_snippets(query, search_results)`. If no snippets are found, it returns
    a message indicating no information was found. If snippets are available, they
    are combined into a single context string and used to generate an answer through
    `generate_answer(query, context)`. Finally, the function returns the generated
    answer based on the context. In the example usage, the function is called with
    the query `"What are the latest advancements in quantum computing?"` and will
    return a generated response based on the relevant search results. In real production
    systems, we should implement retries and error handling around `retrieve_snippets`
    API calls.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Before we move on to the next section, here are some things to remember:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '**API keys**: Make sure you have valid API keys for both SerpApi and OpenAI
    and have replaced the placeholders in the code.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenAI costs**: Be mindful of OpenAI API usage costs. GPT-4o can be more
    expensive than other models.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt engineering**: The quality of the generated answer heavily depends
    on the prompt you provide to GPT-4o. You might need to experiment with different
    prompts to get the best results. Consider adding instructions about the desired
    answer format, length, or style.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`try-except` blocks) to handle potential issues such as network problems, API
    errors, or invalid inputs.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advanced techniques**: This is a basic RAG system. You can improve it further
    by doing the following:'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Better snippet selection**: Consider factors such as source diversity, factuality,
    and snippet length'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterative retrieval**: Retrieve more context if the initial answer is not
    satisfactory'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-tuning**: Fine-tune a smaller, more specialized language model on your
    specific domain for potentially better performance and lower costs'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We’ve successfully built a simple RAG system, covering the core components
    of retrieval and generation. Now that we have a functional RAG system, let’s dive
    deeper into the crucial techniques that enable efficient retrieval from large
    datasets: embedding and indexing. We’ll explore different methods for representing
    text semantically and organizing these representations for fast similarity search.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings and indexing for retrieval in LLM applications
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Embedding and indexing techniques provide efficient and effective retrieval
    in RAG-based LLM applications. They allow LLMs to quickly find and utilize relevant
    information from vast amounts of data. The following subsections provide a breakdown
    of common techniques.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Embeddings are numerical vector representations of data, such as text, images,
    or audio, that map complex, high-dimensional data into a continuous vector space
    where similar items are positioned close to each other. These vectors capture
    the underlying patterns, relationships, and semantic properties of the data, making
    it easier for machine learning models to understand and process. For text, for
    example, word embeddings transform words or phrases into dense vectors that represent
    their meaning in a way that reflects semantic relationships, such as synonyms
    being closer together in the vector space. Embeddings are typically learned from
    large datasets through techniques such as neural networks, and they serve as a
    foundation for tasks such as information retrieval, classification, clustering,
    and recommendation systems. By reducing the dimensionality of data while preserving
    important features, embeddings enable models to generalize better and make sense
    of varied input data efficiently.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: For LLMs, text embeddings are most relevant. They are generated by passing text
    through a neural network (like the Sentence Transformer models we used in the
    previous section).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need embeddings?
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Embeddings are important for RAG applications for the following reasons:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '**Semantic search**: Embeddings enable semantic search, where you find information
    based on meaning rather than just keyword matching'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contextual understanding**: LLMs can use embeddings to understand the relationships
    between different pieces of information, improving their ability to reason and
    generate relevant responses'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficient retrieval**: When combined with appropriate indexing, embeddings
    allow for the fast retrieval of relevant information from large datasets'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common embedding technologies
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Several embedding technologies are commonly used in RAG systems, and these
    vary in their underlying models, methods, and suitability for different applications.
    Here are some prominent embedding technologies for RAG:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-trained transformer-based embeddings (e.g., BERT, RoBERTa, and T5)**:
    Transformer models such as **Bidirectional Encoder Representations from Transformers**
    (**BERT**) and its variants, such as RoBERTa and T5, have been widely used to
    generate dense, contextual embeddings for text. These models are fine-tuned on
    large corpora and capture a rich understanding of language semantics. In a RAG
    system, these embeddings can be used to retrieve relevant passages from a document
    store based on semantic similarity. The embeddings are typically high-dimensional
    and are generated by feeding text through the transformer model to produce a fixed-size
    vector.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence-BERT (SBERT)**: A variation of BERT designed for sentence-level
    embeddings, SBERT focuses on optimizing the model for tasks such as semantic textual
    similarity and clustering. It uses a Siamese network architecture to map sentences
    into a dense vector space where semantically similar sentences are closer together.
    This makes it particularly effective for tasks such as information retrieval in
    RAG, where retrieving semantically relevant passages from a large corpus is essential.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Facebook AI Similarity Search (Faiss)**: Faiss is a library developed by
    Facebook AI Research that provides efficient similarity search through **approximate
    nearest neighbor** (**ANN**) search. Faiss is not an embedding technology by itself
    but works in conjunction with various embedding models to index and search over
    large collections of vectors. When used in RAG, Faiss enables the fast retrieval
    of relevant documents or passages by comparing the similarity of their embeddings
    against a query embedding.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dense retriever models (e.g., DPR and ColBERT)**: **Dense Passage Retrieval**
    (**DPR**) is an approach to information retrieval that uses two separate encoders
    (usually BERT-based models) to encode both queries and passages into dense vectors.
    DPR outperforms traditional sparse retrieval methods by leveraging the contextual
    knowledge encoded in dense embeddings. ColBERT, on the other hand, is another
    dense retrieval model that balances the efficiency of dense retrieval and the
    effectiveness of traditional methods. These models are especially useful for RAG
    when retrieving high-quality passages that are semantically related to a query.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contrastive Language-Image Pre-Training (CLIP)**: While originally designed
    for multimodal applications (text and image), CLIP has been adapted for text-only
    tasks as well. It learns embeddings by aligning text and image data in a shared
    vector space. Although CLIP is primarily used for multimodal tasks, its ability
    to represent language in a common space with images provides a flexible embedding
    framework that can be used in RAG, especially when working with multimodal data.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep semantic similarity models (e.g., USE and InferSent)**: Models such
    as the **Universal Sentence Encoder** (**USE**) and InferSent generate sentence
    embeddings by capturing deeper semantic meaning, which can be used for various
    NLP tasks, including document retrieval. These models produce fixed-size vector
    representations that can be compared for similarity, making them useful for RAG
    when paired with retrieval systems.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Doc2Vec**: An extension of Word2Vec, Doc2Vec generates embeddings for entire
    documents rather than individual words. It maps variable-length text into a fixed-size
    vector, which can be used to retrieve semantically similar documents or passages.
    Though not as powerful as transformer-based models in terms of semantic richness,
    Doc2Vec is still an effective tool for more lightweight retrieval tasks in RAG
    applications.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embedding-based search engines (e.g., Elasticsearch with dense vectors)**:
    Some modern search engines, such as Elasticsearch, have integrated support for
    dense vectors alongside traditional keyword-based indexing. Elasticsearch can
    store and retrieve text embeddings, allowing for more flexible and semantically
    aware searches. When used with RAG, these embeddings can be used to rank documents
    by their relevance to a query, improving retrieval performance.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenAI embeddings (e.g., GPT-based models)**: OpenAI’s embeddings, derived
    from models such as GPT-3, are also used for RAG tasks. These embeddings are based
    on the language model’s ability to generate high-quality text representations,
    which can be indexed and searched over large corpora. While they are not as specifically
    tuned for retrieval as some other models (such as DPR), they are highly flexible
    and can be used in general-purpose RAG applications.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These embedding technologies provide various advantages depending on the specific
    requirements of a RAG system, such as retrieval speed, model accuracy, and the
    scale of the data being processed. Each of them can be fine-tuned and optimized
    for specific use cases, and the choice of embedding technology will depend on
    factors such as the nature of the documents being retrieved, computational resources,
    and latency requirements.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Indexing
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Indexing is the process of organizing embeddings in a data structure that allows
    for fast similarity search. Think of it like the index of a book, but for vectors
    instead of words.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Using a more detailed description using LLM terminology, vector indexing technologies
    optimize embedding storage and retrieval by creating specialized data structures
    that organize high-dimensional vectors according to their similarity relationships,
    rather than sequential order. These structures—whether graph-based (connecting
    similar vectors through navigable pathways), tree-based (recursively partitioning
    the vector space), or quantization-based (compressing vectors while preserving
    similarity)—all serve the fundamental purpose of transforming an otherwise prohibitively
    expensive exhaustive search into a manageable process by strategically limiting
    the search space, enabling vector databases to handle billions of embeddings with
    sub-second query times while maintaining an acceptable trade-off between speed,
    memory efficiency, and result accuracy.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Why is indexing important?
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Indexing is important for LLMs for the following reasons:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '**Speed**: Without indexing, you would have to compare a query embedding to
    every single embedding in your dataset, which is computationally expensive and
    slow'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Indexing allows LLM applications to scale to handle massive
    datasets containing millions or even billions of data points'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common indexing techniques
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s look at some of the common indexing techniques for LLMs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'For a visual diagram of these index techniques, I recommend you check out the
    following website: [https://kdb.ai/learning-hub/articles/indexing-basics/](https://kdb.ai/learning-hub/articles/indexing-basics/)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '**Flat index (****brute force)**:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How it works**: Stores all embeddings in a simple list or array. During a
    search, it calculates the distance (e.g., cosine similarity) between the query
    embedding and every embedding in the index.'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pros**: Simple to implement and perfect accuracy (finds the true nearest
    neighbors).'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**: Slow and computationally expensive for large datasets, as it requires
    an exhaustive search.'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Suitable for**: Very small datasets or when perfect accuracy is an absolute
    requirement.'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inverted file** **index (IVF)**:'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How** **it works**:'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering**: Divides the embedding space into clusters using algorithms
    such as *k*-means'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inverted index**: Creates an inverted index that maps each cluster centroid
    to a list of the embeddings belonging to that cluster'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Search**:'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finds the nearest cluster centroid(s) to the query embedding
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Only searches within those clusters, significantly reducing the search space
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pros**: Faster than a flat index; relatively simple to implement'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**: Approximate (might not always find the true nearest neighbors); accuracy
    depends on the number of clusters'
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Suitable for**: Medium-sized datasets where a good balance between speed
    and accuracy is needed'
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchical navigable small** **world (HNSW)**:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How** **it works**:'
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graph-based**: Constructs a hierarchical graph where each node represents
    an embedding.'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layers**: The graph has multiple layers, with the top layer having long-range
    connections (for faster traversal) and the bottom layer having short-range connections
    (for accurate search).'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Search**: Starts at a random node in the top layer and greedily moves towards
    the query embedding by exploring connections. The search progresses down the layers,
    refining the results.'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pros**: Very fast and accurate; often considered the state of the art for
    ANN search'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**: More complex to implement than IVF, and higher memory overhead due
    to the graph structure'
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Suitable for**: Large datasets where both speed and accuracy are crucial'
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Product** **quantization (PQ)**:'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How** **it works**:'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Subvectors**: Divides each embedding into multiple subvectors.'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Codebooks**: Creates separate codebooks for each subvector using clustering.
    Each codebook contains a set of representative subvectors (centroids).'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encoding**: Encodes each embedding by replacing its subvectors with the closest
    centroids from the corresponding codebooks. This creates a compressed representation
    of the embedding.'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Search**: Calculates the approximate distance between the query and the encoded
    embeddings using pre-computed distances between the query’s subvectors and the
    codebook centroids.'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pros**: Significantly reduces memory usage by compressing embeddings; fast
    search.'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**: Approximate, and accuracy depends on the number of subvectors and
    the size of the codebooks.'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Suitable for**: Very large datasets where memory efficiency is a primary
    concern.'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Locality sensitive** **hashing (LSH)**:'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How it works**: Uses hash functions to map similar embeddings to the same
    “bucket” with high probability'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pros**: Relatively simple; can be distributed across multiple machines'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**: Approximate, and performance depends on the choice of hash functions
    and the number of buckets'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Suitable for**: Very large, high-dimensional datasets'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve covered different indexing methods, let’s introduce some popular
    libraries and tools that implement these indexing techniques, making them easier
    to use in practice. This will provide a practical perspective on how to leverage
    these technologies in your RAG applications.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some libraries and tools for implementing indexing:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '**Faiss**: A highly optimized library developed by Facebook AI for efficient
    similarity search and clustering of dense vectors. It implements many of the indexing
    techniques mentioned previously (flat, IVF, HNSW, and PQ).'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Approximate Nearest Neighbors Oh Yeah (Annoy)**: Another popular library
    for ANN search, known for its ease of use and good performance. It uses a tree-based
    approach.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalable Nearest Neighbors (ScaNN)**: A library developed by Google, designed
    for large-scale, high-dimensional datasets.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vespa.ai**: Provide tools to query, organize, and make inferences in vectors,
    tensors, text, and structured data. It is used by [https://www.perplexity.ai/](https://www.perplexity.ai/).'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pinecone, Weaviate, Milvus, Qdrant**: Vector databases designed specifically
    for storing and searching embeddings. They handle indexing, scaling, and other
    infrastructure concerns.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The best embedding and indexing techniques for your LLM application will depend
    on several factors:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset size**: For small datasets, a flat index might be sufficient. For
    large datasets, consider HNSW, IVF, or PQ.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speed requirements**: If low latency is critical, HNSW is generally the fastest
    option.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accuracy requirements**: If perfect accuracy is required, a flat index is
    the only choice, but it’s not scalable. HNSW often provides the best accuracy
    among approximate methods.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory constraints**: If memory is limited, PQ can significantly reduce storage
    requirements.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Development effort**: Faiss and Annoy offer a good balance between performance
    and ease of implementation. Vector databases simplify infrastructure management.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By carefully considering these factors and understanding the strengths and weaknesses
    of each technique and library, you can choose the most appropriate embedding and
    indexing methods to build efficient and effective LLM applications.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: We’ll now demonstrate an example involving embedding, indexing, and searching
    using Faiss, a powerful library for efficient similarity search. I’ll use the
    `all-mpnet-base-v2` Sentence Transformer model to generate embeddings. Since the
    code will be more than 20 lines, I’ll break it down into blocks with explanations
    preceding each block.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Example code demonstrating embedding, indexing, and searching
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we’ll be showing the code for a typical workflow for using
    embeddings and indexing to enable fast similarity search within a collection of
    text documents: Here’s what it does:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '**Loads a Sentence Transformer model**: Initializes a pre-trained model for
    generating sentence embeddings.'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Creates sample data**: Defines a list of example sentences (you would replace
    this with your actual data).'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`SentenceTransformer` to create embeddings for each sentence.'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`IndexFlatL2` for a flat L2 distance index in this example) to store the embeddings.'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Adds embeddings to the index**: Adds the generated embeddings to the Faiss
    index.'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Defines a search query**: Sets a sample query for which we want to find similar
    sentences.'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Encodes the query**: Creates an embedding for the search query using the
    same Sentence Transformer model.'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Performs a search**: Uses the Faiss index to search for the *k* most similar
    embeddings to the query embedding.'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prints the results**: Displays the indices and distances of the *k* nearest
    neighbors found in the index.'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Before we check out the code, let us install the following dependencies:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let us now see the code example:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the necessary libraries—`sentence_transformers` for creating
    embeddings and `faiss` for indexing and searching—and load the `all-mpnet-base-v2`
    Sentence Transformer model:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We then prepare the data by defining some sample sentences (you can replace
    these with your actual data) and then use the Sentence Transformer model to generate
    embeddings for each sentence (the embeddings are converted to float32, which is
    required by Faiss):'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We then create a Faiss index and add the embeddings to it:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, we’re using IndexFlatL2, which is a flat index that uses L2 distance (Euclidean
    distance) for similarity comparisons. This type of index provides accurate results
    but can be slow for very large datasets. The index is created with the correct
    dimensionality (`768` for this Sentence Transformer model).
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we define a sample search query and encode it into an embedding using
    the same Sentence Transformer model. The query embedding is also converted to
    float32:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, we perform similarity search using the `index.search()` method. We
    search for the two most similar sentences (*k*=2). The method returns the distances
    and the indices of the nearest neighbors. We then print the indices and distances
    of the nearest neighbors found:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following is sample output you might get from running the preceding code
    blocks:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This demonstrates how semantic similarity search works using Sentence Transformers
    and Faiss. Note that actual numbers will vary depending on the hardware, model
    versions, and runtime conditions.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Here’s what’s happening.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: The query `"What is the dog doing?"` is embedded and compared against all embedded
    sentences in the list. Faiss retrieves the two most semantically similar sentences
    based on Euclidean (L2) distance in the embedding space. The smallest distance
    indicates the highest similarity. In this example, the sentence about the man
    walking his dog is closest to the query, which makes sense semantically.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: If you’re running this on your machine, your values may look different due to
    the non-determinism in model initialization and floating-point precision, but
    the closest sentence should consistently be the one most semantically related
    to the query.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Important
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '`IndexIVFFlat` or `IndexHNSWFlat` from Faiss, to improve search speed.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '`faiss-gpu` to significantly speed up indexing and searching.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '**Data preprocessing**: For real-world applications, you might need to perform
    additional data preprocessing steps, such as lowercasing, removing punctuation,
    or stemming/lemmatization, depending on your specific needs and the nature of
    your data.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '**Distance metrics**: Faiss supports different distance metrics. We used L2
    distance here, but you could also use the inner product (IndexFlatIP) or other
    metrics depending on how your embeddings are generated and what kind of similarity
    you want to measure.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '**Vector databases**: For production-level systems, consider using a dedicated
    vector database such as Pinecone, Weaviate, or Milvus to manage your embeddings
    and indexes more efficiently. They often provide features such as automatic indexing,
    scaling, and data management, which simplify the deployment of similarity search
    applications.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve covered the fundamentals of embeddings, indexing, and searching with
    Faiss, along with important considerations for real-world implementation. Now,
    let’s turn our attention to another crucial aspect of RAG: query formulation.
    We’ll explore various strategies to refine and expand user queries, ultimately
    leading to more effective information retrieval from the knowledge base.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Query formulation strategies in LLM-based RAG
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Query formulation strategies in LLM-based RAG systems aim to enhance retrieval
    by improving the expressiveness and coverage of user queries. Common expansion
    strategies include the following:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '**Synonym and paraphrase expansion**: This involves generating semantically
    equivalent alternatives using LLMs or lexical resources. For example, expanding
    “climate change impact” to include “effects of global warming” or “environmental
    consequences of climate change” can help match a broader range of documents.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contextual reformulation**: LLMs can reinterpret queries by inferring their
    intent based on conversational or document context. This helps in tailoring the
    query to better align with how the information might be expressed in the knowledge
    base.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pseudo-relevance feedback**: Also known as blind relevance feedback, this
    strategy involves running an initial query, analyzing the top-ranked documents
    for salient terms, and using these terms to expand the query. While effective,
    it requires safeguards against topic drift.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Template-based augmentation**: Useful in structured domains, this method
    uses domain-specific templates or patterns to systematically generate variants.
    For example, a medical query about “treatment for hypertension” might also include
    “hypertension therapy” or “managing high blood pressure.”'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Entity and concept linking**: Named entities and domain concepts in the query
    are identified and replaced or augmented with their aliases, definitions, or hierarchical
    relations. This is often guided by ontologies or knowledge graphs.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt-based query rewriting**: With LLMs, prompts can be crafted to explicitly
    instruct the model to generate reformulated queries. This is particularly useful
    in multilingual or multi-domain RAG systems, where queries need to be adapted
    to match the style and vocabulary of the target corpus.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each strategy contributes differently to recall and precision. Choosing or combining
    them depends on the structure and variability of the underlying knowledge base.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, the `QueryExpansionRAG` implementation uses a prompt-based
    query rewriting strategy powered by a pre-trained sequence-to-sequence language
    model (specifically, T5-small). This approach instructs the model to generate
    alternative phrasings of the input query by prefixing the prompt with `"expand
    query:"`. The generated expansions reflect paraphrastic reformulation, where the
    model synthesizes semantically related variations to increase retrieval coverage:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This code defines a `QueryExpansionRAG` class that extends a RAG framework by
    incorporating query expansion using a pre-trained T5 model. When a user submits
    a query, the `expand_query` method uses the T5 model through a text-to-text generation
    pipeline to produce multiple alternative phrasings of the query, which are then
    combined with the original query. The `retrieve` method iterates over these expanded
    queries, retrieving documents for each one and aggregating the results while removing
    duplicates. This approach increases the chances of retrieving relevant content
    by broadening the lexical and semantic scope of the original query, making it
    especially effective when the knowledge base expresses information in varied ways.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that poorly expanded queries can introduce noise and reduce retrieval
    precision. In this implementation, expansions generated by the T5 model are combined
    with the original query, increasing coverage. However, to maintain a balance,
    consider reranking results using similarity scores or assigning lower weights
    to generated expansions during retrieval. This helps ensure that expansions improve
    recall without compromising the alignment with the original intent.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve seen how query expansion can enhance retrieval in RAG systems, but it’s
    essential to manage the trade-off between recall and precision. Now, let’s shift
    our focus to the other side of the RAG pipeline: integrating the retrieved information
    with the LLM to generate the final answer. We’ll explore how to craft prompts
    that effectively leverage the retrieved context.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Integrating retrieved information with LLM generation
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To integrate retrieved information with LLM generation, we can create a prompt
    that incorporates the retrieved documents:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the preceding code snippet, the `GenerativeRAG` class extends a RAG pipeline
    by integrating a causal language model for answer generation. It inherits from
    `QueryExpansionRAG`, which already provides retrieval functionality, and adds
    a generator component using Hugging Face’s `AutoModelForCausalLM`. In the constructor,
    it initializes the generator model and tokenizer based on the given model name.
    The `generate_response` method first retrieves relevant documents for a given
    query, concatenates them into a single context string, and constructs a prompt
    that combines this context with the question. This prompt is then tokenized and
    passed into the language model, which generates a text continuation as the answer.
    The final output is obtained by decoding the generated tokens into a string. This
    modular structure separates the retrieval and generation steps, making it easy
    to scale or replace individual components depending on the task or model performance
    requirements.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Having covered the basics of RAG systems, we will now focus on real-world challenges,
    such as scalability, dynamic updates, and multilingual retrieval. Specifically,
    we will discuss how a sharded indexing architecture can improve retrieval efficiency
    at scale, highlighting its impact on performance in data-heavy environments.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Challenges and opportunities in RAG for LLMs
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some key challenges and opportunities in RAG include the following:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability**: Efficiently handling very large knowledge bases.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic knowledge updating**: Keeping the knowledge base current.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross-lingual RAG**: Retrieving and generating in multiple languages.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-modal RAG**: Incorporating non-text information in retrieval and generation.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep in mind that cross-lingual and multi-modal RAG will need specialized retrieval
    pipelines or adapters because standard retrieval approaches often struggle with
    semantic matching across languages or modalities, requiring dedicated components
    that can properly encode, align, and retrieve relevant information regardless
    of the source language or format while maintaining contextual understanding and
    relevance.
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Explainable RAG**: Providing transparency in the retrieval and generation
    process.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To keep this chapter from becoming too long, in this section, we will only
    show an example on how to address the scalability challenge by implementing a
    sharded index. A sharded index refers to a distributed data structure that partitions
    the index into multiple smaller, manageable segments called shards, each stored
    and maintained independently across different nodes or storage units. This approach
    enables parallel processing, reduces lookup time, and mitigates bottlenecks associated
    with centralized indexing, making it suitable for handling large-scale datasets
    or high query volumes commonly encountered in AI applications:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the preceding code, the scalability is handled by dividing the knowledge
    base into multiple smaller indexes, or shards, each containing a portion of the
    overall data. This approach reduces the computational and memory burden on any
    single index and allows retrieval operations to remain efficient even as the dataset
    grows. During a query, the system embeds the query once, searches across all shards
    independently, and then merges the results. This design avoids bottlenecks that
    would arise from searching a single large index and makes it feasible to scale
    to much larger knowledge bases. It also lays the groundwork for further optimizations,
    such as parallelizing shard queries or distributing them across multiple machines.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，可扩展性是通过将知识库划分为多个较小的索引或分片来处理的，每个分片包含整体数据的一部分。这种方法减轻了单个索引的计算和内存负担，并允许检索操作在数据集增长的情况下保持高效。在查询过程中，系统将查询嵌入一次，独立地对所有分片进行搜索，然后合并结果。这种设计避免了搜索单个大型索引时可能出现的瓶颈，并使得扩展到更大的知识库成为可能。它还为进一步的优化奠定了基础，例如并行化分片查询或将它们分布到多台机器上。
- en: Summary
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: RAG is a powerful technique for enhancing LLMs with external knowledge. By implementing
    the strategies and techniques discussed in this chapter, you can create more informed
    and accurate language models capable of accessing and utilizing vast amounts of
    information.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 是一种强大的技术，用于通过外部知识增强大型语言模型（LLMs）。通过实施本章讨论的策略和技术，你可以创建更明智、更准确的语言模型，这些模型能够访问和利用大量信息。
- en: As we move forward, the next chapter will explore graph-based RAG for LLMs,
    which extends the RAG concept to leverage structured knowledge representations.
    This will further enhance the ability of LLMs to reason over complex relationships
    and generate more contextually appropriate responses.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们继续前进，下一章将探讨基于图的大型语言模型（LLMs）的 RAG，这扩展了 RAG 概念以利用结构化知识表示。这将进一步增强 LLMs 在复杂关系上进行推理和生成更符合上下文响应的能力。
