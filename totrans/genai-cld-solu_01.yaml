- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Cloud Computing Meets Generative AI: Bridging Infinite Impossibilities'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During the last few decades, we have seen unprecedented progress in the world
    of **artificial intelligence** (**AI**) and **machine learning** (**ML**) due
    to the rise of computing, especially cloud computing, and the massive influx of
    data from the digital revolution. In 2022, the subset of AI known as generative
    AI emerged as a significant turning point. We have surpassed an inflection point
    in AI and we believe this will boost incredible productivity and growth in society
    in the coming years. This is the field of conversational AI powered by **large
    language models** (**LLMs**), a fascinating paradigm where computers learn and
    generate human-like text, images, audio, and video, engaging with us in increasingly
    interactive and intelligent ways. The transformative potential of LLMs, epitomized
    by models, such as OpenAI’s GPT-based ChatGPT, marks a major shift in how we interact
    with technology. Generative AI models now have improved accuracy and effectiveness.
    Use cases that were unattainable for non-technical users in businesses a couple
    of years ago are now readily implementable. Additionally, the easy availability
    of open source models, which can be tailored to specific business requirements,
    coupled with access to high-performance GPUs via cloud computing, has played a
    crucial role in propelling the advancement of generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter aims to provide a comprehensive introduction to conversational
    and generative AI and delve into its fundamentals and powerful capabilities. ChatGPT,
    a very powerful conversational AI agent, is built on an LLM; hence, to fully understand
    how ChatGPT works and to learn how to implement it in your applications or services
    to harness its power, it’s necessary to understand the evolution of conversational
    AI systems and the broader context of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following main topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Evolution of conversation AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to generative AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trending models and business applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deep dive: open source vs closed source models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud computing for scalability, cost optimization, and automation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From vision to value: navigating the journey to production'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evolution of conversation AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding the evolution of conversational AI is crucial for learning generative
    AI as it provides foundational knowledge and context. This historical perspective
    reveals how AI technologies have progressed from simple, rule-based systems to
    complex machine learning and deep learning models that are core to both conversational
    and generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: This section explores the evolution of conversational AI, culminating in an
    in-depth look at LLMs, the technological backbone of contemporary chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: What is conversational AI?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Conversational AI refers to technologies that enable machines to engage in
    human-like dialogue, comprehend complex commands, and respond intelligently. This
    is achieved through machine learning and natural language processing capabilities,
    enabling the system to learn, understand, and improve over time. The following
    figure demonstrates one such conversation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – Conversations with Alexa](img/B21443_01_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 – Conversations with Alexa
  prefs: []
  type: TYPE_NORMAL
- en: For instance, a customer interacts with a conversational AI to book a flight.
    They might say, “I’d like a flight to New York next Friday.” The system comprehends
    the request, asks for any further specific details (such as departure city or
    preferred time), and delivers the results, all without human intervention.
  prefs: []
  type: TYPE_NORMAL
- en: Some popular conversational AI systems include Microsoft’s Cortana, Amazon Alexa,
    Apple’s Siri, and Google Assistant, which can respond to complex commands and
    respond intelligently.
  prefs: []
  type: TYPE_NORMAL
- en: Evolution of conversational AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Exploring the evolution of conversational AI, from rule-based chatbots to AI-powered
    systems, is vital as it offers historical context, highlights the technological
    advancements from the 1960s and the historical challenges, and sets the stage
    for understanding how LLMs have revolutionized natural language interactions.
    The following figure depicts the conversational AI timeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2 – Timeline showing the evolution of chatbots](img/B21443_01_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 – Timeline showing the evolution of chatbots
  prefs: []
  type: TYPE_NORMAL
- en: Rule-based chatbots
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Chatbots that were initially developed during the 1960s operated on a rule-based
    system. Eliza, the first chatbot software, was created by Joseph Weizenbaum at
    MIT’s Artificial Intelligence Laboratory in 1966\. It used pattern matching and
    substitution technology. Users interacted with Eliza through a text-based platform,
    with the chatbot’s responses being based on scripted templates. Like Eliza, the
    first-generation chatbots were rule-based. They utilized pattern-matching techniques
    to align user inputs with predetermined responses. The chatbot’s conversation
    flows were mapped out by developers who decided how it should respond to anticipated
    customer inquiries. Responses were formulated based on predefined rules and written
    in languages such as **artificial intelligence markup language** (**AIML**), Rivescript,
    Chatscript, and others. These chatbots, typically used as FAQ agents, could answer
    simple questions or common queries about a specific situation.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, rule-based systems had significant limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: Rule-based systems required manual design, forcing developers to program each
    response
  prefs: []
  type: TYPE_NORMAL
- en: They were effective only in the scenarios for which they were specifically trained
  prefs: []
  type: TYPE_NORMAL
- en: It was difficult for developers to anticipate and program all possible responses
  prefs: []
  type: TYPE_NORMAL
- en: These chatbots were unable to identify grammatical or syntactic errors in user
    inputs, often resulting in misunderstandings
  prefs: []
  type: TYPE_NORMAL
- en: They were unable to learn from interactions or generate new responses, limiting
    their adaptability and intelligence
  prefs: []
  type: TYPE_NORMAL
- en: Despite their speed, the inability to understand context or user intents made
    interactions feel mechanical rather than conversational
  prefs: []
  type: TYPE_NORMAL
- en: This mechanical interaction often led to user frustration with systems that
    failed to accurately understand and meet their needs
  prefs: []
  type: TYPE_NORMAL
- en: Over time, there has been a significant increase in demand for intelligent,
    real-time, and personalized interactions in customer support services. As a result,
    rule-based chatbots have evolved into AI-powered chatbots that offer advanced
    features such as human-like voice, intent extraction, sentiment analysis, contextual
    semantic search, grammatical analysis, learning over time, and scalability to
    allow for seamless integration with more demanding applications and services.
  prefs: []
  type: TYPE_NORMAL
- en: LLM-powered chatbots – multimodal, context-aware, and agent-based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In contrast to rule-based systems, AI-based systems utilize natural language
    processing to facilitate natural conversations and extract context from user inputs.
    They can also learn from past interactions aka context. Recently, deep learning
    has significantly advanced conversational AI, even surpassing human performance
    in some tasks, attributed to its incredible reasoning engine. This has decreased
    the reliance on extensive linguistic knowledge and rule-based techniques when
    building language services. As a result, AI-based systems have seen widespread
    adoption across various industries, including media, entertainment, telecommunications,
    finance, healthcare, and retail, to name a few.
  prefs: []
  type: TYPE_NORMAL
- en: 'Current conversational AI systems, leveraging LLMs such as GPT-4-Turbo, differ
    significantly from traditional rule-based systems in their approach and capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: While rule-based systems rely on predefined rules and responses, limiting them
    to specific, anticipated interactions, LLMs harness extensive datasets and advanced
    reasoning abilities to produce responses that are not only natural and varied
    but also highly context-aware
  prefs: []
  type: TYPE_NORMAL
- en: They are also multimodal, which means they can understand and respond to multiple
    forms of communication such as text, voice, image, or video
  prefs: []
  type: TYPE_NORMAL
- en: These exceptional reasoning abilities enable them to handle tasks with increased
    efficiency and sophistication, leading to conversations that closely mimic human
    interaction and understanding
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take the scenario of a customer service interaction as an example to highlight
    the differences between traditional rule-based systems and modern conversational
    AI systems that use LLMs, such as GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a rule-based system example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the rule-based chatbot is programmed to ask for an order number
    as a part of its return process script. It can’t handle the nuance of the customer’s
    situation where they don’t have a receipt. It’s stuck in its predefined rules
    and can’t adapt to the unexpected scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an LLM-powered conversational AI example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The LLM-powered chatbot, on the other hand, understands the context of not having
    a receipt and offers alternative methods for returning the item. It does not require
    the customer to stick to a strict script but instead adapts to the context of
    the conversation and provides a helpful response. This showcases the advanced
    reasoning capabilities of LLMs, allowing for more natural, flexible, and human-like
    conversations.
  prefs: []
  type: TYPE_NORMAL
- en: LLM-powered chatbots also possess inherent limitations, including difficulties
    in generating accurate up-to-date information, a tendency to hallucinate, and
    the reproduction of biases present in their training data. We explore these limitations
    throughout this book, along with strategies to mitigate and eliminate them.
  prefs: []
  type: TYPE_NORMAL
- en: Chatbots and agents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GenAI-based chatbots can also execute tasks or actions with the help of agents.
    LLM agents are programs that enhance standard LLMs by connecting to external tools,
    such as APIs and plugins, and assist in planning and executing tasks. They often
    interact with other software and databases for complex tasks, such as chatbot
    scheduling meetings and needing access to calendars and emails. When a user requests
    a meeting, the chatbot, utilizing its LLM, comprehends the request’s specifics,
    such as time, participants, and purpose. It then autonomously interacts with the
    employees’ digital calendars and email systems to find a suitable time slot, considering
    everyone’s availability. Once it identifies an appropriate time, the chatbot schedules
    the meeting and sends invites via email, managing the entire process without human
    intervention. This showcases the chatbot’s ability to perform complex, multi-step
    tasks efficiently, blending language understanding and reasoning with practical
    action in a business environment. We will learn more about LLM agents in [*Chapter
    6*](B21443_06.xhtml#_idTextAnchor117).
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT, launched in November 2022 by OpenAI, attracted 100 million users within
    just two months due to its advanced language capabilities and broad applicability
    across various tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming section, we will delve into the fundamentals of LLMs as the
    driving force behind modern chatbots and their significance.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generative AI refers to a field of AI (as stated in the preceding figure) that
    focuses on creating or generating new content, such as images, text, music, video,
    code, 3D objects, or synthetic data that is not directly copied or replicated
    from existing data. It involves training deep learning models to understand patterns
    and relationships within a given dataset and then using that knowledge to generate
    novel and unique content. The following is a visualization of what generative
    AI is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – What is generative AI?](img/B21443_01_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 – What is generative AI?
  prefs: []
  type: TYPE_NORMAL
- en: It is a broad field whose primary function is to generate novel content. Examples
    of generative AI models include image generation models such as **DALL-E** and
    **MidJourney**, text generation models such as **GPT-4**, **PaLM**, and **Claude**,
    code generation models such as **Codex**, audio generation tools such as **MusicLM**,
    and video generation models such as **SORA**.
  prefs: []
  type: TYPE_NORMAL
- en: The rise of generative AI in 2022-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Generative AI has reached an inflection point in recent times, and this can
    be attributed to three key factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Size and variety of datasets**: The surge in available data due to the digital
    revolution has been crucial for training AI models to generate human-like content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Innovative deep learning models**: Advancements in model architectures such
    as **generative adversarial networks** (**GANs**) and transformer-based models
    facilitate the learning of complex patterns, resulting in high-quality AI-generated
    outputs. The research paper “Attention Is All You Need” ([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762))
    introduced transformer architecture, enabling significantly more efficient and
    powerful models for natural language processing, which became foundational for
    the development of advanced generative AI models. Progress has also been significantly
    fueled by the availability of open source state-of-the-art pre-trained models
    via platforms such as the Hugging Face Community.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Powerful computing**: Advancements in hardware such as Nvidia GPUs and access
    to computing through cloud computing have enabled the training of complex AI models,
    driving advancements in generative AI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are various types of generative AI models with different underlying architectures.
    Among them, **VAEs**, **diffusion models**, **GANs**, and **autoregressive models**
    are particularly popular. While we won’t delve into every model architecture extensively
    as it is outside the scope of this book. In [*Chapter 2*](B21443_02.xhtml#_idTextAnchor036),
    we will focus on a more detailed discussion of ChatGPT’s LLM architecture, which
    utilizes an **autoregressive-based** **transformer architecture**.
  prefs: []
  type: TYPE_NORMAL
- en: Moving from the topic of generative AI, we now turn our attention to foundation
    models. Often used interchangeably with LLMs, these models are the driving force
    behind the success and possibilities of generative AI. The remarkable strides
    made in foundation models have been instrumental in propelling the advancements
    we witness today in generative AI applications. Their development has not only
    enabled more sophisticated AI capabilities but has also set the stage for a new
    era of innovation and possibilities in AI.
  prefs: []
  type: TYPE_NORMAL
- en: Foundation models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The term foundation models was coined by Stanford in 2021 in the paper “On the
    Opportunities and Risks of Foundation Models” ([https://arxiv.org/pdf/2108.07258.pdf](https://arxiv.org/pdf/2108.07258.pdf)).
    Foundation models are a class of large-scale model that are pre-trained on vast
    amounts of data across various domains and tasks. They serve as a base for further
    fine-tuning or adaptation to a wide range of downstream tasks, not limited to
    language but including vision, sound, and other modalities. The term *foundation*
    signifies that these models provide a foundational layer of understanding and
    capabilities upon which specialized models can be built. They are characterized
    by their ability to learn and generalize from the training data to a variety of
    applications, sometimes with little to no additional training data. The model
    is as follows:.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – Foundation models](img/B21443_01_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 – Foundation models
  prefs: []
  type: TYPE_NORMAL
- en: LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs, on the other hand, are a subset of foundation models that specifically
    deal with natural language processing tasks. They are trained in large text corpora
    and are designed to understand, generate, and translate language at a scale and
    sophistication that closely resembles human language understanding. LLMs are trained
    on massive amounts of data, such as books, articles, and the internet. For example,
    ChatGPT’s base model was trained on 45 TB of data.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs such as GPTs use transformer architecture to process text sequences, training
    themselves to predict the next word in a given sequence. Through exposure to vast
    amounts of text, these models adjust their internal weights based on the difference
    between predicted and actual words, a process known as backpropagation. Over time,
    by repeatedly refining these weights across multiple layers of attention mechanisms,
    they capture intricate statistical patterns and dependencies in the language,
    enabling them to generate contextually relevant text. In [*Chapter 2*](B21443_02.xhtml#_idTextAnchor036),
    we will delve deeper into the transformer architecture of LLMs that enables the
    ChatGPT application.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs traditionally refer to models that handle large-scale language tasks; the
    principles and architecture underlying them can be, and are being, extended to
    other domains such as image generation. This expansion of capabilities reflects
    the versatility and adaptability of the transformer-based models that power both
    LLMs and their multimodal counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: Models such as DALL-E, for instance, are sometimes referred to as LLMs due to
    their foundation in transformer architecture, which was originally developed for
    language tasks. However, DALL-E is more accurately described as a multimodal AI
    model because it understands both text and images and can generate images from
    textual descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: Core attributes of LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the process of creating LLM-based AI applications, it is crucial to understand
    the core attributes of LLMs, such as model parameters, licensing model, privacy,
    cost, quality, and latency. It is important to note that there isn’t a flawless
    model, and making tradeoffs might be necessary to align with the specific business
    requirements of the application. The following content concentrates only on vital
    considerations when designing LLM applications.
  prefs: []
  type: TYPE_NORMAL
- en: Model parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model parameters in LLMs are the internal settings that the model uses to understand
    and generate text. These parameters can be coefficients, weights, and biases and
    are part of large mathematical equations that underlie LLM models. They are adjusted
    through training, where the model learns from vast amounts of data how to predict
    the next word in a sentence, understand context, and generate coherent and relevant
    text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, in the context of LLMs, model parameters are akin to internal notes
    that guide predictions based on learned data patterns. For example, if an LLM
    frequently encounters the phrase “sunny weather” during training, it adjusts its
    parameters to strengthen the connection between “sunny” and “weather.” These adjustments
    are like turning knobs to increase the likelihood of predicting “weather” after
    “sunny” in new sentences. Thus, the model’s parameters encode relationships between
    words, enabling it to generate contextually relevant text based on its training.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The number of parameters indicates the model’s size and complexity, with larger
    models generally capable of capturing more complex patterns and nuances in language
    but requiring more computational resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the parameters in LLMs is crucial for interpreting model behavior,
    customizing and adapting the model, and evaluating and comparing different models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smaller models are more fine-tunable because of the lower number of parameters
    as compared to larger models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While designing applications, it’s crucial to understand whether a smaller model
    can fulfill the needs of a particular use case by means of fine-tuning/in-context
    learning or whether a larger model is necessary. For example, smaller models such
    as GPT-3.5 and FLAN-T5 typically come with lower costs as compared to GPT-4 and
    often prove highly efficient with fine-tuning or in-context learning, especially
    in specific tasks such as conversation summarization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Licensing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Open source models can be used as-is or customized for commercial and non-commercial
    use. They are usually smaller than proprietary LLM models, less expensive, and
    more task-specific. For example, Whisper is an open source speech-to-text model
    developed by Open AI, and Llama from Facebook is an open source model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proprietary models are usually larger models and require licensing. They may
    be restricted for commercial use and modifications. For example, GPT-4 is a proprietary
    model developed by Open AI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When designing applications, it is important to understand whether it is an
    open source or a licensed model and whether it is permitted for commercial use.
    This is crucial to ensure legal compliance, financial planning, ethical considerations,
    customization possibilities, and the long-term success of your application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Privacy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ensuring the security of data used for fine-tuning and prompting LLMs, especially
    when it involves sensitive customer information, is paramount.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guardrails must be established to ensure that customer data is redacted before
    fine-tuning the models and also when using them in prompts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is also crucial to understand how the data will be stored and utilized by
    the model. Data controls can be configured in ChatGPT to prevent chats from being
    saved by the system and thus not allowing them to be used to train the models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When architecting LLM applications, it is important to understand the cost of
    acquiring the model (e.g. licensing costs), infrastructure costs related to data
    storage, computing, data transfer, fine-tuning, and maintenance costs such as
    monitoring.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is crucial for ensuring smooth interaction for users. When deciding on
    models, you must discern whether the output requires real-time or near-real-time
    responses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Larger model APIs may have slightly slower response times and higher costs as
    compared to smaller models, but the quality of outputs may be better in certain
    scenarios. For example, GPT-4 is slightly slower than GPT 3.5 Turbo but may perform
    better in certain scenarios where complex reasoning is involved.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attaining low latency necessitates considering several elements, such as picking
    the right LLM API or hardware infrastructure for self-hosted open source LLMs
    or modifying the length of input and output. The application of methods such as
    cache and load balancing of APIs can drastically reduce response durations, leading
    to a fluid user experience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The core attributes mentioned provide an excellent starting point for shortlisting
    models based on business requirements. However, it’s important to understand that
    some LLMs may exhibit more biases and a higher tendency to hallucinate. In [*Chapter
    3*](B21443_03.xhtml#_idTextAnchor052), we discuss industry-leading benchmarks
    that will help you make informed decisions considering these limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Relationship between generative AI, foundation models, and LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI broadly refers to AI systems that can create new content, such
    as text, image, audio, or video. Foundation models are a subset of generative
    AI, characterized by their large scale and versatility across multiple tasks,
    often trained on extensive and diverse datasets. LLMs, a type of foundation model,
    specifically focus on understanding and generating human language, exemplified
    by systems such as GPT-3.5-Turbo and Llama 2.
  prefs: []
  type: TYPE_NORMAL
- en: Foundation models can be applied to a variety of AI tasks beyond language, such
    as image recognition, whereas LLMs are specifically focused on language-related
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the terms can sometimes be used interchangeably when the context
    is clearly about language tasks, but it’s important to know that the concept of
    foundation models was originally supposed to be broader and encompass a wider
    range of AI capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: However, now, as LLMs such as GPT-4 Turbo are extending to multimodal capabilities,
    this difference between foundation models and LLMs has been narrowing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative AI encompasses a wide array of AI models designed to create new,
    previously unseen content, spanning domains from text and images to music. The
    following image illustrates the relationship between generative AI, LLMs, and
    foundation models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – What is an LLM?](img/B21443_01_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 – What is an LLM?
  prefs: []
  type: TYPE_NORMAL
- en: The LLMs behind ChatGPT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As of early 2024, ChatGPT is a specialized application of GPT-3.5 and GPT-4
    that is fine-tuned for conversational interactions. While GPT-3.5/4 is a general
    language model capable of a variety of language tasks, ChatGPT has been specifically
    trained to respond to prompts in a way that mimics human conversation. The process
    starts with the base foundation model GPT-3.5/4 model that has been pre-trained
    on a large corpus of text from the internet. Then, to create ChatGPT, OpenAI conducts
    further training (fine-tuning) on datasets that include many examples of human
    dialogue. This helps ChatGPT to better understand and generate conversational
    responses. In essence, GPT-3.5/4 can be thought of as the underlying technology,
    and ChatGPT as a specific implementation of that technology optimized for conversation.
  prefs: []
  type: TYPE_NORMAL
- en: Google’s Bard (now known as Gemini) is a similar application to ChatGPT and
    is built on an LLM called PaLM-2.
  prefs: []
  type: TYPE_NORMAL
- en: Open source models such as Llama 2 from Facebook have become more popular lately.
    But how do they contrast with closed source or proprietary models? What are their
    advantages? In the next section, we will explore more about the details of and
    what defines an LLM as an open source model.
  prefs: []
  type: TYPE_NORMAL
- en: Deep dive – open source vs closed source/proprietary models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Open source models such as **Llama 2**, **Mistral**, and **Falcon** have become
    increasingly popular in the recent past. As Gen AI Cloud Architects, the authors
    have witnessed considerable debate on choosing between open source and closed
    source models and identifying the appropriate contexts for their use. This section
    delves into the fundamental distinctions between these models on “What is revealed?”
    and “What is not revealed?” along with key deployment differences, drawing on
    our insights from the field.
  prefs: []
  type: TYPE_NORMAL
- en: Closed source LLMs (e.g., GPT-4, PaLM-2, Claude-2)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'What is revealed is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Functionality and capabilities**: Users know what the model can do, such
    as generating text, answering questions, and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usage guidelines**: Information on how to interact with the model (e.g.,
    APIs) and its intended use cases are revealed. OpenAI provides API access to GPT
    models, but the underlying models are not openly distributed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance metrics**: OpenAI shares details about GPT’s performance in various
    tasks and benchmarks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ethical standards**: OpenAI discusses the ethical considerations and guidelines
    followed during development.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**General architecture overview**: While not in detail, there’s usually some
    high-level information about the model’s architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'What is not revealed is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Source code**: The actual codebase of closed-source models is not publicly
    available'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model weights**: Access to the actual model weights for complete replication
    is restricted'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training data details**: Specifics about the training datasets, including
    their sources and compositions, are generally not disclosed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Detailed model architecture**: The intricate details of the model’s architecture
    and algorithms are proprietary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training process**: Specifics on how the model was trained, including hyperparameters
    and training duration, are not shared'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The above conclusions were drawn up based on the GPT-4 Technical Report ([https://arxiv.org/pdf/2303.08774.pdf](https://arxiv.org/pdf/2303.08774.pdf))
    released by OpenAI. In the report, OpenAI states that due to the competitive landscape
    and safety implications of large-scale models such as GPT-4, it doesn’t reveal
    intricate details about the architecture, including model size, hardware, training
    computing, dataset construction, training method, or similar.
  prefs: []
  type: TYPE_NORMAL
- en: Open source LLMs (e.g., Llama 2, Mistral, Falcon)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'What is revealed is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Source code**: The full codebase is usually available for public access.
    Hence, individuals and businesses can deploy open source models on personal PCs
    and in on-premises or internal servers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model weights**: The weights of the model can be downloaded and used by researchers
    and developers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training process details**: Detailed information about how the model was
    trained, including datasets and hyperparameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Full architecture details**: Comprehensive information on the model’s architecture
    is provided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dataset information**: Although with some constraints, more information about
    the training datasets may be available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'What is not revealed is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resource requirements**: Specific details on the computational resources
    required for training might not be fully disclosed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ethical considerations**: Open source projects may not always have the same
    level of ethical oversight as some closed source projects'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance optimization secrets**: Some nuances of performance optimization
    during training might be left out'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Full training data**: Even in open source models, sharing the entire training
    data can be impractical due to size and licensing issues'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous updates**: Unlike some closed source models, open source models
    may not receive continuous updates or support'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following table details the key deployment differences between open and
    closed source models:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Closed** **source models** | **Open source** **LLMs (OSS)** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Access, cost, and deployment endpoint | Access is typically restricted to
    paid licenses, APIs, or subscription models. The cost can be a barrier for smaller
    organizations or individual developers.Costs associated with such deployments
    are typically associated with the number of tokens in prompts and completions.For
    example, as of early 2024, OpenAI charges $0.01 /1K tokens for prompts and $0.03
    /1K tokens for completions for gpt-4-0125-preview. | Generally, the source code
    is freely available. Deploying open source models necessitates the initial setup
    of compute instances, serving as the foundation for an inference endpoint. This
    endpoint can operate in real time or process data in batches. The expenses linked
    to this deployment strategy primarily involve the operational costs of the compute
    resources.However, new pricing models have emerged, such as MaaS (model-as-a-service),
    which charges just like API-based models based on the tokens used. |'
  prefs: []
  type: TYPE_TB
- en: '| Customization and flexibility | Due to the unavailability of source code,
    customization options are often limited to what the provider allows. Users may
    not be able to modify the model’s core architecture or training datasets. | Greater
    flexibility for customization is offered. Developers can tweak the models, retrain
    with specific datasets, or even adjust the underlying algorithms. |'
  prefs: []
  type: TYPE_TB
- en: '| Support and documentation | Usually, they come with professional support
    and comprehensive documentation, ensuring smoother deployment, and troubleshooting
    processes. | While there is often a community for support, the quality and availability
    of formal support and documentation can vary. |'
  prefs: []
  type: TYPE_TB
- en: '| Integration and compatibility | They might have better integration with other
    proprietary tools or platforms offered by the same provider but could be less
    flexible in terms of compatibility with a wide range of technologies. | They are
    typically designed to be more flexible and compatible with a variety of platforms
    and tools, though integration may require more effort from the user. |'
  prefs: []
  type: TYPE_TB
- en: '| Security and updates | Security updates and patches are typically managed
    by the provider, ensuring a consistent level of maintenance. | Security relies
    on the community and maintainers, which can lead to varying degrees of promptness
    and effectiveness in updates. |'
  prefs: []
  type: TYPE_TB
- en: '| Ethics, compliance, and liability | Providers are generally responsible for
    compliance with regulations, offering a certain level of assurance for businesses.
    | Users often need to ensure compliance themselves, which can be a significant
    consideration for businesses in regulated industries. |'
  prefs: []
  type: TYPE_TB
- en: '| Risks |'
  prefs: []
  type: TYPE_TB
- en: Potentially higher costs due to licensing fees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limited ability to customize to meet business requirements as compared to open
    source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vendor Lock-In
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduced transparency, due to the limited knowledge of the internal workings
    of the LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Potential security vulnerabilities as they are community-driven and might enable
    malicious use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lack of centralized quality control can lead to inconsistencies in updates and
    improvements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reliance on community support may lead to inconsistent troubleshooting and issue
    resolution, affecting projects that need stable, continuous maintenance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.6 – Key deployment differences
  prefs: []
  type: TYPE_NORMAL
- en: 'The decision for organizations to adopt open source or closed source models
    is inherently subjective and hinges on their unique needs and goals. A more pertinent
    question might be: after conducting internal benchmarking, which model emerges
    as the most effective for your specific use case? These benchmarks are available
    on Hugging Face ([https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)).'
  prefs: []
  type: TYPE_NORMAL
- en: Trending models, tasks, and business applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative AI has a wide range of applications across various industries, presenting
    several use cases that can bring significant benefits to businesses, and the applications
    are continuing to grow at a fast pace. In this section, we will discuss popular
    tasks and models and examine the latest emerging business applications that have
    gained significant traction recently.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin with text generation models.
  prefs: []
  type: TYPE_NORMAL
- en: Text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Text generation models can be used for diverse tasks as outlined here. In the
    following, we have mentioned the most popular tasks that we have seen architecting
    solutions with our customers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Summarization**: They can condense long documents, such as textbook chapters
    or detailed product descriptions, into concise summaries while retaining the key
    information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Question answering**: These models can provide accurate answers to questions,
    which is particularly useful in automating the creation of FAQ documents from
    extensive knowledge base content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification**: Text generation models can classify text, assigning labels
    based on criteria such as grammatical correctness or other predefined categories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentiment analysis**: As a specialized form of classification, these models
    can analyze and label the sentiment of a text, identifying emotions such as happiness
    and anger or general positive and negative tones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Entity extraction**: They can extract specific pieces of information, such
    as movie names, from larger text bodies, aiding in information retrieval and organization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Translation**: Language models excel in translation by quickly and accurately
    converting text from one language to another, leveraging vast datasets to understand
    and maintain context and nuances. Code generation can be considered a type of
    translation, where the language model translates human language instructions into
    programming code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These capabilities make text-generation models invaluable tools and have led
    to the creation of innovative applications. Here we have mentioned a few interesting
    business applications we have observed across various industries due to the proliferation
    of text generation models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Enterprise chatbots**: Text generation models power conversational agents
    that can engage in natural language conversations with users, offering customer
    support, HR support, L&D, and assistance with tasks. The top use case in terms
    of popularity that we observed was the implementation of an enterprise chatbot
    grounded on organizational data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Content creation (articles, blog posts, reports, books)**: Text generation
    models can automatically generate high-quality written content on various topics,
    saving time and effort for content creators and enabling seamless Q&A experiences
    on the same. This has been a major productivity booster in the media, marketing,
    entertainment, and publication industries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real estate listings**: Text generation models enable real estate companies
    to effortlessly craft attractive house listings by inputting details such as the
    number of bedrooms, property age, neighborhood information, and other unique selling
    points, significantly enhancing the appeal of properties to potential buyers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automatic email drafting**: Text generation models assist in composing personalized
    and contextually relevant emails, streamlining communication, and improving productivity
    in email correspondence, for example, Microsoft’s Copilot application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Personalized advertising**: These models help tailor marketing messages and
    content to individual users, enhancing the effectiveness of advertising campaigns
    by delivering more relevant and engaging content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Proposal creation**: They significantly streamline the operations of real
    estate companies by automating the creation of proposals for **request for proposal**
    (**RFP**) responses. This tool also facilitated efficient searching through RFP
    submissions and greatly assisted marketing teams in crafting and authoring high-quality
    content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ad campaigns**: In the realm of marketing and advertising campaigns, text
    generation models offer a powerful advantage by providing precise and efficient
    summarization of lengthy content. Moreover, these models seamlessly translate
    text between various languages, effectively dismantling language barriers. This
    capability enhanced cross-cultural communication, enabling marketers to reach
    and resonate with a diverse, global audience more effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code co-pilot**: Developer productivity in organizations has increased tremendously
    due to products such as GitHub Copilot.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following highlights the leading text generation models as of early 2024
    in a rapidly advancing field:'
  prefs: []
  type: TYPE_NORMAL
- en: '**GPT-4-Turbo**: Developed by OpenAI, the most popular model in production
    today. GPT-4 is a large multimodal model with deep learning capabilities, enabling
    it to generate human-like, conversational text. It can accept both text and image
    inputs to produce human-like text outputs. It accepts 128,000 tokens in its context
    window, which is close to 300 pages of text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Llama 2**: The Llama 2 open source models have been trained on 2 trillion
    tokens and offer double the context length (~4K tokens) of their predecessor,
    Llama 1\. These models excel in a variety of benchmarks, including reasoning,
    coding, proficiency, and knowledge tests, and include specialized chat models
    trained on over one million new human annotations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mistral**: Developed by Mistral AI, founded by former Meta and Google AI
    researchers, Mistral is a leading open source model LLM with 7.3 billion parameters,
    capable of generating coherent text and performing various natural language processing
    tasks. It represents a significant advancement over previous models, outperforming
    many existing AI models in a variety of benchmarks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PaLM-2**: Developed by Google, PaLM-2, which stands for pathways language
    model, is a next-generation language model part of a family of LLMs trained on
    a vast amount of data for next-word prediction. It shows improved multilingual,
    reasoning, and coding capabilities, and is extensively trained on multilingual
    text, covering over 100 languages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Claude2**: Developed by Anthropic, Claude2 is an advanced version of its
    predecessor, Claude. This LLM is designed to be safer and more capable, with improved
    performance and longer response capabilities. It can handle a context window of
    up to 100K tokens, allowing it to work with extensive documents. Claude-2 has
    been noted for its focus on AI safety and its potential as a competitor in the
    field of conversational AI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gemini 1.5**: Google’s latest model was released in February 2024 with more
    efficient architecture and enhanced performance. It comes in three sizes: Ultra,
    Pro, and Nano, and can accept up to one million tokens in the context window.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let’s explore image generation models.
  prefs: []
  type: TYPE_NORMAL
- en: Image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the evolving landscape of computer vision, image generation models are advancing,
    with key areas such as image synthesis and classification already somewhat mature.
    Emerging fields include visual question and answer, which interprets images to
    answer queries, and image segmentation, which breaks down images for detailed
    analysis. The key areas are detailed in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image synthesis**: Generating new images or altering existing ones based
    on specific inputs or requirements'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image classification**: Identifying and categorizing objects within an image
    into predefined classes, crucial for applications such as facial recognition and
    automated photo tagging'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visual question answering** (**VQA**): Combining image processing and natural
    language understanding to answer questions about a given image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image segmentation**: Dividing an image into segments or parts for simpler,
    more meaningful analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These capabilities make image-generation models invaluable tools and have led
    to the creation of innovative applications. In the following, we have mentioned
    a few interesting business applications that are emerging across various industries
    due to the advancements in recent image generation models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generating Images from Text Descriptions**: Image generation models can take
    text descriptions as input and create corresponding images. This is valuable in
    applications such as generating illustrations for books, articles, or product
    listings. For example, a text description of a tropical beach scene can be turned
    into a realistic image of that scene, aiding in visual storytelling and marketing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storyboarding**: Entertainment firms are utilizing image-generation models
    for crafting storyboards. These visual aids depict narratives, concepts, or scripts,
    offering a glimpse into how a story might appear when animated or performed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fashion design**: Image generation models are helping fashion designers create
    new clothing designs by generating various apparel designs, patterns, and color
    combinations. Designers can input parameters or inspiration, and the model can
    generate visual concepts to inspire new collections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interior design**: Similarly, for interior designers, these models can generate
    room layouts, furniture arrangements, and decor ideas based on input criteria,
    enabling quick and creative design exploration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automatic photo editing**: Image generation models can be used to automate
    and enhance the photo editing process. They can intelligently adjust color balance,
    contrast, and lighting, remove unwanted objects or blemishes, and apply artistic
    filters or styles to photos. This can significantly reduce the time and effort
    required for manual photo editing tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Creating digital artwork**: Digital artists and illustrators can use image
    generation models to spark their creativity. These models can generate abstract
    or realistic art pieces, offer new design ideas, or assist in creating concept
    art for various projects. Artists can use the generated images as a starting point
    for their work.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Doctor copilot**: This falls under the multimodal category, where the diverse
    functionalities of LLMs are applied to a variety of medical imaging tasks, including
    medical visual question-and-answer scenarios. Essentially, this involves developing
    applications that can respond to queries from doctors regarding X-rays or CT scans
    as well as aid in the generation of radiology reports.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Facial recognition**: Image generation models can enhance facial recognition
    by creating diverse, high-quality training datasets, enabling the algorithms to
    learn and identify a wide range of facial features and expressions under various
    conditions. Additionally, they can assist in reconstructing partial or obscured
    faces in images, improving the accuracy and reliability of recognition systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following highlights the leading image generation models as of December
    2023 in a rapidly advancing field:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DALL-E3**: Developed by OpenAI, DALL-E 3 is an advanced AI model capable
    of generating detailed and imaginative images from textual descriptions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google’s Imagen**: Imagen by Google is a text-to-image diffusion AI model
    known for producing highly photorealistic images from textual prompts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stable Diffusion**: Stable Diffusion, an open source model created by Stability
    AI, is a text-to-image model designed to generate high-quality images based on
    user-provided text descriptions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Midjourney v5.2**: Midjourney v5.2, developed by Midjourney Inc. and launched
    in June 2023, represents the latest and most sophisticated iteration of Midjourney’s
    AI image generation model. This version focuses on enhancing the performance,
    consistency, and quality of the generated images. It is known for producing more
    detailed and sharper results with improved colors, contrast, and compositions
    compared to its predecessors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Segment Anything Model** (**SAM**): The Segment Anything Model developed
    by Facebook’s Meta AI is not primarily an image generation model; instead, it’s
    an image segmentation model. Image segmentation models are designed to identify
    and delineate specific parts or objects within an image, essentially segmenting
    the image into different areas based on the objects present. We have mentioned
    it here as it falls under models within the realm of computer vision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure shows the segmentation of the New York skyline into different
    objects using SAM:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.7 – Image segmentation example](img/B21443_01_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 – Image segmentation example
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on to audio generation models.
  prefs: []
  type: TYPE_NORMAL
- en: Audio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Audio generation models are versatile tools for various applications, as demonstrated
    through our experience in developing solutions with our customers. The most popular
    tasks are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Speech synthesis**: Generating human-like speech from text (text-to-speech)
    and used in voice assistants, audiobooks, and various accessibility tools'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speaker identification**: Recognizing and differentiating between different
    speakers in audio recordings, which can be useful in security systems and personalized
    user experiences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Emotion detection**: Identifying emotions from speech, which can enhance
    customer service interactions or aid in mental health assessments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sound generation**: Creating music or sound effects using AI, which has applications
    in entertainment, gaming, and virtual reality'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Voice cloning**: Generating a synthetic voice that sounds like a specific
    person, which can be used in personalized speech interfaces or entertainment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speech recognition**: Converting spoken language into text, which is fundamental
    in creating transcriptions, automated subtitles, and voice commands'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speech translation**: Translating spoken language from one language to another
    in real-time, facilitating cross-lingual communication'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Audio-based LLMs can generate various forms of audio, such as speech, music,
    and sound effects, based on textual or other input. For instance, here we mention
    a few emerging noteworthy business applications with audio generation models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ChatBot audio and avatar**: Recent advancements in avatar-based experiences
    have led organizations to create immersive audio experiences featuring copilots
    with lifelike avatars'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Music composition and production**: These models are used to create new music
    pieces, simulate various musical styles, and assist composers in exploring new
    soundscapes and melodies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sound effects and Foley in media production**: They can generate realistic
    or imaginative sound effects for use in films, video games, and other multimedia
    projects, offering a cost-effective alternative to traditional Foley artistry'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language learning and pronunciation training**: By generating accurate and
    diverse speech samples, these models aid in language learning applications, helping
    users with pronunciation and listening comprehension'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accessibility applications**: Audio generation models are crucial in developing
    tools for visually impaired individuals, converting text and visual information
    into audio, thus enhancing accessibility in various digital platforms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This space is evolving, but there hasn’t been as much advancement in this domain
    as with text and image generation models. Here we mention a couple of interesting
    audio generation models from Google and OpenAI:'
  prefs: []
  type: TYPE_NORMAL
- en: '**MusicLM**: From Google Research, this is a cutting-edge AI model that transforms
    music creation using text prompts. It generates high-quality music across genres
    from simple text inputs. This innovative model utilizes a sophisticated hierarchical
    sequence-to-sequence approach, trained on a dataset of 5.5K expert-crafted music-text
    pairs, offering valuable opportunities for researchers and music enthusiasts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open AI JukeBox**: This model, created by OpenAI in 2020, generates new music
    samples based on inputs such as genre, artist, and lyrics ([https://github.com/openai/jukebox](https://github.com/openai/jukebox)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we look at video generation models.
  prefs: []
  type: TYPE_NORMAL
- en: Video
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Video generation models, which are advanced forms of AI designed to create,
    manipulate, and analyze video content, can perform a wide range of tasks. Some
    of the key emerging tasks across our customers in this field are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Video synthesis**: Creating new video content from scratch or based on textual
    descriptions, which includes generating realistic scenes, animations, or simulations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deepfake generation**: Creating highly realistic and convincing videos where
    one person’s likeness is replaced with another, often used in film production,
    in education, or for entertainment purposes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Video editing and enhancement**: Automatically editing videos to improve
    their quality, such as enhancing resolution, color correction, and stabilizing
    shaky footage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Video summarization**: Condensing longer videos into shorter summaries while
    retaining the essential content, which is useful for quickly conveying information
    in large video files'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Object tracking and recognition**: Identifying and tracking objects or individuals
    across a video sequence, which is crucial for surveillance, sports analysis, and
    autonomous vehicles'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scene understanding**: Analyzing a video to understand the context, setting,
    or events taking place, which can be applied in video indexing and search systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Motion analysis**: Studying the movement of objects or individuals within
    a video, applicable in sports training, physical therapy, and animation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Facial expression and gesture analysis**: Interpreting facial expressions
    and body language to gauge emotions, reactions, or intentions, which is useful
    in customer service or behavioral studies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Video-to-text transcription**: Converting the visual and auditory components
    of a video into textual descriptions, aiding in content accessibility and searchability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interactive video creation**: Generating interactive videos where viewers
    can influence the storyline or outcome, enhancing user engagement in gaming, education,
    and marketing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Text-to-video models are a type of AI technology that generates video content
    based on textual descriptions. While there have been considerable advancements
    in recent **text-to-video** (**T2V**) generation techniques, most of these developments
    are concentrated on creating short video clips that depict a single event set
    against a single backdrop, essentially limited to single-scene videos. As video
    generation models evolve, exciting new applications are beginning to emerge, offering
    innovative possibilities in this field:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Q&A over video archive**: In the media and entertainment industry, a prominent
    use case emerging involves embedding video data using models such as CLIP and
    then creating enhanced search experiences on top of it'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Film and animation**: These models can aid in rapidly prototyping scenes
    and creating short animations, streamlining the filmmaking and animation process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advertising and marketing**: Businesses can utilize video generation models
    to create engaging content for marketing campaigns and advertisements tailored
    to specific audiences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Education and training**: They can enhance educational content by producing
    custom videos that illustrate complex concepts or simulate real-life scenarios
    for more effective learning and training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gaming and virtual reality**: In gaming, these models can be used to generate
    dynamic environments and characters, enriching the gaming experience, and reducing
    development time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Research and development**: Video generation models are valuable in visualizing
    scientific theories, simulating experiments, or presenting research findings in
    an interactive format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This space is evolving and there hasn’t been as much advancement in the video
    domain as with text and image generation models. Here we mention two models with
    promising capabilities in the video space:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stable Video Diffusion**: Announced in November 2023 by Stability AI, this
    is a model that creates high-resolution videos (576 x 1024) from text or single
    images. It advances latent diffusion models previously limited to 2D images to
    video, maintaining high detail at 14 or 25 frames per second. The research highlights
    the importance of data curation in enhancing high-resolution video generation
    performance ([https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPT-4V**: From OpenAI, this is a multimodal LLM capable of analyzing videos
    but unable to generate videos as of early 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI announced SORA in early 2024, its first text-to-video generation model.
    Although it has not been released to the public as it is undergoing comprehensive
    red teaming testing, based on the samples shared by OpenAI, we think this innovation
    is a significant leap in multimodal LLMs. It allows you to transform text prompts
    into high-quality, one-minute videos.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what SORA brings to the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Complex scene generation**: SORA excels in creating detailed scenes featuring
    multiple characters, various motions, and precise subject and background details.
    The model understands not only what the user has asked for in the prompt, but
    also how those things exist in the physical world.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advanced language comprehension**: With its profound grasp of language, SORA
    can bring prompts to life with characters that showcase a range of emotions. Moreover,
    it can craft multiple shots within a video, maintaining consistency in character
    and visual style.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have highlighted the most prominent LLMs currently known. However, the field
    is advancing swiftly, and fresh models are continuously emerging. For the latest
    and trending models, we suggest regularly visiting the Hugging Face website, which
    maintains an up-to-date list of these innovative and influential models ([https://huggingface.co/models](https://huggingface.co/models)).
  prefs: []
  type: TYPE_NORMAL
- en: Cloud computing for scalability, cost optimization, and security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cloud computing has been instrumental in bringing LLMs to a wider audience.
    LLMs use large-scale GPU processing to learn and generate human-like text, image,
    audio, and video, engaging in increasingly interactive and intelligent ways.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section highlights several advantages of leveraging LLMs in a cloud environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability**: Cloud computing enables users to access high-performance computing
    such as GPUs as necessary to run LLMs. This makes it easy to scale applications
    as required based on consumption needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since LLM models such as GPT are heavy API-driven workloads, there is a need
    for API management services, such as Azure APIM, that help achieve scalability,
    security, and high availability across regions. They can also capture telemetry
    that can help determine token usage and error logging across organizations. We
    discuss scaling strategies on Azure in [*Chapter 7*](B21443_07.xhtml#_idTextAnchor143).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Affordability**: There is no need for large upfront infrastructure investment
    as you can easily access computing power from the cloud, making it more affordable.
    Utilizing a pay-as-you-go service allows you the flexibility to activate instances
    for open source models as needed and terminate them at your convenience, ensuring
    that you have control and adaptability in managing your resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data storage**: LLMs may require a large amount of data for training and
    fine-tuning. Cloud services offer scalable and cheap storage options to manage
    vast amounts of structured and unstructured data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For instance, Azure Blob Storage provides several cheap and flexible storage
    options for storing structured and unstructured data and this can be used in conjunction
    with Azure AI search to enable vector storage with advanced security capabilities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Accessibility and collaboration**: Cloud platforms make it easy to access
    LLMs from anywhere in the world, making it easy for researchers, data scientists,
    cloud architects, and developers to collaborate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Managed services**: Cloud platforms offer managed services that can simplify
    deployment and infrastructure management for LLMs on the cloud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For instance, Microsoft’s model-as-a-service allows you to deploy open source
    models such as Llama 2 as a pay-as-you-go service. Azure handles the infrastructure
    provisioning and charges you based on token usage. This eliminates the management
    overhead of provisioning inference computing for open source models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Speed**: With access to the cloud, you have access to high-speed computer
    power, providing you with more options based on the latency needs of your LLM
    applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Azure, you can get access to several GPU-optimized VM sizes options, such
    as Nvidia A100s V4 series and NCV3 series ([https://learn.microsoft.com/en-us/azure/virtual-machines/sizes-gpu](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes-gpu)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Different LLMs may necessitate varying sizes of GPU computing power that affect
    the latency and cost of running the applications.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Security and compliance**: Top cloud platforms provide comprehensive and
    industry-leading security and compliance services for your data, thus providing
    authentication, authorization, encryption, monitoring, and logging capabilities
    to protect your AI infrastructure. They also provide services to identify potential
    jailbreak attacks. Jailbreak attacks on LLMs are methods used to bypass or manipulate
    the model’s safety and ethical guidelines to elicit prohibited or restricted responses.
    We will learn more about jailbreak attacks in [*Chapter 8*](B21443_08.xhtml#_idTextAnchor163)
    on security.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Responsible AI solutions**: With the advent of new-generation AI applications,
    implementing robust guardrails to detect and filter out harmful content becomes
    crucial. Tools such as Azure Content Safety are designed to moderate text and
    image content, helping to maintain a safe and appropriate user experience. Additionally,
    the use of safety metaprompts, which are essentially guiding instructions or constraints
    embedded in the system messages of LLMs, plays a vital role. These metaprompts
    can instruct the LLM to avoid generating inappropriate, biased, or harmful content,
    acting as an integral part of the model’s ethical framework and ensuring responsible
    AI usage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While it’s possible to deploy certain open source models on personal laptops
    or establish a dedicated infrastructure within an organization, this approach
    often incurs substantial upfront costs, including significant investment in talent
    acquisition and ongoing management overhead. Additionally, maintaining the security
    of such infrastructure might not match the advanced levels offered by cloud service
    providers. Therefore, cloud services emerge as the more advantageous solution,
    offering a wide array of flexible, secure, scalable, and ethically responsible
    options for deploying generative AI solutions. In the next section, we will delve
    into the process of transforming an innovative idea into reality, examining the
    various stages involved in deploying it on the cloud and using our experiences
    as cloud solution architects during the initial stages of generative AI deployments
    across various organizations.
  prefs: []
  type: TYPE_NORMAL
- en: From vision to value – navigating the journey to production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developing an idea and moving it into production is a multi-phase process that
    typically involves ideation, validation, development, testing, and deployment.
    The multi-phase process of developing an idea and moving it into production is
    crucial because it methodically transforms a concept into a viable product.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following image about overlooking a crucial aspect:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.8 – Two entrepreneurs engaging in a humorous discussion about overlooking
    expenses](img/B21443_01_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 – Two entrepreneurs engaging in a humorous discussion about overlooking
    expenses
  prefs: []
  type: TYPE_NORMAL
- en: The above image satirically showcases how some organizations claim to build
    AI from scratch, when in reality, they're just utilizing API calls to services
    like OpenAI. It humorously uncovers this exaggeration when asked about the Open
    AI bills, mocking the notion of starting from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each phase serves a distinct purpose: ideation fosters innovation, validation
    ensures market demand and feasibility, development translates validated ideas
    into tangible products, testing guarantees functionality and user satisfaction,
    and deployment introduces the product into the market. This structured approach
    mitigates risks, optimizes the use of resources, assures product quality, and
    secures market fit. It’s a strategic pathway that allows for informed decision-making
    and efficient allocation of capital and maximizes the chances of commercial success.
    Here’s a structured approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.9 – Stages from ideation to deployment](img/B21443_01_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.9 – Stages from ideation to deployment
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at each stage in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps are involved in ideation:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate and brainstorm ideas without constraints to encourage creativity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prioritize ideas based on factors such as feasibility, market potential, and
    alignment with business goals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hackathon events: fostering innovation in generative AI'
  prefs: []
  type: TYPE_NORMAL
- en: In our early roles as Cloud Architects in the generative AI space, we witnessed
    a surge of hackathon events across various organizations. These events, integral
    to the ideation phase, encouraged rapid problem-solving, innovative thinking,
    and the free exchange of ideas, unencumbered by the usual workplace constraints.
    Participants were exposed to new perspectives and skills, while the event’s structure
    promoted quick development and validation of ideas. The combination of collaboration,
    focused effort, and a supportive community made hackathons an ideal breeding ground
    for creative solutions and new concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps are involved in market research and validation:'
  prefs: []
  type: TYPE_NORMAL
- en: Conduct thorough market research to understand the demand and competition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate the idea through customer interviews, surveys, or focus groups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following steps are involved in **Proof of** **Concept** (**PoC**):'
  prefs: []
  type: TYPE_NORMAL
- en: Create a PoC to demonstrate the idea’s feasibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the PoC to gather initial feedback and iterate on the design
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine success criteria for the PoC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Initial PoCs: leveraging ChatGPT for internal co-pilots'
  prefs: []
  type: TYPE_NORMAL
- en: Drawing from our experience, the initial PoCs typically involve internal-facing
    co-pilots utilizing the ‘ChatGPT on your data’ feature on Azure focused on organizational
    data. These projects were seen as low-hanging fruit, offering rapid wins and valuable
    lessons learned.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps are involved in business case and planning:'
  prefs: []
  type: TYPE_NORMAL
- en: Build a business case by outlining the value proposition, market entry strategy,
    and financial projections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plan the project, including timelines, budget, resources, and risk assessment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine ROI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ROI for generative AI workloads
  prefs: []
  type: TYPE_NORMAL
- en: Assessing the ROI of generative AI workloads poses a significant challenge,
    involving not only the calculation of the end-to-end solution cost but also the
    quantification of returns through automation and the elimination of manual tasks.
    Adding to this, offering the solution as a white-label product for other companies
    can substantially enhance ROI. This approach opens new revenue streams, offers
    cost efficiency for clients, enables scalability, indirectly boosts brand recognition,
    and provides a rich feedback loop for product improvement. By leveraging white
    labeling, businesses can maximize the value and reach of their generative AI solutions,
    making it a strategic move to increase overall returns on investment in a competitive
    market. In [*Chapter 7*](B21443_07.xhtml#_idTextAnchor143), we discuss a few of
    the cost optimization strategies companies can leverage to reduce their overall
    cost of generative AI workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps are involved in prototype/MVP development:'
  prefs: []
  type: TYPE_NORMAL
- en: Develop a prototype that’s closer to the product than the PoC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterate on the prototype based on feedback and technical feasibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop an MVP with the minimal necessary features to satisfy early adopters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The MVP serves to validate product-market fit and gather user feedback
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following steps are involved in testing and quality assurance:'
  prefs: []
  type: TYPE_NORMAL
- en: Perform various types of testing (unit, integration, system, user acceptance)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that the product meets quality standards and is free of critical bugs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following steps are involved in pre-production and staging:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploy the application in a staging environment that closely mimics production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conduct further testing, including load and performance tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following steps are involved in the deployment strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: Develop a deployment strategy, such as blue-green deployments and canary releases
    to minimize risks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plan for rollback procedures in the case of failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following steps are involved in the launch:'
  prefs: []
  type: TYPE_NORMAL
- en: Launch the product to the target user base
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor the product closely for any issues or unexpected behaviors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following steps are involved in continuous monitoring and feedback loop:'
  prefs: []
  type: TYPE_NORMAL
- en: Establish mechanisms for continuous monitoring, error logging, and performance
    tracking through LLMOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create feedback channels for users to report issues or suggest improvements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: '**Large language model operations** (**LLMOps**) focus on deploying, managing,
    and scaling LLMs in production to ensure that they integrate smoothly into applications
    for optimal performance, security, and cost-effectiveness. This involves practices
    such as continuous integration and deployment for automated updates, continuous
    monitoring for performance and cost efficiency, version control for updates without
    disruption, security measures for compliance, and auto-scaling for demand changes.
    LLMOps are crucial for organizations using LLMs in production, simplifying operational
    challenges to foster innovation. More on LLMOps is discussed in [*Chapter 6*](B21443_06.xhtml#_idTextAnchor117).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps are involved in iterative improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: Use data and user feedback to make iterative improvements to the product
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plan for regular updates and feature releases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following steps are involved in scalability:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that the architecture is scalable to handle growth in users or data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularly review infrastructure and optimize as necessary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We recommend
  prefs: []
  type: TYPE_NORMAL
- en: This approach is vital to guarantee superior user experience, ensuring the solution’s
    high availability and incorporating disaster recovery measures. We discuss these
    concepts elaborately in [*Chapter 7*](B21443_07.xhtml#_idTextAnchor143).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps are involved in maintenance and support:'
  prefs: []
  type: TYPE_NORMAL
- en: Provide ongoing maintenance and support to users
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep the product up to date with the latest security patches and compliance
    standards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughout this process, it’s essential to stay agile and be prepared to pivot
    or make changes based on new insights and feedback. Communicate regularly with
    all stakeholders and ensure that there’s a clear understanding of the vision,
    progress, and challenges associated with developing the idea and moving it into
    production.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The aim of this introductory chapter was to highlight the history, core concepts,
    and other essential information necessary for readers to develop an end-to-end
    generative AI solution on the cloud. We have explored the evolution of chatbots
    from simple rule-based systems to multimodal, context-aware, and action-oriented
    agentic LLMs. We delved into the rise of generative AI, focusing on LLMs and foundation
    models as well as their relationship and key attributes. The differences between
    open source and closed source models were examined, alongside trending business
    applications drawn from our experiences. In the rapidly evolving landscape of
    AI, we’ve examined a few leading models, including text, image, audio, and video
    generation. These models represent the forefront of AI technology, showcasing
    remarkable capabilities in creating high-quality, lifelike content. We then highlighted
    how cloud computing facilitates the development of secure, scalable, cost-efficient,
    and ethical generative AI applications. We also outlined a framework for transforming
    ideas into production-ready solutions. In the next chapter, we’ll dive into the
    NLP capabilities of LLMs and their transformer architecture, which is fundamental
    to the functioning of these models.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Nvidia Generative AI: [https://www.nvidia.com/en-us/glossary/data-science/generative-ai/#:~:text=Generative%20AI%20models%20use%20neural,semi%2Dsupervised%20learning%20for%20training](https://www.nvidia.com/en-us/glossary/data-science/generative-ai/#:~:text=Generative%20AI%20models%20use%20neural,semi%2Dsupervised%20learning%20for%20training)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CSET Georgetown University: [https://cset.georgetown.edu/article/what-are-generative-ai-large-language-models-and-foundation-models/#:~:text=Using%20the%20term%20%E2%80%9Cgenerative%20AI,system%20that%20works%20with%20language](https://cset.georgetown.edu/article/what-are-generative-ai-large-language-models-and-foundation-models/#:~:text=Using%20the%20term%20%E2%80%9Cgenerative%20AI,system%20that%20works%20with%20language)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Databricks course: [https://microsoft-academy.databricks.com/learn/course/1765/play/12440/llms-and-generative-ai](https://microsoft-academy.databricks.com/learn/course/1765/play/12440/llms-and-generative-ai)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
