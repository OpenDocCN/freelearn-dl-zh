- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RAG Embedding Vector Stores with Deep Lake and OpenAI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There will come a point in the execution of your project where complexity is
    unavoidable when implementing RAG-driven generative AI. Embeddings transform bulky
    structured or unstructured texts into compact, high-dimensional vectors that capture
    their semantic essence, enabling faster and more efficient information retrieval.
    However, we will inevitably be faced with a storage issue as the creation and
    storage of document embeddings become necessary when managing increasingly large
    datasets. You could ask the question at this point, why not use keywords instead
    of embeddings? And the answer is simple: although embeddings require more storage
    space, they capture the deeper semantic meanings of texts, with more nuanced and
    context-aware retrieval compared to the rigid and often-matched keywords. This
    results in better, more pertinent retrievals. Hence, our option is to turn to
    vector stores in which embeddings are organized and rapidly accessible.'
  prefs: []
  type: TYPE_NORMAL
- en: We will begin this chapter by exploring how to go from raw data to an Activeloop
    Deep Lake vector store via loading OpenAI embedding models. This requires installing
    and implementing several cross-platform packages, which leads us to the architecture
    of such systems. We will organize our RAG pipeline into separate components because
    breaking down the RAG pipeline into independent parts will enable several teams
    to work on a project simultaneously. We will then set the blueprint for a RAG-driven
    generative AI pipeline. Finally, we will build a three-component RAG pipeline
    from scratch in Python with Activeloop Deep Lake, OpenAI, and custom-built functions.
  prefs: []
  type: TYPE_NORMAL
- en: This coding journey will take us into the depths of cross-platform environment
    issues with packages and dependencies. We will also face the challenges of chunking
    data, embedding vectors, and loading them on vector stores. We will augment the
    input of a GPT-4o model with retrieval queries and produce solid outputs. By the
    end of this chapter, you will fully understand how to leverage the power of embedded
    documents in vector stores for generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'To sum up, this chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing document embeddings and vector stores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to break a RAG pipeline into independent components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a RAG pipeline from raw data to Activeloop Deep Lake
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facing the environmental challenge of cross-platform packages and libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging the power of LLMs to embed data with an OpenAI embedding model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying an Activeloop Deep Lake vector store to augment user inputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative solid augmented outputs with OpenAI GPT-4o
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin by learning how to go from raw data to a vector store.
  prefs: []
  type: TYPE_NORMAL
- en: From raw data to embeddings in vector stores
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Embeddings convert any form of data (text, images, or audio) into real numbers.
    Thus, a document is converted into a vector. These mathematical representations
    of documents allow us to calculate the distances between documents and retrieve
    similar data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The raw data (books, articles, blogs, pictures, or songs) is first collected
    and cleaned to remove noise. The prepared data is then fed into a model such as
    OpenAI `text-embedding-3-small`, which will embed the data. Activeloop Deep Lake,
    for example, which we will implement in this chapter, will break a text down into
    pre-defined chunks defined by a certain number of characters. The size of a chunk
    could be 1,000 characters, for instance. We can let the system optimize these
    chunks, as we will implement them in the *Optimizing chunking* section of the
    next chapter. These chunks of text make it easier to process large amounts of
    data and provide more detailed embeddings of a document, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a number and embedding  Description automatically generated
    with medium confidence](img/B31169_02_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: Excerpt of an Activeloop vector store dataset record'
  prefs: []
  type: TYPE_NORMAL
- en: 'Transparency has been the holy grail in AI since the beginning of parametric
    models, in which the information is buried in learned parameters that produce
    black box systems. RAG is a game changer, as shown in *Figure 2.1*, because the
    content is fully traceable:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Left side (Text): In RAG frameworks, every piece of generated content is traceable
    back to its source data, ensuring the output’s transparency. The OpenAI generative
    model will respond, taking the augmented input into account.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Right side (Embeddings): Data embeddings are directly visible and linked to
    the text, contrasting with parametric models where data origins are encoded within
    model parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we have our text and embeddings, the next step is to store them efficiently
    for quick retrieval. This is where *vector stores* come into play. A vector store
    is a specialized database designed to handle high-dimensional data like embeddings.
    We can create datasets on serverless platforms such as Activeloop, as shown in
    *Figure 2.2*. We can create and access them in code through an API, as we will
    do in the *Building a RAG pipeline* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B31169_02_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: Managing datasets with vector stores'
  prefs: []
  type: TYPE_NORMAL
- en: Another feature of vector stores is their ability to retrieve data with optimized
    methods. Vector stores are built with powerful indexing methods, which we will
    discuss in the next chapter. This retrieving capacity allows a RAG model to quickly
    find and retrieve the most relevant embeddings during the generation phase, augment
    user inputs, and increase the model’s ability to produce high-quality output.
  prefs: []
  type: TYPE_NORMAL
- en: We will now see how to organize a RAG pipeline that goes from data collection,
    processing, and retrieval to augmented-input generation.
  prefs: []
  type: TYPE_NORMAL
- en: Organizing RAG in a pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A RAG pipeline will typically collect data and prepare it by cleaning it, for
    example, chunking the documents, embedding them, and storing them in a vector
    store dataset. The vector dataset is then queried to augment the user input of
    a generative AI model to produce an output. However, it is highly recommended
    not to run this sequence of RAG in one single program when it comes to using a
    vector store. We should at least separate the process into three components:'
  prefs: []
  type: TYPE_NORMAL
- en: Data collection and preparation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data embedding and loading into the dataset of a vector store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying the vectorized dataset to augment the input of a generative AI model
    to produce a response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s go through the main reasons for this component approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Specialization**, which will allow each member of a team to do what they
    are best at, either collecting and cleaning data, running embedding models, managing
    vector stores, or tweaking generative AI models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**, making it easier to upgrade separate components as the technology
    evolves and scale the different components with specialized methods. Storing raw
    data, for example, can be scaled on a different server than the cloud platform,
    where the embedded vectors are stored in a vectorized dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallel development**, which allows each team to advance at their pace without
    waiting for others. Improvements can be made continually on one component without
    disrupting the processes of the other components.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maintenance** is component-independent. One team can work on one component
    without affecting the other parts of the system. For example, if the RAG pipeline
    is in production, users can continue querying and running generative AI through
    the vector store while a team fixes the data collection component.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security** concerns and privacy are minimized because each team can work
    separately with specific authorization, access, and roles for each component.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we can see, in real-life production environments or large-scale projects,
    it is rare for a single program or team to manage end-to-end processes. We are
    now ready to draw the blueprint of the RAG pipeline that we will build in Python
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: A RAG-driven generative AI pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s dive into what a real-life RAG pipeline looks like. Imagine we’re a team
    that has to deliver a whole system in just a few weeks. Right off the bat, we’re
    bombarded with questions like:'
  prefs: []
  type: TYPE_NORMAL
- en: Who’s going to gather and clean up all the data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who’s going to handle setting up OpenAI’s embedding model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who’s writing the code to get those embeddings up and running and managing the
    vector store?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who’s going to take care of implementing GPT-4 and managing what it spits out?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Within a few minutes, everyone starts looking pretty worried. The whole thing
    feels overwhelming—like, seriously, who would even think about tackling all that
    alone?
  prefs: []
  type: TYPE_NORMAL
- en: 'So here’s what we do. We split into three groups, each of us taking on different
    parts of the pipeline, as shown in *Figure 2.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a pipeline  Description automatically generated](img/B31169_02_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: RAG pipeline components'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of the three groups has one component to implement:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Collection and Prep (D1 and D2)**: One team takes on collecting the
    data and cleaning it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Embedding and Storage (D2 and D3)**: Another team works on getting the
    data through OpenAI’s embedding model and stores these vectors in an Activeloop
    Deep Lake dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Augmented Generation (D4, G1-G4, and E1)**: The last team handles the big
    job of generating content based on user input and retrieval queries. They use
    GPT-4 for this, and even though it sounds like a lot, it’s actually a bit easier
    because they aren’t waiting on anyone else—they just need the computer to do its
    calculations and evaluate the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suddenly, the project doesn’t seem so scary. Everyone has their part to focus
    on, and we can all work without being distracted by the other teams. This way,
    we can all move faster and get the job done without the hold-ups that usually
    slow things down.
  prefs: []
  type: TYPE_NORMAL
- en: The organization of the project, represented in *Figure 2.3*, is a variant of
    the RAG ecosystem’s framework represented in *Figure 1.3* of *Chapter 1*, *Why
    Retrieval Augmented Generation?*
  prefs: []
  type: TYPE_NORMAL
- en: We can now begin building a RAG pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Building a RAG pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now build a RAG pipeline by implementing the pipeline described in
    the previous section and illustrated in *Figure 2.3*. We will implement three
    components assuming that three teams (`Team #1`, `Team #2`, and `Team #3`) work
    in parallel to implement the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data collection and preparation by `Team #1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data embedding and storage by `Team #2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Augmented generation by `Team #3`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first step is to set up the environment for these components.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s face it here and now. Installing cross-platform, cross-library packages
    with their dependencies can be quite challenging! It is important to take this
    complexity into account and be prepared to get the environment running correctly.
    Each package has dependencies that may have conflicting versions. Even if we adapt
    the versions, an application may not run as expected anymore. So, take your time
    to install the right versions of the packages and dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: We will only describe the environment once in this section for all three components
    and refer to this section when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: The installation packages and libraries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To build the RAG pipeline in this section, we will need packages and need to
    freeze the package versions to prevent dependency conflicts and issues with the
    functions of the libraries, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Possible conflicts between the versions of the dependencies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Possible conflicts when one of the libraries needs to be updated for an application
    to run. For example, in August 2024, installing `Deep Lake` required `Pillow`
    version 10.x.x and Google Colab’s version was 9.x.x. Thus, it was necessary to
    uninstall `Pillow` and reinstall it with a recent version before installing `Deep
    Lake`. Google Colab will no doubt update Pillow. Many cases such as this occur
    in a fast-moving market.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Possible deprecations if the versions remain frozen for too long.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Possible issues if the versions are frozen for too long and bugs are not corrected
    by upgrades.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, if we freeze the versions, an application may remain stable for some time
    but encounter issues. But if we upgrade the versions too quickly, some of the
    other libraries may not work anymore. There is no silver bullet! It’s a continual
    quality control process.
  prefs: []
  type: TYPE_NORMAL
- en: For our program, in this section, we will freeze the versions. Let’s now go
    through the installation steps to create the environment for our pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The components involved in the installation process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s begin by describing the components that are installed in the *Installing
    the environment* section of each notebook. The components are not necessarily
    installed in all notebooks; this section serves as an inventory of the packages.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first pipeline section, *1\. Data collection and preparation*, we will
    only need to install Beautiful Soup and Requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This explains why this component of the pipeline should remain separate. It’s
    a straightforward job for a developer who enjoys creating interfaces to interact
    with the web. It’s also a perfect fit for a junior developer who wants to get
    involved in data collection and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The two other pipeline components we will build in this section, *2\. Data embedding
    and storage* and *3\. Augmented generation*, will require more attention as well
    as the installation of `requirements01.txt`, as explained in the previous section.
    For now, let’s continue with the installation step by step.
  prefs: []
  type: TYPE_NORMAL
- en: Mounting a drive
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this scenario, the program mounts Google Drive in Google Colab to safely
    read the OpenAI API key to access OpenAI models and the Activeloop API token for
    authentication to access Activeloop Deep Lake datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can choose to store your keys and tokens elsewhere. Just make sure they
    are in a safe location.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a subprocess to download files from GitHub
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The goal here is to write a function to download the `grequests.py` file from
    GitHub. This program contains a function to download files using `curl`, with
    the option to add a private token if necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `grequests.py` file contains a function that can, if necessary, accept
    a private token or any other security system that requires credentials when retrieving
    data with `curl` commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Installing requirements
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, we will install the requirements for this section when working with Activeloop
    Deep Lake and OpenAI. We will only need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As of August 2024, Google Colab’s version of Pillow conflicts with `deeplake`'s
    package. However, the `deeplake` installation package deals with this automatically.
    All you have to do is restart the session and run it again, which is why `pip
    install deeplake==3.9.18` is the first line of each notebook it is installed in.
  prefs: []
  type: TYPE_NORMAL
- en: 'After installing the requirements, we must run a line of code for Activeloop
    to activate a public DNS server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Authentication process
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You will need to sign up to OpenAI to obtain an API key: [https://openai.com/](https://openai.com/).
    Make sure to check the pricing policy before using the key. First, let’s activate
    OpenAI’s API key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we activate Activeloop’s API token for Deep Lake:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You will need to sign up on Activeloop to obtain an API token: [https://www.activeloop.ai/](https://www.activeloop.ai/).
    Again, make sure to check the pricing policy before using the Activeloop token.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the environment is installed, you can hide the *Installing the environment*
    cells we just ran to focus on the content of the pipeline components, as shown
    in *Figure 2.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A white background with black text  Description automatically generated](img/B31169_02_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2:4: Hiding the installation cells'
  prefs: []
  type: TYPE_NORMAL
- en: 'The installation cells will then be hidden but can still be run, as shown in
    *Figure 2.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B31169_02_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5: Running hidden cells'
  prefs: []
  type: TYPE_NORMAL
- en: We can now focus on the pipeline components for each pipeline component. Let’s
    begin with data collection and preparation.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Data collection and preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data collection and preparation is the first pipeline component, as described
    earlier in this chapter. `Team #1` will only focus on their component, as shown
    in *Figure 2.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a pipeline component  Description automatically generated](img/B31169_02_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.6: Pipeline component #1: Data collection and preparation'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s jump in and lend a hand to `Team #1`. Our work is clearly defined, so
    we can enjoy the time taken to implement the component. We will retrieve and process
    10 Wikipedia articles that provide a comprehensive view of various aspects of
    space exploration:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Space exploration**: Overview of the history, technologies, missions, and
    plans involved in the exploration of space ([https://en.wikipedia.org/wiki/Space_exploration](https://en.wikipedia.org/wiki/Space_exploration))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apollo program**: Details about the NASA program that landed the first humans
    on the Moon and its significant missions ([https://en.wikipedia.org/wiki/Apollo_program](https://en.wikipedia.org/wiki/Apollo_program))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hubble Space Telescope**: Information on one of the most significant telescopes
    ever built, which has been crucial in many astronomical discoveries ([https://en.wikipedia.org/wiki/Hubble_Space_Telescope](https://en.wikipedia.org/wiki/Hubble_Space_Telescope))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mars rover**: Insight into the rovers that have been sent to Mars to study
    its surface and environment ([https://en.wikipedia.org/wiki/Mars_rover](https://en.wikipedia.org/wiki/Mars_rover))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**International Space Station (ISS)**: Details about the ISS, its construction,
    international collaboration, and its role in space research ([https://en.wikipedia.org/wiki/International_Space_Station](https://en.wikipedia.org/wiki/International_Space_Station))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SpaceX**: Covers the history, achievements, and goals of SpaceX, one of the
    most influential private spaceflight companies ([https://en.wikipedia.org/wiki/SpaceX](https://en.wikipedia.org/wiki/SpaceX))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Juno (spacecraft)**: Information about the NASA space probe that orbits and
    studies Jupiter, its structure, and moons ([https://en.wikipedia.org/wiki/Juno_(spacecraft))](https://en.wikipedia.org/wiki/Juno_(spacecraft))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Voyager program**: Details on the Voyager missions, including their contributions
    to our understanding of the outer solar system and interstellar space ([https://en.wikipedia.org/wiki/Voyager_program](https://en.wikipedia.org/wiki/Voyager_program))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Galileo (spacecraft)**: Overview of the mission that studied Jupiter and
    its moons, providing valuable data on the gas giant and its system ([https://en.wikipedia.org/wiki/Galileo_(spacecraft)](https://en.wikipedia.org/wiki/Galileo_(spacecraft)))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kepler space telescope**: Information about the space telescope designed
    to discover Earth-size planets orbiting other stars ([https://en.wikipedia.org/wiki/Kepler_Space_Telescope](https://en.wikipedia.org/wiki/Kepler_Space_Telescope))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These articles cover a wide range of topics in space exploration, from historical
    programs to modern technological advances and missions.
  prefs: []
  type: TYPE_NORMAL
- en: Now, open `1-Data_collection_preparation.ipynb` in the GitHub repository. We
    will first collect the data.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We just need `import requests` for the HTTP requests, `from bs4 import BeautifulSoup`
    for HTML parsing, and `import re`, the regular expressions module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We then select the URLs we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This list is in code. However, it could be stored in a database, a file, or
    any other format, such as JSON. We can now prepare the data.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we write a cleaning function. This function removes numerical references
    such as [1] [2] from a given text string, using regular expressions, and returns
    the cleaned text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we write a classical fetch and clean function, which will return a nice
    and clean text by extracting the content we need from the documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we write the content in `llm.txt` file for the team working on the
    data embedding and storage functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output confirms that the text has been written:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The program can be modified to save the data in other formats and locations,
    as required for a project’s specific needs. The file can then be verified before
    we move on to the next batch of data to retrieve and process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows the first lines of the document that will be processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This component can be managed by a team that enjoys searching for documents
    on the web or within a company’s data environment. The team will gain experience
    in identifying the best documents for a project, which is the foundation of any
    RAG framework.
  prefs: []
  type: TYPE_NORMAL
- en: '`Team #2` can now work on the data to embed the documents and store them.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Data embedding and storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`Team #2`''s job is to focus on the second component of the pipeline. They
    will receive batches of prepared data to work on. They don’t have to worry about
    retrieving data. `Team #1` has their back with their data collection and preparation
    component.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a pipeline component  Description automatically generated](img/B31169_02_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.7: Pipeline component #2: Data embedding and storage'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now jump in and help `Team #2` to get the job done. Open `2-Embeddings_vector_store.ipynb`
    in the GitHub Repository. We will embed and store the data provided by `Team #1`
    and retrieve a batch of documents to work on.'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving a batch of prepared documents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we download a batch of documents available on a server and provided
    by `Team #1`, which is the first of a continual stream of incoming documents.
    In this case, we assume it’s the space exploration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that `source_text = "llm.txt"` will be used by the function that will
    add the data to our vector store. We then briefly check the document just to be
    sure, knowing that `Team #1` has already verified the information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is satisfactory, as shown in the following excerpt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We will now chunk the data. We will determine a chunk size defined by the number
    of characters. In this case, it is `CHUNK_SIZE = 1000`, but we can select chunk
    sizes using different strategies. *Chapter 7*, *Building Scalable Knowledge-Graph-based
    RAG with Wikipedia API and LlamaIndex*, will take chunk size optimization further
    with automated seamless chunking.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chunking is necessary to optimize data processing: selecting portions of text,
    embedding, and loading the data. It also makes the embedded dataset easier to
    query. The following code chunks a document to complete the preparation process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We are now ready to create a vector store to vectorize data or add data to an
    existing one.
  prefs: []
  type: TYPE_NORMAL
- en: Verifying if the vector store exists and creating it if not
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we need to define the path of our Activeloop vector store path, whether
    our dataset exists or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Make sure to replace `` `hub://denis76/space_exploration_v1` `` with your organization
    and dataset name.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we write a function to attempt to load the vector store or automatically
    create one if it doesn’t exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output confirms that the vector store has been created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We now need to create an embedding function.
  prefs: []
  type: TYPE_NORMAL
- en: The embedding function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The embedding function will transform the chunks of data we created into vectors
    to enable vector-based search. In this program, we will use `"text-embedding-3-small"`
    to embed the documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI has other embedding models that you can use: [https://platform.openai.com/docs/models/embeddings](https://platform.openai.com/docs/models/embeddings).
    *Chapter 6*, *Scaling RAG Bank Customer Data with Pinecone*, provides alternative
    code for embedding models in the *Embedding* section. In any case, it is recommended
    to evaluate embedding models before choosing one in production. Examine the characteristics
    of each embedding model, as described by OpenAI, focusing on their length and
    capacities. `text-embedding-3-small` was chosen in this case because it stands
    out as a robust choice for efficiency and speed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `text-embedding-3-small` text embedding model from OpenAI typically uses
    embeddings with a restricted number of dimensions, to balance obtaining enough
    detail in the embeddings with large computational workloads and storage space.
    Make sure to check the model page and pricing information before running the code:
    [https://platform.openai.com/docs/guides/embeddings/embedding-models](https://platform.openai.com/docs/guides/embeddings/embedding-models).'
  prefs: []
  type: TYPE_NORMAL
- en: We are now all set to begin populating the vector store.
  prefs: []
  type: TYPE_NORMAL
- en: Adding data to the vector store
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We set the adding data flag to `True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The source text, `source_text = "llm.txt"`, has been embedded and stored. A
    summary of the dataset’s structure is displayed, showing that the dataset was
    loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Observe that the dataset contains four tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '`embedding`: Each chunk of data is embedded in a vector'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`id`: The ID is a string of characters and is unique'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata`: The metadata contains the source of the data—in this case, the
    `llm.txt` file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text`: The content of a chunk of text in the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This dataset structure can vary from one project to another, as we will see
    in *Chapter 4*, *Multimodal Modular RAG for Drone Technology*. We can also visualize
    how the dataset is organized at any time to verify the structure. The following
    code will display the summary that was just displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We can also visualize vector store information if we wish.
  prefs: []
  type: TYPE_NORMAL
- en: Vector store information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activeloop’s API reference provides us with all the information we need to
    manage our datasets: [https://docs.deeplake.ai/en/latest/](https://docs.deeplake.ai/en/latest/).
    We can visualize our datasets once we sign in at [https://app.activeloop.ai/datasets/mydatasets/](https://app.activeloop.ai/datasets/mydatasets/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also load our dataset in one line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output provides a path to visualize our datasets and query and explore
    them online:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also access your dataset directly on Activeloop by signing in and going
    to your datasets. You will find online dataset exploration tools to query your
    dataset and more, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a web page  Description automatically generated](img/B31169_02_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.8: Querying and exploring a Deep Lake dataset online.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Among the many functions available, we can display the estimated size of a
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have obtained the size, we can convert it into megabytes and gigabytes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows the size of the dataset in megabytes and gigabytes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '`Team #2`''s pipeline component for data embedding and storage seems to be
    working. Let’s now explore augmented generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Augmented input generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Augmented generation is the third pipeline component. We will use the data
    we retrieved to augment the user input. This component processes the user input,
    queries the vector store, augments the input, and calls `gpt-4-turbo`, as shown
    in *Figure 2.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a pipeline  Description automatically generated](img/B31169_02_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.9: Pipeline component #3: Augmented input generation'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2.9* shows that pipeline component #3 fully deserves its **Retrieval
    Augmented Generation** (**RAG**) name. However, it would be impossible to run
    this component without the work put in by `Team #1` and `Team #2` to provide the
    necessary information to generate augmented input content.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s jump in and see how `Team #3` does the job. Open `3-Augmented_Generation.ipynb`
    in the GitHub repository. The *Installing the environment* section of the notebook
    is described in the *Setting up the environment* section of this chapter. We select
    the vector store (replace the vector store path with your vector store):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we load the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We print a confirmation message that the vector store exists. At this point
    stage, `Team #2` previously ensured that everything was working well, so we can
    just move ahead rapidly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The output confirms that the dataset exists and is loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We assume that pipeline `component #2`, as built in the *Data embedding and
    storage* section, has created and populated the `vector_store` and has verified
    that it can be queried. Let’s now process the user input.'
  prefs: []
  type: TYPE_NORMAL
- en: Input and query retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will need the embedding function to embed the user input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we are using the same embedding model as the data embedding and storage
    component to ensure full compatibility between the input and the vector dataset:
    `text-embedding-ada-002`.'
  prefs: []
  type: TYPE_NORMAL
- en: We can now either use an interactive prompt for an input or process user inputs
    in batches. In this case, we process a user input that has already been entered
    that could be fetched from a user interface, for example.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first ask the user for an input or define one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We then plug the prompt into the search query and store the output in `search_results`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The user prompt and search results stored in `search_results` are formatted
    to be displayed. First, let’s print the user prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also wrap the retrieved text to obtain a formatted output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'However, let’s only select one of the top results and print it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows that we have a reasonably good match:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: We are ready to augment the input with the additional information we have retrieved.
  prefs: []
  type: TYPE_NORMAL
- en: Augmented input
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The program adds the top retrieved text to the user input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The output displays the augmented input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '`gpt-4o` can now process the augmented input and generate content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we are timing the process. We now write the generative AI call, adding
    roles to the message we create for the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The generative model is called with the augmented input; the response time
    is calculated and displayed along with the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the raw output is displayed with the response time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s format the output with `textwrap` and print the result. `print_formatted_response(response)`
    first checks if the response returned contains Markdown features. If so, it will
    format the response; if not, it will perform a standard output text wrap:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is satisfactory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Let’s introduce an evaluation metric to measure the quality of the output.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the output with cosine similarity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will implement cosine similarity to measure the similarity
    between user input and the generative AI model’s output. We will also measure
    the augmented user input with the generative AI model’s output. Let’s first define
    a cosine similarity function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let’s calculate a score that measures the similarity between the user
    prompt and GPT-4’s response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The score is low, although the output seemed acceptable for a human:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: It seems that either we missed something or need to use another metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try to calculate the similarity between the augmented input and GPT-4’s
    response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The score seems better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Can we use another method? Cosine similarity, when using **Term Frequency-Inverse
    Document Frequency** (**TF-IDF**), relies heavily on exact vocabulary overlap
    and takes into account important language features, such as semantic meanings,
    synonyms, or contextual usage. As such, this method may produce lower similarity
    scores for texts that are conceptually similar but differ in word choice.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, using Sentence Transformers to calculate similarity involves embeddings
    that capture deeper semantic relationships between words and phrases. This approach
    is more effective in recognizing the contextual and conceptual similarity between
    texts. Let’s try this approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s install `sentence-transformers`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Be careful installing this library at the end of the session, since it may induce
    potential conflicts with the RAG pipeline’s requirements. Depending on a project’s
    needs, this code could be yet another separate pipeline component.
  prefs: []
  type: TYPE_NORMAL
- en: As of August 2024, using a Hugging Face token is optional. If Hugging Face requires
    a token, sign up to Hugging Face to obtain an API token, check the conditions,
    and set up the key as instructed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now use a MiniLM architecture to perform the task with `all-MiniLM-L6-v2`.
    This model is available through the Hugging Face Model Hub we are using. It’s
    part of the `sentence-transformers` library, which is an extension of the Hugging
    Face Transformers library. We are using this architecture because it offers a
    compact and efficient model, with a strong performance in generating meaningful
    sentence embeddings quickly. Let’s now implement it with the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now call the function to calculate the similarity between the augmented
    user input and GPT-4’s response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that the Sentence Transformer captures semantic similarities
    between the texts more effectively, resulting in a high cosine similarity score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: The choice of metrics depends on the specific requirements of each project phase.
    *Chapter 3*, *Building Index-Based RAG with LlamaIndex, Deep Lake, and OpenAI*,
    will provide advanced metrics when we implement index-based RAG. At this stage,
    however, the RAG pipeline’s three components have been successfully built. Let’s
    summarize our journey and move to the next level!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we tackled the complexities of using RAG-driven generative
    AI, focusing on the essential role of document embeddings when handling large
    datasets. We saw how to go from raw texts to embeddings and store them in vector
    stores. Vector stores such as Activeloop, unlike parametric generative AI models,
    provide API tools and visual interfaces that allow us to see embedded text at
    any moment.
  prefs: []
  type: TYPE_NORMAL
- en: A RAG pipeline detailed the organizational process of integrating OpenAI embeddings
    into Activeloop Deep Lake vector stores. The RAG pipeline was broken down into
    distinct components that can vary from one project to another. This separation
    allows multiple teams to work simultaneously without dependency, accelerating
    development and facilitating specialized focus on individual aspects, such as
    data collection, embedding processing, and query generation for the augmented
    generation AI process.
  prefs: []
  type: TYPE_NORMAL
- en: We then built a three-component RAG pipeline, beginning by highlighting the
    necessity of specific cross-platform packages and careful system architecture
    planning. The resources involved were Python functions built from scratch, Activeloop
    Deep Lake to organize and store the embeddings in a dataset in a vector store,
    an OpenAI embedding model, and OpenAI’s GPT-4o generative AI model. The program
    guided us through building a three-part RAG pipeline using Python, with practical
    steps that involved setting up the environment, handling dependencies, and addressing
    implementation challenges like data chunking and vector store integration.
  prefs: []
  type: TYPE_NORMAL
- en: This journey provided a robust understanding of embedding documents in vector
    stores and leveraging them for enhanced generative AI outputs, preparing us to
    apply these insights to real-world AI applications in well-organized processes
    and teams within an organization. Vector stores enhance the retrieval of documents
    that require precision in information retrieval. Indexing takes RAG further and
    increases the speed and relevance of retrievals. The next chapter will take us
    a step further by introducing advanced indexing methods to retrieve and augment
    inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Answer the following questions with *Yes* or *No*:'
  prefs: []
  type: TYPE_NORMAL
- en: Do embeddings convert text into high-dimensional vectors for faster retrieval
    in RAG?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are keyword searches more effective than embeddings in retrieving detailed semantic
    content?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is it recommended to separate RAG pipelines into independent components?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the RAG pipeline consist of only two main components?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can Activeloop Deep Lake handle both embedding and vector storage?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the text-embedding-3-small model from OpenAI used to generate embeddings
    in this chapter?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are data embeddings visible and directly traceable in an RAG-driven system?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can a RAG pipeline run smoothly without splitting into separate components?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is chunking large texts into smaller parts necessary for embedding and storage?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are cosine similarity metrics used to evaluate the relevance of retrieved information?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenAI Ada documentation for embeddings: [https://platform.openai.com/docs/guides/embeddings/embedding-models](https://platform.openai.com/docs/guides/embeddings/embedding-models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI GPT documentation for content generation: [https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4](https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Activeloop API documentation: [https://docs.deeplake.ai/en/latest/](https://docs.deeplake.ai/en/latest/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MiniLM model reference: [https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenAI’s documentation on embeddings: [https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Activeloop documentation: [https://docs.activeloop.ai/](https://docs.activeloop.ai/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/rag](https://www.packt.link/rag)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code50409000288080484.png)'
  prefs: []
  type: TYPE_IMG
