- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning NLP Basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While working on this book, we were focusing on including recipes that should
    be useful for a wide variety of NLP projects. They range from simple to advanced,
    from dealing with grammar to dealing with visualizations, and in many of them,
    options for languages other than English are included. In this new edition, we
    have included new topics that cover using GPT and other large language models,
    explainable AI, a new chapter on transformers, and natural language understanding.
    We hope you find the book useful.
  prefs: []
  type: TYPE_NORMAL
- en: The format of the book is that of a *programming cookbook*, where each recipe
    is a short mini-project with a concrete goal and a sequence of steps that need
    to be performed. There are few theoretical explanations and a focus on the practical
    goals and what needs to be done to achieve them.
  prefs: []
  type: TYPE_NORMAL
- en: Before we can get on with the real work of NLP, we need to prepare our text
    for processing. This chapter will show you how to do it. By the end of the chapter,
    you will be able to have a list of words in a text with their parts of speech
    and lemmas or stems, and with very frequent words removed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Natural Language Toolkit** (**NLTK**) and **spaCy** are two important packages
    that we will be working with in this chapter and throughout the book. Some other
    packages we will be using in the book are PyTorch and Hugging Face Transformers.
    We will also utilize the OpenAI API with the GPT models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The recipes included in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Dividing text into sentences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dividing sentences into words – tokenization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part of speech tagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining similar words – lemmatization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing stopwords
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, we will use **Poetry** to manage the Python package installations.
    You can use the latest version of Poetry since it conserves the previous versions’
    functionality. Once you install Poetry, managing which packages to install will
    be very easy. We will be using **Python 3.9** throughout the book. You will also
    need to have **Jupyter** installed in order to run the notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You may try to use Google Colab in order to run the notebooks but you will need
    to tweak the code to make it work with Colab.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these installation steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install **Git**: [https://github.com/git-guides/install-git](https://github.com/git-guides/install-git).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install **Poetry**: [https://python-poetry.org/docs/#installation](https://python-poetry.org/docs/#installation).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install **Jupyter**: [https://jupyter.org/install](https://jupyter.org/install).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Clone the GitHub repository that contains all the code from this book ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition))
    by issuing the following command in the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the directory that contains the **pyproject.toml** file, run the commands
    using the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the notebook engine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now, you should be able to run all the notebooks in your cloned repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you prefer not to use Poetry, you can set up a virtual environment using
    the `requirements.txt` file provided in the book repository. You can do this in
    one of two ways. You can use `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use `conda`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Dividing text into sentences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we work with text, we can work with text units on different scales: the
    document itself, such as a newspaper article, the paragraph, the sentence, or
    the word. Sentences are the main unit of processing in many NLP tasks. For example,
    when we send data over to **Large Language Models** (**LLMs**), we frequently
    want to add some context to the prompt. In some cases, we would like that context
    to include sentences from a text so that the model can extract some important
    information from that text. In this section, we will show you how to divide a
    text into sentences.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this part, we will be using the text of the book *The Adventures of Sherlock
    Holmes*. You can find the whole text in the book’s GitHub file ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes.txt](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes.txt)).
    For this recipe we will need just the beginning of the book, which can be found
    in the file at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt).
  prefs: []
  type: TYPE_NORMAL
- en: In order to do this task, you will need the NLTK package and its sentence tokenizers,
    which are part of the Poetry file. Directions to install Poetry are described
    in the *Technical* *requirements* section.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will now divide the text of a small piece of *The Adventures of Sherlock
    Holmes*, outputting a list of sentences. (Reference notebook: [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/dividing_text_into_sentences_1.1.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/dividing_text_into_sentences_1.1.ipynb).)
    Here, we assume that you are running the notebook, so the paths are all relative
    to the notebook location:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the file utility functions from the **util** folder ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/file_utils.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/file_utils.ipynb)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read in the book part text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `read_text_file` function is located in the `util` notebook we imported
    previously. Here is its source code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print out the resulting text to make sure everything worked correctly and the
    file loaded:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The beginning of the printout will look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the **nltk** package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If this is the first time you are running the code, you will need to download
    tokenizer data. You will not need to run this command after that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the tokenizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Divide the text into sentences using the tokenizer. The result will be a list
    of sentences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It should look like this. There are newlines inside the sentences that come
    from the book formatting. They are not necessarily sentence endings:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the number of sentences in the result; there should be 11 sentences in
    total:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This gives the result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Although it might seem straightforward to divide a text into sentences by just
    using a regular expression to split it at the periods, in reality, it is more
    complicated. We use periods in places other than ends of sentences; for example,
    after abbreviations – for example, “Dr. Smith will see you now.” Similarly, while
    all sentences in English start with a capital letter, we also use capital letters
    for proper names. The approach used in `nltk` takes all these points into consideration;
    it is an implementation of an unsupervised algorithm presented in [https://aclanthology.org/J06-4003.pdf](https://aclanthology.org/J06-4003.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also use a different strategy to parse the text into sentences, employing
    the other very popular NLP package, **spaCy**. Here is how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the spaCy package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The first time you run the notebook, you will need to download a spaCy model.
    The model is trained on a large amount of English text and there are several tools
    that can be used with it, including the sentence tokenizer. Here, I’m downloading
    the smallest model, but you might try other ones (see [https://spacy.io/usage/models/](https://spacy.io/usage/models/)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the spaCy engine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Process the text using the spaCy engine. This line assumes that you have the
    **sherlock_holmes_part_of_text** variable initialized. If not, you need to run
    one of the earlier cells where the text is read into this variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the sentences from the processed **doc** object, and print the resulting
    array and its length:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'An important difference between spaCy and NLTK is the time it takes to complete
    the sentence-splitting process. The reason for this is that spaCy loads a language
    model and uses several tools in addition to the tokenizer, while the NLTK tokenizer
    has only one function: to separate the text into sentences. We can time the execution
    by using the `time` package and putting the code to split the sentences into the
    `main` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The spaCy algorithm takes 0.019 seconds, while the NLTK algorithm takes 0.0002\.
    The time is calculated by subtracting the current time (`time.time()`) from the
    start time that is set at the beginning of the code block. It is possible that
    you will get slightly different values.
  prefs: []
  type: TYPE_NORMAL
- en: The reason why you might use spaCy is if you are doing other processing with
    the package along with splitting it into sentences. The spaCy processor does many
    other things, and that is why it takes longer. If you are using other features
    of spaCy, there is no reason to use NLTK just for sentence splitting, and it’s
    better to employ spaCy for the whole pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to use only the tokenizer without other tools from spaCy.
    Please see their documentation for more information: [https://spacy.io/usage/processing-pipelines](https://spacy.io/usage/processing-pipelines).'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: spaCy might be slower, but it is doing many more things in the background, and
    if you are using its other features, use it for sentence splitting as well.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can use NLTK and spaCy to divide texts in languages other than English.
    NLTK includes tokenizer models for Czech, Danish, Dutch, Estonian, Finnish, French,
    German, Greek, Italian, Norwegian, Polish, Portuguese, Slovene, Spanish, Swedish,
    and Turkish. In order to load those models, use the name of the language followed
    by the `.``pickle` extension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'See the NLTK documentation to find out more: [https://www.nltk.org/index.html](https://www.nltk.org/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Likewise, spaCy has models for other languages: Chinese, Dutch, English, French,
    German, Greek, Italian, Japanese, Portuguese, Romanian, Spanish, and others. These
    models are trained on text in those languages. In order to use those models, you
    would have to download them separately. For example, for Spanish, use this command
    to download the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, put this line in the code to use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'See the spaCy documentation to find out more: [https://spacy.io/usage/models](https://spacy.io/usage/models).'
  prefs: []
  type: TYPE_NORMAL
- en: Dividing sentences into words – tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many instances, we rely on individual words when we do NLP tasks. This happens,
    for example, when we build semantic models of texts by relying on the semantics
    – of individual words, or when we are looking for words with a specific part of
    speech. To divide text into words, we can use NLTK and spaCy to do this task for
    us.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this part, we will be using the same text of the book *The Adventures of
    Sherlock Holmes*. You can find the whole text in the book’s GitHub repository.
    For this recipe, we will need just the beginning of the book, which can be found
    in the `sherlock_holmes_1.txt` file.
  prefs: []
  type: TYPE_NORMAL
- en: In order to do this task, you will need the NLTK and spaCy packages, which are
    part of the Poetry file. Directions to install Poetry are described in the *Technical*
    *requirements* section.
  prefs: []
  type: TYPE_NORMAL
- en: '(Notebook reference: [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/dividing_sentences_into_words_1.2.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/dividing_sentences_into_words_1.2.ipynb).)'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The process is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the **file_utils** notebook. Effectively, we run the **file_utils**
    notebook inside this one so we have access to its defined functions and variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read in the book snippet text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the **nltk** package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Divide the input into words. Here, we use the NLTK word tokenizer to split
    the text into individual words. The output of the function is a Python list of
    the words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be the list of words in the text and the length of the **words**
    list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is a list, where each token is either a word or a punctuation mark.
    The NLTK tokenizer uses a set of rules to split the text into words. It splits
    but does not expand contractions, such as *don’t → do n’t* and *men’s → men ’s*,
    as in the preceding example. It treats punctuation and quotes as separate tokens,
    so the result includes words with no other marks.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sometimes, it is useful not to split some words and use them as one unit. One
    example of this is in [*Chapter 3*](B18411_03.xhtml#_idTextAnchor067), in the
    *Representing phrases – phrase2vec* recipe, where we store phrases and not just
    individual words. The NLTK package allows us to do that using its custom tokenizer,
    `MWETokenizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the **MWETokenizer** class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the tokenizer and indicate that the words **dim sum dinner** should
    not be split:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add more words that should be kept together:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the tokenizer to split a sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will contain the tokens split the same way as previously:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split a different sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this case, the tokenizer will put the phrases together into one unit and
    insert underscores instead of spaces:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can also use spaCy to do the tokenization. Word tokenization is one task
    in a larger array of tasks that spaCy accomplishes while processing text.
  prefs: []
  type: TYPE_NORMAL
- en: There's still more
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you are doing further processing on the text, it makes sense to use spaCy.
    Here is how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the **spacy** package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Execute this command only if you have not before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the spaCy engine using the English model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Divide the text into sentences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will notice that the length of the word list is longer when using spaCy
    than NLTK. One of the reasons is that spaCy keeps the newlines, and each newline
    is a separate token. The other difference is that spaCy splits words with a dash,
    such as *high-power*. You can find the exact difference between the two lists
    by running the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'This should result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If you are doing other processing with spaCy, it makes sense to use it. Otherwise,
    NLTK word tokenization is sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The NLTK package only has word tokenization for English.
  prefs: []
  type: TYPE_NORMAL
- en: 'spaCy has models for other languages: Chinese, Dutch, English, French, German,
    Greek, Italian, Japanese, Portuguese, Romanian, Spanish, and others. In order
    to use those models, you would have to download them separately. For example,
    for Spanish, use this command to download the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, put this line in the code to use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'See the spaCy documentation to find out more: [https://spacy.io/usage/models](https://spacy.io/usage/models).'
  prefs: []
  type: TYPE_NORMAL
- en: Part of speech tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many cases, NLP processing depends on determining the parts of speech of
    the words in the text. For example, when we want to find out the named entities
    that appear in a text, we need to know the parts of speech of the words. In this
    recipe, we will again consider NLTK and spaCy algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this part, we will be using the same text of the book *The Adventures of
    Sherlock Holmes*. You can find the whole text in the book’s Github repository.
    For this recipe, we will need just the beginning of the book, which can be found
    in the file at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt).
  prefs: []
  type: TYPE_NORMAL
- en: In order to do this task, you will need the NLTK and spaCy packages, described
    in the *Technical* *requirements* section.
  prefs: []
  type: TYPE_NORMAL
- en: We will also complete this task using the OpenAI API’s GPT model to demonstrate
    that it can complete it as well as spaCy and NLTK. For this part to run, you will
    need the `openai` package, which is included in the Poetry environment. You will
    also need your own OpenAI API key.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will use the spaCy package to label words with their parts
    of speech.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the **util** file and the language **util** file. The language **util**
    file contains an import of spaCy and NLTK, as well as an initialization of the
    small spaCy model into the **small_model** object. These files also include functions
    to read in text from a file and tokenization functions using spaCy and NLTK:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We will define the function that will output parts of speech for every word.
    In this function, we first process the input text using the spaCy model that results
    in a **Document** object. The resulting **Document** object contains an iterator
    with **Token** objects, and each **Token** object has information about parts
    of speech.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use this information to create the two lists, one with words and the other
    one with their respective parts of speech.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we zip the two lists to pair the words with the parts of speech and
    return the resulting list of tuples. We do this in order to easily print the whole
    list with their corresponding parts of speech. When you use part of speech tagging
    in your code, you can just iterate through the list of tokens:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read in the text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the preceding function using the text and the model as input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Part of the result is shown in the following; for the complete output, please
    see the Jupyter notebook ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/part_of_speech_tagging_1.3.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/part_of_speech_tagging_1.3.ipynb)):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting list contains tuples of words and parts of speech. The list of
    part of speech tags is available here: [https://universaldependencies.org/u/pos/](https://universaldependencies.org/u/pos/).'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can compare spaCy’s performance to NLTK in this task. Here are the steps
    for getting the parts of speech with NLTK:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The imports have been taken care of in the language **util** file that we imported,
    so the first thing we do is create a function that outputs parts of speech for
    the words that are input. In it, we utilize the **word_tokenize_nltk** function
    that is also imported from the language **util** notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we apply the function to the text that we read in previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print out the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Part of the output is shown in the following. For the complete output, please
    see the Jupyter notebook:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The list of part of speech tags that NLTK uses is different from what SpaCy
    uses, and can be accessed by running the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Comparing the performance, we see that spaCy takes 0.02 seconds, while NLTK
    takes 0.01 seconds (your numbers might be different), so their performance is
    similar, with NLTK being a little better. However, the part of speech information
    is already available in the spaCy objects after the initial processing has been
    done, so if you are doing any further processing, spaCy is a better choice.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: spaCy does all of its processing at once, and the results are stored in the
    **Doc** object. The part of speech information is available by iterating through
    **Token** objects.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can use the OpenAI API with the GPT-3.5 and GPT-4 models to perform various
    tasks, including many NLP ones. Here, we show how to use the OpenAI API to get
    NLTK-style parts of speech for input text. You can also specify in the prompt
    the output format and the style of the part of speech tags. For this code to run
    correctly, you will need your own OpenAI API key:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import **openai** and create the OpenAI client using your API key. The **OPEN_AI_KEY**
    constant variable is set in the **../****util/file_utils.ipynb** file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the prompt:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Send the request to the OpenAI API. Some of the important parameters that we
    send to the API are the model we want to use, the temperature, which affects how
    much the response from the model will vary, and the maximum amount of tokens the
    model should return as a completion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the response:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To see just the GPT output, do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can use the **literal_eval** function to transform the response into a tuple.
    We request that the GPT model return only the answer without additional explanations
    so that there is no free text inside the answer and we can process it automatically.
    We do this in order to be able to compare the output of the OpenAI API to the
    NLTK output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s time the GPT function so we can compare its performance to the other
    methods we used previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of GPT is very similar to NLTK, but slightly different:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This outputs the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The difference between GPT and NLTK is that GPT tags the word whole as an adjective
    and NLTK tags it as a noun. In this context, NLTK is correct.
  prefs: []
  type: TYPE_NORMAL
- en: We see that the LLM outputs very similar results but is about 400 times slower
    than NLTK.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you would like to tag a text in another language, you can do so by using
    spaCy’s models for other languages. For example, we can load the Spanish spaCy
    model to run it on Spanish text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: In the case that spaCy doesn’t have a model for the language you are working
    with, you can train your own model with spaCy. See [https://spacy.io/usage/training#tagger-parser](https://spacy.io/usage/training#tagger-parser).
  prefs: []
  type: TYPE_NORMAL
- en: Combining similar words – lemmatization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can find the canonical form of the word using **lemmatization**. For example,
    the lemma of the word *cats* is *cat*, and the lemma for the word *ran* is *run*.
    This is useful when we are trying to match some word and don’t want to list out
    all the possible forms. Instead, we can just use its lemma.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be using the spaCy package for this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When the spaCy model processes a piece of text, the resulting `Document` object
    contains an iterator over the `Token` objects within it, as we saw in the *Part
    of speech tagging* recipe. These `Token` objects contain the lemma information
    for each word in the text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps for getting the lemmas:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the file and language **utils** files. This will import spaCy and initialize
    the **small_model** object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a list of words we want to lemmatize:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a **Document** object for each of the words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the words and their lemmas for each of the words in the list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result shows correct lemmatization for all words. However, some words are
    ambiguous. For example, the word *leaves* could either be a verb, in which case
    the lemma is correct, or a noun, in which case this is the wrong lemma. If we
    give the spaCy continuous text instead of individual words, it is likely to correctly
    disambiguate the words.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, apply lemmatization to the longer text. Here, we read in a small portion
    of the *Sherlock Holmes* text and lemmatize its every word:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The partial result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can use the spaCy lemmatizer object to find out whether a word is in its
    base form or not. We might do this while manipulating the grammar of the sentence,
    for example, in the task of turning a passive sentence into an active one. We
    can get to the lemmatizer object by manipulating the spaCy pipeline, which includes
    various tools that are applied to the text. See [https://spacy.io/usage/processing-pipelines/](https://spacy.io/usage/processing-pipelines/)
    for more information. Here are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline components are located in a list of tuples, **(component name,
    component)**. To get the lemmatizer component, we need to loop through this list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can apply the **is_base_form** function call to every word in the Sherlock
    Holmes text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The partial result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Removing stopwords
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we work with words, especially if we are considering the words’ semantics,
    we sometimes need to exclude some very frequent words that do not bring any substantial
    meaning into the sentence (words such as *but*, *can*, *we*, etc.). For example,
    if we want to get a rough sense of the topic of a text, we could count its most
    frequent words. However, in any text, the most frequent words will be stopwords,
    so we want to remove them before processing. This recipe shows how to do that.
    The stopwords list we are using in this recipe comes from the NLTK package and
    might not include all the words you need. You will need to modify the list accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs: []
  type: TYPE_NORMAL
- en: We will remove stopwords using spaCy and NLTK; these packages are part of the
    Poetry environment that we installed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using the *Sherlock Holmes* text referred to earlier. For this recipe,
    we will need just the beginning of the book, which can be found in the file at
    [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt).
  prefs: []
  type: TYPE_NORMAL
- en: In *step 1*, we run the utilities notebooks. In *step 2*, we import the `nltk`
    package and its stopwords list. In *step 3*, we download the stopwords data, if
    necessary. In *step 4*, we print out the stopwords list. In *step 5*, we read
    in a small portion of the *Sherlock Holmes* book. In *step 6*, we tokenize the
    text and print its length, which is 230\. In *step 7*, we remove the stopwords
    from the original words list by using a list comprehension. Then, we print the
    length of the result and see that the list length has been reduced to 105\. You
    will notice that in the list comprehension, we check whether the *lowercase* version
    of the word is in the stopwords list since all the stopwords are lowercase.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the recipe, we will read in the text file, tokenize the text, and remove
    the stopwords from the list:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the file and language utilities notebooks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the NLTK stopwords list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The first time you run the notebook, download the **stopwords** data. You don’t
    need to download the stopwords again the next time you run the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a list of languages that NLTK supports for stopwords: Arabic, Azerbaijani,
    Danish, Dutch, English, Finnish, French, German, Greek, Hungarian, Italian, Kazakh,
    Nepali, Norwegian, Portuguese, Romanian, Russian, Spanish, Swedish, and Turkish.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the stopwords that come with NLTK by printing the list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read in the text file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Tokenize the text and print the length of the resulting list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Remove the stopwords from the list using a list comprehension and print the
    length of the result. You will notice that in the list comprehension, we check
    whether the *lowercase* version of the word is in the stopwords list since all
    the stopwords are lowercase.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The code then filters the stopwords from the text and leaves the words from
    the text only if they do not also appear in the stopwords list. As we see from
    the lengths of the two lists, one unfiltered and the other without the stopwords,
    we remove more than half the words.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: You might find that some of the words in the stopwords list provided are not
    necessary or are missing. You will need to modify the list accordingly. The NLTK
    stopwords list is a Python list and you can add and remove elements using the
    standard Python list functions.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also remove stopwords using spaCy. Here is how to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assign the stopwords to a variable for convenience:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Tokenize the text and print its length:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It will give us the following result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Remove the stopwords from the list using a list comprehension and print the
    resulting length:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be very similar to NLTK :'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The stopwords from spaCy are stored in a set and we can add more words to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similarly, we can remove words if necessary:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can also compile a stopwords list using the text we are working with and
    calculate the frequencies of the words in it. This provides you with an automatic
    way of removing stopwords, without the need for manual review.
  prefs: []
  type: TYPE_NORMAL
- en: There's still more
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will show you two ways of doing so. You will need to use
    the file at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes.txt](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes.txt).
    The `FreqDist` object in the NLTK package counts the number of occurrences of
    each word that we later use to find the most frequent words and remove them as
    stopwords:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the NTLK **FreqDist** class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the function that will compile a list of stopwords:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the stopwords list using the default settings, and print out the result
    and its length:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, use the function with the frequency cut-off of 5% (use the top 5% of the
    most frequent words as stopwords):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, use the absolute frequency cut-off of **100** (take the words that have
    a frequency greater than 100):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And the result is the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The function that creates the stopwords list takes in the text and the `cut_off`
    parameter. It could be a float representing the percentage of frequency-ranked
    words that will be in the stopwords list. Alternatively, it could be an integer
    that represents the absolute threshold frequency, with words above it considered
    stopwords. In the function, we first tokenize the words from the book, then create
    a `FreqDist` object, and then create a list of tuples (word, word’s frequency)
    using the frequency distribution. We sort the list using the word frequency. We
    then check the `cut_off` parameter’s type and raise an error if it is not a float
    or an integer. If it is an integer, we return all words whose frequency is higher
    than the parameter as stopwords. If it is a float, we calculate the number of
    words to be returned using the parameter as the percentage.
  prefs: []
  type: TYPE_NORMAL
