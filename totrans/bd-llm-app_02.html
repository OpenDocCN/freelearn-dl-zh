<html><head></head><body>
<div><h1 class="chapterNumber">2</h1>
<h1 class="chapterTitle" id="_idParaDest-28">LLMs for AI-Powered Applications</h1>
<p class="normal">In <em class="chapterRef">Chapter 1</em>, <em class="italic">Introduction to Large Language Models</em>, we introduced <strong class="keyWord">large language models</strong> (<strong class="keyWord">LLM</strong>s) as powerful foundation models with generative capabilities as well as powerful common-sense reasoning. Now, the next question is: what should I do with those models?</p>
<p class="normal">In this chapter, we are going to see how LLMs are revolutionizing the world of software development, leading to a new era of AI-powered applications. By the end of this chapter, you will have a clearer picture of how LLMs can be embedded in different application scenarios, thanks to the new AI orchestrator frameworks that are populating the market of AI development.</p>
<p class="normal">In this chapter, we will cover the following topics:</p>
<ul>
<li class="bulletList">How LLMs are changing software development</li>
<li class="bulletList">The copilot system</li>
<li class="bulletList">Introducing AI orchestrators to embed LLMs into applications</li>
</ul>
<h1 class="heading-1" id="_idParaDest-29">How LLMs are changing software development</h1>
<p class="normal">LLMs have proven to have extraordinary capabilities: from natural language understanding tasks (summarization, named entity recognition, and classification) to text generation, from common-sense reasoning to brainstorming skills. However, they are not just incredible by themselves. As discussed in <em class="chapterRef">Chapter 1</em>, LLMs and, generally speaking, <strong class="keyWord">large foundation models</strong> (<strong class="keyWord">LFMs</strong>), are revolutionizing software<a id="_idIndexMarker114"/> development by serving as platforms for building powerful applications.</p>
<p class="normal">In fact, instead of starting from scratch, today developers can make API calls to a hosted version of an LLM, with the option of customizing it for their specific needs, as we saw in the previous chapter. This shift allows teams to incorporate the power of AI more easily and efficiently into their applications, similar to the transition from single-purpose computing to time-sharing in the past.</p>
<p class="normal">But what does it mean, concretely, to incorporate LLMs within applications? There are two main aspects to consider when incorporating LLMs within applications:</p>
<ul>
<li class="bulletList"><strong class="keyWord">The technical aspect</strong>, which covers the <em class="italic">how</em>. Integrating<a id="_idIndexMarker115"/> LLMs into applications involves embedding them through REST API calls and managing them with AI orchestrators. This means setting up architectural components that allow seamless communication with the LLMs via API calls. Additionally, using AI orchestrators helps to efficiently manage and coordinate the LLMs’ functionality within the application, as we will discuss later in this chapter.</li>
<li class="bulletList"><strong class="keyWord">The conceptual aspect</strong>, which covers the <em class="italic">what</em>. LLMs bring a plethora<a id="_idIndexMarker116"/> of new capabilities that can be harnessed within applications. These capabilities will be explored in detail later in this book. One way to view LLMs’ impact is by considering them as a new category of software, often referred to as <em class="italic">copilot</em>. This categorization highlights the significant assistance and collaboration provided by LLMs in enhancing application functionalities.</li>
</ul>
<p class="normal">We will delve into the technical aspect later on in this chapter, while the next section will cover a brand-new category of software – the copilot system.</p>
<h1 class="heading-1" id="_idParaDest-30">The copilot system</h1>
<p class="normal">The copilot system is a new category<a id="_idIndexMarker117"/> of software that serves as an expert helper to users trying to accomplish complex tasks. This concept was coined by Microsoft and has already been introduced into its applications, such as M365 Copilot and the new Bing, now powered by GPT-4. With the same framework that is used by these products, developers can now build their own copilots to embed within their applications.</p>
<p class="normal">But what exactly is a copilot?</p>
<p class="normal">As the name suggests, copilots<a id="_idIndexMarker118"/> are meant to be AI assistants that work side by side with users and support them in various activities, from information retrieval to blog writing and posting, from brainstorming ideas to code review and generation.</p>
<p class="normal">The following are some unique features<a id="_idIndexMarker119"/> of copilots:</p>
<ul>
<li class="bulletList"><strong class="keyWord">A copilot is powered by LLMs</strong>, or, more generally, LFMs, meaning that these are the reasoning engines that make the copilot “intelligent.” This reasoning engine is one of its components, but not the only one. A copilot also relies on other technologies, such as apps, data sources, and user interfaces, to provide a useful and engaging experience for users. The following illustration shows how this works:</li>
</ul>
<figure class="mediaobject"><img alt="A cartoon of a person  Description automatically generated" src="img/B21714_02_01.png"/></figure>
<p class="packt_figref">Figure 2.1: A copilot is powered by an LLM</p>
<ul>
<li class="bulletList"><strong class="keyWord">A copilot is designed to have a conversational user interface,</strong> allowing users to interact<a id="_idIndexMarker120"/> with it using natural language. This reduces or even eliminates the knowledge gap between complex systems that need domain-specific taxonomy (for example, querying tabular data needs the knowledge of programming languages such as T-SQL) and users. Let’s look at an example of such a conversation:</li>
</ul>
<figure class="mediaobject"><img alt="" role="presentation" src="img/B21714_02_02.png"/></figure>
<p class="packt_figref">Figure 2.2: An example of a conversational UI to reduce the gap between the user and the database</p>
<ul>
<li class="bulletList"><strong class="keyWord">A copilot has a scope.</strong> This means that it is <strong class="keyWord">grounded</strong> to domain-specific data so that it is entitled to answer only within the perimeter of the application or domain.</li>
</ul>
<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal">Grounding is the process of using LLMs<a id="_idIndexMarker121"/> with information that is use case specific, relevant, and not available as part of the LLM’s trained knowledge. It is crucial for ensuring the quality, accuracy, and relevance of the output. For example, let’s say you want an LLM-powered application that assists you during your research on up-to-date papers (not included in the training dataset of your LLM). You also want your app to only respond if the answer is included in those papers. To do so, you will need to ground your LLM to the set of papers, so that your application will only respond within this perimeter.</p>
</div>
<p class="normal-one">Grounding is achieved through<a id="_idIndexMarker122"/> an architectura<a id="_idIndexMarker123"/>l framework called retrieval-augmented generation (RAG), a technique that enhances the output of LLMs by incorporating information from an external, authoritative knowledge base before generating a response. This process helps to ensure that the generated content is relevant, accurate, and up to date.</p>
<div><p class="normal">What is the difference between<a id="_idIndexMarker124"/> a copilot and a RAG? RAG<a id="_idIndexMarker125"/> can be seen as one of the architectural patterns that feature a copilot. Whenever we want our copilot to be grounded to domain-specific data, we use a RAG framework. Note that RAG is not the only architectural pattern that can feature a copilot: there are further frameworks such as function calling or multi-agents that we will explore throughout the book.</p>
</div>
<p class="normal-one">For example, let’s say we developed a copilot within our company that allows employees to chat with their enterprise knowledge base. As fun as it can be, we cannot provide users with a copilot they can use to plan their summer trip (it would be like providing users with a ChatGPT-like tool at our own hosting cost!); on the contrary, we want the copilot to be grounded only to our enterprise knowledge base so that it can respond only if the answer is pertinent to the domain-specific context.</p>
<p class="normal-one">The following figure shows an example of grounding a copilot system:</p>
<figure class="mediaobject"><img alt="" role="presentation" src="img/B21714_02_03.png"/></figure>
<p class="packt_figref">Figure 2.3: Example of grounding a copilot</p>
<ul>
<li class="bulletList"><strong class="keyWord">The copilot’s capabilities can be extended by skills</strong>, which can be code or calls to other <a id="_idIndexMarker126"/>models. In fact, the LLM (our reasoning engine) might have two kinds of limitations:<ul>
<li class="bulletList level-2"><strong class="keyWord">Limited parametric knowledge.</strong> This is due to the knowledge base cutoff date, which is a physiological feature of LLMs. In fact, their training dataset will always be “outdated,” not in line with the current trends. This can be overcome by adding non-parametric knowledge with grounding, as previously seen.</li>
<li class="bulletList level-2"><strong class="keyWord">Lack of executive power. </strong>This means that LLMs by themselves are not empowered to carry out actions. Let’s consider, for example, the well-known ChatGPT: if we ask it to generate a LinkedIn post about productivity tips, we will then need to copy and paste it onto our LinkedIn profile as ChatGPT is not able to do so by itself. That is the reason why we need plug-ins. Plug-ins are LLMs’ connectors toward the external world that serve not only as input sources to extend LLMs’ non-parametric knowledge (for example, to allow a web search) but also as output sources so that the copilot can actually execute actions. For example, with a LinkedIn plug-in, our copilot powered by an LLM will be able not only to generate the post but also to post it online.</li>
</ul>
<figure class="mediaobject"><img alt="A cartoon of a person pointing at a plug-in  Description automatically generated" src="img/B21714_02_04.png"/></figure>
<p class="packt_figref">Figure 2.4: Example of Wikipedia and LinkedIn plug-ins</p>
</li>
</ul>
<p class="normal">Note that the user’s prompt<a id="_idIndexMarker127"/> in natural language is not the only input the model processes. In fact, it is a crucial component of the backend logic of our LLM-powered applications and the set of instructions we provide to the model. This <em class="italic">metaprompt</em> or system message<a id="_idIndexMarker128"/> is the object of a new discipline called <strong class="keyWord">prompt engineering</strong>.</p>
<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal">Prompt engineering is the process of designing and optimizing prompts to LLMs for a wide variety of applications and research topics. Prompts are short pieces of text that are used to guide the LLM’s output. Prompt engineering skills help to better understand the capabilities and limitations of LLMs.</p>
<p class="normal">Prompt engineering involves<a id="_idIndexMarker129"/> selecting the right words, phrases, symbols, and formats that elicit the desired response from the LLM. Prompt engineering also involves using other controls, such as parameters, examples, or data sources, to influence the LLM’s behavior. For example, if we want our LLM-powered application to generate responses for a 5-year-old child, we can specify this in a system message similar to “Act as a teacher who explains complex concepts to 5-year-old children.”</p>
</div>
<p class="normal">In fact, Andrej Karpathy, the previous<a id="_idIndexMarker130"/> Director of AI at Tesla, who returned to OpenAI in February 2023, tweeted that “English is the hottest new programming language.”</p>
<p class="normal">We will dive deeper into the concept of prompt engineering in <em class="chapterRef">Chapter 4</em>, <em class="italic">Prompt Engineering</em>. In the next section, we are going to focus on the emerging AI orchestrators.</p>
<h1 class="heading-1" id="_idParaDest-31">Introducing AI orchestrators to embed LLMs into applications</h1>
<p class="normal">Earlier in this chapter, we saw that there are two main aspects to consider when incorporating LLMs within applications: a technical aspect and a conceptual aspect. While we can explain the conceptual aspect with the brand-new category of software called Copilot, in this section, we are going<a id="_idIndexMarker131"/> to further explore how to technically embed and orchestrate LLMs within our applications.</p>
<h2 class="heading-2" id="_idParaDest-32">The main components of AI orchestrators</h2>
<p class="normal">From one side, the paradigm <a id="_idIndexMarker132"/>shift of foundation models implies a great simplification in the domain of AI-powered applications: after producing models, now the trend is consuming models. On the other side, many roadblocks might arise in developing this new kind of AI, since there are LLM-related components that are brand new and have never been managed before within an application life cycle. For example, there might be malicious actors that could try to change the LLM instructions (the system message mentioned earlier) so that the application does not follow the correct instructions. This is an example of a new set of security threats that are typical to LLM-powered applications and need to be addressed with powerful counterattacks or preventive techniques.</p>
<p class="normal">The following is an illustration of the main components of such applications:</p>
<figure class="mediaobject"><img alt="A diagram of a computer program  Description automatically generated" src="img/B21714_02_05.png"/></figure>
<p class="packt_figref">Figure 2.5: High-level architecture of LLM-powered applications</p>
<p class="normal">Let’s inspect each of these<a id="_idIndexMarker133"/> components in detail:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Models</strong>: The model is simply the type of LLM we decide to embed in our application. There are two main categories of models:<ul>
<li class="bulletList level-2"><strong class="keyWord">Proprietary LLMs: </strong>Models that are owned by specific companies or organizations. Examples include GPT-3 and GPT-4, developed by OpenAI, or Bard, developed by Google. As their source code and architecture are not available, those models cannot be re-trained from scratch on custom data, yet they can be fine-tuned if needed.</li>
<li class="bulletList level-2"><strong class="keyWord">Open-source: </strong>Models with code and architecture freely available and distributed, hence they can also be trained from scratch on custom data. Examples include Falcon LLM, developed by Abu Dhabi’s <strong class="keyWord">Technology Innovation Institute</strong> (<strong class="keyWord">TII</strong>), or LLaMA, developed by Meta.</li>
</ul>
</li>
</ul>
<p class="normal-one">We will dive deeper into the main set of LLMs available today in <em class="chapterRef">Chapter 3</em>, <em class="italic">Choosing an LLM for Your Application</em>.</p>
<ul>
<li class="bulletList"><strong class="keyWord">Memory</strong>: LLM applications commonly <a id="_idIndexMarker134"/>use a conversational interface, which requires the ability to refer back to earlier information within the conversation. This is achieved through a “memory” system that allows the application to store and retrieve past interactions. Note that past interactions could also constitute additional non-parametric knowledge to be added to the model. To achieve that, it is important to store all the past conversations – properly embedded – into VectorDB, which is at the core of the application’s data.</li>
</ul>
<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal">VectorDB is a type of database<a id="_idIndexMarker135"/> that stores and retrieves information based on vectorized embeddings, the numerical representations that capture the meaning and context of text. By using VectorDB, you can perform semantic search and retrieval based on the similarity of meanings rather than keywords. VectorDB can also help LLMs generate more relevant and coherent text by providing contextual understanding and enriching generation results. Some examples of VectorDBs are Chroma, Elasticsearch, Milvus, Pinecone, Qdrant, Weaviate, and <strong class="keyWord">Facebook AI Similarity Search</strong> (<strong class="keyWord">FAISS</strong>).</p>
<p class="normal">FAISS, developed by Facebook (now Meta) in 2017, was one of the pioneering vector databases. It was designed for efficient similarity search and clustering of dense vectors and is particularly useful for multimedia documents and dense embeddings. It was initially an internal research project at Facebook. Its primary goal was to better utilize GPUs for identifying similarities related to user preferences. Over time, it evolved into the fastest available library for similarity search and can handle billion-scale datasets. FAISS has opened up possibilities for recommendation engines and AI-based assistant systems.</p>
</div>
<ul>
<li class="bulletList"><strong class="keyWord">Plug-ins:</strong> They can be seen as additional modules or components that can be integrated into the LLM to extend its functionality or adapt it to specific tasks and applications. These plug-ins act as add-ons, enhancing the capabilities of the LLM beyond its core language generation or comprehension abilities.</li>
</ul>
<p class="normal-one">The idea behind plug-ins is to make LLMs more versatile and adaptable, allowing developers and users to customize the behavior of the language model for their specific needs. Plug-ins can be created to perform various tasks, and they can be seamlessly incorporated into the LLM’s architecture.</p>
<ul>
<li class="bulletList"><strong class="keyWord">Prompts</strong>: This is probably the most interesting and pivotal component of an LLM-powered application. We’ve already quoted, in the previous section, Andrej Karpathy’s affirmation that “English is the hottest new programming language,” and you will understand why in the upcoming chapters. Prompts can defined at two different levels:<ul>
<li class="bulletList level-2"><strong class="keyWord">“Frontend,” or what the user sees</strong>: A “prompt” refers to the input to the model. It is the way the user interacts with the application, asking things in natural language.</li>
<li class="bulletList level-2"><strong class="keyWord">“Backend,” or what the user does not see</strong>: Natural language is not only the way to interact, as a user, with the frontend; it is also the way we “program” the backend. In fact, on top of the user’s prompt, there are many natural language instructions, or meta-promts, that we give to the model so that it can properly address the user’s query. Meta-prompts are meant to instruct the model to act as it is meant to. For example, if we want to limit our application to answer only questions related to the documentation we provided in VectorDB, we will specify the following in our meta-prompts to the model: “<em class="italic">Answer only if the question is related to the provided documentation.</em>”</li>
</ul>
</li>
</ul>
<p class="normal">Finally, we get to the core of the high-level architecture<a id="_idIndexMarker136"/> shown in <em class="italic">Figure 2.5</em>, that is, the <strong class="keyWord">AI orchestrator</strong>. With the AI orchestrator, we refer to lightweight libraries that make it easier to embed and orchestrate LLMs within applications.</p>
<p class="normal">As LLMs went viral by the end of 2022, many libraries started arising in the market. In the next sections, we are going to focus on three of them: LangChain, Semantic Kernel, and Haystack.</p>
<h2 class="heading-2" id="_idParaDest-33">LangChain</h2>
<p class="normal">LangChain was launched<a id="_idIndexMarker137"/> as an open-source <a id="_idIndexMarker138"/>project by Harrison Chase in October 2022. It can be used both in Python and JS/TS. It is a framework for developing applications powered by language models, making them data-aware (with grounding) and agentic – which means they are able to interact with external environments.</p>
<p class="normal">Let’s take a look at the key components of LangChain:</p>
<figure class="mediaobject"><img alt="" role="presentation" src="img/B21714_02_06.png"/></figure>
<p class="packt_figref">Figure 2.6: LangChain’s components</p>
<p class="normal">Overall, LangChain<a id="_idIndexMarker139"/> has the following<a id="_idIndexMarker140"/> core modules:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Models</strong>: These are the LLMs or LFMs that will be the engine of the application. LangChain supports proprietary models, such as those available in OpenAI and Azure OpenAI, and open-source<a id="_idIndexMarker141"/> models consumable from the <strong class="keyWord">Hugging Face Hub</strong>.</li>
</ul>
<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal">Hugging Face is a company and a community<a id="_idIndexMarker142"/> that builds and shares state-of-the-art models and tools for natural language processing and other machine learning domains. It developed the Hugging Face Hub, a platform where people can create, discover, and collaborate on machine learning models and LLMs, datasets, and demos. The Hugging Face Hub hosts over 120k models, 20k datasets, and 50k demos in various domains and tasks, such as audio, vision, and language.</p>
</div>
<p class="normal-one">Alongside models, LangChain also offers many prompt-related components that make it easier to manage the prompt flow.</p>
<ul>
<li class="bulletList"><strong class="keyWord">Data connectors</strong>: These refer to the building blocks needed to retrieve the additional external knowledge (for example, in RAG-based scenarios) we want to provide the model with. Examples of data connectors are document loaders or text embedding models.</li>
<li class="bulletList"><strong class="keyWord">Memory</strong>: This allows the application to keep references to the user’s interactions, in both the short and long term. It is typically based on vectorized embeddings stored in VectorDB.</li>
<li class="bulletList"><strong class="keyWord">Chains</strong>: These are predetermined sequences of actions and calls to LLMs that make it easier to build complex applications that require chaining LLMs with each other or with other components. An example of a chain might be: take the user query, chunk it into smaller pieces, embed those chunks, search for similar embeddings in VectorDB, use the top three most similar chunks in VectorDB as context to provide the answer, and generate the answer.</li>
<li class="bulletList"><strong class="keyWord">Agents</strong>: Agents are entities that drive decision-making within LLM-powered applications. They have access to a suite of tools and can decide which tool to call based on the user input and the context. Agents are dynamic and adaptive, meaning that they<a id="_idIndexMarker143"/> can change or adjust their actions based on the situation or the goal.</li>
</ul>
<p class="normal">LangChain offers<a id="_idIndexMarker144"/> the following benefits:</p>
<ul>
<li class="bulletList">LangChain provides modular abstractions for the components we previously mentioned that are necessary to work with language models, such as prompts, memory, and plug-ins.</li>
<li class="bulletList">Alongside those components, LangChain also offers pre-built <strong class="keyWord">chains</strong>, which are structured concatenations of components. Those chains can be pre-built for specific use cases or be customized.</li>
</ul>
<p class="normal">In <em class="italic">Part 2</em> of this book, we will go through<a id="_idIndexMarker145"/> a series of hands-on applications, all LangChain based. So, starting from <em class="chapterRef">Chapter 5</em>, <em class="italic">Embedding LLMs within Your Applications</em>, we will focus much deeper on LangChain components and overall frameworks.</p>
<h2 class="heading-2" id="_idParaDest-34">Haystack</h2>
<p class="normal">Haystack is a Python-based<a id="_idIndexMarker146"/> framework developed<a id="_idIndexMarker147"/> by Deepset, a startup founded in 2018 in Berlin by Milos Rusic, Malte Pietsch, and Timo Möller. Deepset provides developers with the tools to build <strong class="keyWord">natural language processing</strong> (<strong class="keyWord">NLP</strong>)-based applications, and with the introduction of Haystack, they are taking them to the next level.</p>
<p class="normal">The following illustration shows the core components of Haystack:</p>
<figure class="mediaobject"><img alt="" role="presentation" src="img/B21714_02_07.png"/></figure>
<p class="packt_figref">Figure 2.7: Haystack’s components</p>
<p class="normal">Let’s look at these <a id="_idIndexMarker148"/>components<a id="_idIndexMarker149"/> in detail:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Nodes</strong>: These are components that perform a specific task or function, such as a retriever, a reader, a generator, a summarizer, etc. Nodes can be LLMs or other utilities that interact with LLMs or other resources. Among LLMs, Haystack supports proprietary models, such as those available in OpenAI and Azure OpenAI, and open-source models consumable from the Hugging Face Hub.</li>
<li class="bulletList"><strong class="keyWord">Pipelines</strong>: These are sequences of calls to nodes that perform natural language tasks or interact with other resources. Pipelines can be querying pipelines or indexing pipelines, depending on whether they perform searches on a set of documents or prepare documents for search. Pipelines are predetermined and hardcoded, meaning that they do not change or adapt based on the user input or the context.</li>
<li class="bulletList"><strong class="keyWord">Agent</strong>: This is an entity that uses LLMs to generate accurate responses to complex queries. An agent has access to a set of tools, which can be pipelines or nodes, and it can decide which tool to call based on the user input and the context. An agent is dynamic and adaptive, meaning that it can change or adjust its actions based on the situation or the goal.</li>
<li class="bulletList"><strong class="keyWord">Tools</strong>: There are functions that an agent can call to perform natural language tasks or interact with other resources. Tools can be pipelines or nodes that are available to the agent and they can be grouped into toolkits, which are sets of tools that can accomplish specific objectives.</li>
<li class="bulletList"><strong class="keyWord">DocumentStores</strong>: These are backends that store and retrieve documents for searches. DocumentStores can be based on different<a id="_idIndexMarker150"/> technologies, also including VectorDB (such as FAISS, Milvus, or Elasticsearch).</li>
</ul>
<p class="normal">Some of the benefits<a id="_idIndexMarker151"/> offered by Haystack are:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Ease of use</strong>: Haystack is user-friendly and straightforward. It’s often chosen for lighter tasks and rapid prototypes.</li>
<li class="bulletList"><strong class="keyWord">Documentation quality</strong>: Haystack’s documentation is considered high-quality, aiding developers in building search systems, question-answering, summarization, and conversational AI.</li>
<li class="bulletList"><strong class="keyWord">End-to-end framework</strong>: Haystack covers the entire LLM project life cycle, from data preprocessing to deployment. It’s ideal for large-scale search systems and information retrieval.</li>
<li class="bulletList">Another nice thing about Haystack is that you can deploy it as a REST API and it can be consumed<a id="_idIndexMarker152"/> directly.</li>
</ul>
<h2 class="heading-2" id="_idParaDest-35">Semantic Kernel</h2>
<p class="normal">Semantic Kernel is the third<a id="_idIndexMarker153"/> open-source<a id="_idIndexMarker154"/> SDK we are going to explore in this chapter. It was developed by Microsoft, originally in C# and now also available in Python.</p>
<p class="normal">This framework takes its name from the concept of a “kernel,” which, generally speaking, refers to the core or essence of a system. In the context of this framework, a kernel is meant to act as the engine that addresses a user’s input by chaining<a id="_idIndexMarker155"/> and concatenating a series of components into pipelines, encouraging <strong class="keyWord">function composition.</strong></p>
<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal">In mathematics, function composition<a id="_idIndexMarker156"/> is a way to combine two functions to create a new function. The idea is to use the output of one function as the input to another function, forming a chain of functions. The composition of two functions <em class="italic">f</em> and <em class="italic">g</em> is denoted as (<em class="italic">f <img alt="" role="presentation" src="img/circle.png"/> g</em>), where the function <em class="italic">g</em> is applied first, followed by the function <em class="italic">f </em><img alt="" role="presentation" src="img/arrow.png"/>(<em class="italic">f <img alt="" role="presentation" src="img/circle.png"/> g</em>)(<em class="italic">x</em>) = <em class="italic">f</em>(<em class="italic">g</em>(<em class="italic">x</em>)).</p>
<p class="normal">Function composition in computer science is a powerful concept that allows for the creation of more sophisticated and reusable code by combining smaller functions into larger ones. It enhances modularity and code organization, making programs easier to read and maintain.</p>
</div>
<p class="normal">The following is an illustration<a id="_idIndexMarker157"/> of the anatomy of Semantic Kernel:</p>
<figure class="mediaobject"><img alt="" role="presentation" src="img/B21714_02_08.png"/></figure>
<p class="packt_figref">Figure 2.8: Anatomy of Semantic Kernel</p>
<p class="normal">Semantic Kernel has the following<a id="_idIndexMarker158"/> main components:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Models</strong>: These are the LLMs or LFMs that will be the engine of the application. Semantic Kernel supports proprietary models, such as those available in OpenAI and Azure OpenAI, and open-source models consumable from the Hugging Face Hub.</li>
<li class="bulletList"><strong class="keyWord">Memory</strong>: It allows the application to keep references to the user’s interactions, both in the short and long term. Within the framework of Semantic Kernel, memories can be accessed in three ways:<ul>
<li class="bulletList level-2"><strong class="keyWord">Key</strong>-<strong class="keyWord">value pairs</strong>: This consists of saving environment variables that store simple information, such as names or dates.</li>
<li class="bulletList level-2"><strong class="keyWord">Local storage</strong>: This consists of saving information to a file that can be retrieved by its filename, such as a CSV or JSON file.</li>
<li class="bulletList level-2"><strong class="keyWord">Semantic memory search</strong>: This is similar to LangChain’s and Haystack’s memory, as it uses embeddings to represent and search for text information based on its meaning.</li>
</ul>
</li>
<li class="bulletList"><strong class="keyWord">Functions</strong>: Functions can be seen as skills that mix LLM prompts and code, with the goal of making users’ asks interpretable and actionable. There are two types of functions:<ul>
<li class="bulletList level-2"><strong class="keyWord">Semantic functions</strong>: These are a type of templated<a id="_idIndexMarker159"/> prompt, which is a natural language query that specifies the input and output format for the LLM, also incorporating prompt configuration, which sets the parameters for the LLM.</li>
<li class="bulletList level-2"><strong class="keyWord">Native functions</strong>: These refer to the native computer<a id="_idIndexMarker160"/> code that can route the intent captured by the semantic function and perform the related task.</li>
</ul>
</li>
</ul>
<p class="normal-one">To make an example, a semantic function<a id="_idIndexMarker161"/> could ask the LLM to write a short paragraph about AI, while a native function could actually post it on social media like LinkedIn.</p>
<ul>
<li class="bulletList"><strong class="keyWord">Plug-ins:</strong> These are connectors toward external sources or systems that are meant to provide additional information or the ability to perform autonomous actions. Semantic Kernel offers out-of-the-box plug-ins, such as the Microsoft Graph connector kit, but you can build a custom plug-in by leveraging functions (both native and semantic, or a mix of the two).</li>
<li class="bulletList"><strong class="keyWord">Planner</strong>: As LLMs can be seen as reasoning engines, they can also be leveraged to auto-create chains or pipelines to address new users’ needs. This goal is achieved with a planner, which is a function that takes as input a user’s task and produces<a id="_idIndexMarker162"/> the set of actions, plug-ins, and functions needed to achieve the goal.</li>
</ul>
<p class="normal">Some benefits of Semantic<a id="_idIndexMarker163"/> Kernel are:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Lightweight and C# support</strong>: Semantic Kernel is more lightweight and includes C# support. It’s a great choice for C# developers or those using the .NET framework.</li>
<li class="bulletList"><strong class="keyWord">Wide range of use cases</strong>: Semantic Kernel is versatile, supporting various LLM-related tasks.</li>
<li class="bulletList"><strong class="keyWord">Industry-led</strong>: Semantic Kernel was developed by Microsoft, and it is the framework the company used to build its own copilots. Hence, it is mainly driven by industry needs<a id="_idIndexMarker164"/> and asks, making it a solid tool for enterprise-scale applications.</li>
</ul>
<h2 class="heading-2" id="_idParaDest-36">How to choose a framework</h2>
<p class="normal">Overall, the three frameworks<a id="_idIndexMarker165"/> offer, more or less, similar core components, sometimes called by a different taxonomy, yet covering all the blocks illustrated within the concept of the copilot system. So, a natural question might be: “Which one should I use to build my LLM-powered application?” Well, there is no right or wrong answer! All three are extremely valid. However, there are some features that might be more relevant for specific use cases or developers’ preferences. The following are some criteria you might want to consider:</p>
<ul>
<li class="bulletList"><strong class="keyWord">The programming language you are comfortable with or prefer to use:</strong> Different frameworks may support different programming languages or have different levels of compatibility or integration with them. For example, Semantic Kernel supports C#, Python, and Java, while LangChain and Haystack are mainly based on Python (even though LangChain also introduced JS/TS support). You may want to choose a framework that matches your existing skills or preferences, or that allows you to use the language that is most suitable for your application domain or environment.</li>
<li class="bulletList"><strong class="keyWord">The type and complexity of the natural language tasks you want to perform or support:</strong> Different frameworks may have different capabilities or features for handling various natural language tasks, such as summarization, generation, translation, reasoning, etc. For example, LangChain and Haystack provide utilities and components for orchestrating and executing natural language tasks, while Semantic Kernel allows you to use natural language semantic functions to invoke LLMs and services. You may want to choose a framework that offers the functionality and flexibility you need or want for your application goals or scenarios.</li>
<li class="bulletList"><strong class="keyWord">The level of customization and control you need or want over the LLMs and their parameters or options</strong>: Different frameworks may have different ways of accessing, configuring, and fine-tuning the LLMs and their parameters or options, such as model selection, prompt design, inference speed, output format, etc. For example, Semantic Kernel provides connectors that make it easy to add memories and models to your AI app, while LangChain and Haystack allow you to plug in different components for the document store, retriever, reader, generator, summarizer, and evaluator. You may want to choose a framework that gives you the level of customization and control you need or want over the LLMs and their parameters or options.</li>
<li class="bulletList"><strong class="keyWord">The availability and quality of the documentation, tutorials, examples, and community support for the framework:</strong> Different frameworks may have different levels of documentation, tutorials, examples, and community support that can help you learn, use, and troubleshoot the framework. For example, Semantic Kernel has a website with documentation, tutorials, examples, and a Discord<a id="_idIndexMarker166"/> community; LangChain has a GitHub repository with documentation, examples, and issues; Haystack has a website with documentation, tutorials, demos, blog posts, and a Slack community. You may want to choose a framework that has the availability and quality of documentation, tutorials, examples, and community support that can help you get started and solve problems with the framework.</li>
</ul>
<p class="normal">Let’s briefly summarize the differences between these orchestrators:</p>
<table class="table-container" id="table001">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Feature</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">LangChain</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Haystack</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Semantic Kernel</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">LLM support</strong></p>
</td>
<td class="table-cell">
<p class="normal">Proprietary and open-source</p>
</td>
<td class="table-cell">
<p class="normal">Proprietary and open source</p>
</td>
<td class="table-cell">
<p class="normal">Proprietary and open source</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Supported languages</strong></p>
</td>
<td class="table-cell">
<p class="normal">Python and JS/TS</p>
</td>
<td class="table-cell">
<p class="normal">Python</p>
</td>
<td class="table-cell">
<p class="normal">C#, Java, and Python</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Process orchestration</strong></p>
</td>
<td class="table-cell">
<p class="normal">Chains</p>
</td>
<td class="table-cell">
<p class="normal">Pipelines of nodes</p>
</td>
<td class="table-cell">
<p class="normal">Pipelines of functions </p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Deployment</strong></p>
</td>
<td class="table-cell">
<p class="normal">No REST API</p>
</td>
<td class="table-cell">
<p class="normal">REST API</p>
</td>
<td class="table-cell">
<p class="normal">No REST API</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Feature</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">LangChain</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Haystack</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Semantic Kernel</strong></p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 2.1: Comparisons among the three AI orchestrators</p>
<p class="normal">Overall, all three frameworks offer a wide range of tools and integrations to build your LLM-powered applications, and a wise approach could be to use the one that is most in line with your current skills<a id="_idIndexMarker167"/> or the company’s overall approach.</p>
<h1 class="heading-1" id="_idParaDest-37">Summary</h1>
<p class="normal">In this chapter, we delved into the new way of developing applications that LLMs have been paving, as we introduced the concept of the copilot and discussed the emergence of new AI orchestrators. Among those, we focused on three projects – LangChain, Haystack, and Semantic Kernel – and we examined their features, main components, and some criteria to decide which one to pick.</p>
<p class="normal">Once we have decided on the AI orchestrator, another pivotal step is to decide which LLM(s) we want to embed into our applications. In <em class="chapterRef">Chapter 3</em>, <em class="italic">Choosing an LLM for Your Application</em>, we are going to see the most prominent LLMs on the market today – both proprietary and open-source – and understand some decision criteria to pick the proper models with respect to the application use cases.</p>
<h1 class="heading-1" id="_idParaDest-38">References</h1>
<ul>
<li class="bulletList">LangChain repository: <a href="https://github.com/langchain-ai/langchain">https://github.com/langchain-ai/langchain</a></li>
<li class="bulletList">Semantic Kernel documentation: <a href="https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages">https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages</a></li>
<li class="bulletList">Copilot stack: <a href="https://build.microsoft.com/en-US/sessions/bb8f9d99-0c47-404f-8212-a85fffd3a59d?source=/speakers/ef864919-5fd1-4215-b611-61035a19db6b">https://build.microsoft.com/en-US/sessions/bb8f9d99-0c47-404f-8212-a85fffd3a59d?source=/speakers/ef864919-5fd1-4215-b611-61035a19db6b</a></li>
<li class="bulletList">The Copilot system: <a href="https://www.youtube.com/watch?v=E5g20qmeKpg">https://www.youtube.com/watch?v=E5g20qmeKpg</a></li>
</ul>
<h1 class="heading-1">Join our community on Discord</h1>
<p class="normal">Join our community’s Discord space for discussions with the author and other readers:</p>
<p class="normal"><a href="https://packt.link/llm ">https://packt.link/llm</a></p>
<p class="normal"><img alt="" role="presentation" src="img/QR_Code214329708533108046.png"/></p>
</div>
</body></html>