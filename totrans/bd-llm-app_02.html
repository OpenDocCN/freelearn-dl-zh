<html><head></head><body>
<div class="Basic-Text-Frame" id="_idContainer063">
<h1 class="chapterNumber"><span class="koboSpan" id="kobo.1.1">2</span></h1>
<h1 class="chapterTitle" id="_idParaDest-28"><span class="koboSpan" id="kobo.2.1">LLMs for AI-Powered Applications</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.3.1">In </span><em class="chapterRef"><span class="koboSpan" id="kobo.4.1">Chapter 1</span></em><span class="koboSpan" id="kobo.5.1">, </span><em class="italic"><span class="koboSpan" id="kobo.6.1">Introduction to Large Language Models</span></em><span class="koboSpan" id="kobo.7.1">, we introduced </span><strong class="keyWord"><span class="koboSpan" id="kobo.8.1">large language models</span></strong><span class="koboSpan" id="kobo.9.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.10.1">LLM</span></strong><span class="koboSpan" id="kobo.11.1">s) as powerful foundation models with generative capabilities as well as powerful common-sense reasoning. </span><span class="koboSpan" id="kobo.11.2">Now, the next question is: what should I do with those models?</span></p>
<p class="normal"><span class="koboSpan" id="kobo.12.1">In this chapter, we are going to see how LLMs are revolutionizing the world of software development, leading to a new era of AI-powered applications. </span><span class="koboSpan" id="kobo.12.2">By the end of this chapter, you will have a clearer picture of how LLMs can be embedded in different application scenarios, thanks to the new AI orchestrator frameworks that are populating the market of AI development.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.13.1">In this chapter, we will cover the following topics:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.14.1">How LLMs are changing software development</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.15.1">The copilot system</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.16.1">Introducing AI orchestrators to embed LLMs into applications</span></li>
</ul>
<h1 class="heading-1" id="_idParaDest-29"><span class="koboSpan" id="kobo.17.1">How LLMs are changing software development</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.18.1">LLMs have proven to have extraordinary capabilities: from natural language understanding tasks (summarization, named entity recognition, and classification) to text generation, from common-sense reasoning to brainstorming skills. </span><span class="koboSpan" id="kobo.18.2">However, they are not just incredible by themselves. </span><span class="koboSpan" id="kobo.18.3">As discussed in </span><em class="chapterRef"><span class="koboSpan" id="kobo.19.1">Chapter 1</span></em><span class="koboSpan" id="kobo.20.1">, LLMs and, generally speaking, </span><strong class="keyWord"><span class="koboSpan" id="kobo.21.1">large foundation models</span></strong><span class="koboSpan" id="kobo.22.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.23.1">LFMs</span></strong><span class="koboSpan" id="kobo.24.1">), are revolutionizing software</span><a id="_idIndexMarker114"/><span class="koboSpan" id="kobo.25.1"> development by serving as platforms for building powerful applications.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.26.1">In fact, instead of starting from scratch, today developers can make API calls to a hosted version of an LLM, with the option of customizing it for their specific needs, as we saw in the previous chapter. </span><span class="koboSpan" id="kobo.26.2">This shift allows teams to incorporate the power of AI more easily and efficiently into their applications, similar to the transition from single-purpose computing to time-sharing in the past.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.27.1">But what does it mean, concretely, to incorporate LLMs within applications? </span><span class="koboSpan" id="kobo.27.2">There are two main aspects to consider when incorporating LLMs within applications:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.28.1">The technical aspect</span></strong><span class="koboSpan" id="kobo.29.1">, which covers the </span><em class="italic"><span class="koboSpan" id="kobo.30.1">how</span></em><span class="koboSpan" id="kobo.31.1">. </span><span class="koboSpan" id="kobo.31.2">Integrating</span><a id="_idIndexMarker115"/><span class="koboSpan" id="kobo.32.1"> LLMs into applications involves embedding them through REST API calls and managing them with AI orchestrators. </span><span class="koboSpan" id="kobo.32.2">This means setting up architectural components that allow seamless communication with the LLMs via API calls. </span><span class="koboSpan" id="kobo.32.3">Additionally, using AI orchestrators helps to efficiently manage and coordinate the LLMs’ functionality within the application, as we will discuss later in this chapter.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.33.1">The conceptual aspect</span></strong><span class="koboSpan" id="kobo.34.1">, which covers the </span><em class="italic"><span class="koboSpan" id="kobo.35.1">what</span></em><span class="koboSpan" id="kobo.36.1">. </span><span class="koboSpan" id="kobo.36.2">LLMs bring a plethora</span><a id="_idIndexMarker116"/><span class="koboSpan" id="kobo.37.1"> of new capabilities that can be harnessed within applications. </span><span class="koboSpan" id="kobo.37.2">These capabilities will be explored in detail later in this book. </span><span class="koboSpan" id="kobo.37.3">One way to view LLMs’ impact is by considering them as a new category of software, often referred to as </span><em class="italic"><span class="koboSpan" id="kobo.38.1">copilot</span></em><span class="koboSpan" id="kobo.39.1">. </span><span class="koboSpan" id="kobo.39.2">This categorization highlights the significant assistance and collaboration provided by LLMs in enhancing application functionalities.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.40.1">We will delve into the technical aspect later on in this chapter, while the next section will cover a brand-new category of software – the copilot system.</span></p>
<h1 class="heading-1" id="_idParaDest-30"><span class="koboSpan" id="kobo.41.1">The copilot system</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.42.1">The copilot system is a new category</span><a id="_idIndexMarker117"/><span class="koboSpan" id="kobo.43.1"> of software that serves as an expert helper to users trying to accomplish complex tasks. </span><span class="koboSpan" id="kobo.43.2">This concept was coined by Microsoft and has already been introduced into its applications, such as M365 Copilot and the new Bing, now powered by GPT-4. </span><span class="koboSpan" id="kobo.43.3">With the same framework that is used by these products, developers can now build their own copilots to embed within their applications.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.44.1">But what exactly is a copilot?</span></p>
<p class="normal"><span class="koboSpan" id="kobo.45.1">As the name suggests, copilots</span><a id="_idIndexMarker118"/><span class="koboSpan" id="kobo.46.1"> are meant to be AI assistants that work side by side with users and support them in various activities, from information retrieval to blog writing and posting, from brainstorming ideas to code review and generation.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.47.1">The following are some unique features</span><a id="_idIndexMarker119"/><span class="koboSpan" id="kobo.48.1"> of copilots:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.49.1">A copilot is powered by LLMs</span></strong><span class="koboSpan" id="kobo.50.1">, or, more generally, LFMs, meaning that these are the reasoning engines that make the copilot “intelligent.” </span><span class="koboSpan" id="kobo.50.2">This reasoning engine is one of its components, but not the only one. </span><span class="koboSpan" id="kobo.50.3">A copilot also relies on other technologies, such as apps, data sources, and user interfaces, to provide a useful and engaging experience for users. </span><span class="koboSpan" id="kobo.50.4">The following illustration shows how this works:</span></li>
</ul>
<figure class="mediaobject"><span class="koboSpan" id="kobo.51.1"><img alt="A cartoon of a person  Description automatically generated" src="../Images/B21714_02_01.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.52.1">Figure 2.1: A copilot is powered by an LLM</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.53.1">A copilot is designed to have a conversational user interface,</span></strong><span class="koboSpan" id="kobo.54.1"> allowing users to interact</span><a id="_idIndexMarker120"/><span class="koboSpan" id="kobo.55.1"> with it using natural language. </span><span class="koboSpan" id="kobo.55.2">This reduces or even eliminates the knowledge gap between complex systems that need domain-specific taxonomy (for example, querying tabular data needs the knowledge of programming languages such as T-SQL) and users. </span><span class="koboSpan" id="kobo.55.3">Let’s look at an example of such a conversation:</span></li>
</ul>
<figure class="mediaobject"><span class="koboSpan" id="kobo.56.1"><img alt="" role="presentation" src="../Images/B21714_02_02.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.57.1">Figure 2.2: An example of a conversational UI to reduce the gap between the user and the database</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.58.1">A copilot has a scope.</span></strong><span class="koboSpan" id="kobo.59.1"> This means that it is </span><strong class="keyWord"><span class="koboSpan" id="kobo.60.1">grounded</span></strong><span class="koboSpan" id="kobo.61.1"> to domain-specific data so that it is entitled to answer only within the perimeter of the application or domain.</span></li>
</ul>
<div class="note">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.62.1">Definition</span></strong></p>
<p class="normal"><span class="koboSpan" id="kobo.63.1">Grounding is the process of using LLMs</span><a id="_idIndexMarker121"/><span class="koboSpan" id="kobo.64.1"> with information that is use case specific, relevant, and not available as part of the LLM’s trained knowledge. </span><span class="koboSpan" id="kobo.64.2">It is crucial for ensuring the quality, accuracy, and relevance of the output. </span><span class="koboSpan" id="kobo.64.3">For example, let’s say you want an LLM-powered application that assists you during your research on up-to-date papers (not included in the training dataset of your LLM). </span><span class="koboSpan" id="kobo.64.4">You also want your app to only respond if the answer is included in those papers. </span><span class="koboSpan" id="kobo.64.5">To do so, you will need to ground your LLM to the set of papers, so that your application will only respond within this perimeter.</span></p>
</div>
<p class="normal-one"><span class="koboSpan" id="kobo.65.1">Grounding is achieved through</span><a id="_idIndexMarker122"/><span class="koboSpan" id="kobo.66.1"> an architectura</span><a id="_idIndexMarker123"/><span class="koboSpan" id="kobo.67.1">l framework called retrieval-augmented generation (RAG), a technique that enhances the output of LLMs by incorporating information from an external, authoritative knowledge base before generating a response. </span><span class="koboSpan" id="kobo.67.2">This process helps to ensure that the generated content is relevant, accurate, and up to date.</span></p>
<div class="note">
<p class="normal"><span class="koboSpan" id="kobo.68.1">What is the difference between</span><a id="_idIndexMarker124"/><span class="koboSpan" id="kobo.69.1"> a copilot and a RAG? </span><span class="koboSpan" id="kobo.69.2">RAG</span><a id="_idIndexMarker125"/><span class="koboSpan" id="kobo.70.1"> can be seen as one of the architectural patterns that feature a copilot. </span><span class="koboSpan" id="kobo.70.2">Whenever we want our copilot to be grounded to domain-specific data, we use a RAG framework. </span><span class="koboSpan" id="kobo.70.3">Note that RAG is not the only architectural pattern that can feature a copilot: there are further frameworks such as function calling or multi-agents that we will explore throughout the book.</span></p>
</div>
<p class="normal-one"><span class="koboSpan" id="kobo.71.1">For example, let’s say we developed a copilot within our company that allows employees to chat with their enterprise knowledge base. </span><span class="koboSpan" id="kobo.71.2">As fun as it can be, we cannot provide users with a copilot they can use to plan their summer trip (it would be like providing users with a ChatGPT-like tool at our own hosting cost!); on the contrary, we want the copilot to be grounded only to our enterprise knowledge base so that it can respond only if the answer is pertinent to the domain-specific context.</span></p>
<p class="normal-one"><span class="koboSpan" id="kobo.72.1">The following figure shows an example of grounding a copilot system:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.73.1"><img alt="" role="presentation" src="../Images/B21714_02_03.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.74.1">Figure 2.3: Example of grounding a copilot</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.75.1">The copilot’s capabilities can be extended by skills</span></strong><span class="koboSpan" id="kobo.76.1">, which can be code or calls to other </span><a id="_idIndexMarker126"/><span class="koboSpan" id="kobo.77.1">models. </span><span class="koboSpan" id="kobo.77.2">In fact, the LLM (our reasoning engine) might have two kinds of limitations:</span><ul>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.78.1">Limited parametric knowledge.</span></strong><span class="koboSpan" id="kobo.79.1"> This is due to the knowledge base cutoff date, which is a physiological feature of LLMs. </span><span class="koboSpan" id="kobo.79.2">In fact, their training dataset will always be “outdated,” not in line with the current trends. </span><span class="koboSpan" id="kobo.79.3">This can be overcome by adding non-parametric knowledge with grounding, as previously seen.</span></li>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.80.1">Lack of executive power. </span></strong><span class="koboSpan" id="kobo.81.1">This means that LLMs by themselves are not empowered to carry out actions. </span><span class="koboSpan" id="kobo.81.2">Let’s consider, for example, the well-known ChatGPT: if we ask it to generate a LinkedIn post about productivity tips, we will then need to copy and paste it onto our LinkedIn profile as ChatGPT is not able to do so by itself. </span><span class="koboSpan" id="kobo.81.3">That is the reason why we need plug-ins. </span><span class="koboSpan" id="kobo.81.4">Plug-ins are LLMs’ connectors toward the external world that serve not only as input sources to extend LLMs’ non-parametric knowledge (for example, to allow a web search) but also as output sources so that the copilot can actually execute actions. </span><span class="koboSpan" id="kobo.81.5">For example, with a LinkedIn plug-in, our copilot powered by an LLM will be able not only to generate the post but also to post it online.</span></li>
</ul>
<figure class="mediaobject"><span class="koboSpan" id="kobo.82.1"><img alt="A cartoon of a person pointing at a plug-in  Description automatically generated" src="../Images/B21714_02_04.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.83.1">Figure 2.4: Example of Wikipedia and LinkedIn plug-ins</span></p>
</li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.84.1">Note that the user’s prompt</span><a id="_idIndexMarker127"/><span class="koboSpan" id="kobo.85.1"> in natural language is not the only input the model processes. </span><span class="koboSpan" id="kobo.85.2">In fact, it is a crucial component of the backend logic of our LLM-powered applications and the set of instructions we provide to the model. </span><span class="koboSpan" id="kobo.85.3">This </span><em class="italic"><span class="koboSpan" id="kobo.86.1">metaprompt</span></em><span class="koboSpan" id="kobo.87.1"> or system message</span><a id="_idIndexMarker128"/><span class="koboSpan" id="kobo.88.1"> is the object of a new discipline called </span><strong class="keyWord"><span class="koboSpan" id="kobo.89.1">prompt engineering</span></strong><span class="koboSpan" id="kobo.90.1">.</span></p>
<div class="note">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.91.1">Definition</span></strong></p>
<p class="normal"><span class="koboSpan" id="kobo.92.1">Prompt engineering is the process of designing and optimizing prompts to LLMs for a wide variety of applications and research topics. </span><span class="koboSpan" id="kobo.92.2">Prompts are short pieces of text that are used to guide the LLM’s output. </span><span class="koboSpan" id="kobo.92.3">Prompt engineering skills help to better understand the capabilities and limitations of LLMs.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.93.1">Prompt engineering involves</span><a id="_idIndexMarker129"/><span class="koboSpan" id="kobo.94.1"> selecting the right words, phrases, symbols, and formats that elicit the desired response from the LLM. </span><span class="koboSpan" id="kobo.94.2">Prompt engineering also involves using other controls, such as parameters, examples, or data sources, to influence the LLM’s behavior. </span><span class="koboSpan" id="kobo.94.3">For example, if we want our LLM-powered application to generate responses for a 5-year-old child, we can specify this in a system message similar to “Act as a teacher who explains complex concepts to 5-year-old children.”</span></p>
</div>
<p class="normal"><span class="koboSpan" id="kobo.95.1">In fact, Andrej Karpathy, the previous</span><a id="_idIndexMarker130"/><span class="koboSpan" id="kobo.96.1"> Director of AI at Tesla, who returned to OpenAI in February 2023, tweeted that “English is the hottest new programming language.”</span></p>
<p class="normal"><span class="koboSpan" id="kobo.97.1">We will dive deeper into the concept of prompt engineering in </span><em class="chapterRef"><span class="koboSpan" id="kobo.98.1">Chapter 4</span></em><span class="koboSpan" id="kobo.99.1">, </span><em class="italic"><span class="koboSpan" id="kobo.100.1">Prompt Engineering</span></em><span class="koboSpan" id="kobo.101.1">. </span><span class="koboSpan" id="kobo.101.2">In the next section, we are going to focus on the emerging AI orchestrators.</span></p>
<h1 class="heading-1" id="_idParaDest-31"><span class="koboSpan" id="kobo.102.1">Introducing AI orchestrators to embed LLMs into applications</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.103.1">Earlier in this chapter, we saw that there are two main aspects to consider when incorporating LLMs within applications: a technical aspect and a conceptual aspect. </span><span class="koboSpan" id="kobo.103.2">While we can explain the conceptual aspect with the brand-new category of software called Copilot, in this section, we are going</span><a id="_idIndexMarker131"/><span class="koboSpan" id="kobo.104.1"> to further explore how to technically embed and orchestrate LLMs within our applications.</span></p>
<h2 class="heading-2" id="_idParaDest-32"><span class="koboSpan" id="kobo.105.1">The main components of AI orchestrators</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.106.1">From one side, the paradigm </span><a id="_idIndexMarker132"/><span class="koboSpan" id="kobo.107.1">shift of foundation models implies a great simplification in the domain of AI-powered applications: after producing models, now the trend is consuming models. </span><span class="koboSpan" id="kobo.107.2">On the other side, many roadblocks might arise in developing this new kind of AI, since there are LLM-related components that are brand new and have never been managed before within an application life cycle. </span><span class="koboSpan" id="kobo.107.3">For example, there might be malicious actors that could try to change the LLM instructions (the system message mentioned earlier) so that the application does not follow the correct instructions. </span><span class="koboSpan" id="kobo.107.4">This is an example of a new set of security threats that are typical to LLM-powered applications and need to be addressed with powerful counterattacks or preventive techniques.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.108.1">The following is an illustration of the main components of such applications:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.109.1"><img alt="A diagram of a computer program  Description automatically generated" src="../Images/B21714_02_05.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.110.1">Figure 2.5: High-level architecture of LLM-powered applications</span></p>
<p class="normal"><span class="koboSpan" id="kobo.111.1">Let’s inspect each of these</span><a id="_idIndexMarker133"/><span class="koboSpan" id="kobo.112.1"> components in detail:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.113.1">Models</span></strong><span class="koboSpan" id="kobo.114.1">: The model is simply the type of LLM we decide to embed in our application. </span><span class="koboSpan" id="kobo.114.2">There are two main categories of models:</span><ul>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.115.1">Proprietary LLMs: </span></strong><span class="koboSpan" id="kobo.116.1">Models that are owned by specific companies or organizations. </span><span class="koboSpan" id="kobo.116.2">Examples include GPT-3 and GPT-4, developed by OpenAI, or Bard, developed by Google. </span><span class="koboSpan" id="kobo.116.3">As their source code and architecture are not available, those models cannot be re-trained from scratch on custom data, yet they can be fine-tuned if needed.</span></li>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.117.1">Open-source: </span></strong><span class="koboSpan" id="kobo.118.1">Models with code and architecture freely available and distributed, hence they can also be trained from scratch on custom data. </span><span class="koboSpan" id="kobo.118.2">Examples include Falcon LLM, developed by Abu Dhabi’s </span><strong class="keyWord"><span class="koboSpan" id="kobo.119.1">Technology Innovation Institute</span></strong><span class="koboSpan" id="kobo.120.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.121.1">TII</span></strong><span class="koboSpan" id="kobo.122.1">), or LLaMA, developed by Meta.</span></li>
</ul>
</li>
</ul>
<p class="normal-one"><span class="koboSpan" id="kobo.123.1">We will dive deeper into the main set of LLMs available today in </span><em class="chapterRef"><span class="koboSpan" id="kobo.124.1">Chapter 3</span></em><span class="koboSpan" id="kobo.125.1">, </span><em class="italic"><span class="koboSpan" id="kobo.126.1">Choosing an LLM for Your Application</span></em><span class="koboSpan" id="kobo.127.1">.</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.128.1">Memory</span></strong><span class="koboSpan" id="kobo.129.1">: LLM applications commonly </span><a id="_idIndexMarker134"/><span class="koboSpan" id="kobo.130.1">use a conversational interface, which requires the ability to refer back to earlier information within the conversation. </span><span class="koboSpan" id="kobo.130.2">This is achieved through a “memory” system that allows the application to store and retrieve past interactions. </span><span class="koboSpan" id="kobo.130.3">Note that past interactions could also constitute additional non-parametric knowledge to be added to the model. </span><span class="koboSpan" id="kobo.130.4">To achieve that, it is important to store all the past conversations – properly embedded – into VectorDB, which is at the core of the application’s data.</span></li>
</ul>
<div class="note-one">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.131.1">Definition</span></strong></p>
<p class="normal"><span class="koboSpan" id="kobo.132.1">VectorDB is a type of database</span><a id="_idIndexMarker135"/><span class="koboSpan" id="kobo.133.1"> that stores and retrieves information based on vectorized embeddings, the numerical representations that capture the meaning and context of text. </span><span class="koboSpan" id="kobo.133.2">By using VectorDB, you can perform semantic search and retrieval based on the similarity of meanings rather than keywords. </span><span class="koboSpan" id="kobo.133.3">VectorDB can also help LLMs generate more relevant and coherent text by providing contextual understanding and enriching generation results. </span><span class="koboSpan" id="kobo.133.4">Some examples of VectorDBs are Chroma, Elasticsearch, Milvus, Pinecone, Qdrant, Weaviate, and </span><strong class="keyWord"><span class="koboSpan" id="kobo.134.1">Facebook AI Similarity Search</span></strong><span class="koboSpan" id="kobo.135.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.136.1">FAISS</span></strong><span class="koboSpan" id="kobo.137.1">).</span></p>
<p class="normal"><span class="koboSpan" id="kobo.138.1">FAISS, developed by Facebook (now Meta) in 2017, was one of the pioneering vector databases. </span><span class="koboSpan" id="kobo.138.2">It was designed for efficient similarity search and clustering of dense vectors and is particularly useful for multimedia documents and dense embeddings. </span><span class="koboSpan" id="kobo.138.3">It was initially an internal research project at Facebook. </span><span class="koboSpan" id="kobo.138.4">Its primary goal was to better utilize GPUs for identifying similarities related to user preferences. </span><span class="koboSpan" id="kobo.138.5">Over time, it evolved into the fastest available library for similarity search and can handle billion-scale datasets. </span><span class="koboSpan" id="kobo.138.6">FAISS has opened up possibilities for recommendation engines and AI-based assistant systems.</span></p>
</div>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.139.1">Plug-ins:</span></strong><span class="koboSpan" id="kobo.140.1"> They can be seen as additional modules or components that can be integrated into the LLM to extend its functionality or adapt it to specific tasks and applications. </span><span class="koboSpan" id="kobo.140.2">These plug-ins act as add-ons, enhancing the capabilities of the LLM beyond its core language generation or comprehension abilities.</span></li>
</ul>
<p class="normal-one"><span class="koboSpan" id="kobo.141.1">The idea behind plug-ins is to make LLMs more versatile and adaptable, allowing developers and users to customize the behavior of the language model for their specific needs. </span><span class="koboSpan" id="kobo.141.2">Plug-ins can be created to perform various tasks, and they can be seamlessly incorporated into the LLM’s architecture.</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.142.1">Prompts</span></strong><span class="koboSpan" id="kobo.143.1">: This is probably the most interesting and pivotal component of an LLM-powered application. </span><span class="koboSpan" id="kobo.143.2">We’ve already quoted, in the previous section, Andrej Karpathy’s affirmation that “English is the hottest new programming language,” and you will understand why in the upcoming chapters. </span><span class="koboSpan" id="kobo.143.3">Prompts can defined at two different levels:</span><ul>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.144.1">“Frontend,” or what the user sees</span></strong><span class="koboSpan" id="kobo.145.1">: A “prompt” refers to the input to the model. </span><span class="koboSpan" id="kobo.145.2">It is the way the user interacts with the application, asking things in natural language.</span></li>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.146.1">“Backend,” or what the user does not see</span></strong><span class="koboSpan" id="kobo.147.1">: Natural language is not only the way to interact, as a user, with the frontend; it is also the way we “program” the backend. </span><span class="koboSpan" id="kobo.147.2">In fact, on top of the user’s prompt, there are many natural language instructions, or meta-promts, that we give to the model so that it can properly address the user’s query. </span><span class="koboSpan" id="kobo.147.3">Meta-prompts are meant to instruct the model to act as it is meant to. </span><span class="koboSpan" id="kobo.147.4">For example, if we want to limit our application to answer only questions related to the documentation we provided in VectorDB, we will specify the following in our meta-prompts to the model: “</span><em class="italic"><span class="koboSpan" id="kobo.148.1">Answer only if the question is related to the provided documentation.</span></em><span class="koboSpan" id="kobo.149.1">”</span></li>
</ul>
</li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.150.1">Finally, we get to the core of the high-level architecture</span><a id="_idIndexMarker136"/><span class="koboSpan" id="kobo.151.1"> shown in </span><em class="italic"><span class="koboSpan" id="kobo.152.1">Figure 2.5</span></em><span class="koboSpan" id="kobo.153.1">, that is, the </span><strong class="keyWord"><span class="koboSpan" id="kobo.154.1">AI orchestrator</span></strong><span class="koboSpan" id="kobo.155.1">. </span><span class="koboSpan" id="kobo.155.2">With the AI orchestrator, we refer to lightweight libraries that make it easier to embed and orchestrate LLMs within applications.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.156.1">As LLMs went viral by the end of 2022, many libraries started arising in the market. </span><span class="koboSpan" id="kobo.156.2">In the next sections, we are going to focus on three of them: LangChain, Semantic Kernel, and Haystack.</span></p>
<h2 class="heading-2" id="_idParaDest-33"><span class="koboSpan" id="kobo.157.1">LangChain</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.158.1">LangChain was launched</span><a id="_idIndexMarker137"/><span class="koboSpan" id="kobo.159.1"> as an open-source </span><a id="_idIndexMarker138"/><span class="koboSpan" id="kobo.160.1">project by Harrison Chase in October 2022. </span><span class="koboSpan" id="kobo.160.2">It can be used both in Python and JS/TS. </span><span class="koboSpan" id="kobo.160.3">It is a framework for developing applications powered by language models, making them data-aware (with grounding) and agentic – which means they are able to interact with external environments.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.161.1">Let’s take a look at the key components of LangChain:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.162.1"><img alt="" role="presentation" src="../Images/B21714_02_06.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.163.1">Figure 2.6: LangChain’s components</span></p>
<p class="normal"><span class="koboSpan" id="kobo.164.1">Overall, LangChain</span><a id="_idIndexMarker139"/><span class="koboSpan" id="kobo.165.1"> has the following</span><a id="_idIndexMarker140"/><span class="koboSpan" id="kobo.166.1"> core modules:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.167.1">Models</span></strong><span class="koboSpan" id="kobo.168.1">: These are the LLMs or LFMs that will be the engine of the application. </span><span class="koboSpan" id="kobo.168.2">LangChain supports proprietary models, such as those available in OpenAI and Azure OpenAI, and open-source</span><a id="_idIndexMarker141"/><span class="koboSpan" id="kobo.169.1"> models consumable from the </span><strong class="keyWord"><span class="koboSpan" id="kobo.170.1">Hugging Face Hub</span></strong><span class="koboSpan" id="kobo.171.1">.</span></li>
</ul>
<div class="note-one">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.172.1">Definition</span></strong></p>
<p class="normal"><span class="koboSpan" id="kobo.173.1">Hugging Face is a company and a community</span><a id="_idIndexMarker142"/><span class="koboSpan" id="kobo.174.1"> that builds and shares state-of-the-art models and tools for natural language processing and other machine learning domains. </span><span class="koboSpan" id="kobo.174.2">It developed the Hugging Face Hub, a platform where people can create, discover, and collaborate on machine learning models and LLMs, datasets, and demos. </span><span class="koboSpan" id="kobo.174.3">The Hugging Face Hub hosts over 120k models, 20k datasets, and 50k demos in various domains and tasks, such as audio, vision, and language.</span></p>
</div>
<p class="normal-one"><span class="koboSpan" id="kobo.175.1">Alongside models, LangChain also offers many prompt-related components that make it easier to manage the prompt flow.</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.176.1">Data connectors</span></strong><span class="koboSpan" id="kobo.177.1">: These refer to the building blocks needed to retrieve the additional external knowledge (for example, in RAG-based scenarios) we want to provide the model with. </span><span class="koboSpan" id="kobo.177.2">Examples of data connectors are document loaders or text embedding models.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.178.1">Memory</span></strong><span class="koboSpan" id="kobo.179.1">: This allows the application to keep references to the user’s interactions, in both the short and long term. </span><span class="koboSpan" id="kobo.179.2">It is typically based on vectorized embeddings stored in VectorDB.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.180.1">Chains</span></strong><span class="koboSpan" id="kobo.181.1">: These are predetermined sequences of actions and calls to LLMs that make it easier to build complex applications that require chaining LLMs with each other or with other components. </span><span class="koboSpan" id="kobo.181.2">An example of a chain might be: take the user query, chunk it into smaller pieces, embed those chunks, search for similar embeddings in VectorDB, use the top three most similar chunks in VectorDB as context to provide the answer, and generate the answer.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.182.1">Agents</span></strong><span class="koboSpan" id="kobo.183.1">: Agents are entities that drive decision-making within LLM-powered applications. </span><span class="koboSpan" id="kobo.183.2">They have access to a suite of tools and can decide which tool to call based on the user input and the context. </span><span class="koboSpan" id="kobo.183.3">Agents are dynamic and adaptive, meaning that they</span><a id="_idIndexMarker143"/><span class="koboSpan" id="kobo.184.1"> can change or adjust their actions based on the situation or the goal.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.185.1">LangChain offers</span><a id="_idIndexMarker144"/><span class="koboSpan" id="kobo.186.1"> the following benefits:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.187.1">LangChain provides modular abstractions for the components we previously mentioned that are necessary to work with language models, such as prompts, memory, and plug-ins.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.188.1">Alongside those components, LangChain also offers pre-built </span><strong class="keyWord"><span class="koboSpan" id="kobo.189.1">chains</span></strong><span class="koboSpan" id="kobo.190.1">, which are structured concatenations of components. </span><span class="koboSpan" id="kobo.190.2">Those chains can be pre-built for specific use cases or be customized.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.191.1">In </span><em class="italic"><span class="koboSpan" id="kobo.192.1">Part 2</span></em><span class="koboSpan" id="kobo.193.1"> of this book, we will go through</span><a id="_idIndexMarker145"/><span class="koboSpan" id="kobo.194.1"> a series of hands-on applications, all LangChain based. </span><span class="koboSpan" id="kobo.194.2">So, starting from </span><em class="chapterRef"><span class="koboSpan" id="kobo.195.1">Chapter 5</span></em><span class="koboSpan" id="kobo.196.1">, </span><em class="italic"><span class="koboSpan" id="kobo.197.1">Embedding LLMs within Your Applications</span></em><span class="koboSpan" id="kobo.198.1">, we will focus much deeper on LangChain components and overall frameworks.</span></p>
<h2 class="heading-2" id="_idParaDest-34"><span class="koboSpan" id="kobo.199.1">Haystack</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.200.1">Haystack is a Python-based</span><a id="_idIndexMarker146"/><span class="koboSpan" id="kobo.201.1"> framework developed</span><a id="_idIndexMarker147"/><span class="koboSpan" id="kobo.202.1"> by Deepset, a startup founded in 2018 in Berlin by Milos Rusic, Malte Pietsch, and Timo Möller. </span><span class="koboSpan" id="kobo.202.2">Deepset provides developers with the tools to build </span><strong class="keyWord"><span class="koboSpan" id="kobo.203.1">natural language processing</span></strong><span class="koboSpan" id="kobo.204.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.205.1">NLP</span></strong><span class="koboSpan" id="kobo.206.1">)-based applications, and with the introduction of Haystack, they are taking them to the next level.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.207.1">The following illustration shows the core components of Haystack:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.208.1"><img alt="" role="presentation" src="../Images/B21714_02_07.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.209.1">Figure 2.7: Haystack’s components</span></p>
<p class="normal"><span class="koboSpan" id="kobo.210.1">Let’s look at these </span><a id="_idIndexMarker148"/><span class="koboSpan" id="kobo.211.1">components</span><a id="_idIndexMarker149"/><span class="koboSpan" id="kobo.212.1"> in detail:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.213.1">Nodes</span></strong><span class="koboSpan" id="kobo.214.1">: These are components that perform a specific task or function, such as a retriever, a reader, a generator, a summarizer, etc. </span><span class="koboSpan" id="kobo.214.2">Nodes can be LLMs or other utilities that interact with LLMs or other resources. </span><span class="koboSpan" id="kobo.214.3">Among LLMs, Haystack supports proprietary models, such as those available in OpenAI and Azure OpenAI, and open-source models consumable from the Hugging Face Hub.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.215.1">Pipelines</span></strong><span class="koboSpan" id="kobo.216.1">: These are sequences of calls to nodes that perform natural language tasks or interact with other resources. </span><span class="koboSpan" id="kobo.216.2">Pipelines can be querying pipelines or indexing pipelines, depending on whether they perform searches on a set of documents or prepare documents for search. </span><span class="koboSpan" id="kobo.216.3">Pipelines are predetermined and hardcoded, meaning that they do not change or adapt based on the user input or the context.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.217.1">Agent</span></strong><span class="koboSpan" id="kobo.218.1">: This is an entity that uses LLMs to generate accurate responses to complex queries. </span><span class="koboSpan" id="kobo.218.2">An agent has access to a set of tools, which can be pipelines or nodes, and it can decide which tool to call based on the user input and the context. </span><span class="koboSpan" id="kobo.218.3">An agent is dynamic and adaptive, meaning that it can change or adjust its actions based on the situation or the goal.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.219.1">Tools</span></strong><span class="koboSpan" id="kobo.220.1">: There are functions that an agent can call to perform natural language tasks or interact with other resources. </span><span class="koboSpan" id="kobo.220.2">Tools can be pipelines or nodes that are available to the agent and they can be grouped into toolkits, which are sets of tools that can accomplish specific objectives.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.221.1">DocumentStores</span></strong><span class="koboSpan" id="kobo.222.1">: These are backends that store and retrieve documents for searches. </span><span class="koboSpan" id="kobo.222.2">DocumentStores can be based on different</span><a id="_idIndexMarker150"/><span class="koboSpan" id="kobo.223.1"> technologies, also including VectorDB (such as FAISS, Milvus, or Elasticsearch).</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.224.1">Some of the benefits</span><a id="_idIndexMarker151"/><span class="koboSpan" id="kobo.225.1"> offered by Haystack are:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.226.1">Ease of use</span></strong><span class="koboSpan" id="kobo.227.1">: Haystack is user-friendly and straightforward. </span><span class="koboSpan" id="kobo.227.2">It’s often chosen for lighter tasks and rapid prototypes.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.228.1">Documentation quality</span></strong><span class="koboSpan" id="kobo.229.1">: Haystack’s documentation is considered high-quality, aiding developers in building search systems, question-answering, summarization, and conversational AI.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.230.1">End-to-end framework</span></strong><span class="koboSpan" id="kobo.231.1">: Haystack covers the entire LLM project life cycle, from data preprocessing to deployment. </span><span class="koboSpan" id="kobo.231.2">It’s ideal for large-scale search systems and information retrieval.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.232.1">Another nice thing about Haystack is that you can deploy it as a REST API and it can be consumed</span><a id="_idIndexMarker152"/><span class="koboSpan" id="kobo.233.1"> directly.</span></li>
</ul>
<h2 class="heading-2" id="_idParaDest-35"><span class="koboSpan" id="kobo.234.1">Semantic Kernel</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.235.1">Semantic Kernel is the third</span><a id="_idIndexMarker153"/><span class="koboSpan" id="kobo.236.1"> open-source</span><a id="_idIndexMarker154"/><span class="koboSpan" id="kobo.237.1"> SDK we are going to explore in this chapter. </span><span class="koboSpan" id="kobo.237.2">It was developed by Microsoft, originally in C# and now also available in Python.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.238.1">This framework takes its name from the concept of a “kernel,” which, generally speaking, refers to the core or essence of a system. </span><span class="koboSpan" id="kobo.238.2">In the context of this framework, a kernel is meant to act as the engine that addresses a user’s input by chaining</span><a id="_idIndexMarker155"/><span class="koboSpan" id="kobo.239.1"> and concatenating a series of components into pipelines, encouraging </span><strong class="keyWord"><span class="koboSpan" id="kobo.240.1">function composition.</span></strong></p>
<div class="note">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.241.1">Definition</span></strong></p>
<p class="normal"><span class="koboSpan" id="kobo.242.1">In mathematics, function composition</span><a id="_idIndexMarker156"/><span class="koboSpan" id="kobo.243.1"> is a way to combine two functions to create a new function. </span><span class="koboSpan" id="kobo.243.2">The idea is to use the output of one function as the input to another function, forming a chain of functions. </span><span class="koboSpan" id="kobo.243.3">The composition of two functions </span><em class="italic"><span class="koboSpan" id="kobo.244.1">f</span></em><span class="koboSpan" id="kobo.245.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.246.1">g</span></em><span class="koboSpan" id="kobo.247.1"> is denoted as (</span><em class="italic"><span class="koboSpan" id="kobo.248.1">f </span><span class="koboSpan" id="kobo.249.1"><img alt="" role="presentation" src="../Images/circle.png"/></span><span class="koboSpan" id="kobo.250.1"> g</span></em><span class="koboSpan" id="kobo.251.1">), where the function </span><em class="italic"><span class="koboSpan" id="kobo.252.1">g</span></em><span class="koboSpan" id="kobo.253.1"> is applied first, followed by the function </span><em class="italic"><span class="koboSpan" id="kobo.254.1">f </span></em><span class="koboSpan" id="kobo.255.1"><img alt="" role="presentation" src="../Images/arrow.png"/></span><span class="koboSpan" id="kobo.256.1">(</span><em class="italic"><span class="koboSpan" id="kobo.257.1">f </span><span class="koboSpan" id="kobo.258.1"><img alt="" role="presentation" src="../Images/circle.png"/></span><span class="koboSpan" id="kobo.259.1"> g</span></em><span class="koboSpan" id="kobo.260.1">)(</span><em class="italic"><span class="koboSpan" id="kobo.261.1">x</span></em><span class="koboSpan" id="kobo.262.1">) = </span><em class="italic"><span class="koboSpan" id="kobo.263.1">f</span></em><span class="koboSpan" id="kobo.264.1">(</span><em class="italic"><span class="koboSpan" id="kobo.265.1">g</span></em><span class="koboSpan" id="kobo.266.1">(</span><em class="italic"><span class="koboSpan" id="kobo.267.1">x</span></em><span class="koboSpan" id="kobo.268.1">)).</span></p>
<p class="normal"><span class="koboSpan" id="kobo.269.1">Function composition in computer science is a powerful concept that allows for the creation of more sophisticated and reusable code by combining smaller functions into larger ones. </span><span class="koboSpan" id="kobo.269.2">It enhances modularity and code organization, making programs easier to read and maintain.</span></p>
</div>
<p class="normal"><span class="koboSpan" id="kobo.270.1">The following is an illustration</span><a id="_idIndexMarker157"/><span class="koboSpan" id="kobo.271.1"> of the anatomy of Semantic Kernel:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.272.1"><img alt="" role="presentation" src="../Images/B21714_02_08.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.273.1">Figure 2.8: Anatomy of Semantic Kernel</span></p>
<p class="normal"><span class="koboSpan" id="kobo.274.1">Semantic Kernel has the following</span><a id="_idIndexMarker158"/><span class="koboSpan" id="kobo.275.1"> main components:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.276.1">Models</span></strong><span class="koboSpan" id="kobo.277.1">: These are the LLMs or LFMs that will be the engine of the application. </span><span class="koboSpan" id="kobo.277.2">Semantic Kernel supports proprietary models, such as those available in OpenAI and Azure OpenAI, and open-source models consumable from the Hugging Face Hub.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.278.1">Memory</span></strong><span class="koboSpan" id="kobo.279.1">: It allows the application to keep references to the user’s interactions, both in the short and long term. </span><span class="koboSpan" id="kobo.279.2">Within the framework of Semantic Kernel, memories can be accessed in three ways:</span><ul>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.280.1">Key</span></strong><span class="koboSpan" id="kobo.281.1">-</span><strong class="keyWord"><span class="koboSpan" id="kobo.282.1">value pairs</span></strong><span class="koboSpan" id="kobo.283.1">: This consists of saving environment variables that store simple information, such as names or dates.</span></li>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.284.1">Local storage</span></strong><span class="koboSpan" id="kobo.285.1">: This consists of saving information to a file that can be retrieved by its filename, such as a CSV or JSON file.</span></li>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.286.1">Semantic memory search</span></strong><span class="koboSpan" id="kobo.287.1">: This is similar to LangChain’s and Haystack’s memory, as it uses embeddings to represent and search for text information based on its meaning.</span></li>
</ul>
</li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.288.1">Functions</span></strong><span class="koboSpan" id="kobo.289.1">: Functions can be seen as skills that mix LLM prompts and code, with the goal of making users’ asks interpretable and actionable. </span><span class="koboSpan" id="kobo.289.2">There are two types of functions:</span><ul>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.290.1">Semantic functions</span></strong><span class="koboSpan" id="kobo.291.1">: These are a type of templated</span><a id="_idIndexMarker159"/><span class="koboSpan" id="kobo.292.1"> prompt, which is a natural language query that specifies the input and output format for the LLM, also incorporating prompt configuration, which sets the parameters for the LLM.</span></li>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.293.1">Native functions</span></strong><span class="koboSpan" id="kobo.294.1">: These refer to the native computer</span><a id="_idIndexMarker160"/><span class="koboSpan" id="kobo.295.1"> code that can route the intent captured by the semantic function and perform the related task.</span></li>
</ul>
</li>
</ul>
<p class="normal-one"><span class="koboSpan" id="kobo.296.1">To make an example, a semantic function</span><a id="_idIndexMarker161"/><span class="koboSpan" id="kobo.297.1"> could ask the LLM to write a short paragraph about AI, while a native function could actually post it on social media like LinkedIn.</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.298.1">Plug-ins:</span></strong><span class="koboSpan" id="kobo.299.1"> These are connectors toward external sources or systems that are meant to provide additional information or the ability to perform autonomous actions. </span><span class="koboSpan" id="kobo.299.2">Semantic Kernel offers out-of-the-box plug-ins, such as the Microsoft Graph connector kit, but you can build a custom plug-in by leveraging functions (both native and semantic, or a mix of the two).</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.300.1">Planner</span></strong><span class="koboSpan" id="kobo.301.1">: As LLMs can be seen as reasoning engines, they can also be leveraged to auto-create chains or pipelines to address new users’ needs. </span><span class="koboSpan" id="kobo.301.2">This goal is achieved with a planner, which is a function that takes as input a user’s task and produces</span><a id="_idIndexMarker162"/><span class="koboSpan" id="kobo.302.1"> the set of actions, plug-ins, and functions needed to achieve the goal.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.303.1">Some benefits of Semantic</span><a id="_idIndexMarker163"/><span class="koboSpan" id="kobo.304.1"> Kernel are:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.305.1">Lightweight and C# support</span></strong><span class="koboSpan" id="kobo.306.1">: Semantic Kernel is more lightweight and includes C# support. </span><span class="koboSpan" id="kobo.306.2">It’s a great choice for C# developers or those using the .NET framework.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.307.1">Wide range of use cases</span></strong><span class="koboSpan" id="kobo.308.1">: Semantic Kernel is versatile, supporting various LLM-related tasks.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.309.1">Industry-led</span></strong><span class="koboSpan" id="kobo.310.1">: Semantic Kernel was developed by Microsoft, and it is the framework the company used to build its own copilots. </span><span class="koboSpan" id="kobo.310.2">Hence, it is mainly driven by industry needs</span><a id="_idIndexMarker164"/><span class="koboSpan" id="kobo.311.1"> and asks, making it a solid tool for enterprise-scale applications.</span></li>
</ul>
<h2 class="heading-2" id="_idParaDest-36"><span class="koboSpan" id="kobo.312.1">How to choose a framework</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.313.1">Overall, the three frameworks</span><a id="_idIndexMarker165"/><span class="koboSpan" id="kobo.314.1"> offer, more or less, similar core components, sometimes called by a different taxonomy, yet covering all the blocks illustrated within the concept of the copilot system. </span><span class="koboSpan" id="kobo.314.2">So, a natural question might be: “Which one should I use to build my LLM-powered application?” </span><span class="koboSpan" id="kobo.314.3">Well, there is no right or wrong answer! </span><span class="koboSpan" id="kobo.314.4">All three are extremely valid. </span><span class="koboSpan" id="kobo.314.5">However, there are some features that might be more relevant for specific use cases or developers’ preferences. </span><span class="koboSpan" id="kobo.314.6">The following are some criteria you might want to consider:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.315.1">The programming language you are comfortable with or prefer to use:</span></strong><span class="koboSpan" id="kobo.316.1"> Different frameworks may support different programming languages or have different levels of compatibility or integration with them. </span><span class="koboSpan" id="kobo.316.2">For example, Semantic Kernel supports C#, Python, and Java, while LangChain and Haystack are mainly based on Python (even though LangChain also introduced JS/TS support). </span><span class="koboSpan" id="kobo.316.3">You may want to choose a framework that matches your existing skills or preferences, or that allows you to use the language that is most suitable for your application domain or environment.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.317.1">The type and complexity of the natural language tasks you want to perform or support:</span></strong><span class="koboSpan" id="kobo.318.1"> Different frameworks may have different capabilities or features for handling various natural language tasks, such as summarization, generation, translation, reasoning, etc. </span><span class="koboSpan" id="kobo.318.2">For example, LangChain and Haystack provide utilities and components for orchestrating and executing natural language tasks, while Semantic Kernel allows you to use natural language semantic functions to invoke LLMs and services. </span><span class="koboSpan" id="kobo.318.3">You may want to choose a framework that offers the functionality and flexibility you need or want for your application goals or scenarios.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.319.1">The level of customization and control you need or want over the LLMs and their parameters or options</span></strong><span class="koboSpan" id="kobo.320.1">: Different frameworks may have different ways of accessing, configuring, and fine-tuning the LLMs and their parameters or options, such as model selection, prompt design, inference speed, output format, etc. </span><span class="koboSpan" id="kobo.320.2">For example, Semantic Kernel provides connectors that make it easy to add memories and models to your AI app, while LangChain and Haystack allow you to plug in different components for the document store, retriever, reader, generator, summarizer, and evaluator. </span><span class="koboSpan" id="kobo.320.3">You may want to choose a framework that gives you the level of customization and control you need or want over the LLMs and their parameters or options.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.321.1">The availability and quality of the documentation, tutorials, examples, and community support for the framework:</span></strong><span class="koboSpan" id="kobo.322.1"> Different frameworks may have different levels of documentation, tutorials, examples, and community support that can help you learn, use, and troubleshoot the framework. </span><span class="koboSpan" id="kobo.322.2">For example, Semantic Kernel has a website with documentation, tutorials, examples, and a Discord</span><a id="_idIndexMarker166"/><span class="koboSpan" id="kobo.323.1"> community; LangChain has a GitHub repository with documentation, examples, and issues; Haystack has a website with documentation, tutorials, demos, blog posts, and a Slack community. </span><span class="koboSpan" id="kobo.323.2">You may want to choose a framework that has the availability and quality of documentation, tutorials, examples, and community support that can help you get started and solve problems with the framework.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.324.1">Let’s briefly summarize the differences between these orchestrators:</span></p>
<table class="table-container" id="table001">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.325.1">Feature</span></strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.326.1">LangChain</span></strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.327.1">Haystack</span></strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.328.1">Semantic Kernel</span></strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.329.1">LLM support</span></strong></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.330.1">Proprietary and open-source</span></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.331.1">Proprietary and open source</span></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.332.1">Proprietary and open source</span></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.333.1">Supported languages</span></strong></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.334.1">Python and JS/TS</span></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.335.1">Python</span></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.336.1">C#, Java, and Python</span></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.337.1">Process orchestration</span></strong></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.338.1">Chains</span></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.339.1">Pipelines of nodes</span></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.340.1">Pipelines of functions </span></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.341.1">Deployment</span></strong></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.342.1">No REST API</span></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.343.1">REST API</span></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.344.1">No REST API</span></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.345.1">Feature</span></strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.346.1">LangChain</span></strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.347.1">Haystack</span></strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.348.1">Semantic Kernel</span></strong></p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref"><span class="koboSpan" id="kobo.349.1">Table 2.1: Comparisons among the three AI orchestrators</span></p>
<p class="normal"><span class="koboSpan" id="kobo.350.1">Overall, all three frameworks offer a wide range of tools and integrations to build your LLM-powered applications, and a wise approach could be to use the one that is most in line with your current skills</span><a id="_idIndexMarker167"/><span class="koboSpan" id="kobo.351.1"> or the company’s overall approach.</span></p>
<h1 class="heading-1" id="_idParaDest-37"><span class="koboSpan" id="kobo.352.1">Summary</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.353.1">In this chapter, we delved into the new way of developing applications that LLMs have been paving, as we introduced the concept of the copilot and discussed the emergence of new AI orchestrators. </span><span class="koboSpan" id="kobo.353.2">Among those, we focused on three projects – LangChain, Haystack, and Semantic Kernel – and we examined their features, main components, and some criteria to decide which one to pick.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.354.1">Once we have decided on the AI orchestrator, another pivotal step is to decide which LLM(s) we want to embed into our applications. </span><span class="koboSpan" id="kobo.354.2">In </span><em class="chapterRef"><span class="koboSpan" id="kobo.355.1">Chapter 3</span></em><span class="koboSpan" id="kobo.356.1">, </span><em class="italic"><span class="koboSpan" id="kobo.357.1">Choosing an LLM for Your Application</span></em><span class="koboSpan" id="kobo.358.1">, we are going to see the most prominent LLMs on the market today – both proprietary and open-source – and understand some decision criteria to pick the proper models with respect to the application use cases.</span></p>
<h1 class="heading-1" id="_idParaDest-38"><span class="koboSpan" id="kobo.359.1">References</span></h1>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.360.1">LangChain repository: </span><a href="https://github.com/langchain-ai/langchain"><span class="url"><span class="koboSpan" id="kobo.361.1">https://github.com/langchain-ai/langchain</span></span></a></li>
<li class="bulletList"><span class="koboSpan" id="kobo.362.1">Semantic Kernel documentation: </span><a href="https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages"><span class="url"><span class="koboSpan" id="kobo.363.1">https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages</span></span></a></li>
<li class="bulletList"><span class="koboSpan" id="kobo.364.1">Copilot stack: </span><a href="https://build.microsoft.com/en-US/sessions/bb8f9d99-0c47-404f-8212-a85fffd3a59d?source=/speakers/ef864919-5fd1-4215-b611-61035a19db6b"><span class="url"><span class="koboSpan" id="kobo.365.1">https://build.microsoft.com/en-US/sessions/bb8f9d99-0c47-404f-8212-a85fffd3a59d?source=/speakers/ef864919-5fd1-4215-b611-61035a19db6b</span></span></a></li>
<li class="bulletList"><span class="koboSpan" id="kobo.366.1">The Copilot system: </span><a href="https://www.youtube.com/watch?v=E5g20qmeKpg"><span class="url"><span class="koboSpan" id="kobo.367.1">https://www.youtube.com/watch?v=E5g20qmeKpg</span></span></a></li>
</ul>
<h1 class="heading-1"><span class="koboSpan" id="kobo.368.1">Join our community on Discord</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.369.1">Join our community’s Discord space for discussions with the author and other readers:</span></p>
<p class="normal"><a href="https://packt.link/llm "><span class="url"><span class="koboSpan" id="kobo.370.1">https://packt.link/llm</span></span></a></p>
<p class="normal"><span class="koboSpan" id="kobo.371.1"><img alt="" role="presentation" src="../Images/QR_Code214329708533108046.png"/></span></p>
</div>
</body></html>