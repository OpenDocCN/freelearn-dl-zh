- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Accessing and Utilizing Models in Amazon Bedrock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provides a practical guide to accessing Amazon Bedrock and uncovering
    its generative AI capabilities. We will start with an overview of the different
    interfaces for invoking Bedrock models, including the console playground, **command-line
    interface** (**CLI**), and **software development kit** (**SDK**). Then, we will
    unveil some of the core Bedrock APIs, along with the code snippets that you can
    run in your environment. Finally, we’ll demonstrate how to leverage Bedrock within
    the LangChain Python framework to build customized pipelines that chain multiple
    models and provide insight into PartyRock, a powerful playground for Amazon Bedrock.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to run and execute applications
    by leveraging SOTA FMs available from Amazon Bedrock as you gain a deeper understanding
    of each of the FMs available and how to utilize them for your needs. You will
    also be able to accelerate your creative thinking regarding building new generative
    AI applications as we dive into building cool apps with PartyRock and learn how
    to integrate Amazon Bedrock into different use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following key topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Accessing Amazon Bedrock
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Amazon Bedrock APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Bedrock integration points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, you’ll need to have access to an **Amazon Web Services** (**AWS**)
    account. If you don’t have one already, you can go to [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    and create one.
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve done this, you’ll need to install and configure the AWS CLI ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/))
    as you’ll need this to access Amazon Bedrock FMs from your local machine. Since
    the majority of the code blocks that we will execute are based on Python, setting
    up an AWS Python SDK (Boto3) ([https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html](https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html))
    would be beneficial. You can set up Python by installing it on your local machine,
    or using AWS Cloud9, or utilizing AWS Lambda, or leveraging Amazon SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: There will be a charge associated with invocating and customizing the FMs of
    Amazon Bedrock. Please refer to [https://aws.amazon.com/bedrock/pricing/](https://aws.amazon.com/bedrock/pricing/)
    to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing Amazon Bedrock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When building a generative AI application, you’re faced with a dizzying array
    of choices. Which FM should you use? How will you ensure security and privacy?
    Do you have the infrastructure to support large-scale deployment? Enter Amazon
    Bedrock.
  prefs: []
  type: TYPE_NORMAL
- en: As you know by now, Amazon Bedrock provides access to a selection of SOTA FMs
    from leading AI companies in the space, including AI21 Labs, Anthropic, Cohere,
    Meta, Stability AI, Amazon, and Mistral. With a single API, you can tap into cutting-edge
    generative AI across modalities such as text, embeddings, and images. You have
    the flexibility to mix and match models to find the perfect fit for your needs.
    Bedrock handles provisioning, scalability, and governance behind the scenes. Hence,
    you can choose the best model suited to your needs and simply invoke the Bedrock
    serverless API to plug those models into your application.
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s jump onto the AWS console and see Amazon Bedrock in action.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you open Amazon Bedrock in the AWS console by navigating to https://console.aws.amazon.com/
    and choosing Bedrock from the search bar, you can explore different FMs, as well
    as a few learning tools, as depicted in *Figure 2**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Amazon Bedrock – Overview](img/B22045_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – Amazon Bedrock – Overview
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Bedrock provides users with the flexibility to experiment with various
    models through its playground interface. Users can access the Bedrock playground
    from the AWS console by navigating to the Amazon Bedrock landing page and clicking
    **Examples** to open the playground environment.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing this book, users will have to initially enable access
    to the models by navigating to the **Model access** link in the left panel within
    the Bedrock console (as shown in *Figure 2**.2*). Once you’ve landed on the **Model
    access** page view, you can click on **Manage model access**, select the list
    of base models you want to leverage for your use cases, and click **Save changes**.
    Instantly, the users will be given access to those models. Users can also review
    the EULA agreement next to the respective base models to view their terms of service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the playground, you can explore the different examples of generative
    AI models available in Bedrock. This allows you to test out and interact with
    the models without needing to configure resources or write any code. Overall,
    the playground provides a convenient way for users to try out the capabilities
    of Bedrock’s generative models. *Figure 2**.2* depicts some of the capabilities
    available within the Amazon Bedrock console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Amazon Bedrock’s capabilities](img/B22045_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Amazon Bedrock’s capabilities
  prefs: []
  type: TYPE_NORMAL
- en: Within the playground, you are given the option to start exploring examples
    based on **Text**, **Chat**, and **Image**. This enables hands-on experimentation
    with the latest generative AI models in a convenient sandbox environment. The
    breadth of options, from conversational chatbots to text and image generation,
    gives you the flexibility to test diverse AI functions firsthand. By providing
    accessible entry points, emerging generative AI becomes more tangible and approachable
    for users to understand. Now, let’s learn about each of these in greater detail.
  prefs: []
  type: TYPE_NORMAL
- en: Chat playground
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon Bedrock gives you access to chat models, which you can experiment with
    in the **Chat playground**.
  prefs: []
  type: TYPE_NORMAL
- en: The **Chat playground** is an experimental interface that allows you to test
    the conversational AI models available through Amazon Bedrock. You can enter sample
    prompts and view the responses that are generated by a selected model. Usage metrics
    are also displayed to evaluate the model’s performance. A compare mode is available
    to contrast the outputs of up to three different models side by side.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following figures, users can select which model they want to
    use (*Figure 2**.3*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Selecting a model](img/B22045_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – Selecting a model
  prefs: []
  type: TYPE_NORMAL
- en: 'Thereafter, users can enter a query in the chat box (*Figure 2**.4*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Querying the chat model in the Chat playground](img/B22045_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Querying the chat model in the Chat playground
  prefs: []
  type: TYPE_NORMAL
- en: Running a query fetches information from the chosen model. This allows you to
    evaluate factors such as accuracy, response length, latency, and suitability for
    your use case. Selecting the optimal model depends on weighing these factors against
    individual needs.
  prefs: []
  type: TYPE_NORMAL
- en: While invoking the FMs, you will see the option to modify the **inference parameters**
    so that you can influence the model’s response in a certain way. While some inference
    parameters are common among LLMs, image models have a separate set of parameters
    that can be tuned by users.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at some of these common parameters.
  prefs: []
  type: TYPE_NORMAL
- en: LLM inference parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Temperature*, *Top P*, *Top K*, *Response length*, *Stop sequences*, and *Max
    tokens* are the inference parameters that we will learn about in detail in this
    section. *Figure 2**.5* shows them on the Amazon Bedrock **Chat playground** screen;
    they can be found in the **Configurations** window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – Common LLM inference parameters](img/B22045_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – Common LLM inference parameters
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a closer look:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Temperature**: This parameter controls the degree of randomness in the output.
    A lower temperature results in more deterministic output, favoring the most likely
    option. On the other hand, a higher temperature promotes randomness, leading to
    a wider range of diverse and creative outputs. For example, in QA tasks, a lower
    temperature ensures more factual and concise responses, whereas if your use case
    revolves around generating creative and diverse output, such as in creative writing
    or advertisement generation, it might be worthwhile to increase the temperature
    value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Top K** **and** **Top P**: Sampling techniques such as **Top K** and **Top
    P** can be employed to enhance the coherence and sense of the output. **Top K**
    limits the number of options to a specified number, ensuring a balance between
    randomness and coherence. **Top P**, on the other hand, restricts the predictions
    with combined probabilities below a specified threshold, preventing highly improbable
    options from being selected. These techniques help strike a balance between generating
    coherent text and maintaining a certain level of randomness, making the text generation
    process more natural and engaging for the reader.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using these together balances novelty and fluency. For example, you could set
    **Top K** to 70 and **Top P** to 0.8\. This allows some uncommon but still relevant
    words via the **Top P** setting, while **Top K** retains focus on more common
    words. The result is text that is fairly fluent with occasional novel words mixed
    in. You can experiment with different values for **Top K** and **Top P** to achieve
    the novelty versus fluency balance you want for a particular generative AI application.
    Start with a **Top K** value around 50 to 100 and a **Top P** value around 0.7
    to 0.9 as reasonable initial settings. The optimal values depend on factors such
    as model size, dataset, and use case.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`bedrock`, the model will stop generating output as soon as it encounters the
    word *bedrock* in the generated text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Write a sentence in` `100 words`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image model inference parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When performing image generation with FMs, several key parameters affect the
    inference process. For instance, in the case of Stable Diffusion models, the model
    takes in a text prompt and random noise vector to produce an image. Several configuration
    settings for the model can influence the final generated image, as depicted in
    *Figure 2**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – Image model inference parameters](img/B22045_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 – Image model inference parameters
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a closer look at these parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt strength**: This controls the degree of randomness. Lowering the **Prompt
    strength** value generates a more random image while increasing it generates a
    more accurate representation of the prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generation step**: Similar to **Prompt strength**, increasing the **Generation
    step** value generates a more intricate and detailed image while decreasing it
    generates a simpler image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seed**: The **Seed** parameter controls the initial state of the random number
    generator, which affects the overall randomness of the generated image. It is
    important to note that the precise values of these parameters can vary depending
    on the specific use case and the desired trade-off between image fidelity and
    randomness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a more detailed description of these parameters, take a look at the Stable
    Diffusion documentation: [https://platform.stability.ai/docs/api-reference#tag/Image-to-Image](https://platform.stability.ai/docs/api-reference#tag/Image-to-Image).'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re using Amazon Titan Image Generator, there are various parameters you
    can use. You can find a full list at [https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-image.html](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-image.html).
  prefs: []
  type: TYPE_NORMAL
- en: Text playground
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Text playground** serves a similar function for evaluating generative
    text models from Amazon Bedrock. You may enter text prompts that the selected
    model will then expand upon or continue as a longer passage of generated text
    reflecting that prompt. The expanded text from the model is shown in the playground’s
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: However, the **Text playground** doesn’t manage conversational context. Essentially,
    it generates a sequence of most likely tokens from the end of the text placed
    in the **Text playground** window. The behavior demonstrated in the **Text playground**
    is a fundamental building block of the chat behavior, and when chained together
    over multiple turns, it can create a chat experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, similar to the **Chat playground**, users can also navigate to the text
    playground, select another model (for instance, Anthropic Claude 3 Sonnet, as
    shown in *Figure 2**.7*), update the inference configuration, and prompt the model
    to generate a response for their use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – Adding a prompt in the text playground](img/B22045_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 – Adding a prompt in the text playground
  prefs: []
  type: TYPE_NORMAL
- en: Image playground
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In **Image playground**, you can try out two different image models: Amazon
    Titan Image Generator and Stability AI’s Stable Diffusion. If these sound new
    to you, please refer to their eponymous sub-sections in [*Chapter 1*](B22045_01.xhtml#_idTextAnchor014).
    These models generate images through text or images and also perform in-painting,
    image editing, and more. Let''s see an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Adding a prompt in the image playground](img/B22045_02_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 – Adding a prompt in the image playground
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 2**.8*, when we provide `High quality, intricate detailed,
    hyper-realistic cat photography, volumetric lighting, full character, 4k, in spacesuit`
    as a prompt, the model generates an image conditioned on the text that was provided.
    Within the configuration, you also have the option to provide a **Negative prompt**
    value, which tells the model what it shouldn’t generate. In addition, you can
    provide a **Reference image** value, which the model will use as a reference to
    generate the image. In [*Chapter 9*](B22045_09.xhtml#_idTextAnchor171), we will
    explore how image generation and editing work with Amazon Bedrock.
  prefs: []
  type: TYPE_NORMAL
- en: API-based approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the major benefits of using a unified API for inference is that it allows
    you to easily experiment with different models from various providers using the
    same interface. Even as new model versions are released, you can swap them in
    with minimal code changes required on your end.
  prefs: []
  type: TYPE_NORMAL
- en: The single API abstraction acts as an insulation layer, shielding your application
    code from underlying model implementation details. This frees you from vendor
    lock-in and grants flexibility to adopt cutting-edge models as they become available.
    With a consistent API shielding this complexity, you can focus on product innovation
    rather than engineering logistics.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Bedrock provides a set of APIs that can be directly accessed and utilized
    via the *AWS CLI* or *AWS SDK*.
  prefs: []
  type: TYPE_NORMAL
- en: AWS CLI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `list-foundation-models` API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, to invoke a model (for example, the Mistral 7B Instruct model),
    you can call the `invoke-model` API of `bedrock-runtime`. At the time of writing,
    users have to request model access from the console. Once granted in the system,
    the following code can be used to invoke the respective model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `body` parameter of the `invoke-model` API call, you can see it is written
    in a particular format (`"{\"prompt\":\"<s>[INST]text [/INST]\"`). Different models
    may require a different structure of prompt while invoking the model. If you search
    for Amazon Bedrock in the AWS console, you can view the actual API request that’s
    sent to the model. Follow these steps to view the API requests:'
  prefs: []
  type: TYPE_NORMAL
- en: Open Amazon Bedrock in the AWS console by navigating to [https://console.aws.amazon.com/](https://console.aws.amazon.com/)
    and choosing **Bedrock** from the search bar.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Providers** under **Getting started**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose any provider and model of your choice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll down to the **Model** section and expand **API request**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In *Figure 2**.9*, you can see the API request in JSON from the *Mistral 7B
    Instruct* model. In the `body` parameter of the API request, we can see the format
    of the prompt needed by the model, along with the inference parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – Mistral 7B Instruct API request](img/B22045_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – Mistral 7B Instruct API request
  prefs: []
  type: TYPE_NORMAL
- en: This enables transparency regarding how the user’s input gets formatted and
    passed to the underlying AI system. Overall, the playground allows users to not
    only test prompts but also inspect the API requests that are made to generate
    the AI responses.
  prefs: []
  type: TYPE_NORMAL
- en: AWS SDK
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AWS provides SDKs for various programming languages, such as JavaScript, Python,
    Java, and more. These SDKs provide wrapper libraries that make it easy to integrate
    Bedrock API calls into your code. It is often beneficial to use an SDK tailored
    to your programming language of choice. Consulting the SDK documentation for your
    chosen language can provide helpful code samples, usage guidelines, and other
    resources to ensure the integration process goes smoothly ([https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html](https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html)).
  prefs: []
  type: TYPE_NORMAL
- en: You can call these Bedrock APIs through AWS SDKs from your local machines or
    use AWS services such as AWS Lambda, Amazon SageMaker Studio notebooks, AWS Cloud9,
    and others. Using the AWS SDK for Python (Boto3), you can call Bedrock APIs to
    build ML workflows. Let’s look at some of the APIs provided by Amazon Bedrock
    and examples of their usage in the **AWS SDK** for Python (Boto3).
  prefs: []
  type: TYPE_NORMAL
- en: Thus far, we have explored the array of FMs that are offered through Amazon
    Bedrock, experimenting with various prompts and tuning inference configurations
    to produce preferred outputs. We’ve tapped into models directly via the Amazon
    Bedrock playground and examined leveraging the AWS CLI and various SDKs to invoke
    FMs programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: Having established this foundation of working knowledge, we’ll pivot to investigating
    Amazon Bedrock’s APIs more deeply. The next section will help us leverage these
    APIs in custom generative AI applications that harness the power of FMs while
    providing developers with more control and customization. We will map out an end-to-end
    workflow – from initializing a client to generating outputs – that will empower
    you to build robust, reliable generative apps powered by industrial-grade FMs.
  prefs: []
  type: TYPE_NORMAL
- en: Using Amazon Bedrock APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like other AWS services, Amazon Bedrock provides several APIs. These APIs can
    be placed under the Control Plane API for managing, training, and deploying FMs
    and the Runtime Plane API for making invocations or inference requests to the
    FMs. Some of the common Control Plane Bedrock APIs include **ListFoundationModels**,
    **GetFoundationModels**, and **CreateModelCustomizationJob**. On the other hand,
    the Runtime Plane API has two APIs: **InvokeModel** and **InvokeModelWithResponseStream**.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, there are separate APIs associated with Agents for Amazon Bedrock,
    something we’ll cover in more detail in [*Chapter 10*](B22045_10.xhtml#_idTextAnchor192).
  prefs: []
  type: TYPE_NORMAL
- en: You can find the complete list of API calls supported by Amazon Bedrock, including
    all the data types and actions you can perform, at [https://docs.aws.amazon.com/bedrock/latest/APIReference/](https://docs.aws.amazon.com/bedrock/latest/APIReference/).
    Let’s look at some of the commonly used Bedrock API calls.
  prefs: []
  type: TYPE_NORMAL
- en: ListFoundationModels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To utilize the generative capabilities of Bedrock, the first step is to discover
    which FMs are available via the service. The **ListFoundationModels** API retrieves
    metadata about the base models, including the unique model ID required to generate
    content using that model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Python code sample demonstrates how to call the ListFoundationModels
    API to enumerate the available base models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let’s consider some of the currently available base models and their respective
    model IDs provided via Amazon Bedrock. You use a model ID as a means to indicate
    the base model when users intend to leverage any of the existing models using
    InvokeModel ([https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html))
    or InvokeModelWithResponseStream ([https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html)).
    With this information, the desired model can be selected and its ID can be used
    to call other Bedrock operations, such as InvokeModel, to generate content tailored
    to your application’s needs.
  prefs: []
  type: TYPE_NORMAL
- en: GetFoundationModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Utilizing Amazon Bedrock, developers can access SOTA generative AI models through
    the **GetFoundationModel** API call. This operation retrieves comprehensive information
    on a specified base model. For example, to return details on Meta’s Llama 3 70B
    Instruct model in Python, you can run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: InvokeModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **InvokeModel** API simplifies the deployment of ML models. With just a
    few API calls, you can deploy your trained models onto the AWS infrastructure
    securely. This eliminates the need for managing complex deployment processes,
    allowing you to focus on the core of your AI application.
  prefs: []
  type: TYPE_NORMAL
- en: You can invoke specified Bedrock models to perform inference using inputs provided
    in the request body. The InvokeModel API allows you to run inference for various
    model types, including text, embedding, and image models. This allows users to
    leverage pretrained models available via Amazon Bedrock to generate predictions
    and insights by passing data into the model and receiving the desired output.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s an example of an API request for sending text to Meta’s Llama 3 70 B
    model. Inference parameters depend on the model that you are going to use.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code block, the `InvokeModel` operation allows you
    to perform inference on models. The `modelId` field specifies the desired model
    to utilize. The process of obtaining `modelId` varies based on the model type.
    By leveraging the `InvokeModel` operation and specifying the appropriate `modelId`
    value, users can harness the power of a plethora of generative AI models to draw
    relevant insights.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using Anthropic Claude models, you can use the Messages API to create
    conversational interfaces to manage the chat between the user and the model. Here’s
    an example of an API request that could be sent to the Anthropic Claude Sonnet
    3 model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The API manages the back-and-forth flow of dialogue by accepting a series of
    messages with alternating *user* and *assistant* roles as input. To learn more
    about the Messages API, you can look at the documentation: [https://docs.anthropic.com/claude/reference/messages_post](https://docs.anthropic.com/claude/reference/messages_post).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon Bedrock also allows you to precisely configure the throughput your models
    need to deliver responsive performance for your applications. With **Provisioned
    Throughput**, you can choose the compute capacity your models require to meet
    your workload demands and latency requirements. Hence, in the case of Amazon and
    third-party base models, and with customized models, users can purchase Provisioned
    Throughput before running inferences. This capability ensures that you get the
    guaranteed throughput your models require for optimal cost and performance. More
    details on Provisioned Throughput can be found here: [https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html](https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html).'
  prefs: []
  type: TYPE_NORMAL
- en: InvokeModelWithResponseStream
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This streaming inference method that’s available via Amazon Bedrock allows FMs
    to produce long, coherent content on demand. Rather than waiting for generation
    to complete, applications can stream results. This allows you to send responses
    from the model in faster chunks rather than having to wait for a complete response.
  prefs: []
  type: TYPE_NORMAL
- en: To run inference with streaming, you can simply invoke the `InvokeModelWithResponseStream`
    operation provided by Amazon Bedrock. This runs inference on the model with the
    given input and returns the generated content progressively in a stream.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how the Claude V2 model can generate a 500-word blog on quantum
    computing.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The following code snippet works when run in a Jupyter Notebook environment.
    Jupyter Notebook provides additional functionality and initialization that allows
    this code to operate correctly. Attempting to run this snippet directly in a terminal
    without the Jupyter environment may result in errors. For the best results, run
    this code in Jupyter Notebook rather than directly in a terminal.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This will print out the generated blog text continuously as it is produced by
    the model. This stream-based approach allows the output to be displayed live while
    Claude V2 is *writing* the blog content. Hence, streaming inference unlocks new
    real-time and interactive use cases for large generative models.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we explored Amazon Bedrock’s key APIs, all of which allow us
    to build generative AI applications. We reviewed how to list the FMs that are
    available through Amazon Bedrock and detailed how to invoke these models to produce
    customized outputs. Next, we will uncover how Amazon Bedrock integrates with LangChain
    to choreograph and address intricate use cases. By leveraging Bedrock’s API and
    LangChain’s orchestration, developers can assemble sophisticated generative solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Converse API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Amazon Bedrock **Converse API** offers a standardized method to interact
    with LLMs available via Amazon Bedrock. It facilitates turn-based communication
    between users and generative AI models and ensures consistent tool definitions
    for models that support functions (referred to as **function calling**).
  prefs: []
  type: TYPE_NORMAL
- en: The significance of the `Converse` API lies in its ability to streamline integration.
    Previously, using the `InvokeModel` API required adapting to varying JSON request
    and response structures from different model providers. With the `Converse` API,
    a uniform format for requests and responses is implemented across all LLMs on
    Amazon Bedrock, simplifying development and ensuring consistent interaction protocols.
  prefs: []
  type: TYPE_NORMAL
- en: Let us walk through an example of using `Converse` API for text generation scenario
    by leveraging Anthropic Claude 3 Sonnet model. Please ensure you have the required
    permission for you require permission for `bedrock:InvokeModel` operation in order
    to call `Converse` API.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Please note that switching the model ID to another text generation FM available
    on Amazon Bedrock allows it to run using the `Converse` API. The code example
    above, along with other `Converse` API examples, has been added to the GitHub
    repository for readers to experiment with in their own accounts.
  prefs: []
  type: TYPE_NORMAL
- en: The `Converse` API can also process documents and images. For instance, you
    can send an image or document in a message and use `Converse` API to have the
    model describe its contents. For more details on supported models and model features
    with `Converse` API, visit [https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-call](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-call)
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the **ConverseStream API** makes it easy to send messages to specific
    Amazon Bedrock models and receive responses in a continuous stream. It provides
    a unified interface that works across all foundational models supported by Amazon
    Bedrock for messaging.
  prefs: []
  type: TYPE_NORMAL
- en: To use the `ConverseStream` API, you can invoke it with the `Converse` API.
    Note that you need the `bedrock:InvokeModelWithResponseStream` operation permission
    to use `ConverseStream.`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: When you run the code sample above, it streams the response output. For more
    information on `ConverseStream,` you can refer to the documentation at [https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ConverseStream.html](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ConverseStream.html).
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Bedrock integration points
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When building end-to-end generative AI applications, architects must follow
    best practices for security, performance, cost optimization, and latency reduction,
    as outlined in the AWS Well-Architected Framework pillars. They aid developers
    in weighing different choices and optimizations when creating end-to-end systems
    on AWS. More information on the AWS Well-Architected Framework can be found here:
    [https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html](https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Many customers looking to build conversational interfaces such as chatbots,
    virtual assistants, or summarization systems integrate Amazon Bedrock’s serverless
    API with other services. Useful integration points include orchestration frameworks
    such as LangChain and AWS Step Functions for invoking Amazon Bedrock models via
    AWS Lambda.
  prefs: []
  type: TYPE_NORMAL
- en: As customers adopt LLMOps approaches to optimize building, scaling, and deploying
    LLMs for enterprise applications, these integration tools and frameworks are becoming
    more widely adopted. The serverless API, orchestration layer, and Lambda functions
    create a robust and scalable pipeline for delivering performant and cost-effective
    generative AI services.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Bedrock with LangChain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let’s take our understanding of Amazon Bedrock and generative AI applications
    to the next level by introducing LangChain integration with Amazon Bedrock!
  prefs: []
  type: TYPE_NORMAL
- en: LangChain is a revolutionary framework that empowers developers to build advanced
    language models and generate human-like text. By chaining together various components,
    you can create sophisticated use cases that were previously unimaginable. For
    example, if you work in the financial services industry, you can create an application
    that can provide insights, a simplified summary, and a Q&A of complex financial
    documents, and by using the LangChain framework, you can abstract the API complexities.
    By bringing Bedrock and LangChain together, developers gain the best of both worlds.
    Need an AI assistant, search engine, or content generator? Spin up a capable model
    with Bedrock, then use LangChain’s templates and pipelines to craft the perfect
    prompt and handle the output. This modular approach allows for immense flexibility,
    adapting as your needs change. By creating a custom prompt template via LangChain,
    you can pass in different input variables on every run. This allows you to generate
    content that’s tailored to your specific use case, whether it’s responding to
    customer feedback or crafting personalized marketing messages.
  prefs: []
  type: TYPE_NORMAL
- en: And it’s easy to get started! LangChain’s Bedrock API component provides a simple
    way to invoke Bedrock APIs from within a LangChain pipeline. Just a few lines
    of code can kick off a request, feeding your input to a beefy model and returning
    the goods. From there, your app has a robust, scalable AI backend ready to go.
  prefs: []
  type: TYPE_NORMAL
- en: The following piece of code showcases the ease with which you can leverage Amazon
    Bedrock with LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Before running the following code, make sure you have the latest version of
    the LangChain package installed. If not, run the package installation cell provided
    next to install LangChain in your environment. Alternatively, you can download
    the package from [https://pypi.org/project/langchain/](https://pypi.org/project/langchain/)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code snippet, users can invoke a particular model
    using a simple prompt by easily leveraging the LLM for the LangChain Bedrock class
    and passing the respective FM’s arguments for inference.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a LangChain custom prompt template
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By creating a template for a prompt, you can pass different input variables
    to it on every run. This is useful when you have to generate content with different
    input variables that you may be fetching from a database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can invoke Bedrock using the prompt template to see a curated response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This integration exemplifies how LangChain’s framework facilitates the creation
    of complex language-based tasks. In this instance, the Bedrock API acts as a bridge
    between the LangChain components and the underlying language model.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, by integrating LangChain and Amazon Bedrock, developers can leverage
    the advanced functionalities of LangChain, such as prompt templates, pipelines,
    and orchestration capabilities with other AI services, to create dynamic and adaptive
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: PartyRock
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we’ve discussed how to access and explore Amazon Bedrock for your applications
    using different techniques, let’s look at another interesting feature. Amazon
    also has a mechanism to quickly build and deploy a fun and intuitive application
    for experimentalists and hobbyists through **PartyRock**, a powerful playground
    for Amazon Bedrock. Within PartyRock, you can create multiple applications and
    experiment with Amazon Bedrock. For example, you can create an optimized party
    plan and budgeting tool for your 5-year-old.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 2**.10*, we have created a sample application that can list different
    Grammy award winners based on the year(s) that users can input in the app. Users
    can simply click on the link provided next and enter a particular year (or years
    in each line) in the left pane. On entering a particular year or a few years,
    the system will generate the Grammy award winners in the right pane. You can check
    out the app at [https://partyrock.aws/u/shikharkwtra/jAJQre8A0/Grammy-Celebrity-Namer](https://partyrock.aws/u/shikharkwtra/jAJQre8A0/Grammy-Celebrity-Namer).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 – PartyRock example – Grammy Celebrity Namer](img/B22045_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 – PartyRock example – Grammy Celebrity Namer
  prefs: []
  type: TYPE_NORMAL
- en: PartyRock provides builders with access to FMs from Amazon Bedrock to learn
    the fundamentals of prompt engineering and generative AI. Users are encouraged
    to build some cool apps with PartyRock and go a step further to understand Amazon
    Bedrock. Simply navigate to [https://partyrock.aws/](https://partyrock.aws/),
    click on **Build your own app**, and get started with your journey to becoming
    a generative AI application developer on PartyRock!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before moving on to the next chapter, let’s quickly recap what we covered in
    this chapter. First, we looked at how to access Amazon Bedrock through the AWS
    console. Utilizing the Bedrock console, we queried the **Text**, **Chat**, and
    **Image playground** APIs and experimented with various inference parameters to
    analyze their impact on model outputs. In addition to interacting with the models
    through the Bedrock console, we investigated accessing the FMs via the AWS CLI
    and AWS SDK.
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging the CLI and SDK, we were able to uncover some of the underlying
    Bedrock APIs that can be used to list available FMs, retrieve detailed information
    about them, and invoke them. We concluded this chapter by looking at some integration
    points of Amazon Bedrock, including the popular LangChain framework, and provided
    a brief overview of PartyRock, a powerful playground in Amazon Bedrock for testing
    prompts and building fun applications.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a good conceptual understanding of Amazon Bedrock and the ability
    to access various Bedrock models, in the next chapter, we will explore some effective
    prompt engineering techniques we can implement when we use Amazon Bedrock.
  prefs: []
  type: TYPE_NORMAL
