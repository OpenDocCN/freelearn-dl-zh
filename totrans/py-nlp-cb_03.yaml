- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Representing Text – Capturing Semantics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Representing the meaning of words, phrases, and sentences in a form that’s understandable
    to computers is one of the pillars of NLP processing. Machine learning, for example,
    represents each data point as a list of numbers (a fixed-size vector), and we
    are faced with the question of how to turn words and sentences into these vectors.
    Most NLP tasks start by representing the text in some numeric form, and in this
    chapter, we show several ways to do that.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will create a simple classifier to demonstrate the effectiveness of
    each method of encoding, and then we will use it to test the different encoding
    methods. We will also learn how to turn phrases such as *fried chicken* into vectors
    – that is, how to train a `word2vec` model for phrases. Finally, we will see how
    to use vector-based search.
  prefs: []
  type: TYPE_NORMAL
- en: For a theoretical background on some of the concepts discussed in this section,
    refer to *Building Machine Learning Systems with Python* by Coelho et al. This
    book will explain the basics of building a machine learning project, such as training
    and test sets, as well as metrics used to evaluate such projects, including precision,
    recall, F1, and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the recipes that are covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a simple classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Putting documents into a bag of words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructing an *N*-gram model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Representing texts with TF-IDF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using word embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training your own embeddings model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using BERT and OpenAI embeddings instead of word embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using **retrieval augmented** **generation** (**RAG**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this chapter is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter03](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter03).
    Packages that are required for this chapter should be installed automatically
    via the `poetry` environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, we will use models and datasets located at the following URLs.
    The Google `word2vec` model is a model that represents words as vectors, and the
    IMDB dataset contains movie titles, genres, and descriptions. Download them into
    the `data` folder inside the `root` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Google** **word2vec** **model**: [https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The IMDB movie dataset**: [https://github.com/venusanvi/imdb-movies/blob/main/IMDB-Movie-Data.csv](https://github.com/venusanvi/imdb-movies/blob/main/IMDB-Movie-Data.csv)
    (also available in the book’s GitHub repo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to the preceding files, we will use various functions from a simple
    classifier that we will create in the first recipe. This file is available at
    [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/util_simple_classifier.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/util_simple_classifier.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Creating a simple classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The reason why we need to represent text as vectors is to make it into a computer-readable
    form. Computers can’t understand words but are good at manipulating numbers. One
    of the main NLP tasks is the classification of texts, and we are going to create
    a classifier for movie reviews. We will use the same classifier code but with
    different methods of creating vectors from text.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will create the classifier that will assign either negative
    or positive sentiment to *Rotten Tomatoes* reviews, a dataset available through
    Hugging Face, a large repository of open source models and datasets. We will then
    use a baseline method, where we encode the text by counting the number of different
    parts of speech present in it (verbs, nouns, proper nouns, adjectives, adverbs,
    auxiliary verbs, pronouns, numbers, and punctuation).
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this recipe, we will have created a separate file with functions
    that create the dataset and train the classifier. We will use this file throughout
    the chapter to test different encoding methods.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will create a simple classifier for movie reviews. It will
    be a `sklearn` package.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will load the Rotten Tomatoes dataset from Hugging Face. We will use just
    part of the dataset so that the training time is not very long:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the file and language **util** notebooks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the train and test datasets from Hugging Face (the **datasets** package).
    For both the training and test sets, we will select the first and last 15% of
    the data instead of loading the full datasets. The full dataset is large, and
    it takes a long time to train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print out the length of each dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we create the **POS_vectorizer** class. This class has a method, **vectorize**,
    that processes the text and counts the number of verbs, nouns, proper nouns, adjectives,
    adverbs, auxiliary verbs, pronouns, numbers, and punctuation marks. The class
    needs a **spaCy** model to process the text. Each piece of text is turned into
    a vector of size 10\. The first element of the vector is the length of the text,
    and the other numbers indicate the number of words in the text of that particular
    part of speech:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can test out the **POS_vectorizer** class. We take the first review’s
    text to process and create the vectorizer using the small **spaCy** model. We
    then vectorize the text using the newly created class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s print the text and the vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should look like this. We can see that the vector correctly counts
    the parts of speech. For example, there are five punctuation marks (two quotes,
    one comma, one dot, and one dash):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now prepare the data for training our classifier. We first import the
    **pandas** and the **numpy** packages and then create two dataframes, one for
    training and the other one for testing. In each dataset, we create a new column
    called **vector** that contains the vector for this piece of text. We use the
    **apply** method to turn the text into vectors and store them in the new column.
    In this method, we pass in a lambda function that takes a piece of text and applies
    the **vectorize** method of the **POS_vectorizer** class to that piece of text.
    We then turn the vector and the label columns into **numpy** arrays to have the
    data in the right format for the classifier. We use the **np.stack** method for
    the vector, since it’s already a list, and the **to_numpy** method for the review
    labels, since they are just numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will train the classifier. We will choose the logistic regression algorithm,
    since it is one of the simplest algorithms, as well as one of the fastest. First,
    we import the **LogisticRegression** class and the **classification_report** methods
    from **sklearn**. Then, we create the **LogisticRegression** object, and finally,
    train it on the data from the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can test the classifier on the test data by applying the **predict** method
    to the vectors in the test data and print out the classification report. We can
    see that the overall accuracy is low, slightly above chance. This is because the
    vector representation we used is very crude. We will use other vectors in the
    next sections and see how they affect the classifier results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should be similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will now turn the preceding code into several functions so that we can only
    vary the vectorizer being used in the construction of the dataset. The resulting
    file is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/util_simple_classifier.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/util_simple_classifier.ipynb).
    The resulting code will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the function that will create and return the training and test dataframes.
    It will create them from the Rotten Tomatoes dataset from Hugging Face:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This function takes the dataframes and the **vectorize** method and creates
    the **numpy** arrays for the training and test data. This will allow us to train
    the logistic regression classifier using the created vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This function trains a logistic regression classifier on the given training
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This final function takes in the test data and the trained classifier and prints
    out the classification report:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In each succeeding section that demonstrates a new vectorizing method, we will
    use this file to pre-load the necessary functions to test the classification result.
    This will allow us to evaluate the different vectorizing methods. We will only
    vary the vectorizer while keeping the classifier the same. When the classifier
    performs better, it reflects how well the underlying vectorizer represents the
    text.
  prefs: []
  type: TYPE_NORMAL
- en: Putting documents into a bag of words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **bag of words** is the simplest way of representing a text. We treat our
    text as a collection of *documents*, where documents are anything from sentences
    to scientific articles to blog posts or whole books. Since we usually compare
    different documents to each other or use them in a larger context of other documents,
    we work with a collection of documents, not just a single document.
  prefs: []
  type: TYPE_NORMAL
- en: The bag of words method uses a “training” text that provides it with a list
    of words that it should consider. When encoding new sentences, it counts the number
    of occurrences each word makes in the document, and the final vector includes
    those counts for each word in the vocabulary. This representation can then be
    fed into a machine learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The reason this vectorizing method is called a *bag of words* is that it does
    not take into account the relationships of words between themselves and only counts
    the number of occurrences of each word. The decision on what represents a document
    lies with the engineer and, in many cases, will be obvious. For example, if you
    are working on classifying tweets that belong to a particular topic, a single
    tweet will be your document. If, conversely, you would like to find out which
    chapters of a book are most similar to a book you already have, then chapters
    are documents.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will create a bag of words for the Rotten Tomatoes reviews.
    Our documents will be the reviews. We then test the encoding using a bag of words
    by building a logistic regression classifier, using code from the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, we will use the `CountVectorizer` class from the `sklearn`
    package. It is included in the `poetry` environment. The `CountVectorizer` class
    is specifically designed to count the number of occurrences of each word in a
    text.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our code will take a set of documents – in this case, reviews – and represent
    them as a matrix of vectors. We will use the Rotten Tomatoes reviews dataset from
    Hugging Face for this task:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the simple classifier **utility** file, and then import the **CountVectorizer**
    object and the **sys** package. We will need the **sys** package to change the
    printing options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the training and testing dataframes by using the function from the **util_simple_classifier.ipynb**
    file. We created this function in the previous recipe, *Creating a simple classifier*.
    The function loads 15% of the Rotten Tomatoes dataset into a **pandas** dataframe
    and randomizes its order. It might take a few minutes to run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the vectorizer, fit it on the training data, and print out the resulting
    matrix. We will use the **max_df** parameter to specify which words should be
    used as stop words. In this case, we specify that words that appear in more than
    40% of the documents should be ignored when constructing the vectorizer. You should
    experiment and see exactly which value of **max_df** would suit your use case.
    We then fit the vectorizer on the **text** column of the **train_df** dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The resulting matrix is a `scipy.sparse._csr.csr_matrix` object, and the beginning
    of its printout looks like this. The format of a sparse matrix is `(row, column)
    value`. In our case, this means (the document index, word index) followed by the
    frequency. In our example, the first review, which is the first document, is document
    number `0`, and it contains words with indices `6578`, `4219`, and others. The
    frequencies of these words are `1` and `2`, respectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In most cases, we use a different format to represent vectors, a dense matrix
    that is easier to use in practice. Instead of specifying rows and columns with
    numbers, they are inferred from the position of the value. We will now create
    a dense matrix and print it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting matrix is a NumPy matrix object, where each review is a vector.
    You see that most values in the matrix are zeroes, as expected, since each review
    only uses a handful of words, while the vector collects counts for each word in
    the vocabulary, or each word in all of the reviews. Any words that are not in
    the vectorizer’s vocabulary will not be included in the vector:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see all the words used in the document set and the length of the vocabulary.
    This can be used as a sanity check and to see whether there are any irregularities
    in the vocabulary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows. If you want to see the full, non-truncated list,
    use the `set_printoptions` function used in *step 8*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also see all the stop words used by the vectorizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result is three words, `and`, `the`, and `of`, that appear in more than
    40% of reviews:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can now also use the **CountVectorizer** object to represent new reviews
    that were not in the original document set. This is done when we have a trained
    model and want to test it on new, unseen samples. We will use the first review
    in the test dataset. To get the first review in the test set, we will use the
    **pandas** **iat** function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The first review looks as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will create both a sparse and a dense vector from the first review.
    The **transform** method of the vectorizer expects a list of strings, so we will
    create a list. We also set the **print** option to print out the whole vector
    instead of just part of it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The dense vector is very long and is mostly zeroes, as expected:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can use a different method to calculate stop words. Here, stop words are
    calculated by setting an absolute threshold on the word frequency. In this case,
    we use all words whose frequency is lower than 300 across documents. You can see
    that the stop-word list is now larger.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can provide our own list of stop words to the vectorizer. These
    words will be ignored by the vectorizer and not represented in the vector. This
    is useful if you have very specific words you would like to ignore:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now test the effect of this bag-of-words vectorizer on the simple classifier,
    using the functions we defined in the previous recipe. First, we create the vectorizer,
    specifying to use only words that appear in less than 80% of the documents. Then,
    we load the training and test dataframes. We fit the vectorizer on the training
    set reviews. We create a vectorize function using the vectorizer and pass it on
    to the **create_train_test_data** function, along with the training and test data
    frames. We then train the classifier and test it on the testing data. We can see
    that this vectorizing method gives us much better results than the simple part-of-speech
    count vector we used in the previous section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Constructing an N-gram model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Representing a document as a bag of words is useful, but semantics is about
    more than just words in isolation. To capture word combinations, an **n-gram model**
    is useful. Its vocabulary consists of not just words but also word sequences,
    or *n*-grams.
  prefs: []
  type: TYPE_NORMAL
- en: We will build a **bigram model** in this recipe, where bigrams are sequences
    of two words.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `CountVectorizer` class is very versatile and allows us to construct *n*-gram
    models. We will use it in this recipe and test it with a simple classifier.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, I make comparisons of the code and its results to the ones in
    the *Putting documents into a bag of words* recipe, since the two are very similar,
    but they have a few differing characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Run the simple classifier notebook and import the **CountVectorizer** class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the training and test dataframes using code from the **util_simple_classifier.ipynb**
    notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a new vectorizer class. In this case, we will use the **ngram_range**
    argument. The **CountVectorizer** class, when the **ngram_range** argument is
    set, counts not only individual words but also word combinations, where the number
    of words in the combinations depends on the numbers provided to the **ngram_range**
    argument. We provided **ngram_range=(1,2)** as the argument, which means that
    the number of words in the combinations ranges from 1 to 2, so unigrams and bigrams
    are counted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the vocabulary of the vectorizer and its length. As you can see, the
    length of the vocabulary is much larger than the length of the unigram vectorizer,
    since we use two-word combinations in addition to single words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we take the first review in the testing dataframe and get its dense vector.
    The result looks very similar to the vector output in the *Putting documents into
    a bag of words* recipe, with the only difference that now the output is longer,
    as it includes not just individual words but also bigrams, or sequences of two
    words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The printout looks like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we train a simple classifier using the new bigram vectorizer. The
    resulting accuracy is slightly worse than the accuracy of the classifier that
    uses a unigram vectorizer from the previous section. There are several possible
    reasons for this. One is that the vectors are now much longer and still mostly
    zeroes. The other is that we can actually see that not all reviews are in English,
    so it is hard for the classifier to generalize the incoming data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use trigrams, quadrigrams, and so on in the vectorizer by providing the
    corresponding tuple to the `ngram_range` argument. The downside of this is the
    ever-expanding vocabulary and the growth of sentence vectors, since each sentence
    vector has to have an entry for each word in the input vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to represent character *n*-grams using the `CountVectorizer`
    class. In this case, you would count the occurrence of character sequences instead
    of word sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Representing texts with TF-IDF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can go one step further and use the TF-IDF algorithm to count words and *n*-grams
    in incoming documents. **TF-IDF** stands for **term frequency-inverse document
    frequency** and gives more weight to words that are unique to a document than
    to words that are frequent but repeated throughout most documents. This allows
    us to give more weight to words uniquely characteristic of particular documents.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will use a different type of vectorizer that can apply the
    TF-IDF algorithm to the input text and build a small classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the `TfidfVectorizer` class from the `sklearn` package. The features
    of the `TfidfVectorizer` class should be familiar from the two previous recipes,
    *Putting documents into a bag of words* and *Constructing an N-gram model*. We
    will again use the Rotten Tomatoes review dataset from Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to build and use the TF-IDF vectorizer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the small classifier notebook and import the **TfidfVectorizer** class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the training and test dataframes using the **load_train_test_dataset_pd()**
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the vectorizer and fit it on the training text. We will use the **max_df**
    parameter to exclude stop words – in this case, words that are more frequent than
    300:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To make sure the result makes sense, we will print the vectorizer vocabulary
    and its length. Since we are just using unigrams, the size of the vocabulary should
    be the same as the one in the bag-of-words recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should be as follows. The length of the vocabulary should be the
    same as the one we get in the bag-of-words recipe, since we are not using *n*-grams:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s take the first review in the test dataframe and vectorizer it. We
    then print the dense vector. To learn more about the difference between sparse
    and dense vectors, see the *Putting documents into a bag of words* recipe. Note
    that the values in the vector are now floats and not integers. This is because
    the individual values are now ratios and not counts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s train the classifier. We can see that the scores are slightly higher
    than those for a bag-of-words classifier, both the unigram and *n*-gram versions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The printout of the test scores will look similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `TfidfVectorizer` class works almost exactly like the `CountVectorizer`
    class, differing only in the way the **word frequencies** are calculated, so most
    of the steps should be familiar. Word frequencies are calculated as follows. For
    each word, the overall frequency is a product of the **term frequency** and the
    **inverse document frequency**. Term frequency is the number of times a word occurs
    in the document. Inverse document frequency is the total number of documents divided
    by the number of documents where the word occurs. Usually, these frequencies are
    logarithmically scaled.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is done using the following formulas:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>T</mi><mi>F</mi><mo>=</mo><mfrac><mrow><mi>N</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>m</mi><mi>a</mi><mi>p</mi><mi>p</mi><mi>e</mi><mi>a</mi><mi>r</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi></mrow><mrow><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>w</mi><mi>o</mi><mi>r</mi><mi>d</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi></mrow></mfrac></mrow></mrow></math>](img/1.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>I</mi><mi>D</mi><mi>F</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>s</mi></mrow><mrow><mi>N</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>s</mi><mi>w</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>e</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>m</mi><mi>o</mi><mi>c</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>s</mi></mrow></mfrac></mrow></mrow></math>](img/2.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>T</mml:mi><mml:mi>F</mml:mi><mml:mo>-</mml:mo><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mi>F</mml:mi><mml:mi>*</mml:mi><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mi>F</mml:mi></mml:math>](img/3.png)'
  prefs: []
  type: TYPE_IMG
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can build `TfidfVectorizer` and use `[t, h, e, w, o, m, a, n, th, he, wo,
    om, ma, an, the, wom, oma, man]` set. In some experimental settings, models based
    on character *n*-grams perform better than word-based *n*-gram models.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the small Sherlock Holmes text file, `sherlock_holmes_1.txt`, found
    at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt),
    and the same class, `TfidfVectorizer`. We will not need a tokenizer function or
    a stop-word list, since the unit of analysis is the character and not the word.
    The steps to create the vectorizer and analyze a sentence are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new vectorizer object that uses the **char_wb** analyzer, and then
    fit it on the training text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the vectorizer vocabulary and its length:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The partial result will look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the **vectorize** method using the new vectorizer, and then create the
    training and test data. Train the classifier and then test it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can find out more about term weighting at [https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information about **TfidfVectorizer**, see [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using word embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will switch gears and learn how to represent *words* using
    word embeddings, which are powerful because they are a result of training a neural
    network that predicts a word from all other words in the sentence. Embeddings
    are also vectors, but usually of a much smaller size, 200 or 300\. The resulting
    vector embeddings are similar for words that occur in similar contexts. Similarity
    is usually measured by calculating the cosine of the angle between two vectors
    in the hyperplane, with 200 or 300 dimensions. We will use the embeddings to show
    these similarities.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will use a pretrained `word2vec` model, which can be found
    at [https://github.com/mmihaltz/word2vec-GoogleNews-vectors](https://github.com/mmihaltz/word2vec-GoogleNews-vectors).
    Download the model and unzip it in the data directory. You should now have a file
    with the `…/``data/GoogleNews-vectors-negative300.bin.gz` path.
  prefs: []
  type: TYPE_NORMAL
- en: We will also use the `gensim` package to load and use the model. It should be
    installed within the `poetry` environment.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.5_word_embeddings.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.5_word_embeddings.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will load the model, demonstrate some features of the `gensim` package,
    and then compute a sentence vector using the word embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the simple classifier file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the **gensim** package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the pretrained model. If you get an error at this step, make sure you
    have downloaded the model in the **data** directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using the pretrained model, we can now load individual word vectors. Here,
    we load the word vector for the word *king*. We have to lowercase it, since all
    the words in the model are lowercase. The result is a long vector that represents
    this word in the **word2vec** model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also get words that are most similar to a given word. For example, let’s
    print out the words most similar to *apple* and *tomato*. The output prints out
    the words that are the most similar (i.e., occur in similar contexts) and the
    similarity score. The score is the cosine distance between a pair of vectors –
    in this case, representing a pair of words. The larger the score, the more similar
    the two words. The results make sense, since the words most similar to *apple*
    are mostly fruits, and the words most similar to *tomato* are mostly vegetables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the next two steps, we compute a sentence vector by averaging all the word
    vectors in the sentence. One of the challenges of this method is representing
    words that are not present in the model, and here, we simply skip such words.
    Let’s define a function that will take a sentence and a model and return a list
    of the sentence word vectors. Words that are not present in the model will return
    **KeyError**, and in such a case, we catch the error and continue:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now define a function that will take the word vector list and compute
    the sentence vector. In order to compute the average, we represent the matrix
    as a **numpy** array and use the **numpy** **mean** function to get the average
    vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Averaging the word vectors to get the sentence vector is only one way of approaching
    this task and is not without its problems. One other alternative is to train a
    **doc2vec** model, where sentences, paragraphs, and whole documents can all be
    units instead of words.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now test the average word embedding as a vectorizer. Our vectorizer
    takes in a string input, gets the word vectors for each word, and then returns
    the sentence vector that we compute in the **get_sentence_vector** function. We
    then load the training and test data and create the datasets. We train the logistic
    regression classifier and test it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see that the scores are much lower than those in previous sections.
    There might be several reasons for this; one of them is that the `word2vec` model
    is English-only, and the data is multilingual. As an exercise, you can write a
    script to filter English-only reviews and see whether that improves the score:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are some other fun things that `gensim` can do with a pretrained model.
    For example, it can find an outlier word from a list of words and find the word
    that is most similar to the given word from a list. Let’s look at these:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compile a list of words with one that doesn’t match, apply the **doesnt_match**
    function to the list, and print the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, let’s find a word that’s most similar to another word.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many other pretrained models available, including some in other languages;
    see [http://vectors.nlpl.eu/repository/](http://vectors.nlpl.eu/repository/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some pretrained models include part-of-speech information, which can be helpful
    when disambiguating words. These models concatenate words with their `cat_NOUN`),
    so keep that in mind when using them.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To learn more about the theory behind **word2vec**, you can start here: [https://jalammar.github.io/illustrated-word2vec/](https://jalammar.github.io/illustrated-word2vec/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training your own embeddings model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can now train our own `word2vec` model on a corpus. This model is a neural
    network that predicts a word when given a sentence with words blanked out. The
    byproduct of the neural network training is the vector representation for each
    word in the training vocabulary. For this task, we will continue using the Rotten
    Tomatoes reviews. The dataset is not very large, so the results are not as good
    as they could be with a larger collection.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the `gensim` package for this task. It should be installed as part
    of the `poetry` environment.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will create the dataset and then train the model on the data. We will then
    test how it performs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary packages and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the training data and check its length:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the **RottenTomatoesCorpus** class. The **word2vec** training algorithm
    requires a class with a defined **__iter__** function that allows you to iterate
    through the data, so that is why we need this class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an instance of **RottenTomatoesCorpus** using the loaded training dataset.
    Since **word2vec** models are trained on text only (they are self-supervised models),
    we don’t need the review score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize the **word2vec** model, train it, and then save
    it to disk. The only required argument is the list of words; some of the other
    important ones are **min_count**, **size**, **window**, and **workers**. The **min_count**
    parameter refers to the minimum number of times a word has to occur in the training
    corpus, the default being 5\. The **size** parameter sets the size of the word
    vector. **window** restricts the maximum number of words between the predicted
    and current words in a sentence. **workers** refers to the number of working threads;
    the more there are, the quicker the training will proceed. When training the model,
    the **epoch** parameter will determine the number of training iterations the model
    will go through. After initializing the model object, we train it on our corpus
    for 100 epochs and, finally, save it to disk:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get 10 words similar to the word *movie*. The words *sequels* and *film* make
    sense with this word; the rest are not that related. This is due to the small
    size of the training corpus. The words you get will be different, since the results
    are different every time the model is trained:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is a possible output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are tools to evaluate a `word2vec` model, although its creation is unsupervised.
    `gensim` comes with a file that lists word analogies, such as what *Athens* is
    to *Greece* being the same as what *Moscow* is to *Russia*. The `evaluate_word_analogies`
    function runs the analogies through the model and calculates how many were correct.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the **evaluate_word_analogies** function to evaluate our trained model.
    We need the **analogies** file, which is available in the book GitHub repository
    at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/questions-words.txt](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/questions-words.txt)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should be similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now evaluate the pretrained model. These commands might take longer to
    run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should be similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We use the **evaluate_word_analogies** function differently in the pretrained
    model and our model case because they are different types. With the pretrained
    model, we just load the vectors (a **KeyedVectors** class, where each word, represented
    by a key, is mapped to a vector), and our model is a full **word2vec** model object.
    We can check the types by using these commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The pretrained model was trained on a much larger corpus and, predictably, performs
    better. You can also construct your own evaluation file with analogies that your
    data requires.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Make sure your evaluation is based on the type of data that you are going to
    use in your application; otherwise, you risk having misleading evaluation results.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is an additional way of evaluating model performance, by comparing the
    similarity between word pairs assigned by a model to the human-assigned judgments.
    You can do this by using the `evaluate_word_pairs` function. See more at [https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_pairs](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_pairs).
  prefs: []
  type: TYPE_NORMAL
- en: Using BERT and OpenAI embeddings instead of word embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instead of word embeddings, we can use **Bidirectional Encoder Representations
    from Transformer** (**BERT**) embeddings. A BERT model, like word embeddings,
    is a pretrained model and gives a vector representation, but it takes context
    into account and can represent a whole sentence instead of individual words.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, we can use the Hugging Face `sentence_transformers` package
    to represent sentences as vectors. We need `PyTorch`, which is installed as part
    of the `poetry` environment.
  prefs: []
  type: TYPE_NORMAL
- en: To get the vectors, we will use the `all-MiniLM-L6-v2` model for this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: We can also use the embeddings from OpenAI that come from their **large language**
    **models** (**LLMs**).
  prefs: []
  type: TYPE_NORMAL
- en: To use the OpenAI embeddings, you will need to create an account and get an
    API key from OpenAI. You can create an account at [https://platform.openai.com/signup](https://platform.openai.com/signup).
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.6_train_own_word2vec.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.6_train_own_word2vec.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hugging Face code makes using BERT very easy. The first time the code runs,
    it will download the necessary model, which might take some time. After the download,
    it’s just a matter of encoding the sentences using the model. We will test the
    simple classifier with these embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the simple classifier notebook to import its functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the **SentenceTransformer** class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Load the sentence transformer model, retrieve the embedding of the sentence
    *I love jazz*, and print it out.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As we can see, it is a vector similar to the word embeddings vector from the
    previous recipe:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we can test our classifier using the BERT embeddings. First, let’s define
    a function that will return a sentence vector. This function takes the input text
    and a model. It then uses the model to encode the text and returns the resulting
    embedding. We need to pass in the text inside of a list to the **encode** method,
    as it expects an iterable. Similarly, we return the first element of the result,
    since it returns a list of embeddings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we define the **vectorize** function, create the training and test data
    using the **load_train_test_dataset_pd** function we created in the *Creating
    a simple classifier* recipe, train the classifier, and test it. We will time the
    dataset creation step, hence the inclusion of the **time** package commands. We
    see that it takes about 11 seconds to vectorize the whole dataset (about 85,000
    entries). We then train the model and test it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result is our best one yet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can now use the OpenAI embeddings to see how they perform:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the **openai** package and assign the API key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Assign the model we are going to use, the sentence and create the embedding.
    The model that we will use is specifically an embeddings model, so it returns
    an embeddings vector for a text input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The partial result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now test our classifier using the OpenAI embeddings. This is the function
    that will return a sentence vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, define the **vectorize** function, create the training and test data,
    train the classifier, and test it. We will time the vectorizing step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the result is quite poor in terms of the score, and it takes more
    than 10 minutes to process the whole dataset. Here, we only use the LLM embeddings
    and then train a logistic regression classifier on those embeddings. This is different
    from using the LLM itself to do the classification.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more pretrained models, see [https://www.sbert.net/docs/pretrained_models.html](https://www.sbert.net/docs/pretrained_models.html).
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval augmented generation (RAG)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will see vector embeddings in action. RAG is a popular method
    of working with LLMs. Since these models are pretrained on widely available internet
    data, they do not have access to our personal data, and we cannot use the model
    as it is to ask questions about it. A way to overcome this is to use vector embeddings
    to represent our data. Then, we can compute cosine similarity between our data
    and the question and include the most similar piece of our data, together with
    the question – hence the name “retrieval augmented generation,” since we first
    retrieve relevant data by using cosine similarity and then generate text using
    the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use an IMDB dataset from **Kaggle**, which can be downloaded from [https://www.kaggle.com/PromptCloudHQ/imdb-data](https://www.kaggle.com/PromptCloudHQ/imdb-data)
    and is also included in the book GitHub repo at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/IMDB-Movie-Data.csv](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/IMDB-Movie-Data.csv).
    Download the dataset and unzip the CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: We will also use the OpenAI embeddings, as well as the `llama_index` package,
    which is included within the `poetry` environment.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.9_vector_search.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.9_vector_search.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will load the IMDB dataset and then create a vector store, using its first
    10 entries. We will then use the `llama_index` package to query the vector store:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the file **utilities** notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the necessary classes and packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read in the CSV data. We will skip the first header row of the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we use the first 10 rows of the data we just read in to first
    create a list of **Document** objects, and then a **VectorStoreIndex** object
    with these **Document** objects. An index is an object used for search, where
    each record contains certain information. A vector store index stores metadata
    as well as the vector representation of each record. For each movie, we assign
    the description as the text that will be embedded and the rest as metadata. We
    print out **document** objects and can see that a unique ID has been assigned
    to each:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The partial output will look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the query engine from the index we just created. The query engine will
    allow us to send in questions about the documents loaded in the index:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the engine to answer a question:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
