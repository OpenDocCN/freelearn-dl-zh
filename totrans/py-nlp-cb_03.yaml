- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Representing Text – Capturing Semantics
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表示文本——捕捉语义
- en: Representing the meaning of words, phrases, and sentences in a form that’s understandable
    to computers is one of the pillars of NLP processing. Machine learning, for example,
    represents each data point as a list of numbers (a fixed-size vector), and we
    are faced with the question of how to turn words and sentences into these vectors.
    Most NLP tasks start by representing the text in some numeric form, and in this
    chapter, we show several ways to do that.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 将单词、短语和句子的意义表示成计算机能理解的形式是NLP处理的基础之一。例如，机器学习将每个数据点表示为一个数字列表（固定大小的向量），我们面临的问题是如何将单词和句子转换为这些向量。大多数NLP任务首先将文本表示成某种数值形式，在本章中，我们将展示几种实现这一目标的方法。
- en: First, we will create a simple classifier to demonstrate the effectiveness of
    each method of encoding, and then we will use it to test the different encoding
    methods. We will also learn how to turn phrases such as *fried chicken* into vectors
    – that is, how to train a `word2vec` model for phrases. Finally, we will see how
    to use vector-based search.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个简单的分类器来展示每种编码方法的有效性，然后我们将用它来测试不同的编码方法。我们还将学习如何将诸如“炸鸡”之类的短语转换为向量——也就是说，如何训练短语用`word2vec`模型。最后，我们将看到如何使用基于向量的搜索。
- en: For a theoretical background on some of the concepts discussed in this section,
    refer to *Building Machine Learning Systems with Python* by Coelho et al. This
    book will explain the basics of building a machine learning project, such as training
    and test sets, as well as metrics used to evaluate such projects, including precision,
    recall, F1, and accuracy.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本节中讨论的一些概念的理论背景，请参阅Coelho等人所著的《用Python构建机器学习系统》。这本书将解释构建机器学习项目的基础，例如训练集和测试集，以及用于评估此类项目的指标，包括精确度、召回率、F1和准确率。
- en: 'Here are the recipes that are covered in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下食谱：
- en: Creating a simple classifier
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个简单的分类器
- en: Putting documents into a bag of words
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文档放入词袋中
- en: Constructing an *N*-gram model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个*N*-gram模型
- en: Representing texts with TF-IDF
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TF-IDF表示文本
- en: Using word embeddings
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用词嵌入
- en: Training your own embeddings model
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练自己的嵌入模型
- en: Using BERT and OpenAI embeddings instead of word embeddings
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用BERT和OpenAI嵌入而不是词嵌入
- en: Using **retrieval augmented** **generation** (**RAG**)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**检索增强****生成**（**RAG**）
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code for this chapter is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter03](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter03).
    Packages that are required for this chapter should be installed automatically
    via the `poetry` environment.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter03](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter03)。本章所需的包应通过`poetry`环境自动安装。
- en: 'In addition, we will use models and datasets located at the following URLs.
    The Google `word2vec` model is a model that represents words as vectors, and the
    IMDB dataset contains movie titles, genres, and descriptions. Download them into
    the `data` folder inside the `root` directory:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将使用以下URL中定位的模型和数据集。谷歌`word2vec`模型是一个将单词表示为向量的模型，IMDB数据集包含电影标题、类型和描述。将它们下载到`root`目录下的`data`文件夹中：
- en: '**The Google** **word2vec** **model**: [https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**谷歌****word2vec****模型**：[https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g)'
- en: '**The IMDB movie dataset**: [https://github.com/venusanvi/imdb-movies/blob/main/IMDB-Movie-Data.csv](https://github.com/venusanvi/imdb-movies/blob/main/IMDB-Movie-Data.csv)
    (also available in the book’s GitHub repo)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IMDB电影数据集**：[https://github.com/venusanvi/imdb-movies/blob/main/IMDB-Movie-Data.csv](https://github.com/venusanvi/imdb-movies/blob/main/IMDB-Movie-Data.csv)（本书的GitHub仓库中也有提供）'
- en: In addition to the preceding files, we will use various functions from a simple
    classifier that we will create in the first recipe. This file is available at
    [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/util_simple_classifier.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/util_simple_classifier.ipynb).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面的文件外，我们还将使用我们将在第一个菜谱中创建的简单分类器中的各种函数。此文件可在[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/util_simple_classifier.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/util_simple_classifier.ipynb)找到。
- en: Creating a simple classifier
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个简单的分类器
- en: The reason why we need to represent text as vectors is to make it into a computer-readable
    form. Computers can’t understand words but are good at manipulating numbers. One
    of the main NLP tasks is the classification of texts, and we are going to create
    a classifier for movie reviews. We will use the same classifier code but with
    different methods of creating vectors from text.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将文本表示为向量的原因是为了将其转换为计算机可读的形式。计算机不能理解单词，但擅长处理数字。NLP的主要任务之一是文本分类，我们将创建一个用于电影评论的分类器。我们将使用相同的分类器代码，但使用不同的从文本创建向量的方法。
- en: In this section, we will create the classifier that will assign either negative
    or positive sentiment to *Rotten Tomatoes* reviews, a dataset available through
    Hugging Face, a large repository of open source models and datasets. We will then
    use a baseline method, where we encode the text by counting the number of different
    parts of speech present in it (verbs, nouns, proper nouns, adjectives, adverbs,
    auxiliary verbs, pronouns, numbers, and punctuation).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将创建一个分类器，该分类器将为*烂番茄*评论分配负面或正面情绪，这是一个通过Hugging Face提供的、包含大量开源模型和数据集的数据库。然后我们将使用基线方法，通过计算文本中存在的不同词性数量（动词、名词、专有名词、形容词、副词、助动词、代词、数字和标点符号）来编码文本。
- en: By the end of this recipe, we will have created a separate file with functions
    that create the dataset and train the classifier. We will use this file throughout
    the chapter to test different encoding methods.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 到本菜谱结束时，我们将创建一个单独的文件，其中包含创建数据集和训练分类器的函数。我们将使用此文件在本章中测试不同的编码方法。
- en: Getting ready
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will create a simple classifier for movie reviews. It will
    be a `sklearn` package.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本菜谱中，我们将创建一个简单的电影评论分类器。它将是一个`sklearn`包。
- en: How to do it…
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We will load the Rotten Tomatoes dataset from Hugging Face. We will use just
    part of the dataset so that the training time is not very long:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从Hugging Face加载Rotten Tomatoes数据集。我们将只使用数据集的一部分，以便训练时间不会很长：
- en: 'Import the file and language **util** notebooks:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入文件和语言**util**笔记本：
- en: '[PRE0]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Load the train and test datasets from Hugging Face (the **datasets** package).
    For both the training and test sets, we will select the first and last 15% of
    the data instead of loading the full datasets. The full dataset is large, and
    it takes a long time to train the model:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Hugging Face（**datasets**包）加载训练和测试数据集。对于训练集和测试集，我们将选择数据的前15%和后15%，而不是加载完整的数据集。完整的数据集很大，训练模型需要很长时间：
- en: '[PRE1]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Print out the length of each dataset:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出每个数据集的长度：
- en: '[PRE2]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output should be as follows:'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应该是这样的：
- en: '[PRE3]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here, we create the **POS_vectorizer** class. This class has a method, **vectorize**,
    that processes the text and counts the number of verbs, nouns, proper nouns, adjectives,
    adverbs, auxiliary verbs, pronouns, numbers, and punctuation marks. The class
    needs a **spaCy** model to process the text. Each piece of text is turned into
    a vector of size 10\. The first element of the vector is the length of the text,
    and the other numbers indicate the number of words in the text of that particular
    part of speech:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们创建了**POS_vectorizer**类。这个类有一个名为**vectorize**的方法，它处理文本并计算动词、名词、专有名词、形容词、副词、助动词、代词、数字和标点符号的数量。该类需要一个**spaCy**模型来处理文本。每段文本被转换成大小为10的向量。向量的第一个元素是文本的长度，其他数字表示该特定词性的文本中的单词数量：
- en: '[PRE4]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, we can test out the **POS_vectorizer** class. We take the first review’s
    text to process and create the vectorizer using the small **spaCy** model. We
    then vectorize the text using the newly created class:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以测试**POS_vectorizer**类。我们取第一篇评论的文本进行处理，并使用小的**spaCy**模型创建向量器。然后我们使用新创建的类对文本进行向量化：
- en: '[PRE5]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s print the text and the vector:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们打印文本和向量：
- en: '[PRE6]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The result should look like this. We can see that the vector correctly counts
    the parts of speech. For example, there are five punctuation marks (two quotes,
    one comma, one dot, and one dash):'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果应该看起来像这样。我们可以看到，向量正确地计算了词性。例如，有五个标点符号（两个引号、一个逗号、一个句号和一个破折号）：
- en: '[PRE7]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We will now prepare the data for training our classifier. We first import the
    **pandas** and the **numpy** packages and then create two dataframes, one for
    training and the other one for testing. In each dataset, we create a new column
    called **vector** that contains the vector for this piece of text. We use the
    **apply** method to turn the text into vectors and store them in the new column.
    In this method, we pass in a lambda function that takes a piece of text and applies
    the **vectorize** method of the **POS_vectorizer** class to that piece of text.
    We then turn the vector and the label columns into **numpy** arrays to have the
    data in the right format for the classifier. We use the **np.stack** method for
    the vector, since it’s already a list, and the **to_numpy** method for the review
    labels, since they are just numbers:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将为训练我们的分类器准备数据。我们首先导入**pandas**和**numpy**包，然后创建两个数据框，一个用于训练，另一个用于测试。在每个数据集中，我们创建一个名为**vector**的新列，其中包含该文本的向量。我们使用**apply**方法将文本转换为向量并将它们存储在新列中。在此方法中，我们传递一个lambda函数，该函数接受一段文本并将其应用于**POS_vectorizer**类的**vectorize**方法。然后，我们将向量列和标签列转换为**numpy**数组，以便数据以正确的格式供分类器使用。我们使用**np.stack**方法对向量进行操作，因为它已经是一个列表，而使用**to_numpy**方法对评论标签进行操作，因为它们只是数字：
- en: '[PRE8]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we will train the classifier. We will choose the logistic regression algorithm,
    since it is one of the simplest algorithms, as well as one of the fastest. First,
    we import the **LogisticRegression** class and the **classification_report** methods
    from **sklearn**. Then, we create the **LogisticRegression** object, and finally,
    train it on the data from the previous step:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将训练分类器。我们将选择逻辑回归算法，因为它是最简单的算法之一，同时也是最快的算法之一。首先，我们从**sklearn**中导入**LogisticRegression**类和**classification_report**方法。然后，我们创建**LogisticRegression**对象，并最终在之前步骤中的数据上对其进行训练：
- en: '[PRE9]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can test the classifier on the test data by applying the **predict** method
    to the vectors in the test data and print out the classification report. We can
    see that the overall accuracy is low, slightly above chance. This is because the
    vector representation we used is very crude. We will use other vectors in the
    next sections and see how they affect the classifier results:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过将**predict**方法应用于测试数据中的向量并打印出分类报告来测试分类器。我们可以看到，整体准确率很低，略高于随机水平。这是因为我们使用的向量表示非常粗糙。在下一节中，我们将使用其他向量并看看它们如何影响分类器结果：
- en: '[PRE10]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output should be similar to this:'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应该类似于这个：
- en: '[PRE11]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: There’s more…
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We will now turn the preceding code into several functions so that we can only
    vary the vectorizer being used in the construction of the dataset. The resulting
    file is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/util_simple_classifier.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/util_simple_classifier.ipynb).
    The resulting code will look as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将前面的代码转换为几个函数，这样我们就可以只改变在构建数据集时使用的向量器。生成的文件位于[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/util_simple_classifier.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/util_simple_classifier.ipynb)。生成的代码将如下所示：
- en: 'Import the necessary packages:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE12]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Define the function that will create and return the training and test dataframes.
    It will create them from the Rotten Tomatoes dataset from Hugging Face:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，该函数将创建并返回训练和测试数据框。它将从Hugging Face的Rotten Tomatoes数据集中创建它们：
- en: '[PRE13]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This function takes the dataframes and the **vectorize** method and creates
    the **numpy** arrays for the training and test data. This will allow us to train
    the logistic regression classifier using the created vectors:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此函数接收数据框和**vectorize**方法，并为训练和测试数据创建**numpy**数组。这将使我们能够使用创建的向量来训练逻辑回归分类器：
- en: '[PRE14]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This function trains a logistic regression classifier on the given training
    data:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此函数在给定的训练数据上训练一个逻辑回归分类器：
- en: '[PRE15]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This final function takes in the test data and the trained classifier and prints
    out the classification report:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此最终函数接收测试数据和训练好的分类器，并打印出分类报告：
- en: '[PRE16]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In each succeeding section that demonstrates a new vectorizing method, we will
    use this file to pre-load the necessary functions to test the classification result.
    This will allow us to evaluate the different vectorizing methods. We will only
    vary the vectorizer while keeping the classifier the same. When the classifier
    performs better, it reflects how well the underlying vectorizer represents the
    text.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个展示新向量化方法的后续部分，我们将使用这个文件来预加载必要的函数以测试分类结果。这将使我们能够评估不同的向量化方法。我们将只改变向量化器，保持分类器不变。当分类器表现更好时，这反映了底层向量化器对文本的表示效果。
- en: Putting documents into a bag of words
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将文档放入词袋中
- en: A **bag of words** is the simplest way of representing a text. We treat our
    text as a collection of *documents*, where documents are anything from sentences
    to scientific articles to blog posts or whole books. Since we usually compare
    different documents to each other or use them in a larger context of other documents,
    we work with a collection of documents, not just a single document.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**词袋**是表示文本的最简单方式。我们将文本视为一组**文档**，其中文档可以是句子、科学文章、博客文章或整本书。由于我们通常将不同的文档相互比较或将它们用于其他文档的更大上下文中，所以我们处理的是文档集合，而不仅仅是一个单独的文档。'
- en: The bag of words method uses a “training” text that provides it with a list
    of words that it should consider. When encoding new sentences, it counts the number
    of occurrences each word makes in the document, and the final vector includes
    those counts for each word in the vocabulary. This representation can then be
    fed into a machine learning algorithm.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋方法使用一个“训练”文本，为它提供一个应该考虑的单词列表。在编码新句子时，它计算每个单词在文档中的出现次数，最终向量包括词汇表中每个单词的这些计数。这种表示可以随后输入到机器学习算法中。
- en: The reason this vectorizing method is called a *bag of words* is that it does
    not take into account the relationships of words between themselves and only counts
    the number of occurrences of each word. The decision on what represents a document
    lies with the engineer and, in many cases, will be obvious. For example, if you
    are working on classifying tweets that belong to a particular topic, a single
    tweet will be your document. If, conversely, you would like to find out which
    chapters of a book are most similar to a book you already have, then chapters
    are documents.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这种向量化的方法被称为“词袋”是因为它不考虑单词之间的相互关系，只计算每个单词出现的次数。关于什么代表一个文档的决定权在工程师手中，在许多情况下，这将是显而易见的。例如，如果你正在对属于特定主题的推文进行分类，那么一条单独的推文就是你的文档。相反，如果你想找出哪本书的章节与你已经拥有的书最相似，那么章节就是文档。
- en: In this recipe, we will create a bag of words for the Rotten Tomatoes reviews.
    Our documents will be the reviews. We then test the encoding using a bag of words
    by building a logistic regression classifier, using code from the previous recipe.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将为Rotten Tomatoes的评论创建一个词袋。我们的文档将是评论。然后，我们通过构建逻辑回归分类器并使用前一个菜谱中的代码来测试编码。
- en: Getting ready
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this recipe, we will use the `CountVectorizer` class from the `sklearn`
    package. It is included in the `poetry` environment. The `CountVectorizer` class
    is specifically designed to count the number of occurrences of each word in a
    text.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个菜谱，我们将使用来自`sklearn`包的`CountVectorizer`类。它包含在`poetry`环境中。`CountVectorizer`类专门设计用来计算文本中每个单词出现的次数。
- en: How to do it…
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Our code will take a set of documents – in this case, reviews – and represent
    them as a matrix of vectors. We will use the Rotten Tomatoes reviews dataset from
    Hugging Face for this task:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码将接受一组文档——在这个例子中，是评论——并将它们表示为一个向量矩阵。我们将使用来自Hugging Face的Rotten Tomatoes评论数据集来完成这项任务：
- en: 'Run the simple classifier **utility** file, and then import the **CountVectorizer**
    object and the **sys** package. We will need the **sys** package to change the
    printing options:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行简单的分类器**实用程序**文件，然后导入**CountVectorizer**对象和**sys**包。我们需要**sys**包来更改打印选项：
- en: '[PRE17]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Load the training and testing dataframes by using the function from the **util_simple_classifier.ipynb**
    file. We created this function in the previous recipe, *Creating a simple classifier*.
    The function loads 15% of the Rotten Tomatoes dataset into a **pandas** dataframe
    and randomizes its order. It might take a few minutes to run:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用来自**util_simple_classifier.ipynb**文件的函数来加载训练和测试数据框。我们在之前的菜谱中创建了此函数，即*创建简单分类器*。该函数将Rotten
    Tomatoes数据集的15%加载到**pandas**数据框中，并随机化其顺序。这可能需要几分钟才能运行：
- en: '[PRE18]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Create the vectorizer, fit it on the training data, and print out the resulting
    matrix. We will use the **max_df** parameter to specify which words should be
    used as stop words. In this case, we specify that words that appear in more than
    40% of the documents should be ignored when constructing the vectorizer. You should
    experiment and see exactly which value of **max_df** would suit your use case.
    We then fit the vectorizer on the **text** column of the **train_df** dataframe:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建向量器，将其拟合到训练数据上，并打印出结果矩阵。我们将使用**max_df**参数来指定哪些单词应作为停用词。在这种情况下，我们指定在构建向量器时，出现超过40%的文档中的单词应被忽略。你应该进行实验，看看**max_df**的确切值哪个适合你的用例。然后我们将向量器拟合到**train_df**数据框的**text**列：
- en: '[PRE19]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The resulting matrix is a `scipy.sparse._csr.csr_matrix` object, and the beginning
    of its printout looks like this. The format of a sparse matrix is `(row, column)
    value`. In our case, this means (the document index, word index) followed by the
    frequency. In our example, the first review, which is the first document, is document
    number `0`, and it contains words with indices `6578`, `4219`, and others. The
    frequencies of these words are `1` and `2`, respectively.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成的矩阵是一个`scipy.sparse._csr.csr_matrix`对象，其打印输出的开头如下。稀疏矩阵的格式是`(行, 列) 值`。在我们的例子中，这意味着（文档索引，单词索引）后面跟着频率。在我们的例子中，第一篇评论，即第一篇文档，是文档编号`0`，它包含索引为`6578`、`4219`等的单词。这些单词的频率分别是`1`和`2`。
- en: '[PRE20]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In most cases, we use a different format to represent vectors, a dense matrix
    that is easier to use in practice. Instead of specifying rows and columns with
    numbers, they are inferred from the position of the value. We will now create
    a dense matrix and print it:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在大多数情况下，我们使用不同的格式来表示向量，这是一种在实际中更容易使用的密集矩阵。我们不是用数字指定行和列，而是从值的位位置推断它们。现在我们将创建一个密集矩阵并打印它：
- en: '[PRE21]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The resulting matrix is a NumPy matrix object, where each review is a vector.
    You see that most values in the matrix are zeroes, as expected, since each review
    only uses a handful of words, while the vector collects counts for each word in
    the vocabulary, or each word in all of the reviews. Any words that are not in
    the vectorizer’s vocabulary will not be included in the vector:'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成的矩阵是一个NumPy矩阵对象，其中每个评论都是一个向量。你可以看到矩阵中的大多数值都是零，正如预期的那样，因为每个评论只使用了一小部分单词，而向量收集了词汇表中的每个单词或所有评论中的每个单词的计数。任何不在向量器词汇表中的单词将不会包含在向量中：
- en: '[PRE22]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can see all the words used in the document set and the length of the vocabulary.
    This can be used as a sanity check and to see whether there are any irregularities
    in the vocabulary:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以看到文档集中使用的所有单词和词汇表的大小。这可以用作合理性检查，并查看词汇表中是否存在任何不规则性：
- en: '[PRE23]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The result will be as follows. If you want to see the full, non-truncated list,
    use the `set_printoptions` function used in *step 8*:'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下。如果你想查看完整的、非截断的列表，请使用在*步骤 8*中使用的`set_printoptions`函数：
- en: '[PRE24]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can also see all the stop words used by the vectorizer:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以看到向量器使用的所有停用词：
- en: '[PRE25]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The result is three words, `and`, `the`, and `of`, that appear in more than
    40% of reviews:'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果是三个单词，`and`、`the`和`of`，它们出现在超过40%的评论中：
- en: '[PRE26]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We can now also use the **CountVectorizer** object to represent new reviews
    that were not in the original document set. This is done when we have a trained
    model and want to test it on new, unseen samples. We will use the first review
    in the test dataset. To get the first review in the test set, we will use the
    **pandas** **iat** function.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们也可以使用**CountVectorizer**对象来表示原始文档集中未出现的新评论。这是在我们有一个训练好的模型并想在新的、未见过的样本上测试它时进行的。我们将使用测试数据集中的第一篇评论。为了获取测试集中的第一篇评论，我们将使用**pandas**的**iat**函数。
- en: '[PRE27]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The first review looks as follows:'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一次审查看起来如下：
- en: '[PRE28]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, we will create both a sparse and a dense vector from the first review.
    The **transform** method of the vectorizer expects a list of strings, so we will
    create a list. We also set the **print** option to print out the whole vector
    instead of just part of it:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将从第一篇评论创建一个稀疏和一个密集向量。向量器的**transform**方法期望一个字符串列表，所以我们将创建一个列表。我们还设置了**print**选项来打印整个向量而不是只打印部分：
- en: '[PRE29]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The dense vector is very long and is mostly zeroes, as expected:'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 稠密向量非常长，大部分是零，正如预期的那样：
- en: '[PRE30]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We can use a different method to calculate stop words. Here, stop words are
    calculated by setting an absolute threshold on the word frequency. In this case,
    we use all words whose frequency is lower than 300 across documents. You can see
    that the stop-word list is now larger.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用不同的方法来计算停用词。在这里，停用词是通过在单词频率上设置绝对阈值来计算的。在这种情况下，我们使用所有在文档中频率低于300的单词。你可以看到，停用词列表现在更大了。
- en: '[PRE31]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The result will be as follows:'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE32]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, we can provide our own list of stop words to the vectorizer. These
    words will be ignored by the vectorizer and not represented in the vector. This
    is useful if you have very specific words you would like to ignore:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以向矢量化器提供自己的停用词列表。这些单词将被矢量化器忽略，不会在矢量化中表示。如果你有非常具体的单词想要忽略，这很有用：
- en: '[PRE33]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We will now test the effect of this bag-of-words vectorizer on the simple classifier,
    using the functions we defined in the previous recipe. First, we create the vectorizer,
    specifying to use only words that appear in less than 80% of the documents. Then,
    we load the training and test dataframes. We fit the vectorizer on the training
    set reviews. We create a vectorize function using the vectorizer and pass it on
    to the **create_train_test_data** function, along with the training and test data
    frames. We then train the classifier and test it on the testing data. We can see
    that this vectorizing method gives us much better results than the simple part-of-speech
    count vector we used in the previous section:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用我们在上一个配方中定义的函数测试这个词袋矢量化器对简单分类器的影响。首先，我们创建矢量化器，指定只使用在不到80%的文档中出现的单词。然后，我们加载训练和测试数据框。我们在训练集评论上拟合矢量化器。我们使用矢量化器创建一个矢量化函数，并将其传递给**create_train_test_data**函数，同时传递训练和测试数据框。然后我们训练分类器并在测试数据上测试它。我们可以看到，这种方法给我们带来的结果比我们在上一节中使用的基本词性计数矢量化器要好得多：
- en: '[PRE34]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The result will be similar to this:'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将类似于以下内容：
- en: '[PRE35]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Constructing an N-gram model
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建N-gram模型
- en: Representing a document as a bag of words is useful, but semantics is about
    more than just words in isolation. To capture word combinations, an **n-gram model**
    is useful. Its vocabulary consists of not just words but also word sequences,
    or *n*-grams.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 将文档表示为词袋是有用的，但语义不仅仅是关于孤立单词。为了捕捉词组合，使用**n-gram模型**是有用的。其词汇不仅包括单词，还包括单词序列，或*n*-gram。
- en: We will build a **bigram model** in this recipe, where bigrams are sequences
    of two words.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将构建一个**bigram模型**，其中bigram是两个单词的序列。
- en: Getting ready
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: The `CountVectorizer` class is very versatile and allows us to construct *n*-gram
    models. We will use it in this recipe and test it with a simple classifier.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountVectorizer`类非常灵活，允许我们构建*n*-gram模型。我们将在此配方中使用它，并用简单的分类器进行测试。'
- en: In this recipe, I make comparisons of the code and its results to the ones in
    the *Putting documents into a bag of words* recipe, since the two are very similar,
    but they have a few differing characteristics.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我将代码及其结果与*将文档放入词袋*配方中的结果进行比较，因为这两个配方非常相似，但它们有一些不同的特性。
- en: How to do it…
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Run the simple classifier notebook and import the **CountVectorizer** class:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行简单的分类器笔记本并导入**CountVectorizer**类：
- en: '[PRE36]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Create the training and test dataframes using code from the **util_simple_classifier.ipynb**
    notebook:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用来自**util_simple_classifier.ipynb**笔记本的代码创建训练和测试数据框：
- en: '[PRE37]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Create a new vectorizer class. In this case, we will use the **ngram_range**
    argument. The **CountVectorizer** class, when the **ngram_range** argument is
    set, counts not only individual words but also word combinations, where the number
    of words in the combinations depends on the numbers provided to the **ngram_range**
    argument. We provided **ngram_range=(1,2)** as the argument, which means that
    the number of words in the combinations ranges from 1 to 2, so unigrams and bigrams
    are counted:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的矢量化器类。在这种情况下，我们将使用**ngram_range**参数。当设置**ngram_range**参数时，**CountVectorizer**类不仅计算单个单词，还计算单词组合，组合中单词的数量取决于提供给**ngram_range**参数的数字。我们提供了**ngram_range=(1,2)**作为参数，这意味着组合中单词的数量范围从1到2，因此计算单语和双语：
- en: '[PRE38]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Print the vocabulary of the vectorizer and its length. As you can see, the
    length of the vocabulary is much larger than the length of the unigram vectorizer,
    since we use two-word combinations in addition to single words:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印矢量化器的词汇及其长度。正如你所见，词汇的长度比单语矢量化器的长度大得多，因为我们除了单字外还使用了双字组合：
- en: '[PRE39]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The result should look like this:'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果应该看起来像这样：
- en: '[PRE40]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now, we take the first review in the testing dataframe and get its dense vector.
    The result looks very similar to the vector output in the *Putting documents into
    a bag of words* recipe, with the only difference that now the output is longer,
    as it includes not just individual words but also bigrams, or sequences of two
    words:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们从测试数据框中取出第一条评论并获取其密集向量。结果看起来与 *将文档放入词袋* 菜谱中的向量输出非常相似，唯一的区别是现在的输出更长，因为它不仅包括单个单词，还包括二元组，即两个单词的序列：
- en: '[PRE41]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The printout looks like this:'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 打印输出看起来像这样：
- en: '[PRE42]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Finally, we train a simple classifier using the new bigram vectorizer. The
    resulting accuracy is slightly worse than the accuracy of the classifier that
    uses a unigram vectorizer from the previous section. There are several possible
    reasons for this. One is that the vectors are now much longer and still mostly
    zeroes. The other is that we can actually see that not all reviews are in English,
    so it is hard for the classifier to generalize the incoming data:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用新的二元向量器训练一个简单的分类器。其结果准确率略低于上一节中使用单语元向量器的分类器的准确率。这可能有几个原因。一个是现在的向量要长得多，而且大部分是零。另一个原因是我们可以看到并非所有评论都是英文的，因此分类器很难泛化输入数据：
- en: '[PRE43]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output will be as follows:'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE44]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: There’s more…
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: We can use trigrams, quadrigrams, and so on in the vectorizer by providing the
    corresponding tuple to the `ngram_range` argument. The downside of this is the
    ever-expanding vocabulary and the growth of sentence vectors, since each sentence
    vector has to have an entry for each word in the input vocabulary.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过提供相应的元组给 `ngram_range` 参数来在向量器中使用三元组、四元组等。这样做的不利之处是词汇表不断扩展，句子向量也在增长，因为每个句子向量都必须为输入词汇表中的每个单词提供一个条目。
- en: It is also possible to represent character *n*-grams using the `CountVectorizer`
    class. In this case, you would count the occurrence of character sequences instead
    of word sequences.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用 `CountVectorizer` 类来表示字符 *n*-gram。在这种情况下，你会计算字符序列的出现次数而不是单词序列。
- en: Representing texts with TF-IDF
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TF-IDF 表示文本
- en: We can go one step further and use the TF-IDF algorithm to count words and *n*-grams
    in incoming documents. **TF-IDF** stands for **term frequency-inverse document
    frequency** and gives more weight to words that are unique to a document than
    to words that are frequent but repeated throughout most documents. This allows
    us to give more weight to words uniquely characteristic of particular documents.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以更进一步，使用 TF-IDF 算法来计算传入文档中的单词和 *n*-gram。**TF-IDF** 代表 **词频-逆文档频率**，它给独特于文档的单词比在整个文档中频繁重复的单词更多的权重。这允许我们给特定文档的独特特征词更多的权重。
- en: In this recipe, we will use a different type of vectorizer that can apply the
    TF-IDF algorithm to the input text and build a small classifier.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用一种不同类型的向量器，该向量器可以将 TF-IDF 算法应用于输入文本并构建一个小型分类器。
- en: Getting ready
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will use the `TfidfVectorizer` class from the `sklearn` package. The features
    of the `TfidfVectorizer` class should be familiar from the two previous recipes,
    *Putting documents into a bag of words* and *Constructing an N-gram model*. We
    will again use the Rotten Tomatoes review dataset from Hugging Face.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自 `sklearn` 包的 `TfidfVectorizer` 类。`TfidfVectorizer` 类的特征应该与之前的两个菜谱 *将文档放入词袋*
    和 *构建 N-gram 模型* 熟悉。我们将再次使用来自 Hugging Face 的 Rotten Tomatoes 评论数据集。
- en: How to do it…
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Here are the steps to build and use the TF-IDF vectorizer:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是构建和使用 TF-IDF 向量器的步骤：
- en: 'Run the small classifier notebook and import the **TfidfVectorizer** class:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行小分类器笔记本并导入 **TfidfVectorizer** 类：
- en: '[PRE45]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Create the training and test dataframes using the **load_train_test_dataset_pd()**
    function:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 **load_train_test_dataset_pd()** 函数创建训练和测试数据框：
- en: '[PRE46]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Create the vectorizer and fit it on the training text. We will use the **max_df**
    parameter to exclude stop words – in this case, words that are more frequent than
    300:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建向量器并在训练文本上拟合。我们将使用 **max_df** 参数来排除停用词——在这种情况下，是指比 300 更频繁的单词：
- en: '[PRE47]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'To make sure the result makes sense, we will print the vectorizer vocabulary
    and its length. Since we are just using unigrams, the size of the vocabulary should
    be the same as the one in the bag-of-words recipe:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了确保结果有意义，我们将打印向量器的词汇表及其长度。由于我们只是使用单语元，词汇表的大小应该与词袋菜谱中的相同：
- en: '[PRE48]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The result should be as follows. The length of the vocabulary should be the
    same as the one we get in the bag-of-words recipe, since we are not using *n*-grams:'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果应该是这样的。词汇表长度应该与我们在词袋配方中得到的相同，因为我们没有使用*n*-grams：
- en: '[PRE49]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now, let’s take the first review in the test dataframe and vectorizer it. We
    then print the dense vector. To learn more about the difference between sparse
    and dense vectors, see the *Putting documents into a bag of words* recipe. Note
    that the values in the vector are now floats and not integers. This is because
    the individual values are now ratios and not counts:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们取测试数据框中的第一个审查并对其进行向量化。然后我们打印密集向量。要了解更多关于稀疏向量和密集向量之间的区别，请参阅*将文档放入词袋*配方。请注意，向量中的值现在是浮点数而不是整数。这是因为单个值现在是比率而不是计数：
- en: '[PRE50]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The result should be as follows:'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果应该是这样的：
- en: '[PRE51]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now, let’s train the classifier. We can see that the scores are slightly higher
    than those for a bag-of-words classifier, both the unigram and *n*-gram versions:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们训练分类器。我们可以看到，分数略高于词袋分类器的分数，无论是单词还是*n*-gram版本：
- en: '[PRE52]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The printout of the test scores will look similar to this:'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 测试分数的打印输出将类似于以下内容：
- en: '[PRE53]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: How it works…
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: The `TfidfVectorizer` class works almost exactly like the `CountVectorizer`
    class, differing only in the way the **word frequencies** are calculated, so most
    of the steps should be familiar. Word frequencies are calculated as follows. For
    each word, the overall frequency is a product of the **term frequency** and the
    **inverse document frequency**. Term frequency is the number of times a word occurs
    in the document. Inverse document frequency is the total number of documents divided
    by the number of documents where the word occurs. Usually, these frequencies are
    logarithmically scaled.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`TfidfVectorizer`类几乎与`CountVectorizer`类完全相同，只是在计算**词频**的方式上有所不同，所以大多数步骤应该是熟悉的。词频是单词在文档中出现的次数。逆文档频率是包含该单词的文档总数除以文档数。通常，这些频率是按对数缩放的。'
- en: 'This is done using the following formulas:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过以下公式完成的：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>T</mi><mi>F</mi><mo>=</mo><mfrac><mrow><mi>N</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>m</mi><mi>a</mi><mi>p</mi><mi>p</mi><mi>e</mi><mi>a</mi><mi>r</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi></mrow><mrow><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>w</mi><mi>o</mi><mi>r</mi><mi>d</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi></mrow></mfrac></mrow></mrow></math>](img/1.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>T</mi><mi>F</mi><mo>=</mo><mfrac><mrow><mi>N</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>r</mi><mi>m</mi><mi>a</mi><mi>p</mi><mi>p</mi><mi>e</mi><mi>a</mi><mi>r</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi></mrow><mrow><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>w</mi><mi>o</mi><mi>r</mi><mi>d</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi></mrow></mfrac></mrow></mrow></math>](img/1.png)'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>I</mi><mi>D</mi><mi>F</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>s</mi></mrow><mrow><mi>N</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>s</mi><mi>w</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>e</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>m</mi><mi>o</mi><mi>c</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>s</mi></mrow></mfrac></mrow></mrow></math>](img/2.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>I</mi><mi>D</mi><mi>F</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>s</mi></mrow><mrow><mi>N</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>s</mi><mi>w</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>e</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>r</mi><mi>m</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>s</mi></mrow></mfrac></mrow></mrow></math>](img/2.png)'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>T</mml:mi><mml:mi>F</mml:mi><mml:mo>-</mml:mo><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mi>F</mml:mi><mml:mi>*</mml:mi><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mi>F</mml:mi></mml:math>](img/3.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>T</mml:mi><mml:mi>F</mml:mi><mml:mo>-</mml:mo><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mi>F</mml:mi><mml:mi>*</mml:mi><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mi>F</mml:mi></mml:math>](img/3.png)'
- en: There’s more…
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: We can build `TfidfVectorizer` and use `[t, h, e, w, o, m, a, n, th, he, wo,
    om, ma, an, the, wom, oma, man]` set. In some experimental settings, models based
    on character *n*-grams perform better than word-based *n*-gram models.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以构建 `TfidfVectorizer` 并使用 `[t, h, e, w, o, m, a, n, th, he, wo, om, ma, an,
    the, wom, oma, man]` 集合。在一些实验设置中，基于字符 *n*-gram 的模型比基于单词的 *n*-gram 模型表现更好。
- en: 'We will use the small Sherlock Holmes text file, `sherlock_holmes_1.txt`, found
    at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt),
    and the same class, `TfidfVectorizer`. We will not need a tokenizer function or
    a stop-word list, since the unit of analysis is the character and not the word.
    The steps to create the vectorizer and analyze a sentence are as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用小型的夏洛克·福尔摩斯文本文件，`sherlock_holmes_1.txt`，位于 [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt)，以及相同的类，`TfidfVectorizer`。由于分析的单位是字符而不是单词，我们不需要标记化函数或停用词列表。创建向量器和分析句子的步骤如下：
- en: 'Create a new vectorizer object that uses the **char_wb** analyzer, and then
    fit it on the training text:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的使用 **char_wb** 分析器的向量器对象，然后将其拟合到训练文本上：
- en: '[PRE54]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Print the vectorizer vocabulary and its length:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印向量器的词汇表及其长度：
- en: '[PRE55]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The partial result will look like this:'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分结果将看起来像这样：
- en: '[PRE56]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Create the **vectorize** method using the new vectorizer, and then create the
    training and test data. Train the classifier and then test it:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新的向量器创建 **vectorize** 方法，然后创建训练数据和测试数据。训练分类器然后测试它：
- en: '[PRE57]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The result will be similar to this:'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将类似于以下内容：
- en: '[PRE58]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: See also
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: You can find out more about term weighting at [https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在 [https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)
    了解更多关于词频逆文档频率（TF-IDF）的词权重信息
- en: For more information about **TfidfVectorizer**, see [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多关于 **TfidfVectorizer** 的信息，请参阅 [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)
- en: Using word embeddings
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用词嵌入
- en: In this recipe, we will switch gears and learn how to represent *words* using
    word embeddings, which are powerful because they are a result of training a neural
    network that predicts a word from all other words in the sentence. Embeddings
    are also vectors, but usually of a much smaller size, 200 or 300\. The resulting
    vector embeddings are similar for words that occur in similar contexts. Similarity
    is usually measured by calculating the cosine of the angle between two vectors
    in the hyperplane, with 200 or 300 dimensions. We will use the embeddings to show
    these similarities.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将转换方向，学习如何使用词嵌入来表示*words*，这是因为它们是训练一个预测句子中所有其他单词的神经网络的产物。嵌入也是向量，但通常大小要小得多，200或300。结果向量嵌入对于在相似上下文中出现的单词是相似的。相似度通常通过计算超平面中两个向量之间角度的余弦值来衡量，维度为200或300。我们将使用嵌入来展示这些相似性。
- en: Getting ready
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will use a pretrained `word2vec` model, which can be found
    at [https://github.com/mmihaltz/word2vec-GoogleNews-vectors](https://github.com/mmihaltz/word2vec-GoogleNews-vectors).
    Download the model and unzip it in the data directory. You should now have a file
    with the `…/``data/GoogleNews-vectors-negative300.bin.gz` path.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用预训练的`word2vec`模型，该模型可在[https://github.com/mmihaltz/word2vec-GoogleNews-vectors](https://github.com/mmihaltz/word2vec-GoogleNews-vectors)找到。下载模型并将其解压缩到数据目录中。现在你应该有一个路径为`…/``data/GoogleNews-vectors-negative300.bin.gz`的文件。
- en: We will also use the `gensim` package to load and use the model. It should be
    installed within the `poetry` environment.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用`gensim`包来加载和使用模型。它应该在`poetry`环境中安装。
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.5_word_embeddings.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.5_word_embeddings.ipynb).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本位于[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.5_word_embeddings.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.5_word_embeddings.ipynb)。
- en: How to do it…
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'We will load the model, demonstrate some features of the `gensim` package,
    and then compute a sentence vector using the word embeddings:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将加载模型，演示`gensim`包的一些功能，然后使用词嵌入计算一个句子向量：
- en: 'Run the simple classifier file:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行简单的分类器文件：
- en: '[PRE59]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Import the **gensim** package:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入**gensim**包：
- en: '[PRE60]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Load the pretrained model. If you get an error at this step, make sure you
    have downloaded the model in the **data** directory:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载预训练模型。如果在这一步出现错误，请确保您已将模型下载到**data**目录中：
- en: '[PRE61]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Using the pretrained model, we can now load individual word vectors. Here,
    we load the word vector for the word *king*. We have to lowercase it, since all
    the words in the model are lowercase. The result is a long vector that represents
    this word in the **word2vec** model:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用预训练模型，我们现在可以加载单个词向量。在这里，我们加载单词*king*的词向量。我们必须将其转换为小写，因为模型中的所有单词都是小写的。结果是表示该单词在**word2vec**模型中的长向量：
- en: '[PRE62]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The result will be as follows:'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE63]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We can also get words that are most similar to a given word. For example, let’s
    print out the words most similar to *apple* and *tomato*. The output prints out
    the words that are the most similar (i.e., occur in similar contexts) and the
    similarity score. The score is the cosine distance between a pair of vectors –
    in this case, representing a pair of words. The larger the score, the more similar
    the two words. The results make sense, since the words most similar to *apple*
    are mostly fruits, and the words most similar to *tomato* are mostly vegetables:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以获取与给定单词最相似的单词。例如，让我们打印出与*apple*和*tomato*最相似的单词。输出将打印出最相似的单词（即出现在相似上下文中）及其相似度分数。分数是两个向量之间的余弦距离——在这种情况下，表示一对单词。分数越大，两个单词越相似。结果是有意义的，因为与*apple*最相似的单词大多是水果，与*tomato*最相似的单词大多是蔬菜：
- en: '[PRE64]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The result is as follows:'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下所示：
- en: '[PRE65]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'In the next two steps, we compute a sentence vector by averaging all the word
    vectors in the sentence. One of the challenges of this method is representing
    words that are not present in the model, and here, we simply skip such words.
    Let’s define a function that will take a sentence and a model and return a list
    of the sentence word vectors. Words that are not present in the model will return
    **KeyError**, and in such a case, we catch the error and continue:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在接下来的两个步骤中，我们通过平均句子中的所有词向量来计算一个句子向量。这种方法的一个挑战是表示模型中不存在的词，在这里，我们简单地跳过这些词。让我们定义一个函数，它将接受一个句子和一个模型，并返回句子词向量的列表。如果模型中不存在词，将返回**KeyError**，在这种情况下，我们捕获错误并继续：
- en: '[PRE66]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Let’s now define a function that will take the word vector list and compute
    the sentence vector. In order to compute the average, we represent the matrix
    as a **numpy** array and use the **numpy** **mean** function to get the average
    vector:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义一个函数，它将接受词向量列表并计算句子向量。为了计算平均值，我们将矩阵表示为一个**numpy**数组，并使用**numpy**的**mean**函数来获取平均向量：
- en: '[PRE67]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Note
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Averaging the word vectors to get the sentence vector is only one way of approaching
    this task and is not without its problems. One other alternative is to train a
    **doc2vec** model, where sentences, paragraphs, and whole documents can all be
    units instead of words.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 通过平均词向量来获取句子向量是处理这个任务的一种方法，但并非没有问题。另一种选择是训练一个**doc2vec**模型，其中句子、段落和整个文档都可以作为单位，而不是词。
- en: 'We can now test the average word embedding as a vectorizer. Our vectorizer
    takes in a string input, gets the word vectors for each word, and then returns
    the sentence vector that we compute in the **get_sentence_vector** function. We
    then load the training and test data and create the datasets. We train the logistic
    regression classifier and test it:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以测试平均词嵌入作为向量器。我们的向量器接受字符串输入，获取每个词的词向量，然后返回我们在**get_sentence_vector**函数中计算的句子向量。然后我们加载训练数据和测试数据，创建数据集。我们训练逻辑回归分类器并对其进行测试：
- en: '[PRE68]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We can see that the scores are much lower than those in previous sections.
    There might be several reasons for this; one of them is that the `word2vec` model
    is English-only, and the data is multilingual. As an exercise, you can write a
    script to filter English-only reviews and see whether that improves the score:'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，分数比前几节低得多。这可能有几个原因；其中之一是`word2vec`模型仅支持英语，而数据是多语言的。作为一个练习，你可以编写一个脚本来过滤仅支持英语的评论，看看是否可以提高分数：
- en: '[PRE69]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: There’s more…
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容…
- en: 'There are some other fun things that `gensim` can do with a pretrained model.
    For example, it can find an outlier word from a list of words and find the word
    that is most similar to the given word from a list. Let’s look at these:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`gensim`使用预训练模型可以做很多有趣的事情。例如，它可以从一个词表中找到一个异常词，并找到与给定词最相似的词。让我们看看这些：'
- en: 'Compile a list of words with one that doesn’t match, apply the **doesnt_match**
    function to the list, and print the results:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译一个包含不匹配词的词表，将**doesnt_match**函数应用于该列表，并打印结果：
- en: '[PRE70]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The result will be as follows:'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE71]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Now, let’s find a word that’s most similar to another word.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们找到一个与另一个词最相似的词。
- en: '[PRE72]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The result will be as follows:'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE73]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: See also
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: There are many other pretrained models available, including some in other languages;
    see [http://vectors.nlpl.eu/repository/](http://vectors.nlpl.eu/repository/).
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有许多其他预训练模型可供选择，包括一些其他语言的模型；参见[http://vectors.nlpl.eu/repository/](http://vectors.nlpl.eu/repository/)。
- en: Some pretrained models include part-of-speech information, which can be helpful
    when disambiguating words. These models concatenate words with their `cat_NOUN`),
    so keep that in mind when using them.
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一些预训练模型包括词性信息，这在区分词时可能很有帮助。这些模型将词与其`cat_NOUN`等属性连接起来，所以使用它们时请记住这一点。
- en: 'To learn more about the theory behind **word2vec**, you can start here: [https://jalammar.github.io/illustrated-word2vec/](https://jalammar.github.io/illustrated-word2vec/).'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解更多关于**word2vec**背后的理论，你可以从这里开始：[https://jalammar.github.io/illustrated-word2vec/](https://jalammar.github.io/illustrated-word2vec/)。
- en: Training your own embeddings model
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练自己的嵌入模型
- en: We can now train our own `word2vec` model on a corpus. This model is a neural
    network that predicts a word when given a sentence with words blanked out. The
    byproduct of the neural network training is the vector representation for each
    word in the training vocabulary. For this task, we will continue using the Rotten
    Tomatoes reviews. The dataset is not very large, so the results are not as good
    as they could be with a larger collection.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以在语料库上训练自己的 `word2vec` 模型。这个模型是一个神经网络，当给定一个带有空格的句子时，可以预测一个单词。神经网络训练的副产品是训练词汇表中每个单词的向量表示。对于这个任务，我们将继续使用
    Rotten Tomatoes 评论。数据集不是很大，所以结果不如拥有更大集合时那么好。
- en: Getting ready
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will use the `gensim` package for this task. It should be installed as part
    of the `poetry` environment.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `gensim` 包来完成这个任务。它应该作为 `poetry` 环境的一部分安装。
- en: How to do it…
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'We will create the dataset and then train the model on the data. We will then
    test how it performs:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建数据集，然后在数据上训练模型。然后我们将测试其性能：
- en: 'Import the necessary packages and functions:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包和函数：
- en: '[PRE74]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Load the training data and check its length:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载训练数据并检查其长度：
- en: '[PRE75]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The result should be as follows:'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果应该是这样的：
- en: '[PRE76]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Create the **RottenTomatoesCorpus** class. The **word2vec** training algorithm
    requires a class with a defined **__iter__** function that allows you to iterate
    through the data, so that is why we need this class:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 **RottenTomatoesCorpus** 类。**word2vec** 训练算法需要一个具有定义的 **__iter__** 函数的类，这样你就可以遍历数据，这就是为什么我们需要这个类的原因：
- en: '[PRE77]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Create an instance of **RottenTomatoesCorpus** using the loaded training dataset.
    Since **word2vec** models are trained on text only (they are self-supervised models),
    we don’t need the review score:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用加载的训练数据集创建一个 **RottenTomatoesCorpus** 实例。由于 **word2vec** 模型仅在文本上训练（它们是自监督模型），我们不需要评论评分：
- en: '[PRE78]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'In this step, we initialize the **word2vec** model, train it, and then save
    it to disk. The only required argument is the list of words; some of the other
    important ones are **min_count**, **size**, **window**, and **workers**. The **min_count**
    parameter refers to the minimum number of times a word has to occur in the training
    corpus, the default being 5\. The **size** parameter sets the size of the word
    vector. **window** restricts the maximum number of words between the predicted
    and current words in a sentence. **workers** refers to the number of working threads;
    the more there are, the quicker the training will proceed. When training the model,
    the **epoch** parameter will determine the number of training iterations the model
    will go through. After initializing the model object, we train it on our corpus
    for 100 epochs and, finally, save it to disk:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们初始化 **word2vec** 模型，训练它，然后将其保存到磁盘。唯一必需的参数是单词列表；其他一些重要的参数是 **min_count**、**size**、**window**
    和 **workers**。**min_count** 参数指的是一个单词在训练语料库中必须出现的最小次数，默认值为 5。**size** 参数设置单词向量的大小。**window**
    限制了句子中预测单词和当前单词之间的最大单词数。**workers** 指的是工作线程的数量；线程越多，训练速度越快。在训练模型时，**epoch** 参数将确定模型将经历的训练迭代次数。在初始化模型对象后，我们在语料库上训练它
    100 个 epoch，最后将其保存到磁盘：
- en: '[PRE79]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Get 10 words similar to the word *movie*. The words *sequels* and *film* make
    sense with this word; the rest are not that related. This is due to the small
    size of the training corpus. The words you get will be different, since the results
    are different every time the model is trained:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出与单词 *movie* 相似的 10 个单词。单词 *sequels* 和 *film* 与这个单词搭配合理；其余的则不太相关。这是因为训练语料库的规模较小。你得到的结果将会有所不同，因为每次训练模型时结果都会不同：
- en: '[PRE80]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'This is a possible output:'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是一个可能的结果：
- en: '[PRE81]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: There’s more…
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: There are tools to evaluate a `word2vec` model, although its creation is unsupervised.
    `gensim` comes with a file that lists word analogies, such as what *Athens* is
    to *Greece* being the same as what *Moscow* is to *Russia*. The `evaluate_word_analogies`
    function runs the analogies through the model and calculates how many were correct.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 有工具可以评估 `word2vec` 模型，尽管其创建是无监督的。`gensim` 包含一个文件，列出了单词类比，例如 *Athens* 对 *Greece*
    的关系与 *Moscow* 对 *Russia* 的关系相同。`evaluate_word_analogies` 函数将类比通过模型运行，并计算正确答案的数量。
- en: 'Here is how to do this:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是如何做到这一点的：
- en: Use the **evaluate_word_analogies** function to evaluate our trained model.
    We need the **analogies** file, which is available in the book GitHub repository
    at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/questions-words.txt](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/questions-words.txt)
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**evaluate_word_analogies**函数评估我们的训练模型。我们需要**类比**文件，该文件可在GitHub存储库的书中找到，地址为[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/questions-words.txt](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/questions-words.txt)。
- en: '[PRE82]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The result should be similar to this:'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果应该类似于以下内容：
- en: '[PRE83]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Let’s now evaluate the pretrained model. These commands might take longer to
    run:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来评估预训练模型。这些命令可能需要更长的时间来运行：
- en: '[PRE84]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'The result should be similar to this:'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果应该类似于以下内容：
- en: '[PRE85]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'We use the **evaluate_word_analogies** function differently in the pretrained
    model and our model case because they are different types. With the pretrained
    model, we just load the vectors (a **KeyedVectors** class, where each word, represented
    by a key, is mapped to a vector), and our model is a full **word2vec** model object.
    We can check the types by using these commands:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在预训练模型和我们的模型案例中使用了不同的**evaluate_word_analogies**函数，因为它们是不同类型的。对于预训练模型，我们只需加载向量（一个**KeyedVectors**类，其中每个由键表示的单词都映射到一个向量），而我们的模型是一个完整的**word2vec**模型对象。我们可以使用以下命令来检查类型：
- en: '[PRE86]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The result will be as follows:'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE87]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: The pretrained model was trained on a much larger corpus and, predictably, performs
    better. You can also construct your own evaluation file with analogies that your
    data requires.
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预训练模型是在一个更大的语料库上训练的，因此，预测地，它的表现更好。您也可以构建自己的评估文件，其中包含您数据所需的概念。
- en: Note
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Make sure your evaluation is based on the type of data that you are going to
    use in your application; otherwise, you risk having misleading evaluation results.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您的评估基于您将在应用程序中使用的类型的数据；否则，您可能会得到误导性的评估结果。
- en: See also
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考信息
- en: There is an additional way of evaluating model performance, by comparing the
    similarity between word pairs assigned by a model to the human-assigned judgments.
    You can do this by using the `evaluate_word_pairs` function. See more at [https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_pairs](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_pairs).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种额外的评估模型性能的方法，即通过比较模型分配给单词对的相似度与人类分配的判断之间的相似度。您可以通过使用`evaluate_word_pairs`函数来完成此操作。更多信息请参阅[https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_pairs](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_pairs)。
- en: Using BERT and OpenAI embeddings instead of word embeddings
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用BERT和OpenAI嵌入而不是词嵌入
- en: Instead of word embeddings, we can use **Bidirectional Encoder Representations
    from Transformer** (**BERT**) embeddings. A BERT model, like word embeddings,
    is a pretrained model and gives a vector representation, but it takes context
    into account and can represent a whole sentence instead of individual words.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用**双向编码器表示从Transformer**（**BERT**）嵌入而不是词嵌入。BERT模型，就像词嵌入一样，是一个预训练模型，它给出一个向量表示，但它考虑上下文，可以表示整个句子而不是单个单词。
- en: Getting ready
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this recipe, we can use the Hugging Face `sentence_transformers` package
    to represent sentences as vectors. We need `PyTorch`, which is installed as part
    of the `poetry` environment.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个食谱，我们可以使用Hugging Face的`sentence_transformers`包将句子表示为向量。我们需要`PyTorch`，它是作为`poetry`环境的一部分安装的。
- en: To get the vectors, we will use the `all-MiniLM-L6-v2` model for this recipe.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取向量，我们将使用`all-MiniLM-L6-v2`模型来完成这个食谱。
- en: We can also use the embeddings from OpenAI that come from their **large language**
    **models** (**LLMs**).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用来自OpenAI的**大型语言模型**（**LLMs**）的嵌入。
- en: To use the OpenAI embeddings, you will need to create an account and get an
    API key from OpenAI. You can create an account at [https://platform.openai.com/signup](https://platform.openai.com/signup).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用OpenAI嵌入，您需要创建一个账户并从OpenAI获取API密钥。您可以在[https://platform.openai.com/signup](https://platform.openai.com/signup)创建账户。
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.6_train_own_word2vec.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.6_train_own_word2vec.ipynb).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本位于 [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.6_train_own_word2vec.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.6_train_own_word2vec.ipynb)。
- en: How to do it…
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'Hugging Face code makes using BERT very easy. The first time the code runs,
    it will download the necessary model, which might take some time. After the download,
    it’s just a matter of encoding the sentences using the model. We will test the
    simple classifier with these embeddings:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 代码使使用 BERT 非常容易。第一次运行代码时，它将下载必要的模型，这可能需要一些时间。下载后，只需使用模型对句子进行编码即可。我们将使用这些嵌入测试简单的分类器：
- en: 'Run the simple classifier notebook to import its functions:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行简单的分类器笔记本以导入其函数：
- en: '[PRE88]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Import the **SentenceTransformer** class:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 **SentenceTransformer** 类：
- en: '[PRE89]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: Load the sentence transformer model, retrieve the embedding of the sentence
    *I love jazz*, and print it out.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载句子转换器模型，检索句子 *我爱爵士* 的嵌入，并打印出来。
- en: '[PRE90]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'As we can see, it is a vector similar to the word embeddings vector from the
    previous recipe:'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如我们所见，它是一个与之前菜谱中的词嵌入向量相似的向量：
- en: '[PRE91]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Now, we can test our classifier using the BERT embeddings. First, let’s define
    a function that will return a sentence vector. This function takes the input text
    and a model. It then uses the model to encode the text and returns the resulting
    embedding. We need to pass in the text inside of a list to the **encode** method,
    as it expects an iterable. Similarly, we return the first element of the result,
    since it returns a list of embeddings.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用 BERT 嵌入来测试我们的分类器。首先，让我们定义一个函数，该函数将返回一个句子向量。这个函数接受输入文本和一个模型。然后，它使用该模型对文本进行编码，并返回结果嵌入。我们需要将文本放入列表中传递给
    **encode** 方法，因为它期望一个可迭代对象。同样，我们返回结果中的第一个元素，因为它返回一个嵌入列表。
- en: '[PRE92]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Now, we define the **vectorize** function, create the training and test data
    using the **load_train_test_dataset_pd** function we created in the *Creating
    a simple classifier* recipe, train the classifier, and test it. We will time the
    dataset creation step, hence the inclusion of the **time** package commands. We
    see that it takes about 11 seconds to vectorize the whole dataset (about 85,000
    entries). We then train the model and test it:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们定义 **vectorize** 函数，使用我们创建在 *创建简单分类器* 菜谱中的 **load_train_test_dataset_pd**
    函数创建训练和测试数据，训练分类器，并对其进行测试。我们将计时数据集创建步骤，因此包含了 **time** 包命令。我们看到整个数据集（约 85,000 条记录）的向量化大约需要
    11 秒。然后我们训练模型并对其进行测试：
- en: '[PRE93]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'The result is our best one yet:'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果是我们迄今为止最好的结果：
- en: '[PRE94]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: There’s more…
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容…
- en: 'We can now use the OpenAI embeddings to see how they perform:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用 OpenAI 嵌入来查看它们的性能：
- en: 'Import the **openai** package and assign the API key:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 **openai** 包并分配 API 密钥：
- en: '[PRE95]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Assign the model we are going to use, the sentence and create the embedding.
    The model that we will use is specifically an embeddings model, so it returns
    an embeddings vector for a text input:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分配我们将使用的模型、句子和创建嵌入。我们将使用的模型是一个特定的嵌入模型，因此它为文本输入返回一个嵌入向量：
- en: '[PRE96]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'The partial result will be as follows:'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分结果将如下所示：
- en: '[PRE97]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Let’s now test our classifier using the OpenAI embeddings. This is the function
    that will return a sentence vector:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用 OpenAI 嵌入来测试我们的分类器。这是将返回句子向量的函数：
- en: '[PRE98]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Now, define the **vectorize** function, create the training and test data,
    train the classifier, and test it. We will time the vectorizing step:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，定义 **vectorize** 函数，创建训练和测试数据，训练分类器，并对其进行测试。我们将计时向量化步骤：
- en: '[PRE99]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'The result will be as follows:'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE100]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: Note that the result is quite poor in terms of the score, and it takes more
    than 10 minutes to process the whole dataset. Here, we only use the LLM embeddings
    and then train a logistic regression classifier on those embeddings. This is different
    from using the LLM itself to do the classification.
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，从得分来看，结果相当差，处理整个数据集需要超过 10 分钟。在这里，我们只使用了 LLM 嵌入，并在这些嵌入上训练了一个逻辑回归分类器。这与使用
    LLM 本身进行分类不同。
- en: See also
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: For more pretrained models, see [https://www.sbert.net/docs/pretrained_models.html](https://www.sbert.net/docs/pretrained_models.html).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 更多预训练模型，请参阅 [https://www.sbert.net/docs/pretrained_models.html](https://www.sbert.net/docs/pretrained_models.html)。
- en: Retrieval augmented generation (RAG)
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）
- en: In this recipe, we will see vector embeddings in action. RAG is a popular method
    of working with LLMs. Since these models are pretrained on widely available internet
    data, they do not have access to our personal data, and we cannot use the model
    as it is to ask questions about it. A way to overcome this is to use vector embeddings
    to represent our data. Then, we can compute cosine similarity between our data
    and the question and include the most similar piece of our data, together with
    the question – hence the name “retrieval augmented generation,” since we first
    retrieve relevant data by using cosine similarity and then generate text using
    the LLM.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将看到向量嵌入的实际应用。RAG 是一种流行的处理大型语言模型（LLM）的方法。由于这些模型是在广泛可用的互联网数据上预训练的，因此它们无法访问我们的个人数据，我们也不能直接使用该模型来对其提问。一种克服这一限制的方法是使用向量嵌入来表示我们的数据。然后，我们可以计算我们的数据与问题之间的余弦相似度，并将最相似的数据片段连同问题一起包含在内——这就是“检索增强生成”这个名字的由来，因为我们首先通过余弦相似度检索相关数据，然后使用大型语言模型生成文本。
- en: Getting ready
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will use an IMDB dataset from **Kaggle**, which can be downloaded from [https://www.kaggle.com/PromptCloudHQ/imdb-data](https://www.kaggle.com/PromptCloudHQ/imdb-data)
    and is also included in the book GitHub repo at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/IMDB-Movie-Data.csv](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/IMDB-Movie-Data.csv).
    Download the dataset and unzip the CSV file.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自 **Kaggle** 的 IMDB 数据集，该数据集可以从 [https://www.kaggle.com/PromptCloudHQ/imdb-data](https://www.kaggle.com/PromptCloudHQ/imdb-data)
    下载，也包含在本书的 GitHub 仓库中 [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/IMDB-Movie-Data.csv](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/IMDB-Movie-Data.csv)。下载数据集并解压
    CSV 文件。
- en: We will also use the OpenAI embeddings, as well as the `llama_index` package,
    which is included within the `poetry` environment.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用 OpenAI 嵌入，以及包含在 `poetry` 环境中的 `llama_index` 包。
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.9_vector_search.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.9_vector_search.ipynb).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本位于 [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.9_vector_search.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.9_vector_search.ipynb)。
- en: How to do it…
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'We will load the IMDB dataset and then create a vector store, using its first
    10 entries. We will then use the `llama_index` package to query the vector store:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将加载 IMDB 数据集，然后使用其前10个条目创建一个向量存储，然后使用 `llama_index` 包查询向量存储：
- en: 'Run the file **utilities** notebook:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 **utilities** 笔记本：
- en: '[PRE101]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Import the necessary classes and packages:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的类和包：
- en: '[PRE102]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Read in the CSV data. We will skip the first header row of the data:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取 CSV 数据。我们将跳过数据的第一行标题：
- en: '[PRE103]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'In this step, we use the first 10 rows of the data we just read in to first
    create a list of **Document** objects, and then a **VectorStoreIndex** object
    with these **Document** objects. An index is an object used for search, where
    each record contains certain information. A vector store index stores metadata
    as well as the vector representation of each record. For each movie, we assign
    the description as the text that will be embedded and the rest as metadata. We
    print out **document** objects and can see that a unique ID has been assigned
    to each:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们使用刚刚读取的数据的前10行来首先创建一个**Document**对象列表，然后创建一个包含这些**Document**对象的**VectorStoreIndex**对象。索引是一个用于搜索的对象，其中每个记录包含某些信息。向量存储索引存储每个记录的元数据以及向量表示。对于每部电影，我们将描述作为将被嵌入的文本，其余部分作为元数据。我们打印出**document**对象，可以看到每个对象都被分配了一个唯一的ID：
- en: '[PRE104]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'The partial output will look like this:'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分输出将类似于以下内容：
- en: '[PRE105]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Create the query engine from the index we just created. The query engine will
    allow us to send in questions about the documents loaded in the index:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从我们刚刚创建的索引中创建查询引擎。查询引擎将允许我们向索引中加载的文档发送问题：
- en: '[PRE106]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Use the engine to answer a question:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用引擎回答问题：
- en: '[PRE107]'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
