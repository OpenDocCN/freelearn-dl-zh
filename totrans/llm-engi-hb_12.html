<html><head></head><body>
  <div id="_idContainer255" class="Basic-Text-Frame">
    <h1 class="chapterNumber">Appendix</h1>
    <h1 id="_idParaDest-300" class="chapterTitle">MLOps Principles</h1>
    <p class="normal">Building robust and scalable ML systems requires more than creating powerful models. It demands an all-encompassing approach to operationalizing the entire ML lifecycle. Let’s explore the <strong class="keyWord">six core principles</strong> that guide the MLOps field. These principles are independent of any tool and are at the core of building robust and scalable ML systems. They provide a guideline for designing production-ready applications, ensuring consistency, reliability, and scalability at every stage.</p>
    <p class="normal">With that in mind, let’s begin with the foundation: automation or operationalization.</p>
    <h1 id="_idParaDest-301" class="heading-1">1. Automation or operationalization</h1>
    <p class="normal">To adopt MLOps, there are <a id="_idIndexMarker1199"/>three core tiers that most applications build up gradually, from manual processing to full automation:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Manual process</strong>: The process is experimental and iterative in the early stages of <a id="_idIndexMarker1200"/>developing an ML application. The data scientist <a id="_idIndexMarker1201"/>manually performs each pipeline step, such as data preparation and validation, model training, and testing. At this point, they commonly use Jupyter notebooks to train their models. This stage’s output is the code used to prepare the data and train the models.</li>
      <li class="bulletList"><strong class="keyWord">Continuous</strong> <strong class="keyWord">training</strong> (<strong class="keyWord">CT</strong>): The <a id="_idIndexMarker1202"/>next level involves automating model training. This is <a id="_idIndexMarker1203"/>known as continuous training, which triggers model retraining whenever required. At this point, you often automate your data and model validation steps. This step is usually done by an orchestration tool, such as ZenML, that glues all your code together and runs it on specific triggers. The most common triggers are on a schedule, for example, every day or when a specific event comes in, such as when new data is uploaded or the monitoring <a id="_idIndexMarker1204"/>system detects a drop in performance, offering <a id="_idIndexMarker1205"/>you the flexibility to adapt to various triggers.</li>
      <li class="bulletList"><strong class="keyWord">CI/CD</strong>: In the final stage, you implement your CI/CD pipelines to enable fast and reliable <a id="_idIndexMarker1206"/>deployment of your ML code into production. The key advancement at this stage is the automatic building, testing, and <a id="_idIndexMarker1207"/>deployment of data, ML models, and training pipeline components. CI/CD is used to quickly push new code into various environments, such as staging or production, ensuring efficient and reliable deployment.</li>
    </ul>
    <p class="normal">As we build <a id="_idIndexMarker1208"/>our LLM system using the <strong class="keyWord">FTI</strong> (<strong class="keyWord">feature</strong>, <strong class="keyWord">training</strong>, <strong class="keyWord">inference</strong>) architecture, we can quickly move from a manual process to CI/CD/CT. In <em class="italic">Figure A.1</em>, we can <a id="_idIndexMarker1209"/>observe that the CT process can be triggered by various events, such as a drop in performance detected by the monitoring pipeline or a batch of fresh data arriving. Also, <em class="italic">Figure A.1</em> is split into two main sections; the first one highlights the automated processes, while at the bottom, we can observe the manual processes performed by the data science team while experimenting with various data processing methods and models. Once they improve the model by tinkering with how the data is processed or the model architecture, they push the code to the code repository, which triggers the CI/CD pipeline to build, test, package, and deploy the new changes to the FTI pipelines.</p>
    <figure class="mediaobject"><img src="../Images/B31105_12_01.png" alt=""/></figure>
    <p class="packt_figref">Figure A.1: CI/CD/CT on top of the FTI architecture</p>
    <p class="normal">To conclude, CT automates <a id="_idIndexMarker1210"/>the FTI pipelines, while <a id="_idIndexMarker1211"/>CI/CD builds, tests, and pushes new versions of the FTI pipeline code to production.</p>
    <h1 id="_idParaDest-302" class="heading-1">2. Versioning</h1>
    <p class="normal">By now, we understand that the whole ML system changes if the code, model, or data changes. Thus, it is <a id="_idIndexMarker1212"/>critical to track and version these three elements individually. But what strategies can we adopt to track the code, model, and data separately?</p>
    <ul>
      <li class="bulletList">The <strong class="keyWord">code</strong> is tracked by Git, which helps us create a new commit (a snapshot of the code) on every <a id="_idIndexMarker1213"/>change added to the codebase. Also, Git-based tools usually allow us to make releases, which typically pack multiple features and bug fixes. While the commits contain unique identifiers that are not human-interpretable, a release follows more common conventions based on their major, minor, and patch versions. For example, in a release with version “v1.2.3,” 1 is the major version, 2 is the minor version, and 3 is the patch version. Popular tools are GitHub and GitLab.</li>
      <li class="bulletList">To version the <strong class="keyWord">model</strong>, you leverage the model registry to store, share, and version all the <a id="_idIndexMarker1214"/>models used within your system. It usually follows the same versioning conventions used in code releases, defined as <strong class="keyWord">Semantic Versioning</strong>, which, along with the major, minor, and patch versions, also supports alpha and beta releases that signal applications. At this point, you can also leverage the ML metadata store to attach information to the stored model, such as what data it was trained on, its architecture, performance, latency, and whatever else makes sense to your specific use case. Doing so creates a clear catalog of models that can easily be navigated across your team and company.</li>
      <li class="bulletList">Versioning the <strong class="keyWord">data</strong> isn’t as straightforward as versioning the code and model because it <a id="_idIndexMarker1215"/>depends on the type of data you have (structured or unstructured) and the scale of data you have (big or small). For example, for structured data, you can leverage a SQL database with a version column that helps you track the changes in the dataset. However, other popular solutions are based on Git-like systems, such as <strong class="keyWord">Data Version Control</strong> (<strong class="keyWord">DVC</strong>), that track every change made to the dataset. Other trendy solutions are based on artifacts similar to a model registry that allows you to add a virtual layer to your dataset, tracking and creating a new version for every change made to your data. Comet.ml, <strong class="keyWord">W&amp;B</strong> (<strong class="keyWord">Weights &amp; Biases</strong>), and ZenML offer powerful artifact features. For all solutions, you must store the data on-premises or use cloud object storage solutions such as AWS S3. These tools provide features that allow you to structure your datasets and versions, track, and access them.</li>
    </ul>
    <h1 id="_idParaDest-303" class="heading-1">3. Experiment tracking</h1>
    <p class="normal">Training ML models is an entirely iterative and experimental process. Unlike traditional software <a id="_idIndexMarker1216"/>development, it involves running multiple parallel experiments, comparing them based on a set of predefined metrics, and deciding which one should advance to production. An experiment tracking tool allows you to log all the necessary information, such as metrics and visual representations of your model predictions, to compare all your experiments and easily select the best model. Popular tools are Comet ML, W&amp;B, MLflow, and Neptune.</p>
    <h1 id="_idParaDest-304" class="heading-1">4. Testing</h1>
    <p class="normal">The same trend is followed <a id="_idIndexMarker1217"/>when testing ML systems. Hence, we must test our application across all three dimensions: the data, the model, and the code. We must also ensure that the feature, training, and inference pipeline are well integrated with external services, such as the feature store, and work together as a system. When working with Python, the most common tool to write your tests is <code class="inlineCode">pytest</code>, which we also recommend.</p>
    <h2 id="_idParaDest-305" class="heading-2">Test types</h2>
    <p class="normal">In the development cycle, six primary types of tests are commonly employed at various stages:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Unit tests</strong>: These <a id="_idIndexMarker1218"/>tests focus on individual components <a id="_idIndexMarker1219"/>with a single responsibility, such as a function that adds two tensors or one that finds an element in a list.</li>
      <li class="bulletList"><strong class="keyWord">Integration tests</strong>: These <a id="_idIndexMarker1220"/>tests evaluate the interaction <a id="_idIndexMarker1221"/>between integrated components or units within a system, such as the data evaluation pipeline or the feature engineering pipeline, and how they are integrated with the data warehouse and feature store.</li>
      <li class="bulletList"><strong class="keyWord">System tests</strong>: System tests <a id="_idIndexMarker1222"/>play a crucial role in the <a id="_idIndexMarker1223"/>development cycle as they examine the entire system, including the complete and integrated application. These tests rigorously evaluate the end-to-end functionality of the system, including performance, security, and overall user experience—for example, testing an entire ML pipeline, from <a id="_idIndexMarker1224"/>data ingestion to model training and <a id="_idIndexMarker1225"/>inference, ensuring the system produces the correct outputs for given inputs.</li>
      <li class="bulletList"><strong class="keyWord">Acceptance tests</strong>: These <a id="_idIndexMarker1226"/>tests, often called <strong class="keyWord">user acceptance testing</strong> (<strong class="keyWord">UAT</strong>), are designed to confirm that the system <a id="_idIndexMarker1227"/>meets specified requirements, ensuring <a id="_idIndexMarker1228"/>it is ready for deployment.</li>
      <li class="bulletList"><strong class="keyWord">Regression tests</strong>: These <a id="_idIndexMarker1229"/>tests check for previously identified <a id="_idIndexMarker1230"/>errors to ensure that new changes do not reintroduce them.</li>
      <li class="bulletList"><strong class="keyWord">Stress tests</strong>: These tests <a id="_idIndexMarker1231"/>evaluate the system’s performance <a id="_idIndexMarker1232"/>and stability under extreme conditions, such as high load or limited resources. They aim to identify breaking points and ensure the system can handle unexpected spikes in demand or adverse situations without failing.</li>
    </ul>
    <figure class="mediaobject"><img src="../Images/B31105_12_02.png" alt=""/></figure>
    <p class="packt_figref">Figure A.2: Test types</p>
    <p class="normal">We’ve intentionally <a id="_idIndexMarker1233"/>left regression tests out of the preceding figure because they aren’t a distinct testing phase. Instead, regression testing is applied across all levels—unit, integration, system, acceptance, and stress tests—to ensure that changes don’t reintroduce previous errors. It’s an ongoing process within these phases, not a separate type of test, which is why it’s not shown as a separate category.</p>
    <h2 id="_idParaDest-306" class="heading-2">What do we test?</h2>
    <p class="normal">When writing most tests, you take a component and treat it as a black box. Thus, what you have control over is the input and output. You want to test that you get an expected output for a given input. With that in mind, here are a few things you should usually test:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Inputs</strong>: Data <a id="_idIndexMarker1234"/>types, format, length, and edge cases (min/max, small/large, etc.)</li>
      <li class="bulletList"><strong class="keyWord">Outputs</strong>: Data <a id="_idIndexMarker1235"/>types, formats, exceptions, and intermediary and final outputs</li>
    </ul>
    <h2 id="_idParaDest-307" class="heading-2">Test examples</h2>
    <p class="normal">When testing your code, you can leverage the standards from classic software engineering. Here are a few examples of code tests you can include when writing unit tests to get a better idea of <a id="_idIndexMarker1236"/>what we want to test at this point—for instance, you want to check that a sentence is cleaned as expected. </p>
    <p class="normal">Also, you can look at your chunking algorithm and assert that it works properly by using various sentences and chunk sizes.</p>
    <p class="normal">When we talk <a id="_idIndexMarker1237"/>about <strong class="keyWord">data</strong> <strong class="keyWord">tests</strong>, we mainly refer to data validity. Your data validity code usually runs when raw data is ingested from the data warehouse or after computing the features. It is part of the feature pipeline. Thus, by writing integration or system tests for your feature pipeline, you can check that your system responds properly to valid and invalid data.</p>
    <p class="normal">Testing data validity depends a lot on your application and data type. For example, when working with tabular data, you can check for non-null values, that a categorical variable contains only the expected values, or that a float value is always positive. You can check for length, character encoding, language, special characters, and grammar errors when working with unstructured data such as text.</p>
    <p class="normal"><strong class="keyWord">Model tests</strong> are the trickiest, as model <a id="_idIndexMarker1238"/>training is the most non-deterministic process of an ML system. However, unlike traditional software, ML systems can successfully complete without throwing any errors. However, the real issue is that they produce incorrect results that can only be observed during evaluations or tests. Some standard model test techniques involve checking:</p>
    <ul>
      <li class="bulletList">The shapes of the input and model output tensors</li>
      <li class="bulletList">That the loss decreases after one batch (or more) of training</li>
      <li class="bulletList">Overfit on a small batch, and the loss approaches 0</li>
      <li class="bulletList">That your training pipeline works on all the supported devices, such as the CPU and GPU</li>
      <li class="bulletList">That your early stopping and checkpoint logic works</li>
    </ul>
    <p class="normal">All the tests are triggered inside the CI pipeline. If some tests are more costly, for example, the model ones, you can execute them only on special terms, such as only when modifying the model code.</p>
    <p class="normal">At the other end <a id="_idIndexMarker1239"/>of the spectrum, you can also perform <strong class="keyWord">behavioral testing</strong> on your <strong class="keyWord">model</strong>, which tries to adopt the strategy from code testing and treats the model as a black box while looking solely at the input data and expected outputs. This makes the behavioral testing methods model agnostic. A fundamental paper in this area is <em class="italic">Beyond Accuracy: Behavioral Testing of NLP Models with CheckList</em>, which we recommend if you want to dig more into the subject. However, as a quick overview, the paper proposes that you test your model against three types of tests. We use a model that extracts the main subject from a sentence as an example:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Invariance</strong>: Changes in <a id="_idIndexMarker1240"/>your input should not affect the output—for example, below is an example based on synonym injection:
        <pre class="programlisting code-one"><code class="hljs-code">model(text=<span class="hljs-string">"The advancements in AI are changing the world rapidly."</span>)
<span class="hljs-comment"># output: ai</span>
model(text=<span class="hljs-string">"The progress in AI is changing the world rapidly."</span>)
<span class="hljs-comment"># output: ai</span>
</code></pre>
      </li>
      <li class="bulletList"><strong class="keyWord">Directional</strong>: Changes in your <a id="_idIndexMarker1241"/>input should affect the outputs—for example, below is an example where we know the outputs should change based on the provided inputs:
        <pre class="programlisting code-one"><code class="hljs-code">model(text=<span class="hljs-string">"Deep learning used for sentiment analysis."</span>)
<span class="hljs-comment"># output: deep-learning</span>
model(text=<span class="hljs-string">"Deep learning used for object detection."</span>)
<span class="hljs-comment"># output: deep-learning</span>
model(text=<span class="hljs-string">"RNNs for sentiment analysis."</span>)
<span class="hljs-comment"># output: rnn</span>
</code></pre>
      </li>
      <li class="bulletList"><strong class="keyWord">Minimum functionality</strong>: The most simple combination of inputs and expected <a id="_idIndexMarker1242"/>outputs—for example, below is a set of simple examples that we expect the model should always get right:
        <pre class="programlisting code-one"><code class="hljs-code">model(text=<span class="hljs-string">"NLP is the next big wave in machine learning."</span>)
<span class="hljs-comment"># output: nlp</span>
model(text=<span class="hljs-string">"MLOps is the next big wave in machine learning."</span>)
<span class="hljs-comment"># output: mlops</span>
model(text=<span class="hljs-string">"This is about graph neural networks."</span>)
<span class="hljs-comment"># output: gnn</span>
</code></pre>
        <div class="note">
          <p class="normal">For more on testing, we recommend reading <em class="italic">Testing Machine Learning Systems: Code, Data, and Models</em> by Goku Mohandas: <a href="https://madewithml.com/courses/mlops/testing/"><span class="url">https://madewithml.com/courses/mlops/testing/</span></a>.</p>
        </div>
      </li>
    </ul>
    <h1 id="_idParaDest-308" class="heading-1">5. Monitoring</h1>
    <p class="normal">Monitoring is vital for any ML system that reaches production. Traditional software systems are <a id="_idIndexMarker1243"/>rule-based and deterministic. Thus, once it is built, it will always work as defined. Unfortunately, that is not the case with ML systems. When implementing ML models, we haven’t explicitly described how they should work. We have used data to compile a probabilistic solution, which means that our ML model will constantly be exposed to a level of degradation. This happens because the data from production might differ from the data the model was trained on. Thus, it is natural that the shipped model doesn’t know how to handle these scenarios.</p>
    <p class="normal">We shouldn’t try to avoid these situations but create a strategy to catch and fix these errors in time. Intuitively, monitoring detects the model’s performance degradation, which triggers an alarm that signals that the model should be retrained manually, automatically, or with a combination of both.</p>
    <p class="normal"><em class="italic">Why retrain the model?</em> As the model performance degrades due to a drift in the training dataset and what it inputs from production, the only solution is to adapt or retrain the model on a new dataset that captures all the new scenarios from production.</p>
    <p class="normal">As training is a costly operation, there are some tricks that you can perform to avoid retraining, but before describing them, let’s quickly understand what we can monitor to understand our ML system’s health.</p>
    <h2 id="_idParaDest-309" class="heading-2">Logs</h2>
    <p class="normal">The approach <a id="_idIndexMarker1244"/>to logging is straightforward, which is to capture everything, such as:</p>
    <ul>
      <li class="bulletList">Document the system configurations.</li>
      <li class="bulletList">Record the query, the results, and any intermediate outputs.</li>
      <li class="bulletList">Log when a component begins, ends, crashes, and so on.</li>
      <li class="bulletList">Ensure that each log entry is tagged and identified in a way that clarifies its origin within the system.</li>
    </ul>
    <p class="normal">While capturing <a id="_idIndexMarker1245"/>all activities can rapidly increase the volume of logs, you can take advantage of numerous tools for automated log analysis and anomaly detection that leverage AI to efficiently scan all the logs, providing you with the confidence to manage the logs effectively.</p>
    <h2 id="_idParaDest-310" class="heading-2">Metrics</h2>
    <p class="normal">To quantify your <a id="_idIndexMarker1246"/>application’s healthiness, you must define a set of metrics. Each <a id="_idIndexMarker1247"/>metric measures different aspects of your application, such as the infrastructure, data, and model.</p>
    <h3 id="_idParaDest-311" class="heading-3">System metrics</h3>
    <p class="normal">The system <a id="_idIndexMarker1248"/>metrics are based on monitoring service-level metrics (latency, throughput, error rates) and infrastructure health (CPU/GPU, memory). These <a id="_idIndexMarker1249"/>metrics are used both in traditional software and ML as they are crucial to understanding whether the infrastructure works well and the system works as expected to provide a good user experience to the end users.</p>
    <h3 id="_idParaDest-312" class="heading-3">Model metrics</h3>
    <p class="normal">Merely monitoring <a id="_idIndexMarker1250"/>the system’s health won’t suffice to identify the deeper <a id="_idIndexMarker1251"/>issues within our model. Therefore, moving on to the next layer of metrics that focus on the model’s performance is crucial. This includes quantitative evaluation metrics like accuracy, precision, and F1 score, as well as essential business metrics influenced by the model, such as ROI and click rate.</p>
    <p class="normal">Analyzing cumulative performance metrics over the entire deployment period is often ineffective. Instead, evaluating performance over time intervals relevant to our application, such as hourly, is essential. Thus, in practice, you window your inputs and compute and aggregate the metrics at the window level. These sliding metrics can provide a clearer picture of the system’s health, allowing us to detect issues more promptly without them being obscured by historical data.</p>
    <p class="normal">We may not always have access to ground-truth outcomes to evaluate the model’s performance on production data. This is particularly challenging when there is a significant delay or when real-life data requires annotation. To address this issue, we can develop <a id="_idIndexMarker1252"/>an approximate signal to estimate the model’s performance or label a small portion of our live dataset to assess performance. When talking <a id="_idIndexMarker1253"/>about ML monitoring, an approximate signal is also known as a <strong class="keyWord">proxy metric</strong>, usually implemented by drift detection methods, which are discussed in the following section.</p>
    <h3 id="_idParaDest-313" class="heading-3">Drifts</h3>
    <p class="normal"><strong class="keyWord">Drifts</strong> are proxy <a id="_idIndexMarker1254"/>metrics that help us detect potential issues with the production <a id="_idIndexMarker1255"/>model in time without requiring any ground truths/labels. <em class="italic">Table A.1</em> shows three kinds of drifts.</p>
    <table id="table001-5" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">What drifts</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Description</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Drift formulation</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><span class="url"><img src="../Images/B31105_12_001.png" alt=""/></span></p>
          </td>
          <td class="table-cell">
            <p class="normal">Inputs (features) </p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="url"><img src="../Images/B31105_12_002.png" alt=""/></span></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><span class="url"><img src="../Images/B31105_12_003.png" alt=""/></span></p>
          </td>
          <td class="table-cell">
            <p class="normal">Outputs (ground truths/labels)</p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="url"><img src="../Images/B31105_12_004.png" alt=""/></span></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><span class="url"><img src="../Images/B31105_12_005.png" alt=""/></span></p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="url"><img src="../Images/B31105_12_006.png" alt=""/></span></p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="url"><img src="../Images/B31105_12_007.png" alt=""/></span></p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table A.1: Relationship between data, model, and code changes</p>
    <h4 class="heading-4">Data drift</h4>
    <p class="normal">Data drift, also called <a id="_idIndexMarker1256"/>feature drift or covariate shift, occurs when the distribution <a id="_idIndexMarker1257"/>of the <a id="_idIndexMarker1258"/>production data deviates from that <a id="_idIndexMarker1259"/>of the training data, as shown in <em class="italic">Figure A.3</em>. This difference means the model cannot handle the changes in feature space, leading to potentially unreliable predictions. Drift can result from natural real-life changes or systemic problems like missing data, pipeline errors, and schema modifications.</p>
    <figure class="mediaobject"><img src="../Images/B31105_12_03.png" alt=""/></figure>
    <p class="packt_figref">Figure A.3: Data drift examples</p>
    <p class="normal">When data begins to drift, the degradation in our model’s performance might not be immediately noticeable, particularly if the model interpolates effectively. Nevertheless, this presents an ideal chance to consider retraining before the drift affects the model’s performance.</p>
    <h4 class="heading-4">Target drift</h4>
    <p class="normal">In addition to <a id="_idIndexMarker1260"/>changes in input data (data drift), we might also encounter <a id="_idIndexMarker1261"/>shifts in output distribution. The shift could involve changes in the shape of the distribution or the addition and removal of classes in categorical tasks. While retraining the model can help reduce performance degradation due to target drift, you can often prevent it by adapting the head processing steps and model head to support the new schema of the output class.</p>
    <p class="normal">For example, if you have a classifier that predicts if an image contains animals or people, and you get a picture with buildings, you can either adapt your model to support an unknown class or adjust the head of the model to add the new class for future predictions.</p>
    <h4 class="heading-4">Concept drift</h4>
    <p class="normal">In addition to <a id="_idIndexMarker1262"/>changes in input and output data, their relationship <a id="_idIndexMarker1263"/>can also shift. This phenomenon, known as <strong class="keyWord">concept drift</strong>, makes our model ineffective because the patterns it previously learned to associate inputs with outputs become outdated. As illustrated in the following figure, concept drifts can manifest in various ways:</p>
    <ul>
      <li class="bulletList">Gradually over time</li>
      <li class="bulletList">Suddenly, due to an external event</li>
      <li class="bulletList">Periodically, due to recurring events</li>
    </ul>
    <figure class="mediaobject"><img src="../Images/B31105_12_04.png" alt=""/></figure>
    <p class="packt_figref">Figure A.4: Concept drift examples</p>
    <p class="normal">For example, this happens when using the model in a different geographic area. Let’s assume you <a id="_idIndexMarker1264"/>want to build a model that predicts whether a person will buy <a id="_idIndexMarker1265"/>a specific car. You initially built it for the American market. Now, you want to use it in the European market, where people tend to buy smaller cars, creating a drift between the size feature of the car and the output probability of purchasing the vehicle. Of course, concept drifts can be more subtle than this example.</p>
    <div class="note">
      <p class="normal">All these types of drift can happen simultaneously, complicating pinpointing the exact sources of drift.</p>
    </div>
    <h4 class="heading-4">How to detect and measure drifts</h4>
    <p class="normal">Now that <a id="_idIndexMarker1266"/>we’ve recognized the various types of drift, it’s crucial to understand <a id="_idIndexMarker1267"/>how to detect and measure it. To do so, you need two types of windows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">A reference window</strong>: This is the collection of data points used as a baseline to compare <a id="_idIndexMarker1268"/>against the production data <a id="_idIndexMarker1269"/>distributions for drift identification. It is usually gathered from the training dataset.</li>
      <li class="bulletList"><strong class="keyWord">A test window</strong>: This <a id="_idIndexMarker1270"/>collects data points gathered <a id="_idIndexMarker1271"/>while the ML system is in production. It is compared with the reference window to ascertain if drift has occurred.</li>
    </ul>
    <p class="normal">To measure the drifts, you leverage hypothesis tests that verify the change in distribution <a id="_idIndexMarker1272"/>between the two windows. For example, you can use the <strong class="keyWord">Kolmogorov-Smirnov</strong> (<strong class="keyWord">KS</strong>) test to monitor a single continuous feature. This is known as a <strong class="keyWord">univariate</strong> (<strong class="keyWord">1D</strong>) test. Thus, you must run it for every feature you want to monitor. You can leverage a chi-squared univariate test to monitor categorical variables and determine if the frequency of events in production is consistent with the reference window distribution.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> alibi_detect.cd <span class="hljs-keyword">import</span> KSDrift
cd = KSDrift(X_ref, p_val=<span class="hljs-number">.05</span>, preprocess_fn=preprocess_fn, input_shape=(max_len,))
</code></pre>
    <p class="normal">When working with text data in an embedding representation, we have to model a multivariate distribution, which is how LLMs work with text. A popular approach is to take the embeddings of the test and reference windows, apply a dimensionality reduction <a id="_idIndexMarker1273"/>algorithm, and apply an algorithm such as <strong class="keyWord">maximum mean discrepancy</strong> (<strong class="keyWord">MMD</strong>). This algorithm is a kernel-based approach that measures the distance between two distributions by computing the distance between the mean of the embeddings of the two windows.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> alibi_detect.cd <span class="hljs-keyword">import</span> MMDDrift
cd = MMDDrift(x_ref, backend=<span class="hljs-string">'pytorch'</span>, p_val=<span class="hljs-number">.05</span>)
preds = cd.predict(x)
</code></pre>
    <h3 id="_idParaDest-314" class="heading-3">Monitoring vs. observability</h3>
    <p class="normal">Monitoring involves the collection and visualization of data, whereas observability provides <a id="_idIndexMarker1274"/>insights into system health by examining its inputs and outputs. For instance, monitoring allows us to track a specific metric to <a id="_idIndexMarker1275"/>detect potential issues. </p>
    <p class="normal">On the other hand, a system is considered observable if it generates meaningful data about its internal state, which is essential for diagnosing root causes.</p>
    <h3 id="_idParaDest-315" class="heading-3">Alerts</h3>
    <p class="normal">Once we <a id="_idIndexMarker1276"/>define our monitoring metrics, we need a way to get notified. The most common approaches are to send an alarm in the following scenarios:</p>
    <ul>
      <li class="bulletList">A metric passes the values of a static threshold—for example, when the accuracy of the classifier is lower than 0.8, send an alarm.</li>
      <li class="bulletList">Tweaking the p-value of the statistical tests that check for drifts. A lower p-value means a higher confidence that the production distribution differs from the reference one.</li>
    </ul>
    <p class="normal">These thresholds and p-values depend on your application. However, it is essential to find the correct values, as you don’t want to overcrowd your alarming system with false positives. In that case, your alarm system won’t be trustworthy, and you will either overreact or not react at all to issues in your system. Some common channels for sending alarms to your stakeholders are Slack, Discord, your email, and PagerDuty. The system’s stakeholders can be the core engineers, managers, or anyone interested in the system.</p>
    <p class="normal">Depending on the nature of the alarm, you have to react differently. But before taking any action, you should be able to inspect it and understand what caused it. You should inspect what metric triggered the alarm, with what value, the time it happened, and anything else that makes sense to your application.</p>
    <p class="normal">When the model’s performance degrades, the first impulse is to retrain it. But that is a costly operation. Thus, you first have to check that the data is valid, the schema hasn’t changed, and the data point was not an isolated outlier. If neither is true, you should trigger the training pipeline and train the model on the newly shifted dataset to solve the drift.</p>
    <h1 id="_idParaDest-316" class="heading-1">6. Reproducibility</h1>
    <p class="normal"><strong class="keyWord">Reproducibility</strong> means that every process within your ML systems should produce identical results <a id="_idIndexMarker1277"/>given the same input. This has two main aspects.</p>
    <p class="normal">The first one is that you should always know what the inputs are—for example, when training a model, you can use a plethora of hyperparameters. Thus, you need a way to always track what assets were used to generate the new assets, such as what dataset version and config were used to train the model.</p>
    <p class="normal">The second aspect is based on the non-deterministic nature of ML processes. For example, when training a model from scratch, all the weights are initially randomly initialized. Thus, even if you use the same dataset and hyperparameters, you might end up with a model with a different performance. This aspect can be solved by always using a seed before generating random numbers, as in reality, we cannot digitally create randomness, only pseudo-random numbers. Thus, by providing a seed, we ensure that we always produce the same trace of pseudo-random numbers. This can also happen at the feature engineering step, in case we impute values with random values or randomly remove data or labels. But as a general rule of thumb, always try to make your processes as deterministic as possible, and in case you have to introduce randomness, always provide a seed that you have control over.</p>
    <h1 id="_idParaDest-317" class="heading-1">Join our book’s Discord space</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/llmeng"><span class="url">https://packt.link/llmeng</span></a></p>
    <p class="normal"><a href="https://packt.link/llmeng"><span class="url"><img src="../Images/QR_Code79969828252392890.png" alt=""/></span></a></p>
  </div>


  <div id="_idContainer259">
    <p class="BM-packtLogo"><img src="../Images/New_Packt_Logo1.png" alt=""/></p>
    <p class="normal"><a href="https://www.packt.com"><span class="url">packt.com</span></a></p>
    <p class="normal">Subscribe to our online digital library for full access to over 7,000 books and videos, as well as industry leading tools to help you plan your personal development and advance your career. For more information, please visit our website.</p>
    <h1 id="_idParaDest-318" class="heading-1">Why subscribe?</h1>
    <ul>
      <li class="bulletList">Spend less time learning and more time coding with practical eBooks and Videos from over 4,000 industry professionals</li>
      <li class="bulletList">Improve your learning with Skill Plans built especially for you</li>
      <li class="bulletList">Get a free eBook or video every month</li>
      <li class="bulletList">Fully searchable for easy access to vital information</li>
      <li class="bulletList">Copy and paste, print, and bookmark content</li>
    </ul>
    <p class="normal">At <a href="https://www.packt.com"><span class="url">www.packt.com</span></a>, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts and offers on Packt books and eBooks.</p>
    <p class="eop"/>
    <h1 id="_idParaDest-319" class="mainHeading">Other Books You May Enjoy</h1>
    <p class="normal">If you enjoyed this book, you may be interested in these other books by Packt:</p>
    <p class="normal"><a href="https://www.packtpub.com/en-in/product/rag-driven-generative-ai-9781836200918"><img src="../Images/9781836200918.png" alt=""/></a></p>
    <p class="BM-bookCover"><strong class="keyWord">RAG-Driven Generative AI</strong></p>

    <p class="normal">Denis Rothman</p>

    <p class="normal">ISBN: 9781836200918</p>
    <ul>
      <li class="bulletList">Scale RAG pipelines to handle large datasets efficiently</li>
      <li class="bulletList">Employ techniques that minimize hallucinations and ensure accurate responses</li>
      <li class="bulletList">Implement indexing techniques to improve AI accuracy with traceable and transparent outputs</li>
      <li class="bulletList">Customize and scale RAG-driven generative AI systems across domains</li>
      <li class="bulletList">Find out how to use Deep Lake and Pinecone for efficient and fast data retrieval</li>
      <li class="bulletList">Control and build robust generative AI systems grounded in real-world data</li>
      <li class="bulletList">Combine text and image data for richer, more informative AI responses</li>
    </ul>
    <p class="normal"><a href="https://www.packtpub.com/en-in/product/building-llm-powered-applications-9781835462317"><img src="../Images/9781835462317.jpg" alt=""/></a></p>
    <p class="eop"/>
    <p class="BM-bookCover"><strong class="keyWord">Building LLM Powered Applications</strong></p>

    <p class="normal">Valentina Alto</p>

    <p class="normal">ISBN: 9781835462317</p>
    <ul>
      <li class="bulletList">Explore the core components of LLM architecture, including encoder-decoder blocks and embeddings</li>
      <li class="bulletList">Understand the unique features of LLMs like GPT-3.5/4, Llama 2, and Falcon LLM</li>
      <li class="bulletList">Use AI orchestrators like LangChain, with Streamlit for the frontend</li>
      <li class="bulletList">Get familiar with LLM components such as memory, prompts, and tools</li>
      <li class="bulletList">Learn how to use non-parametric knowledge and vector databases</li>
      <li class="bulletList">Understand the implications of LFMs for AI research and industry applications</li>
      <li class="bulletList">Customize your LLMs with fine tuning</li>
      <li class="bulletList">Learn about the ethical implications of LLM-powered applications</li>
    </ul>
    <p class="eop"/>
    <h1 id="_idParaDest-320" class="heading-1">Packt is searching for authors like you</h1>
    <p class="normal">If you’re interested in becoming an author for Packt, please visit <a href="https://authors.packtpub.com"><span class="url">authors.packtpub.com</span></a> and apply today. We have worked with thousands of developers and tech professionals, just like you, to help them share their insight with the global tech community. You can make a general application, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.</p>
  </div>
  <div id="_idContainer260">
    <p class="eop"/>
    <h1 id="_idParaDest-321" class="heading-1">Share your thoughts</h1>
    <p class="normal">Now you’ve finished <em class="italic">LLM Engineer’s Handbook, First Edition</em>, we’d love to hear your thoughts! If you purchased the book from Amazon, please <a href="https://packt.link/r/1836200072"><span class="url">click here to go straight to the Amazon review page</span></a> for this book and share your feedback or leave a review on the site that you purchased it from.</p>
    <p class="normal">Your review is important to us and the tech community and will help us make sure we’re delivering excellent quality content.</p>
  </div>
</body></html>