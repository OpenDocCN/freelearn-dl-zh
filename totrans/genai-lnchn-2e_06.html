<html><head></head><body>
<div><div><h1 class="chapterNumber"><a id="_idTextAnchor231"/>5</h1>
<h1 class="chapterTitle" id="_idParaDest-123"><a id="_idTextAnchor232"/>Building Intelligent Agents</h1>
<p class="normal">As generative AI adoption grows, we start using LLMs for more open and complex tasks that require knowledge about fresh events or interaction with the world. This is what is generally called agentic applications. We’ll define what an agent is later in this chapter, but you’ve likely seen the phrase circulating in the media: <em class="italic">2025 is the year of agentic AI</em>. For example, in a recently introduced RE-Bench benchmark that consists of complex open-ended tasks, AI agents outperform humans in some settings (for example, with a thinking budget of 30 minutes) or on some specific class of tasks (like writing Triton kernels).</p>
<p class="normal">To understand how these agentic capabilities are built in practice, we’ll start by discussing tool calling with LLMs and how it is implemented on LangChain. We’ll look in detail at the ReACT pattern, and how LLMs can use tools to interact with the external environment and improve their performance on specific tasks. Then, we’ll touch on how tools are defined in LangChain, and which pre-built tools are available. We’ll also talk about developing your own custom tools, handling errors, and using advanced tool-calling capabilities. As a practical example, we’ll look at how to generate structured outputs with LLM using tools versus utilizing built-in capabilities offered by model providers.</p>
<p class="normal">Finally, we’ll talk about what agents are and look into more advanced patterns of building agents with LangGraph before we then develop our first ReACT agent with LangGraph—a research agent that follows a plan-and-solve design pattern and uses tools such as web search, <em class="italic">arXiv</em>, and <em class="italic">Wikipedia</em>. </p>
<p class="normal">In a nutshell, the following topics will be covered in this chapter:</p>
<div><ul>
<li class="b lletList">What is a tool?</li>
<li class="b lletList">Defining built-in LangChain tools and custom tools</li>
<li class="b lletList">Advanced tool-calling capabilities</li>
<li class="b lletList">Incorporating tools into workflows</li>
<li class="b lletList">What are agents?</li>
</ul>
<div><div><p class="normal">You can find the code for this chapter in the <code class="inlineCode">chapter5/</code> directory of the book’s GitHub repository. Please visit <a href="https://github.com/benman1/generative_ai_with_langchain/tree/second_edition">https://github.com/benman1/generative_ai_with_langchain/tree/second_edition</a> for the latest updates. </p>
<p class="normal">See <a href="E_Chapter_2.xhtml#_idTextAnchor044"><em class="italic">Chapter 2</em></a> for setup instructions. If you have any questions or encounter issues while running the code, please create an issue on GitHub or join the discussion on Discord at <a href="https://packt.link/lang">https://packt.link/lang</a>.</p>
</div>
</div>
<p class="normal"><a id="_idTextAnchor233"/>Let’s begin with tools. Rather than diving straight into defining what an agent is, it’s more helpful to first explore how enhancing LLMs with tools actually works in practice. By walking through this step by step, you’ll see how these integrations unlock new capabilities. So, what exactly are tools, and how do they extend what LLMs can do?</p>
<h1 class="heading-1" id="_idParaDest-124"><a id="_idTextAnchor234"/>What is a tool?</h1>
<p class="normal">LLMs are trained on vast general corpus data (like web data and books), which gives them broad knowledge but limits their effectiveness in tasks that require domain-specific or up-to-date knowledge. However, because LLMs are good at reasoning, they can interact with the external environment through tools—APIs or interfaces that allow the model to interact with the external world. These tools enable LLMs to perform specific tasks and receive feedback from the external world. </p>
<p class="normal">When <a id="_idIndexMarker428"/>using tools, LLMs perform three specific generation tasks:</p>
<ol>
<li class="numberedList" value="1">Choose a tool to use by generating special tokens and the name of the tool.</li>
<li class="numberedList">Generate a payload to be sent to the tool.</li>
<li class="numberedList">Generate a response to a user based on the initial question and a history of interactions with tools (for this specific run).</li>
</ol>
<p class="normal">Now it’s time to figure out how LLMs invoke tools and how we can make LLMs tool-aware<a id="_idTextAnchor235"/>. Consider a somewhat artificial but illustrative question: <em class="italic">What is the square root of the current US president’s age multiplied by 132</em>? This question presents two specific challenges:</p>
<div><ul>
<li class="b lletList">It references current information (as of March 2025) that likely falls outside the model’s training data. </li>
<li class="b lletList">It requires a precise mathematical calculation that LLMs might not be able to answer correctly just by autoregressive token generation. </li>
</ul>
<p class="normal">Rather than forcing an LLM to generate an answer solely based on its internal knowledge, we’ll give an LLM access to two tools: a search engine and a calculator. We expect the model to determine which tools it needs (if any) and how to use them. </p>
<p class="normal">For clarity, let’s <a id="_idIndexMarker429"/>start with a simpler question and mock our tools by creating dummy functions that always give the same response. Later in this chapter, we’ll implement fully functional tools and invoke them:</p>
<pre>question = "how old is the US president?"
raw_prompt_template = (
 "You have access to search engine that provides you an "
 "information about fresh events and news given the query. "
 "Given the question, decide whether you need an additional "
 "information from the search engine (reply with 'SEARCH: "
 "&lt;generated query&gt;' or you know enough to answer the user "
 "then reply with 'RESPONSE &lt;final response&gt;').\n"
 "Now, act to answer a user question:\n{QUESTION}"
)
prompt_template = PromptTemplate.from_template(raw_prompt_template)
result = (prompt_template | llm).invoke(question)
print(result,response)
&gt;&gt; SEARCH: current age of US president</pre>
<p class="normal">Let’s make sure that when the LLM has enough internal knowledge, it replies directly to the user:</p>
<pre>question1 = "What is the capital of Germany?"
result = (prompt_template | llm).invoke(question1)
print(result,response)
&gt;&gt; RESPONSE: Berlin</pre>
<p class="normal">Finally, let’s give the model output of a tool by incorporating it into a prompt:</p>
<pre>query = "age of current US president"
search_result = (
 "Donald Trump ' Age 78 years June 14, 1946\n"</pre>
<div><pre> "Donald Trump 45th and 47th U.S. President Donald John Trump is an American "
 "politician, media personality, and businessman who has served as the 47th "
 "president of the United States since January 20, 2025. A member of the "
 "Republican Party, he previously served as the 45th president from 2017 to 2021. Wikipedia"
)
raw_prompt_template = (
 "You have access to search engine that provides you an "
 "information about fresh events and news given the query. "
 "Given the question, decide whether you need an additional "
 "information from the search engine (reply with 'SEARCH: "
 "&lt;generated query&gt;' or you know enough to answer the user "
 "then reply with 'RESPONSE &lt;final response&gt;').\n"
 "Today is {date}."
 "Now, act to answer a user question and "
 "take into account your previous actions:\n"
 "HUMAN: {question}\n"
 "AI: SEARCH: {query}\n"
 "RESPONSE FROM SEARCH: {search_result}\n"
)
prompt_template = PromptTemplate.from_template(raw_prompt_template)
result = (prompt_template | llm).invoke(
  {"question": question, "query": query, "search_result": search_result,
 "date": "Feb 2025"})
print(result.content)
&gt;&gt;  RESPONSE: The current US President, Donald Trump, is 78 years old.</pre>
<p class="normal">As a last <a id="_idIndexMarker430"/>observation, if the search result is not successful, the LLM will try to refine the query:</p>
<pre>query = "current US president"
search_result = (
 "Donald Trump 45th and 47th U.S."
)</pre>
<div><pre>result = (prompt_template | llm).invoke(
  {"question": question, "query": query, 
 "search_result": search_result, "date": "Feb 2025"})
print(result.content)
&gt;&gt;  SEARCH: Donald Trump age</pre>
<p class="normal">With that, we have demonstrated how tool calling works. Please note that we’ve provided prompt examples for demonstration purposes only. Another foundational LLM might require some prompt engineering, and our prompts are just an illustration. And good news: using tools is easier than it seems from these examples!</p>
<p class="normal">As you can note, we described everything in our prompt, including a tool description and a tool-calling format. These days, most LLMs provide a better API for tool calling since modern LLMs are post-trained on datasets that help them excel in such tasks. The LLMs’ creators know how these datasets were constructed. That’s why, typically, you don’t incorporate a tool description<a id="_idIndexMarker431"/> yourself in the prompt; you just provide both a prompt and a tool description as separate arguments, and they are combined into a single prompt on the provider’s side. Some smaller open-source LLMs expect tool descriptions to be part of the raw prompt, but they would expect a well-defined format.</p>
<p class="normal">LangChain makes it easy to develop pipelines where an LLM invokes different tools and provides access to many helpful built-in tools. Let’s look at how tool handling works with LangCha<a id="_idTextAnchor236"/>in.</p>
<h2 class="heading-2" id="_idParaDest-125"><a id="_idTextAnchor237"/>Tools in LangChain</h2>
<p class="normal">With most<a id="_idIndexMarker432"/> modern LLMs, to use tools, you can provide a list of tool descriptions as a separate argument. As always in LangChain, each particular integration implementation maps the interface to the provider’s API. For tools, this happens through LangChain’s <code class="inlineCode">tools</code> argument to the <code class="inlineCode">invoke</code> method (and some other useful methods such as <code class="inlineCode">bind_tools</code> and others, as we will learn in this chapter).</p>
<p class="normal">When defining a tool, we need to specify its schema in OpenAPI format. We provide a <em class="italic">title</em> and a <em class="italic">description</em> of the tool and also specify its parameters (each parameter has a <em class="italic">type</em>, <em class="italic">title</em>, and <em class="italic">description</em>). We can inherit such a schema from various formats, which LangChain translates into OpenAPI format. As we go through the next few sections, we’ll illustrate how we can do this from functions, docstrings, Pydantic definitions, or by inheriting from a <code class="inlineCode">BaseTool</code> class and providing descriptions directly. For an LLM, a tool is anything that has an OpenAPI specification—in other words, it can be called by some external mechanism. </p>
<div><p class="normal">The LLM itself doesn’t bother about this mechanism, it only produces instructions for when and how to call a tool. For LangChain, a tool is also something that can be called (and we will see later that tools are inherited from <code class="inlineCode">Runnables</code>) when we execute our program.</p>
<p class="normal">The wording that you use in the <em class="italic">title</em> and <em class="italic">description</em> fields is extremely important, and you can treat it as a part of the prompt engineering exercise. Better wording helps LLMs make better decisions on when and how to call a specific tool. Please note that for more complex tools, writing a schema like this can become tedious, and we’ll see a simpler way to define tools later in this chapter:</p>
<pre>search_tool = {
 "title": "google_search",
 "description": "Returns about fresh events and news from Google Search engine based on a query",
 "type": "object",
 "properties": {
 "query": {
 "description": "Search query to be sent to the search engine",
 "title": "search_query",
 "type": "string"},
   },
 "required": ["query"]
}
result = llm.invoke(question, tools=[search_tool])</pre>
<p class="normal">If we <a id="_idIndexMarker433"/>inspect the <code class="inlineCode">result.content</code> field, it would be empty. That’s because the LLM has decided to call a tool, and the output message has a hint for that. What happens under the hood is that LangChain maps a specific output format of the model provider into a unified tool-calling format:</p>
<pre>print(result.tool_calls)
&gt;&gt; [{'name': 'google_search', 'args': {'query': 'age of Donald Trump'}, 'id': '6ab0de4b-f350-4743-a4c1-d6f6fcce9d34', 'type': 'tool_call'}]</pre>
<p class="normal">Keep in mind that some model providers might return non-empty content even in the case of tool calling (for example, there might be reasoning traces on why the model decided to call a tool). You need to look at the model provider specification to understand how to treat such cases.</p>
<p class="normal">As we can see, an LLM returned an array of tool-calling dictionaries—each of them contains a unique identifier, the name of the tool to be called, and a dictionary with arguments to be provided to this tool. Let’s move to the next step and invoke the model again:</p>
<div><pre>from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage
tool_result = ToolMessage(content="Donald Trump ' Age 78 years June 14, 1946\n", tool_call_id=step1.tool_calls[0]["id"])
step2 = llm.invoke([
   HumanMessage(content=question), step1, tool_result], tools=[search_tool])
assert len(step2.tool_calls) == 0
print(step2.content)
&gt;&gt; Donald Trump is 78 years old.</pre>
<p class="normal"><code class="inlineCode">ToolMessage</code> is a special message on LangChain that allows you to feed the output of a tool execution back to the model. The <code class="inlineCode">content</code> field of such a message contains the tool’s output, and a special field <code class="inlineCode">tool_call_id</code> maps it to the specific tool calling that was generated by the model. Now, we can send the whole sequence (consisting of the initial output, the step with tool calling, and the output) back to the model as a list of messages.</p>
<p class="normal">It might be <a id="_idIndexMarker434"/>odd to always pass a list of tools to the LLM (since, typically, such a list is fixed for a given workflow). For that reason, LangChain <code class="inlineCode">Runnables</code> offer a <code class="inlineCode">bind</code> method that memorizes arguments and adds them to every further invocation. Take a look at the following code:</p>
<pre>llm_with_tools = llm.bind(tools=[search_tool])
llm_with_tools.invoke(question)</pre>
<p class="normal">When we call <code class="inlineCode">llm.bind(tools=[search_tool])</code>, LangChain creates a new object (assigned here to <code class="inlineCode">llm_with_tools</code>) that automatically includes <code class="inlineCode">[search_tool]</code> in every subsequent call to a copy of the initial <code class="inlineCode">llm </code>one. Essentially, you no longer need to pass the tools argument with each <code class="inlineCode">invoke</code> method. So, calling the preceding code is the same as doing:</p>
<pre>llm.invoke(question, tools=[search_tool)</pre>
<p class="normal">This is because bind has “memorized” your tools list for all future invocations. It’s mainly a convenience feature—ideal if you want a fixed set of tools for repeated calls rather than specifying them every time. Now let’s see how we can utilize tool calling even more, and improve LLM reas<a id="_idTextAnchor238"/>oning!</p>
<div><h2 class="heading-2" id="_idParaDest-126"><a id="_idTextAnchor239"/>ReACT</h2>
<p class="normal">As you<a id="_idIndexMarker435"/> have probably thought already, LLMs can call multiple tools before generating the final reply to the user (and the next tool to be called or a payload sent to this tool might depend on the outcome from the previous tool calls). This was proposed by a ReACT approach introduced in 2022 by researchers from Princeton University and Google Research: <em class="italic">Reasoning and ACT </em>(<a href="https://arxiv.org/abs/2210.03629">https://arxiv.org/abs/2210.03629</a>). The idea is simple—we should give the LLM access to tools as a way to interact with an external environment, and let the LLM run in a loop: </p>
<ul>
<li class="b lletList"><strong class="keyWord">Reason</strong>: Generate a text output with observations about the current situation and a plan to solve the task.</li>
<li class="b lletList"><strong class="keyWord">Act</strong>: Take an action based on the reasoning above (interact with the environment by calling a tool, or respond to the user).</li>
</ul>
<p class="normal">It has been <a id="_idIndexMarker436"/>demonstrated that ReACT can help reduce hallucination rates compared to CoT prompting, which we discussed in <a href="E_Chapter_3.xhtml#_idTextAnchor107"><em class="italic">Chapter 3</em></a>.</p>
<figure class="mediaobject"><img alt="Figure 5.1: ReACT pattern" src="img/B32363_05_01.png"/></figure>
<p class="packt_figref">Figure 5.1: ReACT pattern</p>
<p class="normal">Let’s build a ReACT application ourselves. First, let’s create mocked search and calculator tools: </p>
<pre>import math
def mocked_google_search(query: str) -&gt; str:
 print(f"CALLED GOOGLE_SEARCH with query={query}")
 return "Donald Trump is a president of USA and he's 78 years old"</pre>
<div><pre>def mocked_calculator(expression: str) -&gt; float:
 print(f"CALLED CALCULATOR with expression={expression}")
 if "sqrt" in expression:
 return math.sqrt(78*132)
 return 78*132</pre>
<p class="normal">In the next section, we’ll see how we can build actual tools. For now, let’s define a schema for the calculator tool and make the LLM aware of both tools it can use. We’ll also use building blocks that we’re already familiar with—<code class="inlineCode">ChatPromptTemplate</code> and <code class="inlineCode">MessagesPlaceholder</code>—to prepend a<a id="_idIndexMarker437"/> predetermined system message when we call our graph:</p>
<pre>from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
calculator_tool = {
 "title": "calculator",
 "description": "Computes mathematical expressions",
 "type": "object",
 "properties": {
 "expression": {
 "description": "A mathematical expression to be evaluated by a calculator",
 "title": "expression",
 "type": "string"},
  },
 "required": ["expression"]
}
prompt = ChatPromptTemplate.from_messages([
   ("system", "Always use a calculator for mathematical computations, and use Google Search for information about fresh events and news."), 
   MessagesPlaceholder(variable_name="messages"),
])
llm_with_tools = llm.bind(tools=[search_tool, calculator_tool]).bind(prompt=prompt)</pre>
<div><p class="normal">Now that we have an LLM that can call tools, let’s create the nodes we need. We need one function that calls an LLM, another function that invokes tools and returns tool-calling results (by appending <code class="inlineCode">ToolMessages</code> to the list of messages in the state), and a function that will determine whether the orchestrator should continue calling tools or whether it can return the result to the user:</p>
<pre>from typing import TypedDict
from langgraph.graph import MessagesState, StateGraph, START, END
def invoke_llm(state: MessagesState):
 return {"messages": [llm_with_tools.invoke(state["messages"])]}
def call_tools(state: MessagesState):
   last_message = state["messages"][-1]
   tool_calls = last_message.tool_calls
   new_messages = []
 for tool_call in tool_calls:
 if tool_call["name"] == "google_search":
       tool_result = mocked_google_search(**tool_call["args"])
       new_messages.append(ToolMessage(content=tool_result, tool_call_id=tool_call["id"]))
 elif tool_call["name"] == "calculator":
       tool_result = mocked_calculator(**tool_call["args"])
       new_messages.append(ToolMessage(content=tool_result, tool_call_id=tool_call["id"]))
 else:
 raise ValueError(f"Tool {tool_call['name']} is not defined!")
 return {"messages": new_messages}
def should_run_tools(state: MessagesState):
   last_message = state["messages"][-1]
 if last_message.tool_calls:
 return "call_tools"
 return END</pre>
<div><p class="normal">Now let’s bring<a id="_idIndexMarker438"/> everything together in a LangGraph workflow:</p>
<pre>builder = StateGraph(MessagesState)
builder.add_node("invoke_llm", invoke_llm)
builder.add_node("call_tools", call_tools)
builder.add_edge(START, "invoke_llm")
builder.add_conditional_edges("invoke_llm", should_run_tools)
builder.add_edge("call_tools", "invoke_llm")
graph = builder.compile()
question = "What is a square root of the current US president's age multiplied by 132?"
result = graph.invoke({"messages": [HumanMessage(content=question)]})
print(result["messages"][-1].content)
&gt;&gt; CALLED GOOGLE_SEARCH with query=age of Donald Trump
CALLED CALCULATOR with expression=78 * 132
CALLED CALCULATOR with expression=sqrt(10296)
The square root of 78 multiplied by 132 (which is 10296) is approximately 101.47.</pre>
<p class="normal">This <a id="_idIndexMarker439"/>demonstrates how the LLM made several calls to handle a complex question—first, to <code class="inlineCode">Google Search</code> and then two calls to <code class="inlineCode">Calculator</code>—and each time, it used the previously received information to adjust its actions. This is the ReACT pattern in action.</p>
<p class="normal">With that, we’ve learned how the ReACT pattern works in detail by building it ourselves. The good news is that LangGraph offers a pre-built implementation of a ReACT pattern, so you don’t need to implement it yourself:</p>
<pre>from langgraph.prebuilt import create_react_agent
agent = create_react_agent(
  llm=llm,
  tools=[search_tool, calculator_tool],
  prompt=system_prompt)</pre>
<div><p class="normal">In <a href="E_Chapter_6.xhtml#_idTextAnchor274"><em class="italic">Chapter 6</em></a>, we’ll see some additional adjustments you can use with the <code class="inlineCode">create_react_agent<a id="_idTextAnchor240"/></code> function.</p>
<h1 class="heading-1" id="_idParaDest-127"><a id="_idTextAnchor241"/>Defining tools</h1>
<p class="normal">So far, we <a id="_idIndexMarker440"/>have defined tools as OpenAPI schemas. But to run the workflow end to end, LangGraph should be able to call tools itself during the execution. Hence, in this section, let’s discuss how we define tools as Python functions or callables. </p>
<p class="normal">A LangChain tool has three essential components: </p>
<ul>
<li class="b lletList"><code class="inlineCode">Name</code>: A unique identifier for the tool </li>
<li class="b lletList"><code class="inlineCode">Description</code>: Text that helps the LLM understand when and how to use the tool </li>
<li class="b lletList"><code class="inlineCode">Payload schema</code>: A structured definition of the inputs the tool accepts</li>
</ul>
<p class="normal">It allows an LLM to decide when and how to call a tool. Another important distinction of a LangChain tool is that it can be executed by an orchestrator, such as LangGraph. The base interface for a tool is <code class="inlineCode">BaseTool</code>, which inherits from a <code class="inlineCode">RunnableSerializable</code> itself. That means it can be invoked or batched as any <code class="inlineCode">Runnable</code>, or serialized or deserialized as any <code class="inlineCode">Se<a id="_idTextAnchor242"/>rializable</code>.</p>
<h2 class="heading-2" id="_idParaDest-128"><a id="_idTextAnchor243"/>Built-in LangChain tools</h2>
<p class="normal">LangChain<a id="_idIndexMarker441"/> has many tools already available across various categories. Since<a id="_idIndexMarker442"/> tools are often provided by third-party vendors, some tools require paid API keys, some of them are completely free, and some of them have a free tier. Some tools are grouped together in toolkits—collections of tools that are supposed to be used together when working on a specific task.  Let’s see some examples of using tools.</p>
<p class="normal">Tools give an <a id="_idIndexMarker443"/>LLM access to search engines, such as Bing, DuckDuckGo, Google, and Tavily. Let’s take a look at <code class="inlineCode">DuckDuckGoSearchRun</code> as this search engine doesn’t require additional registration and an API key. </p>
<p class="normal">Please<a id="_idIndexMarker444"/> see <a href="E_Chapter_2.xhtml#_idTextAnchor044"><em class="italic">Chapter 2</em></a> for setup instructions. If you have any questions or encounter issues while running the code, please create an issue on GitHub or join the discussion on Discord at <a href="https://packt.link/lang">https://packt.link/lang</a>.</p>
<p class="normal">As with any tool, this tool has a name, description, and schema for input arguments:</p>
<pre>from langchain_community.tools import DuckDuckGoSearchRun
search = DuckDuckGoSearchRun()
print(f"Tool's name = {search.name}")</pre>
<div><pre>print(f"Tool's name = {search.description}")
print(f"Tool's arg schema = f{search.args_schema}")
&gt;&gt; Tool's name = fduckduckgo_search
Tool's name = fA wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.
Tool's arg schema = class 'langchain_community.tools.ddg_search.tool.DDGInput'</pre>
<p class="normal">The argument schema, <code class="inlineCode">arg_schema</code>, is a Pydantic model and we’ll see why it’s useful later in this chapter. We can explore its fields either programmatically or by going to the documentation page—it expects only one input field, a query: </p>
<pre>from langchain_community.tools.ddg_search.tool import DDGInput
print(DDGInput.__fields__)
&gt;&gt; {'query': FieldInfo(annotation=str, required=True, description='search query to look up')}</pre>
<p class="normal">Now we can invoke this tool and get a string output back (results from the search engine):</p>
<pre>query = "What is the weather in Munich like tomorrow?"
search_input = DDGInput(query=query)
result = search.invoke(search_input.dict())
print(result)</pre>
<p class="normal">We can also <a id="_idIndexMarker445"/>invoke the LLM with tools, and let’s make sure that the LLM invokes the search tool and does not answer directly:</p>
<pre>result = llm.invoke(query, tools=[search])
print(result.tool_calls[0])
&gt;&gt; {'name': 'duckduckgo_search', 'args': {'query': 'weather in Munich tomorrow'}, 'id': '222dc19c-956f-4264-bf0f-632655a6717d', 'type': 'tool_call'}</pre>
<p class="normal">Our tool is<a id="_idIndexMarker446"/> now a callable that LangGraph can call programmatically. Let’s put everything together and create our first agent. When we stream our graph, we get updates to the state. In our case, these are only messages:</p>
<pre>from langgraph.prebuilt import create_react_agent
agent = create_react_agent(model=llm, tools=[search])</pre>
<div><figure class="mediaobject"><img alt="Figure 5.2: A pre-built ReACT workflow on LangGraph" src="img/B32363_05_02.png"/></figure>
<p class="packt_figref">Figure 5.2: A pre-built ReACT workflow on LangGraph</p>
<p class="normal">That’s exactly what we saw earlier as well—an LLM is calling tools until it decides to stop and return the answer to the user. Let’s test it out! </p>
<p class="normal">When we stream LangGraph, we get new events that are updates to the graph’s state. We’re interested in the <code class="inlineCode">message</code> field of the state. Let’s print out the new messages added:</p>
<pre>for event in agent.stream({"messages": [("user", query)]}):
 update = event.get("agent", event.get("tools", {}))
 for message in update.get("messages", []):
    message.pretty_print()
&gt;&gt; ================================ Ai Message ==================================
Tool Calls:
  duckduckgo_search (a01a4012-bfc0-4eae-9c81-f11fd3ecb52c)
 Call ID: a01a4012-bfc0-4eae-9c81-f11fd3ecb52c
  Args:
    query: weather in Munich tomorrow
================================= Tool Message =================================
Name: duckduckgo_search
The temperature in Munich tomorrow in the early morning is 4 ° C… &lt;TRUNCATED&gt;
================================== Ai Message ==================================</pre>
<div><pre>The weather in Munich tomorrow will be 5°C with a 0% chance of rain in the morning.  The wind will blow at 11 km/h.  Later in the day, the high will be 53°F (approximately 12°C).  It will be clear in the early morning.</pre>
<p class="normal">Our <a id="_idIndexMarker447"/>agent is represented by a list of messages since this is the input<a id="_idIndexMarker448"/> and output that the LLM expects. We’ll see that pattern again when we dive deeper into agentic architectures and discuss it in the next chapter. For now, let’s briefly mention other types of tools that are already available on LangChain:</p>
<div><ul>
<li class="b lletList"><strong class="keyWord">Tools that enhance the LLM’s knowledge besides using a search engine</strong>:<ul><li class="bulletList level-2">Academic research: arXiv and PubMed</li>
<li class="bulletList level-2">Knowledge bases: Wikipedia and Wikidata</li>
<li class="bulletList level-2">Financial data: Alpha Vantage, Polygon, and Yahoo Finance</li>
<li class="bulletList level-2">Weather: OpenWeatherMap</li>
<li class="bulletList level-2">Computation: Wolfram Alpha</li>
</ul></li>
<li class="b lletList"><strong class="keyWord">Tools that enhance your productivity</strong>: You can interact with Gmail, Slack, Office 365, Google Calendar, Jira, Github, etc. For example, <code class="inlineCode">GmailToolkit</code> gives you access to <code class="inlineCode">GmailCreateDraft</code>, <code class="inlineCode">GmailSendMessage</code>, <code class="inlineCode">GmailSearch</code>, <code class="inlineCode">GmailGetMessage</code>, and <code class="inlineCode">GmailGetThread</code> tools that allow you to search, retrieve, create, and send messages with your Gmail account. As you can see, not only can you give the LLM additional context about the user but, with some of these tools, LLMs can take actions that actually influence the outside environment, such as creating a pull request on GitHub or sending a message on Slack!</li>
<li class="b lletList"><strong class="keyWord">Tools that give an LLM access to a code interpreter</strong>: These tools give LLMs access to a code interpreter by remotely launching an isolated container and giving LLMs access to this container. These tools require an API key from a vendor providing the sandboxes. LLMs are especially good at coding, and it’s a widely used pattern to ask <a id="_idIndexMarker449"/>an LLM to solve some complex task by writing code that solves it instead of asking it to generate tokens that represent the solution of <a id="_idIndexMarker450"/>the task. Of course, you should execute code generated by LLMs with caution, and that’s why isolated sandboxes play a huge role. Some examples are:<ul><li class="bulletList level-2">Code execution: Python REPL and Bash</li>
<li class="bulletList level-2">Cloud services: AWS Lambda</li>
<li class="bulletList level-2">API tools: GraphQL and Requests</li>
<li class="bulletList level-2">File operations: File System</li>
</ul></li>
<li class="b lletList"><strong class="keyWord">Tools that give an LLM access to databases by writing and executing SQL code</strong>: For example, <code class="inlineCode">SQLDatabase</code> includes tools to get information about the database and its objects and execute SQL queries. You can also access Google Drive with <code class="inlineCode">GoogleDriveLoader</code> or perform operations with usual file system tools from a <code class="inlineCode">FileManagementToolkit</code>.</li>
<li class="b lletList"><strong class="keyWord">Other tools</strong>: These comprise tools that integrate third-party systems and allow the LLM to gather additional information or act. There are also tools that can integrate data retrieval from Google Maps, NASA, and other platforms and organizations.</li>
<li class="b lletList"><strong class="keyWord">Tools for using other AI systems or automation</strong>:<ul><li class="bulletList level-2">Image generation: DALL-E and Imagen</li>
<li class="bulletList level-2">Speech synthesis: Google Cloud TTS and Eleven Labs</li>
<li class="bulletList level-2">Model access: Hugging Face Hub</li>
<li class="bulletList level-2">Workflow automation: Zapier and IFTTT</li>
</ul></li>
</ul>
<p class="normal">Any external system with an API can be wrapped as a tool if it enhances an LLM like this: </p>
<ul>
<li class="b lletList">Provides relevant domain knowledge to the user or the workflow</li>
<li class="b lletList">Allows an LLM to take actions on the user’s behalf </li>
</ul>
<p class="normal">When integrating such tools with LangChain, consider these key aspects:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Authentication</strong>: Secure access to the external system</li>
<li class="b lletList"><strong class="keyWord">Payload schema</strong>: Define proper data structures for input/output</li>
<li class="b lletList"><strong class="keyWord">Error handling</strong>: Plan for failures and edge cases</li>
<li class="b lletList"><strong class="keyWord">Safety considerations</strong>: For example, when developing a SQL-to-text agent, restrict access to read-only operations to prevent unintended modifications</li>
</ul>
<p class="normal">Therefore, an<a id="_idIndexMarker451"/> important toolkit is the <code class="inlineCode">RequestsToolkit</code>, which allows one to easily wrap any HTTP API:</p>
<pre>from langchain_community.agent_toolkits.openapi.toolkit import RequestsToolkit
from langchain_community.utilities.requests import TextRequestsWrapper
toolkit = RequestsToolkit(
   requests_wrapper=TextRequestsWrapper(headers={}),
   allow_dangerous_requests=True,
)</pre>
<div><pre>for tool in toolkit.get_tools():
 print(tool.name)
&gt;&gt; requests_get
requests_post
requests_patch
requests_put
requests_delete</pre>
<p class="normal">Let’s take<a id="_idIndexMarker452"/> a free open-source currency API (<a href="https://frankfurter.dev/">https://frankfurter.dev/</a>). It’s a random free API we took from the Internet for illustrative purposes only, just to show you how you can wrap any existing API as a tool. First, we need to put together an API spec based on the OpenAPI format. We truncated the spec but you can find the full version on our GitHub:</p>
<pre>api_spec = """
openapi: 3.0.0
info:
 title: Frankfurter Currency Exchange API
 version: v1
 description: API for retrieving currency exchange rates. Pay attention to the base currency and change it if needed.
servers:
 - url: https://api.frankfurter.dev/v1
paths:
 /v1/latest:
   get:
     summary: Get the latest exchange rates.
     parameters:
       - in: query
         name: symbols
         schema:
           type: string
         description: Comma-separated list of currency symbols to retrieve rates for. Example: CHF,GBP
       - in: query
         name: base
         schema:</pre>
<div><pre>           type: string
         description: The base currency for the exchange rates. If not provided, EUR is used as a base currency. Example: USD
   /v1/{date}:
   ...
"""</pre>
<p class="normal">Now let’s <a id="_idIndexMarker453"/>build and<a id="_idIndexMarker454"/> run our ReACT agent; we’ll see that the LLM can query the third-party API and provide fresh answers on currency exchange rates:</p>
<pre>system_message = (
 "You're given the API spec:\n{api_spec}\n"
 "Use the API to answer users' queries if possible. "
)
agent = create_react_agent(llm, toolkit.get_tools(), state_modifier=system_message.format(api_spec=api_spec))
query = "What is the swiss franc to US dollar exchange rate?"
events = agent.stream(
   {"messages": [("user", query)]},
   stream_mode="values",
)
for event in events:
   event["messages"][-1].pretty_print()
&gt;&gt; ============================== Human Message =================================
What is the swiss franc to US dollar exchange rate?
================================== Ai Message ==================================
Tool Calls:
  requests_get (541a9197-888d-4ffe-a354-c726804ad7ff)
 Call ID: 541a9197-888d-4ffe-a354-c726804ad7ff
  Args:
    url: https://api.frankfurter.dev/v1/latest?symbols=CHF&amp;base=USD</pre>
<div><pre>================================= Tool Message =================================
Name: requests_get
{"amount":1.0,"base":"USD","date":"2025-01-31","rates":{"CHF":0.90917}}
================================== Ai Message ==================================
The Swiss franc to US dollar exchange rate is 0.90917.</pre>
<p class="normal">Observe<a id="_idIndexMarker455"/> that, this time, we use a <code class="inlineCode">stream_mode="values"</code> option, and in this option, each time, we get a full current state from the graph. </p>
<div><div><p class="normal">There are over 50 tools already available. You can find a full list on the documentation page: <a href="https://python.langchain.com/docs/integrations/tools/">https://python.langchain.com/docs/integrations/tools/</a>. </p>
</div>
</div>
<h2 class="heading-2" id="_idParaDest-129"><a id="_idTextAnchor244"/>Custom tools</h2>
<p class="normal">We looked<a id="_idIndexMarker456"/> at the<a id="_idIndexMarker457"/> variety of built-in tools offered by LangGraph. Now it’s time to discuss how you can create your own custom tools, besides the example we looked at when we wrapped the third-party API with the <code class="inlineCode">RequestsToolkit</code> by providing an API <a id="_idTextAnchor245"/>spec. Let’s get down to it!</p>
<h3 class="heading-3" id="_idParaDest-130"><a id="_idTextAnchor246"/>Wrapping a Python function as a tool</h3>
<p class="normal">Any <code class="inlineCode">Python</code> function (or callable) can be wrapped as a tool. As we remember, a tool on LangChain<a id="_idIndexMarker458"/> should have a name, a description, and an argument schema. Let’s build our own calculator based on the Python <code class="inlineCode">numexr</code> library—a fast numerical expression evaluator based on NumPy (<a href="https://github.com/pydata/numexpr">https://github.com/pydata/numexpr</a>). We’re going to use a special <code class="inlineCode">@tool</code> decorator that will wrap our function as a tool:</p>
<pre>import math
from langchain_core.tools import tool
import numexpr as ne
@tool
def calculator(expression: str) -&gt; str:
 """Calculates a single mathematical expression, incl. complex numbers.
 </pre>
<div><pre>   Always add * to operations, examples:
     73i -&gt; 73*i
     7pi**2 -&gt; 7*pi**2
   """
   math_constants = {"pi": math.pi, "i": 1j, "e": math.exp}
   result = ne.evaluate(expression.strip(), local_dict=math_constants)
   return str(result)</pre>
<p class="normal">Let’s<a id="_idIndexMarker459"/> explore the calculator object we have! Notice that LangChain auto-inherited the name, the description, and args schema from the docstring and type hints. Please note that we used a few-shot technique (discussed in <a href="E_Chapter_3.xhtml#_idTextAnchor107"><em class="italic">Chapter 3</em></a>) to teach LLMs how to prepare the payload for our tool by adding two examples in the docstring:</p>
<pre>from langchain_core.tools import BaseTool
assert isinstance(calculator, BaseTool)
print(f"Tool schema: {calculator.args_schema.model_json_schema()}")
&gt;&gt; Tool schema: {'description': 'Calculates a single mathematical expression, incl. complex numbers.\n\nAlways add * to operations, examples:\n  73i -&gt; 73*i\n  7pi**2 -&gt; 7*pi**2', 'properties': {'expression': {'title': 'Expression', 'type': 'string'}}, 'required': ['expression'], 'title': 'calculator', 'type': 'object'}</pre>
<p class="normal">Let’s try out our new tool to evaluate an expression with complex numbers, which extend real numbers with a special imaginary unit <code class="inlineCode">i</code> that has a property <code class="inlineCode">i**2=-1</code>:</p>
<pre>query = "How much is 2+3i squared?"
agent = create_react_agent(llm, [calculator])
for event in agent.stream({"messages": [("user", query)]}, stream_mode="values"):
   event["messages"][-1].pretty_print()
&gt;&gt; ===============================Human Message =================================
How much is 2+3i squared?
================================== Ai Message ==================================
Tool Calls:</pre>
<div><pre>  calculator (9b06de35-a31c-41f3-a702-6e20698bf21b)
 Call ID: 9b06de35-a31c-41f3-a702-6e20698bf21b
  Args:
    expression: (2+3*i)**2
================================= Tool Message =================================
Name: calculator
(-5+12j)
================================== Ai Message ==================================
(2+3i)² = -5+12i.</pre>
<p class="normal">With<a id="_idIndexMarker460"/> just a few lines of code, we’ve successfully extended our LLM’s capabilities to work with complex numbers. Now we can put together the example we started with:</p>
<pre>question = "What is a square root of the current US president's age multiplied by 132?"
system_hint = "Think step-by-step. Always use search to get the fresh information about events or public facts that can change over time."
agent = create_react_agent(
   llm, [calculator, search],
   state_modifier=system_hint)
for event in agent.stream({"messages": [("user", question)]}, stream_mode="values"):
   event["messages"][-1].pretty_print()
print(event["messages"][-1].content)
&gt;&gt; The square root of Donald Trump's age multiplied by 132 is approximately 101.47.</pre>
<p class="normal">We <a id="_idIndexMarker461"/>haven’t provided the full output here in the book (you can find it on our GitHub), but if you run this snippet, you should see that the LLM was able to query tools step by step:</p>
<div><ol>
<li class="numberedList" value="1">It called the search engine with the query <code class="inlineCode">"current US president"</code>.</li>
<li class="numberedList">Then, it again called the search engine with the query <code class="inlineCode">"donald trump age"</code>.</li>
<li class="numberedList">As the last step, the LLM called the calculator tool with the expression <code class="inlineCode">"sqrt(78*132)"</code>.</li>
<li class="numberedList">Finally, it returned the correct answer to the user.</li>
</ol>
<p class="normal">At every step, the LLM reasoned based on the previously collected information and then acted with an appropriate tool—that’s the<a id="_idTextAnchor247"/> essence of the ReACT approach.</p>
<h3 class="heading-3" id="_idParaDest-131"><a id="_idTextAnchor248"/>Creating a tool from a Runnable</h3>
<p class="normal">Sometimes, LangChain <a id="_idIndexMarker462"/>might not be able to derive a passing description or args schema from a function, or we might be using a complex callable that is difficult to wrap with a decorator. For example, we can use another LangChain chain or LangGraph graph as a tool. We can create a tool from any <code class="inlineCode">Runnable</code> by explicitly specifying all needed descriptions. Let’s create a calculator tool from a function in an alternative fashion, and we will tune the retry behavior (in our case, we’re going to retry three times and add an exponential backoff between consecutive attempts):</p>
<div><div><p class="normal">Please note that we use the same function as above but we removed the <code class="inlineCode">@tool</code> decorator.</p>
</div>
</div>
<pre>from langchain_core.runnables import RunnableLambda, RunnableConfig
from langchain_core.tools import tool, convert_runnable_to_tool
def calculator(expression: str) -&gt; str:
   math_constants = {"pi": math.pi, "i": 1j, "e": math.exp}
   result = ne.evaluate(expression.strip(), local_dict=math_constants)
 return str(result)
calculator_with_retry = RunnableLambda(calculator).with_retry(
   wait_exponential_jitter=True,
   stop_after_attempt=3,
)
calculator_tool = convert_runnable_to_tool(
   calculator_with_retry,
   name="calculator",</pre>
<div><pre>   description=(
 "Calculates a single mathematical expression, incl. complex numbers."
 "'\nAlways add * to operations, examples:\n73i -&gt; 73*i\n"
 "7pi**2 -&gt; 7*pi**2"
   ),
   arg_types={"expression": "str"},
)</pre>
<p class="normal">Observe <a id="_idIndexMarker463"/>that we defined our function in a similar way to how we define LangGraph nodes—it takes a state (which now is a Pydantic model) and a config. Then, we wrapped this function as <code class="inlineCode">RunnableLambda</code> and added retries. It might be useful if we want to keep our Python function as a function without wrapping it with a decorator, or if we want to wrap an external API (hence, description and arguments schema can’t be auto-inherited from the docstrings). We can use any Runnable (for example, a chain or a graph) to create a tool, and that allows us to build multi-agent systems since now one LLM-based workflow can invoke another LLM-based one. Let’s convert our Runnable to a tool:</p>
<pre>calculator_tool = convert_runnable_to_tool(
   calculator_with_retry,
   name="calculator",
   description=(
 "Calculates a single mathematical expression, incl. complex numbers."
 "'\nAlways add * to operations, examples:\n73i -&gt; 73*i\n"
 "7pi**2 -&gt; 7*pi**2"
   ),
   arg_types={"expression": "str"},
)</pre>
<p class="normal">Let’s test our new <code class="inlineCode">calculator</code> function with the LLM:</p>
<pre>llm.invoke("How much is (2+3i)**2", tools=[calculator_tool]).tool_calls[0]
&gt;&gt; {'name': 'calculator',
 'args': {'__arg1': '(2+3*i)**2'},
 'id': '46c7e71c-4092-4299-8749-1b24a010d6d6',
 'type': 'tool_call'}</pre>
<div><p class="normal">As you can note, LangChain didn’t inherit the <code class="inlineCode">args</code> schema fully; that’s why it created artificial names for arguments like <code class="inlineCode">__arg1</code>. Let’s change our tool to accept a Pydantic model instead, in a similar<a id="_idIndexMarker464"/> fashion to how we define LangGraph nodes:</p>
<pre>from pydantic import BaseModel, Field
from langchain_core.runnables import RunnableConfig
class CalculatorArgs(BaseModel):
   expression: str = Field(description="Mathematical expression to be evaluated")
def calculator(state: CalculatorArgs, config: RunnableConfig) -&gt; str:
   expression = state["expression"]
   math_constants = config["configurable"].get("math_constants", {})
   result = ne.evaluate(expression.strip(), local_dict=math_constants)
 return str(result)</pre>
<p class="normal">Now the full schema is a proper one:</p>
<pre>assert isinstance(calculator_tool, BaseTool)
print(f"Tool name: {calculator_tool.name}")
print(f"Tool description: {calculator_tool.description}")
print(f"Args schema: {calculator_tool.args_schema.model_json_schema()}")
&gt;&gt; Tool name: calculator
Tool description: Calculates a single mathematical expression, incl. complex numbers.'
Always add * to operations, examples:
73i -&gt; 73*i
7pi**2 -&gt; 7*pi**2
Args schema: {'properties': {'expression': {'title': 'Expression', 'type': 'string'}}, 'required': ['expression'], 'title': 'calculator', 'type': 'object'}</pre>
<p class="normal">Let’s test it together with an LLM:</p>
<pre>tool_call = llm.invoke("How much is (2+3i)**2", tools=[calculator_tool]).tool_calls[0]
print(tool_call)
&gt;&gt; {'name': 'calculator', 'args': {'expression': '(2+3*i)**2'}, 'id': 'f8be9cbc-4bdc-4107-8cfb-fd84f5030299', 'type': 'tool_call'}</pre>
<div><p class="normal">We can <a id="_idIndexMarker465"/>call our calculator tool and pass it to the LangGraph configuration in runtime:</p>
<pre>math_constants = {"pi": math.pi, "i": 1j, "e": math.exp}
config = {"configurable": {"math_constants": math_constants}}
calculator_tool.invoke(tool_call["args"], config=config)
&gt;&gt; (-5+12j)</pre>
<p class="normal">With that, we have learned how we can easily convert any Runnable to a tool by providing additional details to LangChain to ensure an <a id="_idTextAnchor249"/>LLM can correctly handle this tool.</p>
<h3 class="heading-3" id="_idParaDest-132"><a id="_idTextAnchor250"/>Subclass StructuredTool or BaseTool</h3>
<p class="normal">Another <a id="_idIndexMarker466"/>method to define a tool is by creating a custom tool by subclassing the <code class="inlineCode">BaseTool</code> class. As with other approaches, you must specify the tool’s name, description, and argument schema. You’ll also need to implement one or two abstract methods: <code class="inlineCode">_run</code> for synchronous execution and, if necessary, <code class="inlineCode">_arun</code> for asynchronous behavior (if it differs from simply wrapping the sync version). This option is particularly useful when your tool needs to be stateful (for example, to maintain long-lived connection clients) or when its logic is too complex to be implemented as a single function or <code class="inlineCode">Runnable</code>.</p>
<p class="normal">If you want more flexibility than a <code class="inlineCode">@tool</code> decorator gives you but don’t want to implement your own class, there’s an intermediate approach. You can also use the <code class="inlineCode">StructuredTool.from_function</code> class method, which allows you to explicitly specify tools’ meta parameters such as description or <code class="inlineCode">args_schema</code> with a few lines of code only:</p>
<pre>from langchain_core.tools import StructuredTool
calculator_tool = StructuredTool.from_function(
   name="calculator",
   description=(
 "Calculates a single mathematical expression, incl. complex numbers."),
   func=calculator,
   args_schema=CalculatorArgs
)
tool_call = llm.invoke(
 "How much is (2+3i)**2", tools=[calculator_tool]).tool_calls[0]</pre>
<div><p class="normal">One last note about synchronous and asynchronous implementations is necessary at this point. If an underlying function besides your tool is a synchronous function, LangChain will wrap it for the tool’s asynchronous implementation by launching it in a separate thread. In most cases, it <a id="_idIndexMarker467"/>doesn’t matter, but if you care about the additional overhead of creating a separate thread, you have two options—either subclass from the <code class="inlineCode">BaseClass</code> and override async implementation, or create a separate async implementation of your function and pass it to the <code class="inlineCode">StructruredTool.from_function</code> as a <code class="inlineCode">coroutine</code> argument. You can also provide only async implementation, but then you won’t be able to invoke your workflows in a synchronous manner.</p>
<p class="normal">To conclude, let’s take another look at three options that we have to create a LangChain tool, and when to use each of them.</p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-5">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Method to create a tool</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">When to use</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">@tool decorator</p>
</td>
<td class="No-Table-Style">
<p class="normal">You have a function with clear docstrings and this function isn’t used anywhere in your code </p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">convert_runnable_to_tool</p>
</td>
<td class="No-Table-Style">
<p class="normal">You have an existing Runnable, or you need more detailed controlled on how arguments or tool descriptions are passed to an LLM (you wrap an existing function by a RunnableLambda in that case)</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">subclass from StructuredTool or BaseTool</p>
</td>
<td class="No-Table-Style">
<p class="normal">You need full control over tool description and logic (for example, you want to handle sync and async requests differently)</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 5.1: Options to create a LangChain tool</p>
<p class="normal">When an LLM generates payloads and calls tools, it might hallucinate or make other mistakes. Therefore, we need to carefully think about error handling. </p>
<h2 class="heading-2" id="_idParaDest-133"><a id="_idTextAnchor251"/>Error handling</h2>
<p class="normal">We already<a id="_idIndexMarker468"/> discussed error handling in <a href="E_Chapter_3.xhtml#_idTextAnchor107"><em class="italic">Chapter 3</em></a>, but it becomes even more important when you enhance an LLM with tools; you need logging, working with exceptions, and <a id="_idIndexMarker469"/>so on even more. One additional consideration is to think about whether you would like your workflow to continue and try to auto-recover if one of your tools fails. LangChain has a special <code class="inlineCode">ToolException</code> that allows the workflow to continue its execution by handling the exception.</p>
<p class="normal"><code class="inlineCode">BaseTool</code> has two special flags: <code class="inlineCode">handle_tool_error</code> and <code class="inlineCode">handle_validation_error</code>. Of course, since <code class="inlineCode">StructuredTool</code> inherits from <code class="inlineCode">BaseTool</code>, you can pass these flags to the <code class="inlineCode">StructuredTool.from_function </code>class method. If this flag is set, LangChain would construct a string to return as a result of tools’ execution if either a <code class="inlineCode">ToolException</code> or a Pydantic <code class="inlineCode">ValidationException</code> (when validating input payload) happens.</p>
<div><p class="normal">To <a id="_idIndexMarker470"/>understand <a id="_idIndexMarker471"/>what happens, let’s take a look at the LangChain source code for the <code class="inlineCode">_handle_tool_error</code> function:</p>
<pre>def _handle_tool_error(
    e: ToolException,
    *,
    flag: Optional[Union[Literal[True], str, Callable[[ToolException], str]]],
) -&gt; str:
 if isinstance(flag, bool):
        content = e.args[0] if e.args else "Tool execution error"
 elif isinstance(flag, str):
        content = flag
 elif callable(flag):
        content = flag(e)
 else:
        msg = (
 f"Got an unexpected type of `handle_tool_error`. Expected bool, str "
 f"or callable. Received: {flag}"
        )
 raise ValueError(msg)  # noqa: TRY004
 return content</pre>
<p class="normal">As we can see, we can set this flag to a Boolean, string, or callable (that converts a <code class="inlineCode">ToolException</code> to a string). Based on this, LangChain would try to handle <code class="inlineCode">ToolException </code>and pass a string to the next stage instead. We can incorporate this feedback into our workflow and add an auto-recover loop.</p>
<p class="normal">Let’s look at an example. We adjust our <code class="inlineCode">calculator</code> function by removing a substitution <code class="inlineCode">i-&gt;j</code> (a substitution from an imaginary unit in math to an imaginary unit in Python), and we also make <code class="inlineCode">StructuredTool</code> auto-inherit descriptions and <code class="inlineCode">arg_schema</code> from the docstring:</p>
<pre>from langchain_core.tools import StructuredTool
def calculator(expression: str) -&gt; str:
 """Calculates a single mathematical expression, incl. complex numbers."""
 return str(ne.evaluate(expression.strip(), local_dict={}))</pre>
<div><pre>calculator_tool = StructuredTool.from_function(
   func=calculator,
   handle_tool_error=True
)
agent = create_react_agent(
   llm, [calculator_tool])
for event in agent.stream({"messages": [("user", "How much is (2+3i)^2")]}, stream_mode="values"):
   event["messages"][-1].pretty_print()
&gt;&gt; ============================== Human Message =================================
How much is (2+3i)^2
================================== Ai Message ==================================
Tool Calls:
  calculator (8bfd3661-d2e1-4b8d-84f4-0be4892d517b)
 Call ID: 8bfd3661-d2e1-4b8d-84f4-0be4892d517b
  Args:
    expression: (2+3i)^2
================================= Tool Message =================================
Name: calculator
Error: SyntaxError('invalid decimal literal', ('&lt;expr&gt;', 1, 4, '(2+3i)^2', 1, 4))
 Please fix your mistakes.
================================== Ai Message ==================================
(2+3i)^2 is equal to -5 + 12i.  I tried to use the calculator tool, but it returned an error. I will calculate it manually for you.
(2+3i)^2 = (2+3i)*(2+3i) = 2*2 + 2*3i + 3i*2 + 3i*3i = 4 + 6i + 6i - 9 = -5 + 12i</pre>
<div><p class="normal">As we can see, now <a id="_idIndexMarker472"/>our execution of a calculator fails, but since the error <a id="_idIndexMarker473"/>description is not clear enough, the LLM decides to respond itself without using the tool. Depending on your use case, you might want to adjust the behavior; for example, provide more meaningful errors from the tool, force the workflow to try to adjust the payload for the tool, etc.</p>
<p class="normal">LangGraph also offers a built-in <code class="inlineCode">ValidationNode</code> that takes the last messages (by inspecting the <code class="inlineCode">messages</code> key in the graph’s state) and checks whether it has tool calls. If that’s the case, LangGraph <a id="_idIndexMarker474"/>validates the schema of the tool call, and if it doesn’t follow the expected <a id="_idIndexMarker475"/>schema, it raises a <code class="inlineCode">ToolMessage</code> with the validation error (and a default command to fix it). You can add a conditional edge that cycles back to the LLM and then the LLM would regenerate the tool call, similar to the pattern we discussed in <a href="E_Chapter_3.xhtml#_idTextAnchor107"><em class="italic">Chapter 3</em></a>.</p>
<p class="normal">Now that we’ve learned what a tool is, how to create one, and how to use built-in LangChain tools, it’s time to take a look at additional instructions that you can pass to an LLM on how to use<a id="_idTextAnchor252"/> tools.</p>
<h1 class="heading-1" id="_idParaDest-134"><a id="_idTextAnchor253"/>Advanced tool-calling capabilities</h1>
<p class="normal">Many LLMs <a id="_idIndexMarker476"/>offer you some additional configuration options on tool calling. First, some models support parallel function calling—specifically, an LLM can call multiple tools at once. LangChain natively supports this since the <code class="inlineCode">tool_calls</code> field of an <code class="inlineCode">AIMessage</code> is a list. When you return <code class="inlineCode">ToolMessage</code> objects as function call results, you should carefully match the <code class="inlineCode">tool_call_id</code> field of a <code class="inlineCode">ToolMessage</code> to the generated payload. This alignment is necessary so that LangChain and the underlying LLM can match them together when doing the next turn.</p>
<p class="normal">Another advanced capability is forcing an LLM to call a tool, or even to call a specific tool. Generally speaking, an LLM decides whether it should call a tool, and if it should, which tool to call from the list of provided tools. Typically, it’s handled by <code class="inlineCode">tool_choice</code> and/or <code class="inlineCode">tool_config</code> arguments passed to the <code class="inlineCode">invoke</code> method, but implementation depends on the model’s provider. Anthropic, Google, OpenAI, and other major providers have slightly different APIs, and although LangChain tries to unify arguments, in such cases, you should double-check details by the model’s provider.</p>
<p class="normal">Typically, the following options are available:</p>
<ul>
<li class="b lletList"><code class="inlineCode">"auto"</code>: An LLM can respond or call one or many tools. </li>
<li class="b lletList"><code class="inlineCode">"any"</code>: An LLM is forced to respond by calling one or many tools.</li>
<li class="b lletList"><code class="inlineCode">"tool"</code> or <code class="inlineCode">"any"</code> with a provided list of tools: An LLM is forced to respond by calling a tool from the restricted list.</li>
<li class="b lletList"><code class="inlineCode">"None"</code>: An LLM is forced to respond without calling a tool.</li>
</ul>
<div><p class="normal">Another important thing to keep in mind is that schemas might become pretty complex—i.e., they might have nullable fields or nested fields, include enums, or reference other schemas. Depending on the model’s provider, some definitions might not be supported (and you will see warning or compiling errors). Although LangChain aims to make switching across vendors seamless, for some complex workflows, this might not be the case, so pay attention to warnings in the error logs. Sometimes, compilations of a provided schema to a schema supported by the model’s provider are done on the best effort basis—for example, a field with a type of <code class="inlineCode">Union[str, int]</code> is compiled to a <code class="inlineCode">str</code> type if an underlying LLM doesn’t support <code class="inlineCode">Union</code> types with tool calling. You’ll get a warning, but ignoring such a warning during a migration might change the behavior of your application unpredictably.</p>
<p class="normal">As a final <a id="_idIndexMarker477"/>note, it is worth mentioning that some providers (for example, OpenAI or Google) offer custom tools, such as a code interpreter or Google search, that can be invoked by the model itself, and the model will use the tool’s output to prepare a final generation. You can think of this as a ReACT agent on the provider’s side, where the model receives an enhanced response based on a tool it calls. This approach reduces latency and costs. In these cases, you typically supply the LangChain wrapper with a custom tool created using the provider’s SDK rather than one built with LangChain (i.e., a tool that doesn’t inherit from the <code class="inlineCode">BaseTool</code> class), which means your code won’t be transferable acros<a id="_idTextAnchor254"/>s models.</p>
<h1 class="heading-1" id="_idParaDest-135"><a id="_idTextAnchor255"/>Incorporating tools into workflows</h1>
<p class="normal">Now that we know how to create and use tools, let’s discuss how we can incorporate the tool-calling paradigm deeper into the workflows we’re de<a id="_idTextAnchor256"/>veloping.</p>
<h2 class="heading-2" id="_idParaDest-136"><a id="_idTextAnchor257"/>Controlled generation</h2>
<p class="normal">In <a href="E_Chapter_3.xhtml#_idTextAnchor107"><em class="italic">Chapter 3</em></a>, we<a id="_idIndexMarker478"/> started to discuss a <em class="italic">controlled</em> generation, when you want an LLM to follow a specific schema. We can improve our parsing workflows not only by creating more sophisticated and reliable parsers but also by being more strict in forcing an LLM to adhere to a certain schema. Calling a tool requires controlled generation since the generated payload should follow a specific schema, but we can take a step back and substitute our expected schema with a forced tool calling that follows the expected schema. LangChain has a built-in mechanism to help with that—an LLM has the <code class="inlineCode">with_structured_output</code> method that takes a schema as a Pydantic model, converts it to a tool, invokes the LLM with a given prompt by forcing it to call this tool, and parses the output by compiling to a corresponding Pydantic model instance.</p>
<div><p class="normal">Later in this chapter, we’ll discuss a plan-and-solve agent, so let’s start preparing a building block. Let’s ask our LLM to generate a plan for a given action, but instead of parsing the plan, let’s define it as a Pydantic model (a <code class="inlineCode">Plan</code> is a list of <code class="inlineCode">Steps</code>):</p>
<pre>from pydantic import BaseModel, Field
class Step(BaseModel):
 """A step that is a part of the plan to solve the task."""
   step: str = Field(description="Description of the step")
class Plan(BaseModel):
 """A plan to solve the task."""
   steps: list[Step]</pre>
<p class="normal">Keep in mind that we use nested models (one field is referencing another), but LangChain will compile a unified schema for us. Let’s put together a simple workflow and run it:</p>
<pre>prompt = PromptTemplate.from_template(
 "Prepare a step-by-step plan to solve the given task.\n"
 "TASK:\n{task}\n"
)
result = (prompt | llm.with_structured_output(Plan)).invoke(
 "How to write a bestseller on Amazon about generative AI?")</pre>
<p class="normal">If we<a id="_idIndexMarker479"/> inspect the output, we’ll see that we got a Pydantic model as a result. We don’t need to parse the output anymore; we got a list of specific steps out of the box (and later, we’ll see how we can use it further):</p>
<pre>assert isinstance(result, Plan)
print(f"Amount of steps: {len(result.steps)}")
for step in result.steps:
 print(step.step)
 break
&gt;&gt; Amount of steps: 21
**1. Idea Generation and Va<a id="_idTextAnchor258"/>lidation:**</pre>
<div><h3 class="heading-3" id="_idParaDest-137"><a id="_idTextAnchor259"/>Controlled generation provided by the vendor</h3>
<p class="normal">Another <a id="_idIndexMarker480"/>way is vendor-dependent. Some foundational model providers offer additional API parameters that can instruct a model to generate a structured output (typically, a JSON or enum). You can force the model to use JSON generation the same way as above using <code class="inlineCode">with_structured_output</code>, but provide another argument, <code class="inlineCode">method="json_mode"</code> (and double-check that the underlying model provider supports controlled generation as JSON):</p>
<pre>plan_schema = {
 "type": "ARRAY",
 "items": {
 "type": "OBJECT",
 "properties": {
 "step": {"type": "STRING"},
         },
     },
}
query = "How to write a bestseller on Amazon about generative AI?"
result = (prompt | llm.with_structured_output(schema=plan_schema, method="json_mode")).invoke(query)</pre>
<p class="normal">Note<a id="_idIndexMarker481"/> that the JSON schema doesn’t contain descriptions of the fields, hence typically, your prompts should be more detailed and informative. But as an output, we get a full-qualified Python dictionary:</p>
<pre>assert(isinstance(result, list))
print(f"Amount of steps: {len(result)}")
print(result[0])
&gt;&gt; Amount of steps: 10
{'step': 'Step 1: Define your niche and target audience. Generative AI is a broad topic. Focus on a specific area, like generative AI in marketing, art, music, or writing. Identify your ideal reader (such as  marketers, artists, developers).'}</pre>
<p class="normal">You can instruct the LLM instance directly to follow controlled generation instructions. Note that specific arguments and functionality might vary from one model provider to another (for example, OpenAI models use a <code class="inlineCode">response_format</code> argument). Let’s look at how to instruct Gemini to return JSON:</p>
<div><pre>from langchain_core.output_parsers import JsonOutputParser
llm_json = ChatVertexAI(
  model_name="gemini-1.5-pro-002", response_mime_type="application/json",
  response_schema=plan_schema)
result = (prompt | llm_json | JsonOutputParser()).invoke(query)
assert(isinstance(result, list))</pre>
<p class="normal">We can also ask Gemini to return an enum—in other words, only one value from a set of values:</p>
<pre>from langchain_core.output_parsers import StrOutputParser
response_schema = {"type": "STRING", "enum": ["positive", "negative", "neutral"]}
prompt = PromptTemplate.from_template(
 "Classify the tone of the following customer's review:"
 "\n{review}\n"
)
review = "I like this movie!"
llm_enum = ChatVertexAI(model_name="gemini-1.5-pro-002", response_mime_type="text/x.enum", response_schema=response_schema)
result = (prompt | llm_enum | StrOutputParser()).invoke(review)
print(result)
&gt;&gt; positive</pre>
<p class="normal">LangChain abstracts<a id="_idIndexMarker482"/> the details of the model provider’s implementation with the <code class="inlineCode">method="json_mode"</code> parameter or by allowing custom <code class="inlineCode">kwargs</code> to be passed to the model. Some of the controlled generation capabilities are model-specific. Check your model’s documentation for supported schema types, constraints, <a id="_idTextAnchor260"/>and arguments.</p>
<h2 class="heading-2" id="_idParaDest-138"><a id="_idTextAnchor261"/>ToolNode</h2>
<p class="normal">To simplify <a id="_idIndexMarker483"/>agent development, LangGraph has built-in capabilities such as <code class="inlineCode">ToolNode</code> and <code class="inlineCode">tool_conditions</code>. The <code class="inlineCode">ToolNode</code> checks the last message in <code class="inlineCode">messages</code> (you can redefine the key name). If this message contains tool calls, it invokes the corresponding tools and updates the state. On the other hand, <code class="inlineCode">tool_conditions</code> is a conditional edge that checks whether <code class="inlineCode">ToolNode</code> should be called (or finishes otherwise). </p>
<div><p class="normal">Now we can build our ReACT engine in minutes:</p>
<pre>from langgraph.prebuilt import ToolNode, tools_condition
def invoke_llm(state: MessagesState):
 return {"messages": [llm_with_tools.invoke(state["messages"])]}
builder = StateGraph(MessagesState)
builder.add_node("invoke_llm", invoke_llm)
builder.add_node("tools", ToolNode([search, calculator]))
builder.add_edge(START, "invoke_llm")
builder.add_conditional_edges("invoke_llm", tools_condition)
builder.add_edge("tools", "invoke_llm")
graph = bu<a id="_idTextAnchor262"/>ilder.compile()</pre>
<h2 class="heading-2" id="_idParaDest-139"><a id="_idTextAnchor263"/>Tool-calling paradigm</h2>
<p class="normal">Tool calling is<a id="_idIndexMarker484"/> a very powerful design paradigm that requires a change in how you develop your applications. In many cases, instead of performing rounds of prompt engineering and many attempts to improve your prompts, think whether you could ask the model to call a tool instead.</p>
<p class="normal">Let’s assume we’re working on an agent that deals with contract cancellations and it should follow certain business logic. First, we need to understand the contract starting date (and dealing with dates might be difficult!). If you try to come up with a prompt that can correctly handle cases like this, you’ll realize it might be quite difficult:</p>
<pre>examples = [
 "I signed my contract 2 years ago",
 "I started the deal with your company in February last year",
 "Our contract started on March 24th two years ago"
]</pre>
<p class="normal">Instead, force a model to call a tool (and maybe even through a ReACT agent!). For example, we have two very native tools in Python—<code class="inlineCode">date</code> and <code class="inlineCode">timedelta</code>:</p>
<pre>from datetime import date, timedelta
@tool
def get_date(year: int, month: int = 1, day: int = 1) -&gt; date:
 """Returns a date object given year, month and day.</pre>
<div><pre>     Default month and day are 1 (January) and 1.
     Examples in YYYY-MM-DD format:
       2023-07-27 -&gt; date(2023, 7, 27)
       2022-12-15 -&gt; date(2022, 12, 15)
       March 2022 -&gt; date(2022, 3)
       2021 -&gt; date(2021)
   """
 return date(year, month, day).isoformat()
@tool
def time_difference(days: int = 0, weeks: int = 0, months: int = 0, years: int = 0) -&gt; date:
 """Returns a date given a difference in days, weeks, months and years relative to the current date.
 
   By default, days, weeks, months and years are 0.
   Examples:
     two weeks ago -&gt; time_difference(weeks=2)
     last year -&gt; time_difference(years=1)
   """
   dt = date.today() - timedelta(days=days, weeks=weeks)
   new_year = dt.year+(dt.month-months) // 12 - years
   new_month = (dt.month-months) % 12
 return dt.replace(year=new_year, month=new_month)</pre>
<p class="normal">Now it works <a id="_idIndexMarker485"/>like a charm:</p>
<pre>from langchain_google_vertexai import ChatVertexAI
llm = ChatVertexAI(model="gemini-1.5-pro-002")
agent = create_react_agent(
   llm, [get_date, time_difference], prompt="Extract the starting date of a contract. Current year is 2025.")
for example in examples:
 result = agent.invoke({"messages": [("user", example)]})
 print(example, result["messages"][-1].content)</pre>
<div><pre>&gt;&gt; I signed my contract 2 years ago The contract started on 2023-02-07.
I started the deal with your company in February last year The contract started on 2024-02-01.
Our contract started on March 24th two years ago The contract started on 2023-03-24</pre>
<p class="normal">We learned how to use tools, or function calls, to enhance LLMs’ performance on complex tasks. This is one of the fundamental architectural patterns behind agents—now it’s time to discuss <a id="_idTextAnchor264"/>what an agent is.</p>
<h1 class="heading-1" id="_idParaDest-140"><a id="_idTextAnchor265"/>What are agents?</h1>
<p class="normal"><a id="_idIndexMarker486"/>Agents are one of the hottest topics of generative AI these days. People talk about agents a lot, but there are many different definitions of what an agent is. LangChain itself defines an agent as “<em class="italic">a system that uses an LLM to decide the control flow of an application</em>.” While we feel it’s a great definition that is worth citing, it missed some aspects.</p>
<p class="normal">As Python <a id="_idIndexMarker487"/>developers, you might be familiar with duck typing to determine an object’s behavior by the so-called duck test: “<em class="italic">If it walks like a duck and it quacks like a duck, then it must be a duck</em>.” With that concept in mind, let’s describe some properties of an agent in the context of generative AI:</p>
<ul>
<li class="b lletList">Agents help a user solve complex non-deterministic tasks without being given an explicit algorithm on how to do it. Advanced agents can even act on behalf of a user.</li>
<li class="b lletList">To solve a task, agents typically perform multiple steps and iterations. They <em class="italic">reason</em> (generate new information based on available context), <em class="italic">act</em> (interact with the external environment), <em class="italic">observe</em> (incorporate feedback from the external environment), and <em class="italic">communicate</em> (interact and/or work collaboratively with other agents or humans).</li>
<li class="b lletList">Agents utilize LLMs for reasoning (and solving tasks).</li>
<li class="b lletList">While agents have certain autonomy (and to a certain extent, they even figure out what is the best way to solve the task by thinking and learning from interacting with the environment), when running an agent, we’d still like to keep a certain degree of control of the execution flow.</li>
</ul>
<p class="normal">Retaining control over an agent’s behavior—an agentic workflow—is a core concept behind LangGraph. While LangGraph provides developers with a rich set of building blocks (such as memory management, tool invocation, and cyclic graphs with recursion depth control), its primary design pattern focuses on managing the flow and level of autonomy that LLMs exercise in executing tasks. Let’s start with an example and<a id="_idTextAnchor266"/> develop our agent.</p>
<div><h2 class="heading-2" id="_idParaDest-141"><a id="_idTextAnchor267"/>Plan-and-solve agent</h2>
<p class="normal">What do we<a id="_idIndexMarker488"/> as humans typically do when we have a complex task ahead of us? We plan! In 2023, Lei Want et al. demonstrated that plan-and-solve prompting <a id="_idIndexMarker489"/>improves LLM reasoning. It has been also demonstrated by multiple studies that LLMs’ performance tends to deteriorate as the complexity (in particular, the length and the number of instructions) of the prompt increases. </p>
<p class="normal">Hence, the first design pattern to keep in mind is <em class="italic">task decomposition</em>—to decompose complex tasks into a sequence of smaller ones, keep your prompts simple and focused on a single task, and don’t hesitate to add examples to your prompts. In our case, we are going to develop a research assistant. </p>
<p class="normal">Faced with a complex task, let’s first ask the LLM to come up with a detailed plan to solve this task, and then use the same LLM to execute on every step. Remember, at the end of the day, LLMs autoregressively generate output tokens based on input tokens. Such simple patterns as ReACT or plan-and-solve help us to better use their implicit reasoning capabilities. </p>
<p class="normal">First, we need <a id="_idIndexMarker490"/>to define our planner. There’s nothing new here; we’re using building blocks <a id="_idIndexMarker491"/>that we have already discussed—chat prompt templates and controlled generation with a Pydantic model:</p>
<pre>from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
class Plan(BaseModel):
 """Plan to follow in future"""
   steps: list[str] = Field(
       description="different steps to follow, should be in sorted order"
   )
system_prompt_template = (
 "For the given task, come up with a step by step plan.\n"
 "This plan should involve individual tasks, that if executed correctly will "
 "yield the correct answer. Do not add any superfluous steps.\n"
 "The result of the final step should be the final answer. Make sure that each "
 "step has all the information needed - do not skip steps."
)
planner_prompt = ChatPromptTemplate.from_messages(
   [("system", system_prompt_template),</pre>
<div><pre>    ("user", "Prepare a plan how to solve the following task:\n{task}\n")])
planner = planner_prompt | ChatVertexAI(
   model_name="gemini-1.5-pro-002", temperature=1.0
).with_structured_output(Plan)</pre>
<p class="normal">For a step execution, let’s use a ReACT agent with built-in tools—DuckDuckGo search, retrievers from arXiv and Wikipedia, and our custom <code class="inlineCode">calculator</code> tool we developed earlier in this chapter:</p>
<pre>from langchain.agents import load_tools
tools = load_tools(
 tool_names=["ddg-search", "arxiv", "wikipedia"],
 llm=llm
) + [calculator_tool]</pre>
<p class="normal">Next, let’s <a id="_idIndexMarker492"/>define our workflow state. We need to keep track of the initial <a id="_idIndexMarker493"/>task and initially generated plan, and let’s add <code class="inlineCode">past_steps</code> and <code class="inlineCode">final_response</code> to the state:</p>
<pre>class PlanState(TypedDict):
   task: str
   plan: Plan
   past_steps: Annotated[list[str], operator.add]
   final_response: str
   past_steps: list[str]
def get_current_step(state: PlanState) -&gt; int:
 """Returns the number of current step to be executed."""
 return len(state.get("past_steps", []))
 
def get_full_plan(state: PlanState) -&gt; str:
 """Returns formatted plan with step numbers and past results."""
 full_plan = []
 for i, step in enumerate(state["plan"]):
   full_step = f"# {i+1}. Planned step: {step}\n"
 if i &lt; get_current_step(state):
     full_step += f"Result: {state['past_steps'][i]}\n"
   full_plan.append(full_step)
 return "\n".join(full_plan)</pre>
<div><p class="normal">Now, it’s time to define our nodes and edges:</p>
<pre>from typing import Literal
from langgraph.graph import StateGraph, START, END
final_prompt = PromptTemplate.from_template(
 "You're a helpful assistant that has executed on a plan."
 "Given the results of the execution, prepare the final response.\n"
 "Don't assume anything\nTASK:\n{task}\n\nPLAN WITH RESUlTS:\n{plan}\n"
 "FINAL RESPONSE:\n"
)
async def _build_initial_plan(state: PlanState) -&gt; PlanState:
 plan = await planner.ainvoke(state["task"])
 return {"plan": plan}
async def _run_step(state: PlanState) -&gt; PlanState:
 plan = state["plan"]
 current_step = get_current_step(state)
 step = await execution_agent.ainvoke({"plan": get_full_plan(plan), "step": plan.steps[current_step], "task": state["task"]})
 return {"past_steps": [step["messages"][-1].content]}
async def _get_final_response(state: PlanState) -&gt; PlanState:
 final_response = await (final_prompt | llm).ainvoke({"task": state["task"], "plan": get_full_plan(state)})
 return {"final_response": final_response}
def _should_continue(state: PlanState) -&gt; Literal["run", "response"]:
 if get_current_step(plan) &lt; len(state["plan"].steps):
 return "run"
 return "final_response"</pre>
<p class="normal">And put <a id="_idIndexMarker494"/>together <a id="_idIndexMarker495"/>the final graph:</p>
<pre>builder = StateGraph(PlanState)
builder.add_node("initial_plan", _build_initial_plan)
builder.add_node("run", _run_step)</pre>
<div><pre>builder.add_node("response", _get_final_response)
builder.add_edge(START, "initial_plan")
builder.add_edge("initial_plan", "run")
builder.add_conditional_edges("run", _should_continue)
builder.add_edge("response", END)
graph = builder.compile()
from IPython.display import Image, display
display(Image(graph.get_graph().draw_mermaid_png()))</pre>
<figure class="mediaobject"><img alt="Figure 5.3: Plan-and-solve agentic workflow" src="img/B32363_05_03.png"/></figure>
<p class="packt_figref">Figure 5.3: Plan-and-solve agentic workflow</p>
<p class="normal">Now we can run the workflow:</p>
<pre>task = "Write a strategic one-pager of building an AI startup"
result = await graph.ainvoke({"task": task})</pre>
<p class="normal">You can see<a id="_idIndexMarker496"/> the full output on our GitHub, and we encourage <a id="_idIndexMarker497"/>you to play with it yourself. It might be especially interesting to investigate whether you like the result more compared to a single L<a id="_idTextAnchor268"/>LM prompt with a given task.</p>
<div><h1 class="heading-1" id="_idParaDest-142"><a id="_idTextAnchor269"/>Summary</h1>
<p class="normal">In this chapter, we explored how to enhance LLMs by integrating tools and design patterns for tool invocation, including the ReACT pattern. We started by building a ReACT agent from scratch and then demonstrated how to create a customized one with just one line of code using LangGraph. </p>
<p class="normal">Next, we delved into advanced techniques for controlled generation—showing how to force an LLM to call any tool or a specific one, and instructing it to return responses in structured formats (such as JSON, enums, or Pydantic models). In that context, we covered LangChain’s <code class="inlineCode">with_structured_output</code> method, which transforms your data structure into a tool schema, prompts the model to call the tool, parses the output, and compiles it into a corresponding Pydantic instance.</p>
<p class="normal">Finally, we built our first plan-and-solve agent with LangGraph, applying all the concepts we’ve learned so far: tool calling, ReACT, structured outputs, and more. In the next chapter, we’ll continue discussing how to develop agents and look into more adva<a id="_idTextAnchor270"/>nced architectural patterns.</p>
<h1 class="heading-1" id="_idParaDest-143"><a id="_idTextAnchor271"/>Questions</h1>
<ol>
<li class="numberedList" value="1">What are the key benefits of using tools with LLMs, and why are they important?</li>
<li class="numberedList">How does LangChain’s ToolMessage class facilitate communication between the LLM and the external environment? </li>
<li class="numberedList">Explain the ReACT pattern. What are its two main steps? How does it improve LLM performance? </li>
<li class="numberedList">How would you define a generative AI agent? How does this relate to or differ from LangChain’s definition?</li>
<li class="numberedList">Explain some advantages and disadvantages of using the with_structured_output method compared to using a controlled generation directly.</li>
<li class="numberedList">How can you programmatically define a custom tool in LangChain?</li>
<li class="numberedList">Explain the purpose of the Runnable.bind() and bind_tools() methods in LangChain.</li>
<li class="numberedList">How does LangChain handle errors that occur during tool execution? What options are available for configuring this behavior?</li>
</ol>
<div><h1 class="heading-1" id="_idParaDest-144"><a id="_idTextAnchor272"/><a id="_idTextAnchor273"/>Subscribe to our weekly newsletter</h1>
<p class="normal">Subscribe to AI_Distilled, the go-to newsletter for AI professionals, researchers, and innovators, at <a href="E_Chapter_5.xhtml">https://packt.link/Q5UyU</a>.</p>
<p class="normal"><img alt="" src="img/Newsletter_QRcode1.jpg"/></p>
</div>
</body></html>