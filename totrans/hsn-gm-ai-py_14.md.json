["```py\nfrom simple_rl.agents import QLearningAgent, RandomAgent, RMaxAgent\nfrom simple_rl.tasks import GridWorldMDP\nfrom simple_rl.run_experiments import run_agents_on_mdp\n\n# Setup MDP.\nmdp = GridWorldMDP(width=4, height=3, init_loc=(1, 1), goal_locs=[(4, 3)], lava_locs=[(4, 2)], gamma=0.95, walls=[(2, 2)], slip_prob=0.05)\n\n# Setup Agents.\nql_agent = QLearningAgent(actions=mdp.get_actions())\nrmax_agent = RMaxAgent(actions=mdp.get_actions())\nrand_agent = RandomAgent(actions=mdp.get_actions())\n\n# Run experiment and make plot.\nrun_agents_on_mdp([ql_agent, rmax_agent, rand_agent], mdp, instances=5, episodes=50, steps=10)\n```", "```py\n!pip install --upgrade --no-cache-dir dopamine-rl\n!pip install cmake\n!pip install atari_py\n!pip install gin-config\n```", "```py\nimport numpy as np\nimport os\nfrom dopamine.agents.dqn import dqn_agent\nfrom dopamine.discrete_domains import run_experiment\nfrom dopamine.colab import utils as colab_utils\nfrom absl import flags\nimport gin.tf\n\nBASE_PATH = '/tmp/colab_dope_run'  # @param\nGAME = 'Asterix'  # @param\n```", "```py\n!gsutil -q -m cp -R gs://download-dopamine-rl/preprocessed-benchmarks/* /content/\nexperimental_data = colab_utils.load_baselines('/content')\n```", "```py\nLOG_PATH = os.path.join(BASE_PATH, 'random_dqn', GAME)\n\nclass MyRandomDQNAgent(dqn_agent.DQNAgent):\n  def __init__(self, sess, num_actions):\n    \"\"\"This maintains all the DQN default argument values.\"\"\"\n    super(MyRandomDQNAgent, self).__init__(sess, num_actions)\n\n  def step(self, reward, observation):\n    \"\"\"Calls the step function of the parent class, but returns a random action.\n    \"\"\"\n    _ = super(MyRandomDQNAgent, self).step(reward, observation)\n    return np.random.randint(self.num_actions)\n\ndef create_random_dqn_agent(sess, environment, summary_writer=None):\n  \"\"\"The Runner class will expect a function of this type to create an agent.\"\"\"\n  return MyRandomDQNAgent(sess, num_actions=environment.action_space.n)\n\nrandom_dqn_config = \"\"\"\nimport dopamine.discrete_domains.atari_lib\nimport dopamine.discrete_domains.run_experiment\natari_lib.create_atari_environment.game_name = '{}'\natari_lib.create_atari_environment.sticky_actions = True\nrun_experiment.Runner.num_iterations = 200\nrun_experiment.Runner.training_steps = 10\nrun_experiment.Runner.max_steps_per_episode = 100\n\"\"\".format(GAME)\ngin.parse_config(random_dqn_config, skip_unknown=False)\n\nrandom_dqn_runner = run_experiment.TrainRunner(LOG_PATH, create_random_dqn_agent)\n```", "```py\nprint('Will train agent, please be patient, may be a while...')\nrandom_dqn_runner.run_experiment()\nprint('Done training!')\n```", "```py\nconda create -n kerasrl python=3.6\nconda activate kerasrl\npip install tensorflow==1.7.1  #not TF 2.0 at time of writing\npip install keras\npip install keras-rl\npip install gym\n```", "```py\nimport numpy as np\nimport gym\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom keras.optimizers import Adam\n\nfrom rl.agents.dqn import DQNAgent\nfrom rl.policy import BoltzmannQPolicy\nfrom rl.memory import SequentialMemory\n\nENV_NAME = 'CartPole-v0'\n\nenv = gym.make(ENV_NAME)\nnp.random.seed(123)\nenv.seed(123)\nnb_actions = env.action_space.n\n\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(1,) + env.observation_space.shape))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(nb_actions))\nmodel.add(Activation('linear'))\nprint(model.summary())\n\nmemory = SequentialMemory(limit=50000, window_length=1)\npolicy = BoltzmannQPolicy()\ndqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n               target_model_update=1e-2, policy=policy)\ndqn.compile(Adam(lr=1e-3), metrics=['mae'])\ndqn.fit(env, nb_steps=50000, visualize=True, verbose=2)\n\ndqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n\ndqn.test(env, nb_episodes=5, visualize=True)\n```", "```py\nmemory = SequentialMemory(limit=50000, window_length=1)\npolicy = BoltzmannQPolicy()\ndqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n               target_model_update=1e-2, policy=policy)\n```", "```py\ndqn.fit(env, nb_steps=50000, visualize=True, verbose=2)\n```", "```py\ndqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n\ndqn.test(env, nb_episodes=5, visualize=True)\n```", "```py\n!pip uninstall -y pyarrow\n!pip install tensorflow ray[rllib] > /dev/null 2>&1\n```", "```py\nimport ray\nfrom ray import tune\n\nray.init()\n```", "```py\ntune.run(\"DQN\", stop={\"episode_reward_mean\": 100},\n    config={\n            \"env\": \"CartPole-v0\",\n            \"num_gpus\": 0,\n           \"num_workers\": 1,\n           \"lr\": tune.grid_search([0.01, 0.001, 0.0001]),\n           \"monitor\": False,    },)\n```", "```py\n!apt-get install xvfb\n!pip install gym[all]\n!pip install 'imageio==2.4.0'\n!pip install PILLOW\n!pip install 'pyglet==1.3.2'\n!pip install pyvirtualdisplay\n!pip install tf-agents-nightly\ntry:\n  %%tensorflow_version 2.x\nexcept:\n  pass\n```", "```py\nenv_name = 'LunarLander-v2'\nenv = suite_gym.load(env_name)\n\n# -- and --\n\nexample_environment = tf_py_environment.TFPyEnvironment(uite_gym.load('LunarLander-v2')) \n```", "```py\nfc_layer_params = (100,)\n\n# change to\n\nfc_layer_params = (256,)\n```", "```py\ntune.run(\"DQN\", stop={\"episode_reward_mean\": 100},\n    config={\n            \"env\": \"CartPole-v0\",\n            \"num_gpus\": 0,\n           \"num_workers\": 1,\n           \"lr\": tune.grid_search([0.01, 0.001, 0.0001]),\n           \"monitor\": False,    },)\n```"]