<html><head></head><body>
<div id="_idContainer016">
<h1 class="chapter-number" id="_idParaDest-16"><a id="_idTextAnchor015"/><span class="koboSpan" id="kobo.1.1">1</span></h1>
<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/><span class="koboSpan" id="kobo.2.1">Understanding Generative AI: An Introduction</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In his influential book </span><em class="italic"><span class="koboSpan" id="kobo.4.1">The Singularity Is Near</span></em><span class="koboSpan" id="kobo.5.1"> (2005), renowned inventor and futurist Ray Kurzweil asserted that we were on the precipice of an exponential acceleration in technological advancements. </span><span class="koboSpan" id="kobo.5.2">He envisioned a future where technological innovation would continue to accelerate, eventually leading to a </span><strong class="bold"><span class="koboSpan" id="kobo.6.1">singularity</span></strong><span class="koboSpan" id="kobo.7.1">—a point where </span><strong class="bold"><span class="koboSpan" id="kobo.8.1">artificial intelligence</span></strong><span class="koboSpan" id="kobo.9.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.10.1">AI</span></strong><span class="koboSpan" id="kobo.11.1">) could</span><a id="_idIndexMarker000"/><span class="koboSpan" id="kobo.12.1"> transcend human intelligence, blurring the lines between humans and machines. </span><span class="koboSpan" id="kobo.12.2">Fast-forward to today and we find ourselves advancing along the trajectory Kurzweil outlined, with generative AI marking a significant stride along this path. </span><span class="koboSpan" id="kobo.12.3">Today, we are experiencing state-of-the-art generative models can behave as collaborators capable of synthetic understanding and generating sophisticated responses that mirror human intelligence.. </span><span class="koboSpan" id="kobo.12.4">The rapid and exponential growth of generative approaches is propelling Kurzweil’s vision forward, fundamentally reshaping how we interact </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">with technology.</span></span></p>
<p><span class="koboSpan" id="kobo.14.1">In this chapter, we lay the conceptual groundwork for anyone hoping to apply generative AI to their work, research, or field of study, broadening a fundamental understanding of what this technology does, how it was derived, and how it can be used. </span><span class="koboSpan" id="kobo.14.2">It establishes how generative models differ from classical </span><strong class="bold"><span class="koboSpan" id="kobo.15.1">machine learning</span></strong><span class="koboSpan" id="kobo.16.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.17.1">ML</span></strong><span class="koboSpan" id="kobo.18.1">) paradigms </span><a id="_idIndexMarker001"/><span class="koboSpan" id="kobo.19.1">and elucidates how they discern complex relationships and idiosyncrasies in data to synthesize human-like text, audio, and video. </span><span class="koboSpan" id="kobo.19.2">We will explore critical foundational generative methods, such as generative adversarial networks (GANs), diffusion </span><a id="_idIndexMarker002"/><span class="koboSpan" id="kobo.20.1">models, and transformers, with a particular emphasis on their </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">real-world applications.</span></span></p>
<p><span class="koboSpan" id="kobo.22.1">Additionally, this chapter hopes to dispel some common misunderstandings surrounding generative AI and provides guidelines to adopt this emerging technology ethically, considering its environmental footprint and advocating for responsible development and adoption. </span><span class="koboSpan" id="kobo.22.2">We will also highlight scenarios where generative models are apt for addressing business challenges. </span><span class="koboSpan" id="kobo.22.3">By the conclusion of this chapter, we will better understand the potential of generative AI and its applications across a wide array of sectors and have critically assessed the risks, limitations, and </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">long-term considerations.</span></span></p>
<p><span class="koboSpan" id="kobo.24.1">Whether your interest is casual, you are a professional transitioning from a different field, or you are an established practitioner in the fields of data science or ML, this chapter offers a contextual understanding to make informed decisions regarding the responsible adoption of </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">generative AI.</span></span></p>
<p><span class="koboSpan" id="kobo.26.1">Ultimately, we aim to establish a foundation through an introductory exploration of generative AI </span><a id="_idIndexMarker003"/><span class="koboSpan" id="kobo.27.1">and </span><strong class="bold"><span class="koboSpan" id="kobo.28.1">large language models</span></strong><span class="koboSpan" id="kobo.29.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.30.1">LLMs</span></strong><span class="koboSpan" id="kobo.31.1">), dissected into </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">two parts.</span></span></p>
<p><span class="koboSpan" id="kobo.33.1">The beginning of the book will introduce the fundamentals and history of generative AI, surveying various types, such as GANs, diffusers, and transformers, tracing the foundations </span><a id="_idIndexMarker004"/><span class="koboSpan" id="kobo.34.1">of </span><strong class="bold"><span class="koboSpan" id="kobo.35.1">natural language generation</span></strong><span class="koboSpan" id="kobo.36.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.37.1">NLG</span></strong><span class="koboSpan" id="kobo.38.1">), and demonstrating the basic steps to implement generative models from prototype to production. </span><span class="koboSpan" id="kobo.38.2">Moving forward, we will focus on slightly more advanced application fundamentals, including fine-tuning generative models, prompt engineering, and addressing ethical considerations toward the responsible adoption of generative AI. </span><span class="koboSpan" id="kobo.38.3">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.39.1">get star</span><a id="_idTextAnchor017"/><span class="koboSpan" id="kobo.40.1">ted.</span></span></p>
<h1 id="_idParaDest-18"><a id="_idTextAnchor018"/><span class="koboSpan" id="kobo.41.1">Generative AI</span></h1>
<p><span class="koboSpan" id="kobo.42.1">In recent decades, AI has</span><a id="_idIndexMarker005"/><span class="koboSpan" id="kobo.43.1"> made incredible strides. </span><span class="koboSpan" id="kobo.43.2">The origins of the field stem from classical statistical models meticulously designed to help us analyze and make sense of data. </span><span class="koboSpan" id="kobo.43.3">As we developed more robust computational methods to process and store data, the field shifted—intersecting computer science and statistics and giving us ML. </span><span class="koboSpan" id="kobo.43.4">ML systems could learn complex relationships and surface latent insights from vast amounts of data, transforming our approach to </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">statistical modeling.</span></span></p>
<p><span class="koboSpan" id="kobo.45.1">This shift laid the groundwork for the rise of deep learning, a substantial step forward that introduced multi-layered neural networks (i.e., a system of interconnected functions) to model complex patterns. </span><span class="koboSpan" id="kobo.45.2">Deep learning enabled powerful discriminative models that became pivotal for advancements in diverse fields of research, including image recognition, voice recognition, and natural </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">language processing.</span></span></p>
<p><span class="koboSpan" id="kobo.47.1">However, the journey continues with the emergence of generative AI. </span><span class="koboSpan" id="kobo.47.2">Generative AI harnesses the power of deep learning to accomplish a broader objective. </span><span class="koboSpan" id="kobo.47.3">Instead of classifying and discriminating data, generative AI seeks to learn and replicate data distributions to “create” entirely new and seemingly original data, mirroring </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">human-like out</span><a id="_idTextAnchor019"/><span class="koboSpan" id="kobo.49.1">put.</span></span></p>
<h1 id="_idParaDest-19"><a id="_idTextAnchor020"/><span class="koboSpan" id="kobo.50.1">Distinguishing generative AI from other AI models</span></h1>
<p><span class="koboSpan" id="kobo.51.1">Again, the critical </span><a id="_idIndexMarker006"/><span class="koboSpan" id="kobo.52.1">distinction between discriminative</span><a id="_idIndexMarker007"/><span class="koboSpan" id="kobo.53.1"> and generative models lies in their objectives. </span><span class="koboSpan" id="kobo.53.2">Discriminative models aim to predict target outputs given input data. </span><span class="koboSpan" id="kobo.53.3">Classification algorithms, such as logistic regression or support vector machines, find decision boundaries in data to categorize inputs as belonging to one or more class. </span><span class="koboSpan" id="kobo.53.4">Neural networks learn input-output mappings by optimizing weights through backpropagation (or tracing back to resolve errors) to make accurate predictions. </span><span class="koboSpan" id="kobo.53.5">Advanced gradient boosting models, such as XGBoost or LightGBM, further enhance these discriminative models by employing decision trees and incorporating the principles of gradient boosting (or the strategic ensembling of models) to make highly </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">accurate predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.55.1">Generative methods learn complex relationships through expansive training in order to generate new data sequences enabling many downstream applications. </span><span class="koboSpan" id="kobo.55.2">Effectively, these models create synthetic outputs by replicating the statistical patterns and properties discovered in training data, capturing nuances and idiosyncrasies that closely reflect </span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">human behaviors.</span></span></p>
<p><span class="koboSpan" id="kobo.57.1">In practice, a discriminative image classifier labels images containing a cat or a dog. </span><span class="koboSpan" id="kobo.57.2">In contrast, a generative model can synthesize diverse, realistic cat or dog images by learning the distributions of pixels and implicit features from existing images. </span><span class="koboSpan" id="kobo.57.3">Moreover, generative models can be trained across modalities to unlock new possibilities in synthesis-focused applications to generate human-like photographs, videos, music, </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">and text.</span></span></p>
<p><span class="koboSpan" id="kobo.59.1">There are several key methods that have formed the foundation for many of the recent advancements in Generative AI, each with unique approaches and strengths. </span><span class="koboSpan" id="kobo.59.2">In the next section, we survey generative advancements over time, including adversarial networks, variational autoencoders, diffusion models, and autoregressive transformers, to better understand their impact </span><span class="No-Break"><span class="koboSpan" id="kobo.60.1">and inf</span><a id="_idTextAnchor021"/><span class="koboSpan" id="kobo.61.1">luence.</span></span></p>
<h2 id="_idParaDest-20"><a id="_idTextAnchor022"/><span class="koboSpan" id="kobo.62.1">Briefly surveying generative approaches</span></h2>
<p><span class="koboSpan" id="kobo.63.1">Modern generative</span><a id="_idIndexMarker008"/><span class="koboSpan" id="kobo.64.1"> modeling encompasses diverse architectures suited to different data types and distinct tasks. </span><span class="koboSpan" id="kobo.64.2">Here, we briefly introduce some of the key approaches that have emerged over the years, bringing us to the </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">state-of-the-art models:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.66.1">Generative adversarial networks (GANs)</span></strong><span class="koboSpan" id="kobo.67.1"> involve two interconnected neural </span><a id="_idIndexMarker009"/><span class="koboSpan" id="kobo.68.1">networks—one acting as a generator to create realistic synthetic data and the other acting as a discriminator that distinguishes between real and synthetic (fake) data points. </span><span class="koboSpan" id="kobo.68.2">The generator and discriminator are adversaries</span><a id="_idIndexMarker010"/><span class="koboSpan" id="kobo.69.1"> in a </span><strong class="bold"><span class="koboSpan" id="kobo.70.1">zero-sum game</span></strong><span class="koboSpan" id="kobo.71.1">, each fighting to outperform the other. </span><span class="koboSpan" id="kobo.71.2">This adversarial relationship gradually improves the generator’s capacity to produce vividly realistic synthetic data, making GANs adept at creating intricate image distributions and achieving photo-realistic </span><span class="No-Break"><span class="koboSpan" id="kobo.72.1">image synthesis.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.73.1">Variational autoencoders</span></strong><span class="koboSpan" id="kobo.74.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.75.1">VAEs</span></strong><span class="koboSpan" id="kobo.76.1">) employ a unique learning </span><a id="_idIndexMarker011"/><span class="koboSpan" id="kobo.77.1">method to compress data into a simpler form (or latent representation). </span><span class="koboSpan" id="kobo.77.2">This process involves an encoder and a decoder that work conjointly (Kingma &amp; Welling, 2013). </span><span class="koboSpan" id="kobo.77.3">While VAEs may not be the top choice for image quality, they are unmatched in efficiently separating and understanding complex </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">data patterns.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.79.1">Diffusion models</span></strong><span class="koboSpan" id="kobo.80.1"> continuously add Gaussian noise to data over multiple steps to corrupt it. </span><span class="koboSpan" id="kobo.80.2">Gaussian </span><a id="_idIndexMarker012"/><span class="koboSpan" id="kobo.81.1">noise can be thought of as random variations applied to a signal to distort it, creating “noise”. </span><span class="koboSpan" id="kobo.81.2">Diffusion models are trained to eliminate the added noise to recover the original data distribution. </span><span class="koboSpan" id="kobo.81.3">This type of reverse engineering process equips diffusion models to generate diverse, high-quality samples that closely replicate the original data distribution, producing diverse high-fidelity images (Ho et </span><span class="No-Break"><span class="koboSpan" id="kobo.82.1">al., 2020).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.83.1">Autoregressive transformers</span></strong><span class="koboSpan" id="kobo.84.1"> leverage parallelizable self-attention to </span><a id="_idIndexMarker013"/><span class="koboSpan" id="kobo.85.1">model complex sequential dependencies, showing exceptional performance in language-related tasks (Vaswani et al., 2017). </span><span class="koboSpan" id="kobo.85.2">Pretrained models such as GPT-4 or Claude have demonstrated the capability for generalizations in natural language </span><a id="_idIndexMarker014"/><span class="koboSpan" id="kobo.86.1">tasks and impressive human-like text generation. </span><span class="koboSpan" id="kobo.86.2">Despite ethical issues and misuse concerns, transformers have emerged as the frontrunners in language modeling and </span><span class="No-Break"><span class="koboSpan" id="kobo.87.1">multimodal generation.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.88.1">Collectively, these methodologies paved the way for advanced generative modeling across a wide array of domains, including images, videos, audio, and text. </span><span class="koboSpan" id="kobo.88.2">While architectural and engineering innovations progress daily, generative methods showcase unparalleled synthesis capabilities across diverse modalities. </span><span class="koboSpan" id="kobo.88.3">Throughout the book, we will explore and apply generative methods to simulate real-world scenarios. </span><span class="koboSpan" id="kobo.88.4">However, before diving in, we further distinguish generative methods from traditional ML methods by </span><a id="_idIndexMarker015"/><span class="koboSpan" id="kobo.89.1">addressing some </span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">common </span><a id="_idTextAnchor023"/><span class="koboSpan" id="kobo.91.1">misconceptions.</span></span></p>
<h2 id="_idParaDest-21"><a id="_idTextAnchor024"/><span class="koboSpan" id="kobo.92.1">Clarifying misconceptions between discriminative and generative paradigms</span></h2>
<p><span class="koboSpan" id="kobo.93.1">To better understand</span><a id="_idIndexMarker016"/><span class="koboSpan" id="kobo.94.1"> the distinctive </span><a id="_idIndexMarker017"/><span class="koboSpan" id="kobo.95.1">capabilities and applications of traditional ML models (often referred to as discriminative) and generative methods, here, we clear up some common misconceptions </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">and myths:</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.97.1">Myth 1</span></strong><span class="koboSpan" id="kobo.98.1">: Generative models cannot recognize patterns as effectively as </span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">discriminative models.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.100.1">Truth</span></strong><span class="koboSpan" id="kobo.101.1">: State-of-the-art generative models are well-known for their impressive abilities to recognize and trace patterns, rivaling some discriminative models. </span><span class="koboSpan" id="kobo.101.2">Despite primarily focusing on creative synthesis, generative models display classification capabilities. </span><span class="koboSpan" id="kobo.101.3">However, the classes output from a generative model can be difficult to explain as generative models are not explicitly trained to learn decision boundaries or predetermined relationships. </span><span class="koboSpan" id="kobo.101.4">Instead, they may only learn to simulate classification based on labels learned implicitly (or organically) during training. </span><span class="koboSpan" id="kobo.101.5">In short, in cases where the explanation of model outcomes is important, classification using a discriminative model may be the </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">better choice.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.103.1">Example</span></strong><span class="koboSpan" id="kobo.104.1">: Consider GPT-4. </span><span class="koboSpan" id="kobo.104.2">In addition to synthesizing human-like text, it can understand context, capture long-range dependencies, and detect patterns in texts. </span><span class="koboSpan" id="kobo.104.3">GPT-4 uses these intrinsic language processing capabilities to discriminate between classes, such as traditional classifiers. </span><span class="koboSpan" id="kobo.104.4">However, because GPT learns semantic relationships through extensive training, explaining its decision-making cannot be accomplished using any </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">established methods.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.106.1">Myth 2</span></strong><span class="koboSpan" id="kobo.107.1">: Generative AI will eventually replace </span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">discriminative AI.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.109.1">Truth</span></strong><span class="koboSpan" id="kobo.110.1">: This is a common misunderstanding. </span><span class="koboSpan" id="kobo.110.2">Discriminative models have consistently been the option for high-stakes prediction tasks because they focus directly on learning the decision boundary between classes, ensuring high precision and reliability. </span><span class="koboSpan" id="kobo.110.3">More importantly, discriminative models can be explained post-hoc, making them the ultimate choice for critical applications in sectors such as healthcare, finance, and security. </span><span class="koboSpan" id="kobo.110.4">However, generative models may increasingly become more popular for high-stakes modeling as explainability </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">techniques emerge.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.112.1">Example</span></strong><span class="koboSpan" id="kobo.113.1">: Consider a discriminative model trained specifically for disease prediction in healthcare. </span><span class="koboSpan" id="kobo.113.2">A specialized model can classify data points (e.g., images of skin) as healthy or unhealthy, giving healthcare professionals a tool for early intervention and treatment plans. </span><span class="koboSpan" id="kobo.113.3">Post-hoc explanation methods, such as SHAP, can be employed to identify and analyze the key features that influence classification outcomes. </span><span class="koboSpan" id="kobo.113.4">This approach offers clear insights into the specific results (i.e., </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">feature attribution).</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.115.1">Myth 3</span></strong><span class="koboSpan" id="kobo.116.1">: Generative models continuously learn from </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">user input.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.118.1">Truth</span></strong><span class="koboSpan" id="kobo.119.1">: Not exactly. </span><span class="koboSpan" id="kobo.119.2">Generative LLMs are trained using a static approach. </span><span class="koboSpan" id="kobo.119.3">This means they learn from a vast training data corpora, and their knowledge is limited to the information contained within that training window. </span><span class="koboSpan" id="kobo.119.4">While models can be augmented with additional data or in-context information to help them contextualize, giving the impression of real-time learning, the underlying model itself is essentially frozen and does not learn in </span><span class="No-Break"><span class="koboSpan" id="kobo.120.1">real time.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.121.1">Example</span></strong><span class="koboSpan" id="kobo.122.1">: GPT-3 was trained in 2020 and only contained information up to that date until its successor GPT-3.5, released in March of 2023. </span><span class="koboSpan" id="kobo.122.2">Naturally, GPT-4 was trained on more recent data, but due to training limitations (including diminishing performance returns), it is reasonable to expect that subsequent training checkpoints will be released periodically and </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">not continuously.</span></span></p>
<p><span class="koboSpan" id="kobo.124.1">While generative and discriminative models have distinct strengths and limitations, knowing when to apply each paradigm requires evaluating several key factors. </span><span class="koboSpan" id="kobo.124.2">As we have clarified </span><a id="_idIndexMarker018"/><span class="koboSpan" id="kobo.125.1">some</span><a id="_idIndexMarker019"/><span class="koboSpan" id="kobo.126.1"> common myths about their capabilities, let’s turn our attention to guidelines for selecting the right approach for a give</span><a id="_idTextAnchor025"/><span class="koboSpan" id="kobo.127.1">n task </span><span class="No-Break"><span class="koboSpan" id="kobo.128.1">or problem.</span></span></p>
<h2 id="_idParaDest-22"><a id="_idTextAnchor026"/><span class="koboSpan" id="kobo.129.1">Choosing the right paradigm</span></h2>
<p><span class="koboSpan" id="kobo.130.1">The choice between</span><a id="_idIndexMarker020"/><span class="koboSpan" id="kobo.131.1"> generative and discriminative </span><a id="_idIndexMarker021"/><span class="koboSpan" id="kobo.132.1">models depends on various factors, such as the task or problem at hand, the quality and quantity of data available, the desired output, and the level of performance required. </span><span class="koboSpan" id="kobo.132.2">The following is a list of </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">key considerations:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.134.1">Task specificity</span></strong><span class="koboSpan" id="kobo.135.1">: Discriminative models are more suitable for high-stakes applications, such as disease diagnosis, fraud detection, or credit risk assessment, where precision is crucial. </span><span class="koboSpan" id="kobo.135.2">However, generative models are more adept at creative tasks such as synthesizing images, text, music, </span><span class="No-Break"><span class="koboSpan" id="kobo.136.1">or video.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.137.1">Data availability</span></strong><span class="koboSpan" id="kobo.138.1">: Discriminative models tend to overfit (or memorize examples) when trained on small datasets, which may lead to poor generalization. </span><span class="koboSpan" id="kobo.138.2">On the other hand, because generative models are often pretrained on vast amounts of data, they can produce a diverse output even with minimal input, making them a viable choice when data </span><span class="No-Break"><span class="koboSpan" id="kobo.139.1">are scarce.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.140.1">Model performance</span></strong><span class="koboSpan" id="kobo.141.1">: Discriminative models outperform generative models in tasks where it is crucial to learn and explain a decision boundary between classes or where expected relationships in the data are well understood. </span><span class="koboSpan" id="kobo.141.2">Generative models usually excel in less constrained tasks that require a measure of perceived creativity </span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">and flexibility.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.143.1">Model explainability</span></strong><span class="koboSpan" id="kobo.144.1">: While both paradigms can include models that are considered “black boxes” or not intrinsically interpretable, generative models can be more difficult, or at times, impossible to explain, as they often involve complex data generation processes that rely on understanding the underlying data distribution. </span><span class="koboSpan" id="kobo.144.2">Alternatively, discriminative models often focus on learning the boundary between classes. </span><span class="koboSpan" id="kobo.144.3">In use cases where model explainability is a key requirement, discriminative models may be more suitable. </span><span class="koboSpan" id="kobo.144.4">However, generative explainability research is </span><span class="No-Break"><span class="koboSpan" id="kobo.145.1">gaining traction.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.146.1">Model complexity</span></strong><span class="koboSpan" id="kobo.147.1">: Generally, discriminative models require less computational power because they learn to directly predict some output given a well-defined set </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">of inputs.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.149.1">Alternatively, generative models may consume more computational resources, as their training objective is to jointly capture the intricate hidden relationships between both inputs and presumed outputs. </span><span class="koboSpan" id="kobo.149.2">Accurately learning these intricacies requires vast amounts of data and large computations. </span><span class="koboSpan" id="kobo.149.3">Computational efficiency in generative LLM training (e.g., quantization) is a vibrant area </span><span class="No-Break"><span class="koboSpan" id="kobo.150.1">of research.</span></span></p></li>
</ul>
<p><span class="koboSpan" id="kobo.151.1">Ultimately, the choice between generative and discriminative models should be made by considering the trade-offs involved. </span><span class="koboSpan" id="kobo.151.2">Moreover, the adoption of these paradigms requires different levels of infrastructure, data curation, and other prerequisites. </span><span class="koboSpan" id="kobo.151.3">Occasionally, a hybrid approach that combines the strengths of both models can serve as an ideal solution. </span><span class="koboSpan" id="kobo.151.4">For example, a pretrained generative model can be fine-tuned as a classifier. </span><span class="koboSpan" id="kobo.151.5">We will learn about task-specific fine</span><a id="_idTextAnchor027"/><span class="koboSpan" id="kobo.152.1">-tuning in </span><a href="B21773_05.xhtml#_idTextAnchor180"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.153.1">Chapter 5</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.154.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.155.1">Now that we have explored the key distinctions between traditional ML (i.e., discriminative) and </span><a id="_idIndexMarker022"/><span class="koboSpan" id="kobo.156.1">generative </span><a id="_idIndexMarker023"/><span class="koboSpan" id="kobo.157.1">paradigms, including their distinct risks, we can look back at how we arrived at this paradigm shift. </span><span class="koboSpan" id="kobo.157.2">In the next section, we take a brief look at the evol</span><a id="_idTextAnchor028"/><span class="koboSpan" id="kobo.158.1">ution of </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">generative AI.</span></span></p>
<h1 id="_idParaDest-23"><a id="_idTextAnchor029"/><span class="koboSpan" id="kobo.160.1">Looking back at the evolution of generative AI</span></h1>
<p><span class="koboSpan" id="kobo.161.1">The field of </span><a id="_idIndexMarker024"/><span class="koboSpan" id="kobo.162.1">generative AI has experienced an unprecedented acceleration, leading to a surge in the development and adoption of foundation models such as GPT. </span><span class="koboSpan" id="kobo.162.2">However, this momentum has been building for several decades, driven by continuous and significant advancements in ML and natural language generation research. </span><span class="koboSpan" id="kobo.162.3">These developments have brought us to the current generation of </span><span class="No-Break"><span class="koboSpan" id="kobo.163.1">state-of-the-art models.</span></span></p>
<p><span class="koboSpan" id="kobo.164.1">To fully appreciate the current state of generative AI, it is important to understand its evolution, beginning with traditional language processing techniques and moving through to m</span><a id="_idTextAnchor030"/><span class="koboSpan" id="kobo.165.1">ore </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">recent advancements.</span></span></p>
<h2 id="_idParaDest-24"><a id="_idTextAnchor031"/><span class="koboSpan" id="kobo.167.1">Overview of traditional methods in NLP</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.168.1">Natural language processing</span></strong><span class="koboSpan" id="kobo.169.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.170.1">NLP</span></strong><span class="koboSpan" id="kobo.171.1">) technology</span><a id="_idIndexMarker025"/><span class="koboSpan" id="kobo.172.1"> has </span><a id="_idIndexMarker026"/><span class="koboSpan" id="kobo.173.1">enabled machines to understand, interpret, and generate human language. </span><span class="koboSpan" id="kobo.173.2">It emerged from traditional statistical techniques such as n-grams and </span><strong class="bold"><span class="koboSpan" id="kobo.174.1">hidden Markov models</span></strong><span class="koboSpan" id="kobo.175.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.176.1">HMMs</span></strong><span class="koboSpan" id="kobo.177.1">), which converted linguistic structures into mathematical</span><a id="_idIndexMarker027"/><span class="koboSpan" id="kobo.178.1"> models that machines </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">could understand.</span></span></p>
<p><span class="koboSpan" id="kobo.180.1">Initially, n-grams and HMMs were the primary methods used in NLP. </span><span class="koboSpan" id="kobo.180.2">N-grams predicted the next word in a sequence based on the last “</span><em class="italic"><span class="koboSpan" id="kobo.181.1">n</span></em><span class="koboSpan" id="kobo.182.1">” words, while HMMs modeled sequences by considering every word as a state in a Markov process. </span><span class="koboSpan" id="kobo.182.2">These early methods were good at capturing local patterns and short-range dependencies </span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">in language.</span></span></p>
<p><span class="koboSpan" id="kobo.184.1">As computational power and data availability grew, more sophisticated techniques for natural language processing emerged. </span><span class="koboSpan" id="kobo.184.2">Among these was the </span><strong class="bold"><span class="koboSpan" id="kobo.185.1">recurrent neural network</span></strong><span class="koboSpan" id="kobo.186.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.187.1">RNN</span></strong><span class="koboSpan" id="kobo.188.1">), which </span><a id="_idIndexMarker028"/><span class="koboSpan" id="kobo.189.1">managed relationships across extended sequences and was proven to be effective in tasks where prior context influenced </span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">future predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.191.1">Subsequently, </span><strong class="bold"><span class="koboSpan" id="kobo.192.1">long short-term memory networks</span></strong><span class="koboSpan" id="kobo.193.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.194.1">LSTMs</span></strong><span class="koboSpan" id="kobo.195.1">) </span><span class="No-Break"><span class="koboSpan" id="kobo.196.1">were</span></span><span class="No-Break"><a id="_idIndexMarker029"/></span><span class="No-Break"><span class="koboSpan" id="kobo.197.1"> developed.</span></span></p>
<p><span class="koboSpan" id="kobo.198.1">Unlike traditional RNNs, LSTMs had a unique ability to retain relevant long-term information while disregarding irrelevant data, maintaining semantic relationships across </span><span class="No-Break"><span class="koboSpan" id="kobo.199.1">prolonged sequences.</span></span></p>
<p><span class="koboSpan" id="kobo.200.1">Further advancements led to the introduction of sequence-to-sequence models, often utilizing LSTMs as their underlying structure. </span><span class="koboSpan" id="kobo.200.2">These models revolutionized fields such as machine translation and text summarization by dramatically improving efficiency </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">and effectiveness.</span></span></p>
<p><span class="koboSpan" id="kobo.202.1">Overall, NLP evolved from traditional statistical methods to advanced neural networks, transforming how we interacted with machines and enabling countless applications, such as machine translation and information retrieval (IR) (or finding relevant text based on a query). </span><span class="koboSpan" id="kobo.202.2">As the NLP field matured, incorporating the strengths of traditional statistical methods and advanced neural networks, a renaissance was forming. </span><span class="koboSpan" id="kobo.202.3">The next generation of NLP advancements would introduce transformer architectures, starting</span><a id="_idIndexMarker030"/><span class="koboSpan" id="kobo.203.1"> with </span><a id="_idIndexMarker031"/><span class="koboSpan" id="kobo.204.1">the seminal paper </span><em class="italic"><span class="koboSpan" id="kobo.205.1">Attention is All You Need</span></em><span class="koboSpan" id="kobo.206.1"> and later the release of models su</span><a id="_idTextAnchor032"/><span class="koboSpan" id="kobo.207.1">ch as BERT and </span><span class="No-Break"><span class="koboSpan" id="kobo.208.1">eventually GPT.</span></span></p>
<h2 id="_idParaDest-25"><a id="_idTextAnchor033"/><span class="koboSpan" id="kobo.209.1">Arrival and evolution of transformer-based models</span></h2>
<p><span class="koboSpan" id="kobo.210.1">The release of the </span><a id="_idIndexMarker032"/><span class="koboSpan" id="kobo.211.1">research </span><a id="_idIndexMarker033"/><span class="koboSpan" id="kobo.212.1">paper titled </span><em class="italic"><span class="koboSpan" id="kobo.213.1">Attention is All You Need</span></em><span class="koboSpan" id="kobo.214.1"> in 2017 served as a paradigm shift in natural language processing. </span><span class="koboSpan" id="kobo.214.2">This pivotal paper introduced the transformer model, an architectural innovation that provided an unprecedented approach to sequential language tasks such as translation. </span><span class="koboSpan" id="kobo.214.3">The transformer model contrasted with prior models that processed sequences serially. </span><span class="koboSpan" id="kobo.214.4">Instead, it simultaneously processed different segments of an input sequence, determining its relevance based on the task. </span><span class="koboSpan" id="kobo.214.5">This innovative processing addressed the complexity of long-range dependencies in sequences, enabling the model to draw out the critical semantic information needed for a task. </span><span class="koboSpan" id="kobo.214.6">The transformer was such a critical advancement that nearly every state-of-the-art generative LLM applies some derivation of the original architecture. </span><span class="koboSpan" id="kobo.214.7">Its importance and influence motivate our detailed exploration and implementation of the original transformer in </span><a href="B21773_03.xhtml#_idTextAnchor081"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.215.1">Chapter 3</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.216.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.217.1">With the transformer came significant advancements in natural language processing, including GPT-1 or Generative Pretrained Transformer 1 (Radford et al., 2018). </span><span class="koboSpan" id="kobo.217.2">GPT-1 introduced a novel directional architecture to tackle diverse </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">NLP tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.219.1">Coinciding with GPT-1 was </span><strong class="bold"><span class="koboSpan" id="kobo.220.1">BERT</span></strong><span class="koboSpan" id="kobo.221.1">, or </span><strong class="bold"><span class="koboSpan" id="kobo.222.1">bidirectional encoder representations from transformers</span></strong><span class="koboSpan" id="kobo.223.1">, a pioneering</span><a id="_idIndexMarker034"/><span class="koboSpan" id="kobo.224.1"> work in the family of transformer-based models. </span><span class="koboSpan" id="kobo.224.2">BERT stood out among its predecessors, analyzing sentences forward and backward (or bi-directionally). </span><span class="koboSpan" id="kobo.224.3">This bidirectional analysis allowed BERT to capture semantic and syntactic nuances more effectively. </span><span class="koboSpan" id="kobo.224.4">At the time, BERT achieved unprecedented results when applied to complex natural language tasks such as named entity recognition, question answering, and sentiment analysis (Devlin et </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">al., 2018).</span></span></p>
<p><span class="koboSpan" id="kobo.226.1">Later, GPT-2, the much larger successor to GPT-1, attracted immense attention, as it greatly outperformed any of its predecessors across various tasks. </span><span class="koboSpan" id="kobo.226.2">In fact, GPT-2 was so unprecedented in its ability to generate human-like output that concerns about potential implications led to a delay in its initial release (</span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">Hern, 2019).</span></span></p>
<p><span class="koboSpan" id="kobo.228.1">Amid early concerns, OpenAI followed up with the development of GPT-3, signaling a leap in the potential of LLMs. </span><span class="koboSpan" id="kobo.228.2">Developers demonstrated the potential of training at a massive scale, reaching 175 billion parameters (or adjustable variables learned during training), surpassing its two predecessors. </span><span class="koboSpan" id="kobo.228.3">GPT-3 was a “general-purpose” learner, capable of performing a wide range of natural language tasks learned implicitly from its training corpus instead of through task-specific fine-tuning. </span><span class="koboSpan" id="kobo.228.4">This capability sparked the exploration of foundation model development for general use across various domains and tasks. </span><span class="koboSpan" id="kobo.228.5">GPT-3’s distinct design and unprecedented scale led to a generation of generative models</span><a id="_idIndexMarker035"/><span class="koboSpan" id="kobo.229.1"> that could perform an indefinite</span><a id="_idIndexMarker036"/><span class="koboSpan" id="kobo.230.1"> number of increasingly complex downstream tasks learned implic</span><a id="_idTextAnchor034"/><span class="koboSpan" id="kobo.231.1">itly through its </span><span class="No-Break"><span class="koboSpan" id="kobo.232.1">extensive training.</span></span></p>
<h2 id="_idParaDest-26"><a id="_idTextAnchor035"/><span class="koboSpan" id="kobo.233.1">Development and impact of GPT-4</span></h2>
<p><span class="koboSpan" id="kobo.234.1">OpenAI’s development</span><a id="_idIndexMarker037"/><span class="koboSpan" id="kobo.235.1"> of GPT-4 marked </span><a id="_idIndexMarker038"/><span class="koboSpan" id="kobo.236.1">a significant advance in the potential of large-scale, multimodal models. </span><span class="koboSpan" id="kobo.236.2">GPT-4, capable of processing image and text inputs and producing text outputs, represented yet another giant leap ahead </span><span class="No-Break"><span class="koboSpan" id="kobo.237.1">of predecessors.</span></span></p>
<p><span class="koboSpan" id="kobo.238.1">GPT-4 exhibited human-level performance on various professional and academic benchmarks. </span><span class="koboSpan" id="kobo.238.2">For instance, it passed a simulated bar exam with a score falling into the top 10% of test-takers (</span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">OpenAI, 2023).</span></span></p>
<p><span class="koboSpan" id="kobo.240.1">A key distinction of GPT-4 is what happens after pretraining. </span><span class="koboSpan" id="kobo.240.2">Open AI applied </span><strong class="bold"><span class="koboSpan" id="kobo.241.1">reinforcement learning with human feedback</span></strong><span class="koboSpan" id="kobo.242.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.243.1">RLHF</span></strong><span class="koboSpan" id="kobo.244.1">)—a type of risk/reward training </span><a id="_idIndexMarker039"/><span class="koboSpan" id="kobo.245.1">derived from the same technique used to teach autonomous vehicles to make decisions based on the environment they encounter. </span><span class="koboSpan" id="kobo.245.2">In the case of GPT-4, the model learned to respond appropriately to a myriad of scenarios, incorporating human feedback along the way. </span><span class="koboSpan" id="kobo.245.3">This novel refinement strategy drastically improved the model’s propensity for factuality and its adherence to desired behaviors. </span><span class="koboSpan" id="kobo.245.4">The integration of RLHF demonstrated how models could be better aligned with human judgment toward the goal of </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">responsible AI.</span></span></p>
<p><span class="koboSpan" id="kobo.247.1">However, despite demonstrating groundbreaking abilities, GPT-4 had similar limitations to earlier GPT models. </span><span class="koboSpan" id="kobo.247.2">It was not entirely reliable and had a limited context window (or input size). </span><span class="koboSpan" id="kobo.247.3">Meaning it could not receive large texts or documents as input. </span><span class="koboSpan" id="kobo.247.4">It was also prone to hallucination. </span><span class="koboSpan" id="kobo.247.5">As discussed, Hallucination is an anthropomorphized way of describing the model’s tendency to generate content that is not grounded in fact or reality. </span><span class="koboSpan" id="kobo.247.6">A hallucination occurs because generative language models (without augmentation) synthesize content purely based on semantic context and don’t perform any logical processing to verify factuality. </span><span class="koboSpan" id="kobo.247.7">This weakness presented meaningful risks, particularly in contexts where fact-based outcomes </span><span class="No-Break"><span class="koboSpan" id="kobo.248.1">are paramount.</span></span></p>
<p><span class="koboSpan" id="kobo.249.1">Despite limitations, GPT-4 made significant strides in language model performance. </span><span class="koboSpan" id="kobo.249.2">As with prior models, GPT-4’s development and potential use underscored the importance of safety and ethical considerations for future AI applications. </span><span class="koboSpan" id="kobo.249.3">As a result, the rise of GPT-4 accentuated the ongoing discussions and research into the potential implications of deploying such powerful models. </span><span class="koboSpan" id="kobo.249.4">In the next section, we briefly survey some of the </span><a id="_idIndexMarker040"/><span class="koboSpan" id="kobo.250.1">known</span><a id="_idTextAnchor036"/> <a id="_idIndexMarker041"/><span class="koboSpan" id="kobo.251.1">risks that are unique to </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">generative AI.</span></span></p>
<h1 id="_idParaDest-27"><a id="_idTextAnchor037"/><span class="koboSpan" id="kobo.253.1">Looking ahead at risks and implications</span></h1>
<p><span class="koboSpan" id="kobo.254.1">Both generative and</span><a id="_idIndexMarker042"/><span class="koboSpan" id="kobo.255.1"> discriminative AI introduce unique risks and benefits that must be weighed carefully. </span><span class="koboSpan" id="kobo.255.2">However, generative methods can not only carry forward but also exacerbate many risks associated with traditional ML while also introducing new risks. </span><span class="koboSpan" id="kobo.255.3">Consequently, before we can adopt generative AI in the real world and at scale, it is essential to understand the risks and establish responsible governance principles to help </span><span class="No-Break"><span class="koboSpan" id="kobo.256.1">mitigate them:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.257.1">Hallucination</span></strong><span class="koboSpan" id="kobo.258.1">: This is a term widely used to describe when models generate factually inaccurate information. </span><span class="koboSpan" id="kobo.258.2">Generative models are adept at producing plausible-sounding output without basis in fact. </span><span class="koboSpan" id="kobo.258.3">As such, it is critical to ground generative models with factual information. </span><span class="koboSpan" id="kobo.258.4">The term “grounding” refers to appending model inputs with additional information that is known to be factual. </span><span class="koboSpan" id="kobo.258.5">We explore grounding techniques in </span><a href="B21773_07.xhtml#_idTextAnchor225"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.259.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.260.1">. </span><span class="koboSpan" id="kobo.260.2">Additionally, it is essential to have a strategy for evaluating model outputs that includes </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">human review.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.262.1">Plagiarism</span></strong><span class="koboSpan" id="kobo.263.1">: Since generative models are sometimes trained on uncrated datasets, some training corpora may have included data without explicit permissions. </span><span class="koboSpan" id="kobo.263.2">Models may produce information that is subject to copyright protections or can be claimed as </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">intellectual property.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.265.1">Accidental memorization</span></strong><span class="koboSpan" id="kobo.266.1">: As with many ML models that train on immense corpora, generative models tend to memorize parts of the training data. </span><span class="koboSpan" id="kobo.266.2">In particular, they are prone to memorizing sparse examples that do not fit neatly into a broader pattern. </span><span class="koboSpan" id="kobo.266.3">In some cases, models could memorize sensitive information that can be extracted and exposed (Brundage et al., 2020; Carlini et al., 2020). </span><span class="koboSpan" id="kobo.266.4">Consequently, whether consuming a pretrained model or fine-tuning (i.e., continued model training), training data curation </span><span class="No-Break"><span class="koboSpan" id="kobo.267.1">is essential.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.268.1">Toxicity and bias</span></strong><span class="koboSpan" id="kobo.269.1">: Another byproduct of large-scale model training is that the model will inevitably learn any societal biases embedded in the training data. </span><span class="koboSpan" id="kobo.269.2">Biases can manifest as gender, racial, or socioeconomic biases in generated text or images, often replicating or amplifying stereotypes. </span><span class="koboSpan" id="kobo.269.3">We detail mitigations for this risk in </span><a href="B21773_08.xhtml#_idTextAnchor251"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.270.1">Chapter 8</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.271.1">.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.272.1">With an understanding of some of the risks, we turn our focus to the nuanced implications of adopting </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">generative AI:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.274.1">Ethical</span></strong><span class="koboSpan" id="kobo.275.1">: As discussed, thes</span><a id="_idIndexMarker043"/><span class="koboSpan" id="kobo.276.1">e models inevitably learn and reproduce the biases inherent in the training data, raising serious ethical questions. </span><span class="koboSpan" id="kobo.276.2">Similarly, concerns about data privacy and security have emerged due to the model’s susceptibility to memorizing and exposing its training data. </span><span class="koboSpan" id="kobo.276.3">This has led to calls for robust ethical guidelines and data privacy regulations (Gebru et </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">al., 2018).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.278.1">Environmental</span></strong><span class="koboSpan" id="kobo.279.1">: LLMs are computational giants, demanding unprecedented resources for training and implementation. </span><span class="koboSpan" id="kobo.279.2">Thus, they inevitably present environmental impacts. </span><span class="koboSpan" id="kobo.279.3">The energy consumption required to train an LLM produces substantial carbon dioxide emissions—roughly the equivalent lifetime emissions of five vehicles. </span><span class="koboSpan" id="kobo.279.4">Consequently, multiple efforts are underway to increase model efficiency and reduce carbon footprints. </span><span class="koboSpan" id="kobo.279.5">For example, techniques such as reduced bit precision training (or quantization) and parameter efficient fine-tuning (discussed in </span><a href="B21773_05.xhtml#_idTextAnchor180"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.280.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.281.1">) reduce overall training time, helping to shrink </span><span class="No-Break"><span class="koboSpan" id="kobo.282.1">carbon footprints.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.283.1">Social</span></strong><span class="koboSpan" id="kobo.284.1">: Along with environmental impacts, LLMs also have social implications. </span><span class="koboSpan" id="kobo.284.2">As these models become proficient at generating text, simulating intelligent conversation, and automating fundamental tasks, they present an unparalleled opportunity for job automation. </span><span class="koboSpan" id="kobo.284.3">Due to various complex factors, this potential for large-scale automation in the US may disproportionately affect marginalized or underrepresented communities. </span><span class="koboSpan" id="kobo.284.4">Thus, this amplifies prior concerns regarding labor rights and the need for additional protections to </span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">minimize harm.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.286.1">Business and labor</span></strong><span class="koboSpan" id="kobo.287.1">: Along with broader socio-economic implications, we must examine more direct impacts on the business sector. </span><span class="koboSpan" id="kobo.287.2">While generative AI opens up new opportunities, changes in the labor market could bring about immense disruption if not addressed responsibly. </span><span class="koboSpan" id="kobo.287.3">Beyond labor impacts, AI advancements also significantly affect various business sectors. </span><span class="koboSpan" id="kobo.287.4">They can result in the creation of new roles, business models, and opportunities, requiring ongoing governance strategy and explorative frameworks that center on inclusivity, ethics, and </span><span class="No-Break"><span class="koboSpan" id="kobo.288.1">responsible adoption.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.289.1">Addressing these challenges will require technical and scientific improvements, data-specific regulations and laws, ethical guidelines, and human-centered AI governance strategies. </span><span class="koboSpan" id="kobo.289.2">These are integral to building an equitable, secure, and inclusive </span><span class="No-Break"><span class="koboSpan" id="kobo.290.1">AI-driven future.</span></span></p>
<p><span class="koboSpan" id="kobo.291.1">Having discussed the</span><a id="_idIndexMarker044"/><span class="koboSpan" id="kobo.292.1"> history, risks, and limitations of generative AI, we are now better equipped to explore the vast opportunities and appl</span><a id="_idTextAnchor038"/><span class="koboSpan" id="kobo.293.1">ications of such </span><span class="No-Break"><span class="koboSpan" id="kobo.294.1">transformative technology.</span></span></p>
<h1 id="_idParaDest-28"><a id="_idTextAnchor039"/><span class="koboSpan" id="kobo.295.1">Introducing use cases of generative AI</span></h1>
<p><span class="koboSpan" id="kobo.296.1">Generative AI has</span><a id="_idIndexMarker045"/><span class="koboSpan" id="kobo.297.1"> already begun to disrupt various sectors. </span><span class="koboSpan" id="kobo.297.2">The technology is making waves across many disciplines, from enhancing language-based tasks to reshaping digital art. </span><span class="koboSpan" id="kobo.297.3">The following section offers examples of real-world applications of generative AI across </span><span class="No-Break"><span class="koboSpan" id="kobo.298.1">different sectors:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.299.1">Traditional natural language processing</span></strong><span class="koboSpan" id="kobo.300.1">: LLMs, such as Open AI’s GPT series, have elevated traditional NLP and NLG. </span><span class="koboSpan" id="kobo.300.2">As discussed, these models have a unique ability to generate coherent, relevant, and human-like text. </span><span class="koboSpan" id="kobo.300.3">The potential of these models was demonstrated when GPT-3 outperformed classical and modern approaches in several language tasks, displaying an unprecedented understanding of human language. </span><span class="koboSpan" id="kobo.300.4">The release of GPT-4 and Claude 3 marked another milestone, raising the standard even further for </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">state-of-the-art models.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.302.1">Digital art creation</span></strong><span class="koboSpan" id="kobo.303.1">: The advent of “generative art” is evidence of the radical impact of generative AI in the field of digital art. </span><span class="koboSpan" id="kobo.303.2">For instance, artists can use AI generative models to create intricate designs, allowing them to focus on the conceptual aspect of art. </span><span class="koboSpan" id="kobo.303.3">It simplifies the process, reducing the need for high-level </span><span class="No-Break"><span class="koboSpan" id="kobo.304.1">technical acumen.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.305.1">Music creation</span></strong><span class="koboSpan" id="kobo.306.1">: In the music industry, generative AI can enhance the composition process. </span><span class="koboSpan" id="kobo.306.2">Several platforms offer high-quality AI-driven music creation tools that can generate long-form musical compositions combining different music styles across various eras </span><span class="No-Break"><span class="koboSpan" id="kobo.307.1">and genres.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.308.1">Streamlining business processes</span></strong><span class="koboSpan" id="kobo.309.1">: Several businesses have started employing generative AI to enable faster and more efficient processes. </span><span class="koboSpan" id="kobo.309.2">Generative AI-enabled operational efficiencies allow employees to focus on more strategic tasks. </span><span class="koboSpan" id="kobo.309.3">For example, fully integrated LLM email clients can organize emails and (combined with other technologies) learn to prioritize critical emails </span><span class="No-Break"><span class="koboSpan" id="kobo.310.1">over time.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.311.1">Entertainment</span></strong><span class="koboSpan" id="kobo.312.1">: While still largely experimental, LLMs show promising potential to disrupt creative writing and storytelling, particularly in the gaming industry. </span><span class="koboSpan" id="kobo.312.2">For</span><a id="_idIndexMarker046"/><span class="koboSpan" id="kobo.313.1"> example, procedural games could apply LLMs to enhance dynamic storytelling and create more engaging, personalized user experiences. </span><span class="koboSpan" id="kobo.313.2">As technology advances, we may see more mainstream adoption of LLMs in gaming, opening up new possibilities for </span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">interactive narratives.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.315.1">Fashion</span></strong><span class="koboSpan" id="kobo.316.1">: In the fashion industry, generative models help designers innovate. </span><span class="koboSpan" id="kobo.316.2">By using a state-of-the-art generative AI model, designers can create and visualize new clothing styles by simply tweaking a </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">few configurations.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.318.1">Architecture and construction</span></strong><span class="koboSpan" id="kobo.319.1">: In the architectural world, generative-enhanced tools can help architects and urban planners optimize and generate design solutions, leading to more efficient and sustainable </span><span class="No-Break"><span class="koboSpan" id="kobo.320.1">architectural designs.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.321.1">Food industry</span></strong><span class="koboSpan" id="kobo.322.1">: Emerging AI-driven cooking assistants can generate unique food combinations, novel recipes, and modified recipes for highly specific </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">dietary needs.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.324.1">Education</span></strong><span class="koboSpan" id="kobo.325.1">: Generative AI-enhanced educational platforms offer the automatic creation of study aids that can facilitate personalized learning experiences and can automatically generate tailored content to accommodate specific and diverse </span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">learning styles.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.327.1">However, we must balance the breadth of opportunities with sophisticated guardrails and the continued promotion of ethical use. </span><span class="koboSpan" id="kobo.327.2">As data scientists, policymakers, and industry leaders, we must continue to work towards fostering an environment conducive to</span><a id="_idIndexMarker047"/><span class="koboSpan" id="kobo.328.1"> responsible AI deployment. </span><span class="koboSpan" id="kobo.328.2">That said, as generative AI continues to evolve, it presents a future re</span><a id="_idTextAnchor040"/><span class="koboSpan" id="kobo.329.1">plete with novel innovations </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">and applications.</span></span></p>
<h1 id="_idParaDest-29"><a id="_idTextAnchor041"/><span class="koboSpan" id="kobo.331.1">The future of generative AI applications</span></h1>
<p><span class="koboSpan" id="kobo.332.1">The relentless advancement </span><a id="_idIndexMarker048"/><span class="koboSpan" id="kobo.333.1">of generative AI presents a future filled with both possibilities and complex challenges. </span><span class="koboSpan" id="kobo.333.2">Imagine a future where a generative model trained on the world’s leading climate change research can offer practical yet groundbreaking counteractive strategies with precise details about </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">their application.</span></span></p>
<p><span class="koboSpan" id="kobo.335.1">However, as we embrace an increasingly AI-centered future, we should not overlook the existing challenges. </span><span class="koboSpan" id="kobo.335.2">These involve the potential misuse of AI tools, unpredictable implications, and the profound ethical considerations underlying AI adoption. </span><span class="koboSpan" id="kobo.335.3">Additionally, sustainable and eco-conscious development is key, as training large-scale models can </span><span class="No-Break"><span class="koboSpan" id="kobo.336.1">be resource-intensive</span></span></p>
<p><span class="koboSpan" id="kobo.337.1">In an age of accelerated progress, collaboration across all stakeholders—from data scientists, AI enthusiasts, and policymakers to industry leaders—is essential. </span><span class="koboSpan" id="kobo.337.2">By being equipped with comprehensive oversight, robust guidelines, and strategic education initiatives, concerted efforts can safeguard a future where generative AI </span><span class="No-Break"><span class="koboSpan" id="kobo.338.1">is ubiquitous.</span></span></p>
<p><span class="koboSpan" id="kobo.339.1">Despite these hurdles, the transformative potential of generative AI remains unquestionable. </span><span class="koboSpan" id="kobo.339.2">With its capacity to reshape industries, redefine societal infrastructures, and alter our ways of living, learning, and working, generative AI serves as a reminder that we are experiencing a pivotal moment—one propelled by decades of scientific research and </span><a id="_idIndexMarker049"/><span class="koboSpan" id="kobo.340.1">computational ingenuity that are coale</span><a id="_idTextAnchor042"/><span class="koboSpan" id="kobo.341.1">scing to bring us forward as </span><span class="No-Break"><span class="koboSpan" id="kobo.342.1">a society.</span></span></p>
<h1 id="_idParaDest-30"><a id="_idTextAnchor043"/><span class="koboSpan" id="kobo.343.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.344.1">In this chapter, we traced the evolution of generative AI, distinguished it from traditional ML, explored its evolution, discussed its risks and implications, and, hopefully, dispelled some common misconceptions. </span><span class="koboSpan" id="kobo.344.2">We contemplated some of the possibilities anchored by consideration for its </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">responsible adoption.</span></span></p>
<p><span class="koboSpan" id="kobo.346.1">As we move on to the next chapter, we will examine the fundamental architectures behind generative AI, giving us a foundational understanding of the key generative methods, including GANs, diffusion models, and transformers. </span><span class="koboSpan" id="kobo.346.2">These ML methods form the backbone of generative AI and have been instrumental in bringing about the remarkable advancements we </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">see today.</span></span></p>
<h1 id="_idParaDest-31"><a id="_idTextAnchor044"/><span class="koboSpan" id="kobo.348.1">References</span></h1>
<p><span class="koboSpan" id="kobo.349.1">This reference section serves as a repository of sources referenced within this book; you can explore these resources to further enhance your understanding and knowledge of the </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">subject matter:</span></span></p>
<ul>
<li><a href="https://doi.org/10.1007/s11023-020-09526-7https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction&#13;"><span class="No-Break"><span class="koboSpan" id="kobo.351.1">https://doi.org/10.1007/s11023-020-09526-7https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction</span></span></a></li>
<li><span class="koboSpan" id="kobo.352.1">Brundage, M., Avin, S., Clark, J., Toner, H., Eckersley, P., Garfinkel, B., Dafoe, A., Scharre, P., Zeitzoff, T., Filar, B., Anderson, H., Roff, H., Allen, G. </span><span class="koboSpan" id="kobo.352.2">C., Steinhardt, J., Flynn, C., Ó hÉigeartaigh, S., Beard, S., Belfield, H., Farquhar, S., &amp; Amodei, D. </span><span class="koboSpan" id="kobo.352.3">(2018). </span><em class="italic"><span class="koboSpan" id="kobo.353.1">The malicious use of artificial intelligence: Forecasting, prevention, and mitigation</span></em><span class="koboSpan" id="kobo.354.1">. </span><span class="koboSpan" id="kobo.354.2">arXiv [</span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">cs.AI]. </span></span><a href="http://arxiv.org/abs/1802.07228"><span class="No-Break"><span class="koboSpan" id="kobo.356.1">http://arxiv.org/abs/1802.07228</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.357.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.358.1"> Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., Oprea, A., &amp; Raffel, C. </span><span class="koboSpan" id="kobo.358.2">(2020). </span><em class="italic"><span class="koboSpan" id="kobo.359.1">Extracting training data from large language models</span></em><span class="koboSpan" id="kobo.360.1">. </span><span class="koboSpan" id="kobo.360.2">arXiv [</span><span class="No-Break"><span class="koboSpan" id="kobo.361.1">cs.CR]. </span></span><a href="http://arxiv.org/abs/2012.07805"><span class="No-Break"><span class="koboSpan" id="kobo.362.1">http://arxiv.org/abs/2012.07805</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.363.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.364.1"> Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. </span><span class="koboSpan" id="kobo.364.2">(2018). </span><em class="italic"><span class="koboSpan" id="kobo.365.1">BERT: Pre-training of deep bidirectional transformers for language understanding</span></em><span class="koboSpan" id="kobo.366.1">. </span><span class="koboSpan" id="kobo.366.2">arXiv [</span><span class="No-Break"><span class="koboSpan" id="kobo.367.1">cs.CL]. </span></span><a href="http://arxiv.org/abs/1810.04805"><span class="No-Break"><span class="koboSpan" id="kobo.368.1">http://arxiv.org/abs/1810.04805</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.369.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.370.1">Hagendorff, T. </span><span class="koboSpan" id="kobo.370.2">(2020). </span><span class="koboSpan" id="kobo.370.3">Publisher correction to </span><em class="italic"><span class="koboSpan" id="kobo.371.1">The ethics of AI ethics: An evaluation of guidelines</span></em><span class="koboSpan" id="kobo.372.1">. </span><em class="italic"><span class="koboSpan" id="kobo.373.1">Minds and Machines</span></em><span class="koboSpan" id="kobo.374.1">, 30(3), </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">457–461. </span></span><a href="https://doi.org/10.1007/s11023-020-09526-7"><span class="No-Break"><span class="koboSpan" id="kobo.376.1">https://doi.org/10.1007/s11023-020-09526-7</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.377.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.378.1"> Hern, A. </span><span class="koboSpan" id="kobo.378.2">(2019, February 14). </span><em class="italic"><span class="koboSpan" id="kobo.379.1">New AI fake text generator may be too dangerous to release, say creators</span></em><span class="koboSpan" id="kobo.380.1">. </span><em class="italic"><span class="koboSpan" id="kobo.381.1">The </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.382.1">Guardian</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">. </span></span><a href="https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction"><span class="No-Break"><span class="koboSpan" id="kobo.384.1">https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.385.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.386.1"> Ho, J., Jain, A., &amp; Abbeel, P. </span><span class="koboSpan" id="kobo.386.2">(2020). </span><em class="italic"><span class="koboSpan" id="kobo.387.1">Denoising diffusion probabilistic models</span></em><span class="koboSpan" id="kobo.388.1">. </span><span class="koboSpan" id="kobo.388.2">arXiv [</span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">cs.LG]. </span></span><a href="http://arxiv.org/abs/2006.11239"><span class="No-Break"><span class="koboSpan" id="kobo.390.1">http://arxiv.org/abs/2006.11239</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.391.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.392.1"> Kaplan, J., McCandlish, S., Henighan, T., Brown, T. </span><span class="koboSpan" id="kobo.392.2">B., Chess, B., Child, R., Gray, S., Kingma, D. </span><span class="koboSpan" id="kobo.392.3">P., &amp; Welling, M. </span><span class="koboSpan" id="kobo.392.4">(2013). </span><em class="italic"><span class="koboSpan" id="kobo.393.1">Auto-encoding variational bayes</span></em><span class="koboSpan" id="kobo.394.1">. </span><span class="koboSpan" id="kobo.394.2">arXiv [</span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">stat.ML]. </span></span><a href="http://arxiv.org/abs/1312.6114"><span class="No-Break"><span class="koboSpan" id="kobo.396.1">http://arxiv.org/abs/1312.6114</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.397.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.398.1">Muhammad, T., Aftab, A. </span><span class="koboSpan" id="kobo.398.2">B., Ahsan, M. </span><span class="koboSpan" id="kobo.398.3">M., Muhu, M. </span><span class="koboSpan" id="kobo.398.4">M., Ibrahim, M., Khan, S. </span><span class="koboSpan" id="kobo.398.5">I., &amp; Alam, M. </span><span class="koboSpan" id="kobo.398.6">S. </span><span class="koboSpan" id="kobo.398.7">(2022). </span><em class="italic"><span class="koboSpan" id="kobo.399.1">Transformer-based deep learning model for stock price prediction: A case study on Bangladesh stock market</span></em><span class="koboSpan" id="kobo.400.1">. </span><span class="koboSpan" id="kobo.400.2">arXiv [</span><span class="No-Break"><span class="koboSpan" id="kobo.401.1">q-fin.ST]. </span></span><a href="http://arxiv.org/abs/2208.08300"><span class="No-Break"><span class="koboSpan" id="kobo.402.1">http://arxiv.org/abs/2208.08300</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.403.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.404.1"> OpenAI. </span><span class="koboSpan" id="kobo.404.2">(2023). </span><em class="italic"><span class="koboSpan" id="kobo.405.1">GPT-4 technical report</span></em><span class="koboSpan" id="kobo.406.1">. </span><span class="koboSpan" id="kobo.406.2">arXiv [</span><span class="No-Break"><span class="koboSpan" id="kobo.407.1">cs.CL]. </span></span><a href="http://arxiv.org/abs/2303.08774"><span class="No-Break"><span class="koboSpan" id="kobo.408.1">http://arxiv.org/abs/2303.08774</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.409.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.410.1">Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., &amp; Sutskever, I. </span><span class="koboSpan" id="kobo.410.2">(2018). </span><em class="italic"><span class="koboSpan" id="kobo.411.1">Language models are unsupervised </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.412.1">multitask learners</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.413.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.414.1">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. </span><span class="koboSpan" id="kobo.414.2">N., Kaiser, L., &amp; Polosukhin, I. </span><span class="koboSpan" id="kobo.414.3">(2017). </span><em class="italic"><span class="koboSpan" id="kobo.415.1">Attention Is All You Need</span></em><span class="koboSpan" id="kobo.416.1">. </span><span class="koboSpan" id="kobo.416.2">arXiv [</span><span class="No-Break"><span class="koboSpan" id="kobo.417.1">cs.CL]. </span></span><a href="http://arxiv.org/abs/1706.03762"><span class="No-Break"><span class="koboSpan" id="kobo.418.1">http://arxiv.org/abs/1706.03762</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.419.1">.</span></span></li>
</ul>
</div>
</body></html>