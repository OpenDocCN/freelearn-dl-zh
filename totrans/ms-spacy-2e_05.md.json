["```py\n    mkdir data\n    wget -P data https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition/blob/main/chapter_05/data/atis_intents.csv\n    ```", "```py\n    import pandas as pd\n    df = pd.read_csv(\"data/atis_intents.csv\", header=None, \n                     names=[\"utterance\", \"text\"])\n    df.shape\n    ```", "```py\n    df[\"utterance\"].value_counts().plot.barh()\n    ```", "```py\n    import spacy\n    from spacy import displacy\n    ```", "```py\n    nlp = spacy.load(\"en_core_web_sm\")\n    text = \"i want to fly from boston at 838 am and arrive in denver at 1110 in the morning\"\n    doc = nlp(text)\n    ```", "```py\n    displacy.render(doc, style='ent')\n    ```", "```py\nnlp.pipe_names\n```", "```py\n    spanruler_component = nlp.add_pipe(\"span_ruler\")\n    ```", "```py\n    patterns_location_spanruler = [\n        {\"label\": \"LOCATION\", \n         \"pattern\": [{\"POS\": \"ADP\"}, {\"ENT_TYPE\": \"GPE\"}]}]\n    spanruler_component.add_patterns(patterns_location_spanruler)\n    ```", "```py\n    doc = nlp(text)\n    options = {\"spans_key\": \"ruler\"}\n    displacy.render(doc, style='span', options=options)\n    ```", "```py\n    nlp.remove_pipe(\"span_ruler\")\n    ```", "```py\n    config = {\"annotate_ents\": True, \"overwrite\": False}\n    ```", "```py\n    spanruler_component_v2 = nlp.add_pipe(\n        \"span_ruler\", config=config)\n    spanruler_component_v2.add_patterns(patterns_location_spanruler)\n    doc = nlp(text)\n    displacy.render(doc, style='ent')\n    ```", "```py\nI bought flowers.\nHe loved his cat.\nHe borrowed my book.\n```", "```py\nI bought\n```", "```py\nYesterday I slept for 8 hours.\nThe cat ran towards me.\nWhen I went out, the sun was shining.\nHer cat died 3 days ago.\n```", "```py\nI slept.\nThe cat ran.\nThe sun was shining.\nHer cat died.\n```", "```py\nI bought flowers.       I bought what?      - flowers\nHe loved his cat.       He loved who?       - his cat\nHe borrowed my book.    He borrowed what?   - my book\n```", "```py\nHe gave me his book.    He gave his book to whom?   - me\nHe gave his book to me. He gave his book to whom?   - me\n```", "```py\ntext = \"show me flights from denver to philadelphia on tuesday\"\ndoc = nlp(text)\ndisplacy.render(doc, style='dep')\n```", "```py\npattern = [\n    {\n        \"RIGHT_ID\": \"direct_object_token\",\n        \"RIGHT_ATTRS\": {\"DEP\": \"dobj\"}\n    }\n]\n```", "```py\npattern = [\n    {\n        \"RIGHT_ID\": \"direct_object_token\",\n        \"RIGHT_ATTRS\": {\"DEP\": \"dobj\"}\n    },\n    {\n        \"LEFT_ID\": \"direct_object_token\",\n        \"REL_OP\": \"<\",\n        \"RIGHT_ID\": \"verb_token\",\n        \"RIGHT_ATTRS\": {\"POS\": \"VERB\"}\n    }\n]\n```", "```py\n    from spacy.matcher import DependencyMatcher\n    matcher = DependencyMatcher(nlp.vocab)\n    ```", "```py\n    def show_intent(matcher, doc, i, matches):\n        match_id, token_ids = matches[i]\n        verb_token = doc[token_ids[1]]\n        dobj_token = doc[token_ids[0]]\n        intent = verb_token.lemma_ + dobj_token.lemma_.capitalize()\n    print(\"Intent:\", intent)\n    ```", "```py\n    matcher.add(\"INTENT\", [pattern], on_match=show_intent)\n    doc = nlp(\"show me flights from denver to philadelphia on tuesday\")\n    matches = matcher(doc)\n    ```", "```py\n    show all flights and fares from denver to san francisco\n    ```", "```py\n    pattern_two = [\n        {\n            \"RIGHT_ID\": \"direct_object_token\",\n            \"RIGHT_ATTRS\": {\"DEP\": \"dobj\"}\n        },\n        {\n            \"LEFT_ID\": \"direct_object_token\",\n            \"REL_OP\": \"<\",\n            \"RIGHT_ID\": \"verb_token\",\n            \"RIGHT_ATTRS\": {\"POS\": \"VERB\"}\n        },\n        {\n            \"LEFT_ID\": \"direct_object_token\",\n            \"REL_OP\": \">\",\n            \"RIGHT_ID\": \"conjunction_token\",\n            \"RIGHT_ATTRS\": {\"DEP\": \"conj\"}\n        }\n    ]\n    ```", "```py\n    def show_two_intents(matcher, doc, i, matches):\n        match_id, token_ids = matches[i]\n        verb_token = doc[token_ids[1]]\n        dobj_token = doc[token_ids[0]]\n        conj_token = doc[token_ids[2]]\n        intent = verb_token.lemma_ + \\\n            dobj_token.lemma_.capitalize() + \";\" + \\\n            verb_token.lemma_ + conj_token.lemma_.capitalize()\n        print(\"Two intents:\", intent)\n    ```", "```py\n    matcher.add(\"TWO_INTENTS\", [pattern_two], \n                on_match=show_two_intents)\n    ```", "```py\n    doc = nlp(\"show all flights and fares from denver to san francisco\")\n    matches = matcher(doc)\n    ```", "```py\n    class IntentComponent:\n        def __init__(self, nlp: Language):\n            self.matcher = DependencyMatcher(nlp.vocab)\n            pattern = [\n                {\n                    \"RIGHT_ID\": \"direct_object_token\",\n                    \"RIGHT_ATTRS\": {\"DEP\": \"dobj\"}\n                },\n                {\n                    \"LEFT_ID\": \"direct_object_token\",\n                    \"REL_OP\": \"<\",\n                    \"RIGHT_ID\": \"verb_token\",\n                    \"RIGHT_ATTRS\": {\"POS\": \"VERB\"}\n                }\n            ]\n            pattern_two = [\n                {\n                    \"RIGHT_ID\": \"direct_object_token\",\n                    \"RIGHT_ATTRS\": {\"DEP\": \"dobj\"}\n                },\n                {\n                    \"LEFT_ID\": \"direct_object_token\",\n                    \"REL_OP\": \"<\",\n                    \"RIGHT_ID\": \"verb_token\",\n                    \"RIGHT_ATTRS\": {\"POS\": \"VERB\"}\n                },\n                {\n                    \"LEFT_ID\": \"direct_object_token\",\n                    \"REL_OP\": \">\",\n                    \"RIGHT_ID\": \"conjunction_token\",\n                    \"RIGHT_ATTRS\": {\"DEP\": \"conj\"}\n                }\n            ]\n            self.matcher.add(\"INTENT\", [pattern])\n            self.matcher.add(\"TWO_INTENTS\", [pattern_two])\n            if not Doc.has_extension(\"intent\"):\n                Doc.set_extension(\"intent\", default=None)\n    ```", "```py\n        def __call__(self, doc: Doc) -> Doc:\n            matches = self.matcher(doc)\n            for match_id, token_ids in matches:\n                string_id = nlp.vocab.strings[match_id]\n                if string_id == \"TWO_INTENTS\":\n                    verb_token = doc[token_ids[1]]\n                    dobj_token = doc[token_ids[0]]\n                    conj_token = doc[token_ids[2]]\n                    intent = verb_token.lemma_ + \\\n                        dobj_token.lemma_.capitalize() + \\\n                        \";\" + verb_token.lemma_ + \\\n                        conj_token.lemma_.capitalize()\n                    doc._.intent = intent\n                    break\n            else:\n                for match_id, token_ids in matches:\n                    string_id = nlp.vocab.strings[match_id]\n                    if string_id == \"INTENT\":\n                        verb_token = doc[token_ids[1]]\n                        dobj_token = doc[token_ids[0]]\n                        intent = verb_token.lemma_ + \\\n                            dobj_token.lemma_.capitalize()\n                        doc._.intent = intent\n            return doc\n    ```", "```py\n    @Language.factory(\"intent_component\")\n    def create_intent_component(nlp: Language, name: str):\n        return IntentComponent(nlp)\n    ```", "```py\nnlp.add_pipe(\"intent_component\")\ntext = \"show all flights and fares from denver to san francisco\"\ndoc = nlp(text)\ndoc._.intent\n```", "```py\nutterance_texts = df.text.to_list()\nprocessed_docs = list(nlp.pipe(utterance_texts))\nprint(processed_docs[0], processed_docs[0]._.intent)\n```", "```py\nimport timestart_time = time.time()\nutterance_texts = df.text.to_list()\nprocessed_docs_vanilla = [nlp(text) for text in utterance_texts]\nend_time = time.time()\nexecution_time = end_time - start_time\nprint(\"Execution time:\", execution_time, \"seconds\")\n>>> Execution time: 27.12 seconds\n```", "```py\nimport timestart_time = time.time()\nutterance_texts = df.text.to_list()\nprocessed_docs_pipe = list(nlp.pipe(utterance_texts))\nend_time = time.time()\nexecution_time = end_time - start_time\nprint(\"Execution time:\", execution_time, \"seconds\")\n>>> Execution time: 5.90 seconds\n```", "```py\nimport spacy\nfrom spacy.language import Language\nfrom spacy.tokens import Doc\nfrom spacy.matcher import DependencyMatcher\n@Language.factory(\"intent_component\")\ndef create_intent_component(nlp: Language, name: str):\n@Language.factory(\"intent_component\")\ndef create_intent_component(nlp: Language, name: str):\n    return IntentComponent(nlp)\nclass IntentComponent:\n    def __init__(self, nlp: Language):\n                self.matcher = DependencyMatcher(nlp.vocab)\n        pattern = [\n            {\n                \"RIGHT_ID\": \"direct_object_token\",\n                \"RIGHT_ATTRS\": {\"DEP\": \"dobj\"}\n            },\n            {\n                \"LEFT_ID\": \"direct_object_token\",\n                \"REL_OP\": \"<\",\n                \"RIGHT_ID\": \"verb_token\",\n                \"RIGHT_ATTRS\": {\"POS\": \"VERB\"}\n            }\n        ]\n        pattern_two = [\n            {\n                \"RIGHT_ID\": \"direct_object_token\",\n                \"RIGHT_ATTRS\": {\"DEP\": \"dobj\"}\n            },\n            {\n                \"LEFT_ID\": \"direct_object_token\",\n                \"REL_OP\": \"<\",\n                \"RIGHT_ID\": \"verb_token\",\n                \"RIGHT_ATTRS\": {\"POS\": \"VERB\"}\n            },\n            {\n                \"LEFT_ID\": \"direct_object_token\",\n                \"REL_OP\": \">\",\n                \"RIGHT_ID\": \"conjunction_token\",\n                \"RIGHT_ATTRS\": {\"DEP\": \"conj\"}\n            }\n        ]\n        self.matcher.add(\"INTENT\", [pattern])\n        self.matcher.add(\"TWO_INTENTS\", [pattern_two])\n        if not Doc.has_extension(\"intent\"):\n            Doc.set_extension(\"intent\", default=None)\n    def __call__(self, doc: Doc) -> Doc:\n        matches = self.matcher(doc)\n        for match_id, token_ids in matches:\n            string_id = nlp.vocab.strings[match_id]\n            if string_id == \"TWO_INTENTS\":\n                verb_token = doc[token_ids[1]]\n                dobj_token = doc[token_ids[0]]\n                conj_token = doc[token_ids[2]]\n                intent = verb_token.lemma_ + \\\n                    dobj_token.lemma_. capitalize() + \";\" + \\\n                    verb_token.lemma_ + \\\n                    conj_token.lemma_. capitalize()\n                doc._.intent = intent\n                break\n        else:\n            for match_id, token_ids in matches:\n                string_id = nlp.vocab.strings[match_id]\n                if string_id == \"INTENT\":\n                    verb_token = doc[token_ids[1]]\n                    dobj_token = doc[token_ids[0]]\n                    intent = verb_token.lemma_ + \\\n                        dobj_token.lemma_. capitalize()\n                    doc._.intent = intent\n       return doc\nnlp = spacy.load(\"en_core_web_sm\")\nnlp.add_pipe(\"intent_component\")\ntext = \"show all flights and fares from denver to san francisco\"\ndoc = nlp(text)\ndoc._.intent\n```"]