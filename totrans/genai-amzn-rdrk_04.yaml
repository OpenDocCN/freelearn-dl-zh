- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Customizing Models for Enhanced Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When general-purpose models fall short of delivering satisfactory results for
    your domain-specific use case, customizing FMs becomes crucial. This chapter delves
    into the process of customizing FMs while using techniques such as fine-tuning
    and continued pre-training to enhance their performance. We’ll begin by examining
    the rationale behind customizing the base FM and exploring the mechanics of fine-tuning.
    Subsequently, we will delve into data preparation techniques to ensure our data
    is formatted appropriately for creating a custom model using both the AWS console
    and APIs. We will understand various components within model customization and
    different customization APIs that you can call from your application.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we will analyze the model’s behavior and perform inference. Finally,
    we will conclude this chapter by discussing guidelines and best practices for
    customizing Bedrock models.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to understand the importance and
    process of customizing a model for your domain-specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following key topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Why is customizing FMs important?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding model customization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a custom model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing the results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guidelines and best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, you need to have access to an *AWS* account. If you don’t
    have one, you can go to [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    and create an AWS account.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have access to an AWS account, you will need to install and configure
    the AWS CLI ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)) so that
    you can access Amazon Bedrock FMs from your local machine. In addition, you will
    need to set up the AWS Python SDK (Boto3) since the majority of the code cells
    we will be executing require it ([https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html](https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html)).
    You can set up Python by installing it on your local machine, using AWS Cloud9,
    utilizing AWS Lambda, or leveraging Amazon SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: There will be a charge associated with invocating and customizing the FMs of
    Amazon Bedrock. Please refer to [https://aws.amazon.com/bedrock/pricing/](https://aws.amazon.com/bedrock/pricing/)
    to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: Why is customizing FMs important?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we looked at several prompt engineering techniques
    to improve the performance of a model. As we also saw in [*Chapter 1*](B22045_01.xhtml#_idTextAnchor014)
    (and shown in *Figure 4**.1*), these FMs are trained on massive amounts of data
    (GBs, TBs, or PBs) with millions to billions of parameters, allowing them to understand
    relationships between words in context to predict subsequent sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Training an FM](img/B22045_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Training an FM
  prefs: []
  type: TYPE_NORMAL
- en: '*So, why do we need to customize* *these models?*'
  prefs: []
  type: TYPE_NORMAL
- en: That’s a fair question since a lot of use cases can be directly solved by using
    prompt engineering and RAG techniques (which we will cover in [*Chapter 5*](B22045_05.xhtml#_idTextAnchor090)).
    However, consider a situation where you require the model to adhere to a particular
    writing style, output format, or domain-specific terminology. For instance, you
    may need the model to analyze financial earnings reports or medical records accurately.
    In such cases, the pre-trained models might not have been exposed to the desired
    writing style or specialized vocabularies, limiting their performance despite
    effective prompt crafting or RAG implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To bridge this gap and enhance the model’s domain-specific language understanding
    and generation capabilities, customization becomes essential. By fine-tuning the
    pre-trained models on domain-specific data or adapting them to the desired writing
    style or output format, you can tailor their performance to meet your unique requirements,
    ensuring more accurate and relevant responses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Generative AI performance techniques](img/B22045_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Generative AI performance techniques
  prefs: []
  type: TYPE_NORMAL
- en: If you look at the spectrum of generative AI performance techniques shown in
    *Figure 4**.2* for improving the performance of FMs, it ranges from prompt engineering
    to training the model from scratch. For domain-specific data, prompt engineering
    techniques may provide low accuracy, but they involve less effort and are cost-effective.
    Prompt engineering is a better option if you have a simple task and don’t need
    a new domain-specific dataset. If you would like to understand how prompt engineering
    works, please go back to [*Chapter 3*](B22045_03.xhtml#_idTextAnchor053).
  prefs: []
  type: TYPE_NORMAL
- en: Next on the spectrum with a little bit of increasing complexity, cost, and accuracy
    is RAG. This technique fetches data from outside the language model, such as from
    internal knowledge bases or external sources. It is a particularly useful technique
    when you have large corpora of documents that do not fit the context length of
    the model. We will discuss RAG in more detail in [*Chapter 5*](B22045_05.xhtml#_idTextAnchor090).
  prefs: []
  type: TYPE_NORMAL
- en: Further on the spectrum, customizing the model is essentially more time-consuming
    and costly. However, it provides greater accuracy to your specialized use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two customization techniques within Amazon Bedrock: fine-tuning and
    continued pretraining.'
  prefs: []
  type: TYPE_NORMAL
- en: In *fine-tuning*, the model is trained with the labeled dataset – a supervised
    learning approach. The labeled dataset that you provide will be specific to your
    use case. Whether you’re working in healthcare, finance, or any other field, you
    can fine-tune your model to become an expert in that particular domain. In healthcare,
    for example, the model can be fine-tuned for medical specialization, allowing
    it to understand and interpret medical records with greater accuracy. Similarly,
    a financial analysis model can be fine-tuned for niche financial analysis, enabling
    it to identify patterns and trends in financial data that may be missed by traditional
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'To fine-tune a model using your own data, you need to have a sufficient amount
    of high-quality data that is relevant to the task you want to perform. This data
    should be labeled and annotated to provide the model with the necessary information
    for training. As shown in *Figure 4**.3*, we can use this labeled dataset to fine-tune
    the base FM, which then generates a custom model. You can then use the custom
    model to generate responses that are tailored to your specific domain and use
    case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Fine-tuning](img/B22045_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Fine-tuning
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s say you work in the medical industry and would like to summarize
    a dialog between two doctors discussing the medical report of the patient, extract
    the information to be put into medical forms, and maybe write it in layman’s terms.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the base FMs might not be trained on the domain-specific dataset.
    Hence, this is an example scenario where when we perform fine-tuning, we will
    provide the model with labeled examples of how the prompt and response should
    look like.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *continued pre-training*, we adapt to a new domain or train the model to
    learn the terminologies of an unfamiliar domain. This involves providing additional
    continuous training to an FM while utilizing large amounts of unlabeled data.
    When we say unlabeled data, we mean that there is no target label, and the model
    will learn the patterns from the provided texts. This contrasts with fine-tuning,
    which involves using smaller quantities of labeled data. *Figure 4**.4* highlights
    the difference between the labeled and unlabeled data that’s required for continued
    pre-training and fine-tuning, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Unlabeled versus labeled data](img/B22045_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Unlabeled versus labeled data
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of continued pre-training can include training the model to learn
    the terminologies of the financial industry so that it can understand financial
    reports, or training the model to learn quantum physics by giving it abundant
    information from books so that it will be able to evaluate/predict the tokens
    associated with string theory with greater accuracy. Let’s say that two physicists
    are having a dialog around string theory, and we pass that dialog as a context
    to the base FM (as shown in *Figure 4**.5*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Quantum physicist dialog and question](img/B22045_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Quantum physicist dialog and question
  prefs: []
  type: TYPE_NORMAL
- en: It could be possible that the base FM we are using here isn’t familiar with
    quantum physics – that is, the base FM hasn’t been trained on a dataset related
    to quantum physics.
  prefs: []
  type: TYPE_NORMAL
- en: So, when we ask the model a question such as `What are E8 x E8 symmetry groups?`,
    the model hallucinates and doesn’t explain this concept since it doesn’t know
    about string theory.
  prefs: []
  type: TYPE_NORMAL
- en: 'With continued pre-training, we train the model on an unfamiliar domain by
    providing the base FM with a large amount of unlabeled datasets. For example,
    we could train the model on textbooks about quantum computing in the desired format,
    as explained in the *Preparing the data* section, which then creates a custom
    model (as shown in *Figure 4**.6*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Continued pre-training](img/B22045_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Continued pre-training
  prefs: []
  type: TYPE_NORMAL
- en: Continued pre-training presents certain challenges. As we are training the entire
    model, the weights and biases are what demand heavy computational resources and
    diverse unlabeled text data.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you’re deciding whether to use custom models over other methods, such
    as prompt engineering and RAG, several factors come into play. These include the
    task that you are working on, the availability of data, computational resources,
    and cost. Here are some guidelines to help you make an informed decision:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Complexity level**: Creating custom models is particularly useful when you
    have tasks that are complex and require the model to understand intricate details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specialized data**: Having a sufficient amount of specialized data for creating
    custom models will provide remarkable results. Make sure your data is clean (free
    from errors, inconsistencies, and duplicates) and prepared (formatted, transformed,
    and split into appropriate subsets) before you start the training process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computational resources and cost**: When you create custom models, you’ll
    need to purchase Provisioned Throughput, which gives you a dedicated capacity
    to deploy the model. Make sure you review the pricing based on the model type
    and commitment terms. We will discuss Provisioned Throughput in detail in the
    *Analyzing the results* section of this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, creating custom models provides you with greater control over how
    you want the model to respond. You can customize it precisely to your needs, making
    it suitable for tasks that require fine-grained customization, such as responding
    in a specific tone, dialect, or inclusive language.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s understand some key concepts of model customization before we start our
    first model customization job.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding model customization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The principle behind fine-tuning and continued pre-training comes from the broad
    concept of **transfer learning**, which, as its name suggests, entails transferring
    knowledge that’s been acquired from one problem to other often related but distinct
    problems. This practice is widely employed in the field of **machine learning**
    (**ML**) to enhance the performance of models on new tasks or domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model customization is a five-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Identify your use case and data**: Identifying the use case/task and how
    it solves your organization’s business objectives is a critical step. Do you want
    to summarize legal documents, perform Q&A on medical reports, or do something
    else? Once you’ve identified the use case, you must gather enough relevant datasets
    that you can use for model customization. The dataset should contain examples
    that the model can learn intricate details from. Remember, how your custom model
    performs on your task-specific use case depends on the quality of the dataset
    that you provide for training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prepare the dataset**: Once you’ve gathered the dataset, you have to clean
    and preprocess it. For fine-tuning, you need to have labeled examples in **JSON
    lines** (**JSONL**) format. For continued pre-training, you need to have unlabeled
    examples in JSONL format. We will discuss this in more detail in the *Preparing
    the* *data* section.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Select the base pre-trained model**: Once the dataset has been prepared,
    you have to select an existing base pretrained model that you would like to fine-tune.
    You can look at the website of the model provider to understand the model attributes.
    If it is fit for your use case, try prompt engineering techniques to check which
    model responds closest to what you are looking for, and also evaluate the FMs
    using **Model evaluation** within Amazon Bedrock or using **Model leaderboards**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Model evaluation*: Bedrock provides two distinct evaluation methods: automatic
    evaluation and human evaluation. Automatic evaluation utilizes predefined metrics
    such as accuracy, robustness, and toxicity screening, whereas with human evaluation,
    you can define custom metrics such as friendliness, stylistic adherence, or alignment
    with brand voice. We will have a more detailed discussion on model evaluation
    in [*Chapter 11*](B22045_11.xhtml#_idTextAnchor207).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model leaderboards*: Several leaderboards are available that rank models based
    on their performance on various tasks, such as text generation, summarization,
    sentiment analysis, and more. Some of the most popular leaderboards include **General
    Language Understanding Evaluation** (**GLUE**), SuperGLUE, HELM, and OpenLLM by
    HuggingFace.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Please note that although it’s good to understand the performance of the FM
    through leaderboards, for real-world use cases, you have to be cautious and not
    rely solely on leaderboards as they may lack the robustness required to mirror
    the complexity of real-world use.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Configure and start the fine-tuning job**: Once you’ve identified the base
    FM and the dataset is ready, you can configure the fine-tuning job by specifying
    hyperparameters, the input and output S3 path for the dataset and store metrics,
    respectively, and networking and security settings. We will discuss this in more
    detail in the *Creating a custom* *model* section.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Evaluate and iterate**: Once the model is ready, you can evaluate and analyze
    it based on the metrics and logs stored by the model. To do so, you can put aside
    a validation set that provides the performance metric of the custom model you’ve
    created. We will discuss this in more detail in the *Analyzing the* *results*
    section.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When we are customizing a model, Amazon Bedrock creates a copy of the base FM,
    on which we essentially update its model weights. **Weights** are key components
    in **artificial neural networks** (**ANNs**) and are attached to the inputs (or
    features). These weights define which features are important in predicting the
    output and getting better at specific tasks. *Figure 4**.7* shows a simplified
    ANN architecture where these inputs, along with their weights, are processed by
    **summation** and the **activation function** (both defined in the model algorithm)
    to get the output (**Y**).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Simplified ANN](img/B22045_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Simplified ANN
  prefs: []
  type: TYPE_NORMAL
- en: For a deeper dive into ANNs, there are numerous online tutorials and courses
    available that provide in-depth explanations and examples of neural network concepts,
    architectures, and training techniques. Additionally, textbook classics such as
    *Neural Networks and Deep Learning, Michael Nielsen, Determination Press* and
    *Deep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville, MIT Press*
    offer comprehensive theoretical and mathematical foundations.
  prefs: []
  type: TYPE_NORMAL
- en: When we perform model customization (fine-tuning or continued pre-training),
    we update the model weights. While updating the model weights, a common problem
    can occur called **catastrophic forgetting**. This is when the model starts to
    forget some information it was originally trained on due to weight modifications,
    which can lead to degraded performance on more generalized tasks. In general,
    this can happen due to overfitting the training data, which means the model provides
    an accurate response to the training data but can’t generalize well and provides
    degraded performance on new information. In addition, customizing the model can
    be costly and resource-intensive, something that requires extensive memory utilization.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome these challenges, a technique called **Parameter-efficient Fine-tuning**
    (**PEFT**) was introduced in the paper *Parameter-Efficient Transfer Learning
    for* *NLP* ([https://arxiv.org/pdf/1902.00751](https://arxiv.org/pdf/1902.00751)).
  prefs: []
  type: TYPE_NORMAL
- en: Note that at the time of writing, Bedrock does not support PEFT. However, it’s
    good to have an understanding of the PEFT technique.
  prefs: []
  type: TYPE_NORMAL
- en: PEFT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In PEFT, you don’t need to fine-tune all the model parameters, something that
    can be quite time-consuming, resource-intensive, and costly. Instead, it freezes
    much of the model weights and you only need to train a small number of them. This
    makes it memory and compute-efficient, less susceptible to catastrophic forgetting,
    and cheaper to store the model on the hardware.
  prefs: []
  type: TYPE_NORMAL
- en: 'When fine-tuning LLMs, various techniques can reduce the number of trainable
    parameters to improve efficiency. We can categorize these PEFT methods into three
    main classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Selective methods**: These update only certain components or layers of the
    original LLM during fine-tuning. This allows you to focus on the most relevant
    parts of the model. However, it can result in suboptimal performance compared
    to full fine-tuning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reparameterization methods**: These introduce low-rank matrices to compress
    the original weights. Examples include such as **Low-Rank Adaptation of Large
    Language Models** (**LoRA**). This reduces parameters while still modifying the
    whole model. The trade-off is increased memory usage during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Additive methods**: These keep the original weights of the LLM frozen and
    add new trainable layers for task-specific adaptation. Additive methods such as
    **adapters** add the trainable layer inside the encoder or decoder component of
    the transformer architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of the PEFT approach involves balancing metrics such as parameter
    and memory efficiency against model quality, training speed, and cost. Selectively
    updating parts of a model offers one end of this trade-off, while adapters and
    prompts maximize parameter efficiency at the cost of some architectural changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, we’ve covered PEFT and its techniques at a very high level. However,
    if you are interested in learning more about it, go to [https://github.com/huggingface/peft](https://github.com/huggingface/peft).
    In addition, the *Generative AI with Large Language Models* course provides in-depth
    information about PEFT methods: [https://www.deeplearning.ai/courses/generative-ai-with-llms/](https://www.deeplearning.ai/courses/generative-ai-with-llms/).'
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to fine-tuning techniques such as PEFT, **hyperparameter tuning**
    also plays a big role in ensuring a model retains its pretrained knowledge. Hyperparameters
    are configuration settings that control the model training process, much like
    knobs that can be tweaked and tuned. Models have various hyperparameters, including
    the learning rate, number of epochs, batch size, beta, gamma, and more. Each model
    may require a different set of optimal hyperparameter values, found through experimentation,
    to achieve the best performance and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The **learning rate** hyperparameter controls how quickly the model is adapted
    to the task. It also controls how much the model’s parameters are adjusted during
    each iteration of the training process. It determines the step size at which the
    model’s parameters are updated based on the calculated gradients (which represent
    the direction and magnitude of the changes needed to minimize the loss function).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider an analogy that might help you visualize the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine that you’re trying to find the lowest point in a hilly landscape, but
    you’re blindfolded. You can only sense the steepness of the slope you’re standing
    on (the gradient) and take steps accordingly. The learning rate determines how
    big or small those steps should be:'
  prefs: []
  type: TYPE_NORMAL
- en: If the learning rate is too high, you might overshoot the lowest point and end
    up on the other side of the hill, continually overshooting and never converging
    to the optimal solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the learning rate is too low, you might take tiny steps and get stuck on
    a plateau or make painfully slow progress toward the lowest point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ideal learning rate allows you to take reasonably sized steps that bring
    you progressively closer to the lowest point (the optimal set of model parameters)
    without overshooting or getting stuck.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, finding the optimal learning rate is often a matter of experimentation
    and tuning. Different models and datasets may require different learning rates
    for the training process to converge effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the concepts behind fine-tuning, let’s start the customization
    process by preparing the data.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve already seen why customizing the model is important to improve its accuracy
    and performance. We’ve also seen that continued pre-training is an unsupervised
    learning approach that needs unlabeled data, whereas fine-tuning is a supervised
    learning approach that needs labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: The type of data we provide to the model can change the way the model responds.
    If the data is biased or has highly correlated features, you might not get the
    right responses from the trained custom model. This is true for any ML models
    you are training, so it is essential to provide high-quality data. While I won’t
    cover data processing and feature engineering concepts in this book, I wanted
    to highlight their importance. If you wish to learn more about these concepts,
    you can go through any ML courses and books, such as *Hands-On Machine Learning
    with Scikit-Learn, Keras, and TensorFlow* by Aurélien Géron, and *Feature Engineering
    for Machine Learning* by Alice Zheng and Amanda Casari.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset that you need for continued pre-training and fine-tuning should
    be in JSONL format. The following documentation explains what JSONL format is,
    its requirements, sample examples, and its validator: [https://jsonlines.org/](https://jsonlines.org/).'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at the data preparation techniques we can use for both methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continued pre-training expects the data to be in `{"input": "<raw_text>"}`
    format, whereas fine-tuning expects the data to be in `{"prompt": "<prompt text>",
    "completion": "<expected generated` `text>"}` format.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '`{"input": "EBITDA stands for Earnings Before Interest, Tax, Depreciation`
    `and Amortization"}`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`{"prompt": "What''s EBITDA?", "completion": "Earnings Before Interest, Tax,
    Depreciation` `and Amortization"}`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your dataset comprises images, then you can fine-tune the text-to-image or
    image-to-embedding model using Titan Image Generator as the base model. At the
    time of writing, continued pre-training only supports text-to-text models, not
    image-generation models.
  prefs: []
  type: TYPE_NORMAL
- en: 'For image data, fine-tuning expects the data to be in `{"image-ref": "s3://path/file1.png",
    "caption": "caption` `text"}` format.'
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve prepared the data, you must split it into train and validation datasets
    and store it in an Amazon S3 bucket. Once you’ve done this, you can create a custom
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a custom model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To create a custom model via the AWS console, go to **Custom models** on the
    Amazon Bedrock console page ([https://console.aws.amazon.com/bedrock/home](https://console.aws.amazon.com/bedrock/home)).
    *Figure 4**.8* shows what the **Custom models** page looks like. It provides information
    on how the customization process works, as well as two tabs called **Models**
    and **Training jobs**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – The Bedrock console – Custom models](img/B22045_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – The Bedrock console – Custom models
  prefs: []
  type: TYPE_NORMAL
- en: Under **Customize model** in the **Models** tab, you can select **Create Fine-tuning
    job** or **Create Continued Pre-training job**. When you select either of these
    options, you can view details about the job, including its status, under the **Training**
    **jobs** tab.
  prefs: []
  type: TYPE_NORMAL
- en: Components of model customization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The main components of model customization (fine-tuning or continued pre-training)
    include the source model, hyperparameters, and input data, as demonstrated in
    *Figure 4**.9*. These inputs are used to create a training job, which outputs
    the custom model alongside its metrics and logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Components of customization job](img/B22045_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Components of customization job
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s learn more about these:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Source model**: A key component of any customization job is selecting the
    source model that you wish to customize. You can find a list of all the supported
    models under the **Model details** section of the **Create Fine-tuning job** and
    **Create Continued Pre-training job** pages, as shown in *Figure 4**.10*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Selecting a model for a customization job](img/B22045_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Selecting a model for a customization job
  prefs: []
  type: TYPE_NORMAL
- en: '**Hyperparameters**: Along with the source models, you can specify a set of
    hyperparameters. These act like external knobs that control how the model is trained.
    These are different from inference parameters, which are set during the inference
    process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input data**: The dataset that is used to train the model is in JSONL format,
    and it’s prepared and stored in an Amazon S3 bucket.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training job**: The inputs (source model, hyperparameters, and input data)
    are used to create a training job. There are other configuration details, such
    as VPC settings, which you can use to securely control access to the data in an
    Amazon S3 bucket, an IAM service role, which provides access to Bedrock to write
    to an S3 bucket, and model encryption, which you can use encrypt the custom model
    at rest using a KMS key. We will cover security and privacy in Amazon Bedrock
    in [*Chapter 12*](B22045_12.xhtml#_idTextAnchor226).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom model**: Once the training process is completed, the custom model
    is stored in the AWS account owned by the AWS Bedrock Service team.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`step_wise_training_metrics.csv` and `validation_metrics.csv` files inside
    the S3 output path. We will learn how to evaluate and analyze results in the *Analyzing
    the* *results* section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For now, let’s look at the API calls we can use to create a custom model.
  prefs: []
  type: TYPE_NORMAL
- en: APIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Amazon Bedrock provides several APIs that allow you to create, monitor, and
    stop customization jobs. This section will examine some of these key APIs: **CreateModelCustomizationJob**,
    **ListModelCustomizationJob**, **GetModelCustomizationJob**, and **StopModelCustomizationJob**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s dive deeper into each of these API calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '`customizationType` to `FINE_TUNING` or `CONTINUED_PRE_TRAINING`, `baseModelIdentifier`
    as the source model you wish to use, relevant hyperparameters, and the input data
    (training and validation dataset). Here’s an example of the job being used in
    the Python SDK (Boto3):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once you run the preceding code, the training job will start.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**ListModelCustomizationJob**: You can use this API call to retrieve a list
    of all the customization jobs that you are running:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`IN_PROGRESS`, `STOPPED`, `FAILED`, or `COMPLETE`. If the model has a status
    of `FAILED`, you will *not* be charged:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Amazon Bedrock also has integration with Amazon EventBridge, where you can receive
    a notification whenever there is a status change. We will dive deeper into the
    EventBridge integration in [*Chapter 11*](B22045_11.xhtml#_idTextAnchor207).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`IN_PROGRESS`, and you would like to stop the job for any reason, you can run
    this API call:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once you start the customization job, the time it takes to complete will vary
    depending on the size of the training dataset you provide. If your dataset contains
    a few thousand records, the training job can take about an hour, while if the
    dataset contains millions of records, the training job can take a few days to
    complete.
  prefs: []
  type: TYPE_NORMAL
- en: Once the customization job has been completed and a custom model has been created,
    we can analyze the results and perform inference on our model.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned previously, when creating a customization job, we provide an output
    S3 path, where the metrics and logs are stored by the training job. You will see
    the `step_wise_training_metrics.csv` and `validation_metrics.csv` files inside
    the S3 output path. Within these files, you will see information such as the step
    number, epoch number, loss, and perplexity. You will see these details in both
    the training and validation sets. Although providing a validation set is optional,
    doing so allows the performance metrics of the custom model that’s been created
    to be evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the size of the dataset, you can decide how much of the validation
    dataset you would like to hold. If your dataset is small (for example, it contains
    hundreds or thousands of records), you can use 90% as the training set and 10%
    as the validation set. If your dataset size is large (for example, it contains
    hundreds of thousands of records), you can reduce the validation set. So, if you
    have hundreds of thousands of records, you can use 99% of them as the training
    set and 1% as the validation set.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics for training and validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two key types of metrics that provide valuable insights into how
    well the model is learning: loss and perplexity. Let’s take a closer look:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Loss**: This ranges from 0 to infinity. The loss value that’s calculated
    during training indicates how well the model fits the training data. Meanwhile,
    the validation loss shows how effectively the model generalizes to new, unseen
    examples after training is completed. Loss is one of the most commonly used metrics
    for evaluating the performance of a model during training. In general, lower loss
    values are preferable and indicate that the model is fitting the data well. Higher
    loss values suggest that the model’s prediction is far off from the actual response
    and it’s making a lot of errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Perplexity**: This ranges from 1 to infinity. It measures a language model’s
    ability to accurately predict the next token in a sequence. A lower perplexity
    score corresponds to better predictions and the model’s capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both loss and perplexity are important metrics for data scientists to analyze
    when training models with Bedrock. A well-performing training run will show the
    training and validation loss values converging over time. This convergence indicates
    that the model is learning from the training data without overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the job is successful and we’ve verified the training and validation metrics,
    we are ready to perform inference on our model. The first thing we need to do
    is purchase Provisioned Throughput, which gives us a dedicated capacity to deploy
    the model. At the time of writing, custom Bedrock models can only be deployed
    through Provisioned Throughput. However, you can also use Provisioned Throughput
    for base FMs supported by Bedrock.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, three commitment terms are available with Bedrock.
  prefs: []
  type: TYPE_NORMAL
- en: '**No commitment** (priced hourly)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1 month**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**6 months**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Model units & commitment term](img/B22045_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Model units & commitment term
  prefs: []
  type: TYPE_NORMAL
- en: 'The `1`. **Model units** are a way to define a throughput that’s measured in
    terms of the maximum number of input and output tokens processed per minute:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Provisioned Throughput](img/B22045_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – Provisioned Throughput
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve purchased Provisioned Throughput, you can see its details in the
    Bedrock console and via the **ListProvisionedModelThroughputs** and **GetProvisionedModelThroughput**
    APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Once Provisioned Throughput has an *Active* status, the custom model that you’ve
    created will be deployed to an endpoint. At this point, you can perform inference
    on the model using either the playground experience or through an API. Both options
    will be discussed next.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Bedrock playground
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Performing inference via the playground experience is pretty straightforward
    and similar to how you perform inference on base FMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of using the base model, you can select the custom model that you’ve
    created, at which point you’re ready to ask questions or provide a prompt to your
    model. *Figure 4**.13* depicts the process of selecting the **custom-titan-1705116361**
    model from the Bedrock playground, where it can be fine-tuned on user-provisioned
    training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – Select model](img/B22045_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – Select model
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Bedrock API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Bedrock also provides the `modelId`, we should provide the `arn` model of the
    provisioned endpoint. You can attain this from the **Bedrock Console – Provisioned
    Throughput** tab or via the **GetProvisionedModelThroughput** API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Now that we understand how to fine-tune models with Amazon Bedrock and leverage
    Provisioned Throughput, let’s learn how to import selective custom models.
  prefs: []
  type: TYPE_NORMAL
- en: Importing custom models in Amazon Bedrock
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To leverage the **Import Models** capability within Amazon Bedrock, navigate
    to the Bedrock console. On the left-hand side panel, under **Foundation models**,
    click **Imported models**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you land on the **Imported models** page, as shown in *Figure 4**.14*,
    you will be able to create a custom model by importing a model directly from Amazon
    SageMaker (where you might have customized FMs already) or by importing the model
    files from an Amazon S3 bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – Imported models](img/B22045_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – Imported models
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, importing a model into Amazon Bedrock creates a custom
    model that supports the following patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Continued pre-training or fine-tuned model**: As explained previously, you
    can refine the pre-trained model by utilizing proprietary data while the maintaining
    structural integrity of the original model configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Domain adaptation**: You can tailor the custom-imported model to a specific
    domain. This adaptation process will enhance the model’s performance within a
    target domain by addressing domain-specific variations. For instance, language
    adaptation can be undertaken so that responses can be generated in regional dialects
    or languages, such as Tamil or Portuguese.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pre-training from scratch**: As you are aware by now, this approach extends
    beyond merely customizing weights and vocabulary. This approach provides you with
    the opportunity to modify fundamental model parameters, including the number of
    attention heads, hidden layers, or context length. Additionally, techniques such
    as post-training quantization or integrating base and adapter weights enable further
    refinement and optimization of the model’s architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To initiate the **Import model** job, you can provide the model details, including
    a relevant model name, import job name, and model import settings.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing this book, the imported model can support the Mistral,
    Flan, Llama2, and Llama3 architectures. As the generative AI landscape evolves,
    Bedrock may expand the list of supported architectures for model import in the
    future.
  prefs: []
  type: TYPE_NORMAL
- en: Once a model import job has been completed successfully, the imported model
    will be listed on the **Models** tab of the **Imported models** page. Here, you
    can view key details about the imported model, such as its ARN, model ID, and
    status. From this page, you can also use the imported model for inference by invoking
    it through the Bedrock API.
  prefs: []
  type: TYPE_NORMAL
- en: Detailed information regarding the different model types and open source architectures
    that Amazon Bedrock’s custom model capability supports can be found at [https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html#model-customization-import-model-architecture](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html#model-customization-import-model-architecture).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Please ensure that your account has sufficient quota limits to execute the
    **CreateModelImportJob** action. If it doesn’t, the following error will be displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 – Error](img/B22045_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – Error
  prefs: []
  type: TYPE_NORMAL
- en: You can request for your quota limit to be increased by navigating to [https://us-east-1.console.aws.amazon.com/servicequotas/home/services/bedrock/quotas](https://us-east-1.console.aws.amazon.com/servicequotas/home/services/bedrock/quotas).
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, we’ve learned how to prepare a dataset, customize an
    FM, and then check its performance and perform inference. Now, let’s look at some
    of the guidelines and best practices we need to consider while trying to customize
    a model.
  prefs: []
  type: TYPE_NORMAL
- en: Guidelines and best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While customizing a model, it’s ideal to consider the following practices for
    optimal results:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Providing the dataset**: The most important thing in ML is the dataset. Most
    of the time, how your model performs depends on the dataset you provide to train
    the model. So, providing quality data that’s aligned with your use case is important.
    If you’ve studied ML in university or worked in this field, you might have learned
    about various feature engineering and data processing techniques you can use to
    clean and process the data. For example, you can handle missing values in the
    dataset, make sure you don’t provide biased data, or ensure that the dataset follows
    the format that the model expects. If you would like to learn more about providing
    quality data, please read *Feature Engineering for Machine Learning* by Alice
    Zheng and Amanda Casari. This same principle applies to generative AI since it
    is essentially a subset of ML.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Choosing the right FM**: Next, you need to select the base FM that you are
    looking to customize. Make sure you look at its attributes, how many tokens it
    supports, what type of data it’s been trained on, and the size of the model. Go
    through the model cards in the Bedrock console, read through the websites of these
    models, and look at their performance by using standardized benchmarks such as
    GLUE, SuperGLUE, HELM, and OpenLLM by HuggingFace. However, keep in mind that
    you shouldn’t completely rely on these benchmark tools as they may not represent
    the complexity and diversity of real-world applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Identifying hyperparameters**: Once you have a quality dataset and the right
    base model has been selected, you need to identify the right hyperparameters for
    customization. Your goal should be to avoid overfitting; the model should be able
    to generalize well to the new unseen information. There are several hyperparameters
    that you can adjust, such as the number of epochs, batch size, learning rate,
    early stopping, and others. You can find a list of hyperparameters that all the
    Bedrock models support at [https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models-hp.html](https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models-hp.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluating performance**: Once you’ve fine-tuned the model, evaluate its
    performance using the validation dataset. The **validation dataset** is the dataset
    that’s held back from training the model and is used for evaluating it instead.
    To learn more about data splitting, go to [https://mlu-explain.github.io/train-test-validation/](https://mlu-explain.github.io/train-test-validation/).
    Here, you can look at different metrics, such as loss and perplexity, or use techniques
    such as accuracy, **Bilingual Evaluation Understudy** (**BLEU**), and **Recall-Oriented
    Understudy for Gisting Evaluation** (**ROUGE**) scores. For context, BLEU scores
    indicate the quality assessment of machine-generated translations compared to
    reference translations set provided by human translators. The ROUGE score is useful
    for text summarization tasks, wherein evaluation is conducted based on the quality
    of machine-generated summaries compared to the respective reference summaries
    created by humans. If the model doesn’t provide the desired performance results,
    you have to readjust the hyperparameters or bring in more datasets. Once the model
    is ready to be used and provides the desired evaluation results, you can perform
    inference on the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adapting the model for specific domains**: Customizing the model to a business
    domain is a promising approach for improving productivity and efficiency. By tailoring
    the model to the specific needs of a particular industry, we can enable it to
    perform tasks that were previously impossible or inefficient and create a more
    competitive and successful business.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adopting these practices can help you make the most of customizing an FM and
    harnessing the true power of generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored two model customization techniques, fine-tuning
    and continued pre-training, the need to customize a model, and understood the
    concepts behind fine-tuning and continued pre-training. Further, we prepared our
    dataset, created a custom model, evaluated the model, and performed inference.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we discussed some of the guidelines and best practices you need to consider
    when customizing your FM.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’re going to uncover the power of RAG in solving real-world
    business problems by using an external data source. We will delve into the various
    use cases and sample architectures and implement RAG with Amazon Bedrock.
  prefs: []
  type: TYPE_NORMAL
