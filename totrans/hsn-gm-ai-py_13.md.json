["```py\nconda activate gameAI\n--- or ---\nactivate gameAI\n```", "```py\npip install gym_unity \n```", "```py\ncd /\nmkdir mlagents\ncd mlagents\n```", "```py\ngit clone https://github.com/Unity-Technologies/ml-agents.git\n```", "```py\nfrom unityagents import UnityEnvironment\n```", "```py\nenv = UnityEnvironment(file_name=\"desktop/gameAI.exe\") # Windows\n#env = UnityEnvironment(file_name=\"desktop/gameAI.app\") # Mac\n#env = UnityEnvironment(file_name=\"desktop/gameAI.x86\") # Linux x86\n#env = UnityEnvironment(file_name=\"desktop/gameAI.x86_64\") # Linux x86_64\n```", "```py\nbrain_name = env.brain_names[0]\nbrain = env.brains[brain_name]\n```", "```py\naction_size = brain.vector_action_space_size\nstate_size = brain.vector_observation_space_size\n\ncurrent_model = RainbowDQN(state_size, action_size, num_atoms, Vmin, Vmax)\ntarget_model = RainbowDQN(state_size, action_size, num_atoms, Vmin, Vmax)\n```", "```py\nenv_info = env.reset(train_mode=True)[brain_name]\nstate = env_info.vector_observations[0]\n```", "```py\nif done:\n        #state = env.reset()\n        env_info = env.reset(train_mode=True)[brain_name] \n        state = env_info.vector_observations[0]\n        all_rewards.append(episode_reward)\n        episode_reward = 0\n```", "```py\nusing UnityEngine;\nusing MLAgents;\n\npublic class BasicAgent : Agent\n```", "```py\npublic override void CollectObservations()\n{\n  AddVectorObs(m_Position, 20);\n}\n```", "```py\n public override void AgentAction(float[] vectorAction, string textAction)\n {\n  var movement = (int)vectorAction[0];\n  var direction = 0;\n  switch (movement)\n  {\n    case 1:\n      direction = -1;\n      break;\n    case 2:\n      direction = 1;\n      break;\n  }\n\n  m_Position += direction;\n  if (m_Position < m_MinPosition) { m_Position = m_MinPosition; }\n  if (m_Position > m_MaxPosition) { m_Position = m_MaxPosition; }\n\n  gameObject.transform.position = new Vector3(m_Position - 10f, 0f, 0f);\n\n  AddReward(-0.01f);\n\n  if (m_Position == m_SmallGoalPosition)\n  {\n    Done();\n    AddReward(0.1f);\n  }\n\n  if (m_Position == m_LargeGoalPosition)\n  {\n    Done();\n    AddReward(1f);\n  }\n}\n```", "```py\nAddReward(-0.01f);\n```", "```py\nDone();\nAddReward(0.1f);  //small goal\n// or\nDone();\nAddReward(1f);  //large goal\n```", "```py\n{\n    \"measure\" : \"progress\",\n    \"thresholds\" : [0.1, 0.3, 0.5],\n    \"min_lesson_length\" : 100,\n    \"signal_smoothing\" : true,\n    \"parameters\" :\n    {\n        \"big_wall_min_height\" : [0.0, 4.0, 6.0, 8.0],\n        \"big_wall_max_height\" : [4.0, 7.0, 8.0, 8.0]\n    }\n}\n```", "```py\nmlagents-learn config/trainer_config.yaml --curriculum=config/curricula/wall-jump/ --run-id=wall-jump-curriculum --train\n```", "```py\npretraining:\n        demo_path: ./demos/Tennis.demo\n        strength: 0.5\n        steps: 10000\n```", "```py\nStudentBrain:\n  trainer: imitation\n  max_steps: 10000\n  summary_freq: 1000\n  brain_to_imitate: TeacherBrain\n  batch_size: 16\n  batches_per_epoch: 5\n  num_layers: 4\n  hidden_units: 64\n  use_recurrent: false\n  sequence_length: 16\n  buffer_size: 128\n```", "```py\nmlagents-learn config/trainer_config.yaml --run-id=tennis1 --train\n```", "```py\nresampling-interval: 5000\n\nbig_wall_min_height:\n    sampler-type: \"uniform\"\n    min_value: 5\n    max_value: 8\n\nbig_wall_max_height:\n    sampler-type: \"uniform\"\n    min_value: 8\n    max_value: 10\n\nsmall_wall_height:\n    sampler-type: \"uniform\"\n    min_value: 2\n    max_value: 5\n\nno_wall_height:\n    sampler-type: \"uniform\"\n    min_value: 0\n    max_value: 3\n```", "```py\nSamplerFactory.register_sampler(*custom_sampler_string_key*, *custom_sampler_object*)\n```", "```py\nclass CustomSampler(Sampler):\n    def __init__(self, argA, argB, argC):\n        self.possible_vals = [argA, argB, argC]\n\n    def sample_all(self):\n        return np.random.choice(self.possible_vals)\n```", "```py\nheight:\n    sampler-type: \"custom-sampler\"\n    argB: 1\n    argA: 2\n    argC: 3\n```", "```py\nmlagents-learn config/trainer_config.yaml --sampler=config/walljump_generalize.yaml\n--run-id=walljump-generalization --train\n```"]