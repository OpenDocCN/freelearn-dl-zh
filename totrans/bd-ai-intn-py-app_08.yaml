- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implementing Vector Search in AI Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vector search is revolutionizing the way people interact with data in AI applications.
    MongoDB Atlas Vector Search allows developers to implement sophisticated search
    capabilities that understand the nuances of discovery and retrieval. It works
    by converting text, video, image, or audio files into numerical vector representations,
    which can then be stored and searched efficiently. MongoDB Atlas can perform similarity
    searches alongside your operational data, making it an essential tool for enhancing
    user experience in applications ranging from e-commerce to content discovery.
    With MongoDB Atlas, setting up vector search is streamlined, enabling developers
    to focus on creating dynamic, responsive, and intelligent applications.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to use the Vector Search feature of MongoDB
    Atlas to build intelligent applications. You will learn how to build **retrieval-augmented
    generation** (**RAG**) architecture systems and delve deeper into the understanding
    and development of various patterns of complex RAG architectures with MongoDB
    Atlas, unraveling the synergies that underpin their joint value and potential.
    Through real-world use cases and practical demonstrations, you will learn how
    this dynamic duo can seamlessly transform businesses across industries, driving
    efficiency, accuracy, and operational excellence.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Leverage vector search and full-text search with MongoDB Atlas, which will later
    help you build a robust retriever for RAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand the various components involved in the development of a RAG system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn about the process and steps involved in the development of simple RAG
    and advanced RAG systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter assumes that you have at least beginner-level expertise in Python
    coding. To follow along with the demos, you’ll need to set up your development
    environment by completing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Install either `python@3.9` or `python@3.11` on the operating system of your
    choice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up a Python virtual environment and activate it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will be using the following packages to develop the demo described in this
    chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`pandas`: Helps with data preprocessing and handling'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy`: Handles numerical data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`openai`: For the embedding model and invoking the LLM'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pymongo`: For the MongoDB Atlas vector store and full-text search'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s3fs`: Allows loading data directly from an S3 bucket'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`langchain_mongodb`: Enables vector search in MongoDB Atlas using a LangChain
    wrapper'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`langchain`: Used to build a RAG application'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`langchain-openai`: Enables you to interact with OpenAI chat models'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`boto3`: Enables you to interact with AWS s3 buckets'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`python-dotenv:` Enables you to load environment variables from a `.``env`
    file'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To install the mentioned packages in your Python virtual environment, run the
    following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You will also need to know how to set up and run JupyterLab or Jupyter Notebook.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Information retrieval with MongoDB Atlas Vector Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Information retrieval is a critical component of RAG systems. It enhances the
    accuracy and relevance of the generated text by sourcing information from extensive
    knowledge bases. This process allows the RAG system to produce responses that
    are not only precise but also deeply rooted in factual content, making it a powerful
    tool for various **natural language processing** (**NLP**) tasks. By effectively
    combining retrieval with generation, RAG addresses challenges related to bias
    and misinformation, contributing to the advancement of AI-related applications
    and tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of information retrieval, it’s essential to distinguish between
    *relevance* and *similarity*. While **similarity** focuses on word matching, **relevance**
    is about the interconnectedness of ideas. While a vector database query can help
    identify semantically related content, more advanced tools are needed to accurately
    retrieve relevant information.
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 5*, *Vector Databases*, you learned about MongoDB Atlas Vector Search
    and how it enhances the retrieval of relevant information by allowing the creation
    and indexing of vector embeddings, which can be generated using machine learning
    models, such as embedding models. This facilitates semantic search capabilities,
    enabling the identification of content that is contextually similar rather than
    just being keyword based. Full-text search complements this by providing robust
    text search capabilities that can handle typos, synonyms, and other variations
    in text, ensuring that searches return the most pertinent results. Together, these
    tools provide a comprehensive search solution that can discern and retrieve information
    based on both the similarity of terms and the relevance of the content.
  prefs: []
  type: TYPE_NORMAL
- en: Vector search tutorial in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the help of an example, let’s see how to load a small dataset in MongoDB
    to perform a vector search along with full-text search to perform information
    retrieval. For this demonstration, you will load a sample movie dataset from an
    S3 bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: Write a simple Python function to accept search terms or phrases and pass it
    through the embeddings API again to get a query vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the resultant query vector embeddings and perform a vector search query
    using the [`$vectorsearch`](https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/)
    operator in the MongoDB aggregation pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pre-filter the documents using meta information to narrow the search across
    your dataset, thereby speeding up the performance of the vector search results
    while retaining accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further, post-filter the retrieved documents that are semantically similar (based
    on relevancy score), if you want to demonstrate a higher degree of control over
    the semantic search behavior.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Initialize the OpenAI API key and MongoDB connection string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, load the dataset from the S3 bucket. Run the following lines of code in
    Jupyter Notebook to read data from an AWS S3 bucket directly to a `pandas` DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: On executing the preceding snippet of code, you should see the following result
    in your Jupyter Notebook cell.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B22495_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: Sample movies data view'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize and run an embedding job to embed the `sample_movies` dataset. In
    the following code example, you create a `final` field, which is a field derived
    from the `text` and `overview` fields that are already available in the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, run this `final` field against the embedding API from OpenAI, as shown
    here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should see that the `sample_movies` dataset is enriched with the OpenAI
    embeddings in the `embedding` field, as shown in *Figure 8**.2*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B22495_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: Sample movies dataset view with OpenAI embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: Next, initialize MongoDB Atlas and insert data into a MongoDB collection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that you have created the vector embeddings for your `sample_movies` dataset,
    you can initialize the MongoDB client and insert the documents into your collection
    of choice by running the following lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You have ingested the test data to build a vector search capability. Now, let’s
    proceed to build a vector search index in the following steps.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s first create vector index definitions. You can create a vector search
    index in the MongoDB Atlas Vector Search UI by following the steps explained in
    *Chapter 5*, *Vector Databases*. The vector index required for this demo tutorial
    is provided here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once the vector index definitions are added under the Vector Search index JSON
    editor in the MongoDB Atlas UI, the process for creating a vector search index
    is triggered and the vector search index is created at the specified `path` field
    mentioned in the vector index definition. Now, you are ready to perform vector
    search queries on the `sample_movies.embed_movies` collection in MongoDB Atlas
    where all the data is stored, and create vector indexes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let’s equip the vector search or the retriever API to use in your RAG framework.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can query a MongoDB vector index using `$vectorSearch`. MongoDB Atlas brings
    the flexibility of using vector search alongside search filters. Additionally,
    you can apply range, string, and numeric filters using the aggregation pipeline.
    This allows the end user to control the behavior of the semantic search response
    from the search engine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The following code example demonstrates how you can perform vector search along
    with pre-filtering on the `year` field to get movies released post `1990`. To
    have better control over the relevance of returned results, you can perform post-filtering
    on the response using the MongoDB Query API.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following code demonstrates how you can perform these steps:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Represent a raw text query as a vector embedding. There are multiple embedding
    models currently available with OpenAI, such as `text-embedding-3-small`, `text-embedding-3-large`
    with variable dimensions, and the `text-embedding-ada-002` model.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Build and perform a vector search query to MongoDB Atlas.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform pre-filtering before performing a vector search on the `year` field.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform post-filtering using the `score` field to better control the relevancy
    and accuracy of the returned results.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following code to initialize a function that can help you achieve vector
    search, pre-filter, and post-filter:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B22495_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: Sample result from running the vector search query with pre-filters'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a sample query with `year` as a pre-filter and a `score`-based post-filter
    to retain only the relevant results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: Sample result from running the vector search query with a pre-filter
    and post-filter'
  prefs: []
  type: TYPE_NORMAL
- en: With this Python method, you were able to filter on the `score` field and the
    `year` field to generate results as well as results for vector similarity. Using
    a heuristic, you were able to control the accuracy of the results to retain only
    the most relevant documents and were also able to apply a range filter query (on
    the `year` field).
  prefs: []
  type: TYPE_NORMAL
- en: Vector Search tutorial with LangChain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Utilizing **LangChain** with MongoDB Atlas Vector Search for building a semantic
    similarity retriever offers several advantages. The following example demonstrates
    how to carry out a vector similarity search using LangChain wrapper classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: Sample result vector search query using the LangChain module for
    MongoDB'
  prefs: []
  type: TYPE_NORMAL
- en: This demonstrates a more sophisticated yet simple approach that is particularly
    beneficial for developers creating RAG applications. The LangChain framework offers
    a suite of APIs and wrapper classes that can be used to integrate with various
    serverless LLM providers, such as OpenAI, and talk to MongoDB Atlas Vector Search
    to build RAG frameworks with very few lines of code. It is also easy to maintain
    and scale.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you were able to build and perform vector similarity search
    using MongoDB Atlas. You developed reusable wrapper classes and functions that
    will be useful in developing a more sophisticated application, such as a chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s delve deep into understanding what RAG architectures are and how
    to develop one using the resources that you’ve created so far.
  prefs: []
  type: TYPE_NORMAL
- en: Building RAG architecture systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the dynamic landscape of modern business, the relentless pursuit of efficiency
    and accuracy urges organizations to adopt cutting-edge technologies. Among these,
    automation stands as a cornerstone, particularly in processing and automating
    workflows. However, traditional methods suffer when they’re subjected to large
    volumes of data with intricate tasks, and human-led processes often fall short
    due to error-prone manual interventions.
  prefs: []
  type: TYPE_NORMAL
- en: This section explores the transformative landscape of automation, discussing
    the pivotal role RAG plays in revolutionizing business operations. MongoDB, known
    for its prowess in data management and flexible schemas, offers a compelling synergy
    with RAG through its vector search and full-text search capabilities. Delving
    into the architectural details of RAG, this section dissects its constituent building
    blocks, offering practical insights into constructing automated document-processing
    workflows that harness the full potential of LLMs and MongoDB Vector Search.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6: Building blocks of RAG architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go over the key components of the RAG architecture in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document loading**: Initially, documents are loaded from data storage. This
    involves text extraction, parsing, formatting, and cleaning to prepare the data
    for document splitting.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Document splitting**: The next step is to break down the documents into smaller,
    manageable segments or chunks. Strategies for splitting can vary, from fixed-size
    chunking to content-aware chunking that considers the content structure.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Text embedding**: These document chunks are then transformed into vector
    representations (embeddings) using techniques such as **OpenAIEmbeddings**, **Sentence
    e-BERT**, and **Instructor Embeddings**. This step is crucial for understanding
    the semantic content of the chunks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Vector store**: The generated vectors, each associated with unique document
    chunks, are stored in a vector store alongside the document chunks and other metadata
    extracted from the MongoDB Atlas collection. Atlas Vector Search indexes and Apache
    Lucene search can be built through the **MongoDB Atlas UI** for easy and fast
    retrieval.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Query processing**: When a user submits a query, it is also converted into
    a vector representation using the same embedding technique as mentioned in *Step
    3*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Document retrieval**: The retriever component locates and fetches document
    chunks that are semantically like the query. This retrieval process employs vector
    similarity search techniques and MongoDB Atlas using the **Hierarchical Navigable
    Small Worlds** (**HNSW)** algorithm to perform a fast nearest neighbor search
    to retrieve relevant documents without compromising the accuracy of the retrieved
    search results.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Document chunk post-filtering**: The relevant document chunks are retrieved
    from the MongoDB collection with the help of the **Unified Query API** and can
    be post-filtered easily to transform the output document chunks into the required
    format.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**LLM prompt creation**: The retrieved document chunks and the query are combined
    to create a context and prompt for the LLM.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Answer generation**: Finally, the LLM generates a response based on the prompt,
    completing the RAG process.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the context of RAG systems, there are two primary types: **simple** (**or
    naive**) **RAG** and **advanced** **RAG**. In practical scenarios, this classification
    helps address different types of personas and questions the applications are handling,
    and it’s common to encounter both simple and complex RAG queries within the same
    workflow and from the same persona. As a developer, it is important to reason
    out the functionalities that the application is expected to serve before deciding
    on the building blocks involved in the RAG architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When building your RAG architecture system, consider the following points to
    help with programming and planning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Workflow specificity**: Define the specific workflow you intend to automate
    with RAG; it may be related to **question answering** (**QA**), data augmentation,
    summarization, reasoning, or assertion. Maybe your customers frequently ask a
    specific set of three or four types of queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User experience**: Collaborate with your target user group to understand
    the types of queries they are likely to ask to identify the user group journey,
    which might be a simple single-state response or a multi-state chat flow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data sources**: First, identify the nature of your data source—whether it’s
    unstructured or structured. Next, map the locations of these data sources. Once
    you’ve done that, classify the data based on whether it serves operational or
    analytical purposes. Finally, observe the data patterns to determine whether answers
    are readily available in one location or if you’ll need to gather information
    from multiple sources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These pointers will help you determine whether you need to go for a simple RAG
    system or an advanced RAG system and also help you to determine the essential
    building blocks to consider while constructing your RAG architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s delve deeper into the building blocks of this architecture with some
    code examples to better explain the nuances. However, before you develop RAG-powered
    applications, let’s look at the fundamentals of how to process source documents
    to maximize the accuracy of the rated responses from the RAG application. The
    following strategies will come in handy while processing documents before storing
    them in a MongoDB Atlas collection.
  prefs: []
  type: TYPE_NORMAL
- en: Chunking or document-splitting strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Chunking** or **document splitting** is a critical step in handling extensive
    texts within RAG systems. When dealing with large documents, the token limits
    imposed by language models (such as **gpt-3.5-turbo**) necessitate breaking them
    into manageable chunks. However, a naive fixed-chunk-size approach can lead to
    fragmented sentences across chunks, affecting subsequent tasks such as QA.'
  prefs: []
  type: TYPE_NORMAL
- en: To address this, consider semantics when dividing documents. Most segmentation
    algorithms use chunk size and overlap principles. **Chunk size** (measured by
    characters, words, or tokens) determines segment length, while **overlaps** ensure
    continuity by sharing context between adjacent chunks. This approach preserves
    semantic context and enhances RAG system performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s delve into the intricacies of document-splitting techniques, particularly
    focusing on content-aware chunking. While fixed-size chunking with overlap is
    straightforward and computationally efficient, more sophisticated methods enhance
    the quality of text segmentation. The following are the various document-splitting
    techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recursive chunking**: This technique includes the following approaches:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchical approach**: Recursive chunking breaks down input text into smaller
    chunks iteratively. It operates hierarchically, using different separators or
    criteria at each level.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customizable structure**: By adjusting the criteria, you can achieve the
    desired chunk size or structure. Recursive chunking adapts well to varying document
    lengths.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence splitting**: Sentence splitting involves various strategies, such
    as the ones listed here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Naive splitting**: This method relies on basic punctuation marks (such as
    periods and new lines) to divide text into sentences. While simple, it might not
    handle complex sentence structures well.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**spaCy**: Another robust NLP library, spaCy, offers accurate sentence segmentation.
    It uses statistical models and linguistic rules.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Natural Language Toolkit (NLTK)**: NLTK, a powerful Python library for NLP,
    provides efficient sentence tokenization. It considers context and punctuation
    patterns.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advanced tools**: Some tools employ smaller models to predict sentence boundaries,
    ensuring precise divisions.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specialized techniques**: Specialized techniques include the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Structured content**: For documents with specific formats (e.g., Markdown,
    LaTeX), specialized techniques come into play.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intelligent division**: These methods analyze the content’s structure and
    hierarchy. They create semantically coherent chunks by understanding headings,
    lists, and other formatting cues.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, while fixed-size chunking serves as a baseline, content-aware techniques
    consider semantics, context, and formatting intricacies. Choosing the right method
    depends on your data’s unique characteristics and the requirements of your RAG
    system. While choosing the retriever for storing and retrieving these chunks,
    you may want to consider solutions such as document hierarchies and knowledge
    graphs. MongoDB Atlas has a flexible schema and a simple unified query API to
    query data from it.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s use the recursive document-splitting strategy to build a simple RAG
    application.
  prefs: []
  type: TYPE_NORMAL
- en: Simple RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A simple RAG architecture implements a naive approach where the model retrieves
    a predetermined number of documents from the knowledge base based on their similarity
    to the user’s query. These retrieved documents are then combined with the query
    and input into the language model for generation, as shown in *Figure 8**.7*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.7: Naive RAG'
  prefs: []
  type: TYPE_NORMAL
- en: To build a simple RAG application, you will use the dataset you loaded to the
    MongoDB Atlas collection in the *Information retrieval with MongoDB Vector Search*
    section of this chapter. With this application, you’ll be able perform queries
    on the available movies and create a recommender system.
  prefs: []
  type: TYPE_NORMAL
- en: LLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This example will use the OpenAI APIs and `gpt-3.5-turbo`, but there are other
    variations of LLM models made available from OpenAI, such as `gpt-4o` and `gpt-4o-mini`.
    The same prompting technique can be used with other LLMs, such as `claude-v2`
    or `mistral8x-7B`, to achieve similar results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the sample code to invoke the OpenAI LLM using LangChain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now that you have the APIs to call MongoDB Atlas Vector Search for retrieval
    and an API for invoking an LLM, you can combine these two tools to create a RAG
    system.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A prompt to an LLM is a user-provided instruction or input that guides the model’s
    response. It can be a question, a statement, or a command, and is designed to
    drive the LLM to respond with a specific output. The effectiveness of a prompt
    can greatly influence the quality of the results generated by a RAG-based system,
    making prompt engineering a crucial aspect for interacting with these models.
    Good prompts are clear, specific, and structured to communicate the user’s intent
    to the LLM, enabling it to generate the most accurate and helpful responses possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a prompt to perform QA on a private knowledge
    base:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: To demonstrate the benefits of RAG over a foundational LLM, let's first ask
    the LLM a question without vector search context and then with it included. This
    will demonstrate how you can improve the accuracy of the results and reduce hallucinations
    while utilizing a foundational LLM, such as `gpt-3.5-turbo`, that was not trained
    on a private knowledge base.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the query response without vector search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Although the LLM’s response shows it struggles with factual accuracy, there
    is still promise in using it alongside human oversight for enterprise applications.
    Together, these systems can work effectively to power applications for businesses.
    To help overcome this issue, you need to add context to the prompt through vector
    search results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how you can use the `invoke_llm` function with the `query_vector_search`
    method to provide the relevant context alongside the user query to generate a
    response with a factually correct answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, you can use the `get_recommendation_prompt` method to generate some
    movie recommendations using a simple RAG framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.8: Sample output from the simple RAG application'
  prefs: []
  type: TYPE_NORMAL
- en: The simple RAG system you just built can handle straightforward queries that
    need answers to the point. Some examples are a customer service chatbot responding
    to a basic question such as “`Where is the customer support center in Bangalore?`”
    or helping you find all the restaurants where your favorite delicacy is served
    in Koramangala. The chatbot can retrieve the contextual piece of information in
    its retrieval step and generate an answer to this question with the help of the
    LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An advanced RAG framework incorporates more complex retrieval techniques, better
    integration of retrieved information, and often, the ability to iteratively refine
    both the retrieval and generation processes. In this section, you will learn how
    to build an intelligent recommendation engine on fashion data that can identify
    the interest of the user and then generate relevant fashion product or accessory
    recommendations only when there is intent to purchase a product in the user’s
    utterance. You will be building an intelligent conversation chatbot that leverages
    the power of LangChain, MongoDB Atlas Vector Search, and OpenAI in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advanced RAG system in the current example will demonstrate the following
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: Utilize an LLM to generate multiple searchable fashion queries given a user’s
    chat utterance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classify the user’s chat utterance as to whether there is an intent to purchase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop a fusion stage that will also fetch vector similarity search results
    from multiple search queries to fuse them as a single recommendation set that
    is reranked with the help of an LLM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The flow of steps when a user queries the RAG system is depicted in *Figure
    8**.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.9: Sample advanced RAG, flowchart for query processing and recommendation'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s walk through the code to load the sample dataset and build the advanced
    RAG system with all the features that were listed at the beginning of this section.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For this example, you will utilize fashion data from a popular e-commerce company.
    The following code shows you how to load a dataset from an S3 bucket to a `pandas`
    DataFrame and then insert these documents into a MongoDB Atlas collection, `search.catalog_final_myn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.10: Sample view of the fashion dataset with OpenAI embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a vector search index
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As you can see in *Figure 8**.10*, the vector embeddings are already provided
    as part of the dataset. Therefore, the next step is to create a vector search
    index. You can create the vector search index by following the steps detailed
    in *Chapter 5*, *Vector Databases*, using the following index mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Fashion recommendations using advanced RAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You have successfully loaded the new fashion dataset into the MongoDB Atlas
    collection and also created a vector search index with all the building blocks
    in place. You can now use the following code to set up an advanced RAG system
    and build a recommender system with the features mentioned earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code carries out the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Importing the necessary modules and functions from various libraries. These
    include `JsonOutputParser` for parsing JSON output, `PromptTemplate` for creating
    prompts, `BaseModel` and `Field` for defining data models, and `MongoDBAtlasVectorSearch`
    for interacting with a MongoDB Atlas vector store. It also imports `MongoClient`
    for connecting to MongoDB, `load_dotenv` for loading environment variables, and
    `lru_cache` for caching function results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It defines three functions, each decorated with `lru_cache` to cache their results
    for efficiency. `get_openai_emb_transformers` returns an instance of `OpenAIEmbeddings`,
    which provides access to OpenAI transformer models for NLP. `get_vector_store`
    retrieves the vector store for MongoDB Atlas. `get_conversation_chain_conv` retrieves
    a conversation chain model for chat conversations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It defines three classes using Pydantic’s `BaseModel` and `Field`. These classes
    represent the status of product recommendations (`ProductRecoStatus`), a product
    (`Product`), and a set of recommendations for products and a message to the user
    (`Recommendations`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating instances of `JsonOutputParser` and `PromptTemplate` for parsing JSON
    output and creating prompts, respectively. These instances are used to create
    conversation chains in the next section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It defines two functions for retrieving the recommendation status for a product
    and retrieving product recommendations based on a given query and chat history.
    `get_product_reco_status` uses a conversation chain to determine the recommendation
    status for a product based on a given query and chat history. `get_product_recommendations`
    retrieves product recommendations based on a given query and chat history, a filter
    query, and a list of recommendation queries. It uses a vector store retriever
    to get relevant documents for each recommendation query, and then uses a conversation
    chain to generate the final recommendations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s now use these methods to create a product recommendations example. Enter
    the following code and then examine its output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the status output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: You can see from the preceding example output that the LLM is able to classify
    the product intent purchase as positive and recommend suitable queries by performing
    vector similarity search on the MongoDB Atlas collection.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the product recommendations output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.11: Sample output from the advanced RAG chatbot with recommendations
    for the user''s search intent'
  prefs: []
  type: TYPE_NORMAL
- en: 'Conversely, you can test the same methods to find a suitable place for a date
    instead of ideas for gifts or what to wear. In this case, the model will classify
    the query as having negative product purchase intent and not provide any search
    term suggestions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the status output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output from the LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.12: Sample output from the advanced RAG system when there is no purchase
    intent in the query'
  prefs: []
  type: TYPE_NORMAL
- en: Advanced RAG introduces the concept of modularity when building RAG architecture
    systems. The above example focuses on developing a user flow-based approach for
    the sample advanced RAG system. It also explores how to leverage LLMs for conditional
    decision making, recommendation generation, and re-ranking the recommendations
    retrieved from the retriever system. The goal is to enhance the user experience
    during interactions with the application.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you explored the pivotal role of vector search in enhancing
    AI-powered systems. The key takeaway is that vector search plays a vital role
    in AI applications, addressing the challenge of efficient search as unstructured
    and multimodal datasets expand. It benefits image recognition, NLP, and recommendation
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB Atlas is used to demonstrate vector search implementation using its
    flexible schema and vector indexing capabilities. You were able to build a RAG
    framework for solving QA use cases that combines retrieval and generation models,
    with a simple RAG system utilizing pre-trained language models and embedding models
    from OpenAI. You also learned how to build an advanced RAG system that employs
    iterative refinement and sophisticated retrieval algorithms with the help of LLMs
    for building a recommendation system for the fashion industry. With these insights,
    you can now build efficient AI applications for any domain or industry.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will delve into the critical aspects of evaluating
    LLM outputs in such RAG applications and explore various evaluation methods, metrics,
    and user feedback. You will also learn about the implementation of guardrails
    to ensure responsible AI deployment and how to better control the behavior of
    LLM-generated responses.
  prefs: []
  type: TYPE_NORMAL
- en: Part 3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Optimizing AI Applications: Scaling, Fine-Tuning, Troubleshooting, Monitoring,
    and Analytics'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This set of chapters shares techniques and practices for evaluating your AI
    application as well as strategies and expert insights for improving your application,
    avoiding pitfalls, and ensuring that your application continues to function optimally
    despite rapid technological changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part of the book includes the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B22495_09.xhtml#_idTextAnchor193), *LLM Output Evaluation*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B22495_10.xhtml#_idTextAnchor214), *Refining the Semantic Data
    Model to Improve Accuracy*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B22495_11.xhtml#_idTextAnchor232), *Common Failures of Generative
    AI*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B22495_12.xhtml#_idTextAnchor253), *Correcting and Optimizing
    Your Generative AI Application*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
