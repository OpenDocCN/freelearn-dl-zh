- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Acquiring and Processing Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data is the most important part of any AI task, and deepfakes are no exception.
    The quality of a swap is limited by the input data, and selecting that data is
    the most important task of a deepfake creator. While there are ways to automate
    parts of data gathering, the process is still largely manual.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will cover the importance of data, what makes quality data,
    how to get your data, and how to improve poor data. These are skills that are
    critical for a deepfake creator and must be developed. This chapter will explain
    the basics of these skills, but they must be practiced for full understanding
    and use.
  prefs: []
  type: TYPE_NORMAL
- en: Upscaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will cover the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Why data is important
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the value of variety
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sourcing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving your data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upscaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why data is important
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks work by taking data that is known and processing it in order
    to train the deepfake AI (see [*Chapter 1*](B17535_01.xhtml#_idTextAnchor015),
    *Surveying Deepfakes*, for an explanation of the whole process). We call this
    set of data, simply enough, a **dataset**. To create a dataset, the data has to
    be processed and prepared for the neural network so that it has something to train
    with. In the case of deepfakes, we use faces, which need to be detected, aligned,
    and cleaned in order to create an effective dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Without a properly formatted and prepared dataset, the neural network simply
    cannot be trained. There is another potential problem when it comes to generative
    networks like deepfakes – a poor quality dataset leads to poor swaps. Unfortunately,
    it’s hard to know at the beginning whether a dataset will produce a good swap
    or not. This is a skill that takes time to learn, and your first few deepfakes
    are unlikely to turn out well as you learn the importance of data.
  prefs: []
  type: TYPE_NORMAL
- en: Time spent cleaning and managing data is time very well spent. While the largest
    time sink in a deepfake is the time spent training, that time requires no input
    from the creator – however, if your data is flawed, that time will be entirely
    wasted. To this end, most deepfakers spend a good deal of time cleaning and testing
    their data before they commit to a long training session.
  prefs: []
  type: TYPE_NORMAL
- en: You may think that **resolution** (the number of pixels in an image) is important
    in your dataset, but it’s really not that important. There are very few high-resolution
    deepfakes available. A video may be 4k, 8k, or even higher, but the face swapped
    will be limited by the AI model, the capabilities of the computer it’s trained
    on, and the time available to spend training. 512x512 pixels is a very high resolution
    in the world of deepfakes, and higher resolutions are reserved for the most dedicated
    and skilled deepfakers.
  prefs: []
  type: TYPE_NORMAL
- en: 'That doesn’t mean you can’t get good results with lower resolutions. One clear
    way to think of the difference is that **fidelity** is not the same as resolution.
    Fidelity measures how little information is lost from its original, that is how
    realistic or lifelike an image is. Think of a small thumbnail picture of a realistic
    painting, such as the image of the Mona Lisa shown in *Figure 3**.1*: it is *highfidelity*
    but *low-resolution*. The opposite in this example would be a drawing by a 5-year-old
    (or in this case, one of the authors) of the Mona Lisa using an advanced drawing
    pad on a computer. That image is *high-resolution* but *low-fidelity*. As you
    train, your deepfake AI will increase its fidelity but the resolution will stay
    the same.'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 3.1 – “The Mona Lisa” by Leonardo DaVinci,\uFEFF a high-fidelity but\
    \ low-resolution image (UL); “A Mona Lisa” by Bryan Lyon, a high-resolution but\
    \ low-fidelity image (LR)](img/B17535_03_001.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – “The Mona Lisa” by Leonardo DaVinci, a high-fidelity but low-resolution
    image (UL); “A Mona Lisa” by Bryan Lyon, a high-resolution but low-fidelity image
    (LR)
  prefs: []
  type: TYPE_NORMAL
- en: Data is very important to your deepfakes, and variety is the most effective
    way to ensure good data.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the value of variety
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Variety is the single most defining trait of a good dataset. The best datasets
    will all have a large variety of poses, expressions, and lighting situations,
    while the worst results will come from data lacking variety in one or more of
    these categories. Some of the areas of variety we’ll cover in this section include
    pose, expression, and lighting.
  prefs: []
  type: TYPE_NORMAL
- en: Pose
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pose is a simple category to both see and understand. Pose is simply the direction
    and placement of the face in the image. When it comes to deepfakes, only the pose
    of the face itself matters – the rest of the body’s pose is ignored. Pose in deepfakes
    is important so that the AI can learn all the angles and directions of the face.
    Without sufficient pose data, the AI will struggle to match the direction of the
    face and you’ll end up with poor results.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Examples of different poses](img/B17535_03_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Examples of different poses
  prefs: []
  type: TYPE_NORMAL
- en: One quick way to get the full range of poses is to simply move the head while
    being filmed. By looking around up, down, and from side to side (along with mixtures
    of these), you give the AI all the directions that it needs. Don’t forget to get
    distant shots with a high **focal length** to zoom into the face, as well as close-ups
    with a neutral focal length. A high focal length “zoom” on a distant face causes
    the “flattening” (the effect is properly called **lens compression**) of the depth
    of the face and is often used for dramatic effect in movies, so you need to match
    the variance in your data as well. Even something such as lens distortion matters
    and can affect the image enough that getting a variety of examples can help get
    quality data.
  prefs: []
  type: TYPE_NORMAL
- en: Expression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Expression is another aspect that is easy to understand – it’s the shape the
    face makes during training. Unfortunately, it’s very hard to ensure that you have
    met the full range without missing important data. Think about how expressive
    a face may be – happy, sad, angry, awake, surprised, and so much more – and think
    about how easy it may be to miss an important subset of those expressions in your
    data.
  prefs: []
  type: TYPE_NORMAL
- en: There is no easy way to ensure that you have all the variety that you need,
    but the more the better. You don’t want to just sit someone down and film their
    face for an hour, as you are unlikely to get all the expressions you really want.
    To really get a good variety of expressions, you need to make sure that you get
    data from different days, in different environments, and with different emotional
    stimuli. An actor may give you drastically different “sad” expressions in different
    contexts or times, and that variety is important for the AI to really understand
    how to recreate those same expressions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Examples of different expressions](img/B17535_03_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – Examples of different expressions
  prefs: []
  type: TYPE_NORMAL
- en: The key here is to get as many different expressions as you can. You can’t get
    all your data at once; if you do, your data will be extremely limited. If you
    have an excellent actor as your target face, you might be able to get away with
    having them match the expressions that you’re swapping to, but you’ll probably
    want to get a much more varied dataset to ensure you’re getting the right data
    in the right contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Lighting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lighting may sound easy and/or trivial but ask any photographer or videographer
    about it and they’ll be able to talk forever. Lighting a photo or video shoot
    is complicated even when you’re not dealing with AI, but when deepfakes are involved,
    it becomes an enormous issue. It’s simply impossible to get a complete variety
    of lighting, but there are some keys to consider.
  prefs: []
  type: TYPE_NORMAL
- en: “Good” lighting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most Hollywood films are examples of expert light management with entire teams
    dedicated to even the simplest setups. However, there are a few good ground rules
    for basic lighting for deepfakes.
  prefs: []
  type: TYPE_NORMAL
- en: Clear lighting cannot come from a single direction. This leads to deep shadows
    in the eyes, next to the nose, and often on one whole side of the face. Even direct
    face-first lighting will lead to shadows at the edges (not to mention probably
    lead to expression changes as the actor squints). Instead, you need to have good
    **ambient**-style lighting – that is, lighting coming evenly from all directions.
    Offices with large fluorescent lights will probably be a good example of this
    lighting (although the color of the light will leave something to be desired).
  prefs: []
  type: TYPE_NORMAL
- en: Shadows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: However, ambient lighting is not the only style of lighting that deepfakes require.
    You need some shadow in the dataset or the AI will never learn how to recreate
    anything but a fully lit face. For this lighting, you can use a normal overhead
    light or lamp. This will enable the AI to pick up some of those deeper shadows
    that really help to bring a face’s depth into perspective.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to get shadow variety is in scenes where the lighting or subject
    is moving, giving you lots of varied shadows that the AI can learn.
  prefs: []
  type: TYPE_NORMAL
- en: Outdoors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, you are going to want to get some outdoor images. The Sun is particularly
    unique among all other light sources. There is nothing that quite matches the
    natural effect of a burning ball of plasma millions of miles away from the subject
    – and believe it or not, the distance really does matter, as it leads to basically
    parallel light rays. Because of this, no matter how much “in-studio” filming you
    do, it’s very important to get at least some of your data outdoors. The Sun’s
    light is truly impossible to reproduce, and getting data that was filmed outdoors
    is indispensable to fill out the lighting data for the AI.
  prefs: []
  type: TYPE_NORMAL
- en: Bringing this variety together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s important to think about all the aspects of variety separately, but you
    actually also need to consider them together as well. You should get data that
    mix and match all three aspects in order to ensure that you get the best results.
    For example, all your different lighting scenarios should have a variety of expressions
    and poses. Only with varied data can the AI learn to create a truly convincing
    deepfake.
  prefs: []
  type: TYPE_NORMAL
- en: Having good variety is critical, but how do you actually get the data you need?
  prefs: []
  type: TYPE_NORMAL
- en: Sourcing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’re working on a deepfake, then you probably already know who you’re going
    to be swapping. Hopefully, you’re lucky and are working on a deepfake that covers
    two people who you have access to so you can film them or gather data from them
    without too much trouble. Unfortunately, not everyone is so lucky, and most of
    the time, one or both of your subjects will be unavailable for custom data (this
    is probably why you’re working on a deepfake in the first place, after all).
  prefs: []
  type: TYPE_NORMAL
- en: These two situations require very different approaches to getting data, and
    sometimes, even if you have good access to your subjects, you will need to get
    some of your data from another source.
  prefs: []
  type: TYPE_NORMAL
- en: Filming your own data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Filming your own sources is a dream position for any deepfaker. The ability
    to build the perfect dataset by putting an actor in front of a camera is liberating,
    but also a wasted opportunity if you don’t know how to capture all that you need.
  prefs: []
  type: TYPE_NORMAL
- en: Author’s note
  prefs: []
  type: TYPE_NORMAL
- en: This book’s scope is unable to even begin to approach the extensive vocabulary
    and techniques that photography and filming involve. If you’re planning on doing
    much actual filming of your own data, it is highly recommended that you read some
    photography or filmography books, watch some tutorials, and talk to experts in
    that field before you get on set so that you can get the most out of the time.
  prefs: []
  type: TYPE_NORMAL
- en: Telling the **gaffer** (the person in charge of lighting on a set) to go against
    all their training and deliberately give you bad lighting can be scary if you’re
    not an expert. Telling an actor to make a “sadder” face or to make extreme expressions
    can be equally awkward. When filming for a deepfake, you need to make it clear
    to the crew that you do need their regular skills, but that you also need to go
    beyond the norm into the weird, esoteric, and uncomfortable.
  prefs: []
  type: TYPE_NORMAL
- en: While you may be tempted to ensure that the data is always done in close with
    the actor’s face filling the frame, don’t be afraid to let the face be smaller
    or farther from the camera; it’s rarely the resolution of the face in the data
    that hurts the results.
  prefs: []
  type: TYPE_NORMAL
- en: You need to consider all aspects of a deepfake when you’re filming. Even when
    filming the video to be swapped, it is important to consider the limitations of
    the deepfake technology. Angled faces, such as profiles, are especially difficult
    to swap without an uncanny effect. Because of this, you will want to minimize
    the number of faces looking far from the axis of the camera. They don’t have to
    be directly facing the camera, especially if you get a lot of angles in the training
    data, but there comes a a point when things things are just too angular for the
    AI to provide a good result.
  prefs: []
  type: TYPE_NORMAL
- en: One important detail is that your deepfakes will improve with practice as you
    develop your skills. For this reason, it is highly recommended that you not rely
    on your first deepfake to be perfect, and if you have an important project coming
    up that requires deepfakes, you may want to create a couple of practice deepfakes
    first so that you can get the necessary practice with the data to ensure that
    while filming your important deepfake you have the experience necessary to get
    quality data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting data from historical sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, you cannot film your subjects – for example, if the actor has passed
    away or you’re deepfaking a younger version of an actor. In these cases, you don’t
    have the luxury of making your own data to meet your situation. That means you
    must rely on older sources for all your training data.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, your subject is recent, and videos and photos of them exist. Some
    subjects cannot be deepfaked due to the sheer lack of original content around
    them – Albert Einstein, for example, will never have a traditional deepfake made
    simply because there isn’t the necessary training data. Your best bet at that
    point is to use an impersonator to some degree. This is the technique that the
    Dalí Museum used to bring Dalí back – impersonators and some limited video interviews
    of the artist.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re lucky enough that data exists for your subject, you can use any data
    available, and you do not need to restrict yourself based on age or resolution
    (see the discussion about the difference between resolution and fidelity in the
    *Why data is important* section). Instead, focus more on getting variety. It can
    be a challenge knowing where to stop but, generally, time spent gathering and
    managing your data is well rewarded. It’s unlikely that time spent on data will
    be wasted in the long run.
  prefs: []
  type: TYPE_NORMAL
- en: Movies, interviews, and even random photos of people can be used for deepfake
    training data. You don’t want to rely too much on any one of these, as they are
    all very limited in domain. A horror movie is unlikely to have many smiles and
    random photos that you can find will be unlikely to have anything besides smiles.
    Finding the balance between the different categories in order to find the variety
    necessary to train is one of the skills that you will simply have to learn.
  prefs: []
  type: TYPE_NORMAL
- en: Especially when dealing with historical data, you’ll often find that it is not
    as high-quality as you want, so how can you get the most out of your limited data?
  prefs: []
  type: TYPE_NORMAL
- en: Improving your data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are no silver bullets to magically make your data better, but there are
    some ways that you can tweak your data to improve the training of the AI.
  prefs: []
  type: TYPE_NORMAL
- en: Linear color
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you’re filming you may film in **logarithmic scale** (**log**) color, where
    the scale represents an exponential change. This is great for storing a large
    color range while filming but does not work well for training a deepfake. To get
    the best results from your training, you’ll want to convert your video into a
    **linear** color space (where a change of some number is consistently represented).
    It doesn’t really matter which one, but all your data and converted videos should
    be the same. Since most content is **Rec.709**, we recommend that you use that
    unless you have a good reason to pick a different color space.
  prefs: []
  type: TYPE_NORMAL
- en: Author’s note
  prefs: []
  type: TYPE_NORMAL
- en: Color science is a very robust field, and a full examination is outside the
    scope of this book. However, a basic understanding can help. **Rec.709** is one
    of many so-called **color spaces**, which are how a computer or TV represents
    colors. There are many different color spaces, including **SRGB** and **Adobe
    RGB** (among others), which can show different colors or represent them in different
    ways. The important thing when it comes to deepfakes is that you must keep all
    your data in the same color space when training and converting.
  prefs: []
  type: TYPE_NORMAL
- en: '**High-dynamic range** (**HDR**) content is also problematic – at least when
    it comes to commercially-released content. Consumer HDR technologies throw out
    a lot of data to fit the color data into the frame. This means that something
    that looks dark in one scene could actually be stored as brighter than something
    that looks bright in another scene. Even when mapped back to a linear range, it
    tends to be highly variable, which causes great difficulty for AI to generalize,
    often leading to the failure of the deepfake model. The best way to deal with
    this is to avoid giving HDR content to the AI model. This can be complicated since
    sometimes the highest quality data you can get is HDR. In that situation, it’s
    useful to know that data of a lower resolution but a consistent color space will
    lead to better results than a higher resolution but inconsistent color.'
  prefs: []
  type: TYPE_NORMAL
- en: Professional cameras recording in RAW will not have the same problem as **HDR**
    since they keep the exact data coming from the sensor and the complete color data
    remains available.
  prefs: []
  type: TYPE_NORMAL
- en: Color science is complicated and sometimes non-intuitive. It’s not necessary
    to become an expert in all the details, but it’s useful to know when the color
    space might be causing a problem.
  prefs: []
  type: TYPE_NORMAL
- en: Data matching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, you’re extremely limited with the data that you have access to for
    one of your subjects. In that case, it can be very hard to get a good swap. One
    way to help get the results you need is to match your data as close as possible
    for at least part of your training. Have the subject that you have access to match
    the data for the other subject as closely as possible. It’s important to match
    the poses, expressions, and lighting all in a single image. This doesn’t have
    to be for your final conversion and shouldn’t be the only data you use, but having
    closely matching data can sometimes help the AI find the details to swap the two.
  prefs: []
  type: TYPE_NORMAL
- en: When doing data matching, you’ll want to train for a very short time on just
    the subset of your data which matches. Both your subject datasets should be cleaned
    to a single short segment and training should be limited to no more than 100 **epochs**
    (an epoch is one full cycle of training the AI model on all the data). Once you’re
    done training with that subset, you’ll want to train with the full data. You may
    want to repeat this process a couple of times with different subsets of your data.
  prefs: []
  type: TYPE_NORMAL
- en: Upscaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One major current trend in AI is in **upscaling** content. This is a great use
    for AI, which can fill in missing data from its training, especially **temporally
    aware upscalers**, which can find missing data by tracking an object across multiple
    frames to get more detail. Unfortunately, when used as training data for generative
    AI such as deepfakes, the AI upscaled data is problematic and prone to training
    failures. Even a very good upscaling AI has glitches and artifacts. The artifacts
    might be difficult for the eye to see, but the deepfake AI searches for patterns
    and will often get tripped up by artifacts, causing the training to fail.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, the best way to deal with upscaling is to not upscale your training
    data but instead to upscale the output. This is even better in many ways since
    it can replace missing face data and improve the resolution of the output at the
    same time. The reason that chaining AIs in that direction doesn’t cause failures
    is that, unlike deepfakes, upscalers are not trained on the data generated.
  prefs: []
  type: TYPE_NORMAL
- en: That said, there are a lot of techniques that are completely safe for upscaling
    your training data. More traditional or temporal solutions that don’t involve
    AI can be used to upscale without leading to failure to train. Upscaling with
    **bicubic** or **Lanczos** filtering is also perfectly acceptable.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning up noise or adjusting colors is also fine and a valid way to maximize
    your data quality, especially if the extract was unable to find faces in some
    of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data is critical for deepfakes, as with all AI. Getting the best data is a skill
    that you have to learn over time. It’s something that you will get better at as
    you become more experienced with deepfakes. That being said, some tasks can be
    learned without heavy investment in the process. Cleaning and organizing your
    data is important – time spent on this can save you time later since your AIs
    will be less likely to fail.
  prefs: []
  type: TYPE_NORMAL
- en: Filming your own data is sometimes necessary and can get you the best results,
    as this will give you enough control to fill in missing data or match limited
    historical data. When you have nothing but historical data, you’re more limited
    and may need to do further work to improve the data you have. Upscaling and filtering
    are possible, but you must be careful, as some techniques can add artifacts that
    interfere with training.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, data is the most important part of training a deepfake and therefore
    is the most important job of a deepfaker. You must spend time and effort learning
    the skill of data management if you are going to excel at creating deepfakes.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be walking you through using Faceswap – a freely
    available open source deepfake software – so that you can generate your own deepfakes.
  prefs: []
  type: TYPE_NORMAL
- en: EBSCOhost - printed on 11/27/2023 6:20 AM via . All use subject to [https://www.ebsco.com/terms-of-use](https://www.ebsco.com/terms-of-use)
  prefs: []
  type: TYPE_NORMAL
