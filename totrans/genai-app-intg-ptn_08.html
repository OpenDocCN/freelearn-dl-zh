<html><head></head><body>
<div><div><h1 class="chapterNumber">8</h1>
<h1 class="chapterTitle" id="_idParaDest-87">Integration Pattern: Real-Time Retrieval Augmented Generation</h1>
<p class="normal">In this chapter, we’ll explore another integration pattern that combines the power of <strong class="keyWord">Retrieval Augmented Generation</strong> (<strong class="keyWord">RAG</strong>) and <a id="_idIndexMarker269"/>generative AI models to build a chatbot capable of answering questions based on the content of PDF files. This approach combines the strengths of both retrieval systems and generative models, allowing us to leverage existing knowledge sources while generating relevant and contextual responses. </p>
<p class="normal">One of the key advantages of the RAG approach is its ability to prevent hallucinations and provide better context for the generated responses. Generative AI models, trained on broad data, can sometimes produce responses that are factually incorrect or outdated due to their training data being limited to up to a point in time or they might lack proper context at inference time. By grounding the model’s generation process in relevant information retrieved from a document corpus, the RAG approach mitigates the risk of hallucinations and ensures that the responses are accurate and contextually relevant.</p>
<p class="normal">For example, the term <em class="italic">refund</em> can have different meanings and implications in different contexts. A refund in the context of retail banking may refer to a customer requesting a refund for a fee or charge, while a refund in the context of taxation may refer to a tax refund from the government. By retrieving the relevant context from the document corpus, the RAG-powered chatbot can generate responses that accurately reflect the intended meaning of <em class="italic">refund</em> based on the specific context of the user’s query.</p>
<p class="normal">The following image illustrates a simple RAG pipeline:</p>
<figure class="mediaobject"><img alt="" height="498" src="img/B22175_08_01.png" width="825"/></figure>
<p class="packt_figref">Figure 8.1: A simple RAG pipeline</p>
<p class="normal">Continuing our examples regarding financial services, these companies often deal with a vast amount of documentation, including legal contracts, regulatory filings, product disclosures, and internal policies and procedures. These document repositories can easily run into the tens of thousands or even hundreds of thousands of pages, making it challenging for employees and customers to quickly find relevant information when needed.</p>
<p class="normal">By implementing a <a id="_idIndexMarker270"/>RAG-based chatbot system, financial services companies can provide a user-friendly interface for employees, customers, and other stakeholders to ask natural language questions and receive concise, relevant answers derived from the vast collection of documents. The RAG approach allows the system to efficiently retrieve relevant information from the document corpus and then generate contextualized responses using a powerful generative AI model.</p>
<p class="normal">For example, a customer service representative could ask the chatbot a question about a specific clause in a loan agreement, and the system would retrieve the relevant section from the document corpus and generate a concise explanation tailored to the user’s query. Similarly, an investment advisor could ask about specific regulations or guidelines related to a financial product, and the chatbot would provide the necessary information from the relevant documents.</p>
<p class="normal">By leveraging the RAG approach, financial services companies can greatly improve the accessibility and usability of their document repositories, enabling faster and more accurate information retrieval and reducing the time and effort required to manually search through thousands of pages of documentation.</p>
<p class="normal">In this chapter, we are going to cover:</p>
<ul>
<li class="bulletList">Use case definition (for a financial services company)</li>
<li class="bulletList">Architecture (overview of <a id="_idIndexMarker271"/>a RAG-based chatbot system):<ul>
<li class="bulletList level-2">Ingestion layer</li>
<li class="bulletList level-2">Document corpus management</li>
<li class="bulletList level-2">AI processing layer</li>
<li class="bulletList level-2">Monitoring and logging</li>
</ul>
</li>
<li class="bulletList">Entry point (a design for handling various input modalities)</li>
<li class="bulletList">Prompt pre-processing and vector database integration</li>
<li class="bulletList">Inference process using Vertex AI’s Gemini 1.5 Flash model</li>
<li class="bulletList">Result post-processing and presentation using Markdown</li>
<li class="bulletList">A demo implementation (using Gradio)</li>
<li class="bulletList">The full code example of the RAG pipeline</li>
</ul>
<h1 class="heading-1" id="_idParaDest-88">Use case definition</h1>
<p class="normal">Let’s consider a scenario where we’re working with a large financial institution that deals with a vast number of legal contracts, regulatory filings, product disclosures, and internal policies and procedures. These documents individually can be into the tens or even hundreds of pages, making it challenging for employees, customers, and other stakeholders to quickly find relevant information when needed. These documents also do not have a consistent format in the way the information is reported, disqualifying non-AI-powered text extractor solutions like regex statements or plain business rules.</p>
<p class="normal">The institution wants to implement a chatbot system that can provide a user-friendly interface for users to ask natural language questions and receive concise, relevant answers derived from the organization’s document corpus. This system should leverage the power of RAG to ensure that the generated responses are accurate, contextual, and grounded in the relevant information from the document corpus.</p>
<h1 class="heading-1" id="_idParaDest-89">Architecture</h1>
<p class="normal">To build our RAG-based chatbot system, we’ll <a id="_idIndexMarker272"/>leverage a serverless, event-driven architecture built on Google Cloud. This approach aligns with the cloud-native principles we have used in previous examples and allows for seamless integration with other cloud services. You can dive deep into a Google Cloud example in this sample architecture: <a href="https://cloud.google.com/architecture/rag-capable-gen-ai-app-using-vertex-ai">https://cloud.google.com/architecture/rag-capable-gen-ai-app-using-vertex-ai</a>.</p>
<p class="normal">For the purpose of this example, the architecture consists of the following key components:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Ingestion layer</strong>: This <a id="_idIndexMarker273"/>layer is responsible for accepting incoming user queries from various channels, such as web forms, chat interfaces, or API endpoints. We’ll use Google Cloud Functions as the entry point for our system, which can be triggered by events from services like Cloud Storage, Pub/Sub, or Cloud Run.</li>
<li class="bulletList"><strong class="keyWord">Document corpus management</strong>: In this<a id="_idIndexMarker274"/> layer, we’ll store embeddings representing the content of the documents. In this case, we can use a wide range of solutions from purpose-built vector databases such as Chroma DB, Pinecone, or Weaviate, to well-known industry standards such as Elastic, MongoDB, Redis, or even databases known for other capabilities such as PostgreSQL, SingleStore, Google AlloyDB, or Google BigQuery.</li>
<li class="bulletList"><strong class="keyWord">AI processing layer</strong>: In this<a id="_idIndexMarker275"/> layer, we’ll integrate Google Gemini through Vertex AI. Once the results are retrieved from the vector database, they will be exposed to Google Gemini as context along with the prompt. This process can be handled by a Cloud function.</li>
<li class="bulletList"><strong class="keyWord">Monitoring and logging</strong>: To ensure the reliability and performance of our system, you should <a id="_idIndexMarker276"/>implement robust monitoring and logging mechanisms. We’ll leverage services like Cloud Logging, Cloud Monitoring, and Cloud operations to gain visibility into our system’s behavior and quickly identify and resolve any issues.</li>
</ul>
<h1 class="heading-1" id="_idParaDest-90">Entry point</h1>
<p class="normal">The entry point<a id="_idIndexMarker277"/> for a RAG-based chatbot system is designed to be user-friendly, allowing users to submit their natural language queries through various interfaces, such as web forms, chat applications, or API endpoints. However, the entry point should not be limited to accepting text-based inputs only; it should also handle different modalities, such as audio files or images, depending on the capabilities of the underlying language model.</p>
<p class="normal">In the case of models like Google Gemini (which support multimodal inputs), the entry point can directly accept and process text, audio, images, or even videos. This versatility enables users to interact with the chatbot system in a more natural and intuitive manner, aligning with the way humans communicate in real-world scenarios.</p>
<p class="normal">In cases where the language model does not natively support multimodal inputs, the entry point can still accommodate various input modalities by pre-processing the data and extracting the textual content. This approach ensures that the chatbot system remains accessible and user-friendly, catering to a diverse range of input formats while leveraging the capabilities of the underlying language model.</p>
<p class="normal">For text inputs, the entry point can simply pass the query directly to the subsequent phases of the RAG pipeline. However, when dealing with audio or image inputs, the entry point needs to perform additional processing to extract the textual content from these modalities.</p>
<p class="normal">For audio inputs, the entry point can leverage speech recognition technologies, such as Google Chirp, Amazon Transcribe, OpenAI Whisper, or open-source libraries like CMU Sphinx, to transcribe the audio data into text format. This process involves converting the audio signals into a sequence of words or phrases that can be understood by the language model.</p>
<p class="normal">Similarly, for image<a id="_idIndexMarker278"/> inputs, the entry point can employ <strong class="keyWord">optical character recognition</strong> (<strong class="keyWord">OCR</strong>) techniques to extract text from the provided images. This can be achieved by integrating with services like Google Cloud Vision API, Amazon Textract, or<a id="_idIndexMarker279"/> open-source tools like Tesseract OCR. These technologies leverage computer vision and machine learning algorithms to accurately identify and extract textual content from images, enabling the chatbot system to understand and process information presented in visual form.</p>
<p class="normal">In this example, we will leverage text; the Python code will look like this:</p>
<pre class="programlisting code"><code class="hljs-code">#In this case we will simulate the input from a chat interface
question = "What is this call about?"
</code></pre>
<p class="normal">Regardless of the input type, the entry point should be designed to handle a wide range of scenarios and input formats. It may need to perform additional pre-processing steps, such as noise removal, format conversion, or data cleaning, to ensure that the input data is in a suitable format for the subsequent phases of the RAG pipeline. It is also best practice to run the raw request through a rigorous security monitoring pipeline to prevent data leakage or model intoxication such as prompt ingestion.</p>
<p class="normal">The following website presents a very interesting point of view regarding the challenges posed by prompt injection in multimodal scenarios: <a href="https://protectai.com/blog/hiding-in-plain-sight-prompt">https://protectai.com/blog/hiding-in-plain-sight-prompt</a>.</p>
<h2 class="heading-2" id="_idParaDest-91">Prompt pre-processing</h2>
<p class="normal">For our<a id="_idIndexMarker280"/> example, we will need to construct our prompt in real time along with its necessary context and instructions. In this step of our RAG pipeline, we will utilize a vector database for efficient vector similarity search. Vector databases are specialized data stores designed to store and retrieve high-dimensional vectors, enabling fast and accurate similarity searches. Although there are numerous vector database providers available, we will use Chroma DB for this specific example.</p>
<p class="normal">The retrieval process in a RAG pipeline is relatively straightforward. First, we generate embeddings from the user’s query using a pretrained language model or embedding technique. These embeddings are numerical representations of the query that capture its semantic meaning. Next, we perform a similarity search on the vector database using the generated query embeddings. The vector database will return vectors that are most similar to the query embeddings, along with their corresponding textual information or context. These vectors are associated with previously ingested text passages.</p>
<p class="normal">Different filtering strategies can be applied to refine the search results further. The specific parameters and techniques available may vary depending on the vector database provider being used. For instance, some vector databases provide a similarity score that measures the closeness between the query embeddings and the retrieved vectors. This score can be leveraged to identify and filter out vectors that fall below a certain similarity threshold.</p>
<p class="normal">Another common filtering strategy is to limit the number of results obtained from the vector database. This approach can be particularly useful when there are constraints on the maximum number of tokens that can be passed to the language model, either due to token limits imposed by the model or for cost optimization purposes. By limiting the number of results, we can control the amount of context information provided to the language model, ensuring efficient processing and cost-effective operation.</p>
<p class="normal">Once the results <a id="_idIndexMarker281"/>are filtered and the relevant textual information is obtained, it is used to construct the prompt that will be passed to the language model. In this example, we use the following prompt template:</p>
<pre class="programlisting code"><code class="hljs-code">prompt_template = """
You are a helpful assistant for an online financial services company that allows users to check their balances, invest in certificates of deposit (CDs), and perform other financial transactions.
Your task is to answer questions from your customers, in order to do so follow these rules:
1. Carefully analyze the question you received.
2. Carefully analyze the context provided.
3. Answer the question using ONLY the information provided in the context, NEVER make up information
4. Always think step by step.
{context}
User question: {query}
Answer:
"""
</code></pre>
<p class="normal">In our example, the <a id="_idIndexMarker282"/>prompt we are submitting to the LLM would look like:</p>
<pre class="programlisting code"><code class="hljs-code">You are a helpful assistant for an online financial services company that allows users to check their balances, invest in certificates of deposit (CDs), and perform other financial transactions.
Your task is to answer questions from your customers, in order to do so follow these rules:
1. Carefully analyze the question you received.
2. Carefully analyze the context provided.
3. Answer the question using ONLY the information provided in the context, NEVER make up information
4. Always think step by step.
&lt;context&gt;
---This information is contained in a document called coca_cola_earnings_call_2023.pdf 
 1-877-FACTSET www.callstreet.com
18 Copyright © 2001-2024 FactSet CallStreet, LLC
The Coca-Cola Co. (KO) Q1 2024 Earnings Call
Corrected Transcript 30-Apr-2024
Operator: Ladies and gentlemen, this concludes today's conference call. Thank you for participating. You may now disconnect.
Disclaimer The information herein is based on sources we believe to be reliable but is not guaranteed by us and does not purport to be a complete or error-free statement or summary of the available data. As such, we do not warrant, endorse or guarantee the completeness, accuracy, integrity, or timeliness of the information. You must evaluate, and bear all risks associated with, the use of any information provided hereunder, including any reliance on the accuracy, completeness, safety or usefulness of such information. This information is not intended to be used as the prim ary basis of investment decisions. It should not be construed as advice designed to meet the particular investment needs of any investor. This report is published solely for information purposes, and is not to be construed as financial or other advice or as an offer to sell or the solicitation of an offer to buy any security in any state where such an offer or solicitation would be illegal. Any information expressed herein on this date is subject to change without notice. Any opinions or assertions contained in this information do not represent the opinions or beliefs of FactSet CallStreet, LLC. FactSet CallStreet, LLC, or one or more of its employees, including the writer of this report, may have a position in any of the securities discussed herein.
---
---This information is contained in a document called coca_cola_earnings_call_2023.pdf 
 All participants will be on listen-only mode until the formal question-and-answer portion of the call. I would like to remind everyone that the purpose of this conference is to talk with investors and, therefore, questions from the media will not be addressed. Media participants should contact Coca-Cola's Media Relations department if they have any questions.
. . . 
---
&lt;/context&gt;
User question: What is this call about?
Answer:
</code></pre>
<h1 class="heading-1" id="_idParaDest-92">Inference</h1>
<pre>generate()</code> function is responsible for sending the prompt to the Gemini 1.5 Flash model and obtaining the generated response:</pre>
<pre class="programlisting code"><code class="hljs-code">#This is the section where we submit the full prompt and 
#context to the LLM
result = generate(prompt)
</code></pre>
<p class="normal">The <code class="inlineCode">generate()</code> function <a id="_idIndexMarker284"/>encapsulates the configuration and settings required for the generation process. It includes two main components: <code class="inlineCode">generation_config</code> and <code class="inlineCode">safety_settings</code>.</p>
<p class="normal">The <code class="inlineCode">generation_config</code> dictionary specifies the parameters that control the behavior of the language model during the generation process. In this example, the following settings are provided:</p>
<pre class="programlisting code"><code class="hljs-code">generation_config = {
   "max_output_tokens": 8192,
   "temperature": 0,
   "top_p": 0.95,
}
</code></pre>
<p class="normal">From Google Gemini’s documentation:</p>
<ul>
<li class="bulletList"><code class="inlineCode">max_output_tokens</code>: Maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60–80 words. </li>
<li class="bulletList"><code class="inlineCode">temperature</code>: The temperature is used for sampling during response generation, which occurs when <code class="inlineCode">top_p</code> and <code class="inlineCode">top_k</code> are applied. <code class="inlineCode">temperature</code> controls the degree of randomness in token selection. Lower temperature values are good for prompts that require a less open-ended or creative response, while higher temperature values can lead to more diverse or creative results. A temperature of 0 means that the highest probability tokens are always selected. In this case, responses for a given prompt are mostly deterministic, but a small amount of variation is still possible.</li>
</ul>
<p class="normal-one">A temperature of 0 means the model will choose the most likely token based on its training data, while higher values introduce more randomness and diversity in the output.</p>
<ul>
<li class="bulletList"><code class="inlineCode">top_p</code>: This parameter changes how the model selects tokens for output. Tokens are selected from the most to least probable until the sum of their probabilities equals the top-p value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1, respectively, and the top-p value is 0.5, then the model will select either A or B as the next token by using temperature and will exclude C as a candidate. </li>
</ul>
<p class="normal-one">In this case, it is set to 0.95, meaning that only the top 95% of tokens with the highest probabilities will be considered during generation.</p>
<p class="normal">Beyond the above, the <code class="inlineCode">safety_settings</code> dictionary specifies the harm categories and corresponding thresholds for filtering potentially harmful or inappropriate content from the generated output. In this example, the following settings are provided:</p>
<pre class="programlisting code"><code class="hljs-code">safety_settings = {
   generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
   generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
   generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
   generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
}
</code></pre>
<p class="normal">These settings<a id="_idIndexMarker285"/> instruct the Gemini 1.5 Flash model to block only highly harmful content related to hate speech, dangerous content, sexually explicit content, and harassment. Any content that falls below the “high” harm threshold for these categories will be allowed in the generated output.</p>
<p class="normal">The <code class="inlineCode">generate()</code> function creates an instance of the <code class="inlineCode">GenerativeModel</code> class, passing the <code class="inlineCode">MODEL</code> parameter; in this example, Gemini 1.5 Flash. It then calls the <code class="inlineCode">generate_content()</code> method on the model instance, providing the prompt, generation configuration, and safety settings. The <code class="inlineCode">stream=False</code> parameter indicates that the generation should happen in a non-streaming mode, meaning the entire response will be generated and returned at once:</p>
<pre class="programlisting code"><code class="hljs-code">def generate(prompt):
 model = GenerativeModel(MODEL)
 responses = model.generate_content(
     [prompt],
     generation_config=generation_config,
     safety_settings=safety_settings,
     stream=False,
 )
 return(responses)
</code></pre>
<p class="normal">The generated response is stored in the <code class="inlineCode">responses</code> variable, which is then returned by the <code class="inlineCode">generate()</code> function.</p>
<p class="normal">By submitting the formatted prompt to Vertex AI’s API endpoint for Gemini 1.5 Flash, leveraging the provided generation configuration and safety settings, this RAG pipeline can obtain a contextualized and relevant response tailored to the user’s query while adhering to the specified parameters and content filtering rules.</p>
<h1 class="heading-1" id="_idParaDest-93">Result post-processing</h1>
<p class="normal">After receiving the<a id="_idIndexMarker286"/> response from the language model, it is often desirable to present the output in a more structured and visually appealing format. Markdown is a lightweight markup language that allows you to add formatting elements such as headings, lists, code blocks, and more. In this example, we use Markdown formatting to enhance the presentation of the question, answer, and context:</p>
<pre class="programlisting code"><code class="hljs-code">#In this section you can format the answer for example with markdown
formatted_result = f"###Question:\n{question}\n\n###Answer:\n{result.text}\n\n&lt;details&gt;&lt;summary&gt;Context&lt;/summary&gt;{context}&lt;/details&gt;"
</code></pre>
<p class="normal">The breakdown of the components of this formatting is:</p>
<ul>
<li class="bulletList"><code class="inlineCode">"###Question:\n{question}"</code></li>
</ul>
<p class="normal-one">This part adds a level 3 Markdown heading (<code class="inlineCode">###</code>) for the <code class="inlineCode">Question</code> section, followed by the user’s original query <code class="inlineCode">({question}</code>) on a new line (<code class="inlineCode">\n</code>).</p>
<ul>
<li class="bulletList"><code class="inlineCode">"\n\n###Answer:\n{result.text}"</code></li>
</ul>
<p class="normal-one">After adding an empty line (<code class="inlineCode">\n\n</code>) at the beginning of the section, this section creates another level 3 Markdown heading for the <code class="inlineCode">Answer</code> section, followed by the generated response from the language model (<code class="inlineCode">{result.text}</code>) on a new line.</p>
<ul>
<li class="bulletList"><code class="inlineCode">"\n\n&lt;details&gt;&lt;summary&gt;Context&lt;/summary&gt;{context}&lt;/details&gt;"</code></li>
</ul>
<p class="normal-one">This part utilizes the Markdown <code class="inlineCode">&lt;details&gt;</code> and <code class="inlineCode">&lt;summary&gt;</code> tags to create a collapsible section for displaying the context information retrieved from the vector database. The <code class="inlineCode">&lt;summary&gt;Context&lt;/summary&gt;</code> text serves as the label for the collapsible section, and the actual context text (<code class="inlineCode">{context}</code>) is enclosed within the <code class="inlineCode">&lt;details&gt;</code> tags.</p>
<h2 class="heading-2" id="_idParaDest-94">Result presentation</h2>
<p class="normal">The <code class="inlineCode">Markdown</code> class<a id="_idIndexMarker287"/> from the <code class="inlineCode">IPython.display</code> module is a utility that allows you to display formatted Markdown content in a Colab notebook or other IPython environments. By passing the <code class="inlineCode">formatted_result</code> string to the Markdown constructor, you create a Markdown object that can be rendered by the <code class="inlineCode">display</code> function:</p>
<pre class="programlisting code"><code class="hljs-code">display(Markdown(formatted_result))
</code></pre>
<p class="normal">When you call <code class="inlineCode">display()</code>, the notebook will render the Markdown-formatted content contained in the <code class="inlineCode">formatted_result</code> string. This allows you to leverage the rich formatting capabilities of Markdown within the notebook environment.</p>
<p class="normal">The following is an example of the Markdown formatted output of our demo:</p>
<figure class="mediaobject"><img alt="" height="169" src="img/B22175_08_02.png" width="686"/></figure>
<p class="packt_figref">Figure 8.2: Screenshot from the formatted result in the Google Colab notebook</p>
<p class="normal">By using the <code class="inlineCode">Markdown</code> class and the <code class="inlineCode">display</code> function, you can take advantage of Markdown’s formatting capabilities within the Google Colab notebook environment. This includes features like headings, bold and italic text, lists, code blocks, links, and more.</p>
<p class="normal">The rendered output will be displayed in the notebook cell, providing a visually appealing and well-structured representation of the question, answer, and context information. This can greatly enhance the readability and usability of the chatbot’s responses, making it easier for users or developers to understand and interpret the results.</p>
<p class="normal">Additionally, the <code class="inlineCode">&lt;details&gt;</code> and <code class="inlineCode">&lt;summary&gt;</code> tags used in the <code class="inlineCode">context</code> section create a collapsible section, allowing users to toggle the visibility of the context information. This can be particularly helpful when dealing with large amounts of context data, as it prevents cluttering<a id="_idIndexMarker288"/> the main output while still providing easy access to the relevant information.</p>
<p class="normal">In the next section, we will dive deep into how all these components work together through a use case demo.</p>
<h1 class="heading-1" id="_idParaDest-95">Use case demo</h1>
<p class="normal">The following <a id="_idIndexMarker289"/>is the code for building a demo using Gradio; in this case, we will use an additional function that will perform the RAG pipeline. When you run this code, a Gradio interface will open in your default web browser, displaying three main sections:</p>
<ul>
<li class="bulletList"><strong class="screenText">Fintech Assistant</strong> heading</li>
<li class="bulletList">Chatbot area</li>
<li class="bulletList">Text input box </li>
</ul>
<p class="normal">Users can type their questions into the input box and submit them. The chat function will be called, which will use the <code class="inlineCode">answer_question</code> function to retrieve the relevant context from the vector database, generate an answer using the RAG pipeline, and update the chatbot interface with the user’s question and the generated response.</p>
<p class="normal">The Gradio interface provides a user-friendly way for users to interact with the RAG pipeline system, making it easier to test and demonstrate its capabilities. Additionally, Gradio offers various customization options and features, such as support for different input and output components, styling, and deployment options. We start by installing Gradio:</p>
<pre class="programlisting code"><code class="hljs-code">#In this case we will use a Gradio interface to interact 
#with the system
#Install Gradio
!pip install --upgrade gradio
</code></pre>
<p class="normal">Next, we define two helper functions that build upon the previously explained <code class="inlineCode">generate()</code> function:</p>
<pre class="programlisting code"><code class="hljs-code">import gradio as gr
def answer_question(query, db, number_of_results):
 context = get_context(query, db, number_of_results)
 answer = generate(prompt_template.format(query=query, context=context))
 return(answer.text)
def chat(message, history):
   response = answer_question(message,db, MAX_RESULTS)
   history.append((message, response))
   return "", history
</code></pre>
<p class="normal">The <code class="inlineCode">answer_question(...)</code> function takes three arguments:</p>
<ul>
<li class="bulletList"><code class="inlineCode">query</code>: The user’s question</li>
<li class="bulletList"><code class="inlineCode">db</code>: The vector database</li>
<li class="bulletList"><code class="inlineCode">number_of_results</code>: The maximum number of context results to retrieve from the database</li>
</ul>
<p class="normal">It then calls the <code class="inlineCode">get_context</code> function (not shown in the provided code) to retrieve the relevant context information – from the vector database, based on the user’s query and the specified number of results. The <a id="_idIndexMarker290"/>retrieved context is then formatted within the <code class="inlineCode">prompt_template</code> string and passed to the <code class="inlineCode">generate</code> function – covered in the previous sections – to obtain the answer.</p>
<p class="normal">At the end of the function execution, the generated answer is returned as a string.</p>
<p class="normal">The <code class="inlineCode">chat(...)</code> function takes two arguments: </p>
<ul>
<li class="bulletList"><code class="inlineCode">message</code>: The user’s question</li>
<li class="bulletList"><code class="inlineCode">history</code>: A list representing the conversation history</li>
</ul>
<p class="normal">It then calls the <code class="inlineCode">answer_question()</code> function with the user’s question, the vector database (<code class="inlineCode">db</code>), and the maximum number of results (<code class="inlineCode">MAX_RESULTS</code>).</p>
<p class="normal">The generated response is appended to the history list, along with the user’s question. The function returns an empty string and the updated history list, which will be used to update the chatbot interface.</p>
<h2 class="heading-2" id="_idParaDest-96">The Gradio app</h2>
<p class="normal">With the helper functions defined, we can now <a id="_idIndexMarker291"/>create the Gradio interface:</p>
<pre class="programlisting code"><code class="hljs-code">with gr.Blocks() as demo:
 gr.Markdown("Fintech Assistant")
 chatbot = gr.Chatbot(show_label=False)
 message = gr.Textbox(placeholder="Enter your question")
 message.submit(chat, [message, chatbot],[message, chatbot]  )
demo.launch(debug=True)
</code></pre>
<p class="normal">Here’s what’s happening in this code:</p>
<ul>
<li class="bulletList"><code class="inlineCode">with gr.Blocks() as demo</code> creates a Gradio interface block called <code class="inlineCode">demo</code>.</li>
<li class="bulletList"><code class="inlineCode">gr.Markdown(...)</code> displays a Markdown-formatted heading for the chatbot interface.</li>
<li class="bulletList"><code class="inlineCode">gr.Chatbot(...)</code> creates a Gradio chatbot component, which will display the conversation history.</li>
<li class="bulletList"><code class="inlineCode">gr.Textbox(...)</code> creates a text input box where users can enter their questions.</li>
<li class="bulletList"><code class="inlineCode">message.submit(...)</code> sets up an event handler for when the user submits their question. It calls the chat function with the user’s input (message) and the chatbot instance and updates the message and chatbot components with the returned values.</li>
<li class="bulletList"><code class="inlineCode">demo.launch(...)</code> launches the Gradio interface in debug mode, allowing you to interact with the chatbot.</li>
</ul>
<p class="normal">Refer to the GitHub directory of this chapter for the complete code that demonstrates how all the pieces described above fit together.</p>
<h1 class="heading-1" id="_idParaDest-97">Summary</h1>
<p class="normal">In this chapter, you’ve explored an integration pattern that combines RAG and generative AI models to build a chatbot capable of answering questions based on a document corpus. You’ve learned that RAG leverages the strengths of retrieval systems and generative models, allowing the system to retrieve relevant context from existing knowledge sources and generate contextual responses, preventing hallucinations and ensuring accuracy.</p>
<p class="normal">We proposed an architecture that utilized a serverless, event-driven approach built on Google Cloud. It consists of an ingestion layer for accepting user queries, a document corpus management layer for storing embeddings, an AI processing layer integrating with Google Gemini on Vertex AI, and monitoring and logging components. The entry point handles various input modalities like text, audio, and images, pre-processing them as needed.</p>
<p class="normal">You’ve learned that the core of the RAG pipeline involves generating embeddings from the user query, performing a vector similarity search on a vector database (in this example, we used Chroma DB), retrieving relevant context, formatting it into a prompt with instructions, and submitting it to the Gemini model on Vertex AI. The generated response can be post-processed, formatted with Markdown, and presented in a user-friendly interface using tools like Gradio.</p>
<p class="normal">You’ve now gained valuable insights into implementing a powerful RAG-based chatbot system. You’ve learned how to combine retrieval and generation techniques to provide contextual, hallucination-free responses, leveraging vector databases for semantic search through content embeddings. The chapter has equipped you with strategies to improve retrieval results and customize user experiences through prompt tuning. These skills will enable you to enhance your organization’s ability to provide accurate and contextualized responses to natural language queries, effectively utilizing existing document repositories and the power of generative AI.</p>
<h1 class="heading-1">Join our community on Discord</h1>
<p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
<p class="normal"><a href="Chapter_08.xhtml">https://packt.link/genpat</a></p>
<p class="normal"><img alt="" height="177" src="img/QR_Code134841911667913109.png" width="177"/></p>
</div>
</div></body></html>