["```py\nfrom fastapi import FastAPI, Request\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.messages import HumanMessage\nimport uvicorn\n# Initialize FastAPI app\napp = FastAPI()\n# Initialize the LLM\nllm = ChatAnthropic(model=\" claude-3-7-sonnet-latest\")\n@app.post(\"/chat\")\nasync def chat(request: Request):\n    data = await request.json()\n    user_message = data.get(\"message\", \"\")\n if not user_message:\n return {\"response\": \"No message provided\"}\n # Create a human message and get response from LLM\n    messages = [HumanMessage(content=user_message)]\n    response = llm.invoke(messages)\n return {\"response\": response.content}\n```", "```py\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n await websocket.accept()\n\n # Create a callback handler for streaming\n    callback_handler = AsyncIteratorCallbackHandler()\n\n # Create a streaming LLM\n    streaming_llm = ChatAnthropic(\n        model=\"claude-3-sonnet-20240229\",\n        callbacks=[callback_handler],\n        streaming=True\n    )\n\n # Process messages\n try:\n while True:\n            data = await websocket.receive_text()\n            user_message = json.loads(data).get(\"message\", \"\")\n\n # Start generation and stream tokens\n            task = asyncio.create_task(\n                streaming_llm.ainvoke([HumanMessage(content=user_message)])\n            )\n\n async for token in callback_handler.aiter():\n await websocket.send_json({\"token\": token})\n\n await task\n\n except WebSocketDisconnect:\n        logger.info(\"Client disconnected\")\n```", "```py\npython main.py\n```", "```py\nimport ray\nimport numpy as np\nfrom langchain_community.document_loaders import RecursiveUrlLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import FAISS\nimport os\n# Initialize Ray\nray.init()\n# Initialize the embedding model\nembeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')\n```", "```py\n# Create a function to preprocess documents\n@ray.remote\ndef preprocess_documents(docs):\n print(f\"Preprocessing batch of {len(docs)} documents\")\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n    chunks = text_splitter.split_documents(docs)\n print(f\"Generated {len(chunks)} chunks\")\n return chunks\n# Create a function to embed chunks in parallel\n@ray.remote\ndef embed_chunks(chunks):\n print(f\"Embedding batch of {len(chunks)} chunks\")\n    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')\n return FAISS.from_documents(chunks, embeddings)\n```", "```py\ndef build_index(base_url=\"https://docs.ray.io/en/master/\", batch_size=50):\n # Create index directory if it doesn't exist\n    os.makedirs(\"faiss_index\", exist_ok=True)\n\n # Choose a more specific section for faster processing\n print(f\"Loading documentation from {base_url}\")\n    loader = RecursiveUrlLoader(base_url)\n    docs = loader.load()\n print(f\"Loaded {len(docs)} documents\")\n\n # Preprocess in parallel with smaller batches\n    chunks_futures = []\n for i in range(0, len(docs), batch_size):\n        batch = docs[i:i+batch_size]\n        chunks_futures.append(preprocess_documents.remote(batch))\n\n print(\"Waiting for preprocessing to complete...\")\n    all_chunks = []\n for chunks in ray.get(chunks_futures):\n        all_chunks.extend(chunks)\n\n print(f\"Total chunks: {len(all_chunks)}\")\n\n # Split chunks for parallel embedding\n    num_workers = 4\n    chunk_batches = np.array_split(all_chunks, num_workers)\n\n # Embed in parallel\n print(\"Starting parallel embedding...\")\n    index_futures = [embed_chunks.remote(batch) for batch in chunk_batches]\n```", "```py\n    indices = ray.get(index_futures)\n\n # Merge indices\n print(\"Merging indices...\")\n    index = indices[0]\n for idx in indices[1:]:\n        index.merge_from(idx)\n\n # Save the index\n print(\"Saving index...\")\n    index.save_local(\"faiss_index\")\n print(\"Index saved to 'faiss_index' directory\")\n\n return index\n```", "```py\nif __name__ == \"__main__\":\n # For faster testing, use a smaller section:\n # index = build_index(\"https://docs.ray.io/en/master/ray-core/\")\n\n # For complete documentation:\n    index = build_index()\n\n # Test the index\n print(\"\\nTesting the index:\")\n    results = index.similarity_search(\"How can Ray help with deploying LLMs?\", k=2)\n for i, doc in enumerate(results):\n print(f\"\\nResult {i+1}:\")\n print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n print(f\"Content: {doc.page_content[:150]}...\")\n```", "```py\nimport ray from ray import serve\nfrom fastapi import FastAPI\nfrom langchain_huggingface import HuggingFaceEmbeddings\n```", "```py\nfrom langchain_community.vectorstores import FAISS\n# initialize Ray\nray.init()\n# define our FastAPI app\napp = FastAPI()\n@serve.deployment class SearchDeployment:\n def init(self):\n print(\"Loading pre-built index...\")\n # Initialize the embedding model\n self.embeddings = HuggingFaceEmbeddings(\n            model_name='sentence-transformers/all-mpnet-base-v2'\n        )\n # Check if index directory exists\n import os\n if not os.path.exists(\"faiss_index\") or not os.path.isdir(\"faiss_index\"):\n        error_msg = \"ERROR: FAISS index directory not found!\"\n print(error_msg)\n raise FileNotFoundError(error_msg)\n\n # Load the pre-built index\n self.index = FAISS.load_local(\"faiss_index\", self.embeddings)\n print(\"SearchDeployment initialized successfully\")\n\nasync def __call__(self, request):\n    query = request.query_params.get(\"query\", \"\")\n if not query:\n return {\"results\": [], \"status\": \"empty_query\", \"message\": \"Please provide a query parameter\"}\n\n try:\n # Search the index\n        results = self.index.similarity_search_with_score(query, k=5)\n\n # Format results for response\n        formatted_results = []\n for doc, score in results:\n            formatted_results.append({\n```", "```py\n \"content\": doc.page_content,\n \"source\": doc.metadata.get(\"source\", \"Unknown\"),\n \"score\": float(score)\n            })\n\n return {\"results\": formatted_results, \"status\": \"success\", \"message\": f\"Found {len(formatted_results)} results\"}\n\n except Exception as e:\n # Error handling omitted for brevity\n return {\"results\": [], \"status\": \"error\", \"message\": f\"Search failed: {str(e)}\"}\n```", "```py\nif name == \"main\": deployment = SearchDeployment.bind() serve.run(deployment) print(\"Service started at: http://localhost:8000/\")\n```", "```py\n    python chapter9/ray/build_index.py\n    ```", "```py\n    python chapter9/ray/serve_index.py\n    ```", "```py\nFROM python:3.11-slim\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY . .\nEXPOSE 8000\nCMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n```", "```py\n# secrets.yaml - Store API keys securely\napiVersion: v1\nkind: Secret\nmetadata:\n  name: langchain-secrets\ntype: Opaque\ndata:\n # Base64 encoded secrets (use: echo -n \"your-key\" | base64)\n  OPENAI_API_KEY: BASE64_ENCODED_KEY_HERE\n```", "```py\n# deployment.yaml - Main application configuration\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: langchain-app\n  labels:\n    app: langchain-app\nspec:\n```", "```py\n  replicas: 2 # For basic high availability\n  selector:\n    matchLabels:\n      app: langchain-app\n  template:\n    metadata:\n      labels:\n        app: langchain-app\n    spec:\n      containers:\n      - name: langchain-app\n        image: your-registry/langchain-app:1.0.0\n        ports:\n        - containerPort: 8000\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"300m\"\n        env:\n          - name: LOG_LEVEL\n            value: \"INFO\"\n          - name: MODEL_NAME\n            value: \"gpt-4\"\n # Mount secrets securely\n        envFrom:\n        - secretRef:\n            name: langchain-secrets\n # Basic health checks\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 5\n          periodSeconds: 10\n```", "```py\n# service.yaml - Expose the application\napiVersion: v1\nkind: Service\nmetadata:\n  name: langchain-app-service\nspec:\n  selector:\n    app: langchain-app\n  ports:\n  - port: 80\n    targetPort: 8000\n type: ClusterIP  # Internal access within cluster\n```", "```py\n# ingress.yaml - External access configuration\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: langchain-app-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n  - host: langchain-app.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n```", "```py\n        backend:\n          service:\n            name: langchain-app-service\n            port:\n              number: 80\n```", "```py\n# Apply each file in appropriate order\nkubectl apply -f secrets.yaml\nkubectl apply -f deployment.yaml\nkubectl apply -f service.yaml\nkubectl apply -f ingress.yaml\n# Verify deployment\nkubectl get pods\nkubectl get services\nkubectl get ingress\n```", "```py\n@app.get(\"/health\")\nasync def health_check():\n try:\n # Test connection to OpenAI\n        response = await llm.agenerate([\"Hello\"])\n # Test connection to vector store\n        vector_store.similarity_search(\"test\")\n return {\"status\": \"healthy\"}\n except Exception as e:\n return JSONResponse(\n            status_code=503,\n            content={\"status\": \"unhealthy\", \"error\": str(e)}\n        )\n```", "```py\npip install --upgrade \"langgraph-cli[inmem]\"\n```", "```py\nlanggraph new path/to/your/app --template react-agent-python\n```", "```py\nmy-app/\n├── my_agent/                # All project code\n│   ├── utils/               # Utilities for your graph\n│   │   ├── __init__.py\n│   │   ├── tools.py         # Tool definitions\n│   │   ├── nodes.py         # Node functions\n│   │   └── state.py         # State definition\n│   ├── requirements.txt     # Package dependencies\n│   ├── __init__.py\n│   └── agent.py             # Graph construction code\n├── .env                     # Environment variables\n└── langgraph.json           # LangGraph configuration\n```", "```py\nlanggraph dev\n```", "```py\nfrom langgraph_sdk import get_client\nclient = get_client(url=\"http://localhost:2024\")\n# Stream a response from the agent\nasync for chunk in client.runs.stream(\n None,  # Threadless run\n \"agent\",  # Name of assistant defined in langgraph.json\n input={\n \"messages\": [{\n \"role\": \"human\",\n \"content\": \"What is LangGraph?\",\n        }],\n    },\n    stream_mode=\"updates\",\n):\n print(f\"Receiving event: {chunk.event}...\")\n print(chunk.data)\n```", "```py\n{\n \"dependencies\": [\"./my_agent\"],\n \"graphs\": {\n \"agent\": \"./my_agent/agent.py:graph\"\n  },\n \"env\": \".env\"\n}\n```", "```py\n# my_agent/agent.py\nfrom langgraph.graph import StateGraph, END, START\n# Define the graph\nworkflow = StateGraph(AgentState)\n# ... add nodes and edges …\n# Compile and export - this variable is referenced in langgraph.json\ngraph = workflow.compile()\n```", "```py\nlanggraph>=0.2.56,<0.4.0\nlanggraph-sdk>=0.1.53\nlangchain-core>=0.2.38,<0.4.0\n# Add other dependencies your application needs\n```", "```py\nLANGSMITH_API_KEY=lsv2…\nOPENAI_API_KEY=sk-...\n# Add other API keys and configuration\n```", "```py\npip install langchain-mcp-adapters\n```", "```py\nfrom mcp.server.fastmcp import FastMCP\nmcp = FastMCP(\"Math\")\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n \"\"\"Add two numbers\"\"\"\n return a + b\n@mcp.tool()\ndef multiply(a: int, b: int) -> int:\n \"\"\"Multiply two numbers\"\"\"\n return a * b\nif __name__ == \"__main__\":\n    mcp.run(transport=\"stdio\")\n```", "```py\npython math_server.py\n```", "```py\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\nfrom langchain_mcp_adapters.tools import load_mcp_tools\nfrom langgraph.prebuilt import create_react_agent\n```", "```py\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\")\nserver_params = StdioServerParameters(\n    command=\"python\",\n # Update with the full absolute path to math_server.py\n    args=[\"/path/to/math_server.py\"],\n)\nasync def run_agent():\n async with stdio_client(server_params) as (read, write):\n async with ClientSession(read, write) as session:\n await session.initialize()\n            tools = await load_mcp_tools(session)\n            agent = create_react_agent(model, tools)\n            response = await agent.ainvoke({\"messages\": \"what's (3 + 5) x 12?\"})\n print(response)\n```", "```py\n# LiteLLM with LangChain\nimport os\nfrom langchain_litellm import ChatLiteLLM, ChatLiteLLMRouter\nfrom litellm import Router\nfrom langchain.chains import LLMChain\nfrom langchain_core.prompts import PromptTemplate\n# Configure multiple model deployments with fallbacks\nmodel_list = [\n    {\n \"model_name\": \"claude-3.7\",\n \"litellm_params\": {\n \"model\": \"claude-3-opus-20240229\",  # Automatic fallback option\n \"api_key\": os.getenv(\"ANTHROPIC_API_KEY\"),\n```", "```py\n        }\n    },\n    {\n \"model_name\": \"gpt-4\",\n \"litellm_params\": {\n \"model\": \"openai/gpt-4\",  # Automatic fallback option\n \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n        }\n    }\n]\n# Setup router with reliability features\nrouter = Router(\n    model_list=model_list,\n    routing_strategy=\"usage-based-routing-v2\",\n    cache_responses=True,          # Enable caching\n    num_retries=3 # Auto-retry failed requests\n)\n# Create LangChain LLM with router\nrouter_llm = ChatLiteLLMRouter(router=router, model_name=\"gpt-4\")\n# Build and use a LangChain\nprompt = PromptTemplate.from_template(\"Summarize: {text}\")\nchain = LLMChain(llm=router_llm, prompt=prompt)\nresult = chain.invoke({\"text\": \"LiteLLM provides reliability for LLM applications\"})\n```", "```py\nimport subprocess\nfrom urllib.parse import urlparse\nfrom pydantic import HttpUrl\nfrom langchain_core.tools import StructuredTool\ndef ping(url: HttpUrl, return_error: bool) -> str:\n \"\"\"Ping the fully specified url. Must include https:// in the url.\"\"\"\n    hostname = urlparse(str(url)).netloc\n    completed_process = subprocess.run(\n        [\"ping\", \"-c\", \"1\", hostname], capture_output=True, text=True\n    )\n    output = completed_process.stdout\n if return_error and completed_process.returncode != 0:\n return completed_process.stderr\n return output\nping_tool = StructuredTool.from_function(ping)\n```", "```py\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain.agents import initialize_agent, AgentType\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0613\", temperature=0)\nagent = initialize_agent(\n    llm=llm,\n    tools=[ping_tool],\n    agent=AgentType.OPENAI_MULTI_FUNCTIONS,\n    return_intermediate_steps=True, # IMPORTANT!\n)\nresult = agent(\"What's the latency like for https://langchain.com?\")\n```", "```py\nThe latency for https://langchain.com is 13.773 ms\n```", "```py\n[(_FunctionsAgentAction(tool='ping', tool_input={'url': 'https://langchain.com', 'return_error': False}, log=\"\\nInvoking: `ping` with `{'url': 'https://langchain.com', 'return_error': False}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'name': 'tool_selection', 'arguments': '{\\n \"actions\": [\\n {\\n \"action_name\": \"ping\",\\n \"action\": {\\n \"url\": \"https://langchain.com\",\\n \"return_error\": false\\n }\\n }\\n ]\\n}'}}, example=False)]), 'PING langchain.com (35.71.142.77): 56 data bytes\\n64 bytes from 35.71.142.77: icmp_seq=0 ttl=249 time=13.773 ms\\n\\n--- langchain.com ping statistics ---\\n1 packets transmitted, 1 packets received, 0.0% packet loss\\nround-trip min/avg/max/stddev = 13.773/13.773/13.773/0.000 ms\\n')]\n```", "```py\ndef check_hallucination(response, query):\n    validator_prompt = f\"\"\"\n    You are a fact-checking assistant.\n```", "```py\n\n    USER QUERY: {query}\n    MODEL RESPONSE: {response}\n\n Evaluate if the response contains any factual errors or unsupported claims.\n    Return a JSON with these keys:\n    - hallucination_detected: true/false\n   - confidence: 1-10\n    - reasoning: brief explanation\n    \"\"\"\n\n    validation_result = validator_llm.invoke(validator_prompt)\n return validation_result\n```", "```py\nfrom fairlearn.metrics import demographic_parity_difference\n# Example of monitoring bias in a classification context\ndemographic_parity = demographic_parity_difference(\n    y_true=ground_truth,\n    y_pred=model_predictions,\n    sensitive_features=demographic_data\n)\n```", "```py\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.output_parsers import StrOutputParser\n```", "```py\nfrom langchain_core.prompts import ChatPromptTemplate\n# Define models with different capabilities and costs\naffordable_model = ChatOpenAI(model=\"gpt-3.5-turbo\")  # ~10× cheaper than gpt-4o\npowerful_model = ChatOpenAI(model=\"gpt-4o\")           # More capable but more expensive\n# Create classifier prompt\nclassifier_prompt = ChatPromptTemplate.from_template(\"\"\"\nDetermine if the following query is simple or complex based on these criteria:\n- Simple: factual questions, straightforward tasks, general knowledge\n- Complex: multi-step reasoning, nuanced analysis, specialized expertise\nQuery: {query}\nRespond with only one word: \"simple\" or \"complex\"\n\"\"\")\n# Create the classifier chain\nclassifier = classifier_prompt | affordable_model | StrOutputParser()\ndef route_query(query):\n \"\"\"Route the query to the appropriate model based on complexity.\"\"\"\n    complexity = classifier.invoke({\"query\": query})\n\n if \"simple\" in complexity.lower():\n print(f\"Using affordable model for: {query}\")\n return affordable_model\n else:\n print(f\"Using powerful model for: {query}\")\n return powerful_model\n# Example usage\ndef process_query(query):\n    model = route_query(query)\n return model.invoke(query)\n```", "```py\nfrom langchain_openai import ChatOpenAI\nfrom langchain.evaluation import load_evaluator\n# Define models with different price points\naffordable_model = ChatOpenAI(model=\"gpt-3.5-turbo\")\npowerful_model = ChatOpenAI(model=\"gpt-4o\")\n# Load an evaluator to assess response quality\nevaluator = load_evaluator(\"criteria\", criteria=\"relevance\", llm=affordable_model)\ndef get_response_with_fallback(query):\n \"\"\"Try affordable model first, fallback to powerful model if quality is low.\"\"\"\n # First attempt with affordable model\n    initial_response = affordable_model.invoke(query)\n\n # Evaluate the response\n    eval_result = evaluator.evaluate_strings(\n        prediction=initial_response.content,\n        reference=query\n    )\n\n # If quality score is too low, use the more powerful model\n if eval_result[\"score\"] < 4.0:  # Threshold on a 1-5 scale\n print(\"Response quality insufficient, using more powerful model\")\n return powerful_model.invoke(query)\n\n return initial_response\n```", "```py\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n# Initialize the LLM with max_tokens parameter\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    max_tokens=150 # Limit to approximately 100-120 words\n)\n# Create a prompt template with length guidance\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that provides concise, accurate information. Your responses should be no more than 100 words unless explicitly asked for more detail.\"),\n    (\"human\", \"{query}\")\n])\n# Create a chain\nchain = prompt | llm | StrOutputParser()\n```", "```py\nfrom langchain.callbacks import get_openai_callback\nfrom langchain_openai import ChatOpenAI\nllm = ChatOpenAI(model=\"gpt-4o\")\nwith get_openai_callback() as cb:\n    response = llm.invoke(\"Explain quantum computing in simple terms\")\n\n print(f\"Total Tokens: {cb.total_tokens}\")\n print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n print(f\"Completion Tokens: {cb.completion_tokens}\")\n print(f\"Total Cost (USD): ${cb.total_cost}\")\n```"]