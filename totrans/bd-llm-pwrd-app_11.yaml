- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Fine-Tuning Large Language Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型微调
- en: Up to this point, we’ve explored the features and applications of **large language
    models** (**LLMs**) in their “base” form, meaning that we consumed them with the
    parameters obtained from their base training. We experimented with many scenarios
    in which, even in their base form, LLMs have been able to adapt to a variety of
    scenarios. Nevertheless, there might be extremely domain-specific cases where
    a general-purpose LLM is not sufficient to fully embrace the taxonomy and knowledge
    of that domain. If this is the case, you might want to fine-tune your model on
    your domain-specific data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了“基础”形式下大型语言模型（LLMs）的功能和应用，这意味着我们使用从其基础训练中获得的参数来消费它们。我们实验了许多场景，即使在基础形式下，LLMs
    也能适应各种场景。然而，可能存在极端特定领域的案例，其中通用 LLM 无法完全拥抱该领域的分类法和知识。如果是这种情况，您可能需要在特定领域的数据上微调您的模型。
- en: '**Definition**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: In the context of fine-tuning language models, “taxonomy” refers to a structured
    classification or categorization system that organizes concepts, terms, and entities
    according to their relationships and hierarchies within a specific domain. This
    system is essential for making the model’s understanding and generation of content
    more relevant and accurate for specialized applications.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言模型微调的背景下，“分类法”指的是一种结构化的分类或分类系统，它根据特定领域内概念、术语和实体之间的关系和层次结构来组织它们。这个系统对于使模型对内容的理解和生成更加相关和准确，对于专业应用至关重要。
- en: A concrete example of taxonomy in a domain-specific sector is in the medical
    field. Here, taxonomy could categorize information into structured groups like
    diseases, symptoms, treatments, and patient demographics. For instance, in the
    “diseases” category, there might be subcategories for types of diseases like “cardiovascular
    diseases,” which could be further divided into more specific conditions such as
    “hypertension” and “coronary artery disease.” This detailed categorization helps
    in fine-tuning language models to understand and generate more precise and contextually
    appropriate responses in medical consultations or documentation.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在特定领域的一个具体分类法示例是医疗领域。在这里，分类法可以将信息分类到结构化组中，如疾病、症状、治疗和患者人口统计信息。例如，在“疾病”类别中，可能会有关于疾病类型的子类别，如“心血管疾病”，这可以进一步细分为更具体的情况，如“高血压”和“冠状动脉疾病”。这种详细的分类有助于微调语言模型，使其在医疗咨询或文档中理解和生成更精确和上下文相关的响应。
- en: In this chapter, we are going to cover the technical details of fine-tuning
    LLMs, from the theory behind it to the hands-on implementation with Python and
    Hugging Face. By the end of this chapter, you will be able to fine-tune an LLM
    on your own data, so that you can build domain-specific applications powered by
    those models.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍微调 LLM 的技术细节，从其背后的理论到使用 Python 和 Hugging Face 的实际操作实现。到本章结束时，您将能够使用自己的数据微调
    LLM，从而可以构建由这些模型驱动的特定领域应用。
- en: 'We will delve into the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将深入探讨以下主题：
- en: Introduction to fine-tuning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调简介
- en: Understanding when you need fine-tuning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解何时需要微调
- en: Preparing your data to fine-tune the model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备您的数据以微调模型
- en: Fine-tuning a base model on your data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在您的数据上微调基础模型
- en: Hosting strategies for your fine-tuned model
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您微调模型的托管策略
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To complete the tasks in this chapter, you will need the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本章的任务，您需要以下内容：
- en: A Hugging Face account and user access token.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face 账户和用户访问令牌。
- en: Python 3.7.1 or later version.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3.7.1 或更高版本。
- en: 'Python packages: Make sure to have the following Python packages installed:
    `python-dotenv`, `huggingface_hub`, `accelerate>=0.16.0`, `<1 transformers[torch]`,
    `safetensors`, `tensorflow`, `datasets`, `evaluate`, and `accelerate`. Those can
    be easily installed via `pip install` in your terminal. If you want to install
    everything from the latest release, you can refer to the original GitHub by running
    `pip install git+https://github.com/huggingface/transformers.git` in your terminal.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 包：请确保已安装以下 Python 包：`python-dotenv`、`huggingface_hub`、`accelerate>=0.16.0`、`<1
    transformers[torch]`、`safetensors`、`tensorflow`、`datasets`、`evaluate` 和 `accelerate`。这些包可以通过在终端中运行
    `pip install` 命令轻松安装。如果您想从最新版本安装所有内容，可以在终端中运行 `pip install git+https://github.com/huggingface/transformers.git`
    来参考原始 GitHub。
- en: You can find all the code and examples in the book’s GitHub repository at [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_11.xhtml).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的 GitHub 仓库中找到所有代码和示例：[https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_11.xhtml)。
- en: What is fine-tuning?
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是微调？
- en: Fine-tuning is a technique of **transfer learning** in which the weights of
    a pretrained neural network are used as the initial values for training a new
    neural network on a different task. This can improve the performance of the new
    network by leveraging the knowledge learned from the previous task, especially
    when the new task has limited data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是一种**迁移学习**技术，其中使用预训练神经网络的权重作为在新的不同任务上训练新神经网络的初始值。这可以通过利用从先前任务中学到的知识来提高新网络的表现，尤其是在新任务数据有限的情况下。
- en: '**Definition**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: Transfer learning is a technique in machine learning that involves using the
    knowledge learned from one task to improve the performance on a related but different
    task. For example, if you have a model that can recognize cars, you can use some
    of its features to help you recognize trucks. Transfer learning can save you time
    and resources by reusing existing models instead of training new ones from scratch.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是机器学习中的一种技术，涉及使用从一项任务中学到的知识来提高相关但不同任务的表现。例如，如果你有一个可以识别汽车的模型，你可以使用其中的一些特征来帮助你识别卡车。通过重用现有模型而不是从头开始训练新模型，迁移学习可以节省你的时间和资源。
- en: To better understand the concepts of transfer learning and fine-tuning, let’s
    consider the following example.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解迁移学习和微调的概念，让我们考虑以下示例。
- en: Imagine you want to train a computer vision neural network to recognize different
    types of flowers, such as roses, sunflowers, and tulips. You have a lot of photos
    of flowers, but not enough to train a model from scratch.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你想要训练一个计算机视觉神经网络来识别不同类型的花朵，例如玫瑰、向日葵和郁金香。你有很多花朵的照片，但不足以从头开始训练一个模型。
- en: Instead, you can use transfer learning, which means taking a model that was
    already trained on a different task and using some of its knowledge for your new
    task. For example, you can take a model that was trained to recognize many vehicles,
    such as cars, trucks, and bicycles. This model has learned how to extract features
    from images, such as edges, shapes, colors, and textures. These features are useful
    for any image recognition task, not just the original one.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，你可以使用迁移学习，这意味着使用已经在不同任务上训练好的模型，并使用其中的一些知识来处理你的新任务。例如，你可以使用一个已经训练来识别许多车辆的模型，如汽车、卡车和自行车。这个模型已经学会了如何从图像中提取特征，例如边缘、形状、颜色和纹理。这些特征对任何图像识别任务都很有用，而不仅仅是原始任务。
- en: You can use this model as a base for your flower recognition model. You only
    need to add a new layer on top of it, which will learn how to classify the features
    into flower types. This layer is called the classifier layer, and it is needed
    for the model to adapt to the new task. Training the classifier layer on top of
    the base model is a process called **feature extraction**. Once this step is done,
    you can further tailor your model with fine-tuning by unfreezing some of the base
    model layers and training them together with the classifier layer. This allows
    you to adjust the base model features to better suit your task.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用这个模型作为你的花朵识别模型的基础。你只需要在其上方添加一个新层，该层将学习如何将特征分类为花朵类型。这个层被称为分类器层，它是模型适应新任务所需的。在基础模型之上训练分类器层的过程称为**特征提取**。一旦完成这一步，你可以通过解冻基础模型的一些层并与分类器层一起训练来进一步调整你的模型，这允许你调整基础模型特征以更好地适应你的任务。
- en: 'The following picture illustrates the computer vision model example:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了计算机视觉模型示例：
- en: '![](img/B21714_11_01.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21714_11_01.png)'
- en: 'Figure 11.1: Example of transfer learning and fine-tuning'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1：迁移学习和微调的示例
- en: Fine-tuning is usually done after feature extraction, as a final step to improve
    the performance of the model. You can decide how many layers to unfreeze based
    on your data size and complexity. A common practice is to unfreeze the last few
    layers of the base model, which are more specific to the original task, and leave
    the first few layers frozen, which are more generic and reusable.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 微调通常在特征提取之后进行，作为提高模型性能的最后一步。你可以根据你的数据大小和复杂性来决定解冻多少层。一种常见的做法是解冻基础模型中最后几层，这些层更具体于原始任务，而保留前几层冻结，这些层更通用且可重用。
- en: To summarize, transfer learning and fine-tuning are techniques that allow you
    to use a pretrained model for a new task. Transfer learning involves adding a
    new classifier layer on top of the base model and training only that layer. Fine-tuning
    involves unfreezing some or all of the base model layers and training them together
    with the classifier layer.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，迁移学习和微调是允许您为新的任务使用预训练模型的技巧。迁移学习涉及在基础模型之上添加新的分类器层，并仅训练该层。微调涉及解冻基础模型的一些或所有层，并将它们与分类器层一起训练。
- en: 'In the context of generative AI, fine-tuning is the process of adapting a pretrained
    language model to a specific task or domain by updating its parameters on a task-specific
    dataset. Fine-tuning can improve the performance and accuracy of the model for
    the target task. The steps involved in fine-tuning are:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成式AI的背景下，微调是通过在特定任务或领域的数据集上更新其参数来调整预训练语言模型的过程。微调可以提高模型在目标任务上的性能和准确性。微调涉及以下步骤：
- en: '**Load the pretrained language model and its tokenizer**: The tokenizer is
    used to convert text into numerical tokens that the model can process. Different
    models have unique architectures and requirements, often coming with their own
    specialized tokenizers designed to handle their specific input formats.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**加载预训练的语言模型及其分词器**：分词器用于将文本转换为模型可以处理的数值标记。不同的模型具有独特的架构和需求，通常附带自己的专用分词器，用于处理其特定的输入格式。'
- en: For instance, **BERT** (which stands for **Bidirectional Encoder Representations
    from Transformers**) uses WordPiece tokenization, while GPT-2 employs **byte-pair
    encoding** (**BPE**). Models also impose token limits due to memory constraints
    during training and inference.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，**BERT**（代表**从Transformer中提取的双向编码器表示**）使用WordPiece分词，而GPT-2采用**字节对编码**（**BPE**）。由于训练和推理过程中的内存限制，模型也设置了标记限制。
- en: These limits determine the maximum sequence length that a model can handle.
    For example, BERT has a maximum token limit of 512 tokens, while the GPT-2 can
    handle longer sequences (e.g., up to 1,024 tokens).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这些限制确定了模型可以处理的最大序列长度。例如，BERT的最大标记限制为512个标记，而GPT-2可以处理更长的序列（例如，最多1,024个标记）。
- en: '**Prepare the task-specific dataset**: The dataset should contain input-output
    pairs that are relevant to the task. For example, for sentiment analysis, the
    input could be a text review and the output could be a sentiment label (positive,
    negative, or neutral).'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**准备特定任务的数据集**：数据集应包含与任务相关的输入-输出对。例如，对于情感分析，输入可以是文本评论，输出可以是情感标签（正面、负面或中性）。'
- en: '**Define the task-specific head**: The head is a layer or a set of layers that
    are added on top of the pretrained model to perform the task. The head should
    match the output format and size of the task. For example, for sentiment analysis,
    the head could be a linear layer with three output units corresponding to the
    three sentiment labels.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义特定任务的头部**：头部是在预训练模型之上添加的一层或一组层，用于执行特定任务。头部应匹配任务的输出格式和大小。例如，对于情感分析，头部可以是一个具有三个输出单元的线性层，这三个输出单元对应于三个情感标签。'
- en: '**Note**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: When dealing with an LLM specifically designed for text generation, the architecture
    differs from models used for classification or other tasks. In fact, unlike classification
    tasks, where we predict labels, an LLM predicts the next word or token in a sequence.
    This layer is added on top of the pretrained transformer-based models with the
    purpose of transforming the contextualized hidden representations from the base
    model into probabilities over the vocabulary.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理专门为文本生成设计的LLM时，其架构与用于分类或其他任务的模型不同。事实上，与预测标签的分类任务不同，LLM预测序列中的下一个单词或标记。这一层被添加到预训练的基于Transformer的模型之上，目的是将基础模型中的上下文化隐藏表示转换为词汇表上的概率。
- en: '**Train the model on the task-specific dataset**: The training process involves
    feeding the input tokens to the model, computing the loss between the model output
    and the true output, and updating the model parameters using an optimizer. The
    training can be done for a fixed number of epochs or until a certain criterion
    is met.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在特定任务的数据集上训练模型**：训练过程包括将输入标记馈送到模型，计算模型输出与真实输出之间的损失，并使用优化器更新模型参数。训练可以是固定数量的epoch，或者直到满足某个标准。'
- en: '**Evaluate the model on a test or validation set**:The evaluation process involves
    measuring the performance of the model on unseen data using appropriate metrics.
    For example, for sentiment analysis, the metric could be accuracy or F1-score
    (which will be discussed later in this chapter). The evaluation results can be
    used to compare different models or fine-tuning strategies.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在测试或验证集上评估模型**：评估过程涉及使用适当的指标来衡量模型在未见数据上的性能。例如，对于情感分析，指标可以是准确率或F1分数（将在本章后面讨论）。评估结果可以用来比较不同的模型或微调策略。'
- en: Even though it is less computationally and time expensive than full training,
    fine-tuning an LLM is not a “light” activity. As LLMs are, by definition, large,
    their fine-tuning has hardware requirements as well as data collection and preprocessing.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 即使微调在计算和时间成本上比完整训练要低，但微调一个LLM并不是一项“轻松”的活动。由于LLM本质上很大，它们的微调需要硬件要求，以及数据收集和预处理。
- en: 'So, the first question that you want to ask yourself while approaching a given
    scenario is: “Do I really need to finetune my LLM?”'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在接近特定场景时，您想要问自己的第一个问题是：“我真的需要微调我的LLM吗？”
- en: When is fine-tuning necessary?
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 何时需要微调？
- en: As we saw in previous chapters, good prompt engineering combined with the non-parametric
    knowledge you can add to your model via embeddings are exceptional techniques
    to customize your LLM, and they can account for around 90% of use cases. However,
    the preceding affirmation tends to hold for the state-of-the-art models, such
    as GPT-4, Llama 2, and PaLM 2\. As discussed, those models have a huge number
    of parameters that make them heavy, hence the need for computational power; plus,
    they might be proprietary and subject to a pay-per-use cost.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中看到的，良好的提示工程以及您可以通过嵌入添加到模型中的非参数知识是定制LLM的卓越技术，它们可以解释大约90%的使用案例。然而，前面的断言往往适用于最先进的模型，如GPT-4、Llama
    2和PaLM 2。正如所讨论的，这些模型具有大量的参数，使它们变得沉重，因此需要计算能力；此外，它们可能是专有的，并且可能需要按使用付费。
- en: Henceforth, fine-tuning might also be useful when you want to leverage a light
    and free-of-charge LLM, such as the Falcon LLM 7B, yet you want it to perform
    as well as a SOTA model in your specific task.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从此以后，当您想利用一个轻量级且免费的LLM，例如Falcon LLM 7B，但又希望它在特定任务中表现与SOTA模型相当时，微调也可能很有用。
- en: 'Some examples of when fine-tuning might be necessary are:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一些可能需要微调的例子包括：
- en: When you want to use an LLM for sentiment analysis on movie reviews, but the
    LLM was pretrained on Wikipedia articles and books. Fine-tuning can help the LLM
    learn the vocabulary, style, and tone of movie reviews, as well as the relevant
    features for sentiment classification.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您想使用LLM对电影评论进行情感分析，但LLM是在维基百科文章和书籍上预训练的。微调可以帮助LLM学习电影评论的词汇、风格和语气，以及与情感分类相关的相关特征。
- en: When you want to use an LLM for text summarization on news articles, but the
    LLM was pretrained on a language modeling objective. Fine-tuning can help the
    LLM learn the structure, content, and length of summaries, as well as the generation
    objective and evaluation metrics.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您想使用LLM对新闻文章进行文本摘要，但LLM是在语言建模目标上预训练的。微调可以帮助LLM学习摘要的结构、内容和长度，以及生成目标和评估指标。
- en: When you want to use an LLM for machine translation between two languages, but
    the LLM was pretrained on a multilingual corpus that does not include those languages.
    Fine-tuning can help the LLM learn the vocabulary, grammar, and syntax of the
    target languages, as well as the translation objective and alignment methods.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您想使用LLM在两种语言之间进行机器翻译，但LLM是在不包含这些语言的跨语言语料库上预训练的。微调可以帮助LLM学习目标语言的词汇、语法和句法，以及翻译目标和对齐方法。
- en: When you want to use an LLM to perform complex **named entity recognition**
    (**NER**) tasks. For example, financial and legal documents contain specialized
    terminology and entities that are not typically prioritized in general language
    models, henceforth a fine-tuning process might be extremely beneficial here.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您想使用LLM执行复杂的**命名实体识别**（NER）任务时。例如，财务和法律文件包含专业术语和实体，这些在通用语言模型中通常不会被优先考虑，因此微调过程在这里可能极为有益。
- en: In this chapter, we will be covering a full-code approach leveraging Hugging
    Face models and libraries. However, be aware that Hugging Face also offers a low-code
    platform called AutoTrain (you can read more about that at [https://huggingface.co/autotrain](https://huggingface.co/autotrain)),
    which might be a good alternative if your organization is more oriented towards
    low-code strategies.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍利用Hugging Face模型和库的全代码方法。然而，请注意，Hugging Face还提供了一个名为AutoTrain的低代码平台（你可以在[https://huggingface.co/autotrain](https://huggingface.co/autotrain)了解更多信息），如果你的组织更倾向于低代码策略，这可能是一个不错的选择。
- en: Getting started with fine-tuning
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始微调
- en: In this section, we are going to cover all the steps needed to fine-tune an
    LLM with a full-code approach. We will be leveraging Hugging Face libraries, such
    as `datasets` (to load data from the Hugging Face datasets ecosystem) and `tokenizers`
    (to provide an implementation of the most popular tokenizers). The scenario we
    are going to address is a sentiment analysis task. Our goal is to fine-tune a
    model to make it an expert binary classifier of emotions, clustered into “positive”
    and “negative.”
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将涵盖使用全代码方法微调一个LLM所需的所有步骤。我们将利用Hugging Face库，如`datasets`（从Hugging Face数据集生态系统加载数据）和`tokenizers`（提供最流行的分词器实现）。我们将要解决的问题是一个情感分析任务。我们的目标是微调一个模型，使其成为一个专家级的二元分类器，将情感分为“正面”和“负面”。
- en: Obtaining the dataset
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取数据集
- en: The first ingredient that we need is the training dataset. For this purpose,
    I will leverage the datasets library available in Hugging Face to load a binary
    classification dataset called IMDB (you can find the dataset card at [https://huggingface.co/datasets/imdb](https://huggingface.co/datasets/imdb)).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的第一个要素是训练数据集。为此，我将利用Hugging Face中可用的数据集库来加载一个名为IMDB的二分类数据集（你可以在[https://huggingface.co/datasets/imdb](https://huggingface.co/datasets/imdb)找到数据集卡片）。
- en: 'The dataset contains movie reviews, which are classified as positive or negative.
    More specifically, the dataset contains two columns:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含电影评论，这些评论被分类为正面或负面。更具体地说，数据集包含两列：
- en: 'Text: The raw text movie review.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本：原始的电影评论。
- en: 'Label: The sentiment of that review. It is mapped as “0” for “Negative” and
    “1” for “Positive.”'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签：该评论的情感。它被映射为“0”表示“负面”，而“1”表示“正面”。
- en: As it is a **supervised learning** problem, the dataset already comes with 25,000
    rows for the training set and 25,000 rows for the validation set.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个**监督学习**问题，数据集已经包含了用于训练集的25,000行和用于验证集的25,000行。
- en: '**Definition**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: Supervised learning is a type of machine learning that uses labeled datasets
    to train algorithms to classify data or predict outcomes accurately. Labeled datasets
    are collections of examples that have both input features and desired output values,
    also known as labels or targets. For example, a labeled dataset for handwriting
    recognition might have images of handwritten digits as input features and the
    corresponding numerical values as labels.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习是一种机器学习方法，它使用有标签的数据集来训练算法以准确地对数据进行分类或预测结果。有标签的数据集是包含输入特征和期望输出值的示例集合，也称为标签或目标。例如，用于手写识别的有标签数据集可能包含手写数字的图像作为输入特征，以及相应的数值作为标签。
- en: Training and validation sets are subsets of the labeled dataset that are used
    for different purposes in the supervised learning process. The training set is
    used to fit the parameters of the model, such as the weights of the connections
    in a neural network. The validation set is used to tune the hyperparameters of
    the model, such as the number of hidden units in a neural network or the learning
    rate. Hyperparameters are settings that affect the overall behavior and performance
    of the model but are not directly learned from the data. The validation set helps
    to select the best model among different candidates by comparing their accuracy
    or other metrics on the validation set.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集和验证集是有标签数据集的子集，在监督学习过程中用于不同的目的。训练集用于拟合模型的参数，例如神经网络中连接的权重。验证集用于调整模型的超参数，例如神经网络中的隐藏单元数量或学习率。超参数是影响模型整体行为和性能的设置，但不是直接从数据中学习的。验证集通过比较不同候选模型在验证集上的准确度或其他指标来帮助选择最佳模型。
- en: Supervised learning differs from another type of machine learning, which is
    **unsupervised learning**. With the latter, the algorithm is tasked with finding
    patterns, structures, or relationships in a dataset without the presence of labeled
    outputs or targets. In other words, in unsupervised learning, the algorithm is
    not provided with specific guidance or labels to direct its learning process.
    Instead, it explores the data and identifies inherent patterns or groupings on
    its own.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习与另一种机器学习方法不同，即**无监督学习**。在后一种学习中，算法的任务是在没有标记输出或目标的情况下，在数据集中寻找模式、结构或关系。换句话说，在无监督学习中，算法没有提供具体的指导或标签来指导其学习过程。相反，它自行探索数据，并识别内在的模式或分组。
- en: 'You can download the IMDB dataset by running the following code:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过运行以下代码下载IMDB数据集：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Hugging Face datasets come with a dictionary schema, which is as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face数据集附带一个字典模式，如下所示：
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To access one observation of a particular Dataset object (for example, `train`),
    you can use slicers, as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问特定数据集对象（例如，`train`）的一个观测值，你可以使用切片器，如下所示：
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This gives us the following output:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下输出：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: So, the 101st observation of the training set contains a review labeled as negative.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，训练集的第101个观测值包含一个标记为负面的评论。
- en: Now that we have the dataset, we need to preprocess it so that can be used to
    train our LLM. To do so, we need to tokenize the provided text, and we will discuss
    this in the next section.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了数据集，我们需要对其进行预处理，以便可以用于训练我们的LLM。为此，我们需要对提供的文本进行分词，我们将在下一节中讨论这个问题。
- en: Tokenizing the data
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据分词
- en: A tokenizer is a component that is responsible for splitting a text into smaller
    units, such as words or subwords, that can be used as inputs for an LLM. Tokenizers
    can be used to encode text efficiently and consistently, as well as to add special
    tokens, such as mask or separator tokens, that are required by some models.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器是一个负责将文本分割成更小单元（如单词或子词）的组件，这些单元可以用作LLM的输入。分词器可以用来高效且一致地编码文本，以及添加一些特殊标记，如掩码或分隔符标记，这些标记是某些模型所必需的。
- en: Hugging Face provides a powerful utility called AutoTokenizer, available in
    the Hugging Face Transformers library, that offers tokenizers for various models,
    such as BERT and GPT-2\. It serves as a generic tokenizer class that dynamically
    selects and instantiates the appropriate tokenizer based on the pretrained model
    you specify.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face提供了一个强大的实用工具，称为AutoTokenizer，它包含在Hugging Face Transformers库中，为各种模型（如BERT和GPT-2）提供分词器。它作为一个通用分词器类，根据你指定的预训练模型动态选择和实例化适当的分词器。
- en: 'The following code snippet shows how we can initialize our tokenizer:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了我们如何初始化我们的分词器：
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that we picked a specific tokenizer called `bert-base-cased`. In fact,
    there is a link between a tokenizer and an LLM, in the sense that the the tokenizer
    prepares the inputs for the model by converting the text into numerical IDs that
    the model can understand.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们选择了一个名为`bert-base-cased`的特定分词器。实际上，分词器和LLM之间存在联系，即分词器通过将文本转换为模型可以理解的数值ID来准备模型的输入。
- en: '**Definition**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: The input IDs are the numerical IDs that correspond to the tokens in the vocabulary
    of the tokenizer. They are returned by the tokenizer function when encoding a
    text input. The input IDs are used as inputs for the model, which expects numerical
    tensors rather than strings. Different tokenizers may have different input IDs
    for the same tokens, depending on their vocabulary and tokenization algorithm.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 输入ID是与分词器词汇表中的标记相对应的数值ID。当编码文本输入时，分词器函数会返回这些输入ID。输入ID被用作模型的输入，模型期望的是数值张量而不是字符串。不同的分词器可能对相同的标记有不同的输入ID，这取决于它们的词汇表和分词算法。
- en: 'Different models may use different tokenization algorithms, such as word-based,
    character-based, or subword-based. Therefore, it is important to use the correct
    tokenizer for each model, otherwise the model may not perform well or even produce
    errors. Let’s look at potential scenarios for each:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的模型可能使用不同的分词算法，如基于词、基于字符或基于子词。因此，为每个模型使用正确的分词器非常重要，否则模型可能表现不佳，甚至产生错误。让我们看看每种可能的场景：
- en: A character-based approach might fit scenarios that deal with rare words or
    languages with complex morphological structures, or when dealing with spelling
    correction tasks
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于字符的方法可能适合处理罕见单词或具有复杂形态结构语言的场景，或者当处理拼写纠正任务时。
- en: The word-based approach might be a good fit for scenarios like NER, sentiment
    analysis, and text classification
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于单词的方法可能适合像命名实体识别（NER）、情感分析和文本分类这样的场景。
- en: The sub-word approach interpolates between the previous two, and it is useful
    when we want to balance the granularity of text representation with efficiency.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子词方法介于前两种方法之间，当我们需要平衡文本表示的粒度与效率时很有用。
- en: As we will see in the next section, we will leverage the **BERT** model for
    this scenario, hence we loaded its pretrained tokenizer (which is a word-based
    tokenizer powered by an algorithm called WordPiece).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将利用**BERT**模型进行此场景，因此我们加载了其预训练的标记器（这是一个由WordPiece算法驱动的基于单词的标记器）。
- en: 'We now need to initialize `tokenize_function`, which will be used to format
    the dataset:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要初始化`tokenize_function`，它将被用来格式化数据集：
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, we also configured the **padding** and **truncation** of `tokenize_function`
    to ensure an output with the right sizing for our BERT model.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们还配置了`tokenize_function`的**填充**和**截断**，以确保输出适合我们BERT模型的大小。
- en: '**Definition**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: Padding and truncation are two techniques that are used to make the input sequences
    of text have the same length. This is often required for some **natural language
    processing** (**NLP**) models, such as the BERT model, that expect fixed-length
    inputs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 填充和截断是两种技术，用于使文本输入序列具有相同的长度。这对于一些期望固定长度输入的自然语言处理（NLP）模型，如BERT模型，通常是必需的。
- en: 'Padding means adding some special tokens, usually zeros, at the end or the
    beginning of a sequence to make it reach the desired length. For example, if we
    have a sequence of length 5 and we want to pad it to a length of 8, we can add
    3 zeros at the end, like this: [1, 2, 3, 4, 5, 0, 0, 0]. This is called post-padding.
    Alternatively, we can add 3 zeros at the beginning, like this: [0, 0, 0, 1, 2,
    3, 4, 5]. This is called pre-padding. The choice of padding strategy depends on
    the model and the task.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 填充意味着在序列的末尾或开头添加一些特殊标记，通常是零，以使其达到所需的长度。例如，如果我们有一个长度为5的序列，而我们想将其填充到长度为8，我们可以在末尾添加3个零，如下所示：[1,
    2, 3, 4, 5, 0, 0, 0]。这被称为后填充。或者，我们可以在开头添加3个零，如下所示：[0, 0, 0, 1, 2, 3, 4, 5]。这被称为前填充。填充策略的选择取决于模型和任务。
- en: 'Truncation means removing some tokens from a sequence to make it fit the desired
    length. For example, if we have a sequence of length 10 and we want to truncate
    it to a length of 8, we can remove 2 tokens from the end or the beginning of the
    sequence. For example, we can remove the last 2 tokens, like this: [1, 2, 3, 4,
    5, 6, 7, 8]. This is called post-truncation. Alternatively, we can remove the
    first 2 tokens, like this: [3, 4, 5, 6, 7, 8, 9, 10]. This is called pre-truncation.
    The choice of truncation strategy also depends on the model and the task.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 截断意味着从序列中删除一些标记以使其符合所需的长度。例如，如果我们有一个长度为10的序列，而我们想将其截断到长度为8，我们可以从序列的末尾或开头删除2个标记。例如，我们可以删除最后的2个标记，如下所示：[1,
    2, 3, 4, 5, 6, 7, 8]。这被称为后截断。或者，我们可以删除前2个标记，如下所示：[3, 4, 5, 6, 7, 8, 9, 10]。这被称为前截断。截断策略的选择也取决于模型和任务。
- en: 'Now, we can apply the function to our dataset and inspect the numerical IDs
    of one entry:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将函数应用于我们的数据集并检查一个条目的数值ID：
- en: '[PRE6]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here is our output:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的输出：
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you can see, the last elements of the vector are zeroes, due to the `padding='max_lenght'`
    parameter passed to the function.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，由于传递给函数的`padding='max_length'`参数，向量的最后元素是零。
- en: 'Optionally, you can decide to reduce the size of your dataset if you want to
    make the training time shorter. In my case, I’ve shrunk the dataset as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，如果您想缩短训练时间，您可以决定减小数据集的大小。在我的情况下，我已经将数据集缩小如下：
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: So, I have two sets – one for training, one for testing – of 500 observations
    each. Now that we have our dataset preprocessed and ready, we need the model to
    be fine-tuned.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我有两个集合——一个用于训练，一个用于测试——每个集合包含500个观测值。现在我们已经预处理并准备好了数据集，我们需要微调模型。
- en: Fine-tuning the model
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调模型
- en: As anticipated in the previous section, the LLM we are going to leverage for
    fine-tuning is the base version of BERT. The BERT model is a transformer-based,
    encoder-only model for natural language understanding introduced by Google researchers
    in 2018\. BERT was the first example of a general-purpose LLM, meaning that it
    was the first model to be able to tackle multiple NLP tasks at once, which was
    different from the task-specific models existing up to that moment.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将用于微调的LLM是BERT的基础版本。BERT模型是由谷歌研究人员在2018年引入的基于transformer的、仅编码器的自然语言理解模型。BERT是第一个通用LLM的例子，这意味着它是第一个能够同时处理多个NLP任务的模型，这与当时存在的特定任务模型不同。
- en: Now, even though it might sound a bit “old fashioned” (in fact, compared to
    today’s model like the GPT-4, it is not even “large,” with only 340 million parameters
    in its large version), given all the new LLMs that have emerged in the market
    in the last few months, BERT and its fine-tuned variants are still a widely adopted
    architecture. In fact, it was thanks to BERT that the standard for language models
    has greatly improved.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，尽管这可能听起来有点“过时”（实际上，与今天的模型如GPT-4相比，它甚至不算“大”，其大型版本只有3.4亿个参数），鉴于过去几个月市场上涌现的所有新的LLM，BERT及其微调变体仍然是一种广泛采用的架构。事实上，正是由于BERT，语言模型的标准得到了极大的提升。
- en: 'The BERT model has two main components:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: BERT模型有两个主要组件：
- en: 'Encoder: The encoder consists of multiple layers of transformer blocks, each
    with a self-attention layer and a feedforward layer. The encoder takes as input
    a sequence of tokens, which are the basic units of text, and outputs a sequence
    of hidden states, which are high-dimensional vectors that represent the semantic
    information of each token.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器：编码器由多个transformer块层组成，每个块都有一个自注意力层和一个前馈层。编码器以一个标记序列作为输入，标记是文本的基本单元，并输出一个隐藏状态序列，这些状态是高维向量，代表每个标记的语义信息。
- en: 'Output layer: The output layer is task-specific and can be different depending
    on the type of task that BERT is used for. For example, for text classification,
    the output layer can be a linear layer that predicts the class label of the input
    text. For question answering, the output layer can be two linear layers that predict
    the start and end positions of the answer span in the input text.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层：输出层是特定于任务的，并且可能因BERT所用的任务类型而异。例如，对于文本分类，输出层可以是一个线性层，用于预测输入文本的类别标签。对于问答，输出层可以是两个线性层，用于预测输入文本中答案范围的开头和结尾位置。
- en: 'The number of layers and parameters of the model depends on the model version.
    In fact, BERT comes in two sizes: BERTbase and BERTlarge. The following illustration
    shows the difference between the two versions:'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的层数和参数数量取决于模型版本。实际上，BERT有两种大小：BERTbase和BERTlarge。以下插图显示了这两个版本之间的差异：
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_11_02.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  自动生成的描述](img/B21714_11_02.png)'
- en: 'Figure 11.2: A comparison between BERTbase and BERTlarge (source: [https://huggingface.co/blog/bert-101](https://huggingface.co/blog/bert-101))'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2：BERTbase和BERTlarge的比较（来源：[https://huggingface.co/blog/bert-101](https://huggingface.co/blog/bert-101))
- en: Later, other versions such as BERT-tiny, BERT-mini, BERT-small, and BERT-medium
    were introduced to reduce the computational cost and memory usage of BERT.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，为了降低BERT的计算成本和内存使用，引入了其他版本，如BERT-tiny、BERT-mini、BERT-small和BERT-medium。
- en: 'The model has been trained on a heterogeneous corpus of around 3.3 billion
    words, belonging to Wikipedia and Google’s BooksCorpus. The training phase involved
    two objectives:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在约33亿个单词的异构语料库上进行了训练，这些语料库属于维基百科和谷歌的BooksCorpus。训练阶段涉及两个目标：
- en: '**Masked language modeling** (**MLM**): MLM aims to teach the model to predict
    the original words that are randomly masked (replaced with a special token) in
    the input text. For example, given the sentence “He bought a new [MASK] yesterday,”
    the model should predict the word “car” or “bike” or something else that makes
    sense. This objective helps the model learn the vocabulary and the syntax of the
    language, as well as the semantic and contextual relations between words.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**掩码语言模型**（**MLM**）：MLM旨在教会模型预测输入文本中随机掩码（用特殊标记替换）的原始单词。例如，给定句子“他昨天买了一辆新的  **车**”，模型应该预测“车”、“自行车”或其他有意义的单词。这个目标有助于模型学习词汇、语法以及词语之间的语义和上下文关系。'
- en: '**Next sentence prediction** (**NSP**): NSP aims to teach the model to predict
    whether two sentences are consecutive or not in the original text. For example,
    given the sentences “She loves reading books” and “Her favorite genre is fantasy,”
    the model should predict that they are consecutive because they are likely to
    appear together in a text. However, given the sentences “She loves reading books”
    and “He plays soccer every weekend,” the model should predict that they are not
    consecutive because they are unlikely to be related. This objective helps the
    model learn the coherence and logic of the text, as well as the discourse and
    pragmatic relations between sentences.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下一句预测**（**NSP**）：NSP旨在教会模型预测两个句子在原文中是否连续。例如，给定句子“她喜欢读书”和“她最喜欢的类型是奇幻”，模型应该预测它们是连续的，因为它们很可能在文本中一起出现。然而，给定句子“她喜欢读书”和“他每个周末都踢足球”，模型应该预测它们不是连续的，因为它们不太可能相关。这个目标有助于模型学习文本的连贯性和逻辑，以及句子之间的语篇和语用关系。'
- en: By using these two objectives (on which the model is trained at the same time),
    the BERT model can learn general language knowledge that can be transferred to
    specific tasks, such as text classification, question answering, and NER. The
    BERT model can achieve better performance on these tasks than previous models
    that only use one direction of context or do not use pre-training at all. In fact,
    it has achieved state-of-the-art results on many benchmarks and tasks, such as
    **General Language Understanding Evaluation** (**GLUE**), **Stanford Question
    Answering Dataset** (**SQuAD**), and **Multi-Genre Natural Language Inference**
    (**MultiNLI**).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这两个目标（模型同时训练），BERT模型可以学习可以转移到特定任务（如文本分类、问答和命名实体识别）的通用语言知识。BERT模型在这些任务上的表现优于仅使用一个方向上下文或根本不使用预训练的先前模型。事实上，它在许多基准和任务上已经实现了最先进的成果，例如**通用语言理解评估**（**GLUE**）、**斯坦福问答数据集**（**SQuAD**）和**多体裁自然语言推理**（**MultiNLI**）。
- en: 'The BERT model is available – along with many fine-tuned versions –in the Hugging
    Face Hub. You can instantiate the model as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: BERT模型及其许多微调版本可在Hugging Face Hub中找到。您可以如下实例化模型：
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that `AutoModelForSequenceClassification` is a subclass of `AutoModel`,
    which can instantiate a model architecture suitable for sequence classification,
    such as text classification or sentiment analysis. It can be used for any task
    that requires a single label or a list of labels for each input sequence. In my
    case, I set the number of output labels equal to two since we are dealing with
    a binary classification problem.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`AutoModelForSequenceClassification`是`AutoModel`的一个子类，它可以实例化一个适合序列分类的模型架构，例如文本分类或情感分析。它可以用于任何需要为每个输入序列提供一个标签或标签列表的任务。在我的情况下，我将输出标签的数量设置为两个，因为我们处理的是一个二元分类问题。
- en: On the other hand, `AutoModel` is a generic class that can instantiate any model
    architecture from the library based on the pretrained model name or path. It can
    be used for any task that does not require a specific output format, such as feature
    extraction or language modeling.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`AutoModel`是一个通用类，可以根据预训练模型名称或路径从库中实例化任何模型架构。它可以用于任何不需要特定输出格式的工作，例如特征提取或语言建模。
- en: The final step before starting the training is to define the evaluation metrics
    we will need to understand how well our model will perform once fine-tuned.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练之前，定义我们将需要的评估指标是为了理解我们的模型一旦微调后表现如何。
- en: Using evaluation metrics
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用评估指标
- en: As we saw in *Chapter 1*, evaluating an LLM in its general-purpose application
    might be cumbersome. As those models are trained on unlabeled text and are not
    task-specific, but rather generic and adaptable given a user’s prompt, traditional
    evaluation metrics were not suitable anymore. Evaluating an LLM means, among other
    things, measuring its language fluency, its coherence, and its ability to emulate
    different styles depending on a user’s request.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在*第一章*中看到的，评估一个通用目的的LLM可能比较繁琐。因为这些模型是在未标记的文本上训练的，并且不是针对特定任务的，而是根据用户的提示具有通用性和适应性，传统的评估指标已经不再适用。评估一个LLM意味着，在众多事情中，测量其语言流畅性、连贯性以及根据用户请求模仿不同风格的能力。
- en: However, we also saw how an LLM can be used for very specific scenarios, as
    in our binary classification task. If this is the case, evaluation metrics boil
    down to those commonly used for that scenario.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们也看到了LLM如何用于非常具体的场景，就像我们的二分类任务一样。如果是这种情况，评估指标将简化为该场景常用的指标。
- en: '**Note**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: 'When it comes to more conversational tasks like summarization, Q&A, and retrieval-augmented
    generation, a new set of evaluation metrics needs to be introduced, often powered
    in turn by LLMs. Some of the most popular metrics are the following:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到更会话化的任务，如摘要、问答和检索增强生成时，需要引入一套新的评估指标，这些指标通常由LLM提供支持。其中一些最受欢迎的指标如下：
- en: 'Fluency: This assesses how naturally and smoothly the generated text reads.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流畅性：这评估生成文本的自然和流畅程度。
- en: 'Coherence: This evaluates the logical flow and connectivity of ideas within
    a text.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致性**：这评估文本中想法的逻辑流程和连通性。'
- en: 'Relevance: This measures how well the generated content aligns with the given
    prompt or context.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关性：这衡量生成内容与给定提示或上下文的一致性程度。
- en: 'GPT-similarity: This quantifies how closely the generated text resembles human-written
    content.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT相似度：这量化了生成文本与人类撰写内容的相似程度。
- en: 'Groundedness: This assesses whether the generated text is based on factual
    information or context.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真实性**：这评估生成文本是否基于事实信息或上下文。'
- en: These evaluation metrics help us understand the quality, naturalness, and relevance
    of LLM-generated text, guiding improvements and ensuring reliable AI assistance.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这些评估指标帮助我们了解LLM生成文本的质量、自然度和相关性，指导改进并确保可靠的AI辅助。
- en: 'When it comes to binary classification, one of the most basic ways to evaluate
    a binary classifier is to use a confusion matrix. A confusion matrix is a table
    that shows how many of the predicted labels match the true labels. It has four
    cells:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到二元分类时，评估二元分类器最基本的方法之一是使用混淆矩阵。混淆矩阵是一个表格，显示了预测标签与真实标签匹配的数量。它有四个单元格：
- en: '**True positive** (**TP**): The number of cases where the classifier correctly
    predicted 1 when the true label was 1.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阳性** (**TP**): 当真实标签为1时，分类器正确预测1的案例数量。'
- en: '**False positive** (**FP**): The number of cases where the classifier incorrectly
    predicted 1 when the true label was 0.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性** (**FP**): 当真实标签为0时，分类器错误预测1的案例数量。'
- en: '**True negative** (**TN**): The number of cases where the classifier correctly
    predicted 0 when the true label was 0.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阴性** (**TN**): 当真实标签为0时，分类器正确预测0的案例数量。'
- en: '**False negative** (**FN**): The number of cases where the classifier incorrectly
    predicted 0 when the true label was 1.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阴性** (**FN**): 当真实标签为1时，分类器错误预测0的案例数量。'
- en: 'Here is an example of a confusion matrix for the sentiment classifier we are
    going to build, knowing that the label 0 is associated with “Negative” and the
    label 1 with “Positive”:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们将要构建的情感分类器的混淆矩阵的示例，知道标签0与“负面”相关，标签1与“正面”相关：
- en: '|  | **Predicted Positive** | **Predicted Negative** |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | **预测正面** | **预测负面** |'
- en: '| **Positive** | 20 (TP) | 5 (FN) |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| **正面** | 20 (TP) | 5 (FN) |'
- en: '| **Negative** | 3 (FP) | 72 (TN) |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| **负面** | 3 (FP) | 72 (TN) |'
- en: 'The confusion matrix can be used to calculate various metrics that measure
    different aspects of the classifier’s performance. Some of the most common metrics
    are:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵可以用来计算各种指标，这些指标衡量分类器性能的不同方面。其中一些最常见的指标是：
- en: '**Accuracy**: The proportion of correct predictions among all predictions.
    It is calculated as `(TP + TN) / (TP + FP + TN + FN)`. For example, the accuracy
    of the sentiment classifier is `(20 + 72) / (20 + 3 + 72 + 5) = 0.92`.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确度**：所有预测中正确预测的比例。它计算为 `(TP + TN) / (TP + FP + TN + FN)`。例如，情感分类器的准确度为 `(20
    + 72) / (20 + 3 + 72 + 5) = 0.92`。'
- en: '**Precision**: The proportion of correct positive predictions among all positive
    predictions. It is calculated as `TP / (TP + FP)`. For example, the precision
    of the sentiment classifier is `20 / (20 + 3) = 0.87`.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确度**：所有正预测中正确正预测的比例。它计算为 `TP / (TP + FP)`。例如，情感分类器的精确度为 `20 / (20 + 3) =
    0.87`。'
- en: '**Recall**: The proportion of correct positive predictions among all positive
    cases. It is also known as sensitivity or true positive rate. It is calculated
    as `TP / (TP + FN)`. For example, the recall of the sentiment classifier is `20
    / (20 + 5) = 0.8`.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**：所有正案例中正确正预测的比例。它也被称为灵敏度或真阳性率。它计算为 `TP / (TP + FN)`。例如，情感分类器的召回率为 `20
    / (20 + 5) = 0.8`。'
- en: '**Specificity**: The proportion of correct negative predictions among all negative
    cases. It is also known as the true negative rate. It is calculated as `TN / (TN
    + FP)`. For example, the specificity of the sentiment classifier is `72 / (72
    + 3) = 0.96`.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特异性**：所有负例中正确负预测的比例。它也被称为真正负率。计算公式为`TN / (TN + FP)`。例如，情感分类器的特异性为`72 / (72
    + 3) = 0.96`。'
- en: '**F1-score**: The harmonic mean of precision and recall. It is a measure of
    balance between precision and recall. It is calculated as `2 * (precision * recall)
    / (precision + recall)`. For example, the F1-score of the sentiment classifier
    is `2 * (0.87 * 0.8) / (0.87 + 0.8) = 0.83`.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F1分数**：精确率和召回率的调和平均值。它是精确率和召回率之间平衡的度量。计算公式为`2 * (precision * recall) / (precision
    + recall)`。例如，情感分类器的F1分数为`2 * (0.87 * 0.8) / (0.87 + 0.8) = 0.83`。'
- en: 'There are many other metrics that can be derived from the confusion matrix
    or other sources, such as the decision score or the probability output of the
    classifier. Some examples are:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从混淆矩阵或其他来源（如决策分数或分类器的概率输出）导出许多其他指标。以下是一些例子：
- en: '**Receiver operating characteristic** (**ROC**) **curve**: A plot of recall
    versus false positive rate (`FP / (FP + TN)`), which shows how well the classifier
    can distinguish between positive and negative cases at different thresholds.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**接收者操作特征**（**ROC**）**曲线**：召回率与假正率（`FP / (FP + TN)`）的图表，显示了分类器在不同阈值下区分正例和负例的能力。'
- en: '**Area under the ROC curve** (**AUC**): The AUC, which measures how well the
    classifier can rank positive cases higher than negative cases. It can be illustrated
    in the following diagram, where the ROC curve and the area under the curve are
    displayed:'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ROC曲线下的面积**（**AUC**）：AUC衡量分类器将正例排名高于负例的能力。它可以通过以下图表说明，其中显示了ROC曲线及其下的面积：'
- en: '![](img/B21714_11_03.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21714_11_03.png)'
- en: 'Figure 11.3: Illustration of a ROC curve, hightlighting a perfect classifier
    and the Area Under the Curve (AUC)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3：ROC曲线的示意图，突出显示完美分类器和曲线下的面积（AUC）
- en: 'In our case, we will simply use the accuracy metric by following these steps:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们将简单地通过以下步骤使用准确度指标：
- en: 'You can import this metric from the `evaluate` library as follows:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过以下方式从`evaluate`库导入此指标：
- en: '[PRE10]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We also need to define a function that computes the accuracy given the output
    of the training phase:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要定义一个函数，该函数根据训练阶段的输出计算准确度：
- en: '[PRE11]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, we need to set our evaluation strategy, which means how often we want
    our model to be tested against the test set while training:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要设置我们的评估策略，这意味着在训练过程中我们希望模型多久对测试集进行一次测试：
- en: '[PRE12]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In our case, we will set `epoch` as the evaluation strategy, meaning that the
    evaluation is done at the end of each epoch.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们将设置`epoch`作为评估策略，这意味着评估在每个epoch结束时进行。
- en: '**Definition**'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: An epoch is a term used in machine learning to describe one complete pass through
    the entire training dataset. It is a hyperparameter that can be tuned to improve
    the performance of a machine-learning model. During an epoch, the model’s weights
    are updated based on the training data and the loss function. An epoch can consist
    of one or more batches, which are smaller subsets of the training data. The number
    of batches in an epoch depends on the batch size, which is another hyperparameter
    that can be adjusted.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 一个epoch是机器学习中用来描述整个训练数据集完整遍历的术语。它是一个可以调整以改进机器学习模型性能的超参数。在epoch期间，模型的权重根据训练数据和损失函数进行更新。一个epoch可以包含一个或多个批次，这些批次是训练数据的小子集。epoch中批次的数量取决于批次大小，这也是另一个可以调整的超参数。
- en: Now we have all the ingredients needed to start our fine-tuning, which will
    be covered in the next section.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了开始微调所需的所有成分，这将在下一节中介绍。
- en: Training and saving
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练和保存
- en: 'The last component we need to fine-tune our model is a `Trainer` object. The
    `Trainer` object is a class that provides an API for feature-complete training
    and evaluation of models in PyTorch, optimized for Hugging Face Transformers.
    You can follow these steps:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要微调模型最后一个组件是一个`Trainer`对象。`Trainer`对象是一个类，它为PyTorch中模型的全功能训练和评估提供API，针对Hugging
    Face Transformers进行了优化。您可以按照以下步骤进行：
- en: 'Let’s first initialize our `Trainer` by specifying the parameters we’ve already
    configured in the previous steps. More specifically, the `Trainer` will need a
    model, some configuration args (such as the number of epochs), a training dataset,
    an evaluation dataset, and the type of evaluation metric to compute:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们通过指定在之前步骤中已经配置好的参数来初始化我们的`Trainer`。更具体地说，`Trainer`需要一个模型、一些配置参数（例如，epoch的数量）、一个训练数据集、一个评估数据集以及要计算的评估指标类型：
- en: '[PRE13]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You can then initiate the process of fine-tuning by calling the `trainer` as
    follows:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以通过以下方式启动微调过程：
- en: '[PRE14]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Depending on your hardware, the training process might take some time. In my
    case, given the reduced size of the dataset and the low number of epochs (only
    2), I don’t expect exceptional results. Nevertheless, the training results for
    only two epochs in terms of accuracy are the following:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的硬件，训练过程可能需要一些时间。在我的情况下，考虑到数据集规模较小和epoch数量较少（只有2个），我不期望有异常的结果。然而，仅就准确率而言，两个epoch的训练结果如下：
- en: '[PRE15]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see, between the two epochs the model gained an accuracy improvement
    of 41.38%, hitting a final accuracy of 82%. Considering the aforementioned elements,
    that’s not bad!
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在两个epoch之间，模型提高了41.38%的准确率，最终准确率达到82%。考虑到上述因素，这已经很不错了！
- en: 'Once the model is trained, we can save it locally, specifying the path as follows:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们就可以将其保存在本地，指定路径如下：
- en: '[PRE16]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To consume and test the model, you can load it with the following code:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要消费和测试模型，你可以使用以下代码加载它：
- en: '[PRE17]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, we need to test our model. To do so, let’s pass a sentence to the
    model (to be first tokenized) on which it can perform sentiment classification:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要测试我们的模型。为此，让我们将一个句子传递给模型（首先进行分词），该模型可以对其进行情感分类：
- en: '[PRE18]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This yields the following output:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE19]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that the model output is a `SequenceClassifierOutput` object, which is
    the base class for outputs of sentence classification models. Within this object,
    we are interested in the logit **tensor**, which is the vector of raw (non-normalized)
    predictions associated with labels that our classification model generated.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，模型输出是一个`SequenceClassifierOutput`对象，它是句子分类模型输出的基类。在这个对象中，我们感兴趣的是logit **张量**，这是我们分类模型生成的与标签关联的原始（非归一化）预测向量。
- en: 'Since we are working with tensors, we will need to leverage the `tensorflow`
    library in Python. Plus, we will use the `softmax` function to obtain the probability
    vector associated with each label, so that we know that the final result corresponds
    to the label with the greatest probability:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们正在处理张量，我们需要利用Python中的`tensorflow`库。此外，我们将使用`softmax`函数来获取与每个标签关联的概率向量，以便我们知道最终结果对应于概率最大的标签：
- en: '[PRE20]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following is the obtained output:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为获得的输出：
- en: '[PRE21]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Our model tells us that the sentiment of the sentence “I can’t stand it anymore”
    is negative, with a probability of 65.71%.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型告诉我们，句子“我再也忍受不了了”的情感是负面的，概率为65.71%。
- en: 'Note that you can also save the model in your Hugging Face account. To do so,
    you first need to allow the notebook to push the code to your account as follows:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，你还可以将模型保存在你的Hugging Face账户中。为此，你首先需要允许笔记本将代码推送到你的账户，如下所示：
- en: '[PRE22]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You will be prompted to the Hugging Face login page, where you have to input
    your access token. Then, you can save the model, specifying your account name
    and model name:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将被提示到Hugging Face登录页面，在那里你必须输入你的访问令牌。然后，你可以保存模型，指定你的账户名和模型名：
- en: '[PRE23]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'By doing so, this model can be consumed via the Hugging Face Hub as easily
    as we saw in the previous chapter, as shown in the following screenshot:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，这个模型可以通过Hugging Face Hub轻松消费，就像我们在上一章中看到的那样，如下面的截图所示：
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_11_04.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图，描述自动生成](img/B21714_11_04.png)'
- en: 'Figure 11.4: Model card within the Hugging Face Hub space'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4：Hugging Face Hub空间中的模型卡片
- en: Furthermore, you can also decide to make the model public, so that everyone
    within Hugging Face can test and consume your creation.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你也可以选择使模型公开，这样Hugging Face中的每个人都可以测试和消费你的创作。
- en: In this section, we fine-tuned a BERT model with just a few lines of code, thanks
    to Hugging Face libraries and accelerators. Again, if your goal is reducing the
    code amount, you can leverage the low-code AutoTrain platform hosted in Hugging
    Face to train and fine-tune models.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们仅用几行代码就微调了一个BERT模型，这得益于Hugging Face库和加速器。再次强调，如果你的目标是减少代码量，你可以利用Hugging
    Face上托管的低代码AutoTrain平台来训练和微调模型。
- en: Hugging Face is definitely a solid platform for training your open-source LLM.
    In addition to that, there are further platforms you might want to leverage since
    proprietary models can also be fine-tuned. For example, OpenAI lets you fine-tune
    the GPT series with your own data, providing the computational power to train
    and host your customized models.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face无疑是一个用于训练开源LLM的稳固平台。除此之外，还有其他平台你可能想要利用，因为专有模型也可以进行微调。例如，OpenAI允许你使用自己的数据对GPT系列进行微调，并提供训练和托管定制模型的计算能力。
- en: Overall, fine-tuning can be the icing on the cake that makes your LLM exceptional
    for your use case. Deciding a strategy to do so based on the framework we explored
    at the beginning is a pivotal step in building a successful application.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，微调可以是你LLM在特定用例中出类拔萃的点睛之笔。基于我们在一开始探讨的框架来决定微调的策略是构建成功应用的关键步骤。
- en: Summary
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered the process of fine-tuning LLMs. We started with
    a definition of fine-tuning and general considerations to take into account if
    you have to decide to fine-tune your LLM.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了微调LLM的过程。我们从一个微调的定义和如果你必须决定微调你的LLM时需要考虑的一般性考虑开始。
- en: We then went hands-on with practical sections on fine-tuning. We covered a scenario
    where, starting from a base BERT model, we wanted a powerful review sentiment
    analyzer. To do so, we fine-tuned the base model on the IMDB dataset using a full-code
    approach with Hugging Face Python libraries.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后亲自动手进行微调的实践部分。我们讨论了一个场景，即从基础BERT模型出发，我们希望得到一个强大的电影评论情感分析器。为此，我们使用Hugging
    Face Python库的全代码方法在IMDB数据集上对基础模型进行微调。
- en: Fine-tuning is a powerful technique to further customize LLMs toward your goal.
    However, along with many other aspects of LLMs, it comes with some concerns and
    considerations in terms of ethics and security. In the next chapter, we are going
    to delve deeper into that, sharing how to establish guardrails with LLMs and,
    more generally, how governments and countries are approaching the problem from
    a regulatory perspective.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是一种强大的技术，可以进一步定制LLM以符合你的目标。然而，与LLM的许多其他方面一样，它在伦理和安全方面也存在一些担忧和考虑。在下一章中，我们将深入探讨这一点，分享如何与LLM建立安全边界，以及更普遍地，各国政府是如何从监管角度来处理这个问题的。
- en: References
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Training dataset: [https://huggingface.co/datasets/imdb](https://huggingface.co/datasets/imdb)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据集：[https://huggingface.co/datasets/imdb](https://huggingface.co/datasets/imdb)
- en: 'HF AutoTrain: [https://huggingface.co/docs/autotrain/index](https://huggingface.co/docs/autotrain/index)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'HF AutoTrain: [https://huggingface.co/docs/autotrain/index](https://huggingface.co/docs/autotrain/index)'
- en: 'BERT paper: *Jacob Devlin*, *Ming-Wei Chang*, *Kenton Lee*, *Kristina Toutanova*,
    2019, *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*:
    [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'BERT论文：*Jacob Devlin*，*Ming-Wei Chang*，*Kenton Lee*，*Kristina Toutanova*，2019，*BERT:
    Pre-training of Deep Bidirectional Transformers for Language Understanding*: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)'
- en: Join our community on Discord
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的社区Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/llm](https://packt.link/llm)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/llm](https://packt.link/llm)'
- en: '![](img/QR_Code214329708533108046.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code214329708533108046.png)'
