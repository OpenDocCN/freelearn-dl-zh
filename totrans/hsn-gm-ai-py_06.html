<html><head></head><body>
        

                            
                    <h1 class="header-title">Exploring SARSA</h1>
                
            
            
                
<p>In this chapter, we continue with our focus on <strong>Temporal Difference Learning</strong> (<strong>TDL</strong>) and expand on it from TD (0) to multi-step TD and beyond. We will look at a new method of <strong>Reinforcement Learning</strong> (<strong>RL</strong>) called SARSA, explore what it is, and how it differs from Q-learning. From there, we will look at a few examples with new continual control learning environments from Gym. Then, we will move to a deeper understanding of TDL and introduce concepts called <strong>TD lambda</strong> (λ) and <strong>eligibility traces</strong>. Finally, we will finish off this chapter by looking at an example of SARSA.</p>
<p>For this chapter, we will extend our discussion of TDL and uncover <strong>State Action Reward State Action</strong> (<strong>SARSA</strong>), continuous action spaces, TD (λ), eligibility traces, and on-policy learning. Here is an overview of what we will cover in this chapter:</p>
<ul>
<li>Exploring SARSA on-policy learning</li>
<li>Using continuous spaces with SARSA</li>
<li>Extending continuous spaces</li>
<li>Working with TD (λ) and eligibility traces</li>
<li>Understanding SARSA (λ)</li>
</ul>
<p>This chapter is very much a continuation of <a href="bb05e528-e21b-4753-9e4c-372b8ed11e96.xhtml"/><a href="bb05e528-e21b-4753-9e4c-372b8ed11e96.xhtml">Chapter 4</a>, <em>Temporal Difference Learning</em>. Please read that chapter before this one. In the next section, we continue right where we left off in the last chapter.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Exploring SARSA on-policy learning</h1>
                
            
            
                
<p class="mce-root">SARSA, which is the process this method emulates. That is, the algorithm works by moving to a state, then choosing an action, receiving a reward, and then moving to the next state action. This makes SARSA an on-policy method, that is, the algorithm works by learning and deciding with the same policy. This differs from Q-learning, as we saw in <a href="bb05e528-e21b-4753-9e4c-372b8ed11e96.xhtml"/><a href="bb05e528-e21b-4753-9e4c-372b8ed11e96.xhtml"/><a href="bb05e528-e21b-4753-9e4c-372b8ed11e96.xhtml">Chapter 4</a>, <em>Temporal Difference Learning</em>, where Q is a form of off-policy learner.</p>
<p class="mce-root">The following diagram shows the difference in backup diagrams for Q-learning and SARSA:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-571 image-border" src="img/cdf4a700-7c4c-40b3-bfd1-59bfa0021c1c.png" style="width:21.83em;height:10.92em;"/></p>
<p>Backup diagrams for Q and SARSA</p>
<p>Recall that our Q-learner is an off-policy learner. That is, it requires the algorithm to update the policy or Q table offline and then later make decisions from that. However, if we want to tackle the TDL problem beyond one step or TD (0), then we need to have an on-policy learner. Our learning agent or algorithm must be able to update its policy in between whatever number of TD steps we may be looking at. This also requires us to update our Q update equation with a new SARSA update equation, as shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/29924182-4562-4609-a89f-9c2186148fa3.png" style="width:33.42em;height:1.50em;"/></p>
<p>Recall that our Q-learning equation was like so:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/550158be-5e57-49e7-ba97-21bc604ee66c.png" style="width:33.67em;height:2.00em;"/></p>
<p>In the previous equation, we have the following:</p>
<ul>
<li><img class="fm-editor-equation" src="img/4606930b-f445-4204-b857-b84ec0ae33a0.png" style="width:5.33em;height:1.25em;"/> The current state-action quality being updated</li>
<li><img style="font-size: 1em;color: #333333;width:6.25em;height:1.17em;" class="fm-editor-equation" src="img/3078a024-51a4-4f8b-9bfa-28230622897d.png"/> The learning rate</li>
<li><img style="font-size: 1em;color: #333333;width:3.75em;height:1.25em;" class="fm-editor-equation" src="img/4215b7a4-a539-4dd9-96e3-863b899564a6.png"/> The reward for the next state</li>
<li><img style="font-size: 1em;color: #333333;width:6.83em;height:0.92em;" class="fm-editor-equation" src="img/7322b8ce-8148-4521-b8cb-be1b4418663c.png"/> Gamma, the discount factor</li>
<li><sub><img style="color: #333333;width:8.08em;height:1.75em;" class="fm-editor-equation" src="img/ce011eeb-f862-4cb7-94a3-f1469aab8d2a.png"/></sub> The maximum best or greedy action</li>
</ul>
<p>We can further visualize this as shown in the following diagram:</p>
<div><img class="aligncenter size-full wp-image-572 image-border" src="img/63ee3993-33e5-4c08-b278-48e01c59b336.png" style="width:22.67em;height:14.25em;"/></div>
<p>SARSA diagram and equation</p>
<p>Notice how that funny <em>max</em> term is gone now in SARSA and we use the expectation now and not just the best. This has to do with the action selection strategy. If you recall in Q-learning, we always used the max or best action according to averaging rewards. Recall that Q-learning assumes that you average the maximum reward. Instead, we want to select the action the agent perceives to be the one that will return the best possible returns. Hopefully, you have also noticed how we have progressed from speaking about rewards to value, state actions, state values, and now returns, where a return represents the perceived value for an action. We will discuss maximizing returns in more detail later in this chapter. </p>
<p>In the next section, we will learn how to solve a new type of problem called <strong>continuous action spaces</strong>. Then, we will look at how to use SARSA to solve a new Gym environment.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using continuous spaces with SARSA</h1>
                
            
            
                
<p>Up until now, we have been exploring the <strong>finite Markov Decision Process</strong> or <strong>finite MDP</strong>. These types of problems are all well and good for simulation and toy problems, but they don't show us how to tackle real-world problems. Real-world problems can be broken down or discretized into finite MDPs, but real problems are not finite. Real problems are infinite, that is, they define no discrete simple states such as showering or having breakfast. Infinite MDPs model problems in what we call continuous space or continuous action space, that is, in problems where we think of a state as a single point in time and state defined as a slice of that time. Hence, the discrete task of <strong>eat breakfast</strong> could be broken down to each time step including individual chewing actions. </p>
<p class="mce-root"/>
<p>Solving an infinite MDP or continuous space problem is not trivial with our current toolset, but it will require us to apply discretization tricks. Applying discretization or breaking the continuous space into discrete spaces will make this problem solvable with our current toolset. In <a href="a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml">Chapter 6</a>, <em>Going Deep with DQN</em>, we will look to apply deep learning to a continuous action space, which allows us to solve these environments without using these discretization tricks.</p>
<p>Many continuous RL environments have more environmental states than atoms in the observable universe, and yes, that is a very big number. We have managed to solve these problems by applying deep learning and hence deep RL starting in <a href="a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml">Chapter 6</a>, <em>Going Deep with DQN</em>.</p>
<p>The code for this chapter was originally sourced from this GitHub repository: <a href="https://github.com/srnand/Reinforcement-Learning-using-OpenAI-Gym">https://github.com/srnand/Reinforcement-Learning-using-OpenAI-Gym</a>. It looks like the author, Shrinand Thakkar, has since moved on to other pursuits and did not complete this excellent work as he intended.</p>
<p>Open <kbd>Chapter_5_1.py</kbd> and follow the exercise shown here:</p>
<ol>
<li>The full source code for the listing is as follows:</li>
</ol>
<pre style="padding-left: 60px">import gym<br/>import math<br/>from copy import deepcopy<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/><br/>env = gym.make('MountainCar-v0')<br/>Q_table = np.zeros((20,20,3))<br/>alpha=0.3<br/>buckets=[20, 20]<br/>gamma=0.99<br/>rewards=[]<br/>episodes = 3000<br/><br/>def to_discrete_states(observation):<br/> interval=[0 for i in range(len(observation))]<br/> max_range=[1.2,0.07] <br/><br/> for i in range(len(observation)):<br/>  data = observation[i]<br/>  inter = int(math.floor((data + max_range[i])/(2*max_range[i]/buckets[i])))<br/> if inter&gt;=buckets[i]:<br/>   interval[i]=buckets[i]-1<br/>  elif inter&lt;0:<br/>   interval[i]=0<br/>  else:<br/>   interval[i]=inter<br/> return interval<br/><br/>def expect_epsilon(t):<br/>  return min(0.015, 1.0 - math.log10((t+1)/220.))<br/><br/>def expect_alpha(t):<br/>  return min(0.1, 1.0 - math.log10((t+1)/125.))<br/><br/>def get_action(observation,t):<br/> if np.random.random()&lt;max(0.001, expect_epsilon(t)):<br/>  return env.action_space.sample()<br/> interval = to_discrete_states(observation) <br/> return np.argmax(np.array(Q_table[tuple(interval)]))<br/><br/>def update_SARSA(observation,reward,action,ini_obs,next_action,t): <br/> interval = to_discrete_states(observation)<br/> Q_next = Q_table[tuple(interval)][next_action]<br/> ini_interval = to_discrete_states(ini_obs)<br/> Q_table[tuple(ini_interval)][action]+=max(0.4, expect_alpha(t))*(reward + gamma*(Q_next) - Q_table[tuple(ini_interval)][action])<br/><br/>for episode in range(episodes):<br/>  observation = env.reset() <br/>  t=0<br/>  done=False<br/>  while (done==False):<br/>    env.render()<br/>    print(observation)<br/>    action = get_action(observation,episode)<br/>    obs_next, reward, done, info = env.step(action)<br/>    next_action = get_action(obs_next,episode)<br/>    update_SARSA(obs_next,reward,action,observation,next_action,episode) <br/>    observation=obs_next<br/>    action = next_action<br/>    t+=1<br/>  rewards.append(t+1)   <br/>  <br/>plt.plot(rewards)<br/>plt.show()</pre>
<p class="mce-root"/>
<ol start="2">
<li>Moving past the imports, we will look at the hyperparameter initialization code:</li>
</ol>
<pre style="padding-left: 60px">env = gym.make('MountainCar-v0')<br/>Q_table = np.zeros((20,20,3))<br/>alpha=0.3<br/>buckets=[20, 20]<br/>gamma=0.99<br/>rewards=[]<br/>episodes = 3000</pre>
<ol start="3">
<li>We start the code block with the instantiation of a new environment, <kbd>MountainCar-v0</kbd>, which is in a continuous space environment. We then see the <kbd>Q_table</kbd> table is initialized with all zeros. Then, we set values for the learning rate, <kbd>alpha</kbd>; the discount factor, <kbd>gamma</kbd>; and the number of <kbd>episodes</kbd>. Also, we see a new list called <kbd>buckets</kbd> constructed. We will cover what buckets do shortly.</li>
<li>From there, jump to the end of the code. We want a high-level overview of what the code does first. Take a look at the episode <kbd>for</kbd> loop, shown here:</li>
</ol>
<pre style="padding-left: 60px">observation = env.reset() <br/>t=0<br/>done=False<br/>while (done==False):<br/> env.render()<br/> print(observation)<br/> action = get_action(observation,episode)<br/> obs_next, reward, done, info = env.step(action)<br/> next_action = get_action(obs_next,episode)<br/> update_SARSA(obs_next,reward,action,observation,next_action,episode)<br/>  observation=obs_next<br/>  action = next_action<br/>  t+=1<br/>rewards.append(t+1) </pre>
<ol start="5">
<li>The preceding code is the episode loop code and very much follows the pattern that we have seen in several previous chapters. The one major point of difference here is the way the algorithm/agent seemingly picks an action twice, as can be seen in the following block of code:</li>
</ol>
<pre style="padding-left: 60px">action = get_action(observation,episode)<br/>obs_next, reward, done, info = env.step(action)<br/>next_action = get_action(obs_next,episode)</pre>
<ol start="6">
<li>The difference from Q-learning here is that the agent in SARSA is on-policy, that is, the action it picks needs to also decide its next action. Recall, in Q-learning, the agent works off-policy, that is, it takes an action from a previously learned policy. Again, this also goes back to TD (0) or one step, where the algorithm is still only looking one step ahead.</li>
<li>At this point, let's run the algorithm to see how this works. A few examples of the car climbing the hill can be seen here:</li>
</ol>
<div><img class="aligncenter size-full wp-image-573 image-border" src="img/6216cdb5-09f4-4b27-8309-332948c02e30.png" style="width:43.17em;height:28.92em;"/></div>
<p>Example output from Chapter_5_1.py</p>
<p>From the preceding screenshot, we can see that the agent is climbing the hill. Let the agent continue climbing until it reaches the flag; it should almost get there. Now, this is cool and fairly powerful stuff, but even more so considering we can do this by assuming our infinite MDP (continuous space) is controllable in discrete steps and hence a finite MDP. To do that, we have to learn how to discretize a continuous action space and we will see how to do that in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Discretizing continuous state spaces</h1>
                
            
            
                
<p>RL is limited to discrete spaces or what we learned previously as a finite MDP. A finite MDP describes a discrete set of steps or states with an action to move between states decided by a probability. The infinite version of this may define an infinite number of state-actions between any set of states. Hence, a basketball player moving from one end of the court to score a basket describes an infinite MDP or continuous space. That is, for each point in time, the ball player could be in an infinite number of positions, dribbling or not dribbling, or shooting the ball and so on. Likewise, in the <kbd>MountainCar</kbd> environment, the car can be moving up or down the hill in either direction at any point in time. This makes the <kbd>MountainCar</kbd> environment a continuous state space, but just barely. Fortunately, we can use a clever trick to discretize the state space as follows:</p>
<div><img class="aligncenter size-full wp-image-863 image-border" src="img/dcb85386-c50e-4a98-b235-c5646f7333e5.png" style="width:35.50em;height:23.00em;"/></div>
<p>Example discretization of MountainCar</p>
<p>In the preceding diagram, we have overlaid a grid on top of the environment to represent state spaces the cart may be in. For the sample, a 4 x 4 grid is used, but in our code, we will use a much larger grid. Doing this allows us to capture the position of the cart as if it was on a grid. Since the goal of this task is to move the cart up the hill, then discretizing the space by applying a gridding technique works quite well. In more complex continuous spaces, your grid may represent multiple dimensions in space or across other features. Fortunately, we won't have to worry about those complex mathematics when we learn how to apply deep learning to continuous spaces.</p>
<p class="mce-root"/>
<p>Now that we understand how the space is discretized, let's jump back to the sample code in <kbd>Chapter_5_1.py</kbd> and review how this works in the following exercise:</p>
<ol>
<li>We will start by picking up where we last left off. At the last point, we were just updating the <kbd>Q_table</kbd> table with the following line inside the episode <kbd>for</kbd> loop:</li>
</ol>
<pre style="padding-left: 60px">update_SARSA(obs_next,reward,action,observation,next_action,episode)</pre>
<ol start="2">
<li>This calls the <kbd>update_SARSA</kbd> function, shown here:</li>
</ol>
<pre style="padding-left: 60px">def update_SARSA(observation,reward,action,ini_obs,next_action,t):  <br/>  interval = <strong>to_discrete_states(observation)</strong> <br/>  Q_next = Q_table[tuple(interval)][next_action]<br/>  ini_interval = <strong>to_discrete_states(ini_obs)</strong><br/>  Q_table[tuple(ini_interval)][action]+=max(0.4, expect_alpha(t))*(reward + gamma*(Q_next) - Q_table[tuple(ini_interval)][action])</pre>
<ol start="3">
<li>For now, ignore the <kbd>Q_table</kbd> update code and instead focus on the highlighted calls to <kbd>to_discrete_states</kbd>. These calls take an observation as input. An observation denotes the cart's absolute position in the <em>x,y</em> coordinates. This is where we discretize the state using the following function:</li>
</ol>
<pre style="padding-left: 60px">def to_discrete_states(observation):<br/> interval=[0 for i in range(len(observation))]<br/> max_range=[1.2,0.07] <br/> for i in range(len(observation)):<br/>   data = observation[i]<br/>   inter = int(math.floor((data + max_range[i])/(2*max_range[i]/buckets[i])))<br/>   if inter&gt;=buckets[i]:<br/>     interval[i]=buckets[i]-1<br/>   elif inter&lt;0:<br/>     interval[i]=0<br/>   else:<br/>     interval[i]=inter<br/> return interval</pre>
<ol start="4">
<li>The <kbd>to_discrete_states</kbd> function returns the grid interval the cart is currently in. Back in the <kbd>update_SARSA</kbd> function, we change the interval list back to a tuple with the following:</li>
</ol>
<pre style="padding-left: 60px">tuple(interval)</pre>
<ol start="5">
<li>Run the sample as you normally would again, just to confirm it works as expected.</li>
</ol>
<p class="mce-root"/>
<p>This simple method of discretization works well for this task but can quickly fall down or become overtly complex depending on the complexity of the environment. Before we move on to other matters, we want to return and look at how we update <kbd>Q_table</kbd> with SARSA in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Expected SARSA</h1>
                
            
            
                
<p>Vanilla SARSA is quite similar to Q-learning in terms of how we choose values. It will generally just use an epsilon-greedy max action strategy, not unlike what we used previously; however, what we find, especially when working on-policy, is that the algorithm needs to be more selective. Now, this is very much the goal of all RL, but, in this particular case, we manage these trade-offs a bit better by introducing an expectation. When we combine this with SARSA, we call it <strong>expected SARSA</strong>.</p>
<p>In expected SARSA, we assume an unknown learning rate alpha, and hence an unknown exploration rate epsilon as well. Instead, we equate the learning rate alpha and exploration rate epsilon using functions based on assigned rewards. We assign a reward of one timepoint for each time step and then calculate the new alpha and epsilon based on those. Open <kbd>Chapter_5_2.py</kbd> back up and let's see how this works by following the exercise here:</p>
<ol>
<li>The two functions of code we are interested in are shown here:</li>
</ol>
<pre style="padding-left: 60px">def expect_epsilon(t):<br/>  return min(0.015, 1.0 - math.log10((t+1)/220.))<br/><br/>def expect_alpha(t):<br/>  return min(0.1, 1.0 - math.log10((t+1)/125.))</pre>
<ol start="2">
<li>The two functions, <kbd>expect_epsilon</kbd> and <kbd>expect_alpha</kbd>, calculate an expectation or ratio based on the rewards returned so far, <kbd>t</kbd>, where <kbd>t</kbd> equals the total time the cart has been moving in the environment.</li>
<li>We can focus on how <kbd>expect_epsilon</kbd> is used by looking at the <kbd>get_action</kbd> function shown here:</li>
</ol>
<pre style="padding-left: 60px">def get_action(observation,t):  <br/>  if np.random.random()&lt;max(0.001, expect_epsilon(t)):<br/>    return env.action_space.sample()<br/>  interval = to_discrete_states(observation) <br/>  return np.argmax(np.array(Q_table[tuple(interval)]))</pre>
<ol start="4">
<li><kbd>get_action</kbd> returns the action based on the observation (<em>x</em> and <em>y</em> positions of the cart). It does this by first checking whether a random action is to be sampled or, instead, the best action. We determine the probability of this by using the <kbd>expect_epsilon</kbd> equation, which calculates epsilon based on the total episode time playing the environment. This effectively means the epsilon in this example will range between 0.001 and 0.0015; see whether you can figure that out in the code.</li>
<li>Next, we do something similar to calculate <kbd>alpha</kbd> shown in the <kbd>update_SARSA</kbd> function. The single line where this is used is shown again:</li>
</ol>
<pre style="padding-left: 60px">Q_table[tuple(ini_interval)][action]+=max(0.4, expect_alpha(t))*(reward + gamma*(Q_next) - Q_table[tuple(ini_interval)][action])</pre>
<ol start="6">
<li>The preceding code should be familiar by now as it looks like our regular policy update equation, except, in this instance, we are tuning the value for <kbd>alpha</kbd> using an expectation based on the current time on the task. You can also think of this in some ways as a secondary reward.</li>
<li>Run the code again and let it finish to completion. Notice the output as we will use that as a comparison soon:</li>
</ol>
<div><img class="aligncenter size-full wp-image-858 image-border" src="img/951548fc-9997-456b-8e53-026009355a53.png" style="width:30.25em;height:25.83em;"/></div>
<p>The output of returns/rewards over training time</p>
<p>The plot shows the accumulated rewards/time the cart spends in the environment. The cost is awarded a time reward for each time slice it remains in the environment, where if the cart remains stationary or relatively still for more than a few time slices, the episode is over. Therefore, the more time the cart stays in the environment also equates to more movement.</p>
<p>Continuous states or continuous observations are not the only things we need to concern ourselves when considering real-time problems. In the real world, we also deal with continuous action spaces as well. Currently, we have been looking at problems with discrete action spaces, that is, environments that use arbitrary discrete actions to control the agent. These actions are typically up, down, left, and right. However, for the real world, we need finer control and often categorize actions as turn left by amount <em>x</em> or right by amount <em>y</em>. By adding continuous action spaces, our RL algorithms become less robotic and provide finer control. Discretizing discrete action spaces into continuous action spaces is more difficult and not something we will concern ourselves with. Instead, we will look at how to convert another more popular continuous action space we use for deep RL in the following section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Extending continuous spaces </h1>
                
            
            
                
<p>Typically, we leave problems with large observation spaces to be tackled with deep learning. Deep learning, as we will learn, is very well-suited to such problems. However, deep learning is not without its own issues and it is sometimes prudent to try and solve an environment without deep learning. Now, not all environments will discretize well, as we mentioned previously, but we do want to look at another example. The next example we will look at is the infamous Cart Pole environment, which is almost always tackled with deep RL, primarily because it uses a continuous action space with four dimensions. Keep in mind that our previous observation spaces only had one dimension, and, in our last example, we only had two.</p>
<p>Being able to convert an agent's observation space can be a useful trick especially in more abstract game environments. Remember, good game mechanics are often more about being fun rather than accurate. This certainly applies to some AI elements in games. </p>
<p>You can find the specifics of the observation and state spaces by going to the environment's GitHub page if it has one. Most of the more popular environments have their own page. The <strong>Cart Pole</strong> and <strong>Mountain Car</strong> observation and action spaces are shown in the following excerpts:</p>
<div><img class="aligncenter size-full wp-image-576 image-border" src="img/1c95f106-1c22-4245-bcc0-04a35c90a30e.png" style="width:40.42em;height:26.50em;"/></div>
<p>Spaces of Mountain Car and Cart Pole environments</p>
<p>The preceding excerpts show a comparison of the <strong>Mountain Car</strong> versus the <strong>Cart Pole</strong> environments. Both environments use discrete action spaces, which is good. However, the <strong>Cart Pole</strong> environment uses a 4-dimensional observation space with values shown in the ranges in the table in the screenshot. This can be a little tricky and it will be helpful to understand how multidimensional observation spaces work in more detail.</p>
<p>Open <kbd>Chapter_5_3.py</kbd> and follow this exercise to see how our last example can be converted into <strong>Cart Pole</strong>:</p>
<ol>
<li>For the most part, the code is identical to the last two examples, so we only need to look at the differences. We will start with the environment construction section at the top, as follows:</li>
</ol>
<pre style="padding-left: 60px">env = gym.make('CartPole-v0')</pre>
<ol start="2">
<li>This constructs the infamous <strong>Cart Pole</strong> environment. Again, switching environments is easy but your code has to adapt to the observation and action spaces. <strong>Cart Pole</strong> and <strong>Mountain Car</strong> share the same observation/action space types. That is, its observation space is continuous but with a discrete action space.</li>
</ol>
<ol start="3">
<li>Next, we will look and see how this affects our <kbd>Q_table</kbd> table initialization with the code here:</li>
</ol>
<pre style="padding-left: 60px">Q_table = np.zeros((20,20,20,20,3))</pre>
<ol start="4">
<li>Notice how the table is now configured with four dimensions at size 20. Previously, this was just two dimensions of size 20. Go back and check the last code examples for comparison if you need to.</li>
<li>With more dimensions added to the <kbd>Q_table</kbd> table, that means we also need to add more dimensions to our discretization buckets, as shown here:</li>
</ol>
<pre style="padding-left: 60px">buckets=[20, 20, 20, 20]</pre>
<ol start="6">
<li>Again, we increase the <kbd>buckets</kbd> array from two dimensions to four, all of size <kbd>20</kbd>. We are arbitrarily using a size of 20 but we could use a larger or smaller value.</li>
<li>The last thing we need to do is redefine the boundaries of the environment's observations. Recall we were able to extract this information from the GitHub page. This is the table that shows the min/max values in the ranges. The line of code we are interested in is just inside the <kbd>to_discrete_states</kbd> function, as shown here:</li>
</ol>
<pre style="padding-left: 60px">def to_discrete_states(observation):<br/> interval=[0 for i in range(len(observation))]<br/> <strong>max_range=[2.4,999999, 41.8,999999]</strong></pre>
<ol start="8">
<li>The line is highlighted and declares the <kbd>max_range</kbd> variable. <kbd>max_range</kbd> sets the max value along each dimension in the observation space. We populate this with the values from the table and, in the case of infinity, we use six 9s (999999), which often works for the upper limits of values with infinity.</li>
<li>Next we, need to update the axis dimensions we use for indexing into the <kbd>Q_table</kbd> table, as shown in the code here:</li>
</ol>
<pre style="padding-left: 60px">Q_table[:,:,:,:,action]+=lr*td_error*(eligibility[:,:,:,:,action])</pre>
<ol start="10">
<li>In the preceding code, notice how we are now indexing to the four dimensions and the action.</li>
</ol>
<ol start="11">
<li>Run the code as you normally would and observe the output; an example is shown here:</li>
</ol>
<div><img class="aligncenter size-full wp-image-577 image-border" src="img/1deb404f-bcf1-472c-bbbc-72cda2095a13.png" style="width:44.67em;height:25.83em;"/></div>
<p>Example Chapter_5_3.py</p>
<p>Eventually, SARSA using a discretized observation space can solve the <kbd>CartPole</kbd> environment. This one may take a while to learn so be patient, but the agent will learn to balance the pole on the cart. You should have a fairly good understanding of how discretization works and SARSA at TD (0). In the next section, we will look at looking ahead/behind more than one step.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Working with TD (λ) and eligibility traces</h1>
                
            
            
                
<p>Up until now, we have looked at the forward view or what the agent perceives to be as the next best reward or state. In <strong>MC</strong>, we looked at the entire episode and then used those values to reverse calculate returns. For TDL methods such as Q-learning and SARSA, we looked a single step ahead or what we referred to as TD (0). However, we want our agents to be able to take into account several steps, <em>n</em>, in advance. If we can do this, then surely our agent will be able to make better decisions. </p>
<p>As we have seen previously, we can average returns across steps using a discount factor, gamma. However, at this point, we need to more careful about how we average or collect returns. Instead, we can define the averaging of all returns over an infinite number of steps forward as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/5b74115d-fb63-4f38-900c-1544016e227c.png" style="width:16.67em;height:4.00em;"/></p>
<p>In the preceding equation, we have the following:</p>
<ul>
<li><img class="fm-editor-equation" src="img/4f80d35d-500d-4f12-8e6b-5331bc40a819.png" style="width:2.50em;height:1.33em;"/> This is the weighted average of all returns.</li>
<li><img style="font-size: 1em;color: #333333;width:4.42em;height:1.33em;" class="fm-editor-equation" src="img/2267d3ca-2648-4c06-8402-c357416c3e78.png"/> This is the return of individual episodes from <em>t</em> to <em>t+n.</em></li>
<li><img style="font-size: 1em;color: #333333;width:1.92em;height:1.08em;" class="fm-editor-equation" src="img/c43f9843-b9df-4acd-bab4-82f9de0d26c5.png"/> Lambda, a weight value between [0,1].</li>
</ul>
<p>Since lambda is less than one, as values of <em>n</em> increase, the amount of contribution to the final average return becomes smaller. This is due to raising lambda (λ) to the power of <em>n</em> as in the preceding equation. Again, this is the same principle as using the discount factor, gamma. Now that we are thinking in terms of <em>n</em> steps or what we will refer to as lambda, we can revisit how this looks in the following diagram:</p>
<div><img class="aligncenter size-full wp-image-578 image-border" src="img/e8eeae23-6867-4dae-b031-7c4ae7f8e6c5.png" style="width:27.25em;height:26.00em;"/></div>
<p>Progression of TD (λ)</p>
<p>To find the general solution for <em>n</em> time steps, where <em>n</em> is an unknown we call lambda (λ), we need to determine a general solution for finding lambda, that is, the value of lambda that generalizes the problem. We can do that by first assuming that any episode will end at time step, <em>t</em>, and then rewriting our previous equation as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/91d7517b-2145-4403-8129-2417c549de74.png" style="width:19.75em;height:3.42em;"/></p>
<p>When a value  of 0 for lambda is used, this represents TD (0). A value of 1 for lambda represents MC or a need for a complete episode lookahead. However, it is complicated to implement this form of lookahead models and, intuitively, looking ahead is a very small part of what biological animals learn. In fact, our primary source of learning is experience, and that is exactly what we start to consider in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Backward views and eligibility traces</h1>
                
            
            
                
<p>Do you recall the last time you found a coin on the floor or street? After you picked up the coin, did you think to yourself: a) "I knew looking down all that time would pay off," or b) "Wow, I found a coin, how did I do that?" In fact, in most cases, it would be option <em>b</em>, that is, we learned something was good and then thought back to how we discovered it. The moment of brilliance in option <em>a</em> is akin to believing in quantum particles, atoms, and bacteria. This is no different in RL, and what we find is that it is often more useful to look back at what happened in the past; however, not so far back as to be a past event as in MC.</p>
<p>We can use TDL to take a backward look at the returns for several steps. However, we can't just use an absolute value for the state transitions. Instead, we need to determine the predicted error for each step back using the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/861c5097-ba1a-4a3e-9c13-5981e348933a.png" style="width:15.17em;height:1.42em;"/></p>
<p>In the preceding equation, we have the following:</p>
<ul>
<li><sub><img class="fm-editor-equation" src="img/416226c6-0388-4019-8e63-6a8526bd534f.png" style="width:2.17em;height:1.17em;"/></sub> This is the TD error or delta.</li>
<li><sub><img style="color: #333333;width:2.42em;height:1.08em;" class="fm-editor-equation" src="img/c9aab2e3-e3f9-4eb9-8d97-c56a31de0381.png"/></sub> The value function, which can be further defined by the following:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/53d9aed7-2b64-4394-9d80-619ceecbbba0.png" style="width:12.17em;height:1.42em;"/></p>
<p>We can further define the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/2a0de061-f7eb-4694-a5c6-ded805cd9141.png" style="width:4.75em;height:1.33em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/6accd72f-5deb-4f13-a909-fdb806eac621.png" style="width:15.75em;height:1.42em;"/></p>
<p>In the preceding equation, <img class="fm-editor-equation" src="img/fc2406d8-23d4-48ac-9816-0c095a62b1dd.png" style="width:4.67em;height:1.08em;"/> assigns the full value of 1 when the state is at <em>s.</em></p>
<p><em>E</em> denotes the eligibility factor or the amount the value should be considered in the TD error. What is happening here is that the value function is being updated by the number of TD errors over <em>n</em> steps, but, instead of looking forward, we look backward. Much like all things in RL, it seems this has to be applied across several variations of algorithms. For <em>n</em> step TDL or TD (λ), we have three variations we concern ourselves with. They are Tabular TD (λ), SARSA (λ), and Q (λ). Each algorithm variation in pseudocode is shown in the following diagram:</p>
<div><img class="alignnone size-full wp-image-1022 image-border" src="img/0090720b-6a78-4a7a-97d2-90b2c9f4dac4.png" style="width:52.67em;height:16.17em;"/></div>
<p>TD (λ), SARSA (λ), and Q (λ)</p>
<p>Each algorithm has a slight variation in the way it calculates values and TD errors. In the next section, we will look at a full implementation of SARSA (λ) in code.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding SARSA (λ)</h1>
                
            
            
                
<p>We could, of course, implement TD (λ) using the tabular online method, which we haven't covered yet, or with Q-learning. However, since this is a chapter on SARSA, it only makes sense that we continue with that theme throughout. Open <kbd>Chapter_5_4.py</kbd> and follow the exercise:</p>
<ol>
<li>The code is quite similar to our previous examples, but let's review the full source code, as follows:</li>
</ol>
<pre style="padding-left: 60px">import gym<br/>import math<br/>from copy import deepcopy<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/><br/>env = gym.make('MountainCar-v0')<br/>Q_table = np.zeros((65,65,3))<br/>alpha=0.3<br/>buckets=[65, 65]<br/>gamma=0.99<br/>rewards=[]<br/>episodes=2000<br/>lambdaa=0.8<br/><br/>def to_discrete_states(observation):<br/> interval=[0 for i in range(len(observation))]<br/> max_range=[1.2,0.07] <br/> for i in range(len(observation)):<br/>  data = observation[i]<br/>  inter = int(math.floor((data + max_range[i])/(2*max_range[i]/buckets[i])))<br/>  if inter&gt;=buckets[i]:<br/>   interval[i]=buckets[i]-1<br/>  elif inter&lt;0:<br/>   interval[i]=0<br/>  else:<br/>   interval[i]=inter<br/> return interval<br/><br/>def expect_epsilon(t):<br/>  return min(0.015, 1.0 - math.log10((t+1)/220.))<br/><br/>def get_action(observation,t):<br/> if np.random.random()&lt;max(0.001, expect_epsilon(t)):<br/>  return env.action_space.sample()<br/> interval = to_discrete_states(observation)<br/> return np.argmax(np.array(Q_table[tuple(interval)]))<br/><br/>def expect_alpha(t):<br/>  return min(0.1, 1.0 - math.log10((t+1)/125.))<br/><br/>def updateQ_SARSA(observation,reward,action,ini_obs,next_action,t,eligibility):<br/> interval = to_discrete_states(observation)<br/> Q_next = Q_table[tuple(interval)][next_action]<br/> ini_interval = to_discrete_states(ini_obs)<br/> lr=max(0.4, expect_alpha(t))<br/> td_error=(reward + gamma*(Q_next) - Q_table[tuple(ini_interval)][action])<br/> Q_table[:,:,action]+=lr*td_error*(eligibility[:,:,action])<br/>for episode in range(episodes):<br/>  observation = env.reset()<br/>  t=0<br/>  eligibility = np.zeros((65,65,3))<br/>  done=False<br/>  while (done==False):<br/>    env.render()<br/>    action = get_action(observation,episode)<br/>    next_obs, reward, done, info = env.step(action)<br/>    interval = to_discrete_states(observation)<br/>    eligibility *= lambdaa * gamma<br/>    eligibility[tuple(interval)][action]+=1<br/>        <br/>    next_action = get_action(next_obs,episode)<br/>    updateQ_SARSA(next_obs,reward,action,observation,next_action,episode,eligibility)<br/>    observation=next_obs<br/>    action = next_action<br/>    t+=1<br/>  rewards.append(t+1)<br/>    <br/>plt.plot(rewards)<br/>plt.show()</pre>
<ol start="2">
<li>The top section of code is quite similar with some notable differences. Notice the initialization of the <kbd>MountainCar</kbd> environment and the <kbd>Q_table</kbd> table setup using the following code:</li>
</ol>
<pre style="padding-left: 60px">env = gym.make('MountainCar-v0')<br/>Q_table = np.zeros((65,65,3))</pre>
<ol start="3">
<li>Notice how we increase the number of discretized states from 20 x 20 to 65 x 65 as we initialize the <kbd>Q_table</kbd> table.</li>
<li>The next major difference now is the calculation of eligibility using lambda. We can find this code in the bottom episode <kbd>for</kbd> loop, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">env.render()<br/>action = get_action(observation,episode)        next_obs, reward, done, info = env.step(action)<br/>interval = to_discrete_states(observation)<br/><strong>eligibility *= lambdaa * gamma</strong><br/><strong>eligibility[tuple(interval)][action]+=1 </strong><br/>next_action = get_action(next_obs,episode)<br/>updateQ_SARSA(next_obs,reward,action,observation,next_action,episode,eligibility)<br/>observation=next_obs<br/>action = next_action<br/>t+=1</pre>
<ol start="5">
<li>The calculation for eligibility is done in the highlighted lines. Notice how we multiply <kbd>eligibility</kbd> by <kbd>lambda</kbd> and <kbd>gamma</kbd>, then add one for the current state. This value is then passed into the <kbd>update_SARSA</kbd> function, as follows:</li>
</ol>
<pre style="padding-left: 60px">def updateQ_SARSA(observation,reward,action,ini_obs,next_action,t,eligibility):<br/> interval = to_discrete_states(observation)<br/> Q_next = Q_table[tuple(interval)][next_action]<br/> ini_interval = to_discrete_states(ini_obs)<br/> lr=max(0.4, expect_alpha(t))<br/> td_error=(reward + gamma*(Q_next) - Q_table[tuple(ini_interval)][action])<br/> Q_table[:,:,action]+=lr*td_error*(eligibility[:,:,action])</pre>
<ol start="6">
<li>Notice how we now update the <kbd>Q_table</kbd> table based on a determination of <kbd>td_error</kbd> and <kbd>eligibility</kbd>. In other words, we take into consideration now how current the information is and how much it was valued in the past.</li>
</ol>
<ol start="7">
<li>Run the code example again as you normally would and watch the agent play the task. The training output for this task is shown in the following diagram:</li>
</ol>
<div><img class="aligncenter size-full wp-image-859 image-border" src="img/ca6f8852-14ba-4f90-bf02-da2fb54b8bcc.png" style="width:36.92em;height:31.92em;"/></div>
<p>Output plot of rewards for SARSA (λ)</p>
<p>It may take a few minutes to generate the plot shown in the preceding diagram, so please be patient. Be sure to note how this compares with the previous examples we already ran in this chapter. You did run all of the sample exercises to completion, right? Notice how the output of returns/rewards of the time on each episode increases quicker and converges quicker.</p>
<p>We want to look at one more complex example that puts our use of discretization to the extreme in the next example.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">SARSA lambda and the Lunar Lander</h1>
                
            
            
                
<p>As the algorithms we develop get more complicated, their capabilities also get more powerful. However, there are limits and it is important to understand the limits of any technology. To test those limits, we want to look at an example that pushes them. For this particular case, we will look at the Lunar Lander environment from Gym. This environment is modeled after the old classic arcade game of the same name, where the object is to land a lunar module on the surface of the moon. In this environment, the observation space is described in eight dimensions and the action space in four. As we will see, this can quickly go beyond our current computational limits.</p>
<p>The <kbd>LunarLander</kbd> environment requires the installation of a special module called <kbd>Box2D</kbd>. This is essentially a graphics package. </p>
<p>Follow the exercise in the next section to set up and run the advanced <kbd>Box2D</kbd> modules for Gym:</p>
<ul>
<li>Follow these steps for Windows (Anaconda):</li>
</ul>
<ol>
<li>Open an Anaconda Prompt as an administrator. Run the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong> conda install swig</strong></pre>
<p style="padding-left: 60px">SWIG is a requirement of <kbd>Box2D</kbd>.</p>
<ol start="2">
<li>Next, run the following command to install Box2D:</li>
</ol>
<pre style="padding-left: 60px"><strong>pip install box2d-py</strong><br/></pre>
<ul>
<li>Follow these steps for Mac/Linux (or Windows without Anaconda):</li>
</ul>
<ol>
<li>Open a Python shell and run the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>pip install gym[all]</strong></pre>
<ol start="2">
<li>If you encounter issues, consult the instructions for the Windows installation.</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>
<p>This installation now allows you to run all of the more advanced Box2D environments. These are far more game-like and interesting to train on as well. Open up <kbd>Chapter_5_5.py</kbd> and follow the exercise to set up and train SARSA on Lunar Lander:</p>
<ol>
<li>The source code for <kbd>Chapter_5_5.py</kbd> is almost identical to <kbd>Chapter_5_4.py</kbd> aside from minor differences in setting up the discrete states. We will first look at how we set up the <kbd>Q_table</kbd> table with the following code:</li>
</ol>
<pre style="padding-left: 60px">env = gym.make('LunarLander-v2')<br/>Q_table = np.zeros((5,5,5,5,5,5,5,5,4))</pre>
<ol start="2">
<li>Notice how we went from values of 65 steps down to 5. The last value denotes the action space size and this has gone from three in <kbd>MountainCar</kbd> to four for <kbd>LunarLander</kbd>. However, with eight dimensions, we have to be careful about the size of the array. Hence, we need to limit each step size to five, in this example.</li>
<li>Next, we initialize the <kbd>buckets</kbd> state:</li>
</ol>
<pre style="padding-left: 60px">buckets=[5,5,5,5,5,5,5,5]</pre>
<ol start="4">
<li>Again, initialized to a size of three for the eight dimensions.</li>
<li>Then, we set the <kbd>max_range</kbd> values for the maximum values we want our step to span, like so:</li>
</ol>
<pre style="padding-left: 60px">max_range=[100,100,100,100,100,100,100,100] </pre>
<ol start="6">
<li>We use a value of 100 here to denote some arbitrary max value. Altering or tweaking these values could improve training efficiency.</li>
<li>Next, we need to expand the <kbd>Q_table</kbd> indexing to include 8 dimensions, like so:</li>
</ol>
<pre style="padding-left: 60px">Q_table[:,:,:,:,:,:,:,:,action]+=lr*td_error*(eligibility[:,:,:,:,:,:,:,:,action])</pre>
<ol start="8">
<li>Be aware of the limits we are applying to the agent in this example. We are effectively making the agent see in big sections, where each section or axis feature is only divided into three slices. It is surprising how effective this method can be.</li>
<li>Run the sample and let it go to completion. Yes, this one will take a while but it is worth it. An example output from the Lunar Lander environment can be seen in the following diagram:</li>
</ol>
<div><img class="aligncenter size-full wp-image-860 image-border" src="img/ef2e7437-e1be-48ee-887e-ac6c42f2b973.png" style="width:57.08em;height:29.33em;"/></div>
<p>Example output from Chapter_5_5.py</p>
<p>In the last example, we briefly looked at using SARSA on another continuous observation space environment, the Lunar Lander. While it can be fun to play with these environments and see how our discretization can manage an infinite MDP adequately, it is time we moved on to using the big guns of deep learning to manage continuous observation spaces. From the output of rewards, we can see that the example does not converge at all. This is likely because the discretization is not fine enough; perhaps you can improve on that?</p>
<p>The discretization process in this example is not optimal and could certainly be improved upon with some DP methods.</p>
<p>Deep learning networks applied to RL allow us to tackle enormous continuous observation and action spaces. As such, discretization of spaces won't be needed regularly going forward but it can be a useful trick or advantage for simpler problems.</p>
<p>This completes this chapter and I encourage you to move on and explore the exercises to improve your own learning.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Exercises</h1>
                
            
            
                
<p>These exercises are here for you to use and learn from. Attempt at least 2-3, and the more you do, the easier later chapters will also be:</p>
<ol>
<li>What is the difference between an online and offline policy agent?</li>
<li>Tune the hyperparameters for any or all of the examples in this chapter, including the new hyperparameter, <kbd>lambda</kbd>.</li>
<li>Change the discretization steps in any example that uses discretization and see what effect it has on training.</li>
<li>Use example <kbd>Chapter_5_3.py</kbd>, <strong>SARSA(0)</strong>, and adapt it to another Gym environment that uses a continuous observation space and discrete action space.</li>
<li>Use example <kbd>Chapter_5_4.py</kbd>, <strong>SARSA(λ)</strong>, and adapt it to another Gym environment that uses a continuous observation space and discrete action space.</li>
<li>There is a hyperparameter shown in the code that is not used. Which parameter is it?</li>
<li>Use example <kbd>Chapter_5_5.py</kbd>, <strong>SARSA(λ)</strong>, Lunar Lander and optimize the discretization so that it performs better. For example, you are still limited by array dimensions but you can increase or decrease some more important dimensions.</li>
<li>Use example <kbd>Chapter_5_5.py</kbd>, <strong>SARSA(λ)</strong>, Lunar Lander and optimize the <kbd>max_range</kbd> values so that it performs better. For example, instead of setting all values to 999, check whether certain values can be narrowed or need expanding.</li>
<li>Update an example to work with a continuous action environment. This will require you to discrete the action space.</li>
<li>Convert one of the samples into Q-learning, that is, it uses an offline policy.</li>
</ol>
<p class="mce-root">Feel free to also explore more on your own. We barely scratched the surface of the intricacies of these methods. Finally, we come to our summary in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>For this chapter, we continued exploring TD learning. We looked at an example of an online TD (0) method called <strong>SARSA</strong>. Then, we looked at how we can discretize an observation space to tackle harder problems but still use the same toolset. From there, we looked at how we could tackle harder continuous space problems such as <kbd>CartPole</kbd>. After that, we revisited TDL and then looked to <em>n</em> step forward views, decided that was less than optimal, and then moved to backward views and eligibility traces, which led to us uncovering TD (λ), SARSA(λ), and Q (λ). Using SARSA(λ), we were able to solve the <kbd>MountainCar</kbd> environment in far less time. Finally, we wanted to tackle a far more difficult environment, <kbd>LunarLander</kbd> using SARSA(λ) without deep learning.</p>
<p>In the next chapter, we look at introducing deep learning and escalate ourselves to deep reinforcement learners.</p>


            

            
        
    </body></html>