<html><head></head><body>
  <div id="_idContainer194" class="Basic-Text-Frame">
    <h1 class="chapterNumber">9</h1>
    <h1 id="_idParaDest-219" class="chapterTitle">RAG Inference Pipeline</h1>
    <p class="normal">Back in <em class="chapterRef">Chapter 4</em>, we implemented the <strong class="keyWord">retrieval-augmented generation</strong> (<strong class="keyWord">RAG</strong>) feature<a id="_idIndexMarker817"/> pipeline to populate the vector <strong class="keyWord">database</strong> (<strong class="keyWord">DB</strong>). Within <a id="_idIndexMarker818"/>the feature pipeline, we gathered data from the data warehouse, cleaned, chunked, and embedded the documents, and, ultimately, loaded them to the vector DB. Thus, at this point, the vector DB is filled with documents and ready to be used for RAG.</p>
    <p class="normal">Based on the RAG methodology, you can split your software architecture into three modules: one for retrieval, one to augment the prompt, and one to generate the answer. We will follow a similar pattern by implementing a retrieval module to query the vector DB. Within this module, we will implement advanced RAG techniques to optimize the search. Afterward, we won’t dedicate a whole module to augmenting the prompt, as that would be overengineering, which we try to avoid. However, we will write an inference service that inputs the user query and context, builds the prompt, and calls the LLM to generate the answer. To summarize, we will implement two core Python modules, one for retrieval and one for calling the LLM using the user’s input and context as input. When we glue these together, we will have an end-to-end RAG flow.</p>
    <p class="normal">In <em class="italic">Chapters 5</em> and<em class="italic"> 6</em>, we fine-tuned our LLM Twin model, and in <em class="italic">Chapter 8</em>, we learned how to optimize it for inference. Thus, at this point, the LLM is ready for production. What is left is to build and deploy the two modules described above. </p>
    <p class="normal">We will dedicate the next chapter entirely to deploying our fine-tuned LLM Twin model to AWS SageMaker, as an AWS SageMaker inference endpoint. Thus, the focus of this chapter is to dig into the advanced RAG retrieval module implementation. We have dedicated a whole chapter to the retrieval step because this is where the magic happens in an RAG system. At the retrieval step (and not when calling the LLM), you write most of the RAG inference code. This step is where you have to wrangle your data to ensure that you retrieve the most relevant data points from the vector DB. Hence, most of the advanced RAG logic goes within the retrieval step.</p>
    <p class="normal">To sum up, in this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bulletList">Understanding the LLM Twin’s RAG inference pipeline</li>
      <li class="bulletList">Exploring the LLM Twin’s advanced RAG techniques</li>
      <li class="bulletList">Implementing the LLM Twin’s RAG inference pipeline</li>
    </ul>
    <p class="normal">By the end of this chapter, you will know how to implement an advanced RAG retrieval module, augment a prompt using the retrieved context, and call an LLM to generate the final answer. Ultimately, you will know how to build a production-ready RAG inference pipeline end to end.</p>
    <h1 id="_idParaDest-220" class="heading-1">Understanding the LLM Twin’s RAG inference pipeline</h1>
    <p class="normal">Before implementing the RAG inference pipeline, we want to discuss its software architecture <a id="_idIndexMarker819"/>and advanced RAG techniques. <em class="italic">Figure 9.1</em> illustrates an overview of the RAG inference flow. The inference pipeline starts with the input query, retrieves the context using the retrieval module (based on the query), and calls the LLM SageMaker service to generate the final answer.</p>
    <figure class="mediaobject"><img src="../Images/B31105_09_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.1: RAG inference pipeline architecture</p>
    <p class="normal">The feature pipeline and the retrieval module, defined in <em class="italic">Figure 9.1</em>, are independent<a id="_idIndexMarker820"/> processes. The feature pipeline runs on a different machine on a schedule to populate the vector DB. At the same time, the retrieval module is called on demand, within the inference pipeline, on every user request. </p>
    <p class="normal">By separating concerns between the two components, the vector DB is always populated with the latest data, ensuring feature freshness, while the retrieval module can access the latest features on every request. The input of the RAG retrieval module is the user’s query, based on which we have to return the most relevant and similar data points from the vector DB, which will be used to guide the LLM in generating the final answer.</p>
    <p class="normal">To fully understand <a id="_idIndexMarker821"/>the dynamics of the RAG inference pipeline, let’s go through the architecture flow from <em class="italic">Figure 9.1</em> step by step:</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">User query</strong>:<strong class="keyWord"> </strong>We begin <a id="_idIndexMarker822"/>with the user who makes a query, such as “Write an article about...”</li>
      <li class="numberedList"><strong class="keyWord">Query expansion</strong>:<strong class="keyWord"> </strong>We expand the initial query to generate multiple queries that reflect different aspects or interpretations of the original user query. Thus, instead of one query, we will use <em class="italic">xN</em> queries. By diversifying the search terms, the retrieval module increases the likelihood of capturing a comprehensive set of relevant data points. This step is crucial when the original query is too narrow or vague.</li>
      <li class="numberedList"><strong class="keyWord">Self-querying</strong>: We extract useful metadata from the original query, such as the author’s name. The extracted metadata will be used as filters for the vector search operation, eliminating redundant data points from the query vector space (making the search more accurate and faster).</li>
      <li class="numberedList"><strong class="keyWord">Filtered vector search</strong>: We embed each query and perform a similarity search to find each search’s top <em class="italic">K</em> data points. We execute xN searches corresponding to the number of expanded queries. We call this step a filtered vector search as we leverage the metadata extracted from the self-query step as query filters.</li>
      <li class="numberedList"><strong class="keyWord">Collecting results</strong>:<strong class="keyWord"> </strong>We get up to<em class="italic"> xK</em> results closest to its specific expanded query interpretation for each search operation. Further, we aggregate the results of all the xN searches, ending up with a list of <em class="italic">N</em> x <em class="italic">K</em> results containing a mix of articles, posts, and repositories chunks. The results include a broader set of potentially relevant chunks, offering multiple relevant angles based on the original query’s different facets.</li>
      <li class="numberedList"><strong class="keyWord">Reranking</strong>:<strong class="keyWord"> </strong>To keep only the top <em class="italic">K</em> most relevant results from the list of <em class="italic">N</em> x <em class="italic">K</em> potential items, we must filter the list further. We will use a reranking algorithm that scores each chunk based on the relevance and importance relative to the initial user query.<strong class="keyWord"> </strong>We will leverage a neural cross-encoder model to compute the score, a value between 0 and 1, where 1 means the result is entirely relevant to the query. Ultimately, we sort the <em class="italic">N</em> x <em class="italic">K</em> results based on the score and pick the top <em class="italic">K</em> items. Thus, the output is a ranked list of <em class="italic">K</em> chunks, with the most relevant data points situated at the top.</li>
      <li class="numberedList"><strong class="keyWord">Build the prompt and call the LLM</strong>:<strong class="keyWord"> </strong>We map the final list of the most relevant K chunks to a string used to build the final prompt. We create the prompt using a prompt template, the retrieved context, and the user’s query. Ultimately, the augmented prompt is sent to the LLM (hosted on AWS SageMaker exposed as an API endpoint).</li>
      <li class="numberedList"><strong class="keyWord">Answer</strong>: We are waiting for the answer to be generated. After the LLM processes <a id="_idIndexMarker823"/>the prompt, the RAG logic finishes by sending the generated response to the user.</li>
    </ol>
    <p class="normal">That wraps up the overview of the RAG inference pipeline. Now, let’s dig deeper into the details.</p>
    <h1 id="_idParaDest-221" class="heading-1">Exploring the LLM Twin’s advanced RAG techniques</h1>
    <p class="normal">Now that we <a id="_idIndexMarker824"/>understand the overall flow of our RAG inference pipeline, let’s explore the advanced RAG techniques we used in our retrieval module:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Pre-retrieval step</strong>: Query expansion and self-querying</li>
      <li class="bulletList"><strong class="keyWord">Retrieval step</strong>: Filtered vector search</li>
      <li class="bulletList"><strong class="keyWord">Post-retrieval step</strong>: Reranking</li>
    </ul>
    <p class="normal">Before digging into each method individually, let’s lay down the Python interfaces we will use in this section, which are available at <a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/base.py"><span class="url">https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/base.py</span></a>.</p>
    <p class="normal">The first is a prompt template factory that standardizes how we instantiate prompt templates. As an interface, it inherits from <code class="inlineCode">ABC</code> and exposes the <code class="inlineCode">create_template()</code> method, which returns a LangChain <code class="inlineCode">PromptTemplate</code> instance. Even if we avoid being heavily reliant on LangChain, as we want to implement everything ourselves to understand the engineering behind the scenes, some objects, such as the <code class="inlineCode">PromptTemplate</code> class, are helpful to speed up the development without hiding too much functionality:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> abc <span class="hljs-keyword">import</span> ABC, abstractmethod
<span class="hljs-keyword">from</span> langchain.prompts <span class="hljs-keyword">import</span> PromptTemplate
<span class="hljs-keyword">from</span> pydantic <span class="hljs-keyword">import</span> BaseModel
<span class="hljs-keyword">class</span> <span class="hljs-title">PromptTemplateFactory</span>(ABC, BaseModel):
<span class="hljs-meta">    @abstractmethod</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">create_template</span>(<span class="hljs-params">self</span>) -&gt; PromptTemplate:
        <span class="hljs-keyword">pass</span>
</code></pre>
    <p class="normal">We also want to <a id="_idIndexMarker825"/>define a <code class="inlineCode">RAGStep</code> interface used to standardize the interface of advanced RAG steps such as query expansion and self-querying. As these steps are often dependent on other LLMs, it has a <code class="inlineCode">mock</code> attribute to reduce costs and debugging time during development:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Any</span>
<span class="hljs-keyword">from</span> llm_engineering.domain.queries <span class="hljs-keyword">import</span> Query
<span class="hljs-keyword">class</span> <span class="hljs-title">RAGStep</span>(<span class="hljs-title">ABC</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, mock: </span><span class="hljs-built_in">bool</span><span class="hljs-params"> = </span><span class="hljs-literal">False</span>) -&gt; <span class="hljs-literal">None</span>:
        self._mock = mock
<span class="hljs-meta">    @abstractmethod</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">generate</span>(<span class="hljs-params">self, query: Query, *args, **kwargs</span>) -&gt; <span class="hljs-type">Any</span>:
        <span class="hljs-keyword">pass</span>
</code></pre>
    <p class="normal">Ultimately, we must understand how we modeled the <code class="inlineCode">Query</code> domain entity to wrap the user’s input with other metadata required for advanced RAG. Thus, let’s look at its implementation. First, we import the necessary classes:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> pydantic <span class="hljs-keyword">import</span> UUID4, Field
<span class="hljs-keyword">from</span> llm_engineering.domain.base <span class="hljs-keyword">import</span> VectorBaseDocument
<span class="hljs-keyword">from</span> llm_engineering.domain.types <span class="hljs-keyword">import</span> DataCategory
</code></pre>
    <p class="normal">Next, we define the <code class="inlineCode">Query</code> entity class, which inherits from the <code class="inlineCode">VectorBaseDocument</code> <strong class="keyWord">object-vector mapping</strong> (<strong class="keyWord">OVM</strong>) class, discussed in <em class="italic">Chapter 4</em>. Thus, each query <a id="_idIndexMarker826"/>can easily be saved or retrieved from the vector DB:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">Query</span>(<span class="hljs-title">VectorBaseDocument</span>):
    content: <span class="hljs-built_in">str</span>
    author_id: UUID4 | <span class="hljs-literal">None</span> = <span class="hljs-literal">None</span>
    author_full_name: <span class="hljs-built_in">str</span> | <span class="hljs-literal">None</span> = <span class="hljs-literal">None</span>
    metadata: <span class="hljs-built_in">dict</span> = Field(default_factory=<span class="hljs-built_in">dict</span>)
<span class="hljs-keyword">class</span> <span class="hljs-title">Config</span>:
        category = DataCategory.QUERIES
</code></pre>
    <p class="normal">What is essential to notice are the class’s attributes used to combine the user’s query with a bunch of metadata fields:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">content</code>: A string containing input query.</li>
      <li class="bulletList"><code class="inlineCode">author_id</code>: An optional UUID4 identifier extracted from the query used as a filter within the vector search operation to retrieve chunks written only by a specific author</li>
      <li class="bulletList"><code class="inlineCode">author_full_name</code>: An optional string used to query the <code class="inlineCode">author_id</code></li>
      <li class="bulletList"><code class="inlineCode">metadata</code>: A dictionary for any additional metadata, initialized as an empty <code class="inlineCode">dict</code> by default</li>
    </ul>
    <p class="normal">Besides the standard definition of a domain class, we also define a <code class="inlineCode">from_str()</code> class method to create a <code class="inlineCode">Query</code> instance directly from a string. This allows us to standardize how we clean the query string before constructing the <code class="inlineCode">query</code> object, such as stripping any leading or trailing whitespace and newline characters:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">    @classmethod</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">from_str</span>(<span class="hljs-params">cls, query: </span><span class="hljs-built_in">str</span>) -&gt; <span class="hljs-string">"Query"</span>:
        <span class="hljs-keyword">return</span> Query(content=query.strip(<span class="hljs-string">"\n "</span>))
</code></pre>
    <p class="normal">Additionally, there’s an instance method called <code class="inlineCode">replace_content()</code> used to create a new <code class="inlineCode">Query</code> instance with updated content while retaining the original query’s <code class="inlineCode">id</code>, <code class="inlineCode">author_id</code>, <code class="inlineCode">author_full_name</code>, and <code class="inlineCode">metadata</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span> <span class="hljs-title">replace_content</span>(<span class="hljs-params">self, new_content: </span><span class="hljs-built_in">str</span>) -&gt; <span class="hljs-string">"Query"</span>:
        <span class="hljs-keyword">return</span> Query(
            <span class="hljs-built_in">id</span>=self.<span class="hljs-built_in">id</span>,
            content=new_content,
            author_id=self.author_id,
            author_full_name=self.author_full_name,
            metadata=self.metadata,
        )
</code></pre>
    <p class="normal">This can be particularly<a id="_idIndexMarker827"/> useful when modifying the query text, for example, during preprocessing or normalization, without losing the associated metadata or identifiers. Following the <code class="inlineCode">Query</code> class, we define the <code class="inlineCode">EmbeddedQuery</code> class:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">EmbeddedQuery</span>(<span class="hljs-title">Query</span>):
    embedding: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">float</span>]
    <span class="hljs-keyword">class</span> <span class="hljs-title">Config</span>:
        category = DataCategory.QUERIES
</code></pre>
    <p class="normal">The <code class="inlineCode">EmbeddedQuery</code> class extends <code class="inlineCode">Query</code> by adding the embedding field. The <code class="inlineCode">EmbeddedQuery</code> entity encapsulates all the data and metadata necessary to perform vector search operations on top of Qdrant (or another vector DB).</p>
    <p class="normal">Now that we understand all the interfaces and new domain entities used within the RAG inference <a id="_idIndexMarker828"/>pipeline, let’s move on to our advanced RAG pre-retrieval optimization techniques.</p>
    <h2 id="_idParaDest-222" class="heading-2">Advanced RAG pre-retrieval optimizations: query expansion and self-querying</h2>
    <p class="normal">We implemented two<a id="_idIndexMarker829"/> methods to optimize the <a id="_idIndexMarker830"/>pre-retrieval optimization step: query expansion and self-querying. The two methods work closely with the filtered vector search step, which we will touch on in the next section. For now, however, we will start with understanding the code for query expansion and move to implementing self-querying.</p>
    <p class="normal">Within these two methods, we will leverage OpenAI’s API to generate variations of the original query within the query expansion step and to extract the necessary metadata within the self-querying algorithm. When we wrote this book, we used <code class="inlineCode">GPT-4o-mini</code> in all our examples, but as OpenAI’s models quickly evolve, the model might get deprecated. But that’s not an issue, as you can quickly change it in your <code class="inlineCode">.env</code> file by configuring the <code class="inlineCode">OPENAI_MODEL_ID</code> environment variable.</p>
    <h3 id="_idParaDest-223" class="heading-3">Query expansion</h3>
    <p class="normal">The <em class="italic">problem</em> in a typical<a id="_idIndexMarker831"/> retrieval step is that you query your vector DB using a single vector representation of your original question. This approach covers only a small area of the embedding space, which can be limiting. If the embedding doesn’t contain all the required information or nuances of your query, the retrieved context may not be relevant. This means essential documents that are semantically related but not near the query vector might be overlooked.</p>
    <p class="normal">The <em class="italic">solution</em> is based on query expansion, which offers a way to overcome this limitation. Using an LLM to generate multiple queries based on your initial question, you create various perspectives that capture different facets of your query. These expanded queries, when embedded, target other areas of the embedding space that are still relevant to your original question. This increases the likelihood of retrieving more relevant documents from the vector DB. </p>
    <p class="normal">Implementing query expansion can be as straightforward as crafting a detailed zero-shot prompt to guide the LLM in generating these alternative queries. Thus, after implementing query expansion, instead of having only one query to search relevant context, you will have xN queries, hence xN searches.</p>
    <p class="normal">Increasing the number of searches can impact your latency. Thus, you must experiment with the number of queries you generate to ensure the retrieval step meets your application requirements. You can also optimize the searches by parallelizing them, drastically<a id="_idIndexMarker832"/> reducing the latency, which we will do in the <code class="inlineCode">ContextRetriever</code> class <a id="_idIndexMarker833"/>implemented at the end of this chapter. </p>
    <div class="note">
      <p class="normal">Query expansion is also known as multi-query, but the principles are the same. For example, this is an example of LangChain’s implementation called <code class="inlineCode">MultiQueryRetriver</code>: <a href="https://python.langchain.com/docs/how_to/MultiQueryRetriever/"><span class="url">https://python.langchain.com/docs/how_to/MultiQueryRetriever/</span></a></p>
    </div>
    <p class="normal">Now, let’s dig into the code. We begin by importing the necessary modules and classes required for query expansion:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> langchain_openai <span class="hljs-keyword">import</span> ChatOpenAI
<span class="hljs-keyword">from</span> llm_engineering.domain.queries <span class="hljs-keyword">import</span> Query
<span class="hljs-keyword">from</span> llm_engineering.settings <span class="hljs-keyword">import</span> settings
<span class="hljs-keyword">from</span> .base <span class="hljs-keyword">import</span> RAGStep
<span class="hljs-keyword">from</span> .prompt_templates <span class="hljs-keyword">import</span> QueryExpansionTemplate
</code></pre>
    <p class="normal">Next, we define the <code class="inlineCode">QueryExpansion</code> class, which generates expanded query versions. The class implementation can be found at <a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/query_expanison.py"><span class="url">https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/query_expanison.py</span></a>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">QueryExpansion</span>(<span class="hljs-title">RAGStep</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title">generate</span>(<span class="hljs-params">self, query: Query, expand_to_n: </span><span class="hljs-built_in">int</span>) -&gt; <span class="hljs-built_in">list</span>[Query]:
        <span class="hljs-keyword">assert</span> expand_to_n &gt; <span class="hljs-number">0</span>, <span class="hljs-string">f"'expand_to_n' should be greater than 0. Got </span><span class="hljs-subst">{expand_to_n}</span><span class="hljs-string">."</span>
        <span class="hljs-keyword">if</span> self._mock:
            <span class="hljs-keyword">return</span> [query <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(expand_to_n)]
</code></pre>
    <p class="normal">In the <code class="inlineCode">generate</code> method, we first ensure that the number of expansions requested (<code class="inlineCode">expand_to_n</code>) is greater than zero. If the instance is in mock mode (<code class="inlineCode">self._mock is True</code>), it simply returns a list containing copies of the original query to simulate expansion without actually calling the API. If not in mock mode, we proceed <a id="_idIndexMarker834"/>to create <a id="_idIndexMarker835"/>the prompt and initialize the language model:</p>
    <pre class="programlisting code"><code class="hljs-code">        query_expansion_template = QueryExpansionTemplate()
        prompt = query_expansion_template.create_template(expand_to_n - <span class="hljs-number">1</span>)
        model = ChatOpenAI(model=settings.OPENAI_MODEL_ID, api_key=settings.OPENAI_API_KEY, temperature=<span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">Here, we instantiate <code class="inlineCode">QueryExpansionTemplate</code> and create a prompt tailored to generate <code class="inlineCode">expand_to_n - 1</code> new queries (excluding the original). We initialize the <code class="inlineCode">ChatOpenAI</code> model with the specified settings and set the temperature to 0 for deterministic output. We then create a LangChain chain by combining the prompt with the model and invoke it with the user’s question:</p>
    <pre class="programlisting code"><code class="hljs-code">        chain = prompt | model
        response = chain.invoke({<span class="hljs-string">"question"</span>: query})
        result = response.content
</code></pre>
    <p class="normal">By piping the prompt into the model (<code class="inlineCode">prompt | model</code>), we set up a chain that generates expanded queries when invoked with the original query. The response from the model is captured in the <code class="inlineCode">result</code> object. After receiving the response, we parse and clean the expanded queries:</p>
    <pre class="programlisting code"><code class="hljs-code">        queries_content = result.strip().split(query_expansion_template.separator)
        queries = [query]
        queries += [
            query.replace_content(stripped_content)
            <span class="hljs-keyword">for</span> content <span class="hljs-keyword">in</span> queries_content
            <span class="hljs-keyword">if</span> (stripped_content := content.strip())
        ]
        <span class="hljs-keyword">return</span> queries  
</code></pre>
    <p class="normal">We split the result using the separator defined in the template to get individual queries. Starting with<a id="_idIndexMarker836"/> a list <a id="_idIndexMarker837"/>containing the original query, we append each expanded query after stripping any extra whitespace.</p>
    <p class="normal">Finally, we define the <code class="inlineCode">QueryExpansionTemplate</code> class, which constructs the prompt used for query expansion. The class and other prompt templates can be accessed at <a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/prompt_templates.py"><span class="url">https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/prompt_templates.py</span></a>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> langchain.prompts <span class="hljs-keyword">import</span> PromptTemplate
<span class="hljs-keyword">from</span> .base <span class="hljs-keyword">import</span> PromptTemplateFactory
<span class="hljs-keyword">class</span> <span class="hljs-title">QueryExpansionTemplate</span>(<span class="hljs-title">PromptTemplateFactory</span>):
    prompt: <span class="hljs-built_in">str</span> = <span class="hljs-string">"""You are an AI language model assistant. Your task is to generate {expand_to_n}</span>
<span class="hljs-string">    different versions of the given user question to retrieve relevant documents from a vector</span>
<span class="hljs-string">    database. By generating multiple perspectives on the user question, your goal is to help</span>
<span class="hljs-string">    the user overcome some of the limitations of the distance-based similarity search.</span>
<span class="hljs-string">    Provide these alternative questions separated by '{separator}'.</span>
<span class="hljs-string">    Original question: {question}"""</span>
<span class="hljs-meta">    @property</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">separator</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">str</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-string">"#next-question#"</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">create_template</span>(<span class="hljs-params">self, expand_to_n: </span><span class="hljs-built_in">int</span>) -&gt; PromptTemplate:
        <span class="hljs-keyword">return</span> PromptTemplate(
            template=self.prompt,
            input_variables=[<span class="hljs-string">"question"</span>],
            partial_variables={
                <span class="hljs-string">"separator"</span>: self.separator,
                <span class="hljs-string">"expand_to_n"</span>: expand_to_n,
            },
        )
</code></pre>
    <p class="normal">This class defines a prompt instructing the language model to generate multiple versions of the user’s question. It uses placeholders like <code class="inlineCode">{expand_to_n}</code>, <code class="inlineCode">{separator}</code>, and <code class="inlineCode">{question}</code> to customize the prompt. </p>
    <p class="normal">It takes <code class="inlineCode">expand_to_n</code> as an input parameter to<a id="_idIndexMarker838"/> define how <a id="_idIndexMarker839"/>many queries we wish to generate while we build the <code class="inlineCode">PromptTemplate</code> instance. The separator property provides a unique string to split the generated queries. The <code class="inlineCode">expand_to_n</code> and <code class="inlineCode">separator</code> variables are passed as <code class="inlineCode">partial_variables</code>, making them immutable at runtime. Meanwhile, the <code class="inlineCode">{question}</code> placeholder will be changed every time the LLM chain is called.</p>
    <p class="normal">Now that we have finished studying the query expansion implementation, let’s look at an example of how to use the <code class="inlineCode">QueryExpansion</code> class. Let’s run the following code using this <code class="inlineCode">python -m llm_engineering.application.rag.query_expansion</code> command:</p>
    <pre class="programlisting code"><code class="hljs-code">query = Query.from_str(<span class="hljs-string">"</span><span class="hljs-string">Write an article about the best types of advanced RAG methods."</span>)
    query_expander = QueryExpansion()
    expanded_queries = query_expander.generate(query, expand_to_n=<span class="hljs-number">3</span>)
    <span class="hljs-keyword">for</span> expanded_query <span class="hljs-keyword">in</span> expanded_queries:
        logger.info(expanded_query.content)
</code></pre>
    <p class="normal">We get the following variations of the original query. As you can observe, the query expansion method was successful in providing more details and different perspectives of the initial query, such as highlighting the effectiveness of advanced RAG methods or the overview of these methods (remember that the first query is the original one):</p>
    <pre class="programlisting con"><code class="hljs-con">2024-09-18 17:51:33.529 | INFO  - Write an article about the best types of advanced RAG methods.
2024-09-18 17:51:33.529 | INFO  - What are the most effective advanced RAG methods, and how can they be applied?
2024-09-18 17:51:33.529 | INFO  - Can you provide an overview of the top advanced retrieval-augmented generation techniques?
</code></pre>
    <p class="normal">Now, let’s move<a id="_idIndexMarker840"/> to the <a id="_idIndexMarker841"/>next pre-retrieval optimization method: self-querying.</p>
    <h3 id="_idParaDest-224" class="heading-3">Self-querying</h3>
    <p class="normal">The <em class="italic">problem</em> when <a id="_idIndexMarker842"/>embedding your query into a vector space is that you cannot guarantee that all the aspects required by your use case are present with enough signal in the embedding vector. For example, you want to be 100% sure that your retrieval depends on the tags provided in the user’s input. Unfortunately, you can’t control the signal left within the embedding that emphasizes the tag. By embedding the query prompt alone, you can never be sure that the tags are sufficiently represented in the embedding vector or have enough signal when computing the distance against other vectors. </p>
    <p class="normal">This problem stands for any other metadata you want to present during the search, such as IDs, names, or categories.</p>
    <p class="normal">The <em class="italic">solution</em> is to use self-querying<strong class="keyWord"> </strong>to extract the tags or other critical metadata within the query and use them alongside the vector search as filters. Self-querying uses an LLM to extract various metadata fields crucial for your business use case, such as tags, IDs, number of comments, likes, shares, etc. Afterward, you have complete control over how the extracted metadata is considered during retrieval. In our LLM Twin use case, we extract the author’s name and use it as a filter. Self-queries work hand-in-hand with filtered vector searches, which we will explain in the next section.</p>
    <p class="normal">Now, let’s move on to the code. We begin by importing the necessary modules and classes on <a id="_idIndexMarker843"/>which our<a id="_idIndexMarker844"/> code relies:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> langchain_openai <span class="hljs-keyword">import</span> ChatOpenAI
<span class="hljs-keyword">from</span> llm_engineering.application <span class="hljs-keyword">import</span> utils
<span class="hljs-keyword">from</span> llm_engineering.domain.documents <span class="hljs-keyword">import</span> UserDocument
<span class="hljs-keyword">from</span> llm_engineering.domain.queries <span class="hljs-keyword">import</span> Query
<span class="hljs-keyword">from</span> llm_engineering.settings <span class="hljs-keyword">import</span> settings
<span class="hljs-keyword">from</span> .base <span class="hljs-keyword">import</span> RAGStep
<span class="hljs-keyword">from</span> .prompt_templates <span class="hljs-keyword">import</span> SelfQueryTemplate
</code></pre>
    <p class="normal">Next, we define the <code class="inlineCode">SelfQuery</code> class, which inherits from <code class="inlineCode">RAGStep</code> and implements the <code class="inlineCode">generate()</code> method. The class can be found at <a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/self_query.py"><span class="url">https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/self_query.py</span></a>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">SelfQuery</span>(<span class="hljs-title">RAGStep</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title">generate</span>(<span class="hljs-params">self, query: Query</span>) -&gt; Query:
        <span class="hljs-keyword">if</span> self._mock:
            <span class="hljs-keyword">return</span> query
</code></pre>
    <p class="normal">In the <code class="inlineCode">generate()</code> method, we check if the <code class="inlineCode">_mock</code> attribute is set to <code class="inlineCode">True</code>. If it is, we will return the original query object unmodified. This allows us to bypass calling the model while testing and debugging. If not in mock mode, we create the prompt template and initialize the language model.</p>
    <pre class="programlisting code"><code class="hljs-code">        prompt = SelfQueryTemplate().create_template()
        model = ChatOpenAI(model=settings.OPENAI_MODEL_ID, api_key=settings.OPENAI_API_KEY, temperature=<span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">Here, we instantiate the prompt using the <code class="inlineCode">SelfQueryTemplate</code> factory class and create a <code class="inlineCode">ChatOpenAI</code> model instance (similar to the query expansion implementation). We then combine the prompt and the model into a chain and invoke it with the user’s query.</p>
    <pre class="programlisting code"><code class="hljs-code">        chain = prompt | model
        response = chain.invoke({<span class="hljs-string">"question"</span>: query})
        user_full_name = response.content.strip(<span class="hljs-string">"\n "</span>)
</code></pre>
    <p class="normal">We extract the content from the LLM response and strip any leading or trailing whitespace<a id="_idIndexMarker845"/> to<a id="_idIndexMarker846"/> obtain the <code class="inlineCode">user_full_name</code> value. Next, we check if the model was able to extract any user information.</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">if</span> user_full_name == <span class="hljs-string">"</span><span class="hljs-string">none"</span>:
            <span class="hljs-keyword">return</span> query
</code></pre>
    <p class="normal">If the response is <code class="inlineCode">"none"</code>, it means no user name was found in the query, so we return the original query object. If a user name is found, we will split the <code class="inlineCode">user_full_name</code> into the <code class="inlineCode">first_name</code> and <code class="inlineCode">last_name</code> variables using a utility function. Then, based on the user’s details, we retrieve or create a <code class="inlineCode">UserDocument</code> user instance:</p>
    <pre class="programlisting code"><code class="hljs-code">        first_name, last_name = utils.split_user_full_name(user_full_name)
        user = UserDocument.get_or_create(first_name=first_name, last_name=last_name)
</code></pre>
    <p class="normal">Finally, we update the query object with the extracted author information and return it:</p>
    <pre class="programlisting code"><code class="hljs-code">        query.author_id = user.<span class="hljs-built_in">id</span>
        query.author_full_name = user.full_name
        <span class="hljs-keyword">return</span> query
</code></pre>
    <p class="normal">The updated query now contains the <code class="inlineCode">author_id</code> and <code class="inlineCode">author_full_name</code> values, which can be used in subsequent steps of the RAG pipeline.</p>
    <p class="normal">Let’s look at<a id="_idIndexMarker847"/> the <code class="inlineCode">SelfQueryTemplate</code> class, which <a id="_idIndexMarker848"/>defines the prompt to extract user information:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> langchain.prompts <span class="hljs-keyword">import</span> PromptTemplate
<span class="hljs-keyword">from</span> .base <span class="hljs-keyword">import</span> PromptTemplateFactory
<span class="hljs-keyword">class</span> <span class="hljs-title">SelfQueryTemplate</span>(<span class="hljs-title">PromptTemplateFactory</span>):
    prompt: <span class="hljs-built_in">str</span> = <span class="hljs-string">"""You are an AI language model assistant. Your task is to extract information from a user question.</span>
<span class="hljs-string">    The required information that needs to be extracted is the user name or user id.</span>
<span class="hljs-string">    Your response should consist of only the extracted user name (e.g., John Doe) or id (e.g. 1345256), nothing else.</span>
<span class="hljs-string">    If the user question does not contain any user name or id, you should return the following token: none.</span>
<span class="hljs-string">   </span>
<span class="hljs-string">    For example:</span>
<span class="hljs-string">    QUESTION 1:</span>
<span class="hljs-string">    My name is Paul Iusztin and I want a post about...</span>
<span class="hljs-string">    RESPONSE 1:</span>
<span class="hljs-string">    Paul Iusztin</span>
<span class="hljs-string">   </span>
<span class="hljs-string">    QUESTION 2:</span>
<span class="hljs-string">    I want to write a post about...</span>
<span class="hljs-string">    RESPONSE 2:</span>
<span class="hljs-string">    none</span>
<span class="hljs-string">   </span>
<span class="hljs-string">    QUESTION 3:</span>
<span class="hljs-string">    My user id is 1345256 and I want to write a post about...</span>
<span class="hljs-string">    RESPONSE 3:</span>
<span class="hljs-string">    1345256</span>
<span class="hljs-string">   </span>
<span class="hljs-string">    User question: {question}"""</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">create_template</span>(<span class="hljs-params">self</span>) -&gt; PromptTemplate:
        <span class="hljs-keyword">return</span> PromptTemplate(template=self.prompt, input_variables=[<span class="hljs-string">"question"</span>])
</code></pre>
    <p class="normal">In the <code class="inlineCode">SelfQueryTemplate</code> class, we define a prompt instructing the AI model to extract the <em class="italic">user name</em> or <em class="italic">ID</em> from the input question. The prompt uses few-shot learning to guide the model on how to respond in different scenarios. When the template is invoked, the <code class="inlineCode">{question}</code> placeholder will be replaced with the actual user question.</p>
    <p class="normal">By implementing self-querying, we ensure that critical metadata required for our use case is explicitly extracted and used during retrieval. This approach overcomes the limitations of relying solely on the semantics of the embeddings to capture all <a id="_idIndexMarker849"/>necessary <a id="_idIndexMarker850"/>aspects of a query.</p>
    <p class="normal">Now that we’ve implemented the <code class="inlineCode">SelfQuery</code> class, let’s provide an example. Run the following code using the <code class="inlineCode">python -m llm_engineering.application.rag.self_query</code> CLI command:</p>
    <pre class="programlisting code"><code class="hljs-code">    query = Query.from_str(<span class="hljs-string">"I am Paul Iusztin. Write an article about the best types of advanced RAG methods."</span>)
    self_query = SelfQuery()
    query = self_query.generate(query)
    logger.info(<span class="hljs-string">f"Extracted author_id: </span><span class="hljs-subst">{query.author_id}</span><span class="hljs-string">"</span>)
    logger.info(<span class="hljs-string">f"Extracted author_full_name: </span><span class="hljs-subst">{query.author_full_name}</span><span class="hljs-string">"</span>)
</code></pre>
    <p class="normal">We get the following results where the author’s full name and ID were extracted correctly:</p>
    <pre class="programlisting con"><code class="hljs-con">2024-09-18 18:02:10.362 | INFO - Extracted author_id: 900fec95-d621-4315-84c6-52e5229e0b96
2024-09-18 18:02:10.362 | INFO - Extracted author_full_name: Paul Iusztin
</code></pre>
    <p class="normal">Now that we understand how self-querying works, let’s explore how it can be used together with<a id="_idIndexMarker851"/> filtered vector<a id="_idIndexMarker852"/> search within the retrieval optimization step.</p>
    <h2 id="_idParaDest-225" class="heading-2">Advanced RAG retrieval optimization: filtered vector search</h2>
    <p class="normal">Vector search is <a id="_idIndexMarker853"/>pivotal<a id="_idIndexMarker854"/> in retrieving relevant information based on semantic similarity. A plain vector search, however, can introduce significant challenges that affect both the accuracy and latency of information retrieval. This is primarily because it operates solely on the numerical proximity of vector embeddings without considering the contextual or categorical nuances that might be crucial for relevance.</p>
    <p class="normal">One of the primary issues with plain vector search is retrieving semantically similar but contextually irrelevant documents. Since vector embeddings capture general semantic meanings, they might assign high similarity scores to content that shares language patterns or topics but doesn’t align with the specific intent or constraints of the query. For instance, searching for “Java” could retrieve documents about the programming language or the Indonesian island, depending solely on semantic similarity, leading to ambiguous or misleading results.</p>
    <p class="normal">Moreover, as the size of the dataset increases, plain vector search can suffer from scalability issues. The lack of filtering means the search algorithm has to compute similarities across the entire vector space, which can significantly increase latency. </p>
    <p class="normal">This exhaustive search slows response times and consumes more computational resources, making it inefficient for real-time or large-scale applications.</p>
    <p class="normal">Filtered vector search emerges as a solution by filtering after additional criteria, such as metadata tags or categories, reducing the search space before computing vector similarities. By applying these filters, the search algorithm narrows the pool of potential results to those contextually aligned with the query’s intent. This targeted approach enhances accuracy by eliminating irrelevant documents that might have otherwise been considered due to their semantic similarities alone.</p>
    <p class="normal">Additionally, filtered vector search improves latency by reducing the number of comparisons the algorithm needs to perform. Working with a smaller, more relevant subset of data decreases the computational overhead, leading to faster response times. This efficiency is crucial for applications requiring real-time interactions or handling large queries.</p>
    <p class="normal">As the metadata used within the filtered vector search is often part of the user’s input, we have to extract it before querying the vector DB. That’s precisely what we did during the self-query step, where we extracted the author’s name to reduce the<a id="_idIndexMarker855"/> vector <a id="_idIndexMarker856"/>space only to the author’s content. Thus, as we processed the query within the self-query step, it went into the pre-retrieval optimization category, whereas when the filtered vector search optimized the query, it went into the retrieval optimization bin.</p>
    <p class="normal">For example, when using Qdrant, to add a filter that looks for a matching <code class="inlineCode">author_id</code> within the metadata of each document, you must implement the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> qdrant_client.models <span class="hljs-keyword">import</span> FieldCondition, Filter, MatchValue
records = qdrant_connection.search(
            collection_name=<span class="hljs-string">"articles"</span>,
            query_vector=query_embedding,
            limit=<span class="hljs-number">3</span>,
            with_payload=<span class="hljs-literal">True</span>,
            query_filter= Filter(
                    must=[
                        FieldCondition(
                            key=<span class="hljs-string">"author_id"</span>,
                            <span class="hljs-keyword">match</span>=MatchValue(
                                value=<span class="hljs-built_in">str</span>(<span class="hljs-string">"1234"</span>),
                            ),
                        )
                    ]
                ),
        )
</code></pre>
    <p class="normal">In essence, while plain vector search provides a foundation for semantic retrieval, its limitations can slow performance in practical applications. Filtered vector search addresses these challenges by combining the strengths of vector embeddings with<a id="_idIndexMarker857"/> contextual filtering, resulting in more accurate and efficient information <a id="_idIndexMarker858"/>retrieval in RAG systems. The last step for optimizing our RAG pipeline is to look into reranking.</p>
    <h2 id="_idParaDest-226" class="heading-2">Advanced RAG post-retrieval optimization: reranking</h2>
    <p class="normal">The <em class="italic">problem</em><strong class="keyWord"> </strong>in RAG <a id="_idIndexMarker859"/>systems is that the<a id="_idIndexMarker860"/> retrieved context may contain irrelevant chunks that only:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Add noise</strong>: The retrieved context might be irrelevant, cluttering the information and potentially confusing the language model.</li>
      <li class="bulletList"><strong class="keyWord">Make the prompt bigger</strong>: Including unnecessary chunks increases the prompt size, leading to higher costs. Moreover, language models are usually biased toward the context’s first and last pieces. So, if you add a large amount of context, there’s a big chance it will miss the essence.</li>
      <li class="bulletList"><strong class="keyWord">Be come unaligned with your question</strong>: Chunks are retrieved based on the similarity between the query and chunk embeddings. The issue is that the embedding model might not be tuned to your question, resulting in high similarity scores for chunks that aren’t entirely relevant.</li>
    </ul>
    <p class="normal">The <em class="italic">solution</em><strong class="keyWord"> </strong>is to<strong class="keyWord"> </strong>use reranking to order all the N × K retrieved chunks based on their relevance relative to the initial question, where the first chunk will be the most relevant and the last the least. N represents the number of searches after query expansion, while K is the number of chunks retrieved per search. Hence, we retrieve a total of N x K chunks. In RAG systems, reranking serves as a critical post-retrieval step that refines the initial results obtained from the retrieval model.</p>
    <p class="normal">We assess each chunk’s relevance to the original query by applying the reranking algorithm, which often uses advanced models like neural cross-encoders. These models evaluate the semantic similarity between the query and each chunk more accurately than initial retrieval methods based on embeddings and the cosine similarity distance, as explained in more detail in <em class="italic">Chapter 4</em> in the <em class="italic">An overview of advanced RAG</em> section.</p>
    <p class="normal">Ultimately, we pick<a id="_idIndexMarker861"/> the top K most relevant chunks from the sorted list of N x K items based on the reranking score. Reranking works well when combined with <strong class="keyWord">query expansion</strong>. First, let’s understand how reranking works without query expansion:</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">Search for &gt; K chunks</strong>: Retrieve more than K chunks to have a broader pool of potentially relevant information.</li>
      <li class="numberedList"><strong class="keyWord">Reorder using rerank</strong>: Apply reranking to this larger set to evaluate the actual relevance of each chunk relative to the query.</li>
      <li class="numberedList"><strong class="keyWord">Take top K</strong>: Select the top K chunks to use them as context in the final prompt.</li>
    </ol>
    <p class="normal">Thus, when combined with query expansion, we gather potential valuable context from multiple points in space rather than just looking for more than K samples in a single location. Now the flow looks like this:</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">Search for N × K chunks</strong>: Retrieve multiple sets of chunks using the expanded queries.</li>
      <li class="numberedList"><strong class="keyWord">Reorder using rerank</strong>: Rerank all the retrieved chunks based on their relevance.</li>
      <li class="numberedList"><strong class="keyWord">Take top K</strong>: Select the most relevant chunks for the final prompt.</li>
    </ol>
    <p class="normal">Integrating reranking into the RAG pipeline enhances the quality and relevance of the retrieved<a id="_idIndexMarker862"/> context and efficiently uses computational resources. Let’s look at implementing the LLM Twin’s reranking step to understand what we described above, which can be accessed on GitHub at <a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/reranking.py"><span class="url">https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/reranking.py</span></a>.</p>
    <p class="normal">We begin by importing the necessary modules and classes for our reranking process:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> llm_engineering.application.networks <span class="hljs-keyword">import</span> CrossEncoderModelSingleton
<span class="hljs-keyword">from</span> llm_engineering.domain.embedded_chunks <span class="hljs-keyword">import</span> EmbeddedChunk
<span class="hljs-keyword">from</span> llm_engineering.domain.queries <span class="hljs-keyword">import</span> Query
<span class="hljs-keyword">from</span> .base <span class="hljs-keyword">import</span> RAGStep
</code></pre>
    <p class="normal">Next, we define<a id="_idIndexMarker863"/> the <code class="inlineCode">Reranker</code> class, which is responsible for reranking the retrieved documents based on their relevance to the query:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">Reranker</span>(<span class="hljs-title">RAGStep</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, mock: </span><span class="hljs-built_in">bool</span><span class="hljs-params"> = </span><span class="hljs-literal">False</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-built_in">super</span>().__init__(mock=mock)
        self._model = CrossEncoderModelSingleton()
</code></pre>
    <p class="normal">In the initializer of the Reranker class, we instantiate our cross-encoder model by creating an instance of <code class="inlineCode">CrossEncoderModelSingleton</code>. This is the cross-encoder model used to score the relevance of each document chunk with respect to the query.</p>
    <p class="normal">The core <a id="_idIndexMarker864"/>functionality of the <code class="inlineCode">Reranker</code> class is implemented in the <code class="inlineCode">generate()</code> method:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span> <span class="hljs-title">generate</span>(<span class="hljs-params">self, query: Query, chunks: </span><span class="hljs-built_in">list</span><span class="hljs-params">[EmbeddedChunk], keep_top_k: </span><span class="hljs-built_in">int</span>) -&gt; <span class="hljs-built_in">list</span>[EmbeddedChunk]:
        <span class="hljs-keyword">if</span> self._mock:
            <span class="hljs-keyword">return</span> chunks
        query_doc_tuples = [(query.content, chunk.content) <span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> chunks]
        scores = self._model(query_doc_tuples)
        scored_query_doc_tuples = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">zip</span>(scores, chunks, strict=<span class="hljs-literal">False</span>))
        scored_query_doc_tuples.sort(key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">0</span>], reverse=<span class="hljs-literal">True</span>)
        reranked_documents = scored_query_doc_tuples[:keep_top_k]
        reranked_documents = [doc <span class="hljs-keyword">for</span> _, doc <span class="hljs-keyword">in</span> reranked_documents]
        <span class="hljs-keyword">return</span> reranked_documents
</code></pre>
    <p class="normal">The <code class="inlineCode">generate()</code> method takes a query, a list of chunks (document segments), and the number of top documents to keep (<code class="inlineCode">keep_top_k</code>). If we’re in mock mode, it simply returns the<a id="_idIndexMarker865"/> original chunks. Otherwise, it performs the following steps:</p>
    <ol>
      <li class="numberedList" value="1">Creates pairs of the query content and each chunk’s content</li>
      <li class="numberedList">Uses the cross-encoder model to score each pair, assessing how well the chunk matches the query</li>
      <li class="numberedList">Zips the scores with the corresponding chunks to create a scored list of tuples</li>
      <li class="numberedList">Sorts this list in descending order based on the scores</li>
      <li class="numberedList">Selects the top <code class="inlineCode">keep_top_k</code> chunks</li>
      <li class="numberedList">Extracts the chunks from the tuples and returns them as the reranked documents</li>
    </ol>
    <p class="normal">Before defining the <code class="inlineCode">CrossEncoder</code> class, we import the necessary components:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sentence_transformers.cross_encoder <span class="hljs-keyword">import</span> CrossEncoder
<span class="hljs-keyword">from</span> .base <span class="hljs-keyword">import</span> SingletonMeta
</code></pre>
    <p class="normal">We import the <code class="inlineCode">CrossEncoder</code> class from the sentence_transformers library, which provides the <a id="_idIndexMarker866"/>functionality for scoring text pairs. We also import <code class="inlineCode">SingletonMeta</code> from our base module to ensure our model class follows the singleton pattern, meaning only one instance of the model exists throughout the application. Now, we define the <code class="inlineCode">CrossEncoderModelSingleton</code> class:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">CrossEncoderModelSingleton</span>(metaclass=SingletonMeta):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(
<span class="hljs-params">        self,</span>
<span class="hljs-params">        model_id: </span><span class="hljs-built_in">str</span><span class="hljs-params"> = settings.RERANKING_CROSS_ENCODER_MODEL_ID,</span>
<span class="hljs-params">        device: </span><span class="hljs-built_in">str</span><span class="hljs-params"> = settings.RAG_MODEL_DEVICE,</span>
<span class="hljs-params">    </span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-string">"""</span>
<span class="hljs-string">        A singleton class that provides a pre-trained cross-encoder model for scoring pairs of input text.</span>
<span class="hljs-string">        """</span>
        self._model_id = model_id
        self._device = device
        self._model = CrossEncoder(
            model_name=self._model_id,
            device=self._device,
        )
        self._model.model.<span class="hljs-built_in">eval</span>()
</code></pre>
    <p class="normal">This class initializes <a id="_idIndexMarker867"/>the cross-encoder <a id="_idIndexMarker868"/>model using the specified <code class="inlineCode">model_id</code> and <code class="inlineCode">device</code> from the global <code class="inlineCode">settings</code> loaded from the <code class="inlineCode">.env</code> file. We set the model to evaluation mode using <code class="inlineCode">self._model.model.eval()</code> to ensure the model is ready for inference.</p>
    <p class="normal">The <code class="inlineCode">CrossEncoderModelSingleton</code> class includes a callable method to score text pairs:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span> <span class="hljs-title">__call__</span>(<span class="hljs-params">self, pairs: </span><span class="hljs-built_in">list</span><span class="hljs-params">[</span><span class="hljs-built_in">tuple</span><span class="hljs-params">[</span><span class="hljs-built_in">str</span><span class="hljs-params">, </span><span class="hljs-built_in">str</span><span class="hljs-params">]], to_list: </span><span class="hljs-built_in">bool</span><span class="hljs-params"> = </span><span class="hljs-literal">True</span>) -&gt; NDArray[np.float32] | <span class="hljs-built_in">list</span>[<span class="hljs-built_in">float</span>]:
        scores = self._model.predict(pairs)
        <span class="hljs-keyword">if</span> to_list:
            scores = scores.tolist()
        <span class="hljs-keyword">return</span> scores
</code></pre>
    <p class="normal">The <code class="inlineCode">__call__</code> method allows us to pass in a list of text <code class="inlineCode">pairs</code> (each consisting of the query and a document chunk) and receive their relevance scores. The method uses the model’s <code class="inlineCode">predict()</code> function to call the model and compute the scores.</p>
    <p class="normal">The <code class="inlineCode">CrossEncoderModelSingleton</code> class is a wrapper over the <code class="inlineCode">CrossEncoder</code> class, which we wrote for two purposes. The first one is for the singleton pattern, which allows us to easily access the same instance of the cross-encoder model from anywhere within the application without loading the model in memory every time we need it. The second reason is that by writing our wrapper, we defined our interface for a cross-encoder model (or any other model used for reranking). This makes the code future-proof as in case we need a different implementation or strategy for reranking, for example, using an API, we only have to write a different wrapper that follows the same interface and swap the old class with the new one. Thus, we can introduce new reranking methods without touching the rest of the code.</p>
    <p class="normal">We now understand <a id="_idIndexMarker869"/>all the advanced RAG techniques used within our architecture. In the next section, we will <a id="_idIndexMarker870"/>examine the <code class="inlineCode">ContextRetriever</code> class that connects all these methods and explain how to use the retrieval module with an LLM for an end-to-end RAG inference pipeline.</p>
    <h1 id="_idParaDest-227" class="heading-1">Implementing the LLM Twin’s RAG inference pipeline</h1>
    <p class="normal">As explained at the <a id="_idIndexMarker871"/>beginning of this chapter, the RAG inference pipeline can mainly be divided into three parts: the retrieval module, the prompt creation, and the answer generation, which boils down to calling an LLM with the augmented prompt. In this section, our primary focus will be implementing the retrieval module, where most of the code and logic go. Afterward, we will look at how to build the final prompt using the retrieved context and user query.</p>
    <p class="normal">Ultimately, we will examine how to combine the retrieval module, prompt creation logic, and the LLM to capture an end-to-end RAG workflow. Unfortunately, we won’t be able to test out the LLM until we finish <em class="italic">Chapter 10</em>, as we haven’t deployed our fine-tuned LLM Twin module to AWS SageMaker.</p>
    <p class="normal">Thus, by the end of this section, you will learn how to implement the RAG inference pipeline, which you can test out end to end only after finishing <em class="italic">Chapter 10</em>. Now, let’s start by looking at the implementation of the retrieval module.</p>
    <h2 id="_idParaDest-228" class="heading-2">Implementing the retrieval module</h2>
    <p class="normal">Let’s dive into <a id="_idIndexMarker872"/>the <code class="inlineCode">ContextRetriever</code> class implementation, which orchestrates the retrieval step in our RAG system by integrating all the advanced techniques we previously used: query expansion, self-querying, reranking, and filtered vector search. The class can be found on GitHub at <a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/retriever.py"><span class="url">https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/retriever.py</span></a>.</p>
    <figure class="mediaobject"><img src="../Images/B31105_09_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.2: Search logic of the RAG retrieval module</p>
    <p class="normal">The entry point function of the <code class="inlineCode">ContextRetriever</code> class is the <code class="inlineCode">search()</code> method, which calls all the advanced steps discussed in this chapter. <em class="italic">Figure 9.2</em> shows in more detail how the search method glues together all the steps required to search results similar to the user’s query. It highlights how the extracted author details from the self-query step are used within the filtered vector search. Also, it zooms in on the search operation itself, where, for each query, we do three searches to the vector DB, looking for articles, posts, or repositories similar to the query. For each search (out of N searches), we want to retrieve a maximum of K results. Thus, we retrieve a maximum of K / 3 items for each data category (as we have three<a id="_idIndexMarker873"/> categories). Therefore, when summed up, we will have a list of <code class="inlineCode">≤ K</code> chunks. The retrieved list is <code class="inlineCode">≤ K</code> (and not equal to K) when a particular data category or more returns <code class="inlineCode">&lt; K / 3</code> items after applying the author filters due to missing chunks for that specific author or data category.</p>
    <figure class="mediaobject"><img src="../Images/B31105_09_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.3: Processing the results flow of the RAG retrieval module</p>
    <p class="normal"><em class="italic">Figure 9.3</em> illustrates <a id="_idIndexMarker874"/>how we process the results returned by the xN searches. As each search returns <code class="inlineCode">≤ K</code> items, we will end up with <code class="inlineCode">≤ N x K</code> chunks that we aggregate into a single list. As some results might overlap between searchers, we must deduplicate the aggregated list to ensure each chunk is unique. Ultimately, we send the results to the rerank model, order them based on their reranking score, and pick the <strong class="keyWord">most</strong> relevant top <strong class="keyWord">K</strong> chunks we will use as context for RAG.</p>
    <p class="normal">Let’s understand how everything from <em class="italic">Figures 9.2</em> and <em class="italic">9.3</em> is implemented in the <code class="inlineCode">ContextRetriever</code> class. First, we initialize the class by setting up instances of the <code class="inlineCode">QueryExpansion</code>, <code class="inlineCode">SelfQuery</code>, and <code class="inlineCode">Reranker</code> classes:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">ContextRetriever</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, mock: </span><span class="hljs-built_in">bool</span><span class="hljs-params"> = </span><span class="hljs-literal">False</span>) -&gt; <span class="hljs-literal">None</span>:
        self._query_expander = QueryExpansion(mock=mock)
        self._metadata_extractor = SelfQuery(mock=mock)
        self._reranker = Reranker(mock=mock)
</code></pre>
    <p class="normal">In the <code class="inlineCode">search()</code> method, we convert the user’s input string into a <code class="inlineCode">query</code> object. We then use the <a id="_idIndexMarker875"/><code class="inlineCode">SelfQuery</code> instance to extract the <code class="inlineCode">author_id</code> and <code class="inlineCode">author_full_name</code> from the query:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span> <span class="hljs-title">search</span>(
<span class="hljs-params">        self,</span>
<span class="hljs-params">        query: </span><span class="hljs-built_in">str</span><span class="hljs-params">,</span>
<span class="hljs-params">        k: </span><span class="hljs-built_in">int</span><span class="hljs-params"> = </span><span class="hljs-number">3</span><span class="hljs-params">,</span>
<span class="hljs-params">        expand_to_n_queries: </span><span class="hljs-built_in">int</span><span class="hljs-params"> = </span><span class="hljs-number">3</span><span class="hljs-params">,</span>
<span class="hljs-params">    </span>) -&gt; <span class="hljs-built_in">list</span>:
        query_model = Query.from_str(query)
        query_model = self._metadata_extractor.generate(query_model)
        logger.info(
            <span class="hljs-string">"Successfully extracted the author_id from the query."</span>,
            author_id=query_model.author_id,
        )
</code></pre>
    <p class="normal">Next, we expand the query to generate multiple semantically similar queries using the <code class="inlineCode">QueryExpansion</code> instance:</p>
    <pre class="programlisting code"><code class="hljs-code">        n_generated_queries = self._query_expander.generate(query_model, expand_to_n=expand_to_n_queries)
        logger.info(
            <span class="hljs-string">"Successfully generated queries for search."</span>,
            num_queries=<span class="hljs-built_in">len</span>(n_generated_queries),
        )
</code></pre>
    <p class="normal">We then perform the search concurrently for all expanded queries using a thread pool. Each query is processed by the <code class="inlineCode">_search()</code> method, which we’ll explore shortly. The results are flattened, deduplicated, and collected into a single list:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">with</span> concurrent.futures.ThreadPoolExecutor() <span class="hljs-keyword">as</span> executor:
            search_tasks = [executor.submit(self._search, _query_model, k) <span class="hljs-keyword">for</span> _query_model <span class="hljs-keyword">in</span> n_generated_queries]
            n_k_documents = [task.result() <span class="hljs-keyword">for</span> task <span class="hljs-keyword">in</span> concurrent.futures.as_completed(search_tasks)]
            n_k_documents = utils.misc.flatten(n_k_documents)
            n_k_documents = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(n_k_documents))
        logger.info(<span class="hljs-string">"All documents retrieved successfully."</span>, num_documents=<span class="hljs-built_in">len</span>(n_k_documents))
</code></pre>
    <p class="normal">After retrieving the documents, we rerank them based on their relevance to the original query <a id="_idIndexMarker876"/>and keep only the top <em class="italic">k</em> documents:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(n_k_documents) &gt; <span class="hljs-number">0</span>:
            k_documents = self.rerank(query, chunks=n_k_documents, keep_top_k=k)
        <span class="hljs-keyword">else</span>:
            k_documents = []
        <span class="hljs-keyword">return</span> k_documents
</code></pre>
    <p class="normal">The <code class="inlineCode">_search()</code> method performs the filtered vector search across different data categories like posts, articles, and repositories. It uses the <code class="inlineCode">EmbeddingDispatcher</code> to convert the query into an <code class="inlineCode">EmbeddedQuery</code>, which includes the query’s embedding vector and any extracted metadata:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span> <span class="hljs-title">_search</span>(<span class="hljs-params">self, query: Query, k: </span><span class="hljs-built_in">int</span><span class="hljs-params"> = </span><span class="hljs-number">3</span>) -&gt; <span class="hljs-built_in">list</span>[EmbeddedChunk]:
        <span class="hljs-keyword">assert</span> k &gt;= <span class="hljs-number">3</span>, <span class="hljs-string">"k should be &gt;= 3"</span>
        <span class="hljs-keyword">def</span> <span class="hljs-title">_search_data_category</span>(
<span class="hljs-params">            data_category_odm: </span><span class="hljs-built_in">type</span><span class="hljs-params">[EmbeddedChunk], embedded_query: EmbeddedQuery</span>
<span class="hljs-params">        </span>) -&gt; <span class="hljs-built_in">list</span>[EmbeddedChunk]:
            <span class="hljs-keyword">if</span> embedded_query.author_id:
                query_filter = Filter(
                    must=[
                        FieldCondition(
                            key=<span class="hljs-string">"author_id"</span>,
                            <span class="hljs-keyword">match</span>=MatchValue(
                                value=<span class="hljs-built_in">str</span>(embedded_query.author_id),
                            ),
                        )
                    ]
                )
            <span class="hljs-keyword">else</span>:
                query_filter = <span class="hljs-literal">None</span>
            <span class="hljs-keyword">return</span> data_category_odm.search(
                query_vector=embedded_query.embedding,
                limit=k // <span class="hljs-number">3</span>,
                query_filter=query_filter,
            )
        embedded_query: EmbeddedQuery = EmbeddingDispatcher.dispatch(query)
</code></pre>
    <p class="normal">We used the same <code class="inlineCode">EmbeddingDispatcher</code> to embed the query as in the RAG feature pipeline to<a id="_idIndexMarker877"/> embed the document chunks stored in the vector DB. Using the same class ensures we use the same embedding model at ingestion and query time, which is critical for the retrieval step.</p>
    <p class="normal">We search each data category separately by leveraging the local <code class="inlineCode">_search_data_category()</code> function. Within the <code class="inlineCode">_search_data_category()</code> function, we apply the filters extracted from the <code class="inlineCode">embedded_query</code> object. For instance, if an <code class="inlineCode">author_id</code> is present, we use it to filter the search results only to include documents from that author. The results from all categories are then combined:</p>
    <pre class="programlisting code"><code class="hljs-code">        post_chunks = _search_data_category(EmbeddedPostChunk, embedded_query)
        articles_chunks = _search_data_category(EmbeddedArticleChunk, embedded_query)
        repositories_chunks = _search_data_category(EmbeddedRepositoryChunk, embedded_query)
        retrieved_chunks = post_chunks + articles_chunks + repositories_chunks
        <span class="hljs-keyword">return</span> retrieved_chunks
</code></pre>
    <p class="normal">Finally, the <code class="inlineCode">rerank()</code> method takes the original query and the list of retrieved documents to <a id="_idIndexMarker878"/>reorder them based on relevance:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span> <span class="hljs-title">rerank</span>(<span class="hljs-params">self, query: </span><span class="hljs-built_in">str</span><span class="hljs-params"> | Query, chunks: </span><span class="hljs-built_in">list</span><span class="hljs-params">[EmbeddedChunk], keep_top_k: </span><span class="hljs-built_in">int</span>) -&gt; <span class="hljs-built_in">list</span>[EmbeddedChunk]:
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(query, <span class="hljs-built_in">str</span>):
            query = Query.from_str(query)
        reranked_documents = self._reranker.generate(query=query, chunks=chunks, keep_top_k=keep_top_k)
        logger.info(<span class="hljs-string">"</span><span class="hljs-string">Documents reranked successfully."</span>, num_documents=<span class="hljs-built_in">len</span>(reranked_documents))
        <span class="hljs-keyword">return</span> reranked_documents
</code></pre>
    <p class="normal">Leveraging the <code class="inlineCode">ContextRetriever</code> class, we can retrieve context from any query with only a few lines of code. For example, let’s take a look at the following code snippet, where we call the entire advanced RAG architecture with a simple call to the <code class="inlineCode">search()</code> method:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> loguru <span class="hljs-keyword">import</span> logger
<span class="hljs-keyword">from</span> llm_engineering.application.rag.retriever <span class="hljs-keyword">import</span> ContextRetriever
query = <span class="hljs-string">"""</span>
<span class="hljs-string">        My name is Paul Iusztin.</span>
<span class="hljs-string">       </span>
<span class="hljs-string">        Could you draft a LinkedIn post discussing RAG systems?</span>
<span class="hljs-string">        I'm particularly interested in:</span>
<span class="hljs-string">            - how RAG works</span>
<span class="hljs-string">            - how it is integrated with vector DBs and large language models (LLMs).</span>
<span class="hljs-string">        """</span>
retriever = ContextRetriever(mock=<span class="hljs-literal">False</span>)
documents = retriever.search(query, k=<span class="hljs-number">3</span>)
logger.info(<span class="hljs-string">"Retrieved documents:"</span>)
<span class="hljs-keyword">for</span> rank, document <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(documents):
    logger.info(<span class="hljs-string">f"</span><span class="hljs-subst">{rank + </span><span class="hljs-number">1</span><span class="hljs-subst">}</span><span class="hljs-string">: </span><span class="hljs-subst">{document}</span><span class="hljs-string">"</span>)
</code></pre>
    <p class="normal">Calling the code <a id="_idIndexMarker879"/>from above using the following CLI command: <code class="inlineCode">poetry poe call-rag-retrieval-module</code>. This outputs the following:</p>
    <pre class="programlisting con"><code class="hljs-con">2024-09-18 19:01:50.588 | INFO - Retrieved documents:
2024-09-18 19:01:50.588 | INFO - 1: id=UUID('541d6c22-d15a-4e6a-924a-68b7b1e0a330') content='4 Advanced RAG Algorithms You Must Know by Paul Iusztin Implement 4 advanced RAG retrieval techniques to optimize your vector DB searches. Integrate the RAG retrieval module into a production LLM system…" platform='decodingml.substack.com' document_id=UUID('32648f33-87e6-435c-b2d7-861a03e72392') author_id=UUID('900fec95-d621-4315-84c6-52e5229e0b96') author_full_name='Paul Iusztin' metadata={'embedding_model_id': 'sentence-transformers/all-MiniLM-L6-v2', 'embedding_size': 384, 'max_input_length': 256} link='https://decodingml.substack.com/p/the-4-advanced-rag-algorithms-you?r=1ttoeh'
2024-09-18 19:01:50.588 | INFO - 2: id=UUID('5ce78438-1314-4874-8a5a-04f5fcf0cb21') content='Overview of advanced RAG optimization techniquesA production RAG system is split into 3 main components ingestion clean, chunk, embed, and load your data to a vector DBretrieval query your vector DB for …" platform='medium' document_id=UUID('bd9021c9-a693-46da-97e7-0d06760ee6bf') author_id=UUID('900fec95-d621-4315-84c6-52e5229e0b96') author_full_name='Paul Iusztin' metadata={'embedding_model_id': 'sentence-transformers/all-MiniLM-L6-v2', 'embedding_size': 384, 'max_input_length': 256} link='https://medium.com/decodingml/the-4-advanced-rag-algorithms-you-must-know-to-implement-5d0c7f1199d2'
2024-09-18 19:02:45.729 | INFO  - 3: id=UUID('0405a5da-4686-428a-91ca-446b8e0446ff') content='Every Medium article will be its own lesson An End to End Framework for Production Ready LLM Systems by Building Your LLM TwinThe Importance of Data Pipelines in the Era of Generative AIChange Data Capture Enabling Event Driven …" platform='medium' document_id=UUID('bd9021c9-a693-46da-97e7-0d06760ee6bf') author_id=UUID('900fec95-d621-4315-84c6-52e5229e0b96') author_full_name='Paul Iusztin' metadata={'embedding_model_id': 'sentence-transformers/all-MiniLM-L6-v2', 'embedding_size': 384, 'max_input_length': 256} link='https://medium.
com/decodingml/the-4-advanced-rag-algorithms-you-must-know-to-implement-5d0c7f1199d2'
</code></pre>
    <p class="normal">As you can observe in the output above, along with the retrieved content, we have access to all kinds of metadata, such as the embedding model used for retrieval or the link from which the chunk was taken. These can quickly be added to a list of references when generating the result for the user, increasing trust in the final results.</p>
    <p class="normal">Now that we <a id="_idIndexMarker880"/>understand how the retrieval module works, let’s take a final step and examine the end-to-end RAG inference pipeline.</p>
    <h2 id="_idParaDest-229" class="heading-2">Bringing everything together into the RAG inference pipeline</h2>
    <p class="normal">To fully implement the <a id="_idIndexMarker881"/>RAG flow, we still have to build the prompt using the context from the retrieval model and call the LLM to generate the answer. This section will discuss these two steps and wrap everything together into a single <code class="inlineCode">rag()</code> function. The functions from this section can be accessed on GitHub at <a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/infrastructure/inference_pipeline_api.py"><span class="url">https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/infrastructure/inference_pipeline_api.py</span></a>.</p>
    <p class="normal">Let’s start by looking at the <code class="inlineCode">call_llm_service()</code>function, responsible for interfacing with the LLM service. It takes in a user’s query and an optional context, sets up the language model endpoint, executes the inference, and returns the generated answer. The context is optional; you can call the LLM without it, as you would when interacting with any other LLM:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">call_llm_service</span>(<span class="hljs-params">query: </span><span class="hljs-built_in">str</span><span class="hljs-params">, context: </span><span class="hljs-built_in">str</span><span class="hljs-params"> | </span><span class="hljs-literal">None</span>) -&gt; <span class="hljs-built_in">str</span>:
    llm = LLMInferenceSagemakerEndpoint(
        endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE, inference_component_name=<span class="hljs-literal">None</span>
    )
    answer = InferenceExecutor(llm, query, context).execute()
    <span class="hljs-keyword">return</span> answer
</code></pre>
    <p class="normal">This function makes an HTTP request to our fine-tuned LLM Twin model, which is hosted as an AWS SageMaker inference endpoint. We will explore all the SageMaker details in the next chapter, where we will dig into the <code class="inlineCode">LLMInferenceSagemakerEndpoint</code> and <code class="inlineCode">InferenceExecutor</code> classes. For now, what is essential to know<a id="_idIndexMarker882"/> is that we use this function to call our fine-tuned LLM. Still, we must highlight how the query and context, passed to the <code class="inlineCode">InferenceExecutor</code> class, are transformed into the final prompt. We do that using a simple prompt template that is customized using the user query and retrieved context:</p>
    <pre class="programlisting code"><code class="hljs-code">prompt = <span class="hljs-string">f"""</span>
<span class="hljs-string">You are a content creator. Write what the user asked you to while using the provided context as the primary source of information for the content.</span>
<span class="hljs-string">User query: </span><span class="hljs-subst">{query}</span>
<span class="hljs-string">Context: </span><span class="hljs-subst">{context}</span>
<span class="hljs-string">          """</span>
</code></pre>
    <p class="normal">Moving on to the <code class="inlineCode">rag()</code> function, this is where the RAG logic comes together. It handles retrieving relevant documents based on the query, mapping the documents to the context that will be injected into the prompt, and obtaining the final answer from the LLM:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">rag</span>(<span class="hljs-params">query: </span><span class="hljs-built_in">str</span>) -&gt; <span class="hljs-built_in">str</span>:
    retriever = ContextRetriever(mock=<span class="hljs-literal">False</span>)
    documents = retriever.search(query, k=<span class="hljs-number">3</span>)
    context = EmbeddedChunk.to_context(documents)
    answer = call_llm_service(query, context)
    <span class="hljs-keyword">return</span> answer
</code></pre>
    <p class="normal">As we modularized all the RAG steps into independent classes, we reduced the high-level <code class="inlineCode">rag()</code> function to five lines of code (encapsulating all the complexities of the system) similar to what we see in tools such as LangChain, LlamaIndex, or Haystack. Instead of their high-level implementation, we learned how to build an advanced RAG service from scratch. Also, by clearly separating the responsibility of each class, we can use them like LEGOs. Thus, you can quickly call the LLM independently without context or use the retrieval module as a query engine on top of your vector DB. In the next chapter, we will see the <code class="inlineCode">rag()</code> function in action after we deploy our fine-tuned LLM to an AWS SageMaker inference endpoint.</p>
    <p class="normal">Before ending this <a id="_idIndexMarker883"/>chapter, we want to discuss potential improvements you could add to the RAG inference pipeline. As we are building a chatbot, the first one is to add a conversation memory that stores all the user prompts and generated answers in memory. Thus, when interacting with the chatbot, it will be aware of the whole conversation, not only the latest prompt. When prompting the LLM, along with the new user input and context, we also pass the conversation history from the memory. As the conversation history can get long, to avoid exceeding the context window or higher costs, you have to implement a way to reduce the size of your memory. As illustrated in <em class="italic">Figure 9.4</em>, the simplest one is to keep only the latest K items from your chat history. Unfortunately, using this strategy, the LLM will never be aware of the whole conversation. </p>
    <p class="normal">Therefore, another way to add the chat history to your prompt is to keep a summary of the conversation along with the latest K replies. There are multiple ways to compute this summary, which might defeat the purpose of this book if we get into them all, but the simplest way is to <a id="_idIndexMarker884"/>always update the summary on every user prompt and generate an answer.</p>
    <figure class="mediaobject"><img src="../Images/B31105_09_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.4: Routing and memory examples</p>
    <p class="normal">As for each search, we send three queries to the vector DB, one for each data category. Thus, the second improvement is to add a router between the query and the search. The router will be a multi-category classifier that predicts the data categories we must retrieve for that specific query. Hence, instead of making three requests for every search, we can often reduce it to one or two. For example, if the user wants to write a theoretical paragraph about RAG for an article, then most probably, it’s valuable to query only the article’s collection. In this case, the router will predict the article class, which we can use to decide what collection we must query. </p>
    <p class="normal">Another example would be if we want to illustrate a piece of code that shows how to build a RAG pipeline. In this case, the router would have to predict the article and repository data category, as we need to look up examples in both collections for an exhaustive context.</p>
    <p class="normal">Usually, the router strategy decides what model to call based on a user’s input, such as whether to use GPT-4 or a self-hosted Llama 3.1 model for that specific query. However, in our particular use case, we can adapt the router algorithm to optimize the retrieval step.</p>
    <p class="normal">We can further<a id="_idIndexMarker885"/> optimize the retrieval using a hybrid search algorithm that combines the vector search (based on embeddings) with a keyword search algorithm, such as BM25. Search algorithms used BM25 (or similar methods) to find similar items in a DB before vector search algorithms became popular. By merging the methods, hybrid search retrieves results that match the exact terms, such as RAG, LLM, or SageMaker, and the query semantics, increasing the accuracy and relevance of your retrieved results. Fundamentally, the hybrid search algorithms follow the next mechanics:</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">Parallel processing</strong>: The search query is processed simultaneously through both the vector search and BM25 algorithms. Each algorithm retrieves a set of relevant documents based on its criteria.</li>
      <li class="numberedList"><strong class="keyWord">Score normalization</strong>: The results from both searches are assigned relevance scores, which are then normalized to ensure comparability. This step is crucial because vector search and BM25 scoring mechanisms work at different scales. Thus, they can’t be compared or merged without normalization.</li>
      <li class="numberedList"><strong class="keyWord">Result merging</strong>: The normalized scores are combined, often through a weighted sum, to produce a final ranking of documents. Adjusting the weights allows for fine-tuning the emphasis on the semantic or keyword search algorithm.</li>
    </ol>
    <p class="normal">To conclude, by combining the semantic and exact keyword search algorithms, you can improve the accuracy of your retrieval step. Vector search helps recognize synonyms or related concepts, ensuring that relevant information isn’t overlooked due to vocabulary differences. Keyword search ensures that documents containing critical keywords are emphasized appropriately, particularly in technical fields with specific terminology.</p>
    <p class="normal">One last improvement we can make to our RAG system is to use multi-index vector structures instead of indexing based only on the content’s embedding. Let’s detail how multi-indexing works. Instead of using the embeddings of a single field to do the vector search for a particular collection, it combines multiple fields. </p>
    <p class="normal">For example, in our LLM Twin use case, we used only the content field of our articles, posts, or repositories to query the vector DB. When using a multi-index strategy, along with the content field, we could index the embeddings of the<a id="_idIndexMarker886"/> platform where the content was posted or when the content was published. This could impact the final accuracy of your retrieval as different platforms have different types of content, or more recent content is usually more relevant. Frameworks such as Superlinked make multi-indexing easy. For example, in the code snippet below, using Superlinked, we defined a multi-index on the content and platform for our article collection in just a few lines of code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> superlinked.framework.common.schema.id_schema_object <span class="hljs-keyword">import</span> IdField
<span class="hljs-keyword">from</span> superlinked.framework.common.schema.schema <span class="hljs-keyword">import</span> schema
<span class="hljs-keyword">from</span> superlinked.framework.common.schema.schema_object <span class="hljs-keyword">import</span> String
… <span class="hljs-comment"># Other Superlinked imports. </span>
<span class="hljs-meta">@schema</span>
<span class="hljs-keyword">class</span> <span class="hljs-title">ArticleSchema</span>:
    <span class="hljs-built_in">id</span>: IdField
    platform: String
    content: String
article = ArticleSchema()
articles_space_content = TextSimilaritySpace(
    text=chunk(article.content, chunk_size=<span class="hljs-number">500</span>, chunk_overlap=<span class="hljs-number">50</span>),
    model=settings.EMBEDDING_MODEL_ID,
)
articles_space_plaform = CategoricalSimilaritySpace(
    category_input=article.platform,
    categories=[<span class="hljs-string">"medium"</span>, <span class="hljs-string">"substack"</span>, "wordpress"],
    negative_filter=-<span class="hljs-number">5.0</span>,
)
article_index = Index(
    [articles_space_content, articles_space_plaform],
    fields=[article.author_id],
)
</code></pre>
    <p class="normal">Superlinked is a powerful Python tool for any use case that includes vector computing, such as RAG, recommender systems, and semantic search. It offers an ecosystem where you can quickly ingest data into a vector DB, write complex queries on top of it, and<a id="_idIndexMarker887"/> deploy the service as a RESTful API.</p>
    <p class="normal">The world of LLMs and RAG is experimental, similar to any other AI domain. Thus, when building real-world products, it’s important to quickly build an end-to-end solution that works but is not necessarily the best. Then, you can reiterate with various experiments until you completely optimize it for your use case. This is standard practice in the industry and lets you iterate fast while providing value to the business and gathering user feedback as quickly as possible in the<a id="_idIndexMarker888"/> product’s lifecycle.</p>
    <h1 id="_idParaDest-230" class="heading-1">Summary</h1>
    <p class="normal">This chapter taught us how to build an advanced RAG inference pipeline. We started by looking into the software architecture of the RAG system. Then, we zoomed in on the advanced RAG methods we used within the retrieval module, such as query expansion, self-querying, filtered vector search, and reranking. Afterward, we saw how to write a modular <code class="inlineCode">ContextRetriever</code> class that glues all the advanced RAG components under a single interface, making searching for relevant documents a breeze. Ultimately, we looked into how to connect all the missing dots, such as the retrieval, the prompt augmentation, and the LLM call, under a single RAG function that will serve as our RAG inference pipeline.</p>
    <p class="normal">As highlighted a few times in this chapter, we couldn’t test our fine-tuned LLM because we haven’t deployed it yet to AWS SageMaker as an inference endpoint. Thus, in the next chapter, we will learn how to deploy the LLM to AWS SageMaker, write an inference interface to call the endpoint, and implement a FastAPI web server to serve as our business layer.</p>
    <h1 id="_idParaDest-231" class="heading-1">References</h1>
    <ul>
      <li class="bulletList"><em class="italic">A real-time retrieval system for social media data | VectorHub by SuperLinked</em>. (n.d.). <a href="https://superlinked.com/vectorhub/articles/real-time-retrieval-system-social-media-data"><span class="url">https://superlinked.com/vectorhub/articles/real-time-retrieval-system-social-media-data</span></a></li>
      <li class="bulletList"><em class="italic">Building a Router from Scratch - LlamaIndex</em>. (n.d.). <a href="https://docs.llamaindex.ai/en/stable/examples/low_level/router/"><span class="url">https://docs.llamaindex.ai/en/stable/examples/low_level/router/</span></a></li>
      <li class="bulletList"><em class="italic">How to add memory to chatbots | LangChain</em>. (n.d.). <a href="https://python.langchain.com/docs/how_to/chatbots_memory/#summary-memory "><span class="url">https://python.langchain.com/docs/how_to/chatbots_memory/#summary-memory</span></a></li>
      <li class="bulletList"><em class="italic">How to do “self-querying” retrieval | LangChain</em>. (n.d.). <a href="https://python.langchain.com/docs/how_to/self_query/"><span class="url">https://python.langchain.com/docs/how_to/self_query/</span></a></li>
      <li class="bulletList"><em class="italic">How to route between sub-chains | LangChain</em>. (n.d.). <a href="https://python.langchain.com/docs/how_to/routing/#routing-by-semantic-similarity"><span class="url">https://python.langchain.com/docs/how_to/routing/#routing-by-semantic-similarity</span></a></li>
      <li class="bulletList"><em class="italic">How to use the MultiQueryRetriever | LangChain</em>. (n.d.). <a href="https://python.langchain.com/docs/how_to/MultiQueryRetriever/"><span class="url">https://python.langchain.com/docs/how_to/MultiQueryRetriever/</span></a></li>
      <li class="bulletList"><em class="italic">Hybrid Search explained</em>. (2023, January 3). Weaviate. <a href="https://weaviate.io/blog/hybrid-search-explained"><span class="url">https://weaviate.io/blog/hybrid-search-explained</span></a></li>
      <li class="bulletList">Iusztin, P. (2024, August 20). 4 Advanced RAG Algorithms You Must Know | Decoding ML. <em class="italic">Medium</em>. <a href="https://medium.com/decodingml/the-4-advanced-rag-algorithms-you-must-know-to-implement-5d0c7f1199d2"><span class="url">https://medium.com/decodingml/the-4-advanced-rag-algorithms-you-must-know-to-implement-5d0c7f1199d2</span></a></li>
      <li class="bulletList">Monigatti, L. (2024, February 19). Advanced Retrieval-Augmented Generation: From Theory to LlamaIndex Implementation. <em class="italic">Medium</em>. <a href="https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930"><span class="url">https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930</span></a></li>
      <li class="bulletList"><em class="italic">Multi-attribute search with vector embeddings | VectorHub by Superlinked</em>. (n.d.). <a href="https://superlinked.com/vectorhub/articles/multi-attribute-semantic-search"><span class="url">https://superlinked.com/vectorhub/articles/multi-attribute-semantic-search</span></a></li>
      <li class="bulletList"><em class="italic">Optimizing RAG with Hybrid Search &amp; Reranking | VectorHub by Superlinked</em>. (n.d.). <a href="https://superlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-reranking"><span class="url">https://superlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-reranking</span></a></li>
      <li class="bulletList">Refactoring.Guru. (2024, January 1). <em class="italic">Singleton</em>. <a href="https://refactoring.guru/design-patterns/singleton"><span class="url">https://refactoring.guru/design-patterns/singleton</span></a></li>
      <li class="bulletList">Stoll, M. (2024, September 7). Visualize your RAG Data—Evaluate your Retrieval-Augmented Generation System with Ragas. <em class="italic">Medium</em>. <a href="https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557"><span class="url">https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557</span></a></li>
      <li class="bulletList"><em class="italic">Using LLM’s for retrieval and reranking—LlamaIndex, data framework for LLM applications</em>. (n.d.). <a href="https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"><span class="url">https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6</span></a></li>
    </ul>
    <h1 id="_idParaDest-232" class="heading-1">Join our book’s Discord space</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/llmeng"><span class="url">https://packt.link/llmeng</span></a></p>
    <p class="normal"><span class="url"><img src="../Images/QR_Code79969828252392890.png" alt=""/></span></p>
  </div>
</body></html>