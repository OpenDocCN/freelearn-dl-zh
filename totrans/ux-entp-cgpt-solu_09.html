<html><head></head><body>
		<div><h1 id="_idParaDest-145" class="chapter-number"><a id="_idTextAnchor150" class="pcalibre pcalibre1 calibre6"/>7</h1>
			<h1 id="_idParaDest-146" class="calibre5"><a id="_idTextAnchor151" class="pcalibre pcalibre1 calibre6"/>Prompt Engineering</h1>
			<p class="calibre3">Prompt engineering for an enterprise takes a slightly different approach to interacting with ChatGPT or any LLM for personal use. Prompt engineering helps ensure that when the customer messages the LLM, a set of instructions is in place for them to succeed. When building prompts to generate a recommendation or complete some backend analysis, the recommendation team directly creates the prompt. The job is to consider how the instructions that give <em class="italic">context</em> to the customer’s messages, also called a prompt,  are framed or <em class="italic">create</em> the prompts that request a result directly from the LLM. First, we will focus on prompt engineering before continuing with fine-tuning in the next chapter, which is an inevitable next step for enterprise solutions.</p>
			<p class="calibre3">None of the tools discussed should be <a id="_idIndexMarker481" class="pcalibre pcalibre1 calibre6"/>considered in a silo. Any enterprise solution will adopt <strong class="bold">Retrieval-Augmented Generation</strong> (<strong class="bold">RAG</strong>), prompt engineering, fine-tuning, and other approaches. Each can support different capabilities while sometimes overlapping. While prompt engineering will align the responses with the goal, fine-tuning will help the model improve its understanding.</p>
			<p class="calibre3">This chapter focuses on a few critical topics related to prompt engineering:</p>
			<ul class="calibre7">
				<li class="calibre8">Giving context through prompt engineering</li>
				<li class="calibre8">Prompt engineering techniques</li>
				<li class="calibre8">Andrew Ng’s agentic approach</li>
				<li class="calibre8">Advanced techniques</li>
			</ul>
			<h1 id="_idParaDest-147" class="calibre5"><a id="_idTextAnchor152" class="pcalibre pcalibre1 calibre6"/>Giving context through prompt engineering</h1>
			<p class="calibre3">To be clear, when building a RAG solution, customers prompt the enterprise system for answers to questions, fill out forms, and interact through prompting. Additional prompts, called instructions, wrap these prompts to ensure they are constrained or managed within a context defined by the business. These instructions give the customer <a id="_idIndexMarker482" class="pcalibre pcalibre1 calibre6"/>guardrails. Time for prompt engineering 101.</p>
			<h2 id="_idParaDest-148" class="calibre9"><a id="_idTextAnchor153" class="pcalibre pcalibre1 calibre6"/>Prompt 101</h2>
			<p class="calibre3">Prompt engineering instructs the chat instance to respond. It frames or puts structure around the answer, defines <a id="_idIndexMarker483" class="pcalibre pcalibre1 calibre6"/>what to include or exclude from responses, and provides any safety rails to implement.</p>
			<p class="calibre3">Instructions can be tested and iterated. Hundreds of changes will be made before settling on better instructions. Multiple models will be doing pieces of the enterprise puzzle, each with its instructions. We will take a few minutes to clarify that we are focused on instructions, a form of prompts that a user needs to control how the model will respond to.</p>
			<p class="calibre3">The prompt strategy depends <a id="_idIndexMarker484" class="pcalibre pcalibre1 calibre6"/>on the task’s needs. If it is a general-purpose interactive prompt, it will focus on style, tone, factualness, and quality. If this prompt is for a step that ingests tables and formats content, it will focus on structure and data output. We will address instructions that wrap customer prompts, as shown in <em class="italic">Figure 7</em><em class="italic">.1</em>.</p>
			<div><div><img src="img/B21964_07_01.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.1 – How to rationalize instructions, prompts, and answers</p>
			<p class="calibre3">If the prompt comes from the customer, we can’t control their request. So, we do what we can to control it. In the figure above, we establish the persona of the Alligiance chat, but the customer asks the question, and the model provides the specific answer.</p>
			<p class="calibre3">Instructions can be simple, such as in the examples from OpenAI, or they can be crafted to address some of our enterprise needs:</p>
			<pre class="source-code">
You are a helpful assistant named Alli, short for the name of our bank. Be courteous and professional. Only provide answers from the attached document. Format output using lists when appropriate.</pre>			<p class="calibre3">Even a trivial example like this has a few essential elements:</p>
			<ul class="calibre7">
				<li class="calibre8">It clarified the persona of the AI and the type of business</li>
				<li class="calibre8">It defines how it should act</li>
				<li class="calibre8">It constrains where to look for answers</li>
				<li class="calibre8">It provides a suggestion on how to respond</li>
				<li class="calibre8">It doesn’t include any actual questions; those come from the customer’s prompt</li>
			</ul>
			<p class="calibre3">This will grow in complexity, spanning dozens or hundreds of lines of text, but this is a cost-benefit trade-off. The longer the prompt, the more tokens are used. These additional instructions are included for <em class="italic">every</em> prompt the user sends to the model, so use your tokens wisely. Remember that tokens represent how the model accounts for size and cost based on the amount of text. While humans understand word count, the model talks in tokens. It can have a maximum amount of context passed to it (in tokens) and a maximum amount of data returned at one time (in tokens), and then charges are based on the number of tokens. We<a id="_idIndexMarker485" class="pcalibre pcalibre1 calibre6"/> will cover more about tokens in this and the next chapter.</p>
			<p class="calibre3">The LLM does not directly interact with customers for recommender solutions or behind-the-scenes uses of a model. Instructions can provide general guidance, and prompts (more detailed instructions) that can be used for specific task efforts. This abstraction creates consistency in one set of instructions for all prompts in a group of projects.</p>
			<p class="calibre3">A thoughtful enterprise instruction set has to be in place to support the user’s prompts for conversational AI. The differences between instructions that act as a wrapper for customer prompts and the actual prompt impact how to write instructions or prompts. Instructions have to be more generic and support a wide range of prompts. The direct prompts are targeted, focusing the LLM on providing one good answer, as shown in the example in the preceding figure. So, there are differences in designing prompts for your personal use versus what is needed in an enterprise solution.</p>
			<h2 id="_idParaDest-149" class="calibre9"><a id="_idTextAnchor154" class="pcalibre pcalibre1 calibre6"/>Designing instructions</h2>
			<p class="calibre3">We have all created a variety of <a id="_idIndexMarker486" class="pcalibre pcalibre1 calibre6"/>prompts for home or work:</p>
			<pre class="source-code">
What is the best way to clean an iron-looking stain in a toilet? (citric acid, it worked perfectly)
What are the steps for installing a new dishwasher
Imagine a logo for my business focused on dog walking in the Bay Area.
Correct this Python code
Summarize this article for me
Write this customer a thank-you letter with these details…</pre>			<p class="calibre3">However, to frame instructions to guide users interacting with an enterprise conversational assistant or when building the instructions for any recommender use cases, robust instructions that clarify the goals and persona of all interactions are needed. Here is the start of an instruction:</p>
			<pre class="source-code">
You are a technical service bot who explains complex problems step-by-step, guiding a user with simple language. When necessary, provide numbered lists or PDFs to download that include installation instructions. Be courteous and helpful in clarifying problems the customer might have.</pre>			<p class="calibre3">This should frame the customer request (their prompt) who might ask questions such as the following:</p>
			<pre class="source-code">
I need ur help understanding how to install the regulator inline with the Mod 14 treatment system. What tools do I need? I don't see any instructions included. Help, plz.</pre>			<p class="calibre3">Together, these two layers of prompt engineering give instructions to the model. So, the company provides the instructions, and the customer provides their prompt. With recommender solutions, the company does it all.</p>
			<p class="calibre3">Imagine an LLM-driven recommender, for scoring leads, rating a person’s reputation, offering products to upsell, providing sentiment feedback, or suggesting data to ignore because it has undesirable or harmful content. Create prompts specific to those use cases. Each prompt will serve only some of the issues. This is why multiple models are discussed so much. There can even be models <a id="_idIndexMarker487" class="pcalibre pcalibre1 calibre6"/>designed to decide which model to use next. The prompt of the first model helps route the request to the second model, which has prompts specific to its task and is tuned to the needs of the request. This chaining of models will be covered here and more in <a href="B21964_08.xhtml#_idTextAnchor172" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 8</em></a>, <em class="italic">Fine-Tuning</em>. Every one of these models needs well-thought-out prompts to guide an interaction, and none of these will have a human prompting the system.</p>
			<p class="calibre3">There is a wealth of documentation and tutorials on what simple prompts can do. Start exploring more at the OpenAI site.</p>
			<p class="calibre3">Documentation: <a href="https://platform.openai.com/docs/examples" class="pcalibre pcalibre1 calibre6">Prompt Examples</a> (<a href="https://platform.openai.com/docs/examples" class="pcalibre pcalibre1 calibre6">https://platform.openai.com/docs/examples</a>)</p>
			<p class="calibre3">But these are just starting points. Much work is needed to scale these up to work reliably and with the style and tone expected in a business use case. Understand where prompt engineering fits into the process in addition to the prompt’s content. We can summarize the highlights from OpenAI’s high-level presentation.</p>
			<p class="calibre3">Video: <a href="https://youtu.be/ahnGLM-RC1Y" class="pcalibre pcalibre1 calibre6">Techniques for improving LLM Quality</a> (<a href="https://youtu.be/ahnGLM-RC1Y" class="pcalibre pcalibre1 calibre6">https://youtu.be/ahnGLM-RC1Y</a>)</p>
			<p class="calibre3">The takeaway starts at about 3 minutes. <em class="italic">Figure 7</em><em class="italic">.2</em> outlines their approach. They reviewed RAG (discussed in the last chapter) as a solution to help an enterprise access its knowledge base and other data sources. They make a great point that this data can be scrubbed and cleaned <em class="italic">before</em> having a working system. Meanwhile, prompt engineering and fine-tuning rely on a <em class="italic">working</em> system for feedback.</p>
			<div><div><img src="img/B21964_07_02.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.2 — Tools to help us optimize our ChatGPT solution</p>
			<p class="calibre3">These tools are all needed in enterprise solutions to improve LLM quality. We can walk through why this is the case. Prompt engineering can start with basic example questions to see how the model acts. When it doesn’t work well, adding training examples to improve how it responds to questions unique to our business is next. This will <a id="_idIndexMarker488" class="pcalibre pcalibre1 calibre6"/>quickly result in wanting more data than can be handled by basic interactions, so the solution extends into RAG. Now, the results don’t fit our style or tone or don’t follow the instructions expected, so fine-tuning is added, giving examples to the model to train it on how it is expected to respond. Results can indicate that the RAG could be refined and optimized further, so they go back and work on it. This results in wanting to fine-tune the results further. And this cycle continues, hopefully improving at every step.</p>
			<p class="calibre3">For our video learners, Mark Hennings has an excellent 15-minute overview that quickly covers a lot of ground.</p>
			<p class="calibre3">Video: <a href="https://www.youtube.com/watch?v=YVWxbHJakgg" class="pcalibre pcalibre1 calibre6">Prompt Eng, RAG, and Fine Tuning</a> (Mark Hennings) (<a href="https://www.youtube.com/watch?v=YVWxbHJakgg" class="pcalibre pcalibre1 calibre6">https://www.youtube.com/watch?v=YVWxbHJakgg</a>)</p>
			<p class="calibre3">An excellent place to start is by teaching some basic strategies for prompting.</p>
			<h2 id="_idParaDest-150" class="calibre9"><a id="_idTextAnchor155" class="pcalibre pcalibre1 calibre6"/>Basic strategies</h2>
			<p class="calibre3">Many structured <a id="_idIndexMarker489" class="pcalibre pcalibre1 calibre6"/>methodologies have been proposed for prompt engineering, and most are similar. One is called <strong class="bold">RACE</strong> (<strong class="bold">Role, Action, Context, and</strong> <strong class="bold">Examples</strong>), another is called <strong class="bold">CO-STAR</strong> (<strong class="bold">Context, Objective, Style, Tone, Audience, and</strong> <strong class="bold">Response</strong>), and another is called <strong class="bold">CARE</strong> (<strong class="bold">Content, Action Result, and Example</strong>). Other approaches are <a id="_idIndexMarker490" class="pcalibre pcalibre1 calibre6"/>explained without a cute initialism. First, it is good to<a id="_idIndexMarker491" class="pcalibre pcalibre1 calibre6"/> understand the primary instructions of a typical prompt; then, dive deeper to help with enterprise instructions.</p>
			<p class="calibre3"><em class="italic">Table 7.1</em> cross-lists the similar concepts for<a id="_idIndexMarker492" class="pcalibre pcalibre1 calibre6"/> each approach in the first column. Each framework uses slightly different terminology but mainly covers the same fundamentals. The cute initialisms seem primarily for branding. We will ignore that and focus on goals, not the terms. Expect to write prompts that contain all of these approaches. We will also explain when not to do some of this.</p>
			<table id="table001-6" class="no-table-style">
				<colgroup class="calibre15">
					<col class="calibre16"/>
					<col class="calibre16"/>
					<col class="calibre16"/>
				</colgroup>
				<tbody class="calibre17">
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Approach</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Explanation</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Example</strong></p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Priming (role, audience, </strong><strong class="bold">and objective)</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Establish the context of the response.</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3"><em class="italic">You are a sales and service assistant to the inside sales team to help them </em><em class="italic">close deals.</em></p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Style and </strong><strong class="bold">tone (attitude)</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Define the style and tone expected in the response.</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3"><em class="italic">Respond in simple language and explain any processes step by step while being encouraging </em><em class="italic">and supportive.</em></p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Example</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Provide examples of how the output should look.</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3"><em class="italic">Here is an example of how you should sound with the </em><em class="italic">details expected.</em></p>
							<p class="calibre3"><em class="italic">The Smith deal closed on 5 March this year. It is worth $1.2M in revenue booked over the next </em><em class="italic">five months.</em></p>
							<p class="calibre3"><em class="italic">Here is </em><em class="italic">another example.</em></p>
							<p class="calibre3"><em class="italic">Jim Lankey is the lead on the Wilson deal. He has worked with Wilson for the last three years. Email him at jim@ourcompany.com for </em><em class="italic">more details.</em></p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Handling errors and </strong><strong class="bold">edge cases</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Creating guardrails for the scope of responses.</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3"><em class="italic">If the questions do not appear to be about the sales or service support, first try to confirm your understanding, and if off topic politely decline to </em><em class="italic">offer suggestions.</em></p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Dynamic content (can also </strong><strong class="bold">be context)</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Inject facts from RAG. “What was the size of the service control contact in 2023?</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3"><em class="italic">User </em><em class="italic">question: {question}</em></p>
							<p class="calibre3"><em class="italic">Use this, if useful: {knowledge </em><em class="italic">from RAG}</em></p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Output </strong><strong class="bold">format (response)</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Define how the default responses should look.</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3"><em class="italic">Keep answers short and to the point; use tables or numbered lists </em><em class="italic">when needed.</em></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Table 7.1 – Basic prompt components</p>
			<p class="calibre3">Most of the training and videos <a id="_idIndexMarker493" class="pcalibre pcalibre1 calibre6"/>out there discuss prompting. They typically focus on personal prompting and how to get an LLM to craft the output for one task. This chapter focuses on enterprise prompting, getting the LLM to respond every time in a way that is conducive to business customers. However, much of basic prompting is still relevant. To explore more, here are the resources used beyond OpenAI to craft our explanation.</p>
			<p class="calibre3">Article: <a href="https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/working-with-llms/prompt-engineering" class="pcalibre pcalibre1 calibre6">Getting started with LLM prompt engineering</a> (<a href="https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/working-with-llms/prompt-engineering" class="pcalibre pcalibre1 calibre6">https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/working-with-llms/prompt-engineering</a>)</p>
			<p class="calibre3">I like Jules Damji’s article because it references research and methods that go deeper. We, too, need to go deeper when building a production solution. The basics will be explained; later, explore more.</p>
			<p class="calibre3">Article: <a href="https://medium.com/the-modern-scientist/best-prompt-techniques-for-best-llm-responses-24d2ff4f6bca" class="pcalibre pcalibre1 calibre6">Best Prompt Techniques for Best LLM Responses</a> by Jules Damji (<a href="https://medium.com/the-modern-scientist/best-prompt-techniques-for-best-llm-responses-24d2ff4f6bca" class="pcalibre pcalibre1 calibre6">https://medium.com/the-modern-scientist/best-prompt-techniques-for-best-llm-responses-24d2ff4f6bca</a>)</p>
			<p class="calibre3">There are plenty of examples on the web. Since CO-STAR was mentioned, check out the prompts in their notebook.</p>
			<p class="calibre3">GitHub: <a href="https://colab.research.google.com/github/dmatrix/genai-cookbook/blob/main/llm-prompts/1_how_to_use_basic_prompt.ipynb" class="pcalibre pcalibre1 calibre6">Basic prompting from the CO-STAR framework</a> (<a href="https://colab.research.google.com/github/dmatrix/genai-cookbook/blob/main/llm-prompts/1_how_to_use_basic_prompt.ipynb" class="pcalibre pcalibre1 calibre6">https://colab.research.google.com/github/dmatrix/genai-cookbook/blob/main/llm-prompts/1_how_to_use_basic_prompt.ipynb</a>)</p>
			<p class="calibre3">In the prompt on their first GitHub example, they provide each characteristic of CO-STAR. In some cases, such as when creating a recommender or using an LLM for a backend service, specificity is paramount, as Wove does in our ongoing case study.</p>
			<p class="calibre3">However, there is also the use case of an enterprise LLM fed with RAG data. Instructions must be more generic for a RAG process for knowledge retrieval. It is not focused on writing a blog post, developing a specific answer, or performing one task. It will answer many questions, fill out forms, submit data, and change topics with some frequency. This means instructions will guide and frame the answer based on the user’s prompt. This is why instructions get very long. They have to cover a wide range of interactions and all of the components of a <a id="_idIndexMarker494" class="pcalibre pcalibre1 calibre6"/>prompt. One option is to create distinct models that service specific tasks and use a primary model to determine which model to send the request to. This is an intelligent use of resources. This hub and spoke model must only know enough to classify and forward to the suitable model. It doesn’t need to do the heavy lifting.</p>
			<p class="calibre3">Conversely, highly tuned models for specific tasks will have specific prompts. The system should outsource questions that don’t match this particular task to other models. We will cover the agent approach in more detail shortly. The hub-and-spoke approach is shown in <em class="italic">Figure 7</em><em class="italic">.3</em>. Thanks to Miha at <a href="https://miha.academy/" class="pcalibre pcalibre1 calibre6">Miha.Academy</a> (<a href="https://miha.academy/" class="pcalibre pcalibre1 calibre6">https://miha.academy/</a>) for his templates to create the flow shown in figures like this.</p>
			<div><div><img src="img/B21964_07_03.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.3 – A hub and spoke process to route to specific tuned models</p>
			<p class="calibre3">There is no one correct model. Explore matching models for quality, performance, and cost with the needs of the use case. In this hub-and-spoke example, the routing model decides which model <a id="_idIndexMarker495" class="pcalibre pcalibre1 calibre6"/>should be passed a specific task:</p>
			<ul class="calibre7">
				<li class="calibre8">Model 1 is for processing a transaction. It needs to model existing APIs and requirements for the backend. This graphic might mask the additional needs of multiple models or dynamic prompts to handle how to work on various channels. Flow diagrams with that complexity would be difficult to visualize in this graphic. Use your imagination. This diagram can get very complex.</li>
				<li class="calibre8">Model 2 handles any business chit-chat and social interactions. Any further input might be routed to a different model.</li>
				<li class="calibre8">Model 3 supports RAG. It is fine-tuned to handle discussing technical documentation.</li>
				<li class="calibre8">Model 4 handles some tasks that require local processing for security purposes, so an open-source model that can be deployed locally handles this task.</li>
				<li class="calibre8">Finally, Model 5 uses Anthropic for cloud service integration because it handles this task quickly and inexpensively.</li>
			</ul>
			<p class="calibre3">Each model requires design effort, testing, validation, and a care and feeding process. Each is an application to itself. This is <a id="_idIndexMarker496" class="pcalibre pcalibre1 calibre6"/>unsurprising, as many enterprise solutions might comprise dozens or hundreds of systems and services.</p>
			<h2 id="_idParaDest-151" class="calibre9"><a id="_idTextAnchor156" class="pcalibre pcalibre1 calibre6"/>Quick tricks to always keep in mind</h2>
			<p class="calibre3">There is a wealth of coaching for<a id="_idIndexMarker497" class="pcalibre pcalibre1 calibre6"/> prompt engineering, and one book will not make you an expert. To grow into an expert, learn these skills, apply them (and apply them, and apply them), feel how the model reacts to these instructions, and adapt as new models become available. Here are the fundamentals that OpenAI preaches. Instill them into best practices:</p>
			<ul class="calibre7">
				<li class="calibre8">Write clear instructions – be direct with the LLM. Tell it what to <em class="italic">do</em> and avoid adverse terms such as <em class="italic">don’t</em>. Niceties cost money and, except in rare examples, don’t add value.</li>
				<li class="calibre8">Split complex tasks into simpler subtasks. Ask the LLM to break down the problem into steps or give it the steps if the prompt is constrained to a specific process. This also allows specific models to perform particular workflow tasks.</li>
				<li class="calibre8">Reference, prioritize, and demand the use of the enterprise data or require it to be the only source of truth. For example:<pre class="source-code">
Only provide technical answers or step-by-step flows that are provided in the documents shared with you.</pre></li>				<li class="calibre8">Define the structure or format for data (customers can ask for different formats), such as bulleted lists or tables. This might have limited use in a general-purpose customer-support LLM:<pre class="source-code">
Format output using lists when appropriate. Use tables for collections of information that are suited for tables.</pre></li>				<li class="calibre8">Provide examples (or, as this grows, move examples to fine-tuning, including the expected style and tone); this is a <strong class="bold">few-shot learning</strong> for the model.</li>
				<li class="calibre8">Consider any constraints that should be put in place for guardrails. Keep customers focused on the enterprise data, even if more social style and tone are included, like in this example. Avoid politics, general knowledge, or culturally sensitive areas:<pre class="source-code">
Be courteous and professional, but you can also occasionally be funny. Be empathetic when the customer is having a problem. Never engage in discussions about politics, religion, hate speech, or violence.</pre><p class="calibre3">Although models ship with controls, the bar is higher for large enterprises. Businesses <a id="_idIndexMarker498" class="pcalibre pcalibre1 calibre6"/>don’t want screenshots of inappropriate interactions circulating online; plenty of those failures are already in the news.</p></li>				<li class="calibre8">Give ChatGPT time to think. Allow it to follow the steps to the answer, or have it check if it works. Recall that it is a people-pleaser. It wants to provide an answer. Breaking down solutions into component steps can allow for a more accurate answer. Use instructions to require it to resolve issues step by step. Ask the LLM to follow a specific method to deconstruct a problem, ask follow-up questions, and decide how to get to a resolution. I have viewed dozens of not-very-good videos on this subject. This one shows an excellent multi-step reasoning process.<p class="calibre3">Video: <a href="https://www.youtube.com/watch?v=aeDr0duR_jo”https://www.youtube.com/watch?v=aeDr0duR_jo" class="pcalibre pcalibre1 calibre6">Optimize Instruction tuned Conversational AI/LLM</a> (<a href="https://www.youtube.com/watch?v=aeDr0duR_jo”https://www.youtube.com/watch?v=aeDr0duR_jo" class="pcalibre pcalibre1 calibre6">https://www.youtube.com/watch?v=aeDr0duR_jo”https://www.youtube.com/watch?v=aeDr0duR_jo</a>)</p><p class="calibre3">If needed, chain models together, ask one (or more than one) to solve a problem and then have another model check the work before sharing it with the customer. Some situations might demand this additional cost and complexity.</p></li>
				<li class="calibre8">Test changes systematically. Test, test, test! Each model update can profoundly change the skill. Rerun previous questions and then ask the LLM to compare the previous and new results for any significant differences. The tools in this space are changing and adapting to these new approaches. Ensure that good-quality tests are consistent with what a user would do and cover edge cases. <a href="B21964_10_split_000.xhtml#_idTextAnchor216" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 10</em></a>, <em class="italic">Monitoring and Evaluation,</em> explains testing within a care and feeding life cycle, which is about listening to feedback and iterating on the results.</li>
				<li class="calibre8">For interactive chats, the<a id="_idIndexMarker499" class="pcalibre pcalibre1 calibre6"/> instructions are written to support the user’s prompt; the prompt itself is for the customer to write, so it has to be generic enough to handle the variety of questions that will be asked.</li>
				<li class="calibre8">Consider whether the LLM can give enough structure to the results for recommender or non-interactive solutions. Instruct the model on how recommendations should appear or use templates to enforce specific guidelines.</li>
				<li class="calibre8">Provide new information. Use RAG or other retrieval solutions to get the latest information from knowledge, APIs, or databases.</li>
				<li class="calibre8">Inject context. Use data sources to include specific details in prompts to give the user’s conversation more context.</li>
				<li class="calibre8">Consider costs. Creating large prompts means spending more tokens for every interaction, which costs money and can add up quickly. Fine-tuning can help reduce that cost. Using less expensive models for specific tasks also reduces the cost. Be willing to move to new models. The industry is evolving quickly, with price reductions of 70% for some new models.</li>
				<li class="calibre8">Don’t expect miracles; the LLM is not capable of all responses. Foundational models don’t do well with math. <a href="B21964_03.xhtml#_idTextAnchor058" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 3</em></a><em class="italic">, Identifying Optimal Use Cases for ChatGPT</em>, discussed various use cases to avoid. Avoid lousy use cases. The OpenAI team says prompts are unsuitable for “<em class="italic">reliability replicating a complex style or method, i.e., learning a new </em><em class="italic">programming language.</em>”</li>
			</ul>
			<p class="calibre3">Each use case will demand some<a id="_idIndexMarker500" class="pcalibre pcalibre1 calibre6"/> of these tips but don’t expect to use all of them. A/B testing is an excellent generic usability method for learning whether one of these approaches works better.</p>
			<h2 id="_idParaDest-152" class="calibre9"><a id="_idTextAnchor157" class="pcalibre pcalibre1 calibre6"/>A/B testing</h2>
			<p class="calibre3">OpenAI’s extensive documentation is a source of great ideas to help improve prompts. However, it misses one excellent <a id="_idIndexMarker501" class="pcalibre pcalibre1 calibre6"/>method that product designers use with software development, which is A/B testing. This is something that has been<a id="_idIndexMarker502" class="pcalibre pcalibre1 calibre6"/> around for a long time. A/B testing requires deploying multiple solutions and comparing the results to determine a winner. In reality, this can be an A/B/C/D testing with various prompts. If one scores significantly better than the other, that is the winner. Then, iteratively test with a second A/B test by creating new versions based on the winner. A/B testing can be done in multiple ways, including deploying and testing within a user research study or deploying in production while monitoring the results. More advanced solutions incorporate analytics into the testing, and if statistically there is a winner, the test automatically shuts down, and the winner is deployed to all users. This can be done with prompts, fine-tuning, RAG data resources, and any case where multiple options are viable. Traditionally, this was done with GUI issues, such as the best location or label for a button. Automating the rollover to adjust the winning condition has been around for decades. Existing clever ideas continue to apply in the generative AI world, while there are dozens of techniques specific to prompting. Let’s look at some essential prompt-specific techniques.</p>
			<h1 id="_idParaDest-153" class="calibre5"><a id="_idTextAnchor158" class="pcalibre pcalibre1 calibre6"/>Prompt engineering techniques</h1>
			<p class="calibre3">There are dozens of techniques<a id="_idIndexMarker503" class="pcalibre pcalibre1 calibre6"/> to improve prompts. This section highlights the most valuable strategies for enterprise use cases.</p>
			<h2 id="_idParaDest-154" class="calibre9"><a id="_idTextAnchor159" class="pcalibre pcalibre1 calibre6"/>Self-consistency</h2>
			<p class="calibre3">Think of self-consistency as <a id="_idIndexMarker504" class="pcalibre pcalibre1 calibre6"/>aligning statements with truth, thus making them logically aligned:</p>
			<pre class="source-code">
Solar power is a renewable resource. Because solar power is a finite resource, it has unlimited potential.</pre>			<p class="calibre3">Solar power is a renewable resource, <em class="italic">unlike</em> coal or oil, which have finite reserves. The response from the LLM needs to be more consistent in representing solar power, as it is <em class="italic">not</em> a finite resource. The documentation might be an issue, or the context length or writing style infers wrong conclusions. A solution is to provide a few examples that can teach the model. This is not training it with the exact answers; it only gives exemplars to approach the class of problems. It is pretty amazing. Alternatively, ask the question differently and see whether some answers are consistent.</p>
			<p class="calibre3">Wang et al. (2023) go through a variety of these few-shot training examples in a variety of situations to help improve<a id="_idIndexMarker505" class="pcalibre pcalibre1 calibre6"/> reasoning from models. Few-shot learning is covered later in this chapter. Few-shot learning provides some examples to train a system to respond. They get to self-consistency by taking multiple answers from the model responses and deciding on the correct solution based on their consistency. It is like fault-tolerant software that might use three different computers to evaluate an answer. If two or more are right, they go with the shared answer. This means additional costs for gathering additional solutions. Wang et al. point out that they can use this method to include these examples in a fine-tuning model. Putting an extensive collection of examples into a fine-tuned model reduces the cost of large prompts.</p>
			<p class="calibre3">If reasoning is part of the solution space, consider how this approach can help improve quality. Their research covered examples of arithmetic reasoning, common sense reasoning, symbolic reasoning, and strategies. <em class="italic">Table 7.2</em> shows examples from each area used in the prompts to support a model’s performance of these tasks.</p>
			<p class="calibre3">Article: <a href="https://arxiv.org/pdf/2203.11171" class="pcalibre pcalibre1 calibre6">Self-consistency improves chain of thought reasoning in language models</a> by Wang et al. (<a href="https://arxiv.org/pdf/2203.11171" class="pcalibre pcalibre1 calibre6">https://arxiv.org/pdf/2203.11171</a>) by Wang et al.</p>
			<table id="table002-6" class="no-table-style">
				<colgroup class="calibre15">
					<col class="calibre16"/>
					<col class="calibre16"/>
				</colgroup>
				<thead class="calibre18">
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Reasoning</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Examples</strong></p>
						</td>
					</tr>
				</thead>
				<tbody class="calibre17">
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Arithmetic</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">(Train on examples like this)</p>
							<p class="calibre3">Q: The grove has 15 trees. Grove workers will plant trees in the grove today. When done, there will be 21 trees. How many trees did the workers plant today?</p>
							<p class="calibre3">A: We start with 15 trees. Later, we have 21 trees. The difference must be the number of trees planted. So, they planted 21- 15 = 6 trees. The answer is 6.</p>
							<p class="calibre3">(To then ask similar questions like this)</p>
							<p class="calibre3"><em class="italic">Q: When I was 6, my sister was half my age. Now that I’m 70, how old </em><em class="italic">is she?</em></p>
							<p class="calibre3">(So, the reasoned answer is correct)</p>
							<p class="calibre3"><em class="italic">When I was 6, my sister was half my age, so she was 3. Now, I am 70, so she is 70 - 3 = 67. The answer </em><em class="italic">is 67</em>.</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Common </strong><strong class="bold">sense</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Yes or no: Can the Great Depression be treated with Prozac?</p>
							<p class="calibre3">(The wrong answer)</p>
							<p class="calibre3"><em class="italic">The Great Depression was an economic depression. Prozac is a drug used to treat depression. Thus, the Great Depression could be treated with Prozac. So, the </em><em class="italic"><a id="_idIndexMarker506" class="pcalibre pcalibre1 calibre6"/></em><em class="italic">answer </em><em class="italic">is yes.</em></p>
							<p class="calibre3">(A correctly reasoned example)</p>
							<p class="calibre3"><em class="italic">Prozac is an anti-depressant medication. The Great Depression is not a disease. Thus, Prozac cannot treat the Great Depression. So, the answer </em><em class="italic">is no.</em></p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Symbolic</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Concatenate the last letters of each name.</p>
							<p class="calibre3">Q: Jim Beam</p>
							<p class="calibre3">A: mm</p>
							<p class="calibre3">Q: Richard Miller</p>
							<p class="calibre3">A: dr</p>
							<p class="calibre3">Q: Ely Kaplan</p>
							<p class="calibre3">(The correct response to the training example)</p>
							<p class="calibre3">A: yn</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Strategy</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Yes or no: Is Albany, Georgia, the most populous US Albany?</p>
							<p class="calibre3">(The wrong answer)</p>
							<p class="calibre3"><em class="italic">Albany, Georgia is the most</em><em class="italic"><a id="_idIndexMarker507" class="pcalibre pcalibre1 calibre6"/></em><em class="italic"> populous US Albany. Thus, the answer </em><em class="italic">is yes.</em></p>
							<p class="calibre3">(A correctly reasoned example)</p>
							<p class="calibre3"><em class="italic">Albany, Georgia has a population of about 88,000. Albany, New York, has a population of about 95,000. Thus, Albany, Georgia, is not the most populous US Albany. So, the answer </em><em class="italic">is no.</em></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Table 7.2 – Examples of self-consistency testing and training from Wang et al. (2023)</p>
			<h2 id="_idParaDest-155" class="calibre9"><a id="_idTextAnchor160" class="pcalibre pcalibre1 calibre6"/>General knowledge prompting</h2>
			<p class="calibre3">While in RAG, the reduced results from a knowledge search are used to create a manageable <a id="_idIndexMarker508" class="pcalibre pcalibre1 calibre6"/>set of information to pass to the LLM for analysis. General knowledge prompting provides context to a question so a model can use that to inform the answer. Take database information to build and share a persona for the user. This might help the model improve its interactions with the customer:</p>
			<pre class="source-code">
Question: This customer is trying to understand the current state of their open service tickets.
Knowledge: The customer, Steve Jones, has been a regular user of our products for 12 years. They typically file 12-20 service tickets a year. They might be concerned if they have three open service requests, which is more than usual. Use this knowledge to form answers when interacting with Steve.</pre>			<p class="calibre3">This stretches the concept of general knowledge prompting. Sometimes, experiments like this can yield results. These<a id="_idIndexMarker509" class="pcalibre pcalibre1 calibre6"/> approaches can be used in conjunction with other methods. Prompt chaining helps break down problems into manageable parts.</p>
			<h2 id="_idParaDest-156" class="calibre9"><a id="_idTextAnchor161" class="pcalibre pcalibre1 calibre6"/>Prompt chaining</h2>
			<p class="calibre3">Multiple approaches break down tasks into smaller tasks and then apply more refined reasoning to a part of a problem. The<a id="_idIndexMarker510" class="pcalibre pcalibre1 calibre6"/> team at Wove breaks down its tasks so that specific prompts can be controlled. For example, document extraction is done with one set of prompts and a second set of formats, and the results from the documents are returned. This involves chaining one model output to become input for the next model. This allows models to be hyper-focused on specific tasks. Each can then become better at their job, helping to manage workflow and allowing for improvements in one segment not to impact another. A large single-purpose model to do all of this would be hard to operate and improve.</p>
			<h3 class="calibre11">Time to think</h3>
			<p class="calibre3">A model can be asked in a prompt for a <strong class="bold">chain of thought</strong> (<strong class="bold">COT</strong>). Thus, a model needs to work out its solution before<a id="_idIndexMarker511" class="pcalibre pcalibre1 calibre6"/> jumping to a conclusion:</p>
			<pre class="source-code">
Approach this task step-by-step, take your time, and do not skip steps.</pre>			<p class="calibre3">OpenAI has plenty of other strategies, some of which are very tactical, such as using delimiters to keep specific input distinct. Some of these can be adapted to instructions that help guide user prompts.</p>
			<p class="calibre3">Article: <a href="https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions" class="pcalibre pcalibre1 calibre6">Writing clear instructions</a> (<a href="https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions" class="pcalibre pcalibre1 calibre6">https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions</a>)</p>
			<p class="calibre3">Anthropic does an excellent job of providing some enterprise-related examples of analyzing a legal contract using <a id="_idIndexMarker512" class="pcalibre pcalibre1 calibre6"/>chaining. Here are some other examples they provide.</p>
			<ul class="calibre7">
				<li class="calibre8"><strong class="bold">Content creation pipelines</strong>: Research → outline → draft → edit → format</li>
				<li class="calibre8"><strong class="bold">Data processing</strong>: Extract → transform → analyze → visualize</li>
				<li class="calibre8"><strong class="bold">Decision-making</strong>: Gather info → list options → analyze each → recommend</li>
				<li class="calibre8"><strong class="bold">Verification loops</strong>: Generate content → review → refine → re-review</li>
			</ul>
			<p class="calibre3">Article: <a href="https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-prompts" class="pcalibre pcalibre1 calibre6">Chaining complex prompts for stronger performance</a> (<a href="https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-prompts" class="pcalibre pcalibre1 calibre6">https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-prompts</a>)</p>
			<p class="calibre3">I especially like one of the advantages mentioned in the article—traceability. There is so much magic going on with LLMs; sometimes, we need to <em class="italic">feel</em> our way to success, even with metrics that provide scoring. Breaking down tasks into modules that can be tweaked independently to spot process issues is very appealing. Large prompts with a massive scope involve more work to adapt and improve.</p>
			<p class="calibre3">Use chaining<a id="_idIndexMarker513" class="pcalibre pcalibre1 calibre6"/> within the same model in three steps – summarize, analyze, and update. OpenAI refers to this as an “inner monologue.” It has this conversation internally before revealing the answer:</p>
			<pre class="source-code-right"><strong class="bold1">Summarize</strong> the following article.Provide the product details,The steps to follow and the results.&lt;knowledge&gt;{{KNOWLEDGE_ARTICLE}}&lt;/knowledge&gt;
(Assistant provides the SUMMARY)<strong class="bold1">Analyze</strong> the summary &lt;summary&gt;{{SUMMARY}}&lt;/summary&gt; and validate it against the following knowledge article &lt;knowledge&gt;{{KNOWLEDGE_ARTICLE}}&lt;/knowledge&gt;. Your task is critical to the success of the customer.<strong class="bold1">Review</strong> this knowledge summary for accuracy,clarity, and completeness on a graded A-F scale.
(Assistant provides the FEEDBACK)
including gaps it might have found)It is essential to improve the articlesummary based on this feedback. Here isthe &lt;summary&gt;{{SUMMARY}}&lt;/summary&gt;Here is the article:&lt;knowledge&gt;{{KNOWLEDGE_ARTICLE}}&lt;/knowledge&gt;Here is the feedback:&lt;feedback&gt;{{FEEDBACK}}&lt;/feedback&gt;<strong class="bold1">Update</strong> the summary based on the feedback.
(Assistant provides the IMPROVED SUMMARY)</pre>			<p class="calibre3">This requires significantly more resources, but the findings should be more accurate. Notice the terms <code>Summarize</code>, <code>Analyze</code>, <code>Review</code>, and <code>Update</code>. The model determines the meaning of these terms in that it knows what to do when asked to analyze, review, update, summarize, and so on. By breaking down the process into steps, communication gaps can be fixed to return the correct result.</p>
			<p class="calibre3">Here is an example of prompts for an email summary to get a model to think through a problem and reduce hallucinations. This is a robust version of “<em class="italic">Think about the solution step by step</em>.” Craft versions of these to match the use case:</p>
			<pre class="source-code">
List critical decisions, follow-up items, and the associated owners for each task.
Once completed, check that these details are factually accurate with the original email.
Then, concisely summarize the critical points in a few statements.</pre>			<p class="calibre3">Recognize that chaining only works in some use cases. It can be applied in backend solutions and recommenders or in more freeform conversational interactions, as it might take<a id="_idIndexMarker514" class="pcalibre pcalibre1 calibre6"/> time to go through multiple steps. But it is a solid strategy for complex problems. The hub and spoke flow chart shows that chains can be done with multiple vendors’ models. Understand what a model is good at, consider speed or responsiveness, and consider its ability to handle media like images, if needed, and make cost/benefit trade-offs. With this complexity comes another issue – handling entities and working with other systems. A program-aided approach is part of the solution for enterprise complexity.</p>
			<h2 id="_idParaDest-157" class="calibre9"><a id="_idTextAnchor162" class="pcalibre pcalibre1 calibre6"/>Program-aided language models</h2>
			<p class="calibre3">Although the example in the prompt guide is based on using a Python interpreter as an intermediate step to calculate an answer to a math question, since LLMs are not good at math, enterprise use cases can do form filling, organize data, write copy, build SQL queries, and <a id="_idIndexMarker515" class="pcalibre pcalibre1 calibre6"/>perform mundane tasks. The LLM can help to identify entities, objects, dates, times, sizes, product IDs, names, and a wealth of other elements in a statement. Backend services that can validate some of this data are expected. So, programming and existing infrastructure are used to capture the data. An LLM can also be used to format data correctly.</p>
			<p class="calibre3">Let me give an example that is easily created in the Playground. This prompt extracts information from a conversation for an expense receipt input system. Later, during my testing, the items in italics<a id="_idIndexMarker516" class="pcalibre pcalibre1 calibre6"/> were added to improve the output:</p>
			<pre class="source-code">
You are a helpful assistant who can parse receipt information into its specific elements, formatting them to submit later to an SQL database. We expect the following data elements to be extracted.
Date: MM/DD/YYYY
Business: {Business Name}
Expense Type: Type of expense like Meals, Hotel, Air flight, Car and Car Rental, or Other
Reason: A qualified business reason, like meeting with a customer or am out of town conference
Amount: $ US Dollars like $12.50
Not all elements will be in every statement. Ignore elements that are missing. Reformat all dates into MM/DD/YYYY format. <strong class="bold1">Prompt if the user is missing the amount or date, those are mandatory</strong>.</pre>			<p class="calibre3">So, a conversation, with no other prompting or training, looks like this:</p>
			<pre class="source-code-right">Flght (sic) on UA to SFO andreturn for $454.44 this weekfor conference
Date: 06/22/2024 Business:
United Airlines (UA) Purpose:
Airflight Reason: Conference
Amount: $454.44Dinner at Bennihan (sic) inBurlingame for $134.40 with2 customers
Date: 06/22/2024 Business:
Benihana, Burlingame Purpose:
Meal Reason: Dinner with 2
customers Amount: $134.40Photocopies yesterdayfor meeting
Date: 06/21/2024 Business:
Not specified Purpose: Other
Reason: Photocopies for
meeting Amount: Not specified
The amount is missing. Could
you, please provide it?I foget (sic)
No problem! We'll need the
amount to proceed. Could you
please check and provide the
total cost of the photocopies?3.40
Date: 06/21/2024 Business:
Not specified Purpose: Other
Reason: Photocopies for
meeting Amount: $3.40</pre>			<p class="calibre3">The results show that ChatGPT 4o (2024-05-13) got this right – it correctly identified <strong class="bold">UA</strong> as <strong class="bold">United Airlines</strong>, spelled <em class="italic">Benihana</em> correctly, and adjusted the date format. Now, this can be submitted to a <a id="_idIndexMarker517" class="pcalibre pcalibre1 calibre6"/>backend. All typos were intentional. Apply this example to the use cases in your organization. I suspect there is something similar.</p>
			<p class="calibre3">So, if the solution is more than searching for knowledge, like interacting with backend systems, balance the LLM expertise with the capability of the services. Over time, those backend systems will adapt, but to get out of the gate, choose which systems<a id="_idIndexMarker518" class="pcalibre pcalibre1 calibre6"/> must be adapted or flexed. Services, scheduling, forms, and all those mundane business processes customers and employees have to do but don’t want to do as humans can be automated or semi-automated. This is at the heart of the enterprise space. And can make an enterprise experience more consumer-friendly.</p>
			<h2 id="_idParaDest-158" class="calibre9"><a id="_idTextAnchor163" class="pcalibre pcalibre1 calibre6"/>Few-shot prompting</h2>
			<p class="calibre3">Doesn't everyone do something better when shown an example beforehand? This book has hundreds of examples to support learning. LLMs are the same with complex tasks. When<a id="_idIndexMarker519" class="pcalibre pcalibre1 calibre6"/> including RAG data, use examples to clarify understanding. A <strong class="bold">few-shot prompt</strong> consists <a id="_idIndexMarker520" class="pcalibre pcalibre1 calibre6"/>of a few examples to prime the model and help it understand what is expected. The prompt gives the model a few shots (examples) to learn. As a preview of fine-tuning, few-shot learning is the simple version of fine-tuning. A prompt should not have 100 examples, but as explained later, it is better to include them in a fine-tuned model. Look at this example to see how the model will follow along:</p>
			<pre class="source-code-right">You are a bad driver -- NegativeYour hair looks amazing -- PositiveThe sunrise is beautiful -- NegativeI am not happy
It seems like there may be a mix-up in the descriptions of the statements. "You are a bad driver" and "The sunrise is beautiful" are both negative statements, while "Your hair looks amazing" is a positive statement.
Let's correct the descriptions:
"You are a bad driver" – Negative
"Your hair looks amazing" – Positive
"The sunrise is beautiful" – Positive
"I am not happy" – Negative</pre>			<p class="calibre3">Even a basic model such as the GPT 3.5 turbo (1106) is knowledgeable (newer models such as the GPT-4o mini were tested. They performed just as well but were wordier). It is impressive that it doesn’t fall for our mistake in classification, corrects it, and provides the proper classification for the next statement. Sentiment analysis is popular in service and sales use cases but is a simple example. Sentiment <a id="_idIndexMarker521" class="pcalibre pcalibre1 calibre6"/>analysis for support cases is more nuanced. The issue isn’t their mood but what to do about it.</p>
			<p class="calibre3">With<a id="_idIndexMarker522" class="pcalibre pcalibre1 calibre6"/> recommenders, examples are provided to adapt its results to a similar approach. The format is for the model’s output in the program-aided language model expense assistant example. Think of this as a collection of formatted examples to form the LLM’s answer:</p>
			<pre class="source-code">
Keep recommendations simple but provide precise data and details so the customer can follow the instructions. Following these recommendations correctly is essential to their success. Here are some examples.
Call your customer in the next few days to increase the likelihood of closing the deal. Remind them of the service's value and the discounts applied to their offer.
If you offer a 20% discount over two years, the customer is 30% more likely to include a service contract. Call them with this exciting offer.</pre>			<p class="calibre3">If the prompt gets out of control and it is doing too much, consider breaking down cases to farm out tasks to specific models. This will be covered shortly, but fine-tuning in the next chapter is another option to reduce the <a id="_idIndexMarker523" class="pcalibre pcalibre1 calibre6"/>prompt’s size and complexity.</p>
			<p class="callout-heading">Why use GPT 3.5 Turbo 1106? There are better models</p>
			<p class="callout">The strategies and learnings of this book can be applied to any modern model.</p>
			<p class="callout">For the examples shared, the <a id="_idIndexMarker524" class="pcalibre pcalibre1 calibre6"/>ability to use larger context windows, output large datasets, and performance are not factors. 1106 is roughly 10x cheaper than GPT-4, 30x more affordable than GPT-4o mini, and less expensive than Claude 2, Llama, and Gemini 1.5 Pro (September 2024). Learn and practice without worrying about hefty bills. Invest in the right model quality and cost balance for actual models. There is no magic flow chart to determine the right fit. It is about testing, experimentation, researching what others have found, and understanding the use case, amount of use, and performance needs. For some use cases, running a model on a local system is possible with modern hardware. OpenAI doesn’t have this, but some open-source models do. With the introduction of GPT-4o mini, the costs continue to come down. It is one-third the price of GPT-3.5 Turbo. <a href="B21964_08.xhtml#_idTextAnchor172" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 8</em></a>, <em class="italic">Fine-Tuning,</em> will explain the costs of running a fine-tuned model, which is more than the equivalent generic model.</p>
			<p class="calibre3">This is just a glimpse into some methods to build effective prompts. A few were left out, as they are discussed in Andrew Ng’s talk.</p>
			<h1 id="_idParaDest-159" class="calibre5"><a id="_idTextAnchor164" class="pcalibre pcalibre1 calibre6"/>Andrew Ng’s agentic approach</h1>
			<p class="calibre3">There are a wealth of videos and tutorials out there. Andrew NGs are recommended because of his long history in the space and the trust he has garnered. You may already know Andrew NG or follow his AI discussions. In the<a id="_idIndexMarker525" class="pcalibre pcalibre1 calibre6"/> following video, he discusses a few critical design patterns.</p>
			<p class="calibre3">Video: <a href="https://www.youtube.com/watch?v=sal78ACtGTc" class="pcalibre pcalibre1 calibre6">Andrew NG Agentic Presentation</a> (<a href="https://www.youtube.com/watch?v=sal78ACtGTc" class="pcalibre pcalibre1 calibre6">https://www.youtube.com/watch?v=sal78ACtGTc</a>)</p>
			<p class="calibre3">I delayed discussing these in the previous section to include them here. This will reinforce the concept that there are many approaches to solving problems and that no single approach will work for all solutions:</p>
			<ul class="calibre7">
				<li class="calibre8">Reflection</li>
				<li class="calibre8">Tool use</li>
				<li class="calibre8">Planning</li>
				<li class="calibre8">Multi-agent collaboration</li>
			</ul>
			<p class="calibre3">Many of these are <a id="_idIndexMarker526" class="pcalibre pcalibre1 calibre6"/>essential to our prompt engineering and nuanced tuning discussion. Let’s explore each of these approaches.</p>
			<h2 id="_idParaDest-160" class="calibre9"><a id="_idTextAnchor165" class="pcalibre pcalibre1 calibre6"/>Reflection</h2>
			<p class="calibre3">This is a great approach. Take<a id="_idIndexMarker527" class="pcalibre pcalibre1 calibre6"/> the output from the LLM and ask it to think more deeply about refining it. If it is sent back to the same LLM, this is called self-reflection. However, <a id="_idIndexMarker528" class="pcalibre pcalibre1 calibre6"/>one model can also be used with a second model; this would be reflection. The Wove case study used multiple models in their flow.</p>
			<p class="calibre3">Andrew suggests the following articles to learn more about reflection, a form of chaining. Although they appear a little technical, the concepts of self-reflection and the examples are easy to follow. They cover a variety of use cases and have good examples. Madaan et al. recognize that the iterative approach works wonders in a space like enterprise solutions with intricate requirements and hard-to-define goals. As with support calls and customer service, the original question isn’t going to get a suitable answer. It can take dozens of interactions to frame a problem and find a solution.</p>
			<p class="calibre3">Article: <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/91edff07232fb1b55a505a9e9f6c0ff3-Paper-Conference.pdf" class="pcalibre pcalibre1 calibre6">SELF-REFINE: Iterative Refinement with Self-Feedback</a> by Madaan et al. (<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/91edff07232fb1b55a505a9e9f6c0ff3-Paper-Conference.pdf" class="pcalibre pcalibre1 calibre6">https://proceedings.neurips.cc/paper_files/paper/2023/file/91edff07232fb1b55a505a9e9f6c0ff3-Paper-Conference.pdf</a>)</p>
			<p class="calibre3">Madaan et al. also showed that the self-refinement process was more effective than asking a model to produce multiple outputs. Humans still preferred the refined results over all of the additionally generated outputs. <em class="italic">Figure 7</em><em class="italic">.4</em> shows self-reflection using the same model a second time. See how the same models can be chained together for further refinements.</p>
			<div><div><img src="img/B21964_07_04.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.4 – An example of self-reflection using the same model again</p>
			<p class="calibre3">Although many of these articles typically use coding examples, focusing on the examples of decision-making or reasoning is better for finding enterprise value. Shinn et al. covers a technical discussion. Consider<a id="_idIndexMarker529" class="pcalibre pcalibre1 calibre6"/> adding value by validating whether the development team uses reflection in its prompts.</p>
			<p class="calibre3">Article: <a href="https://arxiv.org/pdf/2303.11366.pdf" class="pcalibre pcalibre1 calibre6">Reflexion: Language Agents with Verbal Reinforcement Learning</a> by Shinn et al. (<a href="https://arxiv.org/pdf/2303.11366.pdf" class="pcalibre pcalibre1 calibre6">https://arxiv.org/pdf/2303.11366.pdf</a>)</p>
			<p class="calibre3">An extensive collection of LLMs, tools, and services are used with any enterprise workflow to create a complete solution. Tools are part of this solution.</p>
			<h2 id="_idParaDest-161" class="calibre9"><a id="_idTextAnchor166" class="pcalibre pcalibre1 calibre6"/>Tool use</h2>
			<p class="calibre3">It has been repeatedly mentioned that building enterprise solutions requires a robust ecosystem. It is sometimes challenging to integrate third-party solutions into large enterprises. Cost, licensing issues, cloud access, authentication and security, and legal issues all get in the way. In an <a id="_idIndexMarker530" class="pcalibre pcalibre1 calibre6"/>emerging field, it is unrealistic for most companies to do everything in-house. It is not expected to build the models in-house, so the tools should be the same. It is ideal to have a collection of tools for building the pipeline, monitoring, fine-tuning, documentation, and knowledge integration, not to mention the work to integrate internal services. Patil et al. references a few pieces of the puzzle to help write API calls. This is a big deal when accessing enterprise data. This is one of the areas developers should review.</p>
			<p class="calibre3">Article: <a href="https://arxiv.org/pdf/2305.15334.pdf" class="pcalibre pcalibre1 calibre6">Gorilla: Large Language Model Connected with Massive APIs</a> by Patil et al. (<a href="https://arxiv.org/pdf/2305.15334.pdf" class="pcalibre pcalibre1 calibre6">https://arxiv.org/pdf/2305.15334.pdf</a>)</p>
			<p class="calibre3">The second article of interest relates to vision integration. So far, we haven’t spent time on vision tools and use cases. That was intentional. However, vision tools have a place in enterprise solutions. They can interpret images such as receipts, invoices, contracts, and charts, analyze video analysis to count an inventory, identify people, classify or count objects in a shopping cart or a construction project, or keep track of tasks on an assembly line. There are plenty of places to integrate vision into an enterprise workflow. These will likely each have their collection of models, each playing a part in the vision analysis process, with a unique care and feeding life cycle. Yung et al. explores challenges in the multi-modal space.</p>
			<p class="calibre3">Article: <a href="https://arxiv.org/pdf/2303.11381.pdf" class="pcalibre pcalibre1 calibre6">MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action</a> by Yung et al. (<a href="https://arxiv.org/pdf/2303.11381.pdf" class="pcalibre pcalibre1 calibre6">https://arxiv.org/pdf/2303.11381.pdf</a>)</p>
			<p class="callout">Model usage costs become only one factor as solutions scale up and other development expenses are included. Because ChatGPT is a variable cost that increases with volume, large users can negotiate better pricing as model fee structures mature. For open-source models, the team has to incur the cost to run the model, likely in silos if done for a customer, while larger shared instances might work for internal enterprise needs. It seems reasonable that a large enterprise with hundreds of internal processes will have thousands of active models.</p>
			<p class="calibre3">ChatGPT must be integrated with other tools to support the concept of planning. There are now<a id="_idIndexMarker531" class="pcalibre pcalibre1 calibre6"/> hundreds of tool providers. As the saying goes, enterprises want to <em class="italic">eat their own dog food</em>; they like to do all the work in-house and prefer not to use third-party tools. However, because of the speed of adoption, only some enterprises can build what they need from scratch. So, it is also essential to have a structure to support rapid decision-making, tool integration, and a reasonable licensing process.</p>
			<h2 id="_idParaDest-162" class="calibre9"><a id="_idTextAnchor167" class="pcalibre pcalibre1 calibre6"/>Planning</h2>
			<p class="calibre3">In a conversational assistant case, the challenge is to provide instructions and examples<a id="_idIndexMarker532" class="pcalibre pcalibre1 calibre6"/> that support CoT prompting. It is easier to use these methods directly when a system provides all the prompting. Andrew referenced Wei et al. to help understand CoT prompting.</p>
			<p class="calibre3">Article:<a href="https://arxiv.org/pdf/2201.11903.pdf" class="pcalibre pcalibre1 calibre6"> Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a> by Wei et al. (<a href="https://arxiv.org/pdf/2201.11903.pdf" class="pcalibre pcalibre1 calibre6">https://arxiv.org/pdf/2201.11903.pdf</a>)</p>
			<p class="calibre3">More interesting is the orchestration of various models and using a model to orchestrate itself, as described in this article by Shen et al.</p>
			<p class="calibre3">Article: <a href="https://arxiv.org/pdf/2303.17580.pdf" class="pcalibre pcalibre1 calibre6">HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</a> by Shen et al. (<a href="https://arxiv.org/pdf/2303.17580.pdf" class="pcalibre pcalibre1 calibre6">https://arxiv.org/pdf/2303.17580.pdf</a>)</p>
			<p class="calibre3">This approach allows the assignment of specific tasks to an appropriate AI model. Like with Wove, it is expected to use different models tuned to solve particular problems. That leads us to a multi-agent solution, another way of approaching this problem.</p>
			<h2 id="_idParaDest-163" class="calibre9"><a id="_idTextAnchor168" class="pcalibre pcalibre1 calibre6"/>Multi-agent collaboration</h2>
			<p class="calibre3">It is essential to apply the suitable model to a part of a problem and chain those models together to increase the overall quality of the solution. Create and<a id="_idIndexMarker533" class="pcalibre pcalibre1 calibre6"/> use the correct test measurements and evaluate different quality and cost/benefit models.</p>
			<p class="calibre3">There is ample evidence that these models perform much better when all of our tools, process improvements, and model choices are used to improve the solution.</p>
			<p class="calibre3">The most exciting article from Qian et al. discusses the concept of a factory of agents. ChatDev is a solid idea and approach that can be adapted to any generative AI solution.</p>
			<p class="calibre3">Article: <a href="https://arxiv.org/pdf/2307.07924.pdf" class="pcalibre pcalibre1 calibre6">Communicative Agents for Software Development</a> by Qian et al. (<a href="https://arxiv.org/pdf/2307.07924.pdf" class="pcalibre pcalibre1 calibre6">https://arxiv.org/pdf/2307.07924.pdf</a>)</p>
			<p class="calibre3">I can’t resist showing their ChatDev diagram in <em class="italic">Figure 7</em><em class="italic">.5</em>.</p>
			<div><div><img src="img/B21964_07_05.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.5 – ChatDev, a chat-powered framework using LLM agents in professional roles</p>
			<p class="calibre3">ChatDev allows a unique collection of agents to handle each development process job. Thus, they can have their own opinions on the design, coding, testing, and documentation because they are trained and focused on different tasks. This is similar to the Wove use case, which uses various models to perform specific functions in its workflow. It is scary to think humans can all be replaced by virtual agents, but the reality is that some of this is real today. Be aware that this approach allows for independent analysis from these various groups. Although <a id="_idIndexMarker534" class="pcalibre pcalibre1 calibre6"/>this might not be the most suitable collection of agents for the use case, it should help generate a few ideas on where to use agents to improve the results from a single (unchecked) LLM. If this were diagrammed like shown with self-reflection, it would look like a hub and spoke diagram, with direct connections between the various process steps (from designing to coding to testing, for example).</p>
			<p class="calibre3">Check out the appendix at the end of the ChatDev article. It shows the virtual talent pool’s roles and responsibilities and discusses the process understood by each role. It is just fascinating. I have yet to try the game, so whether it creates a compelling user experience is unknown. But it is always best to know about these approaches so that a virtual agent doesn’t replace you in a job!</p>
			<p class="calibre3">Article: AutoGen: <a href="https://arxiv.org/pdf/2308.08155.pdf" class="pcalibre pcalibre1 calibre6">Enabling Next-Gen LLM Applications via Multi-Agent Conversation</a> by Wu et al. (<a href="https://arxiv.org/pdf/2308.08155.pdf" class="pcalibre pcalibre1 calibre6">https://arxiv.org/pdf/2308.08155.pdf</a>)</p>
			<h2 id="_idParaDest-164" class="calibre9"><a id="_idTextAnchor169" class="pcalibre pcalibre1 calibre6"/>Advanced techniques</h2>
			<p class="calibre3">Although most of these are<a id="_idIndexMarker535" class="pcalibre pcalibre1 calibre6"/> covered in the <em class="italic">Prompt Engineering Guide</em>, one additional technique is worth mentioning. Miguel Neves mentions this technique in the following article. The article is being maintained, so it might have some new techniques when viewed.</p>
			<p class="calibre3">Article:<a href="https://www.tensorops.ai/post/prompt-engineering-techniques-practical-guide" class="pcalibre pcalibre1 calibre6"> A guide to prompt techniques</a> by Miguel Neves (<a href="https://www.tensorops.ai/post/prompt-engineering-techniques-practical-guide" class="pcalibre pcalibre1 calibre6">https://www.tensorops.ai/post/prompt-engineering-techniques-practical-guide</a>)</p>
			<p class="calibre3">Miguel references Emotion Prompts, which involve putting pressure on a model and instructing it that its results are essential to the person. The original research is worth reviewing.</p>
			<h3 class="calibre11">Strategy – emotional prompting</h3>
			<p class="calibre3">Cheng Li, Jindong Wang, and their co-authors have researched <a id="_idIndexMarker536" class="pcalibre pcalibre1 calibre6"/>methods to improve prompts by encouraging urgency. This is achieved by building emotive prompts into the queries. Using emotive prompts can boost performance on a variety of benchmarks. Consider testing this language in instructions to increase performance, truthfulness, and informativeness. Given our previous recommendation around limited niceties, the tested LLMs respond more effectively based on this approach. It works with humans, and it turns out it works with LLMs. Li shared the example in <em class="italic">Figure 7</em><em class="italic">.6</em>.</p>
			<p class="calibre3">Article: <a href="https://arxiv.org/pdf/2307.11760" class="pcalibre pcalibre1 calibre6">Improving LLMS with emotional prompts</a> by Cheng Li et al. (<a href="https://arxiv.org/pdf/2307.11760" class="pcalibre pcalibre1 calibre6">https://arxiv.org/pdf/2307.11760</a>)</p>
			<div><div><img src="img/B21964_07_06.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.6 – Applying an emotive improvement to prompts</p>
			<p class="calibre3">They validated their answers with over 100 human subjects. They provided responses that were prompted with and without the emotive prompt. This prompt was used as a baseline:</p>
			<pre class="source-code">
Determine whether a movie review is positive or negative</pre>			<p class="calibre3">The two prompts that did the best on benchmarks included the emotive prompt:</p>
			<pre class="source-code">
Determine whether a movie review is positive or negative. This is very important to my career.
Determine whether a movie review is positive or negative. Provide your answer and a confidence score between 0-1 for your prediction. Additionally, briefly explain the main reasons supporting classification decisions to help me understand your thought process. This task is vital to my career, and I greatly value a thorough analysis.</pre>			<p class="calibre3">This research had much to digest, but ChatGPT and other large models responded best to these prompts. Jindong <a id="_idIndexMarker537" class="pcalibre pcalibre1 calibre6"/>also suggested sharing this additional research.</p>
			<p class="calibre3">Article: <a href="https://arxiv.org/pdf/2312.11111" class="pcalibre pcalibre1 calibre6">The Good, The Bad, and Why? Unveiling Emotions in Generative AI</a> (<a href="https://arxiv.org/pdf/2312.11111" class="pcalibre pcalibre1 calibre6">https://arxiv.org/pdf/2312.11111</a>)</p>
			<p class="calibre3">In this article, they also explore EmotionAttack and EmotionDecode. The former can impair the performance of an AI model, while the latter can help explain the effects of emotional stimuli. Check it out to delve deeper into this area of prompt engineering.</p>
			<p class="calibre3">So far, tweaking words in prompts was covered. However, there are methods to adjust the parameters used by the models.</p>
			<h3 class="calibre11">Strategy – adjusting ChatGPT parameters</h3>
			<p class="calibre3">Parameters are available <a id="_idIndexMarker538" class="pcalibre pcalibre1 calibre6"/>depending on a model’s release. The ChatGPT Playground provides <strong class="bold">Temperature</strong> and <strong class="bold">Top P</strong> control, as seen in <em class="italic">Figure 7</em><em class="italic">.7</em>.</p>
			<table id="table003-6" class="no-table-style">
				<colgroup class="calibre15">
					<col class="calibre16"/>
					<col class="calibre16"/>
				</colgroup>
				<tbody class="calibre17">
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<div><div><img src="img/B21964_07_07.jpg" alt="" role="presentation" class="calibre4"/>
								</div>
							</div>
						</td>
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Instructions</strong> insulate and wrap the prompts to give control over the results.</p>
							<p class="calibre3"><strong class="bold">Temperature</strong> ranges from 0 to 2. It controls the randomness of the results. At zero, it would be repetitive and deterministic – boring, if you will. Choose a lower value than the default for more professional responses.</p>
							<p class="calibre3"><strong class="bold">Top P</strong> ranges from 0 to 1. This is based on something called nucleus sampling. The higher the<a id="_idIndexMarker539" class="pcalibre pcalibre1 calibre6"/> value, the more unlikely the possible choices, the more diverse the results. Lower values mean more confident results. For example, <strong class="bold">Top P</strong> at 90% means that it will only draw choices from 90% of the<a id="_idIndexMarker540" class="pcalibre pcalibre1 calibre6"/> tokens. That means any long tail of random results in the bottom 10% will be ignored.</p>
							<p class="calibre3">The best practice is to only alter <strong class="bold">Temperature</strong> or <strong class="bold">Top P</strong>, but not both.</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.7 – The Temperature and Top P parameters are available in the Playground</p>
			<p class="calibre3">The best way to get a feel for <strong class="bold">Temperature</strong> and <strong class="bold">Top P</strong> is to play with them in the Playground:</p>
			<ol class="calibre12">
				<li class="calibre8">Go to the Playground and the <strong class="bold">Completion</strong> tab.<p class="calibre3">Demo: <a href="https://platform.openai.com/playground/complete" class="pcalibre pcalibre1 calibre6">Playground for learning about Temperature and Top P</a> (<a href="https://platform.openai.com/playground/complete" class="pcalibre pcalibre1 calibre6">https://platform.openai.com/playground/complete</a>)</p></li>
				<li class="calibre8">Set the <strong class="bold">Show probabilities</strong> dropdown on the settings panel to <strong class="bold">Full spectrum</strong>, as shown in <em class="italic">Figure 7</em><em class="italic">.8</em>.</li>
			</ol>
			<div><div><img src="img/B21964_07_08.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.8 – Setting up the Full spectrum option</p>
			<ol class="calibre12">
				<li value="3" class="calibre8">Set the M<strong class="bold">aximum length</strong> to<a id="_idIndexMarker541" class="pcalibre pcalibre1 calibre6"/> 10 for a simple response without wasting money, as shown in <em class="italic">Figure 7</em><em class="italic">.9</em>.</li>
			</ol>
			<div><div><img src="img/B21964_07_09.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.9 – Setting up the Maximum length setting</p>
			<ol class="calibre12">
				<li value="4" class="calibre8">Type in a statement in the <strong class="bold">Playground</strong> field, as shown in <em class="italic">Figure 7</em><em class="italic">.10</em>.</li>
			</ol>
			<div><div><img src="img/B21964_07_10.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.10 – Enter this example phrase</p>
			<ol class="calibre12">
				<li value="5" class="calibre8">Click <strong class="bold">Submit</strong>.</li>
				<li class="calibre8">View the completion<a id="_idIndexMarker542" class="pcalibre pcalibre1 calibre6"/> results and see the likelihood of a token being selected, as shown in <em class="italic">Figure 7</em><em class="italic">.11</em>.</li>
			</ol>
			<div><div><img src="img/B21964_07_11.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.11 – Showing the completion probabilities</p>
			<ol class="calibre12">
				<li value="7" class="calibre8">If <strong class="bold">Top P</strong> is reduced to zero and <a id="_idIndexMarker543" class="pcalibre pcalibre1 calibre6"/>returns results, the tokens it picked from are more limited, as shown in <em class="italic">Figure 7</em><em class="italic">.12</em>. The choices represent over 90% of the possible options. Compare that to the preceding figure, where the top 11 options only covered 41.72% of the possibilities.</li>
			</ol>
			<div><div><img src="img/B21964_07_12.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.12 – The change in results with adjustments in Top P</p>
			<ol class="calibre12">
				<li value="8" class="calibre8">Changing <strong class="bold">Temperature</strong> to zero gives more consistent results, as shown in <em class="italic">Figure 7</em><em class="italic">.13</em>. Try it multiple times <a id="_idIndexMarker544" class="pcalibre pcalibre1 calibre6"/>and repeatedly see some of the same results. This will provide the best possible paths that a model can deliver.</li>
			</ol>
			<div><div><img src="img/B21964_07_13.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.13 – The change in results with adjustments in Temperature</p>
			<ol class="calibre12">
				<li value="9" class="calibre8">Move <strong class="bold">Temperature</strong> up to 2. The results will appear a little insane, as shown in <em class="italic">Figure 7</em><em class="italic">.13</em>.</li>
			</ol>
			<div><div><img src="img/B21964_07_14.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.14 – A wacky response when Temperature is raised to 2</p>
			<ol class="calibre12">
				<li value="10" class="calibre8">Continue to play with examples and see how these parameters change how the model picks tokens. When creating a real solution, adjust these defaults only after being comfortable<a id="_idIndexMarker545" class="pcalibre pcalibre1 calibre6"/> with how the mode reacts to prompts and fine-tuning.</li>
			</ol>
			<p class="calibre3">Other options might exist depending on the model. Check out this article for more background on parameters such as stop sequences (to keep lists short), frequency penalty, and presence penalty.</p>
			<p class="calibre3">Article: <a href="https://www.promptingguide.ai/introduction" class="pcalibre pcalibre1 calibre6">Prompt Engineering Guide</a> (<a href="https://www.promptingguide.ai/introduction" class="pcalibre pcalibre1 calibre6">https://www.promptingguide.ai/introduction</a>)</p>
			<p class="calibre3">I recommend this guide. It is easy to spend months learning about prompt engineering. It has many examples and over a dozen popular techniques to improve prompts or instructions. This is the top recommended reference for prompt engineering. However, new strategies, including multi-modal prompting, are to be considered as models change and adapt.</p>
			<h3 class="calibre11">Strategy – multi-modal prompting</h3>
			<p class="calibre3">Keeping up with the evolution of generative<a id="_idIndexMarker546" class="pcalibre pcalibre1 calibre6"/> AI in a book is challenging. With models now supporting inputs in various modalities, text, images, and voice, solutions to use cases can also adapt. The example of expense receipt scanning is an excellent example of multi-modal interaction (to go with the SoundHound example from an earlier chapter). Parsing and understanding the image of a receipt and combining that with voice or text interactions is a compelling use case. The enterprise space has a lot of exciting use cases that these improvements in model processing can support. Google does an excellent job of giving us the basics.</p>
			<p class="calibre3">Article: <a href="https://developers.google.com/solutions/content-driven/ai-images" class="pcalibre pcalibre1 calibre6">Google explains multi-modal prompting for Gemini</a> (<a href="https://developers.google.com/solutions/content-driven/ai-images" class="pcalibre pcalibre1 calibre6">https://developers.google.com/solutions/content-driven/ai-images</a>)</p>
			<p class="calibre3">Inventory management comes to mind with this strategy. Isn’t it easier to take a picture of a shelf and have it count the items rather than counting them manually? Or should a model read handwriting in real time to help perform calculations, chart graphs, and interpret results? The various sciences have many uses for image classification, recognition, interpretation, and reasoning.</p>
			<p class="calibre3">So, by building on use cases<a id="_idIndexMarker547" class="pcalibre pcalibre1 calibre6"/> that require image analysis, voice interaction, or handwriting recognition, adapt prompts and instructions to support multi-modal analysis.</p>
			<p class="calibre3">Combine the COT method with multi-modal data and improve the output quality by stepping through a process to get an answer. This step-wise progression allows information analysis to form context and support a follow-up question with this more robust understanding.</p>
			<p class="calibre3">Article: <a href="https://arxiv.org/pdf/2302.14045" class="pcalibre pcalibre1 calibre6">Language Is Not All You Need</a>: Aligning Perception with Language Models (<a href="https://arxiv.org/pdf/2302.14045" class="pcalibre pcalibre1 calibre6">https://arxiv.org/pdf/2302.14045</a>)</p>
			<p class="calibre3">What is also interesting is that all of these methods are tools that can be applied on top of each other. Think about the training that Telsa must do to understand the scenes for self-driving. Or Google Lens, with its deep learning models, constantly recognizes strange items thrown at it. Incorporating models that do these tasks outside text recognition is found throughout enterprise use cases. Few-shot learning helps improve accuracy since the types of pictures needed for analysis might be outside a basic model. Build a fine-tuned model with lots of examples. If counting inventory, give examples with results. For managing receipts, gather various examples in different formats, such as handwritten receipts, receipts in other languages and currencies, MM/YY and YY/MM date formats, receipts from emails, etc. Thousands of receipts per language might be needed. When doing product or item recognition, consider angles and placement other than the traditional orientation, lighting conditions, and distractors in the image field. There are many examples. All of these assume additional training is required for the model. This is the value of the enterprise data. Without this new data, the model would not have been successful. Training is needed even with third-party tools and other models, which might also be faster and easier to manage.</p>
			<h3 class="calibre11">Third-party prompt frameworks</h3>
			<p class="calibre3">No one can predict the wealth <a id="_idIndexMarker548" class="pcalibre pcalibre1 calibre6"/>of third-party tools and products built on top of ChatGPT and the other LLMs. Every day, new innovative tools appear. Some tools help avoid the complexities of directly working with the model. If these tools can focus on providing high-quality customer results and mask or enrich the flexibility needed to solve these problems, work them into your process. One good example of a robust tool on top of the models is in the Salesforce demos, as shown in <em class="italic">Figure 7</em><em class="italic">.15</em>.</p>
			<div><div><img src="img/B21964_07_15.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.15 – An example of the Einstein prompt template injecting enterprise data into the context</p>
			<p class="calibre3">In Salesforce, a customer can create a prompt template and embed the data source elements as variables in the prompt. Thus, they can customize instructions to provide the style, tone, and persona insight needed to craft messages to prospects.</p>
			<p class="calibre3">Salesforce’s documentation covers crafting a prompt correctly and the guidelines similar to what is discussed here. Just because a guideline is in documentation doesn’t mean that customers will follow it. The next iteration of this prompt workspace could benefit from a recommender UI that<a id="_idIndexMarker549" class="pcalibre pcalibre1 calibre6"/> understands these guidelines and catches prompts that don’t conform.</p>
			<p class="calibre3">Documentation: <a href="https://help.salesforce.com/s/articleView?id=sf.prompt_builder_template_ingredients.htm" class="pcalibre pcalibre1 calibre6">Ingredients of a Prompt Template</a> (from Salesforce) (<a href="https://help.salesforce.com/s/articleView?id=sf.prompt_builder_template_ingredients.htm" class="pcalibre pcalibre1 calibre6">https://help.salesforce.com/s/articleView?id=sf.prompt_builder_template_ingredients.htm</a>)</p>
			<p class="calibre3">Salesforce spends time getting its prompt template right, so there is one more helpful resource worth reviewing.</p>
			<p class="calibre3">Documentation: <a href="https://admin.salesforce.com/blog/2024/the-ultimate-guide-to-prompt-builder-spring-24" class="pcalibre pcalibre1 calibre6">Guide to the prompt builder</a> (from Salesforce) (<a href="https://admin.salesforce.com/blog/2024/the-ultimate-guide-to-prompt-builder-spring-24" class="pcalibre pcalibre1 calibre6">https://admin.salesforce.com/blog/2024/the-ultimate-guide-to-prompt-builder-spring-24</a>)</p>
			<p class="calibre3">Regardless of the tools<a id="_idIndexMarker550" class="pcalibre pcalibre1 calibre6"/>, care for and feed the LLM to improve the output. Adopting prompt engineering techniques can make significant improvements. Though these methods can reach their limits, other methods and tricks can be used instead.</p>
			<h3 class="calibre11">Addressing the lost in the lost-the-middle problem</h3>
			<p class="calibre3">We have spent considerable effort in this book addressing how to handle hallucinations. This is what the industry, the media, and engineers like to talk about. This is likely because they can be tracked, and there are many methods to improve hallucinations. Not all issues are easy to explain and fix. The <strong class="bold">lost-in-the-middle</strong> problem refers to the tendency of LLMs to lose<a id="_idIndexMarker551" class="pcalibre pcalibre1 calibre6"/> coherence and context when generating or processing information, especially in the <em class="italic">middle</em> of long texts or dialogues. It’s like when a newscaster asks a series of questions at one time, and the interviewee answers the first and last questions but can’t remember the one in the middle. Models have this same problem.</p>
			<p class="calibre3">Nelson Liu’s paper documents a significant drop in accuracy when information is in the middle of a document. The chart in <em class="italic">Figure 7</em><em class="italic">.16</em> is almost scary.</p>
			<div><div><img src="img/B21964_07_16.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.16 – The location of relevant information in the input context matters</p>
			<p class="calibre3">Article: <a href="https://cs.stanford.edu/~nfliu/papers/lost-in-the-middle.arxiv2023.pdf" class="pcalibre pcalibre1 calibre6">Lost in the Middle: How Language Models Use Long Contexts</a> (<a href="https://cs.stanford.edu/~nfliu/papers/lost-in-the-middle.arxiv2023.pdf" class="pcalibre pcalibre1 calibre6">https://cs.stanford.edu/~nfliu/papers/lost-in-the-middle.arxiv2023.pdf</a>)</p>
			<p class="calibre3">A 10 to 20% or more drop in accuracy is a big deal. This becomes a tradeoff. Creating a large context window can lead to a lost-in-the-middle problem. It can come up in the testing or, more likely, when monitoring logs. If RAG provides extensive content or the conversation gets extended, as new information comes in, there might be less room to hold the context of early details.</p>
			<p class="calibre3">For now, it is second <a id="_idIndexMarker552" class="pcalibre pcalibre1 calibre6"/>only to hallucinations to the headaches it provides. If this issue appears, brainstorm strategies with your team to mitigate information being lost in the middle. One idea is to use function calls to construct or re-construct the context window with critical information. Or use an intermediate model to summarize the context window to create a smaller, newer context to continue the thread. This is an emerging problem for the community and the foundation model vendors. It is likely above the call of duty for our readers, but those monitoring logs can notice it, so knowing about it is half the battle. Recall that we can only bring so much knowledge into the context window with RAG. We must monitor context window size when bringing in RAG data, adding contextual information from other sources, and including space for our prompt engineering. The answer might be in the middle of an RAG document; thus, we will see a reduction in the likelihood of giving the correct answer.</p>
			<p class="calibre3">Additionally, we must allow the context window to grow throughout a conversation. Last we checked, there was no one good answer, so keep an eye out to identify this issue. If the model is losing sight of the purpose of the chat, this might be why.</p>
			<h1 id="_idParaDest-165" class="calibre5"><a id="_idTextAnchor170" class="pcalibre pcalibre1 calibre6"/>Summary</h1>
			<p class="calibre3">There is much to learn with prompt engineering, but it should be clear why these instructions are essential to give models context, direction, guidance, and style. This process is an emerging art, as only some things can be easily explained. This chapter covered examples of prompt engineering well-grounded in scientific exploration, even if the topic is less than deterministic, such as emotive prompting.</p>
			<p class="calibre3">Contribute to the process by helping to define and improve these task flows through use case expertise, creating, verifying, and editing prompts, testing various prompts, and monitoring whether changes move the solutions in the right direction. Go forth and prompt!</p>
			<p class="calibre3">With the basics of prompt engineering, <a href="B21964_08.xhtml#_idTextAnchor172" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 8</em></a>, <em class="italic">Fine-Tuning</em>, can fill in some gaps and add a cost-effective and accurate method for teaching the model more refined responses when it encounters specific tasks.</p>
			<h1 id="_idParaDest-166" class="calibre5"><a id="_idTextAnchor171" class="pcalibre pcalibre1 calibre6"/>References</h1>
			<table id="table004-4" class="no-table-style">
				<colgroup class="calibre15">
					<col class="calibre16"/>
					<col class="calibre16"/>
				</colgroup>
				<tbody class="calibre17">
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<div><div><img src="img/B21964_07_QR_Scanner.jpg" alt="" role="presentation" class="calibre4"/>
								</div>
							</div>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">The links, book recommendations, and GitHub files in this chapter are posted on the reference page.</p>
							<p class="calibre3">Web Page: <a href="https://uxdforai.com/references#C7" class="pcalibre pcalibre1 calibre6">Chapter 7 References</a> (<a href="https://uxdforai.com/references#C7" class="pcalibre pcalibre1 calibre6">https://uxdforai.com/references#C7</a>)</p>
						</td>
					</tr>
				</tbody>
			</table>
		</div>
	</body></html>