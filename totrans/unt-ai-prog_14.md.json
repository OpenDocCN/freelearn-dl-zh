["```py\ngit clone --branch release_19 https://github.com/Unity-Technologies/ml-agents.git\n```", "```py\npip3 install torch~=1.7.1 -f https://download.pytorch.org/whl/torch_stable.html\n```", "```py\npython -m pip install mlagents==0.28.0\n```", "```py\n    using System.Collections.Generic;\n    using UnityEngine;\n    using Unity.MLAgents;\n    using Unity.MLAgents.Sensors;\n    using Unity.MLAgents.Actuators;\n    public class SphereAgent : Agent {\n        Rigidbody rBody;\n        public Transform Target;\n        public float forceMultiplier = 10;\n        void Start () {\n            rBody = GetComponent<Rigidbody>();\n        }\n\n        override public void OnEpisodeBegin() {\n            if (transform.position.y < -1.0) {\n                // The agent fell\n                transform.position = Vector3.zero;\n                rBody.angularVelocity = Vector3.zero;\n                rBody.velocity = Vector3.zero;\n            } else {\n                // Move the target to a new spot\n                Target.position = new Vector3(Random.value *\n                  8 - 4, 0.5f, Random.value * 8 - 4);\n            }\n        }\n    }\n    ```", "```py\n        override public void CollectObservations(\n          VectorSensor sensor) {\n            // Calculate relative position\n            Vector3 relativePosition = \n              Target.position - transform.position;\n            // Relative position\n            sensor.AddObservation(relativePosition.x/5);\n            sensor.AddObservation(relativePosition.z / 5);\n            // Distance to edges of platform\n            sensor.AddObservation(\n             (transform.position.x + 5) / 5);\n            sensor.AddObservation(\n              (transform.position.x - 5) / 5);\n            sensor.AddObservation(\n              (transform.position.z + 5) / 5);\n            sensor.AddObservation(\n              (transform.position.z - 5) / 5);\n            // agent velocity\n            sensor.AddObservation(rBody.velocity.x / 5);\n            sensor.AddObservation(rBody.velocity.z / 5);    }\n    ```", "```py\n        public override void OnActionReceived(\n          ActionBuffers actionBuffers) {\n            // Actions, size = 2\n            Vector3 controlSignal = Vector3.zero;\n            controlSignal.x = \n              actionBuffers.ContinuousActions[0];\n            controlSignal.z = \n              actionBuffers.ContinuousActions[1];\n            rBody.AddForce(controlSignal * forceMultiplier);\n            // Rewards\n            float distanceToTarget = Vector3.Distance(this.\n              transform.localPosition, Target.localPosition);\n            // Reached target\n            if (distanceToTarget < 1.42f) {\n                SetReward(1.0f);\n                EndEpisode();\n            }\n            // Fell off platform\n            else if (this.transform.localPosition.y < 0) {\n                EndEpisode();\n            }\n        }}\n    ```", "```py\n        public override void Heuristic(\n          in ActionBuffers actionsOut) {\n            var continuousActionsOut = \n              actionsOut.ContinuousActions;\n            continuousActionsOut[0] = \n              Input.GetAxis(\"Horizontal\");\n            continuousActionsOut[1] = \n              Input.GetAxis(\"Vertical\");\n        }\n    ```", "```py\nbehaviors:\n  MovingSphere:\n    trainer_type: ppo\n    hyperparameters:\n      batch_size: 10\n      buffer_size: 100\n      learning_rate: 3.0e-4\n      beta: 5.0e-4\n      epsilon: 0.2\n      lambd: 0.99\n      num_epoch: 3\n      learning_rate_schedule: linear\n      beta_schedule: constant\n      epsilon_schedule: linear\n    network_settings:\n      normalize: false\n      hidden_units: 128\n      num_layers: 2\n    reward_signals:\n      extrinsic:\n        gamma: 0.99\n        strength: 1.0\n    max_steps: 500000\n    time_horizon: 64\n    summary_freq: 10000\n```", "```py\nmlagents-learn sphere.yaml --run-id=myMovingSphere\n```"]