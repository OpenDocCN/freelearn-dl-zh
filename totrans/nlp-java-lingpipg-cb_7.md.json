["```py\nAlice walked into the garden. She was surprised.\n```", "```py\n    John Smith went to Washington. Mr. Smith is a business man.\n    ```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter7.NamedEntityCoreference\n\n    ```", "```py\n    Reading in file :data/simpleCoref.txt \n    Sentence Text=John Smith went to Washington.\n         mention text=John Smith type=PERSON id=0\n         mention text=Washington type=LOCATION id=1\n    Sentence Text=Mr. Smith is a business man.\n         mention text=Mr. Smith type=PERSON id=0\n    ```", "```py\npublic static void main(String[] args) \n    throws ClassNotFoundException, IOException {\n  String inputDoc = args.length > 0 ? args[0] \n        : \"data/simpleCoref.txt\";\n  System.out.println(\"Reading in file :\" \n      + inputDoc);\n  TokenizerFactory mTokenizerFactory\n    = IndoEuropeanTokenizerFactory.INSTANCE;\n  SentenceModel sentenceModel\n    = new IndoEuropeanSentenceModel();\n  Chunker sentenceChunker \n    = new SentenceChunker(mTokenizerFactory,sentenceModel);\n   File modelFile  \n    = new File(\"models/ne-en-news-\"\n      + \"muc6.AbstractCharLmRescoringChunker\");\n  Chunker namedEntChunker\n    = (Chunker) AbstractExternalizable.readObject(modelFile);\n```", "```py\nMentionFactory mf = new EnglishMentionFactory();\n```", "```py\nWithinDocCoref coref = new WithinDocCoref(mf);\n```", "```py\nFile doc = new File(inputDoc);\nString text = Files.readFromFile(doc,Strings.UTF8);\nChunking sentenceChunking\n  = sentenceChunker.chunk(text);\nIterator sentenceIt \n  = sentenceChunking.chunkSet().iterator();\n\nfor (int sentenceNum = 0; sentenceIt.hasNext(); ++sentenceNum) {\n  Chunk sentenceChunk = (Chunk) sentenceIt.next();\n  String sentenceText \n    = text.substring(sentenceChunk.start(),\n          sentenceChunk.end());\n  System.out.println(\"Sentence Text=\" + sentenceText);\n\n  Chunking neChunking = namedEntChunker.chunk(sentenceText);\n```", "```py\nChunking neChunking = namedEntChunker.chunk(sentenceText);\nfor (Chunk neChunk : neChunking.chunkSet()) {\n```", "```py\nString mentionText\n  = sentenceText.substring(neChunk.start(),\n          neChunk.end());\nString mentionType = neChunk.type();\nMention mention = mf.create(mentionText,mentionType);\n```", "```py\nint mentionId = coref.resolveMention(mention,sentenceNum);\n\nSystem.out.println(\"     mention text=\" + mentionText\n            + \" type=\" + mentionType\n            + \" id=\" + mentionId);\n```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter7.Coreference\n\n    ```", "```py\n    Enter text followed by new line\n    >John Smith went to Washington. He was a senator.\n    Sentence Text=John Smith went to Washington.\n    mention text=John Smith type=PERSON id=0\n    mention text=Washington type=LOCATION id=1\n    Sentence Text= He was a senator.\n    mention text=He type=MALE_PRONOUN id=0\n    ```", "```py\n    >He went to Washington.\n    Sentence Text= He went to Washington.\n    mention text=He type=MALE_PRONOUN id=-1\n    mention text=Washington type=LOCATION id=0\n    ```", "```py\n    >Jay Smith and Jim Jones went to Washington. He was a senator.\n    Sentence Text=Jay Smith and Jim Jones went to Washington.\n    mention text=Jay Smith type=PERSON id=0\n    mention text=Jim Jones type=PERSON id=1\n    mention text=Washington type=LOCATION id=2\n    Sentence Text= He was a senator.\n    mention text=He type=MALE_PRONOUN id=-1\n    ```", "```py\n    Frank Smith went to Washington. She was a senator. \n    Sentence Text=Frank Smith went to Washington.\n         mention text=Frank Smith type=PERSON id=0\n         mention text=Washington type=LOCATION id=1\n    Sentence Text=She was a senator.\n         mention text=She type=FEMALE_PRONOUN id=0\n    ```", "```py\n    John Smith went to Washington. She was a senator. He is now a lobbyist.\n    Sentence Text=John Smith went to Washington.\n         mention text=John Smith type=PERSON id=0\n         mention text=Washington type=LOCATION id=1\n    Sentence Text=She was a senator.\n         mention text=She type=FEMALE_PRONOUN id=0\n    Sentence Text=He is now a lobbyist.\n         mention text=He type=MALE_PRONOUN id=-1\n    ```", "```py\n    >Jane Smith knows her future.\n    Sentence Text=Jane Smith knows her future.\n         mention text=Jane Smith type=PERSON id=0\n         mention text=her type=FEMALE_PRONOUN id=0\n    ```", "```py\n    John is in this sentence. Another sentence about nothing. James is in this sentence. He is here.\n    Sentence Text=John is in this sentence.\n         mention text=John type=PERSON id=0\n    Sentence Text=Another sentence about nothing.\n    Sentence Text=James is in this sentence.\n         mention text=James type=PERSON id=1\n    Sentence Text=He is here.\n         mention text=He type=MALE_PRONOUN id=1\n    ```", "```py\n    John Smith is in this sentence. Random sentence. James Smith is in this sentence. Mr. Smith is mention again here.\n    Sentence Text=John Smith is in this sentence.\n         mention text=John Smith type=PERSON id=0\n    Sentence Text=Random sentence.\n         mention text=Random type=ORGANIZATION id=1\n    Sentence Text=James Smith is in this sentence.\n         mention text=James Smith type=PERSON id=2\n    Sentence Text=Mr. Smith is mention again here.\n         mention text=Mr. Smith type=PERSON id=2\n    ```", "```py\n    John Smith is in this sentence. Random sentence. James Smith is in this sentence. Random sentence. Random sentence. Mr. Smith is here.\n    Sentence Text=John Smith is in this sentence.\n         mention text=John Smith type=PERSON id=0\n    Sentence Text=Random sentence.\n         mention text=Random type=ORGANIZATION id=1\n    Sentence Text=James Smith is in this sentence.\n         mention text=James Smith type=PERSON id=2\n    Sentence Text=Random sentence.\n         mention text=Random type=ORGANIZATION id=1\n    Sentence Text=Random sentence.\n         mention text=Random type=ORGANIZATION id=1\n    Sentence Text=Mr. Smith is here.\n         mention text=Mr. Smith type=PERSON id=3\n    ```", "```py\nChunking mentionChunking\n  = neChunker.chunk(sentenceText);\nSet<Chunk> chunkSet = new TreeSet<Chunk> (Chunk.TEXT_ORDER_COMPARATOR);\nchunkSet.addAll(mentionChunking.chunkSet());\n```", "```py\naddRegexMatchingChunks(MALE_EN_PRONOUNS,\"MALE_PRONOUN\",\n        sentenceText,chunkSet);\naddRegexMatchingChunks(FEMALE_EN_PRONOUNS,\"FEMALE_PRONOUN\",\n        sentenceText,chunkSet);\n```", "```py\nstatic Pattern MALE_EN_PRONOUNS =   Pattern.compile(\"\\\\b(He|he|Him|him)\\\\b\");\n```", "```py\nstatic void addRegexMatchingChunks(Pattern pattern, String type, String text, Set<Chunk> chunkSet) {\n\n  java.util.regex.Matcher matcher = pattern.matcher(text);\n\n  while (matcher.find()) {\n    Chunk regexChunk \n    = ChunkFactory.createChunk(matcher.start(),\n            matcher.end(),\n            type);\n    for (Chunk chunk : chunkSet) {\n    if (ChunkingImpl.overlap(chunk,regexChunk)) {\n      chunkSet.remove(chunk);\n    }\n    }\n  chunkSet.add(regexChunk);\n  }\n```", "```py\n<doc id=\"1\">\n<title/>\n<content>\nBreck Baldwin and Krishna Dayanidhi wrote a book about LingPipe. \n</content>\n</doc>\n\n<doc id=\"2\">\n<title/>\n<content>\nKrishna Dayanidhi is a developer. Breck Baldwin is too. \n</content>\n</doc>\n\n<doc id=\"3\">\n<title/>\n<content>\nK-dog likes to cook as does Breckles.\n</content>\n</doc>\n```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter7.tracker.RunTracker\n\n    ```", "```py\n    <docs>\n    <doc id=\"1\">\n    <title/>\n    <content>\n    <s index=\"0\">\n    <entity id=\"1000000001\" type=\"OTHER\">Breck Baldwin</entity> and <entity id=\"1000000002\" type=\"OTHER\">Krishna Dayanidhi</entity> wrote a book about <entity id=\"1000000003\" type=\"OTHER\">LingPipe.</entity>\n    </s>\n    </content>\n    </doc>\n    <doc id=\"2\">\n    <title/>\n    <content><s index=\"0\">\n    <entity id=\"1000000002\" type=\"OTHER\">Krishna Dayanidhi</entity> is a developer.\n    </s>\n    <s index=\"1\"><entity id=\"1000000001\" type=\"OTHER\">Breck Baldwin</entity> is too.\n    </s>\n    </content>\n    </doc>\n    <doc id=\"3\"><title/><content><s index=\"0\">K-dog likes to cook as does <entity id=\"1000000004\" start=\"28\" type=\"OTHER\">Breckles</entity>.</s></content></doc>\n    </docs>\n    ```", "```py\n    <dictionary>\n    <entity canonical=\"Breck Baldwin\" id=\"1\" speculativeAliases=\"0\" type=\"MALE\">\n      <alias xdc=\"1\">Breck Baldwin</alias>\n      <alias xdc=\"1\">Breckles</alias>\n      <alias xdc=\"0\">Breck</alias>\n    </entity>\n\n    <entity canonical=\"Krishna Dayanidhi\" id=\"2\" speculativeAliases=\"0\" type=\"MALE\">\n      <alias xdc=\"1\">Krishna Dayanidhi</alias>\n      <alias xdc=\"1\">K-Dog</alias>\n      <alias xdc=\"0\">Krishna</alias> \n    </entity>\n    ```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter7.tracker.RunTracker data/xDoc/author-dictionary.xml\n\n    ```", "```py\n    <docs>\n    <doc id=\"1\">\n    <title/>\n    <content>\n    <s index=\"0\">\n    <entity id=\"1\" type=\"MALE\">Breck Baldwin</entity> and <entity id=\"2\" type=\"MALE\">Krishna Dayanidhi</entity> wrote a book about <entity id=\"1000000001\" type=\"OTHER\">LingPipe.</entity>\n    </s>\n    </content>\n    </doc>\n    <doc id=\"2\">\n    <title/>\n    <content>\n    <s index=\"0\">\n    <entity id=\"2\" start=\"0\" type=\"MALE\">K-dog</entity> likes to cook as does <entity id=\"1\" start=\"28\" type=\"MALE\">Breckles</entity>.\n    </s>\n    </content>\n    </doc>\n    </docs>\n    ```", "```py\nid=556 type=ORGANIZATION userDefined=true allowSpec=false user XDC=[Nokia Corp., Nokia] user non-XDC=[] spec XDC=[] spec non-XDC\n=[]\n```", "```py\n    private long mLastId = FIRST_SYSTEM_ID;\n```", "```py\n    private final TokenizerFactory mTokenizerFactory;\n```", "```py\n    private final Map<Long,Entity> mIdToEntity\n        = new HashMap<Long,Entity>();\n```", "```py\nprivate final ObjectToSet<String,Entity> mXdcPhraseToEntitySet\n        = new ObjectToSet<String,Entity>();\n```", "```py\npublic synchronized OutputDocument processDocument(\n            InputDocument document) {\n\n       WithinDocCoref coref\n            = new WithinDocCoref(mMentionFactory);\n\n        String title = document.title();\n        String content = document.content();\n\n        List<String> sentenceTextList = new ArrayList<String>();\n        List<Mention[]> sentenceMentionList \n        = new ArrayList<Mention[]>();\n\n        List<int[]> mentionStartList = new ArrayList<int[]>();\n        List<int[]> mentionEndList = new ArrayList<int[]>();\n\n        int firstContentSentenceIndex\n            = processBlock(title,0,\n                           sentenceTextList,\n                           sentenceMentionList,\n                           mentionStartList,mentionEndList,\n                           coref);\n\n        processBlock(content,firstContentSentenceIndex,\n                     sentenceTextList,\n                     sentenceMentionList,\n                     mentionStartList,mentionEndList,\n                     coref);\n\n        MentionChain[] chains = coref.mentionChains();\n```", "```py\nEntity[] entities  = mXDocCoref.xdocCoref(chains);\n```", "```py\n    public Entity[] xdocCoref(MentionChain[] chains) { Entity[]\n        entities = new Entity[chains.length];\n\n        Map<MentionChain,Entity> chainToEntity\n            = new HashMap<MentionChain,Entity>();\n        ObjectToSet<Entity,MentionChain> entityToChainSet\n            = new ObjectToSet<Entity,MentionChain>();\n\n        for (MentionChain chain : chains)\n            resolveMentionChain((TTMentionChain) chain,\n                                chainToEntity, entityToChainSet);\n\n        for (int i = 0; i < chains.length; ++i) {\n            TTMentionChain chain = (TTMentionChain) chains[i];\n            Entity entity = chainToEntity.get(chain);\n\n            if (entity != null) {\n                if (Tracker.DEBUG) {\n                    System.out.println(\"XDOC: resolved to\" + entity);\n         Set chainSetForEntity = entityToChainSet.get(entity);\n                    if (chainSetForEntity.size() > 1) \n                        System.out.println(\"XDOC: multiple chains resolved to same entity \" + entity.id());\n                }\n                entities[i] = entity;\n                if (entity.addSpeculativeAliases()) \n                    addMentionChainToEntity(chain,entity);\n            } else {\n                Entity newEntity = promote(chain);\n                entities[i] = newEntity;\n            }\n        }\n        return entities;\n    }\n```", "```py\nprivate Entity promote(TTMentionChain chain) {\n    Entity entity\n        = mEntityUniverse.createEntitySpeculative(\n          chain.normalPhrases(),\n                            chain.entityType());\n        if (Tracker.DEBUG)\n            System.out.println(\"XDOC: promoted \" + entity);\n        return entity;\n    }\n```", "```py\n    public Entity createEntitySpeculative(Set<String> phrases,\n                                          String entityType) {\n        Set<String> nonXdcPhrases = new HashSet<String>();\n        Set<String> xdcPhrases = new HashSet<String>();\n        for (String phrase : phrases) {\n            if (isXdcPhrase(phrase,hasMultiWordPhrases)) \n                xdcPhrases.add(phrase);\n            else\n                nonXdcPhrases.add(phrase);\n        }\n        while (mIdToEntity.containsKey(++mLastId)) ; // move up to next untaken ID\n        Entity entity = new Entity(mLastId,entityType,\n                                  null,null,xdcPhrases,nonXdcPhrases);\n        add(entity);\n        return entity;\n    }\n```", "```py\npublic boolean isXdcPhrase(String phrase,\n          boolean hasMultiWordPhrase) {\n\n    if (mXdcPhraseToEntitySet.containsKey(phrase)) {\n        return false;\n    }  \n    if (phrase.indexOf(' ') == -1 && hasMultiWordPhrase) {\n        return false;\n    }\n    if (PronounChunker.isPronominal(phrase)) {\n        return false;\n   }\n    return true;\n}\n```", "```py\npublic void add(Entity e) {\n        if (e.id() > mLastId)\n            mLastId = e.id();\n        mIdToEntity.put(new Long(e.id()),e);\n        for (String phrase : e.xdcPhrases()) {\n            mXdcPhraseToEntitySet.addMember(phrase,e);\n        }\n    }\n```", "```py\nprivate void addMentionChainToEntity(TTMentionChain chain, \n                Entity entity) {\n    for (String phrase : chain.normalPhrases()) {\n             mEntityUniverse.addPhraseToEntity(normalPhrase,\n                entity);\n        }\n    }\n```", "```py\nprivate void resolveMentionChain(TTMentionChain chain, Map<MentionChain,Entity> chainToEntity, ObjectToSet<Entity,MentionChain> entityToChainSet) {\n        if (Tracker.DEBUG)\n            System.out.println(\"XDOC: resolving mention chain \" \n          + chain);\n        int maxLengthAliasOnMentionChain = 0;\n        int maxLengthAliasResolvedToEntityFromMentionChain = -1;\n        Set<String> tokens = new HashSet<String>();\n        Set<Entity> candidateEntities = new HashSet<Entity>();\n        for (String phrase : chain.normalPhrases()) {\n        String[] phraseTokens = mEntityUniverse.normalTokens(phrase);\n         String normalPhrase \n      = mEntityUniverse.concatenateNormalTokens(phraseTokens);\n         for (int i = 0; i < phraseTokens.length; ++i) {\n                    tokens.add(phraseTokens[i]);\n    }\n         int length = phraseTokens.length;       \n         if (length > maxLengthAliasOnMentionChain) {\n                maxLengthAliasOnMentionChain = length;\n        }\n         Set<Entity> matchingEntities\n           = mEntityUniverse.xdcEntitiesWithPhrase(phrase);\n         for (Entity entity : matchingEntities) {\n           if (null != TTMatchers.unifyEntityTypes(\n            chain.entityType(),\n            entity.type())) {\n               if (maxLengthAliasResolvedToEntityFromMentionChain < length) \n                        maxLengthAliasResolvedToEntityFromMentionChain = length;\n  candidateEntities.add(entity);\n}\n}\n}   \nresolveCandidates(chain,\n                  tokens,\n                  candidateEntities,\n                          maxLengthAliasResolvedToEntityFromMentionChain == maxLengthAliasOnMentionChain,\n                          chainToEntity,\n                          entityToChainSet);}\n```", "```py\n        private void resolveCandidates(TTMentionChain chain,\n                                   Set<String> tokens,\n                                   Set<Entity> candidateEntities,\n                               boolean resolvedAtMaxLength,\n                               Map<MentionChain,Entity> chainToEntity,\n                               ObjectToSet<Entity,MentionChain> entityToChainSet) {\n        filterCandidates(chain,tokens,candidateEntities,resolvedAtMaxLength);\n        if (candidateEntities.size() == 0)\n            return;\n        if (candidateEntities.size() == 1) {\n            Entity entity = Collections.<Entity>getFirst(candidateEntities);\n            chainToEntity.put(chain,entity);\n            entityToChainSet.addMember(entity,chain);\n            return;\n        }\n        // BLOWN Uniqueness Presupposition; candidateEntities.size() > 1\n        if (Tracker.DEBUG)\n            System.out.println(\"Blown UP; candidateEntities.size()=\" + candidateEntities.size());\n    }\n```", "```py\nEntity[] entities  = mXDocCoref.xdocCoref(chains);\n```", "```py\nMap<Mention,Entity> mentionToEntityMap\n     = new HashMap<Mention,Entity>();\nfor (int i = 0; i < chains.length; ++i){ \n  for (Mention mention : chains[i].mentions()) {\n         mentionToEntityMap.put(mention,entities[i]);\n  }\n}\n```", "```py\nString[] sentenceTexts\n        = sentenceTextList\n            .<String>toArray(new String[sentenceTextList.size()])\nMention[][] sentenceMentions\n            = sentenceMentionList\n            .<Mention[]>toArray(new Mention[sentenceMentionList.size()][]);\nint[][] mentionStarts\n         = mentionStartList\n            .<int[]>toArray(new int[mentionStartList.size()][]);\n\nint[][] mentionEnds\n            = mentionEndList\n            .<int[]>toArray(new int[mentionEndList.size()][]);\n```", "```py\nChunking[] chunkings = new Chunking[sentenceTexts.length];\n  for (int i = 0; i < chunkings.length; ++i) {\n   ChunkingImpl chunking = new ChunkingImpl(sentenceTexts[i]);\n   chunkings[i] = chunking;\n   for (int j = 0; j < sentenceMentions[i].length; ++j) {\n    Mention mention = sentenceMentions[i][j];\n    Entity entity = mentionToEntityMap.get(mention);\n    if (entity == null) {\n     Chunk chunk = ChunkFactory.createChunk(mentionStarts[i][j],\n       mentionEnds[i][j],\n       mention.entityType()\n       + \":-1\");\n     //chunking.add(chunk); //uncomment to get unresolved ents as -1 indexed.\n    } else {\n     Chunk chunk = ChunkFactory.createChunk(mentionStarts[i][j],\n       mentionEnds[i][j],\n       entity.type()\n       + \":\" + entity.id());\n     chunking.add(chunk);\n    }\n   }\n  }\n```", "```py\n        // needless allocation here and last, but simple\n        Chunking[] titleChunkings = new Chunking[firstContentSentenceIndex];\n        for (int i = 0; i < titleChunkings.length; ++i)\n            titleChunkings[i] = chunkings[i];\n\n        Chunking[] bodyChunkings = new Chunking[chunkings.length - firstContentSentenceIndex];\n        for (int i = 0; i < bodyChunkings.length; ++i)\n            bodyChunkings[i] = chunkings[firstContentSentenceIndex+i];\n\n        String id = document.id();\n\n        OutputDocument result = new OutputDocument(id,titleChunkings,bodyChunkings);\n        return result;\n    }\n```", "```py\npublic static void main(String[] args) \n      throws ClassNotFoundException, IOException {\n    TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;\n    SentenceModel sentenceModel\n    = new IndoEuropeanSentenceModel();\n    SENTENCE_CHUNKER \n    = new SentenceChunker(tokenizerFactory,sentenceModel);\n    File modelFile\n    = new File(\"models/ne-en-news-muc6.AbstractCharLmRescoringChunker\");\n    NAMED_ENTITY_CHUNKER \n    = (Chunker) AbstractExternalizable.readObject(modelFile);\n```", "```py\nTfIdfDocumentDistance tfIdfDist = new TfIdfDocumentDistance(tokenizerFactory);\n```", "```py\npublic class TfIdfDocumentDistance implements Distance<Document> {\n  TfIdfDistance mTfIdfDistance;\n  public TfIdfDocumentDistance (TokenizerFactory tokenizerFactory) {\n  mTfIdfDistance = new TfIdfDistance(tokenizerFactory);\n  }\n\n   public void train(CharSequence text) {\n      mTfIdfDistance.handle(text);\n   }\n\n  @Override\n  public double distance(Document doc1, Document doc2) {\n    return mTfIdfDistance.distance(doc1.mCoreferentText,\n              doc2.mCoreferentText);\n  }\n\n}\n```", "```py\nstatic class Document {\n        final File mFile;\n        final CharSequence mText; \n        final CharSequence mCoreferentText;\n        Document(File file) throws IOException {\n            mFile = file; // includes name\n            mText = Files.readFromFile(file,Strings.UTF8);\n            Set<String> coreferentSents \n      = getCoreferentSents(\".*John \"                        + \"Smith.*\",mText.toString());\n            StringBuilder sb = new StringBuilder();\n            for (String sentence : coreferentSents) {\n              sb.append(sentence);\n            }\n            mCoreferentText = sb.toString();\n        }\n\n        public String toString() {\n            return mFile.getParentFile().getName() + \"/\"  \n            + mFile.getName();\n        }\n    }\n```", "```py\nstatic final Set<String> getCoreferentSents(String targetPhrase, String text) {\n     Chunking sentenceChunking\n    = SENTENCE_CHUNKER.chunk(text);\n  Iterator<Chunk> sentenceIt \n    = sentenceChunking.chunkSet().iterator();\n  int targetId = -2;\n  MentionFactory mentionFactory = new EnglishMentionFactory();\n  WithinDocCoref coref = new WithinDocCoref(mentionFactory);\n  Set<String> matchingSentenceAccumulator \n  = new HashSet<String>();\nfor (int sentenceNum = 0; sentenceIt.hasNext(); ++sentenceNum) {\n  Chunk sentenceChunk = sentenceIt.next();\n  String sentenceText \n    = text.substring(sentenceChunk.start(),\n          sentenceChunk.end());\n  Chunking neChunking\n    = NAMED_ENTITY_CHUNKER.chunk(sentenceText);\n  Set<Chunk> chunkSet \n    = new TreeSet<Chunk>(Chunk.TEXT_ORDER_COMPARATOR);\n  chunkSet.addAll(neChunking.chunkSet());      Coreference.addRegexMatchingChunks(\n    Pattern.compile(\"\\\\bJohn Smith\\\\b\"),\n            \"PERSON\",sentenceText,chunkSet);\n  Iterator<Chunk> neChunkIt = chunkSet.iterator();\n  while (neChunkIt.hasNext()) {\n    Chunk neChunk = neChunkIt.next();\n    String mentionText\n        = sentenceText.substring(neChunk.start(),\n            neChunk.end());\n    String mentionType = neChunk.type();\n    Mention mention \n    = mentionFactory.create(mentionText,mentionType);\n    int mentionId \n    = coref.resolveMention(mention,sentenceNum);\n    if (targetId == -2 && mentionText.matches(targetPhrase)) {\n    targetId = mentionId;\n    }\n    if (mentionId == targetId) {                          matchingSentenceAccumulator.add(sentenceText);\n     System.out.println(\"Adding \" + sentenceText);      \n     System.out.println(\"     mention text=\" + mentionText\n            + \" type=\" + mentionType\n            + \" id=\" + mentionId);\n     }\n  }\n}\nif (targetId == -2) {\n  System.out.println(\"!!!Missed target doc \" + text);\n}\nreturn matchingSentenceAccumulator;\n}\n```", "```py\nFile dir = new File(args[0]);\n       Set<Set<Document>> referencePartition\n            = new HashSet<Set<Document>>();\n        for (File catDir : dir.listFiles()) {\n            System.out.println(\"Category from file=\" + catDir);\n            Set<Document> docsForCat = new HashSet<Document>();\n            referencePartition.add(docsForCat);\n            for (File file : catDir.listFiles()) {\n                Document doc = new Document(file);\n                tfIdfDist.train(doc.mText);\n                docsForCat.add(doc);\n            }\n        }\n```", "```py\n        Set<Document> docSet = new HashSet<Document>();\n        for (Set<Document> cluster : referencePartition) {\n            docSet.addAll(cluster);\n        }\n```", "```py\n\n        HierarchicalClusterer<Document> clClusterer\n            = new CompleteLinkClusterer<Document>(tfIdfDist);\n        Dendrogram<Document> completeLinkDendrogram\n            = clClusterer.hierarchicalCluster(docSet);\n\n        HierarchicalClusterer<Document> slClusterer\n            = new SingleLinkClusterer<Document>(tfIdfDist);\n        Dendrogram<Document> singleLinkDendrogram\n            = slClusterer.hierarchicalCluster(docSet);\n```", "```py\nSystem.out.println();\nSystem.out.println(\" -------------------------------------------\"\n        + \"-------------\");\nSystem.out.println(\"|  K  |  Complete      |  Single        | \"\n        + \" Cross         |\");\nSystem.out.println(\"|     |  P    R    F   |  P    R    F   |  P\"\n        + \"     R    F   |\");\nSystem.out.println(\" -------------------------------------------\"\n        +\"-------------\");\nfor (int k = 1; k <= docSet.size(); ++k) {\n   Set<Set<Document>> clResponsePartition\n       = completeLinkDendrogram.partitionK(k);\n   Set<Set<Document>> slResponsePartition\n       = singleLinkDendrogram.partitionK(k);\n\n   ClusterScore<Document> scoreCL\n       = new ClusterScore<Document>(referencePartition,\n                                    clResponsePartition) PrecisionRecallEvaluation clPrEval \n      = scoreCL.equivalenceEvaluation();\n   ClusterScore<Document> scoreSL\n       = new ClusterScore<Document>(referencePartition,\n                                     slResponsePartition);\nPrecisionRecallEvaluation slPrEval \n  = scoreSL.equivalenceEvaluation();\n\nClusterScore<Document> scoreX\n    = new ClusterScore<Document>(clResponsePartition\n                                 slResponsePartition);\nPrecisionRecallEvaluation xPrEval \n  = scoreX.equivalenceEvaluation();\n\nSystem.out.printf(\"| %3d | %3.2f %3.2f %3.2f | %3.2f %3.2f %3.2f\" \n      + \" | %3.2f %3.2f %3.2f |\\n\",\n                   k,\n                   clPrEval.precision(),\n                   clPrEval.recall(),\n                   clPrEval.fMeasure(),\n                   slPrEval.precision(),\n                   slPrEval.recall(),\n                   slPrEval.fMeasure(),\n                   xPrEval.precision(),\n                   xPrEval.recall(),\n                   xPrEval.fMeasure());\n }\nSystem.out.println(\" --------------------------------------------\"\n         + \"------------\");\n}\n```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter7.JohnSmith\n\n    ```", "```py\n    Category from file=data/johnSmith/0\n    ```", "```py\n    Adding I thought John Smith marries Pocahontas.''\n         mention text=John Smith type=PERSON id=5\n    Adding He's bullets , she's arrows.''\n         mention text=He type=MALE_PRONOUN id=5\n    ```", "```py\n    --------------------------------------------------------\n    |  K  |  Complete      |  Single        \n    |     |  P    R    F   |  P    R    F   \n     --------------------------------------------------------\n    |   1 | 0.23 1.00 0.38 | 0.23 1.00 0.38 \n    |   2 | 0.28 0.64 0.39 | 0.24 1.00 0.38 \n    |   3 | 0.29 0.64 0.40 | 0.24 1.00 0.39 \n    |   4 | 0.30 0.64 0.41 | 0.24 1.00 0.39 \n    |   5 | 0.44 0.63 0.52 | 0.24 0.99 0.39 \n    |   6 | 0.45 0.63 0.52 | 0.25 0.99 0.39 \n    |   7 | 0.45 0.63 0.52 | 0.25 0.99 0.40 \n    |   8 | 0.49 0.62 0.55 | 0.25 0.99 0.40 \n    |   9 | 0.55 0.61 0.58 | 0.25 0.99 0.40 \n    |  10 | 0.55 0.61 0.58 | 0.25 0.99 0.41 \n    |  11 | 0.59 0.61 0.60 | 0.27 0.99 0.42 \n    |  12 | 0.59 0.61 0.60 | 0.27 0.98 0.42 \n    |  13 | 0.56 0.41 0.48 | 0.27 0.98 0.43 \n    |  14 | 0.71 0.41 0.52 | 0.27 0.98 0.43 \n    |  15 | 0.71 0.41 0.52 | 0.28 0.98 0.43 \n    |  16 | 0.68 0.34 0.46 | 0.28 0.98 0.44 \n    |  17 | 0.68 0.34 0.46 | 0.28 0.98 0.44 \n    |  18 | 0.69 0.34 0.46 | 0.29 0.98 0.44 \n    |  19 | 0.67 0.32 0.43 | 0.29 0.98 0.45 \n    |  20 | 0.69 0.29 0.41 | 0.29 0.98 0.45 \n    |  30 | 0.84 0.22 0.35 | 0.33 0.96 0.49 \n    |  40 | 0.88 0.18 0.30 | 0.61 0.88 0.72 \n    |  50 | 0.89 0.16 0.28 | 0.64 0.86 0.73 \n    |  60 | 0.91 0.14 0.24 | 0.66 0.77 0.71 \n    |  61 | 0.91 0.14 0.24 | 0.66 0.75 0.70 \n    |  62 | 0.93 0.14 0.24 | 0.87 0.75 0.81 \n    |  63 | 0.94 0.13 0.23 | 0.87 0.69 0.77 \n    |  64 | 0.94 0.13 0.23 | 0.87 0.69 0.77 \n    |  65 | 0.94 0.13 0.23 | 0.87 0.68 0.77 \n    |  66 | 0.94 0.13 0.23 | 0.87 0.66 0.75 \n    |  67 | 0.95 0.13 0.23 | 0.87 0.66 0.75 \n    |  68 | 0.95 0.13 0.22 | 0.95 0.66 0.78 \n    |  69 | 0.94 0.11 0.20 | 0.95 0.66 0.78 \n    |  70 | 0.94 0.11 0.20 | 0.95 0.65 0.77 \n    |  80 | 0.98 0.11 0.19 | 0.97 0.43 0.59 \n    |  90 | 0.99 0.10 0.17 | 0.97 0.30 0.46 \n    | 100 | 0.99 0.08 0.16 | 0.96 0.20 0.34 \n    | 110 | 0.99 0.07 0.14 | 1.00 0.11 0.19 \n    | 120 | 1.00 0.07 0.12 | 1.00 0.08 0.14 \n    | 130 | 1.00 0.06 0.11 | 1.00 0.06 0.12 \n    | 140 | 1.00 0.05 0.09 | 1.00 0.05 0.10 \n    | 150 | 1.00 0.04 0.08 | 1.00 0.04 0.08 \n    | 160 | 1.00 0.04 0.07 | 1.00 0.04 0.07 \n    | 170 | 1.00 0.03 0.07 | 1.00 0.03 0.07 \n    | 180 | 1.00 0.03 0.06 | 1.00 0.03 0.06 \n    | 190 | 1.00 0.02 0.05 | 1.00 0.02 0.05 \n    | 197 | 1.00 0.02 0.04 | 1.00 0.02 0.04 \n     --------------------------------------------------------\n    ```", "```py\n    B-cubed eval\n    Dist: 0.00 P: 1.00 R: 0.77 size:189\n    Dist: 0.05 P: 1.00 R: 0.80 size:171\n    Dist: 0.10 P: 1.00 R: 0.80 size:164\n    Dist: 0.15 P: 1.00 R: 0.81 size:157\n    Dist: 0.20 P: 1.00 R: 0.81 size:153\n    Dist: 0.25 P: 1.00 R: 0.82 size:148\n    Dist: 0.30 P: 1.00 R: 0.82 size:144\n    Dist: 0.35 P: 1.00 R: 0.83 size:142\n    Dist: 0.40 P: 1.00 R: 0.83 size:141\n    Dist: 0.45 P: 1.00 R: 0.83 size:141\n    Dist: 0.50 P: 1.00 R: 0.83 size:138\n    Dist: 0.55 P: 1.00 R: 0.83 size:136\n    Dist: 0.60 P: 1.00 R: 0.84 size:128\n    Dist: 0.65 P: 1.00 R: 0.84 size:119\n    Dist: 0.70 P: 1.00 R: 0.86 size:108\n    Dist: 0.75 P: 0.99 R: 0.88 size: 90\n    Dist: 0.80 P: 0.99 R: 0.94 size: 60\n    Dist: 0.85 P: 0.95 R: 0.97 size: 26\n    Dist: 0.90 P: 0.91 R: 0.99 size:  8\n    Dist: 0.95 P: 0.23 R: 1.00 size:  1\n    Dist: 1.00 P: 0.23 R: 1.00 size:  1\n    ```"]