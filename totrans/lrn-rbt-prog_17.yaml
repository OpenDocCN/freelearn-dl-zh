- en: '*Chapter 14*: Line-Following with a Camera in Python'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we saw how to use a camera to follow and track objects.
    In this chapter, we will be extending the camera code to create line-sensing behavior.
  prefs: []
  type: TYPE_NORMAL
- en: We will look at where robots use line following and how it is useful. We will
    also learn about some of the different approaches taken to following paths in
    different robots, along with their trade-offs. You will see how to build a simple
    line-following track.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn about some different algorithms to use and then choose a simple
    one. We will make a data flow diagram to see how it works, collect sample images
    to test it with, and then tune its performance based on the sample images. Along
    the way, we'll see more ways to approach computer vision and extract useful data
    from it.
  prefs: []
  type: TYPE_NORMAL
- en: We will enhance our PID code, build our line detection algorithm into robot
    driving behavior, and see the robot running with this. The chapter closes with
    ideas on how you can take this further.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to line following
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making a line-follower test track
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A line-following computer vision pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trying computer vision with test images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Line following with the PID algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding a line again
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The robot and code from [*Chapter 13*](B15660_13_Final_ASB_ePub.xhtml#_idTextAnchor283),
    *Robot Vision – Using a Pi Camera and OpenCV*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some white or some black insulating tape
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some A2 paper or boards – the opposite color to the insulating tape
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pair of scissors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good lighting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code for this section can be found at [https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter14](https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter14).
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action: [https://bit.ly/3slLzbQ](https://bit.ly/3slLzbQ)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to line following
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start building code, let's find out about line-following robot behaviors,
    where and how systems use them, and the different techniques for doing so.
  prefs: []
  type: TYPE_NORMAL
- en: What is line following?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some robots are required to stay on specific paths within their tasks. It is
    simpler for a robot to navigate a line than to plan and map whole rooms or buildings.
  prefs: []
  type: TYPE_NORMAL
- en: In simple terms, line following is being able to follow a marked path autonomously.
    These can be visual markers, such as blue tape or a white line on a black road.
    As the robot drives along the line, it will continually be looking for where the
    line ahead is and correcting its course to follow that line.
  prefs: []
  type: TYPE_NORMAL
- en: In robot competitions, racing on lines is a common challenge, with speed being
    critical after accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Usage in industry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By far the most common usage of line-following behavior is in industry. Robots
    known as **automated guided vehicles** (**AGVs**) need to follow set paths for
    many reasons. These tasks can be warehouse robots staying on tracks between aisles
    of stacked products or factory robots staying on paths clear of other work areas.
    The line may mark a route between a storage shelf and a loading bay or a robot
    charging station and the robot''s work area:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_14_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.1 – IntellCart – a line-following industrial robot by Mukeshhrs [Public
    domain]
  prefs: []
  type: TYPE_NORMAL
- en: IntelliCart, shown in *Figure 14.1*, uses bright blue guide tape, although,
    in most industrial applications, robots use under-floor magnetic tracks.
  prefs: []
  type: TYPE_NORMAL
- en: The route may include choice points, with multiple lines coming from a particular
    location. Depending on their task, the robot may need extra clues to sense that
    it has reached these points. An engineer can set up a repeated path for a fully
    automated system.
  prefs: []
  type: TYPE_NORMAL
- en: Having these demarcated means that you can set safety boundaries and be clear
    on where humans and robots do or do not interact; this means that robots will
    rarely operate outside of well-understood areas.
  prefs: []
  type: TYPE_NORMAL
- en: Types of line following
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a few major branches of line following and related systems.
  prefs: []
  type: TYPE_NORMAL
- en: '**Visual line following** is by far the most commonly practiced and easy-to-set-up
    line following technique. It consists of a painted, drawn, or taped visual line
    that robots detect. Optical lines are simple, but surface dirt and light conditions
    can make this unreliable. How it is detected falls into a couple of major categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Detected with light sensors**: In this case, we''d attach small sensors to
    a robot''s underside close to the line. They are tuned to output a binary on/off
    signal or analog signal. They usually have lights to shine off the surface. These
    are small and cheap but require extra I/O.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Detected with a camera**: This will save space if you already use a camera,
    along with I/O pins. It saves complexity in mounting them and wiring them. However,
    it comes at a trade-off cost of software complexity, as your robot needs computer
    vision algorithms to analyze this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Magnetic line following** is used when the line needs to be protected against
    the elements. Also, for some variations of this, you can guide a robot on multiple
    paths. There are the following variants:'
  prefs: []
  type: TYPE_NORMAL
- en: Running a magnetic strip along a floor allows Hall-effect sensors (such as the
    magnetometer in [*Chapter 12*](B15660_12_Final_ASB_ePub.xhtml#_idTextAnchor251),
    *IMU Programming with Python*) to detect where the strip is. A series of these
    sensors can determine the direction of a line and follow it. This can be easier
    to alter than painting a line but can be a trip hazard.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running a wire with some current through it along or under a floor will achieve
    the same effect. With multiple wires and some different circuits for them, systems
    can steer a robot onto different paths.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concealing the line under a floor removes the trip hazard but means that you
    need to paint warnings for humans on paths that industrial robots follow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, you have seen the two major types of line following; it''s worth giving
    an honorable mention to some other ways to determine a robot''s path in the real
    world:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Beacons**: Ultrasonic, light-emitting, or radio-emitting beacons can be placed
    around an environment to determine the path of a robot. These could just be reflectors
    for laser or other light.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visual clues**: If you place QR codes or other visible markers on walls and
    posts, they can encode an exact position.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You've seen how a robot can perform line sensing with visible lines and hidden
    lines, such as wires under a floor and magnetic sensors. Because it is easier,
    we will use visible lines.
  prefs: []
  type: TYPE_NORMAL
- en: The simple optical sensors require additional wiring, but if we already have
    a camera capable of this, why not make use of it?
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will be focusing on using the camera we already have with
    a visual track and following the lines there. We will accept the code complexity
    while simplifying the hardware aspects of our robot.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have some idea of the different types of line following and where
    to use them, let's create a test track that our robot can follow.
  prefs: []
  type: TYPE_NORMAL
- en: Making a line-follower test track
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since you will be making your robot follow a line, we need to start with a section
    of line to follow. The track will be used at the beginning to test our line detection
    algorithm and can then be extended to more exciting tracks when we turn on the
    motors and start driving along the line. What I will show you in this section
    is easy to make and extendable. It allows you to experiment with different line
    shapes and curves and see how the robot responds.
  prefs: []
  type: TYPE_NORMAL
- en: You can even experiment with different color and contrast options.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the test track materials in place
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following photo shows the main materials required:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_14_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.2 – Materials for making a test track
  prefs: []
  type: TYPE_NORMAL
- en: 'The photo in *Figure 14.2* shows a roll of black electrical tape on a large
    sheet of white paper. For this section, you''ll need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Some A2 plain white paper or board.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some black electrical insulation tape or painter's tape. Make sure this tape
    is opaque.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pair of scissors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could replace the paper with boards if they are white-painted.
  prefs: []
  type: TYPE_NORMAL
- en: You can also swap things around by using dark or black paper and white tape.
    This tape must not be see-through so that it makes a good strong contrast against
    the background.
  prefs: []
  type: TYPE_NORMAL
- en: Making a line
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Lay the sheet of paper flat. Then, make a line along the middle of the paper
    with the tape:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_14_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.3 – Smoothing the tape on the paper
  prefs: []
  type: TYPE_NORMAL
- en: The photos in *Figure 14.3* show the paper with the tape line and me smoothing
    the tape with my finger. Be sure to smooth the tape down. You do not need to worry
    about making it perfectly straight, as the whole point of this system is to follow
    lines even when they curve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have a few lengths of this tape on sheet, why not make a few interesting
    pieces, such as the ones in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_14_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.4 – Some different shapes adjoining a straight line
  prefs: []
  type: TYPE_NORMAL
- en: As *Figure 14.4* shows, you can experiment with curves and intentionally not-quite-straight
    lines. You can join these together with the straight lines to make whole sections,
    like a robotic train set! These will be fun to test with later as you further
    tune and play with the line-following code.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a test track ready, we can think about how we can visually
    process the line.
  prefs: []
  type: TYPE_NORMAL
- en: Line-following computer vision pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we did with the previous computer vision tasks, we will visualize this as
    a pipeline. Before we do, there are many methods for tracking a line with computer
    vision.
  prefs: []
  type: TYPE_NORMAL
- en: Camera line-tracking algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is in our interests to pick one of the simplest ones, but as always, there
    is a trade-off, in that others will cope with more tricky situations or anticipate
    curves better than ours.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a small selection of methods we could use:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Using edge detection**: An edge detection algorithm, such as the Canny edge
    detector, can be run across the image, turning any transitions it finds into edges.
    OpenCV has a built-in edge detection system if we wanted to use this. The system
    can detect dark-to-light and light-to-dark edges. It is more tolerant of less
    sharp edges.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Finding differences along lines**: This is like cheeky edge detection, but
    only on a particular row. By finding the difference between each pixel along a
    row in the image, any edges will show significant differences. It''s simpler and
    cheaper than the Canny algorithm; it can cope with edges going either way but
    requires sharp contrasts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Finding brightness and using a region over an absolute brightness as the
    line**: This is very cheap but a little too simplistic to give good results. It''s
    not tolerant to inversions but isn''t tracking edges, so doesn''t need sharp contrasts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using one of the preceding three methods, you can find the line in one picture
    area and simply aim at that. This means you won't be able to pre-empt course changes.
    It is the easiest way. The chosen area could be a single row near the bottom of
    the screen.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can use the preceding methods to detect the line throughout
    the camera image and make a trajectory for it. This is more complex but better
    able to cope with steeper turns.
  prefs: []
  type: TYPE_NORMAL
- en: It's worth noting that we could make a more efficient but more complicated algorithm
    using the raw YUV data from the Pi Camera. For simplicity, we will stick to the
    simple one. As you trade further up in complexity and understanding, you can find
    far faster and more accurate methods for this.
  prefs: []
  type: TYPE_NORMAL
- en: Another major limitation of our system is the view width of the camera. You
    could use lenses to let a camera take in a wide visual field so that the robot
    will not lose the line so often.
  prefs: []
  type: TYPE_NORMAL
- en: The method we will use is finding the differences along lines due to its simplicity
    and ability to cope with different line colors. We are also going to simply look
    along a single column, which results in more straightforward math.
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can visualize the way we process data as a pipeline. Before we can, let''s
    quickly explain discrete differences. The brightness of each pixel is a number
    between 0 (black) and 255 (white). To get the difference, you subtract each pixel
    from the pixel to the right of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_14_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.5 – Discrete differences between pixels
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 14.5*, there are six sets of pixels in varying shades:'
  prefs: []
  type: TYPE_NORMAL
- en: The first shows two white pixels. There is no difference between them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Where there is a gray pixel followed by a white one, it produces a small difference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A black pixel followed by a white pixel produces a large difference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A white pixel followed by a black pixel produces a large negative difference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A black pixel followed by a black pixel will produce no difference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A gray pixel followed by a black pixel will produce a small negative difference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It should be easy to see that a contrasting line edge will produce the largest
    differences, positive or negative. Our code will look for these.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows how we process camera data for this method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_14_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.6 – Image processing pipeline for finding a line
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 14.6*, we show the process of finding a line to follow. It starts
    with the **camera**, from which we **capture images** at a 320-by-240-pixel resolution.
    The next step in the pipeline is to **convert to grayscale** – we are only interested
    in brightness right now.
  prefs: []
  type: TYPE_NORMAL
- en: Because images can have noise or grain, we **blur** it; this is not strictly
    necessary and depends on how clear the environment you are taking pictures from
    is.
  prefs: []
  type: TYPE_NORMAL
- en: We take that image and **slice out a candidate row**; this shouldn't be too
    high in the picture, as that line may be too far away or there may be random things
    above the horizon depending on the camera position. The row shouldn't be too low
    as it will then be too close to the robot for it to react in time. Above the **slice
    out candidate row** box is an example showing the sliced-out row and the image
    it came from.
  prefs: []
  type: TYPE_NORMAL
- en: We then treat this row as a set of numbers and **get the discrete difference
    across** them. The graph above the **discrete difference** box shows a large negative
    spike as the row goes from light gray to black, followed by a large positive spike
    as the row goes from black to light gray again. Notice that much of the graph
    shows a line along zero as patches of color have no difference.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to **find the maximum and minimum positions**, specifically
    where in the row they are. We want the position/index of the highest point above
    zero and the lowest point below zero. We now know where the boundaries of our
    line probably are.
  prefs: []
  type: TYPE_NORMAL
- en: We can **find the position between** these boundaries to find the center of
    the line, by adding them together and dividing by 2; this would be an *X* position
    of the line relative to the middle of the camera image.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you've seen the pipeline with some test images. It's time to get some test
    images of your own and try this algorithm out with some code.
  prefs: []
  type: TYPE_NORMAL
- en: Trying computer vision with test images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look out how and why to use test images. We will write
    our first chunk of code for this behavior and try it on test images from our robot's
    camera. These tests will prepare us for using the code to drive the robot.
  prefs: []
  type: TYPE_NORMAL
- en: Why use test images?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, our computer vision work has been written directly with robot behaviors;
    this is the end goal of them, but sometimes, you want to try the visual processing
    code in isolation.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps you want to get it working or work out bugs in it, or you may want to
    see whether you can make the code faster and time it. To do this, it makes sense
    to run that particular code away from the robot control systems.
  prefs: []
  type: TYPE_NORMAL
- en: It also makes sense to use test images. So, instead of running the camera and
    needing light conditions, you can run with test images you've already captured
    and compare them against the result you expected from them.
  prefs: []
  type: TYPE_NORMAL
- en: For performance testing, trying the same image 100 times or the same set of
    images will give consistent results for performance measures to be meaningful.
    Avoid using new data every time, as these could result in unexpected or potentially
    noisy results. However, adding new test images to see what would happen is fascinating.
  prefs: []
  type: TYPE_NORMAL
- en: So now that we know why we use them, let's try capturing some test images.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing test images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may recall, from the previous chapter, using `raspistill` to capture an
    image. We are going to do the same here. First, we want to put our camera into
    a new position, facing down, so we are looking down onto the line.
  prefs: []
  type: TYPE_NORMAL
- en: This section requires the setup from the chapter [*Chapter 13*](B15660_13_Final_ASB_ePub.xhtml#_idTextAnchor283),
    *Robot Vision – Using a Pi Camera and OpenCV* and code from [*Chapter 9*](B15660_09_Final_ASB_ePub.xhtml#_idTextAnchor171),
    *Programming RGB Strips in Python*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Turn the motor power on to the Raspberry Pi, then with an `ssh` session into
    the Raspberry Pi on the robot, type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start Python by typing `python3`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we need to import our robot object and create it so that we can interact
    with it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s create the robot object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, use this to set the pan servo to the middle:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The pan servo should center the camera.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we set the tilt servo to face down to look at the line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The servo should look straight down here. It should not be straining or clicking.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, you can exit Python (and release the motors) by pressing *Ctrl* + *D*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The camera is facing downward. You can now turn off the motor switch and put
    this robot onto your test track. Try to position the robot so that the camera
    is right above the line.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the `ssh` terminal, type the following to capture a test image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can now download this image to your PC using FileZilla, as discussed in
    the book''s earlier chapters. The next figure shows a test image, also used for
    the preceding examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_14_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.7 – A test image of a line
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 14.7* shows one of my test images. Note that the line is roughly starting
    in the middle of the picture, but it isn''t exact and doesn''t need to be. Note
    also that the lighting is a bit rough and is creating shadows. These are worth
    watching out for as they could confuse the system.'
  prefs: []
  type: TYPE_NORMAL
- en: Capture a few images of the line at different angles to the robot and slightly
    left or slightly right of the camera.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have test images, we can write code to test them with!
  prefs: []
  type: TYPE_NORMAL
- en: Writing Python to find the edges of the line
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are ready to start writing code, using our test images and the preceding
    pipeline diagram. We can make the results quite visual so that we can see what
    the algorithm is doing.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: In computer vision, it's useful to use the lowest resolution you can to do the
    job. Every additional pixel adds more memory and processing to cope with. At 320*200,
    this is 76,800 pixels. The Raspberry Pi camera can record at 1920 x 1080 – 2,073,600
    pixels – 27 times as much data! We need this to be quick, so we keep the resolution
    low.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code in this section will run on the Raspberry Pi, but you can also run
    it on a PC with Python 3, NumPy, Matplotlib, and Python OpenCV installed:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a file called `test_line_find.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will need to import NumPy to process the image numerically, OpenCV to manipulate
    the image, and Matplotlib to graph the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we load the image. OpenCV can load `jpg` images, but if there is a problem
    in doing so, it produces an empty image. So, we need to check that it loaded something:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The captured image will be at the large default resolution of the camera. To
    keep this fast, we resize it to a smaller image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also only want grayscale; we aren''t interested in the other colors for
    this exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we''ll pick out the row; for now, we''ll use 180 as that is fairly low
    on an image with a height of 240\. The images are stored such that row 0 is the
    top. Note that we are telling NumPy to convert this into an `int32` type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can get a list of differences for every pixel of this row. NumPy makes this
    easy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are going to plot this `diff` list. We will need the *x*-axis to be the
    pixel number; let''s create a NumPy range from 0 to that range:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s plot the `diff` variable against the pixel index (`x`), and save the
    result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Pointing at my test picture, I get the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_14_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.8 – Graph of differences without blurring
  prefs: []
  type: TYPE_NORMAL
- en: 'The graph in *Figure 14.8* has the column number as the *x* axis and the difference
    as the *y* axis. The line has a lot of noise in it. There are two distinct peaks
    – one below the zero line at around column 145 and one above the line at around
    240\. The noise here doesn''t affect this too much as the peaks are very distinct:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try adding the blurring to see how that changes things. Make the following
    change to the code. The bold areas show changed sections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this code, we add the additional blurring step, blurring 5-by-5 chunks a
    little.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'So that we can see different graphs, let''s change the name of our output file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Blurring a little should reduce noise without affecting our sharp peaks too
    much. Indeed, the following figure shows how effective this is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_14_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.9 – The diff graph after blurring
  prefs: []
  type: TYPE_NORMAL
- en: The graph in *Figure 14.9* is similar to *Figure 14.8*. The axes are the same,
    and it has the same peaks. However, there is far less noise around the line at
    0, showing that blurring makes the difference clearer. The question about this
    will be whether it changes the outcome. Looking at the position and size of the
    peaks, I would say not so much. So, we can leave it out of the final follow for
    a little extra speed. Every operation will cost a little time.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the two peaks, let's use them to find the location of the line.
  prefs: []
  type: TYPE_NORMAL
- en: Locating the line from the edges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Those peaks are markers of our line edges. To find the middle of something,
    you add its left and right coordinates, then divide them by 2:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must pick up the coordinates. Let''s write code to ask for the maximum
    and minimum. We''ll add this code between the analysis code and the chart output
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'These are values, but not locations. We now need to find the index of the locations.
    NumPy has an `np.where` function to get indexes from arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To find the middle, we need to add these together and divide them by 2:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have found it, we should display it in some way. We can plot this
    on our graph with three lines. Matplotlib can specify the color and style for
    a plot. Let''s start with the middle line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can do the same for the `highest` and `lowest` locations, this time using
    `g--` for a green dashed line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As we did for blurring, let''s change the name of the output graph so that
    we can compare them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running this should output the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_14_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.10 – Graph showing the highest, lowest, and middle line
  prefs: []
  type: TYPE_NORMAL
- en: The graph in *Figure 14.10* shows that we have found the middle line and the
    two nice clear peaks. This code looks usable for our robot.
  prefs: []
  type: TYPE_NORMAL
- en: However, what happens when things are not so clear?
  prefs: []
  type: TYPE_NORMAL
- en: Trying test pictures without a clear line
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's see what our line-finding code does with a very different test picture.
    We will see what happens here, so we aren't so surprised by how the robot will
    behave and weed out some simple bugs.
  prefs: []
  type: TYPE_NORMAL
- en: For example, what about putting our line on a very noisy surface, such as a
    carpet? Or how about the paper without a line, or the carpet without a line?
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_14_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.11 – diff graphs under noisier conditions
  prefs: []
  type: TYPE_NORMAL
- en: With a set of graphs such as those in *Figure 14.11*, we learn much about our
    system.
  prefs: []
  type: TYPE_NORMAL
- en: The top three images show the original photos. The next three graphs show what
    those images look like when finding the difference and middles without blurring.
    The bottom three graphs show what happens when enabling the blur.
  prefs: []
  type: TYPE_NORMAL
- en: First, when things get as noisy as the first image (and this is pushing it past
    what line following should cope with), the blur makes the difference between finding
    the line and a random artifact; although, in the second graph, a random artifact
    with a similar downward peak size was a close contender. In this case, making
    a larger *Y* blur might smooth out that artifact, leaving only the line.
  prefs: []
  type: TYPE_NORMAL
- en: Looking closely, the scale of those graphs is also not the same. The plain paper
    graph measures a difference with peaks of +10/-10 without blurring, and +1/-1
    with blurring. So, when the differences are that low, should we even be looking
    for a peak? The story is similar in the carpet-only graphs.
  prefs: []
  type: TYPE_NORMAL
- en: We can make a few changes to our system to make it consider these as not-lines.
    The simplest is to add a condition that filters out a minimum above -5 and a maximum
    below 10\. I say -5 since this would otherwise filter out the line in the first
    graph completely. However, a larger blur area might help with that.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the noisiness of the conditions, we will want to enable the blur.
    On a nicely lit track, the blur is probably not needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next figure shows our line on the carpet, with a blur set to (5, 40), blurring
    further between rows and filtering out noise further:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_14_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.12 – The line on carpet with a larger blur
  prefs: []
  type: TYPE_NORMAL
- en: The graph in *Figure 14.12* has far less noise than before, with the blur smoothing
    out noise spikes a lot, while the actual line spikes remain. We would only want
    to do this in a noisy environment, as it risks being slower.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, testing the code on test images has allowed us to learn a lot
    about the system. By taking the same pictures and trying different parameters
    and pipeline changes, you can optimize this for different scenarios. As you experiment
    more with computer vision, make this a habit.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have tried our visual processing code on test images, it's time to put
    it on a robot behavior!
  prefs: []
  type: TYPE_NORMAL
- en: Line following with the PID algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will combine the visual processing seen previously with
    the PID control loops and camera streaming seen in [*Chapter 13*](B15660_13_Final_ASB_ePub.xhtml#_idTextAnchor283),
    *Robot Vision – Using a Pi Camera and OpenCV*. Please start from the code in that
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The files you will need are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pid_controller.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`robot.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`servos.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`camera_stream.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_app_core.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`leds_led_shim.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_counter.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The templates folder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use the same template for displaying this, but we are going to add a
    quick and cheeky way of rendering the `diff` graphs in OpenCV onto our output
    frame. Matplotlib would be too slow for this.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the behavior flow diagram
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we build a new behavior, creating a data flow diagram will help us get
    a picture of what happens to the data after we've processed it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The system will look familiar, as it is very similar to those we made in [*Chapter
    13*](B15660_13_Final_ASB_ePub.xhtml#_idTextAnchor283), *Robot Vision – Using a
    Pi Camera and OpenCV*. Take a look at the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_14_13_NEW.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.13 – The line-following behavior
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 14.13*, we have camera images going through to the **get line from
    image** block. This block outputs an object's *X* position (the middle of the
    line), which goes to our PID. Note that image data also goes from the get line
    to the image queue so that you can see these in a browser.
  prefs: []
  type: TYPE_NORMAL
- en: The PID control also takes a reference middle point, where the middle of the
    camera should be. It uses the error between these to calculate the offset and
    uses that to drive the motors.
  prefs: []
  type: TYPE_NORMAL
- en: The figure shows the motors with a feedback line to the camera, as the indirect
    effects of moving those are that the view changes, so we will see a different
    line.
  prefs: []
  type: TYPE_NORMAL
- en: Before that, we are going to make our PID controller a little smarter again.
  prefs: []
  type: TYPE_NORMAL
- en: Adding time to our PID controller
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our robot behaviors have involved processing frames, then sending error data
    to the PID whenever the process has completed a cycle. There is much going on
    in a cycle, and that timing might vary. When we create the integral, we have been
    adding the data as if the time was constant. For a somewhat more accurate picture,
    we should be multiplying that by the time:'
  prefs: []
  type: TYPE_NORMAL
- en: Open up the `pid_controller.py` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the `handle_integral` method, change the parameters to take `delta_time`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will then use this when adding in the integral term:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We usually update the PID with the `get_value` method; however, since we already
    have code using this, we should make it behave as it did before for them. To do
    this, we will add a `delta_time` parameter but with a default value of `1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When this `get_value` method calls `handle_integral`, it should always pass
    the new `delta_time` parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: While this was not a big change, it will mean we can account for time variations
    between updates to the PID code.
  prefs: []
  type: TYPE_NORMAL
- en: We can now use this in our behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Writing the initial behavior
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can take all the elements we have and combine them to create our line-following
    behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a file named `line_follow_behavior.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Start this with imports for `image_app_core`, NumPy, OpenCV, the camera stream,
    the PID controller, and the robot. We also have `time`, so we can later compute
    the delta time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s make the behavior class. The constructor, as before, takes the robot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we need variables in the constructor to track our behavior. First, we
    should set the row we will look for the differences in and a threshold (under
    which we will not consider it a line):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As with our previous camera behaviors, we have a set point for the center,
    a variable to say whether the motors should be running, and a speed to go forward
    at:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are going to make some interesting displays. We will store the colors we
    plan to use here too – a green crosshair, red for the middle line, and light blue
    for the graph. These are BGR as OpenCV expects that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That is the constructor complete for the behavior.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we need the control to say whether the system is running or should exit.
    This code should be familiar as it is similar to the other camera control behaviors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll make the `run` method, which will perform the main PID loop and
    drive the robot. We are setting the tilt servo to `90` and the pan servo to `0`,
    so it is looking straight down. We''ll set up the camera too:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we set up the PID for the direction. These values aren''t final and may
    need tuning. We have a low proportional value as the directional error can be
    quite large compared with the motor speeds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We sleep for a second so that the camera can initialize and the servos reach
    their position:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We stop the servos so that they won't be pulling further power once they've
    reached position.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Since we are going to be keeping track of time, we store the last time value
    here. The time is a floating-point number in seconds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We start the camera loop and feed the frame to a `process_frame` method (which
    we''ll write shortly). We can also process a control instruction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: From processing a frame, we expect to get an *X* value, and the magnitude is
    the difference between the highest and lowest value in the differences. The gap
    between the peaks helps detect whether it's actually a line and not just noise.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, for the movement, we need to check that the robot is running and that
    the magnitude we found was bigger than the threshold:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If so, we start the PID behavior:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now log this and use the value to change the heading of the robot. We set
    the motor speeds to our base speed, and then add/subtract the motors'' PID output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we''ve handled what happens when we have detected a line. What about when
    we don''t? `else` stops the motors running and resets the PID, so it doesn''t
    accumulate odd values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice how we are still keeping the last time up to date here. Otherwise, there
    would be a big gap between stops and starts, which would feed odd values into
    the PID.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we need to fill in what happens when we process a frame. Let''s add our
    `process_frame` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code should all look familiar; it is the code we made previously for our
    test code.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We should test to see that our readings have put us on either side of the zero
    line, and that we found two different locations. The maximum should not be below
    zero, and the minimum should not be above it. If they fail this, stop here – the
    main loop will consider this not a line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will find the locations on the row as we did before, along with their midpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'So that we can use it to determine that we''ve got a positive match, we''ll
    calculate the magnitude of the difference between the min and max, making sure
    we aren''t picking up something faint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will want to display something useful to the user here. So, this method
    calls a `make_display` method, just like the other camera behaviors. We pass it
    some variables to plot onto that display:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then return the middle point and the magnitude:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This code will drive our robot, but we''ll have a hard time tuning it if we
    can''t see what is going on. So, let''s create the `make_display` method to handle
    that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The first thing we want in the display is the center reference. Let''s make
    a crosshair about the center and the chosen row:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we show where we found the middle in another color:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'So that we can find it, we also plot the bars for the `lowest` and `highest`
    around it, in a different color again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are going to graph `diff` across a new empty frame. Let''s make an
    empty frame – this is just a NumPy array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The array dimensions are rows then columns, so we swap the camera size *X* and
    *Y* values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will then use a method to make a simple graph. We''ll implement this further
    down. Its parameters are the frame to draw the graph into and the *Y* values for
    the graph. The simple graph method implies the *X* values as column numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have the frame and the graph frame, we need to concatenate these,
    as we did for our frames in the color-detecting code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now encode these bytes and put them on the output queue:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next thing we will need to implement is this `make_cv2_simple_graph` method.
    It''s a bit cheeky but draws lines between *Y* points along an *x* axis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need to store the last value we were at, so the code plots the next value
    relative to this – giving a line graph. We start with item 0\. We also set a slightly
    arbitrary middle *Y* point for the graph. Remember that we know the `diff` values
    can be negative:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we should enumerate the data to plot each item:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can plot a line from the last item *Y* position to the current position
    on the next *X* location. Notice how we offset each item by that graph middle:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then need to update the last item to this current one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Okay – nearly there; that will plot the graph on our frame.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Our behavior is complete; we just need the outer code to run it! This code
    should also be similar to the previous camera examples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can now upload this to your robot. Then, switch the motors on and run it.
    Because this is web-based, point a browser at `http://myrobot.local:5001`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_14_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.14 – Screenshot of the line-following behavior output
  prefs: []
  type: TYPE_NORMAL
- en: The screenshot in *Figure 14.14* shows the title above two pictures. On the
    left is the camera picture of the line. Drawn onto this frame is a green crosshair
    showing where the center point is. Also, there is a large red bar showing the
    middle of the line, and either side of this, two shorter red bars showing the
    sides of it. To the right is the graph plotting the intensity differences after
    blurring. The up peak and down peak are visible in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: Below this are the **Start**, **Stop**, and **Exit** buttons.
  prefs: []
  type: TYPE_NORMAL
- en: Place the robot onto the line, with good lighting. If it looks like the preceding
    display, press the **Start** button to see it go. It should start shakily driving
    along the line.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning the PID
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can get a little bolder with trying to track curved lines and find its
    limit. The robot will sometimes overshoot or understeer, which is where the PID
    tuning comes in:'
  prefs: []
  type: TYPE_NORMAL
- en: If it seems to be turning far too slowly, try increasing the proportional constant
    a little. Conversely, if it is oversteering, try lowering the proportional constant
    a fraction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it had a slight continuous error, try increasing the integral constant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PID tuning is a repeating process and requires a lot of patience and testing.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If the behavior isn''t quite working, please try the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: If the tilt servo doesn't look straight down when set to 90 degrees, it may
    not be calibrated correctly. Change the `deflect_90_in_ms` value parameter to
    the `Servos` object – increase in 0.1 increments to get this to 90 degrees.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it is having trouble getting a clear line, ensure that the lighting is adequate,
    that the surface it is on is plain, such as paper, and the line is well contrasting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it is still struggling to find a line, increase the vertical blurring amount
    in steps of 5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it's struggling to turn in time for the line, try reducing the speed in increments
    of 10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you find the camera is wobbling horizontally, you can remove the `self.robot.servos.stop_all()`
    line from `line_follow_behavior`. Beware: this comes at the cost of motor battery
    life.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the robot is finding too much other random junk that isn't the line, try
    increasing the vertical blurring. Also, try increasing the threshold in steps
    of 1 or 2\. The sharper the contrast in brightness, the less you should need to
    do this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that you double-check the code and that you have got the previous examples
    here and from [*Chapter 13*](B15660_13_Final_ASB_ePub.xhtml#_idTextAnchor283),
    *Robot Vision – Using a Pi Camera and OpenCV*, to work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding a line again
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An important thing to consider is what the robot should do if it has lost the
    line. Coming back to our examples of an industrial setting, this could be a safety
    measure.
  prefs: []
  type: TYPE_NORMAL
- en: Our current robot stops. That requires you to put it back on the line. However,
    when you do so, the robot immediately starts moving again. This behavior is fine
    for our little robot, but it could be a dangerous hazard for a large robot.
  prefs: []
  type: TYPE_NORMAL
- en: Another behavior that you could consider is to spin until the robot finds the
    line again. Losing a line can be because the robot has under/oversteered off the
    line and couldn't find it again, or it could be because the robot has gone past
    the end of a line. This behavior is suitable perhaps for small robot competitions.
  prefs: []
  type: TYPE_NORMAL
- en: We need to consider things like this carefully and where you would use the robot.
    Note that for competition-type robots, or industrial robots, they will have either
    multiple sensors at different angles or a wider angled sensor – so, they are far
    less likely to lose the line like ours. Also, spinning for a larger robot, even
    slowly, could be very hazardous behavior. For this reason, let's implement a simple
    additional safety-type feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it fails to find the line, it doesn''t just stop the motors; it will set
    the running flag to false, so you need to start it manually again:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the `line_follow_behavior.py` file again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the `run` method and find the `else:` statement.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we can modify the content of this statement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Save the code to the robot, and run it until it loses the line. This could be
    by going off course or by reaching the end of the line. The robot should stop.
    It should wait for you to press the start button before trying to move again.
    Notice that you'll need to place it back on the line and press start for it to
    go again.
  prefs: []
  type: TYPE_NORMAL
- en: This robot now handles a lost line condition more predictably.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you saw how to use the camera to detect a line and how to plot
    data showing what it found. You then saw how to take this data and put it into
    driving behavior so that the robot follows the line. You added to your OpenCV
    knowledge, and I showed you a sneaky way to put graphs into frames rendered on
    the camera stream output. You saw how to tune the PID to make the line following
    more accurate and how to ensure the robot stops predictably when it has lost the
    line.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how to communicate with our robot via a voice
    agent, Mycroft. You will add a microphone and speakers to a Raspberry Pi, then
    add speech recognition software. This will let us speak commands to a Raspberry
    Pi to send to the robot, and Mycroft will respond to let us know what it has done.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we''ve got this to work, there are ways we could enhance the system
    and make it more interesting:'
  prefs: []
  type: TYPE_NORMAL
- en: Could you use `cv2.putText` to draw values such as the PID data onto the frames
    in the `make_display` method?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider writing the PID and error data versus time to a file, then loading
    it into another Python file, using Matplotlib to show what happened. This change
    might make the under/oversteer clearer in retrospect.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could modify the motor handling code to go faster when the line is closer
    to the middle and slow down when it is further.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A significant enhancement would be to check two rows and find the angle between
    them. You then know how far the line is from the middle, but you also know which
    way the line is headed and could use that to guide your steering further.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These exercises should give you some interesting ways to play and experiment
    with the things you've built and learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following should help you look further into line following:'
  prefs: []
  type: TYPE_NORMAL
- en: Read about an alternative approach for line processing in the Go language on
    the Raspberry Pi from Pi Wars legend Brian Starkey at [https://blog.usedbytes.com/2019/02/autonomous-challenge-blast-off/](https://blog.usedbytes.com/2019/02/autonomous-challenge-blast-off/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is another line-following robot, using an approach like ours but more
    sophisticated: [https://www.raspberrypi.org/blog/an-image-processing-robot-for-robocup-junior/](https://www.raspberrypi.org/blog/an-image-processing-robot-for-robocup-junior/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
