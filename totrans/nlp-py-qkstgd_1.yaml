- en: Getting Started with Text Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several ways that you can learn new ideas and learn new skills. In
    an art class students study colors, but aren't allowed to actually paint until
    college. Sound absurd?
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, this is how most modern machine learning is taught. The experts
    are doing something similar. They tell you that need to know linear algebra, calculus
    and deep learning. This is before they'll teach you how to use **natural language
    Processing** (**NLP**).
  prefs: []
  type: TYPE_NORMAL
- en: In this book, I want us to learn by teaching the the whole game. In every section,
    we see how to solve real-world problems and learn the tools along the way. Then,
    we will dig deeper and deeper into understanding how to make these toolks. This
    learning and teaching style is very much inspired by Jeremy Howard of fast.ai
    fame.
  prefs: []
  type: TYPE_NORMAL
- en: The next focus is to have code examples wherever possible. This is to ensure
    that there is a clear and motivating purpose behind learning a topic. This helps
    us understand with intuition, beyond math formulae with algebraic notation.
  prefs: []
  type: TYPE_NORMAL
- en: In this opening chapter, we will focus on an introduction to NLP. And, then
    jump into a text classification example with code.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what our journey will briefly look like:'
  prefs: []
  type: TYPE_NORMAL
- en: What is NLP?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What does a good NLP workflow look like? This is to improve your success rate
    when working on any NLP project.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text classification as a motivating example for a good NLP pipeline/workflow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is NLP?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Natural language processing is the use of machines to manipulate natural language.
    In this book, we will focus on written language, or in simpler words: text.'
  prefs: []
  type: TYPE_NORMAL
- en: In effect, this is a practitioner's guide to text processing in English.
  prefs: []
  type: TYPE_NORMAL
- en: Humans are the only known species to have developed written languages. Yet,
    children don't learn to read and write on their own. This is to highlight the
    complexity of text processing and NLP.
  prefs: []
  type: TYPE_NORMAL
- en: The study of natural language processing has been around for more than 50 years.
    The famous Turing test for general artificial intelligence uses this language.
    This field has grown both in regard to linguistics and its computational techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In the spirit of being able to build things first, we will learn how to build
    a simple text classification system using Python's scikit-learn and no other dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: We will also address if this book is a good pick for you.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get going!
  prefs: []
  type: TYPE_NORMAL
- en: Why learn about NLP?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The best way to get the most about of this book is by knowing what you want
    NLP to do for you.
  prefs: []
  type: TYPE_NORMAL
- en: A variety of reasons might draw you to NLP. It might be the higher earning potential.
    Maybe you've noticed and are excited by the potential of NLP, for example, regarding
    Uber's customer Service bots. Yes, they mostly use bots to answer your complaints
    instead of humans.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is useful to know your motivation and write it down. This will help you
    select problems and projects that excite you. It will also help you be selective
    when reading this book. This is not an NLP Made Easy or similar book. Let''s be
    honest: this is a challenging topic. Writing down your motivations is a helpful
    reminder.'
  prefs: []
  type: TYPE_NORMAL
- en: As a legal note, the accompanying code has a permissive MIT License. You can
    use it at your work without legal hassle. That being said, each dependent library
    is from a third party, and you should **definitely check** if they **allow commercial
    use or not.**
  prefs: []
  type: TYPE_NORMAL
- en: I don't expect you to be able to use all of the tools and techniques mentioned
    here. Cherry-pick things that make sense.
  prefs: []
  type: TYPE_NORMAL
- en: You have a problem in mind
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You already have a problem in mind, such as an academic project or a problem
    at your work.
  prefs: []
  type: TYPE_NORMAL
- en: Are you looking for the best tools and techniques that you could use to get
    off the ground?
  prefs: []
  type: TYPE_NORMAL
- en: First, flip through to the book's index to check if I have covered your problem
    here. I have shared end-to-end solutions for some of the most common use cases
    here. If it is not shared, fret not—you are still covered. The underlying techniques
    for a lot of tasks are common. I have been careful to select methods that are
    useful to a wider audience.
  prefs: []
  type: TYPE_NORMAL
- en: Technical achievement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Is learning a mark of achievement for you?
  prefs: []
  type: TYPE_NORMAL
- en: NLP and, more generally, data science, are popular terms. You are someone who
    wants to keep up. You are someone who takes joy from learning new tools, techniques,
    and technologies. This is your next big challenge. This is your chance to prove
    your ability to self-teach and meet mastery.
  prefs: []
  type: TYPE_NORMAL
- en: If this sounds like you, you may be interested in using this as a reference
    book. I have dedicated sections where we give you enough understanding of a method.
    I show you how to use it without having to dive down into the latest papers. This
    is an invitation to learning more, and you are not encouraged to stop here. Try
    these code samples out for yourself!
  prefs: []
  type: TYPE_NORMAL
- en: Do something new
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You have some domain expertise. Now, you want to do things in your domain that
    are not possible without these skills. One way to figure out new possibilities
    is to combine your domain expertise with what you learn here. There are several
    very large opportunities that I saw as I wrote this book, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: NLP for non-English languages such as Hindi, Tamil, or Telugu.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specialized NLP for your domain, for example, finance and Bollywood have different
    languages in their own ways. Your models that have been trained on Bollywood news
    are not expected to work for finance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If this sounds like you, you want to pay attention to the text pre-processing
    sections in this book. These sections will help you understand how we make text
    ready for machine consumption.
  prefs: []
  type: TYPE_NORMAL
- en: Is this book for you?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book has been written so that it keeps the preceding use cases and mindsets
    in mind. The methods, technologies, tools, and techniques selected here are a
    fine balance of industry-grade stability and academia-grade results quality. There
    are several tools, such as parfit, and Flashtext, and ideas such as LIME, that
    have never been written about in the context of NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, I understand the importance and excitement of deep learning methods
    and have a dedicated chapter on deep learning for NLP methods.
  prefs: []
  type: TYPE_NORMAL
- en: NLP workflow template
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some of us would love to work on Natural Language Processing for its sheer intellectual
    challenges – across research and engineering. To measure our progress, having
    a workflow with rough time estimates is really valuable. In this short section,
    we will briefly outline what a usual NLP or even most applied machine learning
    processes look like.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most people I''ve learned from like to use a (roughly) five-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding and preparing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Quick wins: proof of concepts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterating and improving the results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation and deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is just a process template. It has a lot of room for customization regarding
    the engineering culture in your company. Any of these steps can be broken down
    further. For instance, data preparation and understanding can be split further
    into analysis and cleaning. Similarly, the proof of concept step may involve multiple
    experiments, and a demo or a report submission of best results from those.
  prefs: []
  type: TYPE_NORMAL
- en: Although this appears to be a strictly linear process, it is not so. More often
    than not, you will want to revisit a previous step and change a parameter or a
    particular data transform to see the effect on later performance.
  prefs: []
  type: TYPE_NORMAL
- en: In order to do so, it is important to factor in the cyclic nature of this process
    in your code. **Write code with well-designed abstractions with each component
    being independently reusable.**
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in how to write better NLP code, especially for research
    or experimentation, consider looking up the slide deck titled *Writing Code for
    NLP Research*, by Joel Grus of AllenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Let's expand a little bit into each of these sections.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will begin by understanding the requirements and constraints from a practical
    business view point. This tends to answer the following the questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the main problem? We will try to understand – formally and informally
    – the assumptions and expectations from our project.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How will I solve this problem? List some ideas that you might have seen earlier
    or in this book. This is the list that you will use to plan your work ahead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding and preparing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text and language is inherently unstructured. We might want to clean it in certain
    ways, such as expanding abbreviations and acronyms, removing punctuation, and
    so on. We also want to select a few samples that are the best representatives
    of the data we might see in the wild.
  prefs: []
  type: TYPE_NORMAL
- en: The other common practice is to prepare a gold dataset. A gold dataset is the
    best available data under reasonable conditions. This is not the best available
    data under ideal conditions. Creating the gold dataset often involves manual tagging
    and cleaning processes.
  prefs: []
  type: TYPE_NORMAL
- en: The next few sections are dedicated to text cleaning and text representations
    at this stage of the NLP workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Quick wins – proof of concept
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We want to quickly spot the types of algorithms and dataset combinations that
    sort of work for us. We can then focus on them and study them in greater detail.
  prefs: []
  type: TYPE_NORMAL
- en: The results from here will help you estimate the amount of work ahead of you.
    For instance, if you are going to develop a search system for documents based
    exclusively on keywords, your main effort will probably be deploying an open source
    solution such as ElasticSearch.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say that you now want to add a similar documents feature. Depending on
    the expected quality of results, you will want to look into techniques such as
    doc2vec and word2vec, or even some convolutional neural network solution using
    Keras/Tensorflow or PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: This step is essential to get a greater buy-in from others around you, such
    as your boss, to invest more energy and resources into this. In an engineering
    role, this demo should highlight parts of your work that the shelf systems usually
    can't do. These are your unique strengths. These are usually insights, customization,
    and control that other systems can't provide.
  prefs: []
  type: TYPE_NORMAL
- en: Iterating and improving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we have a selected list of algorithms, data, and methods that
    have encouraging results for us.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If your algorithms are machine learning or statistical in nature, you will quite
    often have a lot of juice left.
  prefs: []
  type: TYPE_NORMAL
- en: There are quite often parameters for which you simply pick a good enough default
    during the earlier stage. Here, you might want to double down and check for the
    best value of those parameters. This idea is sometimes referred to as parameter
    search, or hyperparameter tuning in machine learning parlance.
  prefs: []
  type: TYPE_NORMAL
- en: You might want to combine the results of one technique with the other in particular
    ways. For instance, some statistical methods might be very good for finding noun
    phrases in your text and using them to classify it, while a deep learning method
    (let's call it DL-LSTM) might be the best suited for text classification of the
    entire document. In that case, you might want to pass the extra information from
    both your noun phrase extraction and DL-LSTM to another model. This will allow
    it to the use the best of both worlds. This idea is sometimes referred to as stacking
    in machine learning parlance. This was quite successful on the machine learning
    contest platform Kaggle until very recently.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simple changes in data pre-processing or the data cleaning stage can quite often
    give you dramatically better results. For instance, making sure that your entire
    corpus is in lowercase can help you reduce the number of unique words (your vocabulary
    size) by a significant fraction.
  prefs: []
  type: TYPE_NORMAL
- en: If your numeric representation of words is skewed by the word frequency, sometimes
    it helps to normalize and/or scale the same. The laziest hack is to simply divide
    by the frequency.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation and deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluation and deployment are critical components in making your work widely
    available. The quality of your evaluation determines how trustworthy your work
    is by other people. Deployment varies widely, but quite often is abstracted out
    in single function calls or REST API calls.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's say you have a model with 99% accuracy in classifying brain tumors. Can
    you trust this model? No.
  prefs: []
  type: TYPE_NORMAL
- en: If your model had said that no-one has a brain tumor, it would still have 99%+
    accuracy. Why?
  prefs: []
  type: TYPE_NORMAL
- en: Because luckily 99% or more of the population does not have a brain tumor!
  prefs: []
  type: TYPE_NORMAL
- en: To use our models for practical use, we need to look beyond accuracy. We need
    to understand what the model gets right or wrong in order to improve it. A minute
    spent understanding the confusion matrix will stop us from going ahead with such
    dangerous models.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we will want to develop an intuition of what the model is doing
    underneath the black box optimization algorithms. Data visualization techniques
    such as t-SNE can assist us with this.
  prefs: []
  type: TYPE_NORMAL
- en: For continuously running NLP applications such as email spam classifiers or
    chatbots, we would want the evaluation of the model quality to happen continuously
    as well. This will help us ensure that the model's performance does not degrade
    with time.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book is written with a programmer-first mindset. We will learn how to deploy
    any machine learning or NLP application as a REST API which can then be used for
    the web and mobile. This architecture is quite prevalent in the industry. For
    instance, we know that this is how data science teams such as those at Amazon
    and LinkedIn deploy their work to the web.
  prefs: []
  type: TYPE_NORMAL
- en: Example – text classification workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The preceding process is fairly generic. What would it look like for one of
    the most common natural language applications – text classification?
  prefs: []
  type: TYPE_NORMAL
- en: 'The following flow diagram was built by Microsoft Azure, and is used here to
    explain how their own technology fits directly into our workflow template. There
    are several new words that they have introduced to feature engineering, such as
    unigrams, TF-IDF, TF, n-grams, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ff7cb56-e4a8-40d3-b1fc-42f8ae3eb381.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The main steps in their flow diagram are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Data preparation'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Step 2**: Text pre-processing'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Step 3**: Feature engineering:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unigrams TF-IDF extraction
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: N-grams TF extraction
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 4**: Train and evaluate models'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Step 5**: Deploy trained models as web services'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This means that it's time to stop talking and start programming. Let's quickly
    set up the environment first and then we will work on building our first text
    classification system in 30 lines of code or less.
  prefs: []
  type: TYPE_NORMAL
- en: Launchpad – programming environment setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use the fast.ai machine learning setup for this exercise. Their setup
    environment is great for personal experimentation and industry-grade proof-of-concept
    projects. I have used the fast.ai environment on both Linux and Windows. We will
    use Python 3.6 here since our code will not run for other Python versions.
  prefs: []
  type: TYPE_NORMAL
- en: A quick search on their forums will also take you to the latest instructions
    on how to set up the same on most cloud computing solutions including AWS, Google
    Cloud Platform, and Paperspace.
  prefs: []
  type: TYPE_NORMAL
- en: 'This environment covers the tools that we will use across most of the major
    tasks that we will perform: text processing (including cleaning), feature extraction,
    machine learning and deep learning models, model evaluation, and deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: It includes spaCy out of the box. spaCy is an open source tool that was made
    for an industry-grade NLP toolkit. If someone recommends that you use NLTK for
    a task, use spaCy instead. The demo ahead works out of the box in their environment.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few more packages that we will need for later tasks. We will install
    and set them up as and when required. We don't want to bloat your installation
    with unnecessary packages that you might not even use.
  prefs: []
  type: TYPE_NORMAL
- en: Text classification in 30 lines of code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s divide the classification problem into the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Text to numbers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Running ML algorithms with sklearn
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Getting the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The 20 newsgroups dataset is a fairly well-known dataset among the NLP community.
    It is near-ideal for demonstration purposes. This dataset has a near-uniform distribution
    across 20 classes. This uniform distribution makes iterating rapidly on classification
    and clustering techniques easy.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the famous 20 newsgroups dataset for our demonstrations as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Most modern NLP methods rely heavily on machine learning methods. These methods
    need words that are written as strings of text to be converted into a numerical
    representation. This numerical representation can be as simple as assigning a
    unique integer ID to slightly more comprehensive vector of float values. In the
    case of the latter, this is sometimes referred to as vectorization.
  prefs: []
  type: TYPE_NORMAL
- en: Text to numbers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be using a bag of words model for our example. We simply convert the
    number of times every word occurs per document. Therefore, each document is a
    bag and we count the frequency of each word in that bag. This also means that
    we lose any *ordering* information that's present in the text. Next, we assign
    each unique word an integer ID. All of these unique words become our vocabulary.
    Each word in our vocabulary is treated as a machine learning feature. Let's make
    our vocabulary first.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-learn has a high-level component that will create feature vectors for
    us. This is called `CountVectorizer`. We recommend reading more about it from
    the scikit-learn docs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: By using `count_vect.fit_transform(twenty_train.data)`, we are learning the
    vocabulary dictionary, which returns a Document-Term matrix of shape [`n_samples`,
    `n_features`]. This means that we have `n_samples` documents or bags with `n_features`
    unique words across them.
  prefs: []
  type: TYPE_NORMAL
- en: We will now be able to extract a meaningful relationship between these words
    and the tags or classes they belong to. One of the simplest ways to do this is
    to count the number of times a word occurs in each class.
  prefs: []
  type: TYPE_NORMAL
- en: We have a small issue with this – long documents then tend to influence the
    result a lot more. We can normalize this effect by dividing the word frequency
    by the total words in that document. We call this Term Frequency, or simply TF.
  prefs: []
  type: TYPE_NORMAL
- en: Words like *the*, *a*, and *of* are common across all documents and don't really
    help us distinguish between document classes or separate them. We want to emphasize
    rarer words, such as *Manmohan* and *Modi*, over common words. One way to do this
    is to use inverse document frequency, or IDF. Inverse document frequency is a
    measure of whether the term is common or rare in all documents.
  prefs: []
  type: TYPE_NORMAL
- en: We multiply TF with IDF to get our TF-IDF metric, which is always greater than
    zero. TF-IDF is calculated for a triplet of term t, document d, and vocab dictionary
    D.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can directly calculate TF-IDF using the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The last line will output the dimension of the Document-Term matrix, which is
    (11314, 130107).
  prefs: []
  type: TYPE_NORMAL
- en: Please note that in the preceding example we used each word as a feature, so
    the TF-IDF was calculated for each word. When we use a single word as a feature,
    we call it a unigram. If we were to use two consecutive words as a feature instead,
    we'd call it a bigram. In general, for n-words, we would call it an n-gram.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Various algorithms can be used for text classification. You can build a classifier
    in scikit using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Let's dissect the preceding code, line by line.
  prefs: []
  type: TYPE_NORMAL
- en: 'The initial two lines are simple imports. We import the fairly well-known Logistic
    Regression model and rename the import LR. The next is a pipeline import:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Sequentially apply a list of transforms and a final estimator. Intermediate
    steps of the pipeline must be "transforms", that is, they must implement fit and
    transform methods. The final estimator only needs to implement fit."'
  prefs: []
  type: TYPE_NORMAL
- en: '- from [sklearn docs](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-learn pipelines are, logistically, lists of operations that are applied,
    one after another. First, we applied the two operations we have already seen:
    `CountVectorizer()` and `TfidfTransformer()`. This was followed by `LR()`. The
    pipeline was created with `Pipeline(...)`, but hasn''t been executed. It is only
    executed when we call the `fit()` function from the `Pipeline` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'When this is called, it calls the transform function of all but the last object.
    For the last object – our Logistic Regression classifier – its `fit()` function
    is called. These transforms and classifiers are also referred to as estimators:'
  prefs: []
  type: TYPE_NORMAL
- en: <q>"All estimators in a pipeline, except the last one, must be transformers
    (that is, they must have a transform method). The last estimator may be any type
    (transformer, classifier, and so on)."</q> <q>- f</q><q>rom [sklearn pipeline
    docs](http://scikit-learn.org/stable/modules/pipeline.html)</q>
  prefs: []
  type: TYPE_NORMAL
- en: Let's calculate the accuracy of this model on the test data. For calculating
    the means on a large number of values, we will be using a scientific library called
    `numpy`*:*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints out the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We used the LR default parameters here. We can later optimize these using `GridSearch`
    or `RandomSearch` to improve the accuracy even more.
  prefs: []
  type: TYPE_NORMAL
- en: If you're going to remember only one thing from this section, remember to try
    a linear model such as logistic regression. They are often quite good for sparse
    high-dimensional data such as text, bag-of-words, or TF-IDF.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to accuracy, it is useful to understand which categories of text
    are being confused for which other categories. We will call this a confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code uses the same variables we used to calculate the test accuracy
    for finding out the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints a giant list of numbers which is not very interpretable. Let''s
    try pretty printing this by using the `print-json` hack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This is slightly better. We now understand that this is a 20 × 20 grid of numbers.
    However, interpreting these numbers is a tedious task unless we can bring some
    visualization into this game. Let''s do that next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following amazing plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ef56a99-2179-4327-8a60-336aa7dd16df.png)'
  prefs: []
  type: TYPE_IMG
- en: This plot highlights information of interest to us in different color schemes.
    For instance, the light diagonal from the lupper-left corner to the lower-right
    corner shows everything we got right. The other grids are darker-colored if we
    confused those more. For instance, 97 samples of one class got wrongly tagged,
    which is quickly visible by the dark black color in row 18 and column 16.
  prefs: []
  type: TYPE_NORMAL
- en: We will dive deeper into both parts of this section – model interpretation and
    data visualization – in slightly more detail later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you got a feel for the broader things we need to make the project
    work. We saw the steps that are involved in this process by using a text classification
    example. We saw how to prepare text for machine learning with scikit-learn. We
    saw Logistic Regression for ML. We also saw a confusion matrix, which is a quick
    and powerful tool for making sense of results in all machine learning, beyond
    NLP.
  prefs: []
  type: TYPE_NORMAL
- en: We are just getting started. From here on out, we will dive deeper into each
    of these steps and see what other methods exist out there. In the next chapter,
    we will look at some common methods for text cleaning and extraction. Since this
    is what we will spend up to 80% of our total time on, it's worth the time and
    energy learning it.
  prefs: []
  type: TYPE_NORMAL
