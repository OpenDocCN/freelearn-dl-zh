- en: Getting Started with Text Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分类入门
- en: There are several ways that you can learn new ideas and learn new skills. In
    an art class students study colors, but aren't allowed to actually paint until
    college. Sound absurd?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过几种方式学习新想法和技能。在美术课上，学生研究颜色，但直到大学才被允许实际绘画。听起来荒谬吗？
- en: Unfortunately, this is how most modern machine learning is taught. The experts
    are doing something similar. They tell you that need to know linear algebra, calculus
    and deep learning. This is before they'll teach you how to use **natural language
    Processing** (**NLP**).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这就是大多数现代机器学习的教学方式。专家们正在做类似的事情。他们会告诉你需要了解线性代数、微积分和深度学习。在他们教你如何使用**自然语言处理**（**NLP**）之前，这些都是必须知道的。
- en: In this book, I want us to learn by teaching the the whole game. In every section,
    we see how to solve real-world problems and learn the tools along the way. Then,
    we will dig deeper and deeper into understanding how to make these toolks. This
    learning and teaching style is very much inspired by Jeremy Howard of fast.ai
    fame.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我希望我们通过教授整个游戏来学习。在每一节中，我们看到如何解决现实世界的问题，并在过程中学习工具。然后，我们将更深入地了解如何制作这些工具。这种学习和教学风格在很大程度上受到了fast.ai的Jeremy
    Howard的启发。
- en: The next focus is to have code examples wherever possible. This is to ensure
    that there is a clear and motivating purpose behind learning a topic. This helps
    us understand with intuition, beyond math formulae with algebraic notation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个重点是尽可能多地提供代码示例。这是为了确保学习一个主题背后有一个清晰和有动机的目的。这有助于我们用直觉理解，而不仅仅是代数符号的数学公式。
- en: In this opening chapter, we will focus on an introduction to NLP. And, then
    jump into a text classification example with code.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章的开头，我们将专注于自然语言处理的介绍。然后，我们将通过代码示例跳入文本分类的例子。
- en: 'This is what our journey will briefly look like:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的旅程将大致看起来像什么：
- en: What is NLP?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是自然语言处理（NLP）？
- en: What does a good NLP workflow look like? This is to improve your success rate
    when working on any NLP project.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个好的自然语言处理工作流程是什么样的？这是为了提高你在任何NLP项目中的成功率。
- en: Text classification as a motivating example for a good NLP pipeline/workflow.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类作为良好自然语言处理管道/工作流程的激励示例。
- en: What is NLP?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是自然语言处理（NLP）？
- en: 'Natural language processing is the use of machines to manipulate natural language.
    In this book, we will focus on written language, or in simpler words: text.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理是使用机器操作自然语言。在这本书中，我们将专注于书面语言，或者用更简单的话说：文本。
- en: In effect, this is a practitioner's guide to text processing in English.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这是一本关于英语文本处理的实践指南。
- en: Humans are the only known species to have developed written languages. Yet,
    children don't learn to read and write on their own. This is to highlight the
    complexity of text processing and NLP.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人类是唯一已知的开发书面语言的物种。然而，孩子们并不是自己学会阅读和写作的。这是为了强调文本处理和自然语言处理的复杂性。
- en: The study of natural language processing has been around for more than 50 years.
    The famous Turing test for general artificial intelligence uses this language.
    This field has grown both in regard to linguistics and its computational techniques.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理的研究已经存在了50多年。著名的通用人工智能图灵测试就是使用这种语言。这个领域在语言学和计算技术方面都得到了发展。
- en: In the spirit of being able to build things first, we will learn how to build
    a simple text classification system using Python's scikit-learn and no other dependencies.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在能够首先构建事物的精神下，我们将学习如何使用Python的scikit-learn和其他依赖项构建一个简单的文本分类系统。
- en: We will also address if this book is a good pick for you.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将讨论这本书是否适合你。
- en: Let's get going!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Why learn about NLP?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要学习自然语言处理（NLP）？
- en: The best way to get the most about of this book is by knowing what you want
    NLP to do for you.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要最大限度地了解这本书，最好的方式是知道你希望自然语言处理为你做什么。
- en: A variety of reasons might draw you to NLP. It might be the higher earning potential.
    Maybe you've noticed and are excited by the potential of NLP, for example, regarding
    Uber's customer Service bots. Yes, they mostly use bots to answer your complaints
    instead of humans.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多原因可能吸引你学习自然语言处理（NLP）。可能是更高的收入潜力。也许你已经注意到并兴奋于NLP的潜力，例如，关于优步的客户服务机器人。是的，他们主要使用机器人来回答你的投诉，而不是人类。
- en: 'It is useful to know your motivation and write it down. This will help you
    select problems and projects that excite you. It will also help you be selective
    when reading this book. This is not an NLP Made Easy or similar book. Let''s be
    honest: this is a challenging topic. Writing down your motivations is a helpful
    reminder.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 了解您的动机并将其写下来是有用的。这有助于您选择让您兴奋的问题和项目。这也有助于您在阅读这本书时进行选择。这不是一本《NLP Made Easy》或类似的书籍。让我们坦诚：这是一个具有挑战性的主题。写下您的动机是一个有用的提醒。
- en: As a legal note, the accompanying code has a permissive MIT License. You can
    use it at your work without legal hassle. That being said, each dependent library
    is from a third party, and you should **definitely check** if they **allow commercial
    use or not.**
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项法律声明，附带的代码具有宽松的MIT许可证。您可以在工作中使用它而无需法律麻烦。但话虽如此，每个依赖库都是第三方提供的，您应该**绝对检查**它们是否**允许商业使用**。
- en: I don't expect you to be able to use all of the tools and techniques mentioned
    here. Cherry-pick things that make sense.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我不期望您能够使用这里提到的所有工具和技术。挑选出有意义的部分。
- en: You have a problem in mind
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 您心中有一个问题
- en: You already have a problem in mind, such as an academic project or a problem
    at your work.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经有一个问题在心中，比如一个学术项目或工作中的问题。
- en: Are you looking for the best tools and techniques that you could use to get
    off the ground?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 您是否在寻找您可以使用来起步的最佳工具和技术？
- en: First, flip through to the book's index to check if I have covered your problem
    here. I have shared end-to-end solutions for some of the most common use cases
    here. If it is not shared, fret not—you are still covered. The underlying techniques
    for a lot of tasks are common. I have been careful to select methods that are
    useful to a wider audience.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，翻到书的索引，看看我是否在这里覆盖了您的问题。我已经在这里分享了某些最常见用例的端到端解决方案。如果没有分享，请不要担心——您仍然受到保护。许多任务的基本技术是通用的。我已经仔细选择了对更广泛受众有用的方法。
- en: Technical achievement
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术成就
- en: Is learning a mark of achievement for you?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 学习对于您来说是一种成就的标志吗？
- en: NLP and, more generally, data science, are popular terms. You are someone who
    wants to keep up. You are someone who takes joy from learning new tools, techniques,
    and technologies. This is your next big challenge. This is your chance to prove
    your ability to self-teach and meet mastery.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: NLP以及更广泛的数据科学是流行的术语。您是那种想要跟上潮流的人。您是那种从学习新工具、技术和技术中获得乐趣的人。这是您的下一个重大挑战。这是您证明自我学习能力并达到精通的机会。
- en: If this sounds like you, you may be interested in using this as a reference
    book. I have dedicated sections where we give you enough understanding of a method.
    I show you how to use it without having to dive down into the latest papers. This
    is an invitation to learning more, and you are not encouraged to stop here. Try
    these code samples out for yourself!
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这听起来像您，您可能会对这个作为参考书感兴趣。我专门设置了部分，让我们为您提供足够的方法理解。我向您展示如何使用它，而无需深入研究最新的论文。这是学习更多知识的邀请，您不应该在这里停下来。亲自尝试这些代码示例！
- en: Do something new
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 做些新鲜事
- en: 'You have some domain expertise. Now, you want to do things in your domain that
    are not possible without these skills. One way to figure out new possibilities
    is to combine your domain expertise with what you learn here. There are several
    very large opportunities that I saw as I wrote this book, including the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 您有一些领域专业知识。现在，您想在您的领域中做一些没有这些技能不可能做到的事情。确定新可能性的一种方法是将您的领域专业知识与在这里学到的知识结合起来。我在写这本书的时候看到了几个非常大的机会，包括以下内容：
- en: NLP for non-English languages such as Hindi, Tamil, or Telugu.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非英语语言如印地语、泰米尔语或泰卢固语的NLP。
- en: Specialized NLP for your domain, for example, finance and Bollywood have different
    languages in their own ways. Your models that have been trained on Bollywood news
    are not expected to work for finance.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为您所在的领域提供专业的NLP，例如，金融和Bollywood在各自的方式中有不同的语言。您在Bollywood新闻上训练的模型并不期望在金融领域工作。
- en: If this sounds like you, you want to pay attention to the text pre-processing
    sections in this book. These sections will help you understand how we make text
    ready for machine consumption.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这听起来像您，您需要关注这本书中的文本预处理部分。这些部分将帮助您了解我们如何使文本准备好供机器消费。
- en: Is this book for you?
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这本书适合您吗？
- en: This book has been written so that it keeps the preceding use cases and mindsets
    in mind. The methods, technologies, tools, and techniques selected here are a
    fine balance of industry-grade stability and academia-grade results quality. There
    are several tools, such as parfit, and Flashtext, and ideas such as LIME, that
    have never been written about in the context of NLP.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的编写是为了保持先前的用例和心态。这里选择的方法、技术、工具和技术是行业级稳定性与学术级结果质量的良好平衡。有几个工具，如parfit和Flashtext，以及像LIME这样的想法，在NLP的背景下从未被提及过。
- en: Lastly, I understand the importance and excitement of deep learning methods
    and have a dedicated chapter on deep learning for NLP methods.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我理解深度学习方法的重要性和兴奋感，并为NLP方法专门编写了一章关于深度学习。
- en: NLP workflow template
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP工作流程模板
- en: Some of us would love to work on Natural Language Processing for its sheer intellectual
    challenges – across research and engineering. To measure our progress, having
    a workflow with rough time estimates is really valuable. In this short section,
    we will briefly outline what a usual NLP or even most applied machine learning
    processes look like.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们中的一些人非常喜欢从事自然语言处理，因为它纯粹是智力上的挑战——跨越研究和工程。为了衡量我们的进度，拥有一个带有粗略时间估计的工作流程是非常有价值的。在本节中，我们将简要概述一个典型的NLP或甚至大多数应用机器学习过程看起来像什么。
- en: 'Most people I''ve learned from like to use a (roughly) five-step process:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我从大多数人那里学到的喜欢使用一个（大致上）五步的过程：
- en: Understanding the problem
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解问题
- en: Understanding and preparing data
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解和准备数据
- en: 'Quick wins: proof of concepts'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速胜利：概念验证
- en: Iterating and improving the results
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代和改进结果
- en: Evaluation and deployment
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估和部署
- en: This is just a process template. It has a lot of room for customization regarding
    the engineering culture in your company. Any of these steps can be broken down
    further. For instance, data preparation and understanding can be split further
    into analysis and cleaning. Similarly, the proof of concept step may involve multiple
    experiments, and a demo or a report submission of best results from those.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个过程模板。它有很多空间来根据你公司的工程文化进行定制。这些步骤中的任何一个都可以进一步分解。例如，数据准备和理解可以进一步分为分析和清理。同样，概念验证步骤可能涉及多个实验，以及从这些实验中提交的最佳结果的演示或报告。
- en: Although this appears to be a strictly linear process, it is not so. More often
    than not, you will want to revisit a previous step and change a parameter or a
    particular data transform to see the effect on later performance.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这看起来是一个严格的线性过程，但实际上并非如此。通常情况下，你将希望回顾前面的步骤并更改参数或特定的数据转换，以查看对后续性能的影响。
- en: In order to do so, it is important to factor in the cyclic nature of this process
    in your code. **Write code with well-designed abstractions with each component
    being independently reusable.**
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，在你的代码中考虑这一过程的循环性质是很重要的。**编写具有良好设计的抽象代码，其中每个组件都是独立可重用的。**
- en: If you are interested in how to write better NLP code, especially for research
    or experimentation, consider looking up the slide deck titled *Writing Code for
    NLP Research*, by Joel Grus of AllenAI.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感兴趣于如何编写更好的NLP代码，尤其是用于研究或实验，可以考虑查阅由AllenAI的Joel Grus提供的名为*Writing Code for
    NLP Research*的幻灯片。
- en: Let's expand a little bit into each of these sections.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微深入到每个这些部分。
- en: Understanding the problem
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解问题
- en: 'We will begin by understanding the requirements and constraints from a practical
    business view point. This tends to answer the following the questions:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从理解来自实际商业视角的需求和约束开始。这通常回答以下问题：
- en: What is the main problem? We will try to understand – formally and informally
    – the assumptions and expectations from our project.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主要问题是什么？我们将尝试正式和非正式地理解我们项目中的假设和期望。
- en: How will I solve this problem? List some ideas that you might have seen earlier
    or in this book. This is the list that you will use to plan your work ahead.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我将如何解决这个问题？列出一些你可能之前或在这本书中看到过的想法。这是你将用来规划未来工作的清单。
- en: Understanding and preparing the data
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解和准备数据
- en: Text and language is inherently unstructured. We might want to clean it in certain
    ways, such as expanding abbreviations and acronyms, removing punctuation, and
    so on. We also want to select a few samples that are the best representatives
    of the data we might see in the wild.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 文本和语言本质上是未结构化的。我们可能想要以某种方式对其进行清理，例如扩展缩写和首字母缩略词，删除标点符号等。我们还想选择一些样本，这些样本是我们可能在野外看到的数据的最佳代表。
- en: The other common practice is to prepare a gold dataset. A gold dataset is the
    best available data under reasonable conditions. This is not the best available
    data under ideal conditions. Creating the gold dataset often involves manual tagging
    and cleaning processes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的做法是准备一个黄金数据集。黄金数据集是在合理条件下可用的最佳数据。这不是在理想条件下可用的最佳数据。创建黄金数据集通常涉及手动标记和清理过程。
- en: The next few sections are dedicated to text cleaning and text representations
    at this stage of the NLP workflow.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的几节将专注于NLP工作流程这一阶段的文本清理和文本表示。
- en: Quick wins – proof of concept
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速胜利 – 概念验证
- en: We want to quickly spot the types of algorithms and dataset combinations that
    sort of work for us. We can then focus on them and study them in greater detail.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望快速识别出对我们来说似乎有效的算法和数据集组合类型。然后我们可以专注于它们，并更深入地研究它们。
- en: The results from here will help you estimate the amount of work ahead of you.
    For instance, if you are going to develop a search system for documents based
    exclusively on keywords, your main effort will probably be deploying an open source
    solution such as ElasticSearch.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这里得到的结果将帮助你估计你面前的工作量。例如，如果你打算开发一个仅基于关键词的文档搜索系统，你的主要工作可能将是部署一个开源解决方案，如ElasticSearch。
- en: Let's say that you now want to add a similar documents feature. Depending on
    the expected quality of results, you will want to look into techniques such as
    doc2vec and word2vec, or even some convolutional neural network solution using
    Keras/Tensorflow or PyTorch.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你现在想要添加一个类似文档的功能。根据预期的结果质量，你可能需要研究诸如doc2vec和word2vec等技术，或者甚至使用Keras/Tensorflow或PyTorch的一些卷积神经网络解决方案。
- en: This step is essential to get a greater buy-in from others around you, such
    as your boss, to invest more energy and resources into this. In an engineering
    role, this demo should highlight parts of your work that the shelf systems usually
    can't do. These are your unique strengths. These are usually insights, customization,
    and control that other systems can't provide.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步对于从你周围的人，如你的老板，那里获得更大的支持，投入更多精力和资源至关重要。在工程角色中，这个演示应该突出你的工作部分，这些部分是货架系统通常无法做到的。这些是你的独特优势。这些通常是其他系统无法提供的见解、定制和控制。
- en: Iterating and improving
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迭代和改进
- en: At this point, we have a selected list of algorithms, data, and methods that
    have encouraging results for us.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们已经选择了一系列算法、数据和方 法，它们对我们来说有令人鼓舞的结果。
- en: Algorithms
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法
- en: If your algorithms are machine learning or statistical in nature, you will quite
    often have a lot of juice left.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的算法是机器学习或统计性质，你通常会剩下很多余量。
- en: There are quite often parameters for which you simply pick a good enough default
    during the earlier stage. Here, you might want to double down and check for the
    best value of those parameters. This idea is sometimes referred to as parameter
    search, or hyperparameter tuning in machine learning parlance.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期阶段，对于一些参数，你可能只需要选择一个足够好的默认值。在这里，你可能想要加大力度，检查这些参数的最佳值。这个想法有时被称为参数搜索，或者按照机器学习的术语，称为超参数调整。
- en: You might want to combine the results of one technique with the other in particular
    ways. For instance, some statistical methods might be very good for finding noun
    phrases in your text and using them to classify it, while a deep learning method
    (let's call it DL-LSTM) might be the best suited for text classification of the
    entire document. In that case, you might want to pass the extra information from
    both your noun phrase extraction and DL-LSTM to another model. This will allow
    it to the use the best of both worlds. This idea is sometimes referred to as stacking
    in machine learning parlance. This was quite successful on the machine learning
    contest platform Kaggle until very recently.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想要以特定方式将一种技术的结果与其他技术结合。例如，某些统计方法可能非常适合在文本中找到名词短语并使用它们进行分类，而深度学习方法（我们可以称之为DL-LSTM）可能最适合整个文档的文本分类。在这种情况下，你可能希望将名词短语提取和DL-LSTM的额外信息传递给另一个模型。这将允许它利用两者的最佳之处。在机器学习的术语中，这个想法有时被称为堆叠。这在最近非常成功的机器学习竞赛平台Kaggle上非常成功。
- en: Pre-processing
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理
- en: Simple changes in data pre-processing or the data cleaning stage can quite often
    give you dramatically better results. For instance, making sure that your entire
    corpus is in lowercase can help you reduce the number of unique words (your vocabulary
    size) by a significant fraction.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据预处理或数据清洗阶段进行简单的更改，通常会带来显著更好的结果。例如，确保您的整个语料库都是小写字母，可以帮助您显著减少唯一单词的数量（您的词汇量）。
- en: If your numeric representation of words is skewed by the word frequency, sometimes
    it helps to normalize and/or scale the same. The laziest hack is to simply divide
    by the frequency.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的词语的数值表示受到词频的影响，有时通过归一化和/或缩放可能会有所帮助。最懒惰的技巧就是简单地除以频率。
- en: Evaluation and deployment
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估和部署
- en: Evaluation and deployment are critical components in making your work widely
    available. The quality of your evaluation determines how trustworthy your work
    is by other people. Deployment varies widely, but quite often is abstracted out
    in single function calls or REST API calls.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 评估和部署是使您的工作广泛可用的重要组件。您评估的质量决定了其他人信任您工作的程度。部署方式多种多样，但通常会被抽象为单个功能调用或 REST API
    调用。
- en: Evaluation
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: Let's say you have a model with 99% accuracy in classifying brain tumors. Can
    you trust this model? No.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您有一个模型在分类脑瘤时达到了 99% 的准确率。您能信任这个模型吗？不能。
- en: If your model had said that no-one has a brain tumor, it would still have 99%+
    accuracy. Why?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的模型说没有人有脑瘤，它仍然会有 99%+ 的准确率。为什么？
- en: Because luckily 99% or more of the population does not have a brain tumor!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，99% 或更多的民众没有脑瘤！
- en: To use our models for practical use, we need to look beyond accuracy. We need
    to understand what the model gets right or wrong in order to improve it. A minute
    spent understanding the confusion matrix will stop us from going ahead with such
    dangerous models.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将我们的模型用于实际应用，我们需要超越准确率。我们需要了解模型在哪些方面做得对或错，以便改进它。花一分钟时间理解混淆矩阵将阻止我们继续使用这样的危险模型。
- en: Additionally, we will want to develop an intuition of what the model is doing
    underneath the black box optimization algorithms. Data visualization techniques
    such as t-SNE can assist us with this.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还想了解模型在黑盒优化算法之下到底在做什么。t-SNE 等数据可视化技术可以帮助我们做到这一点。
- en: For continuously running NLP applications such as email spam classifiers or
    chatbots, we would want the evaluation of the model quality to happen continuously
    as well. This will help us ensure that the model's performance does not degrade
    with time.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于持续运行的 NLP 应用程序，如邮件垃圾邮件分类器或聊天机器人，我们希望模型质量评估也能持续进行。这将帮助我们确保模型性能不会随着时间的推移而下降。
- en: Deployment
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署
- en: This book is written with a programmer-first mindset. We will learn how to deploy
    any machine learning or NLP application as a REST API which can then be used for
    the web and mobile. This architecture is quite prevalent in the industry. For
    instance, we know that this is how data science teams such as those at Amazon
    and LinkedIn deploy their work to the web.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书是以程序员优先的思维方式编写的。我们将学习如何将任何机器学习或 NLP 应用程序作为 REST API 部署，然后可以用于网页和移动设备。这种架构在行业中相当普遍。例如，我们知道亚马逊和领英等数据科学团队就是这样将他们的工作部署到网络上的。
- en: Example – text classification workflow
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 - 文本分类工作流程
- en: The preceding process is fairly generic. What would it look like for one of
    the most common natural language applications – text classification?
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的过程相当通用。对于最常见的一种自然语言应用——文本分类，它看起来会是什么样子？
- en: 'The following flow diagram was built by Microsoft Azure, and is used here to
    explain how their own technology fits directly into our workflow template. There
    are several new words that they have introduced to feature engineering, such as
    unigrams, TF-IDF, TF, n-grams, and so on:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的流程图是由微软 Azure 构建的，这里用它来解释他们的技术如何直接融入到我们的工作流程模板中。他们在特征工程中引入了一些新词，如 unigrams、TF-IDF、TF、n-grams
    等：
- en: '![](img/0ff7cb56-e4a8-40d3-b1fc-42f8ae3eb381.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![流程图](img/0ff7cb56-e4a8-40d3-b1fc-42f8ae3eb381.png)'
- en: 'The main steps in their flow diagram are as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的流程图中的主要步骤如下：
- en: '**Step 1**: Data preparation'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**步骤 1**: 数据准备'
- en: '**Step 2**: Text pre-processing'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**步骤 2**: 文本预处理'
- en: '**Step 3**: Feature engineering:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**步骤 3**: 特征工程：'
- en: Unigrams TF-IDF extraction
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Unigram TF-IDF 提取
- en: N-grams TF extraction
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: N-gram TF 提取
- en: '**Step 4**: Train and evaluate models'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**步骤 4**: 训练和评估模型'
- en: '**Step 5**: Deploy trained models as web services'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**步骤 5**: 将训练好的模型作为网络服务部署'
- en: This means that it's time to stop talking and start programming. Let's quickly
    set up the environment first and then we will work on building our first text
    classification system in 30 lines of code or less.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着是时候停止谈论并开始编程了。让我们先快速设置环境，然后我们将用30行代码或更少的代码构建我们的第一个文本分类系统。
- en: Launchpad – programming environment setup
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Launchpad – 编程环境设置
- en: We will use the fast.ai machine learning setup for this exercise. Their setup
    environment is great for personal experimentation and industry-grade proof-of-concept
    projects. I have used the fast.ai environment on both Linux and Windows. We will
    use Python 3.6 here since our code will not run for other Python versions.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用fast.ai机器学习设置来完成这个练习。他们的设置环境非常适合个人实验和行业级概念验证项目。我已经在Linux和Windows上使用过fast.ai环境。我们将在这里使用Python
    3.6，因为我们的代码在其他Python版本上无法运行。
- en: A quick search on their forums will also take you to the latest instructions
    on how to set up the same on most cloud computing solutions including AWS, Google
    Cloud Platform, and Paperspace.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的论坛上快速搜索也会带你到如何在大多数云计算解决方案上设置相同内容的最新说明，包括AWS、Google Cloud Platform和Paperspace。
- en: 'This environment covers the tools that we will use across most of the major
    tasks that we will perform: text processing (including cleaning), feature extraction,
    machine learning and deep learning models, model evaluation, and deployment.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这个环境涵盖了我们将用于大多数主要任务的工具：文本处理（包括清理）、特征提取、机器学习与深度学习模型、模型评估和部署。
- en: It includes spaCy out of the box. spaCy is an open source tool that was made
    for an industry-grade NLP toolkit. If someone recommends that you use NLTK for
    a task, use spaCy instead. The demo ahead works out of the box in their environment.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 它包含spaCy。spaCy是一个开源工具，它被制作成一个行业级自然语言处理工具包。如果有人建议你使用NLTK来完成一项任务，请改用spaCy。接下来的演示将在他们的环境中直接运行。
- en: There are a few more packages that we will need for later tasks. We will install
    and set them up as and when required. We don't want to bloat your installation
    with unnecessary packages that you might not even use.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一些其他包来完成后续任务。我们将根据需要安装和设置它们。我们不希望因为不必要的包而膨胀你的安装，你可能甚至都不会使用这些包。
- en: Text classification in 30 lines of code
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 30行代码实现文本分类
- en: 'Let''s divide the classification problem into the following steps:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把分类问题划分为以下步骤：
- en: Getting the data
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取数据
- en: Text to numbers
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文本到数字
- en: Running ML algorithms with sklearn
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用sklearn运行ML算法
- en: Getting the data
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据
- en: The 20 newsgroups dataset is a fairly well-known dataset among the NLP community.
    It is near-ideal for demonstration purposes. This dataset has a near-uniform distribution
    across 20 classes. This uniform distribution makes iterating rapidly on classification
    and clustering techniques easy.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 20个新闻组数据集在自然语言处理社区中是一个相当知名的数据集。它对于演示目的几乎是理想的。这个数据集在20个类别中具有几乎均匀的分布。这种均匀分布使得快速迭代分类和聚类技术变得容易。
- en: 'We will use the famous 20 newsgroups dataset for our demonstrations as well:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用著名的20个新闻组数据集进行演示：
- en: '[PRE0]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Most modern NLP methods rely heavily on machine learning methods. These methods
    need words that are written as strings of text to be converted into a numerical
    representation. This numerical representation can be as simple as assigning a
    unique integer ID to slightly more comprehensive vector of float values. In the
    case of the latter, this is sometimes referred to as vectorization.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代自然语言处理方法都严重依赖于机器学习方法。这些方法需要将作为字符串文本编写的单词转换为数值表示。这种数值表示可以是简单地分配一个唯一的整数ID，也可以是稍微更全面的浮点值向量。在后一种情况下，这有时被称为向量化。
- en: Text to numbers
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本到数字
- en: We will be using a bag of words model for our example. We simply convert the
    number of times every word occurs per document. Therefore, each document is a
    bag and we count the frequency of each word in that bag. This also means that
    we lose any *ordering* information that's present in the text. Next, we assign
    each unique word an integer ID. All of these unique words become our vocabulary.
    Each word in our vocabulary is treated as a machine learning feature. Let's make
    our vocabulary first.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在示例中使用著名的20个新闻组数据集。我们简单地转换每个文档中每个单词出现的次数。因此，每个文档都是一个“袋”，我们计算该袋中每个单词的频率。这也意味着我们失去了文本中存在的任何*顺序*信息。接下来，我们为每个唯一的单词分配一个整数ID。所有这些唯一的单词成为我们的词汇表。我们词汇表中的每个单词都被视为一个机器学习特征。让我们首先创建我们的词汇表。
- en: 'Scikit-learn has a high-level component that will create feature vectors for
    us. This is called `CountVectorizer`. We recommend reading more about it from
    the scikit-learn docs:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 有一个高级组件可以为我们创建特征向量。这被称为 `CountVectorizer`。我们建议您从 scikit-learn 文档中了解更多相关信息：
- en: '[PRE1]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: By using `count_vect.fit_transform(twenty_train.data)`, we are learning the
    vocabulary dictionary, which returns a Document-Term matrix of shape [`n_samples`,
    `n_features`]. This means that we have `n_samples` documents or bags with `n_features`
    unique words across them.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 `count_vect.fit_transform(twenty_train.data)`，我们正在学习词汇字典，它返回一个形状为 `['n_samples',
    'n_features']` 的文档-词矩阵。这意味着我们有 `n_samples` 个文档或袋，它们之间有 `n_features` 个独特的单词。
- en: We will now be able to extract a meaningful relationship between these words
    and the tags or classes they belong to. One of the simplest ways to do this is
    to count the number of times a word occurs in each class.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将能够提取这些词与其所属标签或类别之间的有意义的关系。实现这一点的最简单方法之一是计算每个类别中一个词出现的次数。
- en: We have a small issue with this – long documents then tend to influence the
    result a lot more. We can normalize this effect by dividing the word frequency
    by the total words in that document. We call this Term Frequency, or simply TF.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对此有一个小问题——长文档往往会极大地影响结果。我们可以通过将词频除以该文档中的总词数来归一化这种影响。我们称这为词频，简称 TF。
- en: Words like *the*, *a*, and *of* are common across all documents and don't really
    help us distinguish between document classes or separate them. We want to emphasize
    rarer words, such as *Manmohan* and *Modi*, over common words. One way to do this
    is to use inverse document frequency, or IDF. Inverse document frequency is a
    measure of whether the term is common or rare in all documents.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *the*、*a* 和 *of* 这样的词在所有文档中都很常见，并不能真正帮助我们区分文档类别或将它们分开。我们想要强调的是较罕见的词，如 *Manmohan*
    和 *Modi*，而不是常见词。实现这一目标的一种方法是通过逆文档频率，或称 IDF。逆文档频率是衡量一个词在所有文档中是常见还是罕见的度量。
- en: We multiply TF with IDF to get our TF-IDF metric, which is always greater than
    zero. TF-IDF is calculated for a triplet of term t, document d, and vocab dictionary
    D.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 TF 与 IDF 相乘以得到我们的 TF-IDF 指标，该指标总是大于零。TF-IDF 是针对三元组 term t、document d 和词汇字典
    D 计算的。
- en: 'We can directly calculate TF-IDF using the following lines of code:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接使用以下代码行计算 TF-IDF：
- en: '[PRE2]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The last line will output the dimension of the Document-Term matrix, which is
    (11314, 130107).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行将输出文档-词矩阵的维度，其值为 (11314, 130107)。
- en: Please note that in the preceding example we used each word as a feature, so
    the TF-IDF was calculated for each word. When we use a single word as a feature,
    we call it a unigram. If we were to use two consecutive words as a feature instead,
    we'd call it a bigram. In general, for n-words, we would call it an n-gram.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在先前的例子中，我们使用每个词作为特征，因此对每个词计算了 TF-IDF。当我们使用单个词作为特征时，我们称之为 unigram。如果我们使用两个连续的词作为特征，我们称之为
    bigram。一般来说，对于 n 个词，我们称之为 n-gram。
- en: Machine learning
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习
- en: 'Various algorithms can be used for text classification. You can build a classifier
    in scikit using the following code:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用各种算法进行文本分类。您可以使用以下代码在 scikit 中构建分类器：
- en: '[PRE3]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let's dissect the preceding code, line by line.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐行分析先前的代码。
- en: 'The initial two lines are simple imports. We import the fairly well-known Logistic
    Regression model and rename the import LR. The next is a pipeline import:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 前两行是简单的导入。我们导入了一个相当知名的逻辑回归模型，并将其重命名为 LR。接下来是管道导入：
- en: '"Sequentially apply a list of transforms and a final estimator. Intermediate
    steps of the pipeline must be "transforms", that is, they must implement fit and
    transform methods. The final estimator only needs to implement fit."'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '"依次应用一系列转换和一个最终估计器。管道的中间步骤必须是“转换”，也就是说，它们必须实现 fit 和 transform 方法。最终的估计器只需要实现
    fit 方法。"'
- en: '- from [sklearn docs](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '- 来自 [sklearn 文档](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)'
- en: 'Scikit-learn pipelines are, logistically, lists of operations that are applied,
    one after another. First, we applied the two operations we have already seen:
    `CountVectorizer()` and `TfidfTransformer()`. This was followed by `LR()`. The
    pipeline was created with `Pipeline(...)`, but hasn''t been executed. It is only
    executed when we call the `fit()` function from the `Pipeline` object:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 管道在逻辑上是一系列依次应用的运算。首先，我们应用了我们已经看到的两个操作：`CountVectorizer()` 和 `TfidfTransformer()`。接着是
    `LR()`。管道是通过 `Pipeline(...)` 创建的，但尚未执行。它只有在从 `Pipeline` 对象调用 `fit()` 函数时才会执行：
- en: '[PRE4]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'When this is called, it calls the transform function of all but the last object.
    For the last object – our Logistic Regression classifier – its `fit()` function
    is called. These transforms and classifiers are also referred to as estimators:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当调用此函数时，它会调用除了最后一个对象之外的所有对象的转换函数。对于最后一个对象——我们的逻辑回归分类器——它的 `fit()` 函数被调用。这些转换器和分类器也被称为估计器：
- en: <q>"All estimators in a pipeline, except the last one, must be transformers
    (that is, they must have a transform method). The last estimator may be any type
    (transformer, classifier, and so on)."</q> <q>- f</q><q>rom [sklearn pipeline
    docs](http://scikit-learn.org/stable/modules/pipeline.html)</q>
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: <q>"在管道中的所有估计器（除了最后一个），都必须是转换器（也就是说，它们必须有一个转换方法）。最后一个估计器可以是任何类型（转换器、分类器等）。”</q>
    <q>- 来自 [sklearn 管道文档](http://scikit-learn.org/stable/modules/pipeline.html)</q>
- en: Let's calculate the accuracy of this model on the test data. For calculating
    the means on a large number of values, we will be using a scientific library called
    `numpy`*:*
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算这个模型在测试数据上的准确率。为了计算大量值上的平均值，我们将使用一个名为 `numpy` 的科学库：*
- en: '[PRE5]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This prints out the following output:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这会打印出以下输出：
- en: '[PRE6]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We used the LR default parameters here. We can later optimize these using `GridSearch`
    or `RandomSearch` to improve the accuracy even more.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用了 LR 的默认参数。我们可以在以后使用 `GridSearch` 或 `RandomSearch` 来优化这些参数，以进一步提高准确率。
- en: If you're going to remember only one thing from this section, remember to try
    a linear model such as logistic regression. They are often quite good for sparse
    high-dimensional data such as text, bag-of-words, or TF-IDF.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只想记住本节中的一件事，请记住尝试一个线性模型，如逻辑回归。它们通常对稀疏的高维数据（如文本、词袋或 TF-IDF）相当有效。
- en: In addition to accuracy, it is useful to understand which categories of text
    are being confused for which other categories. We will call this a confusion matrix.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 除了准确率之外，了解哪些文本类别被混淆为其他类别也是有用的。我们将称之为混淆矩阵。
- en: 'The following code uses the same variables we used to calculate the test accuracy
    for finding out the confusion matrix:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码使用了我们用来计算测试准确率的相同变量，以找出混淆矩阵：
- en: '[PRE7]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This prints a giant list of numbers which is not very interpretable. Let''s
    try pretty printing this by using the `print-json` hack:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这打印出一个巨大的数字列表，不太容易解释。让我们尝试使用 `print-json` 技巧来美化打印：
- en: '[PRE8]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This returns the following code:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回以下代码：
- en: '[PRE9]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This is slightly better. We now understand that this is a 20 × 20 grid of numbers.
    However, interpreting these numbers is a tedious task unless we can bring some
    visualization into this game. Let''s do that next:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这稍微好一些。我们现在明白这是一个 20 × 20 的数字网格。然而，除非我们能引入一些可视化，否则解释这些数字是一项繁琐的任务。让我们接下来这么做：
- en: '[PRE10]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This gives us the following amazing plot:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们以下惊人的图：
- en: '![](img/3ef56a99-2179-4327-8a60-336aa7dd16df.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3ef56a99-2179-4327-8a60-336aa7dd16df.png)'
- en: This plot highlights information of interest to us in different color schemes.
    For instance, the light diagonal from the lupper-left corner to the lower-right
    corner shows everything we got right. The other grids are darker-colored if we
    confused those more. For instance, 97 samples of one class got wrongly tagged,
    which is quickly visible by the dark black color in row 18 and column 16.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图用不同的颜色方案突出了我们感兴趣的信息。例如，从左上角到右下角的光对角线显示了我们所做的一切正确的事情。如果我们混淆了那些，其他网格会更暗。例如，97
    个样本被错误地标记为某一类，这通过第 18 行和第 16 列的深黑色迅速可见。
- en: We will dive deeper into both parts of this section – model interpretation and
    data visualization – in slightly more detail later in this book.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书的稍后部分更详细地深入了解本节的两个部分——模型解释和数据可视化。
- en: Summary
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you got a feel for the broader things we need to make the project
    work. We saw the steps that are involved in this process by using a text classification
    example. We saw how to prepare text for machine learning with scikit-learn. We
    saw Logistic Regression for ML. We also saw a confusion matrix, which is a quick
    and powerful tool for making sense of results in all machine learning, beyond
    NLP.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你感受到了使项目工作所需的一些更广泛的事情。我们通过使用文本分类示例来查看这个过程所涉及的步骤。我们看到了如何使用 scikit-learn
    准备文本进行机器学习。我们看到了机器学习中的逻辑回归。我们还看到了混淆矩阵，这是一个快速而强大的工具，可以理解所有机器学习的结果，而不仅仅是 NLP。
- en: We are just getting started. From here on out, we will dive deeper into each
    of these steps and see what other methods exist out there. In the next chapter,
    we will look at some common methods for text cleaning and extraction. Since this
    is what we will spend up to 80% of our total time on, it's worth the time and
    energy learning it.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们才刚刚开始。从现在开始，我们将深入探讨每一个步骤，并看看还有哪些其他方法存在。在下一章中，我们将探讨一些常见的文本清洗和提取方法。由于这是我们总共花费高达80%时间的地方，因此花时间和精力去学习它是值得的。
