- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Generating and Transforming Images Using Amazon Bedrock
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用亚马逊Bedrock生成和转换图像
- en: By now, we have explored several LLMs capable of generating textual responses.
    This chapter explores generating images using select FMs that are available on
    Amazon Bedrock. We will start with an overview of image generation, wherein we
    will examine model architectures such as GANs and **variational autoencoders**
    (**VAEs**) Then, we will cover some real-world applications of image generation
    and multimodal models available within Amazon Bedrock. Furthermore, we will dive
    deeper into several multimodal design patterns, as well as some ethical considerations
    and safeguards that are available with Amazon Bedrock.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探索了几个能够生成文本响应的LLMs。本章将探讨使用亚马逊Bedrock上可用的选择FM生成图像。我们将从图像生成的概述开始，其中我们将检查如GANs和**变分自编码器**（**VAEs**）等模型架构。然后，我们将介绍一些图像生成和亚马逊Bedrock中可用的多模态模型的实际应用。此外，我们将深入了解几个多模态设计模式，以及亚马逊Bedrock提供的伦理考虑和保障措施。
- en: By the end of this chapter, you will have gained an understanding of implementing
    image generation and its design patterns with Amazon Bedrock for real-world use
    cases.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将了解如何使用亚马逊Bedrock实现图像生成及其设计模式，以应用于实际案例。
- en: 'Here are the key topics that will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下关键主题：
- en: Image generation overview
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像生成概述
- en: Multimodal models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多模态模型
- en: Multimodal design patterns
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多模态设计模式
- en: Ethical considerations and safeguards
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伦理考虑和保障措施
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter requires you to have access to an AWS account. If you don’t have
    one already, you can go to [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    and create one.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章要求您拥有AWS账户访问权限。如果您还没有账户，可以访问[https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)创建一个。
- en: Secondly, you will need to install and configure AWS CLI at [https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)
    after you create an account, which will be needed to access Amazon Bedrock FMs
    from your local machine. Since the majority of the code cells that we will be
    executing are based in Python, setting up an AWS Python SDK (Boto3) at [https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html](https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html)
    would be beneficial at this point. You can carry out the Python setup in any way.
    Install it on your local machine, or use AWS Cloud9, or utilize AWS Lambda, or
    leverage Amazon SageMaker.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，您需要在[https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)创建账户后安装和配置AWS
    CLI，这将用于从您的本地机器访问亚马逊Bedrock FM。由于我们将执行的多数代码单元都是基于Python的，因此在此时设置AWS Python SDK（Boto3）[https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html](https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html)将非常有用。您可以通过任何方式执行Python设置。在您的本地机器上安装它，或者使用AWS
    Cloud9，或者利用AWS Lambda，或者利用Amazon SageMaker。
- en: Note
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: There will be a charge associated with the invocation and customization of the
    FMs of Amazon Bedrock. Please refer to [https://aws.amazon.com/bedrock/pricing/](https://aws.amazon.com/bedrock/pricing/)
    to learn more.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与亚马逊Bedrock FM的调用和定制相关将产生费用。请参阅[https://aws.amazon.com/bedrock/pricing/](https://aws.amazon.com/bedrock/pricing/)以获取更多信息。
- en: Image generation overview
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像生成概述
- en: Image generation has been a fascinating and rapidly evolving field. Since the
    dawn of advanced deep learning techniques and increasing computational power,
    machines have gained the remarkable ability to create highly realistic and sophisticated
    images from scratch or based on textual prompts. This ability has opened up a
    vast array of applications across various domains, including the creative industries,
    media and entertainment, advertising, product packaging, and many others.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图像生成是一个令人着迷且快速发展的领域。自从深度学习技术的兴起和计算能力的增强以来，机器已经获得了从零开始或基于文本提示创建高度逼真和复杂的图像的非凡能力。这种能力在各种领域打开了广泛的应用，包括创意产业、媒体和娱乐、广告、产品包装等。
- en: The history of image generation can be traced back to early developments in
    computer vision and pattern recognition. Researchers and scientists have long
    sought to understand and replicate the human visual perception system, paving
    the way for the initial techniques in image synthesis and manipulation. However,
    the true breakthrough in image generation came with the emergence of deep learning,
    specifically the introduction of GANs and VAEs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图像生成的历史可以追溯到计算机视觉和模式识别的早期发展。研究人员和科学家长期以来一直在寻求理解和复制人类的视觉感知系统，为图像合成和操纵的初始技术铺平了道路。然而，图像生成的真正突破是在深度学习出现时，特别是GANs和VAEs的引入。
- en: Please note that we are highlighting these techniques for historical reference.
    Current image generation FMs do not use these techniques.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们强调这些技术是为了历史参考。当前的图像生成FM并不使用这些技术。
- en: What are GANs and VAEs?
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GANs和VAEs是什么？
- en: GANs, introduced by Ian Goodfellow and his colleagues in 2014, revolutionized
    the field of image generation. You can read more about it on [https://arxiv.org/pdf/1406.2661](https://arxiv.org/pdf/1406.2661).
    GANs employ a unique training approach whereby two neural networks are pitted
    against each other in competition. The first network is known as the **generator**,
    which is tasked with generating synthetic samples that mimic real data. For example,
    the generator could produce new images, texts, or audio clips. The second network
    is called the **discriminator**. Its role is to analyze examples, both real and
    synthetic, to classify which ones are genuine and which have been artificially
    generated.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: GANs（生成对抗网络），由伊恩·古德费洛及其同事于2014年提出，彻底改变了图像生成领域。您可以在[https://arxiv.org/pdf/1406.2661](https://arxiv.org/pdf/1406.2661)上了解更多相关信息。GANs采用了一种独特的训练方法，其中两个神经网络在竞争中相互对抗。第一个网络被称为**生成器**，其任务是生成模仿真实数据的合成样本。例如，生成器可以生成新的图像、文本或音频剪辑。第二个网络被称为**判别器**。其作用是分析示例，包括真实和合成的，以分类哪些是真实的，哪些是人工生成的。
- en: Through this adversarial process, the generator learns to produce increasingly
    convincing fakes that can fool the discriminator. Meanwhile, the discriminator
    evolves in its ability to detect subtle anomalies that reveal synthetic samples.
    Their competing goals drive both networks to continuously improve. A demonstration
    of GANs can be seen at [https://thispersondoesnotexist.com/](https://thispersondoesnotexist.com/).
    By refreshing the page endlessly, users are presented with an endless stream of
    novel human faces. However, none of those faces are real – all are synthetic portraits
    created solely by a GAN trained on vast databases of images of real human faces.
    The site offers a glimpse into how GANs can synthesize highly realistic outputs
    across many domains.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种对抗过程，生成器学会产生越来越令人信服的伪造品，可以欺骗判别器。同时，判别器在检测揭示合成样本的微妙异常方面的能力也在不断进化。它们竞争的目标推动两个网络不断改进。GANs的演示可以在[https://thispersondoesnotexist.com/](https://thispersondoesnotexist.com/)上看到。通过不断刷新页面，用户会看到一个不断涌现的新颖人脸流。然而，这些面孔都不是真实的——所有这些都是由一个在大量真实人脸图像数据库上训练的GAN生成的合成肖像。该网站展示了GANs如何在许多领域合成高度逼真的输出。
- en: Since the inception of GANs, numerous advancements and variations have been
    implemented, leading to remarkable achievements in image generation. Techniques
    such as StyleGAN, BigGAN, and diffusion models have pushed the boundaries of image
    quality, resolution, and diversity. These models can generate photorealistic images
    of human faces, landscapes, objects, and even artistic creations, blurring the
    line between artificial and real.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 自GANs问世以来，已经实施了许多进步和变化，导致图像生成领域取得了显著成就。如StyleGAN、BigGAN和扩散模型等技术推动了图像质量、分辨率和多样性的边界。这些模型可以生成逼真的人脸、风景、物体甚至艺术作品的图像，模糊了人工和真实之间的界限。
- en: 'VAEs, on the other hand, are a simpler means to train generative AI algorithms.
    They also utilize two neural networks: **encoders** and **decoders**. Encoders
    learn the patterns in the data by mapping it into lower-dimensional latent space;
    decoders use these patterns from the latent space and generate realistic samples.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，VAEs（变分自编码器）是训练生成式AI算法的一种更简单的方法。它们也利用两个神经网络：**编码器**和**解码器**。编码器通过将其映射到低维潜在空间来学习数据中的模式；解码器使用这些潜在空间中的模式来生成逼真的样本。
- en: One of the most exciting developments in image generation has been the integration
    of NLP capabilities. Models such as DALL-E, Stable Diffusion, and Midjourney have
    empowered users to generate images simply by providing textual descriptions or
    prompts. This fusion of language and vision has opened up new avenues for creative
    expression, rapid prototyping, and data augmentation for various ML tasks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图像生成领域最令人兴奋的发展之一就是自然语言处理能力的集成。例如DALL-E、Stable Diffusion和Midjourney等模型，使得用户只需提供文本描述或提示，就能生成图像。这种语言与视觉的结合为创意表达、快速原型设计和各种机器学习任务的数据增强开辟了新的途径。
- en: While the advancements in image generation are remarkable, it is crucial to
    address the ethical considerations and potential risks associated with this technology.
    Issues such as deepfakes, biases, and misuse for malicious purposes must be carefully
    addressed to ensure the responsible and ethical deployment of these powerful tools.
    We will look at this topic in detail in the *Ethical considerations and safeguards*
    section of this chapter.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然图像生成技术的进步令人瞩目，但解决与这项技术相关的伦理考虑和潜在风险至关重要。如深度伪造、偏见和恶意用途等问题必须得到妥善处理，以确保这些强大工具的负责任和道德部署。我们将在本章的“伦理考虑和保障措施”部分详细探讨这一主题。
- en: Let us look at some real-world applications for image generation models.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看图像生成模型的一些实际应用。
- en: Real-world applications
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际应用
- en: 'The applications of image generation are endless. Here are some of the real-world
    applications of image generation:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图像生成的应用是无限的。以下是图像生成的一些实际应用：
- en: '**Advertising and marketing**: In the world of advertising and marketing, visuals
    play a crucial role in capturing attention and conveying messages effectively.
    With image generation, you can revolutionize marketing campaigns by producing
    unique, visually striking images tailored to specific target audiences. Marketers
    can leverage Bedrock models to generate personalized product advertisements, social
    media visuals, and eye-catching graphics that resonate with their desired demographics.
    Furthermore, marketers can create variations of images based on customer preferences,
    ensuring that marketing materials are highly relevant and engaging.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**广告和营销**：在广告和营销的世界里，视觉在吸引注意力和有效传达信息方面发挥着至关重要的作用。通过图像生成，你可以通过生产针对特定目标受众的独特、视觉冲击力强的图像来革新营销活动。营销人员可以利用Bedrock模型生成个性化的产品广告、社交媒体视觉图像和吸引人的图形，这些图像与他们的目标受众产生共鸣。此外，营销人员可以根据客户偏好创建图像变体，确保营销材料高度相关且具有吸引力。'
- en: '**Graphic design and content creation**: Graphic designers and content creators
    often face the challenge of conceptualizing and visualizing ideas before executing
    them. With Bedrock’s image generation models, you can streamline this process
    by relying on this powerful tool for generating initial concepts, illustrations,
    and visual assets. Designers can use image generation models to explore different
    styles, compositions, and color schemes, facilitating quick iterations and experimentation.
    Additionally, content creators can leverage Bedrock models to generate unique
    and captivating images for blog posts, articles, or other marketing materials,
    enhancing their visual appeal and potential for engagement.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图形设计和内容创作**：图形设计师和内容创作者在执行之前，常常面临构思和可视化想法的挑战。借助Bedrock的图像生成模型，你可以通过依赖这个强大的工具来生成初始概念、插图和视觉资产，从而简化这一过程。设计师可以使用图像生成模型来探索不同的风格、构图和配色方案，促进快速迭代和实验。此外，内容创作者可以利用Bedrock模型生成独特且吸引人的图像，用于博客文章、文章或其他营销材料，增强其视觉吸引力并提高参与度。'
- en: '**Product visualization and prototyping**: Effective product visualization
    is essential for iterating designs, gathering feedback, and showcasing offerings.
    With Bedrock image generation models, businesses can generate realistic visualizations
    of product designs, allowing for rapid prototyping and evaluation before investing
    in physical prototypes. Bedrock models can create images of products in various
    environments or from different angles, providing stakeholders with a comprehensive
    understanding of the product’s appearance and functionality. This capability can
    significantly accelerate the product development cycle and aid in marketing and
    sales efforts.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**产品可视化和原型设计**：有效的产品可视化对于迭代设计、收集反馈和展示产品至关重要。借助Bedrock图像生成模型，企业可以生成产品设计的逼真可视化，从而在投资物理原型之前进行快速原型设计和评估。Bedrock模型可以创建产品在不同环境或不同角度的图像，为利益相关者提供产品外观和功能的全面理解。这种能力可以显著加速产品开发周期，并有助于市场营销和销售工作。'
- en: '**Gaming and virtual environments**: The gaming and **Virtual Reality** (**VR**)
    industries heavily rely on visually immersive experiences. Bedrock’s image generation
    models can empower developers to create unique textures, environments, and assets
    for video games, VR, or **Augmented Reality** (**AR**) applications. Bedrock image
    models can generate custom avatars, character designs, and intricate visual elements
    based on user specifications or game narratives. In addition, developers can enhance
    the realism and diversity of their virtual worlds, offering players a more engaging
    and personalized experience.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**游戏和虚拟环境**：游戏和**虚拟现实**（**VR**）行业高度依赖视觉沉浸式体验。Bedrock的图像生成模型可以赋予开发者创建独特纹理、环境和资产的能力，用于视频游戏、VR或**增强现实**（**AR**）应用。Bedrock图像模型可以根据用户规格或游戏叙事生成定制的头像、角色设计和复杂的视觉元素。此外，开发者可以增强其虚拟世界的真实性和多样性，为玩家提供更具吸引力和个性化的体验。'
- en: '**Architecture and interior design**: Visualizing architectural designs and
    interior spaces is crucial for architects and interior designers, as well as their
    clients. Bedrock image models can generate realistic renderings of proposed designs,
    allowing stakeholders to immerse themselves in the envisioned spaces before construction
    or renovation. Bedrock’s capabilities can aid in visualizing different materials,
    furniture arrangements, and lighting conditions, enabling architects and designers
    to refine their concepts and present compelling proposals to clients or decision-makers.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**建筑和室内设计**：对于建筑师、室内设计师以及他们的客户来说，可视化建筑设计和室内空间至关重要。Bedrock图像模型可以生成提议设计的逼真渲染图，使利益相关者在施工或翻新前沉浸于设想的空间。Bedrock的能力有助于可视化不同的材料、家具布局和照明条件，使建筑师和设计师能够完善他们的概念并向客户或决策者展示有说服力的提案。'
- en: '**Fashion and apparel**: In the fashion and apparel industry, Amazon Bedrock
    image models can generate unique textile designs, patterns, and clothing styles,
    enabling fashion designers to explore new concepts and stay ahead of trends. Additionally,
    Bedrock can create visualizations of clothing items on different body types or
    in various environments, allowing customers to preview how garments would look
    in real life before making a purchase. This capability can enhance the shopping
    experience and reduce return rates.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时尚和服装**：在时尚和服装行业中，Amazon Bedrock图像模型可以生成独特的纺织品设计、图案和服装风格，使时尚设计师能够探索新概念并保持对潮流的领先。此外，Bedrock还可以创建不同体型或不同环境中的服装物品的视觉表示，让客户在购买前预览服装的真实效果。这种能力可以提升购物体验并降低退货率。'
- en: '**Scientific visualization**: Effective communication of scientific data, phenomena,
    and simulations is crucial in research and education. Amazon Bedrock’s image generation
    models can assist scientists and researchers in creating visual representations
    of complex concepts, making them more accessible and understandable. Bedrock models
    can generate illustrations, diagrams, or 3D models for scientific publications,
    presentations, or educational materials, facilitating knowledge transfer and fostering
    a deeper understanding of intricate topics.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学可视化**：在研究和教育中，有效地传达科学数据、现象和模拟至关重要。Amazon Bedrock的图像生成模型可以帮助科学家和研究人员创建复杂概念的视觉表示，使它们更加易于获取和理解。Bedrock模型可以为科学出版物、演示或教育材料生成插图、图表或3D模型，促进知识转移并加深对复杂主题的理解。'
- en: '**Art and creative expression**: Artists can leverage Bedrock image models
    to explore new styles, techniques, and concepts by generating unique and imaginative
    images based on textual prompts or conceptual frameworks.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**艺术和创意表达**：艺术家可以利用 Bedrock 图像模型通过基于文本提示或概念框架生成独特和富有想象力的图像来探索新的风格、技术和概念。'
- en: '**E-commerce and product catalogs**: In the e-commerce landscape, high-quality
    product images are essential for attracting customers and driving sales. Amazon
    Bedrock image models can generate visually appealing and accurate product images
    for online catalogs or e-commerce platforms, reducing the need for extensive photoshoots
    and the associated costs. These models can also create visualizations of customized
    products or configurations based on customer preferences, enhancing the shopping
    experience and enabling personalization at scale.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**电子商务和产品目录**：在电子商务领域，高质量的产品图像对于吸引顾客和推动销售至关重要。Amazon Bedrock 图像模型可以为在线目录或电子商务平台生成吸引人的、准确的产品图像，减少大量拍摄和相关的成本。这些模型还可以根据客户偏好创建定制产品或配置的可视化，增强购物体验并实现大规模个性化。'
- en: Now that we have looked at some real-world applications, let us explore various
    multimodal models and their inner workings.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经看到了一些实际应用，让我们探索各种多模态模型及其内部工作原理。
- en: Multimodal models
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态模型
- en: 'So far in this book, we have looked at single-modal model architecture patterns,
    such as text-to-text generation, that includes QA, summarization, code generation,
    and so on. Let us now expand our understanding to another type of generative AI
    model: multimodal models.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这本书中，我们已经探讨了单模态模型架构模式，例如文本到文本生成，这包括问答、摘要、代码生成等。现在让我们扩展我们的理解，到另一种生成式 AI
    模型：多模态模型。
- en: Multimodal models are a type of model that can understand and interpret more
    than one modality, such as image, audio, and video, as shown in *Figure 9**.1*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态模型是一种可以理解和解释多种模态（如图像、音频和视频）的模型，如图 *9.1* 所示。
- en: '![Figure 9.1 – Multimodality](img/B22045_09_01.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – 多模态](img/B22045_09_01.jpg)'
- en: Figure 9.1 – Multimodality
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – 多模态
- en: The response received from these models can also be multimodal. Behind the scenes,
    these FMs comprise multiple single-modal neural networks that process text, image,
    audio, and video separately.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型收到的响应也可以是多模态的。幕后，这些 FM 包含多个处理文本、图像、音频和视频的单模态神经网络。
- en: Now let us look at the multimodality models that are available within Amazon
    Bedrock.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看 Amazon Bedrock 内部可用的多模态模型。
- en: Stable Diffusion
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稳定扩散
- en: '**Stable Diffusion** is a state-of-the-art image generation model that has
    gained significant attention in the field of generative AI. Unlike many other
    image generation models, Stable Diffusion employs a unique diffusion-based approach,
    which sets it apart from other methods or techniques.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**稳定扩散** 是一种在生成式 AI 领域受到广泛关注的最先进的图像生成模型。与许多其他图像生成模型不同，稳定扩散采用了一种独特的基于扩散的方法，使其与其他方法或技术区分开来。'
- en: At the heart of Stable Diffusion is the concept of **diffusion**, which involves
    a forward and a reverse diffusion process. In **forward diffusion**, Gaussian
    noise is progressively added to an image until it becomes entirely random. The
    model then learns to reverse this process, gradually removing the noise to reconstruct
    the original image. This reversal is called **reverse diffusion** and is the key
    to Stable Diffusion’s impressive performance.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散的核心概念是 **扩散**，它涉及正向和反向扩散过程。在 **正向扩散** 中，高斯噪声逐渐添加到图像中，直到它完全随机。然后，模型学习逆转这个过程，逐渐去除噪声以重建原始图像。这种逆转称为
    **反向扩散**，是稳定扩散出色性能的关键。
- en: 'This diffusion process has several key components:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个扩散过程有几个关键组成部分：
- en: '**Contrastive Language-Image Pre-Training** (**CLIP**): CLIP is a neural network
    trained on a vast dataset of image-text pairs, enabling it to understand the semantic
    relationships between visual and textual representations. This component plays
    a crucial role in bridging the gap between natural language prompts and their
    corresponding visual manifestations.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对比语言-图像预训练**（**CLIP**）：CLIP 是在大量图像-文本对数据集上训练的神经网络，使其能够理解视觉和文本表示之间的语义关系。该组件在连接自然语言提示及其相应的视觉表现之间起着至关重要的作用。'
- en: '**U-Net**: This serves as a backbone for the image generation process. U-Net
    is a convolutional neural network designed for image-to-image translation tasks
    such as segmentation and denoising. Segmentation is the process whereby the images
    are partitioned into multiple segments or sets of pixels to locate objects and
    boundaries. Denoising removes the noise from an image to improve its quality.
    In the context of Stable Diffusion, U-Net is responsible for generating and refining
    the output image based on the input prompt and guidance from CLIP.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**U-Net**：这作为图像生成过程的骨干。U-Net是一个用于图像到图像翻译任务的卷积神经网络，如分割和去噪。分割是将图像分割成多个部分或像素集的过程，以定位对象和边界。去噪是从图像中去除噪声以提高其质量。在稳定扩散的上下文中，U-Net负责根据输入提示和CLIP的引导生成和细化输出图像。'
- en: '**VAE**: This is another critical component that helps ensure that the generated
    images are coherent and realistic. In Stable Diffusion, VAE encodes the generated
    image into a compressed representation, which is then decoded to produce the final
    output image.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VAE**：这是另一个关键组件，有助于确保生成的图像既连贯又逼真。在稳定扩散中，VAE将生成的图像编码成一个压缩表示，然后解码以生成最终的输出图像。'
- en: 'As shown in *Figure 9**.2*, here is a high-level overview of how the whole
    process works:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图9.2*所示，这里是对整个过程的高级概述：
- en: The user provides a natural language prompt describing the desired image.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户提供一个描述所需图像的自然语言提示。
- en: The CLIP model analyzes the prompt and generates a corresponding embedding,
    representing the semantic meaning of the text.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CLIP模型分析提示并生成相应的嵌入，表示文本的语义意义。
- en: The U-Net architecture takes this embedding as input, along with an initial
    random noise image.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: U-Net架构将这个嵌入作为输入，以及一个初始的随机噪声图像。
- en: Through a series of convolutional and deconvolutional operations, U-Net iteratively
    refines the noise image, guided by the CLIP embedding, to produce an image that
    matches the input prompt.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过一系列卷积和反卷积操作，U-Net在CLIP嵌入的引导下迭代地细化噪声图像，以生成与输入提示匹配的图像。
- en: The generated image is then passed through the VAE, which encodes and decodes
    it, ensuring coherence and realism.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成的图像随后通过VAE，对其进行编码和解码，确保连贯性和逼真性。
- en: The final output image is produced, reflecting the user’s prompt.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成最终的输出图像，反映了用户的提示。
- en: '![Figure 9.2 –  The Stable Diffusion process](img/B22045_09_02.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图9.2 – 稳定扩散过程](img/B22045_09_02.jpg)'
- en: Figure 9.2 – The Stable Diffusion process
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 – 稳定扩散过程
- en: 'By combining these architectural elements, Stable Diffusion is able to generate
    high-quality, diverse images that are both visually appealing and semantically
    coherent with the input prompts. In order to understand the detailed workings
    of the diffusion process, readers are encouraged to read the research paper *On
    the Design Fundamentals* *of Diffusion Models: A Survey*. It can be found on:
    [https://arxiv.org/pdf/2306.04542.pdf](https://arxiv.org/pdf/2306.04542.pdf).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合这些架构元素，稳定扩散能够生成高质量、多样化的图像，这些图像不仅视觉上吸引人，而且与输入提示在语义上保持一致。为了理解扩散过程的详细工作原理，鼓励读者阅读研究论文*关于扩散模型设计基础*的综述。可以在[https://arxiv.org/pdf/2306.04542.pdf](https://arxiv.org/pdf/2306.04542.pdf)找到。
- en: This paper explains how diffusion models work by gradually adding noise to training
    data and then learning to reverse that process to generate new samples. The paper
    highlights the wide range of applications for diffusion models, including image
    editing, text-to-image generation, and 3D object creation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 本文解释了扩散模型是如何工作的，通过逐渐向训练数据添加噪声，然后学习逆转这一过程以生成新的样本。文章突出了扩散模型广泛的应用范围，包括图像编辑、文本到图像生成和3D物体创建。
- en: Additionally, readers are recommended to explore the *How Diffusion Models Work*
    course from DeepLearning.AI at [https://learn.deeplearning.ai/courses/diffusion-models/](https://learn.deeplearning.ai/courses/diffusion-models/).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，建议读者探索DeepLearning.AI的*扩散模型是如何工作的*课程，[https://learn.deeplearning.ai/courses/diffusion-models/](https://learn.deeplearning.ai/courses/diffusion-models/)。
- en: Titan Image Generator G1
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 泰坦图像生成器G1
- en: '**Titan Image Generator G1** is a proprietary image generation model by Amazon
    that allows users to generate images from text, edit existing images, and create
    variations of images. The model is designed to make it easy for users to iterate
    on image concepts by generating multiple image options based on text descriptions.
    The model is trained on diverse high-quality datasets, so it can understand complex
    prompts with multiple objects and generate realistic images.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**Titan图像生成器G1**是Amazon的一个专有图像生成模型，允许用户从文本生成图像，编辑现有图像，并创建图像的变体。该模型旨在通过根据文本描述生成多个图像选项来简化用户对图像概念的迭代。该模型在多样化的高质量数据集上进行了训练，因此它可以理解包含多个对象的复杂提示并生成逼真的图像。'
- en: This model supports image editing capabilities such as editing with text prompts
    using a built-in segmentation model, generating variations of the image, inpainting
    with an image mask, and outpainting to extend or change the background of an image.
    You can upload an existing image and provide instructions or prompts to modify
    specific aspects of the image. The model can intelligently alter the composition,
    add or remove elements, change colors, or apply various artistic styles, all while
    preserving the overall coherence and realism of the original image.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型支持图像编辑功能，例如使用内置分割模型进行基于文本的编辑，生成图像的变体，使用图像遮罩进行修复，以及扩展或更改图像背景的扩展。您可以上传现有的图像并提供指令或提示来修改图像的特定方面。该模型可以智能地改变构图，添加或删除元素，更改颜色，或应用各种艺术风格，同时保持原始图像的整体一致性和现实主义。
- en: We will dive deeper into these capabilities in the *Multimodal design* *patterns*
    section.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*多模态设计* *模式*部分深入探讨这些功能。
- en: Titan Multimodal Embeddings
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Titan多模态嵌入
- en: The **Titan Multimodal Embeddings** model is part of the Amazon Titan family
    of models designed for use cases such as image search and similarity-based recommendation
    with high accuracy and fast response.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**Titan多模态嵌入**模型是Amazon Titan模型系列的一部分，专为需要高精度和快速响应的应用场景，如图像搜索和基于相似度的推荐而设计。'
- en: The Titan Multimodal Embeddings model’s core strength lies in its ability to
    generate high-dimensional vector representations for both textual and visual data.
    These embeddings encapsulate the semantic relationships between different modalities,
    allowing for efficient and effective search and retrieval operations.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Titan多模态嵌入模型的核心优势在于其能够为文本和视觉数据生成高维向量表示。这些嵌入封装了不同模态之间的语义关系，允许进行高效有效的搜索和检索操作。
- en: The model supports up to 128 tokens as input text in English, as well as image
    sizes of up to 25 MB, and converts those to vector embeddings. The default embedding
    size is 1024 dimensions, providing a rich representation that captures nuanced
    details and complex relationships. However, you can also configure smaller vector
    dimensions to optimize for speed and cost, depending on your specific use case
    and performance requirements.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型支持最多128个token作为英文输入文本，以及高达25MB的图像大小，并将这些转换为向量嵌入。默认的嵌入维度是1024，提供丰富的表示，能够捕捉细微的细节和复杂的关系。然而，您也可以根据您的具体用例和性能要求配置更小的向量维度以优化速度和成本。
- en: Anthropic Claude 3 – Sonnet, Haiku, and Opus
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Anthropic Claude 3 – Sonnet、Haiku和Opus
- en: Anthropic Claude 3 Model variants – *Claude 3 Sonnet*, *Claude 3 Haiku*, and
    *Claude 3 Opus* – are the most recent and advanced family of Anthropic Claude
    models available on Amazon Bedrock. All these models have multimodal capabilities,
    meaning that they are able to perceive and analyze images as well as text input,
    with a 200K context window. We encourage you to refer to the *Anthropic Claude*
    section in [*Chapter 1*](B22045_01.xhtml#_idTextAnchor014) if you would like to
    go over their details again.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic Claude 3模型变体——*Claude 3 Sonnet*、*Claude 3 Haiku*和*Claude 3 Opus*——是Amazon
    Bedrock上可用的Anthropic Claude模型中最新的、最先进的系列。所有这些模型都具有多模态能力，这意味着它们能够感知和分析图像以及文本输入，拥有200K的上下文窗口。如果您想再次了解它们的详细信息，请参阅[*第一章*](B22045_01.xhtml#_idTextAnchor014)中的*Anthropic
    Claude*部分。
- en: Now that we have looked at the multimodal models available within Amazon Bedrock,
    let us explore some of the design patterns.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了Amazon Bedrock中可用的多模态模型，让我们来探索一些设计模式。
- en: Multimodal design patterns
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态设计模式
- en: With multimodal design patterns, we integrate different modalities, such as
    text, images, audio, and so on. With the multimodal models available, the ability
    to generate, manipulate, and understand images from text or other input modalities
    has become increasingly important in a wide range of applications, from creative
    design to scientific visualization and beyond.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过多模态设计模式，我们整合了不同的模态，如文本、图像、音频等。随着可用的多模态模型，从文本或其他输入模态生成、操作和理解图像的能力在广泛的领域中变得越来越重要，从创意设计到科学可视化等。
- en: Numerous patterns can be created with multimodal models. In this section, we
    are going to cover some of the common patterns.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用多模态模型创建多种模式。在本节中，我们将介绍一些常见的模式。
- en: Text-to-image
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本到图像
- en: With a text-to-image pattern, you provide the text as a prompt to the model.
    The model will then generate an image based on that prompt, as shown in *Figure
    9**.3.*
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本到图像模式中，您将文本作为提示提供给模型。然后，模型将根据该提示生成图像，如图*图9.3*所示。
- en: '![Figure 9.3 – A text-to-image pattern](img/B22045_09_03.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3 – 文本到图像模式](img/B22045_09_03.jpg)'
- en: Figure 9.3 – A text-to-image pattern
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 – 文本到图像模式
- en: '**Parameters**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**参数**'
- en: 'At the core of image generation models are a set of customizable inference
    parameters and controls that allow users to get the desired image from the model.
    Let us look at these parameters:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图像生成模型的核心是一组可定制的推理参数和控制，使用户能够从模型中获得所需的图像。让我们看看这些参数：
- en: '`Cloud` and `seating bench` to exclude them from the image, as shown in *Figure
    9**.4*.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Cloud`和`seating bench`来排除它们从图像中，如图*图9.4*所示。'
- en: '![Figure 9.4 – A text-to-image pattern with negative prompts](img/B22045_09_04.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4 – 带有负面提示的文本到图像模式](img/B22045_09_04.jpg)'
- en: Figure 9.4 – A text-to-image pattern with negative prompts
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – 带有负面提示的文本到图像模式
- en: '**Reference image**: This provides users the ability to input a reference image
    to the model, which can be leveraged by the model as a baseline to generate its
    response (generated image). For instance, if we use the image generated from the
    preceding figure and pass it as a reference along with the prompt, the prompt
    would be something like this:'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参考图像**：这为用户提供将参考图像输入到模型的能力，模型可以利用它作为生成响应（生成图像）的基线。例如，如果我们使用前一个图生成的图像并将其作为参考与提示一起传递，提示可能如下所示：'
- en: '`A futuristic cityscape at night, with towering skyscrapers made of glass and
    metal. The buildings are illuminated by neon lights in shades of blue, purple,
    and pink. The streets are lined with holographic billboards` `and advertisements.`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`一个夜晚的未来城市景观，由玻璃和金属构成的摩天大楼。建筑被蓝色、紫色和粉红色的霓虹灯光照亮。街道上排列着全息广告牌和广告。`'
- en: The model will use the reference image and the prompt to generate a new image,
    as shown in *Figure 9**.5*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将使用参考图像和提示来生成一个新的图像，如图*图9.5*所示。
- en: '![Figure 9.5 – A text-to-image pattern using a reference Image](img/B22045_09_05.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图9.5 – 使用参考图像的文本到图像模式](img/B22045_09_05.jpg)'
- en: Figure 9.5 – A text-to-image pattern using a reference Image
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 – 使用参考图像的文本到图像模式
- en: '**Prompt Strength** **(****cfg_scale****)**: Prompt strength, also known as
    **Classifier-Free Guidance scale** (*cfg_scale*) determines the degree to which
    the generated image adheres to the provided text prompt. A higher value indicates
    that the image generation process will adhere more closely to the text prompt,
    while a lower value allows for more creative interpretation and diversity in the
    generated images. Using a cfg_scale value somewhere in the middle (*10-15*) is
    generally recommended, as it strikes a balance between faithfully representing
    the text prompt and allowing for artistic expression. However, the optimal value
    may vary depending on your use case or what you are looking for, complexity of
    the prompt, and the desired level of detail in the generated image.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示强度** **(****cfg_scale****)**: 提示强度，也称为**无分类器指导尺度** (*cfg_scale*)，决定了生成的图像遵循提供文本提示的程度。较高的值表示图像生成过程将更紧密地遵循文本提示，而较低的值允许更多的创造性解释和生成图像的多样性。通常建议使用中间的cfg_scale值（*10-15*），因为它在忠实代表文本提示和允许艺术表达之间取得了平衡。然而，最佳值可能因您的用例或您所寻找的内容而异，提示的复杂性以及生成图像所需细节的水平。'
- en: '**Generation Step** **(****steps****)**: The *steps* parameter in Stable Diffusion
    refers to the number of iterations or cycles the algorithm goes through to generate
    an image from the input text. It’s an important setting that affects the quality
    and detail of the final image. Here is how it works:'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成步骤** **(****步骤****)**: 稳定扩散中的*步骤*参数指的是算法从输入文本生成图像所经历的迭代或循环次数。这是一个影响最终图像质量和细节的重要设置。以下是其工作原理：'
- en: The process starts with random noise, and with each step, some of that noise
    is removed, gradually revealing the intended image.
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该过程从随机噪声开始，并且随着每一步的进行，部分噪声被移除，逐渐揭示出预期的图像。
- en: More steps generally lead to higher-quality images with more detail, but there’s
    a point of diminishing returns.
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤数越多通常会导致图像质量更高，细节更丰富，但也会有一个收益递减的点。
- en: The ideal number of steps can vary depending on the complexity of the image
    you’re trying to generate and your personal preferences. However, going much higher
    may not significantly improve the image but will increase generation time.
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理想步骤数可能因你试图生成的图像的复杂性和个人偏好而异。然而，步骤数过多可能不会显著提高图像质量，但会增加生成时间。
- en: For simple subjects or scenes, fewer steps (around *10-15*) may be sufficient.
    But for more complex or detailed images, you may want to increase the steps to
    *40-50* or even more, depending on how much detailed you are looking for.
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于简单主题或场景，大约*10-15*步可能就足够了。但对于更复杂或详细的图像，你可能需要增加步骤到*40-50*或更多，具体取决于你想要多详细。
- en: What we discussed are just some of the parameters. The following figure highlights
    additional parameters for Stable Diffusion.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所讨论的只是其中的一些参数。以下图例突出了稳定扩散的附加参数。
- en: '![Figure 9.6 – Stable Diffusion text-to-image parameters](img/B22045_09_06.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图9.6 – 稳定扩散文本到图像参数](img/B22045_09_06.jpg)'
- en: Figure 9.6 – Stable Diffusion text-to-image parameters
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 – 稳定扩散文本到图像参数
- en: For a more detailed description of these parameters, you can go through the
    Stable Diffusion documentation at [https://platform.stability.ai/docs/api-reference#tag/Image-to-Image](https://platform.stability.ai/docs/api-reference#tag/Image-to-Image).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要详细了解这些参数，你可以查阅[https://platform.stability.ai/docs/api-reference#tag/Image-to-Image](https://platform.stability.ai/docs/api-reference#tag/Image-to-Image)上的稳定扩散文档。
- en: 'If you are using Amazon Titan Image Generator, here is the list of parameters
    that you can use: [https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-image.html](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-image.html).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用亚马逊Titan图像生成器，以下是你可以使用的参数列表：[https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-image.html](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-image.html).
- en: Image search
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像搜索
- en: Image search has emerged as a powerful tool that enables users to explore and
    leverage vast collections of visual data. With FMs from Amazon Bedrock, you can
    perform image searches to understand and interpret visual content. You can identify
    and understand various elements within an image, such as objects, scenes, colors,
    textures, and even abstract concepts. To illustrate the power of image search,
    let’s consider a practical example.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图像搜索已成为一种强大的工具，使用户能够探索和利用大量的视觉数据集。通过亚马逊Bedrock的FMs，你可以执行图像搜索以理解和解释视觉内容。你可以识别和理解图像中的各种元素，如物体、场景、颜色、纹理，甚至抽象概念。为了说明图像搜索的力量，让我们考虑一个实际例子。
- en: Imagine you’re a fashion retailer with an extensive catalog of clothing items.
    With Bedrock, you can upload your product images and leverage the image search
    capabilities to enable customers to find visually similar items. For instance,
    a customer could upload a picture of a dress that they like and Bedrock would
    return a set of visually similar dresses from your catalog, facilitating a more
    engaging and personalized shopping experience.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你是一家拥有大量服装商品目录的时尚零售商。使用Bedrock，你可以上传你的产品图片，并利用图像搜索功能，让客户能够找到视觉上相似的商品。例如，一位客户可以上传一张他们喜欢的连衣裙图片，Bedrock会从你的目录中返回一组视觉上相似的连衣裙，从而促进更吸引人和个性化的购物体验。
- en: One powerful approach to image search is based on multimodal embeddings, which
    allow for the representation of both text and images in a vector space. These
    vectors capture the visual features and semantic information of the images. The
    vectors, along with metadata such as image paths, are then stored in a searchable
    index vector database such as OpenSearch Serverless, FAISS, or Pinecone. This
    technique enables searching for images using text queries or finding similar images
    based on a given image (or a combination of text and image).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图像搜索的一种强大方法是基于跨模态嵌入，它允许在向量空间中同时表示文本和图像。这些向量捕捉图像的视觉特征和语义信息。然后，这些向量以及如图像路径之类的元数据存储在可搜索的索引向量数据库中，例如OpenSearch
    Serverless、FAISS或Pinecone。这项技术使您能够使用文本查询搜索图像或根据给定图像（或文本和图像的组合）找到相似图像。
- en: When a user initiates a search, their input (text, image, or both) is also converted
    into a vector representation using the same multimodal embedding model. The search
    vector is then compared against the vectors in the index and the most similar
    vectors are retrieved based on the vector similarity scores. This approach allows
    for flexible and intuitive image search, as users can search using natural language
    descriptions, upload example images, or combine text and images for more precise
    results. For example, you could search for `a red sports car on a city street`
    and the model would return relevant images from its data store that match both
    the visual and textual criteria.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户发起搜索时，他们的输入（文本、图像或两者兼有）也会使用相同的跨模态嵌入模型转换为向量表示。然后，搜索向量与索引中的向量进行比较，并根据向量相似度分数检索最相似的向量。这种方法允许灵活直观的图像搜索，因为用户可以使用自然语言描述进行搜索，上传示例图像，或结合文本和图像以获得更精确的结果。例如，您可以搜索“城市街道上的红色跑车”并从其数据存储中返回匹配视觉和文本标准的相关图像。
- en: 'As you might have recognized by now, this process is similar to the RAG process
    that we discussed in [*Chapter 5*](B22045_05.xhtml#_idTextAnchor090). The difference
    here is that the model is retrieving the images from its data store and is not
    generating new images. Here is a great example to try out multimodal embedding
    and searching: [https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/04_Image_and_Multimodal/bedrock-titan-multimodal-embeddings.ipynb](https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/04_Image_and_Multimodal/bedrock-titan-multimodal-embeddings.ipynb).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如您现在可能已经注意到的，这个过程与我们讨论的[*第5章*](B22045_05.xhtml#_idTextAnchor090)中的RAG过程类似。这里的区别在于，模型是从其数据存储中检索图像，而不是生成新图像。以下是一个尝试跨模态嵌入和搜索的绝佳示例：[https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/04_Image_and_Multimodal/bedrock-titan-multimodal-embeddings.ipynb](https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/04_Image_and_Multimodal/bedrock-titan-multimodal-embeddings.ipynb)。
- en: Image search with multimodal embeddings has numerous real-world applications
    across various domains. In e-commerce platforms, it can be used to enhance product
    search and recommendation systems, allowing customers to find visually similar
    products or to search for items using natural language descriptions or example
    images. In the media and entertainment industry, it can assist in content organization,
    tag suggestion, and copyright infringement detection by identifying similar or
    duplicate images.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 跨模态嵌入的图像搜索在各个领域有众多实际应用。在电子商务平台上，它可以用于增强产品搜索和推荐系统，使客户能够找到视觉上相似的产品或使用自然语言描述或示例图像进行搜索。在媒体和娱乐行业，它可以通过识别相似或重复的图像来协助内容组织、标签建议和版权侵权检测。
- en: Image understanding
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像理解
- en: The Anthropic Claude 3 models – Sonnet, Haiku, and Opus – introduce the image
    understanding capability, through which the model can analyze the image and provide
    you with a response based on what you are looking to know. For example, you can
    provide an image of a kitchen or a living room and ask the model to provide a
    detailed description of the image or write a fictional story based on the image.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic Claude 3模型——Sonnet、Haiku和Opus——引入了图像理解能力，通过该能力，模型可以分析图像并根据您想要了解的内容为您提供响应。例如，您可以提供厨房或客厅的图像，并要求模型提供图像的详细描述或根据图像编写虚构故事。
- en: Example 1
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例 1
- en: 'Use the following prompt: `Provide a detailed description of` `this image`.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下提示：“提供此图像的详细描述”。
- en: '![Figure 9.7 – Image understanding and a detailed description in the output](img/B22045_09_07.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图9.7 – 图像理解和输出中的详细描述](img/B22045_09_07.jpg)'
- en: Figure 9.7 – Image understanding and a detailed description in the output
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – 输出中的图像理解和详细描述
- en: In *Figure 9**.7*, we have provided the image of a kitchen to the Anthropic
    Claude 3 model and asked it to provide a detailed description of the image. The
    model is able to provide minute details such as **the room features dark wood
    cabinetry, contrasted by light marble countertops**, and so on.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 9.7* 中，我们向 Anthropic Claude 3 模型提供了厨房的图像，并要求它提供图像的详细描述。模型能够提供诸如 **房间特征为深色木制橱柜，与浅色大理石台面形成对比**
    等细微细节。
- en: Example 2
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例 2
- en: 'Use the following prompt: `Write a fictional story based on the` `image attached`.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下提示：`根据所附图像编写一个虚构故事`。
- en: '![Figure 9.8 – Image understanding with a fictional story](img/B22045_09_08.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.8 – 使用虚构故事进行图像理解](img/B22045_09_08.jpg)'
- en: Figure 9.8 – Image understanding with a fictional story
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 – 使用虚构故事进行图像理解
- en: In *Figure 9**.8*, you can see that the model has generated a fictional story
    based on the image of a library provided to it.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 9.8* 中，您可以看到模型根据提供给它的图书馆图像生成了一个虚构故事。
- en: Example 3
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例 3
- en: 'Use the following prompt: `Provide the list of items/objects present in the
    image and explain` `each item`.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下提示：`提供图像中存在的物品/对象的列表，并解释每个物品`。
- en: '![Figure 9.9 – Image understanding with object identification](img/B22045_09_09.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.9 – 使用对象识别进行图像理解](img/B22045_09_09.jpg)'
- en: Figure 9.9 – Image understanding with object identification
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9 – 使用对象识别进行图像理解
- en: In *Figure 9**.9*, the model is able to identify the items and objects in the
    image along with their details, showcasing the image classification/object recognition
    capability.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 9.9* 中，模型能够识别图像中的物品和对象及其详细信息，展示了图像分类/对象识别能力。
- en: The Claude models’ image understanding capabilities are not limited to those
    discussed in the preceding examples. They can also be utilized for tasks such
    as image captioning, creating detailed image descriptions, identifying subjects,
    and answering questions about the contents of an image. You can look at various
    use cases of image understanding at [https://docs.anthropic.com/claude/docs/use-cases-and-capabilities#vision-capabilities](https://docs.anthropic.com/claude/docs/use-cases-and-capabilities#vision-capabilities).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Claude 模型的图像理解能力不仅限于前面示例中讨论的内容。它们还可以用于图像标题、创建详细的图像描述、识别主题以及回答有关图像内容的问题。您可以在
    [https://docs.anthropic.com/claude/docs/use-cases-and-capabilities#vision-capabilities](https://docs.anthropic.com/claude/docs/use-cases-and-capabilities#vision-capabilities)
    查看图像理解的多种用例。
- en: To use this capability within the Amazon Bedrock console, yo[u can follow the
    ensuing steps:](https://console.aws.amazon.com/bedrock-)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Amazon Bedrock 控制台中使用此功能，您可以按照以下步骤操作：[https://console.aws.amazon.com/bedrock-](https://console.aws.amazon.com/bedrock-)
- en: '[Go t](https://console.aws.amazon.com/bedrock-)o Amazon Bedrock Console at
    https://console.aws.amazon.com/bedrock.'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[前往 Amazon Bedrock 控制台](https://console.aws.amazon.com/bedrock-)：https://console.aws.amazon.com/bedrock。'
- en: Navigate to **Chat Playground**.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到 **Chat Playground**。
- en: Click on **Select model**. Choose the **Anthropic Claude 3 Sonnet**, **Anthropic
    Claude 3 Haiku**, or **Anthropic Claude 3** **Opus** model.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **选择模型**。选择 **Anthropic Claude 3 Sonnet**、**Anthropic Claude 3 Haiku** 或
    **Anthropic Claude 3 Opus** 模型。
- en: Attach the image you want to analyze and provide the prompt based on what you
    are looking for, as shown in *Figure 9**.10*.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 附加您想要分析的图像，并根据您所寻找的内容提供提示，如图 *图 9.10* 所示。
- en: '![Figure 9.10 – How to analyze an image using the Anthropic Claude 3 models](img/B22045_09_10.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.10 – 如何使用 Anthropic Claude 3 模型分析图像](img/B22045_09_10.jpg)'
- en: Figure 9.10 – How to analyze an image using the Anthropic Claude 3 models
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10 – 如何使用 Anthropic Claude 3 模型分析图像
- en: 'If you are using AWS SDK, you can use Anthropic’s `Messages` API to create
    a chat application and provide an image for understanding. Here is some example
    AWS Python SDK code for a multimodal message to the Claude 3 Sonnet model: [https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html#api-inference-examples-claude-multimodal-code-example](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html#api-inference-examples-claude-multimodal-code-example).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用 AWS SDK，您可以使用 Anthropic 的 `Messages` API 创建聊天应用程序并提供图像进行理解。以下是一个用于 Claude
    3 Sonnet 模型的多模态消息的 AWS Python SDK 示例代码：[https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html#api-inference-examples-claude-multimodal-code-example](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html#api-inference-examples-claude-multimodal-code-example)。
- en: Image-to-image patterns
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像到图像模式
- en: When it comes to image-to-image generation, the model takes an existing image
    as input and modifies it based on the prompt or instructions you provide. This
    is different from text-to-image generation, whereby the model creates an entirely
    new image from scratch based solely on a textual description or prompt. In image-to-image
    generation, on the other hand, the model uses the existing image as a starting
    point and then applies the necessary changes or transformations to produce the
    desired output image. This can involve adjusting various aspects such as colors,
    textures, objects, or even the overall composition of the image, all guided by
    the prompt. It’s like having a clay model and reshaping it to your desired outcome
    rather than starting from a lump of raw clay. The ability to modify and manipulate
    existing images opens up a range of creative possibilities and use cases, from
    enhancing and editing photographs to creating artistic interpretations or visualizations.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到图像到图像生成时，模型以现有的图像作为输入，并根据您提供的提示或指令对其进行修改。这与基于文本生成图像不同，后者模型完全从头开始创建一个全新的图像，仅基于文本描述或提示。在图像到图像生成中，模型使用现有的图像作为起点，然后应用必要的更改或转换以生成所需的输出图像。这可能涉及调整颜色、纹理、物体或图像的整体构图等各个方面，所有这些都由提示指导。这就像拥有一个黏土模型，并将其重塑成您期望的形状，而不是从一块原始的黏土开始。修改和操作现有图像的能力为各种创意可能性和用例打开了大门，从增强和编辑照片到创建艺术诠释或可视化。
- en: A simple example of image-to-image generation is shown in *Figure 9**.11*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.11* 展示了图像到图像生成的一个简单示例。'
- en: '![Figure 9.11 – Simple image-to-image generation](img/B22045_09_11.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.11 – 简单的图像到图像生成](img/B22045_09_11.jpg)'
- en: Figure 9.11 – Simple image-to-image generation
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11 – 简单的图像到图像生成
- en: When using the Stable Diffusion model for image-to-image generation, there are
    a few additional parameters to consider along with the text-to-text parameters
    mentioned in *Figure 9**.6*. These additional parameters are highlighted in *Figure
    9**.12.*
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用稳定扩散模型进行图像到图像生成时，除了*图 9.6* 中提到的文本到文本参数外，还有一些额外的参数需要考虑。这些额外的参数在*图 9.12* 中突出显示。
- en: '![Figure 9.12 – Stable Diffusion image-to-image parameters](img/B22045_09_12.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.12 – 稳定扩散图像到图像参数](img/B22045_09_12.jpg)'
- en: Figure 9.12 – Stable Diffusion image-to-image parameters
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12 – 稳定扩散图像到图像参数
- en: 'You can learn more about them here: [https://platform.stability.ai/docs/api-reference#tag/Image-to-Image/operation/imageToImage](https://platform.stability.ai/docs/api-reference#tag/Image-to-Image/operation/imageToImage).'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此处了解更多信息：[https://platform.stability.ai/docs/api-reference#tag/Image-to-Image/operation/imageToImage](https://platform.stability.ai/docs/api-reference#tag/Image-to-Image/operation/imageToImage)。
- en: Next, let us look at some image-to-image patterns.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看一些图像到图像模式。
- en: Image variation
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像变化
- en: '**Image variation**, also known as **image-to-image translation** or **style
    transfer**, is a powerful technique in generative AI that allows for the creation
    of new and unique images by modifying existing ones. This process involves taking
    an input image and applying a desired style or transformation to it, resulting
    in an output image that combines the content of the original with the desired
    aesthetic or visual characteristics.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**图像变化**，也称为**图像到图像翻译**或**风格迁移**，是生成式人工智能中的一个强大技术，它通过修改现有图像来创建新的独特图像。这个过程涉及获取一个输入图像，并对其应用一个期望的风格或转换，从而生成一个输出图像，该图像结合了原始图像的内容和期望的美学或视觉特征。'
- en: One real-world example of image variation is in the field of fashion design.
    Designers can take an existing garment or accessory and apply various styles,
    patterns, or textures to create new and innovative designs without starting from
    scratch. This not only saves time and resources but also allows for rapid experimentation
    and iteration, enabling designers to explore a vast range of possibilities.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一个现实世界的图像变化例子是在时尚设计领域。设计师可以取一个现有的服装或配饰，并应用各种风格、图案或纹理来创建新的创新设计，而无需从头开始。这不仅节省了时间和资源，还允许快速实验和迭代，使设计师能够探索广泛的可能。
- en: Another example can be found in the art world, where image variation techniques
    can be used to create unique and expressive artworks. Artists can take a simple
    photograph or painting and apply various artistic styles, such as impressionism,
    cubism, or abstract expressionism, to create entirely new pieces that blend the
    original content with the desired artistic style. This opens up new avenues for
    creative expression and allows artists to explore unconventional and thought-provoking
    visual interpretations.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在艺术界，图像变化技术也可以用来创建独特和富有表现力的艺术作品。艺术家可以采用简单的照片或绘画，并应用各种艺术风格，如印象派、立体主义或抽象表现主义，以创建完全新的作品，将原始内容与所需的艺术风格融合在一起。这为创造性表达开辟了新的途径，并允许艺术家探索非传统和引人深思的视觉解释。
- en: Image variation also has applications in the fields of interior design and architectural
    visualization. Designers and architects can take existing spaces or structures
    and apply different materials, textures, or lighting conditions to visualize how
    a space might look with different design choices. This can be invaluable in helping
    clients understand and appreciate proposed designs, as well as in enabling designers
    to quickly iterate and refine their concepts.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图像变化在室内设计和建筑可视化领域也有应用。设计师和建筑师可以采用现有的空间或结构，应用不同的材料、纹理或照明条件来可视化空间在不同设计选择下的可能外观。这有助于客户理解和欣赏所提出的设计，同时也使得设计师能够快速迭代和细化他们的概念。
- en: 'With Bedrock, you can utilize Titan Image Generator to create image variations.
    Let us try the following prompt and run it through Titan Image Generator:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Bedrock，您可以利用Titan图像生成器来创建图像变化。让我们尝试以下提示并通过Titan图像生成器运行它：
- en: '`A delicate, nature-inspired pattern with intricate illustrations of birds,
    butterflies, and foliage, perfect for a romantic, bohemian dress` `or scarf.`'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`一种精致的自然图案，细致描绘了鸟类、蝴蝶和叶子的图案，非常适合浪漫波西米亚风格的连衣裙`或围巾`。`'
- en: '![Figure 9.13 – Image variation](img/B22045_09_13.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图9.13 – 图像变化](img/B22045_09_13.jpg)'
- en: Figure 9.13 – Image variation
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13 – 图像变化
- en: As shown in *Figure 9**.13*, Titan Image Generator will create an image (**Original
    Image**). You can generate image variations that leverage the **Original Image**
    as a reference, along with an optional prompt that can be provided to the model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图9**.13所示，Titan图像生成器将创建一个图像（**原始图像**）。您可以使用**原始图像**作为参考来生成图像变化，同时还可以提供可选的提示供模型使用。
- en: Masking
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 遮罩
- en: 'Amazon Bedrock models – Amazon Titan Generator and Stable Diffusion – offer
    two powerful image editing techniques: *masking* and *painting*.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Bedrock模型——Amazon Titan Generator和Stable Diffusion——提供了两种强大的图像编辑技术：*遮罩*和*绘画*。
- en: With masking, we define specific areas within an image and mask them, either
    to be preserved or redrawn. This masking can be done either via an image file
    or a prompt.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 使用遮罩，我们定义图像中的特定区域并将其遮罩，要么保留要么重新绘制。这种遮罩可以通过图像文件或提示来完成。
- en: Image masking
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 图像遮罩
- en: 'The approach of **image masking** utilizes a separate image file, known as
    the **mask image**, to specify the pixels to be masked or preserved in the original
    image. The mask image must adhere to the following requirements:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**图像遮罩**的方法使用一个单独的图像文件，称为**遮罩图像**，来指定要遮罩或保留在原始图像中的像素。遮罩图像必须满足以下要求：'
- en: '**Identical dimensions and resolution as the original image**: When using image
    masking, it’s crucial that the mask image has the exact same dimensions and resolution
    as the original image that you want to mask. This ensures that each pixel in the
    mask image corresponds to a pixel in the original image, allowing for precise
    masking. If the dimensions or resolutions differ, the masking process may produce
    distorted or undesired results.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与原始图像相同的尺寸和分辨率**：在应用图像遮罩时，确保遮罩图像与您想要遮罩的原始图像具有完全相同的尺寸和分辨率至关重要。这确保了遮罩图像中的每个像素都与原始图像中的像素相对应，从而实现精确的遮罩。如果尺寸或分辨率不同，遮罩过程可能会产生扭曲或不理想的结果。'
- en: For example, if your original image has a resolution of 1920 x 1080 pixels,
    the mask image must also have a resolution of 1920 x 1080 pixels. Any discrepancy
    in the dimensions or resolution will cause the mask to misalign with the original
    image, leading to undesirable masking effects.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，如果您的原始图像分辨率为1920 x 1080像素，遮罩图像也必须具有1920 x 1080像素的分辨率。任何尺寸或分辨率的差异都可能导致遮罩与原始图像错位，导致不理想的遮罩效果。
- en: '**No alpha channel**: The mask image should not have an alpha channel, which
    is a separate component in some image formats that represents transparency. While
    the PNG format supports transparency through an alpha channel, for image masking
    purposes, the mask image should rely solely on color values (**Red, Green, Blue**
    (**RGB**) or grayscale) to represent masked and unmasked regions.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无alpha通道**：遮罩图像不应包含alpha通道，这是某些图像格式中的一个单独组件，用于表示透明度。虽然PNG格式通过alpha通道支持透明度，但对于图像遮罩目的，遮罩图像应仅依赖于颜色值（**红色、绿色、蓝色**（**RGB**）或灰度）来表示遮罩和未遮罩区域。'
- en: The absence of an alpha channel simplifies the masking process and ensures that
    the masking is based solely on the pixel colors, without any additional transparency
    information. This approach is often preferred for its simplicity and compatibility
    with a wide range of image processing tools and libraries.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: alpha通道的缺失简化了遮罩过程，并确保遮罩仅基于像素颜色，而不包含任何额外的透明度信息。这种方法通常因其简单性和与广泛图像处理工具和库的兼容性而受到青睐。
- en: '`0`, `0`, `0`) as masked areas, while any non-black pixels are considered unmasked
    regions.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`, `0`, `0`) 作为遮罩区域，而任何非黑色像素都被视为未遮罩区域。'
- en: '`0` (black) to `255` (white). The masking process interprets pixels with a
    value of `0` as masked areas, while pixels with non-zero values are considered
    unmasked regions.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`（黑色）到`255`（白色）。遮罩过程将值为`0`的像素解释为遮罩区域，而将非零值的像素视为未遮罩区域。'
- en: The choice between RGB and grayscale color modes depends on your specific use
    case and on the tools or libraries you’re using for image masking. Some tools
    may have a preference for one color mode over the other.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: RGB和灰度色彩模式的选择取决于您的具体用途以及您用于图像遮罩的工具或库。某些工具可能对某种色彩模式有偏好。
- en: For example, let’s assume that you work in the Food and Beverages industry and
    you want to mask out certain food items from an image to create a transparent
    layer for a menu design. Suppose that you want to mask the bowl of chips in the
    image that follows and maybe remove it from your menu. *Figure 9**.14* shows the
    original image and the masked image, where the masking is done on the bowl of
    chips.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设您在食品和饮料行业工作，并且您想从图像中遮罩掉某些食品项目，以创建菜单设计的透明层。假设您想遮罩下图中芯片碗，并可能将其从菜单中移除。*图9**.14*显示了原始图像和遮罩图像，其中遮罩是在芯片碗上进行的。
- en: '![Figure 9.14 – Image masking](img/B22045_09_14.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图9.14 – 图像遮罩](img/B22045_09_14.jpg)'
- en: Figure 9.14 – Image masking
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.14 – 图像遮罩
- en: 'If you want to experiment with image masking, you can use online photo editing
    tools or apps. There is also the **Python Image Library** (**PIL**), a very popular
    Python library that is worth checking out at: [https://pillow.readthedocs.io/en/stable/reference/Image.html](https://pillow.readthedocs.io/en/stable/reference/Image.html).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想尝试图像遮罩，您可以使用在线照片编辑工具或应用程序。还有**Python图像库**（**PIL**），这是一个非常流行的Python库，值得在[https://pillow.readthedocs.io/en/stable/reference/Image.html](https://pillow.readthedocs.io/en/stable/reference/Image.html)上查看。
- en: 'In addition, we recommend that you experiment with the following GitHub examples
    from the Amazon Bedrock workshop that showcase image masking and painting: [https://github.com/aws-samples/amazon-bedrock-workshop/tree/main/04_Image_and_Multimodal](https://github.com/aws-samples/amazon-bedrock-workshop/tree/main/04_Image_and_Multimodal).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们建议您尝试来自Amazon Bedrock研讨会的工作坊中的以下GitHub示例，这些示例展示了图像遮罩和绘画：[https://github.com/aws-samples/amazon-bedrock-workshop/tree/main/04_Image_and_Multimodal](https://github.com/aws-samples/amazon-bedrock-workshop/tree/main/04_Image_and_Multimodal)。
- en: Mask prompting
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 遮罩提示
- en: '**Mask prompting** involves masking images through the use of textual prompts.
    These textual prompts serve as a guide to the model to comprehend the desired
    masking area within an image.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**遮罩提示**涉及通过使用文本提示来遮罩图像。这些文本提示作为模型的指南，帮助模型理解图像中期望的遮罩区域。'
- en: The benefit of using mask prompting as opposed to image masking lies in the
    dynamic nature of mask prompting. You can effortlessly modify the masking by simply
    altering the textual prompt, allowing for rapid iteration and experimentation.
    This flexibility allows artists, designers, and content creators to explore a
    vast array of visual concepts and narratives without being constrained by traditional
    image editing tools.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 与图像遮罩相比，使用遮罩提示的优点在于其动态性。您只需简单地更改文本提示，就可以轻松地修改遮罩，从而实现快速迭代和实验。这种灵活性使得艺术家、设计师和内容创作者能够探索广泛的视觉概念和叙事，而不会受到传统图像编辑工具的限制。
- en: In addition, mask prompting can be seamlessly integrated into various workflows
    and applications, enabling seamless collaboration and enhancing productivity.
    For instance, in the field of visual storytelling, writers and directors can leverage
    this feature to conceptualize and refine their vision, while designers can quickly
    prototype and iterate on visual concepts before committing to a final design.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，遮罩提示可以无缝集成到各种工作流程和应用中，实现无缝协作并提高生产力。例如，在视觉叙事领域，作家和导演可以利用此功能来构思和细化他们的愿景，而设计师可以在最终确定最终设计之前快速原型化和迭代视觉概念。
- en: To ensure the integrity and originality of the content generated, Amazon Bedrock
    has implemented robust measures to safeguard against plagiarism and unethical
    practices. We will discuss ethical considerations and safeguards in the upcoming
    section.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保生成内容的完整性和原创性，Amazon Bedrock已经实施了强大的措施来防范剽窃和不道德的行为。我们将在下一节讨论道德考虑和保障措施。
- en: Let’s take the same example from the preceding figure. Instead of image masking,
    we want to apply a mask prompt. We’ll say that you want to remove the bowl of
    chips from the original image. With mask prompting, you can provide the `only
    bowl of chips` prompt and then further perform painting.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以先前的图中的相同例子为例。而不是图像遮罩，我们想要应用遮罩提示。我们将说您想要从原始图像中移除薯片碗。通过遮罩提示，您可以提供`只有薯片碗`的提示，然后进一步进行绘画。
- en: '![Figure 9.15 – Mask prompting](img/B22045_09_15.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图9.15 – 遮罩提示](img/B22045_09_15.jpg)'
- en: Figure 9.15 – Mask prompting
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.15 – 遮罩提示
- en: In *Figure 9**.15*, we performed inpainting and changed the bowl of chips to
    a bowl of apple slices. Let us discuss painting in more detail.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图9.15*中，我们执行了图像替换，将薯片碗改为苹果片碗。让我们更详细地讨论绘画。
- en: Painting
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 绘画
- en: '**Painting** is a technique whereby you can fill in the masked regions within
    an image or extend it using an image generation model. There are two methods of
    painting: inpainting and outpainting.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**绘画**是一种技术，您可以使用它填充图像中的遮罩区域或使用图像生成模型扩展它。有两种绘画方法：图像修复和扩展绘画。'
- en: Inpainting
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 图像修复
- en: With **inpainting**, you are essentially reconstructing or filling in missing,
    masked, or corrupted portions of the image. This technique is particularly useful
    in scenarios where an image has been damaged or obscured, or where it contains
    unwanted elements that need to be removed or replaced. By providing the image
    generation model with the surrounding context and effective prompts, it can intelligently
    generate and blend new content in the designated areas, seamlessly reconstructing
    the masked regions.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 通过**图像修复**，您实际上是在重建或填充图像中缺失、被遮罩或损坏的部分。这种技术在图像已被损坏或遮挡，或者其中包含需要移除或替换的不希望存在的元素的场景中特别有用。通过向图像生成模型提供周围环境和有效的提示，它可以在指定区域智能地生成和融合新内容，无缝地重建遮罩区域。
- en: 'Let us look at some examples:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些例子：
- en: '`telephone line` and provide an empty prompt text (`""`).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`电话线`并提供一个空的提示文本(`""`)。'
- en: '![Figure 9.16 – Inpainting removal](img/B22045_09_16.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图9.16 – 图像修复移除](img/B22045_09_16.jpg)'
- en: Figure 9.16 – Inpainting removal
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.16 – 图像修复移除
- en: '**Inpainting replacement**: Suppose that you want to replace any object or
    scene within the image. In that case, you can perform inpainting replacement.
    As shown in *Figure 9**.17*, you can specify within the prompt text what you want
    to replace.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像替换修复**：假设您想要替换图像中的任何对象或场景。在这种情况下，您可以执行图像替换修复。如图*图9.17*所示，您可以在提示文本中指定您想要替换的内容。'
- en: '![Figure 9.17 – Inpainting replacement](img/B22045_09_17.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图9.17 – 图像替换修复](img/B22045_09_17.jpg)'
- en: Figure 9.17 – Inpainting replacement
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.17 – 图像替换修复
- en: Outpainting
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 扩展绘画
- en: '**Outpainting** is the process of extending the image beyond its original boundaries,
    or in other words painting outside the masked regions. Outpainting is useful in
    scenarios where the original image or artwork needs to be extended or augmented
    with additional elements, environments, or perspectives.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**扩展绘画**是指将图像扩展到其原始边界之外的过程，换句话说，是在遮罩区域之外进行绘画。扩展绘画在原始图像或艺术品需要通过添加额外的元素、环境或视角进行扩展或增强的场景中非常有用。'
- en: Let us look at an example.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个例子。
- en: '![Figure 9.18 – Outpainting](img/B22045_09_18.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图9.18 – 扩展绘画](img/B22045_09_18.jpg)'
- en: Figure 9.18 – Outpainting
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.18 – 扩展绘画
- en: In *Figure 9**.18*, you can see that we are painting outside the masked image
    or prompt (in this case, `Indian curry`) to add some details. These include the
    background of a wooden table, as well as adding a spoon, a knife, a side of rice,
    and a chai.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图9.18*中，你可以看到我们在蒙版图像或提示（在这种情况下，`印度咖喱`）之外进行绘画，以添加一些细节。这些包括木质桌子的背景，以及添加勺子、刀子、一碟米饭和一杯奶茶。
- en: If you want to understand the prompt engineering best practices for the Titan
    Image Generator model, please check out [https://tinyurl.com/titan-image-generator-prompt](https://tinyurl.com/titan-image-generator-prompt).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解Titan Image Generator模型的提示工程最佳实践，请查看[https://tinyurl.com/titan-image-generator-prompt](https://tinyurl.com/titan-image-generator-prompt)。
- en: Now that we have looked at different patterns of multimodal and image patterns,
    let us look at the ethical considerations and available safeguards within Amazon
    Bedrock to ensure the responsible use of Generative AI.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了多模态和图像模式的不同模式，让我们来看看Amazon Bedrock中关于伦理考量及可用的安全措施，以确保生成式AI的负责任使用。
- en: Ethical considerations and safeguards
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 伦理考量与安全措施
- en: Generative AI models, particularly those capable of generating highly realistic
    images, raise significant ethical concerns regarding the potential spread of misinformation
    and deepfakes. As these models are becoming increasingly powerful and accessible,
    it is crucial to address these ethical challenges proactively to promote the responsible
    development and deployment of this technology.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI模型，尤其是那些能够生成高度逼真图像的模型，在传播虚假信息和深度伪造方面引发了重大的伦理担忧。随着这些模型变得越来越强大和易于访问，积极应对这些伦理挑战至关重要，以促进这项技术的负责任开发和部署。
- en: One of the primary ethical concerns surrounding image generation models is the
    risk of creating and disseminating misleading or manipulated content. With the
    ability to generate photorealistic images from text prompts comes the potential
    for malicious actors to create and spread false or fabricated visual information.
    This can have far-reaching consequences, such as undermining trust in media, spreading
    disinformation, and even influencing political or social narratives.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 围绕图像生成模型的主要伦理担忧之一是创建和传播误导性或操纵性内容的危险。随着从文本提示生成逼真图像的能力，恶意行为者有可能创建和传播虚假或编造的视觉信息。这可能会产生深远的影响，例如损害对媒体的信任、传播虚假信息，甚至影响政治或社会叙事。
- en: To address this major ethical challenge, it is crucial for organizations and
    researchers to prioritize responsible development and deployment of a generative
    AI life cycle. When using Amazon Bedrock, users can utilize its **watermark detection**
    feature for images generated by Amazon Titan Image Generator.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这一主要伦理挑战，对于组织和研究人员来说，优先考虑生成式AI生命周期的负责任开发和部署至关重要。当使用Amazon Bedrock时，用户可以利用其**水印检测**功能来识别由Amazon
    Titan Image Generator生成的图像。
- en: The watermark detection capability in Amazon Bedrock is designed to promote
    transparency and accountability in the use of AI-generated images. By embedding
    an invisible watermark in every image created by the model, content creators,
    news organizations, risk analysts, and others can quickly verify whether an image
    has been generated using Amazon Titan Image Generator.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Bedrock中的水印检测功能旨在促进AI生成图像使用的透明度和问责制。通过在每个由模型创建的图像中嵌入一个不可见的水印，内容创作者、新闻机构、风险分析师等可以迅速验证图像是否是由Amazon
    Titan Image Generator生成的。
- en: 'This approach serves two primary ethical purposes:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有两个主要的伦理目的：
- en: It helps combat the spread of misinformation and deepfakes by providing a mechanism
    to verify the authenticity of images. This can help build trust and credibility
    in visual content, particularly in domains where the integrity of information
    is critical, such as journalism, law enforcement, and scientific research.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过提供一种验证图像真实性的机制来帮助对抗虚假信息和深度伪造的传播。这有助于在视觉内容中建立信任和信誉，特别是在信息完整性至关重要的领域，如新闻业、执法和科学研究。
- en: The watermark detection feature promotes transparency and accountability in
    the use of image generation models. By making it easier to identify AI-generated
    content, it encourages responsible and ethical practices among content creators
    and stakeholders, fostering a more open dialogue around the use of this technology.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水印检测功能促进了图像生成模型使用中的透明度和问责制。通过使识别AI生成内容变得更加容易，它鼓励内容创作者和利益相关者采取负责任和道德的做法，从而促进围绕这项技术使用的更开放对话。
- en: To try out watermark detection, you can simply navigate to **Watermark detection**
    in the Amazon Bedrock console and upload an image. Amazon Bedrock then analyzes
    the image to detect watermarks embedded by Amazon Titan.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 要尝试水印检测，你只需在 Amazon Bedrock 控制台中导航到**水印检测**并上传一张图像。然后 Amazon Bedrock 会分析图像以检测
    Amazon Titan 嵌入的水印。
- en: In addition to detecting the watermark, you will also receive a confidence score,
    which determines the level of confidence (or certainty) with which the model is
    able to identify that the image was generated by Amazon Titan. Usually, you will
    see a high confidence score when there has been little to no modification in the
    image. However, if you make some modifications to the generated image, you might
    see a lower confidence score.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 除了检测水印外，你还将收到一个置信度分数，该分数决定了模型能够以多大程度（或确定性）识别图像是由 Amazon Titan 生成的。通常，当图像几乎没有修改时，你会看到一个高置信度分数。然而，如果你对生成的图像进行了一些修改，你可能会看到一个较低的置信度分数。
- en: '![Figure 9.19 – Watermark detection](img/B22045_09_19.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.19 – 水印检测](img/B22045_09_19.jpg)'
- en: Figure 9.19 – Watermark detection
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.19 – 水印检测
- en: As shown in *Figure 9**.19*, we have uploaded the image generated by Amazon
    Titan. The watermark detection feature is able to analyze and detect the watermark
    generated by Titan.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如**图 9.19**所示，我们已上传由 Amazon Titan 生成的图像。水印检测功能能够分析和检测 Titan 生成的水印。
- en: '![Figure 9.20 – The watermark is not detected](img/B22045_09_20.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.20 – 水印未检测到](img/B22045_09_20.jpg)'
- en: Figure 9.20 – The watermark is not detected
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.20 – 水印未检测到
- en: In *Figure 9**.20*, we have uploaded the image generated by Stable Diffusion.
    We can see that the watermark is not detected.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如**图 9.20**所示，我们已上传由 Stable Diffusion 生成的图像。我们可以看到水印未检测到。
- en: 'If you want to try using API, you can call `DetectGeneratedContent` to verify
    whether the watermark exists:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要尝试使用 API，你可以调用 `DetectGeneratedContent` 来验证水印是否存在：
- en: '[PRE0]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here is how the response should look:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是响应应该看起来像什么：
- en: '[PRE1]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here is the demo on watermark detection: [https://www.youtube.com/watch?v=M5Vqb3UoXtc](https://www.youtube.com/watch?v=M5Vqb3UoXtc).'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是水印检测的演示：[https://www.youtube.com/watch?v=M5Vqb3UoXtc](https://www.youtube.com/watch?v=M5Vqb3UoXtc)。
- en: While watermark detection is not a solution for all ethical concerns, it is
    one of the ways to move in the right direction. We will have a deeper discussion
    on ethical and responsible AI in [*Chapter 11*](B22045_11.xhtml#_idTextAnchor207)
    of this book. You should now be able to understand image generation and design
    patterns with Amazon Bedrock.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然水印检测不是解决所有道德问题的方案，但它是在正确方向上迈出的一步。我们将在本书的**第 11 章**中更深入地讨论道德和负责任的 AI。你现在应该能够理解使用
    Amazon Bedrock 的图像生成和设计模式。
- en: Summary
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored how image generation works. We also discussed the
    workings of multimodal models within Amazon Bedrock. We also covered several real-world
    applications and multimodal design patterns, including text-to-image, image search,
    image understanding, and image-to-image patterns such as inpainting and outpainting.
    We ended the chapter with a brief look at ethical considerations, as well as a
    look into the watermark detection capability within Amazon Bedrock. Throughout
    the chapter, we gained a deeper understanding of how we can leverage Amazon Bedrock’s
    multimodal models to build applications that can generate, understand, and manipulate
    images based on text and image prompts.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了图像生成的工作原理。我们还讨论了 Amazon Bedrock 内部多模态模型的工作方式。我们还涵盖了几个现实世界的应用和多模态设计模式，包括文本到图像、图像搜索、图像理解和图像到图像模式，如修复和扩展。我们以对道德考虑的简要概述以及对
    Amazon Bedrock 内部水印检测能力的探讨结束本章。在整个章节中，我们更深入地了解了如何利用 Amazon Bedrock 的多模态模型来构建基于文本和图像提示生成、理解和操作图像的应用程序。
- en: In the next chapter, we will explore the topic of building intelligent agents
    with Amazon Bedrock.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨使用 Amazon Bedrock 构建智能代理的主题。
