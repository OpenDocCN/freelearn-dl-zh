- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generating and Transforming Images Using Amazon Bedrock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, we have explored several LLMs capable of generating textual responses.
    This chapter explores generating images using select FMs that are available on
    Amazon Bedrock. We will start with an overview of image generation, wherein we
    will examine model architectures such as GANs and **variational autoencoders**
    (**VAEs**) Then, we will cover some real-world applications of image generation
    and multimodal models available within Amazon Bedrock. Furthermore, we will dive
    deeper into several multimodal design patterns, as well as some ethical considerations
    and safeguards that are available with Amazon Bedrock.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have gained an understanding of implementing
    image generation and its design patterns with Amazon Bedrock for real-world use
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the key topics that will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Image generation overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multimodal models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multimodal design patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ethical considerations and safeguards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter requires you to have access to an AWS account. If you don’t have
    one already, you can go to [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    and create one.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, you will need to install and configure AWS CLI at [https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)
    after you create an account, which will be needed to access Amazon Bedrock FMs
    from your local machine. Since the majority of the code cells that we will be
    executing are based in Python, setting up an AWS Python SDK (Boto3) at [https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html](https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html)
    would be beneficial at this point. You can carry out the Python setup in any way.
    Install it on your local machine, or use AWS Cloud9, or utilize AWS Lambda, or
    leverage Amazon SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: There will be a charge associated with the invocation and customization of the
    FMs of Amazon Bedrock. Please refer to [https://aws.amazon.com/bedrock/pricing/](https://aws.amazon.com/bedrock/pricing/)
    to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: Image generation overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image generation has been a fascinating and rapidly evolving field. Since the
    dawn of advanced deep learning techniques and increasing computational power,
    machines have gained the remarkable ability to create highly realistic and sophisticated
    images from scratch or based on textual prompts. This ability has opened up a
    vast array of applications across various domains, including the creative industries,
    media and entertainment, advertising, product packaging, and many others.
  prefs: []
  type: TYPE_NORMAL
- en: The history of image generation can be traced back to early developments in
    computer vision and pattern recognition. Researchers and scientists have long
    sought to understand and replicate the human visual perception system, paving
    the way for the initial techniques in image synthesis and manipulation. However,
    the true breakthrough in image generation came with the emergence of deep learning,
    specifically the introduction of GANs and VAEs.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that we are highlighting these techniques for historical reference.
    Current image generation FMs do not use these techniques.
  prefs: []
  type: TYPE_NORMAL
- en: What are GANs and VAEs?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GANs, introduced by Ian Goodfellow and his colleagues in 2014, revolutionized
    the field of image generation. You can read more about it on [https://arxiv.org/pdf/1406.2661](https://arxiv.org/pdf/1406.2661).
    GANs employ a unique training approach whereby two neural networks are pitted
    against each other in competition. The first network is known as the **generator**,
    which is tasked with generating synthetic samples that mimic real data. For example,
    the generator could produce new images, texts, or audio clips. The second network
    is called the **discriminator**. Its role is to analyze examples, both real and
    synthetic, to classify which ones are genuine and which have been artificially
    generated.
  prefs: []
  type: TYPE_NORMAL
- en: Through this adversarial process, the generator learns to produce increasingly
    convincing fakes that can fool the discriminator. Meanwhile, the discriminator
    evolves in its ability to detect subtle anomalies that reveal synthetic samples.
    Their competing goals drive both networks to continuously improve. A demonstration
    of GANs can be seen at [https://thispersondoesnotexist.com/](https://thispersondoesnotexist.com/).
    By refreshing the page endlessly, users are presented with an endless stream of
    novel human faces. However, none of those faces are real – all are synthetic portraits
    created solely by a GAN trained on vast databases of images of real human faces.
    The site offers a glimpse into how GANs can synthesize highly realistic outputs
    across many domains.
  prefs: []
  type: TYPE_NORMAL
- en: Since the inception of GANs, numerous advancements and variations have been
    implemented, leading to remarkable achievements in image generation. Techniques
    such as StyleGAN, BigGAN, and diffusion models have pushed the boundaries of image
    quality, resolution, and diversity. These models can generate photorealistic images
    of human faces, landscapes, objects, and even artistic creations, blurring the
    line between artificial and real.
  prefs: []
  type: TYPE_NORMAL
- en: 'VAEs, on the other hand, are a simpler means to train generative AI algorithms.
    They also utilize two neural networks: **encoders** and **decoders**. Encoders
    learn the patterns in the data by mapping it into lower-dimensional latent space;
    decoders use these patterns from the latent space and generate realistic samples.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most exciting developments in image generation has been the integration
    of NLP capabilities. Models such as DALL-E, Stable Diffusion, and Midjourney have
    empowered users to generate images simply by providing textual descriptions or
    prompts. This fusion of language and vision has opened up new avenues for creative
    expression, rapid prototyping, and data augmentation for various ML tasks.
  prefs: []
  type: TYPE_NORMAL
- en: While the advancements in image generation are remarkable, it is crucial to
    address the ethical considerations and potential risks associated with this technology.
    Issues such as deepfakes, biases, and misuse for malicious purposes must be carefully
    addressed to ensure the responsible and ethical deployment of these powerful tools.
    We will look at this topic in detail in the *Ethical considerations and safeguards*
    section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Let us look at some real-world applications for image generation models.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The applications of image generation are endless. Here are some of the real-world
    applications of image generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Advertising and marketing**: In the world of advertising and marketing, visuals
    play a crucial role in capturing attention and conveying messages effectively.
    With image generation, you can revolutionize marketing campaigns by producing
    unique, visually striking images tailored to specific target audiences. Marketers
    can leverage Bedrock models to generate personalized product advertisements, social
    media visuals, and eye-catching graphics that resonate with their desired demographics.
    Furthermore, marketers can create variations of images based on customer preferences,
    ensuring that marketing materials are highly relevant and engaging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graphic design and content creation**: Graphic designers and content creators
    often face the challenge of conceptualizing and visualizing ideas before executing
    them. With Bedrock’s image generation models, you can streamline this process
    by relying on this powerful tool for generating initial concepts, illustrations,
    and visual assets. Designers can use image generation models to explore different
    styles, compositions, and color schemes, facilitating quick iterations and experimentation.
    Additionally, content creators can leverage Bedrock models to generate unique
    and captivating images for blog posts, articles, or other marketing materials,
    enhancing their visual appeal and potential for engagement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Product visualization and prototyping**: Effective product visualization
    is essential for iterating designs, gathering feedback, and showcasing offerings.
    With Bedrock image generation models, businesses can generate realistic visualizations
    of product designs, allowing for rapid prototyping and evaluation before investing
    in physical prototypes. Bedrock models can create images of products in various
    environments or from different angles, providing stakeholders with a comprehensive
    understanding of the product’s appearance and functionality. This capability can
    significantly accelerate the product development cycle and aid in marketing and
    sales efforts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gaming and virtual environments**: The gaming and **Virtual Reality** (**VR**)
    industries heavily rely on visually immersive experiences. Bedrock’s image generation
    models can empower developers to create unique textures, environments, and assets
    for video games, VR, or **Augmented Reality** (**AR**) applications. Bedrock image
    models can generate custom avatars, character designs, and intricate visual elements
    based on user specifications or game narratives. In addition, developers can enhance
    the realism and diversity of their virtual worlds, offering players a more engaging
    and personalized experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Architecture and interior design**: Visualizing architectural designs and
    interior spaces is crucial for architects and interior designers, as well as their
    clients. Bedrock image models can generate realistic renderings of proposed designs,
    allowing stakeholders to immerse themselves in the envisioned spaces before construction
    or renovation. Bedrock’s capabilities can aid in visualizing different materials,
    furniture arrangements, and lighting conditions, enabling architects and designers
    to refine their concepts and present compelling proposals to clients or decision-makers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fashion and apparel**: In the fashion and apparel industry, Amazon Bedrock
    image models can generate unique textile designs, patterns, and clothing styles,
    enabling fashion designers to explore new concepts and stay ahead of trends. Additionally,
    Bedrock can create visualizations of clothing items on different body types or
    in various environments, allowing customers to preview how garments would look
    in real life before making a purchase. This capability can enhance the shopping
    experience and reduce return rates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific visualization**: Effective communication of scientific data, phenomena,
    and simulations is crucial in research and education. Amazon Bedrock’s image generation
    models can assist scientists and researchers in creating visual representations
    of complex concepts, making them more accessible and understandable. Bedrock models
    can generate illustrations, diagrams, or 3D models for scientific publications,
    presentations, or educational materials, facilitating knowledge transfer and fostering
    a deeper understanding of intricate topics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Art and creative expression**: Artists can leverage Bedrock image models
    to explore new styles, techniques, and concepts by generating unique and imaginative
    images based on textual prompts or conceptual frameworks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**E-commerce and product catalogs**: In the e-commerce landscape, high-quality
    product images are essential for attracting customers and driving sales. Amazon
    Bedrock image models can generate visually appealing and accurate product images
    for online catalogs or e-commerce platforms, reducing the need for extensive photoshoots
    and the associated costs. These models can also create visualizations of customized
    products or configurations based on customer preferences, enhancing the shopping
    experience and enabling personalization at scale.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have looked at some real-world applications, let us explore various
    multimodal models and their inner workings.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far in this book, we have looked at single-modal model architecture patterns,
    such as text-to-text generation, that includes QA, summarization, code generation,
    and so on. Let us now expand our understanding to another type of generative AI
    model: multimodal models.'
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal models are a type of model that can understand and interpret more
    than one modality, such as image, audio, and video, as shown in *Figure 9**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Multimodality](img/B22045_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Multimodality
  prefs: []
  type: TYPE_NORMAL
- en: The response received from these models can also be multimodal. Behind the scenes,
    these FMs comprise multiple single-modal neural networks that process text, image,
    audio, and video separately.
  prefs: []
  type: TYPE_NORMAL
- en: Now let us look at the multimodality models that are available within Amazon
    Bedrock.
  prefs: []
  type: TYPE_NORMAL
- en: Stable Diffusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Stable Diffusion** is a state-of-the-art image generation model that has
    gained significant attention in the field of generative AI. Unlike many other
    image generation models, Stable Diffusion employs a unique diffusion-based approach,
    which sets it apart from other methods or techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: At the heart of Stable Diffusion is the concept of **diffusion**, which involves
    a forward and a reverse diffusion process. In **forward diffusion**, Gaussian
    noise is progressively added to an image until it becomes entirely random. The
    model then learns to reverse this process, gradually removing the noise to reconstruct
    the original image. This reversal is called **reverse diffusion** and is the key
    to Stable Diffusion’s impressive performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'This diffusion process has several key components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Contrastive Language-Image Pre-Training** (**CLIP**): CLIP is a neural network
    trained on a vast dataset of image-text pairs, enabling it to understand the semantic
    relationships between visual and textual representations. This component plays
    a crucial role in bridging the gap between natural language prompts and their
    corresponding visual manifestations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**U-Net**: This serves as a backbone for the image generation process. U-Net
    is a convolutional neural network designed for image-to-image translation tasks
    such as segmentation and denoising. Segmentation is the process whereby the images
    are partitioned into multiple segments or sets of pixels to locate objects and
    boundaries. Denoising removes the noise from an image to improve its quality.
    In the context of Stable Diffusion, U-Net is responsible for generating and refining
    the output image based on the input prompt and guidance from CLIP.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VAE**: This is another critical component that helps ensure that the generated
    images are coherent and realistic. In Stable Diffusion, VAE encodes the generated
    image into a compressed representation, which is then decoded to produce the final
    output image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As shown in *Figure 9**.2*, here is a high-level overview of how the whole
    process works:'
  prefs: []
  type: TYPE_NORMAL
- en: The user provides a natural language prompt describing the desired image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The CLIP model analyzes the prompt and generates a corresponding embedding,
    representing the semantic meaning of the text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The U-Net architecture takes this embedding as input, along with an initial
    random noise image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Through a series of convolutional and deconvolutional operations, U-Net iteratively
    refines the noise image, guided by the CLIP embedding, to produce an image that
    matches the input prompt.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The generated image is then passed through the VAE, which encodes and decodes
    it, ensuring coherence and realism.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final output image is produced, reflecting the user’s prompt.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.2 –  The Stable Diffusion process](img/B22045_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – The Stable Diffusion process
  prefs: []
  type: TYPE_NORMAL
- en: 'By combining these architectural elements, Stable Diffusion is able to generate
    high-quality, diverse images that are both visually appealing and semantically
    coherent with the input prompts. In order to understand the detailed workings
    of the diffusion process, readers are encouraged to read the research paper *On
    the Design Fundamentals* *of Diffusion Models: A Survey*. It can be found on:
    [https://arxiv.org/pdf/2306.04542.pdf](https://arxiv.org/pdf/2306.04542.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: This paper explains how diffusion models work by gradually adding noise to training
    data and then learning to reverse that process to generate new samples. The paper
    highlights the wide range of applications for diffusion models, including image
    editing, text-to-image generation, and 3D object creation.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, readers are recommended to explore the *How Diffusion Models Work*
    course from DeepLearning.AI at [https://learn.deeplearning.ai/courses/diffusion-models/](https://learn.deeplearning.ai/courses/diffusion-models/).
  prefs: []
  type: TYPE_NORMAL
- en: Titan Image Generator G1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Titan Image Generator G1** is a proprietary image generation model by Amazon
    that allows users to generate images from text, edit existing images, and create
    variations of images. The model is designed to make it easy for users to iterate
    on image concepts by generating multiple image options based on text descriptions.
    The model is trained on diverse high-quality datasets, so it can understand complex
    prompts with multiple objects and generate realistic images.'
  prefs: []
  type: TYPE_NORMAL
- en: This model supports image editing capabilities such as editing with text prompts
    using a built-in segmentation model, generating variations of the image, inpainting
    with an image mask, and outpainting to extend or change the background of an image.
    You can upload an existing image and provide instructions or prompts to modify
    specific aspects of the image. The model can intelligently alter the composition,
    add or remove elements, change colors, or apply various artistic styles, all while
    preserving the overall coherence and realism of the original image.
  prefs: []
  type: TYPE_NORMAL
- en: We will dive deeper into these capabilities in the *Multimodal design* *patterns*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Titan Multimodal Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Titan Multimodal Embeddings** model is part of the Amazon Titan family
    of models designed for use cases such as image search and similarity-based recommendation
    with high accuracy and fast response.
  prefs: []
  type: TYPE_NORMAL
- en: The Titan Multimodal Embeddings model’s core strength lies in its ability to
    generate high-dimensional vector representations for both textual and visual data.
    These embeddings encapsulate the semantic relationships between different modalities,
    allowing for efficient and effective search and retrieval operations.
  prefs: []
  type: TYPE_NORMAL
- en: The model supports up to 128 tokens as input text in English, as well as image
    sizes of up to 25 MB, and converts those to vector embeddings. The default embedding
    size is 1024 dimensions, providing a rich representation that captures nuanced
    details and complex relationships. However, you can also configure smaller vector
    dimensions to optimize for speed and cost, depending on your specific use case
    and performance requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Anthropic Claude 3 – Sonnet, Haiku, and Opus
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Anthropic Claude 3 Model variants – *Claude 3 Sonnet*, *Claude 3 Haiku*, and
    *Claude 3 Opus* – are the most recent and advanced family of Anthropic Claude
    models available on Amazon Bedrock. All these models have multimodal capabilities,
    meaning that they are able to perceive and analyze images as well as text input,
    with a 200K context window. We encourage you to refer to the *Anthropic Claude*
    section in [*Chapter 1*](B22045_01.xhtml#_idTextAnchor014) if you would like to
    go over their details again.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have looked at the multimodal models available within Amazon Bedrock,
    let us explore some of the design patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal design patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With multimodal design patterns, we integrate different modalities, such as
    text, images, audio, and so on. With the multimodal models available, the ability
    to generate, manipulate, and understand images from text or other input modalities
    has become increasingly important in a wide range of applications, from creative
    design to scientific visualization and beyond.
  prefs: []
  type: TYPE_NORMAL
- en: Numerous patterns can be created with multimodal models. In this section, we
    are going to cover some of the common patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Text-to-image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With a text-to-image pattern, you provide the text as a prompt to the model.
    The model will then generate an image based on that prompt, as shown in *Figure
    9**.3.*
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – A text-to-image pattern](img/B22045_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – A text-to-image pattern
  prefs: []
  type: TYPE_NORMAL
- en: '**Parameters**'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the core of image generation models are a set of customizable inference
    parameters and controls that allow users to get the desired image from the model.
    Let us look at these parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Cloud` and `seating bench` to exclude them from the image, as shown in *Figure
    9**.4*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 9.4 – A text-to-image pattern with negative prompts](img/B22045_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – A text-to-image pattern with negative prompts
  prefs: []
  type: TYPE_NORMAL
- en: '**Reference image**: This provides users the ability to input a reference image
    to the model, which can be leveraged by the model as a baseline to generate its
    response (generated image). For instance, if we use the image generated from the
    preceding figure and pass it as a reference along with the prompt, the prompt
    would be something like this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`A futuristic cityscape at night, with towering skyscrapers made of glass and
    metal. The buildings are illuminated by neon lights in shades of blue, purple,
    and pink. The streets are lined with holographic billboards` `and advertisements.`'
  prefs: []
  type: TYPE_NORMAL
- en: The model will use the reference image and the prompt to generate a new image,
    as shown in *Figure 9**.5*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – A text-to-image pattern using a reference Image](img/B22045_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – A text-to-image pattern using a reference Image
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt Strength** **(****cfg_scale****)**: Prompt strength, also known as
    **Classifier-Free Guidance scale** (*cfg_scale*) determines the degree to which
    the generated image adheres to the provided text prompt. A higher value indicates
    that the image generation process will adhere more closely to the text prompt,
    while a lower value allows for more creative interpretation and diversity in the
    generated images. Using a cfg_scale value somewhere in the middle (*10-15*) is
    generally recommended, as it strikes a balance between faithfully representing
    the text prompt and allowing for artistic expression. However, the optimal value
    may vary depending on your use case or what you are looking for, complexity of
    the prompt, and the desired level of detail in the generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generation Step** **(****steps****)**: The *steps* parameter in Stable Diffusion
    refers to the number of iterations or cycles the algorithm goes through to generate
    an image from the input text. It’s an important setting that affects the quality
    and detail of the final image. Here is how it works:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process starts with random noise, and with each step, some of that noise
    is removed, gradually revealing the intended image.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: More steps generally lead to higher-quality images with more detail, but there’s
    a point of diminishing returns.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The ideal number of steps can vary depending on the complexity of the image
    you’re trying to generate and your personal preferences. However, going much higher
    may not significantly improve the image but will increase generation time.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For simple subjects or scenes, fewer steps (around *10-15*) may be sufficient.
    But for more complex or detailed images, you may want to increase the steps to
    *40-50* or even more, depending on how much detailed you are looking for.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What we discussed are just some of the parameters. The following figure highlights
    additional parameters for Stable Diffusion.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Stable Diffusion text-to-image parameters](img/B22045_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Stable Diffusion text-to-image parameters
  prefs: []
  type: TYPE_NORMAL
- en: For a more detailed description of these parameters, you can go through the
    Stable Diffusion documentation at [https://platform.stability.ai/docs/api-reference#tag/Image-to-Image](https://platform.stability.ai/docs/api-reference#tag/Image-to-Image).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using Amazon Titan Image Generator, here is the list of parameters
    that you can use: [https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-image.html](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-image.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Image search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image search has emerged as a powerful tool that enables users to explore and
    leverage vast collections of visual data. With FMs from Amazon Bedrock, you can
    perform image searches to understand and interpret visual content. You can identify
    and understand various elements within an image, such as objects, scenes, colors,
    textures, and even abstract concepts. To illustrate the power of image search,
    let’s consider a practical example.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you’re a fashion retailer with an extensive catalog of clothing items.
    With Bedrock, you can upload your product images and leverage the image search
    capabilities to enable customers to find visually similar items. For instance,
    a customer could upload a picture of a dress that they like and Bedrock would
    return a set of visually similar dresses from your catalog, facilitating a more
    engaging and personalized shopping experience.
  prefs: []
  type: TYPE_NORMAL
- en: One powerful approach to image search is based on multimodal embeddings, which
    allow for the representation of both text and images in a vector space. These
    vectors capture the visual features and semantic information of the images. The
    vectors, along with metadata such as image paths, are then stored in a searchable
    index vector database such as OpenSearch Serverless, FAISS, or Pinecone. This
    technique enables searching for images using text queries or finding similar images
    based on a given image (or a combination of text and image).
  prefs: []
  type: TYPE_NORMAL
- en: When a user initiates a search, their input (text, image, or both) is also converted
    into a vector representation using the same multimodal embedding model. The search
    vector is then compared against the vectors in the index and the most similar
    vectors are retrieved based on the vector similarity scores. This approach allows
    for flexible and intuitive image search, as users can search using natural language
    descriptions, upload example images, or combine text and images for more precise
    results. For example, you could search for `a red sports car on a city street`
    and the model would return relevant images from its data store that match both
    the visual and textual criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you might have recognized by now, this process is similar to the RAG process
    that we discussed in [*Chapter 5*](B22045_05.xhtml#_idTextAnchor090). The difference
    here is that the model is retrieving the images from its data store and is not
    generating new images. Here is a great example to try out multimodal embedding
    and searching: [https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/04_Image_and_Multimodal/bedrock-titan-multimodal-embeddings.ipynb](https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/04_Image_and_Multimodal/bedrock-titan-multimodal-embeddings.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Image search with multimodal embeddings has numerous real-world applications
    across various domains. In e-commerce platforms, it can be used to enhance product
    search and recommendation systems, allowing customers to find visually similar
    products or to search for items using natural language descriptions or example
    images. In the media and entertainment industry, it can assist in content organization,
    tag suggestion, and copyright infringement detection by identifying similar or
    duplicate images.
  prefs: []
  type: TYPE_NORMAL
- en: Image understanding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Anthropic Claude 3 models – Sonnet, Haiku, and Opus – introduce the image
    understanding capability, through which the model can analyze the image and provide
    you with a response based on what you are looking to know. For example, you can
    provide an image of a kitchen or a living room and ask the model to provide a
    detailed description of the image or write a fictional story based on the image.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Use the following prompt: `Provide a detailed description of` `this image`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Image understanding and a detailed description in the output](img/B22045_09_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Image understanding and a detailed description in the output
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 9**.7*, we have provided the image of a kitchen to the Anthropic
    Claude 3 model and asked it to provide a detailed description of the image. The
    model is able to provide minute details such as **the room features dark wood
    cabinetry, contrasted by light marble countertops**, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Use the following prompt: `Write a fictional story based on the` `image attached`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Image understanding with a fictional story](img/B22045_09_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – Image understanding with a fictional story
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 9**.8*, you can see that the model has generated a fictional story
    based on the image of a library provided to it.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Use the following prompt: `Provide the list of items/objects present in the
    image and explain` `each item`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Image understanding with object identification](img/B22045_09_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – Image understanding with object identification
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 9**.9*, the model is able to identify the items and objects in the
    image along with their details, showcasing the image classification/object recognition
    capability.
  prefs: []
  type: TYPE_NORMAL
- en: The Claude models’ image understanding capabilities are not limited to those
    discussed in the preceding examples. They can also be utilized for tasks such
    as image captioning, creating detailed image descriptions, identifying subjects,
    and answering questions about the contents of an image. You can look at various
    use cases of image understanding at [https://docs.anthropic.com/claude/docs/use-cases-and-capabilities#vision-capabilities](https://docs.anthropic.com/claude/docs/use-cases-and-capabilities#vision-capabilities).
  prefs: []
  type: TYPE_NORMAL
- en: To use this capability within the Amazon Bedrock console, yo[u can follow the
    ensuing steps:](https://console.aws.amazon.com/bedrock-)
  prefs: []
  type: TYPE_NORMAL
- en: '[Go t](https://console.aws.amazon.com/bedrock-)o Amazon Bedrock Console at
    https://console.aws.amazon.com/bedrock.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to **Chat Playground**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Select model**. Choose the **Anthropic Claude 3 Sonnet**, **Anthropic
    Claude 3 Haiku**, or **Anthropic Claude 3** **Opus** model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Attach the image you want to analyze and provide the prompt based on what you
    are looking for, as shown in *Figure 9**.10*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.10 – How to analyze an image using the Anthropic Claude 3 models](img/B22045_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 – How to analyze an image using the Anthropic Claude 3 models
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using AWS SDK, you can use Anthropic’s `Messages` API to create
    a chat application and provide an image for understanding. Here is some example
    AWS Python SDK code for a multimodal message to the Claude 3 Sonnet model: [https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html#api-inference-examples-claude-multimodal-code-example](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html#api-inference-examples-claude-multimodal-code-example).'
  prefs: []
  type: TYPE_NORMAL
- en: Image-to-image patterns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When it comes to image-to-image generation, the model takes an existing image
    as input and modifies it based on the prompt or instructions you provide. This
    is different from text-to-image generation, whereby the model creates an entirely
    new image from scratch based solely on a textual description or prompt. In image-to-image
    generation, on the other hand, the model uses the existing image as a starting
    point and then applies the necessary changes or transformations to produce the
    desired output image. This can involve adjusting various aspects such as colors,
    textures, objects, or even the overall composition of the image, all guided by
    the prompt. It’s like having a clay model and reshaping it to your desired outcome
    rather than starting from a lump of raw clay. The ability to modify and manipulate
    existing images opens up a range of creative possibilities and use cases, from
    enhancing and editing photographs to creating artistic interpretations or visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: A simple example of image-to-image generation is shown in *Figure 9**.11*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.11 – Simple image-to-image generation](img/B22045_09_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 – Simple image-to-image generation
  prefs: []
  type: TYPE_NORMAL
- en: When using the Stable Diffusion model for image-to-image generation, there are
    a few additional parameters to consider along with the text-to-text parameters
    mentioned in *Figure 9**.6*. These additional parameters are highlighted in *Figure
    9**.12.*
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.12 – Stable Diffusion image-to-image parameters](img/B22045_09_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 – Stable Diffusion image-to-image parameters
  prefs: []
  type: TYPE_NORMAL
- en: 'You can learn more about them here: [https://platform.stability.ai/docs/api-reference#tag/Image-to-Image/operation/imageToImage](https://platform.stability.ai/docs/api-reference#tag/Image-to-Image/operation/imageToImage).'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let us look at some image-to-image patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Image variation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Image variation**, also known as **image-to-image translation** or **style
    transfer**, is a powerful technique in generative AI that allows for the creation
    of new and unique images by modifying existing ones. This process involves taking
    an input image and applying a desired style or transformation to it, resulting
    in an output image that combines the content of the original with the desired
    aesthetic or visual characteristics.'
  prefs: []
  type: TYPE_NORMAL
- en: One real-world example of image variation is in the field of fashion design.
    Designers can take an existing garment or accessory and apply various styles,
    patterns, or textures to create new and innovative designs without starting from
    scratch. This not only saves time and resources but also allows for rapid experimentation
    and iteration, enabling designers to explore a vast range of possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: Another example can be found in the art world, where image variation techniques
    can be used to create unique and expressive artworks. Artists can take a simple
    photograph or painting and apply various artistic styles, such as impressionism,
    cubism, or abstract expressionism, to create entirely new pieces that blend the
    original content with the desired artistic style. This opens up new avenues for
    creative expression and allows artists to explore unconventional and thought-provoking
    visual interpretations.
  prefs: []
  type: TYPE_NORMAL
- en: Image variation also has applications in the fields of interior design and architectural
    visualization. Designers and architects can take existing spaces or structures
    and apply different materials, textures, or lighting conditions to visualize how
    a space might look with different design choices. This can be invaluable in helping
    clients understand and appreciate proposed designs, as well as in enabling designers
    to quickly iterate and refine their concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'With Bedrock, you can utilize Titan Image Generator to create image variations.
    Let us try the following prompt and run it through Titan Image Generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '`A delicate, nature-inspired pattern with intricate illustrations of birds,
    butterflies, and foliage, perfect for a romantic, bohemian dress` `or scarf.`'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.13 – Image variation](img/B22045_09_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.13 – Image variation
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 9**.13*, Titan Image Generator will create an image (**Original
    Image**). You can generate image variations that leverage the **Original Image**
    as a reference, along with an optional prompt that can be provided to the model.
  prefs: []
  type: TYPE_NORMAL
- en: Masking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Amazon Bedrock models – Amazon Titan Generator and Stable Diffusion – offer
    two powerful image editing techniques: *masking* and *painting*.'
  prefs: []
  type: TYPE_NORMAL
- en: With masking, we define specific areas within an image and mask them, either
    to be preserved or redrawn. This masking can be done either via an image file
    or a prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Image masking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The approach of **image masking** utilizes a separate image file, known as
    the **mask image**, to specify the pixels to be masked or preserved in the original
    image. The mask image must adhere to the following requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Identical dimensions and resolution as the original image**: When using image
    masking, it’s crucial that the mask image has the exact same dimensions and resolution
    as the original image that you want to mask. This ensures that each pixel in the
    mask image corresponds to a pixel in the original image, allowing for precise
    masking. If the dimensions or resolutions differ, the masking process may produce
    distorted or undesired results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, if your original image has a resolution of 1920 x 1080 pixels,
    the mask image must also have a resolution of 1920 x 1080 pixels. Any discrepancy
    in the dimensions or resolution will cause the mask to misalign with the original
    image, leading to undesirable masking effects.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**No alpha channel**: The mask image should not have an alpha channel, which
    is a separate component in some image formats that represents transparency. While
    the PNG format supports transparency through an alpha channel, for image masking
    purposes, the mask image should rely solely on color values (**Red, Green, Blue**
    (**RGB**) or grayscale) to represent masked and unmasked regions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The absence of an alpha channel simplifies the masking process and ensures that
    the masking is based solely on the pixel colors, without any additional transparency
    information. This approach is often preferred for its simplicity and compatibility
    with a wide range of image processing tools and libraries.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`0`, `0`, `0`) as masked areas, while any non-black pixels are considered unmasked
    regions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0` (black) to `255` (white). The masking process interprets pixels with a
    value of `0` as masked areas, while pixels with non-zero values are considered
    unmasked regions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice between RGB and grayscale color modes depends on your specific use
    case and on the tools or libraries you’re using for image masking. Some tools
    may have a preference for one color mode over the other.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s assume that you work in the Food and Beverages industry and
    you want to mask out certain food items from an image to create a transparent
    layer for a menu design. Suppose that you want to mask the bowl of chips in the
    image that follows and maybe remove it from your menu. *Figure 9**.14* shows the
    original image and the masked image, where the masking is done on the bowl of
    chips.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.14 – Image masking](img/B22045_09_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.14 – Image masking
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to experiment with image masking, you can use online photo editing
    tools or apps. There is also the **Python Image Library** (**PIL**), a very popular
    Python library that is worth checking out at: [https://pillow.readthedocs.io/en/stable/reference/Image.html](https://pillow.readthedocs.io/en/stable/reference/Image.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, we recommend that you experiment with the following GitHub examples
    from the Amazon Bedrock workshop that showcase image masking and painting: [https://github.com/aws-samples/amazon-bedrock-workshop/tree/main/04_Image_and_Multimodal](https://github.com/aws-samples/amazon-bedrock-workshop/tree/main/04_Image_and_Multimodal).'
  prefs: []
  type: TYPE_NORMAL
- en: Mask prompting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Mask prompting** involves masking images through the use of textual prompts.
    These textual prompts serve as a guide to the model to comprehend the desired
    masking area within an image.'
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of using mask prompting as opposed to image masking lies in the
    dynamic nature of mask prompting. You can effortlessly modify the masking by simply
    altering the textual prompt, allowing for rapid iteration and experimentation.
    This flexibility allows artists, designers, and content creators to explore a
    vast array of visual concepts and narratives without being constrained by traditional
    image editing tools.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, mask prompting can be seamlessly integrated into various workflows
    and applications, enabling seamless collaboration and enhancing productivity.
    For instance, in the field of visual storytelling, writers and directors can leverage
    this feature to conceptualize and refine their vision, while designers can quickly
    prototype and iterate on visual concepts before committing to a final design.
  prefs: []
  type: TYPE_NORMAL
- en: To ensure the integrity and originality of the content generated, Amazon Bedrock
    has implemented robust measures to safeguard against plagiarism and unethical
    practices. We will discuss ethical considerations and safeguards in the upcoming
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take the same example from the preceding figure. Instead of image masking,
    we want to apply a mask prompt. We’ll say that you want to remove the bowl of
    chips from the original image. With mask prompting, you can provide the `only
    bowl of chips` prompt and then further perform painting.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.15 – Mask prompting](img/B22045_09_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.15 – Mask prompting
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 9**.15*, we performed inpainting and changed the bowl of chips to
    a bowl of apple slices. Let us discuss painting in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Painting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Painting** is a technique whereby you can fill in the masked regions within
    an image or extend it using an image generation model. There are two methods of
    painting: inpainting and outpainting.'
  prefs: []
  type: TYPE_NORMAL
- en: Inpainting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With **inpainting**, you are essentially reconstructing or filling in missing,
    masked, or corrupted portions of the image. This technique is particularly useful
    in scenarios where an image has been damaged or obscured, or where it contains
    unwanted elements that need to be removed or replaced. By providing the image
    generation model with the surrounding context and effective prompts, it can intelligently
    generate and blend new content in the designated areas, seamlessly reconstructing
    the masked regions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us look at some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '`telephone line` and provide an empty prompt text (`""`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 9.16 – Inpainting removal](img/B22045_09_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.16 – Inpainting removal
  prefs: []
  type: TYPE_NORMAL
- en: '**Inpainting replacement**: Suppose that you want to replace any object or
    scene within the image. In that case, you can perform inpainting replacement.
    As shown in *Figure 9**.17*, you can specify within the prompt text what you want
    to replace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 9.17 – Inpainting replacement](img/B22045_09_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.17 – Inpainting replacement
  prefs: []
  type: TYPE_NORMAL
- en: Outpainting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Outpainting** is the process of extending the image beyond its original boundaries,
    or in other words painting outside the masked regions. Outpainting is useful in
    scenarios where the original image or artwork needs to be extended or augmented
    with additional elements, environments, or perspectives.'
  prefs: []
  type: TYPE_NORMAL
- en: Let us look at an example.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.18 – Outpainting](img/B22045_09_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.18 – Outpainting
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 9**.18*, you can see that we are painting outside the masked image
    or prompt (in this case, `Indian curry`) to add some details. These include the
    background of a wooden table, as well as adding a spoon, a knife, a side of rice,
    and a chai.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to understand the prompt engineering best practices for the Titan
    Image Generator model, please check out [https://tinyurl.com/titan-image-generator-prompt](https://tinyurl.com/titan-image-generator-prompt).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have looked at different patterns of multimodal and image patterns,
    let us look at the ethical considerations and available safeguards within Amazon
    Bedrock to ensure the responsible use of Generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical considerations and safeguards
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative AI models, particularly those capable of generating highly realistic
    images, raise significant ethical concerns regarding the potential spread of misinformation
    and deepfakes. As these models are becoming increasingly powerful and accessible,
    it is crucial to address these ethical challenges proactively to promote the responsible
    development and deployment of this technology.
  prefs: []
  type: TYPE_NORMAL
- en: One of the primary ethical concerns surrounding image generation models is the
    risk of creating and disseminating misleading or manipulated content. With the
    ability to generate photorealistic images from text prompts comes the potential
    for malicious actors to create and spread false or fabricated visual information.
    This can have far-reaching consequences, such as undermining trust in media, spreading
    disinformation, and even influencing political or social narratives.
  prefs: []
  type: TYPE_NORMAL
- en: To address this major ethical challenge, it is crucial for organizations and
    researchers to prioritize responsible development and deployment of a generative
    AI life cycle. When using Amazon Bedrock, users can utilize its **watermark detection**
    feature for images generated by Amazon Titan Image Generator.
  prefs: []
  type: TYPE_NORMAL
- en: The watermark detection capability in Amazon Bedrock is designed to promote
    transparency and accountability in the use of AI-generated images. By embedding
    an invisible watermark in every image created by the model, content creators,
    news organizations, risk analysts, and others can quickly verify whether an image
    has been generated using Amazon Titan Image Generator.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach serves two primary ethical purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: It helps combat the spread of misinformation and deepfakes by providing a mechanism
    to verify the authenticity of images. This can help build trust and credibility
    in visual content, particularly in domains where the integrity of information
    is critical, such as journalism, law enforcement, and scientific research.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The watermark detection feature promotes transparency and accountability in
    the use of image generation models. By making it easier to identify AI-generated
    content, it encourages responsible and ethical practices among content creators
    and stakeholders, fostering a more open dialogue around the use of this technology.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To try out watermark detection, you can simply navigate to **Watermark detection**
    in the Amazon Bedrock console and upload an image. Amazon Bedrock then analyzes
    the image to detect watermarks embedded by Amazon Titan.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to detecting the watermark, you will also receive a confidence score,
    which determines the level of confidence (or certainty) with which the model is
    able to identify that the image was generated by Amazon Titan. Usually, you will
    see a high confidence score when there has been little to no modification in the
    image. However, if you make some modifications to the generated image, you might
    see a lower confidence score.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.19 – Watermark detection](img/B22045_09_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.19 – Watermark detection
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 9**.19*, we have uploaded the image generated by Amazon
    Titan. The watermark detection feature is able to analyze and detect the watermark
    generated by Titan.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.20 – The watermark is not detected](img/B22045_09_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.20 – The watermark is not detected
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 9**.20*, we have uploaded the image generated by Stable Diffusion.
    We can see that the watermark is not detected.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to try using API, you can call `DetectGeneratedContent` to verify
    whether the watermark exists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is how the response should look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the demo on watermark detection: [https://www.youtube.com/watch?v=M5Vqb3UoXtc](https://www.youtube.com/watch?v=M5Vqb3UoXtc).'
  prefs: []
  type: TYPE_NORMAL
- en: While watermark detection is not a solution for all ethical concerns, it is
    one of the ways to move in the right direction. We will have a deeper discussion
    on ethical and responsible AI in [*Chapter 11*](B22045_11.xhtml#_idTextAnchor207)
    of this book. You should now be able to understand image generation and design
    patterns with Amazon Bedrock.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored how image generation works. We also discussed the
    workings of multimodal models within Amazon Bedrock. We also covered several real-world
    applications and multimodal design patterns, including text-to-image, image search,
    image understanding, and image-to-image patterns such as inpainting and outpainting.
    We ended the chapter with a brief look at ethical considerations, as well as a
    look into the watermark detection capability within Amazon Bedrock. Throughout
    the chapter, we gained a deeper understanding of how we can leverage Amazon Bedrock’s
    multimodal models to build applications that can generate, understand, and manipulate
    images based on text and image prompts.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore the topic of building intelligent agents
    with Amazon Bedrock.
  prefs: []
  type: TYPE_NORMAL
