["```py\n    $ python3 -m venv venv\n    $ source venv/bin/activate\n    ```", "```py\n    pip3 install langchain==0.2.14 langchain-community==0.2.12 langchain-core==0.2.33 langchain-mongodb==0.1.8 langchain-openai==0.1.22 langchain-text-splitters==0.2.2 numpy==1.26.4 openai==1.41.1 s3fs==2024.6.1 pymongo==4.8.0 pandas==2.2.2 boto3==1.35.2 python-dotenv==1.0.1\n    ```", "```py\n    import os\n    import getpass\n    # set openai api key\n    try:\n        openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n    except KeyError:\n        openai_api_key = getpass.getpass(\"Please enter your OPENAI API KEY (hit enter): \")\n    # Set MongoDB Atlas connection string\n    try:\n        MONGO_CONN_STR = os.environ[\"MONGODB_CONNECTION_STR\"]\n    except KeyError:\n        MONGO_CONN = getpass.getpass(\"Please enter your MongoDB Atlas Connection String (hit enter): \")\n    ```", "```py\n    import pandas as pd\n    import s3fs\n    df = pd.read_json(\"https://ashwin-partner-bucket.s3.eu-west-1.amazonaws.com/movies_sample_dataset.jsonl\", orient=\"records\", lines=True)\n    df.to_json(\"./movies_sample_dataset.jsonl\", orient=\"records\", lines=True)\n    df[:3]\n    ```", "```py\n    import numpy as np\n    from tqdm import tqdm\n    import openai\n    df['final'] = df['text'] + \"    Overview: \" + df['overview']\n    df['final'][:5]\n    step = int(np.ceil(df['final'].shape[0]/100))\n    embeddings_t = []\n    lines = []\n    # Note that we must split the dataset into smaller batches to not exceed the rate limits imposed by OpenAI API's.\n    for x, y in list(map(lambda x: (x, x+step), list(range(0, df.shape[0], step)))):\n        lines += [df.final.values[x:y].tolist()]\n    for i in tqdm(lines):\n        embeddings_t += openai.embeddings.create(\n            model='text-embedding-ada-002', input=i).data\n    out = []\n    for ele in embeddings_t:\n        out += [ele.embedding]\n    df['embedding'] = out\n    df[:5]\n    ```", "```py\n    from pymongo import MongoClient\n    import osmongo_client = MongoClient(os.environ[\"MONGODB_CONNECTION_STR\"])\n    # Upload documents along with vector embeddings to MongoDB Atlas Collection\n    output_collection = mongo_client[\"sample_movies\"][\"embed_movies\"]\n    if output_collection.count_documents({})>0:\n        output_collection.delete_many({})\n    _ = output_collection.insert_many(df.to_dict(\"records\"))\n    ```", "```py\n    {\n        \"fields\": [\n          {\n            \"type\": \"vector\",\n            \"numDimensions\": 1536,\n            \"path\": \"embedding\",\n            \"similarity\": \"cosine\"\n          },\n          {\n            \"type\": \"filter\",\n            \"path\": \"year\"\n          },\n        ]\n    }\n    ```", "```py\n    def query_vector_search(q, prefilter = {}, postfilter = {},path=\"embedding\",topK=2):\n        ele = openai.embeddings.create(model='text-embedding-ada-002', input=q).data\n        query_embedding = ele[0].embedding\n        vs_query = {\n                    \"index\": \"default\",\n                    \"path\": path,\n                    \"queryVector\": query_embedding,\n                    \"numCandidates\": 10,\n                    \"limit\": topK,\n                }\n        if len(prefilter)>0:\n            vs_query[\"filter\"] = prefilter\n        new_search_query = {\"$vectorSearch\": vs_query}\n        project = {\"$project\": {\"score\": {\"$meta\": \"vectorSearchScore\"},\"_id\": 0,\"title\": 1, \"release_date\": 1, \"overview\": 1,\"year\": 1}}\n        if len(postfilter.keys())>0:\n            postFilter = {\"$match\":postfilter}\n            res = list(output_collection.aggregate([new_search_query, project, postFilter]))\n        else:\n            res = list(output_collection.aggregate([new_search_query, project]))\n        return res\n    query_vector_search(\"I like Christmas movies, any recommendations for movies release after 1990?\", prefilter={\"year\": {\"$gt\": 1990}}, topK=5)\n    ```", "```py\nquery_vector_search(\"I like Christmas movies, any recommendations for movies release after 1990?\", prefilter={\"year\":{\"$gt\": 1990}}, postfilter= {\"score\": {\"$gt\":0.905}},topK=5)\n```", "```py\nfrom langchain_mongodb.vectorstores import MongoDBAtlasVectorSearch\nfrom langchain_openai import OpenAIEmbeddings\nimport json\nembedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\nvector_search = MongoDBAtlasVectorSearch(output_collection, embedding_model, text_key='final')\nfquery = {\"year\": {\"$gt\": 1990}}\nsearch_kwargs = {\n    \"k\": 5,\n    'filter': fquery,\n}\nretriever = vector_search.as_retriever(search_kwargs=search_kwargs)\ndocs = retriever.invoke(\"I like Christmas movies, any recommendations for movies release after 1990?\")\nfor doc in docs:\n    foo = {}\n    foo['title'] = doc.metadata['title']\n    foo['year'] = doc.metadata['year']\n    foo['final'] = doc.metadata['text']\n    print(json.dumps(foo,indent=1))\n```", "```py\nfrom openai import OpenAI\nclient = OpenAI()\ndef invoke_llm(prompt, model_name='gpt-3.5-turbo-0125'):\n    \"\"\"\n    Queries with input prompt to OpenAI API using the chat completion api gets the model's response.\n    \"\"\"\n    response = client.chat.completions.create(\n      model=model_name,\n      messages=[\n        {\n          «role»: «user»,\n          «content»: prompt\n        }\n      ],\n      temperature=0.2,\n      max_tokens=256,\n      top_p=1,\n      frequency_penalty=0,\n      presence_penalty=0\n    )\n    chatbot_response = response.choices[0].message.content.strip()\n    return chatbot_response\ninvoke_llm(\"This is a test\")\n```", "```py\n'Great! What do you need help with?'\n```", "```py\ndef get_prompt(question, context):\n    prompt = f\"\"\"Question: {question}\n            System: Let's think step by step.\n            Context: {context}\n            \"\"\"\n    return prompt\ndef get_recommendation_prompt(query, context):\n    prompt = f\"\"\"\n        From the given movie listing data, choose a few great movie recommendations.\n        User query: {query}\n        Context: {context}\n        Movie Recommendations:\n        1\\. Movie_name: Movie_overview\n        \"\"\"\n    return prompt\n```", "```py\nprint(invoke_llm(\"In which movie does a footballer go completely blind?\"))\n```", "```py\nThe Game of Their Lives\" (2005), where the character Davey Herold, a footballer, goes completely blind after being hit in the head during a game\n```", "```py\nidea = \"In which movie does a footballer go completely blind?\"\nsearch_response = query_vector_search(idea, prefilter={\"year\":{\"$gt\": 1990}}, postfilter={\"score\": {\"$gt\":0.8}},topK=10)\npremise = \"\\n\".join(list(map(lambda x:x['final'], search_response)))\nprint(invoke_llm(get_prompt(idea, premise)))\n```", "```py\nThe movie in which a footballer goes completely blind is \"23 Blast.\"\n```", "```py\nquestion = \"I like Christmas movies, any recommendations for movies release after 1990?\"\nsearch_response = query_vector_search(question,topK=10)\ncontext = \"\\n\".join(list(map(lambda x:x['final'], search_response)))\nprint(invoke_llm(get_recommendation_prompt(\"I like Christmas movies, any recommendations for movies release after 1990?\", context)))\n```", "```py\nimport pandas as pd\nimport s3fs\nimport os\nimport boto3\ns3_uri= \"https://ashwin-partner-bucket.s3.eu-west-1.amazonaws.com/fashion_dataset.jsonl\"\ndf = pd.read_json(s3_uri, orient=\"records\", lines=True)\nprint(df[:3])\nfrom pymongo import MongoClient\nmongo_client = MongoClient(os.environ[\"MONGODB_CONNECTION_STR\"])\n# Upload documents along with vector embeddings to MongoDB Atlas Collection\ncol = mongo_client[\"search\"][\"catalog_final_myn\"]\ncol.insert_many(df.to_dict(orient=\"records\"))\n```", "```py\n{\n    \"fields\": [\n      {\n        \"type\": \"vector\",\n        \"numDimensions\": 1536,\n        \"path\": \"openAIVec\",\n        \"similarity\": \"cosine\"\n      }\n    ]\n}\n```", "```py\nfrom langchain_core.output_parsers import JsonOutputParser # type: ignore\nfrom langchain_core.prompts import PromptTemplate # type: ignore\nfrom langchain_core.pydantic_v1 import BaseModel, Field # type: ignore\nfrom langchain_openai import ChatOpenAI # type: ignore\nfrom langchain_community.embeddings import OpenAIEmbeddings # type: ignore\nfrom langchain_mongodb.vectorstores import MongoDBAtlasVectorSearch # type: ignore\nfrom pymongo import MongoClient # type: ignore\nfrom typing import List\nfrom itertools import chain\nimport certifi # type: ignore\nimport os\nfrom dotenv import load_dotenv # type: ignore\nload_dotenv()\nfrom functools import lru_cache\n@lru_cache\ndef get_openai_emb_transformers():\n    \"\"\"\n    Returns an instance of OpenAIEmbeddings for OpenAI transformer models.\n    This function creates and returns an instance of the OpenAIEmbeddings class,\n    which provides access to OpenAI transformer models for natural language processing.\n    The instance is cached using the lru_cache decorator for efficient reuse.\n    Returns:\n        embeddings (OpenAIEmbeddings): An instance of the OpenAIEmbeddings class.\n    \"\"\"\n    embeddings = OpenAIEmbeddings()\n    return embeddings\n@lru_cache\ndef get_vector_store():\n    \"\"\"\n    Retrieves the vector store for MongoDB Atlas.\n    Returns:\n        MongoDBAtlasVectorSearch: The vector store object.\n    \"\"\"\n    vs = MongoDBAtlasVectorSearch(collection=col, embedding=get_openai_emb_transformers(), index_name=\"vector_index_openAi_cosine\", embedding_key=\"openAIVec\", text_key=\"title\")\n    return vs\n@lru_cache(10)\ndef get_conversation_chain_conv():\n    \"\"\"\n    Retrieves a conversation chain model for chat conversations.\n    Returns:\n        ChatOpenAI: The conversation chain model for chat conversations.\n    \"\"\"\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2, max_tokens=2048)\n    # chain = ConversationChain(llm=llm, memory=ConversationBufferWindowMemory(k=5))\n    return llm\n# Define your desired data structure.\nclass ProductRecoStatus(BaseModel):\n    \"\"\"\n    Represents the status of product recommendations.\n    Attributes:\n        relevancy_status (bool): Product recommendation status conditioned on the context of the input query.\n                                 True if the query is related to purchasing fashion clothing and/or accessories.\n                                 False otherwise.\n        recommendations (List[str]): List of recommended product titles based on the input query context and\n                                     if the relevancy_status is True.\n    \"\"\"\n    relevancy_status: bool = Field(description=\"Product recommendation status is conditioned on the fact if the context of input query is to purchase a fashion clothing and or fashion accessories.\")\n    recommendations: List[str] = Field(description=\"list of recommended product titles based on the input query context and if recommendation_status is true.\")\nclass Product(BaseModel):\n    \"\"\"\n    Represents a product.\n    Attributes:\n        title (str): Title of the product.\n        baseColour (List[str]): List of base colours of the product.\n        gender (List[str]): List of genders the product is targeted for.\n        articleType (str): Type of the article.\n        mfg_brand_name (str): Manufacturer or brand name of the product.\n    \"\"\"\n    title: str = Field(description=\"Title of the product.\")\n    baseColour: List[str] = Field(description=\"List of base colours of the product.\")\n    gender: List[str] = Field(description=\"List of genders the product is targeted for.\")\n    articleType: str = Field(description=\"Type of the article.\")    mfg_brand_name: str = Field(description=\"Manufacturer or brand name of the product.\")\nclass Recommendations(BaseModel):\n    \"\"\"\n    Represents a set of recommendations for products and a message to the user.\n    Attributes:\n        products (List[Product]): List of recommended products.\n        message (str): Message to the user and context of the chat history summary.\n    \"\"\"\n    products: List[Product] = Field(description=\"List of recommended products.\")\n    message: str = Field(description=\"Message to the user and context of the chat history summary.\")\nreco_status_parser = JsonOutputParser(pydantic_object=ProductRecoStatus)\nreco_status_prompt = PromptTemplate(    template=\"You are AI assistant tasked at identifying if there is a product purchase intent in the query and providing suitable fashion recommendations.\\n{format_instructions}\\n{query}\\n\\\n        #Chat History Summary: {chat_history}\\n\\nBased on the context of the query, please provide the relevancy status and list of recommended products.\",\n    input_variables=[\"query\", \"chat_history\"],\n    partial_variables={\"format_instructions\": reco_status_parser.get_format_instructions()},\n)\nreco_parser = JsonOutputParser(pydantic_object=Recommendations)\nreco_prompt = PromptTemplate(\n    input_variables=[\"question\", \"recommendations\", \"chat_history\"],\n    partial_variables={\"format_instructions\": reco_parser.get_format_instructions()},\n    template=\"\\n User query:{question} \\n Chat Summary: {chat_history} \\n Rank and suggest me suitable products for creating grouped product recommendations given all product recommendations below feature atleast one product for each articleType \\n {recommendations} \\n show output in {format_instructions} for top 10 products\"\n)\ndef get_product_reco_status(query: str, chat_history: List[str] = []):\n    \"\"\"\n    Retrieves the recommendation status for a product based on the given query and chat history.\n    Args:\n        query (str): The query to be used for retrieving the recommendation status.\n        chat_history (List[str]): The chat history containing previous conversations.\n    Returns:\n        The response containing the recommendation status.\n    \"\"\"\n    llm = get_conversation_chain_conv()\n    chain = reco_status_prompt | llm | reco_status_parser\n    resp = chain.invoke({\"query\": query, \"chat_history\": chat_history})\n    return resp\ndef get_sorted_results(product_recommendations):\n    all_titles = [rec['title'] for rec in product_recommendations['products']]\n    results = list(col.find({\"title\": {\"$in\":all_titles}}, {\"_id\": 0 , \"id\":1, \"title\": 1, \"price\": 1, \"baseColour\": 1, \"articleType\": 1, \"gender\": 1, \"link\" : 1, \"mfg_brand_name\": 1}))\n    sorted_results = []\n    for title in all_titles:\n        for result in results:\n            if result['title'] == title:\n                sorted_results.append(result)\n                break\n    return sorted_results\ndef get_product_recommendations(query: str, reco_queries: List[str], chat_history: List[str]=[]):\n    \"\"\"\n    Retrieves product recommendations based on the given query and chat history.\n    Args:\n        query (str): The query string for the recommendation.\n        chat_history (List[str]): The list of previous chat messages.\n        filter_query (dict): The filter query to apply during the recommendation retrieval.\n        reco_queries (List[str]): The list of recommendation queries.\n    Returns:\n        dict: The response containing the recommendations.\n    \"\"\"\n    vectorstore = get_vector_store()\n    retr = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n    all_recommendations = list(chain(*retr.batch(reco_queries)))\n    llm = get_conversation_chain_conv()\n    llm_chain = reco_prompt | llm | reco_parser\n    resp = llm_chain.invoke({\"question\": query, \"chat_history\": chat_history, \"recommendations\": [v.page_content for v in all_recommendations]})\n    resp = get_sorted_results(resp)\n    return resp\n```", "```py\nquery = \"Can you suggest me some casual dresses for date occasion with my boyfriend\"\nstatus = get_product_reco_status(query)\nprint(status)\nprint(get_product_recommendations(query, reco_queries=status[\"recommendations\"], chat_history=[])\n```", "```py\n{'relevancy_status': True,\n 'recommendations': ['Floral Print Wrap Dress',\n  'Off-Shoulder Ruffle Dress',\n  'Lace Fit and Flare Dress',\n  'Midi Slip Dress',\n  'Denim Shirt Dress']}\n```", "```py\nquery = \"Where should I take my boy friend for date\"\nstatus = get_product_reco_status(query)\nprint(status)\nprint(get_conversation_chain_conv().invoke(query).content)\n```", "```py\n{'relevancy_status': False, 'recommendations': []}\n```"]