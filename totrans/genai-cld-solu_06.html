<html><head></head><body>
		<div><h1 id="_idParaDest-117" class="chapter-number"><a id="_idTextAnchor117"/>6</h1>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor118"/>Developing and Operationalizing LLM-based Apps: Exploring Dev Frameworks and LLMOps</h1>
			<p>Have you heard about GitHub Copilot? Claude by Anthropic? Jasper?</p>
			<p>If not, these solutions are all applications that have integrated generative AI. That is, they have taken the next step in our AI journey by using LLMs to create more engaging and meaningful interactions with users and other applications. These are just a few examples, with many, many more generative AI-infused applications coming to the market every day!</p>
			<p>As you have content already learned from the start of this book, generative AI is a branch of AI that focuses on creating new or enhancing content using existing data. Of course, generative AI can produce text, images, audio, video, or any other type of data that can be represented digitally, and you know that there are countless <a id="_idIndexMarker541"/>generative AI <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>) already available, with new ones being added each day. Some models are very specific to certain tasks, such as DALL-E, which simply takes your text prompt input and generates an actual image based on that prompt input.</p>
			<p>However, for almost all companies, universities, government entities, or organizations of any size, their business requirements and technical requirements are beyond just a simple text input to then generate an image or use a simple playground to cut and paste some prompts to see their completions.</p>
			<p>This chapter is mainly focused on how the development and operationalization of a generative AI application may contain many new concepts and techniques, especially for those not in software development. We will first cover some of the concepts, such as copilots and agents. Then, we will discuss how to convert these concepts into tactical solutions using popular application programming frameworks related to generative AI, such<a id="_idIndexMarker542"/> as <strong class="bold">Semantic Kernel</strong> (<strong class="bold">SK</strong>), <strong class="bold">LangChain</strong>, and <strong class="bold">LlamaIndex</strong>. These<a id="_idIndexMarker543"/> programming<a id="_idIndexMarker544"/> frameworks enable additional LLM tooling using agents and workflows, allowing developers to build generative AI-aware, intelligent applications and services in a much simpler yet much more powerful way. We will then cover a very exciting topic that we think will take AI to the next level, which is <a id="_idIndexMarker545"/>agent <a id="_idIndexMarker546"/>collaboration frameworks that help you<a id="_idIndexMarker547"/> build <strong class="bold">autonomous agents</strong>, such<a id="_idIndexMarker548"/> as <strong class="bold">Autogen</strong>, <strong class="bold">Taskweaver</strong>, and <strong class="bold">AutoGPT</strong>.</p>
			<p>The final section will focus on operationalizing generative AI applications in production. We will outline a systematic approach to harness the extensive capabilities of generative AI, which fulfills the complex <a id="_idIndexMarker549"/>requirements of organizations, utilizing a process known as <strong class="bold">large language model operations</strong> (<strong class="bold">LLMOps</strong>). Understanding the necessity of adopting LLMOps is crucial; it’s a key element for streamlined operations and a pathway to successfully developing generative AI-aware applications. This section will reiterate the systematic method to leverage generative AI’s broad capabilities and meet organizational needs, highlighting the importance of LLMOps for efficient operations and the development of successful applications.</p>
			<p>We will cover the following main topics in this book:</p>
			<ul>
				<li>Copilots and agents</li>
				<li>Generative AI application development frameworks</li>
				<li>Autonomous agents</li>
				<li>Agent collaboration frameworks</li>
				<li>LLM LLMOps – Operationalizing LLM apps in production</li>
				<li>LLMOps – Case study and best practices</li>
			</ul>
			<div><div><img src="img/B21443_06_1.jpg" alt="Figure 6.1 – Relationships in an autonomous world"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Relationships in an autonomous world</p>
			<p>Before we dive into the modern AI application development frameworks, we need to understand two concepts that haven’t been touched on in the previous chapters: agents and copilots.</p>
			<h1 id="_idParaDest-119"><a id="_idTextAnchor119"/>Copilots and agents</h1>
			<p>Traditional chatbots have undergone significant evolution, transitioning into more sophisticated forms<a id="_idIndexMarker550"/> such <a id="_idIndexMarker551"/>as copilots, agents, and <a id="_idIndexMarker552"/>autonomous agents. In this section, we aim to compare and contrast these advanced chatbot types, exploring their roles and utilization in contemporary applications.</p>
			<div><div><img src="img/B21443_06_02.jpg" alt="Figure 6.2 – Evolution from chatbots to autonomous agents"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Evolution from chatbots to autonomous agents</p>
			<p><strong class="bold">Agents</strong> are<a id="_idIndexMarker553"/> skilled assistants and, in the context described, are pieces of code equipped with AI capabilities. They are designed to complete tasks by interacting with users through applications or other interfaces. Initially, they gather information from users and subsequently utilize this data to <strong class="bold">execute actions</strong>, which may include feeding it into LLMs or a sequence of LLMs, among other possibilities.</p>
			<p>For example, a data analyst agent can analyze your Excel sheets by asking for your raw Excel file and any other questions it may have; then, it will generate its own plan of action intelligently, execute those actions, and provide you the final insights on your data.</p>
			<p><strong class="bold">Copilots</strong> are<a id="_idIndexMarker554"/> collaboration tools in the form of chatbots integrated into applications, and they use LLM to assist users to perform a task specific to that application and get an instant productivity boost. They represent a specialized subset within the broader category of agents.</p>
			<p>Copilots, such as GitHub Copilot<a id="_idIndexMarker555"/> and Power BI Copilot, are integrated into applications to assist users in completing<a id="_idIndexMarker556"/> tasks, such as generating code or offering troubleshooting recommendations based on natural language queries.</p>
			<p>Microsoft employs copilots extensively, integrating them into their next-generation AI-integrated products, such as Microsoft 365 apps. These copilots combine LLMs with user data and other Microsoft applications using the semantic kernel (SK) framework that we discuss in the next section. Copilots<a id="_idIndexMarker557"/> work alongside users, providing AI-powered assistance in tasks such as drafting documents or generating code. Imagine them as helpful copilots in the cockpit of a plane, assisting the pilot. By using a semantic kernel framework, developers can access the same AI integration and orchestration patterns used by Microsoft’s copilots in their own applications. For more information on how Microsoft utilizes AI models and SK in Copilots, refer to Kevin Scott’s Microsoft Build 2023 talk, <em class="italic">The Era of the </em><em class="italic">AI Copilot</em>.</p>
			<p>Now, let’s understand how to convert these concepts (agents, copilots, RAG (this was discussed in <a href="B21443_04.xhtml#_idTextAnchor070"><em class="italic">Chapter 4</em></a>)) into tactical solutions using frameworks such as Semantic Kernel, Langchain, and Llamaindex.</p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor120"/>Generative AI application development frameworks</h1>
			<p>In this section, we will focus on the popular generative AI-based app development frameworks used by developers today in their applications, as they add functionality and extensibility to LLMs.</p>
			<p>But why do we need to modernize existing ML applications to use intelligent generative AI in the first place? Let’s compare and contrast application characteristics without generative AI and the modernized applications infused with generative AI.</p>
			<p>Current ML applications have<a id="_idIndexMarker558"/> some of these common characteristic limitations:</p>
			<ul>
				<li>They are <strong class="bold">constrained with their interactions</strong>, especially with generative AI services.</li>
				<li>They are <strong class="bold">hard-coded</strong> and usually have a <strong class="bold">fixed dataset</strong>. For example, one can leverage certain datasets to train certain ML models, and those models are fixed.</li>
				<li>If they then want to change a model within an application or if they want to <strong class="bold">change the dataset entirely</strong>, they will need to again retrain the model, which is a challenge because of increased costs and increased time to completion.</li>
				<li>Retraining the model involves adding <strong class="bold">enhancements or features</strong>, which is <strong class="bold">quite complex</strong> and<a id="_idIndexMarker559"/> also <strong class="bold">time-consuming</strong> and <strong class="bold">costly</strong>.</li>
			</ul>
			<p>However, with intelligent generative AI applications that use the techniques described in this chapter, you can do the following:</p>
			<ul>
				<li><strong class="bold">Use natural language interactions</strong>. We have seen this in ChatGPT and other applications, where <a id="_idIndexMarker560"/>one can begin chatting as if there is an actual human or assistant. In addition to just using natural language to interact with generative AI applications, you can easily have your own personalized experiences based on human-like characteristics, such as personas and emotional tones, within an interactive session.</li>
				<li><strong class="bold">Generate data-driven</strong> and <strong class="bold">personalized experiences</strong> tailored to a user or set of users. Additionally, these applications can improve over time, autonomously using past experiences.</li>
				<li>Instead of a longer, time-consuming process of traditional software development, you can <strong class="bold">quickly deliver new features and </strong><strong class="bold">product enhancements</strong>.</li>
			</ul>
			<p>As you can see, intelligent <a id="_idIndexMarker561"/>generative AI applications are enabling us to create solutions and address problems never before and at a pace we have also never seen before. Now let’s turn our attention to some modern App Dev frameworks that can help us implement the new and sophisticated features.</p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor121"/>Semantic Kernel</h2>
			<p>Semantic kernel, or <a id="_idIndexMarker562"/>SK, is a lightweight, open-source <strong class="bold">software development kit</strong> (<strong class="bold">SDK</strong>); it is a<a id="_idIndexMarker563"/> modern AI application development framework that enables software developers to build an AI orchestration to build agents, write code that can interact with agents, and also support generative<a id="_idIndexMarker564"/> AI tooling and concepts, such as <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>), which we covered in <a href="B21443_02.xhtml#_idTextAnchor036"><em class="italic">Chapter 2</em></a>.</p>
			<h3>“Kernel” is at the core of everything!</h3>
			<p>Semantic Kernel<a id="_idIndexMarker565"/> revolves around the concept of a “kernel,” which is pivotal and is equipped with the necessary services and plugins to execute both native code and AI services, making it a central element for nearly all SDK components.</p>
			<p>Every prompt or code executed within the semantic kernel passes through this kernel, granting developers a unified platform for configuring and monitoring their AI applications.</p>
			<p>For instance, when a prompt is invoked through the kernel, it undertakes the process of selecting the optimal AI service, constructing the prompt based on a prompt template, dispatching the prompt to the service, and processing the response before delivering it back to the application. Additionally, the kernel allows for the integration of events and middleware at various stages, facilitating tasks such as logging, user updates, and the implementation of responsible AI practices, all from a single, centralized location called “kernel.”</p>
			<p>Moreover, SK allows developers to define the syntax and semantics of natural language expressions and use them as variables, functions, or data structures in their code. SK also provides tools for parsing, analyzing, and generating natural language from code and, vice-versa, generating code from NLP.</p>
			<p><strong class="bold">You can build sophisticated and complex agents without having to be an AI expert by using semantic kernel SDK!</strong> The fundamental building blocks in semantic kernels for building agents are <strong class="bold">plugins, planners, </strong><strong class="bold">and personas</strong>.</p>
			<h3>Fundamental components</h3>
			<p>Let’s dive into each one of them and understand what each one means.</p>
			<ul>
				<li><strong class="bold">Plugins</strong> enhance your <a id="_idIndexMarker566"/>agent’s functionality by allowing you to incorporate additional code. This enables the integration of new functions into plugins, utilizing native programming languages such as C# or Python. Additionally, plugins can facilitate interaction with LLMs through prompts or connect to external services via REST API calls. As an example, consider a plugin for a virtual assistant for a calendar application that allows it to schedule appointments, remind you of upcoming events, or cancel meetings. If you have used ChatGPT, you may be familiar with the concept of plugins, as they are integrated into it (namely, “Code Interpreter” or “Bing Search Plugin”).</li>
				<li><strong class="bold">Planners</strong>: In order to <a id="_idIndexMarker567"/>effectively utilize the plugin and integrate it with subsequent actions, the system must initially design a plan, a process that is facilitated by planners. This is where the planners help. Planners are sophisticated instructions that enable an agent to formulate a strategy for accomplishing a given task, often encapsulated in a simple prompt that guides the agent through function calling to achieve the objective.</li>
				<li>As an example, take the development of a MeetingEventPlanner. This planner would guide the agent through the detailed process of organizing a meeting. It includes steps such as reviewing the availability of attendees’ calendars, sending out confirmation emails, drafting an agenda, and, finally, scheduling the meeting. Each step is carefully outlined to ensure the agent comprehensively addresses all the necessary actions for successful meeting preparation.</li>
				<li><strong class="bold">Personas</strong>: Personas<a id="_idIndexMarker568"/> are sets of instructions that shape the behavior of agents by imbuing them with distinct personalities. Often referred to as “meta prompts,” these guidelines endow agents with characters that can range from friendly and professional to humorous, and so forth. Additionally, they direct agents on the type of response to generate, which can vary from verbose to concise. We have explored meta prompts in great detail in <a href="B21443_05.xhtml#_idTextAnchor098"><em class="italic">Chapter 5</em></a>; this concept is closely related.</li>
			</ul>
			<p>However, now let’s take a step back and understand why we want to use SK and do such things as create natural language interfaces, chatbots, or natural language programming systems in the first place. Consider LLMs as the engine powering generative AI applications, and SKs act as the assembly line, integrating various generative AI services. For software developers, the reusability of code—be it functions or snippets—is crucial to streamline development processes. Furthermore, for expansive organizational applications, the efficient management of prompts, completions, and other agent-specific data is not just an operational preference but a fundamental business necessity. SK emerges as a pivotal framework, enabling the construction of durable and comprehensive generative AI applications by seamlessly integrating these essential facets.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">For LLMs, the engine alone is not able to meet these business requirements any more than an engine without oil, gasoline, or electricity is able to meet a driver’s requirements of providing transportation. You need additional software code to provide a solution, not just the LLMs, and generative AI programming frameworks, such as SK, allow you to accomplish this. You are building around the engine to provide transportation, and you are building around LLMs to provide a generative AI solution.</p>
			<p>For a real-world example, let’s use the company Microsoft. As mentioned earlier, Microsoft itself has embraced the SK framework across its organization, exemplifying its wide applicability and effectiveness. This integration is particularly evident in their next-generation AI-integrated offerings, called “Copilots.” These Copilots harness the capabilities of LLMs, alongside your data and other Microsoft applications, including the Microsoft 365 suite (Word, Excel, and more). All of these components are seamlessly integrated using the SK framework, showcasing a sophisticated and powerful example of AI-enhanced productivity tools.</p>
			<p>Additionally, later in this chapter, we’ll show an actual use case of how a Fortune 500 company transformed their development team and, thus, their applications into state-of-the-art, modern, generative AI-ready applications and solutions using SK.</p>
			<p>If you would like to see more details on SK, you can visit the following link: <em class="italic">microsoft/semantic-kernel: Integrate cutting-edge LLM technology quickly and easily into your apps (</em><a href="http://github.com">github.com</a><em class="italic">)</em>, <a href="https://github.com/microsoft/semantic-kernel">https://github.com/microsoft/semantic-kernel</a>.</p>
			<p><em class="italic">Figure 6</em><em class="italic">.3</em> provides a high-level visual description demonstration of the role of SK as an AI orchestrator between LLMs, AI infrastructure, copilots, and plugins in the Microsoft Copilot system:</p>
			<div><div><img src="img/B21443_06_3.jpg" alt="Figure 6.3 – Role of SK as an AI orchestrator in Microsoft Copilot system"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – Role of SK as an AI orchestrator in Microsoft Copilot system</p>
			<h3>Assistants API</h3>
			<p>The <a id="_idIndexMarker569"/>Assistants API (introduced by Open AI in late 2023) allows you to build AI agents with minimal code on OpenAI’s chat completion models. This is an API that will soon be integrated into Semantic Kernel to build agent-like experiences, as mentioned in a blog by Microsoft (<a href="https://devblogs.microsoft.com/semantic-kernel/assistants-the-future-of-semantic-kernel/">https://devblogs.microsoft.com/semantic-kernel/assistants-the-future-of-semantic-kernel/</a>).</p>
			<p>This API helps developers build high-quality copilot-like experiences in their own applications. As discussed earlier, copilots are AI assistants integrated into applications to help address questions or provide instructional steps to help the user achieve more complex tasks.</p>
			<p>Before, creating custom AI assistants required a lot of work, even for skilled developers. The chat completions API from OpenAI is easy to use and powerful, but it is not stateful (does not have state), which meant developers and/or operations had to manage conversation state and chat threads, tool integrations, the retrieval of documents, and also managing indexes, all while running code manually. In OpenAI’s evolution, the Assistants API is the stateful version of the chat completion API, and it offers a solution to address these problems.</p>
			<p>It is now easier than ever to build customizable, specific generative AI applications and services that can search through data, propose solutions, and automate tasks. Assistants API supports persistent and unlimited (infinitely long) threads. This means that you do not need to create a thread state management system or deal with a model’s context window limitations as developers. You can just add new messages to a thread, and users reply (prompt/completions). The <a id="_idIndexMarker570"/>Assistants API can also access files in different formats, either when creating an assistant or as part of threads. Assistants can also access multiple tools as needed. Some example tools include the following:</p>
			<ul>
				<li><strong class="bold">Function calling</strong>: The Assistants API can call an existing function or code subroutine. With the<a id="_idIndexMarker571"/> Assistants API, your assistant can learn what your app or external APIs do, choose the right time to call those functions, and use the function(s) in response to messages or other behavior.</li>
				<li><strong class="bold">Code interpreter</strong>: With the code interpreter<a id="_idIndexMarker572"/> tool from OpenAI/Azure OpenAI Service, you can write and execute code, such as Python code, in a separate environment. You can use it for various purposes, such as finding solutions to difficult code and math problems step by step, doing advanced data analysis on user-added files in different formats, and creating data visualization such as reports, charts, and graphs. The Assistants API can integrate and run code interpreters as they may deem necessary or as directed.</li>
			</ul>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor122"/>LangChain</h2>
			<p>Like SK, LangChain<a id="_idIndexMarker573"/> is another open-source SDK application development framework and toolkit for building modern AI applications with LLMs. It provides out-of-the-box libraries and templates to develop, productionalize, and deploy your applications.</p>
			<p>LangChain <a id="_idIndexMarker574"/>revolves around <a id="_idIndexMarker575"/>the concept of “<strong class="bold">chaining</strong>”</p>
			<p>A distinctive feature of LangChain is its<a id="_idIndexMarker576"/> use of “<strong class="bold">chains</strong>,” setting it apart from SK, which is centered around a kernel, as previously discussed. In LangChain, the output from one component serves as the input for the next, allowing elements such as prompts, models, and parsers to be connected in sequence before activation. Developers can harness LangChain to assemble new prompt chains, enabling the integration of multiple LLMs in a sequential manner, where the output from one LLM feeds into the next; hence, the term LangChain. Additionally, LangChain includes features that permit LLMs to incorporate new datasets without requiring retraining, similar to SK.</p>
			<p>Benefits for app developers</p>
			<p>We have mentioned a few of the myriad benefits<a id="_idIndexMarker577"/> that LangChain provides in the following list:</p>
			<ul>
				<li><strong class="bold">Link LLMs with data sources</strong>: Finally, LangChain provides AI developers with tools to link language models with any data sources. It consists of different types of parsers and document loader functionalities that help connect to any data source seamlessly.</li>
				<li><strong class="bold">Simplifies RAG implementations</strong>: Development teams can build complex applications that access internal company information and data to improve model responses. In other words, you can create a <strong class="bold">retrieval-augmented generation</strong> (<strong class="bold">RAG</strong>) workflow that <a id="_idIndexMarker578"/>adds context information to the language model during prompting. As you learned in <a href="B21443_04.xhtml#_idTextAnchor070"><em class="italic">Chapter 4</em></a>, using context-aware workflows, such as RAG, reduces model errors and improves response quality.</li>
				<li><strong class="bold">Accelerates development with libraries and templates</strong>: Developers customize sequences to build complex applications easily. Instead of coding business logic, software teams can modify existing templates and libraries that LangChain<a id="_idIndexMarker579"/> provides to reduce development time.</li>
			</ul>
			<p>While both Semantic Kernel<a id="_idIndexMarker580"/> and LangChain are open source and free to use, LangChain<a id="_idIndexMarker581"/> is more widely used at the time of this writing, and LangChain does offer more compatibility with many open source models available on public model repositories, such as Hugging Face. On the flip side, based on the experience and testing by some using real-world applications, Semantic Kernel performs much better in large-scale business applications. We are not suggesting using one service over the other, but understanding that each framework has its benefits and some drawbacks is useful. Both are equally critical in your journey of creating the next-generation generative AI apps.</p>
			<p>If you would like to get more details on LangChain and the plethora of benefits it provides to developers, we suggest checking out the following links:</p>
			<ul>
				<li>langchain-ai/langchain: Building applications with LLMs through composability (github.com) – <a href="https://github.com/langchain-ai/langchain">https://github.com/langchain-ai/langchain</a></li>
				<li><a href="https://python.langchain.com/docs/expression_language/get_started/">https://python.langchain.com/docs/expression_language/get_started/</a></li>
			</ul>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor123"/>LlamaIndex</h2>
			<p>Similar to Semantic Kernel and<a id="_idIndexMarker582"/> LangChain, LlamaIndex is a programming data framework for applications that use LLMs, allowing one to ingest, manage, and retrieve not only domain-specific data (such as industry-specific) but also private data using natural language. LlamaIndex is Python-based.</p>
			<p>LlamaIndex has two main stages: the indexing stage and the querying stage, which can be incorporated into an LLMOps process, and we will cover this a bit later:</p>
			<ul>
				<li><strong class="bold">Indexing stage</strong>: In this<a id="_idIndexMarker583"/> stage, LlamaIndex creates a vector index of your private data. This makes it possible to search through your own organization’s domain-specific knowledge base. You can input text documents, database records, knowledge graphs, and other data types.</li>
				<li><strong class="bold">Querying stage</strong>: In <a id="_idIndexMarker584"/>this stage, the RAG pipeline finds the most relevant information based on the user’s query. This information is then passed to the LLM, along with the query, to generate a more accurate response.</li>
			</ul>
			<p>Finally, LlamaIndex has three main components:</p>
			<ul>
				<li><strong class="bold">Data connectors</strong>: They <a id="_idIndexMarker585"/>allow you to pull data from wherever it is stored, such as APIs, PDFs, databases, or external apps, such as Meta or X.</li>
				<li><strong class="bold">Data indexes</strong>: The<a id="_idIndexMarker586"/> data index component organizes your data so that they are readily available.</li>
				<li><strong class="bold">Engines</strong>: The <a id="_idIndexMarker587"/>heart of this is the engine component, which enables you to use natural language to interact with your data and create applications, agents, and workflows. We will cover exactly what agents and workflows are in the next section.</li>
			</ul>
			<p>Now, the question arises: <strong class="bold">when should each be used?</strong> SK, Langchain, and LlamaIndex are architecturally distinct. SK and Langchain are broader frameworks that excel in scenarios requiring more complex interactions with agents and adding that AI orchestration layer when building chatbots.</p>
			<p>Conversely, LlamaIndex stands out in RAG-based search-focused applications due to its optimization for swift and efficient search capabilities. Employing unique indexing methods significantly improves the pace of data retrieval.</p>
			<p>If you would like to see more details on<a id="_idIndexMarker588"/> LlamaIndex, you can visit the following link: <a href="https://docs.llamaindex.ai/en/stable/">https://docs.llamaindex.ai/en/stable/</a>.</p>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor124"/>Autonomous agents</h1>
			<p><strong class="bold">Autonomous agents</strong> are <a id="_idIndexMarker589"/>a more advanced implementation of standard agents (mentioned in previous section) and are evolving at a rapid pace. Autonomous agents take the concept of agents a little further. These agents could be a team of agents that can perform various tasks and manage other agents automatically, collaborating autonomously without requiring user input or direction. They possess the ability to provide self-feedback and autonomously improve over time.</p>
			<p>For instance, within a creative company, the concept of autonomous agents collaborating as a team can be leveraged to streamline and enhance the creative process.</p>
			<p class="List-Paragraph">The following is a sample scenario:</p>
			<div><div><img src="img/B21443_06_4.jpg" alt="Figure 6.4 – Team of AI autonomous agents"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Team of AI autonomous agents</p>
			<p class="List-Paragraph">Imagine a scenario<a id="_idIndexMarker590"/> where a creative agency is charged with creating an innovative advertising campaign. The team consists of six members, all autonomous agents organized in a hierarchy, managed by a manager who is also an autonomous agent. Here’s an overview of how various AI agents could work together to accomplish this goal. The process would begin with a human user presenting the initial topic, which then triggers the subsequent steps as follows:</p>
			<ul>
				<li><strong class="bold">Trend Analysis Agent</strong>: This AI agent autonomously analyzes the internet, social media, and data sources to detect current consumer trends, popular culture, and industry movements, identifying themes that resonate with the target audience to guide the campaign’s creative direction.</li>
				<li><strong class="bold">Concept Generation Agent</strong>: Leveraging insights from the Trend Analysis Agent, this AI generates a range of creative concepts for the campaign. It uses generative AI models trained on successful advertising campaigns, art, literature, and film to propose original and engaging ideas that align with the identified trends.</li>
				<li><strong class="bold">Design and Visualization Agent</strong>: Once a concept is selected, this agent creates visual mockups of the advertising materials. Using generative AI models trained in graphic design and multimedia production produces high-quality images, videos, and other creative assets that bring the concept to life.</li>
				<li><strong class="bold">Copywriting Agent</strong>: In parallel, a copywriting AI agent generates compelling copy for the campaign. It crafts messages that capture the campaign’s essence, ensuring they are tailored to the target audience’s language and emotional triggers. This agent uses natural language generation technologies to produce a variety of copy options, from headlines to detailed product descriptions.</li>
				<li><strong class="bold">Feedback and Iteration Agent</strong>: This agent collects feedback on the creative outputs from the team, stakeholders, and potentially a selected audience sample. It uses sentiment analysis and feedback loops to understand reactions and suggests modifications to the concept, design, or copy to improve the campaign’s effectiveness.</li>
				<li><strong class="bold">Integration and Strategy Agent</strong> (manager): Finally, an integration agent oversees the assembly of all creative elements into a cohesive campaign. It ensures that the strategy aligns with the company’s branding and marketing goals, adjusting the campaign’s deployment across various channels for maximum impact.</li>
			</ul>
			<p>In this creative company scenario, autonomous<a id="_idIndexMarker591"/> AI agents bring efficiency and innovation to the creative process. By leveraging their specialized skills in trend analysis, concept generation, design, copywriting, and strategy, they enable the company to rapidly develop and iterate groundbreaking advertising campaigns that resonate deeply with the target audience.</p>
			<p>Now that we have learned about the concepts of agents, let’s us understand how to make it a reality with application development frameworks and multi-conversation agent frameworks in the next section.</p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor125"/>Agent collaboration frameworks</h1>
			<p>In this chapter, we have covered <a id="_idIndexMarker592"/>generative AI from the perspectives of developers and operations by introducing programming development frameworks and many of the concepts related to this, including the concept of agents. We feel agents are a very exciting field of focus, where a brand new revolution, the generative AI revolution, will catapult humanity to heights we have not seen before and could only have dreamed of (perhaps in science fiction books!) only a year or two ago.</p>
			<p>In <a href="B21443_02.xhtml#_idTextAnchor036"><em class="italic">Chapter 2</em></a>, we very briefly touched on the exciting concept of autonomous agents, and in this section, we will cover this concept further, but first, let’s revisit what an ‘agent’ is. Recall that an “agent,” when used in the generative AI context, is software code that is AI-aware, and that can complete tasks, such as retrieving and gathering information from the user via an application or other model; it then uses this information to perform an action, such as input this into an LLM or a series of LLMs, to name just one action.</p>
			<p>Let’s visually describe<a id="_idIndexMarker593"/> what an agent is beyond just pieces of code, as there are a few essential components that are needed for an agent to do its job:</p>
			<div><div><img src="img/B21443_06_5.jpg" alt="Figure 6.5 – What makes an agent?"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – What makes an agent?</p>
			<p>According to Ben Tossell, Founder of Ben’s Bites AI Newsletter, “<em class="italic">AI agents will be everywhere. Billion-dollar companies will come from a small team that deploys </em><em class="italic">ai agents</em>.”</p>
			<p>This is quite a statement! However, we feel it to be very accurate and agree with this statement. However, let’s take this one more step. In the general term of an agent, this agent must wait for some sort of interaction or direction by a human, likely via code. This limits any agent in terms of waiting (precious time is wasted) and following whatever only a human knows.</p>
			<p>With “autonomous agents,” as the name suggests, this AI-powered code can now do things by themselves on their own, from completing tasks by taking action to creating new tasks, and they continue doing so until the task is complete. Furthermore, autonomous agents can provide self-feedback and subsequently improve autonomously, allowing for self-growth and improvement! All the while, these autonomous agents can communicate and collaborate with other autonomous agents to build a network of <a id="_idIndexMarker594"/>autonomous and tackle the most complex tasks, all with almost no human interaction! Of course, this will require all the guardrails and protection in place to prevent harm to society.</p>
			<p>Now let’s take a look at two popular frameworks: AutoGen by Microsoft and AutoGPT by Mindstream.</p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor126"/>AutoGen</h2>
			<p><strong class="bold">Autogen</strong>, an <a id="_idIndexMarker595"/>agent collaboration<a id="_idIndexMarker596"/> framework introduced by Microsoft Research, is another major breakthrough in AI. It is an open source platform for building multi-agent systems that work autonomously using LLMs, and we feel this will have one of the most significant impacts in the generative AI space in the upcoming months and years (<a href="https://arxiv.org/abs/2308.08155">https://arxiv.org/abs/2308.08155</a>).</p>
			<p>AutoGen can help build agents that perform tasks such as reasoning, planning, task decomposition, reflection, self-critique, self-improvement, self-evaluation, memory, personalization, and communication by using various prompt engineering techniques, just to name a few areas. Of course, as mentioned above, autonomous agents can call on other autonomous agents to help address the most complex of problems or situations.</p>
			<p>How exciting is it if an Autogen created autonomous agents to collaborate with other specialized agents when a task is quite complex and extremely large, say, the task of building a warp drive; although this is a tongue-in-cheek scenario (or perhaps it’s not), humanity alone cannot tackle these extreme, vastly complex use cases, as in the example of building a warp drive for an engine to propel a craft faster than the speed of light!</p>
			<p>However, as you might be able to conclude, the possibilities are endless once we understand how multiple large language models + AutoGen can work together in different ways, e.g., aligned in a hierarchical way, networked together in an orderly fashion, or swarm together, all with the goal of increasing the computing and reasoning power to solve extremely complex problems, including complex problems that may not even exist today!</p>
			<p>Some tasks <a id="_idIndexMarker597"/>Autogen can perform autonomously include automated task solving with code generation, execution and <a id="_idIndexMarker598"/>debugging, and automated data visualization from a group chat. More exciting examples can be seen here: <a href="https://microsoft.github.io/autogen/docs/Examples#automated-multi-agent-chat">https://microsoft.github.io/autogen/docs/Examples#automated-multi-agent-chat</a>.</p>
			<p>If you want to test out Autogen, check out the Autogen studio developed by Microsoft: <a href="https://autogen-studio.com/autogen-studio-ui">https://autogen-studio.com/autogen-studio-ui</a>.</p>
			<p>Moreover, to learn more about Autogen, we suggest checking out this link: AutoGen | AutoGen (microsoft.github.io) – <a href="https://microsoft.github.io/autogen/">https://microsoft.github.io/autogen/</a>.</p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor127"/>TaskWeaver</h2>
			<p><strong class="bold">TaskWeaver</strong> is yet <a id="_idIndexMarker599"/>another framework developed by Microsoft for building autonomous <a id="_idIndexMarker600"/>agents, but it uses a code-first approach as opposed to the template-based approach taken by Autogen.</p>
			<p>TaskWeaver distinguishes itself by transforming user requests into actionable code and treating the plugins defined by users as if they were callable functions.</p>
			<p>To learn more about <a id="_idIndexMarker601"/>TaskWeaver, we suggest reading this research paper: <a href="https://arxiv.org/pdf/2311.17541.pdf">https://arxiv.org/pdf/2311.17541.pdf</a>.</p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor128"/>AutoGPT</h2>
			<p>Another application that has received a lot of attention in the autonomous agent world is <strong class="bold">AutoGPT</strong> from <a id="_idIndexMarker602"/>Mindstream. AutoGPT is an open source application that aims to make AI <a id="_idIndexMarker603"/>available to everyone. Currently, it uses the GPT-4 model and is also designed to complete autonomous tasks using autonomous agents, similar to AutoGen. A few examples of tasks that AutoGPT can complete include research, coding, or content creation.</p>
			<p>AutoGPT (driven by GPT-4) chains together LLM thoughts to achieve its goals and also allows extensibility. An example of extensibility is where one can extend the functionality of these autonomous agents with plugins or software add-ons, enhancing the capabilities of autonomous agents even further, which allows for variety in data collection, interaction with web platforms, and multi-modal functions.</p>
			<p>AutoGPT is a significant improvement in the field of autonomous agents, enriching AI applications and agents when compared to non-autonomous agents.</p>
			<p>While the concept of autonomous agents may cause some anxiety, this is no longer a concept but a reality. It has already started and is happening now. Some fear the use of autonomous agents may cause a<a id="_idIndexMarker604"/> technological singularity, “<em class="italic">a hypothetical future point in time at which technological growth becomes uncontrollable and irreversible, resulting in unforeseeable consequences for human civilization</em>,” as defined by Wikipedia: <a href="https://en.wikipedia.org/wiki/Technological_singularity">https://en.wikipedia.org/wiki/Technological_singularity</a>.</p>
			<p>However, we feel there will be significant safeguards in place to avoid such a singularity. A delightful concept we came up with is having a “foreman” autonomous agent, or agents, which oversee the tasks of other autonomous agents, or their “crew,” monitoring their activity and taking necessary disciplinary action to prevent any maliciousness. This foreman would be “in charge” of all the other agents, which is no different from a foreman on a construction site overseeing the activities of the construction workers and crew.</p>
			<p>If you <a id="_idIndexMarker605"/>would like to get more <a id="_idIndexMarker606"/>information on AutoGPT, we suggest checking out the following two links:</p>
			<ul>
				<li>Significant-Gravitas/AutoGPT: AutoGPT is the vision of accessible AI for everyone, to use and to build on. Our mission is to provide the tools, so that you can focus on what matters. (github.com) - <a href="https://github.com/Significant-Gravitas/AutoGPT">https://github.com/Significant-Gravitas/AutoGPT</a></li>
				<li>AutoGPT documentation: <a href="https://docs.agpt.co/">https://docs.agpt.co/</a></li>
			</ul>
			<p>Up to this point in our exploration, we’ve delved into a variety of concepts, such as RAG, fine-tuning, prompt engineering, and agents, which serve as the building blocks for crafting cutting-edge generative AI applications. Let’s now shift our focus towards the operationalization aspect, aiming to unpack how we can seamlessly transition these concepts into production. Our goal is to enhance efficiency and automation, ensuring that the theoretical foundations we’ve laid can be applied in practical, real-world scenarios.</p>
			<h1 id="_idParaDest-129"><a id="_idTextAnchor129"/>LLMOps – Operationalizing LLM apps in production</h1>
			<p>In this section, we aim to comprehend what LLMOps<a id="_idIndexMarker607"/> entails. We will then explore the lifecycle of LLMs, the fundamental components of LLMOps, its benefits, and how it compares to traditional MLOps practices. Additionally, we will discuss Azure’s Prompt Flow platform, which facilitates the transformation of this concept into a tactical solution:</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor130"/>What is LLMOps?</h2>
			<ul>
				<li><strong class="bold">Definition</strong>: LLMOps <a id="_idIndexMarker608"/>or large language model operations is a collection of tools and practices focused on managing the lifecycle of generative AI models, including LLMs, small language models (SLMs), and related artifacts in a production environment.</li>
				<li>The <strong class="bold">goal</strong> of LLMOps is<a id="_idIndexMarker609"/> to ensure continuous quality, reliability, security, and ethical standards of generative AI models and their applications in production with enhanced efficiency and automation.</li>
				<li><strong class="bold">LLM Lifecycle activities</strong>: It encompasses a comprehensive workflow that includes a series of critical <a id="_idIndexMarker610"/>activities such as initial data preparation, model creation and tuning, prompt engineering, setting up evaluation frameworks, deploying, monitoring, updating, and eventually retiring Large Language Models (LLMs) when they are deprecated. It is designed to be a scalable and efficient method for managing LLMs.</li>
				<li><strong class="bold">Orchestration and automation</strong>: These activities are typically executed through independent, repeatable <a id="_idIndexMarker611"/>pipelines that are then systematically integrated using a process known as <a id="_idIndexMarker612"/>orchestration. This orchestration ensures that each component of the workflow communicates effectively with the others, allowing for a seamless transition from one stage to the next. By doing so, it enables a more structured and efficient approach to managing the lifecycle of LLMs, from development through to deployment and beyond.</li>
				<li><strong class="bold">Deployment</strong>: LLMOps <a id="_idIndexMarker613"/>automates such orchestration with CI/CD practices that entails the integration of code and trained/fine-tuned models to production, testing, release, and monitoring of LLM-based applications in a systematic manner, incorporating both automated and manual processes depending on the maturity of the tools, processes, and specific requirements of the applications.</li>
			</ul>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor131"/>Why do we need LLMOps?</h2>
			<ul>
				<li>The need for<a id="_idIndexMarker614"/> LLMOps arises from the complexity and scale of deploying and managing generative AI models.</li>
				<li>Drawing parallels with its predecessors—machine learning operations (MLOps) and developer operations (DevOps)—LLMOps aims to simplify the integration of the critical aspects of deployment: people, processes, and technology.</li>
				<li>This integration aims to automate complex manual processes across to accelerate the delivery of LLM-infused software and maximize value to an organization. LLMOps serves as the bridge that combines tools and processes to manage the end-to-end lifecycle of creating, launching, and maintaining applications based on generative AI and LLMs.</li>
			</ul>
			<p>To grasp the essence of<a id="_idIndexMarker615"/> LLMOps, it’s essential to first acquaint ourselves with the processes involved in managing the lifecycle of LLMs. This overarching process lays the groundwork for enabling LLMOps, providing a structured framework through which we can understand the intricate steps of development, deployment, and maintenance of LLMs.</p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor132"/>LLM lifecycle management</h2>
			<p>LLM lifecycle management<a id="_idIndexMarker616"/> is a fairly young concept; however, one fact remains, the LLM lifecycle covers quite a few discipline areas. It is an iterative process and not a linear process, reflecting the multi-faceted nature of real-world applications with these key ingredients: ideation, development, deployment, and management.</p>
			<p>Here is a visual diagram to aid our discussion as we view the process flow; this relates to LLM and, ultimately, LLMOps:</p>
			<div><div><img src="img/B21443_06_6.jpg" alt="Figure 6.6 – LLM lifecycle in the real world"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – LLM lifecycle in the real world</p>
			<p>In the preceding<a id="_idIndexMarker617"/> image above, the three larger circles represent the end-to-end lifecycle phases in managing/developing LLMs, similar to what we might see in traditional application lifecycles. As stated earlier, these phases are not linear, so let us describe what is occurring here, with each circle representing a phase, moving left to right.</p>
			<ul>
				<li><strong class="bold">Phase 1</strong>: On the far left, we first try to understand <strong class="bold">BUSINESS REQUIREMENTS</strong> and begin the exploring and ideation steps in this initial phase. In this phase, let’s call it phase one, some of the tasks we will complete include finding some foundational or other LLMs using benchmarks, model cards, etc., and running a few prompts against this to test some basic business requirements and also test some hypotheses we believe based on our understanding of the business requirements. Usually, in this initial phase, we may also be able to modify the business requirements based on early exploration.</li>
				<li><strong class="bold">Phase 2</strong>: As we advance to the next phase, phase 2, we are now building and augmenting our LLM, using the techniques covered earlier in this book, such as RAG, prompt engineering, or fine-tuning. If there are any errors within our LLM lifecycle processes in the second phase or if RAG is not optimized and fine-tuning is not providing us with the correct results, we can then revert back to the first phase to try to find other existing LLMs or retry a different hypothesis (or even alter our existing hypothesis), and start the LLM lifecycle again. We will also employ the comprehensive evaluation techniques that we discussed <em class="italic">in </em><a href="B21443_05.xhtml#_idTextAnchor098"><em class="italic">Chapter 5</em></a> to evaluate the model.</li>
				<li><strong class="bold">Phase 3</strong>: Once we are successful in completing phase 2, we can move on to the third and final phase of our LLM lifecycle, which is operationalizing the LLM, deploying it as an app, or integrating the LLM app into an existing service. Moreover, within this lifecycle, we have additional operational areas we need to address: monitoring, quota and cost management, safe rollout/staging, and content filtering (we will cover the monitoring, content safety, and quota aspects in further detail in the upcoming chapters). We can also consider any additional feedback from the end users and take this back to phase two, where we may need to conduct additional fine-tuning or additional grounding on our data with RAG.</li>
			</ul>
			<p>Overarching all these<a id="_idIndexMarker618"/> phases and activities is the managing/management loop, which focuses on governance, security, and compliance, which we will cover in the next two chapters. To wrap up this part, as we understand the preceding LLM lifecycle stages, we understand how to balance agility with adherence to standards while meeting business requirements.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">An emerging fourth phase in the lifecycle of LLMs addresses the end-of-life stage when an LLM no longer meets business requirements or becomes obsolete. This phase involves safely decommissioning the outdated LLM, potentially replacing it with a newer, more advanced model. The key actions include migrating APIs and other integrations to the new model, ensuring a seamless transition. This addition marks the beginning of a cyclical process, restarting with the initial phase of deploying a fresh LLM.</p>
			<p>Let’s take a look at the key activities that make up an LLMOps strategy.</p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor133"/>Essential components of LLMOps</h2>
			<p>In this section, we will discuss<a id="_idIndexMarker619"/> some of the key components of LLMOps that entail the process explained previously:</p>
			<div><div><img src="img/B21443_06_07.jpg" alt="Figure 6.7 – The flow of an LLM lifecycle"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – The flow of an LLM lifecycle</p>
			<p>The enterprise LLMOps strategy must include the following steps as a minimum:</p>
			<h3>Data preparation</h3>
			<ul>
				<li><strong class="bold">Initialization and data curation</strong>: This step facilitates the creation of reproducible and versioned datasets. It involves transforming, aggregating, and de-duplicating <a id="_idIndexMarker620"/>data, as well as developing structured and reliable prompts for querying LLMs. Additionally, exploratory analysis is performed to understand the nature of the data and enrich it with any necessary information.</li>
			</ul>
			<h3>Discover and tune</h3>
			<ul>
				<li><strong class="bold">Experimentation</strong>: This<a id="_idIndexMarker621"/> step focuses on identifying the most suitable LLM solutions by researching and discovering LLMs that could match your use case. It involves auditing through rapid iterations of testing various techniques, including prompt engineering, information retrieval optimization, relevance enhancement, model selection, fine-tuning, and hyperparameter adjustments.</li>
				<li><strong class="bold">Evaluation and refinement</strong>: This is the process that defines tailored metrics, and selecting methods of comparing results to them at key points that contribute to overall solution performance. This is an iterative process to see how changes impact solution performance such as optimizing a search index during information retrieval for RAG implementations or refining <a id="_idIndexMarker622"/>few-shot examples through prompt engineering.</li>
			</ul>
			<h3>Deployment</h3>
			<ul>
				<li><strong class="bold">Validation and deployment</strong>: This step includes rigorous model validation to evaluate<a id="_idIndexMarker623"/> performance in production environments and A/B testing to evaluate new and existing solutions before deploying the most performant ones into various environments.</li>
				<li><strong class="bold">Inferencing and serving</strong>: This step involves providing an optimized model tailored for consistent, reliable, low-latency, and high-throughput responses, with batch processing support. Enabling CI/CD to automate the preproduction pipeline. Serving is usually done with a REST API call.</li>
			</ul>
			<h3>Monitoring with human feedback</h3>
			<ul>
				<li><strong class="bold">Monitor models</strong>: Monitoring <a id="_idIndexMarker624"/>within an LLM or LLMOps, is a critical component to ensure the overall health of your LLM over a continued period of time. Items such as <strong class="bold">model data drift</strong>, which occurs when the <a id="_idIndexMarker625"/>distribution of the datasets used with LLM changes over time, can lead to model degradation and performance. This is especially true when doing any predictive analytics, as the input data may be incorrect, thus having a false outcome. Fortunately, there are features within commercial services, such as Azure Machine Learning, which help account for and monitor data drift.<p class="list-inset">The image below, sourced from Microsoft’s blog on LLMOps, depicts a dashboard that monitors a few evaluation metrics related to quality, such as groundedness, relevance, fluency, similarity, and coherence for generative AI applications, illustrating <a id="_idIndexMarker626"/>their changes over time:</p></li>
			</ul>
			<div><div><img src="img/B21443_06_8.jpg" alt="Figure 6.8 – An overview of LLMOps dashboard on Azure Prompt Flow"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 – An overview of LLMOps dashboard on Azure Prompt Flow</p>
			<ul>
				<li><strong class="bold">Infra-monitoring</strong>: With any comprehensive operational plan, monitoring is always an included critical component.<p class="list-inset">The monitoring procedures cover tools and practices to assess and report on system and solution performance and health. Monitored areas include API latency and throughput (Requests per second and Tokens Per second) to ensure optimal user experience. This can be achieved through Azure API Management, which we discuss in the next chapter.</p><p class="list-inset">Metrics to track resource utilization, raising real-time alerts for issues or anomalies or for any data privacy breaches like jailbreak attacks, prompt injections, etc, and evaluating queries and responses for issues such as inappropriate responses. We discuss such metrics related to safe, secure, and responsible AI, in <em class="italic">Chapters</em> <em class="italic">8</em> and <em class="italic">9</em>.</p><p class="list-inset">Finally, most modern monitoring systems can also automatically raise trouble and support<a id="_idIndexMarker627"/> tickets, for human intervention and review, for any alerts, anomalies, or issues.</p></li>
			</ul>
			<h3>Retraining</h3>
			<ul>
				<li><strong class="bold">Collecting feedback</strong>: This <a id="_idIndexMarker628"/>critical step enables seamless mechanism for collecting user feedback or capturing user-provided data for insights, which is then used to enrich the validation datasets to improve the LLM solution’s performance.</li>
			</ul>
			<p>The components and activities identified in the preceding list can be developed into repeatable pipelines. These pipelines can then be efficiently orchestrated into a coherent workflow, as previously discussed. By further enhancing operational efficiency, this orchestrated workflow can be automated and<a id="_idIndexMarker629"/> seamlessly integrated with <strong class="bold">continuous integration/continuous deployment</strong> (<strong class="bold">CI/CD</strong>) workflows. Such pipelines can be easily developed in Python using frameworks, such as Langchain or Semantic Kernel, and then orchestrated and automated on Azure Prompt Flow.</p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor134"/>Benefits of LLMOps</h2>
			<ul>
				<li><strong class="bold">Automation and Efficiency</strong>: Automation significantly reduces the duplication of efforts when<a id="_idIndexMarker630"/> introducing a new use case into production. The workflow, encompassing data ingestion, preparation, fine-tuning, deployment, and monitoring, is automatically triggered. This streamlining makes the entire process of integrating another use case much more efficient.</li>
				<li><strong class="bold">Agility</strong>: LLMOps accelerates model and pipeline development, enhances the quality of models, and speeds up deployment to production, fostering a more agile environment for data teams.</li>
				<li><strong class="bold">Reproducibility</strong>: It facilitates the reproducibility of LLM pipelines, ensuring seamless collaboration across data teams, minimizing conflicts with DevOps and IT, and enhancing release velocity.</li>
				<li><strong class="bold">Risk mitigation</strong>: LLMOps enhances transparency and responsiveness to regulatory scrutiny, ensuring greater compliance with policies and thereby mitigating risks.</li>
				<li><strong class="bold">Scalability management</strong>: Enables extensive scalability and management capabilities, allowing for the oversight, control, management, and monitoring of thousands of <a id="_idIndexMarker631"/>models for continuous integration, delivery, and deployment.</li>
			</ul>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor135"/>Comparing MLOps and LLMOps</h2>
			<p>While it is evident that <a id="_idIndexMarker632"/>MLOps is to machine<a id="_idIndexMarker633"/> learning as LLMOps is to LLMs, LLMOps shares many similarities <strong class="bold">and has some differences</strong> with MLOps. While some of our readers may already be familiar with machine learning and using MLOPs, with LLMOps, we do not have to go through expensive model training, as the LLM models are already pretrained. However, in our LLMOps process, as described in the “discover and tune” section, we still have the discovery process (to determine which LLM model, or models, would fit our use case), the tuning of the prompts using prompt engineering or prompt tuning, and, if necessary, and the fine-tuning of our models for domain-specific grounding.</p>
			<p>Later in this chapter, we will look at a real-life use case where LLMOps played a critical role in a large organization’s management of LLMs; however, for now, it may be beneficial to do a side-by-side comparison of the two in a chart (<em class="italic">Figure 6</em><em class="italic">.9</em>) to understand how the two relate and where they differ:</p>
			<table id="table001-4" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><strong class="bold">Traditional MLOps</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">LLMOps</strong></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Typical target audience</p>
						</td>
						<td class="No-Table-Style">
							<p>ML Engineers</p>
							<p>Data Scientists</p>
							<p>Operational Staff</p>
						</td>
						<td class="No-Table-Style">
							<p>ML Engineers</p>
							<p>Application Developers</p>
							<p>Operational Staff</p>
							<p>Data Engineers</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Assets to share, or the “deliverables.”</p>
						</td>
						<td class="No-Table-Style">
							<p>Model, data, environments, features</p>
						</td>
						<td class="No-Table-Style">
							<p>The actual LLM model, agents, plugins, prompts, chains, and APIs</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Model selection</p>
						</td>
						<td class="No-Table-Style">
							<p>Select a model version or let an <strong class="bold">automated ML</strong> (<strong class="bold">AutoML</strong>) select one</p>
							<p>See reference link at the end of this chapter on <em class="italic">What </em><em class="italic">is AutoML?</em></p>
						</td>
						<td class="No-Table-Style">
							<p>Select a pretrained foundation model that</p>
							<p>can be adapted to your</p>
							<p>use case based on model cards, benchmarks, quick evaluations, etc.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Model training</p>
						</td>
						<td class="No-Table-Style">
							<p>Train the model against selected ML algorithm(s)</p>
						</td>
						<td class="No-Table-Style">
							<p>Fine-tune an existing foundation model, use a RAG pattern, and ground against your own data or perform prompt engineering</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Model validation and metrics</p>
						</td>
						<td class="No-Table-Style">
							<p>Evaluate and validate the ML models using metrics such as Accuracy, AUC, and F1 scores</p>
							<p>Two NLP evaluation and metrics examples include BLEU or ROUGE</p>
						</td>
						<td class="No-Table-Style">
							<p>Use human feedback and/or other LLMs to evaluate prompt responses:</p>
							<p>Quality: accuracy, similarity.</p>
							<p>Harm: bias, toxicity</p>
							<p>Correctness: groundedness</p>
							<p>Cost: token per request</p>
							<p>Latency: response time</p>
							<p>Perplexity</p>
							<p>Metrics such as BLEU or ROUGE discussed in <a href="B21443_03.xhtml#_idTextAnchor052"><em class="italic">Chapter 3</em></a></p>
							<p>Popular evaluation benchmarks such as MMLU, Perplexity, ARC, HellaSwag, TruthfulQA, etc.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Model deployment</p>
						</td>
						<td class="No-Table-Style">
							<p>Allows for the packaging and deploying of an ML model via automated processes and pipelines</p>
						</td>
						<td class="No-Table-Style">
							<p>Deployments are packaged within the application and include additional components such as a vector database with the incorporation of LLM lifecycle techniques</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Model monitoring</p>
						</td>
						<td class="No-Table-Style">
							<p>Monitor for model performance</p>
							<p>Monitor for any drift in the ML model</p>
						</td>
						<td class="No-Table-Style">
							<p>Monitor the actual prompt and completions, content filtering for harmful content, prompt injection attacks, or jailbreaks (Reference <a href="B21443_08.xhtml#_idTextAnchor163"><em class="italic">Chapter 8</em></a> for additional details regarding such attacks).</p>
							<p>Also, monitor for performance and model drift</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.9 – Comparing MLOps and LLMOps</p>
			<p>Hopefully, this summarized table provides some insights into which components of MLOps and LLMOps are similar and where there are differences.</p>
			<p>You should now have<a id="_idIndexMarker634"/> a foundational knowledge of <a id="_idIndexMarker635"/>LLMOps and it’s core component, the LLM lifecycle. As mentioned earlier, while these processes and procedures may seem a bit tedious, the benefits reaped are repeatable, safe generative AI practices within your organization. Teams can achieve faster model and pipeline deployments while providing higher-quality generative AI applications and services.</p>
			<p>For that “tedious” part, there are services that can streamline the LLMOps process. One such service is known as Azure Prompt Flow.</p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor136"/>Platform – using Prompt Flow for LLMOps</h2>
			<p>Microsoft’s <strong class="bold">Azure Prompt Flow</strong> facilitates <a id="_idIndexMarker636"/>LLMOps integration for your <a id="_idIndexMarker637"/>organization, streamlining the operationalization of LLM applications and copilot development. It offers customers secure access to private data with robust controls, prompt engineering, continuous integration and deployment (CI/CD), and iterative experimentation. Additionally, it supports versioning, reproducibility, deployment, and incorporates a layer for safe and responsible AI. In this section, we will cover how Azure Prompt Flow can help you implement LLMOps processes:</p>
			<div><div><img src="img/B21443_06_10.jpg" alt="Figure 6.10 – LLMOps Azure AI Prompt Flow diagram"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.10 – LLMOps Azure AI Prompt Flow diagram</p>
			<p>Let’s describe<a id="_idIndexMarker638"/> the preceding, <em class="italic">Figure 6</em><em class="italic">.10</em>, to describe the Prompt<a id="_idIndexMarker639"/> Flow stages:</p>
			<ul>
				<li>In the top-most section, the <strong class="bold">Design and Development</strong> stage is where machine learning professionals and application developers create and develop prompts. Within this area, you work with LLMs by testing and trying out different prompts and using advanced logic and control flow to make effective prompts. With Prompt Flow, developers can make executable flows that connect LLMs, prompts, and Python tools through a clear, visualized graph.</li>
				<li>In the intermediate (middle) <strong class="bold">Evaluation and Refinement</strong> stage, you assess the prompts for factors such as usefulness, fairness, groundedness, and content safety. Here, you also establish and measure prompt quality and effectiveness using standardized metrics. Prompt flow allows you to build prompt variants and assess and compare their results through large-scale testing, using pre-built and custom evaluations.</li>
				<li>At the final stage at the bottom of the image, in the <strong class="bold">Optimization and Production</strong> stage, you can track and optimize your prompts for security and performance. You will also need to collaborate with others to get feedback. Prompt Flow can assist by launching your flow as an endpoint for real-time inference, test that endpoint with sample data, monitor telemetry for latency and continuously track performance against key evaluation metrics.</li>
			</ul>
			<p>While the preceding image is a simplified view on how to approach Prompt Flow and understand it, let’s look at Prompt Flow and trace the steps through its deployment within an organization. In the following informational graphic image, taken from the Microsoft public website, <em class="italic">LLMOps with Prompt Flow and GitHub</em> (reference link at the end of this chapter), there is a graphical description of Prompt Flow deployment activities.</p>
			<p>There are quite a <a id="_idIndexMarker640"/>few steps involved in Prompt Flow, and we will not go<a id="_idIndexMarker641"/> into too much detail here, leaving you with a link to explore this further (there is both a link to the main Microsoft website for additional documentation and the GitHub site, which has a compelling hand-on exercise in which you can follow along and learn).</p>
			<p class="IMG---Figure"> </p>
			<div><div><img src="img/B21443_06_11.jpg" alt="Figure 6.11 – A summary of the Prompt Flow CI/CD deployment sequence"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.11 – A summary of the Prompt Flow CI/CD deployment sequence</p>
			<p>As you can tell from the robustness of the preceding image, Prompt Flow empowers you and your organization to confidently develop, rigorously test, fine-tune, and deploy CI/CD flows, allowing for the creation of reliable and advanced generative AI solutions, aligned to LLMOps.</p>
			<p>In the preceding image, there are three main environments: <strong class="bold">PR</strong>, <strong class="bold">Dev</strong> and <strong class="bold">Prod</strong>. A <strong class="bold">PR</strong> environment, or <strong class="bold">pull request</strong>, is a short-lived environment containing changes that require review before being merged into the <strong class="bold">Dev</strong> and/or <strong class="bold">Prod</strong> environments. Oftentimes, the PR environment is called a <strong class="bold">test</strong> environment. You can get more detailed information on setting up PR and other environments at Review pull requests in pre-production environments.</p>
			<p>There<a id="_idIndexMarker642"/> are a number <a id="_idIndexMarker643"/>of steps in LLMOps Prompt Flow deployment:</p>
			<ul>
				<li>The initialization stage is where the LLMOps data are prepared in a stage/test environment, such as data preparation and the entire environment setup.</li>
				<li>As with any developer tools that help author CI/CD pipelines, you can then pull requests from the feature branch to the development branch, which will then execute the experimentation flow, as described in the preceding image.</li>
				<li>Once approved, the generative AI code is merged from the Dev branch into the main branch, and the same process repeats both for the Dev environments and the Prod environment, in the middle and right of the image above.</li>
				<li>All of the CI/CD processing is facilitated with the Azure Machine Learning model registry environment, which makes it easy to keep track of and organize various models, from generative AI models to traditional ML models, and this also connects to other model registries/repositories such as Hugging Face.</li>
			</ul>
			<p>The LLMOps CI/CD steps <a id="_idIndexMarker644"/>can all be managed using Azure DevOps or <a id="_idIndexMarker645"/>GitHub. There are a number of steps and details which are better understood with practice. Building this process flow using the Prompt Flow hands-on lab on our GitHub repo will give you the practice, better understanding, and experience you may need. Check out this accelerator on deploying your Prompt Flow CICD pipelines: <a href="https://github.com/microsoft/llmops-promptflow-template">https://github.com/microsoft/llmops-promptflow-template</a>.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">While we have discussed various LLMOps practices, we have not delved into the integration of autonomous agents due to the novelty of this field and the limited number of agent-based applications currently in production. Many such applications are still in the research phase. However, we anticipate that autonomous agents will soon become a significant aspect of LLMOps practices.</p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor137"/>Putting it all together</h1>
			<p>Before we arrive at the last major section of this chapter to look at an actual case study and best practices, we felt it is helpful to put all the generative AI categories together and understand how data flows from one into another and vice-versa.</p>
			<p>Earlier, we shared the CI/CD pipeline flow using Prompt Flow within the LLMOps construct. Now, we will take a macro look, beyond just the LLM, at how the LLM application development stack messages would flow across the generative AI ecosystem, using categories to organize the products and services.</p>
			<p>While we do not endorse any specific services or technology, except our employer, our goal here is to show how a typical LLM flow would appear using various generative AI toolsets/products/services. We have organized each of the workloads by category, represented in the light gray boxes, along with a few of the products or services, as examples within each category. Then, we use arrows to show how typical traffic flow would occur, from queries submitted by users to the output returned to the users, and the contextual data provided by developers to the conditioned LLM outputs. The contextual data may include fine-tuning, RAG, and other techniques that you have learned in this book, such as single-shot, few-shot, etc.:</p>
			<div><div><img src="img/B21443_06_12.jpg" alt="Figure 6.12 – LLM end-to-end flow with services"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.12 – LLM end-to-end flow with services</p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor138"/>LLMOps – case study and best practices</h1>
			<p>With a Fortune 50 company based in the US, in the professional services industry, they had already been working with AI tools and using both Azure OpenAI and Azure ML in the cloud for almost a year. This organization was expanding its successful generative AI pilot worldwide and needed a repeatable way to develop, test, and deploy LLMs for its internal employees. Below are steps we wanted to share so others can know what to expect when applying an LLMOps strategy to an already existing generative AI ecosystem within an organization:</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor139"/>LLMOps field case study</h2>
			<ul>
				<li><strong class="bold">Executive vision and LLMOps strategy</strong>: For any organization to use LLMOps/generative AI/AI successfully, leadership buy-in and support are essential for the <a id="_idIndexMarker646"/>business groups and teams to then build out a repeatable framework. We had already gone through the journey of manually deploying models, and so next, we helped the CIO and his direct staff create a solid LLMOps strategy using the guidelines that we described earlier in this chapter. We helped review the company’s most beneficial generative AI projects and provide suggestions on automating most of their processes using LLMOps to boost business performance and achievement.</li>
				<li><strong class="bold">Demos, demos and more demos</strong>: To help create the vision and ideation, we went through a number of demos which included generative AI and playing with a number of LLM models for those newer to the technology and demos on LLMOps using Prompt Flow for their ML data scientists and software developers.</li>
				<li><strong class="bold">Training</strong>: In order to fully grasp the concepts of using generative AI tools and help improve the client’s knowledge and skills, we recommended both generative AI and Azure OpenAI training for those newer to generative AI subject and help ensure this customer’s internal teams are skilled and informed about the technologies they will be using, operationalizing and managing. This also included custom-created LLMOps training as well for the developer teams and training on Microsoft Semantic Kernel, as both LLMOps and SK were very new to the organization. They were eager to use an orchestration platform to be more agile in their generative AI approach while reducing the cumbersome management of the large technical stack they had already deployed. Semantic Kernel and LLMOps allowed for a more refined generative AI deployment methodology.</li>
				<li><strong class="bold">Hands-on hackathon</strong>: To establish comfort in the tools and technologies, a hands-on “hackathon” was set up, where we took a few existing business challenges where their current processes were not working on non-existing and addressed them in a large group setting over multiple days.</li>
				<li><strong class="bold">LLMOps pilots</strong>: We next assisted two different teams responsible for the development and operational support for the organization to help pilot the LLMOps strategy and processes. We took a lot of the learning, behavior and feedback and refined the process. Recall LLMOps is not only the people and technology/platforms, it is also about processes. In order to successfully implement LLMOps, we needed these various teams within the organization to define and adopt these newly agreed upon processes. Fortunately, this organization already had a well-established DevOps and Mops process in place, so adopting an <a id="_idIndexMarker647"/>LLMOps strategy and applying the processes was not a drastic disruption in business.</li>
			</ul>
			<p>In summary, this Fortune 500 organization has enjoyed the streamlined processes that LLMOps has to offer from the first design and development stage during the hackathon event to the evaluation and refinement in the final stage during the pilots</p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor140"/>LLMOps best practices</h2>
			<p>As we wrap up this <a id="_idIndexMarker648"/>final section, we know that successfully navigating the generative AI and LLM landscape requires effective practice. As this generative AI space is still fairly new and ever-growing, so are the lessons learned and the list of best practices being enhanced. We provide some guidelines to follow for some effective LLMOps practices:</p>
			<ul>
				<li><strong class="bold">Build for the enterprise and build for scalability</strong>: To ensure smooth deployment and growth, organizations should build around enterprise-ready tooling and enterprise-class infrastructure for their LLMOps requirements. Fortunately, many hyperscale cloud vendors make this very simple, as you can build your generative AI applications and services using tested and proven methodologies. Additionally, these hyperscale cloud vendors provide the proper security and guardrails to make your generative AI project a success. We will be going into the enterprise-ready, scalable environments in the next chapter.</li>
				<li><strong class="bold">Remain flexible and use agility</strong>: The world’s journey into LLMOps has just started. We did provide details of this in this chapter, yet with new innovations and challenges, it is essential to remain flexible and evolve as we have this major paradigm shift. Develop an LLMOps strategy, based on the concepts and techniques you have learned in this chapter, yet do not remain rigid as this strategy will also need to evolve as the LLM/generative AI technology evolves.</li>
				<li><strong class="bold">Focus on data quality</strong>: A data quality focus means putting resources into reliable data, applying solid data management practices, and adopting solid review practices. Organizations need to use high-quality data that is relevant, accurate, and unbiased to train and fine-tune LLMs properly. This is also incorporated into the LLM lifecycle phases you learned earlier in this chapter. Also, it is almost given that organizations use version control and deploy using standardized development tooling and clean data pipelines to prepare and manage the data, so having quality data is a must.</li>
				<li><strong class="bold">Improve experimentation while making enhancements</strong>: The LLMOps lifecycle, including <a id="_idIndexMarker649"/>LLM development and deployment, is ongoing. There is a constant demand for new data and behavior improvements and enhancements. Most all of the tooling for experimentation and making enhancements can be automated, however always keep a human-in-the-loop for the quality control and alignment with business outcomes.</li>
			</ul>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor141"/>Summary</h1>
			<p>In this chapter, we covered the basis of generative AI intersecting with software development. We covered three popular programming generative AI application frameworks: Semantic Kernel, LangChain, and LlamaIndex. We also introduced LLMOps, a comprehensive framework for managing the lifecycle of a generative AI ecosystem and how Prompt Flow can simplify the management of an LLMOps strategy; together, all of these components form a comprehensive framework for developing and deploying generative AI applications and services.</p>
			<p>We also described the lifecycle of an LLM model itself to round out the lifecycle discussion.</p>
			<p>As we look at extensibility and automating, we delved into the world of agents and autonomous agents, such as AutoGen and AutoGPT, which can work autonomously to address extremely complex problems by using a few techniques such as chaining or networking LLMs together in collaboration.</p>
			<p>Finally, we looked at an actual case study of a large organization and how they adopted LLMOps. From this, we wrapped up the chapter with some LLMOps best practices.</p>
			<p>While the landscapes of programming language frameworks, tools, and agents are constantly being enhanced on an almost daily basis, we can all agree that the concepts you have learned thus far pave the way for enterprises to embrace generative AI and LLMs and be able to manage and operationalize the tooling and process easily and at scale.</p>
			<p>Now that we have a clearer picture of how LLM models and LLM-based applications are created using programming language frameworks and made more efficient by using LLMOps, let’s slightly change our focus for the next chapter. In the next chapter, let’s expand more on the operational side of the cloud and expand our understanding of how LLM models, such as ChatGPT, are deployed at a large scale from an architecture design perspective. We will also understand the scaling strategies used in the cloud for such large deployments.</p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor142"/>References</h1>
			<ul>
				<li>Microsoft Build Session: Kevin Scott’s talk <em class="italic">The era of the AI </em><em class="italic">Copilot: </em><a href="https://build.microsoft.com/en-US/sessions/bb8f9d99-0c47-404f-8212-a85fffd3a59d">https://build.microsoft.com/en-US/sessions/bb8f9d99-0c47-404f-8212-a85fffd3a59d</a></li>
				<li><em class="italic">What is automated machine learning (</em><em class="italic">AutoML)? </em><a href="https://learn.microsoft.com/en-us/azure/machine-learning/concept-automated-ml">https://learn.microsoft.com/en-us/azure/machine-learning/concept-automated-ml</a></li>
				<li><em class="italic">LLMOps with prompt flow on </em><em class="italic">GitHub</em> <a href="https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/how-to-end-to-end-llmops-with-prompt-flow">https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/how-to-end-to-end-llmops-with-prompt-flow</a></li>
				<li><em class="italic">Review pull requests in pre-production </em><em class="italic">environments: </em><a href="https://learn.microsoft.com/en-us/azure/static-web-apps/review-publish-pull-requests">https://learn.microsoft.com/en-us/azure/static-web-apps/review-publish-pull-requests</a></li>
				<li><em class="italic">Technological singularity </em><em class="italic">defined</em>, Wikipedia.<a href="https://en.wikipedia.org/wiki/Technological_singularity">https://en.wikipedia.org/wiki/Technological_singularity</a></li>
				<li><em class="italic">Architecting AI Apps with Semantic </em><em class="italic">Kernel</em>: <a href="https://devblogs.microsoft.com/semantic-kernel/architecting-ai-apps-with-semantic-kernel/">https://devblogs.microsoft.com/semantic-kernel/architecting-ai-apps-with-semantic-kernel/</a></li>
				<li><em class="italic">Azure OpenAI Assistants function </em><em class="italic">calling</em>: <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/assistant-functions">https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/assistant-functions</a></li>
				<li><em class="italic">An Introduction to LLMOps: Operationalizing and Managing Large Language Models using Azure ML (</em><em class="italic">microsoft.com)</em>: <a href="https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/an-introduction-to-llmops-operationalizing-and-managing-large/ba-p/3910996">https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/an-introduction-to-llmops-operationalizing-and-managing-large/ba-p/3910996</a></li>
				<li><em class="italic">What is </em><em class="italic">LLMOPs?</em> <a href="https://www.databricks.com/glossary/llmops">https://www.databricks.com/glossary/llmops</a></li>
				<li><em class="italic">Azure Prompt Flow CICD </em><em class="italic">Template</em>: <a href="https://github.com/microsoft/llmops-promptflow-template">https://github.com/microsoft/llmops-promptflow-template</a></li>
			</ul>
		</div>
	</body></html>