- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Handling Large Datasets for LLM Training
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理LLM训练中的大规模数据集
- en: In this chapter, you’ll learn advanced techniques for managing and processing
    massive datasets essential for training state-of-the-art LLMs. We’ll explore the
    unique challenges posed by large-scale language datasets and provide you with
    practical solutions to overcome them.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习管理和处理大规模数据集的高级技术，这对于训练最先进的LLMs至关重要。我们将探讨大规模语言数据集带来的独特挑战，并提供解决这些挑战的实际方案。
- en: The aim of this chapter is to equip you with the knowledge and tools to efficiently
    handle data at scale, enabling you to train more powerful and effective LLMs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是让你掌握处理大规模数据的知识和工具，从而能够训练更强大、更有效的LLMs。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Challenges of large datasets
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据集的挑战
- en: Data sampling techniques
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据采样技术
- en: Distributed data processing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式数据处理
- en: Data sharding and parallelization strategies
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分片和并行化策略
- en: Efficient data storage formats
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效的数据存储格式
- en: Streaming data processing for continuous LLM training
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式数据处理以实现持续的LLM训练
- en: Memory-efficient data loading techniques
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存高效的数据加载技术
- en: Challenges of large datasets
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据集的挑战
- en: 'Training LLMs requires enormous datasets, often in the terabytes or even petabytes
    range. This scale introduces several challenges:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 训练LLMs需要巨大的数据集，通常在千兆或甚至太字节范围内。这种规模引入了几个挑战：
- en: '**Storage requirements**: Datasets can exceed the capacity of single machines,
    necessitating distributed storage solutions.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储需求**：数据集可能超过单台机器的容量，需要分布式存储解决方案。'
- en: '**Input/output (I/O) bottlenecks**: Reading large volumes of data can become
    a significant bottleneck, limiting training speed.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入/输出（I/O）瓶颈**：读取大量数据可能成为显著的瓶颈，限制训练速度。'
- en: '**Preprocessing overhead**: Tokenization and other preprocessing steps can
    be time-consuming at scale due to the computational overhead of processing large
    volumes of text data through multiple sequential operations. The challenge arises
    from having to perform multiple steps on each piece of text – tokenization, normalization,
    cleaning, language detection, and other transformations – multiplied across millions
    or billions of text samples. This process is inherently sequential (each step
    depends on the previous one), requires CPU/memory resources, and can involve complex
    operations such as **regular expressions** (**regexes**), dictionary lookups,
    and language-specific rules. When dealing with multilingual or code-mixed data,
    the complexity increases further as different language rules need to be applied,
    and additional steps such as script normalization or language detection are required
    for each text segment, making the preprocessing pipeline a significant bottleneck
    in large-scale **natural language processing** (**NLP**) systems.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理开销**：由于处理大量文本数据需要通过多个顺序操作的计算开销，分词和其他预处理步骤在规模上可能耗时。挑战来自于需要对每段文本执行多个步骤——分词、规范化、清理、语言检测以及其他转换——这些步骤在数百万或数十亿个文本样本上成倍增加。这个过程本质上是顺序的（每一步都依赖于前一步），需要CPU/内存资源，并可能涉及复杂的操作，如**正则表达式**（**regexes**）、字典查找和特定语言规则。当处理多语言或代码混合数据时，复杂性进一步增加，因为需要应用不同的语言规则，并且需要对每个文本段进行额外的步骤，如脚本规范化或语言检测，这使得预处理管道成为大规模**自然语言处理**（**NLP**）系统的一个显著瓶颈。'
- en: '**Memory constraints**: Loading entire datasets into memory is often infeasible,
    requiring streaming or batching approaches.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存限制**：将整个数据集加载到内存中通常是不切实际的，需要流式或批处理方法。'
- en: '**Data quality and diversity**: Ensuring dataset quality and representativeness
    becomes more challenging as size increases.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据质量和多样性**：随着数据集规模的增加，确保数据集质量和代表性变得更加困难。'
- en: 'To address these challenges, we need to employ sophisticated data-handling
    techniques. Let’s explore a Python implementation using the **Datasets** library
    from Hugging Face, which is designed to handle large-scale datasets efficiently:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，我们需要采用复杂的数据处理技术。让我们通过使用Hugging Face的**Datasets**库的Python实现来探索这些技术，该库旨在高效地处理大规模数据集：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this code, we use the Datasets library to efficiently load and process a
    large dataset (in this case, the `C4` dataset). The `num_proc` parameter specifies
    the number of processor cores to use for parallel processing in the dataset mapping
    operation. When preprocessing large datasets, using multiple CPU cores through
    parallel processing can significantly speed up the operation. For example, if
    `num_proc=4`, the preprocessing function will be executed on four processor cores
    simultaneously, processing different batches of data in parallel rather than sequentially.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们使用 Datasets 库高效地加载和处理大型数据集（在这种情况下，是 `C4` 数据集）。`num_proc` 参数指定用于数据集映射操作中的并行处理所使用的处理器核心数。在预处理大型数据集时，通过并行处理使用多个CPU核心可以显著加快操作速度。例如，如果
    `num_proc=4`，则预处理函数将在四个处理器核心上同时执行，并行处理不同的数据批次，而不是顺序处理。
- en: To better understand the context in which large datasets are used, it is helpful
    to explore a specific example. One such dataset used in the preceding code snippet
    is the **Colossal Clean Crawled Corpus** (**C4**) dataset, which plays a significant
    role in training modern LLMs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解大型数据集的使用上下文，探索一个具体的例子很有帮助。前述代码片段中使用的一个此类数据集是 **Colossal Clean Crawled
    Corpus** （**C4**）数据集，它在现代LLM的训练中发挥着重要作用。
- en: The **C4** dataset is a massive, cleaned web-crawled text corpus created by
    Google for training LLMs. Containing approximately 750 GB of English-language
    text, C4 is derived from Common Crawl data and has undergone extensive filtering
    to remove duplicates, non-English content, and offensive material. It comes in
    several variants, including a standard cleaned version, an unfiltered version,
    and a subset focused on news-like content. While publicly available, accessing
    C4 requires some effort, typically through Google Cloud Storage or libraries such
    as Hugging Face datasets. Despite its cleaning process, C4 still has some limitations
    regarding content quality and potential biases, which researchers should consider
    when using it for model training. Nevertheless, it remains a valuable resource
    for NLP tasks and has been instrumental in training prominent models such as **Text-to-Text
    Transfer Transformer** (**T5**) and **Language Model for Dialogue** **Applications**
    (**LaMDA**).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**C4** 数据集是由Google创建的一个庞大的、经过清洗的网页爬取文本语料库，用于训练LLM。包含大约750 GB的英语文本，C4是从Common
    Crawl数据中提取的，并经过广泛的过滤以去除重复内容、非英语内容和冒犯性材料。它有几个变体，包括标准清洗版本、未过滤版本以及专注于新闻类内容的子集。虽然公开可用，但访问C4需要一些努力，通常通过Google
    Cloud Storage或Hugging Face datasets等库进行。尽管经过清洗过程，C4在内容质量和潜在偏差方面仍存在一些局限性，研究人员在使用它进行模型训练时应予以考虑。尽管如此，它仍然是NLP任务中的一个宝贵资源，并在训练像
    **Text-to-Text Transfer Transformer** （**T5**）和 **Language Model for Dialogue
    Applications** （**LaMDA**）这样的突出模型中发挥了关键作用。'
- en: We employ streaming to avoid loading the entire dataset into memory at once.
    The `num_proc` parameter is set to the number of physical CPU cores to maximize
    parallel processing efficiency.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用流式处理以避免一次性将整个数据集加载到内存中。`num_proc` 参数设置为物理CPU核心数，以最大化并行处理效率。
- en: The `preprocess_function` function is where you implement dataset-specific preprocessing
    logic. This function is applied in parallel across the dataset, significantly
    speeding up preprocessing for large datasets.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`preprocess_function` 函数是您实现特定数据集预处理逻辑的地方。此函数在数据集上并行应用，显著加快了大型数据集的预处理速度。'
- en: 'You can also use a GPU for the task. See the following code example (keep in
    mind that while GPU-based preprocessing is particularly beneficial for operations
    such as tokenization and embedding generation, it may not significantly accelerate
    simpler text manipulations):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以使用GPU来完成这项任务。请参阅以下代码示例（请注意，虽然基于GPU的预处理在诸如标记化、嵌入生成等操作中特别有益，但它可能不会显著加速简单的文本操作）：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This code uses the PyTorch and Hugging Face libraries to process a dataset (for
    example, C4) with GPU acceleration. It employs a data loader for efficient batch
    processing, moves data to GPU memory, and uses a pre-trained tokenizer. The main
    GPU benefits come from parallel batch processing and GPU-accelerated tokenization.
    While this setup enables GPU usage, the most significant GPU advantages typically
    occur during model training or inference rather than preprocessing.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码使用PyTorch和Hugging Face库以GPU加速处理数据集（例如，C4）。它使用数据加载器进行高效的批量处理，将数据移动到GPU内存，并使用预训练的标记化器。主要的GPU优势来自并行批量处理和GPU加速的标记化。虽然这种设置使得GPU的使用成为可能，但最大的GPU优势通常出现在模型训练或推理期间，而不是预处理期间。
- en: Data sampling techniques
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据采样技术
- en: Data sampling is a practical approach to reducing the size of large datasets
    without sacrificing representativeness. Several techniques exist, each with specific
    use cases and trade-offs. **Random sampling** selects data points uniformly at
    random from the dataset. It is simple and effective when the data is independently
    and identically distributed, but it may miss important subgroups if the data is
    imbalanced. **Systematic sampling** selects every *k*th item from a list after
    a random starting point. It is more structured than random sampling and can be
    useful when the data is ordered in a meaningful way, though it risks introducing
    bias if the ordering aligns with hidden periodic patterns. **Reservoir sampling**
    is designed for streaming of unknown-size datasets. It maintains a fixed-size
    sample while iterating through the data sequentially and ensures that every item
    has an equal probability of being included. This is particularly useful in online
    or incremental learning scenarios where data arrives in continuous flows.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 数据采样是一种在不牺牲代表性情况下减少大型数据集大小的实用方法。存在几种技术，每种技术都有特定的用例和权衡。**随机采样**从数据集中均匀随机选择数据点。当数据独立同分布时，它简单有效，但如果数据不平衡，可能会错过重要的子群体。**系统采样**在随机起点后从列表中选择每*k*个项。它比随机采样更有结构，当数据以有意义的方式排序时可能很有用，但如果排序与隐藏的周期性模式一致，则可能引入偏差。**蓄水池采样**是为未知大小数据集的流式传输设计的。它在顺序遍历数据的同时维护一个固定大小的样本，并确保每个项目都有相等的机会被包含。这在数据以连续流形式到达的在线或增量学习场景中特别有用。
- en: Due to the length constraints of this chapter, we focus only on **stratified
    sampling**, a technique that preserves the proportional representation of subgroups
    within a dataset. It is especially suitable when certain attributes—such as label
    classes, sentence lengths, or metadata categories—are known to affect model performance
    and need to be maintained in the sampled subset. In NLP, text length is a common
    stratification variable, given its impact on model input dynamics.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本章篇幅限制，我们仅关注**分层采样**，这是一种在数据集中保持子群体比例表示的技术。它特别适用于某些属性（如标签类别、句子长度或元数据类别）已知会影响模型性能并需要在采样子集中保持的情况。在NLP中，文本长度是一个常见的分层变量，因为它对模型输入动态有影响。
- en: 'The following implementation demonstrates how to apply stratified sampling
    based on text length. It divides the dataset into percentile-based strata and
    samples proportionally from each stratum to create a subset that retains the length
    distribution of the full dataset:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下实现演示了如何根据文本长度应用分层采样。它将数据集划分为基于百分比的层级，并从每个层级按比例采样以创建一个子集，该子集保留了整个数据集的长度分布：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This stratified sampling technique ensures that we maintain a representative
    distribution of text lengths in our sampled dataset. We use 10 strata (`num_strata=10`)
    to balance granularity and computational efficiency. Adjust this value based on
    your specific dataset characteristics and sampling requirements.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分层采样技术确保我们在采样数据集中保持文本长度的代表性分布。我们使用10个层级（`num_strata=10`）来平衡粒度和计算效率。根据您特定的数据集特征和采样要求调整此值。
- en: As datasets grow in size and complexity, single-machine processing becomes a
    bottleneck in both speed and scalability. Techniques such as data sampling offer
    partial relief, but they do not resolve the computational limitations inherent
    in centralized architectures. To address these constraints, the next section introduces
    distributed data processing, where computation is spread across multiple machines
    or nodes to improve throughput, reduce latency, and support parallel workflows
    required for large-scale LLM training pipelines.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据集规模和复杂性的增长，单机处理在速度和可扩展性方面都成为瓶颈。数据采样等技术可以提供部分缓解，但它们不能解决集中式架构固有的计算限制。为了解决这些限制，下一节介绍了分布式数据处理，其中计算分布在多个机器或节点上以提高吞吐量、降低延迟并支持大规模LLM训练管道所需的并行工作流程。
- en: Distributed data processing
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式数据处理
- en: For truly massive datasets, distributed processing becomes necessary. Here’s
    an example using **Dask**, a flexible library for parallel computing in Python
    ([https://www.dask.org/](https://www.dask.org/)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于真正庞大的数据集，分布式处理变得必要。以下是一个使用**Dask**的例子，这是一个用于Python并行计算的灵活库（[https://www.dask.org/](https://www.dask.org/))。
- en: Dask and Apache Spark are both distributed computing frameworks, but their main
    differences lie in their architecture and use cases. Spark is built around the
    concept of **resilient distributed datasets** (**RDDs**) and requires a cluster
    setup, making it ideal for large-scale production data processing. Dask, on the
    other hand, is designed to integrate seamlessly with the Python ecosystem and
    can scale from a single laptop to a cluster, using familiar APIs that mirror NumPy,
    pandas, and scikit-learn. While Spark excels at batch processing of massive datasets,
    Dask is more flexible for interactive computing and scientific workflows, particularly
    when working with Python-native libraries and when you need to scale up existing
    Python code with minimal modifications.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Dask和Apache Spark都是分布式计算框架，但它们的主要区别在于其架构和用例。Spark围绕**弹性分布式数据集**（**RDDs**）的概念构建，需要集群设置，使其非常适合大规模生产数据处理。另一方面，Dask旨在无缝集成到Python生态系统，可以从单个笔记本电脑扩展到集群，使用与NumPy、pandas和scikit-learn相似的API。虽然Spark在处理大规模数据集的批处理方面表现出色，但Dask在交互式计算和科学工作流程方面更加灵活，尤其是在使用Python原生库或需要以最小修改扩展现有Python代码时。
- en: 'Let’s get back to our code:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的代码：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this example, we use Dask to distribute the preprocessing workload across
    multiple machines or cores. The `num_partitions` parameter (set to `100`) determines
    the level of parallelism and should be adjusted based on your available computational
    resources and dataset size.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用Dask将预处理工作负载分布到多台机器或核心上。`num_partitions`参数（设置为`100`）决定了并行化级别，应根据您的可用计算资源和数据集大小进行调整。
- en: Data sharding and parallelization strategies
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分片和并行化策略
- en: '**Data sharding** refers to the technique of breaking up a large dataset into
    smaller, more manageable pieces, known as “shards,” which are then distributed
    across multiple machines or storage systems. Each shard can be processed independently,
    making it easier to handle large datasets, especially those that don’t fit into
    the memory of a single machine. This approach is widely used in machine learning
    to distribute the processing of large datasets, thereby allowing for the training
    of larger models or faster computation.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据分片**是指将大型数据集分解成更小、更易于管理的块，称为“分片”，然后分布到多台机器或存储系统中的技术。每个分片可以独立处理，这使得处理大型数据集变得更容易，尤其是那些不适合单台机器内存的数据集。这种方法在机器学习中广泛使用，用于分配大型数据集的处理，从而允许训练更大的模型或进行更快的计算。'
- en: Data sharding enables more efficient use of computational resources as each
    shard can be processed independently, and the results can be aggregated later.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分片使得计算资源的使用更加高效，因为每个分片可以独立处理，并且结果可以在之后进行汇总。
- en: However, careful consideration must be given to ensuring that the sharding strategy
    maintains the integrity and representativeness of the data distribution across
    all shards to avoid biases or inconsistencies in the trained model.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，必须仔细考虑确保分片策略保持所有分片间数据分布的完整性和代表性，以避免训练模型中的偏差或不一致性。
- en: 'Here’s an example of a sharding strategy:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个分片策略的例子：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This sharding strategy uses a hash function to distribute data items across
    shards. The `num_shards` parameter (set to `10`) should be adjusted based on your
    infrastructure and parallelization needs.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分片策略使用哈希函数将数据项分布到各个分片中。`num_shards`参数（设置为`10`）应根据您的基础设施和并行化需求进行调整。
- en: The `shard_data` function distributes items from a dataset into a specified
    number of shards by applying a consistent hashing scheme based on each item’s
    unique identifier. It initializes a list of empty lists, each representing a shard,
    and for every item in the input dataset, it calculates a shard index using the
    `'id'` field. The hash output is converted to an integer and taken modulo the
    number of shards to ensure uniform distribution across shards. This method guarantees
    that items with the same ID are consistently mapped to the same shard across executions,
    which is useful for tasks such as distributed storage or parallel processing where
    determinism and balance are important.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`shard_data`函数通过应用基于每个项目唯一标识符的一致性哈希方案，将数据集中的项目分配到指定的分片数量中。它初始化一个空列表的列表，每个列表代表一个分片，对于输入数据集中的每个项目，它使用`''id''`字段计算一个分片索引。哈希输出被转换为整数，并取模以确保在分片之间均匀分布。这种方法保证了具有相同ID的项目在执行过程中始终映射到相同的分片，这对于分布式存储或并行处理等任务很有用，在这些任务中，确定性和平衡很重要。'
- en: 'Sharding strategies are chosen based on the nature of the data and expected
    query patterns, with each approach offering distinct trade-offs in scalability,
    performance, and complexity:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 分片策略的选择基于数据的性质和预期的查询模式，每种方法在可扩展性、性能和复杂性方面都提供了不同的权衡：
- en: '**Hash sharding**: Suitable for uniformly distributed data by mapping keys
    through a hash function to distribute load evenly across shards'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**哈希分片**：通过哈希函数映射键以均匀分布负载，适用于均匀分布的数据'
- en: '**Range sharding**: Effective for ordered datasets such as time-series logs,
    where each shard holds a contiguous range of data values'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**范围分片**：适用于有序数据集，如时间序列日志，其中每个分片包含连续的数据值范围'
- en: '**Geographic sharding**: Designed to optimize location-based queries by partitioning
    data according to geographical regions'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**地理分片**：通过根据地理区域分区数据来优化基于位置的查询'
- en: '**Key-value sharding**: Enables manual control of hotspots by assigning specific
    key ranges or values to defined shards'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**键值分片**：通过将特定的键范围或值分配给定义的分片，允许手动控制热点'
- en: '**Directory-based sharding**: Supports dynamic shard allocation using a lookup
    service to determine data placement, adapting to changes in data distribution'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于目录的分片**：通过使用查找服务来确定数据放置以支持动态分片分配，适应数据分布的变化'
- en: '**Consistent hashing**: Minimizes data movement when the number of shards changes,
    maintaining stability and reducing rebalancing overhead'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致性哈希**：当分片数量变化时最小化数据移动，保持稳定性并减少重新平衡开销'
- en: '**Round-robin sharding**: Distributes data sequentially across shards, providing
    simplicity but poor performance for range-based queries'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轮询分片**：按顺序将数据分配到各个分片中，提供简单性但范围查询性能较差'
- en: '**Workload-based sharding**: Balances access load by assigning high-traffic
    data to separate shards based on observed query patterns'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于工作负载的分片**：通过根据观察到的查询模式将高流量数据分配到单独的分片中来平衡访问负载'
- en: '**Composite sharding**: Combines multiple strategies to support complex systems
    with diverse data types and query needs'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复合分片**：结合多种策略以支持具有多种数据类型和查询需求复杂系统的支持'
- en: '**Tag-based sharding**: Categorizes data based on labels such as user roles
    or data categories, supporting domain-specific partitioning strategies'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于标签的分片**：根据用户角色或数据类别等标签对数据进行分类，支持特定领域的分区策略'
- en: 'For the preceding code block, we can also define the following function as
    the main orchestrator to process and aggregate shards:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前面的代码块，我们还可以定义以下函数作为主要协调器来处理和聚合分片：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `process_with_sharding` function takes a dataset represented as a list of
    dictionaries and divides it into a specified number of shards using the `shard_data`
    function. It then uses `ProcessPoolExecutor` with as many workers as shards to
    process each shard concurrently by applying the `process_shard` function in parallel.
    After all shards have been processed, it aggregates the individual results from
    each shard into a single list by iterating over the processed shards and extending
    the final result list with their contents.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`process_with_sharding` 函数接收一个表示为字典列表的数据集，并使用 `shard_data` 函数将其划分为指定数量的分片。然后，它使用
    `ProcessPoolExecutor` 并行处理每个分片，每个分片使用 `process_shard` 函数。处理完所有分片后，通过遍历处理过的分片并将它们的内容扩展到最终结果列表中，将每个分片的单个结果聚合到一个列表中。'
- en: Once data has been effectively partitioned and distributed for parallel processing,
    attention must then turn to how it is physically stored and accessed—bringing
    us to the choice of efficient storage formats.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被有效地分区并分配以进行并行处理，就必须关注其物理存储和访问方式——这引出了高效存储格式的选择。
- en: Efficient data storage formats
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高效的数据存储格式
- en: Choosing the right storage format can significantly impact data loading and
    processing speed.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的存储格式可以显著影响数据加载和处理速度。
- en: As an example, we can use **Apache Parquet** ([https://parquet.apache.org/](https://parquet.apache.org/)),
    a columnar storage format that’s particularly efficient for large datasets.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用 **Apache Parquet** ([https://parquet.apache.org/](https://parquet.apache.org/))，这是一种特别适用于大型数据集的列式存储格式。
- en: 'Here’s a table comparing different column formats and their characteristics
    for storing large language datasets:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个比较不同列格式及其在存储大型语言数据集特性方面的表格：
- en: '| **Feature** | **CSV** | **JSON** | **Apache Parquet** | **Apache Arrow**
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| **特性** | **CSV** | **JSON** | **Apache Parquet** | **Apache Arrow** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Storage type** | Row-based | Row-based | Columnar | Columnar |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| **存储类型** | 行式 | 行式 | 列式 | 列式 |'
- en: '| **Compression** | Basic | Poor | Excellent | Excellent |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| **压缩** | 基础 | 差 | 优秀 | 优秀 |'
- en: '| **Query speed** | Slow | Slow | Fast | Very fast |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| **查询速度** | 慢 | 慢 | 快 | 非常快 |'
- en: '| **Nested structures** | No | Yes | Yes | Yes |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| **嵌套结构** | 否 | 是 | 是 | 是 |'
- en: '| **Schema support** | No | Limited | Yes | Yes |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| **模式支持** | 否 | 有限 | 是 | 是 |'
- en: '| **Random access** | Poor | Poor | Good | Excellent |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| **随机访问** | 差 | 差 | 好 | 优秀 |'
- en: '| **Memory efficiency** | Poor | Poor | Good | Excellent |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| **内存效率** | 差 | 差 | 好 | 优秀 |'
- en: '| **Python integration** | Simple | Simple | Good (via PyArrow) | Native |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| **Python 集成** | 简单 | 简单 | 好（通过 PyArrow） | 原生 |'
- en: '| **Typical** **use case** | Small datasets | API responses | Large analytics
    | In-memory processing |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| **典型** **用例** | 小数据集 | API 响应 | 大数据分析 | 内存处理 |'
- en: '| **Loading speed** | Slow | Medium | Fast | Very fast |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| **加载速度** | 慢 | 中等 | 快 | 非常快 |'
- en: '| **NLP** **feature support** | Basic | Good | Excellent | Excellent |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| **NLP** **特征支持** | 基础 | 好 | 优秀 | 优秀 |'
- en: '| **Cross-platform** | Yes | Yes | Yes | Yes |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| **跨平台** | 是 | 是 | 是 | 是 |'
- en: '| **Metadata support** | No | Limited | Yes | Yes |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| **元数据支持** | 不支持 | 有限 | 支持 | 支持 |'
- en: Table 4.1 – Characteristics of different column formats
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.1 – 不同列格式的特性
- en: This table highlights why Parquet is often preferred for LLM datasets due to
    its columnar storage format, efficient compression, and strong support for complex
    data structures commonly found in NLP tasks.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 此表突出了为什么 Parquet 由于其列式存储格式、高效的压缩和强大的对 NLP 任务中常见的复杂数据结构支持，通常被首选用于 LLM 数据集。
- en: 'Here’s an example of how data is typically structured in Apache Parquet columns
    for an NLP dataset:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个示例，说明数据通常如何在 Apache Parquet 列中为 NLP 数据集结构化：
- en: '| **Column Name** | **Data Type** | **Example Values** |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| **列名** | **数据类型** | **示例值** |'
- en: '| --- | --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `text_id` | Integer | `1, 2,` `3, 4` |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| `text_id` | 整数 | `1, 2,` `3, 4` |'
- en: '| `Text` | String | `"This is sample text", "``Another example"` |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| `文本` | 字符串 | `"This is sample text", "``Another example"` |'
- en: '| `Tokens` | List[String] | `["This", "is", "sample", "text"], ["``Another",
    "example"]` |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| `标记` | 字符串列表 | `["This", "is", "sample", "text"], ["``Another", "example"]`
    |'
- en: '| `Embeddings` | List[Float] | `[0.1, 0.2, 0.3], [0.4,` `0.5, 0.6]` |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| `嵌入` | 浮点数列表 | `[0.1, 0.2, 0.3], [0.4,` `0.5, 0.6]` |'
- en: '| `Metadata` | Struct | `{"lang": "en", "source": "web"}, {"lang": "fr", "``source":
    "news"}` |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| `元数据` | 结构体 | `{"lang": "en", "source": "web"}, {"lang": "fr", "``source":
    "news"}` |'
- en: '| `Labels` | Integer | `1, 0,` `1, 0` |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| `标签` | 整数 | `1, 0,` `1, 0` |'
- en: '| `Timestamp` | Timestamp | `2024-01-01 10:30:00,` `2024-01-01 10:31:00` |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| `时间戳` | 时间戳 | `2024-01-01 10:30:00,` `2024-01-01 10:31:00` |'
- en: '| `language_score` | Float | `0.95,` `0.87, 0.92` |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| `language_score` | 浮点数 | `0.95,` `0.87, 0.92` |'
- en: '| `Entities` | List[Struct] | `[{"text": "Google", "type": "ORG"}, {"text":
    "New York", "``type": "LOC"}]` |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| `实体` | 结构体列表 | `[{"text": "Google", "type": "ORG"}, {"text": "New York",
    "``type": "LOC"}]` |'
- en: '| `doc_stats` | Struct | `{"word_count": 150, "char_count": 750, "``sentence_count":
    8}` |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| `doc_stats` | 结构体 | `{"word_count": 150, "char_count": 750, "``sentence_count":
    8}` |'
- en: Table 4.2 – Structure of data in Apache Parquet columns
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.2 – Apache Parquet 列中的数据结构
- en: Each column is stored separately and can be efficiently compressed and accessed
    independently, which is particularly useful for large-scale NLP processing.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 每一列数据都单独存储，并且可以高效地压缩和独立访问，这对于大规模自然语言处理尤其有用。
- en: 'The following code snippet uses the PyArrow library to convert a dataset represented
    as a list of Python dictionaries into a Parquet file and read it back:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段使用 PyArrow 库将表示为 Python 字典列表的数据集转换为 Parquet 文件，并将其读回：
- en: '[PRE6]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the preceding snippet, the `convert_to_parquet` function takes a dataset
    and an output file path, converts the first dictionary in the dataset to a PyArrow
    Table using `pa.Table.from_pydict`, and writes it to a Parquet file with `pq.write_table`.
    The `read_from_parquet` function reads a Parquet file from the specified path
    into a PyArrow Table using `pq.read_table` and then converts it back into a Python
    dictionary using `table.to_pydict`. In the usage example, a variable `large_dataset`
    is serialized to `"large_dataset.parquet"` and then deserialized back into `loaded_dataset`.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，`convert_to_parquet` 函数接收一个数据集和一个输出文件路径，使用 `pa.Table.from_pydict`
    将数据集中的第一个字典转换为 PyArrow 表，并使用 `pq.write_table` 将其写入 Parquet 文件。`read_from_parquet`
    函数使用 `pq.read_table` 从指定路径读取 Parquet 文件到 PyArrow 表，然后使用 `table.to_pydict` 将其转换回
    Python 字典。在用法示例中，一个变量 `large_dataset` 被序列化为 `"large_dataset.parquet"`，然后反序列化回
    `loaded_dataset`。
- en: 'Parquet offers several advantages for LLM datasets:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 为 LLM 数据集提供了几个优势：
- en: Columnar storage for efficient querying
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列存储以实现高效查询
- en: Compression to reduce storage requirements
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 压缩以减少存储需求
- en: Support for complex nested structures common in NLP data
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持在 NLP 数据中常见的复杂嵌套结构
- en: While previous sections have addressed methods for managing large-scale static
    datasets through sampling, distributed computation, and optimized storage strategies,
    these approaches assume a finite and well-defined corpus. However, training scenarios
    increasingly involve continuous inflow of data, such as user interactions, real-time
    telemetry, or evolving content streams. These dynamic contexts require a shift
    from traditional data pipelines to architectures capable of handling real-time
    ingestion and processing. The following section introduces streaming data processing
    as a necessary evolution for sustaining long-context, adaptive training regimes
    in LLMs.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前面的部分已经讨论了通过采样、分布式计算和优化存储策略来管理大规模静态数据集的方法，但这些方法假设语料库是有限且定义良好的。然而，训练场景越来越多地涉及数据的持续流入，如用户交互、实时遥测或不断变化的内容流。这些动态环境需要从传统的数据管道转向能够处理实时摄取和处理的架构。下一节介绍了流数据处理作为在
    LLM 中维持长期、自适应训练机制所必需的演变。
- en: Streaming data processing for continuous LLM training
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于持续 LLM 训练的流数据处理
- en: For scenarios where new data is constantly being generated, streaming processing
    allows for continuous model updates. Here’s an example using **Apache Kafka**
    ([https://kafka.apache.org/](https://kafka.apache.org/)) and **Faust** ([https://faust.readthedocs.io/en/latest/](https://faust.readthedocs.io/en/latest/)).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不断生成新数据的情况，流处理允许持续更新模型。以下是一个使用 **Apache Kafka** ([https://kafka.apache.org/](https://kafka.apache.org/))
    和 **Faust** ([https://faust.readthedocs.io/en/latest/](https://faust.readthedocs.io/en/latest/))
    的示例。
- en: Apache Kafka is a distributed streaming platform that serves as the backbone
    for building real-time data pipelines and streaming applications. It uses a **publish-subscribe**
    (**pub-sub**) model where data producers send messages to topics and consumers
    read from these topics, allowing for scalable, fault-tolerant data distribution
    across multiple brokers. When combined with async processing, these technologies
    enable systems to handle massive amounts of data in real time without blocking
    operations. Multiple brokers in Kafka provide redundancy and load balancing, ensuring
    high availability and throughput. This architecture is particularly useful in
    scenarios requiring real-time data processing, such as log aggregation, metrics
    collection, stream processing, and event sourcing.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka 是一个分布式流平台，它是构建实时数据管道和流应用程序的基础。它使用 **发布-订阅** （**pub-sub**） 模型，其中数据生产者向主题发送消息，消费者从这些主题中读取，允许在多个代理之间进行可扩展、容错的数据分发。当与异步处理结合使用时，这些技术使系统能够在不阻塞操作的情况下实时处理大量数据。Kafka
    中的多个代理提供冗余和负载均衡，确保高可用性和吞吐量。这种架构在需要实时数据处理的情况下特别有用，例如日志聚合、指标收集、流处理和事件溯源。
- en: Faust, on the other hand, is a Python-based stream processing library designed
    to handle real-time data processing tasks by treating data as continuous streams
    of events. Similar to Kafka Streams but written in Python, Faust enables developers
    to build streaming applications that can process, transform, and analyze data
    in real time. It provides high-level abstractions for working with streams, making
    it easier to implement complex streaming workflows while maintaining the simplicity
    and expressiveness of Python. Faust internally uses modern Python features such
    as `async/await` and leverages the power of Python’s asyncio library for handling
    concurrent operations efficiently.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Faust 是一个基于 Python 的流处理库，旨在通过将数据视为连续的事件流来处理实时数据处理任务。与 Kafka Streams 类似，但用
    Python 编写，Faust 允许开发者构建能够实时处理、转换和分析数据的流应用程序。它提供了用于处理流的高级抽象，使得实现复杂的流工作流程变得更容易，同时保持了
    Python 的简单性和表达性。Faust 内部使用现代 Python 功能，如 `async/await`，并利用 Python 的 asyncio 库高效地处理并发操作。
- en: 'The following code defines a simple real-time data processing application using
    Faust, a Python stream processing library built on top of Kafka. It demonstrates
    how to consume messages from a Kafka topic, apply preprocessing logic, and prepare
    data for downstream tasks such as training an LM:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码定义了一个使用 Faust 的简单实时数据处理应用程序，Faust 是一个建立在 Kafka 之上的 Python 流处理库。它演示了如何从 Kafka
    主题中消费消息，应用预处理逻辑，并为下游任务（如训练 LM）准备数据：
- en: '[PRE7]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: First, the code defines a `Text` class using `faust.Record` to represent incoming
    Kafka messages, which are expected to contain a single string field called `content`.
    The Faust application is then created with the `'llm-training'` identifier, and
    it connects to a local Kafka broker running at `kafka://localhost:9092`. The application
    subscribes to a topic named `'raw-text'`, with incoming messages deserialized
    into `Text` objects.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，代码使用`faust.Record`定义了一个`Text`类，用于表示包含单个名为`content`的字符串字段的传入Kafka消息。然后，使用`'llm-training'`标识符创建Faust应用程序，并连接到运行在`kafka://localhost:9092`的本地Kafka代理。应用程序订阅名为`'raw-text'`的主题，并将传入的消息反序列化为`Text`对象。
- en: The core processing logic is implemented in the `process` function, which is
    decorated with `@app.agent(topic)`, making it a Faust agent that processes events
    from the `raw-text` topic. The function asynchronously iterates over each message
    in the stream, applies a `preprocess` function to the `content` field, and prints
    the result. Although the code currently prints the processed text, in a real-world
    setup, this is where one would typically pass the output to a language model training
    pipeline or to further processing stages.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 核心处理逻辑在`process`函数中实现，该函数使用`@app.agent(topic)`装饰器，使其成为处理来自`raw-text`主题事件的Faust代理。该函数异步遍历流中的每条消息，对`content`字段应用`preprocess`函数，并打印结果。尽管当前代码打印处理后的文本，但在实际设置中，这通常是传递输出到语言模型训练管道或进一步处理阶段的典型位置。
- en: Finally, the script includes a standard Python entry point to start the Faust
    application when the script is run directly. Note that the `preprocess` function
    is assumed to be defined elsewhere in the full implementation, as it is not included
    in the provided snippet.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，脚本包括一个标准的Python入口点，当脚本直接运行时，用于启动Faust应用程序。请注意，`preprocess`函数假定在完整实现的其他地方定义，因为它不包括在提供的代码片段中。
- en: This setup allows you to continuously process incoming text data, which can
    then be used to update your LLM in real time or near real time. The `preprocess`
    function would contain your specific preprocessing logic.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设置允许您持续处理传入的文本数据，然后可以实时或近实时地更新您的LLM。`preprocess`函数将包含您的特定预处理逻辑。
- en: Memory-efficient data loading techniques
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存高效的数据加载技术
- en: For datasets too large to fit in memory, we can use **memory mapping** or **chunking**
    techniques.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于太大而无法放入内存的数据集，我们可以使用**内存映射**或**分块**技术。
- en: Memory mapping leverages OS-level functionality to map large files directly
    into memory without loading the entire file. This enables random access to portions
    of the file, making it suitable for scenarios requiring frequent but non-sequential
    access. It is fast for large, structured datasets such as embeddings or tokenized
    text files but may have higher overhead for small, scattered reads.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 内存映射利用操作系统级别的功能，将大文件直接映射到内存中，而不需要加载整个文件。这使得对文件部分的随机访问成为可能，使其适用于需要频繁但非顺序访问的场景。对于大型、结构化数据集（例如嵌入或分词文本文件）来说，这种方法很快，但对于小而分散的读取，可能会有更高的开销。
- en: Chunking, on the other hand, divides data into smaller, sequentially processed
    chunks. This is effective for streaming large, sequentially accessed datasets
    (for example, text or logs) into memory-limited environments. While simpler and
    more portable, chunking may be slower for random access patterns compared to memory
    mapping.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，分块将数据分成更小的、顺序处理的块。这对于将大型、顺序访问的数据集（例如文本或日志）流式传输到内存受限环境中非常有效。虽然分块比内存映射简单且更易于移植，但在随机访问模式中，分块可能比内存映射慢。
- en: 'Here’s an example using NumPy’s `memmap` feature, which creates array-like
    objects that map to files on disk, permitting efficient read and write operations
    without loading the entire array into memory. The `memmap` feature leverages the
    operating system’s virtual memory capabilities to provide seamless array operations
    while minimizing memory usage:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个使用NumPy的`memmap`功能的示例，它创建类似于数组的对象，映射到磁盘上的文件，允许在不将整个数组加载到内存的情况下进行高效的读写操作。`memmap`功能利用操作系统的虚拟内存功能，在最小化内存使用的同时提供无缝的数组操作：
- en: '[PRE8]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This technique allows you to work with datasets larger than available RAM by
    keeping most of the data on disk and only loading the necessary portions into
    memory as needed.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术允许您通过将大部分数据保留在磁盘上，并在需要时仅将必要的部分加载到内存中，来处理比可用RAM更大的数据集。
- en: 'Here is an example of the chunking technique, which is particularly useful
    when working with large datasets that must be processed sequentially but do not
    fit into memory all at once. Unlike memory mapping, which allows random access,
    chunking explicitly loads and processes fixed-size blocks of data in sequence.
    This is a common pattern when dealing with large CSV files, text corpora, or streaming
    logs. In the following example, a large CSV file is processed in chunks using
    pandas, which internally reads blocks of rows into memory, minimizing the peak
    memory footprint:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个分块技术的示例，这在处理必须按顺序处理但一次无法全部装入内存的大型数据集时特别有用。与允许随机访问的内存映射不同，分块明确地按顺序加载和顺序处理固定大小的数据块。这在处理大型CSV文件、文本语料库或流日志时是一个常见的模式。在以下示例中，使用pandas以分块方式处理大型CSV文件，pandas内部将行块读入内存，最小化峰值内存占用：
- en: '[PRE9]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this example, the CSV file is read in blocks of 10,000 rows at a time. Each
    chunk is passed to a processing function, and intermediate results (in this case,
    the mean of a column named `'value'`) are stored for further aggregation or analysis.
    This approach is flexible and easily extended to tasks such as filtering, transformation,
    or writing chunked outputs to new files.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，CSV文件以每次10,000行为单位进行读取。每个块被传递到处理函数中，中间结果（在这种情况下，名为`'value'`的列的平均值）被存储以供进一步聚合或分析。这种方法是灵活的，并且可以轻松扩展到过滤、转换或将分块输出写入新文件等任务。
- en: Chunking is especially appropriate when data is accessed linearly and each chunk
    is independent. However, if random access to individual records or records across
    chunks is required, memory-mapping or indexed database solutions may be more efficient.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 分块特别适用于线性访问数据且每个块相互独立的情况。然而，如果需要随机访问单个记录或跨块记录，内存映射或索引数据库解决方案可能更有效率。
- en: Summary
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this section, we explored advanced techniques for managing and processing
    large datasets for LLM training. You learned about the challenges of large datasets,
    data sampling techniques, distributed processing, efficient storage formats, streaming
    processing, data sharding, and memory-efficient loading.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了用于LLM训练的大型数据集管理和处理的高级技术。你了解了大型数据集的挑战、数据采样技术、分布式处理、高效的存储格式、流处理、数据分片和内存高效加载。
- en: 'These techniques are essential for scaling up LLM training to massive datasets
    while maintaining efficiency and data quality, each with its own contribution
    to processing large datasets for LLMs:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术对于将LLM训练扩展到大规模数据集同时保持效率和数据质量至关重要，每种技术都对处理LLM的大数据集有自己的贡献：
- en: '**Data sampling techniques**: They reduce the computational load by focusing
    on high-impact or representative data, enhancing efficiency and ensuring quality
    without processing the entire dataset'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据采样技术**：通过关注高影响或具有代表性的数据，它们减少了计算负担，提高效率并确保质量，而无需处理整个数据集'
- en: '**Distributed processing**: Speeds up data preparation and training by parallelizing
    tasks across machines, enabling scalability for massive datasets'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式处理**：通过在机器间并行化任务，加快数据准备和训练速度，为大规模数据集提供可扩展性'
- en: '**Efficient storage formats**: They improve data retrieval speed and reduce
    storage size, streamlining access to large datasets and boosting I/O efficiency'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高效的存储格式**：它们提高了数据检索速度并减少了存储大小，简化了对大型数据集的访问并提高了I/O效率'
- en: '**Streaming processing**: Minimizes memory usage by handling data incrementally,
    supporting real-time updates and efficient processing of continuous data streams'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流处理**：通过增量处理数据，最小化内存使用，支持实时更新和连续数据流的有效处理'
- en: '**Data sharding**: Balances workloads and reduces latency by splitting data
    into smaller chunks, enabling parallelism and seamless scaling'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分片**：通过将数据分割成更小的块，平衡工作负载并减少延迟，实现并行性和无缝扩展'
- en: '**Memory-efficient loading**: Limits memory usage by loading data in manageable
    portions, ensuring efficient processing of datasets that exceed memory capacity'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存高效加载**：通过以可管理的部分加载数据，限制内存使用，确保处理超出内存容量的数据集的效率'
- en: 'In the next chapter, we will introduce another pattern: data versioning for
    LLM development.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍另一种模式：LLM开发的版本控制。
