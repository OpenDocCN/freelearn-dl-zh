- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Handling Large Datasets for LLM Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you’ll learn advanced techniques for managing and processing
    massive datasets essential for training state-of-the-art LLMs. We’ll explore the
    unique challenges posed by large-scale language datasets and provide you with
    practical solutions to overcome them.
  prefs: []
  type: TYPE_NORMAL
- en: The aim of this chapter is to equip you with the knowledge and tools to efficiently
    handle data at scale, enabling you to train more powerful and effective LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Challenges of large datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data sampling techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed data processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data sharding and parallelization strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficient data storage formats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming data processing for continuous LLM training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory-efficient data loading techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges of large datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Training LLMs requires enormous datasets, often in the terabytes or even petabytes
    range. This scale introduces several challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Storage requirements**: Datasets can exceed the capacity of single machines,
    necessitating distributed storage solutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input/output (I/O) bottlenecks**: Reading large volumes of data can become
    a significant bottleneck, limiting training speed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Preprocessing overhead**: Tokenization and other preprocessing steps can
    be time-consuming at scale due to the computational overhead of processing large
    volumes of text data through multiple sequential operations. The challenge arises
    from having to perform multiple steps on each piece of text – tokenization, normalization,
    cleaning, language detection, and other transformations – multiplied across millions
    or billions of text samples. This process is inherently sequential (each step
    depends on the previous one), requires CPU/memory resources, and can involve complex
    operations such as **regular expressions** (**regexes**), dictionary lookups,
    and language-specific rules. When dealing with multilingual or code-mixed data,
    the complexity increases further as different language rules need to be applied,
    and additional steps such as script normalization or language detection are required
    for each text segment, making the preprocessing pipeline a significant bottleneck
    in large-scale **natural language processing** (**NLP**) systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory constraints**: Loading entire datasets into memory is often infeasible,
    requiring streaming or batching approaches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data quality and diversity**: Ensuring dataset quality and representativeness
    becomes more challenging as size increases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To address these challenges, we need to employ sophisticated data-handling
    techniques. Let’s explore a Python implementation using the **Datasets** library
    from Hugging Face, which is designed to handle large-scale datasets efficiently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this code, we use the Datasets library to efficiently load and process a
    large dataset (in this case, the `C4` dataset). The `num_proc` parameter specifies
    the number of processor cores to use for parallel processing in the dataset mapping
    operation. When preprocessing large datasets, using multiple CPU cores through
    parallel processing can significantly speed up the operation. For example, if
    `num_proc=4`, the preprocessing function will be executed on four processor cores
    simultaneously, processing different batches of data in parallel rather than sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: To better understand the context in which large datasets are used, it is helpful
    to explore a specific example. One such dataset used in the preceding code snippet
    is the **Colossal Clean Crawled Corpus** (**C4**) dataset, which plays a significant
    role in training modern LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: The **C4** dataset is a massive, cleaned web-crawled text corpus created by
    Google for training LLMs. Containing approximately 750 GB of English-language
    text, C4 is derived from Common Crawl data and has undergone extensive filtering
    to remove duplicates, non-English content, and offensive material. It comes in
    several variants, including a standard cleaned version, an unfiltered version,
    and a subset focused on news-like content. While publicly available, accessing
    C4 requires some effort, typically through Google Cloud Storage or libraries such
    as Hugging Face datasets. Despite its cleaning process, C4 still has some limitations
    regarding content quality and potential biases, which researchers should consider
    when using it for model training. Nevertheless, it remains a valuable resource
    for NLP tasks and has been instrumental in training prominent models such as **Text-to-Text
    Transfer Transformer** (**T5**) and **Language Model for Dialogue** **Applications**
    (**LaMDA**).
  prefs: []
  type: TYPE_NORMAL
- en: We employ streaming to avoid loading the entire dataset into memory at once.
    The `num_proc` parameter is set to the number of physical CPU cores to maximize
    parallel processing efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: The `preprocess_function` function is where you implement dataset-specific preprocessing
    logic. This function is applied in parallel across the dataset, significantly
    speeding up preprocessing for large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use a GPU for the task. See the following code example (keep in
    mind that while GPU-based preprocessing is particularly beneficial for operations
    such as tokenization and embedding generation, it may not significantly accelerate
    simpler text manipulations):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This code uses the PyTorch and Hugging Face libraries to process a dataset (for
    example, C4) with GPU acceleration. It employs a data loader for efficient batch
    processing, moves data to GPU memory, and uses a pre-trained tokenizer. The main
    GPU benefits come from parallel batch processing and GPU-accelerated tokenization.
    While this setup enables GPU usage, the most significant GPU advantages typically
    occur during model training or inference rather than preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Data sampling techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data sampling is a practical approach to reducing the size of large datasets
    without sacrificing representativeness. Several techniques exist, each with specific
    use cases and trade-offs. **Random sampling** selects data points uniformly at
    random from the dataset. It is simple and effective when the data is independently
    and identically distributed, but it may miss important subgroups if the data is
    imbalanced. **Systematic sampling** selects every *k*th item from a list after
    a random starting point. It is more structured than random sampling and can be
    useful when the data is ordered in a meaningful way, though it risks introducing
    bias if the ordering aligns with hidden periodic patterns. **Reservoir sampling**
    is designed for streaming of unknown-size datasets. It maintains a fixed-size
    sample while iterating through the data sequentially and ensures that every item
    has an equal probability of being included. This is particularly useful in online
    or incremental learning scenarios where data arrives in continuous flows.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the length constraints of this chapter, we focus only on **stratified
    sampling**, a technique that preserves the proportional representation of subgroups
    within a dataset. It is especially suitable when certain attributes—such as label
    classes, sentence lengths, or metadata categories—are known to affect model performance
    and need to be maintained in the sampled subset. In NLP, text length is a common
    stratification variable, given its impact on model input dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following implementation demonstrates how to apply stratified sampling
    based on text length. It divides the dataset into percentile-based strata and
    samples proportionally from each stratum to create a subset that retains the length
    distribution of the full dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This stratified sampling technique ensures that we maintain a representative
    distribution of text lengths in our sampled dataset. We use 10 strata (`num_strata=10`)
    to balance granularity and computational efficiency. Adjust this value based on
    your specific dataset characteristics and sampling requirements.
  prefs: []
  type: TYPE_NORMAL
- en: As datasets grow in size and complexity, single-machine processing becomes a
    bottleneck in both speed and scalability. Techniques such as data sampling offer
    partial relief, but they do not resolve the computational limitations inherent
    in centralized architectures. To address these constraints, the next section introduces
    distributed data processing, where computation is spread across multiple machines
    or nodes to improve throughput, reduce latency, and support parallel workflows
    required for large-scale LLM training pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed data processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For truly massive datasets, distributed processing becomes necessary. Here’s
    an example using **Dask**, a flexible library for parallel computing in Python
    ([https://www.dask.org/](https://www.dask.org/)).
  prefs: []
  type: TYPE_NORMAL
- en: Dask and Apache Spark are both distributed computing frameworks, but their main
    differences lie in their architecture and use cases. Spark is built around the
    concept of **resilient distributed datasets** (**RDDs**) and requires a cluster
    setup, making it ideal for large-scale production data processing. Dask, on the
    other hand, is designed to integrate seamlessly with the Python ecosystem and
    can scale from a single laptop to a cluster, using familiar APIs that mirror NumPy,
    pandas, and scikit-learn. While Spark excels at batch processing of massive datasets,
    Dask is more flexible for interactive computing and scientific workflows, particularly
    when working with Python-native libraries and when you need to scale up existing
    Python code with minimal modifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get back to our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we use Dask to distribute the preprocessing workload across
    multiple machines or cores. The `num_partitions` parameter (set to `100`) determines
    the level of parallelism and should be adjusted based on your available computational
    resources and dataset size.
  prefs: []
  type: TYPE_NORMAL
- en: Data sharding and parallelization strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data sharding** refers to the technique of breaking up a large dataset into
    smaller, more manageable pieces, known as “shards,” which are then distributed
    across multiple machines or storage systems. Each shard can be processed independently,
    making it easier to handle large datasets, especially those that don’t fit into
    the memory of a single machine. This approach is widely used in machine learning
    to distribute the processing of large datasets, thereby allowing for the training
    of larger models or faster computation.'
  prefs: []
  type: TYPE_NORMAL
- en: Data sharding enables more efficient use of computational resources as each
    shard can be processed independently, and the results can be aggregated later.
  prefs: []
  type: TYPE_NORMAL
- en: However, careful consideration must be given to ensuring that the sharding strategy
    maintains the integrity and representativeness of the data distribution across
    all shards to avoid biases or inconsistencies in the trained model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of a sharding strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This sharding strategy uses a hash function to distribute data items across
    shards. The `num_shards` parameter (set to `10`) should be adjusted based on your
    infrastructure and parallelization needs.
  prefs: []
  type: TYPE_NORMAL
- en: The `shard_data` function distributes items from a dataset into a specified
    number of shards by applying a consistent hashing scheme based on each item’s
    unique identifier. It initializes a list of empty lists, each representing a shard,
    and for every item in the input dataset, it calculates a shard index using the
    `'id'` field. The hash output is converted to an integer and taken modulo the
    number of shards to ensure uniform distribution across shards. This method guarantees
    that items with the same ID are consistently mapped to the same shard across executions,
    which is useful for tasks such as distributed storage or parallel processing where
    determinism and balance are important.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sharding strategies are chosen based on the nature of the data and expected
    query patterns, with each approach offering distinct trade-offs in scalability,
    performance, and complexity:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hash sharding**: Suitable for uniformly distributed data by mapping keys
    through a hash function to distribute load evenly across shards'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Range sharding**: Effective for ordered datasets such as time-series logs,
    where each shard holds a contiguous range of data values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Geographic sharding**: Designed to optimize location-based queries by partitioning
    data according to geographical regions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Key-value sharding**: Enables manual control of hotspots by assigning specific
    key ranges or values to defined shards'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Directory-based sharding**: Supports dynamic shard allocation using a lookup
    service to determine data placement, adapting to changes in data distribution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consistent hashing**: Minimizes data movement when the number of shards changes,
    maintaining stability and reducing rebalancing overhead'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Round-robin sharding**: Distributes data sequentially across shards, providing
    simplicity but poor performance for range-based queries'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Workload-based sharding**: Balances access load by assigning high-traffic
    data to separate shards based on observed query patterns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Composite sharding**: Combines multiple strategies to support complex systems
    with diverse data types and query needs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tag-based sharding**: Categorizes data based on labels such as user roles
    or data categories, supporting domain-specific partitioning strategies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the preceding code block, we can also define the following function as
    the main orchestrator to process and aggregate shards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `process_with_sharding` function takes a dataset represented as a list of
    dictionaries and divides it into a specified number of shards using the `shard_data`
    function. It then uses `ProcessPoolExecutor` with as many workers as shards to
    process each shard concurrently by applying the `process_shard` function in parallel.
    After all shards have been processed, it aggregates the individual results from
    each shard into a single list by iterating over the processed shards and extending
    the final result list with their contents.
  prefs: []
  type: TYPE_NORMAL
- en: Once data has been effectively partitioned and distributed for parallel processing,
    attention must then turn to how it is physically stored and accessed—bringing
    us to the choice of efficient storage formats.
  prefs: []
  type: TYPE_NORMAL
- en: Efficient data storage formats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Choosing the right storage format can significantly impact data loading and
    processing speed.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, we can use **Apache Parquet** ([https://parquet.apache.org/](https://parquet.apache.org/)),
    a columnar storage format that’s particularly efficient for large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a table comparing different column formats and their characteristics
    for storing large language datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Feature** | **CSV** | **JSON** | **Apache Parquet** | **Apache Arrow**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Storage type** | Row-based | Row-based | Columnar | Columnar |'
  prefs: []
  type: TYPE_TB
- en: '| **Compression** | Basic | Poor | Excellent | Excellent |'
  prefs: []
  type: TYPE_TB
- en: '| **Query speed** | Slow | Slow | Fast | Very fast |'
  prefs: []
  type: TYPE_TB
- en: '| **Nested structures** | No | Yes | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| **Schema support** | No | Limited | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| **Random access** | Poor | Poor | Good | Excellent |'
  prefs: []
  type: TYPE_TB
- en: '| **Memory efficiency** | Poor | Poor | Good | Excellent |'
  prefs: []
  type: TYPE_TB
- en: '| **Python integration** | Simple | Simple | Good (via PyArrow) | Native |'
  prefs: []
  type: TYPE_TB
- en: '| **Typical** **use case** | Small datasets | API responses | Large analytics
    | In-memory processing |'
  prefs: []
  type: TYPE_TB
- en: '| **Loading speed** | Slow | Medium | Fast | Very fast |'
  prefs: []
  type: TYPE_TB
- en: '| **NLP** **feature support** | Basic | Good | Excellent | Excellent |'
  prefs: []
  type: TYPE_TB
- en: '| **Cross-platform** | Yes | Yes | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| **Metadata support** | No | Limited | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: Table 4.1 – Characteristics of different column formats
  prefs: []
  type: TYPE_NORMAL
- en: This table highlights why Parquet is often preferred for LLM datasets due to
    its columnar storage format, efficient compression, and strong support for complex
    data structures commonly found in NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of how data is typically structured in Apache Parquet columns
    for an NLP dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Column Name** | **Data Type** | **Example Values** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `text_id` | Integer | `1, 2,` `3, 4` |'
  prefs: []
  type: TYPE_TB
- en: '| `Text` | String | `"This is sample text", "``Another example"` |'
  prefs: []
  type: TYPE_TB
- en: '| `Tokens` | List[String] | `["This", "is", "sample", "text"], ["``Another",
    "example"]` |'
  prefs: []
  type: TYPE_TB
- en: '| `Embeddings` | List[Float] | `[0.1, 0.2, 0.3], [0.4,` `0.5, 0.6]` |'
  prefs: []
  type: TYPE_TB
- en: '| `Metadata` | Struct | `{"lang": "en", "source": "web"}, {"lang": "fr", "``source":
    "news"}` |'
  prefs: []
  type: TYPE_TB
- en: '| `Labels` | Integer | `1, 0,` `1, 0` |'
  prefs: []
  type: TYPE_TB
- en: '| `Timestamp` | Timestamp | `2024-01-01 10:30:00,` `2024-01-01 10:31:00` |'
  prefs: []
  type: TYPE_TB
- en: '| `language_score` | Float | `0.95,` `0.87, 0.92` |'
  prefs: []
  type: TYPE_TB
- en: '| `Entities` | List[Struct] | `[{"text": "Google", "type": "ORG"}, {"text":
    "New York", "``type": "LOC"}]` |'
  prefs: []
  type: TYPE_TB
- en: '| `doc_stats` | Struct | `{"word_count": 150, "char_count": 750, "``sentence_count":
    8}` |'
  prefs: []
  type: TYPE_TB
- en: Table 4.2 – Structure of data in Apache Parquet columns
  prefs: []
  type: TYPE_NORMAL
- en: Each column is stored separately and can be efficiently compressed and accessed
    independently, which is particularly useful for large-scale NLP processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet uses the PyArrow library to convert a dataset represented
    as a list of Python dictionaries into a Parquet file and read it back:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding snippet, the `convert_to_parquet` function takes a dataset
    and an output file path, converts the first dictionary in the dataset to a PyArrow
    Table using `pa.Table.from_pydict`, and writes it to a Parquet file with `pq.write_table`.
    The `read_from_parquet` function reads a Parquet file from the specified path
    into a PyArrow Table using `pq.read_table` and then converts it back into a Python
    dictionary using `table.to_pydict`. In the usage example, a variable `large_dataset`
    is serialized to `"large_dataset.parquet"` and then deserialized back into `loaded_dataset`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parquet offers several advantages for LLM datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: Columnar storage for efficient querying
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compression to reduce storage requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for complex nested structures common in NLP data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While previous sections have addressed methods for managing large-scale static
    datasets through sampling, distributed computation, and optimized storage strategies,
    these approaches assume a finite and well-defined corpus. However, training scenarios
    increasingly involve continuous inflow of data, such as user interactions, real-time
    telemetry, or evolving content streams. These dynamic contexts require a shift
    from traditional data pipelines to architectures capable of handling real-time
    ingestion and processing. The following section introduces streaming data processing
    as a necessary evolution for sustaining long-context, adaptive training regimes
    in LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming data processing for continuous LLM training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For scenarios where new data is constantly being generated, streaming processing
    allows for continuous model updates. Here’s an example using **Apache Kafka**
    ([https://kafka.apache.org/](https://kafka.apache.org/)) and **Faust** ([https://faust.readthedocs.io/en/latest/](https://faust.readthedocs.io/en/latest/)).
  prefs: []
  type: TYPE_NORMAL
- en: Apache Kafka is a distributed streaming platform that serves as the backbone
    for building real-time data pipelines and streaming applications. It uses a **publish-subscribe**
    (**pub-sub**) model where data producers send messages to topics and consumers
    read from these topics, allowing for scalable, fault-tolerant data distribution
    across multiple brokers. When combined with async processing, these technologies
    enable systems to handle massive amounts of data in real time without blocking
    operations. Multiple brokers in Kafka provide redundancy and load balancing, ensuring
    high availability and throughput. This architecture is particularly useful in
    scenarios requiring real-time data processing, such as log aggregation, metrics
    collection, stream processing, and event sourcing.
  prefs: []
  type: TYPE_NORMAL
- en: Faust, on the other hand, is a Python-based stream processing library designed
    to handle real-time data processing tasks by treating data as continuous streams
    of events. Similar to Kafka Streams but written in Python, Faust enables developers
    to build streaming applications that can process, transform, and analyze data
    in real time. It provides high-level abstractions for working with streams, making
    it easier to implement complex streaming workflows while maintaining the simplicity
    and expressiveness of Python. Faust internally uses modern Python features such
    as `async/await` and leverages the power of Python’s asyncio library for handling
    concurrent operations efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code defines a simple real-time data processing application using
    Faust, a Python stream processing library built on top of Kafka. It demonstrates
    how to consume messages from a Kafka topic, apply preprocessing logic, and prepare
    data for downstream tasks such as training an LM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: First, the code defines a `Text` class using `faust.Record` to represent incoming
    Kafka messages, which are expected to contain a single string field called `content`.
    The Faust application is then created with the `'llm-training'` identifier, and
    it connects to a local Kafka broker running at `kafka://localhost:9092`. The application
    subscribes to a topic named `'raw-text'`, with incoming messages deserialized
    into `Text` objects.
  prefs: []
  type: TYPE_NORMAL
- en: The core processing logic is implemented in the `process` function, which is
    decorated with `@app.agent(topic)`, making it a Faust agent that processes events
    from the `raw-text` topic. The function asynchronously iterates over each message
    in the stream, applies a `preprocess` function to the `content` field, and prints
    the result. Although the code currently prints the processed text, in a real-world
    setup, this is where one would typically pass the output to a language model training
    pipeline or to further processing stages.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the script includes a standard Python entry point to start the Faust
    application when the script is run directly. Note that the `preprocess` function
    is assumed to be defined elsewhere in the full implementation, as it is not included
    in the provided snippet.
  prefs: []
  type: TYPE_NORMAL
- en: This setup allows you to continuously process incoming text data, which can
    then be used to update your LLM in real time or near real time. The `preprocess`
    function would contain your specific preprocessing logic.
  prefs: []
  type: TYPE_NORMAL
- en: Memory-efficient data loading techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For datasets too large to fit in memory, we can use **memory mapping** or **chunking**
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Memory mapping leverages OS-level functionality to map large files directly
    into memory without loading the entire file. This enables random access to portions
    of the file, making it suitable for scenarios requiring frequent but non-sequential
    access. It is fast for large, structured datasets such as embeddings or tokenized
    text files but may have higher overhead for small, scattered reads.
  prefs: []
  type: TYPE_NORMAL
- en: Chunking, on the other hand, divides data into smaller, sequentially processed
    chunks. This is effective for streaming large, sequentially accessed datasets
    (for example, text or logs) into memory-limited environments. While simpler and
    more portable, chunking may be slower for random access patterns compared to memory
    mapping.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example using NumPy’s `memmap` feature, which creates array-like
    objects that map to files on disk, permitting efficient read and write operations
    without loading the entire array into memory. The `memmap` feature leverages the
    operating system’s virtual memory capabilities to provide seamless array operations
    while minimizing memory usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This technique allows you to work with datasets larger than available RAM by
    keeping most of the data on disk and only loading the necessary portions into
    memory as needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of the chunking technique, which is particularly useful
    when working with large datasets that must be processed sequentially but do not
    fit into memory all at once. Unlike memory mapping, which allows random access,
    chunking explicitly loads and processes fixed-size blocks of data in sequence.
    This is a common pattern when dealing with large CSV files, text corpora, or streaming
    logs. In the following example, a large CSV file is processed in chunks using
    pandas, which internally reads blocks of rows into memory, minimizing the peak
    memory footprint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the CSV file is read in blocks of 10,000 rows at a time. Each
    chunk is passed to a processing function, and intermediate results (in this case,
    the mean of a column named `'value'`) are stored for further aggregation or analysis.
    This approach is flexible and easily extended to tasks such as filtering, transformation,
    or writing chunked outputs to new files.
  prefs: []
  type: TYPE_NORMAL
- en: Chunking is especially appropriate when data is accessed linearly and each chunk
    is independent. However, if random access to individual records or records across
    chunks is required, memory-mapping or indexed database solutions may be more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we explored advanced techniques for managing and processing
    large datasets for LLM training. You learned about the challenges of large datasets,
    data sampling techniques, distributed processing, efficient storage formats, streaming
    processing, data sharding, and memory-efficient loading.
  prefs: []
  type: TYPE_NORMAL
- en: 'These techniques are essential for scaling up LLM training to massive datasets
    while maintaining efficiency and data quality, each with its own contribution
    to processing large datasets for LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data sampling techniques**: They reduce the computational load by focusing
    on high-impact or representative data, enhancing efficiency and ensuring quality
    without processing the entire dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed processing**: Speeds up data preparation and training by parallelizing
    tasks across machines, enabling scalability for massive datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficient storage formats**: They improve data retrieval speed and reduce
    storage size, streamlining access to large datasets and boosting I/O efficiency'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streaming processing**: Minimizes memory usage by handling data incrementally,
    supporting real-time updates and efficient processing of continuous data streams'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data sharding**: Balances workloads and reduces latency by splitting data
    into smaller chunks, enabling parallelism and seamless scaling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory-efficient loading**: Limits memory usage by loading data in manageable
    portions, ensuring efficient processing of datasets that exceed memory capacity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the next chapter, we will introduce another pattern: data versioning for
    LLM development.'
  prefs: []
  type: TYPE_NORMAL
