- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using Community-Shared LoRAs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To meet specific needs and generate higher fidelity images, we may need to fine-tune
    a pre-trained Stable Diffusion model, but the fine-tuning process is extremely
    slow without powerful GPUs. Even if you have all the hardware or resources on
    hand, the fine-tuned model is large, usually the same size as the original model
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, researchers from the Large Language Model (LLM) neighbor community
    developed an efficient fine-tuning method, **Low-Rank Adaptation** (**LoRA** —
    “Low” is why the “o” is in lowercase) [1]. With LoRA, the original checkpoint
    is frozen without any modification, and the tuned weight changes are stored in
    an independent file, which we usually call the LoRA file. Additionally, there
    are countless community-shared LoRAs on sites such as CIVITAI [4] and HuggingFace.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to delve into the theory of LoRA, and then introduce
    the Python way to load up LoRA into a Stable Diffusion model. We will also dissect
    a LoRA model to understand the LoRA model structure internally and create a custom
    function to load up a Stable Diffusion V1.5 LoRA.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: How does LoRA work?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using LoRA with Diffusers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying LoRA weight during loading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diving into LoRA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making a function to load LoRA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why LoRA works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, we will be able to use any community LoRA programmatically
    and also understand how and why LoRA works in Stable Diffusion.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have `Diffusers` package running in your computer, you should be able
    to execute all code in this chapter as well as the code used to load LoRA with
    Diffusers.
  prefs: []
  type: TYPE_NORMAL
- en: Diffusers use **PEFT** (**Parameter-Efficient Fine-Tuning**) [10] to manage
    the LoRA loading and offloading. PEFT is a library developed by Hugging Face that
    provides parameter-efficient ways to adapt large pre-trained models for specific
    downstream applications. The key idea behind PEFT is to fine-tune only a small
    fraction of a model’s parameters instead of fine-tuning all of them, resulting
    in significant savings in terms of computation and memory usage. This makes it
    possible to fine-tune very large models even on consumer hardware with limited
    resources. Turn to [*Chapter 21*](B21263_21.xhtml#_idTextAnchor405) for more about
    LoRA.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will need to install the PEFT package to enable Diffusers’ PEFT LoRA loading:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You can also refer to [*Chapter 2*](B21263_02.xhtml#_idTextAnchor037), if you
    encounter other execution errors from the code.
  prefs: []
  type: TYPE_NORMAL
- en: How does LoRA work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LoRA is a technique for quickly fine-tuning diffusion models, first introduced
    by Microsoft researchers in a paper by Edward J. Hu et al [1]. It works by creating
    a small, low-rank model that is adapted for a specific concept. This small model
    can be merged with the main checkpoint model to generate images similar to the
    ones used to train LoRA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use W to denote the original UNet attention weights (Q,K,V), ΔW to denote
    the fine-tuned weights from LoRA, and W′ as the merged weights. The process of
    adding LoRA to a model can be expressed like this:'
  prefs: []
  type: TYPE_NORMAL
- en: W′= W + ΔW
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to control the scale of LoRA weights, we denote the scale as α.
    Adding LoRA to a model can be expressed like this now:'
  prefs: []
  type: TYPE_NORMAL
- en: W′= W + αΔW
  prefs: []
  type: TYPE_NORMAL
- en: 'The range of α can be from `0` to `1.0` [2]. It should be fine if we set α
    slightly larger than `1.0`. The reason why LoRA is so small is that ΔW can be
    represented by two small matrices A and B, such that:'
  prefs: []
  type: TYPE_NORMAL
- en: ΔW = A B T
  prefs: []
  type: TYPE_NORMAL
- en: Where A ∈ ℝ n×d is an n × d matrix, and B ∈ ℝ m×d is an m × d matrix. The transpose
    of B denoted as B T is a d × m matrix.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if ΔW is a 6 × 8 matrix, there are a total of `48` weight numbers.
    Now, in the LoRA file, the 6 × 8 matrix can be represented by two matrices – one
    6 × 2 matrix, `12` numbers in total, and another 2 × 8 matrix, making it `16`
    numbers in total.
  prefs: []
  type: TYPE_NORMAL
- en: The total number of weights is reduced from `48` to `28`. This is why the LoRA
    file can be so small compared to the checkpoint model.
  prefs: []
  type: TYPE_NORMAL
- en: Using LoRA with Diffusers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the contributions from the open source community, loading LoRA with Python
    has never been easier. In this section, we are going to cover the solutions to
    load a LoRA model with Diffusers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following steps, we will first load the base Stable Diffusion V1.5,
    generate an image without LoRA, and then load a LoRA model called `MoXinV1` into
    the base model. We will clearly see the difference with and without the LoRA model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prepare a Stable Diffusion pipeline**: The following code will load up the
    Stable Diffusion pipeline and move the pipeline instance to VRAM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Generate an image without LoRA**: Now, let’s generate an image without LoRA
    loaded. Here, I am going to use the Stable Diffusion default v1.5 model to generate
    “a branch of flower” in a “traditional Chinese ink painting” style:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code uses a non-cherry-picked generator with `default seed:1`.
    The result is shown in *Figure 8**.1*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.1: A branch of flower without LoRA](img/B21263_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: A branch of flower without LoRA'
  prefs: []
  type: TYPE_NORMAL
- en: To be honest, the preceding image isn’t that good, and the “flower” is more
    like black ink dots.
  prefs: []
  type: TYPE_NORMAL
- en: '**Generate an image with LoRA with default settings**: Next, let’s load up
    the LoRA model to the pipeline and see what the MoXin LoRA can do to help image
    generation. Loading LoRA with default settings is just one line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Diffusers downloads the LoRA model file automatically if the model does not
    exist in your model cache.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, run the inference again with the following code (the same code used in
    *step 2*):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will have a new image with a better “flower” in the ink-painting style,
    as shown in *Figure 8**.2*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.2: A branch of flower with LoRA using the default settings](img/B21263_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: A branch of flower with LoRA using the default settings'
  prefs: []
  type: TYPE_NORMAL
- en: This time, the “flower” is more like a flower and, overall, better than the
    one without applying LoRA. However, the code in this section loads LoRA without
    a “weight” applied to it. In the next section, we will load a LoRA model with
    an arbitrary weight (or α ).
  prefs: []
  type: TYPE_NORMAL
- en: Applying a LoRA weight during loading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the *How does LoRA work?* section, we mentioned the α value used to define
    the portion of LoRA weight added to the main model. We can easily achieve this
    using Diffusers with PEFT [10].
  prefs: []
  type: TYPE_NORMAL
- en: What is PEFT? PEFT is a library developed by Hugging Face to efficiently adapt
    pre-trained models, such as **Large Language Models** (**LLMs**) and Stable Diffusion
    models, to new tasks without needing to fine-tune the whole model. PEFT is a broader
    concept representing a collection of methods aimed at efficiently fine-tuning
    LLMs. LoRA, conversely, is a specific technique that falls under the umbrella
    of PEFT.
  prefs: []
  type: TYPE_NORMAL
- en: Before the integration of PEFT, loading and managing LoRAs in Diffusers required
    a lot of custom code and hacking. To make it easier to manage multiple LoRAs with
    weight loading and offloading, Diffusers uses the PEFT library to help manage
    different adapters for inference. In PEFT, the fine-tuned parameters are called
    adapters, which is why you will see some parameters are named `adapters`. LoRA
    is one of the main adapter techniques; you can take LoRA and adapter as referring
    to the same thing through this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Loading a LoRA model with weight is simple, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we gave the LoRA weight as `0.5` to replace the default
    `1.0`. Now, you will see the generated image, as shown in *Figure 8**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3: A branch of flower with LoRA by applying a 0.5 LoRA weight](img/B21263_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: A branch of flower with LoRA by applying a 0.5 LoRA weight'
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 8**.3*, we can observe the difference after applying the `0.5`
    weight to the LoRA model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The PEFT-integrated Diffusers can also load another LoRA by reusing the same
    code we used to load the first LoRA model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, add the weight for the second LoRA model by calling the `set_adapters`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get a new image with style added from the second LoRA, as shown in
    *Figure 8**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4: A branch of flower with two LoRA models](img/B21263_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: A branch of flower with two LoRA models'
  prefs: []
  type: TYPE_NORMAL
- en: We can also use the same code to load LoRA for Stable Diffusion XL pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'With PEFT, we don’t need to restart the pipeline to disable LoRA; we can disable
    all LoRAs with simply one line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Note that the implementation of LoRA loading is somewhat different compared
    with other tools, such as A1111 Stable Diffusion WebUI. Using the same prompt,
    the same settings, and the same LoRA weight, you may get a different result.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t worry – in the next section, we are going to dive into the LoRA model
    internally and implement a solution to use LoRA that outputs the same result,
    with tools such as A1111 Stable Diffusion WebUI.
  prefs: []
  type: TYPE_NORMAL
- en: Diving into the internal structure of LoRA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding how LoRA works internally will help us to implement our own LoRA-related
    features based on specific needs. In this section, we are going to dive into the
    internals of LoRA’s structure and its weights schema, and then manually load the
    LoRA model into the Stable Diffusion model step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we discussed at the beginning of the chapter, applying LoRA is as simple
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: W′= W + αΔW
  prefs: []
  type: TYPE_NORMAL
- en: 'And ΔW can be broken down into A and B:'
  prefs: []
  type: TYPE_NORMAL
- en: ΔW = A B T
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the overall idea of merging LoRA weights to the checkpoint model works
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Find the A and B weight matrix from the LoRA file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Match the LoRA module layer name to the checkpoint module layer name so that
    we know which matrix to merge.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Produce ΔW = A B T.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the checkpoint model weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you have prior experience training a LoRA model, you might be aware that
    a hyperparameter, `alpha`, can be set to a value greater than `1`, such as `4`.
    This is often done in conjunction with setting another parameter, `rank`, to `4`
    as well. However, α used in this context is typically less than 1\. The actual
    value of α is generally computed using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: α =  alpha _ rank
  prefs: []
  type: TYPE_NORMAL
- en: During the training phase, setting both `alpha` and `rank` to `4` will yield
    an α value of `1`. This concept may seem confusing if not properly understood.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s explore the internals of a LoRA model step by step.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the A and B weight matrix from the LoRA file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before start exploring the internals of a LoRA structure, you will need to
    download a LoRA file. You can download the `MoXinV1.safetensors` from the following
    URL: [https://huggingface.co/andrewzhu/MoXinV1/resolve/main/MoXinV1.safetensors](https://huggingface.co/andrewzhu/MoXinV1/resolve/main/MoXinV1.safetensors).'
  prefs: []
  type: TYPE_NORMAL
- en: 'After setting up the LoRA file in the `.safetensors` format, load it using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'When LoRA weights are applied to the text encoder, the key names start with
    `lora_te_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'When LoRA weights are applied to UNet, key names start with `lora_unet_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is of the `string` type. Here are the meanings of the terms that
    appeared in the output LoRA weight keys:'
  prefs: []
  type: TYPE_NORMAL
- en: The `lora_te_` prefix says that the weights are applied to the text encoder;
    `lora_unet_` says that the weights aim at updating the Stable Diffusion `unet`
    module layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`down_blocks_0_attentions_1_proj_in` is the layer name, which should exist
    in the checkpoint model `unet` modules too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.alpha` is the trained weight set to denote how much of the LoRA weight will
    be applied to the main checkpoint model. It holds a float value that is denoted
    as α in W′= W + αΔW. Since the value will be replaced by user input, we can skip
    this value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lora_down.weight` denotes the value of this layer that represents A.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lora_up.weight` denotes the value of this layer that represents B.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `down` in `down_blocks` denotes the downside (the left side of UNet)
    of the `unet` model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following Python code will get the LoRA layer info and also have the model
    object handler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '`key` holds the LoRA module layer name, and `layer_infos` holds the checkpoint
    model layer name extracted from the LoRA layers. The reason we do this is that
    not all layers from the checkpoint model have LoRA weights to adjust, which is
    why we need to get the list of layers that will be updated.'
  prefs: []
  type: TYPE_NORMAL
- en: Finding the corresponding checkpoint model layer name
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Print out the structure of the checkpoint model `unet` structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the module is stored in a tree structure like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Each line is composed of a module name (`down_blocks`), and the module content
    can be `ModuleList` or a specific neural network layer, `Conv2d`. These are the
    components of the UNet. For now, applying LoRA to a specific UNet module isn''t
    required. However, it''s important to understand the UNet''s internal structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The loop-through part is a bit tricky. When looking back to the checkpoint
    model structure, which is layered as a tree, we can’t simply use a `for` loop
    to loop through the list. Instead, we need to use a `while` loop to navigate every
    leaf of the tree. The overall process is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`layer_infos.pop(0)` will return the first name of the list in the `string`
    type and remove it from the list such as `up` from the `layer_infos` list – `[''up'',
    ''blocks'', ''3'', ''attentions'', ''2'', ''transformer'', ''blocks'', ''0'',
    ''ff'', ''``net'', ''2'']`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `curr_layer.__getattr__(temp_name)` to check whether the layer exists or
    not. If it does not exist, an exception will be thrown, and the program will move
    to the `exception` section to continue outputting names from the `layer_infos`
    list and check again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the layer is found but some names are still left in the `layer_infos` list,
    they will keep on popping out.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The names will continue to pop out until no exception is thrown out and we meet
    the `len(layer_infos) == 0` condition, which means that the layer is fully matched.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, the `curr_layer` object points to the checkpoint model weight
    data and can be referenced in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Updating the checkpoint model weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For easier key value referencing, let’s make a `pair_keys = []` list, in which
    `pair_keys[0]` returns the A matrix and `pair_keys[1]` returns the B matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we update the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The `alpha * torch.mm(weight_up, weight_down)` code is the core code used to
    implement αA B T.
  prefs: []
  type: TYPE_NORMAL
- en: And that’s it! Now, the pipeline’s text encoder and `unet` model weights are
    updated by LoRA. Next, let’s put all the parts together to create a full-featured
    function that can load a LoRA model into the Stable Diffusion pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Making a function to load LoRA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s add one more list to store keys that have been visited and put all the
    preceding code together into a function named `load_lora`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'To use the function is easy; simply provide the `pipeline` object, the LoRA
    path, `lora_path`, and the LoRA weight number, `lora_weight`, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s try it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'It works, and works well; see the result shown in *Figure 8**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5: A branch of flower using the custom LoRA loader](img/B21263_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: A branch of flower using the custom LoRA loader'
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering, “Why does a small LoRA file possess such formidable
    capabilities?” Let’s delve deeper into the reasons why a LoRA model is effective.
  prefs: []
  type: TYPE_NORMAL
- en: Why LoRA works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The paper *Intrinsic Dimensionality Explains the Effectiveness of Language
    Model Fine-Tuning* [8] by Armen et al. found that the pre-trained representations’
    intrinsic dimension is way lower than expected, stated by them as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: “We empirically show that common NLP tasks within the context of pre-trained
    representations have an intrinsic dimension several orders of magnitudes less
    than the full parameterization.”
  prefs: []
  type: TYPE_NORMAL
- en: The intrinsic dimension of a matrix is a concept used to determine the effective
    number of dimensions required to represent the important information contained
    within that matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s suppose we have a matrix, `M`, with five rows and three columns, like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Each row of this matrix represents a data point or a vector with three values.
    We can think of these vectors as points in a three-dimensional space. However,
    if we visualize these points, we might find that they lie approximately on a two-dimensional
    plane, rather than occupying the full three-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the intrinsic dimension of the matrix, `M`, would be `2`, indicating
    that the essential structure of the data can be captured effectively using two
    dimensions. The third dimension doesn’t provide much additional information.
  prefs: []
  type: TYPE_NORMAL
- en: A low intrinsic dimension matrix can be represented by two low-rank matrices
    because the data in the matrix can be compressed into a few key features. These
    features can then be represented by two smaller matrices, each of which has a
    rank that is equal to the intrinsic dimension of the original matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'The paper *LoRA: Low-Rank Adaptation of Large Language Models* [1] by Edward
    J. Hu et al goes a step further, introducing the concept of LoRA to leverage the
    low intrinsic dimension nature, boosting the fine-tuning process by breaking down
    the delta weights to two low-rank parts, ΔW = A B T.'
  prefs: []
  type: TYPE_NORMAL
- en: The effectiveness of LoRA was soon discovered to extend beyond LLM models, also
    yielding good results with diffusion models. Simo Ryu published the LoRA [2] code
    and was the first one to try out LoRA training for Stable Diffusion. That was
    in July 2023 and there are now more than 40,000 LoRA models shared at [https://www.civitai.com](https://www.civitai.com).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how to enhance the Stable Diffusion model using
    LoRA, understood what LoRA is, and why it is good for fine-tuning and inference.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we began loading LoRA using the experimental functions from the `Diffusers`
    package and provided LoRA weights through a custom implementation. We used simple
    code to quickly understand what LoRA can bring to the table.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we dived into the internal structure of a LoRA model, walked through the
    detailed steps to extract LoRA weights, and understood how to merge those weights
    into the checkpoint model.
  prefs: []
  type: TYPE_NORMAL
- en: Further, we implemented a function in Python that can load a LoRA safetensors
    file and perform weight merges.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we briefly explored why LoRA works, based on the most recent papers
    from researchers.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to explore another powerful technique – textual
    inversion – to teach a model new “words,” and then use the pre-trained “words”
    to add new concepts to the generated images.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Edward J. et al, LoRA: Low-Rank Adaptation of Large Language Models: [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Simo Ryu (cloneofsimo), `lora`: [https://github.com/cloneofsimo/lora](https://github.com/cloneofsimo/lora)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kohya_lora_loader`: [https://gist.github.com/takuma104/e38d683d72b1e448b8d9b3835f7cfa44](https://gist.github.com/takuma104/e38d683d72b1e448b8d9b3835f7cfa44)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'CIVITAI: [https://www.civitai.com](https://www.civitai.com)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Rinon Gal et al, An Image is Worth One Word: Personalizing Text-to-Image Generation
    using Textual Inversion: [https://textual-inversion.github.io/](https://textual-inversion.github.io/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Diffusers’ `lora_state_dict` function: [https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/modeling_utils.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/modeling_utils.py)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Andrew Zhu, Improving Diffusers Package for High-Quality Image Generation:
    [https://towardsdatascience.com/improving-diffusers-package-for-high-quality-image-generation-a50fff04bdd4](https://towardsdatascience.com/improving-diffusers-package-for-high-quality-image-generation-a50fff04bdd4)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Armen et al, Intrinsic Dimensionality Explains the Effectiveness of Language
    Model Fine-Tuning: [https://arxiv.org/abs/2012.13255](https://arxiv.org/abs/2012.13255)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hugging Face, LoRA: [https://huggingface.co/docs/diffusers/training/lora](https://huggingface.co/docs/diffusers/training/lora)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hugging Face, PEFT: [https://huggingface.co/docs/peft/en/index](https://huggingface.co/docs/peft/en/index)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
