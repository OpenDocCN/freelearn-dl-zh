- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using Community-Shared LoRAs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To meet specific needs and generate higher fidelity images, we may need to fine-tune
    a pre-trained Stable Diffusion model, but the fine-tuning process is extremely
    slow without powerful GPUs. Even if you have all the hardware or resources on
    hand, the fine-tuned model is large, usually the same size as the original model
    file.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, researchers from the Large Language Model (LLM) neighbor community
    developed an efficient fine-tuning method, **Low-Rank Adaptation** (**LoRA** —
    “Low” is why the “o” is in lowercase) [1]. With LoRA, the original checkpoint
    is frozen without any modification, and the tuned weight changes are stored in
    an independent file, which we usually call the LoRA file. Additionally, there
    are countless community-shared LoRAs on sites such as CIVITAI [4] and HuggingFace.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to delve into the theory of LoRA, and then introduce
    the Python way to load up LoRA into a Stable Diffusion model. We will also dissect
    a LoRA model to understand the LoRA model structure internally and create a custom
    function to load up a Stable Diffusion V1.5 LoRA.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: How does LoRA work?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using LoRA with Diffusers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying LoRA weight during loading
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diving into LoRA
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making a function to load LoRA
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why LoRA works
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, we will be able to use any community LoRA programmatically
    and also understand how and why LoRA works in Stable Diffusion.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have `Diffusers` package running in your computer, you should be able
    to execute all code in this chapter as well as the code used to load LoRA with
    Diffusers.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Diffusers use **PEFT** (**Parameter-Efficient Fine-Tuning**) [10] to manage
    the LoRA loading and offloading. PEFT is a library developed by Hugging Face that
    provides parameter-efficient ways to adapt large pre-trained models for specific
    downstream applications. The key idea behind PEFT is to fine-tune only a small
    fraction of a model’s parameters instead of fine-tuning all of them, resulting
    in significant savings in terms of computation and memory usage. This makes it
    possible to fine-tune very large models even on consumer hardware with limited
    resources. Turn to [*Chapter 21*](B21263_21.xhtml#_idTextAnchor405) for more about
    LoRA.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'We will need to install the PEFT package to enable Diffusers’ PEFT LoRA loading:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can also refer to [*Chapter 2*](B21263_02.xhtml#_idTextAnchor037), if you
    encounter other execution errors from the code.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: How does LoRA work?
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LoRA is a technique for quickly fine-tuning diffusion models, first introduced
    by Microsoft researchers in a paper by Edward J. Hu et al [1]. It works by creating
    a small, low-rank model that is adapted for a specific concept. This small model
    can be merged with the main checkpoint model to generate images similar to the
    ones used to train LoRA.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use W to denote the original UNet attention weights (Q,K,V), ΔW to denote
    the fine-tuned weights from LoRA, and W′ as the merged weights. The process of
    adding LoRA to a model can be expressed like this:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: W′= W + ΔW
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to control the scale of LoRA weights, we denote the scale as α.
    Adding LoRA to a model can be expressed like this now:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: W′= W + αΔW
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'The range of α can be from `0` to `1.0` [2]. It should be fine if we set α
    slightly larger than `1.0`. The reason why LoRA is so small is that ΔW can be
    represented by two small matrices A and B, such that:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: ΔW = A B T
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Where A ∈ ℝ n×d is an n × d matrix, and B ∈ ℝ m×d is an m × d matrix. The transpose
    of B denoted as B T is a d × m matrix.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: For example, if ΔW is a 6 × 8 matrix, there are a total of `48` weight numbers.
    Now, in the LoRA file, the 6 × 8 matrix can be represented by two matrices – one
    6 × 2 matrix, `12` numbers in total, and another 2 × 8 matrix, making it `16`
    numbers in total.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: The total number of weights is reduced from `48` to `28`. This is why the LoRA
    file can be so small compared to the checkpoint model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Using LoRA with Diffusers
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the contributions from the open source community, loading LoRA with Python
    has never been easier. In this section, we are going to cover the solutions to
    load a LoRA model with Diffusers.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following steps, we will first load the base Stable Diffusion V1.5,
    generate an image without LoRA, and then load a LoRA model called `MoXinV1` into
    the base model. We will clearly see the difference with and without the LoRA model:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '**Prepare a Stable Diffusion pipeline**: The following code will load up the
    Stable Diffusion pipeline and move the pipeline instance to VRAM:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Generate an image without LoRA**: Now, let’s generate an image without LoRA
    loaded. Here, I am going to use the Stable Diffusion default v1.5 model to generate
    “a branch of flower” in a “traditional Chinese ink painting” style:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The preceding code uses a non-cherry-picked generator with `default seed:1`.
    The result is shown in *Figure 8**.1*:'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.1: A branch of flower without LoRA](img/B21263_08_01.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: A branch of flower without LoRA'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: To be honest, the preceding image isn’t that good, and the “flower” is more
    like black ink dots.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '**Generate an image with LoRA with default settings**: Next, let’s load up
    the LoRA model to the pipeline and see what the MoXin LoRA can do to help image
    generation. Loading LoRA with default settings is just one line of code:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Diffusers downloads the LoRA model file automatically if the model does not
    exist in your model cache.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, run the inference again with the following code (the same code used in
    *step 2*):'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We will have a new image with a better “flower” in the ink-painting style,
    as shown in *Figure 8**.2*:'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.2: A branch of flower with LoRA using the default settings](img/B21263_08_02.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: A branch of flower with LoRA using the default settings'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: This time, the “flower” is more like a flower and, overall, better than the
    one without applying LoRA. However, the code in this section loads LoRA without
    a “weight” applied to it. In the next section, we will load a LoRA model with
    an arbitrary weight (or α ).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Applying a LoRA weight during loading
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the *How does LoRA work?* section, we mentioned the α value used to define
    the portion of LoRA weight added to the main model. We can easily achieve this
    using Diffusers with PEFT [10].
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: What is PEFT? PEFT is a library developed by Hugging Face to efficiently adapt
    pre-trained models, such as **Large Language Models** (**LLMs**) and Stable Diffusion
    models, to new tasks without needing to fine-tune the whole model. PEFT is a broader
    concept representing a collection of methods aimed at efficiently fine-tuning
    LLMs. LoRA, conversely, is a specific technique that falls under the umbrella
    of PEFT.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Before the integration of PEFT, loading and managing LoRAs in Diffusers required
    a lot of custom code and hacking. To make it easier to manage multiple LoRAs with
    weight loading and offloading, Diffusers uses the PEFT library to help manage
    different adapters for inference. In PEFT, the fine-tuned parameters are called
    adapters, which is why you will see some parameters are named `adapters`. LoRA
    is one of the main adapter techniques; you can take LoRA and adapter as referring
    to the same thing through this chapter.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'Loading a LoRA model with weight is simple, as shown in the following code:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In the preceding code, we gave the LoRA weight as `0.5` to replace the default
    `1.0`. Now, you will see the generated image, as shown in *Figure 8**.3*:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3: A branch of flower with LoRA by applying a 0.5 LoRA weight](img/B21263_08_03.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: A branch of flower with LoRA by applying a 0.5 LoRA weight'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 8**.3*, we can observe the difference after applying the `0.5`
    weight to the LoRA model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'The PEFT-integrated Diffusers can also load another LoRA by reusing the same
    code we used to load the first LoRA model:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then, add the weight for the second LoRA model by calling the `set_adapters`
    function:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We will get a new image with style added from the second LoRA, as shown in
    *Figure 8**.4*:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4: A branch of flower with two LoRA models](img/B21263_08_04.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: A branch of flower with two LoRA models'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: We can also use the same code to load LoRA for Stable Diffusion XL pipelines.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'With PEFT, we don’t need to restart the pipeline to disable LoRA; we can disable
    all LoRAs with simply one line of code:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note that the implementation of LoRA loading is somewhat different compared
    with other tools, such as A1111 Stable Diffusion WebUI. Using the same prompt,
    the same settings, and the same LoRA weight, you may get a different result.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Don’t worry – in the next section, we are going to dive into the LoRA model
    internally and implement a solution to use LoRA that outputs the same result,
    with tools such as A1111 Stable Diffusion WebUI.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Diving into the internal structure of LoRA
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding how LoRA works internally will help us to implement our own LoRA-related
    features based on specific needs. In this section, we are going to dive into the
    internals of LoRA’s structure and its weights schema, and then manually load the
    LoRA model into the Stable Diffusion model step by step.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'As we discussed at the beginning of the chapter, applying LoRA is as simple
    as the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: W′= W + αΔW
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'And ΔW can be broken down into A and B:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: ΔW = A B T
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the overall idea of merging LoRA weights to the checkpoint model works
    like this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Find the A and B weight matrix from the LoRA file.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Match the LoRA module layer name to the checkpoint module layer name so that
    we know which matrix to merge.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Produce ΔW = A B T.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the checkpoint model weights.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you have prior experience training a LoRA model, you might be aware that
    a hyperparameter, `alpha`, can be set to a value greater than `1`, such as `4`.
    This is often done in conjunction with setting another parameter, `rank`, to `4`
    as well. However, α used in this context is typically less than 1\. The actual
    value of α is generally computed using the following formula:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: α =  alpha _ rank
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: During the training phase, setting both `alpha` and `rank` to `4` will yield
    an α value of `1`. This concept may seem confusing if not properly understood.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s explore the internals of a LoRA model step by step.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Finding the A and B weight matrix from the LoRA file
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before start exploring the internals of a LoRA structure, you will need to
    download a LoRA file. You can download the `MoXinV1.safetensors` from the following
    URL: [https://huggingface.co/andrewzhu/MoXinV1/resolve/main/MoXinV1.safetensors](https://huggingface.co/andrewzhu/MoXinV1/resolve/main/MoXinV1.safetensors).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'After setting up the LoRA file in the `.safetensors` format, load it using
    the following code:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'When LoRA weights are applied to the text encoder, the key names start with
    `lora_te_`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'When LoRA weights are applied to UNet, key names start with `lora_unet_`:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output is of the `string` type. Here are the meanings of the terms that
    appeared in the output LoRA weight keys:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: The `lora_te_` prefix says that the weights are applied to the text encoder;
    `lora_unet_` says that the weights aim at updating the Stable Diffusion `unet`
    module layers.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`down_blocks_0_attentions_1_proj_in` is the layer name, which should exist
    in the checkpoint model `unet` modules too.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.alpha` is the trained weight set to denote how much of the LoRA weight will
    be applied to the main checkpoint model. It holds a float value that is denoted
    as α in W′= W + αΔW. Since the value will be replaced by user input, we can skip
    this value.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lora_down.weight` denotes the value of this layer that represents A.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lora_up.weight` denotes the value of this layer that represents B.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `down` in `down_blocks` denotes the downside (the left side of UNet)
    of the `unet` model.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following Python code will get the LoRA layer info and also have the model
    object handler:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Python代码将获取LoRA层信息，并具有模型对象处理器：
- en: '[PRE33]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`key` holds the LoRA module layer name, and `layer_infos` holds the checkpoint
    model layer name extracted from the LoRA layers. The reason we do this is that
    not all layers from the checkpoint model have LoRA weights to adjust, which is
    why we need to get the list of layers that will be updated.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`key`持有LoRA模块层名称，而`layer_infos`持有从LoRA层中提取的检查点模型层名称。我们这样做的原因是检查点模型中并非所有层都有LoRA权重进行调整，因此我们需要获取将要更新的层的列表。'
- en: Finding the corresponding checkpoint model layer name
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 找到相应的检查点模型层名称
- en: 'Print out the structure of the checkpoint model `unet` structure:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 打印出检查点模型`unet`结构：
- en: '[PRE34]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We can see that the module is stored in a tree structure like this:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到模块是以这样的树状结构存储的：
- en: '[PRE35]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Each line is composed of a module name (`down_blocks`), and the module content
    can be `ModuleList` or a specific neural network layer, `Conv2d`. These are the
    components of the UNet. For now, applying LoRA to a specific UNet module isn''t
    required. However, it''s important to understand the UNet''s internal structure:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 每行由一个模块名称（`down_blocks`）组成，模块内容可以是`ModuleList`或特定的神经网络层，`Conv2d`。这些都是UNet的组成部分。目前，将LoRA应用于特定的UNet模块不是必需的。然而，了解UNet的内部结构很重要：
- en: '[PRE36]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The loop-through part is a bit tricky. When looking back to the checkpoint
    model structure, which is layered as a tree, we can’t simply use a `for` loop
    to loop through the list. Instead, we need to use a `while` loop to navigate every
    leaf of the tree. The overall process is as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 循环部分有点棘手。当回顾检查点模型结构，它以分层的形式作为树时，我们不能简单地使用`for`循环来遍历列表。相反，我们需要使用`while`循环来导航树的每个叶子。整个过程如下：
- en: '`layer_infos.pop(0)` will return the first name of the list in the `string`
    type and remove it from the list such as `up` from the `layer_infos` list – `[''up'',
    ''blocks'', ''3'', ''attentions'', ''2'', ''transformer'', ''blocks'', ''0'',
    ''ff'', ''``net'', ''2'']`'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`layer_infos.pop(0)`将返回列表中的第一个名称，并将其从列表中移除，例如从`layer_infos`列表中移除`up` – `[''up'',
    ''blocks'', ''3'', ''attentions'', ''2'', ''transformer'', ''blocks'', ''0'',
    ''ff'', ''``net'', ''2'']`'
- en: Use `curr_layer.__getattr__(temp_name)` to check whether the layer exists or
    not. If it does not exist, an exception will be thrown, and the program will move
    to the `exception` section to continue outputting names from the `layer_infos`
    list and check again.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`curr_layer.__getattr__(temp_name)`来检查层是否存在。如果不存在，将抛出异常，程序将移动到`exception`部分继续输出`layer_infos`列表中的名称，并再次检查。
- en: If the layer is found but some names are still left in the `layer_infos` list,
    they will keep on popping out.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果找到了层，但`layer_infos`列表中仍有剩余的名称，它们将继续弹出。
- en: The names will continue to pop out until no exception is thrown out and we meet
    the `len(layer_infos) == 0` condition, which means that the layer is fully matched.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 名称将继续出现，直到没有抛出异常，并且我们遇到`len(layer_infos) == 0`条件，这意味着层已完全匹配。
- en: At this point, the `curr_layer` object points to the checkpoint model weight
    data and can be referenced in the next step.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，`curr_layer`对象指向检查点模型权重数据，可以在下一步中进行引用。
- en: Updating the checkpoint model weights
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新检查点模型权重
- en: 'For easier key value referencing, let’s make a `pair_keys = []` list, in which
    `pair_keys[0]` returns the A matrix and `pair_keys[1]` returns the B matrix:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于键值引用，让我们创建一个`pair_keys = []`列表，其中`pair_keys[0]`返回A矩阵，`pair_keys[1]`返回B矩阵：
- en: '[PRE37]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Then, we update the weights:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们更新权重：
- en: '[PRE38]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The `alpha * torch.mm(weight_up, weight_down)` code is the core code used to
    implement αA B T.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`alpha * torch.mm(weight_up, weight_down)`代码是用于实现αA B T的核心代码。'
- en: And that’s it! Now, the pipeline’s text encoder and `unet` model weights are
    updated by LoRA. Next, let’s put all the parts together to create a full-featured
    function that can load a LoRA model into the Stable Diffusion pipeline.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！现在，管道的文本编码器和`unet`模型权重已经通过LoRA更新。接下来，让我们将所有部分组合起来，创建一个功能齐全的函数，可以将LoRA模型加载到Stable
    Diffusion管道中。
- en: Making a function to load LoRA
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写一个加载LoRA的函数
- en: 'Let’s add one more list to store keys that have been visited and put all the
    preceding code together into a function named `load_lora`:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再添加一个列表来存储已访问的键，并将所有前面的代码组合到一个名为`load_lora`的函数中：
- en: '[PRE39]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'To use the function is easy; simply provide the `pipeline` object, the LoRA
    path, `lora_path`, and the LoRA weight number, `lora_weight`, like this:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用该函数很简单；只需提供`pipeline`对象、LoRA路径`lora_path`和LoRA权重编号`lora_weight`，如下所示：
- en: '[PRE40]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now, let’s try it out:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们试试看：
- en: '[PRE41]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'It works, and works well; see the result shown in *Figure 8**.5*:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 它确实有效，效果很好；请参见*图8**.5*中所示的结果：
- en: '![Figure 8.5: A branch of flower using the custom LoRA loader](img/B21263_08_05.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5：使用自定义LoRA加载器的花朵分支](img/B21263_08_05.jpg)'
- en: 'Figure 8.5: A branch of flower using the custom LoRA loader'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5：使用自定义LoRA加载器的花朵分支
- en: You might be wondering, “Why does a small LoRA file possess such formidable
    capabilities?” Let’s delve deeper into the reasons why a LoRA model is effective.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道，“为什么一个小的LoRA文件具有如此强大的能力？”让我们深入探讨LoRA模型有效的原因。
- en: Why LoRA works
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么LoRA有效
- en: 'The paper *Intrinsic Dimensionality Explains the Effectiveness of Language
    Model Fine-Tuning* [8] by Armen et al. found that the pre-trained representations’
    intrinsic dimension is way lower than expected, stated by them as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Armen等人撰写的论文*内禀维度解释了语言模型微调的有效性* [8]发现，预训练表示的内禀维度远低于预期，他们如下所述：
- en: “We empirically show that common NLP tasks within the context of pre-trained
    representations have an intrinsic dimension several orders of magnitudes less
    than the full parameterization.”
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: “我们通过实验表明，在预训练表示的上下文中，常见的NLP任务具有比完整参数化低几个数量级的内禀维度。”
- en: The intrinsic dimension of a matrix is a concept used to determine the effective
    number of dimensions required to represent the important information contained
    within that matrix.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的内禀维度是一个用于确定表示该矩阵中包含的重要信息所需的有效维数的概念。
- en: 'Let’s suppose we have a matrix, `M`, with five rows and three columns, like
    this:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个矩阵`M`，有五行三列，如下所示：
- en: '[PRE42]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Each row of this matrix represents a data point or a vector with three values.
    We can think of these vectors as points in a three-dimensional space. However,
    if we visualize these points, we might find that they lie approximately on a two-dimensional
    plane, rather than occupying the full three-dimensional space.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵的每一行代表一个包含三个值的数据点或向量。我们可以将这些向量视为三维空间中的点。然而，如果我们可视化这些点，我们可能会发现它们大约位于一个二维平面上，而不是占据整个三维空间。
- en: In this case, the intrinsic dimension of the matrix, `M`, would be `2`, indicating
    that the essential structure of the data can be captured effectively using two
    dimensions. The third dimension doesn’t provide much additional information.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，矩阵`M`的内禀维度将是`2`，这意味着可以使用两个维度有效地捕捉数据的本质结构。第三个维度没有提供太多额外的信息。
- en: A low intrinsic dimension matrix can be represented by two low-rank matrices
    because the data in the matrix can be compressed into a few key features. These
    features can then be represented by two smaller matrices, each of which has a
    rank that is equal to the intrinsic dimension of the original matrix.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 一个低内禀维度的矩阵可以通过两个低秩矩阵来表示，因为矩阵中的数据可以被压缩到几个关键特征。然后，这些特征可以通过两个较小的矩阵来表示，每个矩阵的秩等于原始矩阵的内禀维度。
- en: 'The paper *LoRA: Low-Rank Adaptation of Large Language Models* [1] by Edward
    J. Hu et al goes a step further, introducing the concept of LoRA to leverage the
    low intrinsic dimension nature, boosting the fine-tuning process by breaking down
    the delta weights to two low-rank parts, ΔW = A B T.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Edward J. Hu等人撰写的论文*LoRA：大型语言模型的低秩自适应* [1]更进一步，引入了LoRA的概念，利用低内禀维度的特性，通过将权重差分解为两个低秩部分来加速微调过程，ΔW
    = A B T。
- en: The effectiveness of LoRA was soon discovered to extend beyond LLM models, also
    yielding good results with diffusion models. Simo Ryu published the LoRA [2] code
    and was the first one to try out LoRA training for Stable Diffusion. That was
    in July 2023 and there are now more than 40,000 LoRA models shared at [https://www.civitai.com](https://www.civitai.com).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 很快发现LoRA的有效性不仅限于LLM模型，还与扩散模型结合产生了良好的结果。Simo Ryu发布了LoRA [2]代码，并成为第一个尝试对Stable
    Diffusion进行LoRA训练的人。那是在2023年7月，现在在[https://www.civitai.com](https://www.civitai.com)上共享了超过40,000个LoRA模型。
- en: Summary
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed how to enhance the Stable Diffusion model using
    LoRA, understood what LoRA is, and why it is good for fine-tuning and inference.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何使用LoRA增强Stable Diffusion模型，理解了LoRA是什么，以及为什么它对微调和推理有益。
- en: Then, we began loading LoRA using the experimental functions from the `Diffusers`
    package and provided LoRA weights through a custom implementation. We used simple
    code to quickly understand what LoRA can bring to the table.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们开始使用`Diffusers`包中的实验函数加载LoRA，并通过自定义实现提供LoRA权重。我们使用简单的代码快速了解LoRA能带来什么。
- en: Then, we dived into the internal structure of a LoRA model, walked through the
    detailed steps to extract LoRA weights, and understood how to merge those weights
    into the checkpoint model.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们深入研究了LoRA模型的内部结构，详细介绍了提取LoRA权重的步骤，并了解了如何将这些权重合并到检查点模型中。
- en: Further, we implemented a function in Python that can load a LoRA safetensors
    file and perform weight merges.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们实现了一个Python函数，可以加载LoRA safetensors文件并执行权重合并。
- en: Finally, we briefly explored why LoRA works, based on the most recent papers
    from researchers.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们简要探讨了LoRA为何有效，基于研究人员最新的论文。
- en: In the next chapter, we are going to explore another powerful technique – textual
    inversion – to teach a model new “words,” and then use the pre-trained “words”
    to add new concepts to the generated images.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探索另一种强大的技术——文本反转——来教模型新的“单词”，然后使用预训练的“单词”向生成的图像添加新概念。
- en: References
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Edward J. et al, LoRA: Low-Rank Adaptation of Large Language Models: [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Edward J.等人，LoRA：大型语言模型的低秩自适应：[https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)
- en: 'Simo Ryu (cloneofsimo), `lora`: [https://github.com/cloneofsimo/lora](https://github.com/cloneofsimo/lora)'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Simo Ryu (cloneofsimo), `lora`: [https://github.com/cloneofsimo/lora](https://github.com/cloneofsimo/lora)'
- en: '`kohya_lora_loader`: [https://gist.github.com/takuma104/e38d683d72b1e448b8d9b3835f7cfa44](https://gist.github.com/takuma104/e38d683d72b1e448b8d9b3835f7cfa44)'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kohya_lora_loader`: [https://gist.github.com/takuma104/e38d683d72b1e448b8d9b3835f7cfa44](https://gist.github.com/takuma104/e38d683d72b1e448b8d9b3835f7cfa44)'
- en: 'CIVITAI: [https://www.civitai.com](https://www.civitai.com)'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CIVITAI：[https://www.civitai.com](https://www.civitai.com)
- en: 'Rinon Gal et al, An Image is Worth One Word: Personalizing Text-to-Image Generation
    using Textual Inversion: [https://textual-inversion.github.io/](https://textual-inversion.github.io/)'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rinon Gal等人，一张图片胜过千言万语：使用文本反转个性化文本到图像生成：[https://textual-inversion.github.io/](https://textual-inversion.github.io/)
- en: 'Diffusers’ `lora_state_dict` function: [https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/modeling_utils.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/modeling_utils.py)'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Diffusers的`lora_state_dict`函数：[https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/modeling_utils.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/modeling_utils.py)
- en: 'Andrew Zhu, Improving Diffusers Package for High-Quality Image Generation:
    [https://towardsdatascience.com/improving-diffusers-package-for-high-quality-image-generation-a50fff04bdd4](https://towardsdatascience.com/improving-diffusers-package-for-high-quality-image-generation-a50fff04bdd4)'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Andrew Zhu，改进Diffusers包以实现高质量图像生成：[https://towardsdatascience.com/improving-diffusers-package-for-high-quality-image-generation-a50fff04bdd4](https://towardsdatascience.com/improving-diffusers-package-for-high-quality-image-generation-a50fff04bdd4)
- en: 'Armen et al, Intrinsic Dimensionality Explains the Effectiveness of Language
    Model Fine-Tuning: [https://arxiv.org/abs/2012.13255](https://arxiv.org/abs/2012.13255)'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Armen等人，内在维度解释了语言模型微调的有效性：[https://arxiv.org/abs/2012.13255](https://arxiv.org/abs/2012.13255)
- en: 'Hugging Face, LoRA: [https://huggingface.co/docs/diffusers/training/lora](https://huggingface.co/docs/diffusers/training/lora)'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Hugging Face, LoRA: [https://huggingface.co/docs/diffusers/training/lora](https://huggingface.co/docs/diffusers/training/lora)'
- en: 'Hugging Face, PEFT: [https://huggingface.co/docs/peft/en/index](https://huggingface.co/docs/peft/en/index)'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Hugging Face, PEFT: [https://huggingface.co/docs/peft/en/index](https://huggingface.co/docs/peft/en/index)'
