- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Evaluating LLMs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估大型语言模型（LLMs）
- en: LLM evaluation is a crucial process used to assess the performance and capabilities
    of LLM models. It can take multiple forms, such as multiple-choice question answering,
    open-ended instructions, and feedback from real users. Currently, there is no
    unified approach to measuring a model’s performance but there are patterns and
    recipes that we can adapt to specific use cases.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: LLM评估是用于评估LLM模型性能和能力的关键过程。它可以采取多种形式，如多项选择题回答、开放式指令和真实用户的反馈。目前，没有统一的衡量模型性能的方法，但有一些模式和配方我们可以根据特定用例进行调整。
- en: While general-purpose evaluations are the most popular ones, with benchmarks
    like **Massive Multi-Task Language Understanding** (**MMLU**) or LMSYS Chatbot
    Arena, domain- and task-specific models benefit from more narrow approaches. This
    is particularly true when dealing with entire LLM systems (as opposed to models),
    often centered around a **retrieval-augmented generation** (**RAG**) pipeline.
    In these scenarios, we need to expand our evaluation framework to encompass the
    entire system, including new modules like retrievers and post-processors.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通用评估是最受欢迎的，例如**大规模多任务语言理解**（**MMLU**）或LMSYS聊天机器人竞技场，但特定领域和任务的模型从更狭窄的方法中受益。这在处理整个LLM系统（而不是模型）时尤其如此，通常围绕一个**检索增强生成**（**RAG**）流程。在这些情况下，我们需要扩展我们的评估框架，包括检索器和后处理器等新模块。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Model evaluation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型评估
- en: RAG evaluation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG评估
- en: Evaluating TwinLlama-3.1-8B
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估TwinLlama-3.1-8B
- en: By the end of this chapter, you will know the most popular LLM evaluations and
    how to evaluate models and RAG systems using different techniques.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将了解最流行的LLM评估以及如何使用不同的技术评估模型和RAG系统。
- en: Model evaluation
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估
- en: In model evaluation, the objective is to assess the capabilities of a single
    model without any prompt engineering, RAG pipeline, and so on.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型评估中，目标是评估单个模型的能力，而不需要任何提示工程、RAG流程等。
- en: This evaluation is essential for several reasons, such as selecting the most
    relevant LLM or making sure that the fine-tuning process actually improved the
    model. In this section, we will compare ML and LLM evaluation to understand the
    main differences between these two fields. We will then explore benchmarks for
    general-purpose, domain-specific, and task-specific models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这种评估对于几个原因至关重要，例如选择最相关的LLM或确保微调过程实际上提高了模型。在本节中，我们将比较机器学习（ML）和LLM评估，以了解这两个领域之间的主要区别。然后我们将探讨通用、特定领域和特定任务的基准。
- en: Comparing ML and LLM evaluation
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较ML和LLM评估
- en: ML evaluation is centered on assessing the performance of models designed for
    tasks like prediction, classification, and regression. Unlike the evaluation of
    LLMs, which often focuses on how well a model understands and generates language,
    ML evaluation is more concerned with how accurately and efficiently a model can
    process structured data to produce specific outcomes.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习评估主要集中在评估为预测、分类和回归等任务设计的模型的性能。与LLM评估不同，LLM评估通常关注模型理解和生成语言的能力，ML评估更关注模型如何准确和高效地处理结构化数据以产生特定结果。
- en: This difference comes from the nature of the tasks these models handle. ML models
    are generally designed for narrowly defined problems, such as predicting stock
    prices or detecting outliers, which often involve numerical or categorical data,
    making the evaluation process more straightforward. On the other hand, LLMs are
    tasked with interpreting and generating language, which adds a layer of subjectivity
    to the evaluation process. Instead of relying solely on numerical benchmarks,
    LLM evaluation requires a more nuanced approach and often incorporates qualitative
    assessments, examining how well the model produces coherent, relevant, and contextually
    accurate responses in natural language.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这种差异源于这些模型处理的任务的本质。ML模型通常是为定义狭窄的问题设计的，如预测股价或检测异常值，这通常涉及数值或分类数据，使评估过程更加直接。另一方面，LLM被要求解释和生成语言，这给评估过程增加了一层主观性。LLM评估不仅依赖于数值基准，还需要更细致的方法，并经常结合定性评估，检查模型在自然语言中产生连贯、相关和上下文准确响应的能力。
- en: 'In particular, we can see three key differences in how these models work, which
    impact the evaluation process:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是我们可以看到这些模型工作方式中的三个关键差异，这影响了评估过程：
- en: '**Numerical metrics**: Evaluating ML models typically involves measuring objective
    performance metrics, such as accuracy, precision, recall, or mean squared error,
    depending on the type of task at hand. This is less clear with LLMs, which can
    handle multiple tasks (hence, multiple evaluations) and can rarely rely on the
    same numerical metrics.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数值指标**: 评估机器学习模型通常涉及测量客观性能指标，如准确率、精确率、召回率或均方误差，具体取决于手头任务的类型。对于能够处理多个任务（因此，多个评估）且很少依赖于相同数值指标的LLMs（大型语言模型），这一点并不明确。'
- en: '**Feature engineering**: In traditional ML, a critical part of the process
    involves manually selecting and transforming relevant data features before training
    the model. Evaluating the success of this feature engineering often becomes part
    of the broader model evaluation. LLMs, however, are designed to handle raw text
    data directly, reducing the need for manual feature engineering.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征工程**: 在传统的机器学习中，过程的一个关键部分是在训练模型之前手动选择和转换相关的数据特征。评估这种特征工程的成功往往成为更广泛模型评估的一部分。然而，LLMs被设计为直接处理原始文本数据，减少了手动特征工程的需求。'
- en: '**Interpretability**: With ML models, it is easier to interpret why a model
    made certain predictions or classifications, and this interpretability can be
    a core part of their evaluation. This direct interpretation is not possible with
    LLMs. However, requesting explanations during the generation process can give
    insights into the model’s decision-making process.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释性**: 对于机器学习模型，更容易解释模型为何做出某些预测或分类，这种可解释性可以是它们评估的核心部分。然而，LLMs无法进行这种直接解释。但是，在生成过程中请求解释可以提供关于模型决策过程的见解。'
- en: In the following section, we will see a more fine-grained exploration of different
    types of LLMs. While evaluating general-purpose models is fairly disconnected
    from ML evaluation, task-specific LLMs are more closely aligned with traditional
    ML.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将更细致地探讨不同类型的LLMs。虽然评估通用模型与机器学习评估相对独立，但特定任务的LLMs与传统机器学习更紧密地相关。
- en: General-purpose LLM evaluations
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通用LLMs评估
- en: General-purpose evaluations refer to metrics dedicated to base and general-purpose
    fine-tuned models. They cover a breadth of capabilities that are correlated with
    knowledge and usefulness without focusing on specific tasks or domains. This allows
    developers to get an overview of these capabilities, compare themselves with competitors,
    and identify strengths and weaknesses. Based on these results, it is possible
    to tweak the dataset and hyperparameters, or even modify the architecture.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通用评估指的是针对基础和通用微调模型的指标。它们涵盖了与知识和实用性相关的一系列能力，而不专注于特定任务或领域。这允许开发者对这些能力有一个全面的了解，与竞争对手进行比较，并识别优势和劣势。基于这些结果，可以调整数据集和超参数，甚至修改架构。
- en: 'We can broadly categorize general-purpose evaluations in three phases: during
    pre-training, after pre-training, and after fine-tuning.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将通用评估大致分为三个阶段：预训练期间、预训练之后以及微调之后。
- en: 'During pre-training, we closely monitor how the model learns, as shown at the
    end of *Chapter 5*. The most straightforward metrics are low-level and correspond
    to how models are trained:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练期间，我们密切监控模型的学习过程，如第5章末所示。最直接的指标是低级指标，与模型的训练方式相对应：
- en: '**Training loss**: Based on the cross-entropy loss, measures the difference
    between the model’s predicted probability distribution and the true distribution
    of the next token'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练损失**: 基于交叉熵损失，衡量模型预测的概率分布与下一个标记的真实分布之间的差异'
- en: '**Validation loss**: Calculates the same loss as training loss, but on a held-out
    validation set to assess generalization'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证损失**: 计算与训练损失相同的损失，但是在保留的验证集上，以评估泛化能力'
- en: '**Perplexity**: Exponential of the cross-entropy loss, representing how “surprised”
    the model is by the data (lower is better)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**困惑度**: 交叉熵损失的指数，表示模型对数据的“惊讶”程度（越低越好）'
- en: '**Gradient norm**: Monitors the magnitude of gradients during training to detect
    potential instabilities or vanishing/exploding gradients'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度范数**: 监控训练过程中梯度的幅度，以检测潜在的不稳定性或梯度消失/爆炸'
- en: It’s also possible to include benchmarks like HellaSwag (common sense reasoning)
    during this stage but there’s a risk of overfitting these evaluations.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段也可以包括像HellaSwag（常识推理）这样的基准测试，但存在过度拟合这些评估的风险。
- en: 'After pre-training, it is common to use a suite of evaluations to evaluate
    the base model. This suite can include internal and public benchmarks. Here’s
    a non-exhaustive list of common public pre-training evaluations:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练之后，通常使用一系列评估来评估基模型。这个套件可以包括内部和公共基准。以下是一个非详尽的常见公共预训练评估列表：
- en: '**MMLU (knowledge)**: Tests models on multiple-choice questions across 57 subjects,
    from elementary to professional levels'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MMLU (知识)**: 在57个科目（从基础到专业水平）的多个选择题上测试模型'
- en: '**HellaSwag (reasoning)**: Challenges models to complete a given situation
    with the most plausible ending from multiple choices'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HellaSwag (推理)**: 挑战模型从多个选择中完成给定情境的最合理结局'
- en: '**ARC-C (reasoning)**: Evaluates models on grade-school-level multiple-choice
    science questions requiring causal reasoning'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ARC-C (推理)**: 评估模型在需要因果推理的年级学校水平选择题上的表现'
- en: '**Winogrande (reasoning)**: Assesses common sense reasoning through pronoun
    resolution in carefully crafted sentences'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Winogrande (推理)**: 通过精心制作的句子中的代词解析来评估常识推理'
- en: '**PIQA (reasoning)**: Measures physical common sense understanding through
    questions about everyday physical interactions'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PIQA (推理)**: 通过关于日常物理互动的问题来衡量物理常识理解能力'
- en: Many of these datasets are also used to evaluate general-purpose fine-tuned
    models. In this case, we focus on the difference in a given score between the
    base and the fine-tuned model. For example, bad fine-tuning can degrade the knowledge
    of the model, measured by MMLU. On the contrary, a good one might instill even
    more knowledge and increase the MMLU score.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 许多这些数据集也被用来评估通用微调模型。在这种情况下，我们关注基模型和微调模型之间给定分数的差异。例如，不良的微调可能会降低模型的知识，如通过MMLU衡量的。相反，良好的微调可能会灌输更多的知识并提高MMLU分数。
- en: This can also help identify any contamination issues, where the model might
    have been fine-tuned on data that is too close to a test set. For instance, improving
    the MMLU score of a base model by 10 points during the fine-tuning phase is unlikely.
    This is a sign that the instruction data might be contaminated.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以帮助识别任何污染问题，模型可能被微调在过于接近测试集的数据上。例如，在微调阶段提高基模型的MMLU分数10分是不太可能的。这是一个迹象，表明指令数据可能已被污染。
- en: 'In addition to these pre-trained evaluations, fine-tuned models also have their
    own benchmarks. Here, we use the term “fine-tuned model” to designate a model
    that has been trained with **supervised fine-tuning** (**SFT**) and preference
    alignment. These benchmarks target capabilities connected to the ability of fine-tuned
    models to understand and answer questions. In particular, they test instruction-following,
    multi-turn conversation, and agentic skills:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些预训练评估之外，微调模型也有自己的基准。在这里，我们使用“微调模型”这个术语来指代经过**监督微调**（**SFT**）和偏好对齐训练的模型。这些基准针对与微调模型理解和回答问题能力相关的功能。特别是，它们测试指令遵循、多轮对话和代理技能：
- en: '**IFEval (instruction following)**: Assesses a model’s ability to follow instructions
    with particular constraints, like not outputting any commas in your answer'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IFEval (指令遵循)**: 评估模型在特定约束下（如答案中不输出任何逗号）遵循指令的能力'
- en: '**Chatbot Arena (conversation)**: A framework where humans vote for the best
    answer to an instruction, comparing two models in head-to-head conversations'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Chatbot Arena (对话)**: 一个框架，其中人类投票选择对指令的最佳答案，通过面对面对话比较两个模型'
- en: '**AlpacaEval (instruction following)**: Automatic evaluation for fine-tuned
    models that is highly correlated with Chatbot Arena'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AlpacaEval (指令遵循)**: 与Chatbot Arena高度相关的自动评估，用于微调模型'
- en: '**MT-Bench (conversation)**: Evaluates models on multi-turn conversations,
    testing their ability to maintain context and provide coherent responses'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MT-Bench (对话)**: 评估模型在多轮对话中的表现，测试其保持上下文并提供连贯回答的能力'
- en: '**GAIA (agentic)**: Tests a wide range of abilities like tool use and web browsing,
    in a multi-step fashion'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GAIA (代理)**: 以多步骤的方式测试广泛的技能，如工具使用和网络浏览'
- en: Understanding how these evaluations are designed and used is important to choose
    the best LLM for your application. For example, if you want to fine-tune a model,
    you want the best base model in terms of knowledge and reasoning for a given size.
    This allows you to compare the capabilities of different LLMs and pick the one
    that will offer the strongest foundation for your fine-tuning.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些评估的设计和使用方式对于选择最适合你应用的LLM非常重要。例如，如果你想微调模型，你希望选择在给定大小上知识推理方面最好的基础模型。这允许你比较不同LLM的能力，并选择一个将为你的微调提供最强基础的模型。
- en: Even if you don’t want to fine-tune a model, benchmarks like Chatbot Arena or
    IFEval are a good way to compare different instruct models. For instance, you
    want great conversational abilities if you’re building a chatbot. However, this
    is not necessary if your end goal is something like information extraction from
    unstructured documents. In this case, you will benefit more from excellent instruction-following
    skills to understand and execute tasks.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你不想微调模型，像Chatbot Arena或IFEval这样的基准也是一个比较不同指令模型的好方法。例如，如果你在构建聊天机器人，你希望拥有出色的对话能力。然而，如果你的最终目标是像从非结构化文档中提取信息这样的任务，这就不必要了。在这种情况下，你将更多地受益于出色的指令遵循技能来理解和执行任务。
- en: While these benchmarks are popular and useful, they also suffer from inherent
    flaws. For example, public benchmarks can be gamed by training models on test
    data or samples that are very similar to benchmark datasets. Even human evaluation
    is not perfect and is often biased toward long and confident answers, especially
    when they’re nicely formatted (e.g., using Markdown). On the other hand, private
    test sets have not been scrutinized as much as public ones and might have their
    own issues and biases.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些基准很受欢迎且有用，但它们也存在固有的缺陷。例如，公开基准可以通过在测试数据或与基准数据集非常相似的数据或样本上训练模型来操纵。甚至人类评估也不是完美的，并且往往偏向于长而自信的答案，尤其是当它们格式良好时（例如，使用Markdown）。另一方面，私人测试集没有像公开测试集那样受到仔细审查，可能存在自身的问题和偏见。
- en: This means that benchmarks are not a single source of truth but should be used
    as signals. Once multiple evaluations provide a similar answer, you can raise
    your confidence level about the real capabilities of a model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着基准不是真理的唯一来源，而应该用作信号。一旦多个评估提供了相似的答案，你就可以提高对模型真实能力的信心水平。
- en: Domain-specific LLM evaluations
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 领域特定LLM评估
- en: Domain-specific LLMs don’t have the same scope as general-purpose models. This
    is helpful to target more fine-grained capabilities with more depth than the previous
    benchmarks.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 领域特定LLM的适用范围不如通用模型广泛。这有助于针对比以前基准更精细的能力进行更深入的定位。
- en: Within the category, the choice of benchmarks entirely depends on the domain
    in question. For common applications like a language-specific model or a code
    model, it is recommended to search for relevant evaluations and even benchmark
    suites. These suites encompass different benchmarks and are designed to be reproducible.
    By targeting different aspects of a domain, they often capture domain performance
    more accurately.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在该类别中，基准的选择完全取决于所涉及的领域。对于像特定语言模型或代码模型这样的常见应用，建议搜索相关的评估甚至基准套件。这些套件包含不同的基准，并设计成可复制的。通过针对领域的不同方面，它们通常能更准确地捕捉领域性能。
- en: 'To illustrate this, here is a list of domain-specific evaluations with leaderboards
    on the Hugging Face Hub:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，以下是一个在Hugging Face Hub上有排行榜的领域特定评估列表：
- en: '**Open Medical-LLM Leaderboard**: Evaluates the performance of LLMs in medical
    question-answering tasks. It regroups 9 metrics, with 1,273 questions from the
    US medical license exams (MedQA), 500 questions from PubMed articles (PubMedQA),
    4,183 questions from Indian medical entrance exams (MedMCQA), and 1,089 questions
    from 6 sub-categories of MMLU (clinical knowledge, medical genetics, anatomy,
    professional medicine, college biology, and college medicine).'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Open Medical-LLM排行榜**：评估LLM在医学问答任务中的性能。它重新组合了9个指标，包括来自美国医学执照考试（MedQA）的1,273个问题，来自PubMed文章（PubMedQA）的500个问题，来自印度医学入学考试（MedMCQA）的4,183个问题，以及来自MMLU的6个子类别（临床知识、医学遗传学、解剖学、专业医学、大学生物学和大学医学）的1,089个问题。'
- en: '**BigCodeBench Leaderboard**: Evaluates the performance of code LLMs, featuring
    two main categories: BigCodeBench-Complete for code completion based on structured
    docstrings, and BigCodeBench-Instruct for code generation from natural language
    instructions. Models are ranked by their Pass@1 scores using greedy decoding,
    with an additional Elo rating for the Complete variant. It covers a wide range
    of programming scenarios that test LLMs’ compositional reasoning and instruction-following
    capabilities.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BigCodeBench排行榜**：评估代码LLM的性能，包括两个主要类别：BigCodeBench-Complete，基于结构化文档字符串的代码补全，以及BigCodeBench-Instruct，从自然语言指令生成代码。模型根据其Pass@1分数使用贪婪解码进行排名，对于Complete变体还有一个额外的Elo评分。它涵盖了广泛的编程场景，测试LLM的组合推理和指令遵循能力。'
- en: '**Hallucinations Leaderboard**: Evaluates LLMs’ tendency to produce false or
    unsupported information across 16 diverse tasks spanning 5 categories. These include
    *Question Answering* (with datasets like NQ Open, TruthfulQA, and SQuADv2), *Reading
    Comprehension* (using TriviaQA and RACE), *Summarization* (employing HaluEval
    Summ, XSum, and CNN/DM), *Dialogue* (featuring HaluEval Dial and FaithDial), and
    *Fact Checking* (utilizing MemoTrap, SelfCheckGPT, FEVER, and TrueFalse). The
    leaderboard also assesses instruction-following ability using IFEval.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**幻觉排行榜**：评估LLM在16个涵盖5个类别的多样化任务中产生虚假或不支持信息的能力。这些包括*问答*（使用NQ Open、TruthfulQA和SQuADv2等数据集）、*阅读理解*（使用TriviaQA和RACE）、*摘要*（采用HaluEval
    Summ、XSum和CNN/DM）、*对话*（特色HaluEval Dial和FaithDial）、以及*事实核查*（利用MemoTrap、SelfCheckGPT、FEVER和TrueFalse）。排行榜还使用IFEval评估指令遵循能力。'
- en: '**Enterprise Scenarios Leaderboard**: Evaluates the performance of LLMs on
    six real-world enterprise use cases, covering diverse tasks relevant to business
    applications. The benchmarks include FinanceBench (100 financial questions with
    retrieved context), Legal Confidentiality (100 prompts from LegalBench for legal
    reasoning), Writing Prompts (creative writing evaluation), Customer Support Dialogue
    (relevance in customer service interactions), Toxic Prompts (safety assessment
    for harmful content generation), and Enterprise PII (business safety for sensitive
    information protection). Some test sets are closed-source to prevent gaming of
    the leaderboard. The evaluation focuses on specific capabilities such as answer
    accuracy, legal reasoning, creative writing, contextual relevance, and safety
    measures, providing a comprehensive assessment of LLMs’ suitability for enterprise
    environments.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**企业场景排行榜**：评估LLM在六个现实世界企业用例上的性能，涵盖与商业应用相关的多样化任务。基准包括FinanceBench（100个带有检索上下文的金融问题）、Legal
    Confidentiality（来自LegalBench的100个用于法律推理的提示）、Writing Prompts（创意写作评估）、Customer Support
    Dialogue（客户服务互动中的相关性）、Toxic Prompts（有害内容生成的安全性评估）以及Enterprise PII（敏感信息保护的企业安全）。一些测试集是闭源的，以防止排行榜被操纵。评估侧重于特定能力，如答案准确性、法律推理、创意写作、上下文相关性以及安全措施，为LLM在企业环境中的适用性提供全面评估。'
- en: Leaderboards can have different approaches based on their domain. For example,
    BigCodeBench is significantly different from others because it relies on only
    two metrics that sufficiently capture the entire domain. On the other hand, the
    Hallucinations Leaderboard regroups 16 metrics, including many general-purpose
    evaluations. It shows that in addition to custom benchmarks, reusing general-purpose
    ones can complete your own suite.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 排行榜可以根据其领域采用不同的方法。例如，BigCodeBench与其他排行榜显著不同，因为它仅依赖于两个足以涵盖整个领域的指标。另一方面，幻觉排行榜重新组合了16个指标，包括许多通用评估。这表明，除了定制基准之外，重用通用基准也可以完成自己的套件。
- en: 'In particular, language-specific LLMs often reuse translated versions of general-purpose
    benchmarks. This can be completed with original evaluations in the native language.
    While some of these benchmarks use machine translation, it is better to rely on
    human-translated evaluations to improve their quality. We selected the following
    three task-specific leaderboards and their respective evaluation suites to give
    you an idea of how to build your own:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是语言特定的LLM通常重用通用基准的翻译版本。这可以通过母语中的原始评估来完成。虽然一些基准使用机器翻译，但最好依赖于人工翻译的评估来提高其质量。我们选择了以下三个特定任务的排行榜及其相应的评估套件，以给您提供一个如何构建自己的排行榜的思路：
- en: '**OpenKo-LLM Leaderboard**: Evaluates the performance of Korean LLMs using
    nine metrics. These metrics are a combination of general-purpose benchmarks translated
    into Korean (GPQA, Winogrande, GSM8K, EQ-Bench, and IFEval) and custom evaluations
    (Knowledge, Social Value, Harmlessness, and Helpfulness).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenKo-LLM排行榜**：使用九项指标评估韩语LLM的性能。这些指标包括将通用基准翻译成韩语（GPQA、Winogrande、GSM8K、EQ-Bench和IFEval）以及定制评估（知识、社会价值、无害性和帮助性）。'
- en: '**Open Portuguese LLM Leaderboard**: Evaluates the performance of Portuguese
    language LLMs using nine diverse benchmarks. These benchmarks include educational
    assessments (ENEM with 1,430 questions, and BLUEX with 724 questions from university
    entrance exams), professional exams (OAB Exams with over 2,000 questions), language
    understanding tasks (ASSIN2 RTE and STS, FAQUAD NLI), and social media content
    analysis (HateBR with 7,000 Instagram comments, PT Hate Speech with 5,668 tweets,
    and tweetSentBR).'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Open葡萄牙LLM排行榜**：使用九种不同的基准评估葡萄牙语LLM的性能。这些基准包括教育评估（ENEM有1,430个问题，BLUEX有724个来自大学入学考试的问题）、专业考试（OAB考试有超过2,000个问题）、语言理解任务（ASSIN2
    RTE和STS、FAQUAD NLI）以及社交媒体内容分析（HateBR有7,000个Instagram评论，PT Hate Speech有5,668条推文，tweetSentBR）。'
- en: '**Open Arabic LLM Leaderboard**: Evaluates the performance of Arabic language
    LLMs using a comprehensive set of benchmarks, including both native Arabic tasks
    and translated datasets. The leaderboard features two native Arabic benchmarks:
    AlGhafa and Arabic-Culture-Value-Alignment. Additionally, it incorporates 12 translated
    benchmarks covering various domains, such as MMLU, ARC-Challenge, HellaSwag, and
    PIQA.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Open阿拉伯LLM排行榜**：使用一套全面的基准评估阿拉伯语LLM的性能，包括本土阿拉伯语任务和翻译数据集。排行榜包括两个本土阿拉伯语基准：AlGhafa和阿拉伯文化价值观对齐。此外，它还包含12个翻译基准，涵盖各种领域，如MMLU、ARC-Challenge、HellaSwag和PIQA。'
- en: Both general-purpose and domain-specific evaluations are designed with three
    main principles. First, they should be complex and challenge models to distinguish
    good and bad outputs. Second, they should be diverse and cover as many topics
    and scenarios as possible. When one benchmark is not enough, additional ones can
    create a stronger suite. Finally, they should be practical and easy to run. This
    is more connected to evaluation libraries, which can be more or less complex to
    work with. We recommend lm-evaluation-harness ([github.com/EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness))
    from Eleuther AI and lighteval ([github.com/huggingface/lighteval](https://github.com/huggingface/lighteval))
    from Hugging Face to run your benchmarks.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 通用和领域特定的评估设计遵循三个主要原则。首先，它们应该是复杂的，挑战模型区分好和坏的输出。其次，它们应该是多样化的，尽可能涵盖更多主题和场景。当一项基准不足以时，额外的基准可以创建一个更强的套件。最后，它们应该是实用的，易于运行。这与评估库更为相关，这些库可能更容易或更难使用。我们推荐使用Eleuther
    AI的lm-evaluation-harness（[github.com/EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)）和Hugging
    Face的lighteval（[github.com/huggingface/lighteval](https://github.com/huggingface/lighteval)）来运行您的基准。
- en: Task-specific LLM evaluations
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务特定的LLM评估
- en: While general-purpose and domain-specific evaluations indicate strong base or
    instruct models, they cannot provide insights into how well these models work
    for a given task. This requires benchmarks specifically designed for this purpose,
    measuring downstream performance.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通用和领域特定的评估表明了强大的基础或指导模型，但它们无法提供关于这些模型在特定任务中表现如何的见解。这需要专门为此目的设计的基准，以衡量下游性能。
- en: Because of their narrow focus, task-specific LLMs can rarely rely on pre-existing
    evaluation datasets. This can be advantageous because their outputs also tend
    to be more structured and easier to evaluate using traditional ML metrics. For
    example, a summarization task can leverage the **Recall-Oriented Understudy for
    Gisting Evaluation** (**ROUGE**) metric, which measures the overlap between the
    generated text and reference text using n-grams.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们的关注范围较窄，任务特定的LLM很少能依赖现有的评估数据集。这可能是有利的，因为它们的输出也往往更结构化，更容易使用传统的机器学习指标进行评估。例如，摘要任务可以利用**基于召回的摘要评估辅助工具**（**ROUGE**）指标，该指标通过使用n-gram来衡量生成的文本和参考文本之间的重叠。
- en: 'Likewise, classification tasks also benefit from it and use the following classic
    metrics, among others:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，分类任务也能从中受益，并使用以下经典指标，以及其他指标：
- en: '**Accuracy**: Accuracy refers to the proportion of correctly predicted instances
    compared to the total instances. It’s particularly useful for tasks with categorical
    outputs or where there is a clear distinction between right and wrong answers,
    such as **named entity recognition** (**NER**).'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确性**：准确性指的是正确预测的实例数与总实例数的比例。它对于具有分类输出或存在明显对错之分的任务特别有用，例如**命名实体识别**（NER）。'
- en: '**Precision**: The ratio of true positive predictions to the total positive
    predictions made by the model.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确度**：模型做出的真实阳性预测与模型做出的总阳性预测的比例。'
- en: '**Recall**: The ratio of true positive predictions to the total actual positive
    instances.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**：真实阳性预测与总实际阳性实例的比例。'
- en: '**F1 Score**: The harmonic mean of precision and recall, used to balance both
    metrics. These are particularly useful in tasks such as classification or entity
    extraction.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F1分数**：精确度和召回率的调和平均数，用于平衡这两个指标。这些指标在分类或实体提取等任务中特别有用。'
- en: 'When the task cannot be directly mapped to a traditional ML task, it is possible
    to create a custom benchmark. This benchmark can be inspired by general-purpose
    and domain-specific evaluation datasets. A common and successful pattern is the
    use of multiple-choice question answering. In this framework, the instruction
    consists of a question with several options. See the following example with a
    question from the MMLU dataset (abstract algebra):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当任务不能直接映射到传统的机器学习任务时，可以创建一个自定义基准。这个基准可以受到通用和特定领域评估数据集的启发。一个常见且成功的模式是使用多项选择题回答。在这个框架中，指令由一个问题及其几个选项组成。以下是一个来自MMLU数据集（抽象代数）的问题示例：
- en: '| **Instruction**Find the degree for the given field extension Q(sqrt(2), sqrt(3))
    over Q.A. 0B. 4C. 2D. 6 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| **指令**找到给定域扩展Q(sqrt(2), sqrt(3))在Q.A. 0B. 4C. 2D. 6上的度。|'
- en: '| **Output**B |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| **输出**B |'
- en: '*Table 7.1*: Example from the MMLU dataset'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*表7.1*：来自MMLU数据集的示例'
- en: 'There are two main ways of evaluating models with this scheme—text generation
    and log-likelihood evaluations:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此方案评估模型主要有两种方式——文本生成和对数似然评估：
- en: The first approach involves having the model generate text responses and comparing
    those to predefined answer choices. For example, the model generates a letter
    (A, B, C, or D) as its answer, which is then checked against the correct answer.
    This method tests the model’s ability to produce coherent and accurate responses
    in a format similar to how it would be used in real-world applications.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一种方法涉及让模型生成文本回答，并将这些回答与预定义的答案选项进行比较。例如，模型生成一个字母（A、B、C或D）作为其答案，然后与正确答案进行核对。这种方法测试了模型在类似实际应用格式中产生连贯且准确回答的能力。
- en: Evaluation using probabilities, on the other hand, looks at the model’s predicted
    probabilities for different answer options without requiring text generation.
    For MMLU, lm-evaluation-harness compares the probabilities for the full text of
    each answer choice. This approach allows for a more nuanced assessment of the
    model’s understanding, as it can capture the relative confidence the model has
    in different options, even if it wouldn’t necessarily generate the exact correct
    answer text.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，使用概率进行评估则关注模型对不同答案选项的预测概率，而不需要文本生成。对于MMLU，lm-evaluation-harness比较了每个答案选项全文的概率。这种方法允许对模型的理解进行更细致的评估，因为它可以捕捉模型对不同选项的相对信心，即使它不一定生成确切的正确答案文本。
- en: For simplicity, we recommend the text-generation version of the evaluation that
    mimics human test-taking. It is easier to implement, and generally more discriminative,
    as low-quality models tend to overperform on probability-based evaluations. You
    can adapt this technique to quiz your models about a particular task, and even
    expand it to specific domains.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们建议使用模仿人类考试过程的文本生成版本进行评估。这种方法更容易实现，并且通常更具区分性，因为低质量模型在基于概率的评估中往往表现过好。你可以将这项技术应用于测试模型对特定任务的掌握程度，甚至扩展到特定领域。
- en: Conversely, if the task is too open-ended, traditional ML metrics and multiple-choice
    question answering might not be relevant. In this scenario, the LLM-as-a-judge
    technique introduced in *Chapter 5* can be used to evaluate the quality of the
    answers. If you have ground-truth answers, providing them as additional context
    improves the accuracy of the evaluation. Otherwise, defining different dimensions
    (such as relevance or toxicity, depending on your task) can also ground the evaluation
    in more interpretable categories.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果任务过于开放，传统的机器学习指标和多项选择题回答可能就不相关了。在这种情况下，可以在第5章中介绍的使用LLM作为评判者的技术可以用来评估答案的质量。如果你有真实答案，提供它们作为额外上下文可以提高评估的准确性。否则，定义不同的维度（例如相关性或毒性，取决于你的任务）也可以使评估更易于理解。
- en: It is recommended to use large models for evaluation and to iteratively refine
    your prompt. In this process, the explanations outputted by the model are important
    for understanding errors in its reasoning and fixing them through additional prompt
    engineering.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 建议使用大型模型进行评估，并迭代地改进你的提示。在这个过程中，模型输出的解释对于理解其推理中的错误以及通过额外的提示工程修复它们非常重要。
- en: 'In order to easily parse answers, one can specify a structure in the instruction
    or use some kind of structured generation (like Outlines or OpenAI’s JSON mode).
    Here is an example of an instruction with a structure:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于解析答案，可以在指令中指定结构或使用某种类型的结构化生成（如提纲或OpenAI的JSON模式）。以下是一个带有结构的指令示例：
- en: '| You are an evaluator who assesses the quality of an answer to an instruction.Your
    goal is to provide a score that represents how well the answer addresses the instruction.You
    will use a scale of 1 to 4, where each number represents the following:1\. The
    answer is not relevant to the instruction.2\. The answer is relevant but not helpful.3\.
    The answer is relevant and helpful but could be more detailed.4\. The answer is
    relevant, helpful, and detailed.Please provide your evaluation as follows:##Evaluation##Explanation:
    (analyze the relevant, helpfulness, and complexity of the answer)Total rating:
    (final score as a number between 1 and 4)**Instruction**:{instruction}**Answer**:{answer}##Evaluation##Explanation:
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 你是一个评估者，负责评估指令的答案质量。你的目标是提供一个分数，表示答案如何满足指令。你将使用1到4的刻度，其中每个数字代表以下内容：1. 答案与指令不相关。2.
    答案与指令相关但无帮助。3. 答案与指令相关且有帮助，但可以更详细。4. 答案与指令相关、有帮助且详细。请按照以下方式提供你的评估：##评估##说明：（分析答案的相关性、帮助性和复杂性）总分：（1到4之间的最终分数）**指令**:{instruction}**答案**:{answer}##评估##说明：|'
- en: '*Table 7.2*: Example of general-purpose LLM-as-a-judge prompt for answer evaluation'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*表7.2*：用于答案评估的通用LLM作为评判者的提示示例'
- en: Naturally, you can tweak the scale, add a ground-truth answer to this prompt,
    and customize it for your own use cases.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，你可以调整刻度，将真实答案添加到这个提示中，并根据自己的用例进行定制。
- en: However, judge LLMs can exhibit biases favoring assertive or verbose responses,
    potentially overrating answers that sound more confident but are less accurate.
    They may also lack domain expertise for specialized topics, leading to misjudgments.
    Consistency is also a concern, as LLMs might score similar responses differently.
    Additionally, they could have implicit preferences for certain writing styles
    unrelated to actual answer quality. To mitigate these issues, it’s possible to
    combine LLM evaluations with other metrics, use multiple judges, and carefully
    design prompts to address biases.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，评判LLM可能会表现出偏向于自信或冗长回答的偏见，可能高估了听起来更有信心但准确性较低的答案。它们也可能缺乏特定主题的领域专业知识，导致误判。一致性也是一个问题，因为LLM可能会对相似响应给出不同的评分。此外，它们可能对某些与实际答案质量无关的写作风格有隐含的偏好。为了减轻这些问题，可以将LLM评估与其他指标相结合，使用多个评判者，并精心设计提示以解决偏见。
- en: Once a model has been properly evaluated and works as intended, it might be
    included within a broader system. In the next section, we will see how systems
    change the evaluation framework.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型经过适当的评估并按预期工作，它可能被包含在一个更广泛系统中。在下一节中，我们将看到系统是如何改变评估框架的。
- en: RAG evaluation
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG评估
- en: While traditional LLM evaluation focuses on the model’s inherent capabilities,
    RAG evaluation requires a more comprehensive approach that considers both the
    model’s generative abilities and its interaction with external information sources.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然传统的LLM评估侧重于模型固有的能力，但RAG评估需要更全面的方法，既要考虑模型的生成能力，也要考虑其与外部信息源的交互。
- en: RAG systems combine the strengths of LLMs with information retrieval mechanisms,
    allowing them to generate responses that are not only coherent and contextually
    appropriate but also grounded in up-to-date, externally sourced information. This
    makes RAG particularly valuable in fields where current and accurate information
    is crucial, such as news reporting, research, and customer support.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: RAG系统结合了LLM的优势和信息检索机制，使它们能够生成既连贯又符合上下文、且基于最新、外部来源信息的响应。这使得RAG在当前和准确信息至关重要的领域特别有价值，例如新闻报道、研究和客户支持。
- en: 'The evaluation of RAG systems goes beyond assessing a standalone LLM. It requires
    examining the entire system’s performance, including:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: RAG系统的评估不仅限于评估一个独立的LLM。它需要检查整个系统的性能，包括：
- en: '**Retrieval accuracy**: How well does the system fetch relevant information?'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索准确性**：系统检索相关信息的程度如何？'
- en: '**Integration quality**: How effectively is the retrieved information incorporated
    into the generated response?'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**整合质量**：检索到的信息被有效整合到生成的响应中的程度如何？'
- en: '**Factuality and relevance**: Does the final output address the query appropriately
    while seamlessly blending retrieved and generated content?'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事实性和相关性**：最终输出是否适当地解决了查询，同时无缝地融合了检索和生成内容？'
- en: Key metrics for RAG evaluation include retrieval precision and recall, which
    measure the accuracy and comprehensiveness of the retrieved information. Additionally,
    the quality of integration between retrieved data and generated text is crucial,
    as is the overall factuality and coherence of the output.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: RAG评估的关键指标包括检索精度和召回率，这些指标衡量检索信息的准确性和全面性。此外，检索数据与生成文本之间的整合质量至关重要，同样，输出的整体事实性和连贯性也是关键。
- en: 'To illustrate how these metrics are applied in practice, consider a RAG system
    designed for a customer support chatbot in an e-commerce setting. In this scenario,
    the user asks “What’s your return policy for laptops purchased during the holiday
    sale?” The RAG pipeline finds relevant documents on the electronics return policy
    and documents on holiday sale terms. This additional context is appended at the
    end of the question, and the model uses it to respond:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这些指标在实际中的应用，考虑一个为电子商务环境中的客户支持聊天机器人设计的RAG系统。在这种情况下，用户询问“假日促销期间购买的笔记本电脑的退货政策是什么？”RAG管道找到有关电子产品退货政策的文档和有关假日促销条款的文档。这些额外的上下文附加在问题的末尾，并且模型使用它来回答：
- en: '| For laptops purchased during our holiday sale, you have an extended return
    period of 60 days from the date of purchase. This is longer than our standard
    30-day return policy for electronics. Please ensure the laptop is in its original
    packaging with all accessories to be eligible for a full refund. |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 在我们假日促销期间购买的笔记本电脑，您从购买日期起享有60天的延长退货期。这比我们标准电子产品的30天退货政策更长。请确保笔记本电脑处于原始包装中，并附带所有配件，以便有资格获得全额退款。
    |'
- en: '*Table 7.3*: Example of output from a RAG pipeline designed for customer support'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*表7.3*：为客服设计的RAG管道的输出示例'
- en: In this pipeline, we can evaluate if the retrieved documents correspond to what
    was expected (retrieval accuracy). We can also measure the difference between
    responses with and without additional context (integration quality). Finally,
    we can assess whether the output is relevant and grounded in the information provided
    by the documents (factuality and relevance).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个管道中，我们可以评估检索的文档是否与预期相符（检索准确性）。我们还可以衡量带有和没有额外上下文的响应之间的差异（整合质量）。最后，我们可以评估输出是否与提供的信息相关，并且是否在文档提供的信息基础上（事实性和相关性）。
- en: In this section, we will cover two methods to evaluate how well RAG models incorporate
    external information into their responses.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍两种评估RAG模型如何将外部信息纳入其响应的方法。
- en: Ragas
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ragas
- en: '**Retrieval-Augmented Generation Assessment** (**Ragas**) is an open-source
    toolkit designed to provide developers with a comprehensive set of tools for RAG
    evaluation and optimization. It’s designed around the idea of **metrics-driven
    development** (**MDD**), a product development approach that relies on data to
    make well-informed decisions, involving the ongoing monitoring of essential metrics
    over time to gain valuable insights into an application’s performance. By embracing
    this methodology, Ragas enables developers to objectively assess their RAG systems,
    identify areas for improvement, and track the impact of changes over time.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**检索增强生成评估**（**Ragas**）是一个开源工具包，旨在为开发者提供一套全面的工具，用于RAG评估和优化。它围绕**指标驱动开发**（**MDD**）的理念设计，这是一种产品开发方法，它依赖于数据来做出明智的决策，涉及对关键指标随时间持续监控，以获得有关应用程序性能的宝贵见解。通过采用这种方法，Ragas使开发者能够客观地评估他们的RAG系统，确定改进领域，并跟踪随时间变化的影响。'
- en: One of the key capabilities of Ragas is its ability to synthetically generate
    diverse and complex test datasets. This feature addresses a significant pain point
    in RAG development, as manually creating hundreds of questions, answers, and contexts
    is both time-consuming and labor-intensive. Instead, it uses an evolutionary approach
    paradigm inspired by works like Evol-Instruct to craft questions with varying
    characteristics such as reasoning complexity, conditional elements, and multi-context
    requirements. This approach ensures a comprehensive evaluation of different components
    within the RAG pipeline.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Ragas的关键功能之一是其能够合成生成多样化和复杂的测试数据集。这一功能解决了RAG开发中的一个重大痛点，因为手动创建数百个问题、答案和上下文既耗时又费力。相反，它使用受Evol-Instruct等作品启发的进化方法范式来构建具有不同特征的问题，如推理复杂性、条件元素和多上下文要求。这种方法确保了对RAG管道中不同组件的全面评估。
- en: Additionally, Ragas can generate conversational samples that simulate chat-based
    question-and-follow-up interactions, allowing developers to evaluate their systems
    in more realistic scenarios.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Ragas可以生成模拟基于聊天的问题和后续交互的对话样本，使开发者能够在更真实的场景中评估他们的系统。
- en: '![](img/B31105_07_01.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_07_01.png)'
- en: 'Figure 7.1: Overview of the Ragas evaluation framework'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：Ragas评估框架概述
- en: 'As illustrated in *Figure 7.1*, Ragas provides a suite of LLM-assisted evaluation
    metrics designed to objectively measure different aspects of RAG system performance.
    These metrics include:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图7.1*所示，Ragas提供了一套由LLM辅助的评价指标，旨在客观地衡量RAG系统性能的不同方面。这些指标包括：
- en: '**Faithfulness**: This metric measures the factual consistency of the generated
    answer against the given context. It works by breaking down the answer into individual
    claims and verifying if each claim can be inferred from the provided context.
    The faithfulness score is calculated as the ratio of verifiable claims to the
    total number of claims in the answer.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**忠实度**：此指标衡量生成的答案与给定上下文的事实一致性。它通过将答案分解为单个主张并验证每个主张是否可以从提供的上下文中推断出来来工作。忠实度得分是可验证主张与答案中主张总数的比率。'
- en: '**Answer relevancy**: This metric evaluates how pertinent the generated answer
    is to the given prompt. It uses an innovative approach where an LLM is prompted
    to generate multiple questions based on the answer and then calculates the mean
    cosine similarity between these generated questions and the original question.
    This method helps identify answers that may be factually correct but off-topic
    or incomplete.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**答案相关性**：此指标评估生成的答案与给定提示的相关性。它采用了一种创新的方法，即提示LLM根据答案生成多个问题，然后计算这些生成问题与原始问题之间的平均余弦相似度。这种方法有助于识别那些可能事实正确但与主题无关或不完整的答案。'
- en: '**Context precision**: This metric evaluates whether all the ground-truth relevant
    items present in the contexts are ranked appropriately. It considers the position
    of relevant information within the retrieved context, rewarding systems that place
    the most pertinent information at the top.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文精确度**：此指标评估上下文中存在的所有真实相关项目是否被适当地排序。它考虑检索上下文中相关信息的位置，奖励将最相关信息置于顶部的系统。'
- en: '**Context recall**: This metric measures the extent to which the retrieved
    context aligns with the annotated answer (ground truth). It analyzes each claim
    in the ground truth answer to determine whether it can be attributed to the retrieved
    context, providing insights into the completeness of the retrieved information.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文召回率**：这个指标衡量检索到的上下文与标注答案（真实情况）的一致程度。它分析真实情况答案中的每个主张，以确定它是否可以归因于检索到的上下文，从而为检索信息的完整性提供见解。'
- en: Finally, Ragas also provides building blocks for monitoring RAG quality in production
    environments. This facilitates continuous improvement of RAG systems. By leveraging
    the evaluation results from test datasets and insights gathered from production
    monitoring, developers can iteratively enhance their applications. This might
    involve fine-tuning retrieval algorithms, adjusting prompt engineering strategies,
    or optimizing the balance between retrieved context and LLM generation.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Ragas还提供了用于在生产环境中监控RAG质量的构建模块。这促进了RAG系统的持续改进。通过利用测试数据集的评估结果和生产监控中收集的见解，开发者可以迭代地增强他们的应用程序。这可能包括微调检索算法、调整提示工程策略或优化检索上下文和LLM生成的平衡。
- en: Ragas can be complemented with another approach, based on custom classifiers.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Ragas可以与另一种基于自定义分类器的方法相补充。
- en: ARES
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ARES
- en: ARES (an automated evaluation framework for RAG systems) is a comprehensive
    tool designed to evaluate RAG systems. It offers an automated process that combines
    synthetic data generation with fine-tuned classifiers to assess various aspects
    of RAG performance, including context relevance, answer faithfulness, and answer
    relevance.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ARES（一个用于RAG系统的自动化评估框架）是一个综合性的工具，旨在评估RAG系统。它提供了一个自动化流程，结合合成数据生成和微调的分类器来评估RAG性能的各个方面，包括上下文相关性、答案忠实度和答案相关性。
- en: 'The ARES framework operates in three main stages: synthetic data generation,
    classifier training, and RAG evaluation. Each stage is configurable, allowing
    users to tailor the evaluation process to their specific needs and datasets.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ARES框架在三个主要阶段运行：合成数据生成、分类器训练和RAG评估。每个阶段都是可配置的，允许用户根据他们的特定需求和数据集定制评估过程。
- en: In the synthetic data generation stage, ARES creates datasets that closely mimic
    real-world scenarios for robust RAG testing. Users can configure this process
    by specifying document file paths, few-shot prompt files, and output locations
    for the synthetic queries. The framework supports various pre-trained language
    models for this task, with the default being google/flan-t5-xxl. Users can control
    the number of documents sampled and other parameters to balance between comprehensive
    coverage and computational efficiency.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在合成数据生成阶段，ARES创建与真实世界场景紧密相似的测试数据集，以进行鲁棒的RAG测试。用户可以通过指定文档文件路径、少量提示文件和合成查询的输出位置来配置此过程。该框架支持用于此任务的各种预训练语言模型，默认为google/flan-t5-xxl。用户可以控制采样文档的数量和其他参数，以在全面覆盖和计算效率之间取得平衡。
- en: '![A screenshot of a cell phone  Description automatically generated](img/B31105_07_02.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![手机屏幕截图  描述由系统自动生成](img/B31105_07_02.png)'
- en: 'Figure 7.2: Overview of the ARES evaluation framework'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：ARES评估框架概述
- en: The classifier training stage involves creating high-precision classifiers to
    determine the relevance and faithfulness of RAG outputs. Users can specify the
    classification dataset (typically generated from the previous stage), test set
    for evaluation, label columns, and model choice. ARES uses microsoft/deberta-v3-large
    as the default model but supports other Hugging Face models. Training parameters
    such as the number of epochs, patience value for early stopping, and learning
    rate can be fine-tuned to optimize classifier performance.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类器训练阶段，ARES创建高精度的分类器以确定RAG输出的相关性和忠实度。用户可以指定分类数据集（通常从前一个阶段生成）、用于评估的测试集、标签列和模型选择。ARES使用microsoft/deberta-v3-large作为默认模型，但支持其他Hugging
    Face模型。可以通过微调训练参数，如epoch数量、早期停止的耐心值和学习率，来优化分类器性能。
- en: The final stage, RAG evaluation, leverages the trained classifiers and synthetic
    data to assess the RAG model’s performance. Users provide evaluation datasets,
    few-shot examples for guiding the evaluation, classifier checkpoints, and gold
    label paths. ARES supports various evaluation metrics and can generate confidence
    intervals for its assessments.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的阶段，RAG评估，利用训练好的分类器和合成数据来评估RAG模型的性能。用户提供评估数据集、用于指导评估的少样本示例、分类器检查点和黄金标签路径。ARES支持各种评估指标，并可以为其评估生成置信区间。
- en: ARES offers flexible model execution options, supporting both cloud-based and
    local runs through vLLM integration. The framework also supports various artifact
    types (code snippets, documents, HTML, images, and so on), enabling comprehensive
    evaluation across different RAG system outputs.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ARES通过vLLM集成提供灵活的模型执行选项，支持基于云和本地运行。该框架还支持各种工件类型（代码片段、文档、HTML、图像等），使不同RAG系统输出的全面评估成为可能。
- en: In summary, Ragas and ARES complement each other through their distinct approaches
    to evaluation and dataset generation. Ragas’s strength in production monitoring
    and LLM-assisted metrics can be combined with ARES’s highly configurable evaluation
    process and classifier-based assessments. While Ragas may offer more nuanced evaluations
    based on LLM capabilities, ARES provides consistent and potentially faster evaluations
    once its classifiers are trained. Combining them offers a comprehensive evaluation
    framework, benefiting from quick iterations with Ragas and in-depth, customized
    evaluations with ARES at key stages.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，Ragas和ARES通过它们在评估和数据集生成方面的不同方法相互补充。Ragas在生产监控和LLM辅助指标方面的优势可以与ARES高度可配置的评估过程和基于分类器的评估相结合。虽然Ragas可能基于LLM能力提供更细致的评估，但一旦其分类器训练完成，ARES提供一致且可能更快的评估。将它们结合起来提供了一个全面的评估框架，从Ragas的快速迭代和ARES在关键阶段的深入、定制评估中受益。
- en: In the next section, we will create our own evaluation framework to evaluate
    our task-specific TwinLlama-3.1-8B model.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将创建自己的评估框架来评估我们的特定任务模型TwinLlama-3.1-8B。
- en: Evaluating TwinLlama-3.1-8B
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估TwinLlama-3.1-8B
- en: 'In the previous chapters, we created two models fine-tuned to generate high-quality
    posts and articles: TwinLlama-3.1-8B and TwinLlama-3.1-8B-DPO. Based on this summary,
    we want to assess their abilities to write text that is both accurate and well-written.
    In comparison, general-purpose fine-tuned models are accurate thanks to their
    extensive knowledge but often use overly formal and verbose language. With this
    fine-tuning, we want to adopt a more natural writing style, based on the original
    articles from the training set.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们创建了两个模型，经过微调以生成高质量的文章和帖子：TwinLlama-3.1-8B和TwinLlama-3.1-8B-DPO。基于这个总结，我们想要评估它们编写既准确又流畅文本的能力。相比之下，通用微调模型由于知识广泛而准确，但常常使用过于正式和冗长的语言。通过这次微调，我们希望采用更自然的写作风格，基于训练集中的原始文章。
- en: 'Due to the open-ended nature of this problem, we will leverage a judge LLM
    to evaluate the quality of the generated text. It will take both the instruction
    and the answer as inputs, and score it on a 1–3 scale based on two criteria:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个问题具有开放性，我们将利用一个裁判LLM来评估生成文本的质量。它将指令和答案作为输入，并根据两个标准在1-3的尺度上评分：
- en: '**Accuracy**: The degree of factual correctness and comprehensiveness of the
    information presented in the answer'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确性**：答案中呈现的信息的事实正确性和全面性的程度'
- en: '**Style**: The appropriateness of the tone and writing style for blog posts
    or social media content (no formal or academic expressions)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**风格**：博客文章或社交媒体内容的语气和写作风格的适当性（无正式或学术表达）'
- en: In our evaluation framework, we will use the test split of our instruction dataset
    to get test instructions. We will feed them to our models and generate answers.
    These answers will then be evaluated by our judge LLM (GPT-4o-mini), based on
    a prompt that specifies our criteria. Finally, we will analyze the scores and
    draw conclusions based on qualitative and quantitative evaluations.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的评估框架中，我们将使用指令数据集的测试分割来获取测试指令。我们将将它们输入到我们的模型中并生成答案。然后，这些答案将由我们的裁判LLM（GPT-4o-mini）根据一个指定我们标准的提示进行评估。最后，我们将分析分数，并根据定性和定量评估得出结论。
- en: Generating answers
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成答案
- en: The first step consists of efficiently generating answers for each instruction
    in our test set. In addition to our two models, we will also use meta-llama/Meta-Llama-3.1-8B-Instruct,
    the official instruct version of Llama-3.1-8B, as a reference point to better
    understand the trade-offs we made.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步包括高效地为测试集中的每个指令生成答案。除了我们的两个模型外，我们还将使用 meta-llama/Meta-Llama-3.1-8B-Instruct，这是
    Llama-3.1-8B 的官方 instruct 版本，作为参考点，以更好地理解我们所做的权衡。
- en: 'Let’s start the first stage of the implementation:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始实施的第一个阶段：
- en: 'We import the relevant libraries, including vLLM for fast generation. This
    library is a lot faster than transformers for batch generation with local models:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入相关的库，包括用于快速生成的 vLLM。这个库在本地模型批量生成方面比 transformers 快得多：
- en: '[PRE0]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We define a function called `generate_answers` that will process our dataset
    and generate responses using a specified model. It takes two inputs—the ID of
    the model we want to use and the name of the test dataset:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了一个名为 `generate_answers` 的函数，该函数将处理我们的数据集并使用指定的模型生成响应。它接受两个输入——我们想要使用的模型的
    ID 和测试数据集的名称：
- en: '[PRE1]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We need to format the raw instructions using the chat template our models have
    been trained on. Note that Llama-3.1-8B-Instruct has been used with a different
    template, but it can follow this simple format. Here, we use the same chat template
    with every model for simplicity. We map the entire test set to this template with
    the `format()` function:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要使用我们的模型训练的聊天模板格式化原始指令。请注意，Llama-3.1-8B-Instruct 已经使用了一个不同的模板，但它可以遵循这个简单的格式。在这里，为了简单起见，我们使用与每个模型相同的聊天模板。我们使用
    `format()` 函数将整个测试集映射到这个模板：
- en: '[PRE2]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s initialize the LLM object used by vLLM with a maximum length of 4,096
    tokens. We can also specify sampling parameters, which correspond to variables
    used in the decoding strategy. Here, we use parameters to encourage diversity
    (high temperature) while removing the most unlikely tokens (`top_p` and `min_p`).
    Finally, we start the generation by providing the list of prompts with `dataset["prompt"]`:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们用最大长度为 4,096 个标记初始化 vLLM 使用的 LLM 对象。我们还可以指定采样参数，这些参数对应于解码策略中使用的变量。在这里，我们使用参数来鼓励多样性（高温度）同时移除最不可能的标记（`top_p`
    和 `min_p`）。最后，我们通过提供 `dataset["prompt"]` 中的提示列表来开始生成：
- en: '[PRE3]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This process should take a few minutes with our 334 prompts. Once this is done,
    we extract the answers from the object that is outputted by vLLM. We then add
    these answers as a new column to our dataset. This is useful to log the answers
    and review them later:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们的 334 个提示，这个过程应该需要几分钟。一旦完成，我们就从 vLLM 输出的对象中提取答案。然后，我们将这些答案作为新列添加到我们的数据集中。这有助于记录答案并在以后进行审查：
- en: '[PRE4]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We save our results to the Hugging Face Hub for easy access later. Then, we
    clear our GPU memory to prevent running out of space when we process the next
    model:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将结果保存到 Hugging Face Hub 以便以后方便访问。然后，我们清理 GPU 内存，以防止在处理下一个模型时空间不足：
- en: '[PRE5]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We create a list of the three models we want to test. Then, we run our `generate_answers()`
    function for each of these models, one at a time. This will create and upload
    a separate set of results for each model:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了一个包含我们想要测试的三个模型的列表。然后，我们逐个运行这些模型的 `generate_answers()` 函数。这将为每个模型创建和上传一组单独的结果：
- en: '[PRE6]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now that we have the answer generation, we can move on to the evaluation process.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了答案生成，我们可以继续到评估过程。
- en: Evaluating answers
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估答案
- en: 'To evaluate our answers, we will rely on GPT-4o-mini as a judge. This strategy
    is similar to what we used for data generation. As a matter of fact, you could
    adapt it to filter out bad samples during the data generation process. Here, we
    will score every generated answer from every model in terms of accuracy and style.
    The average scores will inform us about the quality of our fine-tuning compared
    to Llama-3.1-8B-Instruct:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们的答案，我们将依赖 GPT-4o-mini 作为评判者。这种策略与我们用于数据生成时使用的策略相似。事实上，你可以将其修改为在数据生成过程中过滤掉坏样本。在这里，我们将根据准确性和风格对每个模型生成的每个答案进行评分。平均分数将告诉我们我们的微调质量与
    Llama-3.1-8B-Instruct 相比如何：
- en: 'First, we import the required libraries, including `openai`:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入所需的库，包括 `openai`：
- en: '[PRE7]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We then define the `evaluate_answer()` function. This function contains our
    evaluation prompt, which sets up the context for evaluating answers based on accuracy
    and style:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义了 `evaluate_answer()` 函数。这个函数包含我们的评估提示，它根据准确性和风格设置评估答案的上下文：
- en: '[PRE8]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the same prompt, we define our scales for each metric. Those are three-point
    Likert scales with a precise definition for each score:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在相同的提示中，我们为每个指标定义了我们的刻度。这些是三点李克特量表，每个分数都有精确的定义：
- en: '[PRE9]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, we conclude the prompt with two examples to illustrate what we mean
    by “*complex words*” and “*formal* or *academic language*.” We provide the corresponding
    instruction-answer pair and ask the model to return a response in JSON:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们通过两个示例来总结提示，说明我们所说的“*复杂词汇*”和“*正式*或*学术语言*”。我们提供相应的指令-答案对，并要求模型以JSON格式返回响应：
- en: '[PRE10]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This prompt is given as a user query to the GPT-4o-mini model. The system prompt
    reinforces that we are interested in answer evaluation based on accuracy and style:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个提示作为用户查询提供给GPT-4o-mini模型。系统提示强化了我们对基于准确性和风格评估答案的兴趣：
- en: '[PRE11]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As in the previous chapters, we will batch our requests to speed up the process.
    This is why we create an `evaluate_batch()` function, which returns a list of
    parsed structured outputs with their corresponding indices. These indices are
    important to ensure a correct ordering of the evaluations:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前几章所述，我们将批量处理请求以加快处理速度。这就是我们创建`evaluate_batch()`函数的原因，该函数返回一个包含对应索引的解析结构化输出列表。这些索引对于确保评估的正确顺序非常重要：
- en: '[PRE12]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can now orchestrate the previous code in the `evaluate_answers()` function.
    It takes the model ID, number of threads, and batch size as inputs. First, we
    load the dataset with the generations we previously saved:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以在`evaluate_answers()`函数中编排之前的代码。该函数接收模型ID、线程数和批处理大小作为输入。首先，我们加载之前保存的生成数据集：
- en: '[PRE13]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We create batches of instruction-answer pairs from our dataset. Each batch
    contains `batch_size` number of pairs:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从数据集中创建指令-答案对的批次。每个批次包含`batch_size`数量的对：
- en: '[PRE14]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We perform parallel evaluation of batches of instruction-answer pairs using
    multiple threads. We use parallel processing to evaluate multiple batches simultaneously,
    speeding up the overall evaluation process. The `ThreadPoolExecutor` submits each
    batch to `evaluate_batch()`. The results are stored in the evaluations list:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用多个线程并行评估指令-答案对的批次。我们使用并行处理同时评估多个批次，从而加快整体评估过程。`ThreadPoolExecutor`将每个批次提交给`evaluate_batch()`。结果存储在评估列表中：
- en: '[PRE15]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We create a new column with the result of the evaluation process. This column
    will store the raw JSON output of the judge model, including scores and explanations:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个新的列来存储评估过程的结果。这个列将存储裁判模型的原始JSON输出，包括分数和解释：
- en: '[PRE16]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can directly parse this JSON object with `json.loads()` and try to retrieve
    the accuracy and style scores that should have been generated. This generation
    is in best-effort mode, which means that scores are not guaranteed. If there’s
    an error in parsing, we use `None` values as a fallback:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以直接使用`json.loads()`解析这个JSON对象，并尝试检索应该生成的准确性和风格分数。这种生成处于尽力而为模式，这意味着分数不能保证。如果解析出现错误，我们使用`None`值作为后备：
- en: '[PRE17]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We add two new columns to store the accuracy and style scores for further analysis:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们添加两个新列来存储准确性和风格分数，以进行进一步分析：
- en: '[PRE18]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let’s push the final dataset with generated answers, evaluations, and scores
    to the Hugging Face Hub:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将生成的答案、评估和分数的最终数据集推送到Hugging Face Hub：
- en: '[PRE19]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can now call the `evaluate_answers()` function with the three models we
    selected:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以用我们选择的三个模型调用`evaluate_answers()`函数：
- en: '[PRE20]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: By saving intermediate results, our evaluation framework becomes more robust
    to failures. It is also easily extendable to other models, datasets, and criteria.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 通过保存中间结果，我们的评估框架对失败更加健壮。它也容易扩展到其他模型、数据集和标准。
- en: Analyzing results
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析结果
- en: 'There are three elements we can review at the end of this evaluation process:
    model answers, explanations from evaluations, and scores.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个评估过程的最后，我们可以审查三个元素：模型答案、评估的解释和分数。
- en: While reviewing answers is not scalable, it is a crucial step that can help
    to identify a lot of common mistakes, such as a wrong chat template or incorrect
    model. Here, we will focus on a single instruction and read the answers provided
    by our two models and Llama-3.1-8B-Instruct.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然审查答案不可扩展，但它是一个至关重要的步骤，可以帮助识别许多常见错误，例如错误的聊天模板或错误的模型。在这里，我们将关注一个单独的指令，并阅读我们两个模型和Llama-3.1-8B-Instruct提供的答案：
- en: '| **Instruction**Discuss the concept of algorithm bias and its implications.
    |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| **指令**讨论算法偏差的概念及其影响。|'
- en: '| **TwinLlama-3.1-8B**Algorithm bias refers to the tendency of algorithms to
    produce outcomes that are skewed or unfair due to underlying assumptions or programming
    biases. This can have significant implications, particularly in fields like machine
    learning and AI, where algorithms are relied upon to make critical decisions.
    If an algorithm is not properly vetted for bias, it may lead to discriminatory
    outcomes, such as lending practices that disproportionately affect certain demographic
    groups or recruitment processes that unfairly favor certain candidates. It is
    essential to ensure that algorithms are designed and implemented with fairness
    and accuracy in mind to mitigate these risks. |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| **TwinLlama-3.1-8B**算法偏差指的是算法由于潜在假设或编程偏差而倾向于产生偏斜或不公平的结果。这可能会产生重大影响，尤其是在机器学习和人工智能等领域，这些领域依赖于算法做出关键决策。如果一个算法没有经过适当的偏见审查，可能会导致歧视性结果，例如贷款实践对某些人口群体产生不成比例的影响或招聘流程不公平地偏袒某些候选人。确保算法在设计和管理时考虑到公平性和准确性，以减轻这些风险是至关重要的。|'
- en: '| **TwinLlama-3.1-8B-DPO**Algorithm bias refers to the tendency of algorithms
    to produce outcomes that are skewed or unfair due to underlying assumptions or
    programming biases. This can have significant implications, particularly in fields
    like machine learning and AI, where algorithms are relied upon to make critical
    decisions. If an algorithm is not properly vetted for bias, it could lead to discriminatory
    outcomes, such as denying loans to certain demographic groups or perpetuating
    stereotypes. Therefore, it is essential to address algorithmic bias proactively
    to ensure that the technology we develop is equitable and just. |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| **TwinLlama-3.1-8B-DPO**算法偏差指的是算法由于潜在假设或编程偏差而倾向于产生偏斜或不公平的结果。这可能会产生重大影响，尤其是在机器学习和人工智能等领域，这些领域依赖于算法做出关键决策。如果一个算法没有经过适当的偏见审查，可能会导致歧视性结果，例如拒绝向某些人口群体发放贷款或持续传播刻板印象。因此，积极应对算法偏差对于确保我们开发的技术公平和正义至关重要。|'
- en: '| **Llama-3.1-8B-Instruct**Algorithm bias, also known as algorithmic bias,
    refers to the unintended or inherent bias in machine learning models that can
    affect their performance, accuracy, and fairness. This bias can be caused by various
    factors, including the data used to train the model, the algorithm itself, and
    the biases present in the society or culture. One of the most significant implications
    of algorithm bias is its potential to perpetuate and exacerbate existing social
    inequalities. For instance, facial recognition software has been shown to be less
    accurate for people of color, leading to wrongful arrests and discrimination.
    Similarly, language processing models have been found to be biased towards the
    language and dialect of the dominant group, making it difficult for marginalized
    communities to access information and services. Algorithm bias can also have severe
    consequences in areas such as healthcare, education, and employment. For example,
    a medical diagnosis algorithm that is biased towards certain demographics may
    misdiagnose or underdiagnose conditions in other groups, leading to delayed or
    inadequate treatment. In education, an algorithm that is biased towards certain
    learning styles may disproportionately affect students with different learning
    abilities, leading to unequal opportunities. To mitigate the effects of algorithm
    bias, it is essential to address the root causes of bias and implement measures
    to ensure fairness and transparency in machine learning models. This can be achieved
    through techniques such as data cleansing, regularization, and debiasing, as well
    as implementing diverse and representative datasets. Additionally, it is crucial
    to involve diverse stakeholders, including members of marginalized communities,
    in the development and testing of algorithms to ensure that they are fair and
    equitable. In conclusion, algorithm bias is a significant concern that can have
    far-reaching implications for individuals and society as a whole. It is essential
    to recognize the potential biases in machine learning models and take proactive
    steps to mitigate their effects. By doing so, we can create more equitable and
    just systems that benefit everyone, regardless of their background or demographic
    characteristics. |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| **Llama-3.1-8B-Instruct** 算法偏差，也称为算法偏差，指的是机器学习模型中可能影响其性能、准确性和公平性的无意或固有偏差。这种偏差可能由各种因素引起，包括用于训练模型的数据、算法本身以及社会或文化中存在的偏差。算法偏差的最显著影响之一是其可能持续并加剧现有的社会不平等。例如，面部识别软件已被证明对有色人种的准确性较低，导致错误的逮捕和歧视。同样，语言处理模型被发现偏向于主流群体的语言和方言，使得边缘化社区难以获取信息和提供服务。算法偏差在医疗保健、教育和就业等领域也可能产生严重后果。例如，偏向于某些人口统计数据的医疗诊断算法可能会误诊或漏诊其他群体的条件，导致治疗延迟或不充分。在教育领域，偏向于某些学习风格的算法可能会不成比例地影响不同学习能力的学生，导致机会不均等。为了减轻算法偏差的影响，解决偏差的根本原因并实施确保机器学习模型公平性和透明度的措施至关重要。这可以通过数据清洗、正则化和去偏技术实现，以及实施多样化和代表性数据集。此外，至关重要的是要涉及多元化的利益相关者，包括边缘化社区的成员，以确保算法的开发和测试是公平和公正的。总之，算法偏差是一个重大问题，可能对个人和社会整体产生深远影响。认识到机器学习模型中的潜在偏差并采取积极措施减轻其影响至关重要。通过这样做，我们可以创建更加公平和正义的系统，使所有人受益，无论其背景或人口统计特征如何。|'
- en: '*Table 7.4*: Example of generated answers for a given instruction'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '*表7.4*：给定指令生成的答案示例'
- en: Immediately, we can see that the answers from our models are very close to each
    other. This is not the case with Llama-3.1-8B-Instruct’s answer, which is extremely
    verbose and lists many examples. This is typically the kind of answer that is
    correct but can feel artificial and unpleasant to read because of its length.
    On the other hand, the DPO model slightly simplifies the language of the SFT version,
    making it less academic. This is exactly the behavior we want to capture, modifying
    the writing style but not the actual content of the answer.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 立即可以看出，我们模型的答案非常接近。Llama-3.1-8B-Instruct的答案并非如此，它非常冗长，列举了许多例子。这种类型的答案通常是正确的，但由于其长度，可能会感觉人工且不愉快。另一方面，DPO模型略微简化了SFT版本的措辞，使其不那么学术化。这正是我们想要捕捉的行为，即修改写作风格但不改变答案的实际内容。
- en: Let’s now review the evaluations provided by GPT-4o-mini for each answer.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来回顾GPT-4o-mini为每个答案提供的评估。
- en: '| **TwinLlama-3.1-8B** |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| **TwinLlama-3.1-8B** |'
- en: '| **Accuracy**The answer accurately defines algorithm bias and highlights its
    implications in fields like machine learning and AI. It correctly identifies the
    risks of discriminatory outcomes due to biased algorithms. There are no apparent
    factual errors, making this a comprehensive response.Score: 3 | **Style**The tone
    is informative and relatively accessible, though it tends towards a formal tone
    in some sections (e.g., ‘underlying assumptions or programming biases’). While
    it does use some technical terms appropriately, it could be simplified further
    for a broader audience, especially for blog or social media content.Score: 2 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| **准确性**该答案准确定义了算法偏差，并强调了其在机器学习、人工智能等领域的含义。它正确地识别了由于偏差算法导致的歧视性结果的风险。没有明显的事实错误，这使得这是一个全面的回答。评分：3
    | **风格**语气信息丰富且相对易于理解，尽管在某些部分（例如，“潜在假设或编程偏差”）中趋向于正式。虽然它适当地使用了一些技术术语，但可以进一步简化，以适应更广泛的受众，特别是对于博客或社交媒体内容。评分：2
    |'
- en: '| **TwinLlama-3.1-8B-DPO** |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| **TwinLlama-3.1-8B-DPO** |'
- en: '| **Accuracy**The answer accurately defines algorithm bias and outlines its
    implications in critical fields like machine learning and AI. It highlights the
    potential for discriminatory outcomes, making it a comprehensive overview of the
    topic without any factual errors.Score: 3 | **Style**The writing style is clear,
    accessible, and effectively communicates the technical concept of algorithm bias
    without becoming overly formal. While it uses some technical terms, they are appropriate
    and explained well for a general audience.Score: 3 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| **准确性**该答案准确定义了算法偏差，并概述了其在机器学习、人工智能等关键领域的含义。它突出了可能导致歧视性结果的可能性，因此是对该主题的全面概述，没有任何事实错误。评分：3
    | **风格**写作风格清晰、易于理解，有效地传达了算法偏差的技术概念，而没有变得过于正式。虽然它使用了一些技术术语，但对于一般受众来说，它们是适当且解释良好的。评分：3
    |'
- en: '| **Llama-3.1-8B-Instruct** |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| **Llama-3.1-8B-Instruct** |'
- en: '| **Accuracy**The answer accurately defines algorithm bias and discusses its
    causes and implications in various domains such as facial recognition, healthcare,
    and education. It also correctly identifies methods to mitigate bias, such as
    involving diverse stakeholders and employing debiasing techniques. Overall, the
    information is factually correct and comprehensive.Score: 3 | **Style**The tone
    is informative and accessible, with technical terms explained clearly. However,
    some phrases like ‘unintended or inherent bias’ and ‘implementation of diverse
    and representative datasets’ could be simplified for broader audience appeal.
    The writing is generally suitable for blog or social media content, but it could
    benefit from a more conversational tone.Score: 2 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| **准确性**该答案准确定义了算法偏差，并讨论了其在面部识别、医疗保健和教育等各个领域的成因和影响。它还正确地识别了减轻偏差的方法，例如涉及多元化的利益相关者和采用去偏技术。总体而言，信息是事实性的且全面的。评分：3
    | **风格**语气信息丰富且易于理解，技术术语解释清晰。然而，一些短语如“无意或固有的偏差”和“实施多元化和代表性数据集”可能需要简化以吸引更广泛的受众。写作总体上适合博客或社交媒体内容，但可能从更随和的语气中受益。评分：2
    |'
- en: '*Table 7.5*: Evaluations of each answer made by GPT-4o-mini, according to style
    and accuracy'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '*表7.5*：根据风格和准确性对GPT-4o-mini每个答案的评价'
- en: According to our judge LLM, there is no issue with the accuracy of the answers,
    which get a perfect score. However, the style is considered too formal for TwinLlama-3.1-8B
    (SFT) and Llama-3.1-8B-Instruct, with a score of 2\. The judge LLM agreed with
    our previous analysis and assigned a perfect score to TwinLlama-3.1-8B-DPO’s answer
    for communicating “the technical concept of algorithm bias without becoming overly
    formal.”
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的评估者LLM，答案的准确性没有问题，获得了满分。然而，对于TwinLlama-3.1-8B（SFT）和Llama-3.1-8B-Instruct，其风格被认为过于正式，评分为2。评估者LLM同意我们之前的分析，并将满分分配给了TwinLlama-3.1-8B-DPO的答案，因为它“在不过度正式的情况下传达了算法偏差的技术概念”。
- en: 'This trend is confirmed by the average scores obtained by each model:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这种趋势由每个模型获得的平均分数得到证实：
- en: '[PRE21]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In terms of accuracy, our two fine-tuned models get similar scores, while Llama-3.1-8B-Instruct
    achieves the highest accuracy score of 2.62\. This suggests that the instruct-tuned
    Llama model may have a slight edge in providing factually correct information.
    This is probably due to its extensive post-training process with over 10 million
    samples (compared to 13,000 in our case).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在准确性方面，我们的两个微调模型获得了相似的分数，而Llama-3.1-8B-Instruct实现了最高的准确性分数2.62。这表明，经过训练的Llama模型在提供事实性信息方面可能略有优势。这可能是由于其超过1000万个样本的广泛后训练过程（相比之下，我们的案例中为13000个）。
- en: However, when it comes to style, we see a different pattern. TwinLlama-3.1-8B-DPO
    leads with a score of 2.12, successfully achieving a more accessible and less
    formal writing style without sacrificing content quality. TwinLlama-3.1-8B (SFT)
    follows with 2.04, showing improvement but retaining some formality, while Llama-3.1-8B-Instruct
    trails with 1.86, tending toward verbosity.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当涉及到风格时，我们看到了不同的模式。TwinLlama-3.1-8B-DPO以2.12的分数领先，成功实现了更易于理解和不太正式的写作风格，同时没有牺牲内容质量。TwinLlama-3.1-8B（SFT）以2.04的分数紧随其后，显示出改进但保留了一些正式性，而Llama-3.1-8B-Instruct以1.86的分数落后，趋向于冗长。
- en: Based on this feedback and the manual review of the generated answers, we can
    detect mistakes and identify areas for improvement. This is essential for refining
    the data generation process through additional filtering or augmenting the dataset
    with missing information. While this first version already shows promising results,
    iterating over different datasets and models will allow us to significantly outperform
    our baseline and create the best possible model for our use case.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些反馈和手动审查生成的答案，我们可以检测错误并确定改进领域。这对于通过额外过滤或用缺失信息增强数据集来完善数据生成过程至关重要。虽然这个版本已经显示出有希望的结果，但通过迭代不同的数据集和模型，我们将能够显著超越基线，并为我们的用例创建最佳模型。
- en: Summary
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored LLM evaluation with models and RAG systems. We
    saw how to interpret classic benchmarks like MMLU to select strong candidates
    to use or fine-tune. We also detailed how domain-specific and task-specific evaluations
    work, and how to create our own based on publicly available examples.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了使用模型和RAG系统进行LLM评估。我们看到了如何解释经典基准如MMLU来选择使用或微调的强大候选者。我们还详细介绍了特定领域和特定任务的评估如何工作，以及如何根据公开可用的示例创建自己的评估。
- en: We focused on two techniques (multiple-choice question answering and LLM-as-a-judge)
    as the backbone of these custom evaluation frameworks.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些定制评估框架的核心聚焦于两种技术（多项选择题回答和LLM作为裁判）。
- en: 'However, models are commonly integrated into broader systems that provide additional
    context. We introduced two evaluation frameworks for RAG systems, Ragas and ARES.
    We saw both similarities (for example, synthetic data generation) and differences
    in how they evaluate RAG systems (context-based metrics versus trained classifiers).
    Finally, we evaluated TwinLlama-3.1-8B with a judge LLM according to three criteria:
    relevance, coherence, and conciseness. This provided insights into how we can
    improve it.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，模型通常被集成到提供额外上下文的更广泛系统中。我们为RAG系统引入了两个评估框架，Ragas和ARES。我们看到了它们在评估RAG系统方面的相似之处（例如，合成数据生成）和差异（基于上下文的指标与训练分类器）。最后，我们根据相关性、连贯性和简洁性三个标准，使用裁判LLM评估了TwinLlama-3.1-8B。这为我们提供了如何改进它的见解。
- en: In the next chapter, we will explore inference optimization techniques to improve
    speed and reduce memory usage, without significantly compromising model performance.
    We will also delve into optimization methods, model parallelism techniques and
    examine different quantization approaches.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨推理优化技术以提高速度并减少内存使用，同时不会显著影响模型性能。我们还将深入研究优化方法、模型并行技术，并考察不同的量化方法。
- en: References
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Lianmin Zheng et al.. “*Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena*.”
    arXiv preprint arXiv:2306.05685, June 2023.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lianmin Zheng等人。“*使用MT-Bench和聊天机器人竞技场评估LLM作为裁判*。”arXiv预印本arXiv:2306.05685，2023年6月。
- en: Aymeric Roucher. “*Using LLM-as-a-judge for an automated and versatile evaluation
    - Hugging Face Open-Source AI Cookbook*.” huggingface.co, No date found, [https://huggingface.co/learn/cookbook/en/llm_judge](https://huggingface.co/learn/cookbook/en/llm_judge).
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aymeric Roucher。“*使用LLM作为裁判进行自动化和通用的评估 - Hugging Face开源AI食谱*。”huggingface.co，未找到日期，[https://huggingface.co/learn/cookbook/en/llm_judge](https://huggingface.co/learn/cookbook/en/llm_judge)。
- en: LangChain. “*Aligning LLM-as-a-Judge with Human Preferences.” blog.langchain.dev*,
    June 26, 2024, [https://blog.langchain.dev/aligning-llm-as-a-judge-with-human-preferences/](https://blog.langchain.dev/aligning-llm-as-a-judge-with-human-preferences/).
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LangChain。“*将LLM作为裁判与人类偏好对齐。”blog.langchain.dev，2024年6月26日，[https://blog.langchain.dev/aligning-llm-as-a-judge-with-human-preferences/](https://blog.langchain.dev/aligning-llm-as-a-judge-with-human-preferences/)。
- en: Dan Hendrycks et al.. “*Measuring Massive Multitask Language Understanding*.”
    arXiv preprint arXiv:2009.03300, September 2020.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dan Hendrycks等人。“*衡量大规模多任务语言理解*。”arXiv预印本arXiv:2009.03300，2020年9月。
- en: Jeffrey Zhou et al.. “*Instruction-Following Evaluation for Large Language Models*.”
    arXiv preprint arXiv:2311.07911, November 2023.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeffrey Zhou 等人. “*大型语言模型的指令遵循评估*.” arXiv 预印本 arXiv:2311.07911, 2023年11月.
- en: 'Yann Dubois et al.. “*Length-Controlled AlpacaEval: A Simple Way to Debias
    Automatic Evaluators*.” arXiv preprint arXiv:2404.04475, April 2024.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yann Dubois 等人. “*长度控制的 AlpacaEval: 一种简单的方法来消除自动评估器的偏差*.” arXiv 预印本 arXiv:2404.04475,
    2024年4月.'
- en: 'Grégoire Mialon et al.. “*GAIA: a benchmark for General AI Assistants*.” arXiv
    preprint arXiv:2311.12983, November 2023.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Grégoire Mialon 等人. “*GAIA: 通用人工智能助手的基准*.” arXiv 预印本 arXiv:2311.12983, 2023年11月.'
- en: Giwon Hong et al.. “*The Hallucinations Leaderboard -- An Open Effort to Measure
    Hallucinations in Large Language Models*.” arXiv preprint arXiv:2404.05904, April
    2024.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Giwon Hong 等人. “*幻觉排行榜 -- 一个衡量大型语言模型中幻觉的公开努力*.” arXiv 预印本 arXiv:2404.05904,
    2024年4月.
- en: 'Shahul Es et al.. “*RAGAS: Automated Evaluation of Retrieval Augmented Generation*.”
    arXiv preprint arXiv:2309.15217, September 2023.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shahul Es 等人. “*RAGAS: 自动评估检索增强生成*.” arXiv 预印本 arXiv:2309.15217, 2023年9月.'
- en: 'Jon Saad-Falcon et al.. “*ARES: An Automated Evaluation Framework for Retrieval-Augmented
    Generation Systems*.” arXiv preprint arXiv:2311.09476, November 2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jon Saad-Falcon 等人. “*ARES: 检索增强生成系统的自动评估框架*.” arXiv 预印本 arXiv:2311.09476,
    2023年11月.'
- en: Join our book’s Discord space
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/llmeng](https://packt.link/llmeng)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/llmeng](https://packt.link/llmeng)'
- en: '![](img/QR_Code79969828252392890.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code79969828252392890.png)'
