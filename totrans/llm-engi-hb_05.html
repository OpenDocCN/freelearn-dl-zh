<html><head></head><body>
  <div><h1 class="chapterNumber">5</h1>
    <h1 id="_idParaDest-135" class="chapterTitle">Supervised Fine-Tuning</h1>
    <p class="normal"><strong class="keyWord">Supervised Fine-Tuning</strong> (<strong class="keyWord">SFT</strong>) is a<a id="_idIndexMarker418"/> crucial step in preparing LLMs for real-world applications. Following the initial pre-training phase, where an LLM learns to predict the next token in a sequence, SFT refines the model’s capabilities using carefully curated pairs of instructions and corresponding answers. This process serves two primary purposes: it teaches the model to understand and follow a specific chat format, effectively transforming it into a conversational agent, and it allows the model to adapt its broad knowledge base to excel in targeted tasks or specialized domains.</p>
    <p class="normal">The importance of SFT lies in its ability to bridge the gap between a model’s general language understanding and its practical utility. By exposing the model to examples of desired input-output patterns, SFT shapes the LLM’s behavior to align with specific goals, whether they involve task completion (such as summarization or translation) or domain expertise (like medical or legal knowledge). This tailored approach not only enhances the model’s performance in intended areas but also improves its ability to follow instructions and generate more relevant and coherent responses.</p>
    <p class="normal">In this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bulletList">Creating a high-quality instruction dataset</li>
      <li class="bulletList">SFT techniques</li>
      <li class="bulletList">Implementing fine-tuning in practice</li>
    </ul>
    <p class="normal">By the end of this chapter, you will be able to create your own instruction datasets and efficiently fine-tune LLMs on them.</p>
    <div><p class="normal">All the code examples from this chapter can be found on GitHub at <a href="https://github.com/PacktPublishing/LLM-Engineering">https://github.com/PacktPublishing/LLM-Engineering</a>.</p>
    </div>
    <h1 id="_idParaDest-136" class="heading-1">Creating an instruction dataset</h1>
    <p class="normal">In most use cases, creating <a id="_idIndexMarker419"/>an instruction dataset is the most difficult part of the fine-tuning process. This is due to multiple factors. Most use cases can be connected to raw text, but it is rare to find natural pairs of instructions and answers. This raw text needs to be transformed into a format that includes both instructions and answers. Moreover, the quality of the data is also crucial. Because of this, a lot of time is invested in manually checking and verifying individual samples. This careful review helps ensure that the dataset is accurate and useful for training the model.</p>
    <figure class="mediaobject"><img src="img/B31105_05_01.png" alt="A diagram of a data flow  Description automatically generated"/></figure>
    <p class="packt_figref">Figure <em class="italic">5</em>.1 – Overview of the post-training data pipeline covered in this chapter</p>
    <p class="normal">In this section, we will introduce a general framework to create your own instruction datasets, regardless of the final use case. We will then leverage the scraped data from <em class="chapterRef">Chapter 3</em> and transform it into an instruction dataset. The different stages in our data generation pipeline are summarized in <em class="italic">Figure 5.1</em>.</p>
    <h2 id="_idParaDest-137" class="heading-2">General framework</h2>
    <p class="normal">Instruction datasets<a id="_idIndexMarker420"/> are defined as pairs of instructions and answers. The instructions are the inputs of the model, used as context during fine-tuning. The answers are the expected outputs of the model. During fine-tuning, you can choose to train the model on the instructions and answers, or on answers only. Pairs of instructions and answers follow a certain template. Some instruction templates, such as Alpaca, introduce additional fields like <code class="inlineCode">inputs</code> and <code class="inlineCode">system</code>. Both of them can be considered subfields of the <code class="inlineCode">instruction</code> field. In this case, “inputs” contain the data the model needs to complete the instruction, and “system” is a meta-prompt to steer the general behavior of the model. Here is an example from the SlimOrca dataset, with “system” and “instruction”:</p>
    <table id="table001-1" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">System</strong></p>
            <p class="normal">You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Instruction</strong></p>
            <p class="normal">Concepts: building, shop, town</p>
            <p class="normal">Write a sentence that includes all these words.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Output</strong></p>
            <p class="normal">In our little town, there is a shop inside a big building where people go to buy their favorite toys and candies.</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref"><em class="italic">Table 5.1</em> – Example of sample from the Open-Orca/SlimOrca dataset</p>
    <p class="normal">This example illustrates how the “system” field is used to define specific behaviors for the model, such as being helpful, always providing explanations, and tailoring responses as if speaking to a five-year-old. The “instruction” field provides the necessary data (the concepts) and the task (constructing a sentence). The <code class="inlineCode">output</code> field shows the expected answer, which, while not the only possible answer, represents a high-quality response.</p>
    <p class="normal">To build an instruction dataset, we want to curate data that is representative of how the model will be used. Once we have gathered enough samples, our goal is to filter them to only keep high-quality data. In this context, high-quality data can be described <a id="_idIndexMarker421"/>through three main dimensions:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Accuracy</strong>: It refers to the<a id="_idIndexMarker422"/> factual correctness and relevance of the samples. In the context of instruction datasets, this means ensuring that responses are not only factually accurate but also relevant to their corresponding instructions. High accuracy is essential for training models that can provide reliable and trustworthy information.</li>
      <li class="bulletList"><strong class="keyWord">Diversity</strong>: A high-quality dataset should encompass a wide range of use cases, covering the potential queries and tasks the deployed LLM might encounter. This diversity should span topics, contexts, text lengths, and writing styles. By sampling data in a representative manner, we allow models to develop robust instruction-following capabilities.</li>
      <li class="bulletList"><strong class="keyWord">Complexity</strong>: Trivial or overly simplistic samples do little to improve an LLM’s capabilities. Instead, datasets should include complex, multi-step reasoning problems and challenging tasks that push the boundaries of what the model is expected to handle. This complexity helps in developing models capable of tackling complex real-world problems.</li>
    </ul>
    <p class="normal">In the following sections, we will see techniques to filter and evaluate instruction samples <a id="_idIndexMarker423"/>according to these dimensions.</p>
    <h3 id="_idParaDest-138" class="heading-3">Data quantity</h3>
    <p class="normal">The Hugging<a id="_idIndexMarker424"/> Face <a id="_idIndexMarker425"/>Hub contains numerous instruction datasets, which can be general-purpose or designed for particular tasks or domains. When working on a new use case, it can be beneficial to look for related open-source datasets to leverage for fine-tuning. This is particularly important if your number of samples is too low (for example, fewer than 1,000), requiring you to augment it with high-quality data.</p>
    <figure class="mediaobject"><img src="img/B31105_05_02.png" alt="A screenshot of a computer  Description automatically generated"/></figure>
    <p class="packt_figref">Figure <em class="italic">5</em>.2 – Screenshot of the most-liked datasets on the Hugging Face Hub</p>
    <p class="normal">Calculating an ideal<a id="_idIndexMarker426"/> number of samples is a difficult task, as<a id="_idIndexMarker427"/> both the quality of the data and the size of the model can have a dramatic impact. For large models (around 70 billion parameters, for example), this number can be as low as 1,000 high-quality samples (see the LIMA paper in the <em class="italic">References</em> section). This is not true for smaller models (around seven billion parameters, for instance), as they need more samples to simply learn the correct chat template. In any case, the quality of the data is a crucial factor, and a high number of samples is always desirable.</p>
    <p class="normal">To provide additional numbers, we can look at the fine-tuned models developed by companies and the open-source community. We can distinguish two types of finetunes: general-purpose, aimed to reproduce the capabilities of models like GPT, and task- or domain-specific models, designed to optimize their performance for a particular application.</p>
    <p class="normal">General-purpose models cover more topics, which requires additional samples. Among companies, we observe a wide range of values. For instance, Yi models from 01-ai rely on less than 10,000 samples. At the opposite range of the spectrum, Meta reported using 10 million samples for Llama 3 through the entire fine-tuning process (including preference alignment). In the open-source community, models like OpenHermes and Dolphin use around one million samples. Based on the quality of these finetunes, we recommend an instruction dataset of at least one million samples to create a good general-purpose instruct model. On the other hand, models fine-tuned for a specific purpose require fewer samples. Here, we differentiate task-specific models from domain-specific ones.</p>
    <p class="normal">Task-specific and domain-specific models represent two distinct approaches to fine-tuning LLMs. Task-specific models are designed to excel at a particular function, such as translation, summarization, or sentiment analysis. These models benefit from a focused training approach on a single task, allowing for efficient performance even with smaller model sizes (typically less than 8 billion parameters). The data required for task-specific fine-tuning is generally more manageable, ranging from 100 to 100,000 samples. This makes task-specific fine-tuning an attractive option<a id="_idIndexMarker428"/> for many applications where resources<a id="_idIndexMarker429"/> may be limited.</p>
    <p class="normal">Domain-specific models, on the other hand, aim to tweak the LLM with specialized knowledge and familiarity with the vocabulary and linguistic patterns of a particular field. These models are valuable in areas such as medicine, law, finance, e-commerce, engineering, and hospitality. The data requirements for domain-specific fine-tuning can vary widely depending on the complexity and breadth of the domain. Some fields, like medicine or law, may require as much data as general-purpose fine-tuning due to their vast technical corpora. Others, such as e-commerce or hospitality, might need fewer samples, more in line with task-specific fine-tuning.</p>
    <p class="normal">The key factors determining the data needs for domain-specific models are the “size” of the domain (i.e., the extent of its specialized knowledge and vocabulary) and the representation of that domain in the model’s pre-training data. Domains that are well-represented in the original training data may require less fine-tuning, while those that are more specialized or underrepresented may need more extensive datasets. Even with open-source LLMs, many pre-training datasets are <a id="_idIndexMarker430"/>closed-source, which requires making educated <a id="_idIndexMarker431"/>guesses to determine their composition (e.g., 30% code or 20% math).</p>
    <h2 id="_idParaDest-139" class="heading-2">Data curation</h2>
    <p class="normal">When it comes to<a id="_idIndexMarker432"/> procuring data for fine-tuning, the<a id="_idIndexMarker433"/> approaches differ between task-specific and domain-specific models. For task-specific models, data curation often involves collecting examples of the desired task from existing datasets or creating new ones. This might involve gathering pairs of original and summarized texts for a summarization model or collecting sentences in different languages for a translation model.</p>
    <p class="normal">Domain-specific data curation can be more challenging. It often requires collaboration with subject matter experts to gather and validate relevant texts, research papers, technical documents, and other domain-specific content. In some cases, it may involve partnering with organizations or institutions that have access to large repositories of specialized information. The quality and relevance of this data is crucial, as it directly impacts the model’s ability to understand and generate content in the target domain.</p>
    <p class="normal">It’s worth noting that few-shot prompting has emerged as an alternative strategy to fine-tuning, especially for task-specific applications. This approach leverages the capabilities of large, powerful models by providing a few examples of the desired task within the input prompt. While not a replacement for fine-tuning in all scenarios (e.g., when you want to learn a new domain), few-shot prompting can be an efficient way to adapt models to new tasks without the need for extensive additional training.</p>
    <p class="normal">In practice, the line between task-specific and domain-specific models can sometimes blur. For instance, a model fine-tuned for medical diagnosis could be considered both task-specific (focused on diagnosis) and domain-specific (specialized in medical knowledge). The key is to understand the primary goal of the fine-tuning process and tailor the approach accordingly.</p>
    <p class="normal">At this point in the process, we should have a collection of datasets suited for our use case. The next step consists of refining the quality of the samples through rule-based <a id="_idIndexMarker434"/>filtering, data duplication, data <a id="_idIndexMarker435"/>decontamination, and data quality evaluation.</p>
    <h2 id="_idParaDest-140" class="heading-2">Rule-based filtering</h2>
    <p class="normal">Rule-based<a id="_idIndexMarker436"/> filtering is <a id="_idIndexMarker437"/>a systematic approach to data quality control that relies on explicit, predefined rules to evaluate and filter data samples. These rules are typically designed to address common quality issues and can range from simple checks to more complex logical operations. The primary goal of rule-based filtering is to maintain a high standard of data quality by removing samples that do not meet specific criteria.</p>
    <p class="normal"><strong class="keyWord">Length filtering</strong> is a straightforward<a id="_idIndexMarker438"/> yet effective rule-based filtering technique. This method involves setting thresholds for the acceptable length of responses in the dataset. Extremely short responses often lack sufficient information to be meaningful, while excessively long ones may contain irrelevant or redundant content. It’s important to note that the appropriate length thresholds can vary significantly depending on the specific task and domain. For example, a dataset for generating concise summaries might have a lower maximum threshold compared to one for detailed explanations.</p>
    <p class="normal"><strong class="keyWord">Keyword exclusion</strong> is another<a id="_idIndexMarker439"/> powerful rule-based filtering technique that focuses on the content of the samples rather than their structure. This method involves creating a list of keywords or phrases associated with low-quality or inappropriate content, and then filtering out any samples that contain these terms. The keyword list can include obvious indicators of low quality, such as profanities or spam-related terms, as well as domain-specific words that might indicate irrelevant or off-topic content. For instance, in a dataset for a professional writing assistant, you might exclude samples containing slang terms or informal expressions that don’t align with the intended tone and style.</p>
    <p class="normal"><strong class="keyWord">Format checking</strong> is recommended <a id="_idIndexMarker440"/>for datasets that include structured data or follow specific formatting requirements. This technique ensures that all samples adhere to the expected format, maintaining consistency and facilitating processing downstream. Format checking can be particularly important for datasets containing code samples, JSON structures, or other formatted text. For example, in a dataset of programming instructions and solutions, you might implement rules to verify that code samples are syntactically correct and follow specified style guidelines.</p>
    <p class="normal">Rule-based filtering offers significant advantages in preparing instruction datasets. Its speed and efficiency allow for rapid application to large volumes of data, making it highly scalable. The consistency of rule application ensures uniform treatment of data, reducing human error and bias. Furthermore, the explicit definition of filtering criteria provides transparency and interpretability, facilitating easy understanding, auditing, and adjustment. The ability to automate rule-based filtering reduces the need for manual intervention and enables continuous data quality monitoring.</p>
    <p class="normal">However, rule-based<a id="_idIndexMarker441"/> filtering also has limitations <a id="_idIndexMarker442"/>that must be considered. Predefined rules may lack the nuance required to capture the full complexity of language and context, potentially leading to the removal of valid but unusual samples. The typically binary nature of rules (pass/fail) may not always align with the nuanced nature of language and instruction quality. Additionally, as data patterns and quality standards evolve, rules need regular review and updates to remain effective. There’s also a risk that poorly designed<a id="_idIndexMarker443"/> rules could<a id="_idIndexMarker444"/> inadvertently introduce or amplify biases in the dataset.</p>
    <h2 id="_idParaDest-141" class="heading-2">Data deduplication</h2>
    <p class="normal">Dataset diversity<a id="_idIndexMarker445"/> is fundamental to training models <a id="_idIndexMarker446"/>that can generalize well to new, unseen data. When a dataset contains duplicates or near-duplicates, it can lead to several issues:</p>
    <ul>
      <li class="bulletList">Overfitting: Models may memorize specific examples rather than learning general patterns.</li>
      <li class="bulletList">Biased performance: Overrepresented data points may skew the model’s performance towards certain types of inputs.</li>
      <li class="bulletList">Inefficient training: Redundant data can increase training time without providing additional valuable information.</li>
      <li class="bulletList">Inflated evaluation metrics: Duplicate data in test sets may lead to overly optimistic performance estimates.</li>
    </ul>
    <p class="normal">To deduplicate datasets, we distinguish between exact and fuzzy deduplication. <strong class="keyWord">Exact deduplication</strong> removes<a id="_idIndexMarker447"/> identical samples through a straightforward process involving data normalization, hash generation, and duplicate removal. Data normalization standardizes the format of entries, such as converting text to lowercase. Hash generation then creates unique hashes for each entry using algorithms like MD5 or SHA-256. These hashes are compared to find matches, and duplicates are removed, leaving only one instance of each. While effective for identical entries, exact deduplication does not detect near-duplicates or semantically similar content, requiring more advanced techniques for those cases.</p>
    <p class="normal">The most popular approach to <strong class="keyWord">fuzzy deduplication</strong> is MinHash deduplication. Compared to other fuzzy <a id="_idIndexMarker448"/>techniques, it maintains high accuracy while significantly reducing computational complexity. MinHash operates by generating compact representations, or signatures, for each data item. These signatures serve as fingerprints that capture the essence of the data while drastically reducing its dimensionality. In practice, MinHash transforms data items (such as text documents) into sets of shingles, applies multiple hash functions to these sets, and selects the minimum hash values to form signature vectors. These signatures can then be compared using similarity measures like Jaccard similarity to efficiently identify near-duplicates.</p>
    <p class="normal">In addition to exact and fuzzy deduplication, <strong class="keyWord">semantic similarity</strong> takes a different approach by focusing <a id="_idIndexMarker449"/>on the meaning of text for deduplication. This method involves converting words or entire samples into vector representations using various natural language processing techniques. Word embedding models such as Word2Vec, GloVe, and FastText transform individual words into dense vectors, capturing semantic relationships. </p>
    <p class="normal">For more context-aware representations, language models like BERT, sentence transformers, or cross-encoders can generate embeddings for entire sentences or documents. Once these vector representations are obtained, deduplication can be performed by comparing the similarity between vectors. Common similarity measures include cosine similarity or Euclidean distance. Samples with high similarity scores above a predefined threshold can be considered duplicates. For large datasets, clustering techniques may be applied to group similar vectors. Methods like K-means, DBSCAN, or hierarchical clustering can efficiently organize the vector space, allowing for the identification of <a id="_idIndexMarker450"/>clusters<a id="_idIndexMarker451"/> that represent semantically similar content. Within each cluster, a representative sample can be retained while others are marked as duplicates.</p>
    <h2 id="_idParaDest-142" class="heading-2">Data decontamination</h2>
    <p class="normal">Data decontamination<a id="_idIndexMarker452"/> is the process of ensuring<a id="_idIndexMarker453"/> that the training dataset does not contain samples that are identical or highly similar to those in the evaluation or test sets. This step is important for ensuring the quality of the model evaluation and preventing overfitting or memorization of test data.</p>
    <p class="normal">Data decontamination uses techniques from data deduplication. Exact matching can be used to remove any training samples that are identical to those in the evaluation sets. This can be done using hash functions or direct string comparisons. Next, we can also use near-duplicate detection methods to identify and remove training samples that are very similar to evaluation samples, even if they are not exactly the same. This often involves techniques like MinHash or computing similarity scores based on n-grams or embeddings.</p>
    <div><p class="normal">A simple way to perform data decontamination is to add your evaluation set to the instruction dataset during the data deduplication stage. In this case, we want to ensure that we only remove samples from the instruction dataset, which can be implemented in different ways (only filtering out the first duplicate, recording the indexes of the evaluation samples, etc.). Ideally, you can automatically add your evaluation sets in the data deduplication stage to fully automate this process. This is particularly efficient if you iterate over several versions of custom benchmarks.</p>
    </div>
    <p class="normal">Another aspect of data decontamination is filtering out samples that may have been derived from the same source as evaluation data. This can involve checking for overlapping phrases, similar sentence structures, or common metadata. Practitioners may also <a id="_idIndexMarker454"/>use provenance tracking (source the data they use) to identify and exclude data from specific sources that are known to be used in evaluation sets.</p>
    <h2 id="_idParaDest-143" class="heading-2">Data quality evaluation</h2>
    <p class="normal">Data quality <a id="_idIndexMarker455"/>evaluation is a critical aspect of <a id="_idIndexMarker456"/>machine learning, particularly for LLMs. The process involves assessing various characteristics of datasets, including accuracy, diversity, and complexity. While some aspects like mathematical accuracy can be easily verified using tools such as Python interpreters, evaluating subjective or open-ended content remains challenging.</p>
    <p class="normal">Traditional methods of data quality assessment include human annotation, which generally provides high accuracy but is resource-intensive. To address scalability issues, machine learning techniques have been developed to automate the evaluation process. These include using LLMs as judges, reward models, and classifiers trained for quality prediction.</p>
    <p class="normal"><strong class="keyWord">The LLM-as-a-judge</strong> strategy<a id="_idIndexMarker457"/> involves prompting LLMs to evaluate the quality of each sample. This approach has become popular due to its flexibility and ease of use, though it does present some challenges. Different LLMs have different levels of performance across tasks, and their evaluations often align more closely with those of non-experts. With domain-specific datasets, you might want to use domain-specific models instead of better, general-purpose LLMs. Comparative assessment methods (e.g., “Is answer A better than answer B?”) generally outperform absolute scoring approaches (e.g., “Rate answer A between 1 and 4”), though both can be used at scale with sufficient prompt engineering. We recommend iterating through different prompts over a representative subset to manually verify the quality of the responses. <em class="italic">Table 5.2</em> shows an example of a custom prompt <a id="_idIndexMarker458"/>for a judge LLM.</p>
    <table id="table002" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Instruction</strong></p>
            <p class="normal">You are a data quality evaluator. Your goal is to assess an instruction and its corresponding answer, determining how effectively the answer addresses the given task.</p>
            <p class="normal">In your evaluation, you will provide feedback detailing the strengths and weaknesses of the answer, followed by a score on a scale of 1 to 4.</p>
            <p class="normal">A score of 1 means that the answer is terrible and irrelevant to the instruction.</p>
            <p class="normal">A score of 2 means that the answer is not helpful and misses important aspects of the instruction.</p>
            <p class="normal">A score of 3 means that the answer is helpful but could be improved in terms of relevance, accuracy, and depth.</p>
            <p class="normal">A score of 4 means that the answer is excellent and fully addresses the task.</p>
            <p class="normal">Provide your evaluation as follows:</p>
            <p class="normal">Feedback: (strengths and weaknesses you find relevant)</p>
            <p class="normal">Score: (number between 1 and 4)</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref"><em class="italic">Table 5.2</em> – Example of LLM-as-a-judge prompt for data quality evaluation</p>
    <p class="normal">LLM-as-a-judge is known to have several biases. First, it has a position bias in comparative scoring, where the LLM judge favors the first answer. This can be addressed by randomizing the order of answers A and B. In addition, like humans, LLM judges favor long answers. Length normalization techniques can be applied to absolute scoring to mitigate this issue. Finally, LLM judges are known to have intra-model favoritism, meaning that they prefer models from the same family (GPT-4o with GPT-4 and GPT-4o mini, for example). This can be addressed by using several models instead of a single one.</p>
    <p class="normal">In general, to improve <a id="_idIndexMarker459"/>evaluation reliability, strategies such as using multiple LLMs as a jury reduce bias and improve consistency. Leveraging a jury of smaller LLMs can also reduce costs while increasing accuracy and mitigating intra-model favoritism. For specific applications like chatbots, it’s advisable to aim for high agreement between LLM judges and human evaluators (around 80%). Simple grading scales (with few-shot prompting) and task-specific benchmarks are also recommended to<a id="_idIndexMarker460"/> ensure relevant and interpretable evaluations.</p>
    <p class="normal"><strong class="keyWord">Reward models</strong> are another<a id="_idIndexMarker461"/> way to re-purpose LLMs for data quality evaluation. The term “reward model” comes from Reinforcement Learning from Human Feedback (RLHF, see <em class="chapterRef">Chapter 6</em>). They can be broadly defined as models that take an instruction and answer pair and return a score as output. Generally, reward models are created by adding a linear head on top of a decoder-only architecture like Gemma or Llama. They are then trained for this specific purpose, using either reinforcement learning or traditional fine-tuning. <em class="italic">Figure 5.3</em> shows ArmoRM-Llama3-8B-v0.1’s architecture, which adds regression and gating layers on top of a Llama 3 8B model. This model outputs multiple scores to target specific dimensions, such as helpfulness, correctness, coherence, complexity, and verbosity. This <a id="_idIndexMarker462"/>allows for a more fine-grained approach to data quality evaluation.</p>
    <figure class="mediaobject"><img src="img/B31105_05_03.png" alt=""/></figure>
    <p class="packt_figref">Figure <em class="italic">5</em>.3 – Architecture of RLHFlow/ArmoRM-Llama3-8B-v0.1, based on Llama 3 (Source: <a href="https://doi.org/10.48550/arXiv.2406.12845">https://doi.org/10.48550/arXiv.2406.12845</a>)</p>
    <p class="normal">The Allen Institute for AI’s RewardBench leaderboard, hosted on Hugging Face (allenai/reward-bench), is a good resource for comparing different reward models. It combines various types of reward models (generative, classifiers, DPO, etc.) and evaluates them on a curated set of chosen and rejected answers for each instruction. While this task is not directly related to instruction data quality, it is a good resource for finding models capable of differentiating between good and bad answers.</p>
    <p class="normal"><strong class="keyWord">Classifiers or encoder-only models</strong> can<a id="_idIndexMarker463"/> be trained to perform data quality<a id="_idIndexMarker464"/> evaluation. A good<a id="_idIndexMarker465"/> example is HuggingFaceFW/fineweb-edu-classifier, a classifier designed to judge the educational value of web pages. This model was designed as a quality filter for pretraining data but a similar approach can be taken to evaluate instruction samples at scale. In practice, fineweb-edu-classifier adds a classification head to an embedding model (Snowflake/snowflake-arctic-embed-m) and trains it for 20 epochs on 450,000 samples that are annotated by Llama 3 70B Instruct.</p>
    <p class="normal">This approach relies on encoder-only models, which are both smaller and better suited to classification tasks. Thanks to their low number of parameters, these models are faster to run and can scale to millions of samples. However, they are not as accurate as bigger models, particularly for complex reasoning tasks where they lack the ability to capture nuances. At smaller scale, encoder-only models are still valuable to<a id="_idIndexMarker466"/> filter out outliers or as part<a id="_idIndexMarker467"/> of an automated data pipeline, which requires faster processing.</p>
    <h2 id="_idParaDest-144" class="heading-2">Data exploration</h2>
    <p class="normal">Data exploration is <a id="_idIndexMarker468"/>a continuous process that requires<a id="_idIndexMarker469"/> practitioners to become familiar with the training data. It involves both manual inspection and automated analysis, each playing a crucial role in understanding the dataset’s characteristics, strengths, and potential shortcomings.</p>
    <p class="normal"><strong class="keyWord">Manual dataset exploration</strong>, though <a id="_idIndexMarker470"/>time-consuming, is an important step. It reveals errors and inconsistencies that automated processes might miss, including formatting issues, data entry mistakes, incoherent reasoning, and factual inaccuracies. This process provides qualitative insights into the dataset’s content and style. To enhance efficiency, researchers can employ techniques like stratified sampling (selecting diverse samples), systematic review (using a criteria checklist), and collaborative review (involving multiple reviewers). </p>
    <p class="normal"><em class="italic">Figure 5.4</em> shows an example with Argilla, a collaborative platform for manual<a id="_idIndexMarker471"/> data quality evaluation and exploration.</p>
    <figure class="mediaobject"><img src="img/B31105_05_04.png" alt="A screenshot of a computer  Description automatically generated"/></figure>
    <p class="packt_figref">Figure <em class="italic">5</em>.4 – Argilla’s interface for collaborative data quality evaluation and exploration</p>
    <p class="normal"><strong class="keyWord">Statistical analysis</strong> is a<a id="_idIndexMarker472"/> complementary technique that reveals vocabulary diversity, potential biases, and concept representation. This process utilizes natural language processing libraries like NLTK or spaCy for tokenization and analysis of large text volumes. Visualization tools such as Matplotlib or Seaborn create histograms and word clouds, enabling intuitive pattern recognition. These techniques provide insights into dataset composition, language breadth, and possible cultural or<a id="_idIndexMarker473"/> contextual <a id="_idIndexMarker474"/>preferences, which can influence model outputs.</p>
    <p class="normal"><strong class="keyWord">Topic clustering</strong> automatically <a id="_idIndexMarker475"/>groups similar documents or pieces of text together, revealing underlying themes and patterns within the data. This process is especially important for understanding the content of large text corpora, identifying trends, and organizing information in a meaningful way. It is often associated with data visualization, with figures that show clusters of similar samples.</p>
    <p class="normal">Let’s consider the task of building an instruction dataset about various programming languages. You have collected a vast corpus of programming-related text from online forums, documentation, and tutorials. First, topic clustering can help identify the distinct programming languages present in the dataset (Python, JavaScript, etc.). Second, within<a id="_idIndexMarker476"/> each language cluster, you can further identify sub-topics like <code class="inlineCode">error handling</code>, <code class="inlineCode">data structures</code>, and <code class="inlineCode">web frameworks</code>. This allows a balanced representation of each language and sub-topic in the corpus. </p>
    <p class="normal">This makes sure that each<a id="_idIndexMarker477"/> topic is correctly covered for each programming language.</p>
    <figure class="mediaobject"><img src="img/B31105_05_05.png" alt=""/></figure>
    <p class="packt_figref">Figure <em class="italic">5</em>.5 – Representation of the historical TikTok dataset made with Nomic Atlas</p>
    <p class="normal">Several tools are available for<a id="_idIndexMarker478"/> performing topic clustering, each with its own strengths and approaches. For example, Hugging Face’s text-clustering provides a simple pipeline with sentence transformers for embedding text into vector space, UMAP for dimensionality reduction, and DBSCAN for clustering. It also automatically labels clusters using an LLM and can output visualizations. Nomic Atlas (see <em class="italic">Figure 5.5</em>), BunkaTopics, and Lilac are alternatives<a id="_idIndexMarker479"/> proposing<a id="_idIndexMarker480"/> similar approaches with additional features.</p>
    <h2 id="_idParaDest-145" class="heading-2">Data generation</h2>
    <p class="normal">When the available <a id="_idIndexMarker481"/>instruction datasets are not <a id="_idIndexMarker482"/>sufficient, creating custom data becomes necessary. This is particularly relevant for specialized applications where publicly available data is scarce. </p>
    <p class="normal">Additionally, it serves as a method to augment underrepresented areas in a dataset, like insufficient examples of JavaScript error-handling techniques in our previous example. While data can be generated manually by individuals or through crowdsourcing, these approaches often incur significant costs and time investments. Synthetic data generation using LLMs offers a more efficient and scalable alternative. This method, when combined with well-designed prompt engineering, can produce high-quality data at a much larger scale, effectively addressing the limitations of manual data creation processes.</p>
    <p class="normal">The process of synthetic data generation typically begins with the preparation of a set of carefully designed prompts (sometimes called taxonomy). These serve as the foundation for generating new, diverse examples. Five seed prompts used in the original Alpaca dataset can be seen in <em class="italic">Table 5.3</em>. The quality of synthetically generated data largely depends on the prompts and techniques used in the generation process. Well-crafted prompts can guide the language model to produce diverse, relevant, and high-quality instruction-response pairs. These prompts often include specific instructions, examples, and constraints to ensure the generated data aligns with the desired format and content.</p>
    <table id="table003" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Seed instructions</strong></p>
            <ul>
              <li class="bulletList">Is there anything I can eat for breakfast that doesn’t include eggs, yet includes protein, and has roughly 700-1000 calories?</li>
              <li class="bulletList">What is the relation between the given pairs? Input: Night : Day :: Right : Left</li>
              <li class="bulletList">Generate a one-sentence description for each of the following people. Input: -Barack Obama\n- Elon Musk\n- Taylor Swift</li>
              <li class="bulletList">Describe a situation in which the given stereotype can harm you. Input: All Asians are smart!</li>
              <li class="bulletList">Generate an appropriate subjective title for the following email: Input: “Hi [person name],\n\nI’m writing to ask you if you are happy to be a panelist in our workshop on multimodality at CVPR. The workshop will be held on June 20, 2023. \n\nBest,\n[my name]</li>
            </ul>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref"><em class="italic">Table 5.3</em> – Examples of seed prompts used in the original Alpaca dataset</p>
    <p class="normal">Many synthetic data generation pipelines incorporate multiple steps to ensure data quality. This may include generating an initial set of questions or instructions, followed by generating corresponding answers or responses. Some systems also implement validation steps, where another model or set of rules checks the generated pairs for accuracy, relevance, and adherence to specified criteria.</p>
    <p class="normal">An important aspect of synthetic data generation is the ability to control various attributes of the generated data. This includes factors such as the complexity of the instructions, the length of the responses, the tone or style of the language used, and the specific topics or domains covered. By fine-tuning these parameters, it’s possible to create datasets that are tailored to specific training objectives or that complement existing datasets in targeted ways. Structured generation using libraries like Outlines can also be beneficial to adhere to specific formats.</p>
    <p class="normal">Furthermore, synthetic<a id="_idIndexMarker483"/> data generation can be particularly<a id="_idIndexMarker484"/> useful for addressing biases and gaps in existing datasets. By carefully designing the generation process, it’s possible to create more balanced and inclusive datasets that represent a wider range of perspectives, topics, and language styles. This can help in training LLMs that are more equitable and capable of serving diverse user bases.</p>
    <p class="normal">However, synthetic data generation also comes with challenges. One primary concern is the potential for the generated data to inherit biases or errors from the underlying language model used for generation. To mitigate this, many approaches incorporate human oversight, diverse prompts, and additional filtering mechanisms to ensure the quality and appropriateness of the generated data.</p>
    <p class="normal">Another consideration is the need for the generated data to be sufficiently diverse and challenging. If the synthetic data is too simplistic or repetitive, it may not provide the level of complexity required to train a robust LLM. Advanced techniques in synthetic <a id="_idIndexMarker485"/>data generation often focus on creating <a id="_idIndexMarker486"/>varied and nuanced instruction-response pairs that can push the boundaries of what the model can learn.</p>
    <h2 id="_idParaDest-146" class="heading-2">Data augmentation</h2>
    <p class="normal">In this context, data <a id="_idIndexMarker487"/>augmentation refers to the process<a id="_idIndexMarker488"/> of increasing both the quantity and the quality of data samples. Unlike data generation, we use pre-existing instruction samples as inputs in this stage. While it is possible to upsample pairs of instructions and answers, data augmentation is mostly used to increase the quality of existing samples. In particular, it focuses on two aspects: diversity and complexity.</p>
    <p class="normal">A pioneering approach in this field is the Evol-Instruct method, which uses LLMs to evolve simple instructions into more qualitative ones. The evolved instructions can then be used to generate answers using powerful LLMs. This method employs two main strategies: in-depth and in-breadth evolving.</p>
    <p class="normal"><strong class="keyWord">In-depth evolving</strong> focuses on <a id="_idIndexMarker489"/>enhancing the complexity of existing instructions. It includes several techniques:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Constraints</strong>: It involves introducing additional requirements or limitations to the original instruction, making it more challenging to fulfill.</li>
      <li class="bulletList"><strong class="keyWord">Deepening</strong>: Instead of shallow questions, it tries to find more deep questions, requiring more comprehensive responses.</li>
      <li class="bulletList"><strong class="keyWord">Concretizing</strong>: It replaces general concepts with more specific ones, adding detail and precision to the instruction.</li>
      <li class="bulletList"><strong class="keyWord">Increasing reasoning steps</strong>: It modifies instructions to explicitly request multiple-step reasoning, promoting more complex problem-solving.</li>
      <li class="bulletList"><strong class="keyWord">Complicating input</strong>: This involves adding more complex data formats or structures to the<a id="_idIndexMarker490"/> instruction, such as XML, JSON, or code snippets.</li>
    </ul>
    <p class="normal"><strong class="keyWord">In-breadth evolving</strong>, on the<a id="_idIndexMarker491"/> other hand, aims to expand the diversity of the instruction dataset. It generates entirely new instructions inspired by existing ones, focusing on creating more rare or long-tailed examples within the same domain.</p>
    <p class="normal">As an example of concrete implementation, in-depth evolving can be automated with the following prompt, from the AutoEvol paper. You simply need to provide the instruction you want to evolve as input, and a powerful model like GPT-4o will return a more <a id="_idIndexMarker492"/>complex version of the original <a id="_idIndexMarker493"/>instruction.</p>
    <table id="table004" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal">You are an Instruction Rewriter that rewrites the given #Instruction# into a more complex version. Please follow the steps below to rewrite the given “#Instruction#” into a more complex version.</p>
            <ul>
              <li class="bulletList">Step 1: Please read the “#Instruction#” carefully and list all the possible methods to make this instruction more complex (to make it a bit harder for well-known AI assistants such as ChatGPT and GPT4 to handle). Please do not provide methods to</li>
              <li class="bulletList">change the language of the instruction!</li>
              <li class="bulletList">Step 2: Please create a comprehensive plan based on the #Methods List# generated in Step 1 to make the #Instruction# more complex. The plan should include several methods from the #Methods List#.</li>
              <li class="bulletList">Step 3: Please execute the plan step by step and provide the #Rewritten Instruction#. #Rewritten Instruction# can only add 10 to 20 words into the “#Instruction#”.</li>
              <li class="bulletList">Step 4: Please carefully review the #Rewritten Instruction# and identify any unreasonable parts. Ensure that the #Rewritten Instruction# is only a more complex version of the #Instruction#. Just provide the #Finally Rewritten Instruction# without anyexplanation.</li>
            </ul>
            <p class="normal">Please reply strictly in the following format:</p>
            <p class="normal">Step 1 #Methods List#:</p>
            <p class="normal">Step 2 #Plan#:</p>
            <p class="normal">Step 3 #Rewritten Instruction#:</p>
            <p class="normal">Step 4 #Finally Rewritten Instruction#:</p>
            <p class="normal">#Instruction#:</p>
            <p class="normal">{Instruction}</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref"><em class="italic">Table 5.4</em> – Evol LLM prompt from the “Automatic Instruction Evolving for Large Language Models” paper by Zeng et al. (2024)</p>
    <p class="normal"><strong class="keyWord">The UltraFeedback method</strong> is another<a id="_idIndexMarker494"/> innovative approach, focused on answer quality instead of instruction quality. It employs AI feedback to enhance the quality and diversity of model responses. Unlike Evol-Instruct, which evolves instructions, UltraFeedback uses a large pool of diverse instructions and models to generate a wide range of responses.</p>
    <p class="normal">It then leverages advanced language models like GPT-4 to provide detailed critiques and numerical scores for these responses across multiple dimensions such as instruction-following, truthfulness, honesty, and helpfulness.</p>
    <p class="normal">Based on these ideas, you can create your own augmentation techniques to create a more challenging and diverse instruction dataset. By refining and evolving existing instructions<a id="_idIndexMarker495"/> and answers, the resulting dataset <a id="_idIndexMarker496"/>can better train models to handle complex, multi-step tasks, and improve their performance across a wider range of applications.</p>
    <h1 id="_idParaDest-147" class="heading-1">Creating our own instruction dataset</h1>
    <p class="normal">In this section, we <a id="_idIndexMarker497"/>will create our own instruction dataset based on the crawled data from <em class="chapterRef">Chapter 3</em>. To create a high-quality instruction dataset, we need to address two main issues: the unstructured nature of our data and the limited number of articles we can crawl.</p>
    <p class="normal">This unstructured nature comes from the fact that we are dealing with raw text (articles), instead of pairs of instructions and answers. To address this issue, we will use an LLM to perform this transformation. Specifically, we will employ a combination of backtranslation and rephrasing. Backtranslation refers to the process of providing the expected answer as output and generating its corresponding instruction. However, using a chunk of text like a paragraph as an answer might not always be appropriate. This is why we want to rephrase the raw text to ensure we’re outputting properly formatted, high-quality answers. Additionally, we can ask the model to follow the author’s writing style to stay close to the original paragraph. While this process involves extensive prompt engineering, it can be automated and used at scale, as we will see in the following implementation.</p>
    <p class="normal">Our second issue regarding the limited number of samples is quite common in real-world use cases. The number of articles we can retrieve is limited, which constrains the size of the instruction dataset we are able to create. In this example, the more samples we have, the better the model becomes at imitating the original authors. To address this problem, we will divide our articles into chunks and generate three<a id="_idIndexMarker498"/> instruction-answer pairs for each chunk. This will multiply the number of samples we create while maintaining diversity in the final dataset. For simplicity, we will do it using OpenAI’s GPT-4o-mini model, but you can also use open-source models.</p>
    <p class="normal">However, LLMs are not reliable when it comes to producing structured output. Even when given specific templates or instructions, there’s no guarantee that the model will consistently adhere to them. This inconsistency often necessitates additional string parsing to ensure the output meets the desired format. </p>
    <p class="normal">To simplify this process and ensure properly structured results, we can employ structured generation techniques. Structured generation is an effective method to force an LLM to follow a predefined template, such as JSON, pydantic classes, or regular expressions. In the following, we will use OpenAI’s JSON mode feature, which provides a more robust way to return valid JSON objects and reduce the need for extensive post-processing.</p>
    <p class="normal">Based on this description, the following figure summarizes every step of the synthetic data pipeline we want to build.</p>
    <figure class="mediaobject"><img src="img/B31105_05_06.png" alt="A diagram of a tree  Description automatically generated with medium confidence"/></figure>
    <p class="packt_figref">Figure <em class="italic">5</em>.6 – Synthetic data generation pipeline from raw text to instruction dataset</p>
    <p class="normal">Let’s now implement it in Python. You can implement it as part of the LLMOps pipeline, or as a standalone script:</p>
    <ol>
      <li class="numberedList" value="1">We want to <a id="_idIndexMarker499"/>make sure that the following libraries are installed. The OpenAI library will allow us to interact with a model to generate the instruction data, and datasets will format it into a Hugging Face-compatible format. The tqdm library is installed to visualize the progress during the data generation process.
        <pre class="programlisting code-one"><code class="hljs-code">openai==1.37.1
datasets==2.20.0
tqdm==4.66.4
</code></pre>
      </li>
      <li class="numberedList">We import all the required libraries as follows.
        <pre class="programlisting code-one"><code class="hljs-code">import concurrent.futures
import json
import random
import re
from concurrent.futures import ThreadPoolExecutor
from typing import List, Tuple
from datasets import Dataset
from openai import OpenAI
from pydantic import BaseModel, Field
from tqdm.auto import tqdm
</code></pre>
      </li>
      <li class="numberedList">The raw data we have is a JSON file. We create a Hugging Face dataset from this JSON file by extracting specific fields from each article: <code class="inlineCode">id</code>, <code class="inlineCode">content</code>, <code class="inlineCode">platform</code>, <code class="inlineCode">author_id</code>, <code class="inlineCode">author name</code>, and <code class="inlineCode">link</code>.
        <pre class="programlisting code-one"><code class="hljs-code">def load_articles_from_json(file_path: str) -&gt; Dataset:
    with open(file_path, "r") as file:
        data = json.load(file)
    return Dataset.from_dict(
        {
            "id": [item["id"] for item in data["artifact_data"]],
            "content": [item["content"] for item in data["artifact_data"]],
            "platform": [item["platform"] for item in data["artifact_data"]],
            "author_id": [item["author_id"] for item in data["artifact_data"]],
            "author_full_name": [item["author_full_name"] for item in data["artifact_data"]],
            "link": [item["link"] for item in data["artifact_data"]],
        }
    ) 
</code></pre>
      </li>
    </ol>
    <p class="normal-one">If we simply load<a id="_idIndexMarker500"/> our dataset as a pandas dataframe, it returns the following table.</p>
    <table id="table005" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell"/>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">id</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">content</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">platform</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">author_id</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">author_full_name</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">link</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">ab2f9e2e-5459-4dd6-97d6-c291de4a7093</p>
          </td>
          <td class="table-cell">
            <p class="normal">The Importance of Data Pipelines in the Era of...</p>
          </td>
          <td class="table-cell">
            <p class="normal">medium</p>
          </td>
          <td class="table-cell">
            <p class="normal">e6b945ba-6a9a-4cde-b2bf-0890af79732b</p>
          </td>
          <td class="table-cell">
            <p class="normal">Alex Vesa</p>
          </td>
          <td class="table-cell">
            <p class="normal"><a href="https://medium.com/decodingml/the-importance-o...">https://medium.com/decodingml/the-importance-o...</a></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">ccfe70f3-d324-40b6-ba38-86e72786dcf4</p>
          </td>
          <td class="table-cell">
            <p class="normal">Change Data Capture: Enabling Event-Driven Arc...</p>
          </td>
          <td class="table-cell">
            <p class="normal">medium</p>
          </td>
          <td class="table-cell">
            <p class="normal">e6b945ba-6a9a-4cde-b2bf-0890af79732b</p>
          </td>
          <td class="table-cell">
            <p class="normal">Alex Vesa</p>
          </td>
          <td class="table-cell">
            <p class="normal"><a href="https://medium.com/decodingml/the-3nd-out-of-1...">https://medium.com/decodingml/the-3nd-out-of-1...</a></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">4c9f68ae-ec8b-4534-8ad5-92372bf8bb37</p>
          </td>
          <td class="table-cell">
            <p class="normal">The Role of Feature Stores in Fine-Tuning LLMs...</p>
          </td>
          <td class="table-cell">
            <p class="normal">medium</p>
          </td>
          <td class="table-cell">
            <p class="normal">e6b945ba-6a9a-4cde-b2bf-0890af79732b</p>
          </td>
          <td class="table-cell">
            <p class="normal">Alex Vesa</p>
          </td>
          <td class="table-cell">
            <p class="normal"><a href="https://medium.com/decodingml/the-role-of-feat...">https://medium.com/decodingml/the-role-of-feat...</a></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">...</p>
          </td>
          <td class="table-cell">
            <p class="normal">...</p>
          </td>
          <td class="table-cell">
            <p class="normal">...</p>
          </td>
          <td class="table-cell">
            <p class="normal">...</p>
          </td>
          <td class="table-cell">
            <p class="normal">...</p>
          </td>
          <td class="table-cell">
            <p class="normal">...</p>
          </td>
          <td class="table-cell">
            <p class="normal">...</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">73</p>
          </td>
          <td class="table-cell">
            <p class="normal">68795a4d-26c2-43b7-9900-739a80b9b7dc</p>
          </td>
          <td class="table-cell">
            <p class="normal">DML: 4 key ideas you must know to train an LLM...</p>
          </td>
          <td class="table-cell">
            <p class="normal">decodingml.substack.com</p>
          </td>
          <td class="table-cell">
            <p class="normal">1519b1d1-1a5d-444c-a880-926c9eb6539e</p>
          </td>
          <td class="table-cell">
            <p class="normal">Paul Iusztin</p>
          </td>
          <td class="table-cell">
            <p class="normal"><a href="https://decodingml.substack.com/p/dml-4-key-id...">https://decodingml.substack.com/p/dml-4-key-id...</a></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">74</p>
          </td>
          <td class="table-cell">
            <p class="normal">d91b17c0-05d8-4838-bf61-e2abc1573622</p>
          </td>
          <td class="table-cell">
            <p class="normal">DML: How to add real-time monitoring &amp; metrics...</p>
          </td>
          <td class="table-cell">
            <p class="normal">decodingml.substack.com</p>
          </td>
          <td class="table-cell">
            <p class="normal">1519b1d1-1a5d-444c-a880-926c9eb6539e</p>
          </td>
          <td class="table-cell">
            <p class="normal">Paul Iusztin</p>
          </td>
          <td class="table-cell">
            <p class="normal">https://decodingml.substack.com/p/dml-how-to-a...</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">75</p>
          </td>
          <td class="table-cell">
            <p class="normal">dcf55b28-2814-4480-a18b-a77d01d44f5f</p>
          </td>
          <td class="table-cell">
            <p class="normal">DML: Top 6 ML Platform Features You Must Know ...</p>
          </td>
          <td class="table-cell">
            <p class="normal">decodingml.substack.com</p>
          </td>
          <td class="table-cell">
            <p class="normal">1519b1d1-1a5d-444c-a880-926c9eb6539e</p>
          </td>
          <td class="table-cell">
            <p class="normal">Paul Iusztin</p>
          </td>
          <td class="table-cell">
            <p class="normal"><a href="https://decodingml.substack.com/p/dml-top-6-ml...">https://decodingml.substack.com/p/dml-top-6-ml...</a></p>
          </td>
        </tr>
      </tbody>
    </table>
    <ol>
      <li class="numberedList" value="1">If we inspect the content of some articles a little further, we realize that some of them have special characters and redundant whitespaces. We can clean this with a simple regex.</li>
    </ol>
    <p class="normal-one">First, we use <code class="inlineCode">[^\w\s.,!?']</code> to remove non-alphanumeric characters except for apostrophes, periods, commas, exclamation marks, and question marks. Then, we use <code class="inlineCode">\s+</code> to replace multiple consecutive whitespace characters with a single space. </p>
    <p class="normal-one">Finally, we implement <code class="inlineCode">strip()</code> to remove any leading or trailing whitespace.</p>
    <pre class="programlisting code-one"><code class="hljs-code">def clean_text(text):
    text = re.sub(r"[^\w\s.,!?']", " ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()
</code></pre>
    <ol>
      <li class="numberedList" value="5">Now that we can load our articles, we need to chunk them before turning them into pairs of instructions and answers. Ideally, you would want to use headlines or paragraphs to produce semantically meaningful chunking.</li>
    </ol>
    <p class="normal-one">However, in<a id="_idIndexMarker501"/> our example, like in the real world, raw data tends to be messy. Due to improper formatting, we cannot extract paragraphs or headlines for every article in our raw dataset. Instead, we will extract sentences using a regex to get chunks between 1,000 and 2,000 characters. This number can be optimized depending on the density of the information contained in the text.</p>
    <p class="normal-one">The <code class="inlineCode">extract_substrings</code> function processes each article in the dataset by first cleaning the text and then using a regex to split it into sentences. It then builds chunks of text by concatenating these sentences until each chunk is between 1,000 and 2,000 characters long.</p>
    <pre class="programlisting code-one"><code class="hljs-code">def extract_substrings(dataset: Dataset, min_length: int = 1000, max_length: int = 2000) -&gt; List[str]:
    extracts = []
    sentence_pattern = r"(?&lt;!\w\.\w.)(?&lt;![A-Z][a-z]\.)(?&lt;=\.|\?|\!)\s"
    for article in dataset["content"]:
        cleaned_article = clean_text(article)
        sentences = re.split(sentence_pattern, cleaned_article)
        current_chunk = ""
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            if len(current_chunk) + len(sentence) &lt;= max_length:
                current_chunk += sentence + " "
            else:
                if len(current_chunk) &gt;= min_length:
                    extracts.append(current_chunk.strip())
                current_chunk = sentence + " "
        if len(current_chunk) &gt;= min_length:
            extracts.append(current_chunk.strip())
    return extracts
</code></pre>
    <ol>
      <li class="numberedList" value="6">Next, we want to <a id="_idIndexMarker502"/>create instruction-answer pairs from the extracted chunks of text. To manage these pairs effectively, we introduce the <code class="inlineCode">InstructionAnswerSet</code> class. This class allows us to create instances directly from JSON strings, which is useful when parsing the output from the OpenAI API.
        <pre class="programlisting code-one"><code class="hljs-code">class InstructionAnswerSet:
    def __init__(self, pairs: List[Tuple[str, str]]):
        self.pairs = pairs
    @classmethod
    def from_json(cls, json_str: str) -&gt; 'InstructionAnswerSet':
        data = json.loads(json_str)
        pairs = [(pair['instruction'], pair['answer'])
                 for pair in data['instruction_answer_pairs']]
        return cls(pairs)
    def __iter__(self):
        return iter(self.pairs)
</code></pre>
      </li>
      <li class="numberedList">Now that we have a set of extracts from the articles with a reasonable length, we can use an LLM to transform them into pairs of instructions and answers. Note that this step is model-agnostic and can be implemented with any open-source or closed-source model. Because this output is grounded in the context we provide, it doesn’t require complex reasoning or high-performing models.</li>
    </ol>
    <p class="normal-one">For convenience, we will use GPT-4o mini in this example. This choice is motivated by the low cost and good performance of this model. Prompt engineering <a id="_idIndexMarker503"/>is the most important aspect of this data transformation stage and requires several iterations to produce the expected outputs. We recommend starting with simple prompts and adding complexity when required to be more accurate, modify the style, or output multiple responses.</p>
    <p class="normal-one">In our example, we want to create instructions like “Write a paragraph about X topic” and corresponding answers that are factual and imitate the writer’s style. To implement this, we need to provide an extract that will ground the model’s responses. For efficiency, we also choose to generate five instruction-answer pairs for each extract. Here’s the beginning of our function for instruction generation, including our prompt.</p>
    <pre class="programlisting code-one"><code class="hljs-code">def generate_instruction_answer_pairs(
    extract: str, client: OpenAI
) -&gt; List[Tuple[str, str]]:
    prompt = f"""Based on the following extract, generate five instruction-answer pairs. Each instruction \
must ask to write about a specific topic contained in the context. each answer \
must provide a relevant paragraph based on the information found in the \
context. Only use concepts from the context to generate the instructions. \
Instructions must never explicitly mention a context, a system, a course, or an extract. \
Instructions must be self-contained and general. \
Answers must imitate the writing style of the context. \
Example instruction: Explain the concept of an LLM Twin. \
Example answer: An LLM Twin is essentially an AI character that mimics your writing style, personality, and voice. \
It's designed to write just like you by incorporating these elements into a language model. \
The idea is to create a digital replica of your writing habits using advanced AI techniques. \
Provide your response in JSON format with the following structure:
{{
    "instruction_answer_pairs": [
        {{"instruction": "...", "answer": "..."}},
        ...
    ]
}}
Extract:
{extract}
"""
</code></pre>
    <ol>
      <li class="numberedList" value="8">In addition to<a id="_idIndexMarker504"/> the user prompt, we can also specify a system prompt to guide the model into generating the expected instructions. Here, we repeat our high-level task in the system prompt.</li>
    </ol>
    <p class="normal-one">The concatenation of the system and user prompts is fed to the OpenAI API, using the GPT-4o mini model in JSON mode and a maximum of 1,200 tokens in the answer. We also use a standard temperature of <code class="inlineCode">0.7</code> to encourage diverse responses. The generated text is directly parsed using the InstructionAnswerSet class to return pairs of instructions and answers.</p>
    <pre class="programlisting code-one"><code class="hljs-code">    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system", "content": "You are a helpful assistant who \
            generates instruction-answer pairs based on the given context. \
            Provide your response in JSON format.",
            },
            {"role": "user", "content": prompt},
        ],
        response_format={"type": "json_object"},
        max_tokens=1200,
        temperature=0.7,
    )
    # Parse the structured output
    result = InstructionAnswerSet.from_json(completion.choices[0].message.content)
    # Convert to list of tuples
    return result.pairs
</code></pre>
    <ol>
      <li class="numberedList" value="9">Let’s create a<a id="_idIndexMarker505"/> main function to automate the process. It extracts substrings from the input dataset, then uses concurrent processing via Python’s <code class="inlineCode">ThreadPoolExecutor</code> to efficiently generate instruction-answer pairs for each extract. </li>
    </ol>
    <p class="numberedList">We use a default <code class="inlineCode">max_workers</code> value of 4 because higher values tend to exceed OpenAI’s rate limits, potentially causing API request failures or throttling.</p>
    <pre class="programlisting code-one"><code class="hljs-code">def create_instruction_dataset(
    dataset: Dataset, client: OpenAI, num_workers: int = 4
) -&gt; Dataset:
    extracts = extract_substrings(dataset)
    instruction_answer_pairs = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:
        futures = [executor.submit(generate_instruction_answer_pairs, extract, client)
            for extract in extracts
        ]
        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)
        ):
            instruction_answer_pairs.extend(future.result())
    instructions, answers = zip(*instruction_answer_pairs)
    return Dataset.from_dict(
        {"instruction": list(instructions), "output": list(answers)}
    )
</code></pre>
    <ol>
      <li class="numberedList" value="10">We can create <a id="_idIndexMarker506"/>our instruction dataset by calling this function. Running it over the raw data with GPT-4o mini costs less than 0.5$.</li>
      <li class="numberedList">We can now create a main function to orchestrate the entire pipeline. It loads the raw data, creates the instruction dataset, splits it into training and testing sets, and pushes the result to the Hugging Face Hub.
        <pre class="programlisting code-one"><code class="hljs-code">def main(dataset_id: str) -&gt; Dataset:
    client = OpenAI()
    # 1. Load the raw data
    raw_dataset = load_articles_from_json("cleaned_documents.json")
    print("Raw dataset:")
    print(raw_dataset.to_pandas())
    # 2. Create instructiondataset
instruction_dataset = create_instruction_dataset(raw_dataset, client)
    print("Instruction dataset:")
    print(instruction_dataset.to_pandas())
    # 3. Train/test split and export
    filtered_dataset = instruction_dataset.train_test_split(test_size=0.1)
    filtered_dataset.push_to_hub("mlabonne/llmtwin")
    return filtered_dataset
<strong class="hljs-slc">Dataset({</strong>
<strong class="hljs-slc">    features: [</strong><strong class="hljs-string-slc">'instruction'</strong><strong class="hljs-slc">, </strong><strong class="hljs-string-slc">'output'</strong><strong class="hljs-slc">],</strong>
<strong class="hljs-slc">    num_rows: </strong><strong class="hljs-number-slc">3335</strong>
<strong class="hljs-slc">})</strong>
</code></pre>
      </li>
    </ol>
    <p class="normal">We obtained 3,335 pairs with this process. You can find our version of the dataset at <a href="https://huggingface.co/datasets/mlabonne/llmtwin">https://huggingface.co/datasets/mlabonne/llmtwin</a>. The Hugging Face Hub provides a<a id="_idIndexMarker507"/> convenient dataset viewer (see <em class="italic">Figure 5.7</em>) to explore instructions and answers and make sure that there are no obvious mistakes in these samples. Due to the small size of the dataset, there is no need for comprehensive exploration and topic clustering.</p>
    <figure class="mediaobject"><img src="img/B31105_05_07.png" alt=""/></figure>
    <p class="packt_figref">Figure <em class="italic">5</em>.7 – The mlabonne/llmtwin instruction dataset on the Hugging Face Hub</p>
    <p class="normal">As seen in the previous section, we could refine this instruction dataset by increasing the diversity and complexity of our samples. More advanced prompt engineering could also increase the quality of the generated data by providing examples of the expected results, for instance. Finally, quality evaluation could help filter out low-quality samples by reviewing them individually. For conciseness and simplicity, we will keep a straightforward approach for this instruction dataset and<a id="_idIndexMarker508"/> explore more advanced methods in <em class="chapterRef">Chapter 6</em> when we create a preference dataset.</p>
    <p class="normal">In the next section, we will introduce SFT techniques, as well as related concepts.</p>
    <h1 id="_idParaDest-148" class="heading-1">Exploring SFT and its techniques</h1>
    <p class="normal">SFT consists of re-training<a id="_idIndexMarker509"/> pre-trained models on a smaller dataset composed of pairs of instructions and answers. The goal of SFT is to turn a base model, which can only perform next-token prediction, into a useful assistant, capable of answering questions and following instructions. SFT can also be used to improve the general performance of the base model (general-purpose SFT), instill new knowledge (e.g., new languages, domains, etc.), focus on specific tasks, adopt a particular voice, and so on.</p>
    <p class="normal">In this section, we will discuss when to use fine-tuning and explore related concepts with storage formats and chat templates. Finally, we will introduce three popular ways of implementing <a id="_idIndexMarker510"/>SFT: full-finetuning, <strong class="keyWord">Low-Rank Adaptation</strong> (<strong class="keyWord">LoRA</strong>) and <strong class="keyWord">Quantization-aware Low-Rank Adaptation</strong> (<strong class="keyWord">QLoRA</strong>).</p>
    <h2 id="_idParaDest-149" class="heading-2">When to fine-tune</h2>
    <p class="normal">In most scenarios, it is <a id="_idIndexMarker511"/>recommended to start with prompt<a id="_idIndexMarker512"/> engineering instead of directly fine-tuning models. Prompt engineering can be used with either open-weight or closed-source models. By using techniques like few-shot prompting or <strong class="keyWord">retrieval augmented generation</strong> (<strong class="keyWord">RAG</strong>), numerous <a id="_idIndexMarker513"/>problems can efficiently be tackled without SFT. Prompt engineering also allows us to build a robust evaluation pipeline, which measures metrics like accuracy, but also cost and latency. If these results do not match the requirements, we can explore the possibility of creating an instruction dataset, as illustrated in the previous section. If enough data is available, fine-tuning becomes an option.</p>
    <figure class="mediaobject"><img src="img/B31105_05_08.png" alt=""/></figure>
    <p class="packt_figref">Figure <em class="italic">5</em>.8 – Basic flowchart to determine when fine-tuning is an option on a technical level</p>
    <p class="normal">Beyond these technical considerations, SFT answers common needs in terms of control (“know your data”) and customizability (the fine-tuned model is unique). Instead of building applications around a chatbot, fine-tuning allows developers to create more diverse interactions with LLMs, like tool analytics, moderation, and additional context. Note that if we focus on open-weight models in this book, several LLM providers offer automated fine-tuning services. While they don’t offer the same level of control and customizability as managing your own fine-tuning pipeline, it can be an interesting trade-off in specific scenarios (e.g., limited resources in terms of machine learning engineering).</p>
    <p class="normal">Despite these advantages, fine-tuning also has limitations. It is generally understood that SFT leverages pre-existing knowledge in the base model’s weights and refocuses the parameters for a specific purpose. This has several implications. First of all, knowledge that is too distant from what has been learned in the pre-training set (such as an unknown or rare language) can be difficult to learn effectively. </p>
    <p class="normal">Even worse, a study showed that fine-tuning a model on new knowledge could result in more frequent hallucinations. Depending on the SFT technique that is used, we’re also <a id="_idIndexMarker514"/>at risk of erasing knowledge that was <a id="_idIndexMarker515"/>present in the base model (a common issue referred to as “catastrophic forgetting”).</p>
    <h2 id="_idParaDest-150" class="heading-2">Instruction dataset formats</h2>
    <p class="normal">Instruction datasets<a id="_idIndexMarker516"/> are stored in a<a id="_idIndexMarker517"/> particular format to organize instructions and answers. Typically, each sample in the dataset can be represented as a Python dictionary, where keys are prompt types like <code class="inlineCode">system</code>, <code class="inlineCode">instruction</code>, <code class="inlineCode">output</code>, and values corresponding to the actual text. The three most standard formats are Alpaca, ShareGPT, and OpenAI. The following table shows how these data formats are generally organized.</p>
    <table id="table006" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Name</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">JSONL format</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Alpaca</p>
          </td>
          <td class="table-cell">
            <p class="normal">{“instruction”: “...”, “input”: “...”, “output”: “...”}</p>
            <p class="normal">{“instruction”: “...”, “output”: “...”}</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">ShareGPT</p>
          </td>
          <td class="table-cell">
            <p class="normal">{“conversations”: [{“from”: “...”, “value”: “...”}, …]}</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">OpenAI</p>
          </td>
          <td class="table-cell">
            <p class="normal">{“conversations”: [{“role”: “...”, “content”: “...”}, …]}</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">OASST</p>
          </td>
          <td class="table-cell">
            <p class="normal">{“INSTRUCTION”: “...”, “RESPONSE”: “...”}</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Raw text</p>
          </td>
          <td class="table-cell">
            <p class="normal">{“text”: “...”}</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref"><em class="italic">Table 5.5</em> – Examples of instruction data storage format</p>
    <p class="normal">Note that for Alpaca, the “<code class="inlineCode">input</code>" key is optional. The content of the “<code class="inlineCode">input</code>" key is only appended to the content of the “<code class="inlineCode">instruction</code>" key when it exists. We also added the “<code class="inlineCode">raw text</code>" data format to show that SFT is not inherently different from pre-training. If you choose to re-train a model on raw text, this is a type of fine-tuning generally called “continual pre-training.”</p>
    <p class="normal">The dataset we created in the previous section has two columns (“<code class="inlineCode">instruction</code>" and “<code class="inlineCode">output</code>") and corresponds to the Alpaca format. Alpaca is sufficient for single-turn instructions and answers, which means it is limited to one instruction and one answer. When you want to process conversations (multiple instructions and answers), formats like ShareGPT or OpenAI are a better fit. By storing each message as a dictionary in a list, they can represent an arbitrarily long conversation in each sample.</p>
    <p class="normal">The choice of<a id="_idIndexMarker518"/> single-turn and multi-turn conversations<a id="_idIndexMarker519"/> directly impacts the storage type and depends on the end use case.</p>
    <h2 id="_idParaDest-151" class="heading-2">Chat templates</h2>
    <p class="normal">Once the instruction-answer<a id="_idIndexMarker520"/> pairs are parsed from the <a id="_idIndexMarker521"/>dataset format, we want to structure them in a chat template. Chat templates offer a unified way to present the instructions and answers to the model.</p>
    <p class="normal">In general, they also include special tokens to identify the beginning and the end of a message, or who is the author of the message. Since base models are not designed to follow instructions, they don’t have a chat template. This means that you can choose any template when you fine-tune a based model. If you want to fine-tune an instruct model (not recommended), you need to use the same template or it might degrade your performance.</p>
    <p class="normal">Like instruction dataset formats, there are different chat templates: ChatML, Llama 3, Mistral, and many others. In the open-source community, the ChatML template (originally from OpenAI) is a popular option. It simply adds two special tokens <code class="inlineCode">(&lt;|im_start|&gt; and &lt;|im_end|&gt;</code>) to indicate who is speaking. To give you an example, here is what we obtain when we apply the ChatML template to the instruction-answer pair shown in <em class="italic">Table 5.1</em>:</p>
    <table id="table007" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell">
            <pre class="programlisting gen"><code class="hljs">&lt;|im_start|&gt;system
You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.&lt;|im_end|&gt;
&lt;|im_start|&gt;user
Concepts: building, shop, town
Write a sentence that includes all these words.&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
In our little town, there is a shop inside a big building where people go to buy their favorite toys and candies.&lt;|im_end|&gt;
</code></pre>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref"><em class="italic">Table 5.6</em> – Sample from <em class="italic">Table 5.1</em> with the ChatML chat template</p>
    <p class="normal">As you can see, we still have three distinct parts: system, user, and assistant. Each part starts with the <code class="inlineCode">&lt;|im_start|&gt;</code> token and ends with <code class="inlineCode">&lt;|im_end|&gt;.</code> The current speaker is <a id="_idIndexMarker522"/>identified by a string (like “<code class="inlineCode">system</code>") instead of <a id="_idIndexMarker523"/>a special token. This is the exact string that is tokenized and used as input by the model during fine-tuning.</p>
    <p class="normal">However, during inference, we can’t provide the expected answer. In this case, we provide the system and user part as shown in <em class="italic">Figure 5.6</em>, and prompt the model to answer by adding <code class="inlineCode">&lt;|im_start|&gt;assistant\n</code>. </p>
    <p class="normal">Because the model has been fine-tuned with this template, it understands that the next tokens should be an answer relevant to the user instruction and guided by the system prompt. This is how fine-tuned models acquire instruction-following capabilities.</p>
    <p class="normal">A common issue with chat templates is that every single whitespace and line break is extremely important. Adding or removing any character would result in a wrong tokenization, which negatively impacts the performance of the model. For this reason, it is recommended to use reliable templates like Jinja, as implemented in the Transformers library. <em class="italic">Table 5.7</em> shows a few examples of such templates, including Alpaca, which is both the name of an instruction dataset format and a<a id="_idIndexMarker524"/> chat<a id="_idIndexMarker525"/> template.</p>
    <table id="table008" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Name</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Jinja template</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Alpaca</p>
          </td>
          <td class="table-cell">
            <pre class="programlisting gen"><code class="hljs">### Instruction: What is the capital of France?
### Response: The capital of France is Paris.&lt;EOS&gt;
</code></pre>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">ChatML</p>
          </td>
          <td class="table-cell">
            <pre class="programlisting gen"><code class="hljs">&lt;|im_start|&gt;user
What is the capital of France?&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
The capital of France is Paris.&lt;|im_end|&gt;
</code></pre>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Llama 3</p>
          </td>
          <td class="table-cell">
            <pre class="programlisting gen"><code class="hljs">&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;
What is the capital of France?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;
The capital of France is Paris.&lt;|eot_id|&gt;
</code></pre>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Phi-3</p>
          </td>
          <td class="table-cell">
            <pre class="programlisting gen"><code class="hljs">&lt;|user|&gt;
What is the capital of France?&lt;|end|&gt;
&lt;|assistant|&gt;
The capital of France is Paris.&lt;|end|&gt;
</code></pre>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Gemma</p>
          </td>
          <td class="table-cell">
            <pre class="programlisting gen"><code class="hljs">&lt;bos&gt;&lt;start_of_turn&gt;user
What is the capital of France?&lt;end_of_turn&gt;
&lt;start_of_turn&gt;model
The capital of France is Paris.&lt;end_of_turn&gt;
</code></pre>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref"><em class="italic">Table 5.7</em> – Example of common chat templates</p>
    <p class="normal">Jinja implements loops and conditions, which allow the same template to be used for <a id="_idIndexMarker526"/>training<a id="_idIndexMarker527"/> and inference (<code class="inlineCode">add_generation_prompt</code>).</p>
    <h2 id="_idParaDest-152" class="heading-2">Parameter-efficient fine-tuning techniques</h2>
    <p class="normal">While many<a id="_idIndexMarker528"/> techniques exist <a id="_idIndexMarker529"/>in the literature, SFT has converged on three main techniques: full fine-tuning, LoRA, and QLoRA. We will introduce each technique individually, and weigh their pros and cons depending on your use cases.</p>
    <figure class="mediaobject"><img src="img/B31105_05_09.png" alt=""/></figure>
    <p class="packt_figref">Figure 5.9 – Architectural differences of the three main SFT techniques at the module level</p>
    <h3 id="_idParaDest-153" class="heading-3">Full fine-tuning</h3>
    <p class="normal">Full fine-tuning refers<a id="_idIndexMarker530"/> to the<a id="_idIndexMarker531"/> most straightforward SFT technique, consisting of re-training every parameter in the base model. Like pre-training, SFT uses next-token prediction as its training objective. This means that the previously discussed structure of the dataset can be seen as the main difference between continual pre-training and full fine-tuning.</p>
    <p class="normal">This method often provides the best results but requires significant computational resources. Memory usage depends on several factors, including model size, training techniques, and optimization methods. At its simplest, using a single-GPU setting, the<a id="_idIndexMarker532"/> memory <a id="_idIndexMarker533"/>required can be estimated using the following formula:</p>
    <p class="center"><img src="img/B31105_05_001.png" alt=""/></p>
    <p class="normal">For a basic setup using <strong class="keyWord">32-bit floating point </strong>(<strong class="keyWord">fp32</strong>) precision, we can estimate:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Parameters</strong>: Learnable <a id="_idIndexMarker534"/>weights and biases within a neural network. In a large language model, these are typically the weights in the attention mechanisms, feed-forward layers, and embedding layers. Cost: 4 bytes/parameter (FP32) or 2 bytes/parameter (FP16/BF16).</li>
      <li class="bulletList"><strong class="keyWord">Gradients</strong>: Gradients are the partial derivatives of the loss function with respect to each model parameter. They indicate how much each parameter should be adjusted to minimize the loss. During training, gradients are computed for each parameter through backpropagation and are used to update the model parameters. Cost: 4 bytes/parameter.</li>
      <li class="bulletList"><strong class="keyWord">Optimizer states</strong>: Optimizer states are additional values maintained by optimization algorithms like Adam or AdamW. These typically include running averages of past gradients and past squared gradients for each parameter. They help in adapting the learning rate for each parameter and navigating the loss landscape more effectively. For instance, Adam maintains two additional values (momentum and variance) per parameter. Cost: 8 bytes/parameter (for Adam optimizer).</li>
      <li class="bulletList"><strong class="keyWord">Activations</strong>: Activations are the intermediate outputs of each layer in the neural network during the forward pass. For transformer-based models, this includes the outputs of attention mechanisms, feed-forward layers, and normalization layers. Activations need to be kept in memory during the forward pass to compute gradients in the backward pass, unless techniques like activation checkpointing are used. Cost: variable, but <a id="_idIndexMarker535"/>often negligible for small batch sizes.</li>
    </ul>
    <p class="normal">This gives us a baseline of 16 bytes per parameter. This translates into 112 GB of VRAM for a 7 B model and 1,120 GB for a 70 B model. However, this is often an underestimate, as it doesn’t account for additional memory needed for activations, temporary buffers, and overhead from various training techniques.</p>
    <p class="normal">Several techniques can be employed to reduce memory usage during LLM fine-tuning. Model parallelism spreads the workload across multiple GPUs, though it adds some overhead. Gradient accumulation enables larger effective batch sizes without proportional memory increase. Memory-efficient optimizers like 8-bit Adam can reduce the footprint of optimizer states. Activation checkpointing trades computation for memory by recalculating certain activations. When combined, these techniques can significantly lower memory usage. For instance, using mixed precision with model parallelism might reduce costs to around 14-15 bytes per parameter, compared to the 16-byte baseline. However, memory requirements remain substantial for large models even with these optimizations.</p>
    <p class="normal">In addition, full fine-tuning directly modifies the pre-training weights, which makes it destructive by nature. If training doesn’t behave as expected, it might erase previous knowledge and skills – a phenomenon referred to as “catastrophic forgetting.” The same phenomenon can happen with continual pre-training, which <a id="_idIndexMarker536"/>generally makes these<a id="_idIndexMarker537"/> techniques more difficult to use. Due to this additional complexity and its high computational requirements, parameter-efficient techniques are often preferred to full fine-tuning to create task and domain-specific models.</p>
    <h3 id="_idParaDest-154" class="heading-3">LoRA</h3>
    <p class="normal">LoRA is a<a id="_idIndexMarker538"/> parameter-efficient <a id="_idIndexMarker539"/>technique for fine-tuning LLMs. Developed to address the computational challenges associated with adapting massive neural networks, LoRA has quickly become a cornerstone technique in LLM fine-tuning.</p>
    <p class="normal">The primary purpose of LoRA is to enable the fine-tuning of LLMs with significantly reduced computational resources. This is achieved by introducing trainable low-rank matrices that modify the behavior of the model without changing its original parameters. The key advantages of LoRA include:</p>
    <ul>
      <li class="bulletList">Dramatically reduced memory usage during training</li>
      <li class="bulletList">Faster fine-tuning process</li>
      <li class="bulletList">Preservation of pre-trained model weights (non-destructive)</li>
      <li class="bulletList">Ability to switch between tasks efficiently by swapping LoRA weights</li>
    </ul>
    <p class="normal">These benefits have made LoRA particularly attractive for researchers and developers working with limited computational resources, effectively democratizing the process of LLM fine-tuning.</p>
    <p class="normal">At its core, LoRA <a id="_idIndexMarker540"/>employs a low-rank decomposition technique<a id="_idIndexMarker541"/> to update model weights efficiently. Instead of directly modifying the original weight matrix <img src="img/B31105_05_002.png" alt=""/>, LoRA introduces two smaller matrices, <img src="img/B31105_05_003.png" alt=""/> and <img src="img/B31105_05_004.png" alt=""/>, which together form a low-rank update to <img src="img/B31105_05_002.png" alt=""/>.</p>
    <figure class="mediaobject"><img src="img/B31105_05_10.png" alt="A diagram of a diagram  Description automatically generated with medium confidence"/></figure>
    <p class="packt_figref">Figure 5.10 – LoRA adds the two trainable matrices <img src="img/B31105_05_006.png" alt=""/> and <img src="img/B31105_05_007.png" alt=""/> and keeps the pre-trained weights <img src="img/B31105_05_008.png" alt=""/> frozen</p>
    <p class="normal">Mathematically, this can be represented as:</p>
    <p class="center"><img src="img/B31105_05_009.png" alt=""/></p>
    <p class="normal">Here, <img src="img/B31105_05_002.png" alt=""/> is the original weight matrix, <img src="img/B31105_05_007.png" alt=""/> and <img src="img/B31105_05_006.png" alt=""/> are the LoRA matrices, and <img src="img/B31105_05_013.png" alt=""/> is the effective weight matrix used during inference.</p>
    <p class="normal">The dimensions of matrices A and B are chosen such that their product has the same shape as <img src="img/B31105_05_014.png" alt=""/>, but with a much lower rank. This rank, typically denoted as <img src="img/B31105_05_015.png" alt=""/>, is a crucial hyperparameter in LoRA. During training, the original weights <img src="img/B31105_05_014.png" alt=""/> remain frozen, while only <img src="img/B31105_05_006.png" alt=""/> and <img src="img/B31105_05_007.png" alt=""/> are updated. This approach significantly reduces the number of trainable parameters, leading to substantial memory savings and faster training times.</p>
    <p class="normal">To implement LoRA <a id="_idIndexMarker542"/>effectively, we need to select the correct<a id="_idIndexMarker543"/> hyperparameters and target modules. LoRA comes with two hyperparameters:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Rank</strong> (<img src="img/B31105_05_019.png" alt=""/>): Determines the size of the LoRA matrices. A common starting point is <img src="img/B31105_05_020.png" alt=""/>, but values up to 256 have shown good results in some cases. Larger ranks may capture more diverse tasks but could lead to overfitting.</li>
      <li class="bulletList"><strong class="keyWord">Alpha</strong> (<img src="img/B31105_05_021.png" alt=""/>): A scaling factor applied to the LoRA update. In practice, we update the frozen weights <img src="img/B31105_05_022.png" alt=""/> by a factor of <img src="img/B31105_05_023.png" alt=""/>. This is why a common heuristic is to set <img src="img/B31105_05_021.png" alt=""/> to twice the value of <img src="img/B31105_05_019.png" alt=""/>, effectively applying a scaling factor of 2 to the LoRA update. You can experiment with different ratios in case of overfitting or underfitting.</li>
    </ul>
    <p class="normal">In addition, it is possible to add a drop-out layer to prevent overfitting. The dropout rate is usually set between 0 and 0.1 as an optional regularization factor, which slightly decreases training speed.</p>
    <p class="normal">LoRA can be applied to various parts of the model architecture. Initially, LoRA was primarily focused on modifying the attention mechanism, specifically the <strong class="keyWord">query</strong> (<strong class="keyWord">Q</strong>) and <strong class="keyWord">value</strong> (<strong class="keyWord">V</strong>) matrices in transformer layers. However, experiments have demonstrated significant benefits in extending LoRA’s application to other key components of<a id="_idIndexMarker544"/> the model. These additional target modules include:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Key</strong> (<strong class="keyWord">K</strong>) matrices in attention layers</li>
      <li class="bulletList">Output projection layers (often denoted as O) in attention mechanisms</li>
      <li class="bulletList">Feed-forward or <strong class="keyWord">Multi-Layer Perceptron</strong> (<strong class="keyWord">MLP</strong>) blocks between attention layers</li>
      <li class="bulletList">Linear output layers</li>
    </ul>
    <p class="normal">However, it’s important to note that increasing the number of LoRA-adapted modules also increases the number of trainable parameters and, consequently, the memory <a id="_idIndexMarker545"/>requirements.</p>
    <p class="normal">Using LoRA, it’s possible to fine-tune a 7B parameter model on a single GPU with as little as 14-18 GB of VRAM, depending on the specific configuration. This is a dramatic reduction compared to full fine-tuning, which would typically require multiple high-end GPUs. In terms of trainable parameters, LoRA drastically reduces the number compared to full fine-tuning. For example, even when targeting every module with a rank of 16, a Llama 3 8 B model only has 42 million trainable LoRA parameters out of 8 billion parameters, which is 0.5196% of the model’s parameters.</p>
    <p class="normal">In terms of quality, LoRA can also achieve comparable or sometimes better results than full-fine-tuning. Multiple sets of LoRA weights can be combined for different tasks or domains, allowing flexible deployment and task switching without retraining. Different projects are specialized in multiple-LoRA serving, such as LoRAX. It’s also<a id="_idIndexMarker546"/> a feature supported by Hugging Face’s <strong class="keyWord">Text Generation Inference</strong> (<strong class="keyWord">TGI</strong>) and<a id="_idIndexMarker547"/> <strong class="keyWord">Nvidia Inference Microservices</strong> (<strong class="keyWord">NIM</strong>).</p>
    <h3 id="_idParaDest-155" class="heading-3">QLoRA</h3>
    <p class="normal">Introduced by<a id="_idIndexMarker548"/> Dettmers et al., QLoRA <a id="_idIndexMarker549"/>is a method for fine-tuning LLMs that addresses the challenges of high computational costs. By combining quantization techniques with LoRA, QLoRA allows developers to fine-tune models on relatively small, widely available GPUs.</p>
    <p class="normal">The core of QLoRA’s approach involves quantizing the base model parameters to a custom <strong class="keyWord">4-bit NormalFloat</strong> (<strong class="keyWord">NF4</strong>) data <a id="_idIndexMarker550"/>type, which significantly reduces memory usage. Like LoRA, instead of updating all model parameters during fine-tuning, QLoRA introduces small, trainable low-rank matrices (adapters) to specific layers of the model. Only these adapters are updated during training, while the original model weights remain unchanged. To further reduce memory usage, QLoRA employs double quantization, which quantizes the quantization constants themselves. Additionally, it uses paged optimizers to manage memory spikes during training by leveraging Nvidia’s unified memory feature.</p>
    <p class="normal">QLoRA provides significant memory savings compared to LoRA, reducing peak GPU memory usage by up to 75%. For example, for a 7B model, QLoRA reduces peak memory usage from 14 GB to 9.1 GB during initialization, a 35% reduction. During fine-tuning, the memory savings increase to 40%, from 15.6 GB for LoRA to 9.3 GB for QLoRA. However, this memory efficiency comes at the cost of increased training time, with QLoRA being about 30% slower than LoRA. In terms of model performance, QLoRA shows only minor differences compared to LoRA.</p>
    <p class="normal">In summary, QLoRA is particularly beneficial when memory constraints are the primary concern, such as when working with very large models or on hardware with limited GPU memory. However, if training speed is crucial and sufficient memory is available, LoRA might be the preferred choice. </p>
    <p class="normal">The decision between QLoRA and LoRA should be based on the specific requirements of the project, available hardware, and <a id="_idIndexMarker551"/>the need to balance <a id="_idIndexMarker552"/>memory usage, training speed, and model performance.</p>
    <h2 id="_idParaDest-156" class="heading-2">Training parameters</h2>
    <p class="normal">When fine-tuning LLMs, several<a id="_idIndexMarker553"/> hyperparameters guide the training process and significantly impact the model’s convergence, generalization, and overall effectiveness.</p>
    <h3 id="_idParaDest-157" class="heading-3">Learning rate and scheduler</h3>
    <p class="normal">The learning rate <a id="_idIndexMarker554"/>is the most important hyperparameter. It controls how much the model’s parameters are updated during training. It typically ranges from very small values like <code class="inlineCode">1e-6</code> to larger values like <code class="inlineCode">1e-3</code>. A common starting point for transformer models is often around <code class="inlineCode">1e-5</code>. If the learning rate is too low, training progresses slowly and may get stuck in suboptimal solutions. Conversely, if it’s too high, training can become unstable or diverge, leading to poor performance. It’s often beneficial to experiment with different learning rates to find the optimal value for your specific task and model.</p>
    <p class="normal">The learning rate scheduler adjusts the learning rate throughout the training process. It typically starts with a higher learning rate to enable rapid initial progress, then gradually decreases it in later stages to fine-tune the model more precisely. The two most common types of schedulers are linear and cosine. A linear scheduler decreases the learning rate steadily over time, while a cosine scheduler follows a cosine curve, decreasing more slowly at first and then more rapidly toward the end of training. For example, you might start with a learning rate of 3e-4 and decrease it to 1e-7 over the course of training. The specific values and decay schedule depend on your model and dataset, but a common approach is to use a warmup period (e.g., 5% of total steps) where the learning rate increases from 0 to the initial value, followed by a decay period for the remaining 95% of steps. This approach helps stabilize early training and allows for more refined updates as the model converges. In general, linear and cosine schedulers provide the same level of performance.</p>
    <h3 id="_idParaDest-158" class="heading-3">Batch size</h3>
    <p class="normal">The batch size determines<a id="_idIndexMarker555"/> the number of samples processed before the model’s weights are updated. Typical batch sizes for LLM fine-tuning range from 1 to 32, with common values being 1, 2, 4, 8, or 16. Larger batch sizes generally lead to more stable gradient estimates and can improve training speed, as they provide a better approximation of the true gradient of the entire dataset.</p>
    <p class="normal">However, they also require more memory, which can be a limiting factor on GPUs with less VRAM. For instance, a batch size of 16 might work well on a high-end GPU with 24GB of memory, while a smaller GPU with 8 GB might only handle a batch size of 2 or 4.</p>
    <p class="normal">To overcome memory constraints while still benefiting from larger batch sizes, a technique called gradient accumulation can be used. It works by performing multiple forward and backward passes with smaller mini-batches, accumulating the gradients over these steps before applying a single update to the model’s parameters. This approach is particularly useful when working with large models or limited GPU memory. For example, if you want to achieve an effective batch size of 32 but your GPU can only handle 8 samples at a time, you can set the gradient accumulation steps to 4. This means you’ll process 4 mini-batches of 8 samples each, accumulating the gradients, and then update the model as if you had processed all 32 samples at once.</p>
    <p class="normal">The number of <a id="_idIndexMarker556"/>gradient accumulation steps typically ranges from 1 (no accumulation) to 8 or 16, depending on the desired effective batch size and available computational resources. When choosing the number of steps, consider the trade-off between training speed and memory usage. More accumulation steps allow for larger effective batch sizes but increase the time required for each update. Here’s a simple formula to determine the effective batch size:</p>
    <p class="center"><img src="img/B31105_05_026.png" alt=""/></p>
    <p class="normal">For instance, if you’re using 2 GPUs, each processing a batch of 4 samples, with 4 gradient accumulation<a id="_idIndexMarker557"/> steps, your effective batch size would be <code class="inlineCode">4 * 2 * 4 = 32</code> samples.</p>
    <h3 id="_idParaDest-159" class="heading-3">Maximum length and packing</h3>
    <p class="normal">The maximum<a id="_idIndexMarker558"/> sequence length determines the longest input the model can process. It’s typically set between 512 and 4,096 tokens but can go up to 128,000 or more, depending on the task and available GPU memory. For example, a maximum length of 2,048 tokens is common for many language generation tasks, while RAG applications might use up to 8,192 tokens or more. When processing input data, sequences longer than this limit are truncated, meaning excess tokens are removed. Truncation can occur at the beginning (left truncation) or end (right truncation) of the sequence. For instance, with a maximum length of 1,024 tokens, a 1,500-token input would have 476 tokens removed. This parameter directly impacts batch size and memory usage; a batch size of 12 with a max length of 1,024 would contain 12,288 tokens (<code class="inlineCode">12 * 1,024</code>), while the same batch size with a max length of 512 would only contain 6,144 tokens. It’s important to balance this parameter with your GPU capabilities and the nature of your training data to optimize performance and resource utilization.</p>
    <p class="normal">Packing maximizes the utilization of each training batch. Instead of assigning one sample per batch, packing combines multiple smaller samples into a single batch, effectively increasing the amount of data processed in each iteration. For example, if your maximum sequence length is 1,024 tokens, but many of your samples are only 200-300 tokens long, packing could allow you to fit 3-4 samples into each batch slot. This approach can significantly improve training efficiency, especially when dealing with datasets containing many short sequences. However, packing requires careful implementation to ensure that model attention doesn’t cross between<a id="_idIndexMarker559"/> packed samples. This is typically achieved by using attention masks that prevent the model from attending to tokens from different samples within the same packed sequence.</p>
    <h3 id="_idParaDest-160" class="heading-3">Number of epochs</h3>
    <p class="normal">The number of epochs is <a id="_idIndexMarker560"/>another important parameter, representing the number of complete passes through the entire training dataset. For LLM fine-tuning, the typical range is 1 to 10 epochs, with many successful runs using 2 to 5 epochs. The optimal number depends on factors such as task complexity, dataset size, and model architecture. More epochs allow the model to refine its learning, potentially improving performance. However, there’s a crucial trade-off: too few epochs may lead to underfitting, while too many can cause overfitting. For example, a large model fine-tuned on a small dataset might only need 1-3 epochs, while a smaller model fine-tuned on a larger dataset could benefit from 5-10 epochs. It is helpful to monitor validation performance during training and implement early stopping if the model’s performance plateaus or degrades. This approach helps determine the optimal number<a id="_idIndexMarker561"/> of epochs dynamically and prevents overfitting.</p>
    <h3 id="_idParaDest-161" class="heading-3">Optimizers</h3>
    <p class="normal">Optimizers adjust <a id="_idIndexMarker562"/>the model’s parameters to minimize the loss function. For LLM fine-tuning, AdamW (Adaptive Moment Estimation with Weight Decay) is highly recommended, particularly its 8-bit version. AdamW 8-bit performs comparably to the 32-bit version while using less GPU memory (but it doesn’t improve training speed). AdamW combines adaptive learning rates with weight decay regularization, often leading to better training stability and model performance.</p>
    <p class="normal">For scenarios with severe memory constraints, AdaFactor presents an alternative designed for memory efficiency. It works well without explicit learning rate tuning, making it particularly useful in resource-constrained environments. However, it may not always match AdamW’s performance in all cases. In situations involving extremely large models or limited GPU memory, paged versions of optimizers, such as paged AdamW 8-bit, can further reduce memory consumption by offloading to CPU RAM. If memory allows and maximum performance is the priority, the non-quantized <code class="inlineCode">adamw_torch</code> optimizer may be the best choice.</p>
    <h3 id="_idParaDest-162" class="heading-3">Weight decay</h3>
    <p class="normal">Weight decay works by<a id="_idIndexMarker563"/> adding a penalty for large weights to the loss function, encouraging the model to learn simpler, more generalizable features. This helps the model avoid relying too heavily on any single input feature, which can improve its performance on unseen data. Typically, weight decay values range from 0.01 to 0.1, with 0.01 being a common starting point. For example, if you’re using the AdamW optimizer, you might set the weight decay to 0.01.</p>
    <p class="normal">While weight decay can be beneficial, setting it too high can impede learning by making it difficult for the model to capture important patterns in the data. Conversely, setting it too low may not provide sufficient regularization. The optimal weight decay value often depends on the specific model architecture and dataset, so it’s <a id="_idIndexMarker564"/>generally a good practice to experiment with different values.</p>
    <h3 id="_idParaDest-163" class="heading-3">Gradient checkpointing</h3>
    <p class="normal">Gradient checkpointing<a id="_idIndexMarker565"/> is a technique that reduces memory consumption during training by storing only a subset of intermediate activations generated in the forward pass. In standard training procedures, all intermediate activations are retained in memory to facilitate gradient calculation during the backward pass. However, for very deep networks like LLMs, this approach can quickly become impractical due to hardware limitations, especially on GPUs with limited memory capacity.</p>
    <p class="normal">Gradient checkpointing addresses this challenge by selectively saving activations at specific layers within the network. For layers where activations are not saved, they are recomputed during the backward pass as needed for gradient computation. This approach creates a trade-off between computation time and memory usage. While it significantly reduces memory requirements, it may increase overall computation time due to the need to recalculate some activations.</p>
    <p class="normal">Other parameters and techniques exist but play a minor role compared to those previously discussed. In the next section, we will explore how to select and tune these parameters using a concrete example.</p>
    <h1 id="_idParaDest-164" class="heading-1">Fine-tuning in practice</h1>
    <p class="normal">Let’s now fine-tune an <a id="_idIndexMarker566"/>open-source model on our custom dataset. In this section, we will show an example that implements LoRA and QLoRA for efficiency. Depending on the hardware you have available, you can select the technique that best corresponds to your configuration.</p>
    <p class="normal">There are many efficient open-weight models we can leverage for task or domain-specific use cases. To select the most relevant LLM, we need to consider three main parameters:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">License</strong>: Some model licenses only allow non-commercial work, which is a problem if we want to fine-tune for a company. Custom licenses are common in this field, and can target companies with a certain number of users, for example.</li>
      <li class="bulletList"><strong class="keyWord">Budget</strong>: Models with smaller parameter sizes (&lt;10 B) are a lot cheaper to fine-tune and deploy for inference than larger models. This is due to the fact that they can be run on cheaper GPUs and process more tokens per second.</li>
      <li class="bulletList"><strong class="keyWord">Performance</strong>: Evaluating the base model on general-purpose benchmarks or, even better, domain- or task-specific benchmarks relevant to the final use case, is crucial. This helps ensure that the model has the necessary capabilities to perform well on the intended tasks after fine-tuning.</li>
    </ul>
    <p class="normal">In this chapter, we will choose Llama 3.1 8B, an open-weight model released by Meta. It has a permissive custom license (“Llama 3.1 Community License Agreement”) that allows commercial use. With 8B parameters, it is small enough to fit on most GPUs while reaching a high level of performance compared to its competitors. We can verify this using the Open LLM Leaderboard, as well as other benchmarks detailed in the model card.</p>
    <p class="normal">There are specialized tools and <a id="_idIndexMarker567"/>libraries to fine-tune models. In particular, we recommend the following:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">TRL</strong>: This is a library created and maintained by Hugging Face to train LLMs using SFT and preference alignment. It is a popular and reliable library that tends to be the most up-to-date in terms of algorithms. It works in single and multi-GPU settings with FSDP and DeepSpeed.</li>
      <li class="bulletList"><strong class="keyWord">Axolotl</strong>: Created by Wing Lian, this tool streamlines the fine-tuning of LLMs with reusable YAML configuration files. It is based on TRL but includes many additional features, such as automatically combining datasets stored in various formats. It also supports single- and multi-GPU settings with FSDP and DeepSpeed.</li>
      <li class="bulletList"><strong class="keyWord">Unsloth</strong>: Created by Daniel and Michael Han, Unsloth uses custom kernels to speed up training (2-5x) and reduce memory use (up to 80% less memory). It is based on TRL and provides many utilities, such as automatically converting models into the GGUF quantization format. At the time<a id="_idIndexMarker568"/> of writing, it is only available for single-GPU settings.</li>
    </ul>
    <p class="normal">To maximize efficiency, we will perform fine-tuning using the Unsloth library. The following code is <a id="_idIndexMarker569"/>designed as part of our LLMOps pipeline, but can also be used as a stand-alone script. It can also be executed in different environments, like SageMaker, cloud GPUs (like Lambda Labs or RunPod), Google Colab, and many others. We tested it on different GPUs, like A40, A100, and L4.</p>
    <p class="normal">To install the Unsloth library and its dependencies, we recommend directly installing from the GitHub repository of the book (<a href="https://github.com/PacktPublishing/LLM-Engineering">https://github.com/PacktPublishing/LLM-Engineering</a>) or Unsloth’s repo (<a href="https://github.com/unslothai/unsloth">https://github.com/unslothai/unsloth</a>). This approach is recommended because the installation steps are regularly updated to address potential conflicts with dependencies:</p>
    <ol>
      <li class="numberedList" value="1">First, we want to access a gated model and (optionally) upload our fine-tuned model to Hugging Face (<a href="https://huggingface.co/">https://huggingface.co/</a>). This requires being logged in to an account. If you don’t have an account, you can create it and store your API key (<strong class="screenText">Settings | Access Tokens | Create new token</strong>) in the .env file:
        <pre class="programlisting code-one"><code class="hljs-code">HF_TOKEN = YOUR_API_KEY
</code></pre>
      </li>
      <li class="numberedList">Make sure that your Comet ML API key is also in the .env file:
        <pre class="programlisting code-one"><code class="hljs-code">COMET_API_KEY = YOUR_API_KEY
</code></pre>
      </li>
      <li class="numberedList">Import all the necessary packages:
        <pre class="programlisting code-one"><code class="hljs-code">import os
import torch
from trl import SFTTrainer
from datasets import load_dataset, concatenate_datasets
from transformers import TrainingArguments, TextStreamerfrom unsloth import FastLanguageModel, is_bfloat16_supported
</code></pre>
      </li>
      <li class="numberedList">Let’s now load the model to fine-tune and its corresponding tokenizer. We use Unsloth’s FastLaguageModel class with the <code class="inlineCode">.from_pretrained()</code> method. In addition to the model name, we need to specify the max sequence length (2,048 in this example). Finally, the <code class="inlineCode">load_in_4bit</code> argument <a id="_idIndexMarker570"/>indicates if we want to <a id="_idIndexMarker571"/>use <strong class="keyWord">QLoRA</strong> (<strong class="keyWord">quantized pre-trained weights</strong>) or LoRA.</li>
    </ol>
    <p class="normal-one">We’ll use LoRA in this example because of faster training and higher quality, but you can easily switch to QLoRA if you don’t meet the VRAM requirements.</p>
    <pre class="programlisting code-one"><code class="hljs-code">max_seq_length = 2048
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="meta-llama/Meta-Llama-3.1-8B",
    max_seq_length=max_seq_length,
    load_in_4bit=False,
)
</code></pre>
    <ol>
      <li class="numberedList" value="5">Now that the model is loaded, we can define our LoRA configuration. Here, we use a rank of 32 that is large enough to imitate the writing style and copy the knowledge from our instruction samples. You can increase this value to 64 or 128 if your results are underwhelming. We also set an alpha of 32, without dropout and without bias, to speed up training. Finally, we target every linear layer to maximize the quality of the fine-tuning process.
        <pre class="programlisting code-one"><code class="hljs-code">model = FastLanguageModel.get_peft_model(
    model,
    r=32,
    lora_alpha=32,
    lora_dropout=0,
    target_modules=["q_proj", "k_proj", "v_proj", "up_proj", "down_proj", "o_proj", "gate_proj"],
)
</code></pre>
      </li>
      <li class="numberedList">Next, we need to prepare the data in the right format for fine-tuning. In this example, we don’t have a lot of samples in the llmtwin dataset (3,000 samples). This is an issue because the model might not correctly learn the chat template. To address this, we will upsample it with a high-quality<a id="_idIndexMarker572"/> general-purpose dataset called FineTome. This is a filtered version of <code class="inlineCode">arcee-ai/The-Tome using the fineweb-edu-classifier</code>. Instead of using the 100,000 samples of this dataset, we will specify we only want 10,000 in the train split. We concatenate these two datasets to create our final set.
        <pre class="programlisting code-one"><code class="hljs-code">dataset1 = load_dataset("mlabonne/llmtwin")
dataset2 = load_dataset("mlabonne/FineTome-Alpaca-100k", split="train[:10000]")
dataset = concatenate_datasets([dataset1, dataset2])
</code></pre>
      </li>
      <li class="numberedList">Now, we need to format this data using a chat template. Let’s use the Alpaca template for convenience. This template doesn’t require additional tokens, which makes it less error-prone (but can slightly impact performance compared to ChatML). Here, we map all the instructions and answers to the Alpaca template. We manually add the <strong class="keyWord">end of sentence</strong> (<strong class="keyWord">EOS</strong>) token <a id="_idIndexMarker573"/>at the end of each message to ensure that the model learns to output it. Without it, it will keep generating answers without ever stopping.
        <pre class="programlisting code-one"><code class="hljs-code">alpaca_template = """Below is an instruction that describes a task. Write a response that appropriately completes the request.
### Instruction:
{}
### Response:
{}"""
EOS_TOKEN = tokenizer.eos_token
dataset = dataset.map(format_samples, batched=True, remove_columns=dataset.column_names)
</code></pre>
      </li>
      <li class="numberedList">Once the dataset is ready, we can divide it into training (95%) and test (5%) sets for validation during training.
        <pre class="programlisting code-one"><code class="hljs-code">dataset = dataset.train_test_split(test_size=0.05)
</code></pre>
      </li>
      <li class="numberedList">The model is now <a id="_idIndexMarker574"/>ready to be trained. The SFTTrainer() class stores all the hyperparameters for our training. In addition, we provide the model, tokenizer, LoRA configuration, and datasets. Following the recommendations from the previous section, we set a learning rate of <code class="inlineCode">3e-4</code> with a linear scheduler and a maximum sequence length of 2048. We train this model for three epochs with a batch size of 2 and 8 gradient accumulation steps (for an effective batch size of 16). We also choose the <code class="inlineCode">adamw_8bit</code> optimizer with a <code class="inlineCode">weight_decay</code> of 0.01. Depending on the GPU we use, it will automatically use FP16 or BF16 for the activations. Finally, we report our training run to Comet ML for experiment tracking.
        <pre class="programlisting code-one"><code class="hljs-code">trainer = SFTTrainer(
    model=model,
   tokenizer=tokenizer,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    dataset_num_proc=2,
    packing=True,
    args=TrainingArguments(
        learning_rate=3e-4,
        lr_scheduler_type="linear",
        per_device_train_batch_size=2,
        gradient_accumulation_steps=8,
        num_train_epochs=3,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.01,
        warmup_steps=10,
        output_dir="output",
        report_to="comet_ml",
        seed=0,
    ),
)
trainer.train()
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Training this model on our concatenated dataset can take a few hours. For example, it takes 50 minutes on an A100 GPU.</p>
    <ol>
      <li class="numberedList" value="10">Once it’s<a id="_idIndexMarker575"/> done, we can test it with a quick example. The goal is not to properly evaluate the fine-tuned model, but to make sure that there are no obvious errors related to the tokenizer or chat template.</li>
    </ol>
    <p class="normal-one">For fast inference, we can use <code class="inlineCode">FastLanguageModel.for_inference()</code> from Unsloth. We directly format an instruction with the Alpaca format. Note that we provide an empty answer to append the assistant header (<code class="inlineCode">### Response</code>): at the end of the user instruction. This forces the model to answer the instruction instead of completing it. We also use a text streamer to stream the generation instead of waiting for it to be complete before printing it.</p>
    <pre class="programlisting code-one"><code class="hljs-code">FastLanguageModel.for_inference(model)
message = alpaca_prompt.format("Write a paragraph to introduce supervised fine-tuning.", "")
inputs = tokenizer([message], return_tensors="pt").to("cuda")
text_streamer = TextStreamer(tokenizer)
_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=256, use_cache=True)
</code></pre>
    <ol>
      <li class="numberedList" value="11">Here is the<a id="_idIndexMarker576"/> answer provided by our model:
        <pre class="programlisting code-one"><code class="hljs-code">Supervised fine-tuning is a method used to enhance a language model by providing it with a curated dataset of instructions and their corresponding answers. This process is designed to align the model's 
responses with human expectations, thereby improving its accuracy and relevance. The goal is to ensure that the model can respond effectively to a wide range of queries, making it a valuable tool for applications such as chatbots and virtual assistants.
</code></pre>
      </li>
    </ol>
    <p class="normal-one">This is correct and properly formatted with the Alpaca chat template.</p>
    <ol>
      <li class="numberedList" value="12">Now that our model has been successfully fine-tuned, we can save it locally and/or push it to the Hugging Face Hub using the following functions.
        <pre class="programlisting code-one"><code class="hljs-code">model.save_pretrained_merged("model", tokenizer, save_method="merged_16bit")
model.push_to_hub_merged("mlabonne/TwinLlama-3.1-8B", tokenizer, save_method="merged_16bit")
</code></pre>
      </li>
    </ol>
    <p class="normal">Congratulations on fine-tuning a base model from scratch! During training, you can access Comet ML to monitor your training loss, validation loss, and many other metrics. You want to make sure that these metrics correspond to what is expected. <em class="italic">Figure 5.11</em> shows the training run corresponding to the previous code in Comet ML.</p>
    <figure class="mediaobject"><img src="img/B31105_05_11.png" alt="A graph of a graph  Description automatically generated with medium confidence"/></figure>
    <p class="packt_figref">Figure 5.11 – Four monitored metrics during fine-tuning in Comet ML</p>
    <p class="normal">In particular, three<a id="_idIndexMarker577"/> of these metrics are important to monitor:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Training loss</strong>: It measures how well the model is performing on the task it’s being trained for. The loss should continuously decrease on average, indicating improving performance. We expect a rapid decrease at the beginning of training, followed by a long plateau. Spikes and continuous increases in the loss value are signs that the training is failing. In this case, you might want to check the quality of your data, issues with the tokenizer, and tune parameters like learning rate and batch size. In <em class="italic">Figure 5.11</em> (loss), you can see three different phases corresponding to our three epochs.</li>
      <li class="bulletList"><strong class="keyWord">Validation loss</strong>: It measures the loss using the validation set instead of the training set; a well-fitted model typically shows both training and validation losses decreasing and eventually stabilizing, with a small gap between them. This gap should be minimal but is expected to exist as the model will always perform slightly better on the training data. If the training loss continues to decrease while the validation loss starts to increase, it’s a sign of overfitting. Conversely, if both curves remain flat at a relatively high loss value, it indicates underfitting. There are no universal “recommended ranges” for loss values, as these depend on the specific problem and loss function used. However, you should look for convergence and stability in both curves. In <em class="italic">Figure 4.11</em> (eval_loss), we see a slight increase at step 340. This is still acceptable but might indicate that the model starts to overfit.</li>
      <li class="bulletList"><strong class="keyWord">Gradient norm</strong>: It represents the magnitude of the gradient vector during training. Large gradient norms can indicate training instability like overfitting, especially if accompanied by a divergence between training and validation losses. On the other hand, a stable or decreasing gradient norm generally means that the model is converging toward a local optimum. To mitigate issues associated with large gradient norms, gradient clipping can be employed. This technique involves setting a maximum threshold for the gradient norm, effectively limiting the size of parameter updates.</li>
    </ul>
    <p class="normal">It is often <a id="_idIndexMarker578"/>interesting to try different learning rates and select the best model based on the minimal loss. Note that this is a proxy for real evaluations, which are covered in the next chapter.</p>
    <h1 id="_idParaDest-165" class="heading-1">Summary</h1>
    <p class="normal">This chapter covered essential aspects of LLM fine-tuning, both in theory and practice. We examined the instruction data pipeline and how to create high-quality datasets, from curation to augmentation. Each pipeline stage offers optimization opportunities, particularly in quality assessment, data generation, and enhancement. This flexible pipeline can be adapted to your use cases by selecting the most relevant stages and techniques. </p>
    <p class="normal">We applied this framework to real-world data from <em class="chapterRef">Chapter 3</em>, using an LLM to convert raw text into instruction-answer pairs. We then explored SFT techniques. This included an analysis of SFT’s advantages and limitations, methods for storing and parsing instruction datasets with chat templates, and an overview of three primary SFT techniques: full fine-tuning, LoRA, and QLoRA. We compared these methods based on their impact on memory usage, training efficiency, and output quality. The chapter concluded with a practical demonstration that involved fine-tuning a Llama 3.1 8 B model on our custom instruction dataset. This example highlighted key steps and implementation details for successful fine-tuning.</p>
    <p class="normal">In the next chapter, we will use preference alignment techniques to create a new version of TwinLlama-3.1-8B. We will generate a new dataset with chosen and rejected answers that will help us calibrate the type of answers we expect from our model. We will detail many applications that can benefit from this framework and how to implement it.</p>
    <h1 id="_idParaDest-166" class="heading-1">References</h1>
    <ul>
      <li class="bulletList">Tahori, Gulrajani, Zhang, Dubois, et al.. “<em class="italic">Alpaca: A Strong, Replicable Instruction-Following Model</em>” <a href="https://crfm.stanford.edu">crfm.stanford.edu</a>, March 13, 2023, <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">https://crfm.stanford.edu/2023/03/13/alpaca.html</a>.</li>
      <li class="bulletList">Subhabrata Mukherjee et al.. “<em class="italic">Orca: Progressive Learning from Complex Explanation Traces of GPT-4</em>.” arXiv preprint arXiv:2306.02707, June 2023.</li>
      <li class="bulletList">Wing Lian and Bleys Goodson and Eugene Pentland and Austin Cook and Chanvichet Vong and “Teknium”. “<em class="italic">Open-Orca/OpenOrca.” huggingface.co</em>, 2023, <a href="https://huggingface.co/datasets/Open-Orca/OpenOrca">https://huggingface.co/datasets/Open-Orca/OpenOrca</a>.</li>
      <li class="bulletList">Weihao Zeng et al.. “<em class="italic">Automatic Instruction Evolving for Large Language Models</em>.” arXiv preprint arXiv:2406.00770, June 2024.</li>
      <li class="bulletList">Chunting Zhou et al.. “<em class="italic">LIMA: Less Is More for Alignment</em>.” arXiv preprint arXiv:2305.11206, May 2023</li>
      <li class="bulletList">01. AI. “<em class="italic">Yi: Open Foundation Models by 01.AI</em>.” arXiv preprint arXiv:2403.04652, March 2024.</li>
      <li class="bulletList">Alex Birch. “<em class="italic">LLM finetuning memory requirements</em>.” <a href="https://blog.scottlogic.com">blog.scottlogic.com</a>, November 24, 2023, <a href="https://blog.scottlogic.com/2023/11/24/llm-mem.html">https://blog.scottlogic.com/2023/11/24/llm-mem.html</a>.</li>
      <li class="bulletList">Quentin Anthony et al.. “<em class="italic">Transformer Math 101</em>.” blog.eleuther.ai, April 18, 2023, <a href="https://blog.eleuther.ai/transformer-math/">https://blog.eleuther.ai/transformer-math/</a>.</li>
      <li class="bulletList">Edward J. Hu et al.. “<em class="italic">LoRA: Low-Rank Adaptation of Large Language Models</em>.” arXiv preprint arXiv:2106.09685, June 2021.</li>
      <li class="bulletList">Tim Dettmers et al.. “<em class="italic">QLoRA: Efficient Finetuning of Quantized LLMs</em>.” arXiv preprint arXiv:2305.14314, May 2023.</li>
    </ul>
    <h1 id="_idParaDest-167" class="heading-1">Join our book’s Discord space</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/llmeng">https://packt.link/llmeng</a></p>
    <p class="normal"><img src="img/QR_Code79969828252392890.png" alt=""/></p>
  </div>
</body></html>