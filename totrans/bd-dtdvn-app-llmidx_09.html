<html><head></head><body><html>&#13;
 <head>&#13;
  <title>&#13;
   Building Chatbots and Agents with LlamaIndex&#13;
  </title>&#13;
 </head>&#13;
 <body>&#13;
  <div class="epub-source">&#13;
   <h1 class="H1---Chapter" id="_idParaDest-181">&#13;
    Building Chatbots and Agents with LlamaIndex&#13;
   </h1>&#13;
   <div id="_idContainer094">&#13;
    <p style="font-style:italic;">&#13;
     As this ebook edition doesn't have fixed pagination, the page numbers below are hyperlinked for reference only, based on the printed edition of this book.&#13;
    </p>&#13;
    <p>&#13;
     This chapter provides an in-depth look at implementing chatbots and intelligent agents using the capabilities of LlamaIndex. We will explore the various chat engine modes available, from simple chatbots to more advanced context-aware and question-&#13;
     <strong class="bold">&#13;
      condensing engines&#13;
     </strong>&#13;
     . Then, we’ll dive into&#13;
     <strong class="bold">&#13;
      agent architectures&#13;
     </strong>&#13;
     , analyzing tools,&#13;
     <strong class="bold">&#13;
      reasoning loops&#13;
     </strong>&#13;
     , and parallel execution methods. You will gain practical knowledge so that you can build conversational interfaces powered by LLMs that can understand user needs and orchestrate responses or actions by utilizing tools and&#13;
     <span class="No-Break">&#13;
      data sources.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Throughout this chapter, we’re going to cover the following&#13;
     <span class="No-Break">&#13;
      main topics:&#13;
     </span>&#13;
    </p>&#13;
    <ul>&#13;
     <li>&#13;
      Understanding chatbots&#13;
      <span class="No-Break">&#13;
       and agents&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      Implementing agentic strategies in&#13;
      <span class="No-Break">&#13;
       our apps&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      Hands-on – implementing conversation tracking&#13;
      <span class="No-Break">&#13;
       for PITS&#13;
      </span>&#13;
     </li>&#13;
    </ul>&#13;
    <a id="_idTextAnchor181">&#13;
    </a>&#13;
   </div>&#13;
  </div>&#13;
 </body>&#13;
</html>
<html>&#13;
 <head>&#13;
  <title>&#13;
   Technical requirements&#13;
  </title>&#13;
 </head>&#13;
 <body>&#13;
  <div class="epub-source">&#13;
   <h1 id="_idParaDest-182">&#13;
    Technical requirements&#13;
   </h1>&#13;
   <div id="_idContainer094">&#13;
    <p>&#13;
     The following LlamaIndex integration packages will be required for the&#13;
     <span class="No-Break">&#13;
      sample code:&#13;
     </span>&#13;
    </p>&#13;
    <ul>&#13;
     <li>&#13;
      <em class="italic">&#13;
       Database&#13;
      </em>&#13;
      <span class="No-Break">&#13;
       <em class="italic">&#13;
        Tool&#13;
       </em>&#13;
      </span>&#13;
      <span class="No-Break">&#13;
       :&#13;
      </span>&#13;
      <a>&#13;
       <span class="No-Break">&#13;
        https://pypi.org/project/llama-index-tools-database/&#13;
       </span>&#13;
      </a>&#13;
     </li>&#13;
     <li>&#13;
      <em class="italic">&#13;
       OpenAI&#13;
      </em>&#13;
      <span class="No-Break">&#13;
       <em class="italic">&#13;
        Agent&#13;
       </em>&#13;
      </span>&#13;
      <span class="No-Break">&#13;
       :&#13;
      </span>&#13;
      <a>&#13;
       <span class="No-Break">&#13;
        https://pypi.org/project/llama-index-agent-openai/&#13;
       </span>&#13;
      </a>&#13;
     </li>&#13;
     <li>&#13;
      <em class="italic">&#13;
       Wikipedia&#13;
      </em>&#13;
      <span class="No-Break">&#13;
       <em class="italic">&#13;
        Reader&#13;
       </em>&#13;
      </span>&#13;
      <span class="No-Break">&#13;
       :&#13;
      </span>&#13;
      <a>&#13;
       <span class="No-Break">&#13;
        https://pypi.org/search/?q=llama-index-readers-wikipedia&#13;
       </span>&#13;
      </a>&#13;
     </li>&#13;
     <li>&#13;
      <em class="italic">&#13;
       LLM Compiler&#13;
      </em>&#13;
      <span class="No-Break">&#13;
       <em class="italic">&#13;
        Agent&#13;
       </em>&#13;
      </span>&#13;
      <span class="No-Break">&#13;
       :&#13;
      </span>&#13;
      <a>&#13;
       <span class="No-Break">&#13;
        https://pypi.org/project/llama-index-packs-agents-llm-compiler/&#13;
       </span>&#13;
      </a>&#13;
     </li>&#13;
    </ul>&#13;
    <p>&#13;
     All the code samples in this chapter can be found in the&#13;
     <code class="literal">&#13;
      ch8&#13;
     </code>&#13;
     subfolder of this book’s GitHub&#13;
     <span class="No-Break">&#13;
      repository:&#13;
     </span>&#13;
     <a>&#13;
      <span class="No-Break">&#13;
       https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex&#13;
      </span>&#13;
     </a>&#13;
     <span class="No-Break">&#13;
      .&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor182">&#13;
    </a>&#13;
   </div>&#13;
  </div>&#13;
 </body>&#13;
</html>
<html>&#13;
 <head>&#13;
  <title>&#13;
   Understanding chatbots and agents&#13;
  </title>&#13;
 </head>&#13;
 <body>&#13;
  <div class="epub-source">&#13;
   <h1 id="_idParaDest-183">&#13;
    Understanding chatbots and agents&#13;
   </h1>&#13;
   <div id="_idContainer094">&#13;
    chat_engine = index.as_chat_engine()&#13;
response = chat_engine.chat("Hi, how are you?")&#13;
    chat_engine.chat_repl()&#13;
    chat_engine.reset()&#13;
    from llama_index.core.storage.chat_store import SimpleChatStore&#13;
from llama_index.core.chat_engine import SimpleChatEngine&#13;
from llama_index.core.memory import ChatMemoryBuffer&#13;
    try:&#13;
    chat_store = SimpleChatStore.from_persist_path(&#13;
        persist_path="chat_memory.json"&#13;
    )&#13;
except FileNotFoundError:&#13;
    chat_store = SimpleChatStore()&#13;
    memory = ChatMemoryBuffer.from_defaults(&#13;
    token_limit=2000,&#13;
    chat_store=chat_store,&#13;
    chat_store_key="user_X"&#13;
    )&#13;
    chat_engine = SimpleChatEngine.from_defaults(memory=memory)&#13;
while True:&#13;
    user_message = input("You: ")&#13;
    if user_message.lower() == 'exit':&#13;
        print("Exiting chat")&#13;
        break&#13;
    response = chat_engine.chat(user_message)&#13;
    print(f"Chatbot: {response}")&#13;
    chat_store.persist(persist_path="chat_memory.json")&#13;
    from llama_index.core.chat_engine import SimpleChatEngine&#13;
chat_engine = SimpleChatEngine.from_defaults()&#13;
chat_engine.chat_repl()&#13;
    from llama_index.llms.openai import OpenAI&#13;
llm = OpenAI(temperature=0.8, model="gpt-4")&#13;
chat_engine = SimpleChatEngine.from_defaults(llm=llm)&#13;
    from llama_index.core import VectorStoreIndex, SimpleDirectoryReader&#13;
docs = SimpleDirectoryReader(input_dir="files").load_data()&#13;
index = VectorStoreIndex.from_documents(docs)&#13;
chat_engine = index.as_chat_engine(&#13;
    chat_mode="context",&#13;
    system_prompt=(&#13;
        "You're a chatbot, able to talk about "&#13;
        "general topics, as well as answering specific "&#13;
        "questions about ancient Rome."&#13;
    ),&#13;
)&#13;
chat_engine.chat_repl()&#13;
    retriever = index.as_retriever(retriever_mode='default')&#13;
chat_engine = ContextChatEngine.from_defaults(&#13;
    retriever=retriever&#13;
    )&#13;
    from llama_index.core import VectorStoreIndex, SimpleDirectoryReader&#13;
from llama_index.core.chat_engine import CondenseQuestionChatEngine&#13;
from llama_index.core.llms import ChatMessage&#13;
documents = SimpleDirectoryReader("files").load_data()&#13;
index = VectorStoreIndex.from_documents(documents)&#13;
query_engine=index.as_query_engine()&#13;
chat_history = [&#13;
    ChatMessage(&#13;
        role="user",&#13;
        content="Arch of Constantine is a famous"&#13;
        "building in Rome"&#13;
    ),&#13;
    ChatMessage(&#13;
        role="user",&#13;
        content="The Pantheon should not be "&#13;
        "regarded as a famous building"&#13;
    ),&#13;
]&#13;
    chat_engine = CondenseQuestionChatEngine.from_defaults(&#13;
    query_engine=query_engine,&#13;
    chat_history=chat_history&#13;
)&#13;
response = chat_engine.chat(&#13;
    "What are two of the most famous structures in ancient Rome?"&#13;
)&#13;
print(response)&#13;
    The Colosseum and the Pantheon.&#13;
    The Colosseum and the Arch of Constantine are two famous buildings in ancient Rome.&#13;
    index.as_chat_engine(chat_mode="condense_question")&#13;
    index.as_chat_engine(chat_mode="condense_plus_context")&#13;
    <p>&#13;
     In the&#13;
     <a id="_idIndexMarker773">&#13;
     </a>&#13;
     modern business ecosystem, the role of&#13;
     <strong class="bold">&#13;
      chatbot systems&#13;
     </strong>&#13;
     is increasingly important. First appearing in the 1960s (&#13;
     <a>&#13;
      https://en.wikipedia.org/wiki/ELIZA&#13;
     </a>&#13;
     ), chatbots have always fascinated both developers and technology users alike.&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Figure 8&#13;
      </em>&#13;
     </span>&#13;
     <em class="italic">&#13;
      .1&#13;
     </em>&#13;
     shows the user interface of one of these&#13;
     <span class="No-Break">&#13;
      early systems:&#13;
     </span>&#13;
    </p>&#13;
    <div>&#13;
     <div class="IMG---Figure" id="_idContainer078">&#13;
      <img src="../Images/B21861_08_1.jpg"/>&#13;
     </div>&#13;
    </div>&#13;
    <p class="IMG---Caption" lang="en-US">&#13;
     Figure 8.1 – The ELIZA chatbot interface&#13;
    </p>&#13;
    <p>&#13;
     While these&#13;
     <a id="_idIndexMarker774">&#13;
     </a>&#13;
     systems were rudimentary initially and seen as more of an experiment, with the advancement of NLP technologies, the experience they offer has become increasingly interesting and valuable&#13;
     <span class="No-Break">&#13;
      to users.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     <strong class="bold">&#13;
      Chatbot-based support systems&#13;
     </strong>&#13;
     offer today’s consumers a self-service experience. For users, self-service support services have&#13;
     <a id="_idIndexMarker775">&#13;
     </a>&#13;
     two major advantages over&#13;
     <span class="No-Break">&#13;
      human support:&#13;
     </span>&#13;
    </p>&#13;
    <ul>&#13;
     <li>&#13;
      They are available 24/7, even outside normal&#13;
      <span class="No-Break">&#13;
       working hours&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      The user does not have to&#13;
      <em class="italic">&#13;
       hold the line&#13;
      </em>&#13;
      to&#13;
      <span class="No-Break">&#13;
       access them&#13;
      </span>&#13;
     </li>&#13;
    </ul>&#13;
    <p>&#13;
     Even if there is some reluctance to use these systems at first, once they discover these advantages, users soon get used to interacting&#13;
     <span class="No-Break">&#13;
      with them.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Don’t necessarily think of chatbots as a technology designed to replace human support and interaction entirely. Although they have made enormous progress in recent years, these technologies, while getting more and more advanced, still have&#13;
     <span class="No-Break">&#13;
      their shortcomings.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Lacking real empathy and the human touch, even under ideal operating conditions, chatbot-based services are unlikely to replace human support completely. But that doesn’t mean they aren’t extremely valuable, both for organizations and&#13;
     <span class="No-Break">&#13;
      their users.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Perhaps the greatest value they bring is when they work in a blended experience, where users can receive both human support and access to self-service platforms that are interfaced with chatbot technologies. Implemented strategically, these systems can vastly improve not only the support offered to end consumers but also the internal interactions between an&#13;
     <span class="No-Break">&#13;
      organization’s employees.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     <strong class="bold">&#13;
      ChatOps&#13;
     </strong>&#13;
     , for example, is&#13;
     <a id="_idIndexMarker776">&#13;
     </a>&#13;
     a model&#13;
     <a id="_idIndexMarker777">&#13;
     </a>&#13;
     increasingly used by modern&#13;
     <span class="No-Break">&#13;
      organizations (&#13;
     </span>&#13;
     <a>&#13;
      <span class="No-Break">&#13;
       https://www.ibm.com/blog/benefits-of-chatops/&#13;
      </span>&#13;
     </a>&#13;
     <span class="No-Break">&#13;
      ).&#13;
     </span>&#13;
    </p>&#13;
    <p class="callout-heading">&#13;
     Definition&#13;
    </p>&#13;
    <p class="callout">&#13;
     ChatOps refers to the ability to integrate chat platforms with operational workflows, facilitating transparent collaboration among team members, processes, tools, and automated bots to enhance service dependability, accelerate recovery, and boost&#13;
     <span class="No-Break">&#13;
      collaborative productivity.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Based on the&#13;
     <a id="_idIndexMarker778">&#13;
     </a>&#13;
     idea of&#13;
     <strong class="bold">&#13;
      conversation-driven collaboration&#13;
     </strong>&#13;
     , the ChatOps model combines&#13;
     <strong class="bold">&#13;
      DevOps&#13;
     </strong>&#13;
     principles (&#13;
     <a>&#13;
      https://en.wikipedia.org/wiki/DevOps&#13;
     </a>&#13;
     ) by&#13;
     <a id="_idIndexMarker779">&#13;
     </a>&#13;
     simplifying and accelerating interactions between team members&#13;
     <span class="No-Break">&#13;
      using chatbots.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Whether we use them for internal communication or in interactions with our users, chatbots can only be useful to the extent that they can solve real problems. This depends on how well they can understand the context of the interaction and how relevant the answers they&#13;
     <span class="No-Break">&#13;
      provide are.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Figure 8&#13;
      </em>&#13;
     </span>&#13;
     <em class="italic">&#13;
      .2&#13;
     </em>&#13;
     provides&#13;
     <a id="_idIndexMarker780">&#13;
     </a>&#13;
     a visual representation of the&#13;
     <span class="No-Break">&#13;
      ChatOps model:&#13;
     </span>&#13;
    </p>&#13;
    <div>&#13;
     <div class="IMG---Figure" id="_idContainer079">&#13;
      <img src="../Images/B21861_08_2.jpg"/>&#13;
     </div>&#13;
    </div>&#13;
    <p class="IMG---Caption" lang="en-US">&#13;
     Figure 8.2 – The ChatOps paradigm&#13;
    </p>&#13;
    <p>&#13;
     If, in the beginning, the main limitation of chatbots came from the&#13;
     <em class="italic">&#13;
      clumsy&#13;
     </em>&#13;
     way of interacting with the user, with the evolution of NLP technologies, the main shortcoming has become, more recently, the lack of integration with the organization’s&#13;
     <span class="No-Break">&#13;
      knowledge base.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     After all, what good is a natural communication experience if the answers given by the system aren’t useful in solving the&#13;
     <span class="No-Break">&#13;
      user’s requests?&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     This brings us&#13;
     <span class="No-Break">&#13;
      to RAG.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     By now, I think it has become obvious that without being connected to an organization’s knowledge base, a chatbot can, at best, be considered a technology experiment. Even conversational engines based on powerful LLMs such as GPT-4 can, at best, provide generic answers that don’t always address the specific problems of each organization. Perhaps worse, not being anchored in validated documentation, they can&#13;
     <em class="italic">&#13;
      hallucinate&#13;
     </em>&#13;
     very convincingly, creating unpleasant or even potentially&#13;
     <span class="No-Break">&#13;
      dangerous experiences.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     As you’ve probably guessed by now, LlamaIndex&#13;
     <a id="_idIndexMarker781">&#13;
     </a>&#13;
     also offers RAG tools for implementing chatbot technologies. In this chapter, we will explore the options available to us and understand how we can implement very simple systems to advanced&#13;
     <span class="No-Break">&#13;
      chatbot mechanisms.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     But first, let’s see how this functionality is built&#13;
     <span class="No-Break">&#13;
      into LlamaIndex.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor183">&#13;
    </a>&#13;
    <h2 id="_idParaDest-184">&#13;
     Discovering ChatEngine&#13;
    </h2>&#13;
    <p>&#13;
     In the previous chapters, we&#13;
     <a id="_idIndexMarker782">&#13;
     </a>&#13;
     saw how we can build a query engine to run queries based on our data. This mechanism allows us to integrate multiple types of indexes, retrievers, node postprocessors, and response synthesizers at the same time, thus being able to access our proprietary data in multiple ways. Unfortunately, the&#13;
     <code class="literal">&#13;
      QueryEngine&#13;
     </code>&#13;
     class&#13;
     <a id="_idIndexMarker783">&#13;
     </a>&#13;
     does not provide any mechanism to keep the history of a conversation. That means each query is a separate interaction and there is no contextual memory to allow a&#13;
     <span class="No-Break">&#13;
      true&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       conversation&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      .&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     For that purpose, however, we have&#13;
     <strong class="bold">&#13;
      ChatEngine&#13;
     </strong>&#13;
     . Unlike query engines,&#13;
     <code class="literal">&#13;
      ChatEngine&#13;
     </code>&#13;
     allows us to have an actual conversation, giving us both the context of our proprietary data and the history of the chat. To simplify this concept even further, imagine a&#13;
     <code class="literal">&#13;
      QueryEngine&#13;
     </code>&#13;
     class&#13;
     <span class="No-Break">&#13;
      with memory.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     In its simplest form, a chat engine can be initialized just as easily, based on&#13;
     <span class="No-Break">&#13;
      an index:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Once initialized, a chat engine can be queried using&#13;
     <span class="No-Break">&#13;
      various methods:&#13;
     </span>&#13;
    </p>&#13;
    <ul>&#13;
     <li>&#13;
      <code class="literal">&#13;
       chat()&#13;
      </code>&#13;
      : This method initiates a synchronous chat session, processing the user’s message and returning the&#13;
      <span class="No-Break">&#13;
       response immediately.&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       achat()&#13;
      </code>&#13;
      : This method is similar to&#13;
      <code class="literal">&#13;
       chat()&#13;
      </code>&#13;
      but executes the query asynchronously, allowing multiple requests to be processed simultaneously. This can be useful, for example, in a web or mobile application where we want to avoid blocking the main thread during&#13;
      <span class="No-Break">&#13;
       server queries.&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       stream_chat()&#13;
      </code>&#13;
      : This method opens a streaming chat session, where responses can be returned as they are generated, for more dynamic interaction. This is particularly useful for long or complex responses that require significant processing time, allowing the user to start seeing parts of the response before all processing&#13;
      <span class="No-Break">&#13;
       is complete.&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       astream_chat()&#13;
      </code>&#13;
      : This method is an asynchronous version of&#13;
      <code class="literal">&#13;
       stream_chat()&#13;
      </code>&#13;
      that allows us to handle streaming interactions in an&#13;
      <span class="No-Break">&#13;
       asynchronous context.&#13;
      </span>&#13;
     </li>&#13;
    </ul>&#13;
    <p>&#13;
     Another option is to&#13;
     <a id="_idIndexMarker784">&#13;
     </a>&#13;
     initiate a&#13;
     <strong class="bold">&#13;
      Read-Eval-Print&#13;
     </strong>&#13;
     (&#13;
     <strong class="bold">&#13;
      REPL&#13;
     </strong>&#13;
     ) loop&#13;
     <span class="No-Break">&#13;
      with&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <code class="literal">&#13;
       ChatEngine&#13;
      </code>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      :&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     A REPL chat is akin to a&#13;
     <a id="_idIndexMarker785">&#13;
     </a>&#13;
     ChatGPT interface, where a user sends a message or question, the LLM processes the input, generates a response, and then immediately displays it to the user. This loop continues for as long as the user keeps providing input, creating an&#13;
     <span class="No-Break">&#13;
      interactive conversation.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     To reset a chat conversation, you can use the&#13;
     <span class="No-Break">&#13;
      following command:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     This is useful when you want to clear the history and begin a new&#13;
     <span class="No-Break">&#13;
      conversation thread.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     So, the basics are&#13;
     <a id="_idIndexMarker786">&#13;
     </a>&#13;
     very straightforward. Next, let’s talk about the different&#13;
     <strong class="bold">&#13;
      built-in chat modes&#13;
     </strong>&#13;
     available&#13;
     <span class="No-Break">&#13;
      in LlamaIndex.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor184">&#13;
    </a>&#13;
    <h2 id="_idParaDest-185">&#13;
     Understanding the different chat modes&#13;
    </h2>&#13;
    <p>&#13;
     When&#13;
     <a id="_idIndexMarker787">&#13;
     </a>&#13;
     initializing a chat engine, we can use the&#13;
     <code class="literal">&#13;
      chat_mode&#13;
     </code>&#13;
     argument to invoke various chat engine types predefined in LlamaIndex. I will show you how each of these engines works. We will discuss them one by one and get a good understanding of the advantages and use cases best suited for each&#13;
     <span class="No-Break">&#13;
      of them.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     But first, let’s have a short introduction to how chat memory is managed&#13;
     <span class="No-Break">&#13;
      within LlamaIndex.&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Understanding how chat memory works&#13;
    </h3>&#13;
    <p>&#13;
     The&#13;
     <code class="literal">&#13;
      ChatMemoryBuffer&#13;
     </code>&#13;
     class is a&#13;
     <a id="_idIndexMarker788">&#13;
     </a>&#13;
     specialized memory buffer that’s designed to store chat history efficiently while also managing the token limit imposed by different LLMs. This structure is important because we can pass it as an argument when initializing chat engines using the&#13;
     <code class="literal">&#13;
      memory&#13;
     </code>&#13;
     parameter. By saving and restoring this buffer from one session to another, we can implement persistence for&#13;
     <span class="No-Break">&#13;
      our conversations.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     There are two different storage options for the&#13;
     <span class="No-Break">&#13;
      chat store:&#13;
     </span>&#13;
    </p>&#13;
    <ul>&#13;
     <li>&#13;
      The default&#13;
      <code class="literal">&#13;
       SimpleChatStore&#13;
      </code>&#13;
      , which&#13;
      <a id="_idIndexMarker789">&#13;
      </a>&#13;
      stores the conversation&#13;
      <span class="No-Break">&#13;
       in memory&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      The more&#13;
      <a id="_idIndexMarker790">&#13;
      </a>&#13;
      advanced&#13;
      <code class="literal">&#13;
       RedisChatStore&#13;
      </code>&#13;
      , which stores the chat history in a Redis database, eliminating the need to manually persist and load the&#13;
      <span class="No-Break">&#13;
       chat history&#13;
      </span>&#13;
     </li>&#13;
    </ul>&#13;
    <p>&#13;
     The&#13;
     <code class="literal">&#13;
      chat_store&#13;
     </code>&#13;
     attribute, which&#13;
     <a id="_idIndexMarker791">&#13;
     </a>&#13;
     is an&#13;
     <a id="_idIndexMarker792">&#13;
     </a>&#13;
     instance of the&#13;
     <code class="literal">&#13;
      BaseChatStore&#13;
     </code>&#13;
     class, is&#13;
     <a id="_idIndexMarker793">&#13;
     </a>&#13;
     used for the actual storage and retrieval of chat messages. This modular approach allows different storage implementations, such as a simple in-memory store or more complex&#13;
     <span class="No-Break">&#13;
      database-backed stores.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     We also have&#13;
     <a id="_idIndexMarker794">&#13;
     </a>&#13;
     the&#13;
     <code class="literal">&#13;
      chat_store_key&#13;
     </code>&#13;
     parameter, which is used to uniquely identify the chat session or conversation within the chat store. This is useful for retrieving the correct conversation history when there are multiple conversations stored in the&#13;
     <a id="_idIndexMarker795">&#13;
     </a>&#13;
     same chat store. Here’s a basic example of&#13;
     <strong class="bold">&#13;
      conversation history persistence&#13;
     </strong>&#13;
     <span class="No-Break">&#13;
      using&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <code class="literal">&#13;
       SimpleChatStore&#13;
      </code>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      :&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     After importing the necessary libraries, we can try to load the previous conversation. If there is no previous conversation save file, we simply initialize an&#13;
     <span class="No-Break">&#13;
      empty&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <code class="literal">&#13;
       chat_store&#13;
      </code>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      :&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     It’s now time to initialize our memory buffer by using&#13;
     <code class="literal">&#13;
      chat_store&#13;
     </code>&#13;
     as an argument. Although not needed here, for a more detailed illustration, we will also customize&#13;
     <code class="literal">&#13;
      token_limit&#13;
     </code>&#13;
     <span class="No-Break">&#13;
      and&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <code class="literal">&#13;
       chat_store_key&#13;
      </code>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      :&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     OK; we have all&#13;
     <a id="_idIndexMarker796">&#13;
     </a>&#13;
     the necessary pieces. Let’s put them together into a&#13;
     <code class="literal">&#13;
      SimpleChatEngine&#13;
     </code>&#13;
     class and create a&#13;
     <span class="No-Break">&#13;
      chat loop:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Once the user types&#13;
     <code class="literal">&#13;
      exit&#13;
     </code>&#13;
     and we break the loop, we use the&#13;
     <code class="literal">&#13;
      persist()&#13;
     </code>&#13;
     method to store the current conversation for&#13;
     <span class="No-Break">&#13;
      future sessions:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     In case you’re wondering why we haven’t used the&#13;
     <code class="literal">&#13;
      chat_repl()&#13;
     </code>&#13;
     method shown previously and created a chat loop instead, the answer is in the&#13;
     <span class="No-Break">&#13;
      following note.&#13;
     </span>&#13;
    </p>&#13;
    <p class="callout-heading">&#13;
     Important note&#13;
    </p>&#13;
    <p class="callout">&#13;
     While the&#13;
     <code class="literal">&#13;
      chat()&#13;
     </code>&#13;
     ,&#13;
     <code class="literal">&#13;
      achat()&#13;
     </code>&#13;
     ,&#13;
     <code class="literal">&#13;
      stream_chat()&#13;
     </code>&#13;
     , and&#13;
     <code class="literal">&#13;
      astream_chat()&#13;
     </code>&#13;
     methods can benefit from loading and resuming previous conversations, by design, the&#13;
     <code class="literal">&#13;
      chat_repl()&#13;
     </code>&#13;
     method will reset the conversation history&#13;
     <span class="No-Break">&#13;
      during initialization.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     <code class="literal">&#13;
      ChatMemoryBuffer&#13;
     </code>&#13;
     also plays an important role in ensuring that the conversation’s context remains within the token limits of the model being used. Among other parameters available for&#13;
     <code class="literal">&#13;
      ChatMemoryBuffer&#13;
     </code>&#13;
     , the&#13;
     <code class="literal">&#13;
      token_limit&#13;
     </code>&#13;
     attribute specifies the maximum number of tokens that can be stored in the memory buffer. This limit is essential to ensure we stay within the maximum context window size of the current LLM we&#13;
     <span class="No-Break">&#13;
      are using.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     When the&#13;
     <a id="_idIndexMarker797">&#13;
     </a>&#13;
     conversation exceeds the context limit, a sliding window method is applied. Older parts of the conversation are truncated to ensure that the most recent and relevant parts are retained and processed by the LLM within its&#13;
     <span class="No-Break">&#13;
      token constraints.&#13;
     </span>&#13;
    </p>&#13;
    <p class="callout-heading">&#13;
     An analogy to better understand the sliding window method&#13;
    </p>&#13;
    <p class="callout">&#13;
     Imagine a conversation with an LLM as a train journey, where each piece of dialogue adds a carriage. However, the train can only be so long due to the tracks’ length limit, representing the model’s context window limit. To keep the journey going and add new carriages – in our case, messages – older ones need to be detached and left behind. This ensures the train can continue its journey, carrying the most recent and relevant parts of the conversation, while staying within the limits of the track. Just like in a train journey, where we might prioritize which carriages to keep based on their importance, the sliding window method prioritizes newer conversation parts, keeping the dialogue&#13;
     <span class="No-Break">&#13;
      flowing smoothly.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Now that we understand how memory works, let’s talk about the different available&#13;
     <span class="No-Break">&#13;
      chat modes.&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Simple mode&#13;
    </h3>&#13;
    <p>&#13;
     This is&#13;
     <a id="_idIndexMarker798">&#13;
     </a>&#13;
     the most&#13;
     <strong class="bold">&#13;
      basic chat engine&#13;
     </strong>&#13;
     available. It allows for a simple, direct conversation with the LLM, without any connection to our proprietary data.&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Figure 8&#13;
      </em>&#13;
     </span>&#13;
     <em class="italic">&#13;
      .3&#13;
     </em>&#13;
     explains this chat&#13;
     <span class="No-Break">&#13;
      mode visually:&#13;
     </span>&#13;
    </p>&#13;
    <div>&#13;
     <div class="IMG---Figure" id="_idContainer080">&#13;
      <img src="../Images/B21861_08_3.jpg"/>&#13;
     </div>&#13;
    </div>&#13;
    <p class="IMG---Caption" lang="en-US">&#13;
     Figure 8.3 – SimpleChatEngine&#13;
    </p>&#13;
    <p>&#13;
     The user’s experience&#13;
     <a id="_idIndexMarker799">&#13;
     </a>&#13;
     in this mode is defined by the inherent capabilities and limitations of the LLM, such&#13;
     <a id="_idIndexMarker800">&#13;
     </a>&#13;
     as its context window size and&#13;
     <span class="No-Break">&#13;
      overall performance.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     To initialize this mode, we can use the&#13;
     <span class="No-Break">&#13;
      following code:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     If we want, we can customize the LLM using the&#13;
     <span class="No-Break">&#13;
      <code class="literal">&#13;
       llm&#13;
      </code>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      argument:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     As you probably won’t be using this mode too much in your RAG designs, let’s talk about the more advanced options that&#13;
     <span class="No-Break">&#13;
      are available.&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Context mode&#13;
    </h3>&#13;
    <p>&#13;
     <code class="literal">&#13;
      ContextChatEngine&#13;
     </code>&#13;
     is&#13;
     <a id="_idIndexMarker801">&#13;
     </a>&#13;
     designed to enhance chat interactions by&#13;
     <a id="_idIndexMarker802">&#13;
     </a>&#13;
     leveraging our proprietary knowledge. It works by retrieving relevant text from an index based on the user’s input, integrating this retrieved information into the system prompt to provide context, and then generating a response with the help of&#13;
     <span class="No-Break">&#13;
      the LLM.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Have a look at&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Figure 8&#13;
      </em>&#13;
     </span>&#13;
     <em class="italic">&#13;
      .4&#13;
     </em>&#13;
     for a visual representation of this&#13;
     <span class="No-Break">&#13;
      chat mode:&#13;
     </span>&#13;
    </p>&#13;
    <div>&#13;
     <div class="IMG---Figure" id="_idContainer081">&#13;
      <img src="../Images/B21861_08_4.jpg"/>&#13;
     </div>&#13;
    </div>&#13;
    <p class="IMG---Caption" lang="en-US">&#13;
     Figure 8.4 – ContextChatEngine&#13;
    </p>&#13;
    <p>&#13;
     There are&#13;
     <a id="_idIndexMarker803">&#13;
     </a>&#13;
     several&#13;
     <a id="_idIndexMarker804">&#13;
     </a>&#13;
     parameters that&#13;
     <a id="_idIndexMarker805">&#13;
     </a>&#13;
     we can customize for this&#13;
     <span class="No-Break">&#13;
      chat engine:&#13;
     </span>&#13;
    </p>&#13;
    <ul>&#13;
     <li>&#13;
      <code class="literal">&#13;
       retriever&#13;
      </code>&#13;
      : The actual retriever that’s used to retrieve relevant text from the index based on the user’s message. When the chat engine is initialized directly from the index, it will use the default retriever for that particular&#13;
      <span class="No-Break">&#13;
       index type&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       llm&#13;
      </code>&#13;
      : An instance of an LLM, which will be used for&#13;
      <span class="No-Break">&#13;
       generating responses&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       memory&#13;
      </code>&#13;
      : A&#13;
      <code class="literal">&#13;
       ChatMemoryBuffer&#13;
      </code>&#13;
      object, which is used to store and manage the&#13;
      <span class="No-Break">&#13;
       chat history&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       chat_history&#13;
      </code>&#13;
      : This is an optional list of&#13;
      <code class="literal">&#13;
       ChatMessage&#13;
      </code>&#13;
      instances representing the history of the conversation. It can be used to maintain continuity in a conversation. This history includes all messages that have been exchanged in the chat session, including both user and chatbot messages. For instance, it can be used to continue a conversation from a certain point. A&#13;
      <code class="literal">&#13;
       ChatMessage&#13;
      </code>&#13;
      object contains&#13;
      <span class="No-Break">&#13;
       three attributes:&#13;
      </span>&#13;
      <ul>&#13;
       <li>&#13;
        <code class="literal">&#13;
         role&#13;
        </code>&#13;
        : This defaults&#13;
        <span class="No-Break">&#13;
         to&#13;
        </span>&#13;
        <span class="No-Break">&#13;
         <em class="italic">&#13;
          user&#13;
         </em>&#13;
        </span>&#13;
       </li>&#13;
       <li>&#13;
        <code class="literal">&#13;
         content&#13;
        </code>&#13;
        : The&#13;
        <span class="No-Break">&#13;
         actual message&#13;
        </span>&#13;
       </li>&#13;
       <li>&#13;
        Any optional arguments provided&#13;
        <span class="No-Break">&#13;
         via&#13;
        </span>&#13;
        <span class="No-Break">&#13;
         <code class="literal">&#13;
          additional_kwargs&#13;
         </code>&#13;
        </span>&#13;
       </li>&#13;
      </ul>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       prefix_messages&#13;
      </code>&#13;
      : A list of&#13;
      <code class="literal">&#13;
       ChatMessage&#13;
      </code>&#13;
      instances that may be used as predefined messages or prompts before the actual user message. This can be useful for setting a particular tone or context for&#13;
      <span class="No-Break">&#13;
       the chat&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       node_postprocessors&#13;
      </code>&#13;
      : An optional list of&#13;
      <code class="literal">&#13;
       BaseNodePostprocessor&#13;
      </code>&#13;
      instances for further processing the nodes retrieved by the retriever. This can be used to&#13;
      <a id="_idIndexMarker806">&#13;
      </a>&#13;
      implement guardrails, scrub sensitive information from the&#13;
      <a id="_idIndexMarker807">&#13;
      </a>&#13;
      context, or make any other adjustments to the retrieved nodes&#13;
      <span class="No-Break">&#13;
       if required&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       context_template&#13;
      </code>&#13;
      : A string template that can be used to format the prompt that feeds the context to&#13;
      <span class="No-Break">&#13;
       the LLM&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       callback_manager&#13;
      </code>&#13;
      : An optional&#13;
      <code class="literal">&#13;
       CallbackManager&#13;
      </code>&#13;
      instance for managing callbacks during the chat process. This is useful for tracing and&#13;
      <span class="No-Break">&#13;
       debugging purposes&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       system_prompt&#13;
      </code>&#13;
      : An optional string that’s used as a system prompt, providing initial context or instructions for&#13;
      <span class="No-Break">&#13;
       the chatbot&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       service_context&#13;
      </code>&#13;
      : An optional&#13;
      <code class="literal">&#13;
       ServiceContext&#13;
      </code>&#13;
      instance, which can be used to make additional customizations to the&#13;
      <span class="No-Break">&#13;
       chat engine&#13;
      </span>&#13;
     </li>&#13;
    </ul>&#13;
    <p>&#13;
     To implement&#13;
     <code class="literal">&#13;
      ContextChatEngine&#13;
     </code>&#13;
     , we&#13;
     <a id="_idIndexMarker808">&#13;
     </a>&#13;
     must load our data and build an index, then optionally configure the chat engine with different parameters&#13;
     <span class="No-Break">&#13;
      as needed.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Here’s a quick example based on our sample data files, which can be found in the&#13;
     <code class="literal">&#13;
      ch8/files&#13;
     </code>&#13;
     subfolder in this book’s&#13;
     <span class="No-Break">&#13;
      GitHub repository:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     In this&#13;
     <a id="_idIndexMarker809">&#13;
     </a>&#13;
     example, we initialized&#13;
     <code class="literal">&#13;
      chat_engine&#13;
     </code>&#13;
     from the index. Alternatively, we could have defined it standalone, providing a retriever as an argument,&#13;
     <span class="No-Break">&#13;
      like this:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Overall, this&#13;
     <a id="_idIndexMarker810">&#13;
     </a>&#13;
     chat mode is particularly effective for queries that relate to the knowledge contained within our data, supporting both general conversations and more specific discussions based on the&#13;
     <span class="No-Break">&#13;
      indexed content.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Because the engine first retrieves context from the index and uses it to generate responses, this approach makes the chat experience a lot more useful and natural for users seeking specific information from the&#13;
     <span class="No-Break">&#13;
      indexed data.&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Condense question mode&#13;
    </h3>&#13;
    <p>&#13;
     <code class="literal">&#13;
      CondenseQuestionChatEngine&#13;
     </code>&#13;
     streamlines&#13;
     <a id="_idIndexMarker811">&#13;
     </a>&#13;
     the&#13;
     <a id="_idIndexMarker812">&#13;
     </a>&#13;
     user interaction by first&#13;
     <strong class="bold">&#13;
      condensing the conversation&#13;
     </strong>&#13;
     and the latest user message into a standalone question with the help of the LLM. This standalone question, which tries to capture the essential elements of the conversation, is then sent to a query engine built on our proprietary data to generate&#13;
     <span class="No-Break">&#13;
      a response.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The main benefit of using this approach is that it maintains the conversation focused on the topic, preserving the essential points of the entire dialogue throughout every interaction. And it&#13;
     <a id="_idIndexMarker813">&#13;
     </a>&#13;
     always responds in the context of our&#13;
     <span class="No-Break">&#13;
      proprietary data.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Figure 8&#13;
      </em>&#13;
     </span>&#13;
     <em class="italic">&#13;
      .5&#13;
     </em>&#13;
     describes the operation of this particular&#13;
     <span class="No-Break">&#13;
      chat mode:&#13;
     </span>&#13;
    </p>&#13;
    <div>&#13;
     <div class="IMG---Figure" id="_idContainer082">&#13;
      <img src="../Images/B21861_08_5.jpg"/>&#13;
     </div>&#13;
    </div>&#13;
    <p class="IMG---Caption" lang="en-US">&#13;
     Figure 8.5 – CondenseQuestionChatEngine&#13;
    </p>&#13;
    <p>&#13;
     The fact that the final response comes from our retrieved proprietary data and not directly from the LLM can also be a disadvantage sometimes. This chat mode may struggle with more general questions, such as inquiries about previous interactions, due to its reliance&#13;
     <a id="_idIndexMarker814">&#13;
     </a>&#13;
     on querying the knowledge base for&#13;
     <span class="No-Break">&#13;
      every response.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Let’s look at some of&#13;
     <a id="_idIndexMarker815">&#13;
     </a>&#13;
     the key parameters&#13;
     <span class="No-Break">&#13;
      of&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <code class="literal">&#13;
       CondenseQuestionChatEngine&#13;
      </code>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      :&#13;
     </span>&#13;
    </p>&#13;
    <ul>&#13;
     <li>&#13;
      <code class="literal">&#13;
       query_engine&#13;
      </code>&#13;
      : This is a&#13;
      <code class="literal">&#13;
       BaseQueryEngine&#13;
      </code>&#13;
      instance that’s used to query the condensed question. Any type of query engine may be used here, including complex constructs with&#13;
      <span class="No-Break">&#13;
       routing functionality&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       condense_question_prompt&#13;
      </code>&#13;
      : This is a&#13;
      <code class="literal">&#13;
       BasePromptTemplate&#13;
      </code>&#13;
      instance that’s used for condensing the conversation and user message into a single,&#13;
      <span class="No-Break">&#13;
       standalone question&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       Memory&#13;
      </code>&#13;
      : A&#13;
      <code class="literal">&#13;
       ChatMemoryBuffer&#13;
      </code>&#13;
      instance that’s used to manage and store the&#13;
      <span class="No-Break">&#13;
       chat history&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       llm&#13;
      </code>&#13;
      : A language model instance for generating the&#13;
      <span class="No-Break">&#13;
       condensed question&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       verbose&#13;
      </code>&#13;
      : A Boolean flag for printing verbose logs&#13;
      <span class="No-Break">&#13;
       during operation&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       callback_manager&#13;
      </code>&#13;
      : An optional&#13;
      <code class="literal">&#13;
       CallbackManager&#13;
      </code>&#13;
      instance for&#13;
      <span class="No-Break">&#13;
       managing callbacks&#13;
      </span>&#13;
     </li>&#13;
    </ul>&#13;
    <p>&#13;
     To implement&#13;
     <a id="_idIndexMarker816">&#13;
     </a>&#13;
     this chat&#13;
     <a id="_idIndexMarker817">&#13;
     </a>&#13;
     engine, we typically initialize it with a query engine and, optionally, configure it with custom parameters. The conversation is condensed into a question using a predefined template that can be customized using the&#13;
     <code class="literal">&#13;
      condense_question_prompt&#13;
     </code>&#13;
     parameter. The resulting question is then sent to the&#13;
     <span class="No-Break">&#13;
      query engine.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Here’s a brief&#13;
     <a id="_idIndexMarker818">&#13;
     </a>&#13;
     <span class="No-Break">&#13;
      implementation example:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     In the first part of the code, we ingested our sample files, created an index, and then created a simple query engine. Next, we introduced a previous conversation context by creating a&#13;
     <a id="_idIndexMarker819">&#13;
     </a>&#13;
     chat history consisting of two&#13;
     <code class="literal">&#13;
      ChatMessage&#13;
     </code>&#13;
     objects. Specifically, we instructed the chat engine not to consider the Pantheon as a&#13;
     <span class="No-Break">&#13;
      famous building.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Now, let’s create our chat engine and&#13;
     <span class="No-Break">&#13;
      query it:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Let’s see what&#13;
     <a id="_idIndexMarker820">&#13;
     </a>&#13;
     happened in&#13;
     <span class="No-Break">&#13;
      the background:&#13;
     </span>&#13;
    </p>&#13;
    <ol>&#13;
     <li>&#13;
      <code class="literal">&#13;
       CondenseQuestionChatEngine&#13;
      </code>&#13;
      took the user’s message, along with the provided chat history, and condensed them into a standalone question. This process involved using the LLM and&#13;
      <code class="literal">&#13;
       condense_question_prompt&#13;
      </code>&#13;
      to generate a question that encapsulates the essence of the conversation context and the user’s&#13;
      <span class="No-Break">&#13;
       latest query.&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      Then, the engine forwarded this condensed question to the query engine, which searched the indexed data for&#13;
      <span class="No-Break">&#13;
       relevant information.&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      The query engine, having access to the information from&#13;
      <code class="literal">&#13;
       VectorStoreIndex&#13;
      </code>&#13;
      , processed the question and returned an answer. This answer reflects the collective context of the previous conversation and the specific query about famous structures in&#13;
      <span class="No-Break">&#13;
       ancient Rome.&#13;
      </span>&#13;
     </li>&#13;
    </ol>&#13;
    <p>&#13;
     Without the added chat history, the output of the sample would have been similar to&#13;
     <span class="No-Break">&#13;
      the following:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     This is because the two buildings are explicitly mentioned in our&#13;
     <span class="No-Break">&#13;
      sample data.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     However, once we add the new conversational context, the output looks&#13;
     <span class="No-Break">&#13;
      like this:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Another way of initializing this chat engine would be directly from the index,&#13;
     <span class="No-Break">&#13;
      like this:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     This&#13;
     <a id="_idIndexMarker821">&#13;
     </a>&#13;
     chat mode is particularly useful for complex&#13;
     <a id="_idIndexMarker822">&#13;
     </a>&#13;
     conversations where the context and nuances of previous exchanges play a crucial role in understanding and accurately responding to the latest query. It ensures that the chatbot remains aware of the conversation’s history, thus making the interaction more coherent and&#13;
     <span class="No-Break">&#13;
      contextually relevant.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The next chat mode we’ll talk about uses a mix of two&#13;
     <span class="No-Break">&#13;
      other approaches.&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Condense and context mode&#13;
    </h3>&#13;
    <p>&#13;
     <code class="literal">&#13;
      CondensePlusContextChatEngine&#13;
     </code>&#13;
     offers&#13;
     <a id="_idIndexMarker823">&#13;
     </a>&#13;
     an even more comprehensive chat interaction&#13;
     <a id="_idIndexMarker824">&#13;
     </a>&#13;
     by combining the benefits of condensed questions and&#13;
     <span class="No-Break">&#13;
      context retrieval.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     While the previous chat engine we discussed is more straightforward and focuses on simplifying the conversation into a question for response generation,&#13;
     <code class="literal">&#13;
      CondensePlusContextChatEngine&#13;
     </code>&#13;
     takes an extra step to enrich the conversation with additional context from the indexed data, leading to more detailed and context-aware responses. The trade-off here is an increase in response generation time due to the additional step performed. Let’s explore how it works under the hood by looking at&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Figure 8&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       .6&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      :&#13;
     </span>&#13;
    </p>&#13;
    <div>&#13;
     <div class="IMG---Figure" id="_idContainer083">&#13;
      <img src="../Images/B21861_08_6.jpg"/>&#13;
     </div>&#13;
    </div>&#13;
    <p class="IMG---Caption" lang="en-US">&#13;
     Figure 8.6 – CondensePlusContextChatEngine&#13;
    </p>&#13;
    <p>&#13;
     First, this engine&#13;
     <a id="_idIndexMarker825">&#13;
     </a>&#13;
     condenses a conversation and the latest user message into a standalone question. Then, it retrieves relevant context from the index using this condensed question. Finally, it uses both the retrieved context and the condensed question to&#13;
     <a id="_idIndexMarker826">&#13;
     </a>&#13;
     generate a response with&#13;
     <span class="No-Break">&#13;
      the LLM.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Here are some of the&#13;
     <a id="_idIndexMarker827">&#13;
     </a>&#13;
     key parameters&#13;
     <span class="No-Break">&#13;
      of&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <code class="literal">&#13;
       CondensePlusContextChatEngine&#13;
      </code>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      :&#13;
     </span>&#13;
    </p>&#13;
    <ul>&#13;
     <li>&#13;
      <code class="literal">&#13;
       retriever&#13;
      </code>&#13;
      : Used to fetch context based on the&#13;
      <span class="No-Break">&#13;
       condensed question&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       llm&#13;
      </code>&#13;
      : The LLM that’s used to generate the condensed question and the&#13;
      <span class="No-Break">&#13;
       final response&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       memory&#13;
      </code>&#13;
      : A&#13;
      <code class="literal">&#13;
       ChatMemoryBuffer&#13;
      </code>&#13;
      instance for storing and managing&#13;
      <span class="No-Break">&#13;
       chat history&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       context_prompt&#13;
      </code>&#13;
      : A prompt template for formatting the context in the&#13;
      <span class="No-Break">&#13;
       system prompt&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       condense_prompt&#13;
      </code>&#13;
      : A prompt for condensing the conversation into a&#13;
      <span class="No-Break">&#13;
       standalone question&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       system_prompt&#13;
      </code>&#13;
      : A prompt with instructions for&#13;
      <span class="No-Break">&#13;
       the chatbot&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       skip_condense&#13;
      </code>&#13;
      : A Boolean flag to bypass the condensation step&#13;
      <span class="No-Break">&#13;
       if desired&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       node_postprocessors&#13;
      </code>&#13;
      : An optional list of&#13;
      <code class="literal">&#13;
       BaseNodePostprocessors&#13;
      </code>&#13;
      for additional processing of&#13;
      <span class="No-Break">&#13;
       retrieved nodes&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       callback_manager&#13;
      </code>&#13;
      : As usual, this can be used for&#13;
      <span class="No-Break">&#13;
       managing callbacks&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       verbose&#13;
      </code>&#13;
      : A Boolean flag for enabling verbose logging&#13;
      <span class="No-Break">&#13;
       during operation&#13;
      </span>&#13;
     </li>&#13;
    </ul>&#13;
    <p>&#13;
     To build this&#13;
     <a id="_idIndexMarker828">&#13;
     </a>&#13;
     particular chat engine from an index, we can use the&#13;
     <span class="No-Break">&#13;
      following command:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     This chat mode is ideal in scenarios where both the context of the conversation and specific information from the indexed data are crucial for generating accurate and relevant responses. It enhances the chat experience by ensuring the responses are both contextually relevant and enriched with specific details from the&#13;
     <span class="No-Break">&#13;
      indexed content.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     OK. It’s time to discover the more advanced&#13;
     <span class="No-Break">&#13;
      chat modes.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor185">&#13;
    </a>&#13;
   </div>&#13;
  </div>&#13;
 </body>&#13;
</html>
<html>&#13;
 <head>&#13;
  <title>&#13;
   Implementing agentic strategies in our apps&#13;
  </title>&#13;
 </head>&#13;
 <body>&#13;
  <div class="epub-source">&#13;
   <h1 id="_idParaDest-186">&#13;
    Implementing agentic strategies in our apps&#13;
   </h1>&#13;
   <div id="_idContainer094">&#13;
    from llama_index.core.tools import FunctionTool&#13;
def calculate_average(*values):&#13;
    """&#13;
    Calculates the average of the provided values.&#13;
    """&#13;
    return sum(values) / len(values)&#13;
average_tool = FunctionTool.from_defaults(&#13;
    fn=calculate_average)&#13;
    pip install llama-index-tools-database&#13;
    from llama_index.tools.database import DatabaseToolSpec&#13;
    db_tools = DatabaseToolSpec(&lt;db_specific_configuration&gt;)&#13;
    tool_list = db_tools.to_tool_list()&#13;
    pip install llama-index-agent-openai&#13;
    from llama_index.tools.database import DatabaseToolSpec&#13;
from llama_index.core.tools import FunctionTool&#13;
from llama_index.agent.openai import OpenAIAgent&#13;
from llama_index.llms.openai import OpenAI&#13;
    def write_text_to_file(text, filename):&#13;
    """&#13;
    Writes the text to a file with the specified filename.&#13;
    Args:&#13;
        text (str): The text to be written to the file.&#13;
        filename (str): File name to write the text into.&#13;
    Returns: None&#13;
    """&#13;
    with open(filename, 'w') as file:&#13;
        file.write(text)&#13;
    save_tool = FunctionTool.from_defaults(fn=write_text_to_file)&#13;
db_tools = DatabaseToolSpec(uri="sqlite:///files//database//employees.db")&#13;
tools = [save_tool]+db_tools.to_tool_list()&#13;
    llm = OpenAI(model="gpt-4")&#13;
agent = OpenAIAgent.from_tools(&#13;
    tools=tools,&#13;
    llm=llm,&#13;
    verbose=True,&#13;
    max_function_calls=20&#13;
)&#13;
    response = agent.chat(&#13;
    "For each IT department employee with a salary lower "&#13;
    "than the average organization salary, write an email,"&#13;
    "announcing a 10% raise and then save all emails into "&#13;
    "a file called 'emails.txt'")&#13;
print(response)&#13;
    from llama_index.agent.react import ReActAgent&#13;
agent = ReActAgent.from_tools(tools)&#13;
    from llama_index.core.tools.tool_spec.load_and_search.base import (&#13;
    LoadAndSearchToolSpec)&#13;
from llama_index.tools.database import DatabaseToolSpec&#13;
from llama_index.agent.openai import OpenAIAgent&#13;
from llama_index.llms.openai import OpenAI&#13;
db_tools = DatabaseToolSpec(&#13;
    uri="sqlite:///files//database//employees.db")&#13;
tool_list = db_tools.to_tool_list()&#13;
tools=LoadAndSearchToolSpec.from_defaults(&#13;
tool_list[0]&#13;
).to_tool_list()&#13;
    llm = OpenAI(model="gpt-4")&#13;
agent = OpenAIAgent.from_tools(&#13;
    tools=tools,&#13;
    llm=llm,&#13;
    verbose=True&#13;
)&#13;
response = agent.chat(&#13;
    "Who has the highest salary in the Employees table?'")&#13;
print(response)&#13;
    pip install llama-index-readers-wikipedia&#13;
    from llama_index.agent.openai import OpenAIAgent&#13;
from llama_index.core.tools.ondemand_loader_tool import(&#13;
    OnDemandLoaderTool)&#13;
from llama_index.readers.wikipedia import WikipediaReader&#13;
    tool = OnDemandLoaderTool.from_defaults(&#13;
    WikipediaReader(),&#13;
    name="WikipediaReader",&#13;
    description="args: {'pages': [&lt;list of pages&gt;],&#13;
        'query_str': &lt;query&gt;}"&#13;
)&#13;
    agent = OpenAIAgent.from_tools(&#13;
    tools=[tool],&#13;
    verbose=True&#13;
)&#13;
response = agent.chat(&#13;
    "What were some famous buildings in ancient Rome?")&#13;
print(response)&#13;
    pip install llama-index-packs-agents-llm-compiler&#13;
    from llama_index.tools.database import DatabaseToolSpec&#13;
from llama_index.packs.agents_llm_compiler import LLMCompilerAgentPack&#13;
db_tools = DatabaseToolSpec(&#13;
    uri="sqlite:///files//database//employees.db")&#13;
agent = LLMCompilerAgentPack(db_tools.to_tool_list())&#13;
    response = agent.run(&#13;
    "Using only the available tools, "&#13;
    "List the HR department employee "&#13;
    "with the highest salary "&#13;
)&#13;
    from llama_index.core.agent import AgentRunner&#13;
from llama_index.agent.openai import OpenAIAgentWorker&#13;
from llama_index.tools.database import DatabaseToolSpec&#13;
db_tools = DatabaseToolSpec(&#13;
    uri="sqlite:///files//database//employees.db"&#13;
)&#13;
tools = db_tools.to_tool_list()&#13;
    step_engine = OpenAIAgentWorker.from_tools(&#13;
    tools,&#13;
    verbose=True&#13;
)&#13;
    agent = AgentRunner(step_engine)&#13;
input =  (&#13;
    "Find the highest paid HR employee and write "&#13;
    "them an email announcing a bonus"&#13;
)&#13;
    response = agent.chat(input)&#13;
print(response)&#13;
    task = agent.create_task(input)&#13;
step_output = agent.run_step(task.task_id)&#13;
    while not step_output.is_last:&#13;
    step_output = agent.run_step(task.task_id)&#13;
    response = agent.finalize_response(task.task_id)&#13;
print(response)&#13;
    <p>&#13;
     <em class="italic">&#13;
      The name is Bot.&#13;
     </em>&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Chat Bot&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      .&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     At the&#13;
     <a id="_idIndexMarker829">&#13;
     </a>&#13;
     beginning of this chapter, we talked about the growing popularity of the ChatOps model. This model is based on the interaction between groups of human operators and AI agents, who can understand the context of discussions to provide answers to questions but also to perform certain functions, thus playing the role of virtual assistants for the group&#13;
     <span class="No-Break">&#13;
      they serve.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     You probably realize, however, that the chat engine models we have discussed so far can only answer questions and cannot execute functions or interact in ways other than read-only with&#13;
     <span class="No-Break">&#13;
      backend data.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     For these use cases, we&#13;
     <a id="_idIndexMarker830">&#13;
     </a>&#13;
     <span class="No-Break">&#13;
      need&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <strong class="bold">&#13;
       agents&#13;
      </strong>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      .&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The major difference between an agent and a simple chat engine is that an agent operates based&#13;
     <a id="_idIndexMarker831">&#13;
     </a>&#13;
     on a&#13;
     <strong class="bold">&#13;
      reasoning loop&#13;
     </strong>&#13;
     and has several tools at its disposal. After all, who would be Bond without the gadgets that Q&#13;
     <span class="No-Break">&#13;
      always provides?&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Unlike a simple chatbot, which can – at best – answer questions, either directly with the help of an LLM or by extracting proprietary data from a knowledge base, agents are much more powerful and can handle far more complex scenarios. This gives them a lot more utility in a business context, where human interactions augmented by AI are becoming&#13;
     <span class="No-Break">&#13;
      increasingly prevalent.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Let’s understand the core components of an agent: the tools and the&#13;
     <span class="No-Break">&#13;
      reasoning loop.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor186">&#13;
    </a>&#13;
    <h2 id="_idParaDest-187">&#13;
     Building tools and ToolSpec classes for our agents&#13;
    </h2>&#13;
    <p>&#13;
     We&#13;
     <a id="_idIndexMarker832">&#13;
     </a>&#13;
     briefly discussed tools in&#13;
     <a>&#13;
      <span class="No-Break">&#13;
       <em class="italic">&#13;
        Chapter 6&#13;
       </em>&#13;
      </span>&#13;
     </a>&#13;
     ,&#13;
     <em class="italic">&#13;
      Querying Our Data, Part 1 – Context Retrieval.&#13;
     </em>&#13;
     However, because the main topic of&#13;
     <a>&#13;
      <span class="No-Break">&#13;
       <em class="italic">&#13;
        Chapter 6&#13;
       </em>&#13;
      </span>&#13;
     </a>&#13;
     was data querying, I only showed you how different query engines or retrievers can be wrapped in tools and then become components of a router. In many ways, you can think of a router as a very simple type of agent. It uses LLM reasoning to decide which query engine or retriever should be used, depending on their specified purpose and the actual&#13;
     <span class="No-Break">&#13;
      user query.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     But tools can be a lot&#13;
     <span class="No-Break">&#13;
      more useful.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     A tool can also be a wrapper for any kind of user-defined function, capable of reading or writing data, calling functions from external APIs, or executing any kind of code. This means that tools come in two&#13;
     <span class="No-Break">&#13;
      different flavors:&#13;
     </span>&#13;
    </p>&#13;
    <ul>&#13;
     <li>&#13;
      <code class="literal">&#13;
       QueryEngineTool&#13;
      </code>&#13;
      : This&#13;
      <a id="_idIndexMarker833">&#13;
      </a>&#13;
      can encapsulate any existing query engine. This is the kind we covered during&#13;
      <a>&#13;
       <span class="No-Break">&#13;
        <em class="italic">&#13;
         Chapter 6&#13;
        </em>&#13;
       </span>&#13;
      </a>&#13;
      and it can only provide read-only access to&#13;
      <span class="No-Break">&#13;
       our data&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       FunctionTool&#13;
      </code>&#13;
      : This&#13;
      <a id="_idIndexMarker834">&#13;
      </a>&#13;
      enables any user-defined function to be transformed into a tool. This is a universal type of tool as it allows any type of operation to&#13;
      <span class="No-Break">&#13;
       be executed&#13;
      </span>&#13;
     </li>&#13;
    </ul>&#13;
    <p>&#13;
     Because we have already seen examples of how&#13;
     <code class="literal">&#13;
      QueryEngineTool&#13;
     </code>&#13;
     works, let’s focus on&#13;
     <span class="No-Break">&#13;
      <code class="literal">&#13;
       FunctionTool&#13;
      </code>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      instead.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Here’s an example of how we can&#13;
     <span class="No-Break">&#13;
      define one:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     To&#13;
     <a id="_idIndexMarker835">&#13;
     </a>&#13;
     enable agents to assimilate our functions as tools, they must contain descriptive docstrings, just like in the previous example. LlamaIndex relies on&#13;
     <a id="_idIndexMarker836">&#13;
     </a>&#13;
     these&#13;
     <strong class="bold">&#13;
      docstrings&#13;
     </strong>&#13;
     to provide agents with an&#13;
     <em class="italic">&#13;
      understanding&#13;
     </em>&#13;
     of the purpose and proper usage of a particular tool wrapping a&#13;
     <span class="No-Break">&#13;
      user-defined function.&#13;
     </span>&#13;
    </p>&#13;
    <p class="callout-heading">&#13;
     Definition&#13;
    </p>&#13;
    <p class="callout">&#13;
     In Python, a docstring is a string literal that occurs as the first statement in a module, function, class, or method definition. It is used to document the purpose and usage of the code block it describes. Docstrings can be accessed from the code at runtime using the&#13;
     <code class="literal">&#13;
      __doc__&#13;
     </code>&#13;
     attribute on the object they describe, and they are also the primary way that documentation is generated&#13;
     <span class="No-Break">&#13;
      in Python.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     This description will be used by the reasoning loop of an agent to determine which particular tool is fit for solving a specific task, allowing the agent to decide the&#13;
     <span class="No-Break">&#13;
      execution path.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     However, competent agents are usually able to handle more than just&#13;
     <span class="No-Break">&#13;
      one tool.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     For this purpose, LlamaIndex also provides&#13;
     <a id="_idIndexMarker837">&#13;
     </a>&#13;
     the&#13;
     <code class="literal">&#13;
      ToolSpec&#13;
     </code>&#13;
     class. Akin to a collection of individual tools,&#13;
     <code class="literal">&#13;
      ToolSpec&#13;
     </code>&#13;
     specifies a full set of tools for a particular service. It’s like equipping our agent with a complete API for a particular type&#13;
     <span class="No-Break">&#13;
      of technology.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     We can build custom&#13;
     <code class="literal">&#13;
      ToolSpec&#13;
     </code>&#13;
     classes but there is also a growing number of them already available on LlamaHub:&#13;
     <a>&#13;
      https://llamahub.ai/?tab=tools&#13;
     </a>&#13;
     . They cover different types of service integrations, such as Gmail, Slack, SalesForce, Shopify, and&#13;
     <span class="No-Break">&#13;
      many others.&#13;
     </span>&#13;
    </p>&#13;
    <p class="callout-heading">&#13;
     The LlamaHub agent tool repository&#13;
    </p>&#13;
    <p class="callout">&#13;
     The LlamaHub agent tool repository is a key addition to LlamaHub, providing a curated collection of tool specs that enable agents to interact with and extend the functionality of a range of services. This repository simplifies the agent design process for various APIs and includes numerous practical examples in its notebooks for easy integration&#13;
     <span class="No-Break">&#13;
      and use.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Let’s take&#13;
     <a id="_idIndexMarker838">&#13;
     </a>&#13;
     the&#13;
     <code class="literal">&#13;
      DatabaseToolSpec&#13;
     </code>&#13;
     class available on LlamaHub as&#13;
     <span class="No-Break">&#13;
      an example.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     This&#13;
     <code class="literal">&#13;
      ToolSpec&#13;
     </code>&#13;
     class can be found here:&#13;
     <a>&#13;
      https://llamahub.ai/l/tools-database?from=tools&#13;
     </a>&#13;
     . First, let’s have a look at&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Figure 8&#13;
      </em>&#13;
     </span>&#13;
     <em class="italic">&#13;
      .7&#13;
     </em>&#13;
     to understand&#13;
     <span class="No-Break">&#13;
      its structure:&#13;
     </span>&#13;
    </p>&#13;
    <div>&#13;
     <div class="IMG---Figure" id="_idContainer084">&#13;
      <img src="../Images/B21861_08_7.jpg"/>&#13;
     </div>&#13;
    </div>&#13;
    <p class="IMG---Caption" lang="en-US">&#13;
     Figure 8.7 – DatabaseToolSpec&#13;
    </p>&#13;
    <p>&#13;
     Built&#13;
     <a id="_idIndexMarker839">&#13;
     </a>&#13;
     on top of the&#13;
     <a id="_idIndexMarker840">&#13;
     </a>&#13;
     SQLAlchemy library (&#13;
     <a>&#13;
      https://www.sqlalchemy.org/&#13;
     </a>&#13;
     ) this tool collection can access many types of databases while providing three&#13;
     <span class="No-Break">&#13;
      simple tools:&#13;
     </span>&#13;
    </p>&#13;
    <ul>&#13;
     <li>&#13;
      <code class="literal">&#13;
       list_tables&#13;
      </code>&#13;
      : A tool that lists the tables in the&#13;
      <span class="No-Break">&#13;
       database schema&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       describe_tables&#13;
      </code>&#13;
      : A tool that describes the schema of&#13;
      <span class="No-Break">&#13;
       a table&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       load_data&#13;
      </code>&#13;
      : A tool that accepts a SQL query as input and returns the&#13;
      <span class="No-Break">&#13;
       resulting data&#13;
      </span>&#13;
     </li>&#13;
    </ul>&#13;
    <p class="callout-heading">&#13;
     Quick note&#13;
    </p>&#13;
    <p class="callout">&#13;
     SQLAlchemy is a powerful and versatile toolkit for Python that allows developers to work with various databases, such as Microsoft SQL Server, OracleDB, MySQL, and others, in a more Pythonic way, abstracting away many of the complexities of database interaction and&#13;
     <span class="No-Break">&#13;
      query construction.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Because this is not a LlamaIndex core component but comes as an integration package instead, it must be installed in our&#13;
     <span class="No-Break">&#13;
      environment first:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Next, to initialize this&#13;
     <code class="literal">&#13;
      ToolSpec&#13;
     </code>&#13;
     , all we have to do is&#13;
     <span class="No-Break">&#13;
      import it:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Then, we must configure our database access,&#13;
     <span class="No-Break">&#13;
      like this:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Once&#13;
     <a id="_idIndexMarker841">&#13;
     </a>&#13;
     the&#13;
     <code class="literal">&#13;
      ToolSpec&#13;
     </code>&#13;
     class&#13;
     <a id="_idIndexMarker842">&#13;
     </a>&#13;
     has been built, if we want to initialize an agent with it, we have to convert it into a list of tools using the&#13;
     <code class="literal">&#13;
      to_tool_list()&#13;
     </code>&#13;
     method. This is because agents expect a list of tools as&#13;
     <span class="No-Break">&#13;
      an argument.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Here’s how we can easily convert the&#13;
     <code class="literal">&#13;
      ToolSpec&#13;
     </code>&#13;
     class into a list of&#13;
     <span class="No-Break">&#13;
      tool objects:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     At this point, we can pass&#13;
     <code class="literal">&#13;
      tool_list&#13;
     </code>&#13;
     as an argument when initializing any type of agent. Our agent will now be capable of&#13;
     <em class="italic">&#13;
      understanding&#13;
     </em>&#13;
     the schema of the database and extracting any required information from its tables. You can find a full example of how to use this&#13;
     <code class="literal">&#13;
      ToolSpec&#13;
     </code>&#13;
     class later in this chapter in the&#13;
     <em class="italic">&#13;
      OpenAIAgent&#13;
     </em>&#13;
     section. Next, let’s see how reasoning&#13;
     <span class="No-Break">&#13;
      loops work.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor187">&#13;
    </a>&#13;
    <h2 id="_idParaDest-188">&#13;
     Understanding reasoning loops&#13;
    </h2>&#13;
    <p>&#13;
     Having so&#13;
     <a id="_idIndexMarker843">&#13;
     </a>&#13;
     many specialized tools already&#13;
     <a id="_idIndexMarker844">&#13;
     </a>&#13;
     available for our agents is a great advantage. But unfortunately, a box full of some of the best-quality instruments is not always enough. Our agents also need to know&#13;
     <em class="italic">&#13;
      when&#13;
     </em>&#13;
     to use each of&#13;
     <span class="No-Break">&#13;
      these tools.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Specifically, the RAG applications we build need to decide – as autonomously as possible – which tool to use, depending on the specific user query and the dataset they are operating on. Any hard-coded solution will only deliver good results in a limited number of scenarios. This is where reasoning loops&#13;
     <span class="No-Break">&#13;
      come in.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The reasoning loop is a fundamental aspect of agents, enabling them to intelligently decide which tools to use in different scenarios. This aspect is important because, in complex, real-world applications, the requirements can vary significantly and a static approach would limit the&#13;
     <span class="No-Break">&#13;
      agent’s effectiveness.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Figure 8&#13;
      </em>&#13;
     </span>&#13;
     <em class="italic">&#13;
      .8&#13;
     </em>&#13;
     presents a visual representation of the reasoning&#13;
     <span class="No-Break">&#13;
      loop concept:&#13;
     </span>&#13;
    </p>&#13;
    <div>&#13;
     <div class="IMG---Figure" id="_idContainer085">&#13;
      <img src="../Images/B21861_08_8.jpg"/>&#13;
     </div>&#13;
    </div>&#13;
    <p class="IMG---Caption" lang="en-US">&#13;
     Figure 8.8 – The reasoning loop in an agent&#13;
    </p>&#13;
    <p>&#13;
     The&#13;
     <a id="_idIndexMarker845">&#13;
     </a>&#13;
     reasoning loop is responsible for the&#13;
     <a id="_idIndexMarker846">&#13;
     </a>&#13;
     decision-making process. It evaluates the context, understands the requirements of the task at hand, and then selects the appropriate tools from its arsenal to accomplish the task. This dynamic approach allows agents to adapt to various scenarios, making them versatile&#13;
     <span class="No-Break">&#13;
      and efficient.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     In LlamaIndex, the implementation of the reasoning loop is tailored to the type of agent. For instance,&#13;
     <code class="literal">&#13;
      OpenAIAgent&#13;
     </code>&#13;
     uses the Function API to make decisions, while&#13;
     <code class="literal">&#13;
      ReActAgent&#13;
     </code>&#13;
     relies on chat or text completion endpoints for its&#13;
     <span class="No-Break">&#13;
      reasoning process.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     This loop is not just about selecting the right tool, though; it’s also about determining the sequence in which the tools should be used and the specific parameters that should be applied. It’s the brain of the agent, orchestrating the tools to work together seamlessly, much like a skilled craftsman uses a combination of tools to create something greater than the sum of&#13;
     <span class="No-Break">&#13;
      its parts.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     This ability to intelligently interact with various tools and data sources, and read and modify data dynamically, sets agents apart from simpler chat engines and makes them invaluable in a business context where adaptability and intelligence&#13;
     <span class="No-Break">&#13;
      are key.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The remaining types of chat modes that I’m going to describe over the next few pages are not simple chat engines but agents at their core. They all operate using a list of tools but implement the reasoning loop in&#13;
     <span class="No-Break">&#13;
      different ways.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor188">&#13;
    </a>&#13;
    <h2 id="_idParaDest-189">&#13;
     OpenAIAgent&#13;
    </h2>&#13;
    <p>&#13;
     This&#13;
     <a id="_idIndexMarker847">&#13;
     </a>&#13;
     specialized agent leverages the capabilities&#13;
     <a id="_idIndexMarker848">&#13;
     </a>&#13;
     of OpenAI models, particularly those supporting the function calling API. It works with OpenAI models that have been designed to support the function calling API. They can interpret and execute function calls as part of&#13;
     <span class="No-Break">&#13;
      their capabilities.&#13;
     </span>&#13;
    </p>&#13;
    <p class="callout-heading">&#13;
     Quick note&#13;
    </p>&#13;
    <p class="callout">&#13;
     These models are designed to interpret prompts and context to determine when a function call is appropriate. They respond with outputs that adhere to the defined structure of the function, based on the patterns they’ve learned during training. For more information on this topic and a list of supported models, you may consult the official OpenAI&#13;
     <span class="No-Break">&#13;
      documentation:&#13;
     </span>&#13;
     <a>&#13;
      <span class="No-Break">&#13;
       https://platform.openai.com/docs/guides/function-calling&#13;
      </span>&#13;
     </a>&#13;
     <span class="No-Break">&#13;
      .&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The key advantage of this agent type is that the tool selection logic is implemented directly on the model itself. When a task is provided by the user to&#13;
     <strong class="bold">&#13;
      OpenAIAgent&#13;
     </strong>&#13;
     , along with any previous chat history, the function API will analyze the context and decide whether another tool needs to be invoked or if a final response can be returned. If it determines that another tool is required, the function API will output the name of that tool.&#13;
     <code class="literal">&#13;
      OpenAIAgent&#13;
     </code>&#13;
     will then execute the tool, passing the tool’s response back into the chat history. This cycle continues until the API returns a final message, indicating the reasoning loop&#13;
     <span class="No-Break">&#13;
      is complete.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Figure 8&#13;
      </em>&#13;
     </span>&#13;
     <em class="italic">&#13;
      .9&#13;
     </em>&#13;
     explains this&#13;
     <span class="No-Break">&#13;
      process visually:&#13;
     </span>&#13;
    </p>&#13;
    <div>&#13;
     <div class="IMG---Figure" id="_idContainer086">&#13;
      <img src="../Images/B21861_08_9.jpg"/>&#13;
     </div>&#13;
    </div>&#13;
    <p class="IMG---Caption" lang="en-US">&#13;
     Figure 8.9 – The simplified workflow of OpenAIAgent&#13;
    </p>&#13;
    <p>&#13;
     With the&#13;
     <a id="_idIndexMarker849">&#13;
     </a>&#13;
     model handling the complex logic of tool&#13;
     <a id="_idIndexMarker850">&#13;
     </a>&#13;
     selection and chaining,&#13;
     <code class="literal">&#13;
      OpenAIAgent&#13;
     </code>&#13;
     is a great solution for tool orchestration. One tradeoff is less flexibility compared to other architectures as the tool selection logic is hard-coded into&#13;
     <span class="No-Break">&#13;
      the LLM.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     However, for many use cases, the pre-trained capabilities of the function API model are sufficient to enable effective tool orchestration and&#13;
     <span class="No-Break">&#13;
      task completion.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Before proceeding to the next example, make sure you install the required&#13;
     <span class="No-Break">&#13;
      integration package:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     To&#13;
     <a id="_idIndexMarker851">&#13;
     </a>&#13;
     implement OpenAIAgent, we must define the available tools and then initialize the agent with these components, adding any other custom parameters we desire. The best way to explain how they work is through&#13;
     <span class="No-Break">&#13;
      an example.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     For the following example, we are using an SQLite database containing a single table called&#13;
     <em class="italic">&#13;
      Employees&#13;
     </em>&#13;
     . This table contains some randomly chosen salary data for 10 employees from different departments.&#13;
     <em class="italic">&#13;
      Table 8.1&#13;
     </em>&#13;
     displays the contents of the&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Employees&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      table:&#13;
     </span>&#13;
    </p>&#13;
    <table class="No-Table-Style _idGenTablePara-1" id="table001-3">&#13;
     <colgroup>&#13;
      <col/>&#13;
      <col/>&#13;
      <col/>&#13;
      <col/>&#13;
      <col/>&#13;
     </colgroup>&#13;
     <thead>&#13;
      <tr class="No-Table-Style">&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          <strong class="bold">&#13;
           ID&#13;
          </strong>&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          <strong class="bold">&#13;
           Name&#13;
          </strong>&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          <strong class="bold">&#13;
           Department&#13;
          </strong>&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          <strong class="bold">&#13;
           Salary&#13;
          </strong>&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          <strong class="bold">&#13;
           Email&#13;
          </strong>&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
      </tr>&#13;
     </thead>&#13;
     <tbody>&#13;
      <tr class="No-Table-Style">&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         1&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          Alice&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          IT&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          36420.77&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          Alice_IT@org.com&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
      </tr>&#13;
      <tr class="No-Table-Style">&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         2&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          Karen&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          Finance&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          57705.06&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          Alice_Finance@org.com&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
      </tr>&#13;
      <tr class="No-Table-Style">&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         3&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          Helen&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          IT&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          52612.51&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          Helen_IT@org.com&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
      </tr>&#13;
      <tr class="No-Table-Style">&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         4&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          Jackie&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          Finance&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          61374.58&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          Jack_Finance@org.com&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
      </tr>&#13;
      <tr class="No-Table-Style">&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         5&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          David&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          Finance&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          32242.72&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          David_Finance@org.com&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
      </tr>&#13;
      <tr class="No-Table-Style">&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         6&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          Cora&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          HR&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          62040.53&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          Alice_HR@org.com&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
      </tr>&#13;
      <tr class="No-Table-Style">&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         7&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          Ingrid&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          IT&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          70821.96&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          Alice_IT@org.com&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
      </tr>&#13;
      <tr class="No-Table-Style">&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         8&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          Jack&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          IT&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          57268.89&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          Jack_IT@org.com&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
      </tr>&#13;
      <tr class="No-Table-Style">&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         9&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          Bob&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          Finance&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          76868.23&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          Bob_Finance@org.com&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
      </tr>&#13;
      <tr class="No-Table-Style">&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          10&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          Bill&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          HR&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          74161.45&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          Bob_HR@org.com&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
      </tr>&#13;
     </tbody>&#13;
    </table>&#13;
    <p class="IMG---Caption" lang="en-US">&#13;
     Table 8.1 – The sample Employees table from the Employees.db file&#13;
    </p>&#13;
    <p>&#13;
     The&#13;
     <a id="_idIndexMarker852">&#13;
     </a>&#13;
     database file itself can be found in the&#13;
     <code class="literal">&#13;
      ch8/files/database&#13;
     </code>&#13;
     subfolder of this book’s GitHub repository. Let’s have a&#13;
     <a id="_idIndexMarker853">&#13;
     </a>&#13;
     look at&#13;
     <span class="No-Break">&#13;
      the code:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The first part is responsible for&#13;
     <span class="No-Break">&#13;
      the imports.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Next, it’s time to define a simple function that’s going to become a custom tool for our agent. This simple tool will allow us to save files in the local folder. Notice the detailed docstring that we are providing to&#13;
     <span class="No-Break">&#13;
      the agent:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Once the&#13;
     <a id="_idIndexMarker854">&#13;
     </a>&#13;
     function has been defined, we must wrap it into a new tool&#13;
     <span class="No-Break">&#13;
      called&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <code class="literal">&#13;
       save_tool&#13;
      </code>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      .&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     We also&#13;
     <a id="_idIndexMarker855">&#13;
     </a>&#13;
     initialize an entire&#13;
     <code class="literal">&#13;
      ToolSpec&#13;
     </code>&#13;
     class from the imported&#13;
     <code class="literal">&#13;
      DatabaseToolSpec&#13;
     </code>&#13;
     . We need these tools because the agent will have to read data from our SQLite database to solve&#13;
     <span class="No-Break">&#13;
      the task:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Once we’ve created&#13;
     <code class="literal">&#13;
      db_tools&#13;
     </code>&#13;
     , we must join it with&#13;
     <code class="literal">&#13;
      save_tool&#13;
     </code>&#13;
     and put them into a single list called&#13;
     <code class="literal">&#13;
      tools&#13;
     </code>&#13;
     . We’ll use this list as an argument for initializing&#13;
     <span class="No-Break">&#13;
      the agent.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Now, let’s build our agent. Notice that we’re not using the default LLM in this case; instead, we’re configuring our agent to use GPT-4 for&#13;
     <span class="No-Break">&#13;
      more accuracy:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     In the preceding&#13;
     <a id="_idIndexMarker856">&#13;
     </a>&#13;
     code, we initialized our agent using the list of tools we prepared. The&#13;
     <code class="literal">&#13;
      verbose&#13;
     </code>&#13;
     argument will make the agent display every execution step for better visibility of&#13;
     <a id="_idIndexMarker857">&#13;
     </a>&#13;
     the reasoning process. We also set&#13;
     <code class="literal">&#13;
      max_function_calls&#13;
     </code>&#13;
     to a larger value because, for complex tasks, the default value may not be enough to allow the agent to complete&#13;
     <span class="No-Break">&#13;
      the task.&#13;
     </span>&#13;
    </p>&#13;
    <p class="callout-heading">&#13;
     A quick note on the max_function_calls parameter&#13;
    </p>&#13;
    <p class="callout">&#13;
     It may be&#13;
     <a id="_idIndexMarker858">&#13;
     </a>&#13;
     tempting to simply set this to a very large value to avoid exhausting the function calls and increase the chances for the agent to solve the task. Keep in mind, however, that every function call incurs costs, and sometimes, agents have the bad habit of entering infinite loops. I call them&#13;
     <em class="italic">&#13;
      rogue agents&#13;
     </em>&#13;
     when&#13;
     <a id="_idIndexMarker859">&#13;
     </a>&#13;
     they do that. Chances are that if your agent implementation requires a lot of LLM calls to solve even simple tasks, you’re probably doing something wrong when defining or describing the&#13;
     <span class="No-Break">&#13;
      underlying tools.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Let’s continue with our code. It’s time to dispatch the task to&#13;
     <span class="No-Break">&#13;
      our agent:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     As you can see, the task we provided is relatively complex. Multiple steps will be required to solve it. As we are not providing too many details in the query, our agent will have to figure out the structure of the database and then craft a SQL query to extract the average salary in the organization and the list of employees from the IT department who are paid below&#13;
     <span class="No-Break">&#13;
      the average.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Since the&#13;
     <code class="literal">&#13;
      verbose&#13;
     </code>&#13;
     argument is set to&#13;
     <code class="literal">&#13;
      True&#13;
     </code>&#13;
     , running this sample will show you the entire reasoning logic and steps performed by&#13;
     <span class="No-Break">&#13;
      the agent.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Notice&#13;
     <a id="_idIndexMarker860">&#13;
     </a>&#13;
     how, in each step, the agent incorporates&#13;
     <a id="_idIndexMarker861">&#13;
     </a>&#13;
     outputs from the tools into its ongoing reasoning process. Once it has the list of employees, it will compose an email for each one. The final step of the task is to use our custom-created tool and save the results in a&#13;
     <span class="No-Break">&#13;
      local file.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     This is just a simple example. In a more complex implementation, instead of saving the text locally, for example, we&#13;
     <a id="_idIndexMarker862">&#13;
     </a>&#13;
     could import&#13;
     <code class="literal">&#13;
      GmailToolSpec&#13;
     </code>&#13;
     from LlamaHub and create email drafts that can be manually reviewed later and sent by the user. Unfortunately, that would have made the example much longer as&#13;
     <code class="literal">&#13;
      GmailToolSpec&#13;
     </code>&#13;
     requires stored credentials for the Google API, but I leave it to you to experiment with that&#13;
     <code class="literal">&#13;
      ToolSpec&#13;
     </code>&#13;
     class (&#13;
     <a>&#13;
      https://llamahub.ai/l/tools-gmail?from=tools&#13;
     </a>&#13;
     ) and all the other tools available&#13;
     <span class="No-Break">&#13;
      on LlamaHub.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The customizable&#13;
     <a id="_idIndexMarker863">&#13;
     </a>&#13;
     parameters of&#13;
     <code class="literal">&#13;
      OpenAIAgent&#13;
     </code>&#13;
     are&#13;
     <span class="No-Break">&#13;
      as follows:&#13;
     </span>&#13;
    </p>&#13;
    <ul>&#13;
     <li>&#13;
      <code class="literal">&#13;
       tools&#13;
      </code>&#13;
      : A list of&#13;
      <code class="literal">&#13;
       BaseTool&#13;
      </code>&#13;
      instances that the agent can utilize during the chat session. These tools can range from specialized query engines to custom processing modules or collections of tools extracted from&#13;
      <span class="No-Break">&#13;
       <code class="literal">&#13;
        ToolSpec&#13;
       </code>&#13;
      </span>&#13;
      <span class="No-Break">&#13;
       classes&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       llm&#13;
      </code>&#13;
      : Any OpenAI model that supports the function calling API. The default model that’s used&#13;
      <span class="No-Break">&#13;
       is&#13;
      </span>&#13;
      <span class="No-Break">&#13;
       <code class="literal">&#13;
        gpt-3.5-turbo-0613&#13;
       </code>&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       memory&#13;
      </code>&#13;
      : Just like with any chat engine, this is a&#13;
      <code class="literal">&#13;
       ChatMemoryBuffer&#13;
      </code>&#13;
      instance that can be used for storing and managing the&#13;
      <span class="No-Break">&#13;
       chat history&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       prefix_messages&#13;
      </code>&#13;
      : A list of&#13;
      <code class="literal">&#13;
       ChatMessage&#13;
      </code>&#13;
      instances that serve as pre-configured messages or prompts at the start of the&#13;
      <span class="No-Break">&#13;
       chat session&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       max_function_calls&#13;
      </code>&#13;
      : The maximum number of function calls that can be made to the OpenAI model during a single chat interaction. The default&#13;
      <span class="No-Break">&#13;
       is 5&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       default_tool_choice&#13;
      </code>&#13;
      : A string indicating the default choice of tool to be used when multiple tools are available. This is useful for coercing the agent into using a&#13;
      <span class="No-Break">&#13;
       specific tool&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       callback_manager&#13;
      </code>&#13;
      : An optional&#13;
      <code class="literal">&#13;
       CallbackManager&#13;
      </code>&#13;
      instance for managing callbacks&#13;
      <a id="_idIndexMarker864">&#13;
      </a>&#13;
      during the chat process, aiding in tracing,&#13;
      <span class="No-Break">&#13;
       and debugging&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       system_prompt&#13;
      </code>&#13;
      : An optional initial system prompt that provides context or instructions for&#13;
      <span class="No-Break">&#13;
       the agent&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       verbose&#13;
      </code>&#13;
      : A Boolean flag to enable detailed logging&#13;
      <span class="No-Break">&#13;
       during operation&#13;
      </span>&#13;
     </li>&#13;
    </ul>&#13;
    <p>&#13;
     Overall,&#13;
     <code class="literal">&#13;
      OpenAIAgent&#13;
     </code>&#13;
     stands&#13;
     <a id="_idIndexMarker865">&#13;
     </a>&#13;
     out from other chat engines due to its ability to execute complex function calls, on top of contextually rich conversations. This makes it particularly suitable for scenarios where advanced functionalities, such as integrating external tools or processing user queries in more sophisticated ways, are required.&#13;
     <code class="literal">&#13;
      OpenAIAgent&#13;
     </code>&#13;
     provides a versatile and powerful platform for creating engaging and intelligent&#13;
     <span class="No-Break">&#13;
      chat experiences.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     But wait – there are other types of&#13;
     <span class="No-Break">&#13;
      agents too.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor189">&#13;
    </a>&#13;
    <h2 id="_idParaDest-190">&#13;
     ReActAgent&#13;
    </h2>&#13;
    <p>&#13;
     In contrast&#13;
     <a id="_idIndexMarker866">&#13;
     </a>&#13;
     to&#13;
     <code class="literal">&#13;
      OpenAIAgent&#13;
     </code>&#13;
     ,&#13;
     <strong class="bold">&#13;
      ReActAgent&#13;
     </strong>&#13;
     uses&#13;
     <a id="_idIndexMarker867">&#13;
     </a>&#13;
     more generic text completion endpoints that can be driven by any LLM. It operates based on a&#13;
     <strong class="bold">&#13;
      ReAct&#13;
     </strong>&#13;
     loop within a chat mode built on top of a set&#13;
     <span class="No-Break">&#13;
      of tools.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     This loop&#13;
     <a id="_idIndexMarker868">&#13;
     </a>&#13;
     involves deciding whether to use any of the available tools, potentially using it and observing its output, and then deciding whether to repeat the process or provide a final response. This flexibility allows it to choose between using tools or relying solely on the LLM. However, this also means that its performance is heavily dependent on the quality of the LLM, often requiring more nuanced prompting to ensure accurate knowledge base queries, rather than relying on potentially inaccurate&#13;
     <span class="No-Break">&#13;
      model-generated responses.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The input prompt for&#13;
     <code class="literal">&#13;
      ReActAgent&#13;
     </code>&#13;
     is carefully designed to guide the model in tool selection, using a format inspired by the ReAct paper by Yao, S., et al. (2022),&#13;
     <em class="italic">&#13;
      ReAct: Synergizing Reasoning and Acting in Language&#13;
     </em>&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Models&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      (&#13;
     </span>&#13;
     <a>&#13;
      <span class="No-Break">&#13;
       https://arxiv.org/abs/2210.03629&#13;
      </span>&#13;
     </a>&#13;
     <span class="No-Break">&#13;
      ).&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     It presents a list of available tools and asks the model to select one and provide the required parameters in JSON format. This explicit prompt is critical to the agent’s decision-making process. After selecting a tool, the agent executes it and integrates the response into&#13;
     <a id="_idIndexMarker869">&#13;
     </a>&#13;
     the chat history. This cycle of prompting, execution, and response integration continues until a satisfactory response is achieved. For an overall visual representation of the workflow, you may review the diagram that was presented for&#13;
     <code class="literal">&#13;
      OpenAIAgent&#13;
     </code>&#13;
     in&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Figure 8&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       .9&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      .&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Unlike&#13;
     <code class="literal">&#13;
      OpenAIAgent&#13;
     </code>&#13;
     , which uses a function calling API with a model capable of selecting and chaining together multiple tools, the&#13;
     <code class="literal">&#13;
      ReActAgent&#13;
     </code>&#13;
     class’s logic must be fully encoded through&#13;
     <span class="No-Break">&#13;
      its prompts.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     <code class="literal">&#13;
      ReActAgent&#13;
     </code>&#13;
     uses&#13;
     <a id="_idIndexMarker870">&#13;
     </a>&#13;
     a predefined loop with a maximum number of iterations, along with strategic prompting, to mimic a reasoning loop. Nevertheless, with strategic prompt engineering,&#13;
     <code class="literal">&#13;
      ReActAgent&#13;
     </code>&#13;
     can achieve effective tool orchestration and chained execution, similar to the output of the OpenAI&#13;
     <span class="No-Break">&#13;
      Function API.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The key difference is that whereas the logic of the OpenAI Function API is embedded in the model,&#13;
     <code class="literal">&#13;
      ReActAgent&#13;
     </code>&#13;
     relies on the structure of its prompts to induce the desired tool selection behavior. This approach offers considerable flexibility as it can adapt to different language model backends, allowing for different implementations&#13;
     <span class="No-Break">&#13;
      and applications.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     In this case, we have the usual customizable parameters that we discussed for&#13;
     <code class="literal">&#13;
      OpenAIAgent&#13;
     </code>&#13;
     :&#13;
     <code class="literal">&#13;
      tools&#13;
     </code>&#13;
     ,&#13;
     <code class="literal">&#13;
      llm&#13;
     </code>&#13;
     ,&#13;
     <code class="literal">&#13;
      memory&#13;
     </code>&#13;
     ,&#13;
     <code class="literal">&#13;
      callback_manager&#13;
     </code>&#13;
     ,&#13;
     <span class="No-Break">&#13;
      and&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <code class="literal">&#13;
       verbose&#13;
      </code>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      .&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     In addition,&#13;
     <code class="literal">&#13;
      ReActAgent&#13;
     </code>&#13;
     comes with&#13;
     <a id="_idIndexMarker871">&#13;
     </a>&#13;
     a few&#13;
     <span class="No-Break">&#13;
      specific parameters:&#13;
     </span>&#13;
    </p>&#13;
    <ul>&#13;
     <li>&#13;
      <code class="literal">&#13;
       max_iterations&#13;
      </code>&#13;
      : Similar to&#13;
      <code class="literal">&#13;
       max_function_calls&#13;
      </code>&#13;
      , this parameter sets the maximum number of iterations the ReAct loop can execute. This limit ensures that the agent does not enter an endless loop&#13;
      <span class="No-Break">&#13;
       of processing&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       react_chat_formatter&#13;
      </code>&#13;
      : This formats the chat history into a structured list of&#13;
      <code class="literal">&#13;
       ChatMessages&#13;
      </code>&#13;
      , alternating between user and assistant roles, based on the provided tools, chat history, and reasoning steps. This helps maintain clarity and consistency in the&#13;
      <span class="No-Break">&#13;
       reasoning loop&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       output_parser&#13;
      </code>&#13;
      : An optional instance of the&#13;
      <code class="literal">&#13;
       ReActOutputParser&#13;
      </code>&#13;
      class. This parser processes the outputs generated by the agent, helping in interpreting, and formatting&#13;
      <span class="No-Break">&#13;
       them appropriately&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       tool_retriever&#13;
      </code>&#13;
      : An optional instance of&#13;
      <code class="literal">&#13;
       ObjectRetriever&#13;
      </code>&#13;
      for&#13;
      <code class="literal">&#13;
       BaseTool&#13;
      </code>&#13;
      . This&#13;
      <a id="_idIndexMarker872">&#13;
      </a>&#13;
      retriever can be used to dynamically fetch tools based on certain criteria. Similar to how we index nodes, there is also an option to create an&#13;
      <code class="literal">&#13;
       ObjectIndex&#13;
      </code>&#13;
      index to index a set of tools. This can be especially useful when we have to work with a large number of tools. You can find more information about this feature in the official&#13;
      <span class="No-Break">&#13;
       documentation:&#13;
      </span>&#13;
      <a>&#13;
       <span class="No-Break">&#13;
        https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/usage_pattern.html#function-retrieval-agents&#13;
       </span>&#13;
      </a>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       context&#13;
      </code>&#13;
      : An optional string providing initial instructions for&#13;
      <span class="No-Break">&#13;
       the agent&#13;
      </span>&#13;
     </li>&#13;
    </ul>&#13;
    <p>&#13;
     Initializing&#13;
     <a id="_idIndexMarker873">&#13;
     </a>&#13;
     and using&#13;
     <code class="literal">&#13;
      ReActAgent&#13;
     </code>&#13;
     is done the same as with the OpenAI one, except this time, you won’t need to install any integration packages first – this type of agent is part of the core&#13;
     <span class="No-Break">&#13;
      LlamaIndex components:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Overall,&#13;
     <code class="literal">&#13;
      ReActAgent&#13;
     </code>&#13;
     stands out for its flexibility as it can use any LLM to drive its unique ReAct loop, enabling it to smartly choose and use various tools. It’s like having a virtual assistant that not only answers questions but also intelligently decides when to consult external sources, making the conversation more contextually relevant and improving the&#13;
     <span class="No-Break">&#13;
      user experience.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor190">&#13;
    </a>&#13;
    <h2 id="_idParaDest-191">&#13;
     How do we interact with agents?&#13;
    </h2>&#13;
    <p>&#13;
     There&#13;
     <a id="_idIndexMarker874">&#13;
     </a>&#13;
     are two main methods that we can use to interact with an agent:&#13;
     <code class="literal">&#13;
      chat()&#13;
     </code>&#13;
     and&#13;
     <code class="literal">&#13;
      query()&#13;
     </code>&#13;
     . The first method utilizes stored conversation history to provide context-informed responses, making it&#13;
     <a id="_idIndexMarker875">&#13;
     </a>&#13;
     suitable for&#13;
     <span class="No-Break">&#13;
      ongoing dialogues.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     On the other hand, the former method operates in a stateless mode, treating each call independently without reference to past interactions. This is better suited for&#13;
     <span class="No-Break">&#13;
      standalone requests.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor191">&#13;
    </a>&#13;
    <h2 id="_idParaDest-192">&#13;
     Enhancing our agents with the help of utility tools&#13;
    </h2>&#13;
    <p>&#13;
     To improve&#13;
     <a id="_idIndexMarker876">&#13;
     </a>&#13;
     the capabilities of the existing tools, LlamaIndex also provides two very useful so-called&#13;
     <em class="italic">&#13;
      utility tools&#13;
     </em>&#13;
     –&#13;
     <code class="literal">&#13;
      OnDemandLoaderTool&#13;
     </code>&#13;
     and&#13;
     <code class="literal">&#13;
      LoadAndSearchToolSpec&#13;
     </code>&#13;
     . They are universal and can be used with any type of agent to augment the standard tool functionality in&#13;
     <span class="No-Break">&#13;
      certain scenarios.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     One common&#13;
     <a id="_idIndexMarker877">&#13;
     </a>&#13;
     issue when interacting with an API is that we might receive a very long response in return. Our agents may not always be able to handle such&#13;
     <span class="No-Break">&#13;
      large outputs.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Problems may arise because they may overflow the context window of the LLM or sometimes, key context may be diluted by a large amount of data, decreasing the accuracy of the agent’s&#13;
     <span class="No-Break">&#13;
      reasoning logic.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     A good way to understand this issue is by looking at our previous example for&#13;
     <code class="literal">&#13;
      OpenAIAgent&#13;
     </code>&#13;
     . In that case, we used a collection of tools called&#13;
     <code class="literal">&#13;
      DatabaseToolSpec&#13;
     </code>&#13;
     to retrieve data from our sample&#13;
     <em class="italic">&#13;
      Employees&#13;
     </em>&#13;
     table. If you’ve run that particular agent with the&#13;
     <code class="literal">&#13;
      Verbose&#13;
     </code>&#13;
     parameter set to&#13;
     <code class="literal">&#13;
      True&#13;
     </code>&#13;
     , then you’ve probably noticed that the outputs produced by the&#13;
     <code class="literal">&#13;
      load_data&#13;
     </code>&#13;
     tool are in the form of LlamaIndex document objects, as we can see in&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Figure 8&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       .10&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      :&#13;
     </span>&#13;
    </p>&#13;
    <div>&#13;
     <div class="IMG---Figure" id="_idContainer087">&#13;
      <img src="../Images/B21861_08_10.jpg"/>&#13;
     </div>&#13;
    </div>&#13;
    <p class="IMG---Caption" lang="en-US">&#13;
     Figure 8.10 – Sample output for the OpenAIAgent code example&#13;
    </p>&#13;
    <p>&#13;
     This means that whenever the agent calls the&#13;
     <code class="literal">&#13;
      load_data&#13;
     </code>&#13;
     tool, using a SQL query to interrogate the database, instead of simply receiving the output of the query, it gets a whole document&#13;
     <a id="_idIndexMarker878">&#13;
     </a>&#13;
     in return – along with a bunch of additional data, such as the ID of the document, metadata fields, hashes, and so on. The agent has to extract the actual query results from that data using the LLM, hence the aforementioned&#13;
     <span class="No-Break">&#13;
      potential issues.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     So, what if we want to extract&#13;
     <em class="italic">&#13;
      only&#13;
     </em>&#13;
     the result of the query, without all the additional data on top of it? That is the job&#13;
     <span class="No-Break">&#13;
      of&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <code class="literal">&#13;
       LoadAndSearchToolSpec&#13;
      </code>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      .&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Understanding the LoadAndSearchToolSpec utility&#13;
    </h3>&#13;
    <p>&#13;
     This&#13;
     <a id="_idIndexMarker879">&#13;
     </a>&#13;
     utility tool is designed to help the&#13;
     <a id="_idIndexMarker880">&#13;
     </a>&#13;
     agent handle large volumes of data from API endpoints, as demonstrated in&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Figure 8&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       .11&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      :&#13;
     </span>&#13;
    </p>&#13;
    <div>&#13;
     <div class="IMG---Figure" id="_idContainer088">&#13;
      <img src="../Images/B21861_08_11.jpg"/>&#13;
     </div>&#13;
    </div>&#13;
    <p class="IMG---Caption" lang="en-US">&#13;
     Figure 8.11 – Visualization of a direct API call versus interaction via LoadAndSearchToolSpec&#13;
    </p>&#13;
    <p>&#13;
     It takes an existing tool and generates two separate tools: one for loading and indexing data – by default, using a vector index – and another for conducting searches on this indexed data. The agent will now use the&#13;
     <em class="italic">&#13;
      Load&#13;
     </em>&#13;
     tool to ingest the data, and, similar to a caching mechanism, it will store it in an index. In the next step, the agent will use the&#13;
     <em class="italic">&#13;
      Search&#13;
     </em>&#13;
     tool to extract only the needed information using a built-in&#13;
     <span class="No-Break">&#13;
      query engine.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Let’s see how that translates into code. We will adapt the previous&#13;
     <code class="literal">&#13;
      OpenAIAgent&#13;
     </code>&#13;
     example so that it&#13;
     <span class="No-Break">&#13;
      uses&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <code class="literal">&#13;
       LoadAndSearchToolSpec&#13;
      </code>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      :&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Once we&#13;
     <a id="_idIndexMarker881">&#13;
     </a>&#13;
     finished with the imports, we&#13;
     <a id="_idIndexMarker882">&#13;
     </a>&#13;
     initialized our&#13;
     <code class="literal">&#13;
      DatabaseToolSpec&#13;
     </code>&#13;
     utility, which points to the same sample SQLite database as in the previous example. However, this time, we didn’t add any additional tools since we’ll only run a simple query. For that reason, we only pass the first tool from&#13;
     <code class="literal">&#13;
      ToolSpec&#13;
     </code>&#13;
     – that is,&#13;
     <code class="literal">&#13;
      tool_list[0]&#13;
     </code>&#13;
     – as an argument to&#13;
     <code class="literal">&#13;
      LoadAndSearchToolSpec&#13;
     </code>&#13;
     . That’s the&#13;
     <code class="literal">&#13;
      load_data&#13;
     </code>&#13;
     function, by the way. We don’t need the other two functions available in the database’s&#13;
     <code class="literal">&#13;
      ToolSpec&#13;
     </code>&#13;
     <span class="No-Break">&#13;
      this time.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     From this point on, the code is very&#13;
     <span class="No-Break">&#13;
      much straightforward:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     If you look at the output – presented in&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Figure 8&#13;
      </em>&#13;
     </span>&#13;
     <em class="italic">&#13;
      .12&#13;
     </em>&#13;
     – you’ll notice the reduced amount of data the agent has to deal with&#13;
     <span class="No-Break">&#13;
      this time:&#13;
     </span>&#13;
    </p>&#13;
    <div>&#13;
     <div class="IMG---Figure" id="_idContainer089">&#13;
      <img src="../Images/B21861_08_12.jpg"/>&#13;
     </div>&#13;
    </div>&#13;
    <p class="IMG---Caption" lang="en-US">&#13;
     Figure 8.12 – Sample agent output when LoadAndSearchToolSpec is used&#13;
    </p>&#13;
    <p>&#13;
     Instead&#13;
     <a id="_idIndexMarker883">&#13;
     </a>&#13;
     of receiving an entire document&#13;
     <a id="_idIndexMarker884">&#13;
     </a>&#13;
     as a response, the first call returns just a confirmation message that the data has been loaded and indexed, while the second extracts the final response using a query. We’ll talk about another utility&#13;
     <span class="No-Break">&#13;
      tool next.&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Understanding OnDemandLoaderTool&#13;
    </h3>&#13;
    <p>&#13;
     Another&#13;
     <a id="_idIndexMarker885">&#13;
     </a>&#13;
     important utility is&#13;
     <code class="literal">&#13;
      OnDemandLoaderTool&#13;
     </code>&#13;
     . This&#13;
     <a id="_idIndexMarker886">&#13;
     </a>&#13;
     utility is designed to make the process of loading, indexing, and querying data seamless and efficient within an agent’s workflow, particularly when dealing with large volumes of data from&#13;
     <span class="No-Break">&#13;
      various sources.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     It simplifies the process of using data loaders for agents by allowing them to trigger the loading, indexing, and querying of data through a single&#13;
     <span class="No-Break">&#13;
      tool call.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The normal approach in a RAG workflow would be to ingest all data at the start of our application, then chunk it, index it, and build a query engine on it. But that may not always be the most&#13;
     <span class="No-Break">&#13;
      efficient method.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Let’s say we have a large number of data sources. Ingesting and indexing all of them during startup would take a very long time, negatively affecting the user experience. And what if the user asks a question that cannot be answered by the agent based on the ingested data sources alone? That’s where a feature like this&#13;
     <span class="No-Break">&#13;
      becomes useful.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     <code class="literal">&#13;
      OnDemandLoaderTool&#13;
     </code>&#13;
     is especially useful in scenarios where data requirements are dynamic and unpredictable. Instead of pre-loading a vast amount of data at startup, which may not all be relevant to the user’s current needs, this tool enables an agent to fetch, index, and query data on demand. This approach significantly enhances efficiency as it allows the agent to focus only on the relevant data at any given time, rather than handling large datasets that may not be&#13;
     <span class="No-Break">&#13;
      immediately necessary.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     How does it work? It&#13;
     <a id="_idIndexMarker887">&#13;
     </a>&#13;
     takes any existing data loader and wraps it into a tool that can be used by the&#13;
     <a id="_idIndexMarker888">&#13;
     </a>&#13;
     agent as required. Before running the code, make sure you install the Wikipedia&#13;
     <span class="No-Break">&#13;
      integration package:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Here’s the sample code. We’ll start with&#13;
     <span class="No-Break">&#13;
      the imports:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Next, let’s define an on-demand tool for our agent, based&#13;
     <span class="No-Break">&#13;
      on&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <code class="literal">&#13;
       WikipediaReader&#13;
      </code>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      :&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Notice how I provided usage instructions in the description argument. These should help the agent better&#13;
     <em class="italic">&#13;
      understand&#13;
     </em>&#13;
     how to properly use the tool, although it might still take a few tries to get it right. Now, it’s time to initialize&#13;
     <span class="No-Break">&#13;
      the agent:&#13;
     </span>&#13;
    </p>&#13;
    <p class="callout-heading">&#13;
     Important side note&#13;
    </p>&#13;
    <p class="callout">&#13;
     One big advantage of using this approach is that once data has been loaded into the index, it’s also cached. Therefore, subsequent queries on the same topic will&#13;
     <span class="No-Break">&#13;
      run faster.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     In&#13;
     <a id="_idIndexMarker889">&#13;
     </a>&#13;
     addition,&#13;
     <code class="literal">&#13;
      OnDemandLoaderTool&#13;
     </code>&#13;
     can&#13;
     <a id="_idIndexMarker890">&#13;
     </a>&#13;
     be chained together with other, regular tools, allowing the agent to handle more&#13;
     <span class="No-Break">&#13;
      complex scenarios.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     With that, we’ve covered the basics. Now, let’s have a look at more advanced types&#13;
     <span class="No-Break">&#13;
      of agents.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor192">&#13;
    </a>&#13;
    <h2 id="_idParaDest-193">&#13;
     Using the LLMCompiler agent for more advanced scenarios&#13;
    </h2>&#13;
    <p>&#13;
     I saved the best&#13;
     <span class="No-Break">&#13;
      for last.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     While&#13;
     <a id="_idIndexMarker891">&#13;
     </a>&#13;
     they tend to perform well in many scenarios, both OpenAI and ReAct agents have&#13;
     <a id="_idIndexMarker892">&#13;
     </a>&#13;
     some drawbacks. Because current LLMs are not very good at long-term planning, they can sometimes get stuck in an infinite loop without finding the desired solution. At other times, their attention can be distracted by certain outputs they receive during execution, and this can cause them to stop before solving the&#13;
     <span class="No-Break">&#13;
      given task.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     But probably the biggest drawback of these types of agents is their serialized way of working. In other words, the execution of the steps is done in sequence. These agents wait for the output generated by one step to trigger the next step. This is a very inefficient approach in many practical scenarios. Often, a series of steps can be executed in parallel, significantly improving application performance and user experience. Based on these premises, I will now present a more advanced form&#13;
     <span class="No-Break">&#13;
      of agent.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Inspired by the paper by Kim, S., et al. (2023),&#13;
     <em class="italic">&#13;
      An LLM Compiler for Parallel Function Calling&#13;
     </em>&#13;
     (&#13;
     <a>&#13;
      https://arxiv.org/abs/2312.04511&#13;
     </a>&#13;
     ), this agent implementation offers outstanding performance and scalability. The concept is based on the ability of LLMs to execute multiple functions in parallel and draws inspiration from classical compilers to efficiently orchestrate&#13;
     <span class="No-Break">&#13;
      multi-function execution.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The&#13;
     <strong class="bold">&#13;
      LLMCompiler agent&#13;
     </strong>&#13;
     orchestrates these parallel function calls using a three-part system that plans, dispatches, and executes tasks, resulting in faster and more accurate multi-function calls compared to sequential methods. Just as compilers transform and optimize&#13;
     <a id="_idIndexMarker893">&#13;
     </a>&#13;
     code to run efficiently, LLMCompiler transforms natural language queries into optimized sequences of function calls that can be executed in parallel when dependencies allow. This makes calling multiple tools with LLMs faster, cheaper, and potentially more accurate. An additional advantage is that it works with any kind of LLM, including both open source and closed&#13;
     <span class="No-Break">&#13;
      source models.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Under the hood, an LLMCompileraAgent has three&#13;
     <span class="No-Break">&#13;
      main components:&#13;
     </span>&#13;
    </p>&#13;
    <ul>&#13;
     <li>&#13;
      <strong class="bold">&#13;
       LLM planner&#13;
      </strong>&#13;
      : Formulates&#13;
      <a id="_idIndexMarker894">&#13;
      </a>&#13;
      execution strategies and dependencies from user input&#13;
      <span class="No-Break">&#13;
       and examples&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <strong class="bold">&#13;
       Task-fetching unit&#13;
      </strong>&#13;
      : Sends&#13;
      <a id="_idIndexMarker895">&#13;
      </a>&#13;
      and updates function-calling tasks based on&#13;
      <span class="No-Break">&#13;
       the dependencies&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <strong class="bold">&#13;
       Executor&#13;
      </strong>&#13;
      : Executes&#13;
      <a id="_idIndexMarker896">&#13;
      </a>&#13;
      tasks in parallel using&#13;
      <span class="No-Break">&#13;
       associated tools&#13;
      </span>&#13;
     </li>&#13;
    </ul>&#13;
    <p>&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Figure 8&#13;
      </em>&#13;
     </span>&#13;
     <em class="italic">&#13;
      .13&#13;
     </em>&#13;
     explains&#13;
     <a id="_idIndexMarker897">&#13;
     </a>&#13;
     the structure of the LLMCompiler&#13;
     <span class="No-Break">&#13;
      agent visually:&#13;
     </span>&#13;
    </p>&#13;
    <div>&#13;
     <div class="IMG---Figure" id="_idContainer090">&#13;
      <img src="../Images/B21861_08_13.jpg"/>&#13;
     </div>&#13;
    </div>&#13;
    <p class="IMG---Caption" lang="en-US">&#13;
     Figure 8.13 – An overview of the LLMCompiler agent’s architecture&#13;
    </p>&#13;
    <p>&#13;
     The&#13;
     <em class="italic">&#13;
      LLM planner&#13;
     </em>&#13;
     determines the order of function calls and their interdependencies according to&#13;
     <a id="_idIndexMarker898">&#13;
     </a>&#13;
     user input. Next, the&#13;
     <em class="italic">&#13;
      task-fetching unit&#13;
     </em>&#13;
     initiates parallel execution of these functions, replacing variables with the outputs from prior tasks. The&#13;
     <em class="italic">&#13;
      executor&#13;
     </em>&#13;
     then carries out these function calls with the relevant tools. Combined, these elements enhance the efficiency of parallel function calling&#13;
     <span class="No-Break">&#13;
      in LLMs.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The&#13;
     <strong class="bold">&#13;
      directed acyclic graph&#13;
     </strong>&#13;
     (&#13;
     <strong class="bold">&#13;
      DAG&#13;
     </strong>&#13;
     ) of&#13;
     <a id="_idIndexMarker899">&#13;
     </a>&#13;
     tasks is a key data structure created by the&#13;
     <em class="italic">&#13;
      LLM planner&#13;
     </em>&#13;
     from user inputs and examples. This planning graph captures task dependencies&#13;
     <a id="_idIndexMarker900">&#13;
     </a>&#13;
     and enables optimized parallel&#13;
     <span class="No-Break">&#13;
      execution (&#13;
     </span>&#13;
     <a>&#13;
      <span class="No-Break">&#13;
       https://en.wikipedia.org/wiki/Directed_acyclic_graph&#13;
      </span>&#13;
     </a>&#13;
     <span class="No-Break">&#13;
      ).&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The DAG facilitates the simultaneous execution of tasks that do not depend on each other. Should one task rely on the completion of another, the prerequisite task must finish before the dependent task can commence. Independent tasks, on the other hand, are capable of being executed concurrently without any&#13;
     <span class="No-Break">&#13;
      dependency constraints.&#13;
     </span>&#13;
    </p>&#13;
    <p class="callout-heading">&#13;
     Quick note&#13;
    </p>&#13;
    <p class="callout">&#13;
     While OpenAI has already introduced parallel function calling into their API, the LLMCompiler is still superior in its approach because it manifests fault tolerance in case of wrong LLM decisions and can replan, depending on the&#13;
     <span class="No-Break">&#13;
      outputs generated.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     To understand how we can&#13;
     <a id="_idIndexMarker901">&#13;
     </a>&#13;
     implement an agent using the LLMCompiler, let’s have a look at a simple example. But first, to run the example, you’ll need to install the necessary&#13;
     <span class="No-Break">&#13;
      integration package:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Here’s&#13;
     <span class="No-Break">&#13;
      the code:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     After importing&#13;
     <code class="literal">&#13;
      LLMCompilerAgentPack&#13;
     </code>&#13;
     and&#13;
     <code class="literal">&#13;
      DatabaseToolSpec&#13;
     </code>&#13;
     , we initialized the&#13;
     <a id="_idIndexMarker902">&#13;
     </a>&#13;
     database tools and used the tool list to initialize the agent. It’s now time to interact with the agent, this time using the&#13;
     <span class="No-Break">&#13;
      <code class="literal">&#13;
       run()&#13;
      </code>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      method:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Figure 8&#13;
      </em>&#13;
     </span>&#13;
     <em class="italic">&#13;
      .14&#13;
     </em>&#13;
     shows the output of the&#13;
     <span class="No-Break">&#13;
      preceding code:&#13;
     </span>&#13;
    </p>&#13;
    <div>&#13;
     <div class="IMG---Figure" id="_idContainer091">&#13;
      <img src="../Images/B21861_08_14.jpg"/>&#13;
     </div>&#13;
    </div>&#13;
    <p class="IMG---Caption" lang="en-US">&#13;
     Figure 8.14 – Sample output of the LLMCompiler agent&#13;
    </p>&#13;
    <p>&#13;
     Looking&#13;
     <a id="_idIndexMarker903">&#13;
     </a>&#13;
     at the output, we can see both the execution plan generated by the agent and the actual steps performed. Quite neat,&#13;
     <span class="No-Break">&#13;
      isn’t it?&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     In conclusion, LLMCompiler-based agents represent a leap forward in addressing the limitations of serial execution found in traditional agents, pushing the boundaries of what’s possible in terms of chatbot implementations and&#13;
     <span class="No-Break">&#13;
      user interaction.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor193">&#13;
    </a>&#13;
    <h2 id="_idParaDest-194">&#13;
     Using the low-level Agent Protocol API&#13;
    </h2>&#13;
    <p>&#13;
     Taking&#13;
     <a id="_idIndexMarker904">&#13;
     </a>&#13;
     inspiration from&#13;
     <a id="_idIndexMarker905">&#13;
     </a>&#13;
     the&#13;
     <strong class="bold">&#13;
      Agent Protocol&#13;
     </strong>&#13;
     (&#13;
     <a>&#13;
      https://agentprotocol.ai/&#13;
     </a>&#13;
     ) and several research papers, the LlamaIndex community also created a&#13;
     <a id="_idIndexMarker906">&#13;
     </a>&#13;
     more granular way to control the agents. This provides enhanced control and flexibility for executing user queries. It enables users to manage the agent’s actions with finer detail, facilitating the development of more sophisticated&#13;
     <span class="No-Break">&#13;
      agentic systems.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The entire concept is based on two main components,&#13;
     <code class="literal">&#13;
      AgentRunner&#13;
     </code>&#13;
     and&#13;
     <code class="literal">&#13;
      AgentWorker&#13;
     </code>&#13;
     , and works as described in&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Figure 8&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       .15&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      :&#13;
     </span>&#13;
    </p>&#13;
    <div>&#13;
     <div class="IMG---Figure" id="_idContainer092">&#13;
      <img src="../Images/B21861_08_15.jpg"/>&#13;
     </div>&#13;
    </div>&#13;
    <p class="IMG---Caption" lang="en-US">&#13;
     Figure 8.15 – The AgentRunner and AgentWorker orchestration model&#13;
    </p>&#13;
    <p>&#13;
     We use&#13;
     <strong class="bold">&#13;
      agent runners&#13;
     </strong>&#13;
     to&#13;
     <a id="_idIndexMarker907">&#13;
     </a>&#13;
     orchestrate tasks and store conversational memory.&#13;
     <strong class="bold">&#13;
      Agent workers&#13;
     </strong>&#13;
     control&#13;
     <a id="_idIndexMarker908">&#13;
     </a>&#13;
     the execution of each task step by step without storing the state themselves. The agent runner manages the overall process and integrates&#13;
     <span class="No-Break">&#13;
      the results.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     In terms of&#13;
     <a id="_idIndexMarker909">&#13;
     </a>&#13;
     benefits, there are multiple reasons to use agents like this. Firstly, it allows for a clear separation of concerns: agent runners manage the task’s overall orchestration and memory, while agent workers focus only on executing specific steps of a task. This division enhances the maintainability and scalability of&#13;
     <span class="No-Break">&#13;
      the system.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Moreover, the architecture promotes enhanced visibility and control over the agent’s decision-making process. We can observe and intervene at each step, with very good insight into the agent’s operation. This is particularly useful for debugging and refining our&#13;
     <span class="No-Break">&#13;
      agent’s behavior.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Another key benefit is the flexibility it provides. We can tailor the behavior of agents according to the specific needs of the application. We can modify or extend the functionality of agent workers, or integrate custom logic within the agent runner, making the system highly adaptable. This setup also supports modular development. We can build or update individual components without affecting the entire system, facilitating easier updates&#13;
     <span class="No-Break">&#13;
      and iterations.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Here’s a sample&#13;
     <a id="_idIndexMarker910">&#13;
     </a>&#13;
     implementation that takes one of our previous examples and applies this more granular approach. We’ll implement&#13;
     <code class="literal">&#13;
      OpenAIAgent&#13;
     </code>&#13;
     in a low-level fashion by using&#13;
     <code class="literal">&#13;
      AgentRunner&#13;
     </code>&#13;
     <span class="No-Break">&#13;
      and&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <code class="literal">&#13;
       OpenAIAgentWorker&#13;
      </code>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      :&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Here, we’ve&#13;
     <a id="_idIndexMarker911">&#13;
     </a>&#13;
     imported the necessary components and prepared the tool list for the agent. We’re using the same&#13;
     <code class="literal">&#13;
      employees.db&#13;
     </code>&#13;
     database as before. Next, we’ll define the&#13;
     <span class="No-Break">&#13;
      agent worker:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     It’s time to initialize our agent runner and prepare the input that will contain&#13;
     <span class="No-Break">&#13;
      the task:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     There are two distinct methods to engage with our agent now. Let’s take&#13;
     <span class="No-Break">&#13;
      a look.&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Option A – the end-to-end interaction, using the chat() method&#13;
    </h3>&#13;
    <p>&#13;
     The&#13;
     <code class="literal">&#13;
      chat()&#13;
     </code>&#13;
     method&#13;
     <a id="_idIndexMarker912">&#13;
     </a>&#13;
     offers a seamless, end-to-end interaction, executing the task without requiring intervention at each&#13;
     <span class="No-Break">&#13;
      reasoning step:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     It’s very straightforward: just two lines of code, at which point we wait for the agent to solve the task and provide a final response when all the steps&#13;
     <span class="No-Break">&#13;
      are completed.&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Option B – the step-by-step interaction, using the create_task() method&#13;
    </h3>&#13;
    <p>&#13;
     For more&#13;
     <a id="_idIndexMarker913">&#13;
     </a>&#13;
     granular control, we could leverage the agent runner and use a step-by-step method that allows us to create a task, run each step individually, and then finalize&#13;
     <span class="No-Break">&#13;
      the response:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     In the first part, we created a new task for the agent runner and executed the first step of the task. Because this method provides manual control of the execution of each step, we have to manually implement a loop in our code. We will repeatedly call&#13;
     <code class="literal">&#13;
      run_step()&#13;
     </code>&#13;
     until the output indicates all steps&#13;
     <span class="No-Break">&#13;
      are complete:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The previous loop will run until the last step is completed. Then, it’s time to synthesize and display the&#13;
     <span class="No-Break">&#13;
      final answer:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     This&#13;
     <a id="_idIndexMarker914">&#13;
     </a>&#13;
     allows us to execute and observe each reasoning step individually. The&#13;
     <code class="literal">&#13;
      create_task()&#13;
     </code>&#13;
     method initializes a new task,&#13;
     <code class="literal">&#13;
      run_step()&#13;
     </code>&#13;
     executes each step, returning an output, and&#13;
     <code class="literal">&#13;
      finalize_response()&#13;
     </code>&#13;
     generates the final response once all steps&#13;
     <span class="No-Break">&#13;
      are complete.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Overall, this option is particularly useful when you need to monitor the agent’s decisions closely or when you want to step in at certain points to guide the process or to&#13;
     <span class="No-Break">&#13;
      handle exceptions.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Now, it’s time to apply this fresh knowledge and add some chat features to our&#13;
     <span class="No-Break">&#13;
      PITS project.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor194">&#13;
    </a>&#13;
   </div>&#13;
  </div>&#13;
 </body>&#13;
</html>
<html>&#13;
 <head>&#13;
  <title>&#13;
   Hands-on – implementing conversation tracking for PITS&#13;
  </title>&#13;
 </head>&#13;
 <body>&#13;
  <div class="epub-source">&#13;
   <h1 id="_idParaDest-195">&#13;
    Hands-on – implementing conversation tracking for PITS&#13;
   </h1>&#13;
   <div id="_idContainer094">&#13;
    import os&#13;
import json&#13;
import streamlit as st&#13;
from openai import OpenAI&#13;
from llama_index.core import load_index_from_storage&#13;
from llama_index.core import StorageContext&#13;
from llama_index.core.memory import ChatMemoryBuffer&#13;
from llama_index.core.tools import QueryEngineTool, ToolMetadata&#13;
from llama_index.agent.openai import OpenAIAgent&#13;
from llama_index.core.storage.chat_store import SimpleChatStore&#13;
from global_settings import INDEX_STORAGE, CONVERSATION_FILE&#13;
    def load_chat_store():&#13;
    try:&#13;
        chat_store = SimpleChatStore.from_persist_path(&#13;
            CONVERSATION_FILE&#13;
        )&#13;
    except FileNotFoundError:&#13;
        chat_store = SimpleChatStore()&#13;
    return chat_store&#13;
    def display_messages(chat_store, container):&#13;
    with container:&#13;
        for message in chat_store.get_messages(key="0"):&#13;
            with st.chat_message(message.role):&#13;
                st.markdown(message.content)&#13;
    def initialize_chatbot(user_name, study_subject,&#13;
                       chat_store, container, context):&#13;
    memory = ChatMemoryBuffer.from_defaults(&#13;
        token_limit=3000,&#13;
        chat_store=chat_store,&#13;
        chat_store_key="0"&#13;
    )&#13;
        storage_context = StorageContext.from_defaults(&#13;
        persist_dir=INDEX_STORAGE&#13;
    )&#13;
    index = load_index_from_storage(&#13;
        storage_context, index_id="vector"&#13;
    )&#13;
    study_materials_engine = index.as_query_engine(&#13;
        similarity_top_k=3&#13;
    )&#13;
    study_materials_tool = QueryEngineTool(&#13;
        query_engine=study_materials_engine,&#13;
        metadata=ToolMetadata(&#13;
            name="study_materials",&#13;
            description=(&#13;
                f"Provides official information about "&#13;
                f"{study_subject}. Use a detailed plain "&#13;
                f"text question as input to the tool."&#13;
            ),&#13;
        )&#13;
    )&#13;
        agent = OpenAIAgent.from_tools(&#13;
        tools=[study_materials_tool],&#13;
        memory=memory,&#13;
        system_prompt=(&#13;
            f"Your name is PITS, a personal tutor. Your "&#13;
            f"purpose is to help {user_name} study and "&#13;
            f"better understand the topic of: "&#13;
            f"{study_subject}. We are now discussing the "&#13;
            f"slide with the following content: {context}"&#13;
        )&#13;
    )&#13;
    display_messages(chat_store, container)&#13;
    return agent&#13;
    def chat_interface(agent, chat_store, container):&#13;
    prompt = st.chat_input("Type your question here:")&#13;
    if prompt:&#13;
        with container:&#13;
            with st.chat_message("user"):&#13;
                st.markdown(prompt)&#13;
            response = str(agent.chat(prompt))&#13;
            with st.chat_message("assistant"):&#13;
                st.markdown(response)&#13;
        chat_store.persist(CONVERSATION_FILE)&#13;
    <p>&#13;
     In this&#13;
     <a id="_idIndexMarker915">&#13;
     </a>&#13;
     practical section, we’ll use some of our newfound knowledge to further improve our personal tutoring project. Like any professional tutor, eager to teach students and answer their questions, PITS should have a proper conversational engine at its core. It should be able to understand the topic, be aware of the current context, and keep track of the entire interaction with the student. Because the learning process will probably take place through multiple sessions, PITS must be able to persist the entire conversation and resume the interaction when a new session is initiated. We’ll implement all these features in&#13;
     <code class="literal">&#13;
      coversation_engine.py&#13;
     </code>&#13;
     . This&#13;
     <a id="_idIndexMarker916">&#13;
     </a>&#13;
     module is not meant to be used directly in our app architecture. Instead, it will provide three callable functions that we will later import and use in&#13;
     <a id="_idIndexMarker917">&#13;
     </a>&#13;
     the&#13;
     <span class="No-Break">&#13;
      <code class="literal">&#13;
       training_UI.py&#13;
      </code>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      module:&#13;
     </span>&#13;
    </p>&#13;
    <ul>&#13;
     <li>&#13;
      <code class="literal">&#13;
       load_chat_store&#13;
      </code>&#13;
      : This&#13;
      <a id="_idIndexMarker918">&#13;
      </a>&#13;
      function is responsible for retrieving the chatbot conversation from previous sessions. We’re using a generic&#13;
      <code class="literal">&#13;
       chat_store_key="0"&#13;
      </code>&#13;
      key. In a multi-user scenario, this key could be used to store chat conversations for different users in the same&#13;
      <span class="No-Break">&#13;
       chat store.&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       initialize_chatbot&#13;
      </code>&#13;
      : This&#13;
      <a id="_idIndexMarker919">&#13;
      </a>&#13;
      function is responsible for loading the training material vector index from storage, defining a query engine tool on the index, and then initializing&#13;
      <code class="literal">&#13;
       OpenAIAgent&#13;
      </code>&#13;
      using this tool as an argument. It also provides the agent with a system prompt that contains context information describing the purpose of the agent, the username and study topic, as well as the current slide content. The function returns the initialized agent, which will then be used by&#13;
      <code class="literal">&#13;
       chat_interface&#13;
      </code>&#13;
      to implement the&#13;
      <span class="No-Break">&#13;
       actual conversation.&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       chat_interface&#13;
      </code>&#13;
      : This&#13;
      <a id="_idIndexMarker920">&#13;
      </a>&#13;
      function implements the ongoing conversation by taking&#13;
      <a id="_idIndexMarker921">&#13;
      </a>&#13;
      the user input and generating an answer from the agent. It also persists the conversation after each interaction. If the user ends the current session, on resume, the conversation will be continued from&#13;
      <span class="No-Break">&#13;
       that point.&#13;
      </span>&#13;
     </li>&#13;
    </ul>&#13;
    <p>&#13;
     Once implemented in the main training interface, this chat should look similar to what’s shown in&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Figure 8&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       .16&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      :&#13;
     </span>&#13;
    </p>&#13;
    <div>&#13;
     <div class="IMG---Figure" id="_idContainer093">&#13;
      <img src="../Images/B21861_08_16.jpg"/>&#13;
     </div>&#13;
    </div>&#13;
    <p class="IMG---Caption" lang="en-US">&#13;
     Figure 8.16 – Screenshot from the PITS training UI&#13;
    </p>&#13;
    <p>&#13;
     Let’s have a look at the code. The first part contains all the&#13;
     <span class="No-Break">&#13;
      necessary imports:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     You’ll notice&#13;
     <a id="_idIndexMarker922">&#13;
     </a>&#13;
     that in the first part of the code, we imported a lot of components. The&#13;
     <code class="literal">&#13;
      os&#13;
     </code>&#13;
     and&#13;
     <code class="literal">&#13;
      json&#13;
     </code>&#13;
     modules will be used for the chat persistence feature. The specific LlamaIndex elements will be used to implement the agent with all its&#13;
     <span class="No-Break">&#13;
      required components.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     We also imported the&#13;
     <code class="literal">&#13;
      INDEX_STORAGE&#13;
     </code>&#13;
     and&#13;
     <code class="literal">&#13;
      CONVERSATION_FILE&#13;
     </code>&#13;
     locations from the&#13;
     <code class="literal">&#13;
      global_settings.py&#13;
     </code>&#13;
     module. Because the chat conversation will be implemented using Streamlit, we also have to import the&#13;
     <span class="No-Break">&#13;
      <code class="literal">&#13;
       streamlit&#13;
      </code>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      library.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Next, let’s have a look at the&#13;
     <code class="literal">&#13;
      load_chat_store&#13;
     </code>&#13;
     function, which is responsible for resuming the previous conversation by loading the chat history from the local storage file specified&#13;
     <span class="No-Break">&#13;
      by&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <code class="literal">&#13;
       CONVERSATION_FILE&#13;
      </code>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      :&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     As we can&#13;
     <a id="_idIndexMarker923">&#13;
     </a>&#13;
     see, the&#13;
     <code class="literal">&#13;
      load_chat_store&#13;
     </code>&#13;
     function tries to retrieve the conversation history from the local storage file. If the storage file does not exist, a new empty&#13;
     <code class="literal">&#13;
      chat_store&#13;
     </code>&#13;
     is created. The function&#13;
     <span class="No-Break">&#13;
      returns&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <code class="literal">&#13;
       chat_store&#13;
      </code>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      .&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The next function is responsible for displaying the entire conversation history in the&#13;
     <span class="No-Break">&#13;
      Streamlit interface:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The&#13;
     <code class="literal">&#13;
      display_messages&#13;
     </code>&#13;
     function takes a chat store and a Streamlit container as arguments. It extracts all messages from the chat store using&#13;
     <code class="literal">&#13;
      get_messages()&#13;
     </code>&#13;
     . The function iterates over and displays each message from the chat store, assigning appropriate roles –&#13;
     <em class="italic">&#13;
      user&#13;
     </em>&#13;
     or&#13;
     <em class="italic">&#13;
      assistant&#13;
     </em>&#13;
     –&#13;
     <span class="No-Break">&#13;
      to each.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The messages are displayed in the Streamlit container using Streamlit’s&#13;
     <code class="literal">&#13;
      chat_message()&#13;
     </code>&#13;
     method, which has the advantage of automatically adding a corresponding icon for&#13;
     <span class="No-Break">&#13;
      each role.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The next function is responsible for initializing the agent. This function takes&#13;
     <span class="No-Break">&#13;
      five arguments:&#13;
     </span>&#13;
    </p>&#13;
    <ul>&#13;
     <li>&#13;
      <code class="literal">&#13;
       user_name&#13;
      </code>&#13;
      : The name of the user – to enable a more&#13;
      <span class="No-Break">&#13;
       personal experience.&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       study_subject&#13;
      </code>&#13;
      : The topic covered by the&#13;
      <span class="No-Break">&#13;
       study materials.&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       chat_store&#13;
      </code>&#13;
      : Used to initialize the&#13;
      <span class="No-Break">&#13;
       conversation history.&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       container&#13;
      </code>&#13;
      : This is the Streamlit container where the chat conversation will be displayed. It’s not used by this function itself and instead passed further to the&#13;
      <span class="No-Break">&#13;
       <code class="literal">&#13;
        display_messages&#13;
       </code>&#13;
      </span>&#13;
      <span class="No-Break">&#13;
       function.&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       context&#13;
      </code>&#13;
      : This is the content of the current slide being displayed in the training interface. This context will be fed into the agent’s system prompt to ground any answer on the current context of&#13;
      <span class="No-Break">&#13;
       the user.&#13;
      </span>&#13;
     </li>&#13;
    </ul>&#13;
    <p>&#13;
     Let’s see&#13;
     <a id="_idIndexMarker924">&#13;
     </a>&#13;
     the first part of&#13;
     <span class="No-Break">&#13;
      the function:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Here, we have defined a&#13;
     <code class="literal">&#13;
      ChatMemoryBuffer&#13;
     </code>&#13;
     object for the agent, specifying the&#13;
     <code class="literal">&#13;
      chat_store&#13;
     </code>&#13;
     attribute containing the conversation history. We used the same&#13;
     <code class="literal">&#13;
      chat_store_key&#13;
     </code>&#13;
     as before. This is important to allow the agent to correctly retrieve the&#13;
     <span class="No-Break">&#13;
      chat history.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Next, we’ll prepare the tools for&#13;
     <span class="No-Break">&#13;
      the agent:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Here, we first retrieved our vector index by using a&#13;
     <code class="literal">&#13;
      StorageContext&#13;
     </code>&#13;
     object and the&#13;
     <code class="literal">&#13;
      load_index_from_storage()&#13;
     </code>&#13;
     method. We had to specify the&#13;
     <em class="italic">&#13;
      ID&#13;
     </em>&#13;
     of the index –&#13;
     <em class="italic">&#13;
      vector&#13;
     </em>&#13;
     – because in our case, the storage contains more than&#13;
     <span class="No-Break">&#13;
      one index.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     After loading&#13;
     <a id="_idIndexMarker925">&#13;
     </a>&#13;
     the index, we created a simple query engine configured with&#13;
     <code class="literal">&#13;
      similarity_top_k=3&#13;
     </code>&#13;
     and then created a&#13;
     <code class="literal">&#13;
      QueryEngineTool&#13;
     </code>&#13;
     utility, providing a proper description in its metadata so that the agent can&#13;
     <em class="italic">&#13;
      understand&#13;
     </em>&#13;
     its purpose and usage. The top-k similarity parameter is set to&#13;
     <code class="literal">&#13;
      3&#13;
     </code>&#13;
     to retrieve the three most relevant pieces of information from&#13;
     <span class="No-Break">&#13;
      the index.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The next part will&#13;
     <span class="No-Break">&#13;
      initialize&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <code class="literal">&#13;
       OpenAIAgent&#13;
      </code>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      :&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     In the preceding code, we initialized&#13;
     <code class="literal">&#13;
      OpenAIAgent&#13;
     </code>&#13;
     while providing&#13;
     <code class="literal">&#13;
      QueryEngineTool&#13;
     </code>&#13;
     ,&#13;
     <code class="literal">&#13;
      memory&#13;
     </code>&#13;
     , and&#13;
     <code class="literal">&#13;
      system_prompt&#13;
     </code>&#13;
     as arguments. This prompt is used to provide the LLM with background information to contextualize its responses, ensuring they are relevant to the current discussion topic and the user’s&#13;
     <span class="No-Break">&#13;
      study needs.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     As you can&#13;
     <a id="_idIndexMarker926">&#13;
     </a>&#13;
     see, I’ve tried to keep the code as simple as possible. Many things could be improved in this implementation. After initializing the agent, we call&#13;
     <code class="literal">&#13;
      display_messages&#13;
     </code>&#13;
     to display the&#13;
     <span class="No-Break">&#13;
      existing conversation.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Our last function is responsible for handling the actual conversation. It takes&#13;
     <span class="No-Break">&#13;
      three arguments:&#13;
     </span>&#13;
    </p>&#13;
    <ul>&#13;
     <li>&#13;
      <code class="literal">&#13;
       agent&#13;
      </code>&#13;
      : The agent engine that will be used to run&#13;
      <span class="No-Break">&#13;
       the chat&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       chat_store&#13;
      </code>&#13;
      : The&#13;
      <code class="literal">&#13;
       chat_store&#13;
      </code>&#13;
      argument that will be used to persist&#13;
      <span class="No-Break">&#13;
       the conversation&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      <code class="literal">&#13;
       container&#13;
      </code>&#13;
      : The Streamlit container where the messages will&#13;
      <span class="No-Break">&#13;
       be displayed&#13;
      </span>&#13;
     </li>&#13;
    </ul>&#13;
    <p>&#13;
     Let’s have a look at&#13;
     <span class="No-Break">&#13;
      the code:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     This&#13;
     <code class="literal">&#13;
      chat_interface&#13;
     </code>&#13;
     function&#13;
     <a id="_idIndexMarker927">&#13;
     </a>&#13;
     displays a chat input widget using Streamlit’s&#13;
     <code class="literal">&#13;
      chat_input()&#13;
     </code>&#13;
     method. Upon&#13;
     <a id="_idIndexMarker928">&#13;
     </a>&#13;
     receiving input, it does&#13;
     <span class="No-Break">&#13;
      the following:&#13;
     </span>&#13;
    </p>&#13;
    <ul>&#13;
     <li>&#13;
      Adds the user’s question to the chat interface in the&#13;
      <span class="No-Break">&#13;
       specified container&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      Calls the chat method of&#13;
      <code class="literal">&#13;
       OpenAIAgent&#13;
      </code>&#13;
      to process the question and generate&#13;
      <span class="No-Break">&#13;
       a response&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      Adds the chatbot’s response to the chat interface in the&#13;
      <span class="No-Break">&#13;
       specified container&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      Persists the new conversation to&#13;
      <code class="literal">&#13;
       CONVERSATION_FILE&#13;
      </code>&#13;
      using the chat store’s persist method to ensure continuity&#13;
      <span class="No-Break">&#13;
       across sessions&#13;
      </span>&#13;
     </li>&#13;
    </ul>&#13;
    <p>&#13;
     That’s it for now. We’ll talk about more of the features of PITS in the next&#13;
     <span class="No-Break">&#13;
      few chapters.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor195">&#13;
    </a>&#13;
   </div>&#13;
  </div>&#13;
 </body>&#13;
</html>
<html>&#13;
 <head>&#13;
  <title>&#13;
   Summary&#13;
  </title>&#13;
 </head>&#13;
 <body>&#13;
  <div class="epub-source">&#13;
   <h1 id="_idParaDest-196">&#13;
    Summary&#13;
   </h1>&#13;
   <div id="_idContainer094">&#13;
    <p>&#13;
     This chapter provided an in-depth exploration of building chatbots and agents with LlamaIndex. We covered&#13;
     <code class="literal">&#13;
      ChatEngine&#13;
     </code>&#13;
     for conversation tracking and different built-in chat modes, such as simple, context, condense question, and condense&#13;
     <span class="No-Break">&#13;
      plus context.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Then, we explored different agent architectures and strategies using&#13;
     <code class="literal">&#13;
      OpenAIAgent&#13;
     </code>&#13;
     ,&#13;
     <code class="literal">&#13;
      ReActAgent&#13;
     </code>&#13;
     , and the more advanced LLMCompiler agent. Key concepts such as tools, tool orchestration, reasoning loops, and parallel execution&#13;
     <span class="No-Break">&#13;
      were explained.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     We concluded this chapter with a hands-on implementation of conversation tracking for the PITS&#13;
     <span class="No-Break">&#13;
      tutoring application.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Overall, you should now have a comprehensive understanding of leveraging LlamaIndex capabilities to create useful and engaging&#13;
     <span class="No-Break">&#13;
      conversational interfaces.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Throughout the next chapter, we’ll discover how to customize our RAG pipeline and provide a straightforward guide to deploying it with Streamlit. We’ll also explore advanced tracing methods for seamless debugging and unravel strategies for evaluating&#13;
     <span class="No-Break">&#13;
      our applications.&#13;
     </span>&#13;
    </p>&#13;
   </div>&#13;
  </div>&#13;
 </body>&#13;
</html>
<html>&#13;
 <head>&#13;
  <title>&#13;
   Part 4: Customization, Prompt Engineering, and Final Words&#13;
  </title>&#13;
 </head>&#13;
 <body>&#13;
  <div class="epub-source">&#13;
   <h1 id="_idParaDest-197" lang="en-US">&#13;
    Part 4: Customization, Prompt Engineering, and Final Words&#13;
   </h1>&#13;
   <div class="Content" id="_idContainer095">&#13;
   </div>&#13;
   <div id="_idContainer096">&#13;
    <p>&#13;
     In the final part of this book, we explore customizing RAG components for robust, production-ready applications, covering tracing and evaluation methods as well as deployment with platforms such as Streamlit. We also discover techniques for effective prompt engineering and understand how prompts can enhance a RAG workflow. We conclude with reflections on the transformative potential of RAG and AI, emphasizing continuous learning, community engagement, and ethical considerations, alongside a forward-looking perspective on the role of technology and responsible development in shaping&#13;
     <span class="No-Break">&#13;
      the future.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     This part has the&#13;
     <span class="No-Break">&#13;
      following chapters:&#13;
     </span>&#13;
    </p>&#13;
    <ul>&#13;
     <li>&#13;
      <em class="italic">&#13;
       Chapter 9&#13;
      </em>&#13;
      ,&#13;
      <em class="italic">&#13;
       Customizing and Deploying Our LlamaIndex Project&#13;
      </em>&#13;
     </li>&#13;
     <li>&#13;
      <em class="italic">&#13;
       Chapter 10&#13;
      </em>&#13;
      ,&#13;
      <em class="italic">&#13;
       Prompt Engineering Guidelines and Best Practices&#13;
      </em>&#13;
     </li>&#13;
     <li>&#13;
      <a>&#13;
       <em class="italic">&#13;
        Chapter 11&#13;
       </em>&#13;
      </a>&#13;
      ,&#13;
      <em class="italic">&#13;
       Conclusion and Additional Resources&#13;
      </em>&#13;
     </li>&#13;
    </ul>&#13;
   </div>&#13;
   <div>&#13;
    <div id="_idContainer097">&#13;
    </div>&#13;
   </div>&#13;
   <div>&#13;
    <div id="_idContainer098">&#13;
    </div>&#13;
   </div>&#13;
   <div>&#13;
    <div class="Basic-Graphics-Frame" id="_idContainer099">&#13;
    </div>&#13;
   </div>&#13;
  </div>&#13;
 </body>&#13;
</html></body></html>