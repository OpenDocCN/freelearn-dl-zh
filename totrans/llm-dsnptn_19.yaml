- en: '19'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement Learning from Human Feedback
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll dive into **Reinforcement Learning from Human Feedback**
    (**RLHF**), a powerful technique for aligning LLMs with human preferences. RLHF
    combines reinforcement learning with human feedback to fine-tune language models.
    It aims to align the model’s outputs with human preferences, improving the quality
    and safety of generated text.
  prefs: []
  type: TYPE_NORMAL
- en: RLHF differs from standard supervised fine-tuning by optimizing for human preferences
    rather than predefined correct answers. While supervised learning minimizes loss
    against labeled examples, RLHF creates a reward model from human comparisons between
    model outputs and then uses this reward function (typically with **proximal policy
    optimization** (**PPO**)) to update the model’s policy. The process typically
    employs a divergence penalty to prevent excessive drift from the initial model
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key benefits of RLHF are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Improved alignment of models with human values and preferences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhanced control over model outputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduction of harmful or biased content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ability to optimize for specific task performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you’ll be able to implement RLHF techniques to improve
    the alignment and output quality of your LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Components of RLHF systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling RLHF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limitations of RLHF in language modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications of RLHF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Components of RLHF systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A typical RLHF system for LLMs consists of three main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Base language model**: The pre-trained LLM to be fine-tuned'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reward model**: A model trained on human preferences to provide feedback'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policy optimization**: The process of updating the base model using the reward
    signal'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The base language model serves as the starting point. This is the general-purpose
    large language model that has already undergone extensive pre-training on large-scale
    corpora using self-supervised objectives such as next-token prediction. At this
    stage, the model is capable of generating coherent language and demonstrating
    broad linguistic competence. However, it lacks alignment with human preferences,
    task-specific objectives, or context-dependent behavior expected in real-world
    deployment. This pre-trained model is the substrate upon which subsequent tuning
    is performed. Its architecture, training regime, and scaling have already been
    well-documented in literature, and since RLHF builds upon it without altering
    its fundamental structure, further detailing it is unnecessary here.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, let us focus on the reward model and the policy optimization component,
    which work together to guide and reshape the output distribution of the base model
    based on human-aligned criteria. These two parts introduce the core mechanisms
    of feedback-driven adaptation and reinforcement tuning and will be examined in
    the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Reward model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s implement a basic structure for the reward model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This class sets up the basic structure for an RLHF system, including the base
    language model and the reward model. The `generate_text` method produces text
    from a given prompt, while `get_reward` estimates the reward for a given text
    using the reward model.
  prefs: []
  type: TYPE_NORMAL
- en: The reward model is central to the RLHF process, as it translates human preferences
    into a learnable signal. Trained on datasets consisting of human comparisons between
    model outputs—where evaluators choose the better of two responses—it learns to
    predict how a human might rate any given response. During the reinforcement learning
    phase, this reward model serves as an automated proxy for human judgment, allowing
    the base model to receive immediate feedback on thousands of generated outputs.
    The policy model (the language model being optimized) then learns to maximize
    these predicted reward scores through techniques such as PPO, gradually shifting
    its behavior toward generating responses that better align with human preferences
    while maintaining coherence and capabilities through divergence constraints. This
    creates a powerful feedback loop that enables continuous alignment with human
    values, something that would be impossible with static supervised datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simple implementation of reward model training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This code sets up a dataset of human feedback and trains the reward model using
    the Hugging Face Trainer API. The reward model learns to predict human preferences
    based on the provided labels.
  prefs: []
  type: TYPE_NORMAL
- en: Policy optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Policy optimization is the process of updating the base language model using
    the rewards from the reward model. A common approach is PPO, which strikes a balance
    between ease of implementation, sample efficiency, and reliable performance. The
    term “proximal” in PPO refers to its key innovation: limiting how much the policy
    can change in each training step to prevent harmful large updates. It does this
    by using a “clipped” objective function that discourages updates that would move
    the policy too far from its previous version. PPO has become especially popular
    in AI alignment and RLHF because it’s more stable than other policy gradient methods
    – it helps with avoiding the problem where model updates become too aggressive
    and destroy previously learned good behaviors. When used in language models, PPO
    helps gradually shift the model’s outputs to better match human preferences while
    maintaining coherent and fluent text generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simplified implementation of PPO for LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This function performs a single step of PPO, generating text, computing rewards,
    and updating the base model’s parameters to maximize the expected reward. Keep
    in mind that this PPO code is illustrative; actual implementations may require
    more around rewards and safety checks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Direct preference optimization** (**DPO**) is another approach in RLHF that
    focuses on aligning models with human preferences by directly optimizing for preferred
    outcomes. Unlike traditional RL methods, which often rely on reward models to
    guide learning, DPO simplifies the process by using pairs of preferred and dispreferred
    outputs to adjust the model’s behavior. This method enhances efficiency and effectiveness
    in training models so that they generate outputs that align more closely with
    human expectations.'
  prefs: []
  type: TYPE_NORMAL
- en: DPO might be preferred over PPO when computational efficiency and implementation
    simplicity are priorities. This is because DPO eliminates the need for separate
    reward model training and complex reinforcement learning optimization loops. It
    offers a more streamlined approach by directly updating policy parameters from
    preference data, which can be particularly valuable in scenarios with limited
    resources or when PPO training exhibits instability or reward hacking. DPO can
    also make better use of limited human preference datasets without the intermediate
    step of reward modeling. Additionally, it provides a cleaner experimental setup
    for studying how preferences directly impact model behavior without the confounding
    factors introduced by separate reward models and reinforcement learning optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a short code example demonstrating how to implement DPO using Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This code snippet demonstrates how to set up and train a language model using
    DPO, allowing it to better align with human feedback by directly optimizing for
    preferred completions.
  prefs: []
  type: TYPE_NORMAL
- en: Having discussed PPO and DPO, next, we’ll examine scaling strategies for RLHF
    regarding large-scale models.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling RLHF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scaling RLHF to large models presents challenges due to computational requirements.
    Here are some strategies that can be implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed training**: This involves partitioning the training workload
    across multiple devices – typically GPUs or TPUs – by employing data parallelism,
    model parallelism, or pipeline parallelism. In data parallelism, the same model
    is replicated across devices, and each replica processes a different mini-batch
    of data. Gradients are averaged and synchronized after each step. On the other
    hand, model parallelism splits the model itself across multiple devices, enabling
    the training of architectures that are too large to fit on a single device. Finally,
    pipeline parallelism further divides the model into sequential stages across devices,
    which are then trained in a pipelined fashion to improve throughput. Frameworks
    such as DeepSpeed and Megatron-LM provide infrastructure for managing these complex
    parallelization schemes and optimizing communication overheads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.utils.checkpoint` or TensorFlow’s recomputation wrappers make it possible
    to apply this technique without having to rewrite model architectures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mixed-precision training**: This utilizes 16-bit floating-point (FP16 or
    BF16) formats instead of the standard 32-bit (FP32) for most computations. This
    reduces memory footprint and increases throughput due to faster arithmetic and
    lower memory bandwidth usage. To maintain model accuracy and numerical stability,
    a master copy of weights is maintained in FP32, and dynamic loss scaling is often
    used to prevent underflow in gradients. Libraries such as NVIDIA’s Apex or native
    support in PyTorch and TensorFlow enable automatic mixed-precision training. This
    method is especially effective on modern hardware such as NVIDIA’s Tensor Cores
    or Google’s TPUs, which are optimized for low-precision computation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 19**.1* summarizes these strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.1 – Strategies for scaling RLHF](img/B31249_19_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19.1 – Strategies for scaling RLHF
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of how to implement gradient checkpointing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This function enables gradient checkpointing for the model, which can significantly
    reduce memory usage during training, allowing for larger batch sizes or model
    sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of RLHF in language modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While RLHF is powerful, it faces several challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reward hacking**: Models may exploit loopholes in the reward function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limited feedback**: Human feedback may not cover all possible scenarios'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Suboptimal local optima**: The optimization process may get stuck in suboptimal
    solutions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scaling issues**: Obtaining high-quality human feedback at scale is challenging'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To address reward hacking, consider implementing a constrained optimization
    approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This function adds a constraint check before updating the model, helping to
    prevent reward hacking by ensuring the generated text meets certain criteria.
  prefs: []
  type: TYPE_NORMAL
- en: This method modifies the standard training flow by evaluating a generated output
    not just for reward alignment but also for compliance with an external constraint
    model. The process begins with a response being generated from the base model
    using a given prompt. The resulting text is passed through both a reward model
    and a constraint model. The reward model assigns a scalar reward value based on
    its alignment with desired behaviors or objectives. In parallel, the constraint
    model evaluates whether the output satisfies specified limitations, such as avoiding
    harmful content, staying within factual bounds, or respecting legal or ethical
    filters.
  prefs: []
  type: TYPE_NORMAL
- en: The constraint model returns a scalar value that quantifies the degree of constraint
    violation. This value is compared against a predefined threshold. If the value
    exceeds the threshold, indicating that the output violates the constraint, the
    training step is aborted for this sample. No gradient is calculated, and the model
    parameters remain unchanged. This selective update mechanism ensures that only
    outputs that both align with human preferences and satisfy safety or policy constraints
    contribute to learning. This design decouples the constraint signal from the reward
    function, maintaining clear boundaries between learning objectives and constraint
    enforcement. As a result, it preserves the integrity of both components and makes
    the system more interpretable and modular.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of RLHF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'RLHF can be applied to various LLM tasks, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Open-ended text generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dialogue systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Content moderation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s an example of applying RLHF to a summarization task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This function applies RLHF to the task of text summarization, iteratively improving
    the summary based on rewards from the reward model.
  prefs: []
  type: TYPE_NORMAL
- en: The key steps involve generating a summary using a base model, receiving feedback
    from a reward model, and updating the base model iteratively to improve the summarization
    over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a breakdown of how summarization works in this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Summarize the following text:\n{text}\n\nSummary:`. This prompt is sent to
    the base model so that a summary can be generated.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`base_model.generate` function is used to generate a summary from the prompt.
    The generated summary is limited to a maximum length of 100 tokens (`max_length=100`).
    The summary is based on the input text and is the first attempt at summarization.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Reward model feedback**: After the base model generates a summary, the reward
    model evaluates the quality of the summary. The reward model is a separate model
    that measures how well the generated summary aligns with desired qualities (such
    as being accurate, concise, or coherent). The reward function assigns a score
    to the summary, which reflects its quality based on the model’s internal criteria.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`num_iterations` times (in this case, five times by default). Each iteration
    involves generating a new summary, receiving feedback from the reward model, and
    potentially updating the base model to improve the summary in future iterations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`# Update base_model using PPO or another RL algorithm`, indicates that after
    each iteration, the base model should be updated using a reinforcement learning
    algorithm, such as PPO. This update will adjust the parameters of the base model
    to generate better summaries based on feedback from the reward model. However,
    the actual code for model updating isn’t provided here and would typically involve
    reinforcement learning techniques to fine-tune the base model based on the rewards
    it receives.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Final output**: After completing the specified number of iterations, the
    function returns the final summary generated by the base model. This summary is
    expected to be the result of multiple improvements made based on the feedback
    that’s received from the reward model during the iterative process.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'RLHF is a powerful technique used by many frontier model providers, such as
    OpenAI and Anthropic, in fine-tuning pre-trained models. This chapter discussed
    some basic ideas behind this pattern. RLHF still has its limitations since humans
    are involved in the process of training a reward model, and as such, it doesn’t
    scale well. Recently, some more generic reinforcement learning without human feedback
    has been tested by companies such as DeepSeek. However, this is beyond the scope
    of this book. You can refer to the following research paper by DeepSeek for more
    information: [https://arxiv.org/pdf/2501.12948](https://arxiv.org/pdf/2501.12948).'
  prefs: []
  type: TYPE_NORMAL
- en: As we move forward, we’ll explore advanced prompt engineering techniques for
    LLMs. In the next chapter, we’ll delve into sophisticated methods for guiding
    LLM behavior and outputs through carefully crafted prompts, building on the alignment
    techniques we’ve discussed here. These advanced prompting strategies will enable
    you to leverage the full potential of your LLMs while maintaining fine-grained
    control over their outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 4: Advanced Prompt Engineering Techniques'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we explore advanced techniques that enhance the capabilities of
    LLMs through innovative prompting strategies and reasoning methods. You will learn
    how to use chain-of-thought and tree-of-thoughts prompting to guide models through
    complex reasoning processes. We also cover techniques for reasoning without direct
    observation, enabling LLMs to tackle hypothetical scenarios and abstract problems.
    Reflection techniques will show you how to prompt LLMs for iterative self-improvement,
    while methods for automatic multi-step reasoning and tool use will teach you how
    to extend LLMs into sophisticated, multi-functional systems. By mastering these
    advanced approaches, you will gain the ability to unlock the full potential of
    LLMs, allowing them to address even the most challenging problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 20*](B31249_20.xhtml#_idTextAnchor305), *Chain-of-Thought Prompting*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 21*](B31249_21.xhtml#_idTextAnchor315), *Tree-of-Thoughts Prompting*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 22*](B31249_22.xhtml#_idTextAnchor325), *Reasoning and Acting*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 23*](B31249_23.xhtml#_idTextAnchor339), *Reasoning* *WithOut* *Observation*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 24*](B31249_24.xhtml#_idTextAnchor346), *Reflection Techniques*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 25*](B31249_25.xhtml#_idTextAnchor355), *Automatic Multi-Step Reasoning
    and Tool Use*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
