- en: '19'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '19'
- en: Reinforcement Learning from Human Feedback
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从人类反馈中进行强化学习
- en: In this chapter, we’ll dive into **Reinforcement Learning from Human Feedback**
    (**RLHF**), a powerful technique for aligning LLMs with human preferences. RLHF
    combines reinforcement learning with human feedback to fine-tune language models.
    It aims to align the model’s outputs with human preferences, improving the quality
    and safety of generated text.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨**从人类反馈中进行强化学习**（**RLHF**），这是一种将LLM与人类偏好对齐的强大技术。RLHF结合了强化学习和人类反馈来微调语言模型。它的目标是使模型的输出与人类偏好对齐，提高生成文本的质量和安全。
- en: RLHF differs from standard supervised fine-tuning by optimizing for human preferences
    rather than predefined correct answers. While supervised learning minimizes loss
    against labeled examples, RLHF creates a reward model from human comparisons between
    model outputs and then uses this reward function (typically with **proximal policy
    optimization** (**PPO**)) to update the model’s policy. The process typically
    employs a divergence penalty to prevent excessive drift from the initial model
    distribution.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF与标准监督微调不同，它优化的是人类偏好而不是预定义的正确答案。虽然监督学习最小化对标记示例的损失，但RLHF从模型输出之间的人类比较中创建一个奖励模型，然后使用这个奖励函数（通常使用**近端策略优化**（**PPO**））来更新模型的政策。这个过程通常采用一个发散惩罚来防止过度偏离初始模型分布。
- en: 'The key benefits of RLHF are as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF的关键好处如下：
- en: Improved alignment of models with human values and preferences
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型与人类价值观和偏好的改进对齐
- en: Enhanced control over model outputs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对模型输出的增强控制
- en: Reduction of harmful or biased content
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少有害或偏见的内容
- en: Ability to optimize for specific task performance
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化特定任务性能的能力
- en: By the end of this chapter, you’ll be able to implement RLHF techniques to improve
    the alignment and output quality of your LLMs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够实现RLHF技术来提高你LLM的对齐和输出质量。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Components of RLHF systems
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RLHF系统的组成部分
- en: Scaling RLHF
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展RLHF
- en: Limitations of RLHF in language modeling
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RLHF在语言建模中的局限性
- en: Applications of RLHF
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RLHF的应用
- en: Components of RLHF systems
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RLHF系统的组成部分
- en: 'A typical RLHF system for LLMs consists of three main components:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的典型RLHF系统由三个主要组件组成：
- en: '**Base language model**: The pre-trained LLM to be fine-tuned'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础语言模型**：待微调的预训练LLM'
- en: '**Reward model**: A model trained on human preferences to provide feedback'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励模型**：一个基于人类偏好进行训练以提供反馈的模型'
- en: '**Policy optimization**: The process of updating the base model using the reward
    signal'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略优化**：使用奖励信号更新基础模型的过程'
- en: The base language model serves as the starting point. This is the general-purpose
    large language model that has already undergone extensive pre-training on large-scale
    corpora using self-supervised objectives such as next-token prediction. At this
    stage, the model is capable of generating coherent language and demonstrating
    broad linguistic competence. However, it lacks alignment with human preferences,
    task-specific objectives, or context-dependent behavior expected in real-world
    deployment. This pre-trained model is the substrate upon which subsequent tuning
    is performed. Its architecture, training regime, and scaling have already been
    well-documented in literature, and since RLHF builds upon it without altering
    its fundamental structure, further detailing it is unnecessary here.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 基础语言模型是起点。这是一个已经在大规模语料库上使用如下一个标记预测等自监督目标进行广泛预训练的通用大型语言模型。在这个阶段，模型能够生成连贯的语言并展示广泛的语言能力。然而，它缺乏与人类偏好、特定任务目标或实际部署中期望的上下文相关行为的对齐。这个预训练模型是后续微调的基础。其架构、训练方式和扩展已经在文献中得到了很好的记录，并且由于RLHF在它没有改变其基本结构的基础上构建，因此在这里进一步详细说明是不必要的。
- en: Instead, let us focus on the reward model and the policy optimization component,
    which work together to guide and reshape the output distribution of the base model
    based on human-aligned criteria. These two parts introduce the core mechanisms
    of feedback-driven adaptation and reinforcement tuning and will be examined in
    the following sections.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，让我们关注奖励模型和政策优化组件，它们共同工作，根据人类对齐的标准来引导和重塑基础模型的输出分布。这两部分引入了反馈驱动的适应和强化调整的核心机制，将在以下章节中进行探讨。
- en: Reward model
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奖励模型
- en: 'Let’s implement a basic structure for the reward model:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为奖励模型实现一个基本结构：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This class sets up the basic structure for an RLHF system, including the base
    language model and the reward model. The `generate_text` method produces text
    from a given prompt, while `get_reward` estimates the reward for a given text
    using the reward model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此类设置RLHF系统的基本结构，包括基本语言模型和奖励模型。`generate_text`方法从给定的提示生成文本，而`get_reward`方法使用奖励模型估计给定文本的奖励。
- en: The reward model is central to the RLHF process, as it translates human preferences
    into a learnable signal. Trained on datasets consisting of human comparisons between
    model outputs—where evaluators choose the better of two responses—it learns to
    predict how a human might rate any given response. During the reinforcement learning
    phase, this reward model serves as an automated proxy for human judgment, allowing
    the base model to receive immediate feedback on thousands of generated outputs.
    The policy model (the language model being optimized) then learns to maximize
    these predicted reward scores through techniques such as PPO, gradually shifting
    its behavior toward generating responses that better align with human preferences
    while maintaining coherence and capabilities through divergence constraints. This
    creates a powerful feedback loop that enables continuous alignment with human
    values, something that would be impossible with static supervised datasets.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励模型是RLHF过程的核心，因为它将人类偏好转化为可学习的信号。在由模型输出之间的人类比较组成的数据集上训练——评估者选择两个响应中较好的一项——它学会预测人类可能会如何评估任何给定的响应。在强化学习阶段，此奖励模型作为人类判断的自动化代理，使基本模型能够立即获得数千个生成输出的反馈。策略模型（正在优化的语言模型）随后通过PPO等技术学习最大化这些预测奖励分数，逐步将其行为调整为生成与人类偏好更好地对齐的响应，同时通过发散约束保持连贯性和能力。这创建了一个强大的反馈循环，使持续与人类价值观保持一致成为可能，这在静态监督数据集的情况下是不可能的。
- en: 'Here’s a simple implementation of reward model training:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个奖励模型训练的简单实现：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This code sets up a dataset of human feedback and trains the reward model using
    the Hugging Face Trainer API. The reward model learns to predict human preferences
    based on the provided labels.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码设置了一个包含人类反馈的数据集，并使用Hugging Face Trainer API训练奖励模型。奖励模型学会根据提供的标签预测人类偏好。
- en: Policy optimization
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略优化
- en: 'Policy optimization is the process of updating the base language model using
    the rewards from the reward model. A common approach is PPO, which strikes a balance
    between ease of implementation, sample efficiency, and reliable performance. The
    term “proximal” in PPO refers to its key innovation: limiting how much the policy
    can change in each training step to prevent harmful large updates. It does this
    by using a “clipped” objective function that discourages updates that would move
    the policy too far from its previous version. PPO has become especially popular
    in AI alignment and RLHF because it’s more stable than other policy gradient methods
    – it helps with avoiding the problem where model updates become too aggressive
    and destroy previously learned good behaviors. When used in language models, PPO
    helps gradually shift the model’s outputs to better match human preferences while
    maintaining coherent and fluent text generation.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 策略优化是使用奖励模型中的奖励来更新基本语言模型的过程。一种常见的方法是PPO，它在实现简便性、样本效率和可靠性能之间取得平衡。PPO中的“近端”一词指的是其关键创新：限制策略在每个训练步骤中可以改变的程度，以防止有害的大更新。它是通过使用“剪裁”目标函数来实现的，该函数会阻止将策略移动得太远，从而远离其先前版本。PPO因其比其他策略梯度方法更稳定而特别受到AI对齐和RLHF的欢迎——它有助于避免模型更新变得过于激进并破坏先前学习到的良好行为。当用于语言模型时，PPO有助于逐步调整模型的输出，使其更好地匹配人类偏好，同时保持连贯流畅的文本生成。
- en: 'Here’s a simplified implementation of PPO for LLMs:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是LLMs的PPO简化实现：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This function performs a single step of PPO, generating text, computing rewards,
    and updating the base model’s parameters to maximize the expected reward. Keep
    in mind that this PPO code is illustrative; actual implementations may require
    more around rewards and safety checks.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数执行PPO的单步操作，生成文本，计算奖励，并更新基本模型的参数以最大化预期奖励。请注意，此PPO代码仅用于说明；实际实现可能需要更多的奖励和安全性检查。
- en: '**Direct preference optimization** (**DPO**) is another approach in RLHF that
    focuses on aligning models with human preferences by directly optimizing for preferred
    outcomes. Unlike traditional RL methods, which often rely on reward models to
    guide learning, DPO simplifies the process by using pairs of preferred and dispreferred
    outputs to adjust the model’s behavior. This method enhances efficiency and effectiveness
    in training models so that they generate outputs that align more closely with
    human expectations.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**直接偏好优化**（**DPO**）是RLHF（强化学习与人类反馈）中的另一种方法，它通过直接优化首选结果来关注使模型与人类偏好对齐。与传统的RL方法不同，后者通常依赖于奖励模型来指导学习，DPO通过使用首选和不受欢迎的输出对来调整模型的行为，从而简化了过程。这种方法提高了训练模型的效率和效果，使它们生成的输出更接近人类的期望。'
- en: DPO might be preferred over PPO when computational efficiency and implementation
    simplicity are priorities. This is because DPO eliminates the need for separate
    reward model training and complex reinforcement learning optimization loops. It
    offers a more streamlined approach by directly updating policy parameters from
    preference data, which can be particularly valuable in scenarios with limited
    resources or when PPO training exhibits instability or reward hacking. DPO can
    also make better use of limited human preference datasets without the intermediate
    step of reward modeling. Additionally, it provides a cleaner experimental setup
    for studying how preferences directly impact model behavior without the confounding
    factors introduced by separate reward models and reinforcement learning optimization.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当计算效率和实现简单性是重点时，DPO可能比PPO更受欢迎。这是因为DPO消除了单独训练奖励模型和复杂的强化学习优化循环的需求。它通过直接从偏好数据更新策略参数提供了一种更简化的方法，这在资源有限或PPO训练表现出不稳定或奖励黑客行为的情况下尤其有价值。DPO还可以在没有奖励建模的中间步骤的情况下更好地利用有限的人类偏好数据集。此外，它提供了一个更清晰的实验设置，用于研究偏好如何直接影响模型行为，而没有引入由单独的奖励模型和强化学习优化引入的混杂因素。
- en: 'Here’s a short code example demonstrating how to implement DPO using Python:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个简短的代码示例，展示了如何使用Python实现DPO：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This code snippet demonstrates how to set up and train a language model using
    DPO, allowing it to better align with human feedback by directly optimizing for
    preferred completions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码片段展示了如何使用DPO（分布式训练优化）来设置和训练一个语言模型，使其能够通过直接优化首选完成情况来更好地与人类反馈对齐。
- en: Having discussed PPO and DPO, next, we’ll examine scaling strategies for RLHF
    regarding large-scale models.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了PPO和DPO之后，接下来我们将探讨关于大规模模型的RLHF的扩展策略。
- en: Scaling RLHF
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展RLHF
- en: 'Scaling RLHF to large models presents challenges due to computational requirements.
    Here are some strategies that can be implemented:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 将RLHF扩展到大型模型面临着计算需求带来的挑战。以下是一些可以实施的战略：
- en: '**Distributed training**: This involves partitioning the training workload
    across multiple devices – typically GPUs or TPUs – by employing data parallelism,
    model parallelism, or pipeline parallelism. In data parallelism, the same model
    is replicated across devices, and each replica processes a different mini-batch
    of data. Gradients are averaged and synchronized after each step. On the other
    hand, model parallelism splits the model itself across multiple devices, enabling
    the training of architectures that are too large to fit on a single device. Finally,
    pipeline parallelism further divides the model into sequential stages across devices,
    which are then trained in a pipelined fashion to improve throughput. Frameworks
    such as DeepSpeed and Megatron-LM provide infrastructure for managing these complex
    parallelization schemes and optimizing communication overheads.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式训练**：这涉及到通过采用数据并行性、模型并行性或流水线并行性，将训练工作负载分配到多个设备上——通常是GPU或TPU。在数据并行性中，相同的模型在设备上被复制，每个副本处理不同的数据小批量。在每个步骤之后，梯度被平均并同步。另一方面，模型并行性将模型本身分割到多个设备上，使得可以训练那些无法适应单个设备的架构。最后，流水线并行性进一步将模型分割成设备上的顺序阶段，然后以流水线方式训练以提高吞吐量。DeepSpeed和Megatron-LM等框架提供了管理这些复杂并行化方案和优化通信开销的基础设施。'
- en: '`torch.utils.checkpoint` or TensorFlow’s recomputation wrappers make it possible
    to apply this technique without having to rewrite model architectures.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.utils.checkpoint`或TensorFlow的重新计算包装器使得可以在不重写模型架构的情况下应用这项技术。'
- en: '**Mixed-precision training**: This utilizes 16-bit floating-point (FP16 or
    BF16) formats instead of the standard 32-bit (FP32) for most computations. This
    reduces memory footprint and increases throughput due to faster arithmetic and
    lower memory bandwidth usage. To maintain model accuracy and numerical stability,
    a master copy of weights is maintained in FP32, and dynamic loss scaling is often
    used to prevent underflow in gradients. Libraries such as NVIDIA’s Apex or native
    support in PyTorch and TensorFlow enable automatic mixed-precision training. This
    method is especially effective on modern hardware such as NVIDIA’s Tensor Cores
    or Google’s TPUs, which are optimized for low-precision computation.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合精度训练**：这种方法使用16位浮点数（FP16或BF16）格式而不是标准的32位（FP32）格式进行大多数计算。这减少了内存占用并提高了吞吐量，因为算术运算更快，内存带宽使用更低。为了保持模型精度和数值稳定性，权重的主副本保持在FP32格式，并且通常使用动态损失缩放来防止梯度下溢。NVIDIA的Apex库或PyTorch和TensorFlow的本地支持使得自动混合精度训练成为可能。这种方法在NVIDIA的Tensor
    Cores或Google的TPUs等现代硬件上特别有效，这些硬件针对低精度计算进行了优化。'
- en: '*Figure 19**.1* summarizes these strategies:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*图19*.1总结了这些策略：'
- en: '![Figure 19.1 – Strategies for scaling RLHF](img/B31249_19_01.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图19.1 – 扩展RLHF的策略](img/B31249_19_01.jpg)'
- en: Figure 19.1 – Strategies for scaling RLHF
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.1 – 扩展RLHF的策略
- en: 'Here’s an example of how to implement gradient checkpointing:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何实现梯度检查点的示例：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This function enables gradient checkpointing for the model, which can significantly
    reduce memory usage during training, allowing for larger batch sizes or model
    sizes.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此功能为模型启用梯度检查点，这可以在训练期间显著减少内存使用，从而允许使用更大的批量大小或模型大小。
- en: Limitations of RLHF in language modeling
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RLHF在语言模型中的局限性
- en: 'While RLHF is powerful, it faces several challenges:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然RLHF功能强大，但它面临几个挑战：
- en: '**Reward hacking**: Models may exploit loopholes in the reward function'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励黑客攻击**：模型可能会利用奖励函数中的漏洞'
- en: '**Limited feedback**: Human feedback may not cover all possible scenarios'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限的反馈**：人类反馈可能无法涵盖所有可能的场景'
- en: '**Suboptimal local optima**: The optimization process may get stuck in suboptimal
    solutions'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**次优局部最优解**：优化过程可能陷入次优解'
- en: '**Scaling issues**: Obtaining high-quality human feedback at scale is challenging'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩展问题**：以规模获取高质量的人类反馈具有挑战性'
- en: 'To address reward hacking, consider implementing a constrained optimization
    approach:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决奖励黑客攻击问题，考虑实施约束优化方法：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This function adds a constraint check before updating the model, helping to
    prevent reward hacking by ensuring the generated text meets certain criteria.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此功能在更新模型之前添加一个约束检查，通过确保生成的文本满足某些标准来帮助防止奖励黑客攻击。
- en: This method modifies the standard training flow by evaluating a generated output
    not just for reward alignment but also for compliance with an external constraint
    model. The process begins with a response being generated from the base model
    using a given prompt. The resulting text is passed through both a reward model
    and a constraint model. The reward model assigns a scalar reward value based on
    its alignment with desired behaviors or objectives. In parallel, the constraint
    model evaluates whether the output satisfies specified limitations, such as avoiding
    harmful content, staying within factual bounds, or respecting legal or ethical
    filters.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法通过评估生成的输出不仅与奖励对齐，还与外部约束模型的一致性来修改标准的训练流程。该过程从使用给定提示从基础模型生成响应开始。生成的文本通过奖励模型和约束模型。奖励模型根据其与期望行为或目标的对齐情况分配标量奖励值。同时，约束模型评估输出是否满足指定的限制，例如避免有害内容、保持事实界限或遵守法律或伦理过滤器。
- en: The constraint model returns a scalar value that quantifies the degree of constraint
    violation. This value is compared against a predefined threshold. If the value
    exceeds the threshold, indicating that the output violates the constraint, the
    training step is aborted for this sample. No gradient is calculated, and the model
    parameters remain unchanged. This selective update mechanism ensures that only
    outputs that both align with human preferences and satisfy safety or policy constraints
    contribute to learning. This design decouples the constraint signal from the reward
    function, maintaining clear boundaries between learning objectives and constraint
    enforcement. As a result, it preserves the integrity of both components and makes
    the system more interpretable and modular.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 约束模型返回一个标量值，该值量化了违反约束的程度。此值与预定义的阈值进行比较。如果值超过阈值，表明输出违反了约束，则对该样本的训练步骤被终止。不计算梯度，模型参数保持不变。这种选择性更新机制确保只有既符合人类偏好又满足安全或策略约束的输出才对学习做出贡献。这种设计将约束信号与奖励函数解耦，保持学习目标和约束执行之间的清晰界限。因此，它保留了两个组件的完整性，并使系统更具可解释性和模块化。
- en: Applications of RLHF
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RLHF的应用
- en: 'RLHF can be applied to various LLM tasks, including the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF可以应用于各种LLM任务，包括以下内容：
- en: Open-ended text generation
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开放式文本生成
- en: Dialogue systems
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对话系统
- en: Content moderation
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内容审核
- en: Summarization
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摘要
- en: Code generation
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码生成
- en: 'Here’s an example of applying RLHF to a summarization task:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是应用RLHF到摘要任务的一个示例：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This function applies RLHF to the task of text summarization, iteratively improving
    the summary based on rewards from the reward model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将RLHF应用于文本摘要任务，通过根据奖励模型提供的奖励，迭代地改进摘要。
- en: The key steps involve generating a summary using a base model, receiving feedback
    from a reward model, and updating the base model iteratively to improve the summarization
    over time.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 关键步骤包括使用基础模型生成摘要，从奖励模型接收反馈，并迭代地更新基础模型以随着时间的推移改进摘要。
- en: 'Here’s a breakdown of how summarization works in this code:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何在这段代码中实现摘要的分解：
- en: '`Summarize the following text:\n{text}\n\nSummary:`. This prompt is sent to
    the base model so that a summary can be generated.'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`摘要以下文本:\n{text}\n\n摘要:`。此提示发送到基础模型，以便生成摘要。'
- en: '`base_model.generate` function is used to generate a summary from the prompt.
    The generated summary is limited to a maximum length of 100 tokens (`max_length=100`).
    The summary is based on the input text and is the first attempt at summarization.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`base_model.generate`函数从提示生成摘要。生成的摘要长度限制为100个标记（`max_length=100`）。摘要基于输入文本，是第一次尝试摘要。
- en: '**Reward model feedback**: After the base model generates a summary, the reward
    model evaluates the quality of the summary. The reward model is a separate model
    that measures how well the generated summary aligns with desired qualities (such
    as being accurate, concise, or coherent). The reward function assigns a score
    to the summary, which reflects its quality based on the model’s internal criteria.'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**奖励模型反馈**：在基础模型生成摘要后，奖励模型评估摘要的质量。奖励模型是一个独立的模型，它衡量生成的摘要与期望质量（如准确性、简洁性或连贯性）的匹配程度。奖励函数为摘要分配一个分数，该分数反映了其质量，基于模型的内部标准。'
- en: '`num_iterations` times (in this case, five times by default). Each iteration
    involves generating a new summary, receiving feedback from the reward model, and
    potentially updating the base model to improve the summary in future iterations.'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`num_iterations`次（在这种情况下，默认为五次）。每次迭代包括生成新的摘要，从奖励模型接收反馈，并可能更新基础模型以在未来的迭代中改进摘要。'
- en: '`# Update base_model using PPO or another RL algorithm`, indicates that after
    each iteration, the base model should be updated using a reinforcement learning
    algorithm, such as PPO. This update will adjust the parameters of the base model
    to generate better summaries based on feedback from the reward model. However,
    the actual code for model updating isn’t provided here and would typically involve
    reinforcement learning techniques to fine-tune the base model based on the rewards
    it receives.'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`# 使用PPO或其他RL算法更新base_model`，表示在每次迭代后，应使用强化学习算法（如PPO）更新基础模型。此更新将调整基础模型的参数，以便根据奖励模型提供的反馈生成更好的摘要。然而，此处未提供模型更新的实际代码，通常涉及强化学习技术，根据接收到的奖励对基础模型进行微调。'
- en: '**Final output**: After completing the specified number of iterations, the
    function returns the final summary generated by the base model. This summary is
    expected to be the result of multiple improvements made based on the feedback
    that’s received from the reward model during the iterative process.'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**最终输出**：在完成指定次数的迭代后，函数返回由基础模型生成的最终摘要。这个摘要预计是基于在迭代过程中从奖励模型收到的反馈进行多次改进的结果。'
- en: Summary
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'RLHF is a powerful technique used by many frontier model providers, such as
    OpenAI and Anthropic, in fine-tuning pre-trained models. This chapter discussed
    some basic ideas behind this pattern. RLHF still has its limitations since humans
    are involved in the process of training a reward model, and as such, it doesn’t
    scale well. Recently, some more generic reinforcement learning without human feedback
    has been tested by companies such as DeepSeek. However, this is beyond the scope
    of this book. You can refer to the following research paper by DeepSeek for more
    information: [https://arxiv.org/pdf/2501.12948](https://arxiv.org/pdf/2501.12948).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF是一种被许多前沿模型提供商（如OpenAI和Anthropic）用于微调预训练模型的有力技术。本章讨论了这种模式背后的基本思想。由于人类参与了训练奖励模型的过程，因此RLHF仍然存在局限性，并且扩展性不佳。最近，一些公司如DeepSeek测试了无需人类反馈的更通用的强化学习。然而，这超出了本书的范围。您可以参考以下DeepSeek的研究论文以获取更多信息：[https://arxiv.org/pdf/2501.12948](https://arxiv.org/pdf/2501.12948)。
- en: As we move forward, we’ll explore advanced prompt engineering techniques for
    LLMs. In the next chapter, we’ll delve into sophisticated methods for guiding
    LLM behavior and outputs through carefully crafted prompts, building on the alignment
    techniques we’ve discussed here. These advanced prompting strategies will enable
    you to leverage the full potential of your LLMs while maintaining fine-grained
    control over their outputs.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们继续前进，我们将探讨LLMs的高级提示工程技术。在下一章中，我们将深入探讨通过精心设计的提示来引导LLM行为和输出的复杂方法，这些方法基于我们在这里讨论的对齐技术。这些高级提示策略将使您能够充分利用LLMs的潜力，同时保持对其输出的精细控制。
- en: 'Part 4: Advanced Prompt Engineering Techniques'
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4部分：高级提示工程技术
- en: In this part, we explore advanced techniques that enhance the capabilities of
    LLMs through innovative prompting strategies and reasoning methods. You will learn
    how to use chain-of-thought and tree-of-thoughts prompting to guide models through
    complex reasoning processes. We also cover techniques for reasoning without direct
    observation, enabling LLMs to tackle hypothetical scenarios and abstract problems.
    Reflection techniques will show you how to prompt LLMs for iterative self-improvement,
    while methods for automatic multi-step reasoning and tool use will teach you how
    to extend LLMs into sophisticated, multi-functional systems. By mastering these
    advanced approaches, you will gain the ability to unlock the full potential of
    LLMs, allowing them to address even the most challenging problems.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本部分中，我们通过创新的提示策略和推理方法探索增强LLMs能力的高级技术。您将学习如何使用思维链和思维树提示来引导模型通过复杂的推理过程。我们还涵盖了无需直接观察的推理技术，使LLMs能够处理假设情景和抽象问题。反思技术将向您展示如何提示LLMs进行迭代自我改进，而自动多步推理和工具使用的方法将教会您如何将LLMs扩展到复杂的多功能系统。通过掌握这些高级方法，您将获得解锁LLMs全部潜力的能力，使它们能够解决甚至最具有挑战性的问题。
- en: 'This part has the following chapters:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 20*](B31249_20.xhtml#_idTextAnchor305), *Chain-of-Thought Prompting*'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第20章*](B31249_20.xhtml#_idTextAnchor305)，*思维链提示*'
- en: '[*Chapter 21*](B31249_21.xhtml#_idTextAnchor315), *Tree-of-Thoughts Prompting*'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第21章*](B31249_21.xhtml#_idTextAnchor315)，*思维树提示*'
- en: '[*Chapter 22*](B31249_22.xhtml#_idTextAnchor325), *Reasoning and Acting*'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第22章*](B31249_22.xhtml#_idTextAnchor325)，*推理与行动*'
- en: '[*Chapter 23*](B31249_23.xhtml#_idTextAnchor339), *Reasoning* *WithOut* *Observation*'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第23章*](B31249_23.xhtml#_idTextAnchor339)，*无观察推理*'
- en: '[*Chapter 24*](B31249_24.xhtml#_idTextAnchor346), *Reflection Techniques*'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第24章*](B31249_24.xhtml#_idTextAnchor346)，*反思技术*'
- en: '[*Chapter 25*](B31249_25.xhtml#_idTextAnchor355), *Automatic Multi-Step Reasoning
    and Tool Use*'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第25章*](B31249_25.xhtml#_idTextAnchor355)，*自动多步推理与工具使用*'
