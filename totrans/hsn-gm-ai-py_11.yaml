- en: Optimizing for Continuous Control
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, we have considered most of the training/challenge environments
    we've looked at as being episodic; that is, the game or environment has a beginning
    and an end. This is good since most games have a beginning and an end – it is,
    after all, a game. However, in the real world, or for some games, an episode could
    last days, weeks, months, or even years. For these types of environment, we no
    longer think of an episode; rather we work with the concept of an environment
    that requires continuous control. So far, we have looked at a subset of algorithms
    that can solve this type of problem but they don't do so very well. So, like most
    things in RL, we have a special class of algorithms devoted to those types of
    environment, and we'll explore them in this chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll look at improving the policy methods we looked at previously
    for performing continuous control of advanced environments. We'll start off by
    setting up and installing the Mujoco environment, a specialized area we can use
    to test these new algorithms, the first of which will be the proximal policy optimization
    or PPO method. After that, we'll look at a novel improvement called recurrent
    networks for capturing context and learn how that is applied on top of PPO. Then,
    we'll get back into actor-critic and this time look at asynchronous actor-critic
    in a couple of different configurations. Finally, we'll look at ACER and actor-critic
    with experience replay.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a summary of the main topics we will cover in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Understanding continuous control with Mujoco
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing proximal policy optimization
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using PPO with recurrent networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deciding on synchronous and asynchronous actors
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building actor-critic with experience replay
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we'll look at a class of RL methods that attempts to deal specifically
    with real-world problems of robotics or other control systems. Of course, this
    doesn't mean these same algorithms couldn't be used in gaming – they are. In the
    next section, we'll begin by looking at the specialized Mujoco environment.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Understanding continuous control with Mujoco
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The standard environment for building continuous control agents is the Mujoco
    environment. **Mujoco** stands for **Multi-Joint dynamics with Contract** and
    it is a full physics environment for training robotic or simulation agents. This
    environment provides a number of simulations that challenge some form of robotic
    control agent to perform a task, such as walking, crawling, and implementing several
    other physics control-based tasks. An example of the diversity of these environments
    is summarized well in the following image, which has been extracted from the Mujoco
    home page:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48271473-8094-4df7-b3fd-fccd2982a72d.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
- en: Extract of example environments from the Mujoco home page
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, we will want to use this cool environment. However, this package
    is not free and requires a license, but note that a 30-day trial is provided.
    Now for the bad news. The package is extremely difficult to set up, install, and
    train, especially if you are using Windows. In fact, it is so difficult that,
    although we strongly suggest using Mujoco as an environment, we won't be using
    it for the remaining exercises in this chapter. Why? Again, it is extremely difficult
    and we don't want to exclude people who are unable to install the Mujoco environment.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们希望使用这个酷炫的环境。然而，这个包不是免费的，需要许可证，但请注意，提供了一个30天的试用期。现在，坏消息来了。这个包的设置、安装和训练都非常困难，尤其是如果你使用Windows的话。事实上，它如此困难，尽管我们强烈建议使用Mujoco作为环境，但我们不会在本章剩余的练习中使用它。为什么？再次强调，它非常困难，我们不希望排除那些无法安装Mujoco环境的人。
- en: There are plenty of blog posts or Stack Overflow articles available that walk
    through various installations of the various versions of Mujoco for Windows. Mujoco's
    support for Windows was stopped after version 1.5\. While it is still possible
    to install Mujoco on Windows, it is not trivial and is likely to change often.
    As such, if you are inclined to use Windows with Mujoco, your best bet is to look
    to the most recent blogs or forum posts discussing this for help.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多博客文章或Stack Overflow文章可供参考，它们介绍了Windows上Mujoco各种版本的安装方法。Mujoco在1.5版本之后停止了对Windows的支持。尽管在Windows上安装Mujoco仍然可能，但这并不简单，并且可能会经常发生变化。因此，如果你倾向于使用Windows与Mujoco，你最好的选择是查找最近的博客或论坛帖子以获取帮助。
- en: 'In this exercise, we''ll walk through the basic installation of Mujoco (not
    for Windows):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将介绍Mujoco的基本安装（不包括Windows）：
- en: The first thing we need is a license. Open your browser, go to [mujoco.org](http://mujoco.org),
    and locate the License button at the top of the page. Then, click it.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先需要的是一个许可证。打开你的浏览器，访问[mujoco.org](http://mujoco.org)，并找到页面顶部的许可证按钮。然后，点击它。
- en: On the page, you will see an entry for **Computer id**. This will require you
    to download a key generator from the blue links shown to the right. Click one
    of the links to download the key generator.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在页面上，你会看到一个**计算机ID**的条目。这需要你从右侧显示的蓝色链接中下载一个密钥生成器。点击其中一个链接下载密钥生成器。
- en: Run the key generator on your system and enter the key in the **Computer i****d**
    field.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的系统上运行密钥生成器，并在**计算机ID**字段中输入密钥。
- en: 'Fill in the rest of the license information with your name and email and click
    **Submit**, as shown in the following screenshot:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你的姓名和电子邮件填写其余的许可证信息，然后点击**提交**，如图所示：
- en: '![](img/674b587e-a965-42ca-ad66-7f09e85416fa.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/674b587e-a965-42ca-ad66-7f09e85416fa.png)'
- en: Submitting for a Mujoco license
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 提交Mujoco许可证
- en: You should get a key emailed to you in a few minutes with directions as to where
    to put the key. Then, you need to download the binaries for your platform. Click
    on the Products link at the top of the page to be taken to the downloads. Download
    the version you need for your OS.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你应该在几分钟内收到一封电子邮件，其中包含有关将密钥放在何处方向的说明。然后，你需要下载你平台上的二进制文件。点击页面顶部的“产品”链接以转到下载页面。下载你操作系统所需的版本。
- en: Unzip the files into your root user folder, `~/.mujoco/mujoco%version%`, where
    `%version%` denotes the version of the software. On Windows, your user folder
    is `C:\Users\%username%`, where `%username%` denotes the logged in user's name.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文件解压到你的根用户文件夹中，`~/.mujoco/mujoco%version%`，其中`%version%`表示软件的版本。在Windows上，你的用户文件夹是`C:\Users\%username%`，其中`%username%`表示登录用户的名字。
- en: 'Now, you need to build the Mujoco package and set up the `mujoco-py` scripts.
    This varies widely by installation. Use the following commands to build and install
    Mujoco:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你需要构建Mujoco包并设置`mujoco-py`脚本。这取决于安装方式。使用以下命令构建和安装Mujoco：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To test the installation and check for dependencies, run the following command
    to reinstall the entire Gym again:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了测试安装并检查依赖项，运行以下命令重新安装整个Gym：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you run this command and still see errors, you likely need more help. Consult
    online resources for the most current search on `mujoco install` and try those
    instructions. Again, at the time of writing, Windows is no longer supported and
    you may be better off using another platform. Fortunately, setting up a VM or
    Cloud service for this can now be quite easy and you may have more luck there.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行此命令仍然看到错误，你可能需要更多帮助。咨询在线资源，进行关于`mujoco install`的最新搜索，并尝试那些说明。再次强调，在撰写本文时，Windows不再受支持，你可能更适合使用其他平台。幸运的是，现在为这个设置虚拟机或云服务可以相当容易，并且你可能在那里有更多运气。
- en: 'You can test the Mujoco installation and ensure that the license is all set
    up by running `Chapter_9_Mujoco.py` as you normally would. The listing is shown
    here:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过运行`Chapter_9_Mujoco.py`来测试Mujoco的安装，并确保许可证已经全部设置好。列表如下所示：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If you have everything installed correctly, then you should see something similar
    to the following image, which has been taken from the Mujoco environment:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已正确安装所有内容，那么您应该会看到以下类似图像，该图像是从Mujoco环境中获取的：
- en: '![](img/ab253b9c-0b6d-47d0-8e1d-4f1a38b3ea18.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab253b9c-0b6d-47d0-8e1d-4f1a38b3ea18.png)'
- en: The fetch reach Mujoco environment
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Fetch reach Mujoco环境
- en: If you are able to install the Mujoco environments, then great – have fun exploring
    a whole new world of environments. For those of you who are not able to install
    Mujoco, don't fret. We will learn how to create our own physics-based environments
    when we start using Unity in [Chapter 10](ab9a7f4f-60d8-4643-8627-199cf95bcf55.xhtml),
    *Exploiting ML-Agents*. Rest assured that, while Mujoco is indeed cool, much like
    the Atari games we have seen before, it is also not trivial to train. Not unlike
    Atari, Mujoco environments can take millions of training iterations. Therefore,
    to keep things simple and also remain energy-conscious, we will use the regular
    old Gym environments. The additional plus now is that we have a better comparison
    between various algorithms across a single environment.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您能够安装Mujoco环境，那么太好了——尽情探索一个全新的环境世界。对于那些无法安装Mujoco的读者，请不要担心。当我们开始使用Unity时，我们将在第10章[利用ML-Agents](ab9a7f4f-60d8-4643-8627-199cf95bcf55.xhtml)中学习如何创建自己的基于物理的环境。请放心，虽然Mujoco确实很酷，就像我们之前看到的Atari游戏一样，但它也不是那么容易训练。与Atari类似，Mujoco环境可能需要数百万次的训练迭代。因此，为了保持简单并保持节能，我们将使用常规的Gym环境。现在的额外好处是我们可以在单个环境中更好地比较各种算法。
- en: '**Deep reinforcement learning** (**DRL**) and **machine learning** (**ML**)
    in general are getting a bit of a bad name due to the additional energy they consume.
    Many state-of-the-art DRL models can be measured in terms of energy consumption
    and, in most cases, the amount of energy is quite high. In one case, DeepMind
    has admitted that the amount of processing/energy it used to train a single model
    would run a single desktop computer for 45 years. That is an incredible amount
    of energy in a world that needs to be cautious about energy consumption. Therefore,
    wherever applicable, in this book we will favor cheaper training environments.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度强化学习**（**DRL**）和一般**机器学习**（**ML**）由于它们消耗的额外能量而获得了一些坏名声。许多最先进的DRL模型可以从能耗的角度进行衡量，在大多数情况下，能耗相当高。在某个案例中，DeepMind承认，它用于训练单个模型的处理/能耗足以让一台台式电脑运行45年。这在需要谨慎能源消耗的世界中是一个惊人的数字。因此，在适用的情况下，在这本书中，我们将优先考虑成本更低的训练环境。'
- en: In the next section, we look at advancing these policy methods with gradient
    optimization.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨如何通过梯度优化来提升这些策略方法。
- en: Introducing proximal policy optimization
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍近端策略优化
- en: We are now entering areas where we will start looking at state-of-the-art algorithms,
    at least at the time of writing. Of course, that will likely change and things
    will advance. For now, though, the **proximal policy optimization** algorithm
    (**PPO**), was introduced by OpenAI, is considered a state-of-the-art deep reinforcement
    learning algorithm. As such, the sky is the limit as to what environments we can
    throw at this problem. However, in order to quantify our progress and for a variety
    of other reasons, we will continue to baseline against the Lunar Lander environment.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将进入一个领域，我们将开始研究最先进的算法，至少在撰写本文时是这样。当然，这可能会发生变化，事物将会进步。不过，目前，由OpenAI引入的**近端策略优化**算法（**PPO**）被认为是最先进的深度强化学习算法。因此，我们可以将各种环境抛给这个问题。然而，为了量化我们的进展以及各种其他原因，我们将继续以Lunar
    Lander环境为基准。
- en: The PPO algorithm is just an extension and simplification of the **trust region
    policy optimization** (**TRPO**) algorithm we covered in [Chapter 8](42626cbd-87b8-428c-8f2a-ecc06f5e387c.xhtml),
    *Policy Gradient Methods*, but with a few key differences. PPO is also much simpler
    to understand and follow. For these reasons, we will review each feature that
    makes policy optimization with trust regions in the case of TRPO and clipping
    with PPO so powerful.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: PPO算法只是对我们在第8章中介绍的**信任域策略优化**（**TRPO**）算法的扩展和简化，但有一些关键的不同之处。PPO也更容易理解和遵循。出于这些原因，我们将回顾每个使TRPO和PPO中的信任域剪裁策略优化变得如此强大的特征。
- en: The code for this chapter was originally sourced from the following repository: [https://github.com/seungeunrho/minimalRL](https://github.com/seungeunrho/minimalRL).
    A number of modifications have been made to the code so that it fits the examples
    in this book.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码最初来源于以下仓库：[https://github.com/seungeunrho/minimalRL](https://github.com/seungeunrho/minimalRL)。对代码进行了一些修改，以便它符合本书中的示例。
- en: 'Since we have already gone over the major intuition behind this improvement,
    let''s jump into the next coding exercise by opening `Chapter_9_PPO.py`. Perform
    the following steps:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了这一改进的主要直觉，让我们通过打开`Chapter_9_PPO.py`来跳入下一个编码练习。执行以下步骤：
- en: 'The code for this listing is quite similar to many of the other listings we
    have reviewed. As such, we will limit our review to critical sections:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个列表的代码与其他我们已审查的列表非常相似。因此，我们将仅限于审查关键部分：
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Scrolling right to the bottom, we can see that the training code is almost
    identical to our most recent examples in the previous chapters. One key thing
    to notice is the inclusion of a new hyperparameter, `T_horizon`, which we will
    define shortly:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动到最底部，我们可以看到训练代码几乎与前面章节中我们最近的一些示例相同。一个需要注意的关键点是引入了一个新的超参数`T_horizon`，我们将在稍后定义它：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If we scroll back to the top, you will see the definition of new hyperparameters
    for `T_horizon`, `K_epoch`, `eps_clip`, and `lambda`. Just note these new variables
    for now – we will get to their purpose shortly.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们滚动回顶部，你会看到为`T_horizon`、`K_epoch`、`eps_clip`和`lambda`定义的新超参数。现在只需记住这些新变量——我们很快就会了解它们的目的。
- en: 'Let''s jump to some of the other important differences, such as the network
    definition, which can be seen in the `init` method of the `PPO` class as follows:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们跳到一些其他的重要差异，例如网络定义，这可以在`PPO`类的`init`方法中看到如下：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: What we can see is that the network comprises a first input state `Linear` layer
    called `fc1` that is composed of 256 neurons. Then, we can see that the `fc_pi`
    or policy network is defined as `Linear` with 256 neurons and outputs the `num_actions`
    or number of actions. Following that is the definition of `fc_v`, which is the
    value layer. Again, this has 256 neurons and one output, that is, the expected
    value.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以看到，网络由一个名为`fc1`的第一输入状态`Linear`层组成，该层包含256个神经元。然后，我们可以看到`fc_pi`或策略网络被定义为`Linear`，包含256个神经元，并输出`num_actions`或动作的数量。接下来是`fc_v`的定义，这是值层。同样，它也有256个神经元和一个输出，即期望值。
- en: The rest of the code for the PPO class is almost the same as in the previous
    examples and we won't need to cover it here.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PPO类的其余代码几乎与前面的示例相同，我们在这里不需要详细说明。
- en: Run the code as normal. This example will take a while to run but not as long
    as previous versions. We'll leave it up to you whether you want to wait for the
    example to complete before continuing.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按正常方式运行代码。这个示例将需要一段时间才能运行，但不会像之前的版本那样长。我们将由你来决定是否在示例完成之前继续。
- en: One thing you should quickly notice is how much faster the algorithm trains.
    Indeed, the agent gets good quite fast and could actually solve the environment
    in fewer than 10,000 iterations, which is quite impressive. Now that we have seen
    how impressive policy optimization can be, we will look at how this is possible
    in the next section.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该很快就能注意到算法训练的速度有多快。确实，智能体很快就变得很好，实际上可以在少于10,000次迭代中解决环境，这相当令人印象深刻。既然我们已经看到了策略优化的潜力，我们将在下一节中探讨这是如何实现的。
- en: The hows of policy optimization
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略优化的方法
- en: In [Chapter 8](42626cbd-87b8-428c-8f2a-ecc06f5e387c.xhtml), *Policy Gradient
    Methods*, we covered how policy gradient methods can fail and then introduced
    the TRPO method. Here, we talked about the general strategies TRPO uses to address
    the failings in PG methods. However, as we have seen, TRPO is quite complex and
    seeing how it works in code was not much help either. This is the main reason
    we minimized our discussion of the details when we introduced TRPO and instead
    waited until we got to this section to tell
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](42626cbd-87b8-428c-8f2a-ecc06f5e387c.xhtml)“策略梯度方法”中，我们介绍了策略梯度方法可能失败的情况，然后介绍了TRPO方法。在这里，我们讨论了TRPO用来解决PG方法中失败的一般策略。然而，正如我们所看到的，TRPO相当复杂，看到它在代码中的工作也没有太大帮助。这就是我们在介绍TRPO时尽量减少对细节讨论的主要原因，而是等到我们到达这一节才来讲述。
- en: the full story in a concise manner.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以简洁的方式讲述完整的故事。
- en: 'That said, let''s review how policy optimization with TRPO or PPO can do what
    it does:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，让我们回顾一下使用TRPO或PPO进行策略优化的方法是如何做到这一点的：
- en: '**Minorize-Maximization MM algorithm**: Recall that this is where we find the
    minimum of an upper bound function by finding the maximum of a lower bound function
    that is constrained to be within the upper bound function.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小化-最大化MM算法**：回想一下，这是我们在找到上界函数的最小值时，通过找到受限于在上界函数内的下界函数的最大值来实现的。'
- en: '**Line search**: We have seen this being used to define in which direction
    and by what amount we could optimize our function (deep learning network). This
    allows our algorithm to avoid overshooting the target of optimization.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线搜索**：我们已经看到这是用来定义我们如何以及多少可以优化我们的函数（深度学习网络）的方向和量的。这允许我们的算法避免超过优化的目标。'
- en: '**Trust region**: Along with MM and Line Search, we also want the policy function
    to have a stable base or platform to move along on. You can think of this stable
    base as a region of trust or safety. In PPO, this is defined differently, as we
    will see.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信任区域**：除了MM和线搜索之外，我们还想让策略函数有一个稳定的基或平台来移动。你可以把这个稳定的基想象成一个信任或安全区域。在PPO中，这被定义得不同，正如我们将看到的。'
- en: PPO and TRPO share all these improvements as a way of finding a better policy.
    PPO improves on this by also understanding how much we want to change the policy's
    distribution over each iteration. This understanding also allows us to limit the
    amount of change during each iteration. We have seen how TRPO does this to a certain
    extent with KL divergence, but PPO takes this one step further by adjusting or
    adapting to the amount of change. In the next section, we'll look at how this
    adaptation works.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: PPO和TRPO都以寻找更好的策略作为共同改进的方式。PPO通过理解我们每轮迭代想要改变策略分布的程度来改进这一点。这种理解也使我们能够限制每轮迭代中的变化量。我们已经看到TRPO如何通过KL散度在一定程度上做到这一点，但PPO通过调整或适应变化量更进一步。在下一节中，我们将探讨这种适应是如何工作的。
- en: PPO and clipped objectives
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PPO和剪裁目标
- en: Before we get into the finer details of how PPO works, we need to step back
    and understand how we equate the difference in distributed data distributions
    or just distributions. Remember that PG methods look to understand the returns-based
    sampling distribution and then use that to find the optimum action or the probability
    of the optimum action. Due to this, we can use a method called **KL Divergence**
    to determine how different the two distributions are. By understanding this, we
    can determine how much room or area of trust we can allow our optimization algorithm
    to explore with. PPO improves on this by clipping the objective function by using
    two policy networks.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨PPO的工作细节之前，我们需要退一步理解我们如何将分布式数据分布或分布之间的差异等同起来。记住，PG方法试图理解基于回报的采样分布，然后使用它来找到最佳动作或最佳动作的概率。因此，我们可以使用一种称为**KL散度**的方法来确定两个分布之间的差异。通过理解这一点，我们可以确定我们可以允许我们的优化算法探索多少空间或信任区域。PPO通过使用两个策略网络来剪裁目标函数来改进这一点。
- en: Jonathan Hui has a number of insightful blog posts on the mathematics behind
    various RL and PG methods. In particular, his post on PPO ([https://medium.com/@jonathan_hui/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12](https://medium.com/@jonathan_hui/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12is))
    is quite good. Be warned that they do assume a very sophisticated level of mathematics
    knowledge. If you are serious about RL, you will want to be able to read and understand
    this content at some point. However, you can get quite far in DRL by intuitively
    understanding most algorithms, like we're doing here.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 乔纳森·惠（Jonathan Hui）在关于各种强化学习（RL）和策略梯度（PG）方法的数学方面有许多有见地的博客文章。特别是他关于PPO的文章（[https://medium.com/@jonathan_hui/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12](https://medium.com/@jonathan_hui/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12)）相当不错。请注意，它们确实假设了一个非常复杂的数学知识水平。如果你对RL认真，你将需要在某个时候能够阅读和理解这些内容。然而，通过直观地理解大多数算法，就像我们在这里做的那样，你可以通过深度强化学习（DRL）走得很远。
- en: 'Let''s learn how this works in code by opening up `Chapter_9_PPO.py` and performing
    the following steps:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过打开`Chapter_9_PPO.py`并执行以下步骤来学习如何在代码中实现这一点：
- en: 'Having looked through the bulk of the main code, we only want to focus on the
    training code here and, in particular, the `train_net` function from the `PPO`
    class, as shown here:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在查看主要代码的大部分内容后，我们只想关注这里的训练代码，特别是`PPO`类中的`train_net`函数，如下所示：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After the initial `make_batch` function call, in order to build the lists,
    we come to the iteration loop controlled by `K_epoch`. `K_epoch` is a new hyperparameter
    that controls the number of iterations we use to optimize the advantage convergence:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在初始的`make_batch`函数调用之后，为了构建列表，我们进入由`K_epoch`控制的迭代循环。`K_epoch`是一个新的超参数，它控制我们用于优化优势收敛的迭代次数：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The first block of code inside the `K_epoch` iteration is the calculation of `td_target`
    using the reward `r`, plus the discount factor `gamma` times the output of the
    v or value network and `done_mask`. Then, we take the `delta` or TD change and
    convert it into a `numpy` tensor:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`K_epoch`迭代内部的第一个代码块是使用奖励`r`计算`td_target`，加上折扣因子`gamma`乘以v或价值网络的输出和`done_mask`。然后，我们取`delta`或TD变化并将其转换为`numpy`张量：'
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, using the delta, we build a list of advantages using the `advantage`
    function, as follows:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用delta，我们通过`advantage`函数构建一个优势列表，如下所示：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, we push the state **s** into the policy network, `pi`. Next, we gather
    the axis along the first dimension and then take the ratio using the equation ![](img/4c9bd681-0b83-473f-939e-9f9ec792ad3c.png),
    which is used to calculate a possible ratio for the clipping region or area we
    want to use for trust:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将状态**s**推入策略网络`pi`。接下来，我们沿着第一个维度收集轴，然后使用方程 ![](img/4c9bd681-0b83-473f-939e-9f9ec792ad3c.png)计算比率，该方程用于计算我们想要用于信任的剪切区域或区域的可能比率：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We use the `ratio` value to calculate the `surr1` value, which defines the surface
    or clipping region. The next line calculates a second version of the surface, `surr2`,
    by clamping this ratio and using the bounds of our clipping region set by `eps_clip`
    to define the area. Then, it takes the minimum area of either surface and uses
    that to calculate the loss.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`ratio`值来计算`surr1`值，它定义了表面或剪切区域。下一行通过夹紧这个比率并使用由`eps_clip`设置的剪切区域边界来定义区域，计算第二个版本的表面`surr2`。然后，它取两个表面中的最小面积，并使用该面积来计算损失。
- en: We use the term `surface` here to understand that the calculation of loss is
    over a multi-dimensional array of values. Our optimizer works across this surface
    to find the best global minimum or lowest area on the surface.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用术语“表面”来理解损失计算是在一个多维数组值上进行的。我们的优化器在这个表面上工作，以找到最佳的全局最小值或表面的最低区域。
- en: 'The last section of code is our typical gradient descent optimization and is
    shown next for completeness:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码的最后部分是我们的典型梯度下降优化，下面为了完整性而展示：
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'There''s nothing new here. Go ahead and run the sample again or review the
    output from the previous exercise. An example of the training output is shown
    here:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里没有什么新的内容。继续运行样本或回顾之前练习的输出。以下是一个训练输出的示例：
- en: '![](img/801102d6-d763-47f2-a7c2-2562ab1d850d.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/801102d6-d763-47f2-a7c2-2562ab1d850d.png)'
- en: Example output from Chapter_9_PPO.py
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 来自`Chapter_9_PPO.py`的示例输出
- en: A couple of things to note is that we are stilling using discrete action spaces
    and not continuous ones. Again, our primary reason for doing this is to continue
    using a baseline-consistent environment, Lunar Lander v2\. Lunar Lander does have
    a continuous action environment that you can try, but you will need to convert
    the sample so that you can use continuous actions. On the second item of note,
    PPO and other PG methods actually perform better on continuous action environments,
    which means you aren't really seeing their full potential. So, why are we continuing
    to use discrete action spaces? Well, in almost all cases, games and interactive
    environments will use discrete spaces. Since this is a book on games and AI and
    not robotics and AI, we will stick to discrete spaces.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，我们仍在使用离散动作空间，而不是连续动作空间。再次强调，我们这样做的主要原因是继续使用基线一致的环境，即Lunar Lander v2。Lunar
    Lander确实有一个连续动作环境，你可以尝试，但你需要转换样本，以便使用连续动作。第二个需要注意的事项是，PPO和其他PG方法在连续动作环境中表现实际上更好，这意味着你并没有真正看到它们的全部潜力。那么，我们为什么还在继续使用离散动作空间呢？嗯，在几乎所有情况下，游戏和交互式环境都会使用离散空间。由于这本书是关于游戏和AI的，而不是关于机器人和AI的，我们将坚持使用离散空间。
- en: There are various research initiatives on other PG methods but you should consider
    PPO to be a milestone in DRL, not unlike DQN. For those of you who are curious,
    PPO made its name by beating human players in the DOTA2 strategy game. In the
    next section, we'll look at other methods that have been layered on top of PG
    and other methods to improve DRL.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 关于其他PG方法的研究项目有很多，但你应该将PPO视为DRL的一个里程碑，就像DQN一样。对于那些好奇的人来说，PPO通过在DOTA2策略游戏中击败人类玩家而闻名。在下一节中，我们将探讨其他叠加在PG和其他方法之上的方法，以改进DRL。
- en: Using PPO with recurrent networks
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PPO与循环网络结合
- en: In [Chapter 7](42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml), *Going Deeper with
    DDQN*, we saw how we could interpret visual state using a concept called **convolutional
    neural networks** (**CNNs**). CNN networks are used to detect features in visual
    environments such as Atari games. While this technique allowed us to play any
    of a number of games with the same agent, the added CNN layers took much more
    time to train. In the end, the extra training time wasn't worth the cool factor
    of playing Atari games. However, there are other network structures we can put
    on top of our networks in order to make better interpretations of state. One such
    network structure is called recurrent networks. Recurrent network layers allow
    us to add the concept of context or time in our model's interpretation of state.
    This can work very well in any problem where context or memory is important.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml)《使用DDQN深入探索》中，我们看到了如何使用称为**卷积神经网络**（**CNNs**）的概念来解释视觉状态。CNN网络用于检测视觉环境中的特征，例如Atari游戏。虽然这项技术允许我们使用同一个智能体玩许多游戏，但增加的CNN层需要更多的时间来训练。最终，额外的训练时间并不值得玩Atari游戏的酷炫效果。然而，我们可以在网络之上添加其他网络结构，以更好地解释状态。其中一种网络结构称为循环网络。循环网络层允许我们在模型对状态的解释中添加上下文或时间概念。这在任何上下文或记忆重要的问题上都可以非常有效。
- en: Recurrent network layers are a form of deep learning perceptron that essentially
    feeds back its state to previous neurons. This, in effect, gives the network the
    ability to understand time or context. It does this so well that recurrent networks
    are now at the heart of all text and language processing networks. Language is
    especially contextual and recurrent layers, in various configurations, make short
    work of understanding the context. There are various configurations of recurrent
    network layers but the only one we'll focus on here is called **long short term
    memory** (**LSTM**).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 循环网络层是一种深度学习感知器，它本质上将状态反馈给前面的神经元。这实际上赋予了网络理解时间或上下文的能力。它做得如此之好，以至于循环网络现在是所有文本和语言处理网络的核心。语言尤其具有上下文性，循环层，以各种配置，可以轻松理解上下文。循环网络层有多种配置，但在这里我们将关注的一种称为**长短期记忆**（**LSTM**）。
- en: Recurrent networks and LSTM layers are worthy of in-depth study on their own.
    These powerful network layers have been responsible for some very interesting
    discoveries in the last few years. While recurrent layers have turned out to have
    limited use in DRL, it is believed they should have more. After all, understanding
    context in a trajectory must be important.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 循环网络和 LSTM 层值得深入研究。这些强大的网络层在过去几年中负责了一些非常有趣的发现。虽然循环层在 DRL 中已被证明用途有限，但人们认为它们应该有更多用途。毕竟，在轨迹中理解上下文肯定很重要。
- en: 'LSTM layers for the purposes of DRL are quite simple to put in place. Open
    `Chapter_9_PPO_LSTM.py` and follow these steps to see how this works:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 DRL 的 LSTM 层来说，放置起来相当简单。打开 `Chapter_9_PPO_LSTM.py` 并按照以下步骤操作，看看这是如何工作的：
- en: This sample is virtually identical to `Chapter_9_PPO.py` but with the few key
    differences, all of which we will look at here.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个示例几乎与 `Chapter_9_PPO.py` 完全相同，但有一些关键的不同之处，所有这些我们都会在这里查看。
- en: 'Skip to the `PPO` class definition, as shown here:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跳转到 `PPO` 类定义，如下所示：
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The only new part here is the definition of a new layer, `lstm`, which is of
    the `LSTM(64.32)` type. The LSTM layer that's injected at the top of the state
    encoding allows the network to learn the context in actions or memory. Now, instead
    of understanding which state-actions provide the best trajectory, our agent is
    learning which state-action sets are providing the best outcome. In gaming, this
    may be analogous to learning that a special move unlocks a sequence to get some
    special reward.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里唯一的新部分是定义了一个新的层，`lstm`，其类型为 `LSTM(64,32)`。注入到状态编码顶部的 LSTM 层允许网络学习动作或记忆中的上下文。现在，我们的智能体不是学习哪些状态-动作提供最佳轨迹，而是在学习哪些状态-动作集提供最佳结果。在游戏中，这可能类似于学习一个特殊动作可以解锁一系列动作以获得一些特殊奖励。
- en: 'Next, we will move down to the policy pi function and value v function network
    definitions and look at how it has been modified:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将向下移动到策略 pi 函数和值 v 函数的网络定义，并查看它们是如何被修改的：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The `pi` and `v` functions take a hidden layer but only `pi`, or the policy
    function, is used as the output of a hidden layer. We will see how these hidden
    LSTM layers work shortly.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`pi` 和 `v` 函数接受一个隐藏层，但只有 `pi`，即策略函数，被用作隐藏层的输出。我们很快就会看到这些隐藏 LSTM 层是如何工作的。'
- en: 'Then, at the top of the `train_net` function, we can see where the layers are
    extracted from the batching process, `make_batch`:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在 `train_net` 函数的顶部，我们可以看到层是从批处理过程中提取出来的，`make_batch`：
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We use two hidden or middle LSTM layers between our actor-critics, where `second_hidden`
    denotes the output and `first_hidden` denotes the input. Below that, in the `for`
    loop, we can see the calculation of delta using the LSTM input and output:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在 actor-critics 之间使用两个隐藏或中间 LSTM 层，其中 `second_hidden` 表示输出，`first_hidden`
    表示输入。在下面的 `for` 循环中，我们可以看到使用 LSTM 输入和输出的 delta 计算过程：
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The calculation of delta here is done by applying the difference between the
    before and after the LSTM layer was applied, which allows the delta to encapsulate
    the effect the LSTM has on the calculation of value `v`.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里 delta 的计算是通过应用 LSTM 层应用前后的差异来完成的，这使得 delta 能够封装 LSTM 对值 `v` 计算的影响。
- en: 'Run the sample as you normally would and observe the output, as shown here:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照正常方式运行示例，并观察输出，如下所示：
- en: '![](img/87ea8d62-2ae5-4df4-9978-d56fcbbfc35c.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/87ea8d62-2ae5-4df4-9978-d56fcbbfc35c.png)'
- en: Example output of Chapter_9_PPO_LSTM.py
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`Chapter_9_PPO_LSTM.py` 的示例输出'
- en: Notice how this slight improvement increased training performance significantly
    from the vanilla PPO example we looked at in the previous exercise. In the next
    section, we'll improve PPO further by applying parallel environments.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这个轻微的改进是如何显著提高了我们在上一练习中查看的 vanilla PPO 示例的训练性能。在下一节中，我们将通过应用并行环境进一步改进 PPO。
- en: Deciding on synchronous and asynchronous actors
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决定同步和异步的 actors
- en: 'We started off this book with a simple discussion of what **artificial general
    intelligence** (**AGI**) is. In short, AGI is our attempt at generalizing an intelligent
    system to solve multiple tasks. RL is often thought of as a stepping stool to
    AGI primarily because it tries to generalize state-based learning. While both
    RL and AGI take deep inspiration from how we think, be it rewards or possibly
    consciousness itself, the former tends to incorporate direct analogies. The actor-critic
    concept in RL is an excellent example of how we use an interpretation of human
    psychology to create a form of learning. For instance, we humans often consider
    the consequences of our actions and determine the advantages they may or may not
    give us. This example is perfectly analogous to actor-critic and advantage methods
    we use in RL. Take this further and we can consider another human thought process:
    asynchronous and synchronous thought.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这本书的开篇以对**人工通用智能**（**AGI**）的简单讨论开始。简而言之，AGI是我们尝试将智能系统泛化以解决多个任务的尝试。强化学习（RL）通常被视为通往AGI的阶梯，主要是因为它试图泛化基于状态的学习。虽然RL和AGI都从我们的思考方式中汲取了深刻的灵感，无论是奖励还是可能的意识本身，但前者倾向于包含直接的类比。RL中的演员-评论家（actor-critic）概念是我们在创建一种学习形式时如何使用对人类心理学的解释的一个极好例子。例如，我们人类经常考虑我们行为的后果，并确定它们可能或可能不会给我们带来的优势。这个例子与我们在RL中使用的演员-评论家和优势方法完美类比。更进一步，我们可以考虑另一种人类思维过程：异步和同步思维。
- en: A direct example of asynchronous/synchronous thought is when an answer to a
    problem **pops** into your head after being asked a question several hours earlier.
    Perhaps you didn't have the answer then but then it came to you a few hours later.
    Were you thinking about the question all that time? Not likely, and more than
    likely the answer just came to you – it popped into your head. But were you thinking
    of it all the time in some background process or did some process just fire up
    and provide the answer? The animal brain thinks like this all the time and it
    is what we would call in computerese parallel processing. So, could our agents
    not also benefit from this same thought process? As it turns out, yes.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 异步/同步思维的一个直接例子是在被问了一个问题几小时后，答案突然出现在你的脑海中。也许你当时没有答案，但几个小时后它就出现了。你一直在想这个问题吗？不太可能，而且更有可能的是答案突然出现在你的脑海中。但是，你是在某个后台过程中一直在想它，还是某个过程突然启动并提供了答案？动物大脑总是这样思考，我们称之为计算机术语中的并行处理。那么，我们的代理不能也从这种思维过程中受益吗？结果证明，是的。
- en: 'This inspiration likely came in part from the preceding analogy but also has
    a mathematical background in how we can evaluate advantage. The direct evaluation
    of how our brains think is still a big answer but we can assume our thoughts to
    be synchronous or asynchronous. So, instead of just considering one thought process,
    what if we consider several? We can take this analogy a step further and apply
    this back to DRL – in particular, actor-critic. Here, we have a single thought
    process or global network that is fed the output of several worker thought processes.
    An example of this is shown here:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这种灵感可能部分来自前面的类比，但也具有如何评估优势的数学背景。直接评估我们大脑的思考方式仍然是一个大问题，但我们可以假设我们的思维是同步的或异步的。因此，除了考虑一个思维过程之外，如果我们考虑几个会怎样？我们可以将这个类比更进一步，并将其应用于深度强化学习（DRL）——特别是演员-评论家。在这里，我们有一个单一的思维过程或全局网络，它被几个工人思维过程的输出所喂养。这里展示了这样一个例子：
- en: '![](img/eeb6bb78-7555-424c-8ed4-da6407ecc1ab.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/eeb6bb78-7555-424c-8ed4-da6407ecc1ab.png)'
- en: Example of asynchronous AC by Dr. Arthur Juliani
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 亚瑟·朱利安尼博士异步AC的示例
- en: What we can see here is the basic intuition behind the advantage of the actor-critic
    architecture, A2C, and the asynchronous advantage actor-critic architecture, A3C.
    Notice how each worker brain/agent has its own copy of a separate environment.
    All of these worker agents feed their learning into a master brain. Each worker
    brain is then updated iteratively to coincide with the master, not unlike our
    earlier advantage calculations. In the next section, we will see how this is put
    into practice by implementing A2C.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里看到的是演员-评论家架构的优势，A2C，以及异步演员-评论家架构，A3C的基本直觉。注意每个工人大脑/代理都有自己的独立环境副本。所有这些工人代理将他们的学习输入到一个主大脑中。然后，每个工人大脑通过迭代更新以与主大脑同步，这与我们之前的优势计算非常相似。在下一节中，我们将看到如何通过实现A2C来将此付诸实践。
- en: Using A2C
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用A2C
- en: To avoid any confusion, it is important to understand that A2C and A3C both
    use AC, but it is the fashion in which they update their models that differ. In
    A2C, the method is synchronous, so each brain is feeding thoughts into the main
    brain.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免任何混淆，理解A2C和A3C都使用AC，但它们更新模型的方式不同是很重要的。在A2C中，方法是同步的，所以每个大脑都将思想输入到主大脑中。
- en: 'Let''s see how this looks in code by opening the `Chapter_9_A2C.py` file and
    reviewing the hyperparameters inside it:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过打开`Chapter_9_A2C.py`文件并查看其中的超参数来查看代码中的样子：
- en: '[PRE16]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Keep the sample open and follow these steps to continue with this exercise:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 保持样本打开并按照以下步骤继续这个练习：
- en: 'This is a large code example, so we will limit the sections we show here. The
    main thing of note here is the hyperparameters that are listed at the top of the
    file. The only thing new to note is `n_train_processes`, which sets the number
    of worker processes:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是一个大的代码示例，所以我们将限制我们在这里展示的部分。这里需要注意的是文件顶部的超参数列表。唯一需要注意的是`n_train_processes`，它设置了工作进程的数量：
- en: '[PRE17]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next comes the `ActorCritic` class, which is the same class that we used previously:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是`ActorCritic`类，这是我们之前使用的同一个类：
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then comes the definition of the `worker` function. This function is where
    the worker''s brain sends messages between the worker and master brains:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后是`worker`函数的定义。这个函数是工作节点的大脑在工作和主大脑之间发送消息的地方：
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After those functions is the big `ParallelEnv` class. The preceding code just
    shows the `init` function from that class since it''s quite large. This class
    merely coordinates activities between the masters and workers:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这些函数之后是大的`ParallelEnv`类。前面的代码只是展示了该类的`init`函数，因为它相当大。这个类仅仅协调主节点和工作节点之间的活动：
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Scrolling down past the `test` function, or the `play_game` function in our
    other examples, we can see the `compute_target` function. This is the calculation
    of the TD loss and the difference here is the use of the `mask` variable. `mask` is
    just a flag or filter that removes any calculation of discounted G on 0 returns:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动到`test`函数之后，或者在我们其他示例中的`play_game`函数之后，我们可以看到`compute_target`函数。这是TD损失的计算，这里的区别在于使用了`mask`变量。`mask`只是一个标志或过滤器，它会移除任何关于0回报的折现G的计算：
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'After that, we get into an `if` function, which determines whether the current
    process is `''__main__''`. We do this to avoid having additional worker processes
    trying to run this same block of code. After that, we can see the typical environment
    and model setup being completed:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们进入一个`if`函数，它确定当前过程是否为`'__main__'`。我们这样做是为了避免额外的工作进程尝试运行相同的代码块。之后，我们可以看到典型的环境和模型设置完成：
- en: '[PRE22]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The interval training loop code is virtually identical to those from the previous
    examples and should be self-explanatory at this point, for the most part. Something
    to note is the `env.steps` function call. This represents a step in all the worker
    environments, synchronously. Remember that the worker agents are running synchronously
    in A2C:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 间隔训练循环的代码几乎与之前的示例相同，大部分应该已经很直观。需要注意的是`env.steps`函数调用。这代表所有工作环境中的同步步骤。记住，在A2C中工作代理是同步运行的：
- en: '[PRE23]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Then, we come to the outer training loop. In this example, we can see how the
    training targets are pulled from the lists constructed by the workers, where `s_lst` is
    for the state, `a_lst` is for actions, `r_lst` is for rewards, and `mask_lst` is
    for done. Aside from torch tensor manipulation, the calculations are the same
    as in PPO.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们来到外部的训练循环。在这个示例中，我们可以看到训练目标是如何从工人构建的列表中提取的，其中`s_lst`是状态，`a_lst`是动作，`r_lst`是奖励，`mask_lst`是完成。除了torch张量操作外，计算与PPO相同。
- en: 'Run the code as you normally would and visualize the output, an example of
    which is as follows:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照你通常的方式运行代码并可视化输出，以下是一个示例：
- en: '![](img/5d2c9c6d-7807-41c4-8045-167b37dff337.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5d2c9c6d-7807-41c4-8045-167b37dff337.png)'
- en: Example output from Chapter_9_A2C.py
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 来自Chapter_9_A2C.py的示例输出
- en: You will need to tune the hyperparameters to get this example to run perfectly.
    Now, we will move on and look at the asynchronous version of A2C—A3C.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要调整超参数才能使这个示例完美运行。现在，我们将继续并查看A2C的异步版本——A3C。
- en: Using A3C
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用A3C
- en: 'Synchronous actor-critic workers provide a training advantage by essentially
    providing more sampling variations that should, in turn, reduce the amount of
    expected error and thus improve training performance. Mathematically, all we are
    doing is providing a larger sampling space which, as any statistician will tell
    you, just reduces the sampling error. However, if we assume that each worker is
    asynchronous, meaning it updates the global model in its own time, this also provides
    us with more statistical variability in our sampling across the entire trajectory
    space. This can also happen along the sampling space at the same time. In essence,
    we could have workers sampling the trajectory at many different points, as shown
    in the following diagram:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 同步actor-critic工人通过基本上提供更多的采样变体来提供训练优势，这应该反过来减少预期错误的数量，从而提高训练性能。从数学上讲，我们只是在提供更大的采样空间，正如任何统计学家都会告诉你的，这只会减少采样误差。然而，如果我们假设每个工人都是异步的，这意味着它在自己的时间更新全局模型，这也为我们提供了在整个轨迹空间采样中的更多统计变异性。这也可以在采样空间的同时发生。本质上，我们可以在许多不同的点上让工人采样轨迹，如下面的图所示：
- en: '![](img/e93cbeb7-e14c-4ab5-8adc-9d04b8215ff7.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e93cbeb7-e14c-4ab5-8adc-9d04b8215ff7.png)'
- en: Multiple workers sampling across a trajectory space
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在轨迹空间中进行多次工人采样
- en: 'With A3C and asynchronous actor-critic workers, we can get a much better picture
    of the entire trajectory space sooner, which allows our agent to make clearer
    and better decisions. It does this by sampling across the trajectory space asynchronously
    with multiple workers. Let''s see how this works by opening up `Chapter_9_A3C.py`
    and performing the following steps:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 使用A3C和异步actor-critic工人，我们可以更快地获得整个轨迹空间的更清晰图景，这允许我们的代理做出更清晰、更好的决策。它是通过使用多个工人在轨迹空间中进行异步采样来做到这一点的。让我们通过打开`Chapter_9_A3C.py`并执行以下步骤来了解这是如何工作的：
- en: 'We will start by looking at typical hyperparameters and setup code, as follows:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从查看典型的超参数和设置代码开始，如下所示：
- en: '[PRE24]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Here, we can see the inclusion of two new hyperparameters, `max_train_ep` and
    `max_test_ep`. The first variable, `max_train_ep`, sets the maximum number of
    training episodes, while the second variable, `max_text_ep`, is used to evaluate
    performance.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到包含两个新的超参数，`max_train_ep`和`max_test_ep`。第一个变量`max_train_ep`设置了最大训练剧集数，而第二个变量`max_test_ep`用于评估性能。
- en: 'The next section is the `ActorCritic` class and is identical to our previous
    couple of examples, so we won''t need to review it here. After that is the `train`
    function, as shown here:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个部分是`ActorCritic`类，与我们之前的几个例子完全相同，所以在这里我们不需要回顾它。之后是`train`函数，如下所示：
- en: '[PRE25]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The `train` function is quite similar to our previous training code. However,
    notice how we are passing in a `global_model` input. This global model is used
    as the clone for the local model, which we then train on experiences learned by
    the worker agent. One of the keys things to observe about this code is the last
    section, which is where we update the global model using the local model we have
    been training independently.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`train`函数与我们之前的训练代码非常相似。然而，请注意我们传递了一个`global_model`输入。这个全局模型被用作本地模型的克隆，然后我们在工人代理学习到的经验上进行训练。关于这段代码的一个关键观察点是最后部分，这是我们使用独立训练的本地模型更新全局模型的地方。'
- en: 'Next comes the test function. This is where `global_model` is evaluated using
    the following code:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是测试函数。这是使用以下代码评估`global_model`的地方：
- en: '[PRE26]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: All this code does is evaluate the model by using it to play the game and evaluate
    the score. This would certainly be a great place to render the environment for
    visuals while training.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有这些代码所做的只是通过使用它来玩游戏和评估分数来评估模型。这当然是在训练时渲染环境的绝佳位置。
- en: 'Finally, we have the main block of processing code. This block of code is identified
    with the `name` if statement, as follows:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们有主要的处理代码块。这个代码块通过以下`name`if语句被标识：
- en: '[PRE27]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As we can see, this is where the `global_model` model is constructed with shared
    memory. Then, we start up the subprocesses using the first or rank 0 process as
    the test or evaluation process. Finally, we can see that the code ends when we
    join back up all the processes with `p.join`.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如我们所见，这是使用共享内存构建`global_model`模型的地方。然后，我们使用第一个或排名0的进程作为测试或评估进程来启动子进程。最后，我们可以看到代码在所有进程通过`p.join`重新连接时结束。
- en: 'Run the code as you normally would and take a look at the results, an example
    of which is as follows:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照常规运行代码并查看结果，以下是一个示例：
- en: '![](img/29d7c553-5d1d-4ee9-9d29-f4b95612abaf.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29d7c553-5d1d-4ee9-9d29-f4b95612abaf.png)'
- en: Example output from Chapter_9_A3C.py
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 来自Chapter_9_A3C.py的示例输出
- en: Building actor-critic with experience replay
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于经验回放构建actor-critic
- en: We have come to a point in this book where we have learned about all the major
    concepts of DRL. There will be more tools we will throw at you in later chapters,
    such as the one we showed in this section, but if you have made it this far, you
    should consider yourself knowledgeable of DRL. As such, consider building your
    own tools or enhancements to DRL, not unlike the one we'll show in this section.
    If you are wondering if you need to have the math worked out first, then the answer
    is no. It can often be more intuitive to build these models in code first and
    then understand the math later.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们已经学到了DRL的所有主要概念。在后面的章节中，我们还将向您展示更多工具，就像我们在这个部分展示的那样，但如果您已经走到这一步，您应该认为自己对DRL有了一定的了解。因此，考虑构建自己的工具或对DRL进行增强，就像我们在这个部分展示的那样。如果您想知道是否需要先解决数学问题，那么答案是无需。通常，首先在代码中构建这些模型，然后理解数学会更直观。
- en: Actor-critic with experience replay (ACER) provides another advantage by adjusting
    sampling based on past experiences. This concept was originally introduced by
    DeepMind in a paper titled *Sample Efficient Actor-Critic with Experience Replay* and
    developed the concept for ACER. The intuition behind ACER is that we develop dual
    dueling stochastic networks in order to reduce the bias and variance and update
    the trust regions we select in PPO. In the next exercise, we'll explore actor-critic
    combined with experience replay.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 基于经验回放的actor-critic（ACER）通过根据以往经验调整采样提供了另一个优势。这个概念最初由DeepMind在一篇题为《Sample Efficient
    Actor-Critic with Experience Replay》的论文中提出，并开发了ACER的概念。ACER背后的直觉是我们开发双打随机网络来减少偏差和方差，并更新我们在PPO中选择的信任区域。在下一个练习中，我们将探索结合经验回放的actor-critic。
- en: The math behind ACER is best understood by reviewing the aforementioned paper
    or searching for blog posts. It is strongly suggested that you understand the
    math behind TRPO and PPO first, before tackling the paper.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 通过回顾上述论文或搜索博客文章，可以最好地理解ACER背后的数学。强烈建议您在处理论文之前，首先理解TRPO和PPO背后的数学。
- en: 'Open `Chapter_9_ACER.py` and follow these steps to complete this exercise:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 打开`Chapter_9_ACER.py`并按照以下步骤完成这个练习：
- en: 'The first thing you will notice in this example is the `ReplayBuffer` class,
    as shown here:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个示例中，你首先会注意到的是`ReplayBuffer`类，如下所示：
- en: '[PRE28]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This is an updated version of the `ReplayBuffer` class we looked at in previous
    chapters.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是我们在前几章中看到的`ReplayBuffer`类的更新版本。
- en: 'The bulk of the code should be self-explanatory, aside from new sections in
    the `train` function, starting with the first few blocks of code:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了`train`函数中的新部分之外，大部分代码应该是自解释的，首先是代码的前几个块：
- en: '[PRE29]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The new code is the calculation of `rho` from taking the ratio of `pi` over
    the action probability, `prob`. Then, the code gathers the tensor to 1, clamps
    it, and calculates a correction coefficient called `correction_coeff.`
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 新代码是从动作概率`prob`除以`pi`的比率来计算`rho`，然后代码将张量聚合成1，将其夹具，并计算一个称为`correction_coeff`的校正系数。
- en: 'Scrolling past some of the other familiar code, we come to a new section where
    the calculation of loss has been updated with the values `rho_bar` and `correction_coeff`,
    as follows:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚过一些其他熟悉的代码，我们来到了一个新部分，其中损失的计算已经更新为使用`rho_bar`和`correction_coeff`的值，如下所示：
- en: '[PRE30]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Here, we can see that the inverse of `rho_bar` and `correction_coeff` are both
    used to skew the calculations of loss. `rho`, the original value we used to calculate
    these coefficients from, is based on the ratio between previous actions and predicted
    actions. The effect that's produced by applying this bias is narrowing the search
    along the trajectory path. This is a very good thing when it's applied to continuous
    control tasks.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到`rho_bar`和`correction_coeff`的倒数都被用来偏斜损失的计算。`rho`是我们用来计算这些系数的原始值，它基于先前动作和预测动作之间的比率。应用这种偏差产生的效果是缩小沿着轨迹路径的搜索范围。当应用于连续控制任务时，这是一个非常好的效果。
- en: 'Finally, let''s skip to the training loop code and see where the data is appended
    to `ReplayBuffer`:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们跳到训练循环代码，看看数据是如何附加到`ReplayBuffer`的：
- en: '[PRE31]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: What we can see here is that the action probability, `prob`, is entered by detaching
    from the PyTorch tensor using `detach()` and then converting it into a `numpy`
    tensor. This value is what we use to calculate `rho` later in the `train_net`
    function.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里我们可以看到，动作概率`prob`是通过使用`detach()`从PyTorch张量中分离出来，然后将其转换为`numpy`张量来输入的。这个值是我们后来在`train_net`函数中用来计算`rho`的。
- en: 'Run the code as you normally would and observe the output, an example of which
    is as follows:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照正常方式运行代码并观察输出，以下是一个示例：
- en: '![](img/acbb32ad-94ac-4bf2-ab02-ee54cc8b4600.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acbb32ad-94ac-4bf2-ab02-ee54cc8b4600.png)'
- en: Example output from Chapter_9_ACER.py
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 来自`Chapter_9_ACER.py`的示例输出
- en: Here, we can see how the buffer size increases and that the agent becomes smarter.
    This is because we are using those experiences in the replay buffer to adjust
    the understanding of bias and variance from the policy distribution, which, in
    turn, reduces the size or clipping area of the trust regions we use. As we can
    see from this exercise, which is the most impressive one in this chapter, it does
    indeed learn the environment in a more convergent manner than our previous attempts
    in this chapter.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到缓冲区大小如何增加，以及智能体如何变得更聪明。这是因为我们正在使用这些经验在重放缓冲区中调整从策略分布中理解偏差和方差的理解，这反过来又减少了我们使用的信任区域的大小或裁剪区域。正如我们从这个练习中可以看到的，这是本章中最令人印象深刻的练习之一，它确实以比我们本章之前的尝试更收敛的方式学习了环境。
- en: That's all for optimizing PG methods. In the next section, we'll look at some
    exercises that you can carry out on your own.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 关于优化PG方法的内容就到这里。在下一节中，我们将探讨一些你可以自己进行的练习。
- en: Exercises
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'The exercises in this section are for you to explore on your own. Substantially
    advancing any of the techniques we cover from this point forward is an accomplishment,
    so the work you do here could morph into something beyond just learning. Indeed,
    the environments and examples you work on now will likely indicate your working
    preference going forward. As always, try to complete two to three of the following
    exercises:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的练习是为了让你自己探索。从现在开始，显著提高我们涵盖的任何技术都是一个成就，所以你在这里所做的可能不仅仅是学习。确实，你现在工作的环境和示例可能会表明你未来的工作偏好。一如既往，尝试完成以下练习中的两到三个：
- en: Tune the hyperparameters for `Chapter_9_PPO.py` and/or `Chapter_9_PPO_LSTM.py`.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整`Chapter_9_PPO.py`和/或`Chapter_9_PPO_LSTM.py`的超参数。
- en: Tune the hyperparameters for `Chapter_9_A2C.py` and/or `Chapter_9_A3C.py`.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整`Chapter_9_A2C.py`和/或`Chapter_9_A3C.py`的超参数。
- en: Tune the hyperparameters for `Chapter_9_ACER.py`.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整`Chapter_9_ACER.py`的超参数。
- en: Apply LSTM layers to the A2C and/or A3C examples.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将LSTM层应用于A2C和/或A3C示例。
- en: Apply LSTM layers to the ACER example.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将LSTM层应用于ACER示例。
- en: Add a `play_game` function to the A2C and/or A3C examples.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`play_game`函数添加到A2C和/或A3C示例中。
- en: Add a `play_game` function to the ACER example.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`play_game`函数添加到ACER示例中。
- en: Adjust the buffer size in the ACER example and see how that improves training.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在ACER示例中调整缓冲区大小，看看这如何改善训练。
- en: Add synchronous and/or asynchronous workers to the ACER example.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将同步和/或异步工作者添加到ACER示例中。
- en: Add the ability to output results to matplot or TensorBoard. This is quite advanced
    but is something we will cover in later chapters.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加将结果输出到matplot或TensorBoard的功能。这相当高级，但这是我们将在后面的章节中介绍的内容。
- en: These exercises are intended to reinforce what we learned in this chapter. In
    the next section, we will summarize this chapter.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习旨在巩固我们在本章中学到的内容。在下一节中，我们将总结本章内容。
- en: Summary
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how PG methods are not without their own faults
    and looked at ways to fix or correct them. This led us to explore more implementation
    methods that improved sampling efficiency and optimized the objective or clipped
    gradient function. We did this by looking at the PPO method, which uses clipped
    objective functions to optimize the region of trust we use to calculate the gradient.
    After that, we looked at adding a new network layer configuration to understand
    the context in state.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了PG方法并非没有自己的缺陷，并探讨了修复或纠正它们的方法。这使我们探索了更多提高采样效率和优化目标或裁剪梯度函数的实现方法。我们通过查看使用裁剪目标函数来优化我们用来计算梯度的信任区域的PPO方法来实现这一点。在那之后，我们查看添加新的网络层配置来理解状态中的上下文。
- en: Then, we used the new layer type, an LSTM layer, on top of PPO to see the improvements
    it generated. Then, we looked at improving sampling using parallel environments
    and synchronous or asynchronous workers. We did this by implementing synchronous
    workers by building an A2C example, followed by looking at an example of using
    asynchronous workers on A3C. We finished this chapter by looking at another improvement
    we can make to sampling efficiency through the use of ACER, or actor-critic with
    experience replay.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在PPO之上使用了新的层类型，即LSTM层，以观察它带来的改进。接着，我们研究了通过并行环境和同步或异步工作者来提高采样效率的方法。我们通过构建一个A2C示例来实现同步工作者，然后查看在A3C上使用异步工作者的示例。我们通过探讨使用ACER（经验回放的动作-评论家）来提高采样效率的另一种改进方法来结束这一章。
- en: In the next chapter, we'll improve upon our knowledge by looking at different
    methods that are more applicable to gaming. PG methods have been shown to be very
    successful in gaming tasks, but in the next chapter we'll go back to DQN and see
    how it can be made state-of-the-art with varying improvements.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过探讨更多适用于游戏的不同方法来提升我们的知识。PG方法在游戏任务中已被证明非常成功，但在下一章中，我们将回到DQN，看看它如何通过不断改进达到最先进水平。
