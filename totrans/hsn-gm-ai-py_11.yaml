- en: Optimizing for Continuous Control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, we have considered most of the training/challenge environments
    we've looked at as being episodic; that is, the game or environment has a beginning
    and an end. This is good since most games have a beginning and an end – it is,
    after all, a game. However, in the real world, or for some games, an episode could
    last days, weeks, months, or even years. For these types of environment, we no
    longer think of an episode; rather we work with the concept of an environment
    that requires continuous control. So far, we have looked at a subset of algorithms
    that can solve this type of problem but they don't do so very well. So, like most
    things in RL, we have a special class of algorithms devoted to those types of
    environment, and we'll explore them in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll look at improving the policy methods we looked at previously
    for performing continuous control of advanced environments. We'll start off by
    setting up and installing the Mujoco environment, a specialized area we can use
    to test these new algorithms, the first of which will be the proximal policy optimization
    or PPO method. After that, we'll look at a novel improvement called recurrent
    networks for capturing context and learn how that is applied on top of PPO. Then,
    we'll get back into actor-critic and this time look at asynchronous actor-critic
    in a couple of different configurations. Finally, we'll look at ACER and actor-critic
    with experience replay.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a summary of the main topics we will cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding continuous control with Mujoco
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing proximal policy optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using PPO with recurrent networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deciding on synchronous and asynchronous actors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building actor-critic with experience replay
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we'll look at a class of RL methods that attempts to deal specifically
    with real-world problems of robotics or other control systems. Of course, this
    doesn't mean these same algorithms couldn't be used in gaming – they are. In the
    next section, we'll begin by looking at the specialized Mujoco environment.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding continuous control with Mujoco
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The standard environment for building continuous control agents is the Mujoco
    environment. **Mujoco** stands for **Multi-Joint dynamics with Contract** and
    it is a full physics environment for training robotic or simulation agents. This
    environment provides a number of simulations that challenge some form of robotic
    control agent to perform a task, such as walking, crawling, and implementing several
    other physics control-based tasks. An example of the diversity of these environments
    is summarized well in the following image, which has been extracted from the Mujoco
    home page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48271473-8094-4df7-b3fd-fccd2982a72d.png)'
  prefs: []
  type: TYPE_IMG
- en: Extract of example environments from the Mujoco home page
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, we will want to use this cool environment. However, this package
    is not free and requires a license, but note that a 30-day trial is provided.
    Now for the bad news. The package is extremely difficult to set up, install, and
    train, especially if you are using Windows. In fact, it is so difficult that,
    although we strongly suggest using Mujoco as an environment, we won't be using
    it for the remaining exercises in this chapter. Why? Again, it is extremely difficult
    and we don't want to exclude people who are unable to install the Mujoco environment.
  prefs: []
  type: TYPE_NORMAL
- en: There are plenty of blog posts or Stack Overflow articles available that walk
    through various installations of the various versions of Mujoco for Windows. Mujoco's
    support for Windows was stopped after version 1.5\. While it is still possible
    to install Mujoco on Windows, it is not trivial and is likely to change often.
    As such, if you are inclined to use Windows with Mujoco, your best bet is to look
    to the most recent blogs or forum posts discussing this for help.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this exercise, we''ll walk through the basic installation of Mujoco (not
    for Windows):'
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we need is a license. Open your browser, go to [mujoco.org](http://mujoco.org),
    and locate the License button at the top of the page. Then, click it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the page, you will see an entry for **Computer id**. This will require you
    to download a key generator from the blue links shown to the right. Click one
    of the links to download the key generator.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the key generator on your system and enter the key in the **Computer i****d**
    field.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fill in the rest of the license information with your name and email and click
    **Submit**, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/674b587e-a965-42ca-ad66-7f09e85416fa.png)'
  prefs: []
  type: TYPE_IMG
- en: Submitting for a Mujoco license
  prefs: []
  type: TYPE_NORMAL
- en: You should get a key emailed to you in a few minutes with directions as to where
    to put the key. Then, you need to download the binaries for your platform. Click
    on the Products link at the top of the page to be taken to the downloads. Download
    the version you need for your OS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unzip the files into your root user folder, `~/.mujoco/mujoco%version%`, where
    `%version%` denotes the version of the software. On Windows, your user folder
    is `C:\Users\%username%`, where `%username%` denotes the logged in user's name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, you need to build the Mujoco package and set up the `mujoco-py` scripts.
    This varies widely by installation. Use the following commands to build and install
    Mujoco:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To test the installation and check for dependencies, run the following command
    to reinstall the entire Gym again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you run this command and still see errors, you likely need more help. Consult
    online resources for the most current search on `mujoco install` and try those
    instructions. Again, at the time of writing, Windows is no longer supported and
    you may be better off using another platform. Fortunately, setting up a VM or
    Cloud service for this can now be quite easy and you may have more luck there.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can test the Mujoco installation and ensure that the license is all set
    up by running `Chapter_9_Mujoco.py` as you normally would. The listing is shown
    here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If you have everything installed correctly, then you should see something similar
    to the following image, which has been taken from the Mujoco environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab253b9c-0b6d-47d0-8e1d-4f1a38b3ea18.png)'
  prefs: []
  type: TYPE_IMG
- en: The fetch reach Mujoco environment
  prefs: []
  type: TYPE_NORMAL
- en: If you are able to install the Mujoco environments, then great – have fun exploring
    a whole new world of environments. For those of you who are not able to install
    Mujoco, don't fret. We will learn how to create our own physics-based environments
    when we start using Unity in [Chapter 10](ab9a7f4f-60d8-4643-8627-199cf95bcf55.xhtml),
    *Exploiting ML-Agents*. Rest assured that, while Mujoco is indeed cool, much like
    the Atari games we have seen before, it is also not trivial to train. Not unlike
    Atari, Mujoco environments can take millions of training iterations. Therefore,
    to keep things simple and also remain energy-conscious, we will use the regular
    old Gym environments. The additional plus now is that we have a better comparison
    between various algorithms across a single environment.
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep reinforcement learning** (**DRL**) and **machine learning** (**ML**)
    in general are getting a bit of a bad name due to the additional energy they consume.
    Many state-of-the-art DRL models can be measured in terms of energy consumption
    and, in most cases, the amount of energy is quite high. In one case, DeepMind
    has admitted that the amount of processing/energy it used to train a single model
    would run a single desktop computer for 45 years. That is an incredible amount
    of energy in a world that needs to be cautious about energy consumption. Therefore,
    wherever applicable, in this book we will favor cheaper training environments.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we look at advancing these policy methods with gradient
    optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing proximal policy optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are now entering areas where we will start looking at state-of-the-art algorithms,
    at least at the time of writing. Of course, that will likely change and things
    will advance. For now, though, the **proximal policy optimization** algorithm
    (**PPO**), was introduced by OpenAI, is considered a state-of-the-art deep reinforcement
    learning algorithm. As such, the sky is the limit as to what environments we can
    throw at this problem. However, in order to quantify our progress and for a variety
    of other reasons, we will continue to baseline against the Lunar Lander environment.
  prefs: []
  type: TYPE_NORMAL
- en: The PPO algorithm is just an extension and simplification of the **trust region
    policy optimization** (**TRPO**) algorithm we covered in [Chapter 8](42626cbd-87b8-428c-8f2a-ecc06f5e387c.xhtml),
    *Policy Gradient Methods*, but with a few key differences. PPO is also much simpler
    to understand and follow. For these reasons, we will review each feature that
    makes policy optimization with trust regions in the case of TRPO and clipping
    with PPO so powerful.
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter was originally sourced from the following repository: [https://github.com/seungeunrho/minimalRL](https://github.com/seungeunrho/minimalRL).
    A number of modifications have been made to the code so that it fits the examples
    in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we have already gone over the major intuition behind this improvement,
    let''s jump into the next coding exercise by opening `Chapter_9_PPO.py`. Perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this listing is quite similar to many of the other listings we
    have reviewed. As such, we will limit our review to critical sections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Scrolling right to the bottom, we can see that the training code is almost
    identical to our most recent examples in the previous chapters. One key thing
    to notice is the inclusion of a new hyperparameter, `T_horizon`, which we will
    define shortly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If we scroll back to the top, you will see the definition of new hyperparameters
    for `T_horizon`, `K_epoch`, `eps_clip`, and `lambda`. Just note these new variables
    for now – we will get to their purpose shortly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s jump to some of the other important differences, such as the network
    definition, which can be seen in the `init` method of the `PPO` class as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: What we can see is that the network comprises a first input state `Linear` layer
    called `fc1` that is composed of 256 neurons. Then, we can see that the `fc_pi`
    or policy network is defined as `Linear` with 256 neurons and outputs the `num_actions`
    or number of actions. Following that is the definition of `fc_v`, which is the
    value layer. Again, this has 256 neurons and one output, that is, the expected
    value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The rest of the code for the PPO class is almost the same as in the previous
    examples and we won't need to cover it here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the code as normal. This example will take a while to run but not as long
    as previous versions. We'll leave it up to you whether you want to wait for the
    example to complete before continuing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One thing you should quickly notice is how much faster the algorithm trains.
    Indeed, the agent gets good quite fast and could actually solve the environment
    in fewer than 10,000 iterations, which is quite impressive. Now that we have seen
    how impressive policy optimization can be, we will look at how this is possible
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The hows of policy optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 8](42626cbd-87b8-428c-8f2a-ecc06f5e387c.xhtml), *Policy Gradient
    Methods*, we covered how policy gradient methods can fail and then introduced
    the TRPO method. Here, we talked about the general strategies TRPO uses to address
    the failings in PG methods. However, as we have seen, TRPO is quite complex and
    seeing how it works in code was not much help either. This is the main reason
    we minimized our discussion of the details when we introduced TRPO and instead
    waited until we got to this section to tell
  prefs: []
  type: TYPE_NORMAL
- en: the full story in a concise manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, let''s review how policy optimization with TRPO or PPO can do what
    it does:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Minorize-Maximization MM algorithm**: Recall that this is where we find the
    minimum of an upper bound function by finding the maximum of a lower bound function
    that is constrained to be within the upper bound function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Line search**: We have seen this being used to define in which direction
    and by what amount we could optimize our function (deep learning network). This
    allows our algorithm to avoid overshooting the target of optimization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trust region**: Along with MM and Line Search, we also want the policy function
    to have a stable base or platform to move along on. You can think of this stable
    base as a region of trust or safety. In PPO, this is defined differently, as we
    will see.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PPO and TRPO share all these improvements as a way of finding a better policy.
    PPO improves on this by also understanding how much we want to change the policy's
    distribution over each iteration. This understanding also allows us to limit the
    amount of change during each iteration. We have seen how TRPO does this to a certain
    extent with KL divergence, but PPO takes this one step further by adjusting or
    adapting to the amount of change. In the next section, we'll look at how this
    adaptation works.
  prefs: []
  type: TYPE_NORMAL
- en: PPO and clipped objectives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we get into the finer details of how PPO works, we need to step back
    and understand how we equate the difference in distributed data distributions
    or just distributions. Remember that PG methods look to understand the returns-based
    sampling distribution and then use that to find the optimum action or the probability
    of the optimum action. Due to this, we can use a method called **KL Divergence**
    to determine how different the two distributions are. By understanding this, we
    can determine how much room or area of trust we can allow our optimization algorithm
    to explore with. PPO improves on this by clipping the objective function by using
    two policy networks.
  prefs: []
  type: TYPE_NORMAL
- en: Jonathan Hui has a number of insightful blog posts on the mathematics behind
    various RL and PG methods. In particular, his post on PPO ([https://medium.com/@jonathan_hui/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12](https://medium.com/@jonathan_hui/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12is))
    is quite good. Be warned that they do assume a very sophisticated level of mathematics
    knowledge. If you are serious about RL, you will want to be able to read and understand
    this content at some point. However, you can get quite far in DRL by intuitively
    understanding most algorithms, like we're doing here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s learn how this works in code by opening up `Chapter_9_PPO.py` and performing
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Having looked through the bulk of the main code, we only want to focus on the
    training code here and, in particular, the `train_net` function from the `PPO`
    class, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'After the initial `make_batch` function call, in order to build the lists,
    we come to the iteration loop controlled by `K_epoch`. `K_epoch` is a new hyperparameter
    that controls the number of iterations we use to optimize the advantage convergence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The first block of code inside the `K_epoch` iteration is the calculation of `td_target`
    using the reward `r`, plus the discount factor `gamma` times the output of the
    v or value network and `done_mask`. Then, we take the `delta` or TD change and
    convert it into a `numpy` tensor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, using the delta, we build a list of advantages using the `advantage`
    function, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we push the state **s** into the policy network, `pi`. Next, we gather
    the axis along the first dimension and then take the ratio using the equation ![](img/4c9bd681-0b83-473f-939e-9f9ec792ad3c.png),
    which is used to calculate a possible ratio for the clipping region or area we
    want to use for trust:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We use the `ratio` value to calculate the `surr1` value, which defines the surface
    or clipping region. The next line calculates a second version of the surface, `surr2`,
    by clamping this ratio and using the bounds of our clipping region set by `eps_clip`
    to define the area. Then, it takes the minimum area of either surface and uses
    that to calculate the loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use the term `surface` here to understand that the calculation of loss is
    over a multi-dimensional array of values. Our optimizer works across this surface
    to find the best global minimum or lowest area on the surface.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last section of code is our typical gradient descent optimization and is
    shown next for completeness:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'There''s nothing new here. Go ahead and run the sample again or review the
    output from the previous exercise. An example of the training output is shown
    here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/801102d6-d763-47f2-a7c2-2562ab1d850d.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output from Chapter_9_PPO.py
  prefs: []
  type: TYPE_NORMAL
- en: A couple of things to note is that we are stilling using discrete action spaces
    and not continuous ones. Again, our primary reason for doing this is to continue
    using a baseline-consistent environment, Lunar Lander v2\. Lunar Lander does have
    a continuous action environment that you can try, but you will need to convert
    the sample so that you can use continuous actions. On the second item of note,
    PPO and other PG methods actually perform better on continuous action environments,
    which means you aren't really seeing their full potential. So, why are we continuing
    to use discrete action spaces? Well, in almost all cases, games and interactive
    environments will use discrete spaces. Since this is a book on games and AI and
    not robotics and AI, we will stick to discrete spaces.
  prefs: []
  type: TYPE_NORMAL
- en: There are various research initiatives on other PG methods but you should consider
    PPO to be a milestone in DRL, not unlike DQN. For those of you who are curious,
    PPO made its name by beating human players in the DOTA2 strategy game. In the
    next section, we'll look at other methods that have been layered on top of PG
    and other methods to improve DRL.
  prefs: []
  type: TYPE_NORMAL
- en: Using PPO with recurrent networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 7](42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml), *Going Deeper with
    DDQN*, we saw how we could interpret visual state using a concept called **convolutional
    neural networks** (**CNNs**). CNN networks are used to detect features in visual
    environments such as Atari games. While this technique allowed us to play any
    of a number of games with the same agent, the added CNN layers took much more
    time to train. In the end, the extra training time wasn't worth the cool factor
    of playing Atari games. However, there are other network structures we can put
    on top of our networks in order to make better interpretations of state. One such
    network structure is called recurrent networks. Recurrent network layers allow
    us to add the concept of context or time in our model's interpretation of state.
    This can work very well in any problem where context or memory is important.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent network layers are a form of deep learning perceptron that essentially
    feeds back its state to previous neurons. This, in effect, gives the network the
    ability to understand time or context. It does this so well that recurrent networks
    are now at the heart of all text and language processing networks. Language is
    especially contextual and recurrent layers, in various configurations, make short
    work of understanding the context. There are various configurations of recurrent
    network layers but the only one we'll focus on here is called **long short term
    memory** (**LSTM**).
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent networks and LSTM layers are worthy of in-depth study on their own.
    These powerful network layers have been responsible for some very interesting
    discoveries in the last few years. While recurrent layers have turned out to have
    limited use in DRL, it is believed they should have more. After all, understanding
    context in a trajectory must be important.
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTM layers for the purposes of DRL are quite simple to put in place. Open
    `Chapter_9_PPO_LSTM.py` and follow these steps to see how this works:'
  prefs: []
  type: TYPE_NORMAL
- en: This sample is virtually identical to `Chapter_9_PPO.py` but with the few key
    differences, all of which we will look at here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Skip to the `PPO` class definition, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The only new part here is the definition of a new layer, `lstm`, which is of
    the `LSTM(64.32)` type. The LSTM layer that's injected at the top of the state
    encoding allows the network to learn the context in actions or memory. Now, instead
    of understanding which state-actions provide the best trajectory, our agent is
    learning which state-action sets are providing the best outcome. In gaming, this
    may be analogous to learning that a special move unlocks a sequence to get some
    special reward.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we will move down to the policy pi function and value v function network
    definitions and look at how it has been modified:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `pi` and `v` functions take a hidden layer but only `pi`, or the policy
    function, is used as the output of a hidden layer. We will see how these hidden
    LSTM layers work shortly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, at the top of the `train_net` function, we can see where the layers are
    extracted from the batching process, `make_batch`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We use two hidden or middle LSTM layers between our actor-critics, where `second_hidden`
    denotes the output and `first_hidden` denotes the input. Below that, in the `for`
    loop, we can see the calculation of delta using the LSTM input and output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The calculation of delta here is done by applying the difference between the
    before and after the LSTM layer was applied, which allows the delta to encapsulate
    the effect the LSTM has on the calculation of value `v`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the sample as you normally would and observe the output, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/87ea8d62-2ae5-4df4-9978-d56fcbbfc35c.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output of Chapter_9_PPO_LSTM.py
  prefs: []
  type: TYPE_NORMAL
- en: Notice how this slight improvement increased training performance significantly
    from the vanilla PPO example we looked at in the previous exercise. In the next
    section, we'll improve PPO further by applying parallel environments.
  prefs: []
  type: TYPE_NORMAL
- en: Deciding on synchronous and asynchronous actors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We started off this book with a simple discussion of what **artificial general
    intelligence** (**AGI**) is. In short, AGI is our attempt at generalizing an intelligent
    system to solve multiple tasks. RL is often thought of as a stepping stool to
    AGI primarily because it tries to generalize state-based learning. While both
    RL and AGI take deep inspiration from how we think, be it rewards or possibly
    consciousness itself, the former tends to incorporate direct analogies. The actor-critic
    concept in RL is an excellent example of how we use an interpretation of human
    psychology to create a form of learning. For instance, we humans often consider
    the consequences of our actions and determine the advantages they may or may not
    give us. This example is perfectly analogous to actor-critic and advantage methods
    we use in RL. Take this further and we can consider another human thought process:
    asynchronous and synchronous thought.'
  prefs: []
  type: TYPE_NORMAL
- en: A direct example of asynchronous/synchronous thought is when an answer to a
    problem **pops** into your head after being asked a question several hours earlier.
    Perhaps you didn't have the answer then but then it came to you a few hours later.
    Were you thinking about the question all that time? Not likely, and more than
    likely the answer just came to you – it popped into your head. But were you thinking
    of it all the time in some background process or did some process just fire up
    and provide the answer? The animal brain thinks like this all the time and it
    is what we would call in computerese parallel processing. So, could our agents
    not also benefit from this same thought process? As it turns out, yes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This inspiration likely came in part from the preceding analogy but also has
    a mathematical background in how we can evaluate advantage. The direct evaluation
    of how our brains think is still a big answer but we can assume our thoughts to
    be synchronous or asynchronous. So, instead of just considering one thought process,
    what if we consider several? We can take this analogy a step further and apply
    this back to DRL – in particular, actor-critic. Here, we have a single thought
    process or global network that is fed the output of several worker thought processes.
    An example of this is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eeb6bb78-7555-424c-8ed4-da6407ecc1ab.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of asynchronous AC by Dr. Arthur Juliani
  prefs: []
  type: TYPE_NORMAL
- en: What we can see here is the basic intuition behind the advantage of the actor-critic
    architecture, A2C, and the asynchronous advantage actor-critic architecture, A3C.
    Notice how each worker brain/agent has its own copy of a separate environment.
    All of these worker agents feed their learning into a master brain. Each worker
    brain is then updated iteratively to coincide with the master, not unlike our
    earlier advantage calculations. In the next section, we will see how this is put
    into practice by implementing A2C.
  prefs: []
  type: TYPE_NORMAL
- en: Using A2C
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To avoid any confusion, it is important to understand that A2C and A3C both
    use AC, but it is the fashion in which they update their models that differ. In
    A2C, the method is synchronous, so each brain is feeding thoughts into the main
    brain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how this looks in code by opening the `Chapter_9_A2C.py` file and
    reviewing the hyperparameters inside it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Keep the sample open and follow these steps to continue with this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a large code example, so we will limit the sections we show here. The
    main thing of note here is the hyperparameters that are listed at the top of the
    file. The only thing new to note is `n_train_processes`, which sets the number
    of worker processes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next comes the `ActorCritic` class, which is the same class that we used previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then comes the definition of the `worker` function. This function is where
    the worker''s brain sends messages between the worker and master brains:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'After those functions is the big `ParallelEnv` class. The preceding code just
    shows the `init` function from that class since it''s quite large. This class
    merely coordinates activities between the masters and workers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Scrolling down past the `test` function, or the `play_game` function in our
    other examples, we can see the `compute_target` function. This is the calculation
    of the TD loss and the difference here is the use of the `mask` variable. `mask` is
    just a flag or filter that removes any calculation of discounted G on 0 returns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we get into an `if` function, which determines whether the current
    process is `''__main__''`. We do this to avoid having additional worker processes
    trying to run this same block of code. After that, we can see the typical environment
    and model setup being completed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The interval training loop code is virtually identical to those from the previous
    examples and should be self-explanatory at this point, for the most part. Something
    to note is the `env.steps` function call. This represents a step in all the worker
    environments, synchronously. Remember that the worker agents are running synchronously
    in A2C:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Then, we come to the outer training loop. In this example, we can see how the
    training targets are pulled from the lists constructed by the workers, where `s_lst` is
    for the state, `a_lst` is for actions, `r_lst` is for rewards, and `mask_lst` is
    for done. Aside from torch tensor manipulation, the calculations are the same
    as in PPO.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the code as you normally would and visualize the output, an example of
    which is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/5d2c9c6d-7807-41c4-8045-167b37dff337.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output from Chapter_9_A2C.py
  prefs: []
  type: TYPE_NORMAL
- en: You will need to tune the hyperparameters to get this example to run perfectly.
    Now, we will move on and look at the asynchronous version of A2C—A3C.
  prefs: []
  type: TYPE_NORMAL
- en: Using A3C
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Synchronous actor-critic workers provide a training advantage by essentially
    providing more sampling variations that should, in turn, reduce the amount of
    expected error and thus improve training performance. Mathematically, all we are
    doing is providing a larger sampling space which, as any statistician will tell
    you, just reduces the sampling error. However, if we assume that each worker is
    asynchronous, meaning it updates the global model in its own time, this also provides
    us with more statistical variability in our sampling across the entire trajectory
    space. This can also happen along the sampling space at the same time. In essence,
    we could have workers sampling the trajectory at many different points, as shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e93cbeb7-e14c-4ab5-8adc-9d04b8215ff7.png)'
  prefs: []
  type: TYPE_IMG
- en: Multiple workers sampling across a trajectory space
  prefs: []
  type: TYPE_NORMAL
- en: 'With A3C and asynchronous actor-critic workers, we can get a much better picture
    of the entire trajectory space sooner, which allows our agent to make clearer
    and better decisions. It does this by sampling across the trajectory space asynchronously
    with multiple workers. Let''s see how this works by opening up `Chapter_9_A3C.py`
    and performing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by looking at typical hyperparameters and setup code, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see the inclusion of two new hyperparameters, `max_train_ep` and
    `max_test_ep`. The first variable, `max_train_ep`, sets the maximum number of
    training episodes, while the second variable, `max_text_ep`, is used to evaluate
    performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The next section is the `ActorCritic` class and is identical to our previous
    couple of examples, so we won''t need to review it here. After that is the `train`
    function, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `train` function is quite similar to our previous training code. However,
    notice how we are passing in a `global_model` input. This global model is used
    as the clone for the local model, which we then train on experiences learned by
    the worker agent. One of the keys things to observe about this code is the last
    section, which is where we update the global model using the local model we have
    been training independently.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next comes the test function. This is where `global_model` is evaluated using
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: All this code does is evaluate the model by using it to play the game and evaluate
    the score. This would certainly be a great place to render the environment for
    visuals while training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we have the main block of processing code. This block of code is identified
    with the `name` if statement, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, this is where the `global_model` model is constructed with shared
    memory. Then, we start up the subprocesses using the first or rank 0 process as
    the test or evaluation process. Finally, we can see that the code ends when we
    join back up all the processes with `p.join`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the code as you normally would and take a look at the results, an example
    of which is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/29d7c553-5d1d-4ee9-9d29-f4b95612abaf.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output from Chapter_9_A3C.py
  prefs: []
  type: TYPE_NORMAL
- en: Building actor-critic with experience replay
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have come to a point in this book where we have learned about all the major
    concepts of DRL. There will be more tools we will throw at you in later chapters,
    such as the one we showed in this section, but if you have made it this far, you
    should consider yourself knowledgeable of DRL. As such, consider building your
    own tools or enhancements to DRL, not unlike the one we'll show in this section.
    If you are wondering if you need to have the math worked out first, then the answer
    is no. It can often be more intuitive to build these models in code first and
    then understand the math later.
  prefs: []
  type: TYPE_NORMAL
- en: Actor-critic with experience replay (ACER) provides another advantage by adjusting
    sampling based on past experiences. This concept was originally introduced by
    DeepMind in a paper titled *Sample Efficient Actor-Critic with Experience Replay* and
    developed the concept for ACER. The intuition behind ACER is that we develop dual
    dueling stochastic networks in order to reduce the bias and variance and update
    the trust regions we select in PPO. In the next exercise, we'll explore actor-critic
    combined with experience replay.
  prefs: []
  type: TYPE_NORMAL
- en: The math behind ACER is best understood by reviewing the aforementioned paper
    or searching for blog posts. It is strongly suggested that you understand the
    math behind TRPO and PPO first, before tackling the paper.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open `Chapter_9_ACER.py` and follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing you will notice in this example is the `ReplayBuffer` class,
    as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This is an updated version of the `ReplayBuffer` class we looked at in previous
    chapters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The bulk of the code should be self-explanatory, aside from new sections in
    the `train` function, starting with the first few blocks of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The new code is the calculation of `rho` from taking the ratio of `pi` over
    the action probability, `prob`. Then, the code gathers the tensor to 1, clamps
    it, and calculates a correction coefficient called `correction_coeff.`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scrolling past some of the other familiar code, we come to a new section where
    the calculation of loss has been updated with the values `rho_bar` and `correction_coeff`,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that the inverse of `rho_bar` and `correction_coeff` are both
    used to skew the calculations of loss. `rho`, the original value we used to calculate
    these coefficients from, is based on the ratio between previous actions and predicted
    actions. The effect that's produced by applying this bias is narrowing the search
    along the trajectory path. This is a very good thing when it's applied to continuous
    control tasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, let''s skip to the training loop code and see where the data is appended
    to `ReplayBuffer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: What we can see here is that the action probability, `prob`, is entered by detaching
    from the PyTorch tensor using `detach()` and then converting it into a `numpy`
    tensor. This value is what we use to calculate `rho` later in the `train_net`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the code as you normally would and observe the output, an example of which
    is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/acbb32ad-94ac-4bf2-ab02-ee54cc8b4600.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output from Chapter_9_ACER.py
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see how the buffer size increases and that the agent becomes smarter.
    This is because we are using those experiences in the replay buffer to adjust
    the understanding of bias and variance from the policy distribution, which, in
    turn, reduces the size or clipping area of the trust regions we use. As we can
    see from this exercise, which is the most impressive one in this chapter, it does
    indeed learn the environment in a more convergent manner than our previous attempts
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: That's all for optimizing PG methods. In the next section, we'll look at some
    exercises that you can carry out on your own.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The exercises in this section are for you to explore on your own. Substantially
    advancing any of the techniques we cover from this point forward is an accomplishment,
    so the work you do here could morph into something beyond just learning. Indeed,
    the environments and examples you work on now will likely indicate your working
    preference going forward. As always, try to complete two to three of the following
    exercises:'
  prefs: []
  type: TYPE_NORMAL
- en: Tune the hyperparameters for `Chapter_9_PPO.py` and/or `Chapter_9_PPO_LSTM.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the hyperparameters for `Chapter_9_A2C.py` and/or `Chapter_9_A3C.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the hyperparameters for `Chapter_9_ACER.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply LSTM layers to the A2C and/or A3C examples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply LSTM layers to the ACER example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a `play_game` function to the A2C and/or A3C examples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a `play_game` function to the ACER example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adjust the buffer size in the ACER example and see how that improves training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add synchronous and/or asynchronous workers to the ACER example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the ability to output results to matplot or TensorBoard. This is quite advanced
    but is something we will cover in later chapters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These exercises are intended to reinforce what we learned in this chapter. In
    the next section, we will summarize this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how PG methods are not without their own faults
    and looked at ways to fix or correct them. This led us to explore more implementation
    methods that improved sampling efficiency and optimized the objective or clipped
    gradient function. We did this by looking at the PPO method, which uses clipped
    objective functions to optimize the region of trust we use to calculate the gradient.
    After that, we looked at adding a new network layer configuration to understand
    the context in state.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we used the new layer type, an LSTM layer, on top of PPO to see the improvements
    it generated. Then, we looked at improving sampling using parallel environments
    and synchronous or asynchronous workers. We did this by implementing synchronous
    workers by building an A2C example, followed by looking at an example of using
    asynchronous workers on A3C. We finished this chapter by looking at another improvement
    we can make to sampling efficiency through the use of ACER, or actor-critic with
    experience replay.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll improve upon our knowledge by looking at different
    methods that are more applicable to gaming. PG methods have been shown to be very
    successful in gaming tasks, but in the next chapter we'll go back to DQN and see
    how it can be made state-of-the-art with varying improvements.
  prefs: []
  type: TYPE_NORMAL
