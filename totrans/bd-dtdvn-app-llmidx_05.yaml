- en: <title>Indexing data – a bird’s-eye view</title>
  prefs: []
  type: TYPE_NORMAL
- en: Indexing data – a bird’s-eye view
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We briefly discussed the importance and general functioning of Indexes in a
    RAG application in *Chapter 3* , in the section titled *Uncovering the essential
    building blocks of LlamaIndex – documents, nodes, indexes* . Now, it is time to
    have a closer look at the different indexing methods available in LlamaIndex with
    their advantages, disadvantages, and specific use cases.
  prefs: []
  type: TYPE_NORMAL
- en: In principle, data can be accessed even without an Index. But it’s like reading
    a book without a table of contents. As long as it’s about a story that has continuity
    and can be read sequentially, section by section, and chapter by chapter, reading
    will be a pleasure. However, things change when we need to quickly search for
    a specific topic in that book. Without a table of contents, the search process
    will be slow and cumbersome.
  prefs: []
  type: TYPE_NORMAL
- en: In LlamaIndex, however, **Indexes** represent more than just a simple table
    of contents. An Index provides not only the necessary structure for navigation
    but also the concrete mechanisms to update or access it. That includes the logic
    for the **retrievers** and the mechanisms used for fetching data, which we will
    discuss in detail during *Chapter 6* , *Querying Our Data, Part 1 –* *Context
    Retrieval* .
  prefs: []
  type: TYPE_NORMAL
- en: In this book, I’ve kept things simple, giving you the basics of how Indexes
    work and providing some examples to help you understand their usage. Exploring
    every possible way to use and mix these Indexes would be a huge task and that’s
    not what we’re about here.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll talk later about what makes each type of Index unique, but first, let’s
    see what they all have in common.
  prefs: []
  type: TYPE_NORMAL
- en: Common features of all Index types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Each type of Index in LlamaIndex has its own characteristics and functions,
    but because all of them inherit the `BaseIndex` class, there are certain features
    and parameters they share, which can be customized for any kind of Index:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nodes** : All Indexes are based on nodes and we can choose which Nodes are
    included in the Index. Plus, all Index types provide methods to insert new Nodes
    or delete existing ones, allowing for dynamic updates to the Index as your data
    changes. We can either build an Index from pre-existing Nodes by providing the
    Nodes directly to the Index constructor like this `vector_index = VectorStoreIndex(nodes)`
    or we can provide a list of documents as an input using `from_documents()` and
    let the Index extract the Nodes by itself. Keep in mind that we can use `Settings`
    – before actually building the Index – to customize its underlying mechanics.
    As we discussed during *Chapter 3* in the *Understanding how settings can be used
    for customization* section, this simple class allows for different settings such
    as changing the LLM, embedding model, or default Node parser used by an Index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The storage context** : The storage context defines how and where the data
    (documents and nodes) for the Index is stored. This customization is crucial for
    managing data storage efficiently, depending on the application’s requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Progress display** : The `show_progress` option lets us choose whether to
    display progress bars during long-running operations such as building the Index.
    Implemented using the `tqdm` Python library, this feature can be useful for monitoring
    the progress of large indexing tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Different retrieval modes** : Each Index allows for different pre-defined
    retrieval modes, which can be set to match the specific needs of your application.
    And you can also customize or extend the Retriever classes to change how queries
    are processed and how results are retrieved from the Index. More on that during
    *Chapter 6* , *Querying Our Data, Part 1 –* *Context Retrieval.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Asynchronous operations** : The `use_async` parameter implemented by some
    of the Indexes determines whether certain operations should be performed asynchronously.
    Asynchronous processing allows the system to manage multiple operations concurrently,
    rather than waiting for each operation to be completed sequentially. This can
    be important for performance optimization, especially when dealing with large
    datasets or complex operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quick note
  prefs: []
  type: TYPE_NORMAL
- en: An important thing to consider before diving further and starting to tinker
    with the sample code is that indexing often relies on LLM calls for summarizing
    or embedding purposes. Just like in the case of metadata extraction, which we
    covered in *Chapter 4* , *Ingesting Data into Our RAG Workflow* , indexing in
    LlamaIndex may also raise cost and privacy concerns. Make sure you read the cost-related
    section at the end of this chapter before running any large-scale experiments
    to test your ideas.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with our first and most frequently used Index type.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Understanding the VectorStoreIndex</title>
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the VectorStoreIndex
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: from llama_index.core import VectorStoreIndex, SimpleDirectoryReader documents
    = SimpleDirectoryReader("files").load_data() index = VectorStoreIndex.from_documents(documents)
    print("Index created successfully!") pip install llama-index-embeddings-huggingface
    from llama_index.embeddings.huggingface import HuggingFaceEmbedding embedding_model
    = HuggingFaceEmbedding(     model_name="WhereIsAI/UAE-Large-V1" ) embeddings =
    embedding_model.get_text_embedding(     "The quick brown fox jumps over the lazy
    cat!" ) print(embeddings[:15])
  prefs: []
  type: TYPE_NORMAL
- en: As this ebook edition doesn't have fixed pagination, the page numbers below
    are hyperlinked for reference only, based on the printed edition of this book.
  prefs: []
  type: TYPE_NORMAL
- en: In LlamaIndex, the `VectorStoreIndex` stands out as the workhorse, being the
    most commonly utilized type of Index.
  prefs: []
  type: TYPE_NORMAL
- en: For most RAG applications, a `VectorStoreIndex` might be the best solution because
    it facilitates the construction of Indexes on collections of Documents where **embeddings**
    for the input text chunks are stored within the **Vector Store** of the Index.
    Once constructed, this Index can be used for efficient querying because it allows
    for **similarity searches** over the embedded representations of the text, making
    it highly suitable for applications requiring fast retrieval of relevant information
    from a large collection of data. Don’t worry if you’re not yet familiar with terms
    such as embeddings, vector store, or similarity searching, because we’ll cover
    them in the following sections. The `VectorStoreIndex` class in LlamaIndex supports
    these operations by default and also allows for asynchronous calls and progress
    tracking, which can improve performance and user experience in typical RAG scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: A simple usage example for the VectorStoreIndex
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here’s the most basic way of constructing a `VectorStoreIndex`
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, in just a few lines of code, we have ingested the Documents
    and the `VectorStoreIndex` took care of everything. Note that using this approach,
    we have skipped the Node parsing step entirely, because the Index did that by
    itself by using the `from_documents()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several parameters that we can customize for the `VectorStoreIndex`
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '`use_async` : This parameter enables asynchronous calls. By default, it is
    set to `False` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`show_progress` : This parameter shows progress bars during Index construction.
    The default value is `False` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`store_nodes_override` : This parameter forces LlamaIndex to store Node objects
    in the Index store and document store, even if the vector store keeps text. This
    can be useful in scenarios where you need direct access to Node objects, even
    if their content is already stored in the vector store. We’ll talk in more detail
    about the Index store, document store, and vector store later in this chapter.
    The default setting for this parameter is `False` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s have a look at *Figure 5* *.1* for a visual representation of this type
    of Index:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_05_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – The structure of a VectorStoreIndex
  prefs: []
  type: TYPE_NORMAL
- en: The `VectorStoreIndex` took the ingested Documents, breaking them down into
    Nodes. It used the default parameters for text splitter, chunk size, chunk overlap,
    and so on. Of course, we could have customized all these parameters if we wanted
    to.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '**Fixed-size chunking** simply splits text into same-sized chunks, optionally
    with some overlap. Although computationally cheap and simple to implement, this
    simple chunking may not always be the best approach. Performance testing various
    chunk sizes is key to optimizing for an application’s particular needs.'
  prefs: []
  type: TYPE_NORMAL
- en: The nodes containing chunks of the original text were then *embedded* into a
    high-dimensional vector space using a language model. The embedded vectors were
    stored within the vector store component of the Index. Next, when a query is made,
    the query text will be similarly embedded and compared against the stored vectors
    using a **similarity measure** identified with a method called **cosine similarity**
    . The most similar vectors – and thus the most relevant document chunks – will
    be returned as the query result. This process enables rapid, semantically aware
    retrieval of information, leveraging the mathematical properties of vector spaces
    to find the documents that best answer the user’s query.
  prefs: []
  type: TYPE_NORMAL
- en: Sounds a bit confusing? Let’s go through these concepts together in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In simple terms, **vector embeddings** represent a machine-understandable data
    format. They capture meaning and may conceptually represent a word, an entire
    document, or even non-textual information such as images and sounds. In a way,
    embeddings represent a standard language of thought for an LLM. In the context
    of an LLM, they serve as a foundational representation through which the model
    understands and processes information. They transform diverse and complex data
    into a uniform, high-dimensional space where the LLM can perform operations such
    as comparison, association, and prediction more effectively. *Figure 5* *.2* provides
    an illustration of the process of embedding data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_05_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – How an embedding model converts data into numerical representations
  prefs: []
  type: TYPE_NORMAL
- en: Because it’s all about math under the hood. And math works well with numbers
    – more precisely, large lists of floating-point numbers, where each number represents
    a dimension in a hypothetical vector space. The LLM can work with these arrays
    of numbers to understand, interpret, and generate responses based on the input
    it receives. Essentially, these numbers in the vector embeddings allow the LLM
    to *see* and *think* about the data in a way that’s meaningful and structured.
  prefs: []
  type: TYPE_NORMAL
- en: The beauty of this system lies in its ability to handle ambiguity and complexity.
    The model can understand semantic relationships between words, such as synonyms,
    antonyms, and more complex linguistic patterns. In the case of polysemous words,
    the same word can have different meanings in different contexts. For example,
    the word *bank* can refer to the side of a river or a financial institution. Vector
    embeddings help the LLM understand these nuances by providing context-sensitive
    representations. So, in one situation, *bank* might be closely associated with
    words such as *river* and *shore* , while in another, it’s more closely linked
    to *money* and *account* .
  prefs: []
  type: TYPE_NORMAL
- en: Quick note
  prefs: []
  type: TYPE_NORMAL
- en: An important factor to consider is that the size of text chunks being embedded
    impacts precision – too small and context is lost; too large and all that additional
    detail may dilute the meaning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In case you’re not very familiar with embeddings yet, the following example
    could be useful to get a better grasp of the concept. Let’s assign some *arbitrary*
    vector embeddings to three randomly chosen sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentence 1** : The quick brown fox jumps over the lazy dog'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence 2** : A fast dark-colored fox leaps above a sleepy canine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence 3** : Apples are sweet and crunchy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In a real-world scenario, the embeddings associated with each of these sentences
    would be calculated automatically by using an **embedding model** . This is a
    specialized artificial intelligence model used to convert complex data such as
    text, images, or graphs into a numerical format. The embeddings would also normally
    be high-dimensional, but for the sake of explanation, I’ll use simple, three-dimensional,
    arbitrarily chosen vectors. Here are the hypothetical embeddings for the three
    sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentence 1 Embedding** : [0.8, 0.1, 0.3]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence 2 Embedding** : [0.79, 0.14, 0.32]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence 3 Embedding** : [0.2, 0.9, 0.5]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These numbers are purely conceptual and are meant to show that sentences 1
    and 2, which have similar meanings, have embeddings that are closer to each other
    in vector space. *Sentence 3* , which has a different meaning, has an embedding
    that is farther away from the first two. Have a look at *Figure 5* *.3* for a
    straightforward visual comparison of the three embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_05_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – A comparison of the three embedded sentences in a 3D space
  prefs: []
  type: TYPE_NORMAL
- en: When we visualize them in a three-dimensional space, sentences 1 and 2 are plotted
    near each other, while sentence 3 will be plotted at a distance. This spatial
    representation is what allows machine learning models to determine semantic similarity.
  prefs: []
  type: TYPE_NORMAL
- en: When you search using a query on a vector store Index in order to retrieve useful
    context, LlamaIndex converts your search terms into a similar embedding and then
    finds the closest matches among the pre-computed embeddings of your text chunks.
  prefs: []
  type: TYPE_NORMAL
- en: We call this process **similarity** or **distance search** . So, when you encounter
    the term **top-k similarity search** , you should know that it relies on an algorithm
    that calculates the similarity between vector embeddings. It takes a vector embedding
    as an input and returns the most similar *k* number of vectors found in the vector
    store. Because the initial vector and the *top-k* returned neighbors are similar
    to each other, we can consider their meanings to be conceptually similar. Now
    you understand why I have previously called embeddings a *standard language of
    thought* for an LLM. It doesn’t really matter anymore whether they represent text,
    images, or any other types of information. We measure their similarity in numbers.
  prefs: []
  type: TYPE_NORMAL
- en: The only thing that may be implemented differently, depending on our use case,
    is the actual formula for defining that distance or similarity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spoiler alert: a bit of mathematical concepts up next.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding similarity search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the realms of machine learning and deep learning, the concept of similarity
    search is very important. It forms the backbone of many applications, from recommendation
    systems and information retrieval to clustering and classification tasks. As models
    and systems interact with high-dimensional data, identifying patterns and relationships
    between data points becomes essential. This involves measuring how *close* or
    *similar* data elements are, a task that often takes place in a vector space where
    each item is represented as a vector.
  prefs: []
  type: TYPE_NORMAL
- en: Locating points in this space that are near each other enables machines to assess
    similarity and, by extension, to make decisions, draw inferences, or, in our case,
    retrieve information based on that assessment of closeness. With the advent of
    embeddings in deep learning, the need for effective similarity search has grown.
    As embeddings capture the semantic meaning of the data they represent, performing
    similarity searches on these vectors allows machines to understand content at
    a level approaching human cognition.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore the methods that LlamaIndex currently employs to measure the similarity
    between vectors, each with its unique advantages and applicability.
  prefs: []
  type: TYPE_NORMAL
- en: Cosine similarity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This method measures the cosine of *the angle* between two vectors. Imagine
    two arrows pointing in different directions; the smaller the angle between them,
    the more similar they are.
  prefs: []
  type: TYPE_NORMAL
- en: 'Have a look at *Figure 5* *.4* , which depicts a cosine similarity comparison
    between two vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_05_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – How a cosine similarity comparison would look
  prefs: []
  type: TYPE_NORMAL
- en: In terms of embeddings, a small angle (or a high cosine similarity score, close
    to 1) indicates that the content they represent is similar. This method is particularly
    useful in text analysis because it is less affected by the length of the documents
    and focuses more on their direction or orientation in the vector space.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Cosine similarity is also the default method used by LlamaIndex for calculating
    similarity between embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Dot product
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Also called the **scalar product** , because it is represented by a single value,
    this is another method of calculating how well two vectors align with each other.
    To calculate the scalar product of two vectors, the algorithm multiplies the corresponding
    elements of the vectors and then sums these products.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a simple example of *vector A* : [2,3] and *vector B* : [4,1]. The
    **dot product** is calculated by multiplying their corresponding elements: (2×4)
    + (3×1), which gives us 8 + 3 = 11\. Thus, the dot product of these two vectors
    is 11.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5* *.5* exemplifies this concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_05_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Calculating similarity using the dot product method
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, the dot product is visualized by projecting one vector
    onto the other. This projection illustrates the geometric interpretation of the
    dot product. It’s calculated by projecting the components of one vector in the
    direction of the other and then multiplying these projected components by the
    corresponding components of the second vector. The sum of these products gives
    us the dot product. This visualization helps us understand that the dot product
    is not just a measure of how vectors point in the same direction; it also incorporates
    their lengths.
  prefs: []
  type: TYPE_NORMAL
- en: Higher values of the dot product mean higher similarities between vectors. In
    contrast with the cosine method, the dot product is sensitive both to the length
    of the two vectors compared and their relative direction. Unlike the dot product,
    cosine similarity normalizes the dot product by the magnitudes of the vectors.
    This normalization makes cosine similarity solely a measure of the directional
    alignment between vectors, independent of their lengths.
  prefs: []
  type: TYPE_NORMAL
- en: The longer the vectors, the higher the result, and this is an important thing
    to consider in a RAG scenario. Longer vectors, which might represent longer documents
    or more detailed information, could dominate the retrieved results due to their
    inherently larger dot product values. This could bias the system toward retrieving
    longer documents, even if they are not the most relevant.
  prefs: []
  type: TYPE_NORMAL
- en: Euclidean distance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This method is different from the dot product and cosine similarity methods.
    While those methods look at the angle or alignment between vectors, **Euclidean
    distance** is all about how close the actual values of the vectors are to each
    other. This can be especially useful when the values in the vectors represent
    actual counts or measurements, especially where the vector dimensions have real-world
    physical interpretations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at *Figure 5* *.6* for a visual representation of Euclidean distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_05_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – The Euclidean distance between two vectors
  prefs: []
  type: TYPE_NORMAL
- en: You should now have a foundational understanding of embeddings, how vector similarity
    works, and, in particular, how they are implemented in LlamaIndex. If you want
    to familiarize yourself better with this concept, you can find more information
    on the web.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some suggested additional reading resources you could start with:
    https://developers.google.com/machine-learning/clustering/similarity/measuring-similarity'
  prefs: []
  type: TYPE_NORMAL
- en: OK, but how does LlamaIndex generate these embeddings?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The short answer is, *however, you prefer* . By default, the framework is configured
    to rely on OpenAI’s `text-embedding-ada-002` model. This model has been trained
    to produce embeddings that effectively capture semantic meanings of the text,
    enabling applications such as semantic search, topic clustering, anomaly detection,
    and others. It provides a very good balance between quality, performance, and
    cost. LlamaIndex uses this model by default to embed documents during Index construction
    as well as for query embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, though, when you may want to index large volumes of data, the cost
    associated with a hosted model such as this one may be too high for your budget.
    In other instances, you might be concerned about the privacy of your proprietary
    data and prefer to use a local model instead. Or maybe, in some cases, you may
    want to use more specialized models for a particular topic or technical domain.
  prefs: []
  type: TYPE_NORMAL
- en: The great news is that LlamaIndex also supports a variety of other embedding
    models. For example, if you wish to use local models, you can set the service
    context to use a local embedding, which uses a well-balanced default model provided
    by *Hugging Face* ( https://huggingface.co/BAAI/bge-small-en-v1.5 ). This can
    be particularly useful if you aim to reduce costs or have requirements to process
    data locally.
  prefs: []
  type: TYPE_NORMAL
- en: A brief introduction to Hugging Face
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Hugging Face** is a very important resource in the AI field, primarily known
    for its extensive collection of pre-trained machine learning models, especially
    in **natural** **language processing** ( **NLP** ). Its importance lies in democratizing
    access to state-of-the-art AI models, tools, and techniques, enabling developers
    and researchers to implement advanced AI functionalities with relative ease. Similar
    to GitHub, Hugging Face embraces a community-driven approach, where users can
    share, collaborate on, and improve AI models, much like developers share and contribute
    to code repositories on GitHub. This community-centric model accelerates innovation
    and dissemination of AI advancements.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before running the next sample, make sure you install the necessary integration:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This example will show you how to set up a local embedding model:'
  prefs: []
  type: TYPE_NORMAL
- en: On the first run, the code will download the `Universal AnglE Embedding` model
    from Hugging Face. This is one of the best-performing embedding models at the
    moment, offering great overall performance and quality balance.
  prefs: []
  type: TYPE_NORMAL
- en: 'More info is available here: https://huggingface.co/WhereIsAI/UAE-Large-V1
    .'
  prefs: []
  type: TYPE_NORMAL
- en: After downloading and initializing the embedding model, the script calculates
    the embeddings for the sentence and displays the first 15 values of the vector.
  prefs: []
  type: TYPE_NORMAL
- en: For advanced users or specific applications, LlamaIndex makes it easy to integrate
    custom embedding models. You can simply extend the `BaseEmbedding` class provided
    by LlamaIndex and implement your own logic for generating embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, you can find an example of how to define your custom embedding class:
    https://docs.llamaindex.ai/en/stable/examples/embeddings/custom_embeddings.html
    .'
  prefs: []
  type: TYPE_NORMAL
- en: Apart from OpenAI and local models, there are integrations with Langchain, enabling
    you to use any embedding model they offer. You also have the option to use embedding
    models from Azure, CohereAI, and other providers through additional integrations
    offered by LlamaIndex. This great flexibility ensures that no matter your needs
    or constraints, you can configure LlamaIndex to use an embedding model that is
    suitable for your application.
  prefs: []
  type: TYPE_NORMAL
- en: How do I decide which embedding model I should use?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The choice of embedding model can significantly affect the performance, quality,
    and cost of your RAG app. Here are some key points to consider when choosing a
    particular model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Qualitative performance** : Different embedding models may encode the semantics
    of the text in different ways. While embeddings of models such as OpenAI’s Ada
    are designed to have a broad understanding of text, other models might be fine-tuned
    on specific domains or tasks and would outperform in those scenarios. Domain-specific
    models could lead to more accurate representations of specialized topics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quantitative performance** : This includes factors such as how well the model
    captures semantic similarity, its performance on benchmarks, and generalization
    to unseen data. This can vary considerably between different models and domains
    of application. For a general benchmark of the most popular models, you can consult
    the **Massive Text Embedding Benchmark** ( **MTEB** ) Leaderboard ( https://huggingface.co/spaces/mteb/leaderboard
    ) on the Hugging Face website.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latency and throughput** : For applications with real-time constraints or
    large volumes of data, the speed of the embedding model could be a deciding factor.
    Also consider the maximum input chunk sizes that models can handle, which impacts
    how text is divided for embedding. Keep in mind that your Nodes will have embeddings
    computed during ingestion, so that will not affect your overall application performance.
    However, during retrieval, each query will have to be embedded in real time so
    that similarity can be measured and the relevant nodes can be retrieved. This
    is where latency and throughput become important.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To get an idea of how different embedding models may perform, have a look at
    this article: https://blog.getzep.com/text-embedding-latency-a-semi-scientific-look/
    .'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Multilingual support** : Embedding models can be multilingual or trained
    for a specific language. Depending on your use case, this can also become an important
    decision factor. For example, smaller models such as `Mistral` could provide excellent
    results on par with hosted models such as GPT 3.5 for English data, but their
    performance in other languages is clearly inferior'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource requirements** : Embedding models can vary greatly in size and computational
    expense. Large models might provide more accurate embeddings but may require substantially
    more computational resources and thus lead to higher costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Availability** : Some embedding models may only be available through certain
    APIs or require specific software to be installed, which could affect ease of
    integration and usage. Fortunately, you have a high degree of customization available
    in LlamaIndex.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**On-device or local usage** : You may prefer to use a local model when data
    privacy is a concern or when operating in an environment with limited or no internet
    access.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usage cost** : Consider the cost associated with API calls for cloud-based,
    hosted embedding models versus the computational and storage costs of local embedding
    models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The good news is that LlamaIndex supports many out-of-the-box embedding models
    and provides flexibility to use various embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the way, a complete list of supported models can be found here: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#list-of-supported-embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: For most use cases, though, OpenAI’s default embedding model – `text-embedding-ada-002`
    – will provide you with a good balance between all the parameters we’ve discussed.
    However, if you have specific needs or constraints, you might benefit from exploring
    and benchmarking different models to see which provides the best outcomes for
    your particular application.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know about embeddings, let us shift our focus to how to store and
    reuse them.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Persisting and reusing Indexes</title>
  prefs: []
  type: TYPE_NORMAL
- en: Persisting and reusing Indexes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: from llama_index.core import VectorStoreIndex, SimpleDirectoryReader documents
    = SimpleDirectoryReader("data").load_data() index = VectorStoreIndex.from_documents(documents)
    index.storage_context.persist(persist_dir="index_cache") print("Index persisted
    to disk.") from llama_index.core import StorageContext, load_index_from_storage
    storage_context = StorageContext.from_defaults(     persist_dir="index_cache")
    index = load_index_from_storage(storage_context) print("Index loaded successfully!")
    pip install chromadb import chromadb from llama_index.vector_stores.chroma import
    ChromaVectorStore from llama_index.core import (     VectorStoreIndex, SimpleDirectoryReader,
    StorageContext) db = chromadb.PersistentClient(path="chroma_database") chroma_collection
    = db.get_or_create_collection(     "my_chroma_store" ) vector_store = ChromaVectorStore(
        chroma_collection=chroma_collection ) storage_context = StorageContext.from_defaults(
        vector_store=vector_store ) documents = SimpleDirectoryReader("files").load_data()
    index = VectorStoreIndex.from_documents(     documents=documents,     storage_context=storage_context
    ) results = chroma_collection.get() print(results) index = VectorStoreIndex.from_vector_store(
        vector_store=vector_store,     storage_context=storage_context )
  prefs: []
  type: TYPE_NORMAL
- en: An important question arises – where exactly can we store the vector embeddings
    generated during the indexing process?
  prefs: []
  type: TYPE_NORMAL
- en: 'Storing them is important for multiple reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Avoid the computational cost of re-embedding documents and rebuilding Indexes
    in every session. Generating high-quality embeddings for large document collections
    requires significant processing that can become costly over time. Persisting Indexes
    preserves these precomputed artifacts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable low-latency processing. Avoiding runtime embedding and indexing by loading
    the already computed embeddings allows applications to get up and running much
    faster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintain query consistency and accuracy. Reloading an Index guarantees we reuse
    the exact vectors and structure used in the previous sessions. This promises consistent
    and accurate query execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we want to avoid regenerating them on each run, these vector embeddings need
    to reside somewhere – a repository, if you will – that allows for efficient storage
    and retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: This is the job of a vector store within LlamaIndex.
  prefs: []
  type: TYPE_NORMAL
- en: By default, LlamaIndex uses an in-memory vector store, but for persistence,
    it offers a straightforward approach using the `.persist()` method available for
    any type of Index. This method writes all data to disk at a specified location,
    ensuring persistence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how we can persist and then load the vector embeddings. First, we
    create our Index, which handles the embedding of documents:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To persist this data, we use the `persist()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This saves the entire Index data to disk. In future sessions, we can easily
    reload the data:'
  prefs: []
  type: TYPE_NORMAL
- en: By rebuilding a `StorageContext` from the persisted directory and using `load_index_from_storage`
    , we can effectively reconstitute our Index without needing to re-index our data.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the StorageContext
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `StorageContext` serves as the unifying custodian over configurable storage
    components used during indexing and querying. Its key components are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Document store** ( `docstore` ): This manages the storage of documents.
    The data is locally stored in a file named `docstore.json` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **Index Store** ( `index_store` ): This manages the storage of Index structures.
    Indexes are stored locally in a file called `index_store.json` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vector Stores** ( `vector_stores` ): This is a dictionary managing multiple
    vector stores, each potentially serving a different purpose. The vector stores
    are stored locally in `vector_store.json` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **Graph Store** ( `graph_store` ): This manages the storage of graph data
    structures. A file named `graph_store.json` is automatically created by LlamaIndex
    for storing the graphs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `StorageContext` class encapsulates document, vector, index, and graph data
    stores under one umbrella. The files mentioned in the previous list for locally
    storing the data are automatically created by LlamaIndex when we invoke the `persist()`
    method. If we prefer not to save them in the current folder, we can provide a
    specific persistence location from where we can load them in future sessions.
  prefs: []
  type: TYPE_NORMAL
- en: Out-of-the-box, LlamaIndex offers basic local stores, but we can swap them with
    more capable persistence solutions such as *AWS S3,* *Pinecone* , *MongoDB* ,
    and others.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, let’s explore customizing vector storage using ChromaDB, an efficient
    open source vector engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, make sure you install `chromadb` using pip:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part of the code takes care of the necessary imports:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We then continue by initializing the Chroma client and creating a collection
    within Chroma to store our data:'
  prefs: []
  type: TYPE_NORMAL
- en: In ChromaDB, we create **collections** to store data. These are similar to *tables*
    in relational databases. The `my_chroma_store` collection will hold our embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we initialize a tailored vector store using `ChromaVectorStore` and wire
    it into the `StorageContext` :'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re now ready to ingest our documents and build the Index:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now use the `get()` method to display the entire contents of the Chroma
    collection:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Subsequently, restoring this Index in future sessions is also very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: We just rebuilt our original Index.
  prefs: []
  type: TYPE_NORMAL
- en: By wrapping **vector databases** such as ChromaDB, LlamaIndex makes enterprise-scale
    vector storage accessible through a simple storage abstraction. The complexity
    is concealed, enabling you to focus on your application logic while still leveraging
    industrial-strength data infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, LlamaIndex provides flexibility in vector storage – from a simple
    in-memory store for testing to cloud-hosted databases for large, real-world deployments.
    And through storage integrations, swapping any component is a breeze!
  prefs: []
  type: TYPE_NORMAL
- en: The difference between vector stores and vector databases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The terms vector store and vector database are often used in the context of
    managing and querying large sets of vectors, which are commonly used in machine
    learning, particularly in applications involving NLP, image recognition, and similar
    tasks. You may have already noticed that I’m using them quite often in this chapter,
    sometimes implying they are similar concepts. However, there is a subtle distinction
    between the two:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vector store** : This generally refers to a storage system or repository
    where vectors are stored. The vectors are high-dimensional and represent complex
    data such as text, images, or audio in a format that can be processed by machine
    learning models. A vector store focuses primarily on the efficient storage of
    these vectors. It might not have advanced capabilities for querying or analyzing
    the data and its main purpose is to maintain a large repository of vectors that
    can be retrieved and used for various machine learning tasks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vector database** : A vector database, on the other hand, is a more sophisticated
    system that not only stores vectors but also provides advanced functionalities
    for querying and analyzing them. This includes the ability to perform similarity
    searches and other complex operations that are useful in machine learning and
    data analysis. A vector database is designed to handle the nuances of vector data,
    such as their high dimensionality and the need for specialized indexing techniques
    to enable efficient search and retrieval. In a nutshell'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While a vector store is more about the storage aspect, a vector database encompasses
    both storage and the complex querying capabilities required for vector data. This
    makes vector databases particularly important in applications where it’s necessary
    to search through large volumes of vectorized data quickly and accurately.
  prefs: []
  type: TYPE_NORMAL
- en: 'One distinguishing feature usually representative of a vector database and
    less often provided by vector stores is the support for `CRUD` ( `create` **,**
    `read` **,** `update` **,** `delete` ) functions. Whether or not a vector store
    offers `CRUD` functions can vary depending on the specific implementation and
    design of the store. However, in general, a vector store, especially if it’s a
    simplified or basic form of storage for vector data, might not support all the
    `CRUD` operations in the same way a traditional database system would. Let’s break
    down the typical operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Create** : The ability to add new vectors to the store is usually a fundamental
    feature. This is essential for building up the vector repository.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Read** : Reading or retrieving vectors based on some form of identifier or
    criterion is also a common feature. In a basic vector store, this might be limited
    to simple retrieval rather than complex queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Update** : Updating existing vectors in a vector store might not be as straightforward
    or as commonly supported as in traditional databases. This is because vector data,
    often used in machine learning and similar applications, is usually generated
    in a fixed form and not frequently updated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Delete** : The capability to delete vectors may be supported, but like updating,
    it may not be a primary feature, depending on the use case of the vector store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In many machine learning and AI applications, once vectors are created and stored,
    they are not frequently updated or deleted, which is why some vector stores might
    focus more on efficient storage and retrieval (create and read operations) rather
    than full CRUD functionality.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to a simple vector store, a vector database, which is more sophisticated,
    is more likely to offer complete CRUD capabilities, allowing for more dynamic
    and flexible management of the vector data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a good starting point in your journey toward a better understanding
    of vector databases: https://learn.microsoft.com/en-us/semantic-kernel/memories/vector-db
    .'
  prefs: []
  type: TYPE_NORMAL
- en: <title>Exploring other index types in LlamaIndex</title>
  prefs: []
  type: TYPE_NORMAL
- en: Exploring other index types in LlamaIndex
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'from llama_index.core import SummaryIndex, SimpleDirectoryReader documents
    = SimpleDirectoryReader("files").load_data() index = SummaryIndex.from_documents(documents)
    query_engine = index.as_query_engine() response = query_engine.query("How many
    documents have you loaded?") print(response) I have loaded two documents. from
    llama_index.core import (     DocumentSummaryIndex, SimpleDirectoryReader) documents
    = SimpleDirectoryReader("files").load_data() index = DocumentSummaryIndex.from_documents(
        documents,     show_progress=True ) summary1 = index.get_document_summary(documents[0].doc_id)
    summary2 = index.get_document_summary(documents[1].doc_id) print("\n Summary of
    the first document: " + summary1) print("\n Summary of the second document: "
    + summary2) from llama_index.core import KeywordTableIndex, SimpleDirectoryReader
    documents = SimpleDirectoryReader("files").load_data() index = KeywordTableIndex.from_documents(documents)
    query_engine = index.as_query_engine() response = query_engine.query("     What
    famous buildings were in ancient Rome?") print(response) from llama_index.core
    import TreeIndex, SimpleDirectoryReader documents = SimpleDirectoryReader("files").load_data()
    index = TreeIndex.from_documents(documents) query_engine = index.as_query_engine()
    response = query_engine.query("Tell me about dogs") print(response) from llama_index.core
    import (     KnowledgeGraphIndex, SimpleDirectoryReader) documents = SimpleDirectoryReader("files").load_data()
    index = KnowledgeGraphIndex.from_documents(     documents, max_triplets_per_chunk=2,
    use_async=True) query_engine = index.as_query_engine() response = query_engine.query("Tell
    me about dogs.") print(response)'
  prefs: []
  type: TYPE_NORMAL
- en: As this ebook edition doesn't have fixed pagination, the page numbers below
    are hyperlinked for reference only, based on the printed edition of this book.
  prefs: []
  type: TYPE_NORMAL
- en: While the `VectorStoreIndex` may be the star of the show in most of our RAG
    scenarios, LlamaIndex provides many other useful indexing tools. They all have
    specific features and use cases and the following section will explore them in
    more detail.
  prefs: []
  type: TYPE_NORMAL
- en: The SummaryIndex
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `SummaryIndex` offers a straightforward yet powerful way of indexing data
    for retrieval purposes. Unlike the `VectorStoreIndex` , which focuses on embeddings
    within a vector store, the `SummaryIndex` is based on a simple data structure
    where nodes are stored in a sequence. You’ll find a simple depiction of the structure
    of the `SummaryIndex` in *Figure 5* *.7* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_05_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – The structure of a SummaryIndex
  prefs: []
  type: TYPE_NORMAL
- en: When building the Index, it ingests a collection of documents, splits them into
    smaller chunks, and then compiles these chunks into a sequential list. Everything
    runs locally, without involving an LLM or any embedding model.
  prefs: []
  type: TYPE_NORMAL
- en: Practical use case
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine we would create a documentation search tool within a software development
    project. Often, software projects accumulate extensive documentation over time,
    including technical specifications, API documentation, user guides, and developer
    notes. Keeping track of this information can become challenging, especially when
    the team needs to quickly reference specific details. Implementing a `SummaryIndex`
    for the project’s documentation repository allows developers to perform quick
    searches across all documents. For example, a developer could query *What are
    the error handling procedures for the payment gateway API?* The `SummaryIndex`
    would scan through the indexed documentation to retrieve relevant sections where
    error handling is discussed, without the need for complex embedding models or
    intensive computational resources. This Index would be particularly useful in
    environments where maintaining an extensive vector store would not be viable due
    to resource constraints or where simplicity and speed are prioritized.
  prefs: []
  type: TYPE_NORMAL
- en: The `SummaryIndex` is particularly effective for applications where a linear
    scan through data is sufficient or where complex embedding-based retrieval is
    not required. It’s a more basic form of indexing but still versatile enough for
    various use cases, especially in scenarios where you need a simple way to index
    your data.
  prefs: []
  type: TYPE_NORMAL
- en: A simple usage model for the SummaryIndex
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Creating a `SummaryIndex` is a straightforward process:'
  prefs: []
  type: TYPE_NORMAL
- en: Here, Nodes are created from our sample files and the `SummaryIndex` is instantiated
    with these Nodes. This simple model enables quick setup without the complexity
    of embedding or using vector storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have correctly cloned the structure of our book’s GitHub repository
    and have a `files` subfolder containing two text files, the output of the previous
    code snippet should be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the inner workings of the SummaryIndex
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Internally, the `SummaryIndex` operates by storing each node in a list-like
    structure. When a query is executed, the Index iterates through this list to find
    relevant nodes. While this process is less complex than embedding-based searches
    in `VectorStoreIndex` , it’s still effective for many applications.
  prefs: []
  type: TYPE_NORMAL
- en: The Index can be used with various retrievers such as `SummaryIndexRetriever`
    , `SummaryIndexEmbeddingRetriever` , and `SummaryIndexLLMRetriever` , each providing
    different mechanisms for searching and retrieving data. During queries, the `SummaryIndex`
    employs a *create and refine* approach to formulate responses. Initially, it assembles
    a preliminary answer based on the first chunk of text. This initial response is
    subsequently refined by incorporating additional text chunks as contextual information.
    The refinement process involves either maintaining the initial answer, slightly
    modifying it, or entirely rephrasing the original response. We’ll cover the retrieval
    part in detail during *Chapter 6* , *Querying Our Data, Part 1 –* *Context Retrieval*
    .
  prefs: []
  type: TYPE_NORMAL
- en: The DocumentSummaryIndex
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LlamaIndex’s arsenal of indexing tools extends beyond its well-regarded `VectorStoreIndex`
    , encompassing a variety of specialized Indexes designed for diverse applications.
    Among these, the `DocumentSummaryIndex` stands out for its unique approach to
    document management and retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: At its core, the `DocumentSummaryIndex` is designed to optimize information
    retrieval by summarizing Documents and mapping these summaries to their corresponding
    Nodes within the Index. This process facilitates efficient data retrieval, using
    the summaries to quickly identify relevant Documents.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5* *.8* provides a visual representation of this mechanism:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_05_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – The DocumentSummaryIndex
  prefs: []
  type: TYPE_NORMAL
- en: This Index operates by first creating a summary for each ingested Document.
    These summaries are then linked to the Document’s Nodes, forming a structured
    Index that enables fast and accurate data retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: The `DocumentSummaryIndex` is particularly useful for handling queries where
    a succinct overview of the document content can significantly narrow down the
    search space, making it a great tool for applications requiring quick access to
    specific Documents in a large and diverse dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a practical use case for the `DocumentSummaryIndex` is in the development
    of a knowledge management system within a large organization. In such an environment,
    employees often need quick access to a vast array of documents, including reports,
    research papers, policy documents, and technical manuals. These documents are
    typically stored across different departments and may be extensive in length,
    making it challenging to quickly find specific information relevant to a user’s
    query. In addition, multiple documents may contain similar chunks of text, making
    a simple embedding-based retrieval impractical over the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several parameters can be customized for this particular Index:'
  prefs: []
  type: TYPE_NORMAL
- en: '`response_synthesizer` : This parameter allows you to specify a response synthesizer
    that is responsible for generating summaries. By customizing this parameter, you
    can control the summarization process, adjusting it to fit specific needs or preferences
    in how summaries are generated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`summary_query` : This parameter is used to define the query that guides the
    summarization process. Essentially, it tells the response synthesizer what kind
    of summary to generate for each Document. The default query asks for a summary
    that describes what the Document is about and what questions it can answer. Adjusting
    this query allows you to tailor the focus and style of the summaries, making them
    more relevant to the specific use cases of the Index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`show_progress` : This Boolean parameter determines whether to display progress
    bars during operations that can take a significant amount of time. Setting this
    to `True` provides visual feedback on the progress of these operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embed_summaries` : When set to `True` – which is the default – this parameter
    indicates that the summaries should be embedded. Embedded summaries can then be
    used for similarity comparisons and retrieval in an embedding-based search. This
    is particularly useful for scenarios where you want to retrieve Nodes based on
    the similarity between the Document summary content and the user query. We’ll
    cover this in more detail during *Chapter 6* , *Querying Our Data, Part 1 –* *Context
    Retrieval* *.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s now see how to use the `DocumentSummaryIndex` .
  prefs: []
  type: TYPE_NORMAL
- en: A simple usage model for the DocumentSummaryIndex
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Creating a `DocumentSummaryIndex` involves a series of steps, starting with
    the aggregation of Documents and their subsequent summarization. The following
    code snippet demonstrates the basic setup for creating this Index:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This process involves reading documents from a directory, parsing them into
    Nodes, summarizing the Documents, and then associating the corresponding Nodes
    with these summaries for quick retrieval. Next, let’s observe the summaries that
    were generated in the process:'
  prefs: []
  type: TYPE_NORMAL
- en: The second part of the code sample displays the summaries that were generated
    for each Document. These summaries were associated with the underlying Nodes for
    each Document. During retrieval, this association will allow extracting only the
    relevant Nodes, based on the user query and the summary of each Document.
  prefs: []
  type: TYPE_NORMAL
- en: Internally, the `DocumentSummaryIndex` supports both embedding-based and LLM-based
    retrievers, allowing for flexible retrieval mechanisms that cater to different
    needs. By default, the Index also generates embeddings for each summary in order
    to facilitate embedding-based retrieval, which is particularly useful for similarity
    searches.
  prefs: []
  type: TYPE_NORMAL
- en: The KeywordTableIndex
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `KeywordTableIndex` in LlamaIndex implements a clever architecture – similar
    to a glossary of terms – for rapidly matching queries to relevant nodes based
    on important terms. Unlike complex embedding spaces, this structure relies on
    a straightforward keyword table, yet proves highly effective for targeted factual
    lookup. This Index extracts keywords from documents and constructs a keyword-to-node
    mapping, offering a highly efficient search mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s particularly useful in scenarios where precise keyword matching is vital
    for retrieving relevant information. These keywords become the reference keys
    in a central lookup table, each one pointing to associated nodes such as a glossary
    definition. During retrieval, just like scanning a glossary for entries of interest,
    relevant nodes containing a particular keyword are identified and returned. See
    *Figure 5* *.9* for a visual representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_05_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – The structure of a KeywordTableIndex
  prefs: []
  type: TYPE_NORMAL
- en: 'The customizable parameters for the `KeywordTableIndex` are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`keyword_extract_template` : This is an optional prompt template used for keyword
    extraction. Custom prompts can be specified to change how keywords are extracted
    from text, allowing for tailored keyword extraction strategies. We’ll talk more
    about prompt customization during *Chapter 10* *.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_keywords_per_chunk` : This sets the maximum number of keywords to extract
    from each text chunk. By using this parameter, we can make sure the keyword table
    remains manageable and focused on the most relevant keywords. The default value
    is `10` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_async` : This determines whether to use asynchronous calls. This can improve
    performance, especially when handling large datasets or complex operations. Its
    default setting is `False` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next up, we will create the `KeywordTableIndex` .
  prefs: []
  type: TYPE_NORMAL
- en: A simple usage model for the KeywordTableIndex
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Creating a `KeywordTableIndex` is very straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the Index automatically extracts keywords from your data and sets up a
    keyword table, streamlining the process of setting up a keyword-based retrieval
    system.
  prefs: []
  type: TYPE_NORMAL
- en: Just like in the previous example, if you have correctly cloned the structure
    of our GitHub repository and have a `files` subfolder containing two text files,
    the output of the previous code snippet should be something along the lines of
    *The Colosseum and the Pantheon were famous buildings in* *ancient Rome* .
  prefs: []
  type: TYPE_NORMAL
- en: How does the KeywordTableIndex operate?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `KeywordTableIndex` builds and operates a keyword table, akin to a glossary,
    where each keyword is linked to relevant nodes. The Index initially processes
    a collection of documents, breaking them down into smaller chunks. For each chunk,
    the Index uses the LLM with a specially designed prompt to identify and extract
    relevant keywords. These keywords, which may range from simple terms to short
    phrases, are subsequently cataloged in the keyword table. Each keyword in this
    table is directly linked to the chunk of text from which it was derived.
  prefs: []
  type: TYPE_NORMAL
- en: Upon receiving a query, the Index identifies keywords within it and matches
    them with the table entries, enabling rapid and accurate retrieval of related
    chunks containing those keywords. It supports various retrieval modes, including
    simple keyword matching and advanced techniques such as **RAKE** or LLM-based
    keyword extraction and matching. We’ll talk more about these retrieval modes during
    *Chapter 6* , *Querying Our Data, Part 1 –* *Context Retrieval* .
  prefs: []
  type: TYPE_NORMAL
- en: Quick note on the RAKE extraction method
  prefs: []
  type: TYPE_NORMAL
- en: 'This method is particularly effective in identifying phrases or keywords that
    are significant within a body of text. The key idea behind RAKE is that keywords
    often consist of multiple words but rarely include punctuation, stop words, or
    words with minimal lexical meaning. The `KeywordTableIndex` has two similar alternatives
    that are designed to operate without the assistance of an LLM: `SimpleKeywordTableIndex`
    , which uses a simple regex extractor, and `RAKEKeywordTableIndex` , which relies
    on a RAKE keyword extractor based on the `rake_nltk` (Natural Language Toolkit)
    Python package.'
  prefs: []
  type: TYPE_NORMAL
- en: You should know that, just like the `SummaryIndex` , the `KeywordTableIndex`
    also uses a *create and refine* approach when synthesizing the final response.
    The adaptability of the `KeywordTableIndex` makes it a versatile tool for diverse
    applications where keyword precision is key.
  prefs: []
  type: TYPE_NORMAL
- en: The TreeIndex
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `TreeIndex` introduces a hierarchical approach to information organization
    and retrieval. Unlike a simple list, this structure organizes data in a hierarchical
    tree format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Have a look at *Figure 5* *.10* for a diagram depicting the structure of the
    `TreeIndex` :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – The structure of a TreeIndex
  prefs: []
  type: TYPE_NORMAL
- en: Each node in this tree can represent a piece of data or information, similar
    to a branch or leaf on a real tree. This structural formation allows for efficient
    handling and querying of data. The `TreeIndex` first takes in a set of documents
    as input. It then builds up a tree in a bottom-up fashion; each parent node is
    able to summarize the child nodes using a general summarization prompt, and each
    intermediate node contains text summarizing the components below it. This summary
    is generated using an LLM based on a prompt template that can be customized with
    the `summary_prompt` parameter. `TreeIndex` acts like an organizer and summarizer,
    taking lots of individual pieces of data, grouping them together, and creating
    a summary that captures their essence.
  prefs: []
  type: TYPE_NORMAL
- en: Customizable parameters for the TreeIndex
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Apart from the general customization inherited from the `BaseIndex` class,
    the `TreeIndex` provides the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`summary_template` : This is a prompt for summarization, used during Index
    construction. This prompt can be customized for better control of the summarization
    process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`insert_prompt` : This is a prompt used by the Index for tree insertion, facilitating
    Index construction. This prompt facilitates the insertion of nodes into the tree.
    It guides how new information is integrated into the existing tree structure.
    We’ll cover details about prompt customization during *Chapter 10* , *Prompt Engineering
    Guidelines and* *Best Practices.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_children` : This defines the maximum number of child nodes each node should
    have. This parameter controls the breadth of the tree, impacting its level of
    detail at each node. By default, this is set to `10` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`build_tree` : This is a Boolean indicating whether to build the tree during
    Index construction. If we don’t use the default value – which is `True` – the
    Index will build its tree during query time instead of building it during the
    Index construction. Setting the `build_tree` parameter to `False` could be useful
    in scenarios where you might want to manually control the tree-building process
    or modify the tree structure after initial construction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_async` : This determines whether asynchronous operation mode should be
    used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let’s create a simple `TreeIndex` .
  prefs: []
  type: TYPE_NORMAL
- en: A simple usage model for the TreeIndex
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To implement a `TreeIndex` , you can follow this simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: This process involves the `TreeIndex` taking in documents, structuring them
    hierarchically, and then allowing for queries that leverage this structure for
    efficient data retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: The inner mechanics of the TreeIndex
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The index-building process is recursive. After the first level of parent nodes
    is created, the builder can repeat the process, summarizing these parent nodes
    into higher-level nodes, and so on. This creates multiple levels in the tree,
    with each level abstracting and summarizing the information from the level below
    it. Also, for large datasets, the Index can handle data asynchronously with `use_async`
    . This means it can process multiple parts of the data simultaneously, making
    the building process faster and more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: By using LLMs for summaries, the `TreeIndex` can encapsulate a nuanced understanding
    of the data. This is particularly useful for complex datasets where relationships
    and context matter.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in organizations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In organizations with complex hierarchical data such as reports, memos, and
    research papers, a `TreeIndex` can organize this information efficiently, allowing
    for quick retrieval of specific data points within their knowledge management
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: '`TreeIndex` operates by building a tree where each node is a summarized representation
    of its children, offering a clear and organized view of the data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This Index supports several retrieval modes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`TreeSelectLeafRetriever` : This traverses the tree to find leaf nodes that
    can best answer a query. It involves choosing a specific number of child nodes
    at each level for traversal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TreeSelectLeafEmbeddingRetriever` : This utilizes embedding similarity between
    the query and node text to traverse the tree, selecting leaf nodes based on this
    similarity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TreeRootRetriever` : This directly retrieves answers from the root nodes of
    the tree. This method assumes the graph already stores the answer, so it doesn’t
    parse information down the tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TreeAllLeafRetriever` : This builds a query-specific tree from all leaf nodes
    to return a response. It rebuilds the tree for each query, making it suitable
    for scenarios where the tree structure doesn’t need to be built during initialization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'During query time, the tree Index operates in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the provided query string is processed to extract relevant keywords
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Beginning from the root Node, the Index navigates through the tree structure
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At each Node, it determines whether the keywords are found in the Node’s summary
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If keywords are found, the Index proceeds to explore the Node’s child Nodes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the keywords are absent, the Index advances to the subsequent Node
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process persists until a leaf Node is encountered or all Nodes in the tree
    have been examined
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The reached leaf Nodes represent the context with the highest likelihood of
    relevance to the given query.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll cover the retrievers in more detail during *Chapter 6* , *Querying Our
    Data, Part 1 –* *Context Retrieval* .
  prefs: []
  type: TYPE_NORMAL
- en: Some potential drawbacks of using the TreeIndex
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using a `TreeIndex` in our RAG workflow can potentially be less advantageous
    compared to simpler retrieval methods. Here are a few reasons why:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Increased computation* : Building and maintaining a `TreeIndex` requires additional
    computational resources. During the Index construction phase, the tree structure
    needs to be created by recursively summarizing and organizing the Nodes. This
    process involves applying summarization using LLM calls and constructing the hierarchical
    structure, which can be computationally intensive, especially for large datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Recursive retrieval* : When querying the Index, the retrieval process involves
    traversing the tree structure from the root nodes down to the relevant leaf nodes.
    This recursive traversal can require multiple steps and computations, especially
    if the tree is deep or if multiple branches need to be explored. Each step in
    the traversal may involve comparing the query with the Node summaries and making
    decisions on which branches to follow. This recursive process can be more computationally
    expensive compared to retrieving from a flat Index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Summarization overhead* : This Index relies on summarizing the content of
    each node to provide a concise representation of its child Nodes. The summarization
    process needs to be performed during Index construction and potentially during
    updates or insertions, adding to the overall computational overhead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Storage requirements* : Storing a `TreeIndex` requires additional storage
    compared to a flat Index. The Index needs to store the tree structure, Node summaries,
    and metadata associated with each Node. This extra storage overhead can increase
    storage costs, especially for large-scale datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Maintenance and updates* : Maintaining a `TreeIndex` requires regular updates
    and re-organization as new data is added or existing data is modified. Inserting
    new nodes or updating existing nodes in the tree structure may trigger a cascading
    effect, requiring updates to the parent nodes and their summaries. This maintenance
    process can be more complex and time-consuming compared to other Indexes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, it’s important to note that the higher costs associated with using
    a `TreeIndex` can be justified in certain scenarios. If the RAG application deals
    with a large-scale dataset and requires efficient and context-aware retrieval,
    the benefits of using this type of Index may outweigh the additional costs. Its
    hierarchical structure and summarization capabilities can lead to improved retrieval
    performance, reduced search space, and better response generation quality. By
    traversing the tree from the root Nodes and selectively exploring relevant branches,
    the model can quickly narrow down the search to the most promising Nodes. This
    can lead to faster retrieval times and improved efficiency compared to searching
    through a flat Index structure.
  prefs: []
  type: TYPE_NORMAL
- en: The key is to assess the specific requirements, scale, and constraints of the
    RAG scenario to determine whether the benefits of using a `TreeIndex` justify
    the potential increase in costs. Careful evaluation and benchmarking can help
    in making an informed decision based on the trade-offs between retrieval efficiency,
    generation quality, and computational and storage costs.
  prefs: []
  type: TYPE_NORMAL
- en: The KnowledgeGraphIndex
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `KnowledgeGraphIndex` enhances query processing by constructing a **knowledge
    graph** ( **KG** ) from extracted **triplets** . This type of Index primarily
    relies on the LLM to extract triplets from text, but it also provides flexibility
    to use custom extraction functions if needed.
  prefs: []
  type: TYPE_NORMAL
- en: KG Indexes excel in scenarios where understanding complex, interlinked relationships
    and contextual information is important. They are very good at capturing intricate
    connections between entities and concepts, thus offering better insights and context-aware
    responses to queries. Among other use cases, KGs are ideal for answering multifaceted
    questions that require an understanding of the relationships between different
    entities. *Yes, I’m talking about our tutor project,* *PITS, here.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get a visual of how KGs work in *Figure 5* *.11* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – The structure of a KnowledgeGraphIndex
  prefs: []
  type: TYPE_NORMAL
- en: Practical use case
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An interesting use case for a KG could be, for example, a news aggregation app,
    where large volumes of text are ingested every day from various sources such as
    newspapers, blogs, and social media platforms. In such a scenario, KGs could be
    used to represent entities such as people, organizations, locations, and so on,
    and their relationships over time. This would allow users to explore historical
    trends, breaking news events, and related entities based on the graph structure
    and traversal algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Sounds good, right? We will now take a look at how you can work with `KnowledgeGraphIndex`
    .
  prefs: []
  type: TYPE_NORMAL
- en: Customizable parameters for the KnowledgeGraphIndex
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can customize the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kg_triple_extract_template` : This is a prompt template for extracting triplets.
    It can be customized to change how triplets (subject-predicate-object) are identified,
    enabling tailored extraction strategies based on specific use cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_triplets_per_chunk` : This limits the number of triplets extracted per
    text chunk. Setting this value helps manage the size and complexity of the KG.
    The default value is `10` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`graph_store` : This defines the storage type for the graph. Different storage
    types can be used to optimize performance and scalability based on the application’s
    requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`include_embeddings` : This decides whether to include embeddings in the Index.
    This is useful for scenarios where embeddings can enhance the retrieval process,
    such as similarity searches or advanced query understanding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_object_length` : This sets the maximum length – in characters – for the
    object in a triplet. It prevents overly long or complex objects that could complicate
    the graph’s structure and the retrieval process. Its default value is `128` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kg_triplet_extract_fn` : A custom function for triplet extraction can be provided,
    offering the flexibility to use specialized or proprietary methods for extracting
    triplets from text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s create a simple KG next.
  prefs: []
  type: TYPE_NORMAL
- en: A basic usage model for KnowledgeGraphIndex
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here’s a simple way of constructing and querying a KG:'
  prefs: []
  type: TYPE_NORMAL
- en: In this setup, the Index builds a KG by extracting triplets from documents,
    enabling complex relationship queries. Notice that we configured the Index to
    run the build process in asynchronous mode by setting `use_async` to `True` .
    Of course, for the two small documents that we’re using as an example in our case,
    this won’t make too much difference in the total execution time. However, when
    working with large datasets, enabling asynchronous operation for this Index may
    provide an important performance boost.
  prefs: []
  type: TYPE_NORMAL
- en: How does the KnowledgeGraphIndex build its structure?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`KnowledgeGraphIndex` operates by extracting subject-predicate-object triplets
    from text data, forming a KG.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main ways in which this Index can build its structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The default, built-in approach* : In its default implementation, the Index
    uses an internal method to extract triplets from text. This method takes the text
    content of each Node and passes it through a pre-defined prompt template – `DEFAULT_KG_TRIPLET_EXTRACT_PROMPT`
    or a custom template provided during initialization through the `kg_triple_extract_template`
    argument. The prompt template is designed to instruct the LLM to extract knowledge
    triplets from the given text. The LLM’s response is then parsed by a specialized
    internal method to extract the subject, predicate, and object of each triplet.
    This method extracts knowledge triplets in the format of *subject, predicate,
    object* . It applies various checks and string manipulations to ensure the validity
    and consistency of the extracted triplets. Finally, the method returns a list
    of cleaned and well-formatted triplets that can be added to the KG Index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The second approach involves a custom triplet extraction function* : If a
    custom `kg_triplet_extract_fn` function is provided during initialization, it
    will be used instead of the LLM-based method. This allows us to define our own
    function to extract triplets from text based on their specific requirements or
    domain knowledge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regardless of whether we’re using the first or the second approach to generate
    the triplets, the inner components of the Index are responsible for building the
    actual KG from the given Nodes. They iterate over each Node, extract triplets
    using either the LLM-based method or the custom extraction function and add the
    triplets to the Index structure.
  prefs: []
  type: TYPE_NORMAL
- en: If the `include_embeddings` flag is set to `True` , the Index will also generate
    embeddings for each triplet using the specified embedding model. These embeddings
    are stored in the `embedding_dict` of the Index structure.
  prefs: []
  type: TYPE_NORMAL
- en: The `upsert_triplet()` method allows the manual insertion of triplets into the
    KG. It adds the triplet to the graph store and also optionally generates embeddings
    for the triplet if `include_embeddings` are set to `True` .
  prefs: []
  type: TYPE_NORMAL
- en: 'During querying, the Index leverages the KG to retrieve relevant data and help
    provide context-rich responses. There are three distinct retrievers available
    for this Index: `KGTableRetriever` for keyword-focused queries, `KnowledgeGraphRAGRetriever`
    for retrieving sub-graphs based on extracted entities and synonyms, and a hybrid
    mode that combines both keyword and embedding strategies for a comprehensive approach.
    More details about these retrieval capabilities will be explored during *Chapter
    6* , *Querying Our Data, Part 1 –* *Context Retrieval* .'
  prefs: []
  type: TYPE_NORMAL
- en: <title>Building Indexes on top of other Indexes with ComposableGraph</title>
  prefs: []
  type: TYPE_NORMAL
- en: Building Indexes on top of other Indexes with ComposableGraph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: from llama_index.core import (     ComposableGraph, SimpleDirectoryReader,     TreeIndex,
    SummaryIndex) documents = SimpleDirectoryReader("files").load_data() index1 =
    TreeIndex.from_documents([documents[0]]) index2 = TreeIndex.from_documents([documents[1]])
    summary1 = "A short introduction to ancient Rome" summary2 = "Some facts about
    dogs" graph = ComposableGraph.from_indices(     SummaryIndex, [index1, index2],
        index_summaries=[summary1, summary2] ) query_engine = graph.as_query_engine()
    response = query_engine.query("What can you tell me?") print(response)
  prefs: []
  type: TYPE_NORMAL
- en: The `ComposableGraph` in LlamaIndex represents a sophisticated way to structure
    information by **stacking Indexes** on top of each other.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5* *.12* provides an overview of a `ComposableGraph` :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – The structure of a ComposableGraph
  prefs: []
  type: TYPE_NORMAL
- en: This approach allows for the construction of Indexes within individual documents
    – lower-level Indexes – and the aggregation of these Indexes into higher-order
    ones over a collection of documents. For example, you can build a `TreeIndex`
    for the text within each document and a `SummaryIndex` that encompasses each `TreeIndex`
    in a collection.
  prefs: []
  type: TYPE_NORMAL
- en: How to use the ComposableGraph
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here’s a simple code example demonstrating the usage of `ComposableGraph` :'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the `ComposableGraph` facilitates the organization of detailed
    information within Documents and the summarization across Documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first load our two test Documents: one related to ancient Rome and the other
    one describing dogs. We then create a `TreeIndex` for each Document.'
  prefs: []
  type: TYPE_NORMAL
- en: We also define the summaries of the two Documents.
  prefs: []
  type: TYPE_NORMAL
- en: Pro tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As an alternative to manually defining the summaries, we could have also queried
    each individual Index to automatically generate the content summary or used `SummaryExtractor`
    to accomplish the same purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next step, we build a `ComposableGraph` containing the two tree Indexes
    along with their summaries. For this example, the output of the code should be
    something similar to the following: *I can tell you about the ancient Roman civilization
    and dogs and their various breeds, traits,* *and personalities.*'
  prefs: []
  type: TYPE_NORMAL
- en: Once the `ComposableGraph` has been built, the root `SummaryIndex` will have
    an overview of the contents of the individual Indexes for each document.
  prefs: []
  type: TYPE_NORMAL
- en: A more detailed description of this concept
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Under the hood, a `ComposableGraph` enables the creation of hierarchical structures
    by stacking Indexes on top of each other. This allows for the organization of
    detailed information within individual Documents using lower-level Indexes and
    the aggregation of these Indexes into higher-order ones over a collection of Documents.
  prefs: []
  type: TYPE_NORMAL
- en: The process begins by creating individual Indexes for each Document to capture
    the detailed information within the Documents. Additionally, summaries are defined
    for each Document.
  prefs: []
  type: TYPE_NORMAL
- en: The `ComposableGraph` is then constructed using the `from_indices()` class method.
    It takes the root Index class (in our example, the `SummaryIndex` ), the child
    Indexes (in our example, the two `TreeIndex` instances), and their corresponding
    summaries as input. The method creates `IndexNodes` instances for each child Index,
    associating the summary with the respective Index. These `IndexNodes` instances
    are then used to construct the root Index.
  prefs: []
  type: TYPE_NORMAL
- en: During a query, the `ComposableGraph` starts with the top-level summary Index,
    where each Node corresponds to an underlying lower-level Index. The query is executed
    recursively, starting from the root Index, and traversing through the sub-Indexes.
    The `ComposableGraphQueryEngine` is responsible for this recursive querying process.
  prefs: []
  type: TYPE_NORMAL
- en: The query engine retrieves relevant Nodes from the root Index based on the query.
    For each relevant Node, it identifies the corresponding child Index using the
    `index_id` stored in the Node’s relationships. It then queries the child Index
    with the original query to obtain more detailed information. This process continues
    recursively until all relevant sub-Indexes have been queried.
  prefs: []
  type: TYPE_NORMAL
- en: Custom query engines can be configured for each Index within the `ComposableGraph`
    , allowing for tailored retrieval strategies at different levels of the hierarchy.
    This enables a deep, hierarchical understanding of complex datasets by seamlessly
    integrating information from various levels of Indexes.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the `ComposableGraph` allows for the efficient retrieval of relevant
    information from both high-level summaries and detailed, low-level Indexes, enabling
    a comprehensive understanding of the underlying data.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered the Indexes available for our RAG implementation, it’s
    time to address the elephant in the room – **cost** .
  prefs: []
  type: TYPE_NORMAL
- en: <title>Estimating the potential cost of building and querying Indexes</title>
  prefs: []
  type: TYPE_NORMAL
- en: Estimating the potential cost of building and querying Indexes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: import tiktoken from llama_index.core import (     TreeIndex, SimpleDirectoryReader,
    Settings) from llama_index.core.llms.mock import MockLLM from llama_index.core.callbacks
    import (     CallbackManager, TokenCountingHandler) llm = MockLLM(max_tokens=256)
    token_counter = TokenCountingHandler(     tokenizer=tiktoken.encoding_for_model("gpt-3.5-turbo").encode
    ) callback_manager = CallbackManager([token_counter]) Settings.callback_manager=callback_manager
    Settings.llm=llm tiktoken.encoding_for_model("gpt-3.5-turbo").encode). documents
    = SimpleDirectoryReader(     "cost_prediction_samples").load_data() index = TreeIndex.from_documents(
        documents=documents,     num_children=2,     show_progress=True) print("Total
    LLM Token Count:", token_counter.total_llm_token_count) import tiktoken from llama_index.core
    import (     MockEmbedding, VectorStoreIndex,     SimpleDirectoryReader, Settings)
    from llama_index.core.callbacks import (     CallbackManager, TokenCountingHandler)
    from llama_index.core.llms.mock import MockLLM embed_model = MockEmbedding(embed_dim=1536)
    llm = MockLLM(max_tokens=256) token_counter = TokenCountingHandler(     tokenizer=tiktoken.encoding_for_model("gpt-3.5-turbo").encode
    ) callback_manager = CallbackManager([token_counter]) Settings.embed_model=embed_model
    Settings.llm=llm Settings.callback_manager=callback_manager documents = SimpleDirectoryReader(
        "cost_prediction_samples").load_data() index = VectorStoreIndex.from_documents(
        documents=documents,     show_progress=True) print("Embedding Token Count:",
        token_counter.total_embedding_token_count) query_engine = index.as_query_engine(service_context=service_context)
    response = query_engine.query("What's the cat's name?") print("Query LLM Token
    Count:", token_counter.total_llm_token_count) print("Query Embedding Token Count:",
        token_counter.total_embedding_token_count)
  prefs: []
  type: TYPE_NORMAL
- en: In a similar manner to metadata extractors, Indexes pose issues related to costs
    and data privacy. That is because, as we have seen in this chapter, most Indexes
    rely on LLMs to some extent – during building and/or querying.
  prefs: []
  type: TYPE_NORMAL
- en: Repeatedly calling LLMs to process large volumes of text can quickly break your
    budget if you’re not paying attention to your potential costs. For example, if
    you are building a `TreeIndex` or `KeywordTableIndex` from thousands of documents,
    those constant LLM invocations during Index construction will carry a significant
    cost. Embeddings can also rely on calls to external models; therefore, the `VectorStoreIndex`
    is another important source of costs. In my experience, prevention and prediction
    are the best ways to avoid nasty surprises and keep your expenses low.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like with metadata extraction, I’d start first by observing and applying
    some best practices:'
  prefs: []
  type: TYPE_NORMAL
- en: Use Indexes with no LLM calls during building where possible, such as `SummaryIndex`
    or `SimpleKeywordTableIndex` . This eliminates Index building costs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use cheaper LLM models. If full accuracy isn’t critical, cheaper LLM models
    with lower computational demands can be used but be aware of possible quality
    trade-offs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cache and reuse Indexes. Avoid rebuilding Indexes by caching and reusing previously
    constructed ones.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimize query parameters to minimize LLM calls during your search. For example,
    reducing `similarity_top_k` in `VectorStoreIndex` will reduce your query cost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use local models. To further manage costs and maintain data privacy when using
    Indexes in LlamaIndex, consider utilizing local LLM and embedding models instead
    of relying on hosted services. This approach not only offers more control over
    data privacy but also helps in reducing the dependency on external services, which
    can be costly. Using local models can significantly cut down on expenses, particularly
    when handling large volumes of data or when operating within strict budget constraints.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important side note regarding local AI models
  prefs: []
  type: TYPE_NORMAL
- en: Always remember that RAG introduces additional knowledge and contextual information
    into the model’s processing, effectively bridging the gap caused by a smaller
    training dataset. So, even for models that haven’t been trained on extensive or
    diverse data, RAG allows them to access a broader range of information beyond
    their initial training set, thus enhancing their performance and output quality.
  prefs: []
  type: TYPE_NORMAL
- en: These guidelines will definitely help you reduce costs, but it’s still a good
    idea to estimate before indexing larger datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a basic example of how we can estimate the LLM costs of building a
    `TreeIndex` using a `MockLLM` :'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous part, we first took care of the necessary imports. If you’re
    unfamiliar with the reasons to use `tiktoken` as a tokenizer here, head back to
    *Chapter 4* , *Ingesting Data into Our RAG Workflow* where we discussed estimating
    the potential cost of using metadata extractors. Let’s set up the `MockLLM` next:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We just created a `MockLLM` instance with a specified maximum token limit acting
    as a worst-case maximal cost. We then initialized `TokenCountingHandler` with
    a tokenizer that matches our real LLM model using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This handler will track token usage. This construct simulates an LLM without
    actually calling the `gpt-3.5-turbo` API:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve loaded our documents and are now ready to build the `TreeIndex` :'
  prefs: []
  type: TYPE_NORMAL
- en: After building the Index, the script displays the `total_llm_token_count` value
    stored in the `TokenCountingHandler` .
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we’re only using the `MockLLM` class because there are no embeddings
    used for building the `TreeIndex` . This allows us to estimate the worst-case
    LLM token cost before actually building the Index and invoking the real LLM. The
    same method can be applied to estimate query costs.
  prefs: []
  type: TYPE_NORMAL
- en: The main lesson here?
  prefs: []
  type: TYPE_NORMAL
- en: While Indexes unlock many capabilities, overuse without optimization can greatly
    impact costs. Always estimate token usage before indexing larger datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a second example. It’s similar to the previous one, but this time,
    we’re first estimating the embedding costs of building a `VectorStoreIndex` and
    after that, the total cost of querying the Index:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part took care of the imports. Next, we set up the `MockEmbedding`
    and `MockLLM` objects:'
  prefs: []
  type: TYPE_NORMAL
- en: 'After initializing the `MockEmbedding` and `MockLLM` objects, we defined a
    `TokenCountingHandler` and a `CallbackManager` and wrapped them into the custom
    `Settings` . It’s now time to load our sample documents and build the `VectorStoreIndex`
    using the custom `Settings` :'
  prefs: []
  type: TYPE_NORMAL
- en: If you have successfully cloned the book’s GitHub repo, the `cost_prediction_samples`
    subfolder in the `ch5` folder should contain a file with a fictional story about
    `Fluffy the cat` . The `VectorStoreIndex` uses an embedding model to encode document
    text into vectors during indexing. In our second example, we estimated the token
    costs of those embedding calls by using `MockEmbedding` and `TokenCountingHandler`
    . The embedding token count provides an indication of how expensive it will be
    to build this Index per document based on the text lengths.
  prefs: []
  type: TYPE_NORMAL
- en: 'To have a complete view, we can take this a step further and also estimate
    search costs:'
  prefs: []
  type: TYPE_NORMAL
- en: This shows the potential search fees as well by counting tokens for embedding
    lookups and synthesizing the response. We also had to use `MockLLM` to catch the
    LLM tokens hypothetically consumed during response synthesis.
  prefs: []
  type: TYPE_NORMAL
- en: So, in summary, follow preventive best practices and always forecast your Index
    build and query expenses before unleashing them on your full document collection!
  prefs: []
  type: TYPE_NORMAL
- en: It’s time to make some progress with our project. Let’s revisit our PITS project.
  prefs: []
  type: TYPE_NORMAL
