- en: '<html:html><html:head><html:title>Indexing data – a bird’s-eye view</html:title></html:head>
    <html:body><html:div class="epub-source"><html:h1 id="_idParaDest-108">Indexing
    data – a bird’s-eye view</html:h1> <html:div id="_idContainer052"><html:p>We briefly
    discussed the importance <html:a id="_idIndexMarker362"></html:a>and general functioning
    of Indexes in a RAG application in <html:a><html:span class="No-Break"><html:em
    class="italic">Chapter 3</html:em></html:span></html:a> , in the section titled
    <html:em class="italic">Uncovering the essential building blocks of LlamaIndex
    – documents, nodes, indexes</html:em> . Now, it is time to have a closer look
    at the different indexing methods available in LlamaIndex with their advantages,
    disadvantages, and specific <html:span class="No-Break">use cases.</html:span></html:p>
    <html:p>In principle, data can be accessed even without an Index. But it’s like
    reading a book without a table of contents. As long as it’s about a story that
    has continuity and can be read sequentially, section by section, and chapter by
    chapter, reading will be a pleasure. However, things change when we need to quickly
    search for a specific topic in that book. Without a table of contents, the search
    process will be slow <html:span class="No-Break">and cumbersome.</html:span></html:p>
    <html:p>In LlamaIndex, however, <html:strong class="bold">Indexes</html:strong>
    represent more than just <html:a id="_idIndexMarker363"></html:a>a simple table
    of contents. An Index provides not only the necessary structure for navigation
    but also the concrete mechanisms to update or access it. That includes the logic
    for the <html:strong class="bold">retrievers</html:strong> and the mechanisms
    used for fetching <html:a id="_idIndexMarker364"></html:a>data, which we will
    discuss in detail during <html:a><html:span class="No-Break"><html:em class="italic">Chapter
    6</html:em></html:span></html:a> , <html:em class="italic">Querying Our Data,
    Part 1 –</html:em> <html:span class="No-Break"><html:em class="italic">Context
    Retrieval</html:em></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:p>In this book, I’ve kept things simple, giving you the basics of how Indexes
    work and providing some examples to help you understand their usage. Exploring
    every possible way to use and mix these Indexes would be a huge task and that’s
    not what we’re <html:span class="No-Break">about here.</html:span></html:p> <html:p>We’ll
    talk later about what makes each type of Index unique, but first, let’s see what
    they all have <html:span class="No-Break">in common.</html:span></html:p> <html:a
    id="_idTextAnchor108"></html:a><html:h2 id="_idParaDest-109">Common features of
    all Index types</html:h2> <html:p>Each type of Index in LlamaIndex has its own
    characteristics and functions, but because all of them inherit the <html:code
    class="literal">BaseIndex</html:code> class, there are certain features and parameters
    they share, which can be customized <html:a id="_idIndexMarker365"></html:a>for
    any kind <html:span class="No-Break">of Index:</html:span></html:p> <html:ul><html:li><html:strong
    class="bold">Nodes</html:strong> : All Indexes are based on nodes and we can choose
    which Nodes are included in the Index. Plus, all Index types provide methods to
    insert new Nodes or delete existing ones, allowing for dynamic updates to the
    Index as your data changes. We can either build an Index from pre-existing Nodes
    by providing the Nodes directly to the Index constructor like this <html:code
    class="literal">vector_index = VectorStoreIndex(nodes)</html:code> or we can provide
    a list of documents as an input using <html:code class="literal">from_documents()</html:code>
    and let the Index extract the Nodes by itself. Keep in mind that we can use <html:code
    class="literal">Settings</html:code> – before actually building the Index – to
    customize its underlying mechanics. As we discussed during <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 3</html:em></html:span></html:a>
    in the <html:em class="italic">Understanding how settings can be used for customization</html:em>
    section, this simple class allows for different settings such as changing the
    LLM, embedding model, or default Node parser used by <html:span class="No-Break">an
    Index.</html:span></html:li> <html:li><html:strong class="bold">The storage context</html:strong>
    : The storage context defines how and where the data (documents and nodes) for
    the Index is stored. This customization is crucial for managing data storage efficiently,
    depending on the <html:span class="No-Break">application’s requirements.</html:span></html:li>
    <html:li><html:strong class="bold">Progress display</html:strong> : The <html:code
    class="literal">show_progress</html:code> option lets us choose whether to display
    progress bars during long-running operations such as building the Index. Implemented
    using the <html:code class="literal">tqdm</html:code> Python library, this feature
    can be useful for monitoring the progress of large <html:span class="No-Break">indexing
    tasks.</html:span></html:li> <html:li><html:strong class="bold">Different retrieval
    modes</html:strong> : Each Index allows for different pre-defined retrieval modes,
    which can be set to match the specific needs of your application. And you can
    also customize or extend the Retriever classes to change how queries are processed
    and how results are retrieved from the Index. More on that during <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 6</html:em></html:span></html:a>
    , <html:em class="italic">Querying Our Data, Part 1 –</html:em> <html:span class="No-Break"><html:em
    class="italic">Context Retrieval.</html:em></html:span></html:li> <html:li><html:strong
    class="bold">Asynchronous operations</html:strong> : The <html:code class="literal">use_async</html:code>
    parameter implemented by some of the Indexes determines whether certain operations
    should be performed asynchronously. Asynchronous processing allows the system
    to manage multiple operations concurrently, rather than waiting for each operation
    to be completed sequentially. This can be important for performance optimization,
    especially <html:a id="_idIndexMarker366"></html:a>when dealing with large datasets
    or <html:span class="No-Break">complex operations.</html:span></html:li></html:ul>
    <html:p class="callout-heading">Quick note</html:p> <html:p class="callout">An
    important thing to consider before diving further and starting to tinker with
    the sample code is that indexing often relies on LLM calls for summarizing or
    embedding purposes. Just like in the case of metadata extraction, which we covered
    in <html:a><html:span class="No-Break"><html:em class="italic">Chapter 4</html:em></html:span></html:a>
    , <html:em class="italic">Ingesting Data into Our RAG Workflow</html:em> , indexing
    in LlamaIndex may also raise cost and privacy concerns. Make sure you read the
    cost-related section at the end of this chapter before running any large-scale
    experiments to test <html:span class="No-Break">your ideas.</html:span></html:p>
    <html:p>Let’s start with our first and most frequently used <html:span class="No-Break">Index
    type.</html:span></html:p> <html:a id="_idTextAnchor109"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Understanding
    the VectorStoreIndex</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    class="H1---Chapter" id="_idParaDest-110">Understanding the VectorStoreIndex</html:h1>
    <html:div id="_idContainer052">from llama_index.core import VectorStoreIndex,
    SimpleDirectoryReader documents = SimpleDirectoryReader("files").load_data() index
    = VectorStoreIndex.from_documents(documents) print("Index created successfully!")
    pip install llama-index-embeddings-huggingface from llama_index.embeddings.huggingface
    import HuggingFaceEmbedding embedding_model = HuggingFaceEmbedding(     model_name="WhereIsAI/UAE-Large-V1"
    ) embeddings = embedding_model.get_text_embedding(     "The quick brown fox jumps
    over the lazy cat!" ) print(embeddings[:15]) <html:p style="font-style:italic;">As
    this ebook edition doesn''t have fixed pagination, the page numbers below are
    hyperlinked for reference only, based on the printed edition of this book.</html:p>
    <html:p>In LlamaIndex, the <html:code class="literal">VectorStoreIndex</html:code>
    stands out <html:a id="_idIndexMarker367"></html:a>as the workhorse, being the
    most commonly utilized type <html:span class="No-Break">of Index.</html:span></html:p>
    <html:p>For most RAG applications, a <html:code class="literal">VectorStoreIndex</html:code>
    might be the best solution because it facilitates the construction <html:a id="_idIndexMarker368"></html:a>of
    Indexes on collections of Documents where <html:strong class="bold">embeddings</html:strong>
    for the input text chunks <html:a id="_idIndexMarker369"></html:a>are stored within
    the <html:strong class="bold">Vector Store</html:strong> of the Index. Once constructed,
    this Index can be used for efficient <html:a id="_idIndexMarker370"></html:a>querying
    because it allows for <html:strong class="bold">similarity searches</html:strong>
    over the embedded representations of the text, making it highly suitable for applications
    requiring fast retrieval of relevant information from a large collection of data.
    Don’t worry if you’re not yet familiar with terms such as embeddings, vector store,
    or similarity searching, because we’ll cover them in the following sections. The
    <html:code class="literal">VectorStoreIndex</html:code> class in LlamaIndex supports
    these operations by default and also allows for asynchronous calls and progress
    tracking, which can improve performance and user experience <html:a id="_idIndexMarker371"></html:a>in
    typical <html:span class="No-Break">RAG scenarios.</html:span></html:p> <html:a
    id="_idTextAnchor110"></html:a><html:h2 id="_idParaDest-111">A simple usage example
    for the VectorStoreIndex</html:h2> <html:p>Here’s the most basic <html:a id="_idIndexMarker372"></html:a>way
    of constructing <html:span class="No-Break">a</html:span> <html:span class="No-Break"><html:code
    class="literal">VectorStoreIndex</html:code></html:span></html:p> <html:p>As you
    can see, in just a few lines of code, we have ingested the Documents and the <html:code
    class="literal">VectorStoreIndex</html:code> took care of everything. Note that
    using this approach, we have skipped the Node parsing step entirely, because the
    Index did that by itself by using <html:span class="No-Break">the</html:span>
    <html:span class="No-Break"><html:code class="literal">from_documents()</html:code></html:span>
    <html:span class="No-Break">method.</html:span></html:p> <html:p>There are several
    parameters <html:a id="_idIndexMarker373"></html:a>that we can customize for <html:span
    class="No-Break">the</html:span> <html:span class="No-Break"><html:code class="literal">VectorStoreIndex</html:code></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:ul><html:li><html:code
    class="literal">use_async</html:code> : This parameter enables asynchronous calls.
    By default, it is set <html:span class="No-Break">to</html:span> <html:span class="No-Break"><html:code
    class="literal">False</html:code></html:span> <html:span class="No-Break">.</html:span></html:li>
    <html:li><html:code class="literal">show_progress</html:code> : This parameter
    shows progress bars during Index construction. The default value <html:span class="No-Break">is</html:span>
    <html:span class="No-Break"><html:code class="literal">False</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:li> <html:li><html:code class="literal">store_nodes_override</html:code>
    : This parameter forces LlamaIndex to store Node objects in the Index store and
    document store, even if the vector store keeps text. This can be useful in scenarios
    where you need direct access to Node objects, even if their content is already
    stored in the vector store. We’ll talk in more detail about the Index store, document
    store, and vector store later in this chapter. The default setting for this parameter
    <html:span class="No-Break">is</html:span> <html:span class="No-Break"><html:code
    class="literal">False</html:code></html:span> <html:span class="No-Break">.</html:span></html:li></html:ul>
    <html:p>Let’s have a look at <html:span class="No-Break"><html:em class="italic">Figure
    5</html:em></html:span> <html:em class="italic">.1</html:em> for a visual representation
    of this type <html:span class="No-Break">of Index:</html:span></html:p> <html:p
    class="IMG---Caption" lang="en-US">Figure 5.1 – The structure of a VectorStoreIndex</html:p>
    <html:p>The <html:code class="literal">VectorStoreIndex</html:code> took the ingested
    Documents, breaking them down into Nodes. It used the default parameters for text
    splitter, chunk size, chunk overlap, and so on. Of course, we could have customized
    all these parameters if we <html:span class="No-Break">wanted to.</html:span></html:p>
    <html:p class="callout-heading">Note</html:p> <html:p class="callout"><html:strong
    class="bold">Fixed-size chunking</html:strong> simply splits text into same-sized
    <html:a id="_idIndexMarker374"></html:a>chunks, optionally with some overlap.
    Although computationally cheap and simple to implement, this simple chunking may
    not always be the best approach. Performance testing various chunk sizes is key
    to optimizing for an application’s <html:span class="No-Break">particular needs.</html:span></html:p>
    <html:p>The nodes containing chunks <html:a id="_idIndexMarker375"></html:a>of
    the original text were then <html:em class="italic">embedded</html:em> into a
    high-dimensional vector space using a language model. The embedded vectors were
    stored within the vector store component of the Index. Next, when a query is made,
    the query text will be similarly embedded and compared against <html:a id="_idIndexMarker376"></html:a>the
    stored vectors using a <html:strong class="bold">similarity measure</html:strong>
    identified with a method called <html:strong class="bold">cosine similarity</html:strong>
    . The most similar <html:a id="_idIndexMarker377"></html:a>vectors – and thus
    the most relevant document chunks – will be returned as the query result. This
    process enables rapid, semantically aware retrieval of information, leveraging
    the mathematical properties of vector spaces to find the documents that best answer
    the <html:span class="No-Break">user’s query.</html:span></html:p> <html:p>Sounds
    a bit confusing? Let’s go through these concepts together in the <html:span class="No-Break">next
    section.</html:span></html:p> <html:a id="_idTextAnchor111"></html:a><html:h2
    id="_idParaDest-112">Understanding embeddings</html:h2> <html:p>In simple terms,
    <html:strong class="bold">vector embeddings</html:strong> represent a machine-understandable
    <html:a id="_idIndexMarker378"></html:a>data format. They capture meaning and
    may conceptually represent a word, an entire document, or even non-textual information
    such as images and sounds. In a way, embeddings represent a standard language
    of thought for an LLM. In the context of an LLM, they serve as a foundational
    representation through which the model understands and processes information.
    They transform diverse and complex data into a uniform, high-dimensional space
    where the LLM can perform operations such as comparison, association, and prediction
    more effectively. <html:span class="No-Break"><html:em class="italic">Figure 5</html:em></html:span>
    <html:em class="italic">.2</html:em> provides an illustration of the process of
    <html:span class="No-Break">embedding data:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 5.2 – How an embedding model converts data into numerical
    representations</html:p> <html:p>Because it’s all about math under the hood. And
    math works well with numbers – more precisely, large lists of floating-point numbers,
    where each number represents a dimension in a hypothetical vector space. The LLM
    can work with these arrays of numbers to understand, interpret, and generate responses
    based on the input it receives. Essentially, these numbers in the vector embeddings
    allow the LLM to <html:em class="italic">see</html:em> and <html:em class="italic">think</html:em>
    about the data in a way that’s meaningful <html:span class="No-Break">and structured.</html:span></html:p>
    <html:p>The beauty of this system lies in its ability to handle ambiguity and
    complexity. The model can understand semantic relationships between words, such
    as synonyms, antonyms, and more complex linguistic patterns. In the case of polysemous
    words, the same word can have different meanings in different contexts. For example,
    the word <html:em class="italic">bank</html:em> can refer to the side of a river
    or a financial institution. Vector embeddings help the LLM understand these nuances
    by providing context-sensitive representations. So, in one situation, <html:em
    class="italic">bank</html:em> might be closely associated with words such as <html:em
    class="italic">river</html:em> and <html:em class="italic">shore</html:em> , while
    in another, it’s more closely linked to <html:em class="italic">money</html:em>
    <html:span class="No-Break">and</html:span> <html:span class="No-Break"><html:em
    class="italic">account</html:em></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:p class="callout-heading">Quick note</html:p> <html:p class="callout">An
    important factor to consider is that the size of text chunks being embedded impacts
    precision – too small and context is lost; too large and all that additional detail
    may dilute <html:span class="No-Break">the meaning.</html:span></html:p> <html:p>In
    case you’re not very familiar with embeddings yet, the following example could
    be useful to get a better grasp of the concept. Let’s assign some <html:em class="italic">arbitrary</html:em>
    vector embeddings to three randomly <html:span class="No-Break">chosen sentences:</html:span></html:p>
    <html:ul><html:li><html:strong class="bold">Sentence 1</html:strong> : The quick
    brown fox jumps over the <html:span class="No-Break">lazy dog</html:span></html:li>
    <html:li><html:strong class="bold">Sentence 2</html:strong> : A fast dark-colored
    fox leaps above a <html:span class="No-Break">sleepy canine</html:span></html:li>
    <html:li><html:strong class="bold">Sentence 3</html:strong> : Apples are sweet
    <html:span class="No-Break">and crunchy</html:span></html:li></html:ul> <html:p>In
    a real-world scenario, the embeddings associated with each of these sentences
    <html:a id="_idIndexMarker379"></html:a>would be calculated automatically by using
    an <html:strong class="bold">embedding model</html:strong> . This is a specialized
    artificial intelligence model used to convert complex data such as text, images,
    or graphs into a numerical format. The embeddings would also normally be high-dimensional,
    but for the sake of explanation, I’ll use simple, three-dimensional, arbitrarily
    chosen vectors. Here are the hypothetical embeddings for the <html:span class="No-Break">three
    sentences:</html:span></html:p> <html:ul><html:li><html:strong class="bold">Sentence
    1 Embedding</html:strong> : [0.8, <html:span class="No-Break">0.1, 0.3]</html:span></html:li>
    <html:li><html:strong class="bold">Sentence 2 Embedding</html:strong> : [0.79,
    <html:span class="No-Break">0.14, 0.32]</html:span></html:li> <html:li><html:strong
    class="bold">Sentence 3 Embedding</html:strong> : [0.2, <html:span class="No-Break">0.9,
    0.5]</html:span></html:li></html:ul> <html:p>These numbers are purely <html:a
    id="_idIndexMarker380"></html:a>conceptual and are meant to show that sentences
    1 and 2, which have similar meanings, have embeddings that are closer to each
    other in vector space. <html:em class="italic">Sentence 3</html:em> , which has
    a different meaning, has an embedding that is farther away from the first two.
    Have a look at <html:span class="No-Break"><html:em class="italic">Figure 5</html:em></html:span>
    <html:em class="italic">.3</html:em> for a straightforward visual comparison of
    the <html:span class="No-Break">three embeddings:</html:span></html:p> <html:p
    class="IMG---Caption" lang="en-US">Figure 5.3 – A comparison of the three embedded
    sentences in a 3D space</html:p> <html:p>When we visualize them in a three-dimensional
    space, sentences 1 and 2 are plotted near each other, while sentence 3 will be
    plotted at a distance. This spatial representation is what allows machine learning
    models to determine <html:span class="No-Break">semantic similarity.</html:span></html:p>
    <html:p>When you search using a query on a vector store Index in order to retrieve
    useful context, LlamaIndex converts your search terms into a similar embedding
    and then finds the closest matches among the pre-computed embeddings of your <html:span
    class="No-Break">text chunks.</html:span></html:p> <html:p>We call this process
    <html:strong class="bold">similarity</html:strong> or <html:strong class="bold">distance
    search</html:strong> . So, when you encounter <html:a id="_idIndexMarker381"></html:a>the
    term <html:strong class="bold">top-k similarity search</html:strong> , you should
    know that it relies <html:a id="_idIndexMarker382"></html:a>on an algorithm <html:a
    id="_idIndexMarker383"></html:a>that calculates <html:a id="_idIndexMarker384"></html:a>the
    similarity between vector embeddings. It takes a vector embedding as an input
    and returns the most similar <html:em class="italic">k</html:em> number of vectors
    found in the vector store. Because the initial vector and the <html:em class="italic">top-k</html:em>
    returned neighbors are similar to each other, we can consider their meanings to
    be conceptually similar. Now you understand why <html:a id="_idIndexMarker385"></html:a>I
    have previously called embeddings a <html:em class="italic">standard language
    of thought</html:em> for an LLM. It doesn’t really matter anymore whether they
    represent text, images, or any other types of information. We measure their similarity
    <html:span class="No-Break">in numbers.</html:span></html:p> <html:p>The only
    thing that may be implemented differently, depending on our use case, is the actual
    formula for defining <html:a id="_idIndexMarker386"></html:a>that distance <html:span
    class="No-Break">or similarity.</html:span></html:p> <html:p>Spoiler alert: a
    bit of mathematical concepts <html:span class="No-Break">up next.</html:span></html:p>
    <html:a id="_idTextAnchor112"></html:a><html:h2 id="_idParaDest-113">Understanding
    similarity search</html:h2> <html:p>In the realms of machine learning <html:a
    id="_idIndexMarker387"></html:a>and deep learning, the concept of similarity search
    is very important. It forms the backbone of many applications, from recommendation
    systems and information retrieval to clustering and classification tasks. As models
    and systems interact with high-dimensional data, identifying patterns and relationships
    between data points becomes essential. This involves measuring how <html:em class="italic">close</html:em>
    or <html:em class="italic">similar</html:em> data elements are, a task that often
    takes place in a vector space where each item is represented as <html:span class="No-Break">a
    vector.</html:span></html:p> <html:p>Locating points in this space that are near
    each other enables machines to assess similarity and, by extension, to make decisions,
    draw inferences, or, in our case, retrieve information based on that assessment
    of closeness. With the advent of embeddings in deep learning, the need for effective
    similarity search has grown. As embeddings capture the semantic meaning of the
    data they represent, performing similarity searches on these vectors allows machines
    to understand content at a level approaching <html:span class="No-Break">human
    cognition.</html:span></html:p> <html:p>Let’s explore the methods that LlamaIndex
    currently employs to measure the similarity between vectors, each with its unique
    advantages <html:span class="No-Break">and applicability.</html:span></html:p>
    <html:h3>Cosine similarity</html:h3> <html:p>This method measures <html:a id="_idIndexMarker388"></html:a>the
    cosine <html:a id="_idIndexMarker389"></html:a>of <html:em class="italic">the
    angle</html:em> between two vectors. Imagine two arrows pointing in different
    directions; the smaller the angle between them, the more similar <html:span class="No-Break">they
    are.</html:span></html:p> <html:p>Have a look at <html:span class="No-Break"><html:em
    class="italic">Figure 5</html:em></html:span> <html:em class="italic">.4</html:em>
    , which depicts a cosine similarity comparison between <html:span class="No-Break">two
    vectors:</html:span></html:p> <html:p class="IMG---Caption" lang="en-US">Figure
    5.4 – How a cosine similarity comparison would look</html:p> <html:p>In terms
    of embeddings, a small angle (or a high cosine similarity score, close to 1) indicates
    that the content they represent is similar. This method is particularly useful
    in text analysis because it is less affected by the length of the documents and
    focuses more on their direction <html:a id="_idIndexMarker390"></html:a>or orientation
    <html:a id="_idIndexMarker391"></html:a>in the <html:span class="No-Break">vector
    space.</html:span></html:p> <html:p class="callout-heading">Note</html:p> <html:p
    class="callout">Cosine similarity is also the default method used by LlamaIndex
    for calculating similarity <html:span class="No-Break">between embeddings.</html:span></html:p>
    <html:h3>Dot product</html:h3> <html:p>Also called the <html:strong class="bold">scalar
    product</html:strong> , because it is represented <html:a id="_idIndexMarker392"></html:a>by
    a single value, this <html:a id="_idIndexMarker393"></html:a>is another method
    <html:a id="_idIndexMarker394"></html:a>of calculating how well two vectors align
    with each other. To calculate the scalar product of two vectors, the algorithm
    multiplies the corresponding elements of the vectors and then sums <html:span
    class="No-Break">these products.</html:span></html:p> <html:p>Let’s take a simple
    example of <html:em class="italic">vector A</html:em> : [2,3] and <html:em class="italic">vector
    B</html:em> : [4,1]. The <html:strong class="bold">dot product</html:strong> is
    calculated by multiplying their corresponding elements: (2×4) + (3×1), which gives
    us 8 + 3 = 11\. Thus, the dot product of these two vectors <html:span class="No-Break">is
    11.</html:span></html:p> <html:p><html:span class="No-Break"><html:em class="italic">Figure
    5</html:em></html:span> <html:em class="italic">.5</html:em> exemplifies <html:span
    class="No-Break">this concept:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 5.5 – Calculating similarity using the dot product method</html:p>
    <html:p>In the preceding diagram, the dot product is visualized by projecting
    one vector onto the other. This projection illustrates the geometric interpretation
    of the dot product. It’s calculated by projecting the components of one vector
    in the direction of the other and then multiplying these projected components
    by the corresponding components of the second vector. The sum of these products
    gives us the dot product. This visualization helps us understand that the dot
    product is not just a measure of how vectors point in the same direction; it also
    incorporates <html:span class="No-Break">their lengths.</html:span></html:p> <html:p>Higher
    values of the dot product mean higher similarities between vectors. In contrast
    with the cosine method, the dot <html:a id="_idIndexMarker395"></html:a>product
    is sensitive both to the length <html:a id="_idIndexMarker396"></html:a>of the
    two vectors compared and their relative direction. Unlike the dot product, cosine
    similarity normalizes the dot product by the magnitudes of the vectors. This normalization
    makes cosine similarity solely a measure of the directional alignment between
    vectors, independent of <html:span class="No-Break">their lengths.</html:span></html:p>
    <html:p>The longer the vectors, the higher the result, and this is an important
    thing to consider in a RAG scenario. Longer vectors, which might represent longer
    documents or more detailed information, could dominate the retrieved results due
    to their inherently larger dot product values. This could bias the system toward
    retrieving longer <html:a id="_idIndexMarker397"></html:a>documents, even if they
    <html:a id="_idIndexMarker398"></html:a>are not the <html:span class="No-Break">most
    relevant.</html:span></html:p> <html:h3>Euclidean distance</html:h3> <html:p>This
    method is different <html:a id="_idIndexMarker399"></html:a>from the dot product
    and cosine similarity <html:a id="_idIndexMarker400"></html:a>methods. While those
    methods look at the angle or alignment between vectors, <html:strong class="bold">Euclidean
    distance</html:strong> is all about how close the actual values of the vectors
    are to each other. This can be especially useful when the values in the vectors
    represent actual counts or measurements, especially where the vector dimensions
    have real-world <html:span class="No-Break">physical interpretations.</html:span></html:p>
    <html:p>Take a look at <html:span class="No-Break"><html:em class="italic">Figure
    5</html:em></html:span> <html:em class="italic">.6</html:em> for a visual representation
    of <html:span class="No-Break">Euclidean distance:</html:span></html:p> <html:p
    class="IMG---Caption" lang="en-US">Figure 5.6 – The Euclidean distance between
    two vectors</html:p> <html:p>You should now have a foundational understanding
    of embeddings, how vector similarity works, and, in particular, how they are implemented
    in LlamaIndex. If you want to familiarize yourself better with this concept, you
    can find more information on <html:span class="No-Break">the web.</html:span></html:p>
    <html:p>Here are some <html:a id="_idIndexMarker401"></html:a>suggested <html:a
    id="_idIndexMarker402"></html:a>additional reading resources <html:a id="_idIndexMarker403"></html:a>you
    could start <html:span class="No-Break">with:</html:span> <html:a><html:span class="No-Break">https://developers.google.com/machine-learning/clustering/similarity/measuring-similarity</html:span></html:a></html:p>
    <html:a id="_idTextAnchor113"></html:a><html:h2 id="_idParaDest-114">OK, but how
    does LlamaIndex generate these embeddings?</html:h2> <html:p>The short answer
    is, <html:em class="italic">however, you prefer</html:em> . By default, the framework
    <html:a id="_idIndexMarker404"></html:a>is configured to rely on OpenAI’s <html:code
    class="literal">text-embedding-ada-002</html:code> model. This model has been
    trained to produce embeddings that effectively capture semantic meanings of the
    text, enabling applications such as semantic search, topic clustering, anomaly
    detection, and others. It provides a very good balance between quality, performance,
    and cost. LlamaIndex uses this model by default to embed documents during Index
    construction as well as for <html:span class="No-Break">query embeddings.</html:span></html:p>
    <html:p>Sometimes, though, when you may want to index large volumes of data, the
    cost associated with a hosted model such as this one may be too high for your
    budget. In other instances, you might be concerned about the privacy of your proprietary
    data and prefer to use a local model instead. Or maybe, in some cases, you may
    want to use more specialized models for a particular topic or <html:span class="No-Break">technical
    domain.</html:span></html:p> <html:p>The great news is that LlamaIndex also supports
    a variety of other embedding models. For example, if you wish to use local models,
    you can set the service context to use a local embedding, which uses a well-balanced
    <html:a id="_idIndexMarker405"></html:a>default model provided by <html:em class="italic">Hugging
    Face</html:em> ( <html:a>https://huggingface.co/BAAI/bge-small-en-v1.5</html:a>
    ). This can be particularly useful if you aim to reduce costs or have requirements
    to process <html:span class="No-Break">data locally.</html:span></html:p> <html:h3>A
    brief introduction to Hugging Face</html:h3> <html:p><html:strong class="bold">Hugging
    Face</html:strong> is a very important resource <html:a id="_idIndexMarker406"></html:a>in
    the AI field, primarily known <html:a id="_idIndexMarker407"></html:a>for its
    extensive collection <html:a id="_idIndexMarker408"></html:a>of pre-trained machine
    learning models, especially in <html:strong class="bold">natural</html:strong>
    <html:strong class="bold">language processing</html:strong> ( <html:strong class="bold">NLP</html:strong>
    ). Its importance lies in democratizing access to state-of-the-art AI models,
    tools, and techniques, enabling developers and researchers to implement advanced
    AI functionalities <html:a id="_idIndexMarker409"></html:a>with relative ease.
    Similar to GitHub, Hugging Face embraces <html:a id="_idIndexMarker410"></html:a>a
    community-driven approach, where users can share, collaborate on, and improve
    AI models, much like developers share and contribute to code repositories on GitHub.
    This community-centric model accelerates innovation and dissemination of <html:span
    class="No-Break">AI advancements.</html:span></html:p> <html:p>Before running
    the next sample, make sure you install the <html:span class="No-Break">necessary
    integration:</html:span></html:p> <html:p>This example will show you how to set
    up a local <html:span class="No-Break">embedding model:</html:span></html:p> <html:p>On
    the first run, the code will download the <html:code class="literal">Universal
    AnglE Embedding</html:code> model from Hugging Face. This is one of the best-performing
    embedding models at the moment, offering great overall performance and <html:span
    class="No-Break">quality balance.</html:span></html:p> <html:p>More info is available
    <html:span class="No-Break">here:</html:span> <html:a><html:span class="No-Break">https://huggingface.co/WhereIsAI/UAE-Large-V1</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>After downloading and
    initializing the embedding model, the script calculates the embeddings for the
    sentence and displays the first 15 values of <html:span class="No-Break">the vector.</html:span></html:p>
    <html:p>For advanced users or specific applications, LlamaIndex makes it easy
    to integrate custom embedding models. You can simply extend the <html:code class="literal">BaseEmbedding</html:code>
    class provided by LlamaIndex and implement your own logic for <html:span class="No-Break">generating
    embeddings.</html:span></html:p> <html:p>Here, you can find <html:a id="_idIndexMarker411"></html:a>an
    example of how to define your custom embedding <html:span class="No-Break">class:</html:span>
    <html:a><html:span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/embeddings/custom_embeddings.html</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>Apart from OpenAI and
    local <html:a id="_idIndexMarker412"></html:a>models, there are integrations with
    Langchain, enabling <html:a id="_idIndexMarker413"></html:a>you to use any embedding
    model they offer. You also have the option to use embedding models from Azure,
    CohereAI, and other providers through additional integrations offered by LlamaIndex.
    This great flexibility ensures that no matter your needs or constraints, you can
    configure LlamaIndex to use an embedding model that is suitable for <html:span
    class="No-Break">your application.</html:span></html:p> <html:a id="_idTextAnchor114"></html:a><html:h2
    id="_idParaDest-115">How do I decide which embedding model I should use?</html:h2>
    <html:p>The choice of embedding model <html:a id="_idIndexMarker414"></html:a>can
    significantly affect the performance, quality, and cost of your RAG app. Here
    are some key points to consider when choosing a <html:span class="No-Break">particular
    model:</html:span></html:p> <html:ul><html:li><html:strong class="bold">Qualitative
    performance</html:strong> : Different embedding models may encode the semantics
    of the text in different ways. While embeddings of models such as OpenAI’s Ada
    are designed to have a broad understanding of text, other models might be fine-tuned
    on specific domains or tasks and would outperform in those scenarios. Domain-specific
    models could lead to more accurate representations of <html:span class="No-Break">specialized
    topics</html:span></html:li> <html:li><html:strong class="bold">Quantitative performance</html:strong>
    : This includes factors such as how well the model captures semantic similarity,
    its performance on benchmarks, and generalization to unseen data. This can vary
    considerably between different models and domains of application. For a general
    benchmark <html:a id="_idIndexMarker415"></html:a>of the most popular models,
    you can consult the <html:strong class="bold">Massive Text Embedding Benchmark</html:strong>
    ( <html:strong class="bold">MTEB</html:strong> ) Leaderboard ( <html:a>https://huggingface.co/spaces/mteb/leaderboard</html:a>
    ) on the Hugging <html:span class="No-Break">Face website.</html:span></html:li>
    <html:li><html:strong class="bold">Latency and throughput</html:strong> : For
    applications with real-time constraints or large volumes of data, the speed of
    the embedding model could be a deciding factor. Also consider the maximum input
    chunk sizes that models can handle, which impacts how text is divided for embedding.
    Keep in mind that your Nodes will have embeddings computed during ingestion, so
    that will not affect your overall application performance. However, during retrieval,
    each query will have to be embedded in real time so that similarity can be measured
    and the relevant nodes can be retrieved. This is where latency and throughput
    <html:span class="No-Break">become important.</html:span> <html:p class="list-inset">To
    get an idea of how different embedding models may perform, have a look at this
    <html:span class="No-Break">article:</html:span> <html:a><html:span class="No-Break">https://blog.getzep.com/text-embedding-latency-a-semi-scientific-look/</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p></html:li> <html:li><html:strong
    class="bold">Multilingual support</html:strong> : Embedding models can be multilingual
    or trained for a specific language. Depending on your use case, this can also
    become an important decision factor. For example, smaller models such as <html:code
    class="literal">Mistral</html:code> could provide excellent results on par with
    hosted models such as GPT 3.5 for English data, but their performance in other
    languages is <html:span class="No-Break">clearly inferior</html:span></html:li>
    <html:li><html:strong class="bold">Resource requirements</html:strong> : Embedding
    models can vary greatly in size and computational expense. Large models might
    provide more accurate embeddings but may require substantially more computational
    resources and thus lead to <html:span class="No-Break">higher costs.</html:span></html:li>
    <html:li><html:strong class="bold">Availability</html:strong> : Some embedding
    models may only be available through certain APIs or require specific software
    to be installed, which could affect ease of integration and usage. Fortunately,
    you have a high degree of customization available <html:span class="No-Break">in
    LlamaIndex.</html:span></html:li> <html:li><html:strong class="bold">On-device
    or local usage</html:strong> : You may prefer to use a local model when data privacy
    is a concern or when operating in an environment with limited or no <html:span
    class="No-Break">internet access.</html:span></html:li> <html:li><html:strong
    class="bold">Usage cost</html:strong> : Consider the cost associated with API
    calls for cloud-based, hosted embedding models <html:a id="_idIndexMarker416"></html:a>versus
    the computational and storage costs of local <html:span class="No-Break">embedding
    models.</html:span></html:li></html:ul> <html:p>The good news is that LlamaIndex
    supports many out-of-the-box embedding models and provides flexibility to use
    <html:span class="No-Break">various embeddings.</html:span></html:p> <html:p>By
    the way, a complete list <html:a id="_idIndexMarker417"></html:a>of supported
    models can be found <html:span class="No-Break">here:</html:span> <html:a><html:span
    class="No-Break">https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#list-of-supported-embeddings</html:span></html:a></html:p>
    <html:p>For most use cases, though, OpenAI’s default embedding model – <html:code
    class="literal">text-embedding-ada-002</html:code> – will provide you with a good
    balance between all the parameters we’ve discussed. However, if you have specific
    needs or constraints, you might benefit from exploring and benchmarking different
    models to see which provides the best outcomes for your <html:span class="No-Break">particular
    application.</html:span></html:p> <html:p>Now that we know about embeddings, let
    us shift our focus to how to store and <html:span class="No-Break">reuse them.</html:span></html:p>
    <html:a id="_idTextAnchor115"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Persisting
    and reusing Indexes</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-116">Persisting and reusing Indexes</html:h1> <html:div id="_idContainer052">from
    llama_index.core import VectorStoreIndex, SimpleDirectoryReader documents = SimpleDirectoryReader("data").load_data()
    index = VectorStoreIndex.from_documents(documents) index.storage_context.persist(persist_dir="index_cache")
    print("Index persisted to disk.") from llama_index.core import StorageContext,
    load_index_from_storage storage_context = StorageContext.from_defaults(     persist_dir="index_cache")
    index = load_index_from_storage(storage_context) print("Index loaded successfully!")
    pip install chromadb import chromadb from llama_index.vector_stores.chroma import
    ChromaVectorStore from llama_index.core import (     VectorStoreIndex, SimpleDirectoryReader,
    StorageContext) db = chromadb.PersistentClient(path="chroma_database") chroma_collection
    = db.get_or_create_collection(     "my_chroma_store" ) vector_store = ChromaVectorStore(
        chroma_collection=chroma_collection ) storage_context = StorageContext.from_defaults(
        vector_store=vector_store ) documents = SimpleDirectoryReader("files").load_data()
    index = VectorStoreIndex.from_documents(     documents=documents,     storage_context=storage_context
    ) results = chroma_collection.get() print(results) index = VectorStoreIndex.from_vector_store(
        vector_store=vector_store,     storage_context=storage_context ) <html:p>An
    important question <html:a id="_idIndexMarker418"></html:a>arises – where exactly
    <html:a id="_idIndexMarker419"></html:a>can we store the vector embeddings generated
    during the <html:span class="No-Break">indexing process?</html:span></html:p>
    <html:p>Storing them is important <html:a id="_idIndexMarker420"></html:a>for
    <html:span class="No-Break">multiple reasons:</html:span></html:p> <html:ul><html:li>Avoid
    the computational cost of re-embedding documents and rebuilding Indexes in every
    session. Generating high-quality embeddings for large document collections requires
    significant processing that can become costly over time. Persisting Indexes preserves
    these <html:span class="No-Break">precomputed artifacts</html:span></html:li>
    <html:li>Enable low-latency processing. Avoiding runtime embedding and indexing
    by loading the already computed embeddings allows applications to get up and running
    <html:span class="No-Break">much faster</html:span></html:li> <html:li>Maintain
    query consistency and accuracy. Reloading an Index guarantees we reuse the exact
    vectors and structure used in the previous sessions. This promises consistent
    and accurate <html:span class="No-Break">query execution</html:span></html:li></html:ul>
    <html:p>If we want to avoid regenerating them on each run, these vector embeddings
    need to reside somewhere – a repository, if you will – that allows for efficient
    storage <html:span class="No-Break">and retrieval.</html:span></html:p> <html:p>This
    is the job of a vector store <html:span class="No-Break">within LlamaIndex.</html:span></html:p>
    <html:p>By default, LlamaIndex uses an in-memory vector store, but for persistence,
    it offers a straightforward approach using the <html:code class="literal">.persist()</html:code>
    method available for any type of Index. This method writes all data to disk at
    a specified location, <html:span class="No-Break">ensuring persistence.</html:span></html:p>
    <html:p>Let’s see how we can persist and then load the vector embeddings. First,
    we create our Index, which handles the embedding <html:span class="No-Break">of
    documents:</html:span></html:p> <html:p>To persist this data, we use <html:span
    class="No-Break">the</html:span> <html:span class="No-Break"><html:code class="literal">persist()</html:code></html:span>
    <html:span class="No-Break">method:</html:span></html:p> <html:p>This saves the
    entire Index data to disk. In future sessions, we can easily reload <html:span
    class="No-Break">the data:</html:span></html:p> <html:p>By rebuilding a <html:code
    class="literal">StorageContext</html:code> from <html:a id="_idIndexMarker421"></html:a>the
    persisted <html:a id="_idIndexMarker422"></html:a>directory and using <html:code
    class="literal">load_index_from_storage</html:code> , we can effectively reconstitute
    our Index without needing to re-index <html:span class="No-Break">our data.</html:span></html:p>
    <html:a id="_idTextAnchor116"></html:a><html:h2 id="_idParaDest-117">Understanding
    the StorageContext</html:h2> <html:p>The <html:code class="literal">StorageContext</html:code>
    serves as the unifying custodian <html:a id="_idIndexMarker423"></html:a>over
    configurable storage components <html:a id="_idIndexMarker424"></html:a>used during
    indexing and querying. Its key components are <html:span class="No-Break">as follows:</html:span></html:p>
    <html:ul><html:li>The <html:strong class="bold">Document store</html:strong> (
    <html:code class="literal">docstore</html:code> ): This manages the storage <html:a
    id="_idIndexMarker425"></html:a>of documents. The data is locally stored in a
    file <html:span class="No-Break">named</html:span> <html:span class="No-Break"><html:code
    class="literal">docstore.json</html:code></html:span> <html:span class="No-Break">.</html:span></html:li>
    <html:li>The <html:strong class="bold">Index Store</html:strong> ( <html:code
    class="literal">index_store</html:code> ): This manages the storage <html:a id="_idIndexMarker426"></html:a>of
    Index structures. Indexes are stored locally in a file <html:span class="No-Break">called</html:span>
    <html:span class="No-Break"><html:code class="literal">index_store.json</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:li> <html:li><html:strong class="bold">Vector
    Stores</html:strong> ( <html:code class="literal">vector_stores</html:code> ):
    This is a dictionary managing <html:a id="_idIndexMarker427"></html:a>multiple
    vector stores, each potentially serving a different purpose. The vector stores
    are stored locally <html:span class="No-Break">in</html:span> <html:span class="No-Break"><html:code
    class="literal">vector_store.json</html:code></html:span> <html:span class="No-Break">.</html:span></html:li>
    <html:li>The <html:strong class="bold">Graph Store</html:strong> ( <html:code
    class="literal">graph_store</html:code> ): This manages the storage <html:a id="_idIndexMarker428"></html:a>of
    graph data structures. A file named <html:code class="literal">graph_store.json</html:code>
    is automatically created by LlamaIndex for storing <html:span class="No-Break">the
    graphs.</html:span></html:li></html:ul> <html:p>The <html:code class="literal">StorageContext</html:code>
    class encapsulates document, vector, index, and graph data stores under one umbrella.
    The files mentioned in the previous list for locally storing the data are automatically
    created by LlamaIndex when we invoke the <html:code class="literal">persist()</html:code>
    method. If we prefer not to save them in the current folder, we can provide a
    specific persistence location from where we can load them in <html:span class="No-Break">future
    sessions.</html:span></html:p> <html:p>Out-of-the-box, LlamaIndex offers basic
    local stores, but we can swap them with more capable persistence solutions such
    as <html:em class="italic">AWS S3,</html:em> <html:em class="italic">Pinecone</html:em>
    , <html:em class="italic">MongoDB</html:em> , <html:span class="No-Break">and
    others.</html:span></html:p> <html:p>As an example, let’s explore customizing
    vector storage using ChromaDB, an efficient open source <html:span class="No-Break">vector
    engine.</html:span></html:p> <html:p>First, make sure <html:a id="_idIndexMarker429"></html:a>you
    install <html:code class="literal">chromadb</html:code> <html:span class="No-Break">using
    pip:</html:span></html:p> <html:p>The first part of the code takes care of the
    <html:span class="No-Break">necessary imports:</html:span></html:p> <html:p>We
    then continue by initializing the Chroma client and creating a collection within
    Chroma to store <html:span class="No-Break">our data:</html:span></html:p> <html:p>In
    ChromaDB, we create <html:strong class="bold">collections</html:strong> to store
    data. These are similar <html:a id="_idIndexMarker430"></html:a>to <html:em class="italic">tables</html:em>
    in relational databases. The <html:code class="literal">my_chroma_store</html:code>
    collection will hold <html:span class="No-Break">our embeddings.</html:span></html:p>
    <html:p>Next, we initialize a tailored vector store using <html:code class="literal">ChromaVectorStore</html:code>
    and wire it into <html:span class="No-Break">the</html:span> <html:span class="No-Break"><html:code
    class="literal">StorageContext</html:code></html:span> <html:span class="No-Break">:</html:span></html:p>
    <html:p>We’re now ready to ingest our documents and build <html:span class="No-Break">the
    Index:</html:span></html:p> <html:p>We can now use the <html:code class="literal">get()</html:code>
    method to display the entire contents of the <html:span class="No-Break">Chroma
    collection:</html:span></html:p> <html:p>Subsequently, restoring this Index in
    future sessions is also <html:span class="No-Break">very simple:</html:span></html:p>
    <html:p>We just rebuilt our <html:span class="No-Break">original Index.</html:span></html:p>
    <html:p>By wrapping <html:strong class="bold">vector databases</html:strong> such
    as ChromaDB, LlamaIndex <html:a id="_idIndexMarker431"></html:a>makes enterprise-scale
    vector storage accessible through a simple storage abstraction. The complexity
    is concealed, enabling you to focus on your application logic while still leveraging
    industrial-strength <html:span class="No-Break">data infrastructure.</html:span></html:p>
    <html:p>In summary, LlamaIndex <html:a id="_idIndexMarker432"></html:a>provides
    flexibility in vector storage – from a simple in-memory store for testing to cloud-hosted
    databases for large, real-world deployments. And through storage <html:a id="_idIndexMarker433"></html:a>integrations,
    swapping any component is <html:span class="No-Break">a breeze!</html:span></html:p>
    <html:a id="_idTextAnchor117"></html:a><html:h2 id="_idParaDest-118">The difference
    between vector stores and vector databases</html:h2> <html:p>The terms vector
    store and vector database <html:a id="_idIndexMarker434"></html:a>are often used
    in the context of managing and querying large sets of vectors, which are commonly
    used in machine learning, particularly in applications involving NLP, image recognition,
    and similar tasks. You may have already noticed that I’m using them quite often
    in this chapter, sometimes implying they are similar concepts. However, there
    is a subtle distinction between <html:span class="No-Break">the two:</html:span></html:p>
    <html:ul><html:li><html:strong class="bold">Vector store</html:strong> : This
    generally refers to a storage system or repository where vectors are stored. The
    vectors are high-dimensional and represent complex data such as text, images,
    or audio in a format that can be processed by machine learning models. A vector
    store focuses primarily on the efficient storage of these vectors. It might not
    have advanced capabilities for querying or analyzing the data and its main purpose
    is to maintain a large repository of vectors that can be retrieved and used for
    various machine <html:span class="No-Break">learning tasks</html:span></html:li>
    <html:li><html:strong class="bold">Vector database</html:strong> : A vector database,
    on the other hand, is a more sophisticated system that not only stores vectors
    but also provides advanced functionalities for querying and analyzing them. This
    includes the ability to perform similarity searches and other complex operations
    that are useful in machine learning and data analysis. A vector database is designed
    to handle the nuances of vector data, such as their high dimensionality and the
    need for specialized indexing techniques to enable efficient search and retrieval.
    In <html:span class="No-Break">a nutshell</html:span></html:li></html:ul> <html:p>While
    a vector store is more about <html:a id="_idIndexMarker435"></html:a>the storage
    aspect, a vector database encompasses both storage and the complex querying capabilities
    required for vector data. This makes vector databases particularly important in
    applications where it’s necessary to search through large volumes of vectorized
    data quickly <html:span class="No-Break">and accurately.</html:span></html:p>
    <html:p>One distinguishing feature usually representative of a vector database
    and less often provided <html:a id="_idIndexMarker436"></html:a>by vector stores
    is the support for <html:code class="literal">CRUD</html:code> ( <html:code class="literal">create</html:code>
    <html:strong class="bold">,</html:strong> <html:code class="literal">read</html:code>
    <html:strong class="bold">,</html:strong> <html:code class="literal">update</html:code>
    <html:strong class="bold">,</html:strong> <html:code class="literal">delete</html:code>
    ) functions. Whether or not a vector store offers <html:code class="literal">CRUD</html:code>
    functions can vary depending on the specific implementation and design of the
    store. However, in general, a vector store, especially if it’s a simplified or
    basic form of storage for vector data, might not support all the <html:code class="literal">CRUD</html:code>
    operations in the same way a traditional database system would. Let’s break down
    the <html:span class="No-Break">typical operations:</html:span></html:p> <html:ul><html:li><html:strong
    class="bold">Create</html:strong> : The ability to add new vectors <html:a id="_idIndexMarker437"></html:a>to
    the store is usually a fundamental feature. This is essential for building up
    the <html:span class="No-Break">vector repository.</html:span></html:li> <html:li><html:strong
    class="bold">Read</html:strong> : Reading or retrieving vectors based on some
    form of identifier or criterion is also a common feature. In a basic vector store,
    this might be limited to simple retrieval rather than <html:span class="No-Break">complex
    queries.</html:span></html:li> <html:li><html:strong class="bold">Update</html:strong>
    : Updating existing vectors in a vector store might not be as straightforward
    or as commonly supported as in traditional databases. This is because vector data,
    often used in machine learning and similar applications, is usually generated
    in a fixed form and not <html:span class="No-Break">frequently updated.</html:span></html:li>
    <html:li><html:strong class="bold">Delete</html:strong> : The capability to delete
    vectors may be supported, but like updating, it may not be a primary feature,
    depending on the use case of the <html:span class="No-Break">vector store.</html:span></html:li></html:ul>
    <html:p>In many machine learning and AI applications, once vectors are created
    and stored, they are not frequently updated or deleted, which is why some vector
    stores might focus more on efficient storage and retrieval (create and read operations)
    rather than full <html:span class="No-Break">CRUD functionality.</html:span></html:p>
    <html:p>In contrast to a simple vector store, a vector database, which is more
    sophisticated, is more likely to offer complete CRUD <html:a id="_idIndexMarker438"></html:a>capabilities,
    allowing for more dynamic and flexible management of the <html:span class="No-Break">vector
    data.</html:span></html:p> <html:p>Here’s a good starting point in your journey
    <html:a id="_idIndexMarker439"></html:a>toward a better understanding of vector
    <html:span class="No-Break">databases:</html:span> <html:a><html:span class="No-Break">https://learn.microsoft.com/en-us/semantic-kernel/memories/vector-db</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:a id="_idTextAnchor118"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Exploring
    other index types in LlamaIndex</html:title></html:head> <html:body><html:div
    class="epub-source"><html:h1 class="H1---Chapter" id="_idParaDest-119">Exploring
    other index types in LlamaIndex</html:h1> <html:div id="_idContainer052">from
    llama_index.core import SummaryIndex, SimpleDirectoryReader documents = SimpleDirectoryReader("files").load_data()
    index = SummaryIndex.from_documents(documents) query_engine = index.as_query_engine()
    response = query_engine.query("How many documents have you loaded?") print(response)
    I have loaded two documents. from llama_index.core import (     DocumentSummaryIndex,
    SimpleDirectoryReader) documents = SimpleDirectoryReader("files").load_data()
    index = DocumentSummaryIndex.from_documents(     documents,     show_progress=True
    ) summary1 = index.get_document_summary(documents[0].doc_id) summary2 = index.get_document_summary(documents[1].doc_id)
    print("\n Summary of the first document: " + summary1) print("\n Summary of the
    second document: " + summary2) from llama_index.core import KeywordTableIndex,
    SimpleDirectoryReader documents = SimpleDirectoryReader("files").load_data() index
    = KeywordTableIndex.from_documents(documents) query_engine = index.as_query_engine()
    response = query_engine.query("     What famous buildings were in ancient Rome?")
    print(response) from llama_index.core import TreeIndex, SimpleDirectoryReader
    documents = SimpleDirectoryReader("files").load_data() index = TreeIndex.from_documents(documents)
    query_engine = index.as_query_engine() response = query_engine.query("Tell me
    about dogs") print(response) from llama_index.core import (     KnowledgeGraphIndex,
    SimpleDirectoryReader) documents = SimpleDirectoryReader("files").load_data()
    index = KnowledgeGraphIndex.from_documents(     documents, max_triplets_per_chunk=2,
    use_async=True) query_engine = index.as_query_engine() response = query_engine.query("Tell
    me about dogs.") print(response) <html:p style="font-style:italic;">As this ebook
    edition doesn''t have fixed pagination, the page numbers below are hyperlinked
    for reference only, based on the printed edition of this book.</html:p> <html:p>While
    the <html:code class="literal">VectorStoreIndex</html:code> may be the star <html:a
    id="_idIndexMarker440"></html:a>of the show in most of our RAG scenarios, LlamaIndex
    provides many other useful indexing tools. They all have specific features and
    use cases and the following section will explore them in <html:span class="No-Break">more
    detail.</html:span></html:p> <html:a id="_idTextAnchor119"></html:a><html:h2 id="_idParaDest-120">The
    SummaryIndex</html:h2> <html:p>The <html:code class="literal">SummaryIndex</html:code>
    offers <html:a id="_idIndexMarker441"></html:a>a straightforward <html:a id="_idIndexMarker442"></html:a>yet
    powerful way of indexing data for retrieval purposes. Unlike the <html:code class="literal">VectorStoreIndex</html:code>
    , which focuses on embeddings within a vector store, the <html:code class="literal">SummaryIndex</html:code>
    is based on a simple data structure where nodes are stored in a sequence. You’ll
    find a simple depiction of the structure of the <html:code class="literal">SummaryIndex</html:code>
    in <html:span class="No-Break"><html:em class="italic">Figure 5</html:em></html:span>
    <html:span class="No-Break"><html:em class="italic">.7</html:em></html:span> <html:span
    class="No-Break">:</html:span></html:p> <html:p class="IMG---Caption" lang="en-US">Figure
    5.7 – The structure of a SummaryIndex</html:p> <html:p>When building the Index,
    it ingests a collection of documents, splits them into smaller chunks, and then
    compiles these chunks into a sequential list. Everything runs locally, without
    involving an LLM <html:a id="_idIndexMarker443"></html:a>or any <html:span class="No-Break">embedding</html:span>
    <html:span class="No-Break"><html:a id="_idIndexMarker444"></html:a></html:span><html:span
    class="No-Break">model.</html:span></html:p> <html:h3>Practical use case</html:h3>
    <html:p>Imagine we would create <html:a id="_idIndexMarker445"></html:a>a documentation
    search tool within a software development project. Often, software projects accumulate
    extensive documentation over time, including technical specifications, API documentation,
    user guides, and developer notes. Keeping track of this information can become
    challenging, especially when the team needs to quickly reference specific details.
    Implementing a <html:code class="literal">SummaryIndex</html:code> for the project’s
    documentation repository allows developers to perform quick searches across all
    documents. For example, a developer could query <html:em class="italic">What are
    the error handling procedures for the payment gateway API?</html:em> The <html:code
    class="literal">SummaryIndex</html:code> would scan through the indexed documentation
    to retrieve relevant sections where error handling is discussed, without the need
    for complex embedding models or intensive computational resources. This Index
    would be particularly useful in environments where maintaining an extensive vector
    store would not be viable due to resource constraints or where simplicity and
    speed <html:span class="No-Break">are prioritized.</html:span></html:p> <html:p>The
    <html:code class="literal">SummaryIndex</html:code> is particularly effective
    for applications where a linear scan through data is sufficient or where complex
    embedding-based retrieval is not required. It’s a more basic form of indexing
    but still versatile enough for various use cases, especially in scenarios <html:a
    id="_idIndexMarker446"></html:a>where you need a simple way to index <html:span
    class="No-Break">your data.</html:span></html:p> <html:h3>A simple usage model
    for the SummaryIndex</html:h3> <html:p>Creating a <html:code class="literal">SummaryIndex</html:code>
    is a <html:span class="No-Break">straightforward</html:span> <html:span class="No-Break"><html:a
    id="_idIndexMarker447"></html:a></html:span><html:span class="No-Break">process:</html:span></html:p>
    <html:p>Here, Nodes are created from our sample files and the <html:code class="literal">SummaryIndex</html:code>
    is instantiated with these Nodes. This simple model enables quick setup without
    the complexity of embedding or using <html:span class="No-Break">vector storage.</html:span></html:p>
    <html:p>If you have correctly cloned the structure of our book’s GitHub repository
    and have a <html:code class="literal">files</html:code> subfolder containing two
    text <html:a id="_idIndexMarker448"></html:a>files, the output of the previous
    code snippet should be <html:span class="No-Break">the following:</html:span></html:p>
    <html:h3>Understanding the inner workings of the SummaryIndex</html:h3> <html:p>Internally,
    the <html:code class="literal">SummaryIndex</html:code> operates by storing <html:a
    id="_idIndexMarker449"></html:a>each node in a list-like structure. When a query
    is executed, the Index iterates through this list to find relevant nodes. While
    this process is less complex than embedding-based searches in <html:code class="literal">VectorStoreIndex</html:code>
    , it’s still effective for <html:span class="No-Break">many applications.</html:span></html:p>
    <html:p>The Index can be used with various retrievers such as <html:code class="literal">SummaryIndexRetriever</html:code>
    , <html:code class="literal">SummaryIndexEmbeddingRetriever</html:code> , and
    <html:code class="literal">SummaryIndexLLMRetriever</html:code> , each providing
    different mechanisms for searching and retrieving data. During queries, the <html:code
    class="literal">SummaryIndex</html:code> employs a <html:em class="italic">create
    and refine</html:em> approach to formulate responses. Initially, it assembles
    a preliminary answer based on the first chunk of text. This initial response is
    subsequently refined by incorporating additional text chunks as contextual information.
    The refinement process involves either maintaining the initial answer, slightly
    modifying it, or entirely <html:a id="_idIndexMarker450"></html:a>rephrasing the
    original response. We’ll cover the retrieval part in detail during <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 6</html:em></html:span></html:a>
    , <html:em class="italic">Querying Our Data, Part 1 –</html:em> <html:span class="No-Break"><html:em
    class="italic">Context Retrieval</html:em></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:a id="_idTextAnchor120"></html:a><html:h2 id="_idParaDest-121">The DocumentSummaryIndex</html:h2>
    <html:p>LlamaIndex’s arsenal <html:a id="_idIndexMarker451"></html:a>of indexing
    tools <html:a id="_idIndexMarker452"></html:a>extends beyond its well-regarded
    <html:code class="literal">VectorStoreIndex</html:code> , encompassing a variety
    of specialized Indexes designed for diverse applications. Among these, the <html:code
    class="literal">DocumentSummaryIndex</html:code> stands out for its unique approach
    to document management <html:span class="No-Break">and retrieval.</html:span></html:p>
    <html:p>At its core, the <html:code class="literal">DocumentSummaryIndex</html:code>
    is designed to optimize information retrieval by summarizing Documents and mapping
    these summaries to their corresponding Nodes within the Index. This process facilitates
    efficient data retrieval, using the summaries to quickly identify <html:span class="No-Break">relevant
    Documents.</html:span></html:p> <html:p><html:span class="No-Break"><html:em class="italic">Figure
    5</html:em></html:span> <html:em class="italic">.8</html:em> provides a visual
    representation of <html:span class="No-Break">this mechanism:</html:span></html:p>
    <html:p class="IMG---Caption" lang="en-US">Figure 5.8 – The DocumentSummaryIndex</html:p>
    <html:p>This Index operates by first creating a summary for each ingested Document.
    These summaries are then linked to the Document’s Nodes, forming a structured
    Index that enables fast and accurate <html:span class="No-Break">data retrieval.</html:span></html:p>
    <html:p>The <html:code class="literal">DocumentSummaryIndex</html:code> is particularly
    useful for handling queries where a succinct overview of the document content
    can significantly narrow down the search space, making it a great tool for applications
    requiring quick access to specific Documents in a large and <html:span class="No-Break">diverse
    dataset.</html:span></html:p> <html:p>For example, a practical <html:a id="_idIndexMarker453"></html:a>use
    case for the <html:code class="literal">DocumentSummaryIndex</html:code> is in
    the development of a knowledge <html:a id="_idIndexMarker454"></html:a>management
    system within a large organization. In such an environment, employees often need
    quick access to a vast array of documents, including reports, research papers,
    policy documents, and technical manuals. These documents are typically stored
    across different departments and may be extensive in length, making it challenging
    to quickly find specific information relevant to a user’s query. In addition,
    multiple documents may contain similar chunks of text, making a simple embedding-based
    retrieval impractical over the <html:span class="No-Break">entire dataset.</html:span></html:p>
    <html:p>Several parameters <html:a id="_idIndexMarker455"></html:a>can be customized
    for this <html:span class="No-Break">particular Index:</html:span></html:p> <html:ul><html:li><html:code
    class="literal">response_synthesizer</html:code> : This parameter allows you to
    specify a response synthesizer that is responsible for generating summaries. By
    customizing this parameter, you can control the summarization process, adjusting
    it to fit specific needs or preferences in how summaries <html:span class="No-Break">are
    generated.</html:span></html:li> <html:li><html:code class="literal">summary_query</html:code>
    : This parameter is used to define the query that guides the summarization process.
    Essentially, it tells the response synthesizer what kind of summary to generate
    for each Document. The default query asks for a summary that describes what the
    Document is about and what questions it can answer. Adjusting this query allows
    you to tailor the focus and style of the summaries, making them more relevant
    to the specific use cases of <html:span class="No-Break">the Index.</html:span></html:li>
    <html:li><html:code class="literal">show_progress</html:code> : This Boolean parameter
    determines whether to display progress bars during operations that can take a
    significant amount of time. Setting this to <html:code class="literal">True</html:code>
    provides visual feedback on the progress of <html:span class="No-Break">these
    operations.</html:span></html:li> <html:li><html:code class="literal">embed_summaries</html:code>
    : When set to <html:code class="literal">True</html:code> – which is the default
    – this parameter indicates that the summaries should be embedded. Embedded summaries
    can then be used for similarity comparisons and retrieval in an embedding-based
    search. This is particularly useful for scenarios where you want to retrieve Nodes
    based on the similarity between the Document summary content and the user <html:a
    id="_idIndexMarker456"></html:a>query. We’ll cover this in more detail during
    <html:a><html:span class="No-Break"><html:em class="italic">Chapter 6</html:em></html:span></html:a>
    , <html:em class="italic">Querying Our Data, Part 1 –</html:em> <html:span class="No-Break"><html:em
    class="italic">Context Retrieval</html:em></html:span> <html:span class="No-Break"><html:em
    class="italic">.</html:em></html:span></html:li></html:ul> <html:p>Let’s now see
    how to use <html:span class="No-Break">the</html:span> <html:span class="No-Break"><html:code
    class="literal">DocumentSummaryIndex</html:code></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:h3>A simple usage model for the DocumentSummaryIndex</html:h3> <html:p>Creating
    a <html:code class="literal">DocumentSummaryIndex</html:code> involves a series
    <html:a id="_idIndexMarker457"></html:a>of steps, starting with the aggregation
    of Documents and their subsequent summarization. The following code snippet demonstrates
    the basic setup for creating <html:span class="No-Break">this Index:</html:span></html:p>
    <html:p>This process involves reading documents from a directory, parsing them
    into Nodes, summarizing the Documents, and then associating the corresponding
    Nodes with these summaries for quick retrieval. Next, let’s observe the summaries
    that were generated in <html:span class="No-Break">the process:</html:span></html:p>
    <html:p>The second part of the code sample displays the summaries that were generated
    for each Document. These summaries were associated with the underlying Nodes for
    each Document. During retrieval, this association will allow extracting only the
    relevant Nodes, based on the user query and the summary of <html:span class="No-Break">each
    Document.</html:span></html:p> <html:p>Internally, the <html:code class="literal">DocumentSummaryIndex</html:code>
    supports both embedding-based and LLM-based retrievers, allowing <html:a id="_idIndexMarker458"></html:a>for
    flexible retrieval mechanisms that cater to different needs. By default, the Index
    also generates embeddings for each summary in order to facilitate embedding-based
    retrieval, which is particularly useful for <html:span class="No-Break">similarity
    searches.</html:span></html:p> <html:a id="_idTextAnchor121"></html:a><html:h2
    id="_idParaDest-122">The KeywordTableIndex</html:h2> <html:p>The <html:code class="literal">KeywordTableIndex</html:code>
    in LlamaIndex implements <html:a id="_idIndexMarker459"></html:a>a clever <html:a
    id="_idIndexMarker460"></html:a>architecture – similar to a glossary of terms
    – for rapidly matching queries to relevant nodes based on important terms. Unlike
    complex embedding spaces, this structure relies on a straightforward keyword table,
    yet proves highly effective for targeted factual lookup. This Index extracts keywords
    from documents and constructs a keyword-to-node mapping, offering a highly efficient
    <html:span class="No-Break">search mechanism.</html:span></html:p> <html:p>It’s
    particularly useful in scenarios where precise keyword matching is vital for retrieving
    relevant information. These keywords become the reference keys in a central lookup
    table, each one pointing to associated nodes such as a glossary definition. During
    retrieval, just like scanning a glossary for entries of interest, relevant nodes
    containing a particular keyword are identified <html:a id="_idIndexMarker461"></html:a>and
    returned. See <html:span class="No-Break"><html:em class="italic">Figure 5</html:em></html:span>
    <html:em class="italic">.9</html:em> for a <html:span class="No-Break">visual
    representation:</html:span></html:p> <html:p class="IMG---Caption" lang="en-US">Figure
    5.9 – The structure of a KeywordTableIndex</html:p> <html:p>The customizable <html:a
    id="_idIndexMarker462"></html:a>parameters for the <html:code class="literal">KeywordTableIndex</html:code>
    are <html:span class="No-Break">as follows:</html:span></html:p> <html:ul><html:li><html:code
    class="literal">keyword_extract_template</html:code> : This is an optional prompt
    template used for keyword extraction. Custom prompts can be specified to change
    how keywords are extracted from text, allowing for tailored keyword extraction
    strategies. We’ll talk more about prompt customization during <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 10</html:em></html:span></html:a>
    <html:span class="No-Break"><html:em class="italic">.</html:em></html:span></html:li>
    <html:li><html:code class="literal">max_keywords_per_chunk</html:code> : This
    sets the maximum number of keywords to extract from each text chunk. By using
    this parameter, we can make sure the keyword table remains manageable and focused
    on the most relevant keywords. The default value <html:span class="No-Break">is</html:span>
    <html:span class="No-Break"><html:code class="literal">10</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:li> <html:li><html:code class="literal">use_async</html:code>
    : This determines whether to use asynchronous calls. This can improve performance,
    especially when handling large datasets or complex operations. Its default <html:a
    id="_idIndexMarker463"></html:a>setting <html:span class="No-Break">is</html:span>
    <html:span class="No-Break"><html:code class="literal">False</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:li></html:ul> <html:p>Next up,
    we will create <html:span class="No-Break">the</html:span> <html:span class="No-Break"><html:code
    class="literal">KeywordTableIndex</html:code></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:h3>A simple usage model for the KeywordTableIndex</html:h3> <html:p>Creating
    a <html:code class="literal">KeywordTableIndex</html:code> is <html:span class="No-Break">very</html:span>
    <html:span class="No-Break"><html:a id="_idIndexMarker464"></html:a></html:span><html:span
    class="No-Break">straightforward:</html:span></html:p> <html:p>Here, the Index
    automatically extracts keywords from your data and sets up a keyword table, streamlining
    the process of setting up a keyword-based <html:span class="No-Break">retrieval
    system.</html:span></html:p> <html:p>Just like in the previous example, if you
    have correctly cloned the structure of our GitHub repository and have a <html:code
    class="literal">files</html:code> subfolder containing two text files, the output
    of the previous code snippet should be something along the lines of <html:em class="italic">The
    Colosseum and the Pantheon were famous buildings in</html:em> <html:span class="No-Break"><html:em
    class="italic">ancient Rome</html:em></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:h3>How does the KeywordTableIndex operate?</html:h3> <html:p>The <html:code
    class="literal">KeywordTableIndex</html:code> builds and operates <html:a id="_idIndexMarker465"></html:a>a
    keyword table, akin to a glossary, where each keyword is linked to relevant nodes.
    The Index initially processes a collection of documents, breaking them down into
    smaller chunks. For each chunk, the Index uses the LLM with a specially designed
    prompt to identify and extract relevant keywords. These keywords, which may range
    from simple terms to short phrases, are subsequently cataloged in the keyword
    table. Each keyword in this table is directly linked to the chunk of text from
    which it <html:span class="No-Break">was derived.</html:span></html:p> <html:p>Upon
    receiving a query, the Index identifies keywords within it and matches them with
    the table entries, enabling rapid and accurate retrieval of related chunks containing
    those keywords. It supports various retrieval modes, including simple keyword
    matching and advanced <html:a id="_idIndexMarker466"></html:a>techniques such
    as <html:strong class="bold">RAKE</html:strong> or LLM-based keyword extraction
    and matching. We’ll talk more about these retrieval modes during <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 6</html:em></html:span></html:a>
    , <html:em class="italic">Querying Our Data, Part 1 –</html:em> <html:span class="No-Break"><html:em
    class="italic">Context Retrieval</html:em></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:p class="callout-heading">Quick note on the RAKE extraction method</html:p>
    <html:p class="callout">This method is particularly effective <html:a id="_idIndexMarker467"></html:a>in
    identifying phrases or keywords that are significant within a body of text. The
    key idea behind RAKE is that keywords often consist of multiple words but rarely
    include punctuation, stop words, or words with minimal lexical meaning. The <html:code
    class="literal">KeywordTableIndex</html:code> has two similar alternatives that
    are designed to operate without the assistance of an LLM: <html:code class="literal">SimpleKeywordTableIndex</html:code>
    , which uses a simple regex extractor, and <html:code class="literal">RAKEKeywordTableIndex</html:code>
    , which relies on a RAKE keyword extractor based on the <html:code class="literal">rake_nltk</html:code>
    (Natural Language Toolkit) <html:span class="No-Break">Python package.</html:span></html:p>
    <html:p>You should know that, just like the <html:code class="literal">SummaryIndex</html:code>
    , the <html:code class="literal">KeywordTableIndex</html:code> also uses a <html:em
    class="italic">create and refine</html:em> approach when synthesizing <html:a
    id="_idIndexMarker468"></html:a>the final response. The adaptability of the <html:code
    class="literal">KeywordTableIndex</html:code> makes it a versatile tool for diverse
    applications where keyword precision <html:span class="No-Break">is key.</html:span></html:p>
    <html:a id="_idTextAnchor122"></html:a><html:h2 id="_idParaDest-123">The TreeIndex</html:h2>
    <html:p>The <html:code class="literal">TreeIndex</html:code> introduces a <html:a
    id="_idIndexMarker469"></html:a>hierarchical approach <html:a id="_idIndexMarker470"></html:a>to
    information organization and retrieval. Unlike a simple list, this structure organizes
    data in a hierarchical <html:span class="No-Break">tree format.</html:span></html:p>
    <html:p>Have a look at <html:span class="No-Break"><html:em class="italic">Figure
    5</html:em></html:span> <html:em class="italic">.10</html:em> for a diagram depicting
    the structure of <html:span class="No-Break">the</html:span> <html:span class="No-Break"><html:code
    class="literal">TreeIndex</html:code></html:span> <html:span class="No-Break">:</html:span></html:p>
    <html:p class="IMG---Caption" lang="en-US">Figure 5.10 – The structure of a TreeIndex</html:p>
    <html:p>Each node in this tree can represent a piece of data or information, similar
    to a branch or leaf on a real tree. This structural formation allows for efficient
    handling and querying of data. The <html:code class="literal">TreeIndex</html:code>
    first takes in a set of documents as input. It then builds up a tree in a bottom-up
    fashion; each parent node is able to summarize the child nodes using a general
    summarization prompt, and each intermediate node contains text summarizing the
    components below it. This summary is generated using an LLM based on a prompt
    template that can be customized with the <html:code class="literal">summary_prompt</html:code>
    parameter. <html:code class="literal">TreeIndex</html:code> acts like an organizer
    <html:a id="_idIndexMarker471"></html:a>and summarizer, taking lots of individual
    pieces of data, grouping them together, and creating a summary <html:a id="_idIndexMarker472"></html:a>that
    captures <html:span class="No-Break">their essence.</html:span></html:p> <html:h3>Customizable
    parameters for the TreeIndex</html:h3> <html:p>Apart from the general customization
    <html:a id="_idIndexMarker473"></html:a>inherited from the <html:code class="literal">BaseIndex</html:code>
    class, the <html:code class="literal">TreeIndex</html:code> provides the <html:span
    class="No-Break">following parameters:</html:span></html:p> <html:ul><html:li><html:code
    class="literal">summary_template</html:code> : This is a prompt for summarization,
    used during Index construction. This prompt can be customized for better control
    of the <html:span class="No-Break">summarization process.</html:span></html:li>
    <html:li><html:code class="literal">insert_prompt</html:code> : This is a prompt
    used by the Index for tree insertion, facilitating Index construction. This prompt
    facilitates the insertion of nodes into the tree. It guides how new information
    is integrated into the existing tree structure. We’ll cover details about prompt
    customization during <html:a><html:span class="No-Break"><html:em class="italic">Chapter
    10</html:em></html:span></html:a> , <html:em class="italic">Prompt Engineering
    Guidelines and</html:em> <html:span class="No-Break"><html:em class="italic">Best
    Practices.</html:em></html:span></html:li> <html:li><html:code class="literal">num_children</html:code>
    : This defines the maximum number of child nodes each node should have. This parameter
    controls the breadth of the tree, impacting its level of detail at each node.
    By default, this is set <html:span class="No-Break">to</html:span> <html:span
    class="No-Break"><html:code class="literal">10</html:code></html:span> <html:span
    class="No-Break">.</html:span></html:li> <html:li><html:code class="literal">build_tree</html:code>
    : This is a Boolean indicating whether to build the tree during Index construction.
    If we don’t use the default value – which is <html:code class="literal">True</html:code>
    – the Index will build its tree during query time instead of building it during
    the Index construction. Setting the <html:code class="literal">build_tree</html:code>
    parameter to <html:code class="literal">False</html:code> could be useful in scenarios
    where you might want to manually control the tree-building process or modify the
    tree structure after <html:span class="No-Break">initial construction.</html:span></html:li>
    <html:li><html:code class="literal">use_async</html:code> : This determines whether
    asynchronous <html:a id="_idIndexMarker474"></html:a>operation mode should <html:span
    class="No-Break">be used.</html:span></html:li></html:ul> <html:p>Next, let’s
    create a <html:span class="No-Break">simple</html:span> <html:span class="No-Break"><html:code
    class="literal">TreeIndex</html:code></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:h3>A simple usage model for the TreeIndex</html:h3> <html:p>To implement
    a <html:code class="literal">TreeIndex</html:code> , you can <html:a id="_idIndexMarker475"></html:a>follow
    this <html:span class="No-Break">simple example:</html:span></html:p> <html:p>This
    process involves the <html:code class="literal">TreeIndex</html:code> taking in
    documents, structuring them hierarchically, and then allowing for queries <html:a
    id="_idIndexMarker476"></html:a>that leverage this structure for efficient <html:span
    class="No-Break">data retrieval.</html:span></html:p> <html:h3>The inner mechanics
    of the TreeIndex</html:h3> <html:p>The index-building process <html:a id="_idIndexMarker477"></html:a>is
    recursive. After the first level of parent nodes is created, the builder can repeat
    the process, summarizing these parent nodes into higher-level nodes, and so on.
    This creates multiple levels in the tree, with each level abstracting and summarizing
    the information from the level below it. Also, for large datasets, the Index can
    handle data asynchronously with <html:code class="literal">use_async</html:code>
    . This means it can process multiple parts of the data simultaneously, making
    the building process faster and <html:span class="No-Break">more efficient.</html:span></html:p>
    <html:p>By using LLMs for summaries, the <html:code class="literal">TreeIndex</html:code>
    can encapsulate a nuanced understanding of the data. This is particularly useful
    for complex datasets where relationships and <html:span class="No-Break">context
    matter.</html:span></html:p> <html:h3>For example, in organizations</html:h3>
    <html:p>In organizations with complex hierarchical data <html:a id="_idIndexMarker478"></html:a>such
    as reports, memos, and research papers, a <html:code class="literal">TreeIndex</html:code>
    can organize this information efficiently, allowing for quick retrieval of specific
    data points within their knowledge <html:span class="No-Break">management systems.</html:span></html:p>
    <html:p><html:code class="literal">TreeIndex</html:code> operates by building
    a tree where each node is a summarized representation of its children, offering
    a clear and organized view of <html:span class="No-Break">the data.</html:span></html:p>
    <html:p>This Index supports <html:a id="_idIndexMarker479"></html:a>several <html:span
    class="No-Break">retrieval modes:</html:span></html:p> <html:ul><html:li><html:code
    class="literal">TreeSelectLeafRetriever</html:code> : This traverses the tree
    to find leaf nodes that can best answer a query. It involves choosing a specific
    number of child nodes at each level <html:span class="No-Break">for traversal.</html:span></html:li>
    <html:li><html:code class="literal">TreeSelectLeafEmbeddingRetriever</html:code>
    : This utilizes embedding similarity between the query and node text to traverse
    the tree, selecting leaf nodes based on <html:span class="No-Break">this similarity.</html:span></html:li>
    <html:li><html:code class="literal">TreeRootRetriever</html:code> : This directly
    retrieves answers from the root nodes of the tree. This method assumes the graph
    already stores the answer, so it doesn’t parse information down <html:span class="No-Break">the
    tree.</html:span></html:li> <html:li><html:code class="literal">TreeAllLeafRetriever</html:code>
    : This builds a query-specific tree from all leaf nodes to return a response.
    It rebuilds the tree for each query, making it suitable for scenarios <html:a
    id="_idIndexMarker480"></html:a>where the tree structure doesn’t need to be built
    <html:span class="No-Break">during initialization.</html:span></html:li></html:ul>
    <html:p>During query time, the tree Index <html:a id="_idIndexMarker481"></html:a>operates
    in the <html:span class="No-Break">following way:</html:span></html:p> <html:ol><html:li>First,
    the provided query string is processed to extract <html:span class="No-Break">relevant
    keywords</html:span></html:li> <html:li>Beginning from the root Node, the Index
    navigates through the <html:span class="No-Break">tree structure</html:span></html:li>
    <html:li>At each Node, it determines whether the keywords are found in the <html:span
    class="No-Break">Node’s summary</html:span></html:li> <html:li>If keywords are
    found, the Index proceeds to explore the Node’s <html:span class="No-Break">child
    Nodes</html:span></html:li> <html:li>If the keywords are absent, the Index advances
    to the <html:span class="No-Break">subsequent Node</html:span></html:li> <html:li>This
    process persists until a leaf Node is encountered or all Nodes in the tree have
    <html:span class="No-Break">been examined</html:span></html:li></html:ol> <html:p>The
    reached leaf Nodes represent the context with the highest likelihood of relevance
    to the <html:span class="No-Break">given query.</html:span></html:p> <html:p>We’ll
    cover the retrievers in more detail during <html:a><html:span class="No-Break"><html:em
    class="italic">Chapter 6</html:em></html:span></html:a> , <html:em class="italic">Querying
    Our Data, Part 1 –</html:em> <html:span class="No-Break"><html:em class="italic">Context
    Retrieval</html:em></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:h3>Some potential drawbacks of using the TreeIndex</html:h3> <html:p>Using
    a <html:code class="literal">TreeIndex</html:code> in our RAG workflow <html:a
    id="_idIndexMarker482"></html:a>can potentially be less advantageous compared
    to simpler retrieval methods. Here are a few <html:span class="No-Break">reasons
    why:</html:span></html:p> <html:ul><html:li><html:em class="italic">Increased
    computation</html:em> : Building and maintaining a <html:code class="literal">TreeIndex</html:code>
    requires additional computational resources. During the Index construction phase,
    the tree structure needs to be created by recursively summarizing and organizing
    the Nodes. This process involves applying summarization using LLM calls and constructing
    the hierarchical structure, which can be computationally intensive, especially
    for <html:span class="No-Break">large datasets.</html:span></html:li> <html:li><html:em
    class="italic">Recursive retrieval</html:em> : When querying the Index, the retrieval
    process involves traversing the tree structure from the root nodes down to the
    relevant leaf nodes. This recursive traversal can require multiple steps and computations,
    especially if the tree is deep or if multiple branches need to be explored. Each
    step in the traversal may involve comparing the query with the Node summaries
    and making decisions on which branches to follow. This recursive process can be
    more computationally expensive compared to retrieving from a <html:span class="No-Break">flat
    Index.</html:span></html:li> <html:li><html:em class="italic">Summarization overhead</html:em>
    : This Index relies on summarizing the content of each node to provide a concise
    representation of its child Nodes. The summarization process needs to be performed
    during Index construction and potentially during updates or insertions, adding
    to the overall <html:span class="No-Break">computational overhead.</html:span></html:li>
    <html:li><html:em class="italic">Storage requirements</html:em> : Storing a <html:code
    class="literal">TreeIndex</html:code> requires additional storage compared to
    a flat Index. The Index needs to store the tree structure, Node summaries, and
    metadata associated with each Node. This extra storage overhead can increase storage
    costs, especially for <html:span class="No-Break">large-scale datasets.</html:span></html:li>
    <html:li><html:em class="italic">Maintenance and updates</html:em> : Maintaining
    a <html:code class="literal">TreeIndex</html:code> requires regular updates and
    re-organization as new data is added or existing data is modified. Inserting new
    nodes or updating existing nodes in the tree structure may trigger a cascading
    effect, requiring updates to the parent nodes and their summaries. This maintenance
    process can be more complex and time-consuming compared to <html:span class="No-Break">other
    Indexes.</html:span></html:li></html:ul> <html:p>However, it’s important to note
    that the higher costs associated with using a <html:code class="literal">TreeIndex</html:code>
    can be justified in certain scenarios. If the RAG application deals with a large-scale
    dataset and requires efficient and context-aware retrieval, the benefits of using
    this type of Index may outweigh the additional costs. Its hierarchical structure
    and summarization capabilities can lead to improved retrieval performance, reduced
    search space, and better response generation quality. By traversing the tree from
    the root Nodes and selectively exploring relevant branches, the model can quickly
    narrow down the search to the most promising Nodes. This can lead to faster retrieval
    times and improved efficiency compared to searching through a flat <html:span
    class="No-Break">Index structure.</html:span></html:p> <html:p>The key is to assess
    <html:a id="_idIndexMarker483"></html:a>the specific requirements, scale, and
    constraints of the RAG scenario to determine whether the benefits of using a <html:code
    class="literal">TreeIndex</html:code> justify the potential increase in costs.
    Careful evaluation and benchmarking can help in making an informed decision based
    on the trade-offs between retrieval efficiency, generation quality, and computational
    and <html:span class="No-Break">storage costs.</html:span></html:p> <html:a id="_idTextAnchor123"></html:a><html:h2
    id="_idParaDest-124">The KnowledgeGraphIndex</html:h2> <html:p>The <html:code
    class="literal">KnowledgeGraphIndex</html:code> enhances query <html:a id="_idIndexMarker484"></html:a>processing
    <html:a id="_idIndexMarker485"></html:a>by constructing a <html:strong class="bold">knowledge
    graph</html:strong> ( <html:strong class="bold">KG</html:strong> ) from extracted
    <html:strong class="bold">triplets</html:strong> . This type of Index primarily
    <html:a id="_idIndexMarker486"></html:a>relies on the LLM <html:a id="_idIndexMarker487"></html:a>to
    extract triplets from text, but it also provides flexibility to use custom extraction
    functions <html:span class="No-Break">if needed.</html:span></html:p> <html:p>KG
    Indexes excel in scenarios where understanding complex, interlinked relationships
    and contextual information is important. They are very good at capturing intricate
    connections between entities and concepts, thus offering better insights and context-aware
    responses to queries. Among other use cases, KGs are ideal for answering multifaceted
    questions that require an understanding of the relationships between different
    entities. <html:em class="italic">Yes, I’m talking about our tutor project,</html:em>
    <html:span class="No-Break"><html:em class="italic">PITS, here.</html:em></html:span></html:p>
    <html:p>Let’s get a visual of how KGs work in <html:span class="No-Break"><html:em
    class="italic">Figure 5</html:em></html:span> <html:span class="No-Break"><html:em
    class="italic">.11</html:em></html:span> <html:span class="No-Break">:</html:span></html:p>
    <html:p class="IMG---Caption" lang="en-US">Figure 5.11 – The structure of a KnowledgeGraphIndex</html:p>
    <html:h3>Practical use case</html:h3> <html:p>An interesting use case <html:a
    id="_idIndexMarker488"></html:a>for a KG could be, for example, a news aggregation
    app, where large volumes of text are ingested every day from various sources such
    as newspapers, blogs, and social media platforms. In such a scenario, KGs could
    be used to represent entities such as people, organizations, locations, and so
    on, and their relationships over time. This would allow users to explore historical
    trends, breaking news events, and related entities based on the graph structure
    and <html:span class="No-Break">traversal algorithms.</html:span></html:p> <html:p>Sounds
    good, right? We will now take a look at how you can work <html:span class="No-Break">with</html:span>
    <html:span class="No-Break"><html:code class="literal">KnowledgeGraphIndex</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:h3>Customizable parameters
    for the KnowledgeGraphIndex</html:h3> <html:p>You can customize <html:a id="_idIndexMarker489"></html:a>the
    <html:span class="No-Break">following parameters:</html:span></html:p> <html:ul><html:li><html:code
    class="literal">kg_triple_extract_template</html:code> : This is a prompt template
    for extracting triplets. It can be customized to change how triplets (subject-predicate-object)
    are identified, enabling tailored extraction strategies based on specific <html:span
    class="No-Break">use cases.</html:span></html:li> <html:li><html:code class="literal">max_triplets_per_chunk</html:code>
    : This limits the number of triplets extracted per text chunk. Setting this value
    helps manage the size and complexity of the KG. The default value <html:span class="No-Break">is</html:span>
    <html:span class="No-Break"><html:code class="literal">10</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:li> <html:li><html:code class="literal">graph_store</html:code>
    : This defines the storage type for the graph. Different storage types can be
    used to optimize performance and scalability based on the <html:span class="No-Break">application’s
    requirements.</html:span></html:li> <html:li><html:code class="literal">include_embeddings</html:code>
    : This decides whether to include embeddings in the Index. This is useful for
    scenarios where embeddings can enhance the retrieval process, such as similarity
    searches or advanced <html:span class="No-Break">query understanding.</html:span></html:li>
    <html:li><html:code class="literal">max_object_length</html:code> : This sets
    the maximum length – in characters – for the object in a triplet. It prevents
    overly long or complex objects that could complicate the graph’s structure and
    the retrieval process. Its default value <html:span class="No-Break">is</html:span>
    <html:span class="No-Break"><html:code class="literal">128</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:li> <html:li><html:code class="literal">kg_triplet_extract_fn</html:code>
    : A custom function for triplet extraction can be provided, offering the flexibility
    to use specialized or proprietary methods for extracting triplets <html:a id="_idIndexMarker490"></html:a><html:span
    class="No-Break">from text.</html:span></html:li></html:ul> <html:p>Let’s create
    a simple <html:span class="No-Break">KG next.</html:span></html:p> <html:h3>A
    basic usage model for KnowledgeGraphIndex</html:h3> <html:p>Here’s a simple way
    <html:a id="_idIndexMarker491"></html:a>of constructing and querying <html:span
    class="No-Break">a KG:</html:span></html:p> <html:p>In this setup, the Index builds
    a KG by extracting triplets from documents, enabling complex relationship queries.
    Notice that we configured the Index to run the build process in asynchronous mode
    by setting <html:code class="literal">use_async</html:code> to <html:code class="literal">True</html:code>
    . Of course, for the two small documents that we’re using as an example in our
    case, this won’t make too much difference in the total execution time. However,
    when working with large datasets, enabling asynchronous operation <html:a id="_idIndexMarker492"></html:a>for
    this Index may provide an important <html:span class="No-Break">performance boost.</html:span></html:p>
    <html:h3>How does the KnowledgeGraphIndex build its structure?</html:h3> <html:p><html:code
    class="literal">KnowledgeGraphIndex</html:code> operates by extracting subject-predicate-object
    triplets from text data, forming <html:span class="No-Break">a KG.</html:span></html:p>
    <html:p>There are two main ways in which <html:a id="_idIndexMarker493"></html:a>this
    Index can build <html:span class="No-Break">its structure:</html:span></html:p>
    <html:ul><html:li><html:em class="italic">The default, built-in approach</html:em>
    : In its default implementation, the Index uses an internal method to extract
    triplets from text. This method takes the text content of each Node and passes
    it through a pre-defined prompt template – <html:code class="literal">DEFAULT_KG_TRIPLET_EXTRACT_PROMPT</html:code>
    or a custom template provided during initialization through the <html:code class="literal">kg_triple_extract_template</html:code>
    argument. The prompt template is designed to instruct the LLM to extract knowledge
    triplets from the given text. The LLM’s response is then parsed by a specialized
    internal method to extract the subject, predicate, and object of each triplet.
    This method extracts knowledge triplets in the format of <html:em class="italic">subject,
    predicate, object</html:em> . It applies various checks and string manipulations
    to ensure the validity and consistency of the extracted triplets. Finally, the
    method returns a list of cleaned and well-formatted triplets that can be added
    to the <html:span class="No-Break">KG Index.</html:span></html:li> <html:li><html:em
    class="italic">The second approach involves a custom triplet extraction function</html:em>
    : If a custom <html:code class="literal">kg_triplet_extract_fn</html:code> function
    is provided during initialization, it will be used instead of the LLM-based method.
    This allows us to define our own function to extract triplets from text based
    on their specific requirements or <html:span class="No-Break">domain knowledge.</html:span></html:li></html:ul>
    <html:p>Regardless of whether we’re using the first or the second approach to
    generate the triplets, the inner components of the Index are responsible for building
    the actual KG from the given Nodes. They iterate over each Node, extract triplets
    using either the LLM-based method or the custom extraction function and add the
    triplets to the <html:span class="No-Break">Index structure.</html:span></html:p>
    <html:p>If the <html:code class="literal">include_embeddings</html:code> flag
    is set to <html:code class="literal">True</html:code> , the Index will also generate
    embeddings for each triplet using the specified embedding model. These embeddings
    are stored in the <html:code class="literal">embedding_dict</html:code> of the
    <html:span class="No-Break">Index structure.</html:span></html:p> <html:p>The
    <html:code class="literal">upsert_triplet()</html:code> method allows the manual
    insertion of triplets into the KG. It adds the triplet to the graph store and
    also optionally generates embeddings for the triplet if <html:code class="literal">include_embeddings</html:code>
    are set <html:span class="No-Break">to</html:span> <html:span class="No-Break"><html:code
    class="literal">True</html:code></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:p>During querying, the Index leverages the KG to retrieve relevant data
    and help provide context-rich responses. There are three distinct retrievers available
    for this Index: <html:code class="literal">KGTableRetriever</html:code> for keyword-focused
    queries, <html:code class="literal">KnowledgeGraphRAGRetriever</html:code> for
    retrieving sub-graphs based on extracted entities and synonyms, and a hybrid mode
    that combines both keyword and embedding strategies for a comprehensive <html:a
    id="_idIndexMarker494"></html:a>approach. More details about these retrieval capabilities
    will be explored during <html:a><html:span class="No-Break"><html:em class="italic">Chapter
    6</html:em></html:span></html:a> , <html:em class="italic">Querying Our Data,
    Part 1 –</html:em> <html:span class="No-Break"><html:em class="italic">Context
    Retrieval</html:em></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:a id="_idTextAnchor124"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Building
    Indexes on top of other Indexes with ComposableGraph</html:title></html:head>
    <html:body><html:div class="epub-source"><html:h1 id="_idParaDest-125">Building
    Indexes on top of other Indexes with ComposableGraph</html:h1> <html:div id="_idContainer052">from
    llama_index.core import (     ComposableGraph, SimpleDirectoryReader,     TreeIndex,
    SummaryIndex) documents = SimpleDirectoryReader("files").load_data() index1 =
    TreeIndex.from_documents([documents[0]]) index2 = TreeIndex.from_documents([documents[1]])
    summary1 = "A short introduction to ancient Rome" summary2 = "Some facts about
    dogs" graph = ComposableGraph.from_indices(     SummaryIndex, [index1, index2],
        index_summaries=[summary1, summary2] ) query_engine = graph.as_query_engine()
    response = query_engine.query("What can you tell me?") print(response) <html:p>The
    <html:code class="literal">ComposableGraph</html:code> in LlamaIndex represents
    <html:a id="_idIndexMarker495"></html:a>a sophisticated way <html:a id="_idIndexMarker496"></html:a>to
    structure information by <html:strong class="bold">stacking Indexes</html:strong>
    on top of <html:span class="No-Break">each other.</html:span></html:p> <html:p><html:span
    class="No-Break"><html:em class="italic">Figure 5</html:em></html:span> <html:em
    class="italic">.12</html:em> provides an overview of <html:span class="No-Break">a</html:span>
    <html:span class="No-Break"><html:code class="literal">ComposableGraph</html:code></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 5.12 – The structure of a ComposableGraph</html:p> <html:p>This
    approach allows for the construction <html:a id="_idIndexMarker497"></html:a>of
    Indexes within individual documents – lower-level Indexes – and the aggregation
    of these Indexes into higher-order ones over a collection of documents. For example,
    you can build a <html:code class="literal">TreeIndex</html:code> for the text
    within each document and a <html:code class="literal">SummaryIndex</html:code>
    that encompasses each <html:code class="literal">TreeIndex</html:code> in <html:span
    class="No-Break">a collection.</html:span></html:p> <html:a id="_idTextAnchor125"></html:a><html:h2
    id="_idParaDest-126">How to use the ComposableGraph</html:h2> <html:p>Here’s a
    simple code example demonstrating <html:a id="_idIndexMarker498"></html:a>the
    usage <html:span class="No-Break">of</html:span> <html:span class="No-Break"><html:code
    class="literal">ComposableGraph</html:code></html:span> <html:span class="No-Break">:</html:span></html:p>
    <html:p>In this example, the <html:code class="literal">ComposableGraph</html:code>
    facilitates the organization of detailed information within Documents and the
    summarization <html:span class="No-Break">across Documents.</html:span></html:p>
    <html:p>We first load our two test Documents: one related to ancient Rome and
    the other one describing dogs. We then create a <html:code class="literal">TreeIndex</html:code>
    for <html:span class="No-Break">each Document.</html:span></html:p> <html:p>We
    also define the summaries <html:a id="_idIndexMarker499"></html:a>of the <html:span
    class="No-Break">two Documents.</html:span></html:p> <html:h3>Pro tip</html:h3>
    <html:p>As an alternative to manually <html:a id="_idIndexMarker500"></html:a>defining
    the summaries, we could have also queried each individual Index to automatically
    generate the content summary or used <html:code class="literal">SummaryExtractor</html:code>
    to accomplish the <html:span class="No-Break">same purpose.</html:span></html:p>
    <html:p>In the next step, we build a <html:code class="literal">ComposableGraph</html:code>
    containing the two tree Indexes along with their summaries. For this example,
    the output of the code should be something similar to the following: <html:em
    class="italic">I can tell you about the ancient Roman civilization and dogs and
    their various breeds, traits,</html:em> <html:span class="No-Break"><html:em class="italic">and
    personalities.</html:em></html:span></html:p> <html:p>Once the <html:code class="literal">ComposableGraph</html:code>
    has been built, the root <html:code class="literal">SummaryIndex</html:code> will
    have an overview of the contents of the individual Indexes for <html:span class="No-Break">each
    document.</html:span></html:p> <html:a id="_idTextAnchor126"></html:a><html:h2
    id="_idParaDest-127">A more detailed description of this concept</html:h2> <html:p>Under
    the hood, a <html:code class="literal">ComposableGraph</html:code> enables the
    creation <html:a id="_idIndexMarker501"></html:a>of hierarchical structures by
    stacking Indexes on top of each other. This allows for the organization of detailed
    information within individual Documents using lower-level Indexes and the aggregation
    of these Indexes into higher-order ones over a collection <html:span class="No-Break">of
    Documents.</html:span></html:p> <html:p>The process begins by creating individual
    Indexes for each Document to capture the detailed information within the Documents.
    Additionally, summaries are defined for <html:span class="No-Break">each Document.</html:span></html:p>
    <html:p>The <html:code class="literal">ComposableGraph</html:code> is then constructed
    <html:a id="_idIndexMarker502"></html:a>using the <html:code class="literal">from_indices()</html:code>
    class method. It takes the root Index class (in our example, the <html:code class="literal">SummaryIndex</html:code>
    ), the child Indexes (in our example, the two <html:code class="literal">TreeIndex</html:code>
    instances), and their corresponding summaries as input. The method creates <html:code
    class="literal">IndexNodes</html:code> instances for each child Index, associating
    the summary with the respective Index. These <html:code class="literal">IndexNodes</html:code>
    instances are then used to construct the <html:span class="No-Break">root Index.</html:span></html:p>
    <html:p>During a query, the <html:code class="literal">ComposableGraph</html:code>
    starts with the top-level summary Index, where each Node corresponds to an underlying
    lower-level Index. The query is executed recursively, starting from the root Index,
    and traversing through the sub-Indexes. The <html:code class="literal">ComposableGraphQueryEngine</html:code>
    is responsible for this recursive <html:span class="No-Break">querying process.</html:span></html:p>
    <html:p>The query engine retrieves relevant Nodes from the root Index based on
    the query. For each relevant Node, it identifies the corresponding child Index
    using the <html:code class="literal">index_id</html:code> stored in the Node’s
    relationships. It then queries the child Index with the original query to obtain
    more detailed information. This process continues recursively until all relevant
    sub-Indexes have <html:span class="No-Break">been queried.</html:span></html:p>
    <html:p>Custom query engines can be configured for each Index within the <html:code
    class="literal">ComposableGraph</html:code> , allowing for tailored retrieval
    strategies at different levels of the hierarchy. This enables a deep, hierarchical
    understanding of complex datasets by seamlessly integrating information from various
    levels <html:span class="No-Break">of Indexes.</html:span></html:p> <html:p>Overall,
    the <html:code class="literal">ComposableGraph</html:code> allows for the efficient
    retrieval of relevant information from both high-level summaries and detailed,
    low-level Indexes, enabling a comprehensive understanding <html:a id="_idIndexMarker503"></html:a>of
    the <html:span class="No-Break">underlying data.</html:span></html:p> <html:p>Now
    that we have covered the Indexes available for our RAG implementation, it’s time
    to address the elephant in the room – <html:span class="No-Break"><html:strong
    class="bold">cost</html:strong></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:a id="_idTextAnchor127"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Estimating
    the potential cost of building and querying Indexes</html:title></html:head> <html:body><html:div
    class="epub-source"><html:h1 id="_idParaDest-128">Estimating the potential cost
    of building and querying Indexes</html:h1> <html:div id="_idContainer052">import
    tiktoken from llama_index.core import (     TreeIndex, SimpleDirectoryReader,
    Settings) from llama_index.core.llms.mock import MockLLM from llama_index.core.callbacks
    import (     CallbackManager, TokenCountingHandler) llm = MockLLM(max_tokens=256)
    token_counter = TokenCountingHandler(     tokenizer=tiktoken.encoding_for_model("gpt-3.5-turbo").encode
    ) callback_manager = CallbackManager([token_counter]) Settings.callback_manager=callback_manager
    Settings.llm=llm tiktoken.encoding_for_model("gpt-3.5-turbo").encode). documents
    = SimpleDirectoryReader(     "cost_prediction_samples").load_data() index = TreeIndex.from_documents(
        documents=documents,     num_children=2,     show_progress=True) print("Total
    LLM Token Count:", token_counter.total_llm_token_count) import tiktoken from llama_index.core
    import (     MockEmbedding, VectorStoreIndex,     SimpleDirectoryReader, Settings)
    from llama_index.core.callbacks import (     CallbackManager, TokenCountingHandler)
    from llama_index.core.llms.mock import MockLLM embed_model = MockEmbedding(embed_dim=1536)
    llm = MockLLM(max_tokens=256) token_counter = TokenCountingHandler(     tokenizer=tiktoken.encoding_for_model("gpt-3.5-turbo").encode
    ) callback_manager = CallbackManager([token_counter]) Settings.embed_model=embed_model
    Settings.llm=llm Settings.callback_manager=callback_manager documents = SimpleDirectoryReader(
        "cost_prediction_samples").load_data() index = VectorStoreIndex.from_documents(
        documents=documents,     show_progress=True) print("Embedding Token Count:",
        token_counter.total_embedding_token_count) query_engine = index.as_query_engine(service_context=service_context)
    response = query_engine.query("What''s the cat''s name?") print("Query LLM Token
    Count:", token_counter.total_llm_token_count) print("Query Embedding Token Count:",
        token_counter.total_embedding_token_count) <html:p>In a similar manner to
    metadata <html:a id="_idIndexMarker504"></html:a>extractors, Indexes pose issues
    related to costs and data privacy. That is because, as we have seen in this chapter,
    most Indexes rely on LLMs to some extent – during building <html:span class="No-Break">and/or
    querying.</html:span></html:p> <html:p>Repeatedly calling LLMs <html:a id="_idIndexMarker505"></html:a>to
    process large volumes of text can quickly break your budget if you’re not paying
    attention to your potential costs. For example, if you are building a <html:code
    class="literal">TreeIndex</html:code> or <html:code class="literal">KeywordTableIndex</html:code>
    from thousands of documents, those constant LLM invocations during Index construction
    will carry a significant cost. Embeddings can also rely on calls to external models;
    therefore, the <html:code class="literal">VectorStoreIndex</html:code> is another
    important source of costs. In my experience, prevention and prediction are the
    best ways to avoid nasty surprises and keep your <html:span class="No-Break">expenses
    low.</html:span></html:p> <html:p>Just like with metadata extraction, I’d start
    first by observing and applying some <html:span class="No-Break">best practices:</html:span></html:p>
    <html:ul><html:li>Use Indexes with no LLM calls during building where possible,
    such as <html:code class="literal">SummaryIndex</html:code> or <html:code class="literal">SimpleKeywordTableIndex</html:code>
    . This eliminates Index <html:span class="No-Break">building costs.</html:span></html:li>
    <html:li>Use cheaper LLM models. If full accuracy isn’t critical, cheaper LLM
    models with lower computational demands can be used but be aware of possible <html:span
    class="No-Break">quality trade-offs.</html:span></html:li> <html:li>Cache and
    reuse Indexes. Avoid rebuilding Indexes by caching and reusing previously <html:span
    class="No-Break">constructed ones.</html:span></html:li> <html:li>Optimize query
    parameters to minimize LLM calls during your search. For example, reducing <html:code
    class="literal">similarity_top_k</html:code> in <html:code class="literal">VectorStoreIndex</html:code>
    will reduce your <html:span class="No-Break">query cost.</html:span></html:li>
    <html:li>Use local models. To further manage costs and maintain data privacy when
    using Indexes in LlamaIndex, consider utilizing local LLM and embedding models
    instead of relying on hosted services. This approach not only offers more control
    over data privacy but also helps in reducing the dependency on external services,
    which can be costly. Using local models can significantly cut down on expenses,
    particularly when handling large volumes of data or when operating within strict
    <html:span class="No-Break">budget constraints.</html:span></html:li></html:ul>
    <html:p class="callout-heading">Important side note regarding local AI models</html:p>
    <html:p class="callout">Always remember that RAG <html:a id="_idIndexMarker506"></html:a>introduces
    additional knowledge and contextual information into the model’s processing, effectively
    bridging the gap caused by a smaller training dataset. So, even for models that
    haven’t been trained on extensive or diverse data, RAG allows them to access a
    broader range of information beyond their initial training set, thus enhancing
    their performance and <html:span class="No-Break">output quality.</html:span></html:p>
    <html:p>These guidelines will definitely <html:a id="_idIndexMarker507"></html:a>help
    you reduce costs, but it’s still a good idea to estimate before indexing <html:span
    class="No-Break">larger datasets.</html:span></html:p> <html:p>Here is a basic
    example of how we can estimate the LLM costs of building a <html:code class="literal">TreeIndex</html:code>
    using <html:span class="No-Break">a</html:span> <html:span class="No-Break"><html:code
    class="literal">MockLLM</html:code></html:span> <html:span class="No-Break">:</html:span></html:p>
    <html:p>In the previous part, we first took care of the necessary imports. If
    you’re unfamiliar with the reasons to use <html:code class="literal">tiktoken</html:code>
    as a tokenizer here, head back to <html:a><html:span class="No-Break"><html:em
    class="italic">Chapter 4</html:em></html:span></html:a> , <html:em class="italic">Ingesting
    Data into Our RAG Workflow</html:em> where we discussed estimating the potential
    cost of using metadata extractors. Let’s set up the <html:span class="No-Break"><html:code
    class="literal">MockLLM</html:code></html:span> <html:span class="No-Break">next:</html:span></html:p>
    <html:p>We just created a <html:code class="literal">MockLLM</html:code> instance
    with a specified maximum token limit acting as a worst-case maximal cost. We then
    initialized <html:code class="literal">TokenCountingHandler</html:code> with a
    tokenizer that matches our real LLM model using <html:span class="No-Break">the
    following:</html:span></html:p> <html:p>This handler will track token usage. This
    construct simulates an LLM without actually calling the <html:span class="No-Break"><html:code
    class="literal">gpt-3.5-turbo</html:code></html:span> <html:span class="No-Break">API:</html:span></html:p>
    <html:p>We’ve loaded our documents and are now ready to build <html:span class="No-Break">the</html:span>
    <html:span class="No-Break"><html:code class="literal">TreeIndex</html:code></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p>After building the
    Index, the script <html:a id="_idIndexMarker508"></html:a>displays the <html:code
    class="literal">total_llm_token_count</html:code> value stored in <html:span class="No-Break">the</html:span>
    <html:span class="No-Break"><html:code class="literal">TokenCountingHandler</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>In this example, we’re
    only using the <html:code class="literal">MockLLM</html:code> class because there
    are no embeddings used for building the <html:code class="literal">TreeIndex</html:code>
    . This allows us to estimate the worst-case LLM token cost before actually building
    the Index and invoking the real LLM. The same method can be applied to estimate
    <html:span class="No-Break">query costs.</html:span></html:p> <html:p class="callout-heading">The
    main lesson here?</html:p> <html:p class="callout">While Indexes unlock many capabilities,
    overuse without optimization can greatly impact costs. Always estimate token usage
    before indexing <html:span class="No-Break">larger datasets.</html:span></html:p>
    <html:p>Here is a second example. It’s similar to the previous one, but this time,
    we’re first estimating the embedding costs of building a <html:code class="literal">VectorStoreIndex</html:code>
    and after that, the total cost of querying <html:span class="No-Break">the Index:</html:span></html:p>
    <html:p>The first part took care <html:a id="_idIndexMarker509"></html:a>of the
    imports. Next, we set up the <html:code class="literal">MockEmbedding</html:code>
    and <html:span class="No-Break"><html:code class="literal">MockLLM</html:code></html:span>
    <html:span class="No-Break">objects:</html:span></html:p> <html:p>After initializing
    the <html:code class="literal">MockEmbedding</html:code> and <html:code class="literal">MockLLM</html:code>
    objects, we defined a <html:code class="literal">TokenCountingHandler</html:code>
    and a <html:code class="literal">CallbackManager</html:code> and wrapped them
    into the custom <html:code class="literal">Settings</html:code> . It’s now time
    to load our sample documents and build the <html:code class="literal">VectorStoreIndex</html:code>
    using the <html:span class="No-Break">custom</html:span> <html:span class="No-Break"><html:code
    class="literal">Settings</html:code></html:span> <html:span class="No-Break">:</html:span></html:p>
    <html:p>If you have successfully cloned the book’s GitHub repo, the <html:code
    class="literal">cost_prediction_samples</html:code> subfolder in the <html:code
    class="literal">ch5</html:code> folder should contain a file with a fictional
    story about <html:code class="literal">Fluffy the cat</html:code> . The <html:code
    class="literal">VectorStoreIndex</html:code> uses an embedding model to encode
    document text into vectors during indexing. In our second example, we estimated
    the token costs of those embedding calls by using <html:code class="literal">MockEmbedding</html:code>
    and <html:code class="literal">TokenCountingHandler</html:code> . The embedding
    token count provides an indication of how expensive it will be to build this Index
    per document based on the <html:span class="No-Break">text lengths.</html:span></html:p>
    <html:p>To have a complete view, we can take this a step further and also estimate
    <html:span class="No-Break">search costs:</html:span></html:p> <html:p>This shows
    the potential search fees as well by counting tokens for embedding lookups and
    synthesizing the response. We also had to use <html:code class="literal">MockLLM</html:code>
    to catch the LLM tokens hypothetically consumed during <html:span class="No-Break">response
    synthesis.</html:span></html:p> <html:p>So, in summary, follow preventive <html:a
    id="_idIndexMarker510"></html:a>best practices and always forecast your Index
    build and query expenses before unleashing them on your full <html:span class="No-Break">document
    collection!</html:span></html:p> <html:p>It’s time to make some progress with
    our project. Let’s revisit our <html:span class="No-Break">PITS project.</html:span></html:p><html:a
    id="_idTextAnchor128"></html:a></html:div></html:div></html:body></html:html>'
  prefs: []
  type: TYPE_NORMAL
