<html><head></head><body>
<div id="_idContainer212">
<h1 class="chapter-number" id="_idParaDest-114"><a id="_idTextAnchor113"/><span class="koboSpan" id="kobo.1.1">7</span></h1>
<h1 id="_idParaDest-115"><a id="_idTextAnchor114"/><span class="koboSpan" id="kobo.2.1">Creating and Connecting a Knowledge Graph to an AI Agent</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In the previous two chapters, we discussed the RAG framework in detail. </span><span class="koboSpan" id="kobo.3.2">We started with naïve RAG and then saw how we could add different components, replace others, or modify the entire pipeline for our needs. </span><span class="koboSpan" id="kobo.3.3">The whole system is extremely flexible, but some concepts remain the same. </span><span class="koboSpan" id="kobo.3.4">First, we start with a corpus (or multiple corpora of texts) and conduct embedding of these texts to obtain a database of vectors. </span><span class="koboSpan" id="kobo.3.5">Once the user query arrives, we conduct a similarity search on this database of vectors. </span><span class="koboSpan" id="kobo.3.6">Regardless of the scope or type of texts, our pipeline is based on the concept of vectorizing these texts in some way and then providing the information contained in the discovered texts to </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">the LLM.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">Texts are often full of redundant information, and in the previous chapter, we saw that LLMs are sensitive to the amount of noise in the input. </span><span class="koboSpan" id="kobo.5.2">Most people have seen the benefit of creating schematic notes or mind maps. </span><span class="koboSpan" id="kobo.5.3">These schematics are concise because of the principle that underlining everything in a book is like underlining nothing. </span><span class="koboSpan" id="kobo.5.4">The principle of these diagrams is to extract the key information to remember that will enable us to answer questions in the future. </span><span class="koboSpan" id="kobo.5.5">Schematics should present the fundamental information and the relationships that connect them. </span><span class="koboSpan" id="kobo.5.6">These schemas can be represented as a graph and, more precisely, as a knowledge graph. </span><span class="koboSpan" id="kobo.5.7">The advantage of these graphs is that they are compact, represent knowledge as entities and relationships, and we can conduct analyses and use graph search algorithms on them. </span><span class="koboSpan" id="kobo.5.8">Over the years, these </span><strong class="bold"><span class="koboSpan" id="kobo.6.1">knowledge graphs</span></strong><span class="koboSpan" id="kobo.7.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.8.1">KGs</span></strong><span class="koboSpan" id="kobo.9.1">) have been built by major companies or institutions and are now available for use. </span><span class="koboSpan" id="kobo.9.2">Many of these KGs have been used for information extraction, where information is extracted with a series of queries to answer questions. </span><span class="koboSpan" id="kobo.9.3">This extracted information is a series of entities and relationships, rich in knowledge but less understandable to us humans. </span><span class="koboSpan" id="kobo.9.4">The natural step is to use this information for the context of an LLM and then generate a natural language response. </span><span class="koboSpan" id="kobo.9.5">This paradigm is called </span><strong class="bold"><span class="koboSpan" id="kobo.10.1">GraphRAG,</span></strong><span class="koboSpan" id="kobo.11.1"> and we will discuss it in detail in </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">this chapter.</span></span></p>
<p><span class="koboSpan" id="kobo.13.1">In any case, nothing prohibits us from using an LLM for all the steps in KG. </span><span class="koboSpan" id="kobo.13.2">In fact, LLMs have a number of innate capabilities that make them useful even for tasks for which they are not trained. </span><span class="koboSpan" id="kobo.13.3">This is precisely why we will see that we can use LLMs to extract relationships and entities and build our KGs. </span><span class="koboSpan" id="kobo.13.4">LLMs, though, also possess reasoning capabilities, and in this chapter, we will discuss how we can use these models to reason both about the information contained in graphs and about the structure of the graphs themselves. </span><span class="koboSpan" id="kobo.13.5">Finally, we will discuss what perspectives and questions remain open, and the advantages and disadvantages of the </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">proposed approaches.</span></span></p>
<p><span class="koboSpan" id="kobo.15.1">In this chapter, we’ll be covering the </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.17.1">Introduction to </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">knowledge graphs</span></span></li>
<li><span class="koboSpan" id="kobo.19.1">Creating a knowledge graph with </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">your LLM</span></span></li>
<li><span class="koboSpan" id="kobo.21.1">Retrieving information with a knowledge graph and </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">an LLM</span></span></li>
<li><span class="koboSpan" id="kobo.23.1">Understanding </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">graph reasoning</span></span></li>
<li><span class="koboSpan" id="kobo.25.1">Ongoing challenges in knowledge graphs </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">and GraphRAG</span></span></li>
</ul>
<h1 id="_idParaDest-116"><a id="_idTextAnchor115"/><span class="koboSpan" id="kobo.27.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.28.1">Most of this code can be run on a CPU, but it is preferable to be run on a GPU. </span><span class="koboSpan" id="kobo.28.2">The code is written in PyTorch and uses standard libraries for the most part (PyTorch, Hugging Face Transformers, LangChain, ChromaDB, </span><strong class="source-inline"><span class="koboSpan" id="kobo.29.1">sentence-transformer</span></strong><span class="koboSpan" id="kobo.30.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.31.1">faiss-cpu</span></strong><span class="koboSpan" id="kobo.32.1">, and </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">so on).</span></span></p>
<p><span class="koboSpan" id="kobo.34.1">In this chapter, we also use Neo4j as the database for the graph. </span><span class="koboSpan" id="kobo.34.2">Although we will do all operations with Python, Neo4j must be installed and you must be registered to use it. </span><span class="koboSpan" id="kobo.34.3">The code can be found on </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">GitHub: </span></span><a href="https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr7"><span class="No-Break"><span class="koboSpan" id="kobo.36.1">https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr7</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.37.1">.</span></span></p>
<h1 id="_idParaDest-117"><a id="_idTextAnchor116"/><span class="koboSpan" id="kobo.38.1">Introduction to knowledge graphs</span></h1>
<p><span class="koboSpan" id="kobo.39.1">Knowledge representation is </span><a id="_idIndexMarker748"/><span class="koboSpan" id="kobo.40.1">one of the open problems of AI and has very ancient roots (Leibniz believed that the whole knowledge could be represented and used to conduct calculations). </span><span class="koboSpan" id="kobo.40.2">The interest in knowledge representation is based on the fact that it represents the first step in conducting computer reasoning. </span><span class="koboSpan" id="kobo.40.3">Once this knowledge is organized in an orderly manner, it can be used to design inference algorithms and solve reasoning problems. </span><span class="koboSpan" id="kobo.40.4">Early studies focused on using deduction to solve problems about organized entities (e.g., through the use of ontologies). </span><span class="koboSpan" id="kobo.40.5">This has worked well for many toy problems, but it is laborious, often requires a whole set of hardcoded rules, and risks succumbing to combinatorial explosion. </span><span class="koboSpan" id="kobo.40.6">Because search in these</span><a id="_idIndexMarker749"/><span class="koboSpan" id="kobo.41.1"> spaces could be extremely computationally expensive, an attempt was made to define </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">two concepts:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.43.1">Limited rationality</span></strong><span class="koboSpan" id="kobo.44.1">: Finding a solution but</span><a id="_idIndexMarker750"/><span class="koboSpan" id="kobo.45.1"> also considering the cost </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">of it</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.47.1">Heuristic search</span></strong><span class="koboSpan" id="kobo.48.1">: Limiting the</span><a id="_idIndexMarker751"/><span class="koboSpan" id="kobo.49.1"> search in space, thus finding a semi-optimal solution (a local but not </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">global optima)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.51.1">These principles have inspired a whole series of algorithms that have since allowed information searches to be conducted more efficiently and tractably. </span><span class="koboSpan" id="kobo.51.2">Interest in these algorithms grew strongly in the late 1990s with the advent of the World Wide Web and the need to conduct internet searches quickly and accurately. </span><span class="koboSpan" id="kobo.51.3">Regarding data, the World Wide Web is also based on three </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">technological principles:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.53.1">Distributed data</span></strong><span class="koboSpan" id="kobo.54.1">: Data is distributed </span><a id="_idIndexMarker752"/><span class="koboSpan" id="kobo.55.1">across the world and </span><a id="_idIndexMarker753"/><span class="koboSpan" id="kobo.56.1">accessible from all parts of </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">the world</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.58.1">Connected data</span></strong><span class="koboSpan" id="kobo.59.1">: Data is</span><a id="_idIndexMarker754"/><span class="koboSpan" id="kobo.60.1"> interconnected </span><a id="_idIndexMarker755"/><span class="koboSpan" id="kobo.61.1">and not isolated; the data’s meaning is a function of its connection with </span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">other data</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.63.1">Semantic metadata</span></strong><span class="koboSpan" id="kobo.64.1">: In</span><a id="_idIndexMarker756"/><span class="koboSpan" id="kobo.65.1"> addition to the data itself, we have information about its relationships, and this metadata allows </span><a id="_idIndexMarker757"/><span class="koboSpan" id="kobo.66.1">us to </span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">search efficiently</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.68.1">For this reason, we began to search for a technology that could respect the nature of this new data. </span><span class="koboSpan" id="kobo.68.2">It came naturally to turn to representations of a graphical nature. </span><span class="koboSpan" id="kobo.68.3">In fact, by definition, graphs model relationships between different entities. </span><span class="koboSpan" id="kobo.68.4">This approach began to be used in web searches in 2012 when Google began adding knowledge cards for each concept searched. </span><span class="koboSpan" id="kobo.68.5">These knowledge cards can be seen as graphs of name entities in which the connections are the graph links. </span><span class="koboSpan" id="kobo.68.6">These cards then allow for more relevant search and user facilitation. </span><span class="koboSpan" id="kobo.68.7">Subsequently, the term </span><em class="italic"><span class="koboSpan" id="kobo.69.1">knowledge graph</span></em><span class="koboSpan" id="kobo.70.1"> came to mean any graph that connects entities through a series</span><a id="_idIndexMarker758"/><span class="koboSpan" id="kobo.71.1"> of meaningful relationships. </span><span class="koboSpan" id="kobo.71.2">These relationships generally represent semantic relationships </span><span class="No-Break"><span class="koboSpan" id="kobo.72.1">between entities.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer181">
<span class="koboSpan" id="kobo.73.1"><img alt="Figure 7.1 – Knowledge card in Google" src="image/B21257_07_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.74.1">Figure 7.1 – Knowledge card in Google</span></p>
<h2 id="_idParaDest-118"><a id="_idTextAnchor117"/><span class="koboSpan" id="kobo.75.1">A formal definition of graphs and knowledge graphs</span></h2>
<p><span class="koboSpan" id="kobo.76.1">Since KGs are a subtype of graphs, we will start with a brief introduction of graphs. </span><span class="koboSpan" id="kobo.76.2">Graphs are data structures composed of </span><a id="_idIndexMarker759"/><span class="koboSpan" id="kobo.77.1">nodes (or vertices) that are connected by relationships (or edges) to represent a model of a domain. </span><span class="koboSpan" id="kobo.77.2">A graph can then represent knowledge in a compact manner while trying to reduce noise. </span><span class="koboSpan" id="kobo.77.3">There are different types </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">of graphs:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.79.1">Undirected</span></strong><span class="koboSpan" id="kobo.80.1">: Edges have</span><a id="_idIndexMarker760"/> <span class="No-Break"><span class="koboSpan" id="kobo.81.1">no </span></span><span class="No-Break"><a id="_idIndexMarker761"/></span><span class="No-Break"><span class="koboSpan" id="kobo.82.1">direction</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.83.1">Directed</span></strong><span class="koboSpan" id="kobo.84.1">: Edges have a </span><a id="_idIndexMarker762"/><span class="koboSpan" id="kobo.85.1">defined direction (there is a defined beginning </span><a id="_idIndexMarker763"/><span class="No-Break"><span class="koboSpan" id="kobo.86.1">and end)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.87.1">Weighted</span></strong><span class="koboSpan" id="kobo.88.1">: Edges carry</span><a id="_idIndexMarker764"/><span class="koboSpan" id="kobo.89.1"> weights, representing </span><a id="_idIndexMarker765"/><span class="koboSpan" id="kobo.90.1">the strength or cost of </span><span class="No-Break"><span class="koboSpan" id="kobo.91.1">the relationship</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.92.1">Labeled</span></strong><span class="koboSpan" id="kobo.93.1">: Nodes are </span><a id="_idIndexMarker766"/><span class="koboSpan" id="kobo.94.1">associated </span><a id="_idIndexMarker767"/><span class="koboSpan" id="kobo.95.1">with features </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">and labels</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.97.1">Multigraph</span></strong><span class="koboSpan" id="kobo.98.1">: Multiple </span><a id="_idIndexMarker768"/><span class="koboSpan" id="kobo.99.1">edges (relationships) exist</span><a id="_idIndexMarker769"/><span class="koboSpan" id="kobo.100.1"> between the same pair </span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">of nodes</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.102.1">The following figure shows a visual representation of </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">these graphs:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer182">
<span class="koboSpan" id="kobo.104.1"><img alt="Figure 7.2 – Different types of graph architecture" src="image/B21257_07_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.105.1">Figure 7.2 – Different types of graph architecture</span></p>
<p><span class="koboSpan" id="kobo.106.1">A KG is thus a subgraph with three </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">main properties:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.108.1">Nodes represent real-world entities</span></strong><span class="koboSpan" id="kobo.109.1">: These entities can represent people, places, or domain-specific </span><a id="_idIndexMarker770"/><span class="koboSpan" id="kobo.110.1">entities (genes, proteins, diseases, financial products, and </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">so on)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.112.1">Relationships define semantic connections between nodes</span></strong><span class="koboSpan" id="kobo.113.1">: For example, two people may be linked by a relationship that represents friendship, or a specific gene is associated with a </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">particular disease</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.115.1">Nodes and edges may have associated properties</span></strong><span class="koboSpan" id="kobo.116.1">: For example, all people will have as a property that they are human beings (a label) but they can also have quantitative properties (date of birth, a specific identifier, and </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">so on)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.118.1">So, a little more formally, we can say that we have a knowledge base (a database of facts) represented in the form of factual triplets. </span><span class="koboSpan" id="kobo.118.2">A factual triples has the form of </span><strong class="source-inline"><span class="koboSpan" id="kobo.119.1">(head, relation, tail)</span></strong><span class="koboSpan" id="kobo.120.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.121.1">(subject, predicate, object)</span></strong><span class="koboSpan" id="kobo.122.1">, or more succinctly, </span><strong class="source-inline"><span class="koboSpan" id="kobo.123.1">(e1,r1,e2)</span></strong><span class="koboSpan" id="kobo.124.1">, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.125.1">(Napoleon, BornIn, Ajaccio)</span></strong><span class="koboSpan" id="kobo.126.1">. </span><span class="koboSpan" id="kobo.126.2">The KG is a representation of this knowledge base that allows us to conduct interpretation. </span><span class="koboSpan" id="kobo.126.3">Given the structure of these triplets, a KG is a directed graph where the nodes are these entities and the edges are </span><span class="No-Break"><span class="koboSpan" id="kobo.127.1">factual relationships.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer183">
<span class="koboSpan" id="kobo.128.1"><img alt="Figure 7.3 – Example of a knowledge base and knowledge graph (https://arxiv.org/pdf/2002.00388)" src="image/B21257_07_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.129.1">Figure 7.3 – Example of a knowledge base and knowledge graph (</span><a href="https://arxiv.org/pdf/2002.00388"><span class="koboSpan" id="kobo.130.1">https://arxiv.org/pdf/2002.00388</span></a><span class="koboSpan" id="kobo.131.1">)</span></p>
<p><span class="koboSpan" id="kobo.132.1">A KG is defined as a </span><a id="_idIndexMarker771"/><span class="koboSpan" id="kobo.133.1">graph consisting of a set of entities, </span><em class="italic"><span class="koboSpan" id="kobo.134.1">E</span></em><span class="koboSpan" id="kobo.135.1">, relations, </span><em class="italic"><span class="koboSpan" id="kobo.136.1">R</span></em><span class="koboSpan" id="kobo.137.1">, and facts, </span><em class="italic"><span class="koboSpan" id="kobo.138.1">F</span></em><span class="koboSpan" id="kobo.139.1">, where each fact </span><em class="italic"><span class="koboSpan" id="kobo.140.1">f</span></em><span class="koboSpan" id="kobo.141.1"> is </span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">a triplet:</span></span></p>
<p><span class="_-----MathTools-_Math_Symbol_Extended"><math display="block"><mrow><mrow><mrow><mi mathvariant="script">G</mi><mo>=</mo><mfenced close="}" open="{"><mrow><mi mathvariant="script">E</mi><mo>,</mo><mi mathvariant="script">R</mi><mo>,</mo><mi mathvariant="script">F</mi></mrow></mfenced><mo>;</mo><mi>f</mi><mo>=</mo><mo>(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo>)</mo></mrow></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.143.1">As you can see, a KG is an alternative representation of knowledge. </span><span class="koboSpan" id="kobo.143.2">The same kind of data can be represented in either a table or a graph. </span><span class="koboSpan" id="kobo.143.3">We can directly create triplets from a table and then directly represent them in a KG. </span><span class="koboSpan" id="kobo.143.4">We do not need table headers, and it is easier to conduct the update of such a structure. </span><span class="koboSpan" id="kobo.143.5">Graphs are considered universal data representations because they can be applied to any type of data. </span><span class="koboSpan" id="kobo.143.6">In fact, not only can we map tabular data to a KG, but we can also get data from other formats (JSON, XML, CSV, and so on). </span><span class="koboSpan" id="kobo.143.7">Graphs also allow us to nimbly represent recursive structures (such as trees and documents) or cyclical structures (such as social networks). </span><span class="koboSpan" id="kobo.143.8">Also, if we do not have the information for all properties, the table representation will be full of missing values; in a KG, we do not have </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">this problem.</span></span></p>
<p><span class="koboSpan" id="kobo.145.1">Graphs </span><em class="italic"><span class="koboSpan" id="kobo.146.1">per se</span></em><span class="koboSpan" id="kobo.147.1"> represent network structures. </span><span class="koboSpan" id="kobo.147.2">This is very useful for many business cases (e.g., in finance and medicine) where a lot of data is already structured as networks. </span><span class="koboSpan" id="kobo.147.3">Another advantage is that it is much easier to conduct a merge of graphs than of tables. </span><span class="koboSpan" id="kobo.147.4">Merging tables is usually a complicated task (where you have to choose which columns to merge, avoid creating duplicates, and other potential problems). </span><span class="koboSpan" id="kobo.147.5">If the data is in triplets, it is extremely easy to merge two KG databases. </span><span class="koboSpan" id="kobo.147.6">For example, look how simple it is to transform this table into a graph; they </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">are equivalent:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer184">
<span class="koboSpan" id="kobo.149.1"><img alt="Figure 7.4 – Table and graph are equivalent" src="image/B21257_07_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.150.1">Figure 7.4 – Table and graph are equivalent</span></p>
<p><span class="koboSpan" id="kobo.151.1">A KG should not be seen </span><a id="_idIndexMarker772"/><span class="koboSpan" id="kobo.152.1">as a static entity. </span><span class="koboSpan" id="kobo.152.2">In fact, knowledge evolves; this causes new entities or relationships to be added. </span><span class="koboSpan" id="kobo.152.3">One of the most widely used tasks in </span><strong class="bold"><span class="koboSpan" id="kobo.153.1">knowledge graph reasoning</span></strong><span class="koboSpan" id="kobo.154.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.155.1">KGR</span></strong><span class="koboSpan" id="kobo.156.1">) is to predict new relationships. </span><span class="koboSpan" id="kobo.156.2">For example, if A is the </span><a id="_idIndexMarker773"/><span class="koboSpan" id="kobo.157.1">husband of B and father of C, this implies that B is the mother of C, which could be derived with logical ruling: </span><em class="italic"><span class="koboSpan" id="kobo.158.1">(A, husband of, B) ^ (A, father of C) -&gt; (B, mother of, C)</span></em><span class="koboSpan" id="kobo.159.1">. </span><span class="koboSpan" id="kobo.159.2">In this pre-existing datum, we have inferred a </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">missing relationship.</span></span></p>
<p><span class="koboSpan" id="kobo.161.1">Another very important task is how to conduct the update of a KG once we have obtained new triplets (this may require complex preprocessing). </span><span class="koboSpan" id="kobo.161.2">This is very important because once we have integrated some new knowledge, we can conduct further analysis and further reasoning. </span><span class="koboSpan" id="kobo.161.3">Also, being a graph, we can use graph analysis algorithms (such as centrality measures, connectivity, clustering, and so on) for our business cases. </span><span class="koboSpan" id="kobo.161.4">Leveraging these algorithms makes it much easier to conduct searches or complex queries in a KG than in </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">relational databases.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer185">
<span class="koboSpan" id="kobo.163.1"><img alt="Figure 7.5 – A KG is a dynamic entity" src="image/B21257_07_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.164.1">Figure 7.5 – A KG is a dynamic entity</span></p>
<p><span class="koboSpan" id="kobo.165.1">In addition, KGs are much more flexible and adaptable than people think. </span><span class="koboSpan" id="kobo.165.2">KGs can be adapted for different tasks. </span><span class="koboSpan" id="kobo.165.3">For example, there are extensions of KGs such as </span><strong class="bold"><span class="koboSpan" id="kobo.166.1">hierarchical KGs</span></strong><span class="koboSpan" id="kobo.167.1"> where we </span><a id="_idIndexMarker774"/><span class="koboSpan" id="kobo.168.1">have multiple levels. </span><span class="koboSpan" id="kobo.168.2">In hierarchical KGs, entities from one level can be connected to the next level (this, for example, is very useful when we have ontologies). </span><span class="koboSpan" id="kobo.168.3">Entities</span><a id="_idIndexMarker775"/><span class="koboSpan" id="kobo.169.1"> can also be </span><strong class="bold"><span class="koboSpan" id="kobo.170.1">multimodal</span></strong><span class="koboSpan" id="kobo.171.1">, so a node can represent an image to which other entities (textual or other images, or other types of modalities) are connected. </span><span class="koboSpan" id="kobo.171.2">Another type of KG is a </span><strong class="bold"><span class="koboSpan" id="kobo.172.1">temporal KG</span></strong><span class="koboSpan" id="kobo.173.1">, in which</span><a id="_idIndexMarker776"/><span class="koboSpan" id="kobo.174.1"> we incorporate a temporal dimension. </span><span class="koboSpan" id="kobo.174.2">This type of KG can be very useful for predictive analysis. </span><span class="koboSpan" id="kobo.174.3">We can see these KGs in the </span><span class="No-Break"><span class="koboSpan" id="kobo.175.1">following figure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer186">
<span class="koboSpan" id="kobo.176.1"><img alt="Figure 7.6 – Different types of knowledge graphs" src="image/B21257_07_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.177.1">Figure 7.6 – Different types of knowledge graphs</span></p>
<h2 id="_idParaDest-119"><a id="_idTextAnchor118"/><span class="koboSpan" id="kobo.178.1">Taxonomies and ontologies</span></h2>
<p><span class="koboSpan" id="kobo.179.1">The main difference between a graph and a KG is that the former is a given structure representing relationships between entities, while the latter makes semantic relationships explicit by allowing reasoning and inference to humans and machines. </span><span class="koboSpan" id="kobo.179.2">So, the advantage of a KG is that we can use algorithms for both graphs and specific reasoning algorithms (we will see later, in the </span><em class="italic"><span class="koboSpan" id="kobo.180.1">Understanding graph reasoning</span></em><span class="koboSpan" id="kobo.181.1"> section, some approaches in detail). </span><span class="koboSpan" id="kobo.181.2">These capabilities are enhanced by incorporating metadata. </span><span class="koboSpan" id="kobo.181.3">Indeed, we can </span><a id="_idIndexMarker777"/><span class="koboSpan" id="kobo.182.1">construct </span><strong class="bold"><span class="koboSpan" id="kobo.183.1">KG taxonomies</span></strong><span class="koboSpan" id="kobo.184.1">, which can be seen as hierarchical structures of similar semantic meaning (usually tree-like, for example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.185.1">dog</span></strong><span class="koboSpan" id="kobo.186.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.187.1">cat</span></strong><span class="koboSpan" id="kobo.188.1"> entities might be grouped under </span><strong class="source-inline"><span class="koboSpan" id="kobo.189.1">mammals</span></strong><span class="koboSpan" id="kobo.190.1">). </span><span class="koboSpan" id="kobo.190.2">In addition, multiple taxonomies can be integrated if necessary (thus having multiple trees to allow for more refined searching). </span><span class="koboSpan" id="kobo.190.3">These taxonomies help in searching or when we need to filter and work with very large KGs. </span><strong class="bold"><span class="koboSpan" id="kobo.191.1">KG ontologies</span></strong><span class="koboSpan" id="kobo.192.1"> are </span><a id="_idIndexMarker778"/><span class="koboSpan" id="kobo.193.1">similar to taxonomies and may have a similar hierarchical structure, but they are a more flexible and expressive structure. </span><span class="koboSpan" id="kobo.193.2">In fact, an ontology is used to define the relationships, properties, and classification of a group of entities. </span><span class="koboSpan" id="kobo.193.3">It also allows rules to be defined about how these entities may interact (union, complement, cardinality, and transitive or intransitive properties of relationships). </span><span class="koboSpan" id="kobo.193.4">Ontologies also enable a shared vocabulary, thus allowing information to be integrated consistently. </span><span class="koboSpan" id="kobo.193.5">Ontologies can thus allow conducting reasoning and solving more complex problems. </span><span class="koboSpan" id="kobo.193.6">For example, we can add properties and then use these properties to solve a problem (Bob owns a car that has a </span><strong class="source-inline"><span class="koboSpan" id="kobo.194.1">maximum_speed</span></strong><span class="koboSpan" id="kobo.195.1"> property of </span><strong class="source-inline"><span class="koboSpan" id="kobo.196.1">100</span></strong><span class="koboSpan" id="kobo.197.1"> km, so Bob will not be able to get there in less than an hour because his job has a </span><strong class="source-inline"><span class="koboSpan" id="kobo.198.1">distance_from_home</span></strong><span class="koboSpan" id="kobo.199.1"> property of </span><strong class="source-inline"><span class="koboSpan" id="kobo.200.1">120</span></strong><span class="koboSpan" id="kobo.201.1"> km). </span><span class="koboSpan" id="kobo.201.2">Rules allow us to be able to improve search and solve tasks that were too complex before (for example, we can assign different properties to relations: if </span><strong class="source-inline"><span class="koboSpan" id="kobo.202.1">married_to</span></strong><span class="koboSpan" id="kobo.203.1"> is transitive, we can automatically infer information about a person without the relation being specified). </span><span class="koboSpan" id="kobo.203.2">Thanks to ontologies, we can</span><a id="_idIndexMarker779"/><span class="koboSpan" id="kobo.204.1"> conduct certain types of reasoning effectively and quickly, such as deductive reasoning, class inference, transitive reasoning, and </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">so on.</span></span></p>
<p><span class="koboSpan" id="kobo.206.1">Ontologies are generally grouped into </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">two groups:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.208.1">Domain-independent ontologies</span></strong><span class="koboSpan" id="kobo.209.1">: Ontologies that provide fundamental concepts that are not tied to a </span><a id="_idIndexMarker780"/><span class="koboSpan" id="kobo.210.1">particular domain. </span><span class="koboSpan" id="kobo.210.2">They </span><a id="_idIndexMarker781"/><span class="koboSpan" id="kobo.211.1">provide a high-level view that helps with data integration, especially when there are several domains. </span><span class="koboSpan" id="kobo.211.2">Commonly, these are a small number, they represent the first level, and they are the first </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">ones built.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.213.1">Domain ontologies</span></strong><span class="koboSpan" id="kobo.214.1">: These are</span><a id="_idIndexMarker782"/><span class="koboSpan" id="kobo.215.1"> focused on a domain and are used to provide the fundamental terminology. </span><span class="koboSpan" id="kobo.215.2">They </span><a id="_idIndexMarker783"/><span class="koboSpan" id="kobo.216.1">are most useful for specialized domains such as medicine and finance. </span><span class="koboSpan" id="kobo.216.2">They are usually found at levels below the domain-independent one and are a subclass </span><span class="No-Break"><span class="koboSpan" id="kobo.217.1">of it.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.218.1">In this section, we have seen how KGs are flexible systems that can store data and be able to easily find knowledge. </span><span class="koboSpan" id="kobo.218.2">This flexibility makes them powerful tools for subsequent analysis, but at the same time, does not make them easy to build. </span><span class="koboSpan" id="kobo.218.3">In the next section, we will see how we can build a KG from a collection </span><span class="No-Break"><span class="koboSpan" id="kobo.219.1">of texts.</span></span></p>
<h1 id="_idParaDest-120"><a id="_idTextAnchor119"/><span class="koboSpan" id="kobo.220.1">Creating a knowledge graph with your LLM</span></h1>
<p><span class="koboSpan" id="kobo.221.1">The construction of a KG is generally a multistep</span><a id="_idIndexMarker784"/><span class="koboSpan" id="kobo.222.1"> process consisting of </span><span class="No-Break"><span class="koboSpan" id="kobo.223.1">the </span></span><span class="No-Break"><a id="_idIndexMarker785"/></span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">following:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.225.1">Knowledge creation</span></strong><span class="koboSpan" id="kobo.226.1">: The first step, in which we define the purpose of this KG, is to gather the sources from which to extract knowledge. </span><span class="koboSpan" id="kobo.226.2">In this step, we have to decide how we build our KG but also where we maintain it. </span><span class="koboSpan" id="kobo.226.3">Once built, the KG has to be stored, and we have to have an efficient structure to </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">query it.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.228.1">Knowledge assessment</span></strong><span class="koboSpan" id="kobo.229.1">: In this step, we assess the quality of the </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">KG obtained.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.231.1">Knowledge cleaning</span></strong><span class="Annotation-reference"><span class="koboSpan" id="kobo.232.1">:</span></span><span class="koboSpan" id="kobo.233.1"> There are several steps and procedures to make sure there are no errors and then correct them. </span><span class="koboSpan" id="kobo.233.2">This step can be conducted at the same time as knowledge assessment, and some pipelines conduct </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">them together.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.235.1">Knowledge enrichment</span></strong><span class="koboSpan" id="kobo.236.1">: This involves a series of steps to identify whether there are gaps in knowledge. </span><span class="koboSpan" id="kobo.236.2">We can also integrate additional sources (extract information from other datasets, integrate databases, or merge </span><span class="No-Break"><span class="koboSpan" id="kobo.237.1">multiple KGs).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.238.1">Knowledge deployment</span></strong><span class="koboSpan" id="kobo.239.1">: In this final </span><a id="_idIndexMarker786"/><span class="koboSpan" id="kobo.240.1">step, the KG is deployed </span><a id="_idIndexMarker787"/><span class="koboSpan" id="kobo.241.1">either as a standalone application (e.g., as a graph database) or used within </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">another application.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.243.1">We can see the process in the </span><span class="No-Break"><span class="koboSpan" id="kobo.244.1">following figure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer187">
<span class="koboSpan" id="kobo.245.1"><img alt="Figure 7.7 – KG construction pipeline" src="image/B21257_07_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.246.1">Figure 7.7 – KG construction pipeline</span></p>
<h2 id="_idParaDest-121"><a id="_idTextAnchor120"/><span class="koboSpan" id="kobo.247.1">Knowledge creation</span></h2>
<p><span class="koboSpan" id="kobo.248.1">In general, when building a KG from scratch, the definition of ontologies is the first step. </span><span class="koboSpan" id="kobo.248.2">There are several guides on how to </span><a id="_idIndexMarker788"/><span class="koboSpan" id="kobo.249.1">build them (both libraries and tools to visualize them). </span><span class="koboSpan" id="kobo.249.2">Efforts are made to build ontologies that are clear, verifiable, and reusable. </span><span class="koboSpan" id="kobo.249.3">Defining ontologies should be done with a purpose in mind, discussing what the purpose of a KG is with various stakeholders, and then defining ontologies accordingly. </span><span class="koboSpan" id="kobo.249.4">The most relevant ones should be chosen (the first level of the KG), following which the hierarchy and properties should be defined. </span><span class="koboSpan" id="kobo.249.5">There are two approaches: top-down (define core ontologies first and then more specialized ones) or bottom-up (define specialized ontologies and then group them into superclasses). </span><span class="koboSpan" id="kobo.249.6">Especially for specialized domains, we could start from ontologies that have already been built (there are several </span><a id="_idIndexMarker789"/><span class="koboSpan" id="kobo.250.1">defined for finance, medicine, and academic research) and this ensures </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">better interoperability.</span></span></p>
<p><span class="koboSpan" id="kobo.252.1">The next step is to extract knowledge from our sources. </span><span class="koboSpan" id="kobo.252.2">In this step, we have to extract triplets (or a set of facts) from a text corpus or another source (a database, or structured and unstructured data). </span><span class="koboSpan" id="kobo.252.3">Two tasks can </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">be defined:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.254.1">Named entity recognition</span></strong><span class="koboSpan" id="kobo.255.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.256.1">NER</span></strong><span class="koboSpan" id="kobo.257.1">): NER is the task of </span><a id="_idIndexMarker790"/><span class="koboSpan" id="kobo.258.1">extracting entities from text and </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">classifying them</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.260.1">Relation extraction</span></strong><span class="koboSpan" id="kobo.261.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.262.1">RE</span></strong><span class="koboSpan" id="kobo.263.1">): RE is the </span><a id="_idIndexMarker791"/><span class="koboSpan" id="kobo.264.1">task of identifying connections between various entities in </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">a context</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.266.1">NER is one of the most common tasks in </span><strong class="bold"><span class="koboSpan" id="kobo.267.1">natural language processing</span></strong><span class="koboSpan" id="kobo.268.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.269.1">NLP</span></strong><span class="koboSpan" id="kobo.270.1">) and is used not only for KG creation but also as a </span><a id="_idIndexMarker792"/><span class="koboSpan" id="kobo.271.1">key step when we want to move from unstructured text to structured data. </span><span class="koboSpan" id="kobo.271.2">It generally requires a pipeline consisting of several steps (text preprocessing, entity identification and classification, contextual analysis, and data post-processing). </span><span class="koboSpan" id="kobo.271.3">During NER, we try to identify entities by first conducting a preprocessing step to avoid errors in the pipeline (e.g., proper tokenization). </span><span class="koboSpan" id="kobo.271.4">Once entities are identified, they are usually classified (e.g., by adding a label such as people, organizations, or places). </span><span class="koboSpan" id="kobo.271.5">In addition, surrounding text is attempted to be used to disambiguate them (e.g., trying to recognize whether </span><em class="italic"><span class="koboSpan" id="kobo.272.1">Apple</span></em><span class="koboSpan" id="kobo.273.1"> in the text represents the fruit or the company). </span><span class="koboSpan" id="kobo.273.2">A preprocessing step is then conducted to resolve ambiguities or merge </span><span class="No-Break"><span class="koboSpan" id="kobo.274.1">multi-token entities.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer188">
<span class="koboSpan" id="kobo.275.1"><img alt="Figure 7.8 – Example of NER (https://arxiv.org/pdf/2401.10825)" src="image/B21257_07_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.276.1">Figure 7.8 – Example of NER (</span><a href="https://arxiv.org/pdf/2401.10825"><span class="koboSpan" id="kobo.277.1">https://arxiv.org/pdf/2401.10825</span></a><span class="koboSpan" id="kobo.278.1">)</span></p>
<p><span class="koboSpan" id="kobo.279.1">RE is the task in which we understand the relationships between the various extracted entities. </span><span class="koboSpan" id="kobo.279.2">More formally, we use a model to identify and categorize the connections between entities in a text (e.g., in the sentence </span><em class="italic"><span class="koboSpan" id="kobo.280.1">Bob works at Apple</span></em><span class="koboSpan" id="kobo.281.1">, we extract the relationship </span><em class="italic"><span class="koboSpan" id="kobo.282.1">works at</span></em><span class="koboSpan" id="kobo.283.1">). </span><span class="koboSpan" id="kobo.283.2">It can be considered a separate task or, in some cases, conducted together with NER (e.g., with a single model). </span><span class="koboSpan" id="kobo.283.3">Also, RE is a key step for KG creation and, at the same time, useful for several other NLP tasks (such as question answering, information retrieval, and </span><span class="No-Break"><span class="koboSpan" id="kobo.284.1">so on).</span></span></p>
<p><span class="koboSpan" id="kobo.285.1">There are several methods to be</span><a id="_idIndexMarker793"/><span class="koboSpan" id="kobo.286.1"> able to conduct NER and RE. </span><span class="koboSpan" id="kobo.286.2">The earliest and most laborious methods were knowledge-based or rule-based. </span><span class="koboSpan" id="kobo.286.3">For example, one of the simplest approaches to identifying company names in financial documents was to use indicators such as capital letters (identify </span><strong class="source-inline"><span class="koboSpan" id="kobo.287.1">Mr.</span></strong><span class="koboSpan" id="kobo.288.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.289.1">Ms.</span></strong><span class="koboSpan" id="kobo.290.1"> elements to extract people, and so on). </span><span class="koboSpan" id="kobo.290.2">Rule-based worked very well for standardized documents (such as clinical notes or official documents) but demonstrated little scalability. </span><span class="koboSpan" id="kobo.290.3">These methods require establishing laborious upstream rules and specific knowledge, risking missing many entities in </span><span class="No-Break"><span class="koboSpan" id="kobo.291.1">different datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.292.1">Statistical methods based on the hidden Markov model, conditional random fields, or maximum entropy (methods that rely on predicting the entity based on likelihood learned from training data) have allowed for greater scalability. </span><span class="koboSpan" id="kobo.292.2">These methods, though, require large, quality datasets that have defined labels. </span><span class="koboSpan" id="kobo.292.3">Other supervised learning algorithms have been used to predict entities and then extract them. </span><span class="koboSpan" id="kobo.292.4">These algorithms have worked well with high computational costs and especially the need for labels. </span><span class="koboSpan" id="kobo.292.5">Obtaining labels is expensive and these datasets quickly become outdated (new companies, new products, and so on </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">are created).</span></span></p>
<p><span class="koboSpan" id="kobo.294.1">Recently, given the advances in unsupervised learning (models such as the transformer), it has been decided to use LLMs also for NER and RE and for constructing KGs (in some studies, these are called </span><strong class="bold"><span class="koboSpan" id="kobo.295.1">LLM-augmented KGs</span></strong><span class="koboSpan" id="kobo.296.1">). </span><span class="koboSpan" id="kobo.296.2">The ability to </span><a id="_idIndexMarker794"/><span class="koboSpan" id="kobo.297.1">process large corpora of text, the knowledge acquired during pre-training, and their versatility make LLMs useful for the construction of KGs (and other related tasks that we will </span><span class="No-Break"><span class="koboSpan" id="kobo.298.1">see later).</span></span></p>
<p><span class="koboSpan" id="kobo.299.1">Due to their ability to leverage contextual information and linguistic abilities, state-of-the-art methods generally </span><a id="_idIndexMarker795"/><span class="koboSpan" id="kobo.300.1">employ transformer-based models for NER tasks. </span><span class="koboSpan" id="kobo.300.2">Previous methods had problems with texts that had complex structures (a token that belongs to several entities, or entities that are not contiguous in the text) while transformers are superior in solving these cases. </span><span class="koboSpan" id="kobo.300.3">BERT-based models were previously used, which were then later fine-tuned for different tasks. </span><span class="koboSpan" id="kobo.300.4">Today, however, we exploit the capabilities of an LLM that does not need to be fine-tuned but can learn a task without training through in-context learning. </span><span class="koboSpan" id="kobo.300.5">An LLM can then directly extract entities from text without the need for labels and provide them in the desired format (for example, we might want the LLM to provide a list of triplets or the triplet plus a given label). </span><span class="koboSpan" id="kobo.300.6">To avoid disambiguation problems, we can ask the LLM to provide additional information when conducting the extraction. </span><span class="koboSpan" id="kobo.300.7">For example, in music, </span><em class="italic"><span class="koboSpan" id="kobo.301.1">Apple</span></em><span class="koboSpan" id="kobo.302.1"> can refer to Apple Music, the British psychedelic rock band Apple, or the singer Fiona Apple. </span><span class="koboSpan" id="kobo.302.2">LLMs can help us disambiguate which of these entities it refers to based on the context of the period. </span><span class="koboSpan" id="kobo.302.3">At the same time, the flexibility of LLMs allows us to tie entities to various ontologies </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">during extraction.</span></span></p>
<p><span class="koboSpan" id="kobo.304.1">Similarly, an LLM can help with the RE task. </span><span class="koboSpan" id="kobo.304.2">There are several ways to do this. </span><span class="koboSpan" id="kobo.304.3">One of the simplest is to conduct sentence-level RE, in which you provide the model with a sentence and it must extract the relationship between the two entities. </span><span class="koboSpan" id="kobo.304.4">The extension of this approach is to extract all the relationships between entities at the level of an entire document. </span><span class="koboSpan" id="kobo.304.5">Since this is not an easy task, more sophisticated approaches with more than one LLM can be used to make sure that we can understand relationships at the local and global levels of the document (for example, in a document, a local relationship between two entities is in the same sentence, but we can also have global relationships where an entity mentioned at the beginning of the document is related to an entity that is present at the end of </span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">the document).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer189">
<span class="koboSpan" id="kobo.306.1"><img alt="Figure 7.9 – General framework of LLM-based KG construction (This information was taken from an article published in 2023; https://arxiv.org/pdf/2306.08302)" src="image/B21257_07_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.307.1">Figure 7.9 – General framework of LLM-based KG construction (This information was taken from an article published in 2023; </span><a href="https://arxiv.org/pdf/2306.08302"><span class="koboSpan" id="kobo.308.1">https://arxiv.org/pdf/2306.08302</span></a><span class="koboSpan" id="kobo.309.1">)</span></p>
<p><span class="koboSpan" id="kobo.310.1">As mentioned, the two tasks need not be conducted separately (NER and RE), but an LLM provides the flexibility to conduct them in a single step. </span><span class="koboSpan" id="kobo.310.2">In this case, it is of great importance to define the</span><a id="_idIndexMarker796"/><span class="koboSpan" id="kobo.311.1"> right prompt in which we instruct the model in extracting entities and relationships and in which format we want the output. </span><span class="koboSpan" id="kobo.311.2">We can then proceed iteratively, extracting entities and relationships for a large body of text. </span><span class="koboSpan" id="kobo.311.3">Alternatively, we can use a set of prompts for different tasks (one for entity extraction, one for relation extraction, and so on) and scan the corpus and these prompts automatically with the LLM. </span><span class="koboSpan" id="kobo.311.4">In some approaches, to maintain more flexibility, one LLM is used for extraction and then a smaller LLM is used </span><span class="No-Break"><span class="koboSpan" id="kobo.312.1">for correction.</span></span></p>
<p><span class="koboSpan" id="kobo.313.1">Another interesting perspective is that an LLM is enough to create a KG. </span><span class="koboSpan" id="kobo.313.2">In fact, LLMs are trained with a huge amount of text (the latest LLMs are trained with trillions of tokens that include scraping the internet and thousands of books). </span><span class="koboSpan" id="kobo.313.3">Several studies today show that even small LLMs (around 7 billion parameters) have considerable knowledge, especially about facts (the definition of knowledge in an LLM is also complicated because this is not associated with a single parameter but widespread). </span><span class="koboSpan" id="kobo.313.4">Therefore, some authors have proposed distilling knowledge directly from the LLM. </span><span class="koboSpan" id="kobo.313.5">In this case, by exploiting prompts constructed for the</span><a id="_idIndexMarker797"/><span class="koboSpan" id="kobo.314.1"> task, we conduct what is called a </span><strong class="bold"><span class="koboSpan" id="kobo.315.1">knowledge search</span></strong><span class="koboSpan" id="kobo.316.1"> of the LLM to extract triplets. </span><span class="koboSpan" id="kobo.316.2">In this way, by extracting facts directly from the LLM, we can then directly </span><a id="_idIndexMarker798"/><span class="koboSpan" id="kobo.317.1">construct our KG. </span><span class="koboSpan" id="kobo.317.2">KGs constructed in this way are competitive in quality, diversity, and novelty with those constructed with large </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">text datasets.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer190">
<span class="koboSpan" id="kobo.319.1"><img alt="Figure 7.10 – General framework of distilling KGs from LLMs (https://arxiv.org/pdf/2306.08302)" src="image/B21257_07_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.320.1">Figure 7.10 – General framework of distilling KGs from LLMs (</span><a href="https://arxiv.org/pdf/2306.08302"><span class="koboSpan" id="kobo.321.1">https://arxiv.org/pdf/2306.08302</span></a><span class="koboSpan" id="kobo.322.1">)</span></p>
<h2 id="_idParaDest-122"><a id="_idTextAnchor121"/><span class="koboSpan" id="kobo.323.1">Creating a knowledge graph with an LLM</span></h2>
<p><span class="koboSpan" id="kobo.324.1">In this tutorial, I will use Neo4j and LangChain to create a KG with an LLM. </span><span class="koboSpan" id="kobo.324.2">LangChain allows us to use LLMs efficiently to </span><a id="_idIndexMarker799"/><span class="koboSpan" id="kobo.325.1">extract information from a text corpus, while</span><a id="_idIndexMarker800"/><span class="koboSpan" id="kobo.326.1"> Neo4j is a program for analyzing and visualizing graphs. </span><span class="koboSpan" id="kobo.326.2">The complete code is in the book’s GitHub repository (</span><a href="https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr7"><span class="koboSpan" id="kobo.327.1">https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr7</span></a><span class="koboSpan" id="kobo.328.1">); here, we will describe the general process and the most important code snippets. </span><span class="koboSpan" id="kobo.328.2">We can have </span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">two methods:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.330.1">Custom method</span></strong><span class="koboSpan" id="kobo.331.1">: LLMs have innate abilities to be able to accomplish tasks; we can take advantage of these </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">generalist abilities</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.333.1">LangChain graph transformers</span></strong><span class="koboSpan" id="kobo.334.1">: Today, there are libraries that make the job easier and allow just a few lines of code to achieve the </span><span class="No-Break"><span class="koboSpan" id="kobo.335.1">same result</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.336.1">The custom method is simply to define a prompt that allows the model to understand the task and execute it efficiently. </span><span class="koboSpan" id="kobo.336.2">In this case, our prompt is structured with the </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">following elements:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.338.1">A clear definition of the task with a set of bullet points. </span><span class="koboSpan" id="kobo.338.2">The task description can contain both what the model must do and what it must </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">not do.</span></span></li>
<li><span class="koboSpan" id="kobo.340.1">Additional context that allows the model to better understand how to perform the task. </span><span class="koboSpan" id="kobo.340.2">Since these </span><a id="_idIndexMarker801"/><span class="koboSpan" id="kobo.341.1">models are trained for dialogic tasks, providing</span><a id="_idIndexMarker802"/><span class="koboSpan" id="kobo.342.1"> them with information about what role they should play helps </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">the performance.</span></span></li>
<li><span class="koboSpan" id="kobo.344.1">Some examples to explain how to perform </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">the task.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.346.1">This approach builds on what we learned in </span><a href="B21257_03.xhtml#_idTextAnchor042"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.347.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.348.1">. </span><span class="koboSpan" id="kobo.348.2">The model we are using is instruction-tuned (trained to perform tasks) so providing clear instructions helps the model understand the task and perform it. </span><span class="koboSpan" id="kobo.348.3">The addition of some examples leverages in-context learning. </span><span class="koboSpan" id="kobo.348.4">Using a crafted prompt allows us to be flexible and be able to adapt the prompt to </span><span class="No-Break"><span class="koboSpan" id="kobo.349.1">our needs:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.350.1">
#Custom method
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage
from langchain_core.output_parsers import StrOutputParser
prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content="""
    You are a helpful assistant in creates knowledge graphs by Generating Cypher Queries.\n
    Task:
     *  Identify Entities, Relationships and Property Keys from Context.\n
     *  Generate Cypher Query to Create Knowledge Graph from the Entities Relationships and Property Keys discovered.\n
     *  Extract ALL Entities and RelationShips as Possible.\n
     *  Always extract a person Profession as an Entity.\n
     *  Be creative.
</span><span class="koboSpan" id="kobo.350.2">     *  Understand hidden relationships from the network.
</span><span class="koboSpan" id="kobo.350.3">     Note: Read the Context twice and carefully before generating Cypher Query.\n
     Note: Do not return anything other than the Cypher Query.\n
     Note: Do not include any explanations or apologies in your responses.\n
     Note: Do not hallucinate.\n
     Entities include Person, Place, Product, WorkPlaces, Companies, City, Country, Animals, Tags like peoples Profession and more \n
     Few Shot Prompts:
      Example Context:
       Mary was born in 1995. </span><span class="koboSpan" id="kobo.350.4">She is Friends with Jane and John. </span><span class="koboSpan" id="kobo.350.5">Jane is 2 years older than Mary.
</span><span class="koboSpan" id="kobo.350.6">       Mary has a dog named Max,and is 3 years old. </span><span class="koboSpan" id="kobo.350.7">She is also married to John. </span><span class="koboSpan" id="kobo.350.8">Mary is from USA and a Software Engineer.
</span><span class="koboSpan" id="kobo.350.9">      Answer:
        MERGE (Mary:Person {name: "Mary", birth_year: 1995})
        MERGE (Jane:Person {name: "Jane", age:1993})
        MERGE (John:Person {name: "John"})
        MERGE (Mary)-[:FRIENDS_WITH]-&gt;(Jane)
        MERGE (Mary)-[:FRIENDS_WITH]-&gt;(John)
        MERGE (Jane)-[:FRIENDS_WITH]-&gt;(Mary)
        MERGE (John)-[:FRIENDS_WITH]-&gt;(Mary)
        MERGE (Mary)-[:HAS_DOG]-&gt;(Max:Dog {name: "Max", age: 3})
        MERGE (Mary)-[:MARRIED_TO]-&gt;(John)
        MERGE (Mary)-[:HAS_PROFESSION]-&gt;(SoftwareEngineer:Profession {name: "Software Engineer"})
        MERGE (Mary)-[:FROM]-&gt;(USA:Country {name: "USA"})
    """),
    ("human", "Context:{text}"),
])</span></pre> <p><span class="koboSpan" id="kobo.351.1">Executing the </span><a id="_idIndexMarker803"/><span class="koboSpan" id="kobo.352.1">preceding should </span><a id="_idIndexMarker804"/><span class="koboSpan" id="kobo.353.1">yield the </span><span class="No-Break"><span class="koboSpan" id="kobo.354.1">following result:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer191">
<span class="koboSpan" id="kobo.355.1"><img alt="Figure 7.11 – Screenshot of the results" src="image/B21257_07_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.356.1">Figure 7.11 – Screenshot of the results</span></p>
<p><span class="koboSpan" id="kobo.357.1">The results show how a crafted prompt succeeds in generating triplets (which we can then use to construct </span><a id="_idIndexMarker805"/><span class="koboSpan" id="kobo.358.1">our KG). </span><span class="koboSpan" id="kobo.358.2">This highlights the great flexibility </span><span class="No-Break"><span class="koboSpan" id="kobo.359.1">of LLMs.</span></span></p>
<p><span class="koboSpan" id="kobo.360.1">We do not always want a </span><a id="_idIndexMarker806"/><span class="koboSpan" id="kobo.361.1">custom approach but might want to use a more established pipeline. </span><span class="koboSpan" id="kobo.361.2">LangChain provides the ability to do this with just a few lines </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">of code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.363.1">
from langchain_core.documents import Document
from langchain_experimental.graph_transformers import LLMGraphTransformer
llm_transformer = LLMGraphTransformer(llm=llm)
documents = [Document(page_content=content)]
graph_documents = llm_transformer.convert_to_graph_documents(documents)</span></pre> <p><span class="koboSpan" id="kobo.364.1">LangChain gives us the same </span><a id="_idIndexMarker807"/><span class="koboSpan" id="kobo.365.1">result in an already structured format that simplifies </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">our work.</span></span></p>
<p><span class="koboSpan" id="kobo.367.1">The graph can then be</span><a id="_idIndexMarker808"/><span class="koboSpan" id="kobo.368.1"> visualized in Neo4j and we can work on the graph, conduct searches, select nodes, and </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">so on.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer192">
<span class="koboSpan" id="kobo.370.1"><img alt="Figure 7.12 – Screenshot of the graph from Neo4j" src="image/B21257_07_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.371.1">Figure 7.12 – Screenshot of the graph from Neo4j</span></p>
<p><span class="koboSpan" id="kobo.372.1">Once the graph is generated, we can use it for our queries. </span><span class="koboSpan" id="kobo.372.2">Obviously, we can conduct these queries in Neo4j, but it is </span><a id="_idIndexMarker809"/><span class="koboSpan" id="kobo.373.1">also possible to do it in Python. </span><span class="koboSpan" id="kobo.373.2">For </span><a id="_idIndexMarker810"/><span class="koboSpan" id="kobo.374.1">example, LangChain allows us to conduct queries of </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">our KG:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.376.1">
from langchain.chains import GraphCypherQAChain
graphchain = GraphCypherQAChain.from_llm(
    llm, graph=graph, verbose=True, return_intermediate_steps=True
)
results = graphchain.invoke({"query":"People who have kids"})
print(results["result"])</span></pre> <p><span class="koboSpan" id="kobo.377.1">Once executed the code, you should obtain </span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">these results:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer193">
<span class="koboSpan" id="kobo.379.1"><img alt="Figure 7.13 – Querying the KG" src="image/B21257_07_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.380.1">Figure 7.13 – Querying the KG</span></p>
<p><span class="koboSpan" id="kobo.381.1">As we can see, LangChain in this case </span><a id="_idIndexMarker811"/><span class="koboSpan" id="kobo.382.1">generates a corresponding query in Cypher and then conducts the graph query. </span><span class="koboSpan" id="kobo.382.2">In this way, we are using an LLM to generate a query in Cypher, while we can write directly in </span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">natural language.</span></span></p>
<h2 id="_idParaDest-123"><a id="_idTextAnchor122"/><span class="koboSpan" id="kobo.384.1">Knowledge assessment</span></h2>
<p><span class="koboSpan" id="kobo.385.1">Once the KG has been </span><a id="_idIndexMarker812"/><span class="koboSpan" id="kobo.386.1">created, it is necessary to check for errors and the overall quality of the KG. </span><span class="koboSpan" id="kobo.386.2">The quality of a KG is assessed by a set of dimensions (there are metrics associated with each of these dimensions) that are used to monitor the KG in terms of accessibility, representation, context, and intrinsic quality. </span><span class="koboSpan" id="kobo.386.3">Some of the metrics are </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.388.1">Accuracy</span></strong><span class="koboSpan" id="kobo.389.1">: This metric assesses accuracy in syntactic and </span><span class="No-Break"><span class="koboSpan" id="kobo.390.1">semantic terms.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.391.1">Completeness</span></strong><span class="koboSpan" id="kobo.392.1">: This metric measures how much knowledge a KG contains with respect to a certain domain or task. </span><span class="koboSpan" id="kobo.392.2">It usually measures whether a KG contains all the necessary entities and relationships for a domain (sometimes a comparison with a golden standard KG </span><span class="No-Break"><span class="koboSpan" id="kobo.393.1">is used).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.394.1">Conciseness</span></strong><span class="koboSpan" id="kobo.395.1">: KGs allow knowledge to</span><a id="_idIndexMarker813"/><span class="koboSpan" id="kobo.396.1"> be expressed efficiently, but they risk scaling quickly. </span><span class="koboSpan" id="kobo.396.2">Blank nodes (specific types of nodes that represent anonymous or unnamed entities, used when a node is needed in the graph but is not precisely indicated) can often be generated during the creation process. </span><span class="koboSpan" id="kobo.396.3">If care is not taken, one risks filling the KG with </span><span class="No-Break"><span class="koboSpan" id="kobo.397.1">blank nodes.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.398.1">Timeliness</span></strong><span class="koboSpan" id="kobo.399.1">: Knowledge should also be updated regularly because it can change and become outdated. </span><span class="koboSpan" id="kobo.399.2">Therefore, it is important to decide the frequency </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">of updates.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.401.1">Accessibility, ease of manipulation, and operation</span></strong><span class="koboSpan" id="kobo.402.1">: KGs are used for searches or other tasks; metrics exist today that measure the usefulness of KGs. </span><span class="koboSpan" id="kobo.402.2">In fact, for a KG to be useful, it must be easily accessible, be able to be manipulated, and be able to conduct research </span><span class="No-Break"><span class="koboSpan" id="kobo.403.1">and updates.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.404.1">Ease of understanding</span></strong><span class="koboSpan" id="kobo.405.1">: Since the KG is meant to be used to represent knowledge for humans, some authors have proposed measuring the degree to which the KG is interpretable for humans. </span><span class="koboSpan" id="kobo.405.2">Indeed, today, there is a greater emphasis on transparency and interpretation of models </span><span class="No-Break"><span class="koboSpan" id="kobo.406.1">in AI.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.407.1">Security, privacy, and traceability</span></strong><span class="koboSpan" id="kobo.408.1">: Metrics also exist today to control who accesses the KG and whether it is secure from outside access. </span><span class="koboSpan" id="kobo.408.2">Similarly, knowledge also needs to be tracked, as we need to be sure which sources it comes from. </span><span class="koboSpan" id="kobo.408.3">Traceability also allows us to be in privacy compliance. </span><span class="koboSpan" id="kobo.408.4">For example, our KG may contain sensitive data about users or come from erroneous or problematic documents. </span><span class="koboSpan" id="kobo.408.5">Traceability allows us to correct these errors, delete data from users who require their data to be deleted, and </span><span class="No-Break"><span class="koboSpan" id="kobo.409.1">so on.</span></span></li>
</ul>
<h2 id="_idParaDest-124"><a id="_idTextAnchor123"/><span class="koboSpan" id="kobo.410.1">Knowledge cleaning</span></h2>
<p><span class="koboSpan" id="kobo.411.1">Having assessed the quality of our KG, we can see that there are errors. </span><span class="koboSpan" id="kobo.411.2">In general, error detection and correction are</span><a id="_idIndexMarker814"/><span class="koboSpan" id="kobo.412.1"> together called </span><strong class="bold"><span class="koboSpan" id="kobo.413.1">knowledge cleaning</span></strong><span class="koboSpan" id="kobo.414.1">. </span><span class="koboSpan" id="kobo.414.2">Different types of errors can occur in </span><span class="No-Break"><span class="koboSpan" id="kobo.415.1">a KG:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.416.1">We may have entities or relationships that have </span><span class="No-Break"><span class="koboSpan" id="kobo.417.1">syntactic errors</span></span></li>
<li><span class="koboSpan" id="kobo.418.1">Some errors may be ontology-related (assigning to ontologies that do not exist, connecting them to the wrong ontologies, wrong properties of ontologies, and </span><span class="No-Break"><span class="koboSpan" id="kobo.419.1">so on)</span></span></li>
<li><span class="koboSpan" id="kobo.420.1">Some may be semantic and may be more difficult </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">to identify</span></span></li>
<li><span class="koboSpan" id="kobo.422.1">There are also knowledge errors</span><a id="_idIndexMarker815"/><span class="koboSpan" id="kobo.423.1"> that may result from errors in the sources for creating the knowledge (symptom </span><em class="italic"><span class="koboSpan" id="kobo.424.1">x</span></em><span class="koboSpan" id="kobo.425.1"> is not a symptom of disease </span><em class="italic"><span class="koboSpan" id="kobo.426.1">y</span></em><span class="koboSpan" id="kobo.427.1">, person </span><em class="italic"><span class="koboSpan" id="kobo.428.1">x</span></em><span class="koboSpan" id="kobo.429.1"> is not the CEO of company </span><em class="italic"><span class="koboSpan" id="kobo.430.1">y</span></em><span class="koboSpan" id="kobo.431.1">, and </span><span class="No-Break"><span class="koboSpan" id="kobo.432.1">so on)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.433.1">There are several methods to detect these errors. </span><span class="koboSpan" id="kobo.433.2">The simplest methods are statistical and seek to identify outliers in a KG by exploiting probabilistic and statistical modeling. </span><span class="koboSpan" id="kobo.433.3">There are also more sophisticated variations that exploit simple machine-learning models. </span><span class="koboSpan" id="kobo.433.4">These models are not particularly accurate. </span><span class="koboSpan" id="kobo.433.5">Since we can use logical reasoning and ontologies with KGs, there are knowledge-based reasoning methods to identify outliers (e.g., an instance of a person cannot also be an instance of a place, so by exploiting similar rules, we can identify outliers). </span><span class="koboSpan" id="kobo.433.6">Finally, there are methods based on AI, and one can also use an LLM to check for errors. </span><span class="koboSpan" id="kobo.433.7">An LLM possesses both knowledge and reasoning skills so it can be used to verify that facts are correct. </span><span class="koboSpan" id="kobo.433.8">For example, if for some error, we have the triplet </span><strong class="source-inline"><span class="koboSpan" id="kobo.434.1">(Vienna, CapitalOf, Hungary)</span></strong><span class="koboSpan" id="kobo.435.1">, an LLM can identify the error). </span><span class="koboSpan" id="kobo.435.2">Then, there are similar methods for conducting KG correction. </span><span class="koboSpan" id="kobo.435.3">However, several frameworks have already been built and established to conduct detection </span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">and correction.</span></span></p>
<h2 id="_idParaDest-125"><a id="_idTextAnchor124"/><span class="koboSpan" id="kobo.437.1">Knowledge enrichment</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.438.1">Knowledge enrichment</span></strong><span class="koboSpan" id="kobo.439.1"> (or KG completion) is generally the next step. </span><span class="koboSpan" id="kobo.439.2">KGs are notoriously incomplete, so several rounds of KG completion and correction can occur. </span><span class="koboSpan" id="kobo.439.3">The completeness of a KG is sometimes </span><a id="_idIndexMarker816"/><span class="koboSpan" id="kobo.440.1">complicated to define and is contextual to the domain and application tasks. </span><span class="koboSpan" id="kobo.440.2">The first step in completing a KG is usually to identify additional sources of information (e.g., for a medical KG, it could be an additional biomedical database or an additional corpus of scientific articles). </span><span class="koboSpan" id="kobo.440.3">Often, in the first step of building the KG, we use only one data type (unstructured text) and then extend the extraction in a second step to other data types (CSV, XML, JSON, images, PDF, and so on). </span><span class="koboSpan" id="kobo.440.4">Each of these data types presents different challenges, so we should modify our pipeline. </span><span class="koboSpan" id="kobo.440.5">The more sources we use, the more crucial KG cleaning and alignment tasks become. </span><span class="koboSpan" id="kobo.440.6">For example, the more heterogeneous the sources, the greater the importance of entity resolution (identifying duplicate</span><a id="_idIndexMarker817"/><span class="koboSpan" id="kobo.441.1"> entities at the </span><span class="No-Break"><span class="koboSpan" id="kobo.442.1">KG level).</span></span></p>
<p><span class="koboSpan" id="kobo.443.1">An interesting alternative is to infer knowledge using an LLM (or other transformer models). </span><span class="koboSpan" id="kobo.443.2">For example, three possible approaches have </span><span class="No-Break"><span class="koboSpan" id="kobo.444.1">been explored:</span></span></p>
<ul>
<li> <strong class="bold"><span class="koboSpan" id="kobo.445.1">Joint encoding</span></strong><span class="koboSpan" id="kobo.446.1">: The triplet </span><strong class="source-inline"><span class="koboSpan" id="kobo.447.1">(h,r,t)</span></strong><span class="koboSpan" id="kobo.448.1"> is given to a model as a transformer model to predict the</span><a id="_idIndexMarker818"/><span class="koboSpan" id="kobo.449.1"> probability that it exists (</span><strong class="source-inline"><span class="koboSpan" id="kobo.450.1">0</span></strong><span class="koboSpan" id="kobo.451.1"> represents that the triplet is invalid, while </span><strong class="source-inline"><span class="koboSpan" id="kobo.452.1">1</span></strong><span class="koboSpan" id="kobo.453.1"> is a valid triplet). </span><span class="koboSpan" id="kobo.453.2">A variation of this approach is to take the final hidden state from the model and train a linear classifier to predict in a binary fashion whether the triplet is valid </span><span class="No-Break"><span class="koboSpan" id="kobo.454.1">or not.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.455.1">Masked language model (MLM) encoding</span></strong><span class="koboSpan" id="kobo.456.1">: Instead of conducting the entire embedding, one of</span><a id="_idIndexMarker819"/><span class="koboSpan" id="kobo.457.1"> the components of the triplet is masked and the model must predict it. </span><span class="koboSpan" id="kobo.457.2">In other words, if we have </span><strong class="source-inline"><span class="koboSpan" id="kobo.458.1">(h,r,?)</span></strong><span class="koboSpan" id="kobo.459.1">, we can try to complete </span><span class="No-Break"><span class="koboSpan" id="kobo.460.1">the gap.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.461.1">Separated encoding</span></strong><span class="koboSpan" id="kobo.462.1">: The</span><a id="_idIndexMarker820"/><span class="koboSpan" id="kobo.463.1"> triplet is separated into two components, </span><strong class="source-inline"><span class="koboSpan" id="kobo.464.1">(h,r)</span></strong><span class="koboSpan" id="kobo.465.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.466.1">(t)</span></strong><span class="koboSpan" id="kobo.467.1">. </span><span class="koboSpan" id="kobo.467.2">In this way, we get two representations from the model (the final hidden state of the model is used). </span><span class="koboSpan" id="kobo.467.3">After that, we use a scoring function to predict whether this triplet is valid. </span><span class="koboSpan" id="kobo.467.4">This approach is definitely more accurate but risks a combinatorial explosion. </span><span class="koboSpan" id="kobo.467.5">As you can see, in this approach, we are trying to calculate the similarity between two textual representations (the representation of </span><strong class="source-inline"><span class="koboSpan" id="kobo.468.1">(h,r)</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.469.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.470.1">(t)</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.471.1">).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.472.1">These approaches are, in fact, very similar to those we have seen in previous chapters when trying to calculate the similarity between </span><span class="No-Break"><span class="koboSpan" id="kobo.473.1">two sentences.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer194">
<span class="koboSpan" id="kobo.474.1"><img alt="Figure 7.14 – LLMs as encoders for KG completion (https://arxiv.org/pdf/2306.08302)" src="image/B21257_07_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.475.1">Figure 7.14 – LLMs as encoders for KG completion (</span><a href="https://arxiv.org/pdf/2306.08302"><span class="koboSpan" id="kobo.476.1">https://arxiv.org/pdf/2306.08302</span></a><span class="koboSpan" id="kobo.477.1">)</span></p>
<p><span class="koboSpan" id="kobo.478.1">Alternatively, one can use few-shot examples or other prompting techniques and ask an LLM to complete them directly. </span><span class="koboSpan" id="kobo.478.2">In addition, this approach allows you to be able to provide additional items in the </span><a id="_idIndexMarker821"/><span class="koboSpan" id="kobo.479.1">prompt. </span><span class="koboSpan" id="kobo.479.2">In previous approaches, we provided only the triplet </span><strong class="source-inline"><span class="koboSpan" id="kobo.480.1">(h,r,t)</span></strong><span class="koboSpan" id="kobo.481.1">; with prompt engineering, we can also provide other contextual elements (relationship descriptions, entity descriptions, etc.) or add instructions to better complete </span><span class="No-Break"><span class="koboSpan" id="kobo.482.1">the task.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer195">
<span class="koboSpan" id="kobo.483.1"><img alt="Figure 7.15 – Prompt-based KG completion (https://arxiv.org/pdf/2306.08302)" src="image/B21257_07_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.484.1">Figure 7.15 – Prompt-based KG completion (</span><a href="https://arxiv.org/pdf/2306.08302"><span class="koboSpan" id="kobo.485.1">https://arxiv.org/pdf/2306.08302</span></a><span class="koboSpan" id="kobo.486.1">)</span></p>
<h2 id="_idParaDest-126"><a id="_idTextAnchor125"/><span class="koboSpan" id="kobo.487.1">Knowledge hosting and deployment</span></h2>
<p><span class="koboSpan" id="kobo.488.1">The last step is the hosting and deployment of the KG. </span><span class="koboSpan" id="kobo.488.2">The KG is a set of nodes and relationships, so we can use a graph-specific paradigm to be able to store the data. </span><span class="koboSpan" id="kobo.488.3">Of course, hosting a KG is not </span><span class="No-Break"><span class="koboSpan" id="kobo.489.1">without </span></span><span class="No-Break"><a id="_idIndexMarker822"/></span><span class="No-Break"><span class="koboSpan" id="kobo.490.1">challenges:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.491.1">Size</span></strong><span class="koboSpan" id="kobo.492.1">: The larger the KG, the more complex its </span><span class="No-Break"><span class="koboSpan" id="kobo.493.1">management becomes</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.494.1">Data model</span></strong><span class="koboSpan" id="kobo.495.1">: We have to choose the system that allows us to optimally access the information for our tasks, as different systems have different advantages </span><span class="No-Break"><span class="koboSpan" id="kobo.496.1">and disadvantages</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.497.1">Heterogeneity</span></strong><span class="koboSpan" id="kobo.498.1">: The graph may contain multiple modes, thus making storage </span><span class="No-Break"><span class="koboSpan" id="kobo.499.1">more complex</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.500.1">Speed</span></strong><span class="koboSpan" id="kobo.501.1">: The more it grows, the more complex knowledge </span><span class="No-Break"><span class="koboSpan" id="kobo.502.1">updates become</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.503.1">User needs</span></strong><span class="koboSpan" id="kobo.504.1">: Users may have heterogeneous needs that may be conflicting, requiring us to have to implement rules </span><span class="No-Break"><span class="koboSpan" id="kobo.505.1">and constraints</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.506.1">Deployment</span></strong><span class="koboSpan" id="kobo.507.1">: The system must be accessible to users and allow </span><span class="No-Break"><span class="koboSpan" id="kobo.508.1">easy inference</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.509.1">There are different alternatives for the storage of </span><span class="No-Break"><span class="koboSpan" id="kobo.510.1">a KG:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.511.1">KGs can be hosted in classic </span><strong class="bold"><span class="koboSpan" id="kobo.512.1">relational databases</span></strong><span class="koboSpan" id="kobo.513.1"> (e.g., </span><strong class="bold"><span class="koboSpan" id="kobo.514.1">Structured Query Language</span></strong><span class="koboSpan" id="kobo.515.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.516.1">SQL</span></strong><span class="koboSpan" id="kobo.517.1">) where entities and </span><a id="_idIndexMarker823"/><span class="koboSpan" id="kobo.518.1">relationships are stored in</span><a id="_idIndexMarker824"/><span class="koboSpan" id="kobo.519.1"> tables). </span><span class="koboSpan" id="kobo.519.2">From these tables, one can then reconstruct the graph’s relational structure using projections. </span><span class="koboSpan" id="kobo.519.3">Using a relational database </span><a id="_idIndexMarker825"/><span class="koboSpan" id="kobo.520.1">to store a large KG can result in large tables that are impractical or a multitude of tables with a </span><span class="No-Break"><span class="koboSpan" id="kobo.521.1">complex hierarchy.</span></span></li>
<li><span class="koboSpan" id="kobo.522.1">An alternative is the </span><strong class="bold"><span class="koboSpan" id="kobo.523.1">document model</span></strong><span class="koboSpan" id="kobo.524.1"> where </span><a id="_idIndexMarker826"/><span class="koboSpan" id="kobo.525.1">data is stored as tuples (key-value pairs) and then organized into collections. </span><span class="koboSpan" id="kobo.525.2">This structure can be convenient for searching; it is a schematic system that allows the speed of writing, but updating knowledge in nested collections can be </span><span class="No-Break"><span class="koboSpan" id="kobo.526.1">a nightmare.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.527.1">Graph databases</span></strong><span class="koboSpan" id="kobo.528.1"> are databases</span><a id="_idIndexMarker827"/><span class="koboSpan" id="kobo.529.1"> that are optimized for </span><a id="_idIndexMarker828"/><span class="koboSpan" id="kobo.530.1">storing and searching graphs and data transformation. </span><span class="koboSpan" id="kobo.530.2">The graph data model then has nodes and edges with various metadata that are attached. </span><span class="koboSpan" id="kobo.530.3">The query language is also adapted to this structure (and is vaguely reminiscent of SQL). </span><span class="koboSpan" id="kobo.530.4">Graph databases also have the advantage of allowing heterogeneity and supporting speed. </span><span class="koboSpan" id="kobo.530.5">Neo4j is one of the most widely used and uses an adapted query </span><span class="No-Break"><span class="koboSpan" id="kobo.531.1">language (Cypher).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.532.1">Triplet stores</span></strong><span class="koboSpan" id="kobo.533.1"> are where </span><a id="_idIndexMarker829"/><span class="koboSpan" id="kobo.534.1">the database is made up directly of triplets. </span><span class="koboSpan" id="kobo.534.2">Databases exist today that save information in triplets and allow queries to be conducted in the database. </span><span class="koboSpan" id="kobo.534.3">Typically, these databases also have native support for ontologies and for conducting </span><span class="No-Break"><span class="koboSpan" id="kobo.535.1">logical reasoning.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.536.1">Hosting comes with its own set of challenges, and the choice of data model should be conducted with subsequent applications in mind. </span><span class="koboSpan" id="kobo.536.2">For example, if our KG needs to retrieve data from our relational database, using this system has advantages for integration. </span><span class="koboSpan" id="kobo.536.3">On the other hand, though, in this case, we will sacrifice performance for heterogeneity and speed. </span><span class="koboSpan" id="kobo.536.4">A graph database handles performance and the structural nature of the graph better, but it may integrate poorly with other components of the system. </span><span class="koboSpan" id="kobo.536.5">Whatever system we use for storage, we can either build hybrid systems depending on the applications or create a KG as a layer on top of </span><span class="No-Break"><span class="koboSpan" id="kobo.537.1">the database.</span></span></p>
<p><span class="koboSpan" id="kobo.538.1">Deployment is the last step in the pipeline. </span><span class="koboSpan" id="kobo.538.2">That doesn’t mean it’s the end of the story, though. </span><span class="koboSpan" id="kobo.538.3">A KG can become outdated easily, so we need to have pipelines in mind for knowledge updates or to be able to handle new applications. </span><span class="koboSpan" id="kobo.538.4">Similarly, the entry of new knowledge means that we must have pipelines for knowledge assessment (monitoring the quality of the KG, ensuring that no errors are entered or that there are no conflicts). </span><span class="koboSpan" id="kobo.538.5">Some knowledge may be outdated or need to be deleted for legal or privacy issues; therefore, we should have pipelines for cleaning the KG. </span><span class="koboSpan" id="kobo.538.6">Other pipelines should instead focus on controlling access and </span><a id="_idIndexMarker830"/><span class="koboSpan" id="kobo.539.1">security of </span><span class="No-Break"><span class="koboSpan" id="kobo.540.1">our system.</span></span></p>
<p><span class="koboSpan" id="kobo.541.1">In this section, we have seen all the steps necessary to create and deploy a KG. </span><span class="koboSpan" id="kobo.541.2">Now that we have our KG, we can use it; in the next section, we will discuss how to find information and use it as a context for </span><span class="No-Break"><span class="koboSpan" id="kobo.542.1">our LLM.</span></span></p>
<h1 id="_idParaDest-127"><a id="_idTextAnchor126"/><span class="koboSpan" id="kobo.543.1">Retrieving information with a knowledge graph and an LLM</span></h1>
<p><span class="koboSpan" id="kobo.544.1">In the previous two chapters, we discussed the capabilities of RAG and its role in reducing hallucinations generated by</span><a id="_idIndexMarker831"/><span class="koboSpan" id="kobo.545.1"> LLMs. </span><span class="koboSpan" id="kobo.545.2">Although RAG has been</span><a id="_idIndexMarker832"/><span class="koboSpan" id="kobo.546.1"> widely used in both research and industrial applications, there are </span><span class="No-Break"><span class="koboSpan" id="kobo.547.1">still limitations:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.548.1">Neglecting relationships</span></strong><span class="koboSpan" id="kobo.549.1">: The text in the databases is interconnected and not isolated. </span><span class="koboSpan" id="kobo.549.2">For example, a document is divided into chunks; since these chunks belong to a single document, there is a semantic connection between them. </span><span class="koboSpan" id="kobo.549.3">RAG fails to capture structured relational knowledge when this cannot be captured by semantic similarity. </span><span class="koboSpan" id="kobo.549.4">Some authors point out that, in science, there are important relationships between an article and previous works, and these relationships are usually highlighted with a citation network. </span><span class="koboSpan" id="kobo.549.5">Using RAG, we can find articles that are similar to the query but we cannot find this citation network, losing this </span><span class="No-Break"><span class="koboSpan" id="kobo.550.1">relational information.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.551.1">Redundant information</span></strong><span class="koboSpan" id="kobo.552.1">: The context that comes to the LLM is a series of concatenated chunks. </span><span class="koboSpan" id="kobo.552.2">Today with LLMs, we can add more and more context (the context length of models is getting longer and longer) but they struggle with the presence of redundant information. </span><span class="koboSpan" id="kobo.552.3">The more chunks we add to the context, the greater the amount of redundant or non-essential information to answer the query. </span><span class="koboSpan" id="kobo.552.4">The presence of this redundant information reduces the performance of </span><span class="No-Break"><span class="koboSpan" id="kobo.553.1">the model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.554.1">Lacking global information</span></strong><span class="koboSpan" id="kobo.555.1">: RAG finds a set of documents but fails to find global information because the set of documents is not representative of global information. </span><span class="koboSpan" id="kobo.555.2">This is a problem, especially for </span><span class="No-Break"><span class="koboSpan" id="kobo.556.1">summarization tasks.</span></span></li>
</ul>
<p><strong class="bold"><span class="koboSpan" id="kobo.557.1">Graph retrieval-augmented generation</span></strong><span class="koboSpan" id="kobo.558.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.559.1">GraphRAG</span></strong><span class="koboSpan" id="kobo.560.1">) has</span><a id="_idIndexMarker833"/><span class="koboSpan" id="kobo.561.1"> emerged as a new paradigm to try to solve these challenges. </span><span class="koboSpan" id="kobo.561.2">In traditional RAG, we find text chunks by conducting a similarity analysis on the </span><a id="_idIndexMarker834"/><span class="koboSpan" id="kobo.562.1">embedded vectors. </span><span class="koboSpan" id="kobo.562.2">In GraphRAG, we conduct the search on the knowledge graph, and</span><a id="_idIndexMarker835"/><span class="koboSpan" id="kobo.563.1"> the found triplets are provided to the context. </span><span class="koboSpan" id="kobo.563.2">So, the main difference is that upon arrival of a query from a user, we conduct the search in the KG and use the information contained in the graph to answer </span><span class="No-Break"><span class="koboSpan" id="kobo.564.1">the query.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer196">
<span class="koboSpan" id="kobo.565.1"><img alt="Figure 7.16 – Comparison between direct LLM, RAG, and GraphRAG (https://arxiv.org/pdf/2408.08921)" src="image/B21257_07_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.566.1">Figure 7.16 – Comparison between direct LLM, RAG, and GraphRAG (</span><a href="https://arxiv.org/pdf/2408.08921"><span class="koboSpan" id="kobo.567.1">https://arxiv.org/pdf/2408.08921</span></a><span class="koboSpan" id="kobo.568.1">)</span></p>
<p><span class="koboSpan" id="kobo.569.1">Formally, we can define GraphRAG as a framework that exploits a KG to provide context to an LLM and produce a better response. </span><span class="koboSpan" id="kobo.569.2">The system, therefore, is very similar to classical RAG; to avoid confusion, in this </span><a id="_idIndexMarker836"/><span class="koboSpan" id="kobo.570.1">context, we will call it </span><em class="italic"><span class="koboSpan" id="kobo.571.1">vector RAG</span></em><span class="koboSpan" id="kobo.572.1">. </span><span class="koboSpan" id="kobo.572.2">In GraphRAG, the KG is the knowledge</span><a id="_idIndexMarker837"/><span class="koboSpan" id="kobo.573.1"> base, and from this, we find information on entities and relationships. </span><span class="koboSpan" id="kobo.573.2">GraphRAG consists of three </span><span class="No-Break"><span class="koboSpan" id="kobo.574.1">main steps:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.575.1">Graph-based indexing</span></strong><span class="koboSpan" id="kobo.576.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.577.1">G-indexing</span></strong><span class="koboSpan" id="kobo.578.1">): In this initial </span><a id="_idIndexMarker838"/><span class="koboSpan" id="kobo.579.1">phase, the goal is to build a graph database and index </span><span class="No-Break"><span class="koboSpan" id="kobo.580.1">it correctly.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.581.1">Graph-guided retrieval</span></strong><span class="koboSpan" id="kobo.582.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.583.1">G-retrieval</span></strong><span class="koboSpan" id="kobo.584.1">): Once we have a KG, when a query arrives, we have to find and extract the various information needed to answer the </span><a id="_idIndexMarker839"/><span class="koboSpan" id="kobo.585.1">query. </span><span class="koboSpan" id="kobo.585.2">Given a query, </span><strong class="source-inline"><span class="koboSpan" id="kobo.586.1">q</span></strong><span class="koboSpan" id="kobo.587.1">, in natural language, we want to extract a subgraph that we can use to correctly answer </span><span class="No-Break"><span class="koboSpan" id="kobo.588.1">the query.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.589.1">Graph-enhanced generation</span></strong><span class="koboSpan" id="kobo.590.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.591.1">G-generation</span></strong><span class="koboSpan" id="kobo.592.1">): The last </span><a id="_idIndexMarker840"/><span class="koboSpan" id="kobo.593.1">step concerns using the found knowledge for generation. </span><span class="koboSpan" id="kobo.593.2">This step is conducted with an LLM that receives the context and generates </span><span class="No-Break"><span class="koboSpan" id="kobo.594.1">the answer.</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer197">
<span class="koboSpan" id="kobo.595.1"><img alt="Figure 7.17 – Overview of the GraphRAG framework for a question-answering task (https://arxiv.org/pdf/2408.08921)" src="image/B21257_07_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.596.1">Figure 7.17 – Overview of the GraphRAG framework for a question-answering task (</span><a href="https://arxiv.org/pdf/2408.08921"><span class="koboSpan" id="kobo.597.1">https://arxiv.org/pdf/2408.08921</span></a><span class="koboSpan" id="kobo.598.1">)</span></p>
<p><span class="koboSpan" id="kobo.599.1">In the following subsections, we will talk about each step </span><span class="No-Break"><span class="koboSpan" id="kobo.600.1">in detail.</span></span></p>
<h2 id="_idParaDest-128"><a id="_idTextAnchor127"/><span class="koboSpan" id="kobo.601.1">Graph-based indexing</span></h2>
<p><span class="koboSpan" id="kobo.602.1">In the first step, we need to choose what our graph data will be. </span><span class="koboSpan" id="kobo.602.2">Generally, two types of KGs are used: open KGs or self-constructed KGs. </span><span class="koboSpan" id="kobo.602.3">In the first case, we </span><a id="_idIndexMarker841"/><span class="koboSpan" id="kobo.603.1">can use a KG that is already available and adapt it to our GraphRAG. </span><span class="koboSpan" id="kobo.603.2">Today, many KGs have already been built and are available (for example, Wikidata is a knowledge base that collects data from various Wikipedia-related projects). </span><span class="koboSpan" id="kobo.603.3">Several KGs are specialized in a particular domain; these KGs have a greater understanding of a particular domain (some of these KGs are open and usable). </span><span class="koboSpan" id="kobo.603.4">Alternatively, it is possible to build your </span><span class="No-Break"><span class="koboSpan" id="kobo.604.1">own KG.</span></span></p>
<p><span class="koboSpan" id="kobo.605.1">When building it or before using it in GraphRAG, you should pay attention to indexing. </span><span class="koboSpan" id="kobo.605.2">Proper indexing allows us to have a faster and more efficient GraphRAG. </span><span class="koboSpan" id="kobo.605.3">Although we can imagine the KG visually as a graph, it is still stored in a database. </span><span class="koboSpan" id="kobo.605.4">Indexing allows us access to the information we want to find. </span><span class="koboSpan" id="kobo.605.5">Thus, there are several types of indexing. </span><span class="koboSpan" id="kobo.605.6">For example, we can have text descriptions associated with nodes, triplets, or ontologies that are then used during the search. </span><span class="koboSpan" id="kobo.605.7">Another way is to transform graph data into vectors and conduct the search on these vector spaces (embedding). </span><span class="koboSpan" id="kobo.605.8">We can also use indexing that better respects the</span><a id="_idIndexMarker842"/><span class="koboSpan" id="kobo.606.1"> graph nature of the data or </span><span class="No-Break"><span class="koboSpan" id="kobo.607.1">hybrid versions.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer198">
<span class="koboSpan" id="kobo.608.1"><img alt="Figure 7.18 – Overview of graph-based indexing (https://arxiv.org/pdf/2408.08921)" src="image/B21257_07_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.609.1">Figure 7.18 – Overview of graph-based indexing (</span><a href="https://arxiv.org/pdf/2408.08921"><span class="koboSpan" id="kobo.610.1">https://arxiv.org/pdf/2408.08921</span></a><span class="koboSpan" id="kobo.611.1">)</span></p>
<h2 id="_idParaDest-129"><a id="_idTextAnchor128"/><span class="koboSpan" id="kobo.612.1">Graph-guided retrieval</span></h2>
<p><span class="koboSpan" id="kobo.613.1">In GraphRAG, retrieval is crucial for the quality of response generation (similar to vector RAG). </span><span class="koboSpan" id="kobo.613.2">The search for a KG has two challenges that need to </span><span class="No-Break"><span class="koboSpan" id="kobo.614.1">be solved:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.615.1">Explosive candidate subgraphs</span></strong><span class="koboSpan" id="kobo.616.1">: As the graph grows, the number of subgraphs in the KG increases </span><a id="_idIndexMarker843"/><span class="koboSpan" id="kobo.617.1">exponentially. </span><span class="koboSpan" id="kobo.617.2">This means we need efficient algorithms to explore the KG and find the relevant subgraphs. </span><span class="koboSpan" id="kobo.617.3">Some of these algorithms use heuristic methods to be </span><span class="No-Break"><span class="koboSpan" id="kobo.618.1">more efficient.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.619.1">Insufficient similarity measurement</span></strong><span class="koboSpan" id="kobo.620.1">: Our query is in text form, but we want to conduct our similarity search on a graph. </span><span class="koboSpan" id="kobo.620.2">This means that our algorithm must be able to understand both textual and structural information and be able to succeed in comparing the similarity between data from </span><span class="No-Break"><span class="koboSpan" id="kobo.621.1">different sources.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.622.1">We can have different types of retrievers. </span><span class="koboSpan" id="kobo.622.2">The simplest are </span><strong class="bold"><span class="koboSpan" id="kobo.623.1">non-parametric retrievers</span></strong><span class="koboSpan" id="kobo.624.1">. </span><span class="koboSpan" id="kobo.624.2">These are based on the use of</span><a id="_idIndexMarker844"/><span class="koboSpan" id="kobo.625.1"> heuristic rules or other traditional graph search algorithms. </span><span class="koboSpan" id="kobo.625.2">For example, given a query, we search for entities present and take </span><em class="italic"><span class="koboSpan" id="kobo.626.1">k</span></em><span class="koboSpan" id="kobo.627.1">-hop paths (in simple words, for the query </span><em class="italic"><span class="koboSpan" id="kobo.628.1">Where is Obama born?</span></em><span class="koboSpan" id="kobo.629.1">, starting from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.630.1">Obama</span></strong><span class="koboSpan" id="kobo.631.1"> entity, in KG, we take neighboring entities, </span><strong class="source-inline"><span class="koboSpan" id="kobo.632.1">k-hop=1</span></strong><span class="koboSpan" id="kobo.633.1">, or even neighbors of neighbors, </span><strong class="source-inline"><span class="koboSpan" id="kobo.634.1">k-hop=2</span></strong><span class="koboSpan" id="kobo.635.1">, and so on). </span><span class="koboSpan" id="kobo.635.2">Non-parametric retrievers are the simplest and also the fastest systems, but they suffer from inaccurate retrieval (they can be improved by learning). </span><span class="koboSpan" id="kobo.635.3">There are machine and deep learning models that are natively trained on graphs. </span><span class="koboSpan" id="kobo.635.4">GNN-based retrievers are one example. </span><strong class="bold"><span class="koboSpan" id="kobo.636.1">Graph neural networks</span></strong><span class="koboSpan" id="kobo.637.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.638.1">GNNs</span></strong><span class="koboSpan" id="kobo.639.1">) are neural </span><a id="_idIndexMarker845"/><span class="koboSpan" id="kobo.640.1">networks that natively handle graphs and can be used for many tasks on graphs (node classification, edge prediction, and so on) so they search the graph for subgraphs similar to </span><span class="No-Break"><span class="koboSpan" id="kobo.641.1">the query.</span></span></p>
<p><span class="koboSpan" id="kobo.642.1">Alternatively, we can use an LLM-based retriever where we have a transformer-based model that conducts the search. </span><span class="koboSpan" id="kobo.642.2">The model then processes and interprets the query to conduct the search. </span><span class="koboSpan" id="kobo.642.3">Several of these LLMs are models that have been trained on the text, and then fine-tuning is conducted to search the graphs. </span><span class="koboSpan" id="kobo.642.4">One advantage is that an LLM can be used as an agent and use different tools or functions to search the graph. </span><span class="koboSpan" id="kobo.642.5">Both LLM-based retrievers and GNN-based retrievers significantly improve retrieval accuracy but at a high computational cost. </span><span class="koboSpan" id="kobo.642.6">There are also alternatives today that use different methods (conduct with both a GNN and LLM, or use heuristic methods together with an LLM) or the process can be multistage (e.g., conduct an initial search with an LLM and then refine </span><span class="No-Break"><span class="koboSpan" id="kobo.643.1">the results).</span></span></p>
<p><span class="koboSpan" id="kobo.644.1">As was done for the vector RAG, we can add additional components to conduct enhancement. </span><span class="koboSpan" id="kobo.644.2">For example, in the previous chapter, we saw that we can rewrite a query or decompose queries that are too complex. </span><span class="koboSpan" id="kobo.644.3">Query modification helps to better capture the meaning of the query (because sometimes the query does not capture the implicit meaning intended by the user). </span><span class="koboSpan" id="kobo.644.4">Retrieval can also be a flexible process. </span><span class="koboSpan" id="kobo.644.5">In the previous chapter, we saw that in naïve RAG, retrieval was conducted only once, but then variations in advanced and modular RAG were established where retrieval can be multistage or iterative. </span><span class="koboSpan" id="kobo.644.6">Even more sophisticated variations make the process adaptive depending on the query, so for simpler queries, only one retrieval is conducted, and for more complex queries, multiple iterations may be conducted. </span><span class="koboSpan" id="kobo.644.7">Similarly, the</span><a id="_idIndexMarker846"/><span class="koboSpan" id="kobo.645.1"> results obtained after retrieval can also be modified. </span><span class="koboSpan" id="kobo.645.2">For example, even with GraphRAG, we can conduct a compression of the retrieved knowledge. </span><span class="koboSpan" id="kobo.645.3">In fact, we may also find redundant information if we conduct multiple retrieval stages and thus it is convenient to filter out </span><span class="No-Break"><span class="koboSpan" id="kobo.646.1">irrelevant information.</span></span></p>
<p><span class="koboSpan" id="kobo.647.1">Today, there are also reranking approaches to reorder the retrieved results with GraphRAG. </span><span class="koboSpan" id="kobo.647.2">One example is to reorder the various subgraphs found and perhaps choose the top </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.648.1">k</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.649.1"> subgraphs.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer199">
<span class="koboSpan" id="kobo.650.1"><img alt="Figure 7.19 – General architectures of graph-based retrieval (https://arxiv.org/pdf/2408.08921)" src="image/B21257_07_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.651.1">Figure 7.19 – General architectures of graph-based retrieval (</span><a href="https://arxiv.org/pdf/2408.08921"><span class="koboSpan" id="kobo.652.1">https://arxiv.org/pdf/2408.08921</span></a><span class="koboSpan" id="kobo.653.1">)</span></p>
<p><span class="koboSpan" id="kobo.654.1">Another difference with vector RAG is how we control search granularity. </span><span class="koboSpan" id="kobo.654.2">In vector RAG, granularity is controlled by deciding the size of the chunks. </span><span class="koboSpan" id="kobo.654.3">In the case of GraphRAG, we do not conduct chunking or find chunks. </span><span class="koboSpan" id="kobo.654.4">We can, however, control granularity during retrieval by choosing what </span><span class="No-Break"><span class="koboSpan" id="kobo.655.1">we find:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.656.1">Nodes</span></strong><span class="koboSpan" id="kobo.657.1">: In GraphRAG, we can </span><a id="_idIndexMarker847"/><span class="koboSpan" id="kobo.658.1">retrieve individual entities. </span><span class="koboSpan" id="kobo.658.2">Nodes can have properties associated with them and then only add entities and their properties to the context. </span><span class="koboSpan" id="kobo.658.3">This can be useful for </span><span class="No-Break"><span class="koboSpan" id="kobo.659.1">target queries.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.660.1">Triplets</span></strong><span class="koboSpan" id="kobo.661.1">: By </span><a id="_idIndexMarker848"/><span class="koboSpan" id="kobo.662.1">expanding the search granularity, we choose to retrieve triplets (so not only nodes, but also their relationships). </span><span class="koboSpan" id="kobo.662.2">This is useful when we are interested not only in the entity itself but also in </span><span class="No-Break"><span class="koboSpan" id="kobo.663.1">their relationships.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.664.1">Paths</span></strong><span class="koboSpan" id="kobo.665.1">: In this case, we</span><a id="_idIndexMarker849"/><span class="koboSpan" id="kobo.666.1"> still </span><a id="_idIndexMarker850"/><span class="koboSpan" id="kobo.667.1">expand the retrieval. </span><span class="koboSpan" id="kobo.667.2">A path is a chain of nodes and relationships, so starting from entity </span><em class="italic"><span class="koboSpan" id="kobo.668.1">X</span></em><span class="koboSpan" id="kobo.669.1"> and arriving at entity </span><em class="italic"><span class="koboSpan" id="kobo.670.1">Y</span></em><span class="koboSpan" id="kobo.671.1">, the path is all the chain of entities and relationships that connect them. </span><span class="koboSpan" id="kobo.671.2">Obviously, there are multiple paths between different entities, and these grow exponentially as the size of the graph increases. </span><span class="koboSpan" id="kobo.671.3">So, we generally define rules, use GNNs, or choose the </span><span class="No-Break"><span class="koboSpan" id="kobo.672.1">shortest path.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.673.1">Subgraphs</span></strong><span class="koboSpan" id="kobo.674.1">: A subgraph</span><a id="_idIndexMarker851"/><span class="koboSpan" id="kobo.675.1"> can be defined as a subset of nodes and relationships internal to the KG. </span><span class="koboSpan" id="kobo.675.2">Extracting a subgraph allows us to answer complex queries because it allows us to analyze complex patterns and dependencies between entities. </span><span class="koboSpan" id="kobo.675.3">There are several ways to extract a subgraph: we can use specific patterns or conduct a merge of </span><span class="No-Break"><span class="koboSpan" id="kobo.676.1">different paths.</span></span></li>
<li> <strong class="bold"><span class="koboSpan" id="kobo.677.1">Hybrid granularities</span></strong><span class="koboSpan" id="kobo.678.1">: We </span><a id="_idIndexMarker852"/><span class="koboSpan" id="kobo.679.1">can use different granularities at the same time or choose an </span><span class="No-Break"><span class="koboSpan" id="kobo.680.1">adaptive system.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.681.1">In the case of GraphRAG, balancing granularity and efficiency is important. </span><span class="koboSpan" id="kobo.681.2">We do not want to saturate the context with elements to prevent later LLM struggles with irrelevant information. </span><span class="koboSpan" id="kobo.681.3">It also depends on the complexity of the query: for simple queries, even low granularity is enough, whereas complex queries benefit from higher granularity. </span><span class="koboSpan" id="kobo.681.4">An adaptive approach can make the system more efficient while maintaining nuances </span><span class="No-Break"><span class="koboSpan" id="kobo.682.1">when needed.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer200">
<span class="koboSpan" id="kobo.683.1"><img alt="Figure 7.20 – Different levels of retrieval granularity" src="image/B21257_07_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.684.1">Figure 7.20 – Different levels of retrieval granularity</span></p>
<p><span class="koboSpan" id="kobo.685.1">Once the knowledge is found and cleaned up, we can provide it to the LLM to generate a response to the query. </span><span class="koboSpan" id="kobo.685.2">This knowledge enters the prompt provided to the LLM and the model generates a response. </span><span class="koboSpan" id="kobo.685.3">Alternatively, this found knowledge can be used for certain types of models that are used for some specific tasks (e.g., a GNN to answer multiple-choice questions). </span><span class="koboSpan" id="kobo.685.4">The main problem is that the graph has a non-Euclidean nature and integration with textual information is not </span><a id="_idIndexMarker853"/><span class="koboSpan" id="kobo.686.1">optimal. </span><span class="koboSpan" id="kobo.686.2">For this, graph translators that convert the found graph information into more digestible information for an LLM can be used. </span><span class="koboSpan" id="kobo.686.3">This conversion increases the LLM’s ability to understand the information. </span><span class="koboSpan" id="kobo.686.4">So, once we find the information in graph form (nodes, relationships, path, or subgraph), we put it in the context of the LLM. </span><span class="koboSpan" id="kobo.686.5">There are </span><span class="No-Break"><span class="koboSpan" id="kobo.687.1">some alternatives:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.688.1">Graph formats</span></strong><span class="koboSpan" id="kobo.689.1">: We can directly add the set of relationships and nodes in the prompt, or we can use a form of graph structure representation such as adjacency or edge tables. </span><span class="koboSpan" id="kobo.689.2">The</span><a id="_idIndexMarker854"/><span class="koboSpan" id="kobo.690.1"> latter compactly conveys a better relational structure. </span><span class="koboSpan" id="kobo.690.2">Another idea is a node sequence, which is generated according to a predeterminate rule. </span><span class="koboSpan" id="kobo.690.3">This is a compact representation that contains the order of the nodes in </span><span class="No-Break"><span class="koboSpan" id="kobo.691.1">the graph.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.692.1">Natural language</span></strong><span class="koboSpan" id="kobo.693.1">: There are specific graph languages that can be used to transform information into </span><a id="_idIndexMarker855"/><span class="koboSpan" id="kobo.694.1">natural language, a representation that is more congenial to LLM. </span><span class="koboSpan" id="kobo.694.2">In this process, we convert the found subgraph into a descriptive form. </span><span class="koboSpan" id="kobo.694.3">Templates can be used to transform the graph where it is filled with nodes and relationships. </span><span class="koboSpan" id="kobo.694.4">In some templates, you can define which are the nearest neighbors of a node and which are the most distant (1-hop and 2-hop in the graph), or you can use an LLM to transform this subgraph into a natural </span><span class="No-Break"><span class="koboSpan" id="kobo.695.1">language description.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.696.1">Syntax tree</span></strong><span class="koboSpan" id="kobo.697.1">: The graph is flattened and represented as a syntax tree. </span><span class="koboSpan" id="kobo.697.2">These trees have the advantage of </span><a id="_idIndexMarker856"/><span class="koboSpan" id="kobo.698.1">a hierarchical structure and maintain the topological order of the graph. </span><span class="koboSpan" id="kobo.698.2">This approach maintains the properties of the graph but makes the information more digestible for </span><span class="No-Break"><span class="koboSpan" id="kobo.699.1">the LLM.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.700.1">Code-like forms</span></strong><span class="koboSpan" id="kobo.701.1">: Retrieved graphs </span><a id="_idIndexMarker857"/><span class="koboSpan" id="kobo.702.1">can be converted into a standard format such as </span><strong class="bold"><span class="koboSpan" id="kobo.703.1">Graph Markup Language</span></strong><span class="koboSpan" id="kobo.704.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.705.1">GraphML</span></strong><span class="koboSpan" id="kobo.706.1">). </span><span class="koboSpan" id="kobo.706.2">These languages are specifically designed for graphs but are </span><a id="_idIndexMarker858"/><span class="koboSpan" id="kobo.707.1">a structural and </span><span class="No-Break"><span class="koboSpan" id="kobo.708.1">textual hybrid.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.709.1">Conversion still presents difficulties because it must ensure that the result is concise and complete but, at the same time, understandable by the LLM. </span><span class="koboSpan" id="kobo.709.2">Optimally, this representation should also include the structural information of the graph. </span><span class="koboSpan" id="kobo.709.3">Whether</span><a id="_idIndexMarker859"/><span class="koboSpan" id="kobo.710.1"> the retrieved subgraph is converted or not, the result is entered into the LLM prompt and a response </span><span class="No-Break"><span class="koboSpan" id="kobo.711.1">is generated.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer201">
<span class="koboSpan" id="kobo.712.1"><img alt="Figure 7.21 – Subgraph transformation to enhance generation (https://arxiv.org/pdf/2408.08921)" src="image/B21257_07_21.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.713.1">Figure 7.21 – Subgraph transformation to enhance generation (</span><a href="https://arxiv.org/pdf/2408.08921"><span class="koboSpan" id="kobo.714.1">https://arxiv.org/pdf/2408.08921</span></a><span class="koboSpan" id="kobo.715.1">)</span></p>
<h2 id="_idParaDest-130"><a id="_idTextAnchor129"/><span class="koboSpan" id="kobo.716.1">GraphRAG applications</span></h2>
<p><span class="koboSpan" id="kobo.717.1">GraphRAG has several applications. </span><span class="koboSpan" id="kobo.717.2">The first is question answering (so the same application as RAG) where we extract the subgraphs and the LLM uses them for subsequent reasoning and answering. </span><span class="koboSpan" id="kobo.717.3">A </span><a id="_idIndexMarker860"/><span class="koboSpan" id="kobo.718.1">sub-branch of question answering is commonsense reasoning question answering where it often takes the format of multiple choice questions. </span><span class="koboSpan" id="kobo.718.2">For this subtask, we often do not use an LLM but a GNN or other machine learning (ML) model instead. </span><span class="koboSpan" id="kobo.718.3">However, KGs (and therefore also GraphRAG) have an extensive application for information retrieval, for example, if we want to investigate the relationships between some entities of interest. </span><span class="koboSpan" id="kobo.718.4">A KG can be used by itself to extract relationships between entities, but the addition of generation with an LLM allows us to explore these relationships and contextual nuances better. </span><span class="koboSpan" id="kobo.718.5">This is an attractive factor for academic and literature research. </span><span class="koboSpan" id="kobo.718.6">In fact, in academia, an article is authored by multiple authors who are part of different institutions. </span><span class="koboSpan" id="kobo.718.7">An article builds on previous research, and so for each article, there is a network of citations. </span><span class="koboSpan" id="kobo.718.8">These structural elements are easily modeled for a graph. </span><span class="koboSpan" id="kobo.718.9">An interesting application recently published shows how Ghafarollahi et al. </span><span class="koboSpan" id="kobo.718.10">used multiple KG agents to analyze published literature and propose </span><a id="_idIndexMarker861"/><span class="koboSpan" id="kobo.719.1">new research hypotheses. </span><span class="koboSpan" id="kobo.719.2">In short, they extracted paths or subgraphs from a KG (constructed from 1,000 articles), then an agent analyzed the ontologies, and then a new research hypothesis was generated from this. </span><span class="koboSpan" id="kobo.719.3">In this interesting application, a number of agents collaborate to create new potential searches for </span><span class="No-Break"><span class="koboSpan" id="kobo.720.1">new materials.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer202">
<span class="koboSpan" id="kobo.721.1"><img alt="Figure 7.22 – Overview of the multi-agent graph-reasoning system for scientific discovery assistance (https://arxiv.org/pdf/2409.05556v1)" src="image/B21257_07_22.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.722.1">Figure 7.22 – Overview of the multi-agent graph-reasoning system for scientific discovery assistance (</span><a href="https://arxiv.org/pdf/2409.05556v1"><span class="koboSpan" id="kobo.723.1">https://arxiv.org/pdf/2409.05556v1</span></a><span class="koboSpan" id="kobo.724.1">)</span></p>
<p><span class="koboSpan" id="kobo.725.1">One of the reasons for interest in GraphRAG is that KGs used to be used for fact verification (after all, a KG is a collection of facts) so providing facts to an LLM should reduce LLM hallucinations. </span><span class="koboSpan" id="kobo.725.2">This </span><a id="_idIndexMarker862"/><span class="koboSpan" id="kobo.726.1">aspect makes it particularly attractive for biomedical applications. </span><span class="koboSpan" id="kobo.726.2">In fact, hallucinations are a serious problem for medical decision-making applications. </span><span class="koboSpan" id="kobo.726.3">In medicine, if the vector RAG can reduce hallucinations, it does not allow for a holistic view, especially when an overview is needed to answer a question. </span><span class="koboSpan" id="kobo.726.4">Therefore, Wu et al. </span><span class="koboSpan" id="kobo.726.5">suggest using a GraphRAG-based approach called </span><strong class="bold"><span class="koboSpan" id="kobo.727.1">MedGraphRAG</span></strong><span class="koboSpan" id="kobo.728.1">. </span><span class="koboSpan" id="kobo.728.2">In this work, they use several medical sources to create their system and take advantage of the hierarchical </span><a id="_idIndexMarker863"/><span class="koboSpan" id="kobo.729.1">nature of KGs. </span><span class="koboSpan" id="kobo.729.2">They construct three levels for their KG. </span><span class="koboSpan" id="kobo.729.3">At the first level, there are user-provided documents (medical reports from a hospital). </span><span class="koboSpan" id="kobo.729.4">Entities at this level are then connected to a more foundational level of commonly accepted information. </span><span class="koboSpan" id="kobo.729.5">The second level is constructed from medical textbooks and scientific articles. </span><span class="koboSpan" id="kobo.729.6">Finally, at the third level, there are well-defined medical terms and knowledge relationships that have been obtained from standardized and reliable sources. </span><span class="koboSpan" id="kobo.729.7">Leveraging retrieval from this KG obtains state-of-the-art results on major medical question-answering benchmark datasets. </span><span class="koboSpan" id="kobo.729.8">The advantage is that this system also</span><a id="_idIndexMarker864"/><span class="koboSpan" id="kobo.730.1"> outperforms models that are fine-tuned on medical knowledge, thus with large </span><span class="No-Break"><span class="koboSpan" id="kobo.731.1">computational savings.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer203">
<span class="koboSpan" id="kobo.732.1"><img alt="Figure 7.23 – MedGraphRAG framework (https://arxiv.org/pdf/2408.04187)" src="image/B21257_07_23.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.733.1">Figure 7.23 – MedGraphRAG framework (</span><a href="https://arxiv.org/pdf/2408.04187"><span class="koboSpan" id="kobo.734.1">https://arxiv.org/pdf/2408.04187</span></a><span class="koboSpan" id="kobo.735.1">)</span></p>
<p><span class="koboSpan" id="kobo.736.1">A further interest in GraphRAG comes from the use of KGs to propose recommendations to users. </span><span class="koboSpan" id="kobo.736.2">In e-commerce platforms, recommendation systems are used to predict the future purchasing intentions of users and suggest other products of interest. </span><span class="koboSpan" id="kobo.736.3">Thus, it was proposed that the system matches a new user to subgraphs derived from past users with similar behavior, and leverages these to predict likely future purchases and suggest appropriate products. </span><span class="koboSpan" id="kobo.736.4">In addition, this approach can also be useful for the legal and financial fields. </span><span class="koboSpan" id="kobo.736.5">In the legal field, there are extensive citations between cases and judicial opinions, and judges use past cases and opinions to make new decisions. </span><span class="koboSpan" id="kobo.736.6">Given a legal case, GraphRAG could suggest previous legal cases and help in decision-making. </span><span class="koboSpan" id="kobo.736.7">In finance, GraphRAG might suggest previous financial transactions or </span><span class="No-Break"><span class="koboSpan" id="kobo.737.1">customer cases.</span></span></p>
<p><span class="koboSpan" id="kobo.738.1">Finally, up to this point, we have suggested that GraphRAG and vector RAG are antagonistic. </span><span class="koboSpan" id="kobo.738.2">Actually, both systems have advantages and disadvantages so it would be more useful to use them in synergy. </span><span class="koboSpan" id="kobo.738.3">Sarmah et al., proposed </span><strong class="bold"><span class="koboSpan" id="kobo.739.1">HybridRAG</span></strong><span class="koboSpan" id="kobo.740.1">, where both GraphRAG and vector RAG are used in one system. </span><span class="koboSpan" id="kobo.740.2">Their </span><a id="_idIndexMarker865"/><span class="koboSpan" id="kobo.741.1">system shows advantages in financial responses. </span><span class="koboSpan" id="kobo.741.2">In the future, there may be systems that exploit both one and the other approach with the addition of a router that can choose whether to search for the KG </span><a id="_idIndexMarker866"/><span class="koboSpan" id="kobo.742.1">or the vector database. </span><span class="koboSpan" id="kobo.742.2">Alternatively, there could be more complex systems for knowledge fusion in context (especially if KG search and chunks provide some </span><span class="No-Break"><span class="koboSpan" id="kobo.743.1">redundant information).</span></span></p>
<p><span class="koboSpan" id="kobo.744.1">In this section, we discussed how an LLM can be connected to a KG, and how it can be used to find information that enriches the context of the LLM. </span><span class="koboSpan" id="kobo.744.2">In the next section, we will discuss other tasks for which the synergy of LLMs and KGs </span><span class="No-Break"><span class="koboSpan" id="kobo.745.1">is useful.</span></span></p>
<h1 id="_idParaDest-131"><a id="_idTextAnchor130"/><span class="koboSpan" id="kobo.746.1">Understanding graph reasoning</span></h1>
<p><span class="koboSpan" id="kobo.747.1">This section is devoted to a discussion of </span><a id="_idIndexMarker867"/><span class="koboSpan" id="kobo.748.1">how to solve graph data tasks. </span><span class="koboSpan" id="kobo.748.2">In this section, we will discuss some of the approaches used to solve tasks on knowledge graphs: KG embeddings, GNNs, and LLMs. </span><span class="koboSpan" id="kobo.748.3">KG embeddings and GNNs would require at least one chapter each; hence, these topics are outside the scope of the book, but we believe that an introduction to them would be beneficial to a practitioner. </span><span class="koboSpan" id="kobo.748.4">In fact, both embedding and GNNs can be used synergistically with LLMs </span><span class="No-Break"><span class="koboSpan" id="kobo.749.1">and agents.</span></span></p>
<p><span class="koboSpan" id="kobo.750.1">There are many tasks in which a model is required to understand the structure to solve, and these are collectively called </span><strong class="bold"><span class="koboSpan" id="kobo.751.1">graph structure understanding tasks</span></strong><span class="koboSpan" id="kobo.752.1">. </span><span class="koboSpan" id="kobo.752.2">Many of these tasks are solved using algorithms or models </span><a id="_idIndexMarker868"/><span class="koboSpan" id="kobo.753.1">designed specifically to learn these tasks. </span><span class="koboSpan" id="kobo.753.2">Today, a new paradigm is being developed in which we try to use LLMs to solve these tasks; we will discuss this in depth at the end of this section. </span><span class="koboSpan" id="kobo.753.3">Examples of tasks might be degree calculation (how many neighbors a node has), path search (defining a path between two nodes, calculating which is the minimum path, and so on), Hamilton path (identifying a path that visits each node only once), topological sorting (identifying whether nodes can be visited in topological order), and many others. </span><span class="koboSpan" id="kobo.753.4">Some of these tasks are simple (degree calculation and path search) but others are much more complex (topological sorting and </span><span class="No-Break"><span class="koboSpan" id="kobo.754.1">Hamilton path).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer204">
<span class="koboSpan" id="kobo.755.1"><img alt="Figure 7.24 – Graph structure understanding tasks (https://arxiv.org/pdf/2404.14809)" src="image/B21257_07_24.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.756.1">Figure 7.24 – Graph structure understanding tasks (</span><a href="https://arxiv.org/pdf/2404.14809"><span class="koboSpan" id="kobo.757.1">https://arxiv.org/pdf/2404.14809</span></a><span class="koboSpan" id="kobo.758.1">)</span></p>
<p><span class="koboSpan" id="kobo.759.1">Graph learning tasks, on the other hand, require the model to include not only the structure of the graph but also the attributes of the graph (features of nodes, edges, and the graph), thus understanding</span><a id="_idIndexMarker869"/><span class="koboSpan" id="kobo.760.1"> the semantic information of graphs. </span><span class="koboSpan" id="kobo.760.2">Examples of some of the tasks are node classification (classify the node according to its attributes and according to the attributes of its neighbors), graph classification (you have to understand the whole graph to classify it), edge classification, and node feature explanation (explain a feature of the node). </span><strong class="bold"><span class="koboSpan" id="kobo.761.1">Knowledge graph question answering</span></strong><span class="koboSpan" id="kobo.762.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.763.1">KGQA</span></strong><span class="koboSpan" id="kobo.764.1">) is a task that falls into</span><a id="_idIndexMarker870"/><span class="koboSpan" id="kobo.765.1"> this group, as we need to understand both the structure and meaning of entities and relationships to answer questions. </span><span class="koboSpan" id="kobo.765.2">A similar task is conducting KG queries to generate text (this can also be seen as a subtask). </span><span class="koboSpan" id="kobo.765.3">KG embeddings capture multi-relational semantics and latent patterns in the graph, making them particularly useful for relational reasoning and symbolic link prediction tasks (KG link prediction, for example). </span><span class="koboSpan" id="kobo.765.4">GNNs, on the other hand, capture</span><a id="_idIndexMarker871"/><span class="koboSpan" id="kobo.766.1"> graph structure and node/edge features; these make them perform well for tasks that require inductive reasoning, use of features, or local/global representation of graph structure (node or </span><span class="No-Break"><span class="koboSpan" id="kobo.767.1">graph classification/regression).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer205">
<span class="koboSpan" id="kobo.768.1"><img alt="Figure 7.25 – Graph learning tasks (https://arxiv.org/pdf/2404.14809)" src="image/B21257_07_25.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.769.1">Figure 7.25 – Graph learning tasks (</span><a href="https://arxiv.org/pdf/2404.14809"><span class="koboSpan" id="kobo.770.1">https://arxiv.org/pdf/2404.14809</span></a><span class="koboSpan" id="kobo.771.1">)</span></p>
<h2 id="_idParaDest-132"><a id="_idTextAnchor131"/><span class="koboSpan" id="kobo.772.1">Knowledge graph embeddings</span></h2>
<p><span class="koboSpan" id="kobo.773.1">KGs are effective in representing knowledge, but </span><a id="_idIndexMarker872"/><span class="koboSpan" id="kobo.774.1">they are complex to manipulate at scale. </span><span class="koboSpan" id="kobo.774.2">This complicates their use when we are interested in particular tasks such as</span><a id="_idIndexMarker873"/><span class="koboSpan" id="kobo.775.1"> link prediction and entity classification. </span><span class="koboSpan" id="kobo.775.2">Therefore, </span><strong class="bold"><span class="koboSpan" id="kobo.776.1">knowledge graph embedding</span></strong><span class="koboSpan" id="kobo.777.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.778.1">KGE</span></strong><span class="koboSpan" id="kobo.779.1">) was proposed to be able to simplify these tasks. </span><span class="koboSpan" id="kobo.779.2">We have already discussed the concept of embedding in the first chapter. </span><span class="koboSpan" id="kobo.779.3">An embedding is a projection of data into a low-dimensional and continuous vector space, which is especially useful when our data has a sparse representation (such as text and graphs). </span><span class="koboSpan" id="kobo.779.4">For a KG, an embedding is the projection of the graph (nodes, edges, and their feature vectors) in this reduced space. </span><span class="koboSpan" id="kobo.779.5">A KGE model then tries to learn a projection that preserves both structure and information, so that it can then be used for </span><span class="No-Break"><span class="koboSpan" id="kobo.780.1">downstream tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.781.1">Learning this representation is not an easy task, and several types of algorithms have been proposed. </span><span class="koboSpan" id="kobo.781.2">For example, some KGEs try to preserve relational patterns between entities. </span><span class="koboSpan" id="kobo.781.3">TransE is an approach that embeds KGs in Euclidean space where the relationships between entities are vectors. </span><span class="koboSpan" id="kobo.781.4">TransE is based on the idea that two entities connected in the triplet should be as close as possible in space. </span><span class="koboSpan" id="kobo.781.5">Furthermore, from a triplet </span><strong class="source-inline"><span class="koboSpan" id="kobo.782.1">(h, r, t)</span></strong><span class="koboSpan" id="kobo.783.1">, it tries to learn a space where </span><em class="italic"><span class="koboSpan" id="kobo.784.1">h + r ≈ t</span></em><span class="koboSpan" id="kobo.785.1">, thus allowing us to do various mathematical operations. </span><span class="koboSpan" id="kobo.785.2">RotatE, another approach, also tries to preserve other relational patterns (symmetry, antisymmetry, inversion, and composition) using a complex vector space. </span><span class="koboSpan" id="kobo.785.3">This is quite useful when we want to answer questions that require this notion of symmetry (</span><em class="italic"><span class="koboSpan" id="kobo.786.1">marriage</span></em><span class="koboSpan" id="kobo.787.1"> is symmetric) or composition (</span><em class="italic"><span class="koboSpan" id="kobo.788.1">my nephew is my brother’s son</span></em><span class="koboSpan" id="kobo.789.1">). </span><span class="koboSpan" id="kobo.789.2">Other methods, however, try to preserve structural patterns. </span><span class="koboSpan" id="kobo.789.3">In fact, larger KGs contain complex and compound </span><a id="_idIndexMarker874"/><span class="koboSpan" id="kobo.790.1">structures that are lost during the embedding process. </span><span class="koboSpan" id="kobo.790.2">For example, hierarchical, chain structure, and ring structure are lost during classical embeddings. </span><span class="koboSpan" id="kobo.790.3">These structures are important when we want to conduct reasoning or extract</span><a id="_idIndexMarker875"/><span class="koboSpan" id="kobo.791.1"> subgraphs for some tasks. </span><span class="koboSpan" id="kobo.791.2">ATTH (another KG embedding method) uses hyperbolic space to preserve hierarchical structures and logical patterns at the same time. </span><span class="koboSpan" id="kobo.791.3">Other methods, however, try to model the uncertainties of entities and relationships, making tasks such as link </span><span class="No-Break"><span class="koboSpan" id="kobo.792.1">prediction easier.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer206">
<span class="koboSpan" id="kobo.793.1"><img alt="Figure 7.26 – Illustration of three typical structures in KGs (https://arxiv.org/pdf/2211.03536)" src="image/B21257_07_26.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.794.1">Figure 7.26 – Illustration of three typical structures in KGs (</span><a href="https://arxiv.org/pdf/2211.03536"><span class="koboSpan" id="kobo.795.1">https://arxiv.org/pdf/2211.03536</span></a><span class="koboSpan" id="kobo.796.1">)</span></p>
<p><span class="koboSpan" id="kobo.797.1">KGEs have been used extensively for several tasks such as link prediction. </span><span class="koboSpan" id="kobo.797.2">In this case, exploiting the small space is used to try to identify the most likely missing links. </span><span class="koboSpan" id="kobo.797.3">Similarly, continuous space allows models to be used to conduct triple classification. </span><span class="koboSpan" id="kobo.797.4">A further application is to use learned embeddings to </span><span class="No-Break"><span class="koboSpan" id="kobo.798.1">recommender systems.</span></span></p>
<h2 id="_idParaDest-133"><a id="_idTextAnchor132"/><span class="koboSpan" id="kobo.799.1">Graph neural networks</span></h2>
<p><span class="koboSpan" id="kobo.800.1">There are several challenges in using graphs natively with ML algorithms. </span><span class="koboSpan" id="kobo.800.2">First, classical ML models take data that is in a rectangular </span><a id="_idIndexMarker876"/><span class="koboSpan" id="kobo.801.1">or grid-like form, making it non-intuitive how to apply it to graphs. </span><span class="koboSpan" id="kobo.801.2">In addition, for a graph, there are several pieces of information that we want to use to solve tasks: nodes, edges, global context, and connectivity. </span><span class="koboSpan" id="kobo.801.3">The last one is particularly difficult to represent, and we usually use </span><a id="_idIndexMarker877"/><span class="koboSpan" id="kobo.802.1">an adjacency matrix. </span><span class="koboSpan" id="kobo.802.2">This representation is sparse, grows largely with the number of nodes in the graph, and thus is space inefficient. </span><span class="koboSpan" id="kobo.802.3">Also, since there is no order in the graph, we can get several adjacency matrices that convey the same information but may not be recognized by </span><span class="No-Break"><span class="koboSpan" id="kobo.803.1">a model.</span></span></p>
<p><span class="koboSpan" id="kobo.804.1">A GNN is a deep learning model that natively takes a graph and also exploits its structure during its learning process. </span><span class="koboSpan" id="kobo.804.2">There are different types of GNNs (in the </span><em class="italic"><span class="koboSpan" id="kobo.805.1">Further reading</span></em><span class="koboSpan" id="kobo.806.1"> section, there are some reviews so you can go into more detail on this topic) but here we will focus on the main </span><a id="_idIndexMarker878"/><span class="koboSpan" id="kobo.807.1">framework: message passing. </span><span class="koboSpan" id="kobo.807.2">Most GNNs can be seen as graph convolution networks in which we aggregate for each node the information coming from its neighbors. </span><span class="koboSpan" id="kobo.807.3">One of the advantages of a GNN is that at</span><a id="_idIndexMarker879"/><span class="koboSpan" id="kobo.808.1"> the same time as it is being trained for a task, it learns an embedding for each node. </span><span class="koboSpan" id="kobo.808.2">At each step, this node embedding is updated with information from </span><span class="No-Break"><span class="koboSpan" id="kobo.809.1">its neighbors.</span></span></p>
<p><span class="koboSpan" id="kobo.810.1">The message-passing framework is in a sense very similar to a neural network, as we saw earlier. </span><span class="koboSpan" id="kobo.810.2">In this case, there are two main steps: gather the embeddings of the various neighboring nodes and then follow by an aggregation function (which can be different depending on various architectures) and a nonlinearity. </span><span class="koboSpan" id="kobo.810.3">Then, at each step, we conduct an update of the embedding of each node, learning a new representation of the graph. </span><span class="koboSpan" id="kobo.810.4">A classical GNN can be composed of a series of GNN blocks and then a final layer that exploits the learned representation to accomplish a task. </span><span class="koboSpan" id="kobo.810.5">This can be written in a formula </span><span class="No-Break"><span class="koboSpan" id="kobo.811.1">like this:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><mml:math display="block"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>∙</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo><mml:mo>⋃</mml:mo><mml:mo>{</mml:mo><mml:mi>v</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></span></p>
<p><span class="koboSpan" id="kobo.812.1">Here, at layer </span><em class="italic"><span class="koboSpan" id="kobo.813.1">l+1</span></em><span class="koboSpan" id="kobo.814.1">, we learn a representation, </span><em class="italic"><span class="koboSpan" id="kobo.815.1">h</span></em><span class="koboSpan" id="kobo.816.1">, based on the previous embedding. </span><em class="italic"><span class="koboSpan" id="kobo.817.1">W</span></em><span class="koboSpan" id="kobo.818.1"> is a layer-specific weight matrix, </span><em class="italic"><span class="koboSpan" id="kobo.819.1">v</span></em><span class="koboSpan" id="kobo.820.1"> is a node, </span><em class="italic"><span class="koboSpan" id="kobo.821.1">w</span></em><span class="koboSpan" id="kobo.822.1"> is the set of neighbors, and </span><em class="italic"><span class="koboSpan" id="kobo.823.1">c</span></em><span class="koboSpan" id="kobo.824.1"> is the </span><span class="No-Break"><span class="koboSpan" id="kobo.825.1">normalization coefficient.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer207">
<span class="koboSpan" id="kobo.826.1"><img alt="Figure 7.27 – GNNs" src="image/B21257_07_27.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.827.1">Figure 7.27 – GNNs</span></p>
<p><span class="koboSpan" id="kobo.828.1">In this case, we are assuming that each neighbor’s contribution is the same. </span><span class="koboSpan" id="kobo.828.2">This may not be the case, so inspired by the attention mechanism of RNNs and transformers, </span><strong class="bold"><span class="koboSpan" id="kobo.829.1">graph attention networks</span></strong><span class="koboSpan" id="kobo.830.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.831.1">GATs</span></strong><span class="koboSpan" id="kobo.832.1">) have been </span><a id="_idIndexMarker880"/><span class="koboSpan" id="kobo.833.1">proposed. </span><span class="koboSpan" id="kobo.833.2">In this type of GNN, the model learns different levels of importance for each neighbor. </span><span class="koboSpan" id="kobo.833.3">Several models of GNN layers exist today, but basically, the principle</span><a id="_idIndexMarker881"/><span class="koboSpan" id="kobo.834.1"> does not </span><span class="No-Break"><span class="koboSpan" id="kobo.835.1">change much.</span></span></p>
<p><span class="koboSpan" id="kobo.836.1">GNNs have been successfully used for several graph tasks but still have some limitations such as difficult scalability, problems with batching, and so on. </span><span class="koboSpan" id="kobo.836.2">They have also been applied to KGs, but they </span><span class="No-Break"><span class="koboSpan" id="kobo.837.1">increase complexity.</span></span></p>
<h2 id="_idParaDest-134"><a id="_idTextAnchor133"/><span class="koboSpan" id="kobo.838.1">LLMs reasoning on knowledge graphs</span></h2>
<p><span class="koboSpan" id="kobo.839.1">LLMs have the advantage that they are not trained for a specific task but acquire a broad spectrum of skills during training. </span><span class="koboSpan" id="kobo.839.2">In addition, LLMs have reasoning skills that can be improved with specific approaches. </span><span class="koboSpan" id="kobo.839.3">Therefore, several researchers have suggested conducting graph reasoning with LLMs. </span><span class="koboSpan" id="kobo.839.4">The </span><a id="_idIndexMarker882"/><span class="koboSpan" id="kobo.840.1">main method of approaching an LLM is to use a prompt as input. </span><span class="koboSpan" id="kobo.840.2">There are </span><span class="No-Break"><span class="koboSpan" id="kobo.841.1">three approaches:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.842.1">Manual prompt</span></strong><span class="koboSpan" id="kobo.843.1">: The simplest</span><a id="_idIndexMarker883"/><span class="koboSpan" id="kobo.844.1"> approach is to </span><a id="_idIndexMarker884"/><span class="koboSpan" id="kobo.845.1">provide a prompt to </span><a id="_idIndexMarker885"/><span class="koboSpan" id="kobo.846.1">an LLM in which they are asked to solve a task on a graph. </span><span class="koboSpan" id="kobo.846.2">In the prompt, the graph is entered and additional information can be added (e.g., if we want the LLM to </span><a id="_idIndexMarker886"/><span class="koboSpan" id="kobo.847.1">conduct a </span><strong class="bold"><span class="koboSpan" id="kobo.848.1">depth-first search</span></strong><span class="koboSpan" id="kobo.849.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.850.1">DFS</span></strong><span class="koboSpan" id="kobo.851.1">) algorithm to solve the task, we provide a succinct explanation of how this algorithm works). </span><span class="koboSpan" id="kobo.851.2">A limitation to these prompts is that it is not possible to insert wide graphs within a prompt (limitation due to the context length of </span><span class="No-Break"><span class="koboSpan" id="kobo.852.1">the LLM).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.853.1">Self-prompting</span></strong><span class="koboSpan" id="kobo.854.1">: The</span><a id="_idIndexMarker887"/><span class="koboSpan" id="kobo.855.1"> LLM conducts a continuous update of the prompt to make it easier for the LLM to solve tasks. </span><span class="koboSpan" id="kobo.855.2">In other words, given an original prompt, the LLM conducts prompt</span><a id="_idIndexMarker888"/><span class="koboSpan" id="kobo.856.1"> updates to better define tasks and how to resolve them. </span><span class="koboSpan" id="kobo.856.2">Then, based on the output of the LLM, a new prompt is generated and fed back to the LLM. </span><span class="koboSpan" id="kobo.856.3">This process can be conducted multiple times to refine </span><span class="No-Break"><span class="koboSpan" id="kobo.857.1">the output.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.858.1">API call prompts</span></strong><span class="koboSpan" id="kobo.859.1">: This type </span><a id="_idIndexMarker889"/><span class="koboSpan" id="kobo.860.1">of prompt is</span><a id="_idIndexMarker890"/><span class="koboSpan" id="kobo.861.1"> inspired by agents, in which the LLM is provided with a set of APIs that it can invoke to conduct reasoning about graphs or other </span><span class="No-Break"><span class="koboSpan" id="kobo.862.1">external tools.</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer208">
<span class="koboSpan" id="kobo.863.1"><img alt="Figure 7.28 – Prompting methods for the LLM applied to graph tasks (https://arxiv.org/pdf/2404.14809)" src="image/B21257_07_28.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.864.1">Figure 7.28 – Prompting methods for the LLM applied to graph tasks (</span><a href="https://arxiv.org/pdf/2404.14809"><span class="koboSpan" id="kobo.865.1">https://arxiv.org/pdf/2404.14809</span></a><span class="koboSpan" id="kobo.866.1">)</span></p>
<p><span class="koboSpan" id="kobo.867.1">The alternative to finding complex prompting strategies is the </span><strong class="bold"><span class="koboSpan" id="kobo.868.1">supervised fine-tuning</span></strong><span class="koboSpan" id="kobo.869.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.870.1">SFT</span></strong><span class="koboSpan" id="kobo.871.1">) method. </span><span class="koboSpan" id="kobo.871.2">In this case, a dataset</span><a id="_idIndexMarker891"/><span class="koboSpan" id="kobo.872.1"> with graph tasks and their solutions is used to train the model to improve its </span><span class="No-Break"><span class="koboSpan" id="kobo.873.1">reasoning skills.</span></span></p>
<p><span class="koboSpan" id="kobo.874.1">Another interesting aspect of LLMs is that they can also be used in combination with other models. </span><span class="koboSpan" id="kobo.874.2">This allows their qualities to be exploited with models that are specialized and better suited for certain tasks. </span><span class="koboSpan" id="kobo.874.3">For example, we can combine LLMs with GNNs, in which case LLMs can function as enhancers of GNNs. </span><span class="koboSpan" id="kobo.874.4">The GNN handles graph structure much better than the LLM, but the latter </span><a id="_idIndexMarker892"/><span class="koboSpan" id="kobo.875.1">handles textual attributes much better. </span><span class="koboSpan" id="kobo.875.2">We can then capitalize on the strengths </span><a id="_idIndexMarker893"/><span class="koboSpan" id="kobo.876.1">of the two models to have a much stronger synergistic model. </span><span class="koboSpan" id="kobo.876.2">LLMs possess greater semantic and syntactic capacity than other models, and this allows them to create powerful textual embeddings. </span><span class="koboSpan" id="kobo.876.3">An LLM can then generate numerical embeddings that are then used by the GNN as node features. </span><span class="koboSpan" id="kobo.876.4">For example, we have a citation network between scientific articles (our graph where each node is an article) and we want to classify articles into various topics. </span><span class="koboSpan" id="kobo.876.5">We can take the abstract for each article and use an LLM to create an embedding of the abstract. </span><span class="koboSpan" id="kobo.876.6">These number vectors will be the node features for the articles. </span><span class="koboSpan" id="kobo.876.7">At this point, we can train our GNN with better results than without features. </span><span class="koboSpan" id="kobo.876.8">Alternatively, if the node features are textual, we can use an LLM to generate labels. </span><span class="koboSpan" id="kobo.876.9">For example, for our article network, we have the article titles associated with each node, and we use the LLM in a zero-shot setting to generate a set of labels. </span><span class="koboSpan" id="kobo.876.10">This is useful because manual annotation is expensive, so we can get labels much more quickly. </span><span class="koboSpan" id="kobo.876.11">When we have obtained the labels, we can train a GNN on the graph. </span><span class="koboSpan" id="kobo.876.12">Alternatively, we can also think of conducting fine-tuning of the LLM and GNN at the same time </span><span class="No-Break"><span class="koboSpan" id="kobo.877.1">for tasks.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer209">
<span class="koboSpan" id="kobo.878.1"><img alt="Figure 7.29 – LLM and GNN synergy" src="image/B21257_07_29.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.879.1">Figure 7.29 – LLM and GNN synergy</span></p>
<p><span class="koboSpan" id="kobo.880.1">Another interesting approach is graph-formed reasoning. </span><span class="koboSpan" id="kobo.880.2">Several of the prompting techniques that have been used for reasoning</span><a id="_idIndexMarker894"/><span class="koboSpan" id="kobo.881.1"> do not take into account that human thinking is not linear, so according to</span><a id="_idIndexMarker895"/><span class="koboSpan" id="kobo.882.1"> some, this method of reasoning can be approximated using graphs. </span><span class="koboSpan" id="kobo.882.2">There are two types of approaches that take advantage of </span><span class="No-Break"><span class="koboSpan" id="kobo.883.1">this idea:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.884.1">Think on the graph</span></strong><span class="koboSpan" id="kobo.885.1">: In this approach, the LLM reasons in the form of a graph where each node represents an intermediate of thought (i.e., an intermediate step of reasoning or conclusion during the reasoning necessary to arrive at a solution) and the edges represent the relationship between these thoughts and the direction in which the reasoning is headed. </span><span class="koboSpan" id="kobo.885.2">An example of this approach is a </span><strong class="bold"><span class="koboSpan" id="kobo.886.1">graph of thoughts</span></strong><span class="koboSpan" id="kobo.887.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.888.1">GoT</span></strong><span class="koboSpan" id="kobo.889.1">), where the</span><a id="_idIndexMarker896"/><span class="koboSpan" id="kobo.890.1"> reasoning intermediates are presented as a graph. </span><span class="koboSpan" id="kobo.890.2">This approach is expensive (multiple calls in inference) and typically used for problems that require reasoning, such as math problems. </span><span class="koboSpan" id="kobo.890.3">For example, to solve </span><em class="italic"><span class="koboSpan" id="kobo.891.1">A train travels 60 miles in 1.5 hours. </span><span class="koboSpan" id="kobo.891.2">What is its average speed?</span></em><span class="koboSpan" id="kobo.892.1">, the model might create nodes such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.893.1">Distance = 60 km</span></strong><span class="koboSpan" id="kobo.894.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.895.1">Time = 1.5 hours</span></strong><span class="koboSpan" id="kobo.896.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.897.1">Use speed = distance ÷ time</span></strong><span class="koboSpan" id="kobo.898.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.899.1">Speed = 40 km/h</span></strong><span class="koboSpan" id="kobo.900.1">, with edges showing how each thought leads to the next. </span><span class="koboSpan" id="kobo.900.2">This graph structure enables the model to reason step by step, explore alternatives, or </span><span class="No-Break"><span class="koboSpan" id="kobo.901.1">verify calculations.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.902.1">Verify on the graph</span></strong><span class="koboSpan" id="kobo.903.1">: In this approach, we use a graph to verify the correctness and consistency of the reasoning. </span><span class="koboSpan" id="kobo.903.2">For example, if different paths should lead to a logical conclusion, they should be the same or similar. </span><span class="koboSpan" id="kobo.903.3">So, if there is a contradiction, it means</span><a id="_idIndexMarker897"/><span class="koboSpan" id="kobo.904.1"> the reasoning is wrong. </span><span class="koboSpan" id="kobo.904.2">Generally, one generates several reasonings</span><a id="_idIndexMarker898"/><span class="koboSpan" id="kobo.905.1"> for a question, structures them as a graph, and analyzes them to improve the final answer. </span><span class="koboSpan" id="kobo.905.2">This approach requires a verifier who analyzes this graph, usually </span><span class="No-Break"><span class="koboSpan" id="kobo.906.1">another LLM.</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer210">
<span class="koboSpan" id="kobo.907.1"><img alt="Figure 7.30 – Think on graphs and verify on graphs (https://arxiv.org/pdf/2404.14809)" src="image/B21257_07_30.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.908.1">Figure 7.30 – Think on graphs and verify on graphs (</span><a href="https://arxiv.org/pdf/2404.14809"><span class="koboSpan" id="kobo.909.1">https://arxiv.org/pdf/2404.14809</span></a><span class="koboSpan" id="kobo.910.1">)</span></p>
<p><span class="koboSpan" id="kobo.911.1">In this section, we discussed the intricate relationship between graphs and LLMs and how they can enable us to solve some tasks that were previously conducted with graph ML algorithms. </span><span class="koboSpan" id="kobo.911.2">In the next section, we will discuss exciting perspectives in the field of some questions that </span><span class="No-Break"><span class="koboSpan" id="kobo.912.1">remain open.</span></span></p>
<h1 id="_idParaDest-135"><a id="_idTextAnchor134"/><span class="koboSpan" id="kobo.913.1">Ongoing challenges in knowledge graphs and GraphRAG</span></h1>
<p><span class="koboSpan" id="kobo.914.1">KGs are a powerful medium for storing and organizing information, but there are still limitations and open questions. </span><span class="koboSpan" id="kobo.914.2">Especially for</span><a id="_idIndexMarker899"/><span class="koboSpan" id="kobo.915.1"> large KGs, scalability is important; a </span><a id="_idIndexMarker900"/><span class="koboSpan" id="kobo.916.1">balance must be struck between expressiveness and computational efficiency. </span><span class="koboSpan" id="kobo.916.2">Plus building a KG requires a lot of computational effort (using an LLM to extract triplets from a large corpus of text can be expensive and require adequate infrastructure). </span><span class="koboSpan" id="kobo.916.3">In addition, once the KG is built, it must be evaluated and cleaned, which also requires some effort (manual or computational). </span><span class="koboSpan" id="kobo.916.4">Moreover, growth in the KG also means growth in the infrastructural cost to enable access or use. </span><span class="koboSpan" id="kobo.916.5">Querying large KGs requires having optimized algorithms to avoid the risk of </span><a id="_idIndexMarker901"/><span class="koboSpan" id="kobo.917.1">increasingly large latency times. </span><span class="koboSpan" id="kobo.917.2">Industrial KGs can contain billions of entities and </span><a id="_idIndexMarker902"/><span class="koboSpan" id="kobo.918.1">relationships, representing an intricate and complex scale. </span><span class="koboSpan" id="kobo.918.2">Many of the algorithms are designed for small-scale KGs (up to thousands of entities) so retrieval in large-scale KGs still </span><span class="No-Break"><span class="koboSpan" id="kobo.919.1">remains challenging.</span></span></p>
<p><span class="koboSpan" id="kobo.920.1">In addition, KGs are notoriously incomplete, which means that one must have pipelines in order to complete the KG. </span><span class="koboSpan" id="kobo.920.2">This means having both pipelines to add additional sources and pipelines to conduct the update of data sources. </span><span class="koboSpan" id="kobo.920.3">Most databases are static, so creating dynamic systems is challenging. </span><span class="koboSpan" id="kobo.920.4">This is critical to make the best use of KGs for domains such as finance, where we want to account for rapid market changes. </span><span class="koboSpan" id="kobo.920.5">On a side note, KGs can be multimodal, but integrating these modes is not easy at all. </span><span class="koboSpan" id="kobo.920.6">While adding modalities significantly improves the reasoning process, the understanding of the nuances of stored knowledge, and the richness of the KG, it significantly increases management complexity (more storage required, more sophisticated pipelines, more complex knowledge harmonization, and </span><span class="No-Break"><span class="koboSpan" id="kobo.921.1">so on).</span></span></p>
<p><span class="koboSpan" id="kobo.922.1">GraphRAG is a relatively new technology and still not fully optimized. </span><span class="koboSpan" id="kobo.922.2">For one thing, information retrieval could be improved, especially the transition between the user’s text query and retrieval on the KG. </span><span class="koboSpan" id="kobo.922.3">The more the KG grows, the more there is a risk of finding redundant information that harms the generation process. </span><span class="koboSpan" id="kobo.922.4">After retrieval, we could then end up with a long context that is provided to the LLM for generation. </span><span class="koboSpan" id="kobo.922.5">To reduce noise and reduce computation, we can compress the context, but this carries the risk of information loss. </span><span class="koboSpan" id="kobo.922.6">At present, lossless compression is an active field of research, while current methods allow, at most, a trade-off between compression and information preservation. </span><span class="koboSpan" id="kobo.922.7">There is also currently a lack of benchmark standards to evaluate new GraphRAG approaches; this does not allow for an easy comparison of either current or future methods. </span><span class="koboSpan" id="kobo.922.8">GraphRAG allows for considering inter-entity relationships and structural knowledge information, reducing redundant text information, and being able to find global information again. </span><span class="koboSpan" id="kobo.922.9">At the same time, though, the nuances of the text are lost, and GraphRAG underperforms in abstractive question-answering tasks or when there is no explicit entity mentioned in the question. </span><span class="koboSpan" id="kobo.922.10">So, the union of vector and GraphRAG (or HybridRAG) is an exciting prospect for the future. </span><span class="koboSpan" id="kobo.922.11">It remains interesting to understand how these two technologies will be integrated in an </span><span class="No-Break"><span class="koboSpan" id="kobo.923.1">optimal way.</span></span></p>
<p><span class="koboSpan" id="kobo.924.1">An important note is that LLMs are not specifically trained for graph tasks. </span><span class="koboSpan" id="kobo.924.2">As we mentioned in </span><a href="B21257_03.xhtml#_idTextAnchor042"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.925.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.926.1">, LLMs are trained to predict the next word in a sequence. </span><span class="koboSpan" id="kobo.926.2">By optimizing this simple goal, they acquire most of their skills. </span><span class="koboSpan" id="kobo.926.3">Obviously, it is difficult to assimilate a spatial understanding simply from text. </span><span class="koboSpan" id="kobo.926.4">This means that LLMs generally struggle with structural data. </span><span class="koboSpan" id="kobo.926.5">This is highlighted with tabular data, where LLMs have problems with understanding tables and relationships. </span><span class="koboSpan" id="kobo.926.6">The first problem is that LLMs struggle with numerical representation </span><a id="_idIndexMarker903"/><span class="koboSpan" id="kobo.927.1">since the tokenization step makes it difficult for an LLM to understand the whole</span><a id="_idIndexMarker904"/><span class="koboSpan" id="kobo.928.1"> number (lack of consistent decimal representation, and problems with numerical operation). </span><span class="koboSpan" id="kobo.928.2">This then impacts the execution of graph tasks where this numerical understanding is necessary. </span><span class="koboSpan" id="kobo.928.3">Specific studies on graph understanding show that LLMs have a basic understanding of graph structure. </span><span class="koboSpan" id="kobo.928.4">LLMs understand these graphs in linear form and better understand the labels associated with the nodes more than the topological structure of the graph. </span><span class="koboSpan" id="kobo.928.5">According to these studies, LLMs have a basic understanding, which is strongly impacted by prompt design, prompt techniques, semantic information provided, and the presence of examples. </span><span class="koboSpan" id="kobo.928.6">Next-generation, multi-parameter LLMs succeed in solving simple tasks on small graphs, but their performance decays rapidly as both graph and task complexity increase. </span><span class="koboSpan" id="kobo.928.7">There are two reasons for this lack of understanding of structural data. </span><span class="koboSpan" id="kobo.928.8">The first is that in the large text corpora used for training LLMs, there is not much graph-derived data. </span><span class="koboSpan" id="kobo.928.9">So, LLMs can only learn basic spatial relationships because these are described in the texts. </span><span class="koboSpan" id="kobo.928.10">Therefore, SFT on graph datasets allows for results that are better than much </span><span class="No-Break"><span class="koboSpan" id="kobo.929.1">larger models.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer211">
<span class="koboSpan" id="kobo.930.1"><img alt="Figure 7.31 – SFT on graph data allows better performance for small LLMs than larger LLMs (https://arxiv.org/pdf/2403.04483)" src="image/B21257_07_31.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.931.1">Figure 7.31 – SFT on graph data allows better performance for small LLMs than larger LLMs (</span><a href="https://arxiv.org/pdf/2403.04483"><span class="koboSpan" id="kobo.932.1">https://arxiv.org/pdf/2403.04483</span></a><span class="koboSpan" id="kobo.933.1">)</span></p>
<p><span class="koboSpan" id="kobo.934.1">The second reason, on the other hand, stems from why humans understand spatial structures well. </span><span class="koboSpan" id="kobo.934.2">Humans learn </span><a id="_idIndexMarker905"/><span class="koboSpan" id="kobo.935.1">spatial relationships from their experiences in the outside world. </span><span class="koboSpan" id="kobo.935.2">The brain creates</span><a id="_idIndexMarker906"/><span class="koboSpan" id="kobo.936.1"> mental maps that allow us to orient ourselves in space. </span><span class="koboSpan" id="kobo.936.2">These maps also enable us to better understand abstract spatial concepts such as graphs. </span><span class="koboSpan" id="kobo.936.3">LLMs do not have a mental map nor can they have the experience of the outside world, thus making them disadvantaged in understanding abstract </span><span class="No-Break"><span class="koboSpan" id="kobo.937.1">spatial concepts.</span></span></p>
<h1 id="_idParaDest-136"><a id="_idTextAnchor135"/><span class="koboSpan" id="kobo.938.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.939.1">In </span><em class="italic"><span class="koboSpan" id="kobo.940.1">Chapters 5</span></em><span class="koboSpan" id="kobo.941.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.942.1">6</span></em><span class="koboSpan" id="kobo.943.1">, the main question was how to find information and how to use this information to generate an answer to users’ questions. </span><span class="koboSpan" id="kobo.943.2">Finding information dynamically allows us to reduce the hallucinations of our model and keep its knowledge up </span><span class="No-Break"><span class="koboSpan" id="kobo.944.1">to date.</span></span></p>
<p><span class="koboSpan" id="kobo.945.1">In this chapter, we started with a text corpus and created a system to find the most relevant information for generating an answer (naïve RAG). </span><span class="koboSpan" id="kobo.945.2">Next, we created a more sophisticated system to try to extract only the relevant information and avoid redundant information or noise. </span><span class="koboSpan" id="kobo.945.3">For some researchers, by its nature, text contains relevant information intermixed with background noise. </span><span class="koboSpan" id="kobo.945.4">What matters are the entities present and their relationships. </span><span class="koboSpan" id="kobo.945.5">From this reductionist approach comes the idea of representing essential knowledge in a knowledge graph. </span><span class="koboSpan" id="kobo.945.6">The graph allows us to use algorithms to search for information or explore possible connections. </span><span class="koboSpan" id="kobo.945.7">For a long time, graph reasoning and LLMs have run on parallel tracks, but recently, their stories have begun to intertwine. </span><span class="koboSpan" id="kobo.945.8">We have seen how this interaction between LLM and KG can be conducted in various ways. </span><span class="koboSpan" id="kobo.945.9">For example, an LLM can be used to extract relationships and entities for our graph construction, or an LLM can be used to conduct reasoning about the KG. </span><span class="koboSpan" id="kobo.945.10">Similarly, we can use the KG to find knowledge and enrich the context of the LLM, thus enabling it to effectively answer a </span><span class="No-Break"><span class="koboSpan" id="kobo.946.1">user question.</span></span></p>
<p><span class="koboSpan" id="kobo.947.1">Right now, there is a sort of a Manichean definition: either the vector RAG or the GraphRAG. </span><span class="koboSpan" id="kobo.947.2">Both have merits and demerits, and the research points toward a unification of these two worlds (HybridRAG). </span><span class="koboSpan" id="kobo.947.3">In the future, we will find more sophisticated ways of uniting KGs and vectors. </span><span class="koboSpan" id="kobo.947.4">Also, the understanding of the graph structure on one side of an LLM is still immature. </span><span class="koboSpan" id="kobo.947.5">With the growth of training datasets, the new generation LLMs are exposed to more examples of graphs. </span><span class="koboSpan" id="kobo.947.6">However, understanding spatial relationships in an abstract concept such as a graph also means understanding them in problems with greater real-world relevance. </span><span class="koboSpan" id="kobo.947.7">Therefore, this is an active field of research, especially for robots that must interact in space and </span><span class="No-Break"><span class="koboSpan" id="kobo.948.1">use AI.</span></span></p>
<p><span class="koboSpan" id="kobo.949.1">Moving into space is one of the next frontiers of AI. </span><span class="koboSpan" id="kobo.949.2">Interaction in space presents peculiar challenges, such as balancing exploration and exploitation. </span><span class="koboSpan" id="kobo.949.3">In the next chapter, we will discuss this concept more abstractly. </span><span class="koboSpan" id="kobo.949.4">We will focus on reinforcement learning and agent behavior in the relationship to space. </span><span class="koboSpan" id="kobo.949.5">Whether chess, a video game, or a real-world environment, an agent must learn how to interact with space to achieve a goal. </span><span class="koboSpan" id="kobo.949.6">In the next chapter, we will look at how to enable an agent to explore the world without losing sight of </span><span class="No-Break"><span class="koboSpan" id="kobo.950.1">the aim.</span></span></p>
<h1 id="_idParaDest-137"><a id="_idTextAnchor136"/><span class="koboSpan" id="kobo.951.1">Further reading</span></h1>
<ul>
<li><span class="koboSpan" id="kobo.952.1">Ghafarollahi, </span><em class="italic"><span class="koboSpan" id="kobo.953.1">SciAgents: Automating Scientific Discovery through Multi-agent Intelligent Graph Reasoning</span></em><span class="koboSpan" id="kobo.954.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.955.1">2024, </span></span><a href="https://arxiv.org/pdf/2409.05556v1"><span class="No-Break"><span class="koboSpan" id="kobo.956.1">https://arxiv.org/pdf/2409.05556v1</span></span></a></li>
<li><span class="koboSpan" id="kobo.957.1">Raieli, </span><em class="italic"><span class="koboSpan" id="kobo.958.1">A Brave New World for Scientific Discovery: Are AI Research Ideas Better?</span></em><span class="koboSpan" id="kobo.959.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.960.1">2024, </span></span><a href="https://levelup.gitconnected.com/a-brave-new-world-for-scientific-discovery-are-ai-research-ideas-better-5692c5aa8182"><span class="No-Break"><span class="koboSpan" id="kobo.961.1">https://levelup.gitconnected.com/a-brave-new-world-for-scientific-discovery-are-ai-research-ideas-better-5692c5aa8182</span></span></a></li>
<li><span class="koboSpan" id="kobo.962.1">Raieli, </span><em class="italic"><span class="koboSpan" id="kobo.963.1">How the LLM Got Lost in the Network and Discovered Graph Reasoning</span></em><span class="koboSpan" id="kobo.964.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.965.1">2024, </span></span><a href="https://towardsdatascience.com/how-the-llm-got-lost-in-the-network-and-discovered-graph-reasoning-e2736bd04efa"><span class="No-Break"><span class="koboSpan" id="kobo.966.1">https://towardsdatascience.com/how-the-llm-got-lost-in-the-network-and-discovered-graph-reasoning-e2736bd04efa</span></span></a></li>
<li><span class="koboSpan" id="kobo.967.1">Wu, </span><em class="italic"><span class="koboSpan" id="kobo.968.1">Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation</span></em><span class="koboSpan" id="kobo.969.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.970.1">2024, </span></span><a href="https://arxiv.org/abs/2408.04187"><span class="No-Break"><span class="koboSpan" id="kobo.971.1">https://arxiv.org/abs/2408.04187</span></span></a></li>
<li><span class="koboSpan" id="kobo.972.1">Raieli, </span><em class="italic"><span class="koboSpan" id="kobo.973.1">The Convergence of Graph and Vector RAGs: A New Era in Information Retrieval</span></em><span class="koboSpan" id="kobo.974.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.975.1">2024, </span></span><a href="https://medium.com/gitconnected/the-convergence-of-graph-and-vector-rags-a-new-era-in-information-retrieval-b5773a723615"><span class="No-Break"><span class="koboSpan" id="kobo.976.1">https://medium.com/gitconnected/the-convergence-of-graph-and-vector-rags-a-new-era-in-information-retrieval-b5773a723615</span></span></a></li>
<li><span class="koboSpan" id="kobo.977.1">Sarmah, </span><em class="italic"><span class="koboSpan" id="kobo.978.1">HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction</span></em><span class="koboSpan" id="kobo.979.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.980.1">2024, </span></span><a href="https://arxiv.org/pdf/2408.04948"><span class="No-Break"><span class="koboSpan" id="kobo.981.1">https://arxiv.org/pdf/2408.04948</span></span></a></li>
<li><span class="koboSpan" id="kobo.982.1">Liang, </span><em class="italic"><span class="koboSpan" id="kobo.983.1">Survey of Graph Neural Networks and Applications</span></em><span class="koboSpan" id="kobo.984.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.985.1">2022, </span></span><a href="https://onlinelibrary.wiley.com/doi/10.1155/2022/9261537"><span class="No-Break"><span class="koboSpan" id="kobo.986.1">https://onlinelibrary.wiley.com/doi/10.1155/2022/9261537</span></span></a></li>
<li><span class="koboSpan" id="kobo.987.1">Arora, </span><em class="italic"><span class="koboSpan" id="kobo.988.1">A Survey on Graph Neural Networks for Knowledge Graph Completion</span></em><span class="koboSpan" id="kobo.989.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.990.1">2020, </span></span><a href="https://arxiv.org/pdf/2007.12374"><span class="No-Break"><span class="koboSpan" id="kobo.991.1">https://arxiv.org/pdf/2007.12374</span></span></a></li>
<li><span class="koboSpan" id="kobo.992.1">Huang, </span><em class="italic"><span class="koboSpan" id="kobo.993.1">Can LLMs Effectively Leverage Graph Structural Information through Prompts, and Why?</span></em><span class="koboSpan" id="kobo.994.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.995.1">2023, </span></span><a href="https://arxiv.org/abs/2309.16595"><span class="No-Break"><span class="koboSpan" id="kobo.996.1">https://arxiv.org/abs/2309.16595</span></span></a></li>
<li><span class="koboSpan" id="kobo.997.1">Liu, </span><em class="italic"><span class="koboSpan" id="kobo.998.1">Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis</span></em><span class="koboSpan" id="kobo.999.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1000.1">2023, </span></span><a href="https://arxiv.org/abs/2308.11224"><span class="No-Break"><span class="koboSpan" id="kobo.1001.1">https://arxiv.org/abs/2308.11224</span></span></a></li>
</ul>
</div>
</body></html>