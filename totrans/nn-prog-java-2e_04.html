<html><head></head><body><div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Chapter 4. Self-Organizing Maps"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title"><a id="ch04" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Chapter 4. Self-Organizing Maps</h1></div></div></div><p class="calibre11">In this chapter, we present a neural network architecture that is suitable for unsupervised learning: self-organizing maps, also known as Kohonen networks. This particular type of neural network is able to categorize records of data without any target output or find a representation of the data in a smaller dimension. Throughout this chapter, we are going to explore how this is achieved, as well as examples to attest to its capacity. The subtopics of this chapter are as follows:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Neural networks unsupervised learning</li><li class="listitem">Competitive learning</li><li class="listitem">Kohonen self-organizing maps</li><li class="listitem">One-dimensional SOMs</li><li class="listitem">Two-dimensional SOMs</li><li class="listitem">Problems solved with unsupervised learning</li><li class="listitem">Java implementation</li><li class="listitem">Data visualization</li><li class="listitem">Practical problems</li></ul></div><div class="calibre2" title="Neural networks unsupervised learning"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch04lvl1sec32" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Neural networks unsupervised learning</h1></div></div></div><p class="calibre11">In <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch02.xhtml" title="Chapter 2. Getting Neural Networks to Learn">Chapter 2</a>, <span class="strong1"><em class="calibre16">Getting Neural Networks to Learn</em></span> we've been acquainted with unsupervised <a id="id244" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>learning, and now we are going to <a id="id245" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>explore the features of this learning paradigm in more detail. The mission of unsupervised learning algorithms is to find patterns in datasets, where the parameters (weights in the case of neural networks) are adjusted without any error measure (there are no target values).</p><p class="calibre11">While the supervised algorithms provide an output comparable to the dataset that was presented, the unsupervised algorithms do not need to know the output values. The fundamentals of unsupervised learning are inspired by the fact that, in neurology, similar stimuli produce similar responses. So applying this to artificial neural networks, we can say that similar data produces similar outputs, so those outputs can be grouped or clustered.</p><p class="calibre11">Although this <a id="id246" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>learning may be used in other mathematical <a id="id247" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>fields, such as statistics, its core functionality is intended and designed for machine learning problems such as data mining, pattern recognition, and so on. Neural networks are a subfield in the machine learning discipline, and provided that their structure allows iterative learning, they serve as a good framework to apply this concept to.</p><p class="calibre11">Most of unsupervised learning applications are aimed at clustering tasks, which means that similar data points are to be clustered together, while different data points form different clusters. Also, one application that unsupervised learning is suitable for is dimensionality reduction or data compression, as long as simpler and smaller representations of the data can be found among huge datasets.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Unsupervised learning algorithms"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch04lvl1sec33" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Unsupervised learning algorithms</h1></div></div></div><p class="calibre11">Unsupervised <a id="id248" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>algorithms are not unique to neural networks, as K-means, expectation maximization, and methods of moments are also examples of unsupervised learning algorithms. One common feature of all learning algorithms is the absence of mapping among variables in the current dataset; instead, one wishes to find a different meaning of this data, and that's the goal of any unsupervised learning algorithm.</p><p class="calibre11">While in supervised learning algorithms, we usually have a smaller number of outputs, for unsupervised learning, there is a need to produce an abstract data representation that may require a high number of outputs, but, except for classification tasks, their meaning is totally different than the one presented in the supervised learning. Usually, each output neuron is responsible for representing a feature or a class present in the input data. In most architectures, not all output neurons need to be activated at a time; only a restricted set of output neurons may fire, meaning that that neuron is able to better represent most of the information being fed at the neural input.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title4"><a id="tip10" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre17">One advantage of unsupervised learning over supervised learning is that less computational power required by the first for the learning of huge datasets. Time consumption grows linearly while for the supervised learning it grows exponentially.</p></div></div><p class="calibre11">In this chapter, we are going to explore two unsupervised learning algorithms: competitive learning and Kohonen self-organizing maps.</p><div class="calibre2" title="Competitive learning"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec48" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Competitive learning</h2></div></div></div><p class="calibre11">As the <a id="id249" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>name implies, competitive learning <a id="id250" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>handles a competition between the output neurons to determine which one is the winner. In competitive learning, the winning neuron is usually determined by comparing the weights against the inputs (they have the same dimensionality). To facilitate understanding, suppose we want to train a single layer neural network with two inputs and four outputs:</p><div class="mediaobject"><img src="Images/B05964_04_01.jpg" alt="Competitive learning" class="calibre123"/></div><p class="calibre11">Every output neuron is then connected to these two inputs, hence for each neuron there are two weights.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="tip11" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre17">For this learning, the bias is dropped from the neurons, so the neurons will process only the weighted inputs.</p></div></div><p class="calibre11">The competition starts after the data has been processed by the neurons. The winner neuron will be the one whose weights are <span class="strong1"><em class="calibre16">near</em></span> to the input values. One additional difference compared to the supervised learning algorithm is that only the winner neuron may update<a id="id251" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> their weights, while the other ones remain unchanged. This is the so-called <span class="strong1"><strong class="calibre12">winner-takes-all</strong></span> rule. This intention to bring the neuron <span class="strong1"><em class="calibre16">nearer</em></span> to the input that caused it to win the competition.</p><p class="calibre11">Considering <a id="id252" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>that every input neuron <span class="strong1"><em class="calibre16">i</em></span> is<a id="id253" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> connected to all output neurons <span class="strong1"><em class="calibre16">j</em></span> through a weight <span class="strong1"><em class="calibre16">wij</em></span>, in our case, we would have a set of weights:</p><div class="mediaobject"><img src="Images/B05964_04_01_01.jpg" alt="Competitive learning" class="calibre124"/></div><p class="calibre11">Provided that the weights of every neuron have the same dimensionality of the input data, let's consider all the input data points together in a plot with the weights of each neuron:</p><div class="mediaobject"><img src="Images/B05964_04_02.jpg" alt="Competitive learning" class="calibre125"/></div><p class="calibre11">In this chart, let's consider the circles as the data points and the squares as the neuron weights. We can see that some data points are closer to certain weights, while others are farther but nearer to others. The neural network performs computations on the distance on the inputs and the weights:</p><div class="mediaobject"><img src="Images/B05964_04_02_01.jpg" alt="Competitive learning" class="calibre126"/></div><p class="calibre11">The result of this equation will determine how much <span class="strong1"><em class="calibre16">stronger</em></span> a neuron is against its competitors. The neuron whose weight distance to the input is the smaller is considered the winner. After many iterations, the weights are driven near enough to the data points that give<a id="id254" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> more cause the corresponding neuron<a id="id255" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> to win that the changes are either too small or the weights fall in a zig-zag setting. Finally, when the network is already trained, the chart takes another shape:</p><div class="mediaobject"><img src="Images/B05964_04_03.jpg" alt="Competitive learning" class="calibre127"/></div><p class="calibre11">As can be seen, the neurons form centroids surrounding the points capable of making the corresponding neuron stronger than its competitors.</p><p class="calibre11">In an unsupervised neural network, the number of outputs is completely arbitrary. Sometimes only some neurons are able to change their weights, while in other cases, all the neurons may respond differently to the same input, causing the neural network to never learn. In these cases, it is recommended either to review the number of output neurons, or<a id="id256" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> consider another type of <a id="id257" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>unsupervised learning.</p><p class="calibre11">Two stopping conditions are preferable in competitive learning:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Predefined number of epochs: This prevents our algorithm from running for a longer time without convergence</li><li class="listitem">Minimum value of weight update: Prevents the algorithm from running longer than necessary</li></ul></div></div><div class="calibre2" title="Competitive layer"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec49" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Competitive layer</h2></div></div></div><p class="calibre11">This type <a id="id258" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of neural layer is particular, as<a id="id259" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the outputs won't be necessarily the same as its neuron's outputs. Only one neuron will be fired at a time, thereby requiring a special rule to calculate the outputs. So, let's create a new class called <code class="literal">CompetitiveLayer</code> that will inherit from <code class="literal">OutputLayer</code> and starting with two new attributes: <code class="literal">winnerNeuron</code> and <code class="literal">winnerIndex</code>:</p><div class="calibre2"><pre class="programlisting">public class CompetitiveLayer extends OutputLayer {
    public Neuron winnerNeuron;
    public int[] winnerIndex;
//…
}</pre></div><p class="calibre11">This new<a id="id260" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> class of neural layer will <a id="id261" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>override the method <code class="literal">calc()</code> and add some new particular methods to get the weights:</p><div class="calibre2"><pre class="programlisting">@Override
public void calc(){
  if(input!=null &amp;&amp; neuron!=null){
    double[] result = new double[numberOfNeuronsInLayer];
    for(int i=0;i&lt;numberOfNeuronsInLayer;i++){
      neuron.get(i).setInputs(this.input);
     //perform the normal calculation
      neuron.get(i).calc();
      //calculate the distance and store in a vector
      result[i]=getWeightDistance(i);
      //sets all outputs to zero
      try{
        output.set(i,0.0);
      }
      catch(IndexOutOfBoundsException iobe){
        output.add(0.0);
      }
    }
    //determine the index and the neuron that was the winner
    winnerIndex[0]=ArrayOperations.indexmin(result);
    winnerNeuron=neuron.get(winnerIndex[0]);
    // sets the output of this particular neuron to 1.0
    output.set(winnerIndex[0], 1.0);
  }
}</pre></div><p class="calibre11">In the next <a id="id262" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>sections, we will define the<a id="id263" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> class Kohonen for the Kohonen neural network. In this class, there will be an <code class="literal">enum</code> called <code class="literal">distanceCalculation</code>, which will have the different methods to calculate distance. In this chapter (and book), we'll stick to the Euclidian distance.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="tip12" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre17">A new class called <code class="literal">ArrayOperations</code> was created to provide methods that facilitate operations with arrays. Functionalities such as getting the index of the maximum or minimum or getting a subset of the array are implemented in this class.</p></div></div><p class="calibre11">The distance between the weights of a particular neuron and the input is calculated by the method <code class="literal">getWeightDistance( )</code>, which is called inside the <code class="literal">calc</code> method:</p><div class="calibre2"><pre class="programlisting">public double getWeightDistance(int neuron){
  double[] inputs = this.getInputs();
  double[] weights = this.getNeuronWeights(neuron);
  int n=this.numberOfInputs;
  double result=0.0;
  switch(distanceCalculation){
    case EUCLIDIAN:
    //for simplicity, let's consider only the euclidian distance
    default:
      for(int i=0;i&lt;n;i++){
        result+=Math.pow(inputs[i]-weights[i],2);
      }
      result=Math.sqrt(result);
  }
  return result;
}</pre></div><p class="calibre11">The method <code class="literal">getNeuronWeights( )</code> returns the weights of the neuron corresponding to the index passed in the array. Since it is simple and to save space here, we invite the reader to see the code to check its implementation.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Kohonen self-organizing maps"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch04lvl1sec34" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Kohonen self-organizing maps</h1></div></div></div><p class="calibre11">This network <a id="id264" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>architecture was created by the Finnish professor Teuvo Kohonen at the beginning of the 80s. It consists of one single layer neural network capable of providing a <span class="strong1"><em class="calibre16">visualization</em></span> of the data in one or two dimensions.</p><p class="calibre11">In this book, we are going to use Kohonen networks also as a basic competitive layer with no links between the neurons. In this case, we are going to consider it as zero dimension (0-D).</p><p class="calibre11">Theoretically, a <a id="id265" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Kohonen Network would be able to provide a 3-D (or even in more dimensions) representation of the data; however, in printed material such as this book, it is not practicable to show 3-D charts without overlapping some data. Thus in this book, we are going to deal only with 0-D, 1-D, and 2-D Kohonen networks.</p><p class="calibre11">Kohonen <span class="strong1"><strong class="calibre12">Self-Organizing Maps</strong></span> (<span class="strong1"><strong class="calibre12">SOMs</strong></span>), in addition to the traditional single layer competitive neural networks (in this book, the 0-D Kohonen network), add the concept of neighborhood neurons. A dimensional SOM takes into account the index of the neurons in the competitive layer, letting the neighborhood of neurons play a relevant role during the learning phase.</p><p class="calibre11">An SOM has two modes of functioning: mapping and learning. In the mapping mode, the input data is classified in the most appropriate neuron, while in the learning mode, the input data helps the learning algorithm to build the <span class="strong1"><em class="calibre16">map</em></span>. This map can be interpreted as a lower-dimension representation from a certain dataset.</p><div class="calibre2" title="Extending the neural network code to Kohonen"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec50" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Extending the neural network code to Kohonen</h2></div></div></div><p class="calibre11">In our code, let's <a id="id266" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>create a new class inherited from <code class="literal">NeuralNet</code>, since it will be a particular type of neural network. This class will be called Kohonen, which will use the class <code class="literal">CompetitiveLayer</code> as the output layer. The following class diagram shows how these new classes are arranged:</p><div class="mediaobject"><img src="Images/B05964_04_04.jpg" alt="Extending the neural network code to Kohonen" class="calibre128"/></div><p class="calibre11">Three <a id="id267" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>types of SOMs are covered in this chapter: zero-, one- and two-dimensional. These configurations are defined in an <code class="literal">enum MapDimension</code>:</p><div class="calibre2"><pre class="programlisting">public enum MapDimension {ZERO,ONE_DIMENSION,TWO_DIMENSION};</pre></div><p class="calibre11">The Kohonen constructor defines the dimension of the Kohonen neural network:</p><div class="calibre2"><pre class="programlisting">public Kohonen(int numberofinputs, int numberofoutputs, WeightInitialization _weightInitialization, int dim){
  weightInitialization=_weightInitialization;
  activeBias=false;
  numberOfHiddenLayers=0; //no hidden layers
//…
  numberOfInputs=numberofinputs;
  numberOfOutputs=numberofoutputs;
  input=new ArrayList&lt;&gt;(numberofinputs);
  inputLayer=new InputLayer(this,numberofinputs);
// the competitive layer will be defined according to the dimension passed in the argument dim
  outputLayer=new CompetitiveLayer(this,numberofoutputs, numberofinputs,dim);
  inputLayer.setNextLayer(outputLayer);
  setNeuralNetMode(NeuralNetMode.RUN);       
  deactivateBias();
}</pre></div></div><div class="calibre2" title="Zero-dimensional SOM"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec51" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Zero-dimensional SOM</h2></div></div></div><p class="calibre11">This is the <a id="id268" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>pure <a id="id269" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>competitive layer, where the order of the neurons is irrelevant. Features such as neighborhood functions are not taken into account. Only the winner neuron weights are affected during the learning phase. The map will be composed only of unconnected dots.</p><p class="calibre11">The following code snippet define a zero-dimensional SOM:</p><div class="calibre2"><pre class="programlisting">int numberOfInputs=2;
int numberOfNeurons=10;
Kohonen kn0 = new Kohonen(numberOfInputs,numberOfNeurons,new UniformInitialization(-1.0,1.0),0);</pre></div><p class="calibre11">Note the value <code class="literal">0</code> passed in the argument dim (the last of the constructor).</p></div><div class="calibre2" title="One-dimensional SOM"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec52" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>One-dimensional SOM</h2></div></div></div><p class="calibre11">This<a id="id270" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> architecture<a id="id271" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> is similar to the network presented in the last section, <span class="strong1"><strong class="calibre12">Competitive learning</strong></span>, with the addition of neighborhood amongst the output neurons:</p><div class="mediaobject"><img src="Images/B05964_04_05.jpg" alt="One-dimensional SOM" class="calibre129"/></div><p class="calibre11">Note that <a id="id272" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>every neuron on<a id="id273" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the output layer has one or two neighbors. Similarly, the neuron that fires the greatest value updates its weights, but in a SOM, the neighbor neurons also update their weights in a smaller rate.</p><p class="calibre11">The effect of the neighborhood extends the activation area to a wider area of the map, provided that all the output neurons must observe an organization, or a path in the one-dimensional case. The neighborhood function also allows for a better exploration of the properties of the input space, since it forces the neural network to keep the connections between neurons, therefore resulting in more information in addition to the clusters that are formed.</p><p class="calibre11">In a plot of the input data points with the neural weights, we can see the path formed by the neurons:</p><div class="mediaobject"><img src="Images/B05964_04_06.jpg" alt="One-dimensional SOM" class="calibre130"/></div><p class="calibre11">In the <a id="id274" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>chart here <a id="id275" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>presented, for simplicity, we plotted only the output weights to demonstrate how the map is designed in a (in this case) 2-D space. After training over many iterations, the neural network converges to a final shape that represent all data points. Provided that structure, a certain set of data may cause the Kohonen Network to design another shape in the space. This is a good example of dimensionality reduction, since a multidimensional dataset when presented to the Self-Organizing Map is able to produce one single line (in the 1-D SOM) that summarizes the entire dataset.</p><p class="calibre11">To define a one-dimensional SOM, we need to pass the value <code class="literal">1</code> as the argument <code class="literal">dim</code>:</p><div class="calibre2"><pre class="programlisting">Kohonen kn1 = new Kohonen(numberOfInputs,numberOfNeurons,new UniformInitialization(-1.0,1.0),1);</pre></div></div><div class="calibre2" title="Two-dimensional SOM"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec53" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Two-dimensional SOM</h2></div></div></div><p class="calibre11">This is the most<a id="id276" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> used<a id="id277" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> architecture to demonstrate the Kohonen neural network power in a visual way. The output layer is one matrix containing M x N neurons, interconnected like a grid:</p><div class="mediaobject"><img src="Images/B05964_04_07.jpg" alt="Two-dimensional SOM" class="calibre131"/></div><p class="calibre11">In the 2-D SOMs, every neuron now has up to four neighbors (in the square configuration), although in some representations, the diagonal neurons may also be considered, thus resulting in up to eight neighbors. Hexagonal representations are also useful. Let's see one example of what a 3x3 SOM plot looks like in a 2-D chart (considering two input variables):</p><div class="mediaobject"><img src="Images/B05964_04_08.jpg" alt="Two-dimensional SOM" class="calibre132"/></div><p class="calibre11">At first, the<a id="id278" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> untrained Kohonen Network shows a very strange and screwed-up shape. The shaping of the <a id="id279" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>weights will depend solely on the input data that is going to be fed to the SOM. Let's see an example of how the map starts to organize itself:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Suppose we have the dense data set shown in the following plot:<div class="mediaobject1"><img src="Images/B05964_04_09.jpg" alt="Two-dimensional SOM" class="calibre133"/></div></li><li class="listitem">Applying <a id="id280" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>SOM, the 2-D shape gradually changes, until it achieves the final <a id="id281" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>configuration:<div class="mediaobject1"><img src="Images/B05964_04_10.jpg" alt="Two-dimensional SOM" class="calibre134"/></div></li></ul></div><p class="calibre11">The final shape of a 2-D SOM may not always be a perfect square; instead, it will resemble a shape that could be drawn from the dataset. The neighborhood function is one important component in the learning process because it approximates the neighbor neurons in the plot, and the structure moves to a configuration that is more <span class="strong1"><em class="calibre16">organized</em></span>.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="tip13" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre17">The grid<a id="id282" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> on<a id="id283" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> a chart is just the more used and didactic. There are other ways of showing the SOM diagram, such as the U-matrix and the cluster boundaries.</p></div></div></div><div class="calibre2" title="2D competitive layer"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec54" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>2D competitive layer</h2></div></div></div><p class="calibre11">In order to <a id="id284" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>better<a id="id285" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> represent the neurons of a 2D competitive layer in a grid form, we're creating the CompetitiveLayer2D class, which inherits from <code class="literal">CompetitiveLayer</code>. In this class, we can define the number of neurons in the form of a grid of M x N neurons:</p><div class="calibre2"><pre class="programlisting">public class CompetitiveLayer2D extends CompetitiveLayer {
    
    protected int sizeMapX; // neurons in dimension X
    protected int sizeMapY; // neurons in dimension Y
    
    protected int[] winner2DIndex;// position of the neuron in grid

    public CompetitiveLayer2D(NeuralNet _neuralNet,int numberOfNeuronsX,int numberOfNeuronsY,int numberOfInputs){
        super(_neuralNet,numberOfNeuronsX*numberOfNeuronsY,
numberOfInputs);
        this.dimension=Kohonen.MapDimension.TWO_DIMENSION;
        this.winnerIndex=new int[1];
        this.winner2DIndex=new int[2];
        this.coordNeuron=new int[numberOfNeuronsX*numberOfNeuronsY][2];
        this.sizeMapX=numberOfNeuronsX;
        this.sizeMapY=numberOfNeuronsY;
        //each neuron is assigned a coordinate in the grid
        for(int i=0;i&lt;numberOfNeuronsY;i++){
            for(int j=0;j&lt;numberOfNeuronsX;j++){
                coordNeuron[i*numberOfNeuronsX+j][0]=i;
                coordNeuron[i*numberOfNeuronsX+j][1]=j;
            }
        }
    }</pre></div><p class="calibre11">The<a id="id286" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> coordinate<a id="id287" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> system in the 2D competitive layer is analogous to the Cartesian. Every neuron is assigned a position in the grid, with indexes starting from <code class="literal">0</code>:</p><div class="mediaobject"><img src="Images/B05964_04_11.jpg" alt="2D competitive layer" class="calibre135"/></div><p class="calibre11">In the illustration above, 12 neurons are arranged in a 3 x 4 grid. Another feature added in this class is the indexing of neurons by the position in the grid. This allows us to get subsets of neurons (and weights), one entire specific row or column of the grid, for example:</p><div class="calibre2"><pre class="programlisting">public double[] getNeuronWeights(int x, int y){
  double[] nweights = neuron.get(x*sizeMapX+y).getWeights();
  double[] result = new double[nweights.length-1];
  for(int i=0;i&lt;result.length;i++){
    result[i]=nweights[i];
  }
  return result;
}
    
public double[][] getNeuronWeightsColumnGrid(int y){
  double[][] result = new double[sizeMapY][numberOfInputs];
  for(int i=0;i&lt;sizeMapY;i++){
    result[i]=getNeuronWeights(i,y);
  }
  return result;
}

public double[][] getNeuronWeightsRowGrid(int x){
  double[][] result = new double[sizeMapX][numberOfInputs];
  for(int i=0;i&lt;sizeMapX;i++){
    result[i]=getNeuronWeights(x,i);
  }
  return result;
}</pre></div></div><div class="calibre2" title="SOM learning algorithm"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec55" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>SOM learning algorithm</h2></div></div></div><p class="calibre11">A self-organizing map aims at classifying the input data by clustering those data points that trigger<a id="id288" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the same response<a id="id289" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> on the output. Initially, the untrained network will produce random outputs, but as more examples are presented, the neural network identifies which neurons are activated more often and then their <span class="strong1"><em class="calibre16">position</em></span> in the SOM output space is changed. This algorithm is based on competitive learning, which means a winner neuron (also known as best matching unit, or BMU) will update its weights and its neighbor weights.</p><p class="calibre11">The following flowchart illustrates the learning process of a SOM Network:</p><div class="mediaobject"><img src="Images/B05964_04_12.jpg" alt="SOM learning algorithm" class="calibre136"/></div><p class="calibre11">The<a id="id290" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> learning resembles a bit<a id="id291" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> those algorithms addressed in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch02.xhtml" title="Chapter 2. Getting Neural Networks to Learn">Chapter 2</a>, <span class="strong1"><em class="calibre16">Getting Neural Networks to Learn</em></span> and <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Perceptrons and Supervised Learning">Chapter 3</a>, <span class="strong1"><em class="calibre16">Perceptrons and Supervised Learning</em></span>. Three major differences are the determination of the BMU with the distance, the weight update rule, and the absence of an error measure. The distance implies that nearer points should produce similar outputs, thus here the criterion to determine the lowest BMU is the neuron which presents a lower distance to some data point. This Euclidean distance is usually used, and in this book we will apply it for simplicity:</p><div class="mediaobject"><img src="Images/B05964_04_12.jpg" alt="SOM learning algorithm" class="calibre136"/></div><p class="calibre11">The weight-to-input distance is calculated by the method <code class="literal">getWeightDistance( )</code> of the <code class="literal">CompetitiveLayer</code> class for a specific neuron i (argument neuron). This method was described above.</p></div><div class="calibre2" title="Effect of neighboring neurons – the neighborhood function"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec56" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Effect of neighboring neurons – the neighborhood function</h2></div></div></div><p class="calibre11">The weight <a id="id292" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>update rule uses a<a id="id293" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> neighborhood function <span class="strong1"><em class="calibre16">Θ(u,v,s,t)</em></span> which states how much a neighbor neuron u (BMU unit) is close to a neuron <span class="strong1"><em class="calibre16">v</em></span>. Remember that in a dimensional SOM, the BMU neuron is updated together with its neighbor neurons. This update is also dependent on a neighborhood radius, which takes into account the number of epoch's s and a reference epoch <span class="strong1"><em class="calibre16">t</em></span>:</p><div class="mediaobject"><img src="Images/B05964_04_12_02.jpg" alt="Effect of neighboring neurons – the neighborhood function" class="calibre137"/></div><p class="calibre11">Here, <span class="strong1"><em class="calibre16">du,v</em></span> is the neuron distance between neurons u and v in the grid. The radius is calculated as follows:</p><div class="mediaobject"><img src="Images/B05964_04_12_03.jpg" alt="Effect of neighboring neurons – the neighborhood function" class="calibre138"/></div><p class="calibre11">Here, is the initial radius. The effect of the number of epochs (s) and the reference epoch (<span class="strong1"><em class="calibre16">t</em></span>) is the decreasing of the neighborhood radius and thereby the effect of neighborhood. This is useful because in the beginning of the training, the weights need to be updated more often, because they are usually randomly initialized. As the training process continues, the updates need to be weaker, otherwise the neural network will continue to change its weights forever and will never converge.</p><div class="mediaobject"><img src="Images/B05964_04_12_04.jpg" alt="Effect of neighboring neurons – the neighborhood function" class="calibre139"/></div><p class="calibre11">The neighborhood function and the neuron distance are implemented in the <code class="literal">CompetitiveLayer</code> class, with overridden versions for the <code class="literal">CompetitiveLayer2D</code> class:</p><div class="informaltable"><table border="1" class="calibre20"><colgroup class="calibre21"><col class="calibre22"/><col class="calibre22"/></colgroup><thead class="calibre23"><tr class="calibre24"><th valign="bottom" class="calibre25">
<p class="calibre26">CompetitiveLayer</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">CompetitiveLayer2D</p>
</th></tr></thead><tbody class="calibre27"><tr class="calibre31"><td class="calibre29">
<div class="calibre2"><pre class="programlisting2">public double neighborhood(int u, int v, int s,int t){
  double result;
  switch(dimension){
case ZERO:
  if(u==v) result=1.0;
  else result=0.0;
  break;
case ONE_DIMENSION:
default:
  double exponent=-(neuronDistance(u,v)
/neighborhoodRadius(s,t));
  result=Math.exp(exponent);
  }
  return result;
}</pre></div>
</td><td class="calibre29">
<div class="calibre2"><pre class="programlisting2">@Override
public double neighborhood(int u, int v, int s,int t){
  double result;
  double exponent=-(neuronDistance(u,v)
/neighborhoodRadius(s,t));
  result=Math.exp(exponent);
  return result;
}</pre></div>
</td></tr><tr class="calibre37"><td class="calibre29">
<div class="calibre2"><pre class="programlisting2">public double neuronDistance(int u,int v){
  return Math.abs(coordNeuron[u][0]-coordNeuron[v][0]);
}</pre></div>
</td><td class="calibre29">
<div class="calibre2"><pre class="programlisting2">@Override
public double neuronDistance(int u,int v){
  double distance=
Math.pow(coordNeuron[u][0]
-coordNeuron[v][0],2);
  distance+=
Math.pow(coordNeuron[u][1]-coordNeuron[v][1],2);
  return Math.sqrt(distance);
}</pre></div>
</td></tr></tbody></table></div><p class="calibre11">The<a id="id294" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> neighborhood<a id="id295" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> radius function is the same for both classes:</p><div class="calibre2"><pre class="programlisting">public double neighborhoodRadius(int s,int t){
  return this.initialRadius*Math.exp(-((double)s/(double)t));
}</pre></div></div><div class="calibre2" title="The learning rate"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec57" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The learning rate</h2></div></div></div><p class="calibre11">The<a id="id296" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> learning rate also becomes weaker as the training goes on:</p><div class="mediaobject"><img src="Images/B05964_04_12_05.jpg" alt="The learning rate" class="calibre140"/></div><div class="mediaobject"><img src="Images/B05964_04_12_06.jpg" alt="The learning rate" class="calibre141"/></div><p class="calibre11">The parameter is the initial learning rate. Finally, considering the neighborhood function and the learning rate, the weight update rule is as follows:</p><div class="mediaobject"><img src="Images/B05964_04_12_07.jpg" alt="The learning rate" class="calibre142"/></div><div class="mediaobject"><img src="Images/B05964_04_12_08.jpg" alt="The learning rate" class="calibre143"/></div><p class="calibre11">Here, <span class="strong1"><em class="calibre16">X</em></span>
<span class="strong1"><em class="calibre16"><sub class="calibre144">k</sub></em></span> is <a id="id297" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the <span class="strong1"><em class="calibre16">kth</em></span> input, and <span class="strong1"><em class="calibre16">W</em></span>
<span class="strong1"><em class="calibre16"><sub class="calibre144">kj</sub></em></span> is the weight connecting the <span class="strong1"><em class="calibre16">kth</em></span> input to the <span class="strong1"><em class="calibre16">jth</em></span> output.</p></div><div class="calibre2" title="A new class for competitive learning"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec58" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>A new class for competitive learning</h2></div></div></div><p class="calibre11">Now that <a id="id298" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>we<a id="id299" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> have a competitive layer, a Kohonen neural network, and defined the methods for neighboring functions, let's create a new class for competitive learning. This class will inherit from <code class="literal">LearningAlgorithm</code> and will receive Kohonen objects for learning:</p><div class="mediaobject"><img src="Images/B05964_04_13.jpg" alt="A new class for competitive learning" class="calibre145"/></div><p class="calibre11">As seen in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch02.xhtml" title="Chapter 2. Getting Neural Networks to Learn">Chapter 2</a>, <span class="strong1"><em class="calibre16">Getting Neural Networks to Learn</em></span> a <code class="literal">LearningAlgorithm</code> object receives a neural <a id="id300" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>dataset for training. This property is inherited by the <code class="literal">CompetitiveLearning</code> object, which implements new methods and properties to realize the competitive<a id="id301" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> learning procedure:</p><div class="calibre2"><pre class="programlisting">public class CompetitiveLearning extends LearningAlgorithm {
    // indicates the index of the current record of the training dataset  
    private int currentRecord=0;
    //stores the new weights until they will be applied
    private ArrayList&lt;ArrayList&lt;Double&gt;&gt; newWeights;
    //saves the current weights for update
    private ArrayList&lt;ArrayList&lt;Double&gt;&gt; currWeights;
    // initial learning rate
    private double initialLearningRate = 0.3;
    //default reference epoch
    private int referenceEpoch = 30;
    //saves the index of winner neurons for each training record
    private int[] indexWinnerNeuronTrain;
//…
}</pre></div><p class="calibre11">The learning rate, as opposed to the previous algorithms, now changes over the training process, and it will be returned by the method <code class="literal">getLearningRate( )</code>:</p><div class="calibre2"><pre class="programlisting">public double getLearningRate(int epoch){
  double exponent=(double)(epoch)/(double)(referenceEpoch);
  return initialLearningRate*Math.exp(-exponent);
}</pre></div><p class="calibre11">This method is <a id="id302" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>used<a id="id303" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> in the <code class="literal">calcWeightUpdate( )</code>:</p><div class="calibre2"><pre class="programlisting">@Override
public double calcNewWeight(int layer,int input,int neuron)
            throws NeuralException{
//…
  Double deltaWeight=getLearningRate(epoch);
  double xi=neuralNet.getInput(input);
  double wi=neuralNet.getOutputLayer().getWeight(input, neuron);
  int wn = indexWinnerNeuronTrain[currentRecord];
  CompetitiveLayer cl = ((CompetitiveLayer)(((Kohonen)(neuralNet))
                   .getOutputLayer()));
  switch(learningMode){
    case BATCH:
    case ONLINE: //The same rule for batch and online modes
      deltaWeight*=cl.neighborhood(wn, neuron, epoch, referenceEpoch) *(xi-wi);
      break;
  }
  return deltaWeight;
}</pre></div><p class="calibre11">The <code class="literal">train( )</code> method is adapted as well for competitive learning:</p><div class="calibre2"><pre class="programlisting">@Override
public void train() throws NeuralException{
//…
  epoch=0;
  int k=0;
  forward();
//…
  currentRecord=0;
  forward(currentRecord);
  while(!stopCriteria()){
    // first it calculates the new weights for each neuron and input
    for(int j=0;j&lt;neuralNet.getNumberOfOutputs();j++){
      for(int i=0;i&lt;neuralNet.getNumberOfInputs();i++){
        double newWeight=newWeights.get(j).get(i);
        newWeights.get(j).set(i,newWeight+calcNewWeight(0,i,j));
      }
    }   
    //the weights are promptly updated in the online mode
    switch(learningMode){
      case BATCH:
        break;
      case ONLINE:
      default:
        applyNewWeights();
    }
    currentRecord=++k;
    if(k&gt;=trainingDataSet.numberOfRecords){
      //for the batch mode, the new weights are applied once an epoch
      if(learningMode==LearningAlgorithm.LearningMode.BATCH){
        applyNewWeights();
      }
      k=0;
      currentRecord=0;
      epoch++;
      forward(k);
//…
    }
  }
}</pre></div><p class="calibre11">The<a id="id304" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> implementation for the method <code class="literal">appliedNewWeights( )</code> is analogous to the one presented in<a id="id305" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the previous chapter, with the exception that there is no bias and there is only one output layer.</p><p class="calibre11">
<span class="strong1"><strong class="calibre12">Time to play</strong></span>: SOM applications in action. Now it is time to get hands-on and implement the Kohonen neural network in Java. There are many applications of self-organizing maps, most of them being in the field of clustering, data abstraction, and dimensionality reduction. But the clustering applications are the most interesting because of the many possibilities one may apply them on. The real advantage of clustering is that there is no need to worry<a id="id306" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> about input/output relationship, rather the problem solver may concentrate<a id="id307" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> on the input data. One example of clustering application will be explored in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch07.xhtml" title="Chapter 7. Clustering Customer Profiles">Chapter 7</a>, <span class="strong1"><em class="calibre16">Clustering Customer Profiles</em></span>.</p></div><div class="calibre2" title="Visualizing the SOMs"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec59" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Visualizing the SOMs</h2></div></div></div><p class="calibre11">In this <a id="id308" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>section, we are going to introduce the plotting feature. Charts can be drawn in Java by using the freely available package <code class="literal">JFreeChart</code> (which can be downloaded from <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.jfree.org/jfreechart/">http://www.jfree.org/jfreechart/</a>). This package is attached with this chapter's source<a id="id309" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> code. So, we <a id="id310" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>designed a class called <span class="strong1"><strong class="calibre12">Chart</strong></span>:</p><div class="calibre2"><pre class="programlisting">public class Chart {
  //title of the chart
  private String chartTitle;
  //datasets to be rendered in the chart
  private ArrayList&lt;XYDataset&gt; dataset = new ArrayList&lt;XYDataset&gt;();
  //the chart object
  private JFreeChart jfChart;
  //colors of each dataseries
  private ArrayList&lt;Paint&gt; seriesColor = new ArrayList&lt;&gt;();
  //types of series (dots or lines for now)    
  public enum SeriesType {DOTS,LINES};
  //collections of types for each series
  public ArrayList&lt;SeriesType&gt; seriesTypes = new ArrayList&lt;SeriesType&gt;();

//…
}</pre></div><p class="calibre11">The methods<a id="id311" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> implemented in this class are for plotting line and scatter plots. The main difference between them lies in the fact that line plots take all data series over one x-axis (usually the time axis) where each data series is a line; scatter plots, on the other hand, show dots in a 2D plane indicating its position in relation to each of the axis. Charts below show graphically the difference between them and the codes to generate them:</p><div class="mediaobject"><img src="Images/B05964_04_14.jpg" alt="Visualizing the SOMs" class="calibre146"/></div><div class="calibre2"><pre class="programlisting">int numberOfPoints=10;
        
double[][] dataSet = {
{1.0, 1.0},{2.0,2.0}, {3.0,4.0}, {4+.0, 8.0},{5.0,16.0}, {6.0,32.0},
{7.0,64.0},{8.0,128.0}};

String[] seriesNames = {"Line Plot"};
Paint[] seriesColor = {Color.BLACK};
            
Chart chart = new Chart("Line Plot", dataSet, seriesNames, 0, seriesColor, Chart.SeriesType.LINE);

ChartFrame frame = new ChartFrame("Line Plot", chart.linePlot("X Axis", "Y Axis"));

frame.pack();
frame.setVisibile(true);</pre></div><div class="mediaobject"><img src="Images/B05964_04_15.jpg" alt="Visualizing the SOMs" class="calibre147"/></div><div class="calibre2"><pre class="programlisting">int numberOfInputs=2;
int numberOfPoints=100;
        
double[][] rndDataSet =
RandomNumberGenerator
.GenerateMatrixBetween
(numberOfPoints
, numberOfInputs, -10.0, 10.0);
String[] seriesNames = {"Scatter Plot"};
Paint[] seriesColor = {Color.WHITE};
            
Chart chart = new Chart("Scatter Plot", rndDataSet, seriesNames, 0, seriesColor, Chart.SeriesType.DOTS);

ChartFrame frame = new ChartFrame("Scatter Plot", chart.scatterPlot("X Axis", "Y Axis"));

frame.pack();</pre></div><p class="calibre11">We're<a id="id312" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> suppressing the codes for chart generation (methods <code class="literal">linePlot( )</code> and <code class="literal">scatterPlot( ));</code> however, in the file <code class="literal">Chart.java</code>, the reader can find their implementation.</p></div><div class="calibre2" title="Plotting 2D training datasets and neuron weights"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec60" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Plotting 2D training datasets and neuron weights</h2></div></div></div><p class="calibre11">Now that<a id="id313" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> we have the <a id="id314" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>methods for plotting charts, let's plot the training dataset and neuron weights. Any 2D dataset can be plotted in the same way shown in the diagram of the last section. To plot the weights, we need to get the weights of the Kohonen neural network using the following code:</p><div class="calibre2"><pre class="programlisting">CompetitiveLayer cl = ((CompetitiveLayer)(neuralNet.getOutputLayer()));
double[][] neuronsWeights = cl.getWeights();</pre></div><p class="calibre11">In competitive learning, we can check visually how the weights <span class="strong1"><em class="calibre16">move</em></span> around the dataset space. So we're going to add a method (<code class="literal">showPlot2DData( )</code>) to plot the dataset and the weights, a property (<code class="literal">plot2DData</code>) to hold the reference to the <code class="literal">ChartFrame</code>, and a flag (<code class="literal">show2DData</code>) to determine whether the plot is going to be shown for every epoch:</p><div class="calibre2"><pre class="programlisting">protected ChartFrame plot2DData;

public boolean show2DData=false;

public void showPlot2DData(){
  double[][] data= ArrayOperations. arrayListToDoubleMatrix( trainingDataSet.inputData.data);
  String[] seriesNames = {"Training Data"};
  Paint[] seriesColor = {Color.WHITE};
            
  Chart chart = new Chart("Training epoch n°"+String.valueOf(epoch)+" ",data,seriesNames,0,seriesColor,Chart.SeriesType.DOTS);
  if(plot2DData ==null){
    plot2DData = new ChartFrame("Training",chart.scatterPlot("X","Y"));
  }
       
  Paint[] newColor={Color.BLUE};
  String[] neuronsNames={""};
  CompetitiveLayer cl = ((CompetitiveLayer)(neuralNet.getOutputLayer()));
  double[][] neuronsWeights = cl.getWeights();
  switch(cl.dimension){
    case TWO_DIMENSION:
      ArrayList&lt;double[][]&gt; gridWeights = ((CompetitiveLayer2D)(cl)). getGridWeights();
      for(int i=0;i&lt;gridWeights.size();i++){
        chart.addSeries(gridWeights.get(i),neuronsNames, 0,newColor, Chart.SeriesType.LINES);
      }
      break;
    case ONE_DIMENSION:
      neuronsNames[0]="Neurons Weights";
      chart.addSeries(neuronsWeights, neuronsNames, 0, newColor, Chart.SeriesType.LINES);
      break;
    case ZERO:
      neuronsNames[0]="Neurons Weights";
    default:
      chart.addSeries(neuronsWeights, neuronsNames, 0,newColor, Chart.SeriesType.DOTS);
  }
  plot2DData.getChartPanel().setChart(chart.scatterPlot("X", "Y"));
}</pre></div><p class="calibre11">This method <a id="id315" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>will <a id="id316" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>be called from the <code class="literal">train</code> method at the end of each epoch. A property called <span class="strong1"><strong class="calibre12">sleep</strong></span> will determine for how many milliseconds the chart will be displayed until the next epoch's chart replaces it:</p><div class="calibre2"><pre class="programlisting">if(show2DData){
  showPlot2DData();
  if(sleep!=-1)
    try{ Thread.sleep(sleep); }
    catch(Exception e){}
}</pre></div></div><div class="calibre2" title="Testing Kohonen learning"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec61" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Testing Kohonen learning</h2></div></div></div><p class="calibre11">Let's now<a id="id317" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> define a Kohonen network and see how it works. First, we're creating a Kohonen with zero dimension:</p><div class="calibre2"><pre class="programlisting">RandomNumberGenerator.seed=0;
int numberOfInputs=2;
int numberOfNeurons=10;
int numberOfPoints=100;
     
// create a random dataset between -10.0 and 10.0   
double[][] rndDataSet = RandomNumberGenerator. GenerateMatrixBetween(numberOfPoints, numberOfInputs, -10.0, 10.0);

// create the Kohonen with uniform initialization of weights        
Kohonen kn0 = new Kohonen(numberOfInputs,numberOfNeurons,new UniformInitialization(-1.0,1.0),0);

//add the dataset to the neural dataset
NeuralDataSet neuralDataSet = new NeuralDataSet(rndDataSet,2);
        
//create an instance of competitive learning in the online mode
CompetitiveLearning complrn=new CompetitiveLearning(kn0,neuralDataSet, LearningAlgorithm.LearningMode.ONLINE);

//sets the flag to show the plot
complrn.show2DData=true;

        
try{
// give names and colors for the dataset
  String[] seriesNames = {"Training Data"};
  Paint[] seriesColor = {Color.WHITE};
  //this instance will create the plot with the random series
  Chart chart = new Chart("Training",rndDataSet,seriesNames,0, seriesColor);
  ChartFrame frame = new ChartFrame("Training", chart.scatterPlot("X", "Y"));
  frame.pack();
  frame.setVisible(true);

  // we pass the reference of the frame to the complrn object
  complrn.setPlot2DFrame(frame);
  // show the first epoch
  complrn.showPlot2DData();
//wait for the user to hit an enter
  System.in.read();
//training begins, and for each epoch a new plot will be shown
  complrn.train();
}
catch(Exception ne){
            
}</pre></div><p class="calibre11">By running<a id="id318" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> this code, we get the first plot:</p><div class="mediaobject"><img src="Images/B05964_04_16.jpg" alt="Testing Kohonen learning" class="calibre148"/></div><p class="calibre11">As the training <a id="id319" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>starts, the weights begin to distribute over the input data space, until finally it converges by being distributed uniformly along the input data space:</p><div class="mediaobject"><img src="Images/B05964_04_17.jpg" alt="Testing Kohonen learning" class="calibre149"/></div><p class="calibre11">For one<a id="id320" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> dimension, let's try something funkier. Let's create the dataset over a cosine function with random noise: </p><div class="calibre2"><pre class="programlisting">int numberOfPoints=1000;
int numberOfInputs=2;
int numberOfNeurons=20;
double[][] rndDataSet;
        
for (int i=0;i&lt;numberOfPoints;i++){
  rndDataSet[i][0]=i;            
  rndDataSet[i][0]+=RandomNumberGenerator.GenerateNext();
  rndDataSet[i][1]=Math.cos(i/100.0)*1000;            
  rndDataSet[i][1]+=RandomNumberGenerator.GenerateNext()*400;
}
Kohonen kn1 = new Kohonen(numberOfInputs,numberOfNeurons,new UniformInitialization(0.0,1000.0),1);</pre></div><p class="calibre11">By running the same previous code and changing the object to <span class="strong1"><em class="calibre16">kn1</em></span>, we get a line connecting all the weight points:</p><div class="mediaobject"><img src="Images/B05964_04_18.jpg" alt="Testing Kohonen learning" class="calibre150"/></div><p class="calibre11">As the<a id="id321" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> training continues, the lines tend to be organized along the data wave:</p><div class="mediaobject"><img src="Images/B05964_04_19.jpg" alt="Testing Kohonen learning" class="calibre151"/></div><p class="calibre11">See the file <code class="literal">Kohonen1DTest.java</code> if you want to change the initial learning rate, maximum<a id="id322" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> number of epochs, and other parameters.</p><p class="calibre11">Finally, let's see the two-dimensional Kohonen chart. The code will be a little bit different, since now, instead of giving the number of neurons, we're going to inform the Kohonen constructor the dimensions of our neural grid.</p><p class="calibre11">The dataset used here will be a circle with random noise added:</p><div class="calibre2"><pre class="programlisting">int numberOfPoints=1000;
for (int i=0;i&lt;numberOfPoints;i++){
  rndDataSet[i][0]*=Math.sin(i);            
  rndDataSet[i][0]+=RandomNumberGenerator.GenerateNext()*50;
  rndDataSet[i][1]*=Math.cos(i);            
  rndDataSet[i][1]+=RandomNumberGenerator.GenerateNext()*50;
}</pre></div><p class="calibre11">Now let's construct the two-dimensional Kohonen:</p><div class="calibre2"><pre class="programlisting">int numberOfInputs=2;
int neuronsGridX=12;
int neuronsGridY=12;
Kohonen kn2 = new Kohonen(numberOfInputs,neuronsGridX,neuronsGridY,new GaussianInitialization(500.0,20.0));</pre></div><p class="calibre11">Note that we are using <code class="literal">GaussianInitialization</code> with mean 500.0 and standard deviation 20.0, that is, the weights will be generated at the position (500.0,500.0) while the data is centered around (50.0,50.0):</p><div class="mediaobject"><img src="Images/B05964_04_20.jpg" alt="Testing Kohonen learning" class="calibre152"/></div><p class="calibre11">Now let's train the neural network. The neuron weights quickly move to the circle in the first epochs:</p><div class="mediaobject"><img src="Images/B05964_04_21.jpg" alt="Testing Kohonen learning" class="calibre153"/></div><p class="calibre11">By the end<a id="id323" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> of the training, the majority of the weights will be distributed over the circle, while in the center there will be an empty space, as the grid will be totally stretched out:</p><div class="mediaobject"><img src="Images/B05964_04_22.jpg" alt="Testing Kohonen learning" class="calibre154"/></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Summary"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch04lvl1sec35" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Summary</h1></div></div></div><p class="calibre11">In this chapter, we've seen how to apply unsupervised learning algorithms on neural networks. We've been presented a new and suitable architecture for that end, the self-organizing maps of Kohonen. Unsupervised learning has proved to be as powerful as the supervised learning methods, because they concentrate only on the input data, without need to make input-output mappings. We've seen graphically how the training algorithms are able to drive the weights nearer to the input data, thereby playing a role in clustering and dimensionality reduction. In addition to these examples, Kohonen SOMs are also able to classify clusters of data, as each neuron will provide better responses for a particular set of inputs.</p></div></div>



  </body></html>