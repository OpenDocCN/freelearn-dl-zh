<html><head></head><body>
<div><h1 class="chapternumber">5</h1>
<h1 class="chaptertitle" id="_idParaDest-71">Embedding LLMs within Your Applications</h1>
<p class="normal">This chapter kickstarts the hands-on portions<a id="_idIndexMarker302" class="calibre3"/> of this book, focusing on how we can <strong class="screentext">leverage large language models</strong> (<strong class="screentext">LLMs</strong>) to build powerful AI applications. In fact, LLMs have introduced a whole new paradigm in software development, paving the way for new families of applications that have the peculiarity of making the communication between the user and the machine smooth and conversational. Plus, those models enhanced existing applications, such as chatbots and recommendation systems, with their unique reasoning capabilities.</p>
<p class="normal1">Developing LLM-powered applications is becoming a key factor for enterprises to keep themselves competitive in the market, and this leads to the spreading of new libraries and frameworks that make it easier to embed LLMs within applications. Some examples are Semantic Kernel, Haystack, LlamaIndex, and LangChain. In this chapter, we are going to cover LangChain and use its modules to build hands-on examples. By the end of this chapter, you will have the technical foundations to start developing your LLM-powered applications using LangChain and open-source Hugging Face models.</p>
<p class="normal1">In this chapter, we will cover the following topics:</p>
<ul class="calibre14">
<li class="bulletlist">A brief note about LangChain</li>
<li class="bulletlist1">Getting started with LangChain</li>
<li class="bulletlist1">Working with LLMs via the Hugging Face Hub</li>
</ul>
<h1 class="heading" id="_idParaDest-72">Technical requirements</h1>
<p class="normal">To complete the hands-on sections of this chapter, the following prerequisites are needed:</p>
<ul class="calibre14">
<li class="bulletlist">A Hugging Face account and user access token.</li>
<li class="bulletlist1">An OpenAI account and user access token.</li>
<li class="bulletlist1">Python 3.7.1 or later version.</li>
<li class="bulletlist1">Python packages: Make sure to have the following Python packages installed: <code class="inlinecode">langchain</code>, <code class="inlinecode">python-dotenv</code>, <code class="inlinecode">huggingface_hub</code>, <code class="inlinecode">google-search-results</code>, <code class="inlinecode">faiss</code>, and <code class="inlinecode">tiktoken</code>. Those can be easily installed via <code class="inlinecode">pip install</code> in your terminal.</li>
</ul>
<p class="normal1">You can find all the code and examples used in this chapter in the book’s GitHub repository at <a href="Chapter_05.xhtml" class="calibre3">https://github.com/PacktPublishing/Building-LLM-Powered-Applications</a></p>
<h1 class="heading" id="_idParaDest-73">A brief note about LangChain</h1>
<p class="normal">Just as generative AI has evolved <a id="_idIndexMarker303" class="calibre3"/>so rapidly over the last year, so has LangChain. In the months between the writing of this book and its publication, the AI orchestrator has gone through massive changes. The most remarkable traces back to January 2024, when the first stable version of LangChain was released, introducing a new organization of packages and libraries.</p>
<p class="normal1">It consists of the following:</p>
<ul class="calibre14">
<li class="bulletlist">A core backbone where all the abstractions and runtime logic are stored</li>
<li class="bulletlist1">A layer of third-party integrations and components</li>
<li class="bulletlist1">A set of pre-built architectures and templates to leverage</li>
<li class="bulletlist1">A serving layer to consume chains as APIs</li>
<li class="bulletlist1">An observability layer to monitor your applications in the development, testing, and production stages</li>
</ul>
<p class="normal1">You can look at the architecture<a id="_idIndexMarker304" class="calibre3"/> in greater detail at <a href="https://python.langchain.com/docs/get_started/introduction" class="calibre3">https://python.langchain.com/docs/get_started/introduction</a>.</p>
<p class="normal1">There are three packages<a id="_idIndexMarker305" class="calibre3"/> you can install to start using LangChain:</p>
<ul class="calibre14">
<li class="bulletlist"><code class="inlinecode">langchain-core</code>: This contains the base abstractions and runtime for the whole LangChain ecosystem.</li>
<li class="bulletlist1"><code class="inlinecode">langchain-experimental</code>: This holds experimental LangChain code, intended for research and experimental uses.</li>
<li class="bulletlist1"><code class="inlinecode">langchain-community</code>: This contains<a id="_idIndexMarker306" class="calibre3"/> all third-party integrations.</li>
</ul>
<p class="normal1">On top of that, there are three additional packages that we’re not going to cover in this book, yet can be leveraged to monitor and maintain your LangChain applications:</p>
<ul class="calibre14">
<li class="bulletlist"><code class="inlinecode">langserve</code>: LangServe is a tool that<a id="_idIndexMarker307" class="calibre3"/> lets you deploy <strong class="screentext">LangChain runnables and chains</strong> as a REST API, making it easier to integrate LangChain applications into production environments.</li>
<li class="bulletlist1"><code class="inlinecode">langsmith</code>: Think of LangSmith as an <strong class="screentext">innovative testing framework</strong> for evaluating language<a id="_idIndexMarker308" class="calibre3"/> models and AI applications. It helps visualize inputs and outputs at each step in the chain, aiding understanding and intuition during development.</li>
<li class="bulletlist1"><code class="inlinecode">langchain-cli</code>: The <strong class="screentext">official command-line interface</strong> for LangChain, it facilitates interactions<a id="_idIndexMarker309" class="calibre3"/> with LangChain projects, including template usage and quickstarts.</li>
</ul>
<p class="normal1">Last but not least, LangChain introduced the <strong class="screentext">LangChain Expression Language</strong> (<strong class="screentext">LCEL</strong>) to enhance the efficiency<a id="_idIndexMarker310" class="calibre3"/> and flexibility of text processing tasks.</p>
<p class="normal1">Key features<a id="_idIndexMarker311" class="calibre3"/> of LCEL include:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Streaming asynchronous support</strong>: This allows for the efficient handling of data streams.</li>
<li class="bulletlist1"><strong class="screentext">Batch support</strong>: This enables processing data in batches.</li>
<li class="bulletlist1"><strong class="screentext">Parallel execution</strong>: This enhances performance by executing tasks concurrently.</li>
<li class="bulletlist1"><strong class="screentext">Retries and fallbacks</strong>: This ensures robustness by handling failures gracefully.</li>
<li class="bulletlist1"><strong class="screentext">Dynamically routing logic</strong>: This allows logic flow based on input and output.</li>
<li class="bulletlist1"><strong class="screentext">Message history</strong>: This keeps track of interactions for context-aware processing.</li>
</ul>
<p class="normal1">We are not going to cover LCEL in this book; however, all the code samples can be converted into LCEL<a id="_idIndexMarker312" class="calibre3"/> if you want to speed up your development and leverage its native integration with the end-to-end LangChain development stack.</p>
<div><p class="normal1"><strong class="screentext">Important note</strong></p>
<p class="normal1">Before we start working with LangChain, it is important<a id="_idIndexMarker313" class="calibre3"/> to note that all packages are versioned slightly differently, yet all releases are cut with high frequency by a maintainer with a clearer communication strategy for breaking changes.</p>
<p class="normal1">In the upcoming chapters, you will see some packages that have been moved, for example, to the <code class="inlinecode">experimental</code> package, meaning that they are more prone to experimental uses. Similarly, some third-party integrations have been moved to the <code class="inlinecode">community</code> package.</p>
</div>
<p class="normal1">Starting from the next section, we are going to cover the backbone concepts – such as memory, VectorDB, and agents – that remain solid in the LangChain framework and, more generally, in the landscape of LLM development.</p>
<h1 class="heading" id="_idParaDest-74">Getting started with LangChain</h1>
<p class="normal">As introduced in <em class="italic">Chapter 2</em>, LangChain is a lightweight<a id="_idIndexMarker314" class="calibre3"/> framework meant to make it easier to integrate and orchestrate LLMs and their components within applications. It is mainly Python based, yet it recently extended its support to JavaScript and TypeScript.</p>
<p class="normal1">In addition to LLM integration (which we will cover in an upcoming dedicated section), we saw that LangChain offers<a id="_idIndexMarker315" class="calibre3"/> the following main components:</p>
<ul class="calibre14">
<li class="bulletlist">Models and prompt templates</li>
<li class="bulletlist1">Data connections</li>
<li class="bulletlist1">Memory</li>
<li class="bulletlist1">Chains</li>
<li class="bulletlist1">Agents</li>
</ul>
<p class="normal1">These components are illustrated in the following diagram:</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="img/B21714_05_01.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 5.1: LangChain’s components</p>
<p class="normal1">The next sections will take a deep dive into each of these components.</p>
<h2 class="heading1" id="_idParaDest-75">Models and prompts</h2>
<p class="normal">LangChain offers more<a id="_idIndexMarker316" class="calibre3"/> than 50 integrations<a id="_idIndexMarker317" class="calibre3"/> with third-party vendors and platforms, including <strong class="screentext">OpenAI</strong>, Azure OpenAI, Databricks, and MosaicML, as well as the integration with the Hugging Face Hub and the world of open-source LLMs. In <em class="italic">Part 2</em> of this book, we will be trying various LLMs, both proprietary and open-source, and leveraging LangChain’s integrations.</p>
<p class="normal1">Just to provide an example, let’s see how easy<a id="_idIndexMarker318" class="calibre3"/> it is to consume the OpenAI GPT-3 model (you can retrieve your OpenAI API key at <a href="https://platform.openai.com/account/api-keys" class="calibre3">https://platform.openai.com/account/api-keys</a>):</p>
<pre class="programlisting"><code class="hljs-code">from langchain.llms import OpenAI
llm = OpenAI(openai_api_key="your-api-key")
print(llm('tell me a joke'))
</code></pre>
<p class="normal1">Here is the corresponding output:</p>
<pre class="programlisting1"><code class="hljs-con">Q: What did one plate say to the other plate?
A: Dinner's on me!
</code></pre>
<div><p class="normal1"><strong class="screentext">Note</strong></p>
<p class="normal1">While running examples with LLMs, the output will vary at each run, due to the stochasticity of the models themselves. If you want to reduce the margin of variations in your output, you can make your model more “deterministic” by tuning the temperature hyperparameter. This parameter ranges from 0 (deterministic) to 1 (stochastic).</p>
</div>
<p class="normal1">By default, the <strong class="screentext">OpenAI</strong> module<a id="_idIndexMarker319" class="calibre3"/> uses the <code class="inlinecode">gpt-3.5-turbo-instruct</code> as a model. You can specify the model you want to use by passing the model’s name as a parameter.</p>
<p class="normal1">As said previously, we will dive deeper into LLMs in the next section; so, for now, let’s focus on prompts. There<a id="_idIndexMarker320" class="calibre3"/> are two main components related to LLM prompts and prompts design/engineering:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Prompt templates</strong>: A prompt template is a component<a id="_idIndexMarker321" class="calibre3"/> that defines how to generate a prompt for a language model. It can include variables, placeholders, prefixes, suffixes, and other elements that can be customized according to the data and the task.</li>
</ul>
<p class="normal-one">For example, suppose you want to use a language model to generate a translation from one language to another. You can use a prompt template like this:</p>
<pre class="programlisting2"><code class="hljs-code">Sentence: {sentence}
Translation in {language}:
</code></pre>
<p class="normal-one"><code class="inlinecode">{sentence}</code> is a variable that will be replaced by the actual text. <code class="inlinecode">Translation in {language}:</code> is a prefix that indicates the task and the expected output format.</p>
<p class="normal-one">You can easily implement this template as follows:</p>
<pre class="programlisting2"><code class="hljs-code">from langchain import PromptTemplate
template = """Sentence: {sentence}
Translation in {language}:"""
prompt = PromptTemplate(template=template, input_variables=["sentence", "language"])
print(prompt.format(sentence = "the cat is on the table", language = "spanish"))
</code></pre>
<p class="normal-one">Here is the output:</p>
<pre class="programlisting3"><code class="hljs-con">Sentence: the cat is on the table
Translation in spanish:
</code></pre>
<p class="normal-one">Generally speaking, prompt<a id="_idIndexMarker322" class="calibre3"/> templates tend to be agnostic <a id="_idIndexMarker323" class="calibre3"/>with respect to the LLM you might decide to use, and it is adaptable to both completion and chat models.</p>
<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1">A completion model is a type of LLM<a id="_idIndexMarker324" class="calibre3"/> that takes a text input and generates a text output, which is called a completion. The completion model tries to continue the prompt in a coherent and relevant way, according to the task and the data it was trained on. For example, a completion model can generate summaries, translations, stories, code, lyrics, and more, depending on the prompt.</p>
<p class="normal1">A chat model is a special kind of completion model that is designed to generate conversational responses. A chat model takes a list of messages as input, where each message has a role (either system, user, or assistant) and content. The chat model tries to generate a new message for the assistant role, based on the previous messages and the system instruction.</p>
<p class="normal1">The main difference between completion and chat models is that completion models expect a single text input as a prompt, while chat models expect a list of messages as input.</p>
</div>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Example selector</strong>: An example selector is a component<a id="_idIndexMarker325" class="calibre3"/> in LangChain that allows you to choose which examples to include in a prompt for a language model. A prompt is a text input that guides the language model to produce a desired output. Examples are pairs of inputs and outputs that demonstrate the task and the format of the output as follows:
        <pre class="programlisting3"><code class="hljs-con">{"prompt": "&lt;prompt text&gt;", "completion": "&lt;ideal generated text&gt;"}
</code></pre>
</li>
</ul>
<p class="normal-one">The idea recalls <a id="_idIndexMarker326" class="calibre3"/>the concept of few-shot learning we covered in <em class="italic">Chapter 1</em>.</p>
<p class="normal-one">LangChain offers the example<a id="_idIndexMarker327" class="calibre3"/> selector class called <code class="inlinecode">BaseExampleSelector</code><a id="_idIndexMarker328" class="calibre3"/> that you can import and modify<a id="_idIndexMarker329" class="calibre3"/> as you wish. You can find the API reference at <a href="https://platform.openai.com/account/api-keys" class="calibre3">https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/</a>.</p>
<h2 class="heading1" id="_idParaDest-76">Data connections</h2>
<p class="normal">Data connections refer to the building<a id="_idIndexMarker330" class="calibre3"/> blocks needed to retrieve<a id="_idIndexMarker331" class="calibre3"/> the additional non-parametric knowledge we want to provide the model with.</p>
<p class="normal1">The idea is to cover the typical flow of incorporating user-specific data into applications that are made of five main blocks, as illustrated in the following figure:</p>
<figure class="mediaobject"><img alt="data_connection_diagram" src="img/B21714_05_02.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 5.2: Incorporating user-specific knowledge into LLMs (source: <a href="https://python.langchain.com/docs/modules/data_connection/" class="calibre3">https://python.langchain.com/docs/modules/data_connection/</a>)</p>
<p class="normal1">Those blocks are addressed with the following LangChain tools:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Document loaders</strong>: They are in charge of loading<a id="_idIndexMarker332" class="calibre3"/> documents from different sources such as CSV, file directory, HTML, JSON, Markdown, and PDF. Document loaders expose a <code class="inlinecode">.load</code> method for loading data as documents from a configured source. The output is a <code class="inlinecode">Document</code> object that contains a piece of text and associated metadata.</li>
</ul>
<p class="normal-one">For example, let’s consider <a id="_idIndexMarker333" class="calibre3"/>a sample CSV file to be loaded (you can find the whole code in the book’s GitHub repository at <a href="Chapter_05.xhtml" class="calibre3">https://github.com/PacktPublishing/Building-LLM-Powered-Applications</a>):</p>
<pre class="programlisting2"><code class="hljs-code">from langchain.document_loaders.csv_loader import CSVLoader
loader = CSVLoader(file_path='sample.csv')
data = loader.load()
print(data)
</code></pre>
<p class="normal-one">Here is the output:</p>
<pre class="programlisting3"><code class="hljs-con">[Document(page_content='Name: John\nAge: 25\nCity: New York', metadata={'source': 'sample.csv', 'row': 0}), Document(page_content='Name: Emily\nAge: 28\nCity: Los Angeles', metadata={'source': 'sample.csv', 'row': 1}), Document(page_content='Name: Michael\nAge: 22\nCity: Chicago', metadata={'source': 'sample.csv', 'row': 2})]
</code></pre>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Document transformers</strong>: After importing your<a id="_idIndexMarker334" class="calibre3"/> documents, it’s common<a id="_idIndexMarker335" class="calibre3"/> to modify them to better<a id="_idIndexMarker336" class="calibre3"/> match your needs. A basic instance of this is breaking down a lengthy document into smaller chunks that fit your model’s context window. Within LangChain, there are various<a id="_idIndexMarker337" class="calibre3"/> pre-built document transformers available called <strong class="screentext">text splitters</strong>. The idea of text splitters is to make it easier to split documents into chunks that are semantically related so that we do not lose context or relevant information.</li>
</ul>
<p class="normal-one">With text splitters, you can decide how to split the text (for example, by character, heading, token, and so on) and how to measure the length of the chunk (for example, by number of characters).</p>
<p class="normal-one">For example, let’s split a document using the <code class="inlinecode">RecursiveCharacterTextSplitter</code> module, which operates at a character level. For this purpose, we will be using a <code class="inlinecode">.txt</code> file about mountains (you can find the whole code in the book’s GitHub repository at <a href="Chapter_05.xhtml" class="calibre3">https://github.com/PacktPublishing/Building-LLM-Powered-Applications</a>):</p>
<pre class="programlisting2"><code class="hljs-code">with open('mountain.txt') as f:
    mountain = f.read()
from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 100, #number of characters for each chunk
    chunk_overlap  = 20,#number of characters overlapping between a preceding and following chunk
    length_function = len #function used to measure the number of characters
)
texts = text_splitter.create_documents([mountain])
print(texts[0])
print(texts[1])
print(texts[2])
</code></pre>
<p class="normal-one">Here, <code class="inlinecode">chunk_size</code> refers to the number<a id="_idIndexMarker338" class="calibre3"/> of characters in each chunk<a id="_idIndexMarker339" class="calibre3"/> while <code class="inlinecode">chunk_overlap</code> represents the number<a id="_idIndexMarker340" class="calibre3"/> of characters overlapping between successive chunks. Here is the output:</p>
<pre class="programlisting3"><code class="hljs-con">page_content="Amidst the serene landscape, towering mountains stand as majestic guardians of nature's beauty." metadata={}
page_content='The crisp mountain air carries whispers of tranquility, while the rustling leaves compose a' metadata={}
</code></pre>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Text embedding models</strong>: In <em class="italic">Chapter 1</em>, in the <em class="italic">Under the hood of an LLM</em> section, we introduced<a id="_idIndexMarker341" class="calibre3"/> the concept of embedding as a way to represent words, subwords, or characters in a continuous vector space.</li>
</ul>
<p class="normal-one">Embeddings are the key step in incorporating non-parametric knowledge into LLMs. In fact, once properly stored in a VectorDB (which will be covered in the next section), they become the non-parametric knowledge against which we can measure the distance of a user’s query.</p>
<p class="normal-one">To get started with embedding, you will need an embedding model.</p>
<p class="normal-one">Then, LangChain offers the <code class="inlinecode">Embedding</code> class with two main modules, which address the embedding of, respectively, the non-parametric knowledge (multiple input text) and the user query (single input text).</p>
<p class="normal-one">For example, let’s consider<a id="_idIndexMarker342" class="calibre3"/> the embeddings<a id="_idIndexMarker343" class="calibre3"/> using the <strong class="screentext">OpenAI</strong><strong class="screentext"><a id="_idIndexMarker344" class="calibre3"/></strong> embedding model <code class="inlinecode">text-embedding-ada-002</code> (for more details about OpenAI embedding models, you can refer to<a id="_idIndexMarker345" class="calibre3"/> the official documentation at <a href="https://platform.openai.com/docs/guides/embeddings/what-are-embeddings" class="calibre3">https://platform.openai.com/docs/guides/embeddings/what-are-embeddings</a>):</p>
<pre class="programlisting2"><code class="hljs-code">from langchain.embeddings import OpenAIEmbeddings
from dotenv import load_dotenv
load_dotenv()
os.environ["OPENAI_API_KEY"]
embeddings_model = OpenAIEmbeddings(model ='text-embedding-ada-002' )
embeddings = embeddings_model.embed_documents(
    [
        "Good morning!",
        "Oh, hello!",
        "I want to report an accident",
        "Sorry to hear that. May I ask your name?",
        "Sure, Mario Rossi."
    ]
)
print("Embed documents:")
print(f"Number of vector: {len(embeddings)}; Dimension of each vector: {len(embeddings[0])}")
embedded_query = embeddings_model.embed_query("What was the name mentioned in the conversation?")
print("Embed query:")
print(f"Dimension of the vector: {len(embedded_query)}")
print(f"Sample of the first 5 elements of the vector: {embedded_query[:5]}")
</code></pre>
<p class="normal-one">Here is the output:</p>
<pre class="programlisting3"><code class="hljs-con">Embed documents:
Number of vector: 5; Dimension of each vector: 1536
Embed query:
Dimension of the vector: 1536
Sample of the first 5 elements of the vector: [0.00538721214979887, -0.0005941778072156012, 0.03892524912953377, -0.002979141427204013, -0.008912666700780392]
</code></pre>
<p class="normal-one">Once we have both documents<a id="_idIndexMarker346" class="calibre3"/> and the query<a id="_idIndexMarker347" class="calibre3"/> embedded, the next step will be to compute the similarity <a id="_idIndexMarker348" class="calibre3"/>between the two elements and retrieve the most suitable information from the document embedding. We will see the details of this when talking about vector stores.</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Vector stores</strong>: A vector store (or VectorDB) is a type of database<a id="_idIndexMarker349" class="calibre3"/> that can store and search over unstructured data, such as text, images, audio, or video, by using embeddings. By using embeddings, vector stores can perform a fast and accurate similarity search, which means finding the most relevant data for a given query.</li>
</ul>
<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1">Similarity is a measure of how<a id="_idIndexMarker350" class="calibre3"/> close or related two vectors are in a vector space. In the context of LLMs, vectors are numerical representations of sentences, words, or documents that capture their semantic meaning, and the distance between those vectors should be representative of their semantic similarity.</p>
<p class="normal1">There are different ways to measure similarity between vectors, and while working with LLMs, one of the most popular measures in use is cosine similarity.</p>
<p class="normal1">This is the cosine of the angle between two vectors in a multidimensional space. It is computed as the dot product of the vectors divided by the product of their lengths. Cosine similarity<a id="_idIndexMarker351" class="calibre3"/> is insensitive to scale and location, and it ranges from -1 to 1, where 1 means identical, 0 means orthogonal, and -1 means opposite.</p>
</div>
<p class="normal-one">The following is an illustration of the typical flow while using a vector store.</p>
<figure class="mediaobject"><img alt="vector store diagram" src="img/B21714_05_03.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 5.3: Sample architecture of a vector store (source: <a href="https://python.langchain.com/docs/modules/data_connection/vectorstores/" class="calibre3">https://python.langchain.com/docs/modules/data_connection/vectorstores/</a>)</p>
<p class="normal-one">LangChain offers more than 40 integrations<a id="_idIndexMarker352" class="calibre3"/> with third-party vector stores. Some examples are <strong class="screentext">Facebook AI Similarity Search</strong> (<strong class="screentext">FAISS</strong>), Elasticsearch, MongoDB Atlas, and Azure Search. For an exhaustive list and descriptions<a id="_idIndexMarker353" class="calibre3"/> of all the integrations, you can check the official documentation at <a href="https://python.langchain.com/docs/integrations/vectorstores/" class="calibre3">https://python.langchain.com/docs/integrations/vectorstores/</a>.</p>
<p class="normal-one">As an example, let’s leverage<a id="_idIndexMarker354" class="calibre3"/> the FAISS vector<a id="_idIndexMarker355" class="calibre3"/> store, which has been developed by Meta AI research for efficient similarity search and clustering of dense vectors. We are going to leverage the same <code class="inlinecode">dialogue.txt</code> file saved in the previous section:</p>
<pre class="programlisting2"><code class="hljs-code">from langchain.document_loaders import TextLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from dotenv import load_dotenv
load_dotenv()
os.environ["OPENAI_API_KEY"]
# Load the document, split it into chunks, embed each chunk and load it into the vector store.
raw_documents = TextLoader('dialogue.txt').load()
text_splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=0, separator = "\n",)
documents = text_splitter.split_documents(raw_documents)
db = FAISS.from_documents(documents, OpenAIEmbeddings())
</code></pre>
<p class="normal-one">Now that we’ve embedded<a id="_idIndexMarker356" class="calibre3"/> and saved the non-parametric<a id="_idIndexMarker357" class="calibre3"/> knowledge, let’s also embed a user’s query so that it can be used to search the most similar text chunk using cosine similarity as a measure:</p>
<pre class="programlisting2"><code class="hljs-code">query = "What is the reason for calling?"
docs = db.similarity_search(query)
print(docs[0].page_content)
</code></pre>
<p class="normal-one">The following is the output:</p>
<pre class="programlisting3"><code class="hljs-con">I want to report an accident
</code></pre>
<p class="normal-one">As you can see, the output is the piece of text that is more likely to contain the answer to the question. In an end-to-end scenario, it will be used as context to the LLM to generate a conversational response.</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Retrievers</strong>: A retriever is a component in LangChain<a id="_idIndexMarker358" class="calibre3"/> that can return documents relevant to an unstructured query, such as a natural language question or a keyword. A retriever does not need to store the documents itself, but only to retrieve them from a source. A retriever can use different methods to find relevant documents, such as keyword matching, semantic search, or ranking algorithms.</li>
</ul>
<p class="normal-one">The difference between a retriever and a vector store is that a retriever is more general and flexible than a vector store. A retriever can use any method to find relevant documents, while a vector store relies on embeddings and similarity metrics. A retriever can also use different sources of documents, such as web pages, databases, or files, while a vector store needs to store the data itself.</p>
<p class="normal-one">However, a vector store can also be used as the backbone of a retriever if the data is embedded and indexed by a vector store. In that case, the retriever can use the vector store to perform a similarity search over the embedded data and return the most<a id="_idIndexMarker359" class="calibre3"/> relevant documents. This is one of the main types of retrievers<a id="_idIndexMarker360" class="calibre3"/> in LangChain, and it is called a vector store<a id="_idIndexMarker361" class="calibre3"/> retriever.</p>
<p class="normal-one">For example, let’s consider the FAISS vector store we previously initialized and “mount” a retriever on top of that:</p>
<pre class="programlisting2"><code class="hljs-code">from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
retriever = db.as_retriever()
qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type="stuff", retriever=retriever)
query = "What was the reason of the call?"
qa.run(query)
</code></pre>
<p class="normal-one">Here is the output:</p>
<pre class="programlisting3"><code class="hljs-con">' The reason for the call was to report an accident.'
</code></pre>
<p class="normal1">Overall, data connection modules offer a plethora of integrations and pre-built templates that make it easier to manage the flow of your LLM-powered application. We will see some concrete applications of these building blocks in the upcoming chapters, but in the next section, we are going<a id="_idIndexMarker362" class="calibre3"/> to take a deep dive into another one of LangChain’s main components.</p>
<h2 class="heading1" id="_idParaDest-77">Memory</h2>
<p class="normal">In the context of LLM-powered applications, memory<a id="_idIndexMarker363" class="calibre3"/> allows the application to keep references to user interactions, both in the short and long term. For example, let’s consider the well-known ChatGPT. While interacting with the application, you have the possibility to ask follow-up questions referencing previous interactions without explicitly telling the model. </p>
<p class="normal1">Plus, all conversations are saved into threads, so that, if you want to follow up on a previous conversation, you can re-open the thread without providing ChatGPT with all the contexts. This is made possible thanks to ChatGPT’s ability to store users’ interactions into a memory variable and use this memory as context while addressing follow-up questions.</p>
<p class="normal1">LangChain offers several modules for designing your memory system within your applications, enabling it with both reading and writing skills.</p>
<p class="normal1">The first step to do with your memory<a id="_idIndexMarker364" class="calibre3"/> system is to actually store your human interactions somewhere. To do so, you can leverage numerous built-in memory integrations with third-party providers, including Redis, Cassandra, and Postgres.</p>
<p class="normal1">Then, when it comes to defining how to query your memory system, there are various memory types you can leverage:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Conversation buffer memory</strong>: This is the “plain vanilla” memory type available in <a id="_idIndexMarker365" class="calibre3"/>LangChain. It allows you to store your chat messages and extract them in a variable.</li>
<li class="bulletlist1"><strong class="screentext">Conversation buffer window memory</strong>: It is identical to the previous one, with the only difference<a id="_idIndexMarker366" class="calibre3"/> being allowing a sliding window over only <em class="italic">K</em> interactions so that you can manage longer chat history over time.</li>
<li class="bulletlist1"><strong class="screentext">Entity memory</strong>: Entity memory is a feature <a id="_idIndexMarker367" class="calibre3"/>of LangChain that allows the language model to remember given facts about specific entities in a conversation. An entity is a person, place, thing, or concept that can be identified and distinguished from others. For example, in the sentence “Deven and Sam are working on a hackathon in Italy,” Deven and Sam are entities (person), as well as hackathon (thing) and Italy (place).</li>
</ul>
<p class="normal-one">Entity memory works by extracting information on entities from the input text using an LLM. It then builds up its knowledge about that entity over time by storing the extracted facts in a memory store. The memory store can be accessed and updated by the language model whenever it needs to recall or learn new<a id="_idIndexMarker368" class="calibre3"/> information about an entity.</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Conversation knowledge graph memory</strong>: This type of memory uses a knowledge graph<a id="_idIndexMarker369" class="calibre3"/> to recreate memory.<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1">A knowledge graph is a way of representing<a id="_idIndexMarker370" class="calibre3"/> and organizing knowledge in a graph structure, where nodes are entities and edges are relationships between them. A knowledge graph can store and integrate data from various sources, and encode the semantics and context of the data. A knowledge graph can also support various tasks, such as search, question answering, reasoning, and generation.</p>
<p class="normal1">Another example of a knowledge graph is DBpedia, which is a community project that extracts structured data from Wikipedia and makes it available on the web. DBpedia covers topics such as geography, music, sports, and films, and provides links to other datasets like GeoNames and WordNet.</p>
</div>
</li>
</ul>
<p class="normal-one">You can use this type of memory to save the input and output of each conversation turn as knowledge triplets (such as subject, predicate, and object) and then use them to generate relevant and consistent responses based on the current context. You can also query the knowledge graph to get the current entities or the history of the conversation.</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Conversation summary memory</strong>: When it comes to longer <a id="_idIndexMarker371" class="calibre3"/>conversations to be stored, this type of memory can be very useful, since it creates a summary of the conversation over time (leveraging an LLM).</li>
<li class="bulletlist1"><strong class="screentext">Conversation summary buffer memory</strong>: This type of memory combines<a id="_idIndexMarker372" class="calibre3"/> the ideas behind buffer memory and conversation summary memory. It keeps a buffer of recent interactions in memory, but rather than just completely flushing old interactions (as occurs for the conversation buffer memory) it compiles them into a summary<a id="_idIndexMarker373" class="calibre3"/> and uses both.</li>
<li class="bulletlist1"><strong class="screentext">Conversation token buffer memory</strong>: It is similar to the previous one, with the difference<a id="_idIndexMarker374" class="calibre3"/> that, to determine when to start summarizing the interactions, this type of memory uses token lengths rather than the number of interactions (as occurs in summary buffer memory).</li>
<li class="bulletlist1"><strong class="screentext">Vector store-backed memory</strong>: This type of memory leverages<a id="_idIndexMarker375" class="calibre3"/> the concepts of embeddings and vector stores previously covered. It is different from all the previous memories since it stores interactions as vectors, and then retrieves the top <em class="italic">K</em> most similar texts every time it is queried, using a retriever.</li>
</ul>
<p class="normal1">LangChain provides specific modules<a id="_idIndexMarker376" class="calibre3"/> for each of those memory types. Let’s consider an example with the conversation summary memory, where we will also need an LLM to generate the summary of the interactions:</p>
<pre class="programlisting"><code class="hljs-code">from langchain.memory import ConversationSummaryMemory, ChatMessageHistory
from langchain.llms import OpenAI
memory = ConversationSummaryMemory(llm=OpenAI(temperature=0))
memory.save_context({"input": "hi, I'm looking for some ideas to write an essay in AI"}, {"output": "hello, what about writing on LLMs?"})
memory.load_memory_variables({})
</code></pre>
<p class="normal1">Here is the output:</p>
<pre class="programlisting1"><code class="hljs-con">{'history': '\nThe human asked for ideas to write an essay in AI and the AI suggested writing on LLMs.'}
</code></pre>
<p class="normal1">As you can see, the memory summarized the conversation, leveraging the <strong class="screentext">OpenAI</strong> LLM we initialized.</p>
<p class="normal1">There is no recipe to define which memory<a id="_idIndexMarker377" class="calibre3"/> to use within your applications; however, there are some scenarios that might be particularly suitable for specific memories. For example, a knowledge graph memory is useful for applications that need to access information from a large and diverse corpus of data and generate responses based on semantic relationships, while a conversation summary buffer memory could be suitable for creating conversational agents that can maintain a coherent and consistent context over multiple turns, while also being able to compress and summarize the previous dialogue history.</p>
<h2 class="heading1" id="_idParaDest-78">Chains</h2>
<p class="normal">Chains are predetermined sequences<a id="_idIndexMarker378" class="calibre3"/> of actions and calls to LLMs that make it easier to build complex applications that require combining LLMs with each other or with other components.</p>
<p class="normal1">LangChain offers four main types of chain to get started with:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">LLMChain</strong>: This is the most common <a id="_idIndexMarker379" class="calibre3"/>type of chain. It consists<a id="_idIndexMarker380" class="calibre3"/> of a prompt template, an LLM, and an optional <strong class="screentext">output parser</strong>.</li>
</ul>
<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1">An output parser is a component that helps<a id="_idIndexMarker381" class="calibre3"/> structure language model responses. It is a class that implements two main methods: <code class="inlinecode">get_format_instructions</code> and <code class="inlinecode">parse.</code> The <code class="inlinecode">get_format_instructions</code> method returns a string containing instructions for how the output of a language model should be formatted. The <code class="inlinecode">parse</code> method takes in a string (assumed to be the response from a language model) and parses it into some structure, such as a dictionary, a list, or a custom object.</p>
</div>
<p class="normal-one">This chain takes multiple input variables, uses <code class="inlinecode">PromptTemplate</code> to format them into a prompt, passes it to the model, and then uses <code class="inlinecode">OutputParser</code> (if provided) to parse the output of the LLM into a final format.</p>
<p class="normal-one">For example, let’s retrieve the prompt template we built in the previous section:</p>
<pre class="programlisting2"><code class="hljs-code">from langchain import PromptTemplate
template = """Sentence: {sentence}
Translation in {language}:"""
prompt = PromptTemplate(template=template, input_variables=["sentence", "language"])
</code></pre>
<p class="normal-one">Now, let’s put <a id="_idIndexMarker382" class="calibre3"/>it into an LLMChain:</p>
<pre class="programlisting2"><code class="hljs-code">from langchain import OpenAI, LLMChain
llm = OpenAI(temperature=0)
llm_chain = LLMChain(prompt=prompt, llm=llm)
llm_chain.predict(sentence="the cat is on the table", language="spanish")
</code></pre>
<p class="normal-one">Here is the<a id="_idIndexMarker383" class="calibre3"/> output:</p>
<pre class="programlisting3"><code class="hljs-con">' El gato está en la mesa.'
</code></pre>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">RouterChain</strong>: This is a type of chain<a id="_idIndexMarker384" class="calibre3"/> that allows you to route the input variables to different chains based on some conditions. You can specify the conditions as functions or expressions that return a Boolean value. You can also specify the default chain to use if none of the conditions are met.</li>
</ul>
<p class="normal-one">For example, you can use this chain to create a chatbot that can handle different types of requests, such as planning an itinerary or booking a restaurant reservation. To achieve this goal, you might want to differentiate two different prompts, depending on the type of query the user will make:</p>
<pre class="programlisting2"><code class="hljs-code">itinerary_template = """You are a vacation itinerary assistant. \
You help customers finding the best destinations and itinerary. \
You help customer screating an optimized itinerary based on their preferences.
Here is a question:
{input}"""
restaurant_template = """You are a restaurant booking assistant. \
You check with customers number of guests and food preferences. \
You pay attention whether there are special conditions to take into account.
Here is a question:
{input}"""
</code></pre>
<p class="normal-one">Thanks to RouterChain, we can build<a id="_idIndexMarker385" class="calibre3"/> a chain that is able to activate<a id="_idIndexMarker386" class="calibre3"/> a different prompt depending on the user’s query. I won’t post the whole code here (you can find the notebook on the book’s GitHub at <a href="Chapter_05.xhtml" class="calibre3">https://github.com/PacktPublishing/Building-LLM-Powered-Applications</a>), but you can see a sample output of how the chain reacts to two different user’s queries:</p>
<pre class="programlisting2"><code class="hljs-code">print(chain.run("I'm planning a trip from Milan to Venice by car. What can I visit in between?"))
</code></pre>
<p class="normal-one">Here is the output:</p>
<pre class="programlisting3"><code class="hljs-con">&gt; Entering new MultiPromptChain chain...
itinerary: {'input': "I'm planning a trip from Milan to Venice by car. What attractions can I visit in between?"}
&gt; Finished chain.
Answer:
There are many attractions that you can visit while traveling from Milan to Venice by car. Some of the most popular attractions include Lake Como, Verona, the Dolomites, and the picturesque towns of Bergamo and Brescia. You can also visit the stunning UNESCO World Heritage Sites in Mantua and Ferrara. Additionally, you can explore some of the local wineries and sample some of the wines of the region.
</code></pre>
<p class="normal-one">Here it is with a second query:</p>
<pre class="programlisting2"><code class="hljs-code">print(chain.run("I want to book a table for tonight"))
</code></pre>
<p class="normal-one">Here is the output:</p>
<pre class="programlisting3"><code class="hljs-con">&gt; Entering new MultiPromptChain chain...
restaurant: {'input': 'I want to book a table for tonight'}
&gt; Finished chain.
. How many people are in your party?
Hi there! How many people are in your party for tonight's reservation?
</code></pre>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">SequentialChain</strong>: This is a type of chain that allows<a id="_idIndexMarker387" class="calibre3"/> you to execute multiple chains<a id="_idIndexMarker388" class="calibre3"/> in a sequence. You can specify the order of the chains and how they pass their outputs to the next chain. The simplest module of a sequential chain, takes by default the output of one chain as the input of the next chain. However, you can also use a more complex module to have more flexibility to set input and output among chains.</li>
</ul>
<p class="normal-one">As an example, let’s consider an AI system that is meant to first generate a joke on a given topic, and then translate it in to another language. To do so, we will first create two chains:</p>
<pre class="programlisting2"><code class="hljs-code">from langchain.llms import OpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
llm = OpenAI(temperature=.7)
template = """You are a comedian. Generate a joke on the following {topic}
Joke:"""
prompt_template = PromptTemplate(input_variables=["topic"], template=template)
joke_chain = LLMChain(llm=llm, prompt=prompt_template)
template = """You are translator. Given a text input, translate it to {language}
Translation:"""
.prompt_template = PromptTemplate(input_variables=["language"], template=template)
translator_chain = LLMChain(llm=llm, prompt=prompt_template)
</code></pre>
<p class="normal-one">Now, let’s combine them using the <code class="inlinecode">SimpleSequentialChain</code> module:</p>
<pre class="programlisting2"><code class="hljs-code"># This is the overall chain where we run these two chains in sequence.
from langchain.chains import SimpleSequentialChain
overall_chain = SimpleSequentialChain(chains=[joke_chain, translator_chain], verbose=True)
translated_joke = overall_chain.run("Cats and Dogs")
</code></pre>
<p class="normal-one">Here is the output:</p>
<pre class="programlisting3"><code class="hljs-con">&gt; Entering new SimpleSequentialChain chain...
Why did the cat cross the road? To prove to the dog that it could be done!
 ¿Por qué cruzó el gato la carretera? ¡Para demostrarle al perro que se podía hacer!
&gt; Finished chain.
</code></pre>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">TransformationChain</strong>: This is a type of chain that allows <a id="_idIndexMarker389" class="calibre3"/>you to transform<a id="_idIndexMarker390" class="calibre3"/> the input variables or the output of another chain using some functions or expressions. You can specify the transformation as a function that takes the input or output as an argument and returns a new value, as well as specify the output format of the chain.</li>
</ul>
<p class="normal-one">For example, let’s say we want to summarize a text, but before that, we want to rename one of the protagonists of the story (a cat) as “Silvester the Cat.” As a sample text, I asked Bing Chat to generate a story about cats and dogs (you can find the whole <code class="inlinecode">.txt</code> file in the GitHub repository of this book):</p>
<pre class="programlisting2"><code class="hljs-code">from langchain.chains import TransformChain, LLMChain, SimpleSequentialChain
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
transform_chain = TransformChain(
    input_variables=["text"], output_variables=["output_text"], transform=rename_cat
)
template = """Summarize this text:
{output_text}
Summary:"""
prompt = PromptTemplate(input_variables=["output_text"], template=template)
llm_chain = LLMChain(llm=OpenAI(), prompt=prompt)
sequential_chain = SimpleSequentialChain(chains=[transform_chain, llm_chain])
sequential_chain.run(cats_and_dogs)
</code></pre>
<p class="normal-one">As you can see, we’ve combined a simple sequential chain with a transformation chain, where we set as a transformation function the <code class="inlinecode">rename_cat</code> function (you can see the whole code in the GitHub repository).</p>
<p class="normal-one">The output is the following:</p>
<pre class="programlisting3"><code class="hljs-con">" Silvester the Cat and a dog lived together but did not get along. Silvester the Cat played a prank on the dog which made him angry. When their owner found them fighting, she scolded them and made them apologize. After that, they became friends and learned to respect each other's differences and appreciate each other's strengths."
</code></pre>
<p class="normal1">Overall, LangChain chains are a powerful way to combine different language models and tasks into a single workflow. Chains are flexible, scalable, and easy to use, and they enable users to leverage the power of language models for various purposes and domains. Starting from the next chapter, we are going to see chains in action in concrete use cases, but before getting there, we need to cover the last component of LangChain: agents.</p>
<h2 class="heading1" id="_idParaDest-79">Agents</h2>
<p class="normal">Agents are entities that<a id="_idIndexMarker391" class="calibre3"/> drive decision-making<a id="_idIndexMarker392" class="calibre3"/> within LLM-powered applications. They have access to a suite of tools and can decide which tool to call based on the user input and the context. Agents are dynamic and adaptive, meaning that they can change or adjust their actions based on the situation or the goal: in fact, while in a chain, the sequence of actions is hardcoded, in agents, the LLM is used as the reasoning engine with the goal of planning and executing the right actions in the right order.</p>
<p class="normal1">A core concept while talking about agents is that of tools. In fact, an agent might be good at planning all the right actions to fulfill a user’s query, but what if it cannot actually execute them, since it is missing information or executive power? For example, imagine I want to build an agent that is capable of answering my questions by searching the web. By itself, the agent has no access to the web, so I need to provide it with this tool. I will do so by using<a id="_idIndexMarker393" class="calibre3"/> SerpApi (the Google Search API) integration provided by LangChain (you can retrieve your API key at <a href="https://serpapi.com/dashboard" class="calibre3">https://serpapi.com/dashboard</a>).</p>
<p class="normal1">Let’s see it in Python:</p>
<pre class="programlisting"><code class="hljs-code">from langchain import SerpAPIWrapper
from langchain.agents import AgentType, initialize_agent
from langchain.llms import OpenAI
from langchain.tools import BaseTool, StructuredTool, Tool, tool
import os
from dotenv import load_dotenv
load_dotenv()
os.environ["SERPAPI_API_KEY"]
search = SerpAPIWrapper()
tools = [Tool.from_function(
        func=search.run,
        name="Search",
        description="useful for when you need to answer questions about current events"
    )]
agent = initialize_agent(tools, llm = OpenAI(), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)
agent.run("When was Avatar 2 released?")
</code></pre>
<p class="normal1">The following <a id="_idIndexMarker394" class="calibre3"/>is the output:</p>
<pre class="programlisting1"><code class="hljs-con">&gt; Entering new AgentExecutor chain...
 I need to find out when Avatar 2 was released.
Action: Search
Action Input: "Avatar 2 release date"
Observation: December 16, 2022
Thought: I now know the final answer.
Final Answer: Avatar 2 was released on December 16, 2022.
&gt; Finished chain.
'Avatar 2 was released on December 16, 2022.'
</code></pre>
<p class="normal1">Note that, while initializing my agent, I set<a id="_idIndexMarker395" class="calibre3"/> the agent type as <code class="inlinecode">ZERO_SHOT_REACT_DESCRIPTION</code>. This is one<a id="_idIndexMarker396" class="calibre3"/> of the configurations we can pick and, specifically, it configures the agent to decide which tool to pick based solely on the tool’s description with a ReAct approach:</p>
<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1">The ReAct approach<a id="_idIndexMarker397" class="calibre3"/> is a way of using LLMs to solve various language reasoning and decision-making tasks. It was introduced in the paper <em class="italic">ReAct: Synergizing Reasoning and Acting in Language Models</em> by Shunyu Yao et al., back in October 2022.</p>
<p class="normal1">The ReAct approach prompts LLMs to generate both verbal reasoning traces and text actions in an interleaved manner, allowing for greater synergy between the two. Reasoning traces help the model to plan, track, and update its actions, as well as handle exceptions. Actions allow the model to interact with external sources, such as knowledge bases or environments, to gather additional information.</p>
</div>
<p class="normal1">On top of this configuration, LangChain also offers the following types of agents:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Structured input ReAct</strong>: This is an agent type that uses <a id="_idIndexMarker398" class="calibre3"/>the ReAct framework to generate natural language responses based on structured input data. The agent can handle different types of input data, such as tables, lists, or key-value pairs. The agent uses a language model and a prompt to generate responses that are informative, concise, and coherent.</li>
<li class="bulletlist1"><strong class="screentext">OpenAI Functions</strong>: This is an agent type that<a id="_idIndexMarker399" class="calibre3"/> uses the OpenAI Functions API to access various language models and tools from OpenAI. The agent can use different functions, such as GPT-3, Codex, DALL-E, CLIP, or ImageGPT. The agent uses a language model and a prompt to generate requests to the OpenAI Functions API and parse the responses.</li>
<li class="bulletlist1"><strong class="screentext">Conversational</strong>: This is an agent type that uses<a id="_idIndexMarker400" class="calibre3"/> a language model to engage in natural language conversations with the user. The agent can handle different types of conversational tasks, such as chit-chat, question answering, or task completion. The agent uses a language model and a prompt to generate responses that are relevant, fluent, and engaging.</li>
<li class="bulletlist1"><strong class="screentext">Self ask with search</strong>: This is an agent type<a id="_idIndexMarker401" class="calibre3"/> that uses a language model to generate questions for itself and then search for answers on the web. The agent can use this technique to learn new information or test its own knowledge.</li>
<li class="bulletlist1"><strong class="screentext">ReAct document store</strong>: This is an agent type that uses<a id="_idIndexMarker402" class="calibre3"/> the ReAct framework to generate natural language responses based on documents stored in a database. The agent can handle different types of documents, such as news articles, blog posts, or research papers.</li>
<li class="bulletlist1"><strong class="screentext">Plan-and-execute agents</strong>: This is an experimental agent<a id="_idIndexMarker403" class="calibre3"/> type that uses a language model to choose a sequence of actions to take based on the user’s input and a goal. The agent can use different tools or models to execute the actions it chooses. The agent uses a language model and a prompt to generate plans and actions and then uses <code class="inlinecode">AgentExecutor</code> to run them.</li>
</ul>
<p class="normal1">LangChain agents are pivotal whenever you want to let your LLMs interact with the external world. Plus, it is interesting to see how agents leverage LLMs not only to retrieve and generate responses, but also as reasoning engines to plan an optimized sequence of actions.</p>
<p class="normal1">Together with all the LangChain<a id="_idIndexMarker404" class="calibre3"/> components covered in this section, agents can be the core of LLM-powered applications, as we will see in the next chapters. In the next section, we are going to shift toward the world of open-source LLMs, introducing the Hugging Face Hub and its native integration with LangChain.</p>
<h1 class="heading" id="_idParaDest-80">Working with LLMs via the Hugging Face Hub</h1>
<p class="normal">Now that we are familiar<a id="_idIndexMarker405" class="calibre3"/> with LangChain components, it is time<a id="_idIndexMarker406" class="calibre3"/> to start using our LLMs. If you want to use open-source LLMs, leveraging the Hugging Face Hub integration is extremely versatile. In fact, with just one access token you can leverage all the open-source LLMs available in Hugging Face’s repositories.</p>
<p class="normal1">As it is a non-production scenario, I will be using the free Inference API; however, if you are meant to build production-ready applications, you can easily scale to the Inference Endpoint, which grants you a dedicated and fully managed infrastructure to host and consume your LLMs.</p>
<p class="normal1">So, let’s see how to start integrating LangChain with the Hugging Face Hub.</p>
<h2 class="heading1" id="_idParaDest-81">Create a Hugging Face user access token</h2>
<p class="normal">To access the free Inference API, you will need<a id="_idIndexMarker407" class="calibre3"/> a user access token, the credential that allows you to run the service. The following are the steps to activate the user access token:</p>
<ol class="calibre15">
<li class="bulletlist1" value="1"><strong class="screentext">Create a Hugging Face account</strong>: You can create<a id="_idIndexMarker408" class="calibre3"/> a Hugging Face account for free at <a href="https://huggingface.co/join" class="calibre3">https://huggingface.co/join</a>.</li>
<li class="bulletlist1"><strong class="screentext">Retrieve your user access token</strong>: Once you have your account, go to the upper-right corner of your profile and go to <strong class="screentext">Settings</strong> | <strong class="screentext">Access Tokens</strong>. From that tab, you will be able to copy your secret token and use it to access Hugging Face models.</li>
</ol>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="img/B21714_05_04.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 5.4: Retrieving access tokens from the Hugging Face account (source: <a href="https://huggingface.co/settings/tokens" class="calibre3">https://huggingface.co/settings/tokens</a>)</p>
<ol class="calibre15">
<li class="bulletlist1" value="3"><strong class="screentext">Set permissions</strong>: Access tokens<a id="_idIndexMarker409" class="calibre3"/> enable users, applications, and notebooks to perform specific actions based on their assigned roles. There are two available roles:<ul class="calibre17">
<li class="bulletlist2"><strong class="screentext">Read: </strong>This allows tokens to provide read access to repositories you have permission to read. This includes public and private repositories owned by you or your organization. This role is suitable for tasks like downloading private models or inference.</li>
<li class="bulletlist3"><strong class="screentext">Write: </strong>In addition to read access, tokens with this role grant write access to repositories where you have writing privileges. This token is useful for activities like training models or updating model cards.</li>
</ul>
</li>
</ol>
<p class="normal-one">In our series of use cases, we will keep a write permission on our token.</p>
<ol class="calibre15">
<li class="bulletlist1" value="4"><strong class="screentext">Managing your user access token</strong>: Within your profile, you can create and manage multiple access tokens, so that you can also differentiate permissions. To create a new token, you can click on the <strong class="screentext">New token</strong> button:</li>
</ol>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="img/B21714_05_05.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 5.5: Creating a new token</p>
<ol class="calibre15">
<li class="bulletlist1" value="5">Finally, at any time, you can delete <a id="_idIndexMarker410" class="calibre3"/>or refresh your token under the <strong class="screentext">Manage</strong> button:</li>
</ol>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="img/B21714_05_06.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 5.6: Managing tokens</p>
<p class="normal-one">It is important not to leak your token, and a good practice<a id="_idIndexMarker411" class="calibre3"/> is to periodically regenerate it.</p>
<h2 class="heading1" id="_idParaDest-82">Storing your secrets in an .env file</h2>
<p class="normal">With our user access token<a id="_idIndexMarker412" class="calibre3"/> generated in the previous<a id="_idIndexMarker413" class="calibre3"/> section, we have the first secret to be managed.</p>
<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1">Secrets are data that needs<a id="_idIndexMarker414" class="calibre3"/> to be protected from unauthorized access, such as passwords, tokens, keys, and credentials. Secrets are used to authenticate and authorize requests to API endpoints, as well as to encrypt and decrypt sensitive data.</p>
</div>
<p class="normal1">Throughout this hands-on portion of the book, we will keep all our secrets within an <code class="inlinecode">.env</code> file.</p>
<p class="normal1">Storing Python secrets in an <code class="inlinecode">.env</code> file is a common practice to enhance security and maintainability in projects. To do this, create a file named <code class="inlinecode">.env</code> in your project directory and list your sensitive information as key-value pairs: in our scenario, we will have <code class="inlinecode">HUGGINGFACEHUB_API_TOKEN="your_user_access_token"</code>. This file should be added to your project’s <code class="inlinecode">.gitignore</code> to prevent accidental exposure.</p>
<p class="normal1">To access these secrets in your Python code, use the <code class="inlinecode">python-dotenv</code> library to load the <code class="inlinecode">.env</code> file’s values as environment variables. You can easily install it in your terminal via <code class="inlinecode">pip install python-dotenv</code>.</p>
<p class="normal1">This approach keeps sensitive data separate from your code base and helps ensure that confidential information remains confidential throughout the development and deployment processes.</p>
<p class="normal1">Here, you can see an example of how to retrieve your access token and set it as an environmental variable:</p>
<pre class="programlisting"><code class="hljs-code">import os
from dotenv import load_dotenv
load_dotenv()
os.environ["HUGGINGFACEHUB_API_TOKEN"]
</code></pre>
<p class="normal1">Note that, by default, <code class="inlinecode">load_dotenv</code> will look for the <code class="inlinecode">.env</code> file in the current working directory; however, you can also specify the path to your secrets file:</p>
<pre class="programlisting"><code class="hljs-code">from dotenv import load_dotenv
from pathlib import Path
dotenv_path = Path('path/to/.env')
load_dotenv(dotenv_path=dotenv_path)
</code></pre>
<p class="normal1">Now that we have all the ingredients<a id="_idIndexMarker415" class="calibre3"/> to start coding, it is time to try out some<a id="_idIndexMarker416" class="calibre3"/> open-source LLMs.</p>
<h2 class="heading1" id="_idParaDest-83">Start using open-source LLMs</h2>
<p class="normal">The nice thing about the Hugging Face Hub<a id="_idIndexMarker417" class="calibre3"/> integration is that you can navigate its portal and decide, within the model catalog, what to use. Models are also clustered per category (<strong class="screentext">Computer Vision</strong>, <strong class="screentext">Natural Language Processing</strong>, <strong class="screentext">Audio</strong>, and so on) and, within each category, per capability (within <strong class="screentext">Natural Language Processing</strong>, we have summarization, classification, Q&amp;A, and so on), as shown in the following screenshot:</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="img/B21714_05_07.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 5.7: Home page of Hugging Face’s model catalog</p>
<p class="normal1">Since we are interested in LLMs, we will focus<a id="_idIndexMarker418" class="calibre3"/> on the text generation category. For this first experiment, let’s try Falcon LLM-7B:</p>
<pre class="programlisting"><code class="hljs-code">from langchain import HuggingFaceHub
repo_id = "tiiuae/falcon-7b-instruct" 
llm = HuggingFaceHub(
    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 1000}
)
print(llm("what was the first disney movie?"))
</code></pre>
<p class="normal1">Here is the corresponding output:</p>
<pre class="programlisting1"><code class="hljs-con">The first Disney movie was 'Snow White and the Seven Dwarfs'
</code></pre>
<p class="normal1">As you can see, with just a few lines of code, we integrated an LLM from the Hugging Face Hub. With analogous code, you can test and consume all the LLMs available in the Hub.</p>
<p class="normal1">Note that, throughout this book, we will be leveraging specific models for each application, both proprietary and open source. However, the idea is that you can use the model you prefer by simply initializing it as the main LLM and running the code as it is, simply changing the LangChain LLM integration. This is one of the main advantages of LLM-powered applications since you don’t have<a id="_idIndexMarker419" class="calibre3"/> to change the whole code to adapt to different LLMs.</p>
<h1 class="heading" id="_idParaDest-84">Summary</h1>
<p class="normal">In this chapter, we dove deeper into the fundamentals of LangChain, since it will be the AI orchestrator used in the upcoming chapters: we got familiar with LangChain components such as memory, agents, chains, and prompt templates. We also covered how to start integrating LangChain with the Hugging Face Hub and its model catalog, and how to use the available LLMs and start embedding them into your code.</p>
<p class="normal1">From now on, we will look at a series of concrete end-to-end use cases, starting from a semantic Q&amp;A search app, which we are going to develop in the next chapter.</p>
<h1 class="heading" id="_idParaDest-85">References</h1>
<ul class="calibre16">
<li class="bulletlist">LangChain’s integration with OpenAI – <a href="https://python.langchain.com/docs/integrations/llms/openai" class="calibre3">https://python.langchain.com/docs/integrations/llms/openai</a></li>
<li class="bulletlist1">LangChain’s prompt templates – <a href="https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/" class="calibre3">https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/</a></li>
<li class="bulletlist1">LangChain’s vector stores – <a href="https://python.langchain.com/docs/integrations/vectorstores/" class="calibre3">https://python.langchain.com/docs/integrations/vectorstores/</a></li>
<li class="bulletlist1">FAISS index – <a href="https://faiss.ai/" class="calibre3">https://faiss.ai/</a></li>
<li class="bulletlist1">LangChain’s chains – <a href="https://python.langchain.com/docs/modules/chains/" class="calibre3">https://python.langchain.com/docs/modules/chains/</a></li>
<li class="bulletlist1">ReAct approach – <a href="https://arxiv.org/abs/2210.03629" class="calibre3">https://arxiv.org/abs/2210.03629</a></li>
<li class="bulletlist1">LangChain’s agents – <a href="https://python.langchain.com/docs/modules/agents/agent_types/" class="calibre3">https://python.langchain.com/docs/modules/agents/agent_types/</a></li>
<li class="bulletlist1">Hugging Face documentation – <a href="https://huggingface.co/docs" class="calibre3">https://huggingface.co/docs</a></li>
<li class="bulletlist1">LangChain Expression Language (LCEL) – <a href="https://python.langchain.com/docs/expression_language/" class="calibre3">https://python.langchain.com/docs/expression_language/</a></li>
<li class="bulletlist1">LangChain stable version – <a href="https://blog.langchain.dev/langchain-v0-1-0/" class="calibre3">https://blog.langchain.dev/langchain-v0-1-0/</a></li>
</ul>
<h1 class="heading">Join our community on Discord</h1>
<p class="normal">Join our community’s Discord space for discussions with the author and other readers:</p>
<p class="normal1"><a href="https://packt.link/llm" class="calibre3">https://packt.link/llm</a></p>
<p class="normal1"><img alt="" role="presentation" src="img/QR_Code214329708533108046.png" class="calibre4"/></p>
</div>
</body></html>