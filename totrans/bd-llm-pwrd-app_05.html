<html><head></head><body>
<div class="calibre1" id="_idContainer113">
<h1 class="chapternumber"><span class="kobospan" id="kobo.1.1">5</span></h1>
<h1 class="chaptertitle" id="_idParaDest-71"><span class="kobospan" id="kobo.2.1">Embedding LLMs within Your Applications</span></h1>
<p class="normal"><span class="kobospan" id="kobo.3.1">This chapter kickstarts the hands-on portions</span><a id="_idIndexMarker302" class="calibre3"/><span class="kobospan" id="kobo.4.1"> of this book, focusing on how we can </span><strong class="screentext"><span class="kobospan" id="kobo.5.1">leverage large language models</span></strong><span class="kobospan" id="kobo.6.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.7.1">LLMs</span></strong><span class="kobospan" id="kobo.8.1">) to build powerful AI applications. </span><span class="kobospan" id="kobo.8.2">In fact, LLMs have introduced a whole new paradigm in software development, paving the way for new families of applications that have the peculiarity of making the communication between the user and the machine smooth and conversational. </span><span class="kobospan" id="kobo.8.3">Plus, those models enhanced existing applications, such as chatbots and recommendation systems, with their unique reasoning capabilities.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.9.1">Developing LLM-powered applications is becoming a key factor for enterprises to keep themselves competitive in the market, and this leads to the spreading of new libraries and frameworks that make it easier to embed LLMs within applications. </span><span class="kobospan" id="kobo.9.2">Some examples are Semantic Kernel, Haystack, LlamaIndex, and LangChain. </span><span class="kobospan" id="kobo.9.3">In this chapter, we are going to cover LangChain and use its modules to build hands-on examples. </span><span class="kobospan" id="kobo.9.4">By the end of this chapter, you will have the technical foundations to start developing your LLM-powered applications using LangChain and open-source Hugging Face models.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.10.1">In this chapter, we will cover the following topics:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.11.1">A brief note about LangChain</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.12.1">Getting started with LangChain</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.13.1">Working with LLMs via the Hugging Face Hub</span></li>
</ul>
<h1 class="heading" id="_idParaDest-72"><span class="kobospan" id="kobo.14.1">Technical requirements</span></h1>
<p class="normal"><span class="kobospan" id="kobo.15.1">To complete the hands-on sections of this chapter, the following prerequisites are needed:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.16.1">A Hugging Face account and user access token.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.17.1">An OpenAI account and user access token.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.18.1">Python 3.7.1 or later version.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.19.1">Python packages: Make sure to have the following Python packages installed: </span><code class="inlinecode"><span class="kobospan" id="kobo.20.1">langchain</span></code><span class="kobospan" id="kobo.21.1">, </span><code class="inlinecode"><span class="kobospan" id="kobo.22.1">python-dotenv</span></code><span class="kobospan" id="kobo.23.1">, </span><code class="inlinecode"><span class="kobospan" id="kobo.24.1">huggingface_hub</span></code><span class="kobospan" id="kobo.25.1">, </span><code class="inlinecode"><span class="kobospan" id="kobo.26.1">google-search-results</span></code><span class="kobospan" id="kobo.27.1">, </span><code class="inlinecode"><span class="kobospan" id="kobo.28.1">faiss</span></code><span class="kobospan" id="kobo.29.1">, and </span><code class="inlinecode"><span class="kobospan" id="kobo.30.1">tiktoken</span></code><span class="kobospan" id="kobo.31.1">. </span><span class="kobospan" id="kobo.31.2">Those can be easily installed via </span><code class="inlinecode"><span class="kobospan" id="kobo.32.1">pip install</span></code><span class="kobospan" id="kobo.33.1"> in your terminal.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.34.1">You can find all the code and examples used in this chapter in the book’s GitHub repository at </span><a href="Chapter_05.xhtml" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.35.1">https://github.com/PacktPublishing/Building-LLM-Powered-Applications</span></span></a></p>
<h1 class="heading" id="_idParaDest-73"><span class="kobospan" id="kobo.36.1">A brief note about LangChain</span></h1>
<p class="normal"><span class="kobospan" id="kobo.37.1">Just as generative AI has evolved </span><a id="_idIndexMarker303" class="calibre3"/><span class="kobospan" id="kobo.38.1">so rapidly over the last year, so has LangChain. </span><span class="kobospan" id="kobo.38.2">In the months between the writing of this book and its publication, the AI orchestrator has gone through massive changes. </span><span class="kobospan" id="kobo.38.3">The most remarkable traces back to January 2024, when the first stable version of LangChain was released, introducing a new organization of packages and libraries.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.39.1">It consists of the following:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.40.1">A core backbone where all the abstractions and runtime logic are stored</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.41.1">A layer of third-party integrations and components</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.42.1">A set of pre-built architectures and templates to leverage</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.43.1">A serving layer to consume chains as APIs</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.44.1">An observability layer to monitor your applications in the development, testing, and production stages</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.45.1">You can look at the architecture</span><a id="_idIndexMarker304" class="calibre3"/><span class="kobospan" id="kobo.46.1"> in greater detail at </span><a href="https://python.langchain.com/docs/get_started/introduction" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.47.1">https://python.langchain.com/docs/get_started/introduction</span></span></a><span class="kobospan" id="kobo.48.1">.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.49.1">There are three packages</span><a id="_idIndexMarker305" class="calibre3"/><span class="kobospan" id="kobo.50.1"> you can install to start using LangChain:</span></p>
<ul class="calibre14">
<li class="bulletlist"><code class="inlinecode"><span class="kobospan" id="kobo.51.1">langchain-core</span></code><span class="kobospan" id="kobo.52.1">: This contains the base abstractions and runtime for the whole LangChain ecosystem.</span></li>
<li class="bulletlist1"><code class="inlinecode"><span class="kobospan" id="kobo.53.1">langchain-experimental</span></code><span class="kobospan" id="kobo.54.1">: This holds experimental LangChain code, intended for research and experimental uses.</span></li>
<li class="bulletlist1"><code class="inlinecode"><span class="kobospan" id="kobo.55.1">langchain-community</span></code><span class="kobospan" id="kobo.56.1">: This contains</span><a id="_idIndexMarker306" class="calibre3"/><span class="kobospan" id="kobo.57.1"> all third-party integrations.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.58.1">On top of that, there are three additional packages that we’re not going to cover in this book, yet can be leveraged to monitor and maintain your LangChain applications:</span></p>
<ul class="calibre14">
<li class="bulletlist"><code class="inlinecode"><span class="kobospan" id="kobo.59.1">langserve</span></code><span class="kobospan" id="kobo.60.1">: LangServe is a tool that</span><a id="_idIndexMarker307" class="calibre3"/><span class="kobospan" id="kobo.61.1"> lets you deploy </span><strong class="screentext"><span class="kobospan" id="kobo.62.1">LangChain runnables and chains</span></strong><span class="kobospan" id="kobo.63.1"> as a REST API, making it easier to integrate LangChain applications into production environments.</span></li>
<li class="bulletlist1"><code class="inlinecode"><span class="kobospan" id="kobo.64.1">langsmith</span></code><span class="kobospan" id="kobo.65.1">: Think of LangSmith as an </span><strong class="screentext"><span class="kobospan" id="kobo.66.1">innovative testing framework</span></strong><span class="kobospan" id="kobo.67.1"> for evaluating language</span><a id="_idIndexMarker308" class="calibre3"/><span class="kobospan" id="kobo.68.1"> models and AI applications. </span><span class="kobospan" id="kobo.68.2">It helps visualize inputs and outputs at each step in the chain, aiding understanding and intuition during development.</span></li>
<li class="bulletlist1"><code class="inlinecode"><span class="kobospan" id="kobo.69.1">langchain-cli</span></code><span class="kobospan" id="kobo.70.1">: The </span><strong class="screentext"><span class="kobospan" id="kobo.71.1">official command-line interface</span></strong><span class="kobospan" id="kobo.72.1"> for LangChain, it facilitates interactions</span><a id="_idIndexMarker309" class="calibre3"/><span class="kobospan" id="kobo.73.1"> with LangChain projects, including template usage and quickstarts.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.74.1">Last but not least, LangChain introduced the </span><strong class="screentext"><span class="kobospan" id="kobo.75.1">LangChain Expression Language</span></strong><span class="kobospan" id="kobo.76.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.77.1">LCEL</span></strong><span class="kobospan" id="kobo.78.1">) to enhance the efficiency</span><a id="_idIndexMarker310" class="calibre3"/><span class="kobospan" id="kobo.79.1"> and flexibility of text processing tasks.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.80.1">Key features</span><a id="_idIndexMarker311" class="calibre3"/><span class="kobospan" id="kobo.81.1"> of LCEL include:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.82.1">Streaming asynchronous support</span></strong><span class="kobospan" id="kobo.83.1">: This allows for the efficient handling of data streams.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.84.1">Batch support</span></strong><span class="kobospan" id="kobo.85.1">: This enables processing data in batches.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.86.1">Parallel execution</span></strong><span class="kobospan" id="kobo.87.1">: This enhances performance by executing tasks concurrently.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.88.1">Retries and fallbacks</span></strong><span class="kobospan" id="kobo.89.1">: This ensures robustness by handling failures gracefully.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.90.1">Dynamically routing logic</span></strong><span class="kobospan" id="kobo.91.1">: This allows logic flow based on input and output.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.92.1">Message history</span></strong><span class="kobospan" id="kobo.93.1">: This keeps track of interactions for context-aware processing.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.94.1">We are not going to cover LCEL in this book; however, all the code samples can be converted into LCEL</span><a id="_idIndexMarker312" class="calibre3"/><span class="kobospan" id="kobo.95.1"> if you want to speed up your development and leverage its native integration with the end-to-end LangChain development stack.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.96.1">Important note</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.97.1">Before we start working with LangChain, it is important</span><a id="_idIndexMarker313" class="calibre3"/><span class="kobospan" id="kobo.98.1"> to note that all packages are versioned slightly differently, yet all releases are cut with high frequency by a maintainer with a clearer communication strategy for breaking changes.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.99.1">In the upcoming chapters, you will see some packages that have been moved, for example, to the </span><code class="inlinecode"><span class="kobospan" id="kobo.100.1">experimental</span></code><span class="kobospan" id="kobo.101.1"> package, meaning that they are more prone to experimental uses. </span><span class="kobospan" id="kobo.101.2">Similarly, some third-party integrations have been moved to the </span><code class="inlinecode"><span class="kobospan" id="kobo.102.1">community</span></code><span class="kobospan" id="kobo.103.1"> package.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.104.1">Starting from the next section, we are going to cover the backbone concepts – such as memory, VectorDB, and agents – that remain solid in the LangChain framework and, more generally, in the landscape of LLM development.</span></p>
<h1 class="heading" id="_idParaDest-74"><span class="kobospan" id="kobo.105.1">Getting started with LangChain</span></h1>
<p class="normal"><span class="kobospan" id="kobo.106.1">As introduced in </span><em class="italic"><span class="kobospan" id="kobo.107.1">Chapter 2</span></em><span class="kobospan" id="kobo.108.1">, LangChain is a lightweight</span><a id="_idIndexMarker314" class="calibre3"/><span class="kobospan" id="kobo.109.1"> framework meant to make it easier to integrate and orchestrate LLMs and their components within applications. </span><span class="kobospan" id="kobo.109.2">It is mainly Python based, yet it recently extended its support to JavaScript and TypeScript.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.110.1">In addition to LLM integration (which we will cover in an upcoming dedicated section), we saw that LangChain offers</span><a id="_idIndexMarker315" class="calibre3"/><span class="kobospan" id="kobo.111.1"> the following main components:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.112.1">Models and prompt templates</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.113.1">Data connections</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.114.1">Memory</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.115.1">Chains</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.116.1">Agents</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.117.1">These components are illustrated in the following diagram:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.118.1"><img alt="A screenshot of a computer  Description automatically generated" src="../Images/B21714_05_01.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.119.1">Figure 5.1: LangChain’s components</span></p>
<p class="normal1"><span class="kobospan" id="kobo.120.1">The next sections will take a deep dive into each of these components.</span></p>
<h2 class="heading1" id="_idParaDest-75"><span class="kobospan" id="kobo.121.1">Models and prompts</span></h2>
<p class="normal"><span class="kobospan" id="kobo.122.1">LangChain offers more</span><a id="_idIndexMarker316" class="calibre3"/><span class="kobospan" id="kobo.123.1"> than 50 integrations</span><a id="_idIndexMarker317" class="calibre3"/><span class="kobospan" id="kobo.124.1"> with third-party vendors and platforms, including </span><strong class="screentext"><span class="kobospan" id="kobo.125.1">OpenAI</span></strong><span class="kobospan" id="kobo.126.1">, Azure OpenAI, Databricks, and MosaicML, as well as the integration with the Hugging Face Hub and the world of open-source LLMs. </span><span class="kobospan" id="kobo.126.2">In </span><em class="italic"><span class="kobospan" id="kobo.127.1">Part 2</span></em><span class="kobospan" id="kobo.128.1"> of this book, we will be trying various LLMs, both proprietary and open-source, and leveraging LangChain’s integrations.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.129.1">Just to provide an example, let’s see how easy</span><a id="_idIndexMarker318" class="calibre3"/><span class="kobospan" id="kobo.130.1"> it is to consume the OpenAI GPT-3 model (you can retrieve your OpenAI API key at </span><a href="https://platform.openai.com/account/api-keys" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.131.1">https://platform.openai.com/account/api-keys</span></span></a><span class="kobospan" id="kobo.132.1">):</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.133.1">from</span></span><span class="kobospan" id="kobo.134.1"> langchain.llms </span><span class="hljs-keyword"><span class="kobospan" id="kobo.135.1">import</span></span><span class="kobospan" id="kobo.136.1"> OpenAI
llm = OpenAI(openai_api_key=</span><span class="hljs-string"><span class="kobospan" id="kobo.137.1">"your-api-key"</span></span><span class="kobospan" id="kobo.138.1">)
</span><span class="hljs-built_in"><span class="kobospan" id="kobo.139.1">print</span></span><span class="kobospan" id="kobo.140.1">(llm(</span><span class="hljs-string"><span class="kobospan" id="kobo.141.1">'tell me a joke'</span></span><span class="kobospan" id="kobo.142.1">))
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.143.1">Here is the corresponding output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.144.1">Q: What did one plate say to the other plate?
</span><span class="kobospan" id="kobo.144.2">A: Dinner's on me!
</span></code></pre>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.145.1">Note</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.146.1">While running examples with LLMs, the output will vary at each run, due to the stochasticity of the models themselves. </span><span class="kobospan" id="kobo.146.2">If you want to reduce the margin of variations in your output, you can make your model more “deterministic” by tuning the temperature hyperparameter. </span><span class="kobospan" id="kobo.146.3">This parameter ranges from 0 (deterministic) to 1 (stochastic).</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.147.1">By default, the </span><strong class="screentext"><span class="kobospan" id="kobo.148.1">OpenAI</span></strong><span class="kobospan" id="kobo.149.1"> module</span><a id="_idIndexMarker319" class="calibre3"/><span class="kobospan" id="kobo.150.1"> uses the </span><code class="inlinecode"><span class="kobospan" id="kobo.151.1">gpt-3.5-turbo-instruct</span></code><span class="kobospan" id="kobo.152.1"> as a model. </span><span class="kobospan" id="kobo.152.2">You can specify the model you want to use by passing the model’s name as a parameter.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.153.1">As said previously, we will dive deeper into LLMs in the next section; so, for now, let’s focus on prompts. </span><span class="kobospan" id="kobo.153.2">There</span><a id="_idIndexMarker320" class="calibre3"/><span class="kobospan" id="kobo.154.1"> are two main components related to LLM prompts and prompts design/engineering:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.155.1">Prompt templates</span></strong><span class="kobospan" id="kobo.156.1">: A prompt template is a component</span><a id="_idIndexMarker321" class="calibre3"/><span class="kobospan" id="kobo.157.1"> that defines how to generate a prompt for a language model. </span><span class="kobospan" id="kobo.157.2">It can include variables, placeholders, prefixes, suffixes, and other elements that can be customized according to the data and the task.</span></li>
</ul>
<p class="normal-one"><span class="kobospan" id="kobo.158.1">For example, suppose you want to use a language model to generate a translation from one language to another. </span><span class="kobospan" id="kobo.158.2">You can use a prompt template like this:</span></p>
<pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.159.1">Sentence: {sentence}
Translation </span><span class="hljs-keyword"><span class="kobospan" id="kobo.160.1">in</span></span><span class="kobospan" id="kobo.161.1"> {language}:
</span></code></pre>
<p class="normal-one"><code class="inlinecode"><span class="kobospan" id="kobo.162.1">{sentence}</span></code><span class="kobospan" id="kobo.163.1"> is a variable that will be replaced by the actual text. </span><code class="inlinecode"><span class="kobospan" id="kobo.164.1">Translation in {language}:</span></code><span class="kobospan" id="kobo.165.1"> is a prefix that indicates the task and the expected output format.</span></p>
<p class="normal-one"><span class="kobospan" id="kobo.166.1">You can easily implement this template as follows:</span></p>
<pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.167.1">from</span></span><span class="kobospan" id="kobo.168.1"> langchain </span><span class="hljs-keyword"><span class="kobospan" id="kobo.169.1">import</span></span><span class="kobospan" id="kobo.170.1"> PromptTemplate
template = </span><span class="hljs-string"><span class="kobospan" id="kobo.171.1">"""Sentence: {sentence}</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.172.1">Translation in {language}:"""</span></span><span class="kobospan" id="kobo.173.1">
prompt = PromptTemplate(template=template, input_variables=[</span><span class="hljs-string"><span class="kobospan" id="kobo.174.1">"sentence"</span></span><span class="kobospan" id="kobo.175.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.176.1">"language"</span></span><span class="kobospan" id="kobo.177.1">])
</span><span class="hljs-built_in"><span class="kobospan" id="kobo.178.1">print</span></span><span class="kobospan" id="kobo.179.1">(prompt.</span><span class="hljs-built_in"><span class="kobospan" id="kobo.180.1">format</span></span><span class="kobospan" id="kobo.181.1">(sentence = </span><span class="hljs-string"><span class="kobospan" id="kobo.182.1">"the cat is on the table"</span></span><span class="kobospan" id="kobo.183.1">, language = </span><span class="hljs-string"><span class="kobospan" id="kobo.184.1">"spanish"</span></span><span class="kobospan" id="kobo.185.1">))
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.186.1">Here is the output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.187.1">Sentence: the cat is on the table
Translation in spanish:
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.188.1">Generally speaking, prompt</span><a id="_idIndexMarker322" class="calibre3"/><span class="kobospan" id="kobo.189.1"> templates tend to be agnostic </span><a id="_idIndexMarker323" class="calibre3"/><span class="kobospan" id="kobo.190.1">with respect to the LLM you might decide to use, and it is adaptable to both completion and chat models.</span></p>
<div class="note-one">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.191.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.192.1">A completion model is a type of LLM</span><a id="_idIndexMarker324" class="calibre3"/><span class="kobospan" id="kobo.193.1"> that takes a text input and generates a text output, which is called a completion. </span><span class="kobospan" id="kobo.193.2">The completion model tries to continue the prompt in a coherent and relevant way, according to the task and the data it was trained on. </span><span class="kobospan" id="kobo.193.3">For example, a completion model can generate summaries, translations, stories, code, lyrics, and more, depending on the prompt.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.194.1">A chat model is a special kind of completion model that is designed to generate conversational responses. </span><span class="kobospan" id="kobo.194.2">A chat model takes a list of messages as input, where each message has a role (either system, user, or assistant) and content. </span><span class="kobospan" id="kobo.194.3">The chat model tries to generate a new message for the assistant role, based on the previous messages and the system instruction.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.195.1">The main difference between completion and chat models is that completion models expect a single text input as a prompt, while chat models expect a list of messages as input.</span></p>
</div>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.196.1">Example selector</span></strong><span class="kobospan" id="kobo.197.1">: An example selector is a component</span><a id="_idIndexMarker325" class="calibre3"/><span class="kobospan" id="kobo.198.1"> in LangChain that allows you to choose which examples to include in a prompt for a language model. </span><span class="kobospan" id="kobo.198.2">A prompt is a text input that guides the language model to produce a desired output. </span><span class="kobospan" id="kobo.198.3">Examples are pairs of inputs and outputs that demonstrate the task and the format of the output as follows:
        </span><pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.199.1">{"prompt": "&lt;prompt text&gt;", "completion": "&lt;ideal generated text&gt;"}
</span></code></pre>
</li>
</ul>
<p class="normal-one"><span class="kobospan" id="kobo.200.1">The idea recalls </span><a id="_idIndexMarker326" class="calibre3"/><span class="kobospan" id="kobo.201.1">the concept of few-shot learning we covered in </span><em class="italic"><span class="kobospan" id="kobo.202.1">Chapter 1</span></em><span class="kobospan" id="kobo.203.1">.</span></p>
<p class="normal-one"><span class="kobospan" id="kobo.204.1">LangChain offers the example</span><a id="_idIndexMarker327" class="calibre3"/><span class="kobospan" id="kobo.205.1"> selector class called </span><code class="inlinecode"><span class="kobospan" id="kobo.206.1">BaseExampleSelector</span></code><a id="_idIndexMarker328" class="calibre3"/><span class="kobospan" id="kobo.207.1"> that you can import and modify</span><a id="_idIndexMarker329" class="calibre3"/><span class="kobospan" id="kobo.208.1"> as you wish. </span><span class="kobospan" id="kobo.208.2">You can find the API reference at </span><a href="https://platform.openai.com/account/api-keys" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.209.1">https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/</span></span></a><span class="kobospan" id="kobo.210.1">.</span></p>
<h2 class="heading1" id="_idParaDest-76"><span class="kobospan" id="kobo.211.1">Data connections</span></h2>
<p class="normal"><span class="kobospan" id="kobo.212.1">Data connections refer to the building</span><a id="_idIndexMarker330" class="calibre3"/><span class="kobospan" id="kobo.213.1"> blocks needed to retrieve</span><a id="_idIndexMarker331" class="calibre3"/><span class="kobospan" id="kobo.214.1"> the additional non-parametric knowledge we want to provide the model with.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.215.1">The idea is to cover the typical flow of incorporating user-specific data into applications that are made of five main blocks, as illustrated in the following figure:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.216.1"><img alt="data_connection_diagram" src="../Images/B21714_05_02.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.217.1">Figure 5.2: Incorporating user-specific knowledge into LLMs (source: </span><a href="https://python.langchain.com/docs/modules/data_connection/" class="calibre3"><span class="kobospan" id="kobo.218.1">https://python.langchain.com/docs/modules/data_connection/</span></a><span class="kobospan" id="kobo.219.1">)</span></p>
<p class="normal1"><span class="kobospan" id="kobo.220.1">Those blocks are addressed with the following LangChain tools:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.221.1">Document loaders</span></strong><span class="kobospan" id="kobo.222.1">: They are in charge of loading</span><a id="_idIndexMarker332" class="calibre3"/><span class="kobospan" id="kobo.223.1"> documents from different sources such as CSV, file directory, HTML, JSON, Markdown, and PDF. </span><span class="kobospan" id="kobo.223.2">Document loaders expose a </span><code class="inlinecode"><span class="kobospan" id="kobo.224.1">.load</span></code><span class="kobospan" id="kobo.225.1"> method for loading data as documents from a configured source. </span><span class="kobospan" id="kobo.225.2">The output is a </span><code class="inlinecode"><span class="kobospan" id="kobo.226.1">Document</span></code><span class="kobospan" id="kobo.227.1"> object that contains a piece of text and associated metadata.</span></li>
</ul>
<p class="normal-one"><span class="kobospan" id="kobo.228.1">For example, let’s consider </span><a id="_idIndexMarker333" class="calibre3"/><span class="kobospan" id="kobo.229.1">a sample CSV file to be loaded (you can find the whole code in the book’s GitHub repository at </span><a href="Chapter_05.xhtml" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.230.1">https://github.com/PacktPublishing/Building-LLM-Powered-Applications</span></span></a><span class="kobospan" id="kobo.231.1">):</span></p>
<pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.232.1">from</span></span><span class="kobospan" id="kobo.233.1"> langchain.document_loaders.csv_loader </span><span class="hljs-keyword"><span class="kobospan" id="kobo.234.1">import</span></span><span class="kobospan" id="kobo.235.1"> CSVLoader
loader = CSVLoader(file_path=</span><span class="hljs-string"><span class="kobospan" id="kobo.236.1">'sample.csv'</span></span><span class="kobospan" id="kobo.237.1">)
data = loader.load()
</span><span class="hljs-built_in"><span class="kobospan" id="kobo.238.1">print</span></span><span class="kobospan" id="kobo.239.1">(data)
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.240.1">Here is the output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.241.1">[Document(page_content='Name: John\nAge: 25\nCity: New York', metadata={'source': 'sample.csv', 'row': 0}), Document(page_content='Name: Emily\nAge: 28\nCity: Los Angeles', metadata={'source': 'sample.csv', 'row': 1}), Document(page_content='Name: Michael\nAge: 22\nCity: Chicago', metadata={'source': 'sample.csv', 'row': 2})]
</span></code></pre>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.242.1">Document transformers</span></strong><span class="kobospan" id="kobo.243.1">: After importing your</span><a id="_idIndexMarker334" class="calibre3"/><span class="kobospan" id="kobo.244.1"> documents, it’s common</span><a id="_idIndexMarker335" class="calibre3"/><span class="kobospan" id="kobo.245.1"> to modify them to better</span><a id="_idIndexMarker336" class="calibre3"/><span class="kobospan" id="kobo.246.1"> match your needs. </span><span class="kobospan" id="kobo.246.2">A basic instance of this is breaking down a lengthy document into smaller chunks that fit your model’s context window. </span><span class="kobospan" id="kobo.246.3">Within LangChain, there are various</span><a id="_idIndexMarker337" class="calibre3"/><span class="kobospan" id="kobo.247.1"> pre-built document transformers available called </span><strong class="screentext"><span class="kobospan" id="kobo.248.1">text splitters</span></strong><span class="kobospan" id="kobo.249.1">. </span><span class="kobospan" id="kobo.249.2">The idea of text splitters is to make it easier to split documents into chunks that are semantically related so that we do not lose context or relevant information.</span></li>
</ul>
<p class="normal-one"><span class="kobospan" id="kobo.250.1">With text splitters, you can decide how to split the text (for example, by character, heading, token, and so on) and how to measure the length of the chunk (for example, by number of characters).</span></p>
<p class="normal-one"><span class="kobospan" id="kobo.251.1">For example, let’s split a document using the </span><code class="inlinecode"><span class="kobospan" id="kobo.252.1">RecursiveCharacterTextSplitter</span></code><span class="kobospan" id="kobo.253.1"> module, which operates at a character level. </span><span class="kobospan" id="kobo.253.2">For this purpose, we will be using a </span><code class="inlinecode"><span class="kobospan" id="kobo.254.1">.txt</span></code><span class="kobospan" id="kobo.255.1"> file about mountains (you can find the whole code in the book’s GitHub repository at </span><a href="Chapter_05.xhtml" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.256.1">https://github.com/PacktPublishing/Building-LLM-Powered-Applications</span></span></a><span class="kobospan" id="kobo.257.1">):</span></p>
<pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.258.1">with</span></span> <span class="hljs-built_in"><span class="kobospan" id="kobo.259.1">open</span></span><span class="kobospan" id="kobo.260.1">(</span><span class="hljs-string"><span class="kobospan" id="kobo.261.1">'mountain.txt'</span></span><span class="kobospan" id="kobo.262.1">) </span><span class="hljs-keyword"><span class="kobospan" id="kobo.263.1">as</span></span><span class="kobospan" id="kobo.264.1"> f:
    mountain = f.read()
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.265.1">from</span></span><span class="kobospan" id="kobo.266.1"> langchain.text_splitter </span><span class="hljs-keyword"><span class="kobospan" id="kobo.267.1">import</span></span><span class="kobospan" id="kobo.268.1"> RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = </span><span class="hljs-number"><span class="kobospan" id="kobo.269.1">100</span></span><span class="kobospan" id="kobo.270.1">, </span><span class="hljs-comment"><span class="kobospan" id="kobo.271.1">#number of characters for each chunk</span></span><span class="kobospan" id="kobo.272.1">
    chunk_overlap  = </span><span class="hljs-number"><span class="kobospan" id="kobo.273.1">20</span></span><span class="kobospan" id="kobo.274.1">,</span><span class="hljs-comment"><span class="kobospan" id="kobo.275.1">#number of characters overlapping between a preceding and following chunk</span></span><span class="kobospan" id="kobo.276.1">
    length_function = </span><span class="hljs-built_in"><span class="kobospan" id="kobo.277.1">len</span></span> <span class="hljs-comment"><span class="kobospan" id="kobo.278.1">#function used to measure the number of characters</span></span><span class="kobospan" id="kobo.279.1">
)
texts = text_splitter.create_documents([mountain])
</span><span class="hljs-built_in"><span class="kobospan" id="kobo.280.1">print</span></span><span class="kobospan" id="kobo.281.1">(texts[</span><span class="hljs-number"><span class="kobospan" id="kobo.282.1">0</span></span><span class="kobospan" id="kobo.283.1">])
</span><span class="hljs-built_in"><span class="kobospan" id="kobo.284.1">print</span></span><span class="kobospan" id="kobo.285.1">(texts[</span><span class="hljs-number"><span class="kobospan" id="kobo.286.1">1</span></span><span class="kobospan" id="kobo.287.1">])
</span><span class="hljs-built_in"><span class="kobospan" id="kobo.288.1">print</span></span><span class="kobospan" id="kobo.289.1">(texts[</span><span class="hljs-number"><span class="kobospan" id="kobo.290.1">2</span></span><span class="kobospan" id="kobo.291.1">])
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.292.1">Here, </span><code class="inlinecode"><span class="kobospan" id="kobo.293.1">chunk_size</span></code><span class="kobospan" id="kobo.294.1"> refers to the number</span><a id="_idIndexMarker338" class="calibre3"/><span class="kobospan" id="kobo.295.1"> of characters in each chunk</span><a id="_idIndexMarker339" class="calibre3"/><span class="kobospan" id="kobo.296.1"> while </span><code class="inlinecode"><span class="kobospan" id="kobo.297.1">chunk_overlap</span></code><span class="kobospan" id="kobo.298.1"> represents the number</span><a id="_idIndexMarker340" class="calibre3"/><span class="kobospan" id="kobo.299.1"> of characters overlapping between successive chunks. </span><span class="kobospan" id="kobo.299.2">Here is the output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.300.1">page_content="Amidst the serene landscape, towering mountains stand as majestic guardians of nature's beauty." </span><span class="kobospan" id="kobo.300.2">metadata={}
page_content='The crisp mountain air carries whispers of tranquility, while the rustling leaves compose a' metadata={}
</span></code></pre>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.301.1">Text embedding models</span></strong><span class="kobospan" id="kobo.302.1">: In </span><em class="italic"><span class="kobospan" id="kobo.303.1">Chapter 1</span></em><span class="kobospan" id="kobo.304.1">, in the </span><em class="italic"><span class="kobospan" id="kobo.305.1">Under the hood of an LLM</span></em><span class="kobospan" id="kobo.306.1"> section, we introduced</span><a id="_idIndexMarker341" class="calibre3"/><span class="kobospan" id="kobo.307.1"> the concept of embedding as a way to represent words, subwords, or characters in a continuous vector space.</span></li>
</ul>
<p class="normal-one"><span class="kobospan" id="kobo.308.1">Embeddings are the key step in incorporating non-parametric knowledge into LLMs. </span><span class="kobospan" id="kobo.308.2">In fact, once properly stored in a VectorDB (which will be covered in the next section), they become the non-parametric knowledge against which we can measure the distance of a user’s query.</span></p>
<p class="normal-one"><span class="kobospan" id="kobo.309.1">To get started with embedding, you will need an embedding model.</span></p>
<p class="normal-one"><span class="kobospan" id="kobo.310.1">Then, LangChain offers the </span><code class="inlinecode"><span class="kobospan" id="kobo.311.1">Embedding</span></code><span class="kobospan" id="kobo.312.1"> class with two main modules, which address the embedding of, respectively, the non-parametric knowledge (multiple input text) and the user query (single input text).</span></p>
<p class="normal-one"><span class="kobospan" id="kobo.313.1">For example, let’s consider</span><a id="_idIndexMarker342" class="calibre3"/><span class="kobospan" id="kobo.314.1"> the embeddings</span><a id="_idIndexMarker343" class="calibre3"/><span class="kobospan" id="kobo.315.1"> using the </span><strong class="screentext"><span class="kobospan" id="kobo.316.1">OpenAI</span></strong><strong class="screentext"><a id="_idIndexMarker344" class="calibre3"/></strong><span class="kobospan" id="kobo.317.1"> embedding model </span><code class="inlinecode"><span class="kobospan" id="kobo.318.1">text-embedding-ada-002</span></code><span class="kobospan" id="kobo.319.1"> (for more details about OpenAI embedding models, you can refer to</span><a id="_idIndexMarker345" class="calibre3"/><span class="kobospan" id="kobo.320.1"> the official documentation at </span><a href="https://platform.openai.com/docs/guides/embeddings/what-are-embeddings" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.321.1">https://platform.openai.com/docs/guides/embeddings/what-are-embeddings</span></span></a><span class="kobospan" id="kobo.322.1">):</span></p>
<pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.323.1">from</span></span><span class="kobospan" id="kobo.324.1"> langchain.embeddings </span><span class="hljs-keyword"><span class="kobospan" id="kobo.325.1">import</span></span><span class="kobospan" id="kobo.326.1"> OpenAIEmbeddings
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.327.1">from</span></span><span class="kobospan" id="kobo.328.1"> dotenv </span><span class="hljs-keyword"><span class="kobospan" id="kobo.329.1">import</span></span><span class="kobospan" id="kobo.330.1"> load_dotenv
load_dotenv()
os.environ[</span><span class="hljs-string"><span class="kobospan" id="kobo.331.1">"OPENAI_API_KEY"</span></span><span class="kobospan" id="kobo.332.1">]
embeddings_model = OpenAIEmbeddings(model =</span><span class="hljs-string"><span class="kobospan" id="kobo.333.1">'text-embedding-ada-002'</span></span><span class="kobospan" id="kobo.334.1"> )
embeddings = embeddings_model.embed_documents(
    [
        </span><span class="hljs-string"><span class="kobospan" id="kobo.335.1">"Good morning!"</span></span><span class="kobospan" id="kobo.336.1">,
        </span><span class="hljs-string"><span class="kobospan" id="kobo.337.1">"Oh, hello!"</span></span><span class="kobospan" id="kobo.338.1">,
        </span><span class="hljs-string"><span class="kobospan" id="kobo.339.1">"I want to report an accident"</span></span><span class="kobospan" id="kobo.340.1">,
        </span><span class="hljs-string"><span class="kobospan" id="kobo.341.1">"Sorry to hear that. </span><span class="kobospan" id="kobo.341.2">May I ask your name?"</span></span><span class="kobospan" id="kobo.342.1">,
        </span><span class="hljs-string"><span class="kobospan" id="kobo.343.1">"Sure, Mario Rossi."</span></span><span class="kobospan" id="kobo.344.1">
    ]
)
</span><span class="hljs-built_in"><span class="kobospan" id="kobo.345.1">print</span></span><span class="kobospan" id="kobo.346.1">(</span><span class="hljs-string"><span class="kobospan" id="kobo.347.1">"Embed documents:"</span></span><span class="kobospan" id="kobo.348.1">)
</span><span class="hljs-built_in"><span class="kobospan" id="kobo.349.1">print</span></span><span class="kobospan" id="kobo.350.1">(</span><span class="hljs-string"><span class="kobospan" id="kobo.351.1">f"Number of vector: </span></span><span class="hljs-subst"><span class="kobospan" id="kobo.352.1">{</span></span><span class="hljs-built_in"><span class="kobospan" id="kobo.353.1">len</span></span><span class="hljs-subst"><span class="kobospan" id="kobo.354.1">(embeddings)}</span></span><span class="hljs-string"><span class="kobospan" id="kobo.355.1">; Dimension of each vector: </span></span><span class="hljs-subst"><span class="kobospan" id="kobo.356.1">{</span></span><span class="hljs-built_in"><span class="kobospan" id="kobo.357.1">len</span></span><span class="hljs-subst"><span class="kobospan" id="kobo.358.1">(embeddings[</span></span><span class="hljs-number"><span class="kobospan" id="kobo.359.1">0</span></span><span class="hljs-subst"><span class="kobospan" id="kobo.360.1">])}</span></span><span class="hljs-string"><span class="kobospan" id="kobo.361.1">"</span></span><span class="kobospan" id="kobo.362.1">)
embedded_query = embeddings_model.embed_query(</span><span class="hljs-string"><span class="kobospan" id="kobo.363.1">"What was the name mentioned in the conversation?"</span></span><span class="kobospan" id="kobo.364.1">)
</span><span class="hljs-built_in"><span class="kobospan" id="kobo.365.1">print</span></span><span class="kobospan" id="kobo.366.1">(</span><span class="hljs-string"><span class="kobospan" id="kobo.367.1">"Embed query:"</span></span><span class="kobospan" id="kobo.368.1">)
</span><span class="hljs-built_in"><span class="kobospan" id="kobo.369.1">print</span></span><span class="kobospan" id="kobo.370.1">(</span><span class="hljs-string"><span class="kobospan" id="kobo.371.1">f"Dimension of the vector: </span></span><span class="hljs-subst"><span class="kobospan" id="kobo.372.1">{</span></span><span class="hljs-built_in"><span class="kobospan" id="kobo.373.1">len</span></span><span class="hljs-subst"><span class="kobospan" id="kobo.374.1">(embedded_query)}</span></span><span class="hljs-string"><span class="kobospan" id="kobo.375.1">"</span></span><span class="kobospan" id="kobo.376.1">)
</span><span class="hljs-built_in"><span class="kobospan" id="kobo.377.1">print</span></span><span class="kobospan" id="kobo.378.1">(</span><span class="hljs-string"><span class="kobospan" id="kobo.379.1">f"Sample of the first 5 elements of the vector: </span></span><span class="hljs-subst"><span class="kobospan" id="kobo.380.1">{embedded_query[:</span></span><span class="hljs-number"><span class="kobospan" id="kobo.381.1">5</span></span><span class="hljs-subst"><span class="kobospan" id="kobo.382.1">]}</span></span><span class="hljs-string"><span class="kobospan" id="kobo.383.1">"</span></span><span class="kobospan" id="kobo.384.1">)
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.385.1">Here is the output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.386.1">Embed documents:
Number of vector: 5; Dimension of each vector: 1536
Embed query:
Dimension of the vector: 1536
Sample of the first 5 elements of the vector: [0.00538721214979887, -0.0005941778072156012, 0.03892524912953377, -0.002979141427204013, -0.008912666700780392]
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.387.1">Once we have both documents</span><a id="_idIndexMarker346" class="calibre3"/><span class="kobospan" id="kobo.388.1"> and the query</span><a id="_idIndexMarker347" class="calibre3"/><span class="kobospan" id="kobo.389.1"> embedded, the next step will be to compute the similarity </span><a id="_idIndexMarker348" class="calibre3"/><span class="kobospan" id="kobo.390.1">between the two elements and retrieve the most suitable information from the document embedding. </span><span class="kobospan" id="kobo.390.2">We will see the details of this when talking about vector stores.</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.391.1">Vector stores</span></strong><span class="kobospan" id="kobo.392.1">: A vector store (or VectorDB) is a type of database</span><a id="_idIndexMarker349" class="calibre3"/><span class="kobospan" id="kobo.393.1"> that can store and search over unstructured data, such as text, images, audio, or video, by using embeddings. </span><span class="kobospan" id="kobo.393.2">By using embeddings, vector stores can perform a fast and accurate similarity search, which means finding the most relevant data for a given query.</span></li>
</ul>
<div class="note-one">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.394.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.395.1">Similarity is a measure of how</span><a id="_idIndexMarker350" class="calibre3"/><span class="kobospan" id="kobo.396.1"> close or related two vectors are in a vector space. </span><span class="kobospan" id="kobo.396.2">In the context of LLMs, vectors are numerical representations of sentences, words, or documents that capture their semantic meaning, and the distance between those vectors should be representative of their semantic similarity.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.397.1">There are different ways to measure similarity between vectors, and while working with LLMs, one of the most popular measures in use is cosine similarity.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.398.1">This is the cosine of the angle between two vectors in a multidimensional space. </span><span class="kobospan" id="kobo.398.2">It is computed as the dot product of the vectors divided by the product of their lengths. </span><span class="kobospan" id="kobo.398.3">Cosine similarity</span><a id="_idIndexMarker351" class="calibre3"/><span class="kobospan" id="kobo.399.1"> is insensitive to scale and location, and it ranges from -1 to 1, where 1 means identical, 0 means orthogonal, and -1 means opposite.</span></p>
</div>
<p class="normal-one"><span class="kobospan" id="kobo.400.1">The following is an illustration of the typical flow while using a vector store.</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.401.1"><img alt="vector store diagram" src="../Images/B21714_05_03.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.402.1">Figure 5.3: Sample architecture of a vector store (source: </span><a href="https://python.langchain.com/docs/modules/data_connection/vectorstores/" class="calibre3"><span class="kobospan" id="kobo.403.1">https://python.langchain.com/docs/modules/data_connection/vectorstores/</span></a><span class="kobospan" id="kobo.404.1">)</span></p>
<p class="normal-one"><span class="kobospan" id="kobo.405.1">LangChain offers more than 40 integrations</span><a id="_idIndexMarker352" class="calibre3"/><span class="kobospan" id="kobo.406.1"> with third-party vector stores. </span><span class="kobospan" id="kobo.406.2">Some examples are </span><strong class="screentext"><span class="kobospan" id="kobo.407.1">Facebook AI Similarity Search</span></strong><span class="kobospan" id="kobo.408.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.409.1">FAISS</span></strong><span class="kobospan" id="kobo.410.1">), Elasticsearch, MongoDB Atlas, and Azure Search. </span><span class="kobospan" id="kobo.410.2">For an exhaustive list and descriptions</span><a id="_idIndexMarker353" class="calibre3"/><span class="kobospan" id="kobo.411.1"> of all the integrations, you can check the official documentation at </span><a href="https://python.langchain.com/docs/integrations/vectorstores/" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.412.1">https://python.langchain.com/docs/integrations/vectorstores/</span></span></a><span class="kobospan" id="kobo.413.1">.</span></p>
<p class="normal-one"><span class="kobospan" id="kobo.414.1">As an example, let’s leverage</span><a id="_idIndexMarker354" class="calibre3"/><span class="kobospan" id="kobo.415.1"> the FAISS vector</span><a id="_idIndexMarker355" class="calibre3"/><span class="kobospan" id="kobo.416.1"> store, which has been developed by Meta AI research for efficient similarity search and clustering of dense vectors. </span><span class="kobospan" id="kobo.416.2">We are going to leverage the same </span><code class="inlinecode"><span class="kobospan" id="kobo.417.1">dialogue.txt</span></code><span class="kobospan" id="kobo.418.1"> file saved in the previous section:</span></p>
<pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.419.1">from</span></span><span class="kobospan" id="kobo.420.1"> langchain.document_loaders </span><span class="hljs-keyword"><span class="kobospan" id="kobo.421.1">import</span></span><span class="kobospan" id="kobo.422.1"> TextLoader
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.423.1">from</span></span><span class="kobospan" id="kobo.424.1"> langchain.embeddings.openai </span><span class="hljs-keyword"><span class="kobospan" id="kobo.425.1">import</span></span><span class="kobospan" id="kobo.426.1"> OpenAIEmbeddings
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.427.1">from</span></span><span class="kobospan" id="kobo.428.1"> langchain.text_splitter </span><span class="hljs-keyword"><span class="kobospan" id="kobo.429.1">import</span></span><span class="kobospan" id="kobo.430.1"> CharacterTextSplitter
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.431.1">from</span></span><span class="kobospan" id="kobo.432.1"> langchain.vectorstores </span><span class="hljs-keyword"><span class="kobospan" id="kobo.433.1">import</span></span><span class="kobospan" id="kobo.434.1"> FAISS
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.435.1">from</span></span><span class="kobospan" id="kobo.436.1"> dotenv </span><span class="hljs-keyword"><span class="kobospan" id="kobo.437.1">import</span></span><span class="kobospan" id="kobo.438.1"> load_dotenv
load_dotenv()
os.environ[</span><span class="hljs-string"><span class="kobospan" id="kobo.439.1">"OPENAI_API_KEY"</span></span><span class="kobospan" id="kobo.440.1">]
</span><span class="hljs-comment"><span class="kobospan" id="kobo.441.1"># Load the document, split it into chunks, embed each chunk and load it into the vector store.</span></span><span class="kobospan" id="kobo.442.1">
raw_documents = TextLoader(</span><span class="hljs-string"><span class="kobospan" id="kobo.443.1">'dialogue.txt'</span></span><span class="kobospan" id="kobo.444.1">).load()
text_splitter = CharacterTextSplitter(chunk_size=</span><span class="hljs-number"><span class="kobospan" id="kobo.445.1">50</span></span><span class="kobospan" id="kobo.446.1">, chunk_overlap=</span><span class="hljs-number"><span class="kobospan" id="kobo.447.1">0</span></span><span class="kobospan" id="kobo.448.1">, separator = </span><span class="hljs-string"><span class="kobospan" id="kobo.449.1">"\n"</span></span><span class="kobospan" id="kobo.450.1">,)
documents = text_splitter.split_documents(raw_documents)
db = FAISS.from_documents(documents, OpenAIEmbeddings())
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.451.1">Now that we’ve embedded</span><a id="_idIndexMarker356" class="calibre3"/><span class="kobospan" id="kobo.452.1"> and saved the non-parametric</span><a id="_idIndexMarker357" class="calibre3"/><span class="kobospan" id="kobo.453.1"> knowledge, let’s also embed a user’s query so that it can be used to search the most similar text chunk using cosine similarity as a measure:</span></p>
<pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.454.1">query = </span><span class="hljs-string"><span class="kobospan" id="kobo.455.1">"What is the reason for calling?"</span></span><span class="kobospan" id="kobo.456.1">
docs = db.similarity_search(query)
</span><span class="hljs-built_in"><span class="kobospan" id="kobo.457.1">print</span></span><span class="kobospan" id="kobo.458.1">(docs[</span><span class="hljs-number"><span class="kobospan" id="kobo.459.1">0</span></span><span class="kobospan" id="kobo.460.1">].page_content)
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.461.1">The following is the output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.462.1">I want to report an accident
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.463.1">As you can see, the output is the piece of text that is more likely to contain the answer to the question. </span><span class="kobospan" id="kobo.463.2">In an end-to-end scenario, it will be used as context to the LLM to generate a conversational response.</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.464.1">Retrievers</span></strong><span class="kobospan" id="kobo.465.1">: A retriever is a component in LangChain</span><a id="_idIndexMarker358" class="calibre3"/><span class="kobospan" id="kobo.466.1"> that can return documents relevant to an unstructured query, such as a natural language question or a keyword. </span><span class="kobospan" id="kobo.466.2">A retriever does not need to store the documents itself, but only to retrieve them from a source. </span><span class="kobospan" id="kobo.466.3">A retriever can use different methods to find relevant documents, such as keyword matching, semantic search, or ranking algorithms.</span></li>
</ul>
<p class="normal-one"><span class="kobospan" id="kobo.467.1">The difference between a retriever and a vector store is that a retriever is more general and flexible than a vector store. </span><span class="kobospan" id="kobo.467.2">A retriever can use any method to find relevant documents, while a vector store relies on embeddings and similarity metrics. </span><span class="kobospan" id="kobo.467.3">A retriever can also use different sources of documents, such as web pages, databases, or files, while a vector store needs to store the data itself.</span></p>
<p class="normal-one"><span class="kobospan" id="kobo.468.1">However, a vector store can also be used as the backbone of a retriever if the data is embedded and indexed by a vector store. </span><span class="kobospan" id="kobo.468.2">In that case, the retriever can use the vector store to perform a similarity search over the embedded data and return the most</span><a id="_idIndexMarker359" class="calibre3"/><span class="kobospan" id="kobo.469.1"> relevant documents. </span><span class="kobospan" id="kobo.469.2">This is one of the main types of retrievers</span><a id="_idIndexMarker360" class="calibre3"/><span class="kobospan" id="kobo.470.1"> in LangChain, and it is called a vector store</span><a id="_idIndexMarker361" class="calibre3"/><span class="kobospan" id="kobo.471.1"> retriever.</span></p>
<p class="normal-one"><span class="kobospan" id="kobo.472.1">For example, let’s consider the FAISS vector store we previously initialized and “mount” a retriever on top of that:</span></p>
<pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.473.1">from</span></span><span class="kobospan" id="kobo.474.1"> langchain.chains </span><span class="hljs-keyword"><span class="kobospan" id="kobo.475.1">import</span></span><span class="kobospan" id="kobo.476.1"> RetrievalQA
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.477.1">from</span></span><span class="kobospan" id="kobo.478.1"> langchain.llms </span><span class="hljs-keyword"><span class="kobospan" id="kobo.479.1">import</span></span><span class="kobospan" id="kobo.480.1"> OpenAI
retriever = db.as_retriever()
qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=</span><span class="hljs-string"><span class="kobospan" id="kobo.481.1">"stuff"</span></span><span class="kobospan" id="kobo.482.1">, retriever=retriever)
query = </span><span class="hljs-string"><span class="kobospan" id="kobo.483.1">"What was the reason of the call?"</span></span><span class="kobospan" id="kobo.484.1">
qa.run(query)
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.485.1">Here is the output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.486.1">' The reason for the call was to report an accident.'
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.487.1">Overall, data connection modules offer a plethora of integrations and pre-built templates that make it easier to manage the flow of your LLM-powered application. </span><span class="kobospan" id="kobo.487.2">We will see some concrete applications of these building blocks in the upcoming chapters, but in the next section, we are going</span><a id="_idIndexMarker362" class="calibre3"/><span class="kobospan" id="kobo.488.1"> to take a deep dive into another one of LangChain’s main components.</span></p>
<h2 class="heading1" id="_idParaDest-77"><span class="kobospan" id="kobo.489.1">Memory</span></h2>
<p class="normal"><span class="kobospan" id="kobo.490.1">In the context of LLM-powered applications, memory</span><a id="_idIndexMarker363" class="calibre3"/><span class="kobospan" id="kobo.491.1"> allows the application to keep references to user interactions, both in the short and long term. </span><span class="kobospan" id="kobo.491.2">For example, let’s consider the well-known ChatGPT. </span><span class="kobospan" id="kobo.491.3">While interacting with the application, you have the possibility to ask follow-up questions referencing previous interactions without explicitly telling the model. </span></p>
<p class="normal1"><span class="kobospan" id="kobo.492.1">Plus, all conversations are saved into threads, so that, if you want to follow up on a previous conversation, you can re-open the thread without providing ChatGPT with all the contexts. </span><span class="kobospan" id="kobo.492.2">This is made possible thanks to ChatGPT’s ability to store users’ interactions into a memory variable and use this memory as context while addressing follow-up questions.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.493.1">LangChain offers several modules for designing your memory system within your applications, enabling it with both reading and writing skills.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.494.1">The first step to do with your memory</span><a id="_idIndexMarker364" class="calibre3"/><span class="kobospan" id="kobo.495.1"> system is to actually store your human interactions somewhere. </span><span class="kobospan" id="kobo.495.2">To do so, you can leverage numerous built-in memory integrations with third-party providers, including Redis, Cassandra, and Postgres.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.496.1">Then, when it comes to defining how to query your memory system, there are various memory types you can leverage:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.497.1">Conversation buffer memory</span></strong><span class="kobospan" id="kobo.498.1">: This is the “plain vanilla” memory type available in </span><a id="_idIndexMarker365" class="calibre3"/><span class="kobospan" id="kobo.499.1">LangChain. </span><span class="kobospan" id="kobo.499.2">It allows you to store your chat messages and extract them in a variable.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.500.1">Conversation buffer window memory</span></strong><span class="kobospan" id="kobo.501.1">: It is identical to the previous one, with the only difference</span><a id="_idIndexMarker366" class="calibre3"/><span class="kobospan" id="kobo.502.1"> being allowing a sliding window over only </span><em class="italic"><span class="kobospan" id="kobo.503.1">K</span></em><span class="kobospan" id="kobo.504.1"> interactions so that you can manage longer chat history over time.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.505.1">Entity memory</span></strong><span class="kobospan" id="kobo.506.1">: Entity memory is a feature </span><a id="_idIndexMarker367" class="calibre3"/><span class="kobospan" id="kobo.507.1">of LangChain that allows the language model to remember given facts about specific entities in a conversation. </span><span class="kobospan" id="kobo.507.2">An entity is a person, place, thing, or concept that can be identified and distinguished from others. </span><span class="kobospan" id="kobo.507.3">For example, in the sentence “Deven and Sam are working on a hackathon in Italy,” Deven and Sam are entities (person), as well as hackathon (thing) and Italy (place).</span></li>
</ul>
<p class="normal-one"><span class="kobospan" id="kobo.508.1">Entity memory works by extracting information on entities from the input text using an LLM. </span><span class="kobospan" id="kobo.508.2">It then builds up its knowledge about that entity over time by storing the extracted facts in a memory store. </span><span class="kobospan" id="kobo.508.3">The memory store can be accessed and updated by the language model whenever it needs to recall or learn new</span><a id="_idIndexMarker368" class="calibre3"/><span class="kobospan" id="kobo.509.1"> information about an entity.</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.510.1">Conversation knowledge graph memory</span></strong><span class="kobospan" id="kobo.511.1">: This type of memory uses a knowledge graph</span><a id="_idIndexMarker369" class="calibre3"/><span class="kobospan" id="kobo.512.1"> to recreate memory.</span><div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.513.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.514.1">A knowledge graph is a way of representing</span><a id="_idIndexMarker370" class="calibre3"/><span class="kobospan" id="kobo.515.1"> and organizing knowledge in a graph structure, where nodes are entities and edges are relationships between them. </span><span class="kobospan" id="kobo.515.2">A knowledge graph can store and integrate data from various sources, and encode the semantics and context of the data. </span><span class="kobospan" id="kobo.515.3">A knowledge graph can also support various tasks, such as search, question answering, reasoning, and generation.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.516.1">Another example of a knowledge graph is DBpedia, which is a community project that extracts structured data from Wikipedia and makes it available on the web. </span><span class="kobospan" id="kobo.516.2">DBpedia covers topics such as geography, music, sports, and films, and provides links to other datasets like GeoNames and WordNet.</span></p>
</div>
</li>
</ul>
<p class="normal-one"><span class="kobospan" id="kobo.517.1">You can use this type of memory to save the input and output of each conversation turn as knowledge triplets (such as subject, predicate, and object) and then use them to generate relevant and consistent responses based on the current context. </span><span class="kobospan" id="kobo.517.2">You can also query the knowledge graph to get the current entities or the history of the conversation.</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.518.1">Conversation summary memory</span></strong><span class="kobospan" id="kobo.519.1">: When it comes to longer </span><a id="_idIndexMarker371" class="calibre3"/><span class="kobospan" id="kobo.520.1">conversations to be stored, this type of memory can be very useful, since it creates a summary of the conversation over time (leveraging an LLM).</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.521.1">Conversation summary buffer memory</span></strong><span class="kobospan" id="kobo.522.1">: This type of memory combines</span><a id="_idIndexMarker372" class="calibre3"/><span class="kobospan" id="kobo.523.1"> the ideas behind buffer memory and conversation summary memory. </span><span class="kobospan" id="kobo.523.2">It keeps a buffer of recent interactions in memory, but rather than just completely flushing old interactions (as occurs for the conversation buffer memory) it compiles them into a summary</span><a id="_idIndexMarker373" class="calibre3"/><span class="kobospan" id="kobo.524.1"> and uses both.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.525.1">Conversation token buffer memory</span></strong><span class="kobospan" id="kobo.526.1">: It is similar to the previous one, with the difference</span><a id="_idIndexMarker374" class="calibre3"/><span class="kobospan" id="kobo.527.1"> that, to determine when to start summarizing the interactions, this type of memory uses token lengths rather than the number of interactions (as occurs in summary buffer memory).</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.528.1">Vector store-backed memory</span></strong><span class="kobospan" id="kobo.529.1">: This type of memory leverages</span><a id="_idIndexMarker375" class="calibre3"/><span class="kobospan" id="kobo.530.1"> the concepts of embeddings and vector stores previously covered. </span><span class="kobospan" id="kobo.530.2">It is different from all the previous memories since it stores interactions as vectors, and then retrieves the top </span><em class="italic"><span class="kobospan" id="kobo.531.1">K</span></em><span class="kobospan" id="kobo.532.1"> most similar texts every time it is queried, using a retriever.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.533.1">LangChain provides specific modules</span><a id="_idIndexMarker376" class="calibre3"/><span class="kobospan" id="kobo.534.1"> for each of those memory types. </span><span class="kobospan" id="kobo.534.2">Let’s consider an example with the conversation summary memory, where we will also need an LLM to generate the summary of the interactions:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.535.1">from</span></span><span class="kobospan" id="kobo.536.1"> langchain.memory </span><span class="hljs-keyword"><span class="kobospan" id="kobo.537.1">import</span></span><span class="kobospan" id="kobo.538.1"> ConversationSummaryMemory, ChatMessageHistory
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.539.1">from</span></span><span class="kobospan" id="kobo.540.1"> langchain.llms </span><span class="hljs-keyword"><span class="kobospan" id="kobo.541.1">import</span></span><span class="kobospan" id="kobo.542.1"> OpenAI
memory = ConversationSummaryMemory(llm=OpenAI(temperature=</span><span class="hljs-number"><span class="kobospan" id="kobo.543.1">0</span></span><span class="kobospan" id="kobo.544.1">))
memory.save_context({</span><span class="hljs-string"><span class="kobospan" id="kobo.545.1">"input"</span></span><span class="kobospan" id="kobo.546.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.547.1">"hi, I'm looking for some ideas to write an essay in AI"</span></span><span class="kobospan" id="kobo.548.1">}, {</span><span class="hljs-string"><span class="kobospan" id="kobo.549.1">"output"</span></span><span class="kobospan" id="kobo.550.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.551.1">"hello, what about writing on LLMs?"</span></span><span class="kobospan" id="kobo.552.1">})
memory.load_memory_variables({})
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.553.1">Here is the output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.554.1">{'history': '\nThe human asked for ideas to write an essay in AI and the AI suggested writing on LLMs.'}
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.555.1">As you can see, the memory summarized the conversation, leveraging the </span><strong class="screentext"><span class="kobospan" id="kobo.556.1">OpenAI</span></strong><span class="kobospan" id="kobo.557.1"> LLM we initialized.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.558.1">There is no recipe to define which memory</span><a id="_idIndexMarker377" class="calibre3"/><span class="kobospan" id="kobo.559.1"> to use within your applications; however, there are some scenarios that might be particularly suitable for specific memories. </span><span class="kobospan" id="kobo.559.2">For example, a knowledge graph memory is useful for applications that need to access information from a large and diverse corpus of data and generate responses based on semantic relationships, while a conversation summary buffer memory could be suitable for creating conversational agents that can maintain a coherent and consistent context over multiple turns, while also being able to compress and summarize the previous dialogue history.</span></p>
<h2 class="heading1" id="_idParaDest-78"><span class="kobospan" id="kobo.560.1">Chains</span></h2>
<p class="normal"><span class="kobospan" id="kobo.561.1">Chains are predetermined sequences</span><a id="_idIndexMarker378" class="calibre3"/><span class="kobospan" id="kobo.562.1"> of actions and calls to LLMs that make it easier to build complex applications that require combining LLMs with each other or with other components.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.563.1">LangChain offers four main types of chain to get started with:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.564.1">LLMChain</span></strong><span class="kobospan" id="kobo.565.1">: This is the most common </span><a id="_idIndexMarker379" class="calibre3"/><span class="kobospan" id="kobo.566.1">type of chain. </span><span class="kobospan" id="kobo.566.2">It consists</span><a id="_idIndexMarker380" class="calibre3"/><span class="kobospan" id="kobo.567.1"> of a prompt template, an LLM, and an optional </span><strong class="screentext"><span class="kobospan" id="kobo.568.1">output parser</span></strong><span class="kobospan" id="kobo.569.1">.</span></li>
</ul>
<div class="note-one">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.570.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.571.1">An output parser is a component that helps</span><a id="_idIndexMarker381" class="calibre3"/><span class="kobospan" id="kobo.572.1"> structure language model responses. </span><span class="kobospan" id="kobo.572.2">It is a class that implements two main methods: </span><code class="inlinecode"><span class="kobospan" id="kobo.573.1">get_format_instructions</span></code><span class="kobospan" id="kobo.574.1"> and </span><code class="inlinecode"><span class="kobospan" id="kobo.575.1">parse.</span></code><span class="kobospan" id="kobo.576.1"> The </span><code class="inlinecode"><span class="kobospan" id="kobo.577.1">get_format_instructions</span></code><span class="kobospan" id="kobo.578.1"> method returns a string containing instructions for how the output of a language model should be formatted. </span><span class="kobospan" id="kobo.578.2">The </span><code class="inlinecode"><span class="kobospan" id="kobo.579.1">parse</span></code><span class="kobospan" id="kobo.580.1"> method takes in a string (assumed to be the response from a language model) and parses it into some structure, such as a dictionary, a list, or a custom object.</span></p>
</div>
<p class="normal-one"><span class="kobospan" id="kobo.581.1">This chain takes multiple input variables, uses </span><code class="inlinecode"><span class="kobospan" id="kobo.582.1">PromptTemplate</span></code><span class="kobospan" id="kobo.583.1"> to format them into a prompt, passes it to the model, and then uses </span><code class="inlinecode"><span class="kobospan" id="kobo.584.1">OutputParser</span></code><span class="kobospan" id="kobo.585.1"> (if provided) to parse the output of the LLM into a final format.</span></p>
<p class="normal-one"><span class="kobospan" id="kobo.586.1">For example, let’s retrieve the prompt template we built in the previous section:</span></p>
<pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.587.1">from</span></span><span class="kobospan" id="kobo.588.1"> langchain </span><span class="hljs-keyword"><span class="kobospan" id="kobo.589.1">import</span></span><span class="kobospan" id="kobo.590.1"> PromptTemplate
template = </span><span class="hljs-string"><span class="kobospan" id="kobo.591.1">"""Sentence: {sentence}</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.592.1">Translation in {language}:"""</span></span><span class="kobospan" id="kobo.593.1">
prompt = PromptTemplate(template=template, input_variables=[</span><span class="hljs-string"><span class="kobospan" id="kobo.594.1">"sentence"</span></span><span class="kobospan" id="kobo.595.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.596.1">"language"</span></span><span class="kobospan" id="kobo.597.1">])
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.598.1">Now, let’s put </span><a id="_idIndexMarker382" class="calibre3"/><span class="kobospan" id="kobo.599.1">it into an LLMChain:</span></p>
<pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.600.1">from</span></span><span class="kobospan" id="kobo.601.1"> langchain </span><span class="hljs-keyword"><span class="kobospan" id="kobo.602.1">import</span></span><span class="kobospan" id="kobo.603.1"> OpenAI, LLMChain
llm = OpenAI(temperature=</span><span class="hljs-number"><span class="kobospan" id="kobo.604.1">0</span></span><span class="kobospan" id="kobo.605.1">)
llm_chain = LLMChain(prompt=prompt, llm=llm)
llm_chain.predict(sentence=</span><span class="hljs-string"><span class="kobospan" id="kobo.606.1">"the cat is on the table"</span></span><span class="kobospan" id="kobo.607.1">, language=</span><span class="hljs-string"><span class="kobospan" id="kobo.608.1">"spanish"</span></span><span class="kobospan" id="kobo.609.1">)
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.610.1">Here is the</span><a id="_idIndexMarker383" class="calibre3"/><span class="kobospan" id="kobo.611.1"> output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.612.1">' El gato está en la mesa.'
</span></code></pre>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.613.1">RouterChain</span></strong><span class="kobospan" id="kobo.614.1">: This is a type of chain</span><a id="_idIndexMarker384" class="calibre3"/><span class="kobospan" id="kobo.615.1"> that allows you to route the input variables to different chains based on some conditions. </span><span class="kobospan" id="kobo.615.2">You can specify the conditions as functions or expressions that return a Boolean value. </span><span class="kobospan" id="kobo.615.3">You can also specify the default chain to use if none of the conditions are met.</span></li>
</ul>
<p class="normal-one"><span class="kobospan" id="kobo.616.1">For example, you can use this chain to create a chatbot that can handle different types of requests, such as planning an itinerary or booking a restaurant reservation. </span><span class="kobospan" id="kobo.616.2">To achieve this goal, you might want to differentiate two different prompts, depending on the type of query the user will make:</span></p>
<pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.617.1">itinerary_template = </span><span class="hljs-string"><span class="kobospan" id="kobo.618.1">"""You are a vacation itinerary assistant. </span><span class="kobospan" id="kobo.618.2">\</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.619.1">You help customers finding the best destinations and itinerary. </span><span class="kobospan" id="kobo.619.2">\</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.620.1">You help customer screating an optimized itinerary based on their preferences.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.621.1">Here is a question:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.622.1">{input}"""</span></span><span class="kobospan" id="kobo.623.1">
restaurant_template = </span><span class="hljs-string"><span class="kobospan" id="kobo.624.1">"""You are a restaurant booking assistant. </span><span class="kobospan" id="kobo.624.2">\</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.625.1">You check with customers number of guests and food preferences. </span><span class="kobospan" id="kobo.625.2">\</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.626.1">You pay attention whether there are special conditions to take into account.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.627.1">Here is a question:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.628.1">{input}"""</span></span>
</code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.629.1">Thanks to RouterChain, we can build</span><a id="_idIndexMarker385" class="calibre3"/><span class="kobospan" id="kobo.630.1"> a chain that is able to activate</span><a id="_idIndexMarker386" class="calibre3"/><span class="kobospan" id="kobo.631.1"> a different prompt depending on the user’s query. </span><span class="kobospan" id="kobo.631.2">I won’t post the whole code here (you can find the notebook on the book’s GitHub at </span><a href="Chapter_05.xhtml" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.632.1">https://github.com/PacktPublishing/Building-LLM-Powered-Applications</span></span></a><span class="kobospan" id="kobo.633.1">), but you can see a sample output of how the chain reacts to two different user’s queries:</span></p>
<pre class="programlisting2"><code class="hljs-code"><span class="hljs-built_in"><span class="kobospan" id="kobo.634.1">print</span></span><span class="kobospan" id="kobo.635.1">(chain.run(</span><span class="hljs-string"><span class="kobospan" id="kobo.636.1">"I'm planning a trip from Milan to Venice by car. </span><span class="kobospan" id="kobo.636.2">What can I visit in between?"</span></span><span class="kobospan" id="kobo.637.1">))
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.638.1">Here is the output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="hljs-con-meta"><span class="kobospan" id="kobo.639.1">&gt; </span></span><span><span class="kobospan" id="kobo.640.1">Entering new MultiPromptChain chain...</span></span><span class="kobospan" id="kobo.641.1">
itinerary: {'input': "I'm planning a trip from Milan to Venice by car. </span><span class="kobospan" id="kobo.641.2">What attractions can I visit in between?"}
</span><span class="hljs-con-meta"><span class="kobospan" id="kobo.642.1">&gt; </span></span><span><span class="kobospan" id="kobo.643.1">Finished chain.</span></span><span class="kobospan" id="kobo.644.1">
Answer:
There are many attractions that you can visit while traveling from Milan to Venice by car. </span><span class="kobospan" id="kobo.644.2">Some of the most popular attractions include Lake Como, Verona, the Dolomites, and the picturesque towns of Bergamo and Brescia. </span><span class="kobospan" id="kobo.644.3">You can also visit the stunning UNESCO World Heritage Sites in Mantua and Ferrara. </span><span class="kobospan" id="kobo.644.4">Additionally, you can explore some of the local wineries and sample some of the wines of the region.
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.645.1">Here it is with a second query:</span></p>
<pre class="programlisting2"><code class="hljs-code"><span class="hljs-built_in"><span class="kobospan" id="kobo.646.1">print</span></span><span class="kobospan" id="kobo.647.1">(chain.run(</span><span class="hljs-string"><span class="kobospan" id="kobo.648.1">"I want to book a table for tonight"</span></span><span class="kobospan" id="kobo.649.1">))
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.650.1">Here is the output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="hljs-con-meta"><span class="kobospan" id="kobo.651.1">&gt; </span></span><span><span class="kobospan" id="kobo.652.1">Entering new MultiPromptChain chain...</span></span><span class="kobospan" id="kobo.653.1">
restaurant: {'input': 'I want to book a table for tonight'}
</span><span class="hljs-con-meta"><span class="kobospan" id="kobo.654.1">&gt; </span></span><span><span class="kobospan" id="kobo.655.1">Finished chain.</span></span><span class="kobospan" id="kobo.656.1">
. </span><span class="kobospan" id="kobo.656.2">How many people are in your party?
</span><span class="kobospan" id="kobo.656.3">Hi there! </span><span class="kobospan" id="kobo.656.4">How many people are in your party for tonight's reservation?
</span></code></pre>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.657.1">SequentialChain</span></strong><span class="kobospan" id="kobo.658.1">: This is a type of chain that allows</span><a id="_idIndexMarker387" class="calibre3"/><span class="kobospan" id="kobo.659.1"> you to execute multiple chains</span><a id="_idIndexMarker388" class="calibre3"/><span class="kobospan" id="kobo.660.1"> in a sequence. </span><span class="kobospan" id="kobo.660.2">You can specify the order of the chains and how they pass their outputs to the next chain. </span><span class="kobospan" id="kobo.660.3">The simplest module of a sequential chain, takes by default the output of one chain as the input of the next chain. </span><span class="kobospan" id="kobo.660.4">However, you can also use a more complex module to have more flexibility to set input and output among chains.</span></li>
</ul>
<p class="normal-one"><span class="kobospan" id="kobo.661.1">As an example, let’s consider an AI system that is meant to first generate a joke on a given topic, and then translate it in to another language. </span><span class="kobospan" id="kobo.661.2">To do so, we will first create two chains:</span></p>
<pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.662.1">from</span></span><span class="kobospan" id="kobo.663.1"> langchain.llms </span><span class="hljs-keyword"><span class="kobospan" id="kobo.664.1">import</span></span><span class="kobospan" id="kobo.665.1"> OpenAI
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.666.1">from</span></span><span class="kobospan" id="kobo.667.1"> langchain.chains </span><span class="hljs-keyword"><span class="kobospan" id="kobo.668.1">import</span></span><span class="kobospan" id="kobo.669.1"> LLMChain
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.670.1">from</span></span><span class="kobospan" id="kobo.671.1"> langchain.prompts </span><span class="hljs-keyword"><span class="kobospan" id="kobo.672.1">import</span></span><span class="kobospan" id="kobo.673.1"> PromptTemplate
llm = OpenAI(temperature=</span><span class="hljs-number"><span class="kobospan" id="kobo.674.1">.7</span></span><span class="kobospan" id="kobo.675.1">)
template = </span><span class="hljs-string"><span class="kobospan" id="kobo.676.1">"""You are a comedian. </span><span class="kobospan" id="kobo.676.2">Generate a joke on the following {topic}</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.677.1">Joke:"""</span></span><span class="kobospan" id="kobo.678.1">
prompt_template = PromptTemplate(input_variables=[</span><span class="hljs-string"><span class="kobospan" id="kobo.679.1">"topic"</span></span><span class="kobospan" id="kobo.680.1">], template=template)
joke_chain = LLMChain(llm=llm, prompt=prompt_template)
template = </span><span class="hljs-string"><span class="kobospan" id="kobo.681.1">"""You are translator. </span><span class="kobospan" id="kobo.681.2">Given a text input, translate it to {language}</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.682.1">Translation:"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.683.1">.</span></span><span class="kobospan" id="kobo.684.1">prompt_template = PromptTemplate(input_variables=[</span><span class="hljs-string"><span class="kobospan" id="kobo.685.1">"language"</span></span><span class="kobospan" id="kobo.686.1">], template=template)
translator_chain = LLMChain(llm=llm, prompt=prompt_template)
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.687.1">Now, let’s combine them using the </span><code class="inlinecode"><span class="kobospan" id="kobo.688.1">SimpleSequentialChain</span></code><span class="kobospan" id="kobo.689.1"> module:</span></p>
<pre class="programlisting2"><code class="hljs-code"><span class="hljs-comment"><span class="kobospan" id="kobo.690.1"># This is the overall chain where we run these two chains in sequence.</span></span>
<span class="hljs-keyword"><span class="kobospan" id="kobo.691.1">from</span></span><span class="kobospan" id="kobo.692.1"> langchain.chains </span><span class="hljs-keyword"><span class="kobospan" id="kobo.693.1">import</span></span><span class="kobospan" id="kobo.694.1"> SimpleSequentialChain
overall_chain = SimpleSequentialChain(chains=[joke_chain, translator_chain], verbose=</span><span class="hljs-literal"><span class="kobospan" id="kobo.695.1">True</span></span><span class="kobospan" id="kobo.696.1">)
translated_joke = overall_chain.run(</span><span class="hljs-string"><span class="kobospan" id="kobo.697.1">"Cats and Dogs"</span></span><span class="kobospan" id="kobo.698.1">)
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.699.1">Here is the output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="hljs-con-meta"><span class="kobospan" id="kobo.700.1">&gt; </span></span><span><span class="kobospan" id="kobo.701.1">Entering new SimpleSequentialChain chain...</span></span><span class="kobospan" id="kobo.702.1">
Why did the cat cross the road? </span><span class="kobospan" id="kobo.702.2">To prove to the dog that it could be done!
 </span><span class="kobospan" id="kobo.702.3">¿Por qué cruzó el gato la carretera? </span><span class="kobospan" id="kobo.702.4">¡Para demostrarle al perro que se podía hacer!
</span><span class="hljs-con-meta"><span class="kobospan" id="kobo.703.1">&gt; </span></span><span><span class="kobospan" id="kobo.704.1">Finished chain.</span></span>
</code></pre>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.705.1">TransformationChain</span></strong><span class="kobospan" id="kobo.706.1">: This is a type of chain that allows </span><a id="_idIndexMarker389" class="calibre3"/><span class="kobospan" id="kobo.707.1">you to transform</span><a id="_idIndexMarker390" class="calibre3"/><span class="kobospan" id="kobo.708.1"> the input variables or the output of another chain using some functions or expressions. </span><span class="kobospan" id="kobo.708.2">You can specify the transformation as a function that takes the input or output as an argument and returns a new value, as well as specify the output format of the chain.</span></li>
</ul>
<p class="normal-one"><span class="kobospan" id="kobo.709.1">For example, let’s say we want to summarize a text, but before that, we want to rename one of the protagonists of the story (a cat) as “Silvester the Cat.” </span><span class="kobospan" id="kobo.709.2">As a sample text, I asked Bing Chat to generate a story about cats and dogs (you can find the whole </span><code class="inlinecode"><span class="kobospan" id="kobo.710.1">.txt</span></code><span class="kobospan" id="kobo.711.1"> file in the GitHub repository of this book):</span></p>
<pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.712.1">from</span></span><span class="kobospan" id="kobo.713.1"> langchain.chains </span><span class="hljs-keyword"><span class="kobospan" id="kobo.714.1">import</span></span><span class="kobospan" id="kobo.715.1"> TransformChain, LLMChain, SimpleSequentialChain
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.716.1">from</span></span><span class="kobospan" id="kobo.717.1"> langchain.llms </span><span class="hljs-keyword"><span class="kobospan" id="kobo.718.1">import</span></span><span class="kobospan" id="kobo.719.1"> OpenAI
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.720.1">from</span></span><span class="kobospan" id="kobo.721.1"> langchain.prompts </span><span class="hljs-keyword"><span class="kobospan" id="kobo.722.1">import</span></span><span class="kobospan" id="kobo.723.1"> PromptTemplate
transform_chain = TransformChain(
    input_variables=[</span><span class="hljs-string"><span class="kobospan" id="kobo.724.1">"text"</span></span><span class="kobospan" id="kobo.725.1">], output_variables=[</span><span class="hljs-string"><span class="kobospan" id="kobo.726.1">"output_text"</span></span><span class="kobospan" id="kobo.727.1">], transform=rename_cat
)
template = </span><span class="hljs-string"><span class="kobospan" id="kobo.728.1">"""Summarize this text:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.729.1">{output_text}</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.730.1">Summary:"""</span></span><span class="kobospan" id="kobo.731.1">
prompt = PromptTemplate(input_variables=[</span><span class="hljs-string"><span class="kobospan" id="kobo.732.1">"output_text"</span></span><span class="kobospan" id="kobo.733.1">], template=template)
llm_chain = LLMChain(llm=OpenAI(), prompt=prompt)
sequential_chain = SimpleSequentialChain(chains=[transform_chain, llm_chain])
sequential_chain.run(cats_and_dogs)
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.734.1">As you can see, we’ve combined a simple sequential chain with a transformation chain, where we set as a transformation function the </span><code class="inlinecode"><span class="kobospan" id="kobo.735.1">rename_cat</span></code><span class="kobospan" id="kobo.736.1"> function (you can see the whole code in the GitHub repository).</span></p>
<p class="normal-one"><span class="kobospan" id="kobo.737.1">The output is the following:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.738.1">" Silvester the Cat and a dog lived together but did not get along. </span><span class="kobospan" id="kobo.738.2">Silvester the Cat played a prank on the dog which made him angry. </span><span class="kobospan" id="kobo.738.3">When their owner found them fighting, she scolded them and made them apologize. </span><span class="kobospan" id="kobo.738.4">After that, they became friends and learned to respect each other's differences and appreciate each other's strengths."
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.739.1">Overall, LangChain chains are a powerful way to combine different language models and tasks into a single workflow. </span><span class="kobospan" id="kobo.739.2">Chains are flexible, scalable, and easy to use, and they enable users to leverage the power of language models for various purposes and domains. </span><span class="kobospan" id="kobo.739.3">Starting from the next chapter, we are going to see chains in action in concrete use cases, but before getting there, we need to cover the last component of LangChain: agents.</span></p>
<h2 class="heading1" id="_idParaDest-79"><span class="kobospan" id="kobo.740.1">Agents</span></h2>
<p class="normal"><span class="kobospan" id="kobo.741.1">Agents are entities that</span><a id="_idIndexMarker391" class="calibre3"/><span class="kobospan" id="kobo.742.1"> drive decision-making</span><a id="_idIndexMarker392" class="calibre3"/><span class="kobospan" id="kobo.743.1"> within LLM-powered applications. </span><span class="kobospan" id="kobo.743.2">They have access to a suite of tools and can decide which tool to call based on the user input and the context. </span><span class="kobospan" id="kobo.743.3">Agents are dynamic and adaptive, meaning that they can change or adjust their actions based on the situation or the goal: in fact, while in a chain, the sequence of actions is hardcoded, in agents, the LLM is used as the reasoning engine with the goal of planning and executing the right actions in the right order.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.744.1">A core concept while talking about agents is that of tools. </span><span class="kobospan" id="kobo.744.2">In fact, an agent might be good at planning all the right actions to fulfill a user’s query, but what if it cannot actually execute them, since it is missing information or executive power? </span><span class="kobospan" id="kobo.744.3">For example, imagine I want to build an agent that is capable of answering my questions by searching the web. </span><span class="kobospan" id="kobo.744.4">By itself, the agent has no access to the web, so I need to provide it with this tool. </span><span class="kobospan" id="kobo.744.5">I will do so by using</span><a id="_idIndexMarker393" class="calibre3"/><span class="kobospan" id="kobo.745.1"> SerpApi (the Google Search API) integration provided by LangChain (you can retrieve your API key at </span><a href="https://serpapi.com/dashboard" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.746.1">https://serpapi.com/dashboard</span></span></a><span class="kobospan" id="kobo.747.1">).</span></p>
<p class="normal1"><span class="kobospan" id="kobo.748.1">Let’s see it in Python:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.749.1">from</span></span><span class="kobospan" id="kobo.750.1"> langchain </span><span class="hljs-keyword"><span class="kobospan" id="kobo.751.1">import</span></span><span class="kobospan" id="kobo.752.1"> SerpAPIWrapper
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.753.1">from</span></span><span class="kobospan" id="kobo.754.1"> langchain.agents </span><span class="hljs-keyword"><span class="kobospan" id="kobo.755.1">import</span></span><span class="kobospan" id="kobo.756.1"> AgentType, initialize_agent
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.757.1">from</span></span><span class="kobospan" id="kobo.758.1"> langchain.llms </span><span class="hljs-keyword"><span class="kobospan" id="kobo.759.1">import</span></span><span class="kobospan" id="kobo.760.1"> OpenAI
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.761.1">from</span></span><span class="kobospan" id="kobo.762.1"> langchain.tools </span><span class="hljs-keyword"><span class="kobospan" id="kobo.763.1">import</span></span><span class="kobospan" id="kobo.764.1"> BaseTool, StructuredTool, Tool, tool
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.765.1">import</span></span><span class="kobospan" id="kobo.766.1"> os
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.767.1">from</span></span><span class="kobospan" id="kobo.768.1"> dotenv </span><span class="hljs-keyword"><span class="kobospan" id="kobo.769.1">import</span></span><span class="kobospan" id="kobo.770.1"> load_dotenv
load_dotenv()
os.environ[</span><span class="hljs-string"><span class="kobospan" id="kobo.771.1">"SERPAPI_API_KEY"</span></span><span class="kobospan" id="kobo.772.1">]
search = SerpAPIWrapper()
tools = [Tool.from_function(
        func=search.run,
        name=</span><span class="hljs-string"><span class="kobospan" id="kobo.773.1">"Search"</span></span><span class="kobospan" id="kobo.774.1">,
        description=</span><span class="hljs-string"><span class="kobospan" id="kobo.775.1">"useful for when you need to answer questions about current events"</span></span><span class="kobospan" id="kobo.776.1">
    )]
agent = initialize_agent(tools, llm = OpenAI(), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=</span><span class="hljs-literal"><span class="kobospan" id="kobo.777.1">True</span></span><span class="kobospan" id="kobo.778.1">)
agent.run(</span><span class="hljs-string"><span class="kobospan" id="kobo.779.1">"When was Avatar 2 released?"</span></span><span class="kobospan" id="kobo.780.1">)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.781.1">The following </span><a id="_idIndexMarker394" class="calibre3"/><span class="kobospan" id="kobo.782.1">is the output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="hljs-con-meta"><span class="kobospan" id="kobo.783.1">&gt; </span></span><span><span class="kobospan" id="kobo.784.1">Entering new AgentExecutor chain...</span></span><span class="kobospan" id="kobo.785.1">
 I need to find out when Avatar 2 was released.
</span><span class="kobospan" id="kobo.785.2">Action: Search
Action Input: "Avatar 2 release date"
Observation: December 16, 2022
Thought: I now know the final answer.
</span><span class="kobospan" id="kobo.785.3">Final Answer: Avatar 2 was released on December 16, 2022.
</span><span class="hljs-con-meta"><span class="kobospan" id="kobo.786.1">&gt; </span></span><span><span class="kobospan" id="kobo.787.1">Finished chain.</span></span><span class="kobospan" id="kobo.788.1">
'Avatar 2 was released on December 16, 2022.'
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.789.1">Note that, while initializing my agent, I set</span><a id="_idIndexMarker395" class="calibre3"/><span class="kobospan" id="kobo.790.1"> the agent type as </span><code class="inlinecode"><span class="kobospan" id="kobo.791.1">ZERO_SHOT_REACT_DESCRIPTION</span></code><span class="kobospan" id="kobo.792.1">. </span><span class="kobospan" id="kobo.792.2">This is one</span><a id="_idIndexMarker396" class="calibre3"/><span class="kobospan" id="kobo.793.1"> of the configurations we can pick and, specifically, it configures the agent to decide which tool to pick based solely on the tool’s description with a ReAct approach:</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.794.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.795.1">The ReAct approach</span><a id="_idIndexMarker397" class="calibre3"/><span class="kobospan" id="kobo.796.1"> is a way of using LLMs to solve various language reasoning and decision-making tasks. </span><span class="kobospan" id="kobo.796.2">It was introduced in the paper </span><em class="italic"><span class="kobospan" id="kobo.797.1">ReAct: Synergizing Reasoning and Acting in Language Models</span></em><span class="kobospan" id="kobo.798.1"> by Shunyu Yao et al., back in October 2022.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.799.1">The ReAct approach prompts LLMs to generate both verbal reasoning traces and text actions in an interleaved manner, allowing for greater synergy between the two. </span><span class="kobospan" id="kobo.799.2">Reasoning traces help the model to plan, track, and update its actions, as well as handle exceptions. </span><span class="kobospan" id="kobo.799.3">Actions allow the model to interact with external sources, such as knowledge bases or environments, to gather additional information.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.800.1">On top of this configuration, LangChain also offers the following types of agents:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.801.1">Structured input ReAct</span></strong><span class="kobospan" id="kobo.802.1">: This is an agent type that uses </span><a id="_idIndexMarker398" class="calibre3"/><span class="kobospan" id="kobo.803.1">the ReAct framework to generate natural language responses based on structured input data. </span><span class="kobospan" id="kobo.803.2">The agent can handle different types of input data, such as tables, lists, or key-value pairs. </span><span class="kobospan" id="kobo.803.3">The agent uses a language model and a prompt to generate responses that are informative, concise, and coherent.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.804.1">OpenAI Functions</span></strong><span class="kobospan" id="kobo.805.1">: This is an agent type that</span><a id="_idIndexMarker399" class="calibre3"/><span class="kobospan" id="kobo.806.1"> uses the OpenAI Functions API to access various language models and tools from OpenAI. </span><span class="kobospan" id="kobo.806.2">The agent can use different functions, such as GPT-3, Codex, DALL-E, CLIP, or ImageGPT. </span><span class="kobospan" id="kobo.806.3">The agent uses a language model and a prompt to generate requests to the OpenAI Functions API and parse the responses.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.807.1">Conversational</span></strong><span class="kobospan" id="kobo.808.1">: This is an agent type that uses</span><a id="_idIndexMarker400" class="calibre3"/><span class="kobospan" id="kobo.809.1"> a language model to engage in natural language conversations with the user. </span><span class="kobospan" id="kobo.809.2">The agent can handle different types of conversational tasks, such as chit-chat, question answering, or task completion. </span><span class="kobospan" id="kobo.809.3">The agent uses a language model and a prompt to generate responses that are relevant, fluent, and engaging.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.810.1">Self ask with search</span></strong><span class="kobospan" id="kobo.811.1">: This is an agent type</span><a id="_idIndexMarker401" class="calibre3"/><span class="kobospan" id="kobo.812.1"> that uses a language model to generate questions for itself and then search for answers on the web. </span><span class="kobospan" id="kobo.812.2">The agent can use this technique to learn new information or test its own knowledge.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.813.1">ReAct document store</span></strong><span class="kobospan" id="kobo.814.1">: This is an agent type that uses</span><a id="_idIndexMarker402" class="calibre3"/><span class="kobospan" id="kobo.815.1"> the ReAct framework to generate natural language responses based on documents stored in a database. </span><span class="kobospan" id="kobo.815.2">The agent can handle different types of documents, such as news articles, blog posts, or research papers.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.816.1">Plan-and-execute agents</span></strong><span class="kobospan" id="kobo.817.1">: This is an experimental agent</span><a id="_idIndexMarker403" class="calibre3"/><span class="kobospan" id="kobo.818.1"> type that uses a language model to choose a sequence of actions to take based on the user’s input and a goal. </span><span class="kobospan" id="kobo.818.2">The agent can use different tools or models to execute the actions it chooses. </span><span class="kobospan" id="kobo.818.3">The agent uses a language model and a prompt to generate plans and actions and then uses </span><code class="inlinecode"><span class="kobospan" id="kobo.819.1">AgentExecutor</span></code><span class="kobospan" id="kobo.820.1"> to run them.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.821.1">LangChain agents are pivotal whenever you want to let your LLMs interact with the external world. </span><span class="kobospan" id="kobo.821.2">Plus, it is interesting to see how agents leverage LLMs not only to retrieve and generate responses, but also as reasoning engines to plan an optimized sequence of actions.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.822.1">Together with all the LangChain</span><a id="_idIndexMarker404" class="calibre3"/><span class="kobospan" id="kobo.823.1"> components covered in this section, agents can be the core of LLM-powered applications, as we will see in the next chapters. </span><span class="kobospan" id="kobo.823.2">In the next section, we are going to shift toward the world of open-source LLMs, introducing the Hugging Face Hub and its native integration with LangChain.</span></p>
<h1 class="heading" id="_idParaDest-80"><span class="kobospan" id="kobo.824.1">Working with LLMs via the Hugging Face Hub</span></h1>
<p class="normal"><span class="kobospan" id="kobo.825.1">Now that we are familiar</span><a id="_idIndexMarker405" class="calibre3"/><span class="kobospan" id="kobo.826.1"> with LangChain components, it is time</span><a id="_idIndexMarker406" class="calibre3"/><span class="kobospan" id="kobo.827.1"> to start using our LLMs. </span><span class="kobospan" id="kobo.827.2">If you want to use open-source LLMs, leveraging the Hugging Face Hub integration is extremely versatile. </span><span class="kobospan" id="kobo.827.3">In fact, with just one access token you can leverage all the open-source LLMs available in Hugging Face’s repositories.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.828.1">As it is a non-production scenario, I will be using the free Inference API; however, if you are meant to build production-ready applications, you can easily scale to the Inference Endpoint, which grants you a dedicated and fully managed infrastructure to host and consume your LLMs.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.829.1">So, let’s see how to start integrating LangChain with the Hugging Face Hub.</span></p>
<h2 class="heading1" id="_idParaDest-81"><span class="kobospan" id="kobo.830.1">Create a Hugging Face user access token</span></h2>
<p class="normal"><span class="kobospan" id="kobo.831.1">To access the free Inference API, you will need</span><a id="_idIndexMarker407" class="calibre3"/><span class="kobospan" id="kobo.832.1"> a user access token, the credential that allows you to run the service. </span><span class="kobospan" id="kobo.832.2">The following are the steps to activate the user access token:</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="1"><strong class="screentext"><span class="kobospan" id="kobo.833.1">Create a Hugging Face account</span></strong><span class="kobospan" id="kobo.834.1">: You can create</span><a id="_idIndexMarker408" class="calibre3"/><span class="kobospan" id="kobo.835.1"> a Hugging Face account for free at </span><a href="https://huggingface.co/join" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.836.1">https://huggingface.co/join</span></span></a><span class="kobospan" id="kobo.837.1">.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.838.1">Retrieve your user access token</span></strong><span class="kobospan" id="kobo.839.1">: Once you have your account, go to the upper-right corner of your profile and go to </span><strong class="screentext"><span class="kobospan" id="kobo.840.1">Settings</span></strong><span class="kobospan" id="kobo.841.1"> | </span><strong class="screentext"><span class="kobospan" id="kobo.842.1">Access Tokens</span></strong><span class="kobospan" id="kobo.843.1">. </span><span class="kobospan" id="kobo.843.2">From that tab, you will be able to copy your secret token and use it to access Hugging Face models.</span></li>
</ol>
<figure class="mediaobject"><span class="kobospan" id="kobo.844.1"><img alt="A screenshot of a computer  Description automatically generated" src="../Images/B21714_05_04.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.845.1">Figure 5.4: Retrieving access tokens from the Hugging Face account (source: </span><a href="https://huggingface.co/settings/tokens" class="calibre3"><span class="kobospan" id="kobo.846.1">https://huggingface.co/settings/tokens</span></a><span class="kobospan" id="kobo.847.1">)</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="3"><strong class="screentext"><span class="kobospan" id="kobo.848.1">Set permissions</span></strong><span class="kobospan" id="kobo.849.1">: Access tokens</span><a id="_idIndexMarker409" class="calibre3"/><span class="kobospan" id="kobo.850.1"> enable users, applications, and notebooks to perform specific actions based on their assigned roles. </span><span class="kobospan" id="kobo.850.2">There are two available roles:</span><ul class="calibre17">
<li class="bulletlist2"><strong class="screentext"><span class="kobospan" id="kobo.851.1">Read: </span></strong><span class="kobospan" id="kobo.852.1">This allows tokens to provide read access to repositories you have permission to read. </span><span class="kobospan" id="kobo.852.2">This includes public and private repositories owned by you or your organization. </span><span class="kobospan" id="kobo.852.3">This role is suitable for tasks like downloading private models or inference.</span></li>
<li class="bulletlist3"><strong class="screentext"><span class="kobospan" id="kobo.853.1">Write: </span></strong><span class="kobospan" id="kobo.854.1">In addition to read access, tokens with this role grant write access to repositories where you have writing privileges. </span><span class="kobospan" id="kobo.854.2">This token is useful for activities like training models or updating model cards.</span></li>
</ul>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.855.1">In our series of use cases, we will keep a write permission on our token.</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="4"><strong class="screentext"><span class="kobospan" id="kobo.856.1">Managing your user access token</span></strong><span class="kobospan" id="kobo.857.1">: Within your profile, you can create and manage multiple access tokens, so that you can also differentiate permissions. </span><span class="kobospan" id="kobo.857.2">To create a new token, you can click on the </span><strong class="screentext"><span class="kobospan" id="kobo.858.1">New token</span></strong><span class="kobospan" id="kobo.859.1"> button:</span></li>
</ol>
<figure class="mediaobject"><span class="kobospan" id="kobo.860.1"><img alt="A screenshot of a computer  Description automatically generated" src="../Images/B21714_05_05.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.861.1">Figure 5.5: Creating a new token</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="5"><span class="kobospan" id="kobo.862.1">Finally, at any time, you can delete </span><a id="_idIndexMarker410" class="calibre3"/><span class="kobospan" id="kobo.863.1">or refresh your token under the </span><strong class="screentext"><span class="kobospan" id="kobo.864.1">Manage</span></strong><span class="kobospan" id="kobo.865.1"> button:</span></li>
</ol>
<figure class="mediaobject"><span class="kobospan" id="kobo.866.1"><img alt="A screenshot of a computer  Description automatically generated" src="../Images/B21714_05_06.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.867.1">Figure 5.6: Managing tokens</span></p>
<p class="normal-one"><span class="kobospan" id="kobo.868.1">It is important not to leak your token, and a good practice</span><a id="_idIndexMarker411" class="calibre3"/><span class="kobospan" id="kobo.869.1"> is to periodically regenerate it.</span></p>
<h2 class="heading1" id="_idParaDest-82"><span class="kobospan" id="kobo.870.1">Storing your secrets in an .env file</span></h2>
<p class="normal"><span class="kobospan" id="kobo.871.1">With our user access token</span><a id="_idIndexMarker412" class="calibre3"/><span class="kobospan" id="kobo.872.1"> generated in the previous</span><a id="_idIndexMarker413" class="calibre3"/><span class="kobospan" id="kobo.873.1"> section, we have the first secret to be managed.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.874.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.875.1">Secrets are data that needs</span><a id="_idIndexMarker414" class="calibre3"/><span class="kobospan" id="kobo.876.1"> to be protected from unauthorized access, such as passwords, tokens, keys, and credentials. </span><span class="kobospan" id="kobo.876.2">Secrets are used to authenticate and authorize requests to API endpoints, as well as to encrypt and decrypt sensitive data.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.877.1">Throughout this hands-on portion of the book, we will keep all our secrets within an </span><code class="inlinecode"><span class="kobospan" id="kobo.878.1">.env</span></code><span class="kobospan" id="kobo.879.1"> file.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.880.1">Storing Python secrets in an </span><code class="inlinecode"><span class="kobospan" id="kobo.881.1">.env</span></code><span class="kobospan" id="kobo.882.1"> file is a common practice to enhance security and maintainability in projects. </span><span class="kobospan" id="kobo.882.2">To do this, create a file named </span><code class="inlinecode"><span class="kobospan" id="kobo.883.1">.env</span></code><span class="kobospan" id="kobo.884.1"> in your project directory and list your sensitive information as key-value pairs: in our scenario, we will have </span><code class="inlinecode"><span class="kobospan" id="kobo.885.1">HUGGINGFACEHUB_API_TOKEN="your_user_access_token"</span></code><span class="kobospan" id="kobo.886.1">. </span><span class="kobospan" id="kobo.886.2">This file should be added to your project’s </span><code class="inlinecode"><span class="kobospan" id="kobo.887.1">.gitignore</span></code><span class="kobospan" id="kobo.888.1"> to prevent accidental exposure.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.889.1">To access these secrets in your Python code, use the </span><code class="inlinecode"><span class="kobospan" id="kobo.890.1">python-dotenv</span></code><span class="kobospan" id="kobo.891.1"> library to load the </span><code class="inlinecode"><span class="kobospan" id="kobo.892.1">.env</span></code><span class="kobospan" id="kobo.893.1"> file’s values as environment variables. </span><span class="kobospan" id="kobo.893.2">You can easily install it in your terminal via </span><code class="inlinecode"><span class="kobospan" id="kobo.894.1">pip install python-dotenv</span></code><span class="kobospan" id="kobo.895.1">.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.896.1">This approach keeps sensitive data separate from your code base and helps ensure that confidential information remains confidential throughout the development and deployment processes.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.897.1">Here, you can see an example of how to retrieve your access token and set it as an environmental variable:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.898.1">import</span></span><span class="kobospan" id="kobo.899.1"> os
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.900.1">from</span></span><span class="kobospan" id="kobo.901.1"> dotenv </span><span class="hljs-keyword"><span class="kobospan" id="kobo.902.1">import</span></span><span class="kobospan" id="kobo.903.1"> load_dotenv
load_dotenv()
os.environ[</span><span class="hljs-string"><span class="kobospan" id="kobo.904.1">"HUGGINGFACEHUB_API_TOKEN"</span></span><span class="kobospan" id="kobo.905.1">]
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.906.1">Note that, by default, </span><code class="inlinecode"><span class="kobospan" id="kobo.907.1">load_dotenv</span></code><span class="kobospan" id="kobo.908.1"> will look for the </span><code class="inlinecode"><span class="kobospan" id="kobo.909.1">.env</span></code><span class="kobospan" id="kobo.910.1"> file in the current working directory; however, you can also specify the path to your secrets file:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.911.1">from</span></span><span class="kobospan" id="kobo.912.1"> dotenv </span><span class="hljs-keyword"><span class="kobospan" id="kobo.913.1">import</span></span><span class="kobospan" id="kobo.914.1"> load_dotenv
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.915.1">from</span></span><span class="kobospan" id="kobo.916.1"> pathlib </span><span class="hljs-keyword"><span class="kobospan" id="kobo.917.1">import</span></span><span class="kobospan" id="kobo.918.1"> Path
dotenv_path = Path(</span><span class="hljs-string"><span class="kobospan" id="kobo.919.1">'path/to/.env'</span></span><span class="kobospan" id="kobo.920.1">)
load_dotenv(dotenv_path=dotenv_path)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.921.1">Now that we have all the ingredients</span><a id="_idIndexMarker415" class="calibre3"/><span class="kobospan" id="kobo.922.1"> to start coding, it is time to try out some</span><a id="_idIndexMarker416" class="calibre3"/><span class="kobospan" id="kobo.923.1"> open-source LLMs.</span></p>
<h2 class="heading1" id="_idParaDest-83"><span class="kobospan" id="kobo.924.1">Start using open-source LLMs</span></h2>
<p class="normal"><span class="kobospan" id="kobo.925.1">The nice thing about the Hugging Face Hub</span><a id="_idIndexMarker417" class="calibre3"/><span class="kobospan" id="kobo.926.1"> integration is that you can navigate its portal and decide, within the model catalog, what to use. </span><span class="kobospan" id="kobo.926.2">Models are also clustered per category (</span><strong class="screentext"><span class="kobospan" id="kobo.927.1">Computer Vision</span></strong><span class="kobospan" id="kobo.928.1">, </span><strong class="screentext"><span class="kobospan" id="kobo.929.1">Natural Language Processing</span></strong><span class="kobospan" id="kobo.930.1">, </span><strong class="screentext"><span class="kobospan" id="kobo.931.1">Audio</span></strong><span class="kobospan" id="kobo.932.1">, and so on) and, within each category, per capability (within </span><strong class="screentext"><span class="kobospan" id="kobo.933.1">Natural Language Processing</span></strong><span class="kobospan" id="kobo.934.1">, we have summarization, classification, Q&amp;A, and so on), as shown in the following screenshot:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.935.1"><img alt="A screenshot of a computer  Description automatically generated" src="../Images/B21714_05_07.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.936.1">Figure 5.7: Home page of Hugging Face’s model catalog</span></p>
<p class="normal1"><span class="kobospan" id="kobo.937.1">Since we are interested in LLMs, we will focus</span><a id="_idIndexMarker418" class="calibre3"/><span class="kobospan" id="kobo.938.1"> on the text generation category. </span><span class="kobospan" id="kobo.938.2">For this first experiment, let’s try Falcon LLM-7B:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.939.1">from</span></span><span class="kobospan" id="kobo.940.1"> langchain </span><span class="hljs-keyword"><span class="kobospan" id="kobo.941.1">import</span></span><span class="kobospan" id="kobo.942.1"> HuggingFaceHub
repo_id = </span><span class="hljs-string"><span class="kobospan" id="kobo.943.1">"tiiuae/falcon-7b-instruct"</span></span><span class="kobospan" id="kobo.944.1"> 
llm = HuggingFaceHub(
    repo_id=repo_id, model_kwargs={</span><span class="hljs-string"><span class="kobospan" id="kobo.945.1">"temperature"</span></span><span class="kobospan" id="kobo.946.1">: </span><span class="hljs-number"><span class="kobospan" id="kobo.947.1">0.5</span></span><span class="kobospan" id="kobo.948.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.949.1">"max_length"</span></span><span class="kobospan" id="kobo.950.1">: </span><span class="hljs-number"><span class="kobospan" id="kobo.951.1">1000</span></span><span class="kobospan" id="kobo.952.1">}
)
</span><span class="hljs-built_in"><span class="kobospan" id="kobo.953.1">print</span></span><span class="kobospan" id="kobo.954.1">(llm(</span><span class="hljs-string"><span class="kobospan" id="kobo.955.1">"what was the first disney movie?"</span></span><span class="kobospan" id="kobo.956.1">))
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.957.1">Here is the corresponding output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.958.1">The first Disney movie was 'Snow White and the Seven Dwarfs'
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.959.1">As you can see, with just a few lines of code, we integrated an LLM from the Hugging Face Hub. </span><span class="kobospan" id="kobo.959.2">With analogous code, you can test and consume all the LLMs available in the Hub.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.960.1">Note that, throughout this book, we will be leveraging specific models for each application, both proprietary and open source. </span><span class="kobospan" id="kobo.960.2">However, the idea is that you can use the model you prefer by simply initializing it as the main LLM and running the code as it is, simply changing the LangChain LLM integration. </span><span class="kobospan" id="kobo.960.3">This is one of the main advantages of LLM-powered applications since you don’t have</span><a id="_idIndexMarker419" class="calibre3"/><span class="kobospan" id="kobo.961.1"> to change the whole code to adapt to different LLMs.</span></p>
<h1 class="heading" id="_idParaDest-84"><span class="kobospan" id="kobo.962.1">Summary</span></h1>
<p class="normal"><span class="kobospan" id="kobo.963.1">In this chapter, we dove deeper into the fundamentals of LangChain, since it will be the AI orchestrator used in the upcoming chapters: we got familiar with LangChain components such as memory, agents, chains, and prompt templates. </span><span class="kobospan" id="kobo.963.2">We also covered how to start integrating LangChain with the Hugging Face Hub and its model catalog, and how to use the available LLMs and start embedding them into your code.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.964.1">From now on, we will look at a series of concrete end-to-end use cases, starting from a semantic Q&amp;A search app, which we are going to develop in the next chapter.</span></p>
<h1 class="heading" id="_idParaDest-85"><span class="kobospan" id="kobo.965.1">References</span></h1>
<ul class="calibre16">
<li class="bulletlist"><span class="kobospan" id="kobo.966.1">LangChain’s integration with OpenAI – </span><a href="https://python.langchain.com/docs/integrations/llms/openai" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.967.1">https://python.langchain.com/docs/integrations/llms/openai</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.968.1">LangChain’s prompt templates – </span><a href="https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.969.1">https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.970.1">LangChain’s vector stores – </span><a href="https://python.langchain.com/docs/integrations/vectorstores/" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.971.1">https://python.langchain.com/docs/integrations/vectorstores/</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.972.1">FAISS index – </span><a href="https://faiss.ai/" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.973.1">https://faiss.ai/</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.974.1">LangChain’s chains – </span><a href="https://python.langchain.com/docs/modules/chains/" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.975.1">https://python.langchain.com/docs/modules/chains/</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.976.1">ReAct approach – </span><a href="https://arxiv.org/abs/2210.03629" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.977.1">https://arxiv.org/abs/2210.03629</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.978.1">LangChain’s agents – </span><a href="https://python.langchain.com/docs/modules/agents/agent_types/" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.979.1">https://python.langchain.com/docs/modules/agents/agent_types/</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.980.1">Hugging Face documentation – </span><a href="https://huggingface.co/docs" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.981.1">https://huggingface.co/docs</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.982.1">LangChain Expression Language (LCEL) – </span><a href="https://python.langchain.com/docs/expression_language/" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.983.1">https://python.langchain.com/docs/expression_language/</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.984.1">LangChain stable version – </span><a href="https://blog.langchain.dev/langchain-v0-1-0/" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.985.1">https://blog.langchain.dev/langchain-v0-1-0/</span></span></a></li>
</ul>
<h1 class="heading"><span class="kobospan" id="kobo.986.1">Join our community on Discord</span></h1>
<p class="normal"><span class="kobospan" id="kobo.987.1">Join our community’s Discord space for discussions with the author and other readers:</span></p>
<p class="normal1"><a href="https://packt.link/llm" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.988.1">https://packt.link/llm</span></span></a></p>
<p class="normal1"><span class="kobospan" id="kobo.989.1"><img alt="" role="presentation" src="../Images/QR_Code214329708533108046.png" class="calibre4"/></span></p>
</div>
</body></html>