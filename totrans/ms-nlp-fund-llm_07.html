<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-142"><a id="_idTextAnchor365" class="calibre5 pcalibre1 pcalibre"/>7</h1>
<h1 id="_idParaDest-143" class="calibre4"><a id="_idTextAnchor366" class="calibre5 pcalibre1 pcalibre"/>Demystifying Large Language Models: Theory, Design, and Langchain Implementation</h1>
<p class="calibre6">In this chapter, we delve deep into the intricate world of <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>) and the underpinning mathematical concepts that fuel their performance. The advent of these models has revolutionized the field of <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>), offering unparalleled proficiency in understanding, generating, and interacting with human language.</p>
<p class="calibre6">LLMs are a subset of <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) models that can understand and generate human-like text. They achieve this by being trained on a diverse range of internet text, thus learning an extensive array of facts about the world. They also learn to predict what comes next in a piece of text, which enables them to generate creative, fluent, and contextually coherent sentences.</p>
<p class="calibre6">As we explore the operations of LLMs, we will introduce the key metric of <strong class="bold">perplexity</strong>, a measurement of uncertainty that is pivotal in determining the performance of these models. A lower perplexity indicates the confidence that a <strong class="bold">language model</strong> (<strong class="bold">LM</strong>) has in predicting the next word in a sequence, thus showcasing its proficiency.</p>
<p class="calibre6">This chapter draws on multiple insightful publications that delve into the mathematical insights of LLMs. Some of these include <em class="italic">A Neural Probabilistic Language Model</em>, <em class="italic">Attention is All You Need</em>, and <em class="italic">PaLM: Scaling Language Modeling with Pathways</em>. These sources will guide us in understanding the robust mechanisms that underpin LLMs and their exceptional capabilities.</p>
<p class="calibre6">We will also explore the emerging field of <strong class="bold">reinforcement learning from human feedback</strong> (<strong class="bold">RLHF</strong>) in the context of LMs. RLHF has proven to be a powerful tool in fine-tuning the performance of LLMs, thereby leading to more accurate and meaningful generated texts.</p>
<p class="calibre6">With a comprehensive understanding of the mathematical foundations of LLMs and a deep dive into RLHF, we will gain a robust knowledge of these advanced AI systems, paving the way for future innovations and advancements in the field.</p>
<p class="calibre6">Finally, we will discuss the detailed architecture and design of recent models, such as <strong class="bold">Pathways Language Model</strong> (<strong class="bold">PaLM</strong>), <strong class="bold">Large Language Model Meta AI</strong> (<strong class="bold">LLaMA</strong>), and GPT-4.</p>
<p class="calibre6">Now, let’s look at the topics covered in this chapter:</p>
<ul class="calibre14">
<li class="calibre15">What are LLMs and how are they different from LMs?</li>
<li class="calibre15">Motivations for developing and using LLMs</li>
<li class="calibre15">Challenges in developing LLMs</li>
</ul>
<h1 id="_idParaDest-144" class="calibre4"><a id="_idTextAnchor367" class="calibre5 pcalibre1 pcalibre"/>Technical requirements</h1>
<p class="calibre6">For this chapter, you are expected to possess a solid foundation in <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) concepts, particularly in the areas of <strong class="bold">Transformers</strong> and <strong class="bold">reinforcement learning</strong>. An understanding of Transformer-based models, which underpin many of today’s LLMs, is vital. This includes familiarity with concepts such as self-attention mechanisms, positional encoding, and the structure of encoder-decoder architectures.</p>
<p class="calibre6">Knowledge of reinforcement learning principles is also essential, as we will delve into the application of RLHF in the fine-tuning of LMs. Familiarity with concepts such as policy gradients, reward functions, and Q-learning will greatly enhance your comprehension of this content.</p>
<p class="calibre6">Lastly, coding proficiency, specifically in Python, is crucial. This is because many of the concepts will be demonstrated and explored through the lens of programming. Experience with PyTorch or TensorFlow, popular ML libraries, and Hugging Face’s Transformers library, a key resource for working with transformer models, will also be beneficial.</p>
<p class="calibre6">However, don’t be discouraged if you feel you’re lacking in some areas. This chapter aims to walk you through the complexities of these subjects, bridging any knowledge gaps along the way. So, come prepared with a mindset for learning, and let’s delve into the fascinating world of LLMs!</p>
<h1 id="_idParaDest-145" class="calibre4"><a id="_idTextAnchor368" class="calibre5 pcalibre1 pcalibre"/>What are LLMs and how are they different from LMs?</h1>
<p class="calibre6">An LM is a type of ML model that is trained to predict the next word (or character or subword, depending<a id="_idIndexMarker719" class="calibre5 pcalibre1 pcalibre"/> on the granularity of the model) in a sequence, given the words that came before it (or in some models, the surrounding words). It’s a probabilistic model that is capable of generating text that follows a certain linguistic style or pattern.</p>
<p class="calibre6">Before the advent of Transformer-based <a id="_idIndexMarker720" class="calibre5 pcalibre1 pcalibre"/>models such as <strong class="bold">generative pretrained Transformers</strong> (<strong class="bold">GPTs</strong>) and <strong class="bold">Bidirectional Encoder Representations from Transformers</strong> (<strong class="bold">BERT</strong>), there were several other types of LMs widely <a id="_idIndexMarker721" class="calibre5 pcalibre1 pcalibre"/>used in NLP tasks. The following subsections discuss a few of the<a id="_idTextAnchor369" class="calibre5 pcalibre1 pcalibre"/>m.</p>
<h2 id="_idParaDest-146" class="calibre7"><a id="_idTextAnchor370" class="calibre5 pcalibre1 pcalibre"/>n-gram models</h2>
<p class="calibre6">These are some of the<a id="_idIndexMarker722" class="calibre5 pcalibre1 pcalibre"/> simplest LMs. An <em class="italic">n</em>-gram model uses the (<em class="italic">n</em>-1) previous words to predict the <em class="italic">n</em>th word in a sentence. For example, in a bigram (2-gram) model, we would use the previous word to predict<a id="_idIndexMarker723" class="calibre5 pcalibre1 pcalibre"/> the next word. These models are easy to implement and computationally efficient, but they typically don’t perform as well as more complex models because they don’t capture long-range dependencies between words. Their performance also degrades as <em class="italic">n</em> increases, as they suffer from data sparsity issues (not having enough data to accurately estimate the probabilities for all possible <em class="italic">n</em>-gra<a id="_idTextAnchor371" class="calibre5 pcalibre1 pcalibre"/>ms).</p>
<h2 id="_idParaDest-147" class="calibre7"><a id="_idTextAnchor372" class="calibre5 pcalibre1 pcalibre"/>Hidden Markov models (HMMs)</h2>
<p class="calibre6">These models consider the “hidden” states that <a id="_idIndexMarker724" class="calibre5 pcalibre1 pcalibre"/>generate the observed data. In the context of language modeling, each word would be an observed state, and the “hidden” state would be <a id="_idIndexMarker725" class="calibre5 pcalibre1 pcalibre"/>some kind of linguistic feature that’s not directly observable (such as the part of speech of the word). However, like <em class="italic">n</em>-gram models, HMMs struggle to capture long-range dependencies between <a id="_idTextAnchor373" class="calibre5 pcalibre1 pcalibre"/>words.</p>
<h2 id="_idParaDest-148" class="calibre7"><a id="_idTextAnchor374" class="calibre5 pcalibre1 pcalibre"/>Recurrent neural networks (RNNs)</h2>
<p class="calibre6">These are a type of neural network where connections between nodes form a directed graph<a id="_idIndexMarker726" class="calibre5 pcalibre1 pcalibre"/> along a temporal sequence. This allows them to use their internal state (memory) to<a id="_idIndexMarker727" class="calibre5 pcalibre1 pcalibre"/> process sequences of inputs, making them ideal for language modeling. They can capture long-range dependencies between words, but they struggle with the so-called vanishing gradient problem, which makes it difficult to learn these dependencies in p<a id="_idTextAnchor375" class="calibre5 pcalibre1 pcalibre"/>ractice.</p>
<h3 class="calibre8">Long short-term memory (LSTM) networks</h3>
<p class="calibre6">An LSTM network is<a id="_idIndexMarker728" class="calibre5 pcalibre1 pcalibre"/> a special kind of RNN that is designed to learn long-term dependencies. They do this by using a series of “gates” that control the flow of information in and out of the<a id="_idIndexMarker729" class="calibre5 pcalibre1 pcalibre"/> memory state of the network. LSTMs were a big step forward in the state of the art of language<a id="_idTextAnchor376" class="calibre5 pcalibre1 pcalibre"/> modeling.</p>
<h4 class="calibre135">Gated recurrent unit (GRU) networks</h4>
<p class="calibre6">These are a variation<a id="_idIndexMarker730" class="calibre5 pcalibre1 pcalibre"/> of LSTMs that use a slightly different set of gates in their architecture. They’re often simpler and faster to train than LSTMs, but whether they perform better or worse than LSTMs tends to depend on the specific task at hand.</p>
<p class="calibre6">Each of these models has its own strengths and weaknesses, and none of them are inherently better or worse than the others – it all depends on the specific task and dataset. However, Transformer-based models have generally outperformed all of these models in a wide range of tasks, leading to their current popularity in the fi<a id="_idTextAnchor377" class="calibre5 pcalibre1 pcalibre"/>eld of NLP.</p>
<h1 id="_idParaDest-149" class="calibre4"><a id="_idTextAnchor378" class="calibre5 pcalibre1 pcalibre"/>How LLMs stand out</h1>
<p class="calibre6">LLMs, such as GPT-3 and <a id="_idIndexMarker731" class="calibre5 pcalibre1 pcalibre"/>GPT-4, are simply LMs that are trained on a very large amount of text and have a very large number of parameters. The larger the model (in terms of parameters and training data), the more capable it is of understanding and generating complex and varied texts. Here are some key ways in which LLMs differ from smaller LMs:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Data</strong>: LLMs are trained <a id="_idIndexMarker732" class="calibre5 pcalibre1 pcalibre"/>on vast amounts of data. This allows them to learn from a wide range of linguistic patterns, styles, and topics.</li>
<li class="calibre15"><strong class="bold">Parameters</strong>: LLMs have a huge number of parameters. Parameters in an ML model are the parts of the model that are learned from the training data. The more parameters a model has, the more complex patterns it can learn.</li>
<li class="calibre15"><strong class="bold">Performance</strong>: Because they’re trained on more data and have more parameters, LLMs generally perform better than smaller ones. They’re capable of generating more coherent and diverse texts, and they’re better at understanding context, making inferences, and even answering questions or generating texts on a wide range of topics.</li>
<li class="calibre15"><strong class="bold">Compute resources</strong>: LLMs require a significant amount of computational resources to train, both in terms of processing power and memory. They also take longer to train.</li>
<li class="calibre15"><strong class="bold">Storage and inference time</strong>: Large models also require more storage space, and it takes longer to generate predictions (although this inference time is still typically quite fast on modern hardware).</li>
</ul>
<p class="calibre6">Thus, we can say that LLMs are essentially scaled-up versions of smaller LMs. They’re trained on more data, have more parameters, and are generally capable of producing higher-quality results, but they also require more resources to train and use. Besides that, an important advantage of an LLM is that we can train them unsupervised on a large corpus of data and then fine-tune them with a limited amount of data for dif<a id="_idTextAnchor379" class="calibre5 pcalibre1 pcalibre"/>ferent tasks.</p>
<h1 id="_idParaDest-150" class="calibre4"><a id="_idTextAnchor380" class="calibre5 pcalibre1 pcalibre"/>Motivations for developing and using LLMs</h1>
<p class="calibre6">The motivation to develop and use LLMs arises from several factors related to the capabilities of these models, and the potential<a id="_idIndexMarker733" class="calibre5 pcalibre1 pcalibre"/> benefits <a id="_idIndexMarker734" class="calibre5 pcalibre1 pcalibre"/>they can bring in diverse applications. The following subsections detail a few of these k<a id="_idTextAnchor381" class="calibre5 pcalibre1 pcalibre"/>ey motivations.</p>
<h2 id="_idParaDest-151" class="calibre7"><a id="_idTextAnchor382" class="calibre5 pcalibre1 pcalibre"/>Improved performance</h2>
<p class="calibre6">LLMs, when trained with sufficient data, generally demonstrate better performance compared to smaller <a id="_idIndexMarker735" class="calibre5 pcalibre1 pcalibre"/>models. They are more capable of understanding context, identifying nuances, and generating coherent and contextually relevant responses. This performance gain applies to a wide range of tasks in NLP, including text classification, named entity recognition, sentiment analysis, machine translation, question answering, and text generation. As shown in <em class="italic">Table 7.1</em>, the performance of BERT – one of the first well-known LLMs – and GPT is compared to the previous models on the <strong class="bold">General Language Understanding Evaluation</strong> (<strong class="bold">GLUE</strong>) benchmark. The GLUE benchmark is a<a id="_idIndexMarker736" class="calibre5 pcalibre1 pcalibre"/> collection of diverse <strong class="bold">natural language understanding</strong> (<strong class="bold">NLU</strong>) tasks designed to evaluate the performance of models across multiple linguistic challenges. The benchmark<a id="_idIndexMarker737" class="calibre5 pcalibre1 pcalibre"/> encompasses tasks such as sentiment analysis, question answering, and textual entailment, among others. It’s a widely recognized standard in the field of NLU, providing a comprehensive suite for comparing and improving language understanding models. It can be seen that its performance is better in all tasks:</p>
<table class="no-table-style" id="table001-4">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">Model</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">Average (in </strong><strong class="bold">all tasks)</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">Sentiment analysis</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">Grammatical</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">Similarity</strong></p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">BERT large</p>
</td>
<td class="no-table-style2">
<p class="calibre6">82.1</p>
</td>
<td class="no-table-style2">
<p class="calibre6">94.9</p>
</td>
<td class="no-table-style2">
<p class="calibre6">60.5</p>
</td>
<td class="no-table-style2">
<p class="calibre6">86.5</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">BERT base</p>
</td>
<td class="no-table-style2">
<p class="calibre6">79.6</p>
</td>
<td class="no-table-style2">
<p class="calibre6">93.5</p>
</td>
<td class="no-table-style2">
<p class="calibre6">52.1</p>
</td>
<td class="no-table-style2">
<p class="calibre6">85.8</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">OpenAI GPT</p>
</td>
<td class="no-table-style2">
<p class="calibre6">75.1</p>
</td>
<td class="no-table-style2">
<p class="calibre6">91.3</p>
</td>
<td class="no-table-style2">
<p class="calibre6">45.4</p>
</td>
<td class="no-table-style2">
<p class="calibre6">80.0</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">Pre-open AI <strong class="bold">State of the </strong><strong class="bold">Art </strong>(<strong class="bold">STOA</strong>)</p>
</td>
<td class="no-table-style2">
<p class="calibre6">74.0</p>
</td>
<td class="no-table-style2">
<p class="calibre6">93.2</p>
</td>
<td class="no-table-style2">
<p class="calibre6">35.0</p>
</td>
<td class="no-table-style2">
<p class="calibre6">81.0</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">Bidirectional Long Short-Term memory (BiLSTM) + Embeddings from Language Model (ELMo) + Attention</p>
</td>
<td class="no-table-style2">
<p class="calibre6">71.0</p>
</td>
<td class="no-table-style2">
<p class="calibre6">90.4</p>
</td>
<td class="no-table-style2">
<p class="calibre6">36.0</p>
</td>
<td class="no-table-style2">
<p class="calibre6">73.3</p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US">Table 7.1 – Comparing different models’ performance on GLUE (this comparison is based on 2018 when BERT and GPT were released)</p>
<h2 id="_idParaDest-152" class="calibre7"><a id="_idTextAnchor383" class="calibre5 pcalibre1 pcalibre"/>Broad generalization</h2>
<p class="calibre6">LLMs trained on diverse <a id="_idIndexMarker738" class="calibre5 pcalibre1 pcalibre"/>datasets can generalize better across different tasks, domains, or styles of language. They can effectively learn from the training data to identify and understand a wide range <a id="_idIndexMarker739" class="calibre5 pcalibre1 pcalibre"/>of linguistic patterns, styles, and topics. This broad generalization capability makes them versatile for various applications, from chatbots to content creation to information retrieval.</p>
<p class="calibre6">When an LM is bigger, it means it has more parameters. These parameters allow the model to capture and encode more complex relationships and nuances within the data. In other words, a bigger model can learn and retain more information from the training data. As such, it is better equipped to handle a wider array of tasks and contexts post-training. It is this increased complexity and capacity that makes bigger LMs more generalizable across different tasks. As we can see in <em class="italic">Figure 7</em><em class="italic">.1</em>, the bigger LMs perform better in different tasks.</p>
<div><div><img alt="Figure 7.1 – LLMs performance based on their size and training" src="img/B18949_07_1.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.1 – LLMs performance based on their size and training</p>
<p class="calibre6">We can also see the <a id="_idIndexMarker740" class="calibre5 pcalibre1 pcalibre"/>progress in the<a id="_idIndexMarker741" class="calibre5 pcalibre1 pcalibre"/> development of the LLMs within the last three years in <em class="italic">Figure 7</em><em class="italic">.2</em>.</p>
<div><div><img alt="Figure 7.2 – The released LMs within 2019 ﻿to 2023  (the publicly available models are highlighted) ﻿" src="img/B18949_07_2.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.2 – The released LMs within 2019 to 2023  (the publicly available models are highlighted) </p>
<p class="calibre6">However, it’s important to note that while larger models tend to be more generalizable, they also pose challenges such as increased computational requirements and the<a id="_idIndexMarker742" class="calibre5 pcalibre1 pcalibre"/> risk of overfitting. It is also essential to ensure that the training data is representative of the tasks and domains the model is expected to perform in, as models might carry over any biases present in the training dat<a id="_idTextAnchor384" class="calibre5 pcalibre1 pcalibre"/>a.</p>
<h2 id="_idParaDest-153" class="calibre7"><a id="_idTextAnchor385" class="calibre5 pcalibre1 pcalibre"/>Few-shot learning</h2>
<p class="calibre6">LLMs such as GPT-3, GPT-3.5, and GPT-4 have demonstrated impressive few-shot learning capabilities. Given a<a id="_idIndexMarker743" class="calibre5 pcalibre1 pcalibre"/> few examples (the “shots”), these models can generalize to complete similar tasks effectively. This makes adjusting and deploying these models in real-world applications more efficient. The prompts can be designed to include information for the model to refer to, such as example questions and their respective answers.</p>
<p class="calibre6">The model temporarily learns from given examples and refers to given information as an additional source. For example, when the LLM is used as a personal assistant or advisor, background information about the user can be appended to the prompt, allowing the<a id="_idIndexMarker744" class="calibre5 pcalibre1 pcalibre"/> model to “get to know you,” as it<a id="_idIndexMarker745" class="calibre5 pcalibre1 pcalibre"/> uses your personal information prompts as a reference.</p>
<h2 id="_idParaDest-154" class="calibre7">Understanding complex con<a id="_idTextAnchor386" class="calibre5 pcalibre1 pcalibre"/>texts</h2>
<p class="calibre6">LLMs have the advantage of understanding complex contexts due to their extensive training on a wide range of data, including various topics, literary styles, and nuances as well as their deep architecture and large parameter space. This capacity allows them to <a id="_idIndexMarker746" class="calibre5 pcalibre1 pcalibre"/>comprehend and generate appropriate responses even in complex or nuanced situations.</p>
<p class="calibre6">For example, consider a scenario where a user asks the model to summarize a complicated scientific article. An LLM can understand the context and the technical language used in the article and generate a coherent and simplified su<a id="_idTextAnchor387" class="calibre5 pcalibre1 pcalibre"/>mmary.</p>
<h2 id="_idParaDest-155" class="calibre7"><a id="_idTextAnchor388" class="calibre5 pcalibre1 pcalibre"/>Multilingual capabilities</h2>
<p class="calibre6">LLMs can handle <a id="_idIndexMarker747" class="calibre5 pcalibre1 pcalibre"/>multiple languages effectively, making them suitable for global applications. Here are a few well-known <a id="_idIndexMarker748" class="calibre5 pcalibre1 pcalibre"/>multiling<a id="_idTextAnchor389" class="calibre5 pcalibre1 pcalibre"/>ual LMs.</p>
<h3 class="calibre8">mBERT (multilingual BERT)</h3>
<p class="calibre6">An extension of BERT, mBERT is pretrained on the top 104 languages with the largest Wikipedia <a id="_idIndexMarker749" class="calibre5 pcalibre1 pcalibre"/>using a masked LM o<a id="_idTextAnchor390" class="calibre5 pcalibre1 pcalibre"/>bjective.</p>
<h3 class="calibre8">Cross-lingual language model (XLM)</h3>
<p class="calibre6">This is trained in<a id="_idIndexMarker750" class="calibre5 pcalibre1 pcalibre"/> 100 languages. It extends the BERT model to include several methods for cross-lingual model<a id="_idTextAnchor391" class="calibre5 pcalibre1 pcalibre"/> training.</p>
<h3 class="calibre8">XLM-RoBERTa</h3>
<p class="calibre6">XLM-RoBERTa extends RoBERTa, which itself is an optimized version of BERT, and is trained on a <a id="_idIndexMarker751" class="calibre5 pcalibre1 pcalibre"/>much larger multilingual corpus covering more<a id="_idTextAnchor392" class="calibre5 pcalibre1 pcalibre"/> languages.</p>
<h3 class="calibre8">MarianMT</h3>
<p class="calibre6">Part of <a id="_idIndexMarker752" class="calibre5 pcalibre1 pcalibre"/>Hugging Face’s Transformers<a id="_idIndexMarker753" class="calibre5 pcalibre1 pcalibre"/> library, MarianMT is a state-of-the-art Transformer-based model optimized for trans<a id="_idTextAnchor393" class="calibre5 pcalibre1 pcalibre"/>lation tasks.</p>
<h3 class="calibre8">DistilBERT Multilingual</h3>
<p class="calibre6">This is a smaller and faster <a id="_idIndexMarker754" class="calibre5 pcalibre1 pcalibre"/>version of mBERT, achieved through a distill<a id="_idTextAnchor394" class="calibre5 pcalibre1 pcalibre"/>ation process.</p>
<h3 class="calibre8">T2T (T5) Multilingual</h3>
<p class="calibre6">This is a variant of the <strong class="bold">Text-to-Text Transfer Transformer</strong> (<strong class="bold">T5</strong>) model, which is fine-tuned for translation<a id="_idIndexMarker755" class="calibre5 pcalibre1 pcalibre"/> tasks.</p>
<p class="calibre6">These models have achieved<a id="_idIndexMarker756" class="calibre5 pcalibre1 pcalibre"/> significant results in a variety of tasks, such as translation, named entity recognition, part-of-speech tagging, and sentiment analysis in mul<a id="_idTextAnchor395" class="calibre5 pcalibre1 pcalibre"/>tiple languages.</p>
<h2 id="_idParaDest-156" class="calibre7"><a id="_idTextAnchor396" class="calibre5 pcalibre1 pcalibre"/>Human-like text generation</h2>
<p class="calibre6">LLMs have shown a remarkable capability in generating human-like text. They can create contextually<a id="_idIndexMarker757" class="calibre5 pcalibre1 pcalibre"/> appropriate responses in <a id="_idIndexMarker758" class="calibre5 pcalibre1 pcalibre"/>conversations, write essays, and generate creative content such as poetry and stories. Models such as GPT-3, ChatGPT, and GPT-4 have shown good results in text generation tasks.</p>
<p class="calibre6">While the advantages are many, it’s important to note that there are also challenges and potential risks associated with the use of LLMs. They require significant computational resources to train and deploy, and there are ongoing concerns related to their potential to generate harmful or biased content, their interpretability, and their environmental impact. Researchers are actively working on ways to mitigate these issues while leveraging the powerful capabilities of these models.</p>
<p class="calibre6">Due to these reasons, companies are trying to implement and train larger LMs (<em class="italic">Figure 7</em><em class="italic">.3</em>):</p>
<div><div><img alt="Figure 7.3 – Newer LMs and their size, as well a﻿s the developers" src="img/B18949_07_3.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Newer LMs and their size, as well a<a id="_idTextAnchor397" class="calibre5 pcalibre1 pcalibre"/>s the developers</p>
<h1 id="_idParaDest-157" class="calibre4"><a id="_idTextAnchor398" class="calibre5 pcalibre1 pcalibre"/>Challenges in developing LLMs</h1>
<p class="calibre6">Developing LLMs <a id="_idIndexMarker759" class="calibre5 pcalibre1 pcalibre"/>poses a unique set of challenges, including but not limited to handling massive amounts of data, requiring vast computational resources, and the risk of introducing or perpetuating bias. The following subsections outline the detailed explanations of<a id="_idTextAnchor399" class="calibre5 pcalibre1 pcalibre"/> these challenges.</p>
<h2 id="_idParaDest-158" class="calibre7"><a id="_idTextAnchor400" class="calibre5 pcalibre1 pcalibre"/>Amounts of data</h2>
<p class="calibre6">LLMs require enormous amounts of data for training. As the model size grows, so does the need for diverse, high-quality training data. However, collecting and curating such large<a id="_idIndexMarker760" class="calibre5 pcalibre1 pcalibre"/> datasets is a challenging task. It can be time - consuming and expensive. There’s also the risk of inadvertently including sensitive or inappropriate data in the training set. To have more of an idea, BERT has been trained using 3.3 billion words from Wikipedia and BookCorpus. GPT-2 has been trained on 40 GB of text data, and GPT-3 has been trained on 570 GB of text data. <em class="italic">Table 7.2</em> shows the number of parameters and size of training data of a few recent LMs.</p>
<table class="no-table-style" id="table002-2">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">Model</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">Parameters</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">Size of </strong><strong class="bold">training data</strong></p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">GPT-3.5</p>
</td>
<td class="no-table-style2">
<p class="calibre6">175 B</p>
</td>
<td class="no-table-style2">
<p class="calibre6">300 billion tokens</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">GPT-3</p>
</td>
<td class="no-table-style2">
<p class="calibre6">175 B</p>
</td>
<td class="no-table-style2">
<p class="calibre6">300 billion tokens</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">PaLM</p>
</td>
<td class="no-table-style2">
<p class="calibre6">540 B</p>
</td>
<td class="no-table-style2">
<p class="calibre6">780 billion tokens</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">LLaMA</p>
</td>
<td class="no-table-style2">
<p class="calibre6">65 B</p>
</td>
<td class="no-table-style2">
<p class="calibre6">1.4 trillion tokens</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">Bloom</p>
</td>
<td class="no-table-style2">
<p class="calibre6">176 B</p>
</td>
<td class="no-table-style2">
<p class="calibre6">366 billion tokens</p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US">Table 7.2 – Number of parameters and training data of a few recent LMs</p>
<h2 id="_idParaDest-159" class="calibre7"><a id="_idTextAnchor401" class="calibre5 pcalibre1 pcalibre"/>Computational resources</h2>
<p class="calibre6">Training LLMs requires substantial computational resources. These models often have billions or even trillions of parameters and need to process vast amounts of data during training, which requires high-performance hardware (such as GPUs or TPUs) and a significant amount of time. This can be costly and could limit the accessibility of developing such models to only those who have these resources. For example, training GPT-3 took 1 million GPU hours, which cost around 4.6 million dollars (in 2020). <em class="italic">Table 7.3</em> shows the computational resources and training time of a few recent LMs.</p>
<table class="no-table-style" id="table003-1">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">Model</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">Hardware</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">Training time</strong></p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">PaLM</p>
</td>
<td class="no-table-style2">
<p class="calibre6">6144 TPU v4</p>
</td>
<td class="no-table-style2">
<p class="calibre6">-</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">LLaMA</p>
</td>
<td class="no-table-style2">
<p class="calibre6">2048 80G A100</p>
</td>
<td class="no-table-style2">
<p class="calibre6">21 days</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">Bloom</p>
</td>
<td class="no-table-style2">
<p class="calibre6">384 80G A100</p>
</td>
<td class="no-table-style2">
<p class="calibre6">105 days</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">GPT-3</p>
</td>
<td class="no-table-style2">
<p class="calibre6">1024x A100</p>
</td>
<td class="no-table-style2">
<p class="calibre6">34 days</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">GPT-4</p>
</td>
<td class="no-table-style2">
<p class="calibre6">25000 A100</p>
</td>
<td class="no-table-style2">
<p class="calibre6">90–100 days</p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US">Table 7.3 – The hardware and training time of a few recent LMs</p>
<h2 id="_idParaDest-160" class="calibre7"><a id="_idTextAnchor402" class="calibre5 pcalibre1 pcalibre"/>Risk of bias</h2>
<p class="calibre6">LLMs can<a id="_idIndexMarker761" class="calibre5 pcalibre1 pcalibre"/> learn and perpetuate biases present in their training data. This could be explicit bias, such as racial or gender bias in the way language is used, or more subtle forms of bias, such as the underrepresentation of certain topics or perspectives. This issue can be challenging to address because bias in language is a deeply rooted societal issue, and it’s often not easy to even identify what might be considered bias in a given context.<a id="_idTextAnchor403" class="calibre5 pcalibre1 pcalibre"/></p>
<h2 id="_idParaDest-161" class="calibre7"><a id="_idTextAnchor404" class="calibre5 pcalibre1 pcalibre"/>Model robustness</h2>
<p class="calibre6">It’s challenging to ensure that LLMs will perform well in all possible scenarios, particularly on inputs that differ from their training data. This includes dealing with ambiguous queries, handling out-of-distribution data, and ensuring a level of consistency in the responses. Making sure that the model is not overtrained can help to have a more robust model, but much more is needed to have a robust model.<a id="_idTextAnchor405" class="calibre5 pcalibre1 pcalibre"/></p>
<h2 id="_idParaDest-162" class="calibre7"><a id="_idTextAnchor406" class="calibre5 pcalibre1 pcalibre"/>Interpretability and debugging</h2>
<p class="calibre6">LLMs, like most <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) models, are often described as “black boxes.” It’s not easy to understand why they’re making a particular prediction or how they’re arriving at a conclusion. This makes debugging challenging if the model starts to produce incorrect or inappropriate outputs. Improving interpretability is an active area of research. For example, some libraries attempt to elucidate the decision-making process of an LM by employing techniques such as feature importance analysis, which involves removing some words and analyzing the change in gradients.</p>
<p class="calibre6">One such method is the input perturbation technique. In this approach, a word (or words) from the input text is perturbed or removed, and the change in the model’s output is analyzed. The rationale behind this is to understand the influence of a specific input word on the model’s output prediction. If the removal of a certain word significantly changes the model’s prediction, it can be inferred that the model deemed this word as important for its prediction.</p>
<p class="calibre6">Analyzing gradient changes is another popular method. By investigating how the gradient of the output with respect to the input changes when a certain word is removed, one can gain insight into how the model’s decision-making process is influenced by that<a id="_idIndexMarker762" class="calibre5 pcalibre1 pcalibre"/> specific word.</p>
<p class="calibre6">These interpretation techniques provide a more transparent view into the complex decision-making process of LLMs, enabling researchers to better understand and improve their models. Libraries such as LIME and SHAP offer tools for model interpretation tasks, thus making the process more accessible to researchers<a id="_idTextAnchor407" class="calibre5 pcalibre1 pcalibre"/>.</p>
<h2 id="_idParaDest-163" class="calibre7"><a id="_idTextAnchor408" class="calibre5 pcalibre1 pcalibre"/>Environmental impact</h2>
<p class="calibre6">The high computational resources needed for training LLMs can have significant environmental implications. The energy required for training these models can contribute to carbon emissions, which is a concern from a sustainability perspective.</p>
<p class="calibre6">Besides that, there are concerns about privacy and security in LLMs. For example, it is recommended not to share models that are trained using patients’ medical information, or not to feed sensitive information into publicly available LLMs such as ChatGPT, since it<a id="_idIndexMarker763" class="calibre5 pcalibre1 pcalibre"/> can return it to other users as the answer to their question<a id="_idTextAnchor409" class="calibre5 pcalibre1 pcalibre"/>s.</p>
<h1 id="_idParaDest-164" class="calibre4"><a id="_idTextAnchor410" class="calibre5 pcalibre1 pcalibre"/>Different types of LLMs</h1>
<p class="calibre6">LLMs are generally neural network architectures that are trained on a large corpus of text data. The<a id="_idIndexMarker764" class="calibre5 pcalibre1 pcalibre"/> term “large” refers to the size of these models in terms of the number of parameters and the scale of training data. Here are some examples of LLMs.</p>
<h2 id="_idParaDest-165" class="calibre7"><a id="_idTextAnchor411" class="calibre5 pcalibre1 pcalibre"/>Transformer models</h2>
<p class="calibre6">Transformer models have<a id="_idIndexMarker765" class="calibre5 pcalibre1 pcalibre"/> been at the forefront of the recent wave of LLMs. They are based on the “Transformer” architecture, which uses self-attention mechanisms to weigh the relevance of different words<a id="_idIndexMarker766" class="calibre5 pcalibre1 pcalibre"/> in the input when making predictions. Transformers are a type of neural network architecture introduced in the paper <em class="italic">Attention is All You Need</em> by Vaswani et al. One of their significant advantages, particularly for training LLMs, is their suitability for parallel computing.</p>
<p class="calibre6">In traditional RNN models, such as LSTM and GRU, the sequence of tokens (words, subwords, or characters in the text) must be processed sequentially. That’s because each token’s representation depends not only on the token itself but also on the previous tokens in the sequence. The inherent sequential nature of these models makes it difficult to parallelize their operations, which can limit the speed and efficiency of the training process.</p>
<p class="calibre6">Transformers, in contrast, eliminate the necessity for sequential processing by using a mechanism called self-attention (or <a id="_idIndexMarker767" class="calibre5 pcalibre1 pcalibre"/>scaled dot-product attention). In the self-attention process, each token’s representation is computed as a weighted sum of all tokens in the sequence, with the weights determined by the attention mechanism. Importantly, these computations for each token are independent of the computations for other tokens, and thus they can be performed in parallel.</p>
<p class="calibre6">This parallelization capability brings several advantages for training LLMs as we will discuss next.</p>
<h3 class="calibre8">Speed</h3>
<p class="calibre6">By parallelizing <a id="_idIndexMarker768" class="calibre5 pcalibre1 pcalibre"/>the computations, Transformers can process large amounts of data more quickly than RNNs. This speed can significantly reduce the training time of LLMs, which often need to process vast amounts of data.</p>
<h3 class="calibre8">Scalability</h3>
<p class="calibre6">Transformers’ parallelization makes it easier to scale up the model size and the amount of training data. This capability is crucial for developing LLMs, as these models often benefit<a id="_idIndexMarker769" class="calibre5 pcalibre1 pcalibre"/> from being trained on larger datasets and having a larger number of parameters.</p>
<h3 class="calibre8">Long-range dependencies</h3>
<p class="calibre6">Transformers can better <a id="_idIndexMarker770" class="calibre5 pcalibre1 pcalibre"/>capture long-range dependencies between tokens because they consider all tokens in the sequence simultaneously, rather than processing them one at a time. This capability is valuable in many language tasks and can improve the performance of LLMs.</p>
<p class="calibre6">Each of these models has its own strengths and weaknesses, and the best choice of model can depend on the specific task, the amount and type of available training data, and the computational resources a<a id="_idTextAnchor412" class="calibre5 pcalibre1 pcalibre"/>vailable.</p>
<h1 id="_idParaDest-166" class="calibre4"><a id="_idTextAnchor413" class="calibre5 pcalibre1 pcalibre"/>Example designs of state-of-the-art LLMs</h1>
<p class="calibre6">In this part, we are<a id="_idIndexMarker771" class="calibre5 pcalibre1 pcalibre"/> going to dig more into the design and architecture of some of the newest LLMs at the time of writing this book.</p>
<h2 id="_idParaDest-167" class="calibre7">GPT-3.5 a<a id="_idTextAnchor414" class="calibre5 pcalibre1 pcalibre"/>nd ChatGPT</h2>
<p class="calibre6">The core <a id="_idIndexMarker772" class="calibre5 pcalibre1 pcalibre"/>of ChatGPT is a Transformer, a type of model architecture that uses self-attention mechanisms to weigh the relevance of different words in the input when making predictions. It allows the model to consider the full context of the input when generating <a id="_idTextAnchor415" class="calibre5 pcalibre1 pcalibre"/>a response.</p>
<h3 class="calibre8">The GPT model</h3>
<p class="calibre6">ChatGPT is based on the GPT version of the Transformer. The GPT models are trained to predict the next word in a sequence of words, given all the previous words. They process text from left to right (unidirectional context), which makes them well-suited for text generation tasks. For instance, GPT-3, one of the versions of GPT on which <a id="_idIndexMarker773" class="calibre5 pcalibre1 pcalibre"/>ChatGPT is based, contains 175 billion<a id="_idTextAnchor416" class="calibre5 pcalibre1 pcalibre"/> parameters.</p>
<h3 class="calibre8">Two-step training process</h3>
<p class="calibre6">The training process for ChatGPT is done in two steps: pretraining and fine-tuning.</p>
<h4 class="calibre135">Pretraining</h4>
<p class="calibre6">In this step, the model is trained <a id="_idIndexMarker774" class="calibre5 pcalibre1 pcalibre"/>on a large corpus of publicly available text from the internet. However, it’s worth noting that it does not know specifics about which documents were in its training set or have access to any specific documents or sources.</p>
<h4 class="calibre135">Fine-tuning</h4>
<p class="calibre6">After pretraining, the base model is further trained (fine-tuned) on custom datasets created by OpenAI, which include demonstrations of correct behavior as well as comparisons to rank different<a id="_idIndexMarker775" class="calibre5 pcalibre1 pcalibre"/> responses. Some prompts are from users of the Playground and ChatGPT apps, but they are anonymized and stripped of personally identifiabl<a id="_idTextAnchor417" class="calibre5 pcalibre1 pcalibre"/>e information.</p>
<h3 class="calibre8">RLHF</h3>
<p class="calibre6">Part of the fine-tuning process involves <a id="_idIndexMarker776" class="calibre5 pcalibre1 pcalibre"/>RLHF, where human AI trainers provide feedback on model outputs for a range of example inputs, and this feedback is used to improve the model’s responses. RLHF is a key component of the fine-tuning process used to train ChatGPT. It’s a technique for refining the performance of the model by learning from feedback provided by human evaluators. Here, we first explain the general idea of RLHF, and in the next section, we explain it step by step.</p>
<p class="calibre6">The first step in RLHF is to collect human feedback. For ChatGPT, this often involves having human AI trainers participate in conversations where they play both sides (the user and the AI assistant). The trainers also have access to model-written suggestions to help them compose responses. This dialogue, in which AI trainers are essentially having a conversation with themselves, is added to the dataset for fine-tuning.</p>
<p class="calibre6">In addition to the dialogues, comparison data is created where multiple model responses are ranked by quality. This is done by taking a conversation turn, generating several different completions (responses), and having human evaluators rank them. The evaluators don’t <a id="_idIndexMarker777" class="calibre5 pcalibre1 pcalibre"/>just rank the responses on factual correctness but also on how useful and safe they judged the response to be.</p>
<p class="calibre6">The model is then fine-tuned using <strong class="bold">proximal policy optimization</strong> (<strong class="bold">PPO</strong>), a reinforcement learning algorithm. PPO attempts to improve the model’s responses based on human feedback, making<a id="_idIndexMarker778" class="calibre5 pcalibre1 pcalibre"/> small adjustments to the model’s parameters to increase the likelihood of better-rated responses and decrease the likelihood of worse-rated responses.</p>
<p class="calibre6">RLHF is an iterative process. The procedure of collecting human feedback, creating comparison data, and fine-tuning the model using PPO is repeated multiple times to incrementally improve the model. Next, we will explain in more detail how PPO works.</p>
<p class="calibre6">PPO is a reinforcement learning algorithm used to optimize the π policy of an agent. The policy defines how the agent selects actions based on its current state. PPO aims to optimize this policy to maximize the expected cumulative rewards.</p>
<p class="calibre6">Before diving into PPO, it’s important to define the reward model. In the context of reinforcement learning, the reward model is a <code>R</code>(<code>s</code>, <code>a</code>) function, which assigns a reward value to every state-action pair (<code>s</code>, <code>a</code>). The goal of the agent is to learn a policy π that maximizes the expected sum of these rewards.</p>
<p class="calibre6">Mathematically, the objective of reinforcement learning can be defined as follows:</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;π&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;E&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;π&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;R&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/307.png" class="calibre306"/></p>
<p class="calibre6">In this formula, <em class="italic">E</em>π[.] is the expectation over trajectories (sequences of state-action pairs) generated by following policy <em class="italic">π</em>, <em class="italic">s</em>_t is the state at time <em class="italic">t</em>, <em class="italic">a</em>_t is the action taken at time <em class="italic">t</em>, and <em class="italic">R(s</em>_t<em class="italic">, a</em>_t<em class="italic">)</em> is the reward received at time <em class="italic">t</em>.</p>
<p class="calibre6">PPO modifies this objective to encourage exploration of the policy space while preventing too drastic changes in the policy at each update. This is done by introducing a ratio, <em class="italic">r</em>_t(θ), which represents the ratio of the probabilities of the current policy <em class="italic">π</em>_θ to the old policy <em class="italic">π_θ_old</em>:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/308.png" class="calibre307"/></p>
<p class="calibre6">The objective of PPO is<a id="_idIndexMarker779" class="calibre5 pcalibre1 pcalibre"/> then defined as follows:</p>
<p class="calibre6" lang="en-US" xml:lang="en-US"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/mfenced&gt;&lt;msub&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/309.png" class="calibre308"/></p>
<p class="calibre6">Here, <em class="italic">A_t</em> is the advantage function that measures how much better the taking action <em class="italic">a_t</em> is compared to the average action at state <em class="italic">s_</em>t, and <em class="italic">clip(r_</em>t<em class="italic">(θ), 1 - ε, 1 + ε)</em> is a clipped version of <em class="italic">r_</em>t<em class="italic">(θ)</em> that discourages too large policy updates.</p>
<p class="calibre6">The algorithm then optimizes this objective using stochastic gradient ascent, adjusting the policy parameters <em class="italic">θ</em> to increase <em class="italic">J_</em>PPO<em class="italic">(π)</em>.</p>
<p class="calibre6">In the context of ChatGPT and RLHF, the states correspond to the conversation histories, the actions correspond to the model-generated messages, and the rewards correspond to the human feedback on these messages. PPO is used to adjust the model parameters to improve the quality of the generated messages as judged by the human feedback.</p>
<p class="calibre6">The human rankings are used to create a reward model, which quantifies how good each response is. The reward model is a function that takes in a state and an action (in this case, a conversation context and a model-generated message), and outputs a scalar reward. During training, the model tries to maximize its expected cumulative reward.</p>
<p class="calibre6">The goal of RLHF is to align the model’s behavior with human values and to improve its ability to generate useful and safe responses. By learning from human feedback, ChatGPT can adapt to a wider range of conversational contexts and provide more appropriate and helpful responses. It’s worth noting that despite these efforts, the system might still make mistakes, and handling these errors and improving the RLHF process is an area of<a id="_idTextAnchor418" class="calibre5 pcalibre1 pcalibre"/> ongoing research.</p>
<h3 class="calibre8">Generating responses</h3>
<p class="calibre6">When generating a response, ChatGPT takes as input a conversation history, which includes previous messages in the <a id="_idIndexMarker780" class="calibre5 pcalibre1 pcalibre"/>conversation along with the most recent user message and produces a model-generated message as output. The conversation history is tokenized and fed into the model, which generates a sequence of tokens in response, and these tokens are then detokenized to form the<a id="_idTextAnchor419" class="calibre5 pcalibre1 pcalibre"/> <a id="_idIndexMarker781" class="calibre5 pcalibre1 pcalibre"/>final output text.</p>
<h3 class="calibre8">System-level controls</h3>
<p class="calibre6">OpenAI has also implemented some system-level controls to mitigate harmful or untruthful outputs from ChatGPT. This<a id="_idIndexMarker782" class="calibre5 pcalibre1 pcalibre"/> includes a Moderation API that warns or blocks certain typ<a id="_idTextAnchor420" class="calibre5 pcalibre1 pcalibre"/>es of unsafe content.</p>
<h3 class="calibre8">Step by step process <a id="_idTextAnchor421" class="calibre5 pcalibre1 pcalibre"/>of RLHF in ChatGPT</h3>
<p class="calibre6">Since RLHF is an important part of ChatGPT and several other <strong class="bold">State of the Art</strong> (<strong class="bold">SOTA</strong>) models, understanding it better is useful to the you. In recent years, LMs have demonstrated remarkable abilities, creating<a id="_idIndexMarker783" class="calibre5 pcalibre1 pcalibre"/> varied and compelling text based on human-generated prompts. Nonetheless, it’s challenging to precisely define what constitutes “good” text as it is inherently subjective and depends on the context. For instance, while crafting stories demands creativity, informative pieces require accuracy, and code snippets need to be executable.</p>
<p class="calibre6">Defining a loss function to encapsulate these attributes seems virtually impossible, hence most LMs are trained using a basic next-token prediction loss, such as cross-entropy. To overcome the limitations of the loss function, individuals have developed metrics that better align with human preferences, such as BLEU or <strong class="bold">ROUGE</strong>. The <strong class="bold">BLEU</strong> score, or Bilingual evaluation understudy, is a metric which is used to measure how well machine-translated text compares to a set of reference translations. Although these metrics are more effective at assessing performance, they are inherently limited as they merely compare the generated text to references using basic rules.</p>
<p class="calibre6">Wouldn’t it be transformative if we could use human feedback on generated text as a performance measure, or even better, as a loss to optimize the model? This is the concept behind RLHF – leveraging reinforcement learning techniques to directly optimize an LM using human feedback. RLHF has begun to enable LMs to align a model trained on a general text corpus with intricate human values.</p>
<p class="calibre6">One of the recent successful applications of RLHF has been in the development of ChatGPT.</p>
<p class="calibre6">The concept of RLHF presents a formidable challenge due to its multifaceted model training process and various deployment phases. Here, we’ll dissect the training procedure into its three essential components:</p>
<ul class="calibre14">
<li class="calibre15">Initial pretraining of an LM</li>
<li class="calibre15">Data collection and reward model training</li>
<li class="calibre15">Refining the LM using reinforcement learning</li>
</ul>
<p class="calibre6">We’ll begin by examining the pret<a id="_idTextAnchor422" class="calibre5 pcalibre1 pcalibre"/>raining phase for LMs.</p>
<h2 id="_idParaDest-168" class="calibre7"><a id="_idTextAnchor423" class="calibre5 pcalibre1 pcalibre"/>LM pretraining</h2>
<p class="calibre6">As a foundation, RLHF utilizes an LM that’s already been pretrained using traditional<a id="_idIndexMarker784" class="calibre5 pcalibre1 pcalibre"/> pretraining objectives, which means that we create the tokenizer based on our training data, design model architecture, and then pretrain the model using the training data. For its initial well-received RLHF model, InstructGPT, OpenAI employed a smaller version of GPT-3. On the other hand, Anthropic used transformer models ranging from 10 million to 52 billion parameters trained for this task, and DeepMind utilized its 280 billion parameter model, Gopher.</p>
<p class="calibre6">This preliminary model may be further refined on extra text or particular conditions, although it’s not always necessary. As an example, OpenAI chose to refine its model using human-generated text identified as “preferable.” This dataset is used to further fine-tune the model using the RLHF model, distilling the original LM model based on contextual hints from humans.</p>
<p class="calibre6">Generally speaking, there’s no definitive answer to the question of “which model” serves as the best launching point for RLHF. The array of options available for RLHF training has not been extensively explored.</p>
<p class="calibre6">Moving on, once an LM is in place, it’s necessary to generate data to train a reward model. This step is crucial for integrating human preferences into the system.</p>
<div><div><img alt="Figur﻿e 7.4 – Pretraining LM" src="img/B18949_07_4.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figur<a id="_idTextAnchor424" class="calibre5 pcalibre1 pcalibre"/>e 7.4 – Pretraining LM</p>
<h2 id="_idParaDest-169" class="calibre7"><a id="_idTextAnchor425" class="calibre5 pcalibre1 pcalibre"/>Training the reward model</h2>
<p class="calibre6">In the newly proposed method, RLHF is being used as the RM, which is known as a preference model as <a id="_idIndexMarker785" class="calibre5 pcalibre1 pcalibre"/>well. The main idea here is to get a text and return a scalar reward that reflects human preferences. This approach can be implemented in two ways. First, implement an end-to-end LLM, which gives us the preferred output. This process can be performed by fine-tuning a LLM or training a LLM from scratch. Second, have an extra component that ranks different outputs of the LLM and returns the best one.</p>
<p class="calibre6">The dataset used for training the RM is a set of prompt-generation pairs. Prompts are sampled from a predetermined dataset (Anthropic’s data). These prompts undergo processing by the initial LM to generate fresh text.</p>
<p class="calibre6">Human annotators rank the text outputs generated by the LM. It might seem intuitive to have humans directly assign a scalar score to each text piece to generate a reward model, but it proves challenging in reality. Varied human values render these scores unstandardized and unreliable. Consequently, rankings are employed to compare <a id="_idIndexMarker786" class="calibre5 pcalibre1 pcalibre"/>multiple model outputs, thereby creating a substantially better regularized dataset.</p>
<p class="calibre6">There are several strategies for text ranking. One successful approach involves users comparing the text produced by two LMs given the same prompt. By evaluating model outputs in direct comparison, an <strong class="bold">Elo rating system</strong>, which we will soon describe, can generate a ranking of models and <a id="_idIndexMarker787" class="calibre5 pcalibre1 pcalibre"/>outputs relative to each other. These varying ranking methods are then normalized into a scalar reward signal for training. The Elo rating system, originally developed for chess, is also applicable to RLHF for LMs.</p>
<p class="calibre6">In the context of LMs, each model or variant (e.g., models at different stages of training) can be seen as a “player.” Its Elo rating reflects how well it performs in terms of generating human-preferred outputs.</p>
<p class="calibre6">The fundamental mechanics of the Elo rating system remain the same. Here’s how it can be adapted for RLHF in LMs:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Initialization</strong>: All models start with the same Elo rating, often 1,000 or 1,500.</li>
<li class="calibre15"><strong class="bold">Comparison</strong>: For a given prompt, two models (A and B) generate their outputs. A human evaluator then ranks these outputs. If the evaluator considers the output from model A to be better, model A “wins” the match, and model B “loses.”</li>
</ul>
<p class="calibre6">The Elo ratings are updated in this way after each evaluation. Over time, they provide an ongoing, dynamic ranking of the models based on human preferences. This is useful for tracking progress over the course of training and for comparing different models or model variants.</p>
<p class="calibre6">Successful RLHF systems have employed diverse-sized reward LMs relative to text generation. For example, OpenAI used a 175 B LM with a 6 B reward model, Anthropic utilized LM and reward models ranging from 10 B to 52 B, and DeepMind employed 70 B Chinchilla models for both the LM and reward model. This is because preference models must match the capacity needed to understand a text as a model would need to generate it. At this juncture in the RLHF process, we possess an initial LM capable of text generation and a preference model that assigns a score to any text based on human perception. We next apply reinforcement learning to optimize the original <a id="_idIndexMarker788" class="calibre5 pcalibre1 pcalibre"/>LM concerning the reward model.</p>
<div><div><img alt="Figure 7.5 – The reward model f﻿or reinforcement learning" src="img/B18949_07_5.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.5 – The reward model f<a id="_idTextAnchor426" class="calibre5 pcalibre1 pcalibre"/>or reinforcement learning</p>
<h2 id="_idParaDest-170" class="calibre7">How to fine-tune the model usi<a id="_idTextAnchor427" class="calibre5 pcalibre1 pcalibre"/>ng reinforcement learning</h2>
<p class="calibre6">For a considerable period, the prospect of training an LM using reinforcement learning was considered unattainable due to both technical and algorithmic challenges. However, several <a id="_idIndexMarker789" class="calibre5 pcalibre1 pcalibre"/>organizations have achieved fine-tuning some or all parameters of a replica of the initial LM with a policy-gradient reinforcement learning algorithm – namely, PPO. Parameters of the LM are kept static because fine-tuning an entire model with 10 B or 100 B+ parameters is prohibitively expensive (for further details, refer<a id="_idIndexMarker790" class="calibre5 pcalibre1 pcalibre"/> to <strong class="bold">Low-Rank Adaptation</strong> (<strong class="bold">LoRA</strong>) for LMs or DeepMind’s Sparrow LM). PPO, an established method for some time now, has abundant available guides explaining its functioning. This maturity made it an attractive choice for scaling up to the novel application of distributed training for RLHF. It appears that significant strides in RLHF have been made by determining how to update such a colossal model with a known algorithm (more on that later).</p>
<p class="calibre6">We can articulate this fine-tuning task as a reinforcement learning problem. Initially, the policy is an LM that accepts a prompt and produces a sequence of text (or merely probability distributions over text). The action space of this policy is all the tokens aligning with the LM’s vocabulary (typically around 50 K tokens), and the observation space is the distribution of possible input token sequences, which is also notably large in light of reinforcement learning’s prior uses (the dimension approximates the vocabulary size power (<code>^</code>) length of the input token sequence). The reward function melds the preference model with a constraint on policy shift.</p>
<p class="calibre6">The reward function is the juncture where the system integrates all the models discussed into a single RLHF process. Given a prompt, <em class="italic">x</em>, from the dataset, the text, <em class="italic">y</em>, is created by the current iteration of the fine-tuned policy. This text, coupled with the original prompt, is passed to the preference model, which returns a scalar measure of “preferability”,<img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/310.png" class="calibre309"/> .</p>
<p class="calibre6">Additionally, per-token probability distributions from the reinforcement learning policy are contrasted with those from the initial model to compute a penalty on their difference. In several papers from OpenAI, Anthropic, and DeepMind, this penalty has been constructed as a scaled version of the <strong class="bold">Kullback–Leibler</strong> (<strong class="bold">KL</strong>) divergence between these sequences of distributions over tokens, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;K&lt;/mml:mi&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/311.png" class="calibre310"/> . The KL divergence term penalizes the reinforcement learning policy from veering significantly from the initial pretrained model with each training batch, ensuring the production of reasonably coherent text snippets.</p>
<p class="calibre6">Without this penalty, the optimization might start generating gibberish text that somehow deceives the reward model into granting a high reward. In practical terms, the KL divergence is approximated via sampling from both distributions. The final reward transmitted to the reinforcement learning update rule is as follows:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/312.png" class="calibre311"/></p>
<p class="calibre6">Additional terms have been incorporated into the reward function by some RLHF systems. For<a id="_idIndexMarker791" class="calibre5 pcalibre1 pcalibre"/> instance, OpenAI’s InstructGPT successfully tried the blending of additional pretraining gradients (from the human annotation set) into the update rule for PPO. It is anticipated that as RLHF continues to be studied, the formulation of this reward function will continue to evolve.</p>
<p class="calibre6">Finally, the update rule is the parameter update from PPO that optimizes the reward metrics in the current data batch (PPO is on-policy, meaning the parameters are only updated with the current batch of prompt-generation pairs). PPO is a trust region optimization algorithm that employs constraints on the gradient to ensure the update step does not destabilize the learning process. DeepMind utilized a similar reward setup for Gopher but employed a synchronous advantage actor.</p>
<div><div><img alt="Figure 7.6 – Fine-tuning the model using reinforcement learning" src="img/B18949_07_6.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.6 – Fine-tuning the model using reinforcement learning</p>
<p class="calibre6">The preceding diagram may suggest that both models produce different responses for the same prompt, but what actually occurs is that the reinforcement learning policy generates text, which<a id="_idIndexMarker792" class="calibre5 pcalibre1 pcalibre"/> is then supplied to the initial model to derive its relative probabilities for the KL penalty.</p>
<p class="calibre6">Optionally, RLHF can advance from this stage by cyclically updating both the reward model and the policy. As the reinforcement learning policy evolves, users can maintain the ranking of these outputs against the model’s previous versions. However, most papers haven’t yet addressed the implementation of this operation since the mode of deployment required to collect this type of data only works for dialogue agents who can access an active user base. Anthropic mentions this alternative as <strong class="bold">iterated online RLHF</strong> (as referred to in the original paper), where iterations of the policy are incorporated into the Elo<a id="_idIndexMarker793" class="calibre5 pcalibre1 pcalibre"/> ranking system across models. This brings about complex dynamics of the policy and reward model evolving, representing a complex and unresolved research question. In the next section, we will explain some <a id="_idIndexMarker794" class="calibre5 pcalibre1 pcalibre"/>well-kno<a id="_idTextAnchor428" class="calibre5 pcalibre1 pcalibre"/>wn open-source tools for RLHF.</p>
<h2 id="_idParaDest-171" class="calibre7"><a id="_idTextAnchor429" class="calibre5 pcalibre1 pcalibre"/>GPT-4</h2>
<p class="calibre6">At the time of writing this book, we<a id="_idIndexMarker795" class="calibre5 pcalibre1 pcalibre"/> know very little about the GPT-4 model design. As OpenAI is<a id="_idIndexMarker796" class="calibre5 pcalibre1 pcalibre"/> slow to reveal, it is assumed that GPT-4 is not a single model but a combination of eight 220-billion-parameter models, an assumption that is confirmed by key figures in the AI community. This assumption suggests OpenAI used a “mixture of experts” strategy, an ML design tactic that dates even before LLMs, to create the model. However, while we, the authors, support this assumption, it has not been officially confirmed by OpenAI.</p>
<p class="calibre6">Despite the speculation, GPT-4’s impressive performance is undeniable, regardless of its internal structure. Its capabilities in writing and coding tasks are remarkable, and the specifics of whether it’s one model or eight bundled together does not change its impact.</p>
<p class="calibre6">A common narrative suggests that OpenAI has managed expectations around GPT-4 deftly, focusing on its power and opting not to disclose specifications due to competitive pressures. The secrecy surrounding GPT-4 has led many to belie<a id="_idTextAnchor430" class="calibre5 pcalibre1 pcalibre"/>ve it to be a scientific marvel.</p>
<h2 id="_idParaDest-172" class="calibre7"><a id="_idTextAnchor431" class="calibre5 pcalibre1 pcalibre"/>LLaMA</h2>
<p class="calibre6">Meta has<a id="_idIndexMarker797" class="calibre5 pcalibre1 pcalibre"/> publicly launched LLaMA, a high-performing LLM <a id="_idIndexMarker798" class="calibre5 pcalibre1 pcalibre"/>aimed at aiding researchers in AI. This move allows individuals with limited access to extensive infrastructure to examine these models, thus broadening access in this rapidly evolving field.</p>
<p class="calibre6">LLaMA models are attractive because they require significantly less computational power and resources, allowing for the exploration of new approaches and use cases. Available in several sizes, these models are designed to be fine-tuned for various tasks and have been developed with responsible AI practices.</p>
<p class="calibre6">LLMs, despite their advancements, have limited research accessibility due to the resources required to train and run them. Smaller models, such as LLaMA, trained on more tokens, are simpler to retrain and adjust for specific use cases.</p>
<p class="calibre6">Similar to other<a id="_idIndexMarker799" class="calibre5 pcalibre1 pcalibre"/> models, LLaMA takes a sequence of words as input to predict the next word and generate text. Despite its capabilities, LLaMA shares the same challenges as other <a id="_idIndexMarker800" class="calibre5 pcalibre1 pcalibre"/>models regarding bias, toxic comments, and hallucinations. By sharing LLaMA’s code, Meta enables researchers to test new ways of addressing these issues in LLMs.</p>
<p class="calibre6">Meta emphasizes the need for cooperation across the AI community to establish guidelines around responsible AI and LLMs. They anticipate that LLaMA will facilitate new le<a id="_idTextAnchor432" class="calibre5 pcalibre1 pcalibre"/>arning and development in the field.</p>
<h2 id="_idParaDest-173" class="calibre7"><a id="_idTextAnchor433" class="calibre5 pcalibre1 pcalibre"/>PaLM</h2>
<p class="calibre6">PaLM is a 540-billion-parameter, densely-activated <a id="_idIndexMarker801" class="calibre5 pcalibre1 pcalibre"/>Transformer LM that was trained on 6,144 TPU v4 chips using Pathways, a new ML system, that <a id="_idIndexMarker802" class="calibre5 pcalibre1 pcalibre"/>enables highly efficient training across multiple TPU pods.</p>
<p class="calibre6">PaLM has been shown to achieve breakthrough performance on a variety of natural language tasks, including the following:</p>
<ul class="calibre14">
<li class="calibre15">Multi-step reasoning tasks</li>
<li class="calibre15">The recently released <strong class="bold">Beyond the Imitation Game </strong><strong class="bold">Benchmark</strong> (<strong class="bold">BIG-bench</strong>)</li>
<li class="calibre15">Multilingual tasks</li>
<li class="calibre15">Source code generation</li>
</ul>
<p class="calibre6">The BIG-bench benchmark is worth expanding on as it serves as a recognized collection of benchmarks to measure against. The BIG-bench is an extensive assessment mechanism specifically designed for large-scale LMs. It is a broad-based, community-focused benchmark that presents a diversity of tasks to evaluate a model’s performance in different disciplines and its competence in natural language comprehension, problem solving, and reasoning. With a total of 204 tasks from 450 contributors across 132 institutions, BIG-bench covers an eclectic mix of subjects including linguistics, childhood development, mathematics, common-sense reasoning, biology, physics, software development, and even social bias. It concentrates on challenges believed to be currently beyond the reach of existing LMs. The primary goal of BIG-bench extends beyond mere imitation or Turing test-style evaluations, aiming instead for a deeper, more nuanced appraisal of the abilities and constraints of <a id="_idIndexMarker803" class="calibre5 pcalibre1 pcalibre"/>these large models. This initiative is founded on the conviction that an open, collaborative approach to evaluation paves the way for a more comprehensive understanding of these LMs and their potential societal ramifications.</p>
<p class="calibre6">PaLM 540B excels beyond the fine-tuned state-of-the-art across various multi-step reasoning tasks and surpasses average human performance on the BIG-bench benchmark. Many BIG-bench tasks exhibit significant leaps in performance as PaLM scales to its largest size, demonstrating discontinuous improvements from the model scale. PaLM also has strong capabilities in multilingual tasks and source code generation. For example, PaLM can translate between 50 languages, and it can generate code in a variety of programming languages.</p>
<p class="calibre6">The authors of the <a id="_idIndexMarker804" class="calibre5 pcalibre1 pcalibre"/>PaLM paper also discuss the ethical considerations related to LLMs, and they discuss potential mitigation strategies. For example, they suggest that it is important to be aware of the potential for bias in LLMs and that it is important to develop techn<a id="_idTextAnchor434" class="calibre5 pcalibre1 pcalibre"/>iques for detecting and mitigating bias.</p>
<h3 class="calibre8">PaLM architecture</h3>
<p class="calibre6">PaLM employs the conventional Transformer model architecture in a decoder-exclusive setup, which allows each timestep to attend only to itself and preceding timesteps. Several modifications<a id="_idIndexMarker805" class="calibre5 pcalibre1 pcalibre"/> were applied to this setup, including the following:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">SwiGLU activation</strong>: Instead of standard ReLU, GeLU, or Swish activations, PaLM utilizes SwiGLU activations (<em class="italic">Swish(xW) · xV</em>) for the <strong class="bold">multilayer perceptron</strong> (<strong class="bold">MLP</strong>) intermediate activations due to their superior performance in enhancing quality. This approach, however, requires three matrix multiplications in the MLP as opposed to the conventional two.</li>
<li class="calibre15"><strong class="bold">Parallel layers</strong>: Rather than the typical “serialized” approach, PaLM uses a “parallel” formulation<a id="_idIndexMarker806" class="calibre5 pcalibre1 pcalibre"/> for each Transformer block.<p class="calibre6">The standard structure is given by the following:</p><p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/313.png" class="calibre312"/></p><p class="calibre6">The parallel structure is instead the following:</p><p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/314.png" class="calibre313"/></p><p class="calibre6">This leads to roughly 15% quicker training speed at larger scales due to the fusion of MLP and attention input matrix multiplications.</p></li>
<li class="calibre15"><strong class="bold">Multi-query attention</strong>: In the conventional Transformer formulation, <em class="italic">k</em> attention heads are employed. For each timestep, the input vector is linearly projected into query, key, and value tensors, which have a shape of [<em class="italic">k, h</em>]<em class="italic">,</em> where <em class="italic">h</em> denotes the size of the attention head. In the new approach, the projections for “key” and “value” are shared across all heads, meaning “key” and “value” are projected to [<em class="italic">1, h</em>], while “query” maintains the shape [<em class="italic">k, h</em>]. The authors claimed that this approach doesn’t notably affect model quality or training speed while it does result in significant cost reductions during autoregressive decoding. The reason for this lies in the inefficiency of standard multi-headed attention on accelerator hardware during autoregressive decoding, as the key/value tensors are not shared across examples and only one token is decoded at each moment.</li>
<li class="calibre15"><strong class="bold">Rotary Position Embedding (RoPE) embeddings</strong>: RoPE embeddings, shown to perform better on longer sequence lengths, are preferred over absolute or relative position embeddings.</li>
<li class="calibre15"><strong class="bold">Shared input-output embeddings</strong>: The input and output embedding matrices are shared, a practice that is common, though not universal, in previous work.</li>
<li class="calibre15"><strong class="bold">No biases</strong>: The model abstains from using biases in any dense kernels or layer norms, which enhances training stability for larger models.</li>
<li class="calibre15"><strong class="bold">Vocabulary</strong>: PaLM uses a 256k-token SentencePiece vocabulary designed for diverse languages in the training corpus, ensuring efficient training without excessive tokenization. This preserves all whitespaces and out-of-vocabulary Unicode characters while splitting numbers into individual digit tokens for clarity.</li>
</ul>
<p class="calibre6">Overall, PaLM is a powerful LM that has the potential to be used for a wide variety of applications. It is still under development, but it has already demonstrated the ability to achieve<a id="_idIndexMarker807" class="calibre5 pcalibre1 pcalibre"/> bre<a id="_idTextAnchor435" class="calibre5 pcalibre1 pcalibre"/>akthrough performance on a number of tasks.</p>
<h2 id="_idParaDest-174" class="calibre7"><a id="_idTextAnchor436" class="calibre5 pcalibre1 pcalibre"/>Open-source tools for RLHF</h2>
<p class="calibre6">OpenAI released the first open-source code to perform RLHF in 2019. They have implemented this approach to improve GPT-2 for different use cases such as summarization. Based on<a id="_idIndexMarker808" class="calibre5 pcalibre1 pcalibre"/> human feedback, the model is optimized to have outputs similar to <a id="_idIndexMarker809" class="calibre5 pcalibre1 pcalibre"/>humans, such as copying parts of the note. More information about this project can be found at the following link: <a href="https://openai.com/research/fine-tuning-gpt-2" class="calibre5 pcalibre1 pcalibre">https://openai.com/research/fine-tuning-gpt-2</a>.</p>
<p class="calibre6">The code is also available at the following link: <a href="https://github.com/openai/lm-human-preferences" class="calibre5 pcalibre1 pcalibre">https://github.com/openai/lm-human-preferences</a>.</p>
<p class="calibre6"><strong class="bold">Transformers Reinforcement Learning</strong> (<strong class="bold">TRL</strong>) is a tool crafted for fine-tuning pretrained LMs using PPO within the <a id="_idIndexMarker810" class="calibre5 pcalibre1 pcalibre"/>Hugging Face ecosystem. TRLX, an enhanced fork developed by CarperAI, is capable of handling larger models for both online and offline training. Currently, TRLX is equipped with a production-ready API supporting RLHF with PPO and <strong class="bold">implicit language Q-learning</strong> (<strong class="bold">ILQL</strong>) for<a id="_idIndexMarker811" class="calibre5 pcalibre1 pcalibre"/> deploying LLMs of up to 33 billion parameters. Future versions of TRLX aim to accommodate LMs of up to 200 billion parameters, making it ideal for ML engineers working at such scales.</p>
<ul class="calibre14">
<li class="calibre15">The code for TRL is available at the following link: <a href="https://github.com/lvwerra/trl" class="calibre5 pcalibre1 pcalibre">https://github.com/lvwerra/trl</a></li>
<li class="calibre15">The code for TRLX can be found at the following link: <a href="https://github.com/CarperAI/trlx" class="calibre5 pcalibre1 pcalibre">https://github.com/CarperAI/trlx</a>.</li>
</ul>
<p class="calibre6">Another good library is <strong class="bold">Reinforcement Learning for Language Models</strong> (<strong class="bold">RL4LMs</strong>). The RL4LMs project addresses the<a id="_idIndexMarker812" class="calibre5 pcalibre1 pcalibre"/> challenge of training LLMs to align with human preference metrics. It recognizes that many NLP tasks can be seen as sequence learning problems, but their application is limited due to issues such as reinforcement learning training instability, high variance in automated NLP metrics, and reward hacking. The <a id="_idIndexMarker813" class="calibre5 pcalibre1 pcalibre"/>project offers solutions by doing the following:</p>
<ul class="calibre14">
<li class="calibre15">Giving guidelines on when to use reinforcement learning and suggesting suitable NLP tasks/metrics via a continually updated benchmark called GRUE</li>
<li class="calibre15">Introducing a new reinforcement learning algorithm, <strong class="bold">Natural Language Policy Optimization </strong>(<strong class="bold">NLPO</strong>), designed to handle large language action spaces and reward variance better</li>
<li class="calibre15">Offering practical <a id="_idIndexMarker814" class="calibre5 pcalibre1 pcalibre"/>advice with high-quality implementations and hyperparameters of reinforcement learning, as well as other reinforcement learning algorithms, for training Transformers in the Hugging Face library</li>
</ul>
<p class="calibre6">The code for this project can be found at the <a id="_idTextAnchor437" class="calibre5 pcalibre1 pcalibre"/>following link: <a href="https://github.com/allenai/RL4LMs" class="calibre5 pcalibre1 pcalibre">https://github.com/allenai/RL4LMs</a>.</p>
<h1 id="_idParaDest-175" class="calibre4"><a id="_idTextAnchor438" class="calibre5 pcalibre1 pcalibre"/>Summary</h1>
<p class="calibre6">In this chapter, we’ve delved into the dynamic and complex world of state-of-the-art LLMs. We’ve discussed their remarkable generalization capabilities, making them versatile tools for a wide range of tasks. We also highlighted the crucial aspect of understanding complex contexts, where these models excel by grasping the nuances of language and the intricacies of various subject matters.</p>
<p class="calibre6">Additionally, we explored the paradigm of RLHF and how it is being employed to enhance LMs. RLHF leverages scalar feedback to improve LMs by mimicking human judgments, thereby helping to mitigate some of the common pitfalls encountered in NLP tasks.</p>
<p class="calibre6">We discussed technical requirements for working with these models, emphasizing the need for foundational knowledge in areas such as Transformers, reinforcement learning, and coding skills.</p>
<p class="calibre6">This chapter also touched on some prominent LMs such as GPT-4 and LLaMA, discussing their architecture, methods, and performance. We highlighted the strategies some libraries employ to interpret LM predictions, such as the removal of certain words and analyzing gradient changes.</p>
<p class="calibre6">To sum up, this chapter offers a comprehensive overview of the current state of LLMs, exploring their capabilities, challenges, the methods used to refine them, and the evolving tools and measures for their evaluation and interpretation.</p>
<h1 id="_idParaDest-176" class="calibre4"><a id="_idTextAnchor439" class="calibre5 pcalibre1 pcalibre"/>References</h1>
<ul class="calibre14">
<li class="calibre15"><em class="italic">Hugging </em><em class="italic">Face</em>: <a href="http://huggingface.co" class="calibre5 pcalibre1 pcalibre">huggingface.co</a></li>
<li class="calibre15"><em class="italic">Large language </em><em class="italic">model</em>: <a href="https://en.m.wikipedia.org/wiki/Large_language_model#" class="calibre5 pcalibre1 pcalibre">https://en.m.wikipedia.org/wiki/Large_language_model#</a></li>
<li class="calibre15"><em class="italic">Zhao, Wayne Xin, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min et al. “A survey of large language models.”</em> arXiv preprint arXiv:2303.18223 (2023).</li>
<li class="calibre15"><em class="italic">Introducing LLaMA: A foundational, 65-billion-parameter large language </em><em class="italic">model</em>: <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/" class="calibre5 pcalibre1 pcalibre">https://ai.facebook.com/blog/large-language-model-llama-meta-ai/</a></li>
<li class="calibre15"><em class="italic">Model </em><em class="italic">Details</em>: <a href="https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md" class="calibre5 pcalibre1 pcalibre">https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md</a></li>
<li class="calibre15"><em class="italic">Touvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière et al. “Llama: Open and efficient foundation language models.” arXiv </em>preprint arXiv:2302.13971 (2023).</li>
<li class="calibre15"><em class="italic">Elo rating </em><em class="italic">system</em>: <a href="https://en.wikipedia.org/wiki/Elo_rating_system" class="calibre5 pcalibre1 pcalibre">https://en.wikipedia.org/wiki/Elo_rating_system</a></li>
<li class="calibre15"><em class="italic">Chowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham et al. “Palm: Scaling language modeling with pathways.” </em>arXiv preprint arXiv:2204.02311 (2022).</li>
<li class="calibre15"><em class="italic">BIG-bench</em>: <a href="https://github.com/google/BIG-bench" class="calibre5 pcalibre1 pcalibre">https://github.com/google/BIG-bench</a></li>
<li class="calibre15"><em class="italic">Srivastava, Aarohi, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown et al. “Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.” </em>arXiv preprint arXiv:2206.04615 (2022).</li>
</ul>
</div>
</body></html>