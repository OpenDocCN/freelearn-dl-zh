<html><head></head><body>
<div id="_idContainer361" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-142"><a id="_idTextAnchor365" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1.1">7</span></h1>
<h1 id="_idParaDest-143" class="calibre4"><a id="_idTextAnchor366" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2.1">Demystifying Large Language Models: Theory, Design, and Langchain Implementation</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.3.1">In this chapter, we delve deep into the intricate world of </span><strong class="bold"><span class="kobospan" id="kobo.4.1">large language models</span></strong><span class="kobospan" id="kobo.5.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.6.1">LLMs</span></strong><span class="kobospan" id="kobo.7.1">) and the underpinning mathematical concepts that fuel their performance. </span><span class="kobospan" id="kobo.7.2">The advent of these models has revolutionized the field of </span><strong class="bold"><span class="kobospan" id="kobo.8.1">natural language processing</span></strong><span class="kobospan" id="kobo.9.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.10.1">NLP</span></strong><span class="kobospan" id="kobo.11.1">), offering unparalleled proficiency in understanding, generating, and interacting with </span><span><span class="kobospan" id="kobo.12.1">human language.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.13.1">LLMs are a subset of </span><strong class="bold"><span class="kobospan" id="kobo.14.1">artificial intelligence</span></strong><span class="kobospan" id="kobo.15.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.16.1">AI</span></strong><span class="kobospan" id="kobo.17.1">) models that can understand and generate human-like text. </span><span class="kobospan" id="kobo.17.2">They achieve this by being trained on a diverse range of internet text, thus learning an extensive array of facts about the world. </span><span class="kobospan" id="kobo.17.3">They also learn to predict what comes next in a piece of text, which enables them to generate creative, fluent, and contextually </span><span><span class="kobospan" id="kobo.18.1">coherent sentences.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.19.1">As we explore the operations of LLMs, we will introduce the key metric of </span><strong class="bold"><span class="kobospan" id="kobo.20.1">perplexity</span></strong><span class="kobospan" id="kobo.21.1">, a measurement of uncertainty that is pivotal in determining the performance of these models. </span><span class="kobospan" id="kobo.21.2">A lower perplexity indicates the confidence that a </span><strong class="bold"><span class="kobospan" id="kobo.22.1">language model</span></strong><span class="kobospan" id="kobo.23.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.24.1">LM</span></strong><span class="kobospan" id="kobo.25.1">) has in predicting the next word in a sequence, thus showcasing </span><span><span class="kobospan" id="kobo.26.1">its proficiency.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.27.1">This chapter draws on multiple insightful publications that delve into the mathematical insights of LLMs. </span><span class="kobospan" id="kobo.27.2">Some of these include </span><em class="italic"><span class="kobospan" id="kobo.28.1">A Neural Probabilistic Language Model</span></em><span class="kobospan" id="kobo.29.1">, </span><em class="italic"><span class="kobospan" id="kobo.30.1">Attention is All You Need</span></em><span class="kobospan" id="kobo.31.1">, and </span><em class="italic"><span class="kobospan" id="kobo.32.1">PaLM: Scaling Language Modeling with Pathways</span></em><span class="kobospan" id="kobo.33.1">. </span><span class="kobospan" id="kobo.33.2">These sources will guide us in understanding the robust mechanisms that underpin LLMs and their </span><span><span class="kobospan" id="kobo.34.1">exceptional capabilities.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.35.1">We will also explore the emerging field of </span><strong class="bold"><span class="kobospan" id="kobo.36.1">reinforcement learning from human feedback</span></strong><span class="kobospan" id="kobo.37.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.38.1">RLHF</span></strong><span class="kobospan" id="kobo.39.1">) in the context of LMs. </span><span class="kobospan" id="kobo.39.2">RLHF has proven to be a powerful tool in fine-tuning the performance of LLMs, thereby leading to more accurate and meaningful </span><span><span class="kobospan" id="kobo.40.1">generated texts.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.41.1">With a comprehensive understanding of the mathematical foundations of LLMs and a deep dive into RLHF, we will gain a robust knowledge of these advanced AI systems, paving the way for future innovations and advancements in </span><span><span class="kobospan" id="kobo.42.1">the field.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.43.1">Finally, we will discuss the detailed architecture and design of recent models, such as </span><strong class="bold"><span class="kobospan" id="kobo.44.1">Pathways Language Model</span></strong><span class="kobospan" id="kobo.45.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.46.1">PaLM</span></strong><span class="kobospan" id="kobo.47.1">), </span><strong class="bold"><span class="kobospan" id="kobo.48.1">Large Language Model Meta AI</span></strong><span class="kobospan" id="kobo.49.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.50.1">LLaMA</span></strong><span class="kobospan" id="kobo.51.1">), </span><span><span class="kobospan" id="kobo.52.1">and GPT-4.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.53.1">Now, let’s look at the topics covered in </span><span><span class="kobospan" id="kobo.54.1">this chapter:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.55.1">What are LLMs and how are they different </span><span><span class="kobospan" id="kobo.56.1">from LMs?</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.57.1">Motivations for developing and </span><span><span class="kobospan" id="kobo.58.1">using LLMs</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.59.1">Challenges in </span><span><span class="kobospan" id="kobo.60.1">developing LLMs</span></span></li>
</ul>
<h1 id="_idParaDest-144" class="calibre4"><a id="_idTextAnchor367" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.61.1">Technical requirements</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.62.1">For this chapter, you are expected to possess a solid foundation in </span><strong class="bold"><span class="kobospan" id="kobo.63.1">machine learning</span></strong><span class="kobospan" id="kobo.64.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.65.1">ML</span></strong><span class="kobospan" id="kobo.66.1">) concepts, particularly in the areas of </span><strong class="bold"><span class="kobospan" id="kobo.67.1">Transformers</span></strong><span class="kobospan" id="kobo.68.1"> and </span><strong class="bold"><span class="kobospan" id="kobo.69.1">reinforcement learning</span></strong><span class="kobospan" id="kobo.70.1">. </span><span class="kobospan" id="kobo.70.2">An understanding of Transformer-based models, which underpin many of today’s LLMs, is vital. </span><span class="kobospan" id="kobo.70.3">This includes familiarity with concepts such as self-attention mechanisms, positional encoding, and the structure of </span><span><span class="kobospan" id="kobo.71.1">encoder-decoder architectures.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.72.1">Knowledge of reinforcement learning principles is also essential, as we will delve into the application of RLHF in the fine-tuning of LMs. </span><span class="kobospan" id="kobo.72.2">Familiarity with concepts such as policy gradients, reward functions, and Q-learning will greatly enhance your comprehension of </span><span><span class="kobospan" id="kobo.73.1">this content.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.74.1">Lastly, coding proficiency, specifically in Python, is crucial. </span><span class="kobospan" id="kobo.74.2">This is because many of the concepts will be demonstrated and explored through the lens of programming. </span><span class="kobospan" id="kobo.74.3">Experience with PyTorch or TensorFlow, popular ML libraries, and Hugging Face’s Transformers library, a key resource for working with transformer models, will also </span><span><span class="kobospan" id="kobo.75.1">be beneficial.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.76.1">However, don’t be discouraged if you feel you’re lacking in some areas. </span><span class="kobospan" id="kobo.76.2">This chapter aims to walk you through the complexities of these subjects, bridging any knowledge gaps along the way. </span><span class="kobospan" id="kobo.76.3">So, come prepared with a mindset for learning, and let’s delve into the fascinating world </span><span><span class="kobospan" id="kobo.77.1">of LLMs!</span></span></p>
<h1 id="_idParaDest-145" class="calibre4"><a id="_idTextAnchor368" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.78.1">What are LLMs and how are they different from LMs?</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.79.1">An LM is a type of ML model that is trained to predict the next word (or character or subword, depending</span><a id="_idIndexMarker719" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.80.1"> on the granularity of the model) in a sequence, given the words that came before it (or in some models, the surrounding words). </span><span class="kobospan" id="kobo.80.2">It’s a probabilistic model that is capable of generating text that follows a certain linguistic style </span><span><span class="kobospan" id="kobo.81.1">or pattern.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.82.1">Before the advent of Transformer-based </span><a id="_idIndexMarker720" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.83.1">models such as </span><strong class="bold"><span class="kobospan" id="kobo.84.1">generative pretrained Transformers</span></strong><span class="kobospan" id="kobo.85.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.86.1">GPTs</span></strong><span class="kobospan" id="kobo.87.1">) and </span><strong class="bold"><span class="kobospan" id="kobo.88.1">Bidirectional Encoder Representations from Transformers</span></strong><span class="kobospan" id="kobo.89.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.90.1">BERT</span></strong><span class="kobospan" id="kobo.91.1">), there were several other types of LMs widely </span><a id="_idIndexMarker721" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.92.1">used in NLP tasks. </span><span class="kobospan" id="kobo.92.2">The following subsections discuss a few </span><span><span class="kobospan" id="kobo.93.1">of the</span><a id="_idTextAnchor369" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.94.1">m.</span></span></p>
<h2 id="_idParaDest-146" class="calibre7"><a id="_idTextAnchor370" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.95.1">n-gram models</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.96.1">These are some of the</span><a id="_idIndexMarker722" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.97.1"> simplest LMs. </span><span class="kobospan" id="kobo.97.2">An </span><em class="italic"><span class="kobospan" id="kobo.98.1">n</span></em><span class="kobospan" id="kobo.99.1">-gram model uses the (</span><em class="italic"><span class="kobospan" id="kobo.100.1">n</span></em><span class="kobospan" id="kobo.101.1">-1) previous words to predict the </span><em class="italic"><span class="kobospan" id="kobo.102.1">n</span></em><span class="kobospan" id="kobo.103.1">th word in a sentence. </span><span class="kobospan" id="kobo.103.2">For example, in a bigram (2-gram) model, we would use the previous word to predict</span><a id="_idIndexMarker723" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.104.1"> the next word. </span><span class="kobospan" id="kobo.104.2">These models are easy to implement and computationally efficient, but they typically don’t perform as well as more complex models because they don’t capture long-range dependencies between words. </span><span class="kobospan" id="kobo.104.3">Their performance also degrades as </span><em class="italic"><span class="kobospan" id="kobo.105.1">n</span></em><span class="kobospan" id="kobo.106.1"> increases, as they suffer from data sparsity issues (not having enough data to accurately estimate the probabilities for all </span><span><span class="kobospan" id="kobo.107.1">possible </span></span><span><em class="italic"><span class="kobospan" id="kobo.108.1">n</span></em></span><span><span class="kobospan" id="kobo.109.1">-gra</span><a id="_idTextAnchor371" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.110.1">ms).</span></span></p>
<h2 id="_idParaDest-147" class="calibre7"><a id="_idTextAnchor372" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.111.1">Hidden Markov models (HMMs)</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.112.1">These models consider the “hidden” states that </span><a id="_idIndexMarker724" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.113.1">generate the observed data. </span><span class="kobospan" id="kobo.113.2">In the context of language modeling, each word would be an observed state, and the “hidden” state would be </span><a id="_idIndexMarker725" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.114.1">some kind of linguistic feature that’s not directly observable (such as the part of speech of the word). </span><span class="kobospan" id="kobo.114.2">However, like </span><em class="italic"><span class="kobospan" id="kobo.115.1">n</span></em><span class="kobospan" id="kobo.116.1">-gram models, HMMs struggle to capture long-range dependencies </span><span><span class="kobospan" id="kobo.117.1">between </span><a id="_idTextAnchor373" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.118.1">words.</span></span></p>
<h2 id="_idParaDest-148" class="calibre7"><a id="_idTextAnchor374" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.119.1">Recurrent neural networks (RNNs)</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.120.1">These are a type of neural network where connections between nodes form a directed graph</span><a id="_idIndexMarker726" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.121.1"> along a temporal sequence. </span><span class="kobospan" id="kobo.121.2">This allows them to use their internal state (memory) to</span><a id="_idIndexMarker727" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.122.1"> process sequences of inputs, making them ideal for language modeling. </span><span class="kobospan" id="kobo.122.2">They can capture long-range dependencies between words, but they struggle with the so-called vanishing gradient problem, which makes it difficult to learn these dependencies </span><span><span class="kobospan" id="kobo.123.1">in p</span><a id="_idTextAnchor375" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.124.1">ractice.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.125.1">Long short-term memory (LSTM) networks</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.126.1">An LSTM network is</span><a id="_idIndexMarker728" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.127.1"> a special kind of RNN that is designed to learn long-term dependencies. </span><span class="kobospan" id="kobo.127.2">They do this by using a series of “gates” that control the flow of information in and out of the</span><a id="_idIndexMarker729" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.128.1"> memory state of the network. </span><span class="kobospan" id="kobo.128.2">LSTMs were a big step forward in the state of the art of </span><span><span class="kobospan" id="kobo.129.1">language</span><a id="_idTextAnchor376" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.130.1"> modeling.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.131.1">Gated recurrent unit (GRU) networks</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.132.1">These are a variation</span><a id="_idIndexMarker730" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.133.1"> of LSTMs that use a slightly different set of gates in their architecture. </span><span class="kobospan" id="kobo.133.2">They’re often simpler and faster to train than LSTMs, but whether they perform better or worse than LSTMs tends to depend on the specific task </span><span><span class="kobospan" id="kobo.134.1">at hand.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.135.1">Each of these models has its own strengths and weaknesses, and none of them are inherently better or worse than the others – it all depends on the specific task and dataset. </span><span class="kobospan" id="kobo.135.2">However, Transformer-based models have generally outperformed all of these models in a wide range of tasks, leading to their current popularity in the fi</span><a id="_idTextAnchor377" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.136.1">eld </span><span><span class="kobospan" id="kobo.137.1">of NLP.</span></span></p>
<h1 id="_idParaDest-149" class="calibre4"><a id="_idTextAnchor378" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.138.1">How LLMs stand out</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.139.1">LLMs, such as GPT-3 and </span><a id="_idIndexMarker731" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.140.1">GPT-4, are simply LMs that are trained on a very large amount of text and have a very large number of parameters. </span><span class="kobospan" id="kobo.140.2">The larger the model (in terms of parameters and training data), the more capable it is of understanding and generating complex and varied texts. </span><span class="kobospan" id="kobo.140.3">Here are some key ways in which LLMs differ from </span><span><span class="kobospan" id="kobo.141.1">smaller LMs:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.142.1">Data</span></strong><span class="kobospan" id="kobo.143.1">: LLMs are trained </span><a id="_idIndexMarker732" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.144.1">on vast amounts of data. </span><span class="kobospan" id="kobo.144.2">This allows them to learn from a wide range of linguistic patterns, styles, </span><span><span class="kobospan" id="kobo.145.1">and topics.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.146.1">Parameters</span></strong><span class="kobospan" id="kobo.147.1">: LLMs have a huge number of parameters. </span><span class="kobospan" id="kobo.147.2">Parameters in an ML model are the parts of the model that are learned from the training data. </span><span class="kobospan" id="kobo.147.3">The more parameters a model has, the more complex patterns it </span><span><span class="kobospan" id="kobo.148.1">can learn.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.149.1">Performance</span></strong><span class="kobospan" id="kobo.150.1">: Because they’re trained on more data and have more parameters, LLMs generally perform better than smaller ones. </span><span class="kobospan" id="kobo.150.2">They’re capable of generating more coherent and diverse texts, and they’re better at understanding context, making inferences, and even answering questions or generating texts on a wide range </span><span><span class="kobospan" id="kobo.151.1">of topics.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.152.1">Compute resources</span></strong><span class="kobospan" id="kobo.153.1">: LLMs require a significant amount of computational resources to train, both in terms of processing power and memory. </span><span class="kobospan" id="kobo.153.2">They also take longer </span><span><span class="kobospan" id="kobo.154.1">to train.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.155.1">Storage and inference time</span></strong><span class="kobospan" id="kobo.156.1">: Large models also require more storage space, and it takes longer to generate predictions (although this inference time is still typically quite fast on </span><span><span class="kobospan" id="kobo.157.1">modern hardware).</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.158.1">Thus, we can say that LLMs are essentially scaled-up versions of smaller LMs. </span><span class="kobospan" id="kobo.158.2">They’re trained on more data, have more parameters, and are generally capable of producing higher-quality results, but they also require more resources to train and use. </span><span class="kobospan" id="kobo.158.3">Besides that, an important advantage of an LLM is that we can train them unsupervised on a large corpus of data and then fine-tune them with a limited amount of data for </span><span><span class="kobospan" id="kobo.159.1">dif</span><a id="_idTextAnchor379" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.160.1">ferent tasks.</span></span></p>
<h1 id="_idParaDest-150" class="calibre4"><a id="_idTextAnchor380" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.161.1">Motivations for developing and using LLMs</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.162.1">The motivation to develop and use LLMs arises from several factors related to the capabilities of these models, and the potential</span><a id="_idIndexMarker733" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.163.1"> benefits </span><a id="_idIndexMarker734" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.164.1">they can bring in diverse applications. </span><span class="kobospan" id="kobo.164.2">The following subsections detail a few of these </span><span><span class="kobospan" id="kobo.165.1">k</span><a id="_idTextAnchor381" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.166.1">ey motivations.</span></span></p>
<h2 id="_idParaDest-151" class="calibre7"><a id="_idTextAnchor382" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.167.1">Improved performance</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.168.1">LLMs, when trained with sufficient data, generally demonstrate better performance compared to smaller </span><a id="_idIndexMarker735" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.169.1">models. </span><span class="kobospan" id="kobo.169.2">They are more capable of understanding context, identifying nuances, and generating coherent and contextually relevant responses. </span><span class="kobospan" id="kobo.169.3">This performance gain applies to a wide range of tasks in NLP, including text classification, named entity recognition, sentiment analysis, machine translation, question answering, and text generation. </span><span class="kobospan" id="kobo.169.4">As shown in </span><em class="italic"><span class="kobospan" id="kobo.170.1">Table 7.1</span></em><span class="kobospan" id="kobo.171.1">, the performance of BERT – one of the first well-known LLMs – and GPT is compared to the previous models on the </span><strong class="bold"><span class="kobospan" id="kobo.172.1">General Language Understanding Evaluation</span></strong><span class="kobospan" id="kobo.173.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.174.1">GLUE</span></strong><span class="kobospan" id="kobo.175.1">) benchmark. </span><span class="kobospan" id="kobo.175.2">The GLUE benchmark is a</span><a id="_idIndexMarker736" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.176.1"> collection of diverse </span><strong class="bold"><span class="kobospan" id="kobo.177.1">natural language understanding</span></strong><span class="kobospan" id="kobo.178.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.179.1">NLU</span></strong><span class="kobospan" id="kobo.180.1">) tasks designed to evaluate the performance of models across multiple linguistic challenges. </span><span class="kobospan" id="kobo.180.2">The benchmark</span><a id="_idIndexMarker737" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.181.1"> encompasses tasks such as sentiment analysis, question answering, and textual entailment, among others. </span><span class="kobospan" id="kobo.181.2">It’s a widely recognized standard in the field of NLU, providing a comprehensive suite for comparing and improving language understanding models. </span><span class="kobospan" id="kobo.181.3">It can be seen that its performance is better in </span><span><span class="kobospan" id="kobo.182.1">all tasks:</span></span></p>
<table class="no-table-style" id="table001-4">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><strong class="bold"><span class="kobospan" id="kobo.183.1">Model</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.184.1">Average (in </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.185.1">all tasks)</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><strong class="bold"><span class="kobospan" id="kobo.186.1">Sentiment analysis</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><strong class="bold"><span class="kobospan" id="kobo.187.1">Grammatical</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><strong class="bold"><span class="kobospan" id="kobo.188.1">Similarity</span></strong></span></p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.189.1">BERT large</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.190.1">82.1</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.191.1">94.9</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.192.1">60.5</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.193.1">86.5</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.194.1">BERT base</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.195.1">79.6</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.196.1">93.5</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.197.1">52.1</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.198.1">85.8</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.199.1">OpenAI GPT</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.200.1">75.1</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.201.1">91.3</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.202.1">45.4</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.203.1">80.0</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.204.1">Pre-open AI </span><strong class="bold"><span class="kobospan" id="kobo.205.1">State of the </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.206.1">Art </span></strong></span><span><span class="kobospan" id="kobo.207.1">(</span></span><span><strong class="bold"><span class="kobospan" id="kobo.208.1">STOA</span></strong></span><span><span class="kobospan" id="kobo.209.1">)</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.210.1">74.0</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.211.1">93.2</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.212.1">35.0</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.213.1">81.0</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.214.1">Bidirectional Long Short-Term memory (BiLSTM) + Embeddings from Language Model (ELMo) + </span><span><span class="kobospan" id="kobo.215.1">Attention</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.216.1">71.0</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.217.1">90.4</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.218.1">36.0</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.219.1">73.3</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.220.1">Table 7.1 – Comparing different models’ performance on GLUE (this comparison is based on 2018 when BERT and GPT were released)</span></p>
<h2 id="_idParaDest-152" class="calibre7"><a id="_idTextAnchor383" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.221.1">Broad generalization</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.222.1">LLMs trained on diverse </span><a id="_idIndexMarker738" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.223.1">datasets can generalize better across different tasks, domains, or styles of language. </span><span class="kobospan" id="kobo.223.2">They can effectively learn from the training data to identify and understand a wide range </span><a id="_idIndexMarker739" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.224.1">of linguistic patterns, styles, and topics. </span><span class="kobospan" id="kobo.224.2">This broad generalization capability makes them versatile for various applications, from chatbots to content creation to </span><span><span class="kobospan" id="kobo.225.1">information retrieval.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.226.1">When an LM is bigger, it means it has more parameters. </span><span class="kobospan" id="kobo.226.2">These parameters allow the model to capture and encode more complex relationships and nuances within the data. </span><span class="kobospan" id="kobo.226.3">In other words, a bigger model can learn and retain more information from the training data. </span><span class="kobospan" id="kobo.226.4">As such, it is better equipped to handle a wider array of tasks and contexts post-training. </span><span class="kobospan" id="kobo.226.5">It is this increased complexity and capacity that makes bigger LMs more generalizable across different tasks. </span><span class="kobospan" id="kobo.226.6">As we can see in </span><span><em class="italic"><span class="kobospan" id="kobo.227.1">Figure 7</span></em></span><em class="italic"><span class="kobospan" id="kobo.228.1">.1</span></em><span class="kobospan" id="kobo.229.1">, the bigger LMs perform better in </span><span><span class="kobospan" id="kobo.230.1">different tasks.</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer347">
<span class="kobospan" id="kobo.231.1"><img alt="Figure 7.1 – LLMs performance based on their size and training" src="image/B18949_07_1.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.232.1">Figure 7.1 – LLMs performance based on their size and training</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.233.1">We can also see the </span><a id="_idIndexMarker740" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.234.1">progress in the</span><a id="_idIndexMarker741" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.235.1"> development of the LLMs within the last three years in </span><span><em class="italic"><span class="kobospan" id="kobo.236.1">Figure 7</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.237.1">.2</span></em></span><span><span class="kobospan" id="kobo.238.1">.</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer348">
<span class="kobospan" id="kobo.239.1"><img alt="Figure 7.2 – The released LMs within 2019 ﻿to 2023  (the publicly available models are highlighted) ﻿" src="image/B18949_07_2.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.240.1">Figure 7.2 – The released LMs within 2019 to 2023  (the publicly available models are highlighted) </span></p>
<p class="calibre6"><span class="kobospan" id="kobo.241.1">However, it’s important to note that while larger models tend to be more generalizable, they also pose challenges such as increased computational requirements and the</span><a id="_idIndexMarker742" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.242.1"> risk of overfitting. </span><span class="kobospan" id="kobo.242.2">It is also essential to ensure that the training data is representative of the tasks and domains the model is expected to perform in, as models might carry over any biases present in the </span><span><span class="kobospan" id="kobo.243.1">training dat</span><a id="_idTextAnchor384" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.244.1">a.</span></span></p>
<h2 id="_idParaDest-153" class="calibre7"><a id="_idTextAnchor385" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.245.1">Few-shot learning</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.246.1">LLMs such as GPT-3, GPT-3.5, and GPT-4 have demonstrated impressive few-shot learning capabilities. </span><span class="kobospan" id="kobo.246.2">Given a</span><a id="_idIndexMarker743" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.247.1"> few examples (the “shots”), these models can generalize to complete similar tasks effectively. </span><span class="kobospan" id="kobo.247.2">This makes adjusting and deploying these models in real-world applications more efficient. </span><span class="kobospan" id="kobo.247.3">The prompts can be designed to include information for the model to refer to, such as example questions and their </span><span><span class="kobospan" id="kobo.248.1">respective answers.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.249.1">The model temporarily learns from given examples and refers to given information as an additional source. </span><span class="kobospan" id="kobo.249.2">For example, when the LLM is used as a personal assistant or advisor, background information about the user can be appended to the prompt, allowing the</span><a id="_idIndexMarker744" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.250.1"> model to “get to know you,” as it</span><a id="_idIndexMarker745" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.251.1"> uses your personal information prompts as </span><span><span class="kobospan" id="kobo.252.1">a reference.</span></span></p>
<h2 id="_idParaDest-154" class="calibre7"><span class="kobospan" id="kobo.253.1">Understanding complex con</span><a id="_idTextAnchor386" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.254.1">texts</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.255.1">LLMs have the advantage of understanding complex contexts due to their extensive training on a wide range of data, including various topics, literary styles, and nuances as well as their deep architecture and large parameter space. </span><span class="kobospan" id="kobo.255.2">This capacity allows them to </span><a id="_idIndexMarker746" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.256.1">comprehend and generate appropriate responses even in complex or </span><span><span class="kobospan" id="kobo.257.1">nuanced situations.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.258.1">For example, consider a scenario where a user asks the model to summarize a complicated scientific article. </span><span class="kobospan" id="kobo.258.2">An LLM can understand the context and the technical language used in the article and generate a coherent and </span><span><span class="kobospan" id="kobo.259.1">simplified su</span><a id="_idTextAnchor387" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.260.1">mmary.</span></span></p>
<h2 id="_idParaDest-155" class="calibre7"><a id="_idTextAnchor388" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.261.1">Multilingual capabilities</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.262.1">LLMs can handle </span><a id="_idIndexMarker747" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.263.1">multiple languages effectively, making them suitable for global applications. </span><span class="kobospan" id="kobo.263.2">Here are a few well-known </span><a id="_idIndexMarker748" class="calibre5 pcalibre1 pcalibre"/><span><span class="kobospan" id="kobo.264.1">multiling</span><a id="_idTextAnchor389" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.265.1">ual LMs.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.266.1">mBERT (multilingual BERT)</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.267.1">An extension of BERT, mBERT is pretrained on the top 104 languages with the largest Wikipedia </span><a id="_idIndexMarker749" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.268.1">using a masked </span><span><span class="kobospan" id="kobo.269.1">LM o</span><a id="_idTextAnchor390" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.270.1">bjective.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.271.1">Cross-lingual language model (XLM)</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.272.1">This is trained in</span><a id="_idIndexMarker750" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.273.1"> 100 languages. </span><span class="kobospan" id="kobo.273.2">It extends the BERT model to include several methods for cross-lingual </span><span><span class="kobospan" id="kobo.274.1">model</span><a id="_idTextAnchor391" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.275.1"> training.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.276.1">XLM-RoBERTa</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.277.1">XLM-RoBERTa extends RoBERTa, which itself is an optimized version of BERT, and is trained on a </span><a id="_idIndexMarker751" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.278.1">much larger multilingual corpus covering </span><span><span class="kobospan" id="kobo.279.1">more</span><a id="_idTextAnchor392" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.280.1"> languages.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.281.1">MarianMT</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.282.1">Part of </span><a id="_idIndexMarker752" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.283.1">Hugging Face’s Transformers</span><a id="_idIndexMarker753" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.284.1"> library, MarianMT is a state-of-the-art Transformer-based model optimized for </span><span><span class="kobospan" id="kobo.285.1">trans</span><a id="_idTextAnchor393" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.286.1">lation tasks.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.287.1">DistilBERT Multilingual</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.288.1">This is a smaller and faster </span><a id="_idIndexMarker754" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.289.1">version of mBERT, achieved through a </span><span><span class="kobospan" id="kobo.290.1">distill</span><a id="_idTextAnchor394" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.291.1">ation process.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.292.1">T2T (T5) Multilingual</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.293.1">This is a variant of the </span><strong class="bold"><span class="kobospan" id="kobo.294.1">Text-to-Text Transfer Transformer</span></strong><span class="kobospan" id="kobo.295.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.296.1">T5</span></strong><span class="kobospan" id="kobo.297.1">) model, which is fine-tuned for </span><span><span class="kobospan" id="kobo.298.1">translation</span></span><span><a id="_idIndexMarker755" class="calibre5 pcalibre1 pcalibre"/></span><span><span class="kobospan" id="kobo.299.1"> tasks.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.300.1">These models have achieved</span><a id="_idIndexMarker756" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.301.1"> significant results in a variety of tasks, such as translation, named entity recognition, part-of-speech tagging, and sentiment analysis in </span><span><span class="kobospan" id="kobo.302.1">mul</span><a id="_idTextAnchor395" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.303.1">tiple languages.</span></span></p>
<h2 id="_idParaDest-156" class="calibre7"><a id="_idTextAnchor396" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.304.1">Human-like text generation</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.305.1">LLMs have shown a remarkable capability in generating human-like text. </span><span class="kobospan" id="kobo.305.2">They can create contextually</span><a id="_idIndexMarker757" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.306.1"> appropriate responses in </span><a id="_idIndexMarker758" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.307.1">conversations, write essays, and generate creative content such as poetry and stories. </span><span class="kobospan" id="kobo.307.2">Models such as GPT-3, ChatGPT, and GPT-4 have shown good results in text </span><span><span class="kobospan" id="kobo.308.1">generation tasks.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.309.1">While the advantages are many, it’s important to note that there are also challenges and potential risks associated with the use of LLMs. </span><span class="kobospan" id="kobo.309.2">They require significant computational resources to train and deploy, and there are ongoing concerns related to their potential to generate harmful or biased content, their interpretability, and their environmental impact. </span><span class="kobospan" id="kobo.309.3">Researchers are actively working on ways to mitigate these issues while leveraging the powerful capabilities of </span><span><span class="kobospan" id="kobo.310.1">these models.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.311.1">Due to these reasons, companies are trying to implement and train larger LMs (</span><span><em class="italic"><span class="kobospan" id="kobo.312.1">Figure 7</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.313.1">.3</span></em></span><span><span class="kobospan" id="kobo.314.1">):</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer349">
<span class="kobospan" id="kobo.315.1"><img alt="Figure 7.3 – Newer LMs and their size, as well a﻿s the developers" src="image/B18949_07_3.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.316.1">Figure 7.3 – Newer LMs and their size, as well a</span><a id="_idTextAnchor397" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.317.1">s the developers</span></p>
<h1 id="_idParaDest-157" class="calibre4"><a id="_idTextAnchor398" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.318.1">Challenges in developing LLMs</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.319.1">Developing LLMs </span><a id="_idIndexMarker759" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.320.1">poses a unique set of challenges, including but not limited to handling massive amounts of data, requiring vast computational resources, and the risk of introducing or perpetuating bias. </span><span class="kobospan" id="kobo.320.2">The following subsections outline the detailed explanations of</span><a id="_idTextAnchor399" class="calibre5 pcalibre1 pcalibre"/> <span><span class="kobospan" id="kobo.321.1">these challenges.</span></span></p>
<h2 id="_idParaDest-158" class="calibre7"><a id="_idTextAnchor400" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.322.1">Amounts of data</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.323.1">LLMs require enormous amounts of data for training. </span><span class="kobospan" id="kobo.323.2">As the model size grows, so does the need for diverse, high-quality training data. </span><span class="kobospan" id="kobo.323.3">However, collecting and curating such large</span><a id="_idIndexMarker760" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.324.1"> datasets is a challenging task. </span><span class="kobospan" id="kobo.324.2">It can be time - consuming and expensive. </span><span class="kobospan" id="kobo.324.3">There’s also the risk of inadvertently including sensitive or inappropriate data in the training set. </span><span class="kobospan" id="kobo.324.4">To have more of an idea, BERT has been trained using 3.3 billion words from Wikipedia and BookCorpus. </span><span class="kobospan" id="kobo.324.5">GPT-2 has been trained on 40 GB of text data, and GPT-3 has been trained on 570 GB of text data. </span><em class="italic"><span class="kobospan" id="kobo.325.1">Table 7.2</span></em><span class="kobospan" id="kobo.326.1"> shows the number of parameters and size of training data of a few </span><span><span class="kobospan" id="kobo.327.1">recent LMs.</span></span></p>
<table class="no-table-style" id="table002-2">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><strong class="bold"><span class="kobospan" id="kobo.328.1">Model</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><strong class="bold"><span class="kobospan" id="kobo.329.1">Parameters</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.330.1">Size of </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.331.1">training data</span></strong></span></p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.332.1">GPT-3.5</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.333.1">175 B</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.334.1">300 </span><span><span class="kobospan" id="kobo.335.1">billion tokens</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.336.1">GPT-3</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.337.1">175 B</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.338.1">300 </span><span><span class="kobospan" id="kobo.339.1">billion tokens</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.340.1">PaLM</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.341.1">540 B</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.342.1">780 </span><span><span class="kobospan" id="kobo.343.1">billion tokens</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.344.1">LLaMA</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.345.1">65 B</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.346.1">1.4 </span><span><span class="kobospan" id="kobo.347.1">trillion tokens</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.348.1">Bloom</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.349.1">176 B</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.350.1">366 </span><span><span class="kobospan" id="kobo.351.1">billion tokens</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.352.1">Table 7.2 – Number of parameters and training data of a few recent LMs</span></p>
<h2 id="_idParaDest-159" class="calibre7"><a id="_idTextAnchor401" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.353.1">Computational resources</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.354.1">Training LLMs requires substantial computational resources. </span><span class="kobospan" id="kobo.354.2">These models often have billions or even trillions of parameters and need to process vast amounts of data during training, which requires high-performance hardware (such as GPUs or TPUs) and a significant amount of time. </span><span class="kobospan" id="kobo.354.3">This can be costly and could limit the accessibility of developing such models to only those who have these resources. </span><span class="kobospan" id="kobo.354.4">For example, training GPT-3 took 1 million GPU hours, which cost around 4.6 million dollars (in 2020). </span><em class="italic"><span class="kobospan" id="kobo.355.1">Table 7.3</span></em><span class="kobospan" id="kobo.356.1"> shows the computational resources and training time of a few </span><span><span class="kobospan" id="kobo.357.1">recent LMs.</span></span></p>
<table class="no-table-style" id="table003-1">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><strong class="bold"><span class="kobospan" id="kobo.358.1">Model</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><strong class="bold"><span class="kobospan" id="kobo.359.1">Hardware</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><strong class="bold"><span class="kobospan" id="kobo.360.1">Training time</span></strong></span></p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.361.1">PaLM</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.362.1">6144 </span><span><span class="kobospan" id="kobo.363.1">TPU v4</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.364.1">-</span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.365.1">LLaMA</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.366.1">2048 </span><span><span class="kobospan" id="kobo.367.1">80G A100</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.368.1">21 days</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.369.1">Bloom</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.370.1">384 </span><span><span class="kobospan" id="kobo.371.1">80G A100</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.372.1">105 days</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.373.1">GPT-3</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.374.1">1024x A100</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.375.1">34 days</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.376.1">GPT-4</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.377.1">25000 A100</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.378.1">90–100 days</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.379.1">Table 7.3 – The hardware and training time of a few recent LMs</span></p>
<h2 id="_idParaDest-160" class="calibre7"><a id="_idTextAnchor402" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.380.1">Risk of bias</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.381.1">LLMs can</span><a id="_idIndexMarker761" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.382.1"> learn and perpetuate biases present in their training data. </span><span class="kobospan" id="kobo.382.2">This could be explicit bias, such as racial or gender bias in the way language is used, or more subtle forms of bias, such as the underrepresentation of certain topics or perspectives. </span><span class="kobospan" id="kobo.382.3">This issue can be challenging to address because bias in language is a deeply rooted societal issue, and it’s often not easy to even identify what might be considered bias in a </span><span><span class="kobospan" id="kobo.383.1">given context.</span></span><a id="_idTextAnchor403" class="calibre5 pcalibre1 pcalibre"/></p>
<h2 id="_idParaDest-161" class="calibre7"><a id="_idTextAnchor404" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.384.1">Model robustness</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.385.1">It’s challenging to ensure that LLMs will perform well in all possible scenarios, particularly on inputs that differ from their training data. </span><span class="kobospan" id="kobo.385.2">This includes dealing with ambiguous queries, handling out-of-distribution data, and ensuring a level of consistency in the responses. </span><span class="kobospan" id="kobo.385.3">Making sure that the model is not overtrained can help to have a more robust model, but much more is needed to have a </span><span><span class="kobospan" id="kobo.386.1">robust model.</span></span><a id="_idTextAnchor405" class="calibre5 pcalibre1 pcalibre"/></p>
<h2 id="_idParaDest-162" class="calibre7"><a id="_idTextAnchor406" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.387.1">Interpretability and debugging</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.388.1">LLMs, like most </span><strong class="bold"><span class="kobospan" id="kobo.389.1">deep learning</span></strong><span class="kobospan" id="kobo.390.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.391.1">DL</span></strong><span class="kobospan" id="kobo.392.1">) models, are often described as “black boxes.” </span><span class="kobospan" id="kobo.392.2">It’s not easy to understand why they’re making a particular prediction or how they’re arriving at a conclusion. </span><span class="kobospan" id="kobo.392.3">This makes debugging challenging if the model starts to produce incorrect or inappropriate outputs. </span><span class="kobospan" id="kobo.392.4">Improving interpretability is an active area of research. </span><span class="kobospan" id="kobo.392.5">For example, some libraries attempt to elucidate the decision-making process of an LM by employing techniques such as feature importance analysis, which involves removing some words and analyzing the change </span><span><span class="kobospan" id="kobo.393.1">in gradients.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.394.1">One such method is the input perturbation technique. </span><span class="kobospan" id="kobo.394.2">In this approach, a word (or words) from the input text is perturbed or removed, and the change in the model’s output is analyzed. </span><span class="kobospan" id="kobo.394.3">The rationale behind this is to understand the influence of a specific input word on the model’s output prediction. </span><span class="kobospan" id="kobo.394.4">If the removal of a certain word significantly changes the model’s prediction, it can be inferred that the model deemed this word as important for </span><span><span class="kobospan" id="kobo.395.1">its prediction.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.396.1">Analyzing gradient changes is another popular method. </span><span class="kobospan" id="kobo.396.2">By investigating how the gradient of the output with respect to the input changes when a certain word is removed, one can gain insight into how the model’s decision-making process is influenced by that</span><a id="_idIndexMarker762" class="calibre5 pcalibre1 pcalibre"/> <span><span class="kobospan" id="kobo.397.1">specific word.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.398.1">These interpretation techniques provide a more transparent view into the complex decision-making process of LLMs, enabling researchers to better understand and improve their models. </span><span class="kobospan" id="kobo.398.2">Libraries such as LIME and SHAP offer tools for model interpretation tasks, thus making the process more accessible </span><span><span class="kobospan" id="kobo.399.1">to researchers</span><a id="_idTextAnchor407" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.400.1">.</span></span></p>
<h2 id="_idParaDest-163" class="calibre7"><a id="_idTextAnchor408" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.401.1">Environmental impact</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.402.1">The high computational resources needed for training LLMs can have significant environmental implications. </span><span class="kobospan" id="kobo.402.2">The energy required for training these models can contribute to carbon emissions, which is a concern from a </span><span><span class="kobospan" id="kobo.403.1">sustainability perspective.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.404.1">Besides that, there are concerns about privacy and security in LLMs. </span><span class="kobospan" id="kobo.404.2">For example, it is recommended not to share models that are trained using patients’ medical information, or not to feed sensitive information into publicly available LLMs such as ChatGPT, since it</span><a id="_idIndexMarker763" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.405.1"> can return it to other users as the answer to </span><span><span class="kobospan" id="kobo.406.1">their question</span><a id="_idTextAnchor409" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.407.1">s.</span></span></p>
<h1 id="_idParaDest-164" class="calibre4"><a id="_idTextAnchor410" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.408.1">Different types of LLMs</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.409.1">LLMs are generally neural network architectures that are trained on a large corpus of text data. </span><span class="kobospan" id="kobo.409.2">The</span><a id="_idIndexMarker764" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.410.1"> term “large” refers to the size of these models in terms of the number of parameters and the scale of training data. </span><span class="kobospan" id="kobo.410.2">Here are some examples </span><span><span class="kobospan" id="kobo.411.1">of LLMs.</span></span></p>
<h2 id="_idParaDest-165" class="calibre7"><a id="_idTextAnchor411" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.412.1">Transformer models</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.413.1">Transformer models have</span><a id="_idIndexMarker765" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.414.1"> been at the forefront of the recent wave of LLMs. </span><span class="kobospan" id="kobo.414.2">They are based on the “Transformer” architecture, which uses self-attention mechanisms to weigh the relevance of different words</span><a id="_idIndexMarker766" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.415.1"> in the input when making predictions. </span><span class="kobospan" id="kobo.415.2">Transformers are a type of neural network architecture introduced in the paper </span><em class="italic"><span class="kobospan" id="kobo.416.1">Attention is All You Need</span></em><span class="kobospan" id="kobo.417.1"> by Vaswani et al. </span><span class="kobospan" id="kobo.417.2">One of their significant advantages, particularly for training LLMs, is their suitability for </span><span><span class="kobospan" id="kobo.418.1">parallel computing.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.419.1">In traditional RNN models, such as LSTM and GRU, the sequence of tokens (words, subwords, or characters in the text) must be processed sequentially. </span><span class="kobospan" id="kobo.419.2">That’s because each token’s representation depends not only on the token itself but also on the previous tokens in the sequence. </span><span class="kobospan" id="kobo.419.3">The inherent sequential nature of these models makes it difficult to parallelize their operations, which can limit the speed and efficiency of the </span><span><span class="kobospan" id="kobo.420.1">training process.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.421.1">Transformers, in contrast, eliminate the necessity for sequential processing by using a mechanism called self-attention (or </span><a id="_idIndexMarker767" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.422.1">scaled dot-product attention). </span><span class="kobospan" id="kobo.422.2">In the self-attention process, each token’s representation is computed as a weighted sum of all tokens in the sequence, with the weights determined by the attention mechanism. </span><span class="kobospan" id="kobo.422.3">Importantly, these computations for each token are independent of the computations for other tokens, and thus they can be performed </span><span><span class="kobospan" id="kobo.423.1">in parallel.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.424.1">This parallelization capability brings several advantages for training LLMs as we will </span><span><span class="kobospan" id="kobo.425.1">discuss next.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.426.1">Speed</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.427.1">By parallelizing </span><a id="_idIndexMarker768" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.428.1">the computations, Transformers can process large amounts of data more quickly than RNNs. </span><span class="kobospan" id="kobo.428.2">This speed can significantly reduce the training time of LLMs, which often need to process vast amounts </span><span><span class="kobospan" id="kobo.429.1">of data.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.430.1">Scalability</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.431.1">Transformers’ parallelization makes it easier to scale up the model size and the amount of training data. </span><span class="kobospan" id="kobo.431.2">This capability is crucial for developing LLMs, as these models often benefit</span><a id="_idIndexMarker769" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.432.1"> from being trained on larger datasets and having a larger number </span><span><span class="kobospan" id="kobo.433.1">of parameters.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.434.1">Long-range dependencies</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.435.1">Transformers can better </span><a id="_idIndexMarker770" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.436.1">capture long-range dependencies between tokens because they consider all tokens in the sequence simultaneously, rather than processing them one at a time. </span><span class="kobospan" id="kobo.436.2">This capability is valuable in many language tasks and can improve the performance </span><span><span class="kobospan" id="kobo.437.1">of LLMs.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.438.1">Each of these models has its own strengths and weaknesses, and the best choice of model can depend on the specific task, the amount and type of available training data, and the computational </span><span><span class="kobospan" id="kobo.439.1">resources a</span><a id="_idTextAnchor412" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.440.1">vailable.</span></span></p>
<h1 id="_idParaDest-166" class="calibre4"><a id="_idTextAnchor413" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.441.1">Example designs of state-of-the-art LLMs</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.442.1">In this part, we are</span><a id="_idIndexMarker771" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.443.1"> going to dig more into the design and architecture of some of the newest LLMs at the time of writing </span><span><span class="kobospan" id="kobo.444.1">this book.</span></span></p>
<h2 id="_idParaDest-167" class="calibre7"><span class="kobospan" id="kobo.445.1">GPT-3.5 a</span><a id="_idTextAnchor414" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.446.1">nd ChatGPT</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.447.1">The core </span><a id="_idIndexMarker772" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.448.1">of ChatGPT is a Transformer, a type of model architecture that uses self-attention mechanisms to weigh the relevance of different words in the input when making predictions. </span><span class="kobospan" id="kobo.448.2">It allows the model to consider the full context of the input when generating </span><a id="_idTextAnchor415" class="calibre5 pcalibre1 pcalibre"/><span><span class="kobospan" id="kobo.449.1">a response.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.450.1">The GPT model</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.451.1">ChatGPT is based on the GPT version of the Transformer. </span><span class="kobospan" id="kobo.451.2">The GPT models are trained to predict the next word in a sequence of words, given all the previous words. </span><span class="kobospan" id="kobo.451.3">They process text from left to right (unidirectional context), which makes them well-suited for text generation tasks. </span><span class="kobospan" id="kobo.451.4">For instance, GPT-3, one of the versions of GPT on which </span><a id="_idIndexMarker773" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.452.1">ChatGPT is based, contains 175 </span><span><span class="kobospan" id="kobo.453.1">billion</span><a id="_idTextAnchor416" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.454.1"> parameters.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.455.1">Two-step training process</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.456.1">The training process for ChatGPT is done in two steps: pretraining </span><span><span class="kobospan" id="kobo.457.1">and fine-tuning.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.458.1">Pretraining</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.459.1">In this step, the model is trained </span><a id="_idIndexMarker774" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.460.1">on a large corpus of publicly available text from the internet. </span><span class="kobospan" id="kobo.460.2">However, it’s worth noting that it does not know specifics about which documents were in its training set or have access to any specific documents </span><span><span class="kobospan" id="kobo.461.1">or sources.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.462.1">Fine-tuning</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.463.1">After pretraining, the base model is further trained (fine-tuned) on custom datasets created by OpenAI, which include demonstrations of correct behavior as well as comparisons to rank different</span><a id="_idIndexMarker775" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.464.1"> responses. </span><span class="kobospan" id="kobo.464.2">Some prompts are from users of the Playground and ChatGPT apps, but they are anonymized and stripped of personally </span><span><span class="kobospan" id="kobo.465.1">identifiabl</span><a id="_idTextAnchor417" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.466.1">e information.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.467.1">RLHF</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.468.1">Part of the fine-tuning process involves </span><a id="_idIndexMarker776" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.469.1">RLHF, where human AI trainers provide feedback on model outputs for a range of example inputs, and this feedback is used to improve the model’s responses. </span><span class="kobospan" id="kobo.469.2">RLHF is a key component of the fine-tuning process used to train ChatGPT. </span><span class="kobospan" id="kobo.469.3">It’s a technique for refining the performance of the model by learning from feedback provided by human evaluators. </span><span class="kobospan" id="kobo.469.4">Here, we first explain the general idea of RLHF, and in the next section, we explain it step </span><span><span class="kobospan" id="kobo.470.1">by step.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.471.1">The first step in RLHF is to collect human feedback. </span><span class="kobospan" id="kobo.471.2">For ChatGPT, this often involves having human AI trainers participate in conversations where they play both sides (the user and the AI assistant). </span><span class="kobospan" id="kobo.471.3">The trainers also have access to model-written suggestions to help them compose responses. </span><span class="kobospan" id="kobo.471.4">This dialogue, in which AI trainers are essentially having a conversation with themselves, is added to the dataset </span><span><span class="kobospan" id="kobo.472.1">for fine-tuning.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.473.1">In addition to the dialogues, comparison data is created where multiple model responses are ranked by quality. </span><span class="kobospan" id="kobo.473.2">This is done by taking a conversation turn, generating several different completions (responses), and having human evaluators rank them. </span><span class="kobospan" id="kobo.473.3">The evaluators don’t </span><a id="_idIndexMarker777" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.474.1">just rank the responses on factual correctness but also on how useful and safe they judged the response </span><span><span class="kobospan" id="kobo.475.1">to be.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.476.1">The model is then fine-tuned using </span><strong class="bold"><span class="kobospan" id="kobo.477.1">proximal policy optimization</span></strong><span class="kobospan" id="kobo.478.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.479.1">PPO</span></strong><span class="kobospan" id="kobo.480.1">), a reinforcement learning algorithm. </span><span class="kobospan" id="kobo.480.2">PPO attempts to improve the model’s responses based on human feedback, making</span><a id="_idIndexMarker778" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.481.1"> small adjustments to the model’s parameters to increase the likelihood of better-rated responses and decrease the likelihood of </span><span><span class="kobospan" id="kobo.482.1">worse-rated responses.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.483.1">RLHF is an iterative process. </span><span class="kobospan" id="kobo.483.2">The procedure of collecting human feedback, creating comparison data, and fine-tuning the model using PPO is repeated multiple times to incrementally improve the model. </span><span class="kobospan" id="kobo.483.3">Next, we will explain in more detail how </span><span><span class="kobospan" id="kobo.484.1">PPO works.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.485.1">PPO is a reinforcement learning algorithm used to optimize the π policy of an agent. </span><span class="kobospan" id="kobo.485.2">The policy defines how the agent selects actions based on its current state. </span><span class="kobospan" id="kobo.485.3">PPO aims to optimize this policy to maximize the expected </span><span><span class="kobospan" id="kobo.486.1">cumulative rewards.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.487.1">Before diving into PPO, it’s important to define the reward model. </span><span class="kobospan" id="kobo.487.2">In the context of reinforcement learning, the reward model is a </span><strong class="source-inline"><span class="kobospan" id="kobo.488.1">R</span></strong><span class="kobospan" id="kobo.489.1">(</span><strong class="source-inline"><span class="kobospan" id="kobo.490.1">s</span></strong><span class="kobospan" id="kobo.491.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.492.1">a</span></strong><span class="kobospan" id="kobo.493.1">) function, which assigns a reward value to every state-action pair (</span><strong class="source-inline"><span class="kobospan" id="kobo.494.1">s</span></strong><span class="kobospan" id="kobo.495.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.496.1">a</span></strong><span class="kobospan" id="kobo.497.1">). </span><span class="kobospan" id="kobo.497.2">The goal of the agent is to learn a policy π that maximizes the expected sum of </span><span><span class="kobospan" id="kobo.498.1">these rewards.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.499.1">Mathematically, the objective of reinforcement learning can be defined </span><span><span class="kobospan" id="kobo.500.1">as follows:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.501.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;π&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;E&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;π&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;R&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/307.png" class="calibre306"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.502.1">In this formula, </span><em class="italic"><span class="kobospan" id="kobo.503.1">E</span></em><span class="subscript"><span class="kobospan1" id="kobo.504.1">π[.]</span></span><span class="kobospan" id="kobo.505.1"> is the expectation over trajectories (sequences of state-action pairs) generated by following policy </span><em class="italic"><span class="kobospan" id="kobo.506.1">π</span></em><span class="kobospan" id="kobo.507.1">, </span><em class="italic"><span class="kobospan" id="kobo.508.1">s</span></em><span class="subscript"><span class="kobospan1" id="kobo.509.1">_t</span></span><span class="kobospan" id="kobo.510.1"> is the state at time </span><em class="italic"><span class="kobospan" id="kobo.511.1">t</span></em><span class="kobospan" id="kobo.512.1">, </span><em class="italic"><span class="kobospan" id="kobo.513.1">a</span></em><span class="subscript"><span class="kobospan1" id="kobo.514.1">_t</span></span><span class="kobospan" id="kobo.515.1"> is the action taken at time </span><em class="italic"><span class="kobospan" id="kobo.516.1">t</span></em><span class="kobospan" id="kobo.517.1">, and </span><em class="italic"><span class="kobospan" id="kobo.518.1">R(s</span></em><span class="subscript"><span class="kobospan1" id="kobo.519.1">_t</span></span><em class="italic"><span class="kobospan" id="kobo.520.1">, a</span></em><span class="subscript"><span class="kobospan1" id="kobo.521.1">_t</span></span><em class="italic"><span class="kobospan" id="kobo.522.1">)</span></em><span class="kobospan" id="kobo.523.1"> is the reward received at </span><span><span class="kobospan" id="kobo.524.1">time </span></span><span><em class="italic"><span class="kobospan" id="kobo.525.1">t</span></em></span><span><span class="kobospan" id="kobo.526.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.527.1">PPO modifies this objective to encourage exploration of the policy space while preventing too drastic changes in the policy at each update. </span><span class="kobospan" id="kobo.527.2">This is done by introducing a ratio, </span><em class="italic"><span class="kobospan" id="kobo.528.1">r</span></em><span class="subscript"><span class="kobospan1" id="kobo.529.1">_t(θ)</span></span><span class="kobospan" id="kobo.530.1">, which represents the ratio of the probabilities of the current policy </span><em class="italic"><span class="kobospan" id="kobo.531.1">π</span></em><span class="subscript"><span class="kobospan1" id="kobo.532.1">_θ </span></span><span class="kobospan" id="kobo.533.1">to the old </span><span><span class="kobospan" id="kobo.534.1">policy </span></span><span><em class="italic"><span class="kobospan" id="kobo.535.1">π_θ_old</span></em></span><span><span class="kobospan" id="kobo.536.1">:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.537.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/308.png" class="calibre307"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.538.1">The objective of PPO is</span><a id="_idIndexMarker779" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.539.1"> then defined </span><span><span class="kobospan" id="kobo.540.1">as follows:</span></span></p>
<p class="calibre6" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.541.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/mfenced&gt;&lt;msub&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/309.png" class="calibre308"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.542.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.543.1">A_t</span></em><span class="kobospan" id="kobo.544.1"> is the advantage function that measures how much better the taking action </span><em class="italic"><span class="kobospan" id="kobo.545.1">a_t</span></em><span class="kobospan" id="kobo.546.1"> is compared to the average action at state </span><em class="italic"><span class="kobospan" id="kobo.547.1">s_</span></em><span class="subscript"><span class="kobospan1" id="kobo.548.1">t</span></span><span class="kobospan" id="kobo.549.1">, and </span><em class="italic"><span class="kobospan" id="kobo.550.1">clip(r_</span></em><span class="subscript"><span class="kobospan1" id="kobo.551.1">t</span></span><em class="italic"><span class="kobospan" id="kobo.552.1">(θ), 1 - ε, 1 + ε)</span></em><span class="kobospan" id="kobo.553.1"> is a clipped version of </span><em class="italic"><span class="kobospan" id="kobo.554.1">r_</span></em><span class="subscript"><span class="kobospan1" id="kobo.555.1">t</span></span><em class="italic"><span class="kobospan" id="kobo.556.1">(θ)</span></em><span class="kobospan" id="kobo.557.1"> that discourages too large </span><span><span class="kobospan" id="kobo.558.1">policy updates.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.559.1">The algorithm then optimizes this objective using stochastic gradient ascent, adjusting the policy parameters </span><em class="italic"><span class="kobospan" id="kobo.560.1">θ</span></em><span class="kobospan" id="kobo.561.1"> to </span><span><span class="kobospan" id="kobo.562.1">increase </span></span><span><em class="italic"><span class="kobospan" id="kobo.563.1">J_</span></em></span><span><span class="subscript"><span class="kobospan1" id="kobo.564.1">PPO</span></span></span><span><em class="italic"><span class="kobospan" id="kobo.565.1">(π)</span></em></span><span><span class="kobospan" id="kobo.566.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.567.1">In the context of ChatGPT and RLHF, the states correspond to the conversation histories, the actions correspond to the model-generated messages, and the rewards correspond to the human feedback on these messages. </span><span class="kobospan" id="kobo.567.2">PPO is used to adjust the model parameters to improve the quality of the generated messages as judged by the </span><span><span class="kobospan" id="kobo.568.1">human feedback.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.569.1">The human rankings are used to create a reward model, which quantifies how good each response is. </span><span class="kobospan" id="kobo.569.2">The reward model is a function that takes in a state and an action (in this case, a conversation context and a model-generated message), and outputs a scalar reward. </span><span class="kobospan" id="kobo.569.3">During training, the model tries to maximize its expected </span><span><span class="kobospan" id="kobo.570.1">cumulative reward.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.571.1">The goal of RLHF is to align the model’s behavior with human values and to improve its ability to generate useful and safe responses. </span><span class="kobospan" id="kobo.571.2">By learning from human feedback, ChatGPT can adapt to a wider range of conversational contexts and provide more appropriate and helpful responses. </span><span class="kobospan" id="kobo.571.3">It’s worth noting that despite these efforts, the system might still make mistakes, and handling these errors and improving the RLHF process is an area of</span><a id="_idTextAnchor418" class="calibre5 pcalibre1 pcalibre"/> <span><span class="kobospan" id="kobo.572.1">ongoing research.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.573.1">Generating responses</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.574.1">When generating a response, ChatGPT takes as input a conversation history, which includes previous messages in the </span><a id="_idIndexMarker780" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.575.1">conversation along with the most recent user message and produces a model-generated message as output. </span><span class="kobospan" id="kobo.575.2">The conversation history is tokenized and fed into the model, which generates a sequence of tokens in response, and these tokens are then detokenized to form the</span><a id="_idTextAnchor419" class="calibre5 pcalibre1 pcalibre"/> <a id="_idIndexMarker781" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.576.1">final </span><span><span class="kobospan" id="kobo.577.1">output text.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.578.1">System-level controls</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.579.1">OpenAI has also implemented some system-level controls to mitigate harmful or untruthful outputs from ChatGPT. </span><span class="kobospan" id="kobo.579.2">This</span><a id="_idIndexMarker782" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.580.1"> includes a Moderation API that warns or blocks certain typ</span><a id="_idTextAnchor420" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.581.1">es of </span><span><span class="kobospan" id="kobo.582.1">unsafe content.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.583.1">Step by step process </span><a id="_idTextAnchor421" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.584.1">of RLHF in ChatGPT</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.585.1">Since RLHF is an important part of ChatGPT and several other </span><strong class="bold"><span class="kobospan" id="kobo.586.1">State of the Art</span></strong><span class="kobospan" id="kobo.587.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.588.1">SOTA</span></strong><span class="kobospan" id="kobo.589.1">) models, understanding it better is useful to the you. </span><span class="kobospan" id="kobo.589.2">In recent years, LMs have demonstrated remarkable abilities, creating</span><a id="_idIndexMarker783" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.590.1"> varied and compelling text based on human-generated prompts. </span><span class="kobospan" id="kobo.590.2">Nonetheless, it’s challenging to precisely define what constitutes “good” text as it is inherently subjective and depends on the context. </span><span class="kobospan" id="kobo.590.3">For instance, while crafting stories demands creativity, informative pieces require accuracy, and code snippets need to </span><span><span class="kobospan" id="kobo.591.1">be executable.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.592.1">Defining a loss function to encapsulate these attributes seems virtually impossible, hence most LMs are trained using a basic next-token prediction loss, such as cross-entropy. </span><span class="kobospan" id="kobo.592.2">To overcome the limitations of the loss function, individuals have developed metrics that better align with human preferences, such as BLEU or </span><strong class="bold"><span class="kobospan" id="kobo.593.1">ROUGE</span></strong><span class="kobospan" id="kobo.594.1">. </span><span class="kobospan" id="kobo.594.2">The </span><strong class="bold"><span class="kobospan" id="kobo.595.1">BLEU</span></strong><span class="kobospan" id="kobo.596.1"> score, or Bilingual evaluation understudy, is a metric which is used to measure how well machine-translated text compares to a set of reference translations. </span><span class="kobospan" id="kobo.596.2">Although these metrics are more effective at assessing performance, they are inherently limited as they merely compare the generated text to references using </span><span><span class="kobospan" id="kobo.597.1">basic rules.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.598.1">Wouldn’t it be transformative if we could use human feedback on generated text as a performance measure, or even better, as a loss to optimize the model? </span><span class="kobospan" id="kobo.598.2">This is the concept behind RLHF – leveraging reinforcement learning techniques to directly optimize an LM using human feedback. </span><span class="kobospan" id="kobo.598.3">RLHF has begun to enable LMs to align a model trained on a general text corpus with intricate </span><span><span class="kobospan" id="kobo.599.1">human values.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.600.1">One of the recent successful applications of RLHF has been in the development </span><span><span class="kobospan" id="kobo.601.1">of ChatGPT.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.602.1">The concept of RLHF presents a formidable challenge due to its multifaceted model training process and various deployment phases. </span><span class="kobospan" id="kobo.602.2">Here, we’ll dissect the training procedure into its three </span><span><span class="kobospan" id="kobo.603.1">essential components:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.604.1">Initial pretraining of </span><span><span class="kobospan" id="kobo.605.1">an LM</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.606.1">Data collection and reward </span><span><span class="kobospan" id="kobo.607.1">model training</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.608.1">Refining the LM using </span><span><span class="kobospan" id="kobo.609.1">reinforcement learning</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.610.1">We’ll begin by examining the pret</span><a id="_idTextAnchor422" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.611.1">raining phase </span><span><span class="kobospan" id="kobo.612.1">for LMs.</span></span></p>
<h2 id="_idParaDest-168" class="calibre7"><a id="_idTextAnchor423" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.613.1">LM pretraining</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.614.1">As a foundation, RLHF utilizes an LM that’s already been pretrained using traditional</span><a id="_idIndexMarker784" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.615.1"> pretraining objectives, which means that we create the tokenizer based on our training data, design model architecture, and then pretrain the model using the training data. </span><span class="kobospan" id="kobo.615.2">For its initial well-received RLHF model, InstructGPT, OpenAI employed a smaller version of GPT-3. </span><span class="kobospan" id="kobo.615.3">On the other hand, Anthropic used transformer models ranging from 10 million to 52 billion parameters trained for this task, and DeepMind utilized its 280 billion parameter </span><span><span class="kobospan" id="kobo.616.1">model, Gopher.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.617.1">This preliminary model may be further refined on extra text or particular conditions, although it’s not always necessary. </span><span class="kobospan" id="kobo.617.2">As an example, OpenAI chose to refine its model using human-generated text identified as “preferable.” </span><span class="kobospan" id="kobo.617.3">This dataset is used to further fine-tune the model using the RLHF model, distilling the original LM model based on contextual hints </span><span><span class="kobospan" id="kobo.618.1">from humans.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.619.1">Generally speaking, there’s no definitive answer to the question of “which model” serves as the best launching point for RLHF. </span><span class="kobospan" id="kobo.619.2">The array of options available for RLHF training has not been </span><span><span class="kobospan" id="kobo.620.1">extensively explored.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.621.1">Moving on, once an LM is in place, it’s necessary to generate data to train a reward model. </span><span class="kobospan" id="kobo.621.2">This step is crucial for integrating human preferences into </span><span><span class="kobospan" id="kobo.622.1">the system.</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer353">
<span class="kobospan" id="kobo.623.1"><img alt="Figur﻿e 7.4 – Pretraining LM" src="image/B18949_07_4.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.624.1">Figur</span><a id="_idTextAnchor424" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.625.1">e 7.4 – Pretraining LM</span></p>
<h2 id="_idParaDest-169" class="calibre7"><a id="_idTextAnchor425" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.626.1">Training the reward model</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.627.1">In the newly proposed method, RLHF is being used as the RM, which is known as a preference model as </span><a id="_idIndexMarker785" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.628.1">well. </span><span class="kobospan" id="kobo.628.2">The main idea here is to get a text and return a scalar reward that reflects human preferences. </span><span class="kobospan" id="kobo.628.3">This approach can be implemented in two ways. </span><span class="kobospan" id="kobo.628.4">First, implement an end-to-end LLM, which gives us the preferred output. </span><span class="kobospan" id="kobo.628.5">This process can be performed by fine-tuning a LLM or training a LLM from scratch. </span><span class="kobospan" id="kobo.628.6">Second, have an extra component that ranks different outputs of the LLM and returns the </span><span><span class="kobospan" id="kobo.629.1">best one.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.630.1">The dataset used for training the RM is a set of prompt-generation pairs. </span><span class="kobospan" id="kobo.630.2">Prompts are sampled from a predetermined dataset (Anthropic’s data). </span><span class="kobospan" id="kobo.630.3">These prompts undergo processing by the initial LM to generate </span><span><span class="kobospan" id="kobo.631.1">fresh text.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.632.1">Human annotators rank the text outputs generated by the LM. </span><span class="kobospan" id="kobo.632.2">It might seem intuitive to have humans directly assign a scalar score to each text piece to generate a reward model, but it proves challenging in reality. </span><span class="kobospan" id="kobo.632.3">Varied human values render these scores unstandardized and unreliable. </span><span class="kobospan" id="kobo.632.4">Consequently, rankings are employed to compare </span><a id="_idIndexMarker786" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.633.1">multiple model outputs, thereby creating a substantially better </span><span><span class="kobospan" id="kobo.634.1">regularized dataset.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.635.1">There are several strategies for text ranking. </span><span class="kobospan" id="kobo.635.2">One successful approach involves users comparing the text produced by two LMs given the same prompt. </span><span class="kobospan" id="kobo.635.3">By evaluating model outputs in direct comparison, an </span><strong class="bold"><span class="kobospan" id="kobo.636.1">Elo rating system</span></strong><span class="kobospan" id="kobo.637.1">, which we will soon describe, can generate a ranking of models and </span><a id="_idIndexMarker787" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.638.1">outputs relative to each other. </span><span class="kobospan" id="kobo.638.2">These varying ranking methods are then normalized into a scalar reward signal for training. </span><span class="kobospan" id="kobo.638.3">The Elo rating system, originally developed for chess, is also applicable to RLHF </span><span><span class="kobospan" id="kobo.639.1">for LMs.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.640.1">In the context of LMs, each model or variant (e.g., models at different stages of training) can be seen as a “player.” </span><span class="kobospan" id="kobo.640.2">Its Elo rating reflects how well it performs in terms of generating </span><span><span class="kobospan" id="kobo.641.1">human-preferred outputs.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.642.1">The fundamental mechanics of the Elo rating system remain the same. </span><span class="kobospan" id="kobo.642.2">Here’s how it can be adapted for RLHF </span><span><span class="kobospan" id="kobo.643.1">in LMs:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.644.1">Initialization</span></strong><span class="kobospan" id="kobo.645.1">: All models start with the same Elo rating, often 1,000 </span><span><span class="kobospan" id="kobo.646.1">or 1,500.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.647.1">Comparison</span></strong><span class="kobospan" id="kobo.648.1">: For a given prompt, two models (A and B) generate their outputs. </span><span class="kobospan" id="kobo.648.2">A human evaluator then ranks these outputs. </span><span class="kobospan" id="kobo.648.3">If the evaluator considers the output from model A to be better, model A “wins” the match, and model </span><span><span class="kobospan" id="kobo.649.1">B “loses.”</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.650.1">The Elo ratings are updated in this way after each evaluation. </span><span class="kobospan" id="kobo.650.2">Over time, they provide an ongoing, dynamic ranking of the models based on human preferences. </span><span class="kobospan" id="kobo.650.3">This is useful for tracking progress over the course of training and for comparing different models or </span><span><span class="kobospan" id="kobo.651.1">model variants.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.652.1">Successful RLHF systems have employed diverse-sized reward LMs relative to text generation. </span><span class="kobospan" id="kobo.652.2">For example, OpenAI used a 175 B LM with a 6 B reward model, Anthropic utilized LM and reward models ranging from 10 B to 52 B, and DeepMind employed 70 B Chinchilla models for both the LM and reward model. </span><span class="kobospan" id="kobo.652.3">This is because preference models must match the capacity needed to understand a text as a model would need to generate it. </span><span class="kobospan" id="kobo.652.4">At this juncture in the RLHF process, we possess an initial LM capable of text generation and a preference model that assigns a score to any text based on human perception. </span><span class="kobospan" id="kobo.652.5">We next apply reinforcement learning to optimize the original </span><a id="_idIndexMarker788" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.653.1">LM concerning the </span><span><span class="kobospan" id="kobo.654.1">reward model.</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer354">
<span class="kobospan" id="kobo.655.1"><img alt="Figure 7.5 – The reward model f﻿or reinforcement learning" src="image/B18949_07_5.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.656.1">Figure 7.5 – The reward model f</span><a id="_idTextAnchor426" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.657.1">or reinforcement learning</span></p>
<h2 id="_idParaDest-170" class="calibre7"><span class="kobospan" id="kobo.658.1">How to fine-tune the model usi</span><a id="_idTextAnchor427" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.659.1">ng reinforcement learning</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.660.1">For a considerable period, the prospect of training an LM using reinforcement learning was considered unattainable due to both technical and algorithmic challenges. </span><span class="kobospan" id="kobo.660.2">However, several </span><a id="_idIndexMarker789" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.661.1">organizations have achieved fine-tuning some or all parameters of a replica of the initial LM with a policy-gradient reinforcement learning algorithm – namely, PPO. </span><span class="kobospan" id="kobo.661.2">Parameters of the LM are kept static because fine-tuning an entire model with 10 B or 100 B+ parameters is prohibitively expensive (for further details, refer</span><a id="_idIndexMarker790" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.662.1"> to </span><strong class="bold"><span class="kobospan" id="kobo.663.1">Low-Rank Adaptation</span></strong><span class="kobospan" id="kobo.664.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.665.1">LoRA</span></strong><span class="kobospan" id="kobo.666.1">) for LMs or DeepMind’s Sparrow LM). </span><span class="kobospan" id="kobo.666.2">PPO, an established method for some time now, has abundant available guides explaining its functioning. </span><span class="kobospan" id="kobo.666.3">This maturity made it an attractive choice for scaling up to the novel application of distributed training for RLHF. </span><span class="kobospan" id="kobo.666.4">It appears that significant strides in RLHF have been made by determining how to update such a colossal model with a known algorithm (more on </span><span><span class="kobospan" id="kobo.667.1">that later).</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.668.1">We can articulate this fine-tuning task as a reinforcement learning problem. </span><span class="kobospan" id="kobo.668.2">Initially, the policy is an LM that accepts a prompt and produces a sequence of text (or merely probability distributions over text). </span><span class="kobospan" id="kobo.668.3">The action space of this policy is all the tokens aligning with the LM’s vocabulary (typically around 50 K tokens), and the observation space is the distribution of possible input token sequences, which is also notably large in light of reinforcement learning’s prior uses (the dimension approximates the vocabulary size power (</span><strong class="source-inline"><span class="kobospan" id="kobo.669.1">^</span></strong><span class="kobospan" id="kobo.670.1">) length of the input token sequence). </span><span class="kobospan" id="kobo.670.2">The reward function melds the preference model with a constraint on </span><span><span class="kobospan" id="kobo.671.1">policy shift.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.672.1">The reward function is the juncture where the system integrates all the models discussed into a single RLHF process. </span><span class="kobospan" id="kobo.672.2">Given a prompt, </span><em class="italic"><span class="kobospan" id="kobo.673.1">x</span></em><span class="kobospan" id="kobo.674.1">, from the dataset, the text, </span><em class="italic"><span class="kobospan" id="kobo.675.1">y</span></em><span class="kobospan" id="kobo.676.1">, is created by the current iteration of the fine-tuned policy. </span><span class="kobospan" id="kobo.676.2">This text, coupled with the original prompt, is passed to the preference model, which returns a scalar measure of “</span><span><span class="kobospan" id="kobo.677.1">preferability”,</span></span><span><span class="kobospan" id="kobo.678.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/310.png" class="calibre309"/></span></span><span><span class="kobospan" id="kobo.679.1"> .</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.680.1">Additionally, per-token probability distributions from the reinforcement learning policy are contrasted with those from the initial model to compute a penalty on their difference. </span><span class="kobospan" id="kobo.680.2">In several papers from OpenAI, Anthropic, and DeepMind, this penalty has been constructed as a scaled version of the </span><strong class="bold"><span class="kobospan" id="kobo.681.1">Kullback–Leibler</span></strong><span class="kobospan" id="kobo.682.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.683.1">KL</span></strong><span class="kobospan" id="kobo.684.1">) divergence between these sequences of distributions over tokens, </span><span class="kobospan" id="kobo.685.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;K&lt;/mml:mi&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/311.png" class="calibre310"/></span><span class="kobospan" id="kobo.686.1"> . </span><span class="kobospan" id="kobo.686.2">The KL divergence term penalizes the reinforcement learning policy from veering significantly from the initial pretrained model with each training batch, ensuring the production of reasonably coherent </span><span><span class="kobospan" id="kobo.687.1">text snippets.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.688.1">Without this penalty, the optimization might start generating gibberish text that somehow deceives the reward model into granting a high reward. </span><span class="kobospan" id="kobo.688.2">In practical terms, the KL divergence is approximated via sampling from both distributions. </span><span class="kobospan" id="kobo.688.3">The final reward transmitted to the reinforcement learning update rule is </span><span><span class="kobospan" id="kobo.689.1">as follows:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.690.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/312.png" class="calibre311"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.691.1">Additional terms have been incorporated into the reward function by some RLHF systems. </span><span class="kobospan" id="kobo.691.2">For</span><a id="_idIndexMarker791" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.692.1"> instance, OpenAI’s InstructGPT successfully tried the blending of additional pretraining gradients (from the human annotation set) into the update rule for PPO. </span><span class="kobospan" id="kobo.692.2">It is anticipated that as RLHF continues to be studied, the formulation of this reward function will continue </span><span><span class="kobospan" id="kobo.693.1">to evolve.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.694.1">Finally, the update rule is the parameter update from PPO that optimizes the reward metrics in the current data batch (PPO is on-policy, meaning the parameters are only updated with the current batch of prompt-generation pairs). </span><span class="kobospan" id="kobo.694.2">PPO is a trust region optimization algorithm that employs constraints on the gradient to ensure the update step does not destabilize the learning process. </span><span class="kobospan" id="kobo.694.3">DeepMind utilized a similar reward setup for Gopher but employed a synchronous </span><span><span class="kobospan" id="kobo.695.1">advantage actor.</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer358">
<span class="kobospan" id="kobo.696.1"><img alt="Figure 7.6 – Fine-tuning the model using reinforcement learning" src="image/B18949_07_6.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.697.1">Figure 7.6 – Fine-tuning the model using reinforcement learning</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.698.1">The preceding diagram may suggest that both models produce different responses for the same prompt, but what actually occurs is that the reinforcement learning policy generates text, which</span><a id="_idIndexMarker792" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.699.1"> is then supplied to the initial model to derive its relative probabilities for the </span><span><span class="kobospan" id="kobo.700.1">KL penalty.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.701.1">Optionally, RLHF can advance from this stage by cyclically updating both the reward model and the policy. </span><span class="kobospan" id="kobo.701.2">As the reinforcement learning policy evolves, users can maintain the ranking of these outputs against the model’s previous versions. </span><span class="kobospan" id="kobo.701.3">However, most papers haven’t yet addressed the implementation of this operation since the mode of deployment required to collect this type of data only works for dialogue agents who can access an active user base. </span><span class="kobospan" id="kobo.701.4">Anthropic mentions this alternative as </span><strong class="bold"><span class="kobospan" id="kobo.702.1">iterated online RLHF</span></strong><span class="kobospan" id="kobo.703.1"> (as referred to in the original paper), where iterations of the policy are incorporated into the Elo</span><a id="_idIndexMarker793" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.704.1"> ranking system across models. </span><span class="kobospan" id="kobo.704.2">This brings about complex dynamics of the policy and reward model evolving, representing a complex and unresolved research question. </span><span class="kobospan" id="kobo.704.3">In the next section, we will explain some </span><a id="_idIndexMarker794" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.705.1">well-kno</span><a id="_idTextAnchor428" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.706.1">wn open-source tools </span><span><span class="kobospan" id="kobo.707.1">for RLHF.</span></span></p>
<h2 id="_idParaDest-171" class="calibre7"><a id="_idTextAnchor429" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.708.1">GPT-4</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.709.1">At the time of writing this book, we</span><a id="_idIndexMarker795" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.710.1"> know very little about the GPT-4 model design. </span><span class="kobospan" id="kobo.710.2">As OpenAI is</span><a id="_idIndexMarker796" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.711.1"> slow to reveal, it is assumed that GPT-4 is not a single model but a combination of eight 220-billion-parameter models, an assumption that is confirmed by key figures in the AI community. </span><span class="kobospan" id="kobo.711.2">This assumption suggests OpenAI used a “mixture of experts” strategy, an ML design tactic that dates even before LLMs, to create the model. </span><span class="kobospan" id="kobo.711.3">However, while we, the authors, support this assumption, it has not been officially confirmed </span><span><span class="kobospan" id="kobo.712.1">by OpenAI.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.713.1">Despite the speculation, GPT-4’s impressive performance is undeniable, regardless of its internal structure. </span><span class="kobospan" id="kobo.713.2">Its capabilities in writing and coding tasks are remarkable, and the specifics of whether it’s one model or eight bundled together does not change </span><span><span class="kobospan" id="kobo.714.1">its impact.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.715.1">A common narrative suggests that OpenAI has managed expectations around GPT-4 deftly, focusing on its power and opting not to disclose specifications due to competitive pressures. </span><span class="kobospan" id="kobo.715.2">The secrecy surrounding GPT-4 has led many to belie</span><a id="_idTextAnchor430" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.716.1">ve it to be a </span><span><span class="kobospan" id="kobo.717.1">scientific marvel.</span></span></p>
<h2 id="_idParaDest-172" class="calibre7"><a id="_idTextAnchor431" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.718.1">LLaMA</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.719.1">Meta has</span><a id="_idIndexMarker797" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.720.1"> publicly launched LLaMA, a high-performing LLM </span><a id="_idIndexMarker798" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.721.1">aimed at aiding researchers in AI. </span><span class="kobospan" id="kobo.721.2">This move allows individuals with limited access to extensive infrastructure to examine these models, thus broadening access in this rapidly </span><span><span class="kobospan" id="kobo.722.1">evolving field.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.723.1">LLaMA models are attractive because they require significantly less computational power and resources, allowing for the exploration of new approaches and use cases. </span><span class="kobospan" id="kobo.723.2">Available in several sizes, these models are designed to be fine-tuned for various tasks and have been developed with responsible </span><span><span class="kobospan" id="kobo.724.1">AI practices.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.725.1">LLMs, despite their advancements, have limited research accessibility due to the resources required to train and run them. </span><span class="kobospan" id="kobo.725.2">Smaller models, such as LLaMA, trained on more tokens, are simpler to retrain and adjust for specific </span><span><span class="kobospan" id="kobo.726.1">use cases.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.727.1">Similar to other</span><a id="_idIndexMarker799" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.728.1"> models, LLaMA takes a sequence of words as input to predict the next word and generate text. </span><span class="kobospan" id="kobo.728.2">Despite its capabilities, LLaMA shares the same challenges as other </span><a id="_idIndexMarker800" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.729.1">models regarding bias, toxic comments, and hallucinations. </span><span class="kobospan" id="kobo.729.2">By sharing LLaMA’s code, Meta enables researchers to test new ways of addressing these issues </span><span><span class="kobospan" id="kobo.730.1">in LLMs.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.731.1">Meta emphasizes the need for cooperation across the AI community to establish guidelines around responsible AI and LLMs. </span><span class="kobospan" id="kobo.731.2">They anticipate that LLaMA will facilitate new le</span><a id="_idTextAnchor432" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.732.1">arning and development in </span><span><span class="kobospan" id="kobo.733.1">the field.</span></span></p>
<h2 id="_idParaDest-173" class="calibre7"><a id="_idTextAnchor433" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.734.1">PaLM</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.735.1">PaLM is a 540-billion-parameter, densely-activated </span><a id="_idIndexMarker801" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.736.1">Transformer LM that was trained on 6,144 TPU v4 chips using Pathways, a new ML system, that </span><a id="_idIndexMarker802" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.737.1">enables highly efficient training across multiple </span><span><span class="kobospan" id="kobo.738.1">TPU pods.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.739.1">PaLM has been shown to achieve breakthrough performance on a variety of natural language tasks, including </span><span><span class="kobospan" id="kobo.740.1">the following:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.741.1">Multi-step </span><span><span class="kobospan" id="kobo.742.1">reasoning tasks</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.743.1">The recently released </span><strong class="bold"><span class="kobospan" id="kobo.744.1">Beyond the Imitation Game </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.745.1">Benchmark</span></strong></span><span><span class="kobospan" id="kobo.746.1"> (</span></span><span><strong class="bold"><span class="kobospan" id="kobo.747.1">BIG-bench</span></strong></span><span><span class="kobospan" id="kobo.748.1">)</span></span></li>
<li class="calibre15"><span><span class="kobospan" id="kobo.749.1">Multilingual tasks</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.750.1">Source </span><span><span class="kobospan" id="kobo.751.1">code generation</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.752.1">The BIG-bench benchmark is worth expanding on as it serves as a recognized collection of benchmarks to measure against. </span><span class="kobospan" id="kobo.752.2">The BIG-bench is an extensive assessment mechanism specifically designed for large-scale LMs. </span><span class="kobospan" id="kobo.752.3">It is a broad-based, community-focused benchmark that presents a diversity of tasks to evaluate a model’s performance in different disciplines and its competence in natural language comprehension, problem solving, and reasoning. </span><span class="kobospan" id="kobo.752.4">With a total of 204 tasks from 450 contributors across 132 institutions, BIG-bench covers an eclectic mix of subjects including linguistics, childhood development, mathematics, common-sense reasoning, biology, physics, software development, and even social bias. </span><span class="kobospan" id="kobo.752.5">It concentrates on challenges believed to be currently beyond the reach of existing LMs. </span><span class="kobospan" id="kobo.752.6">The primary goal of BIG-bench extends beyond mere imitation or Turing test-style evaluations, aiming instead for a deeper, more nuanced appraisal of the abilities and constraints of </span><a id="_idIndexMarker803" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.753.1">these large models. </span><span class="kobospan" id="kobo.753.2">This initiative is founded on the conviction that an open, collaborative approach to evaluation paves the way for a more comprehensive understanding of these LMs and their potential </span><span><span class="kobospan" id="kobo.754.1">societal ramifications.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.755.1">PaLM 540B excels beyond the fine-tuned state-of-the-art across various multi-step reasoning tasks and surpasses average human performance on the BIG-bench benchmark. </span><span class="kobospan" id="kobo.755.2">Many BIG-bench tasks exhibit significant leaps in performance as PaLM scales to its largest size, demonstrating discontinuous improvements from the model scale. </span><span class="kobospan" id="kobo.755.3">PaLM also has strong capabilities in multilingual tasks and source code generation. </span><span class="kobospan" id="kobo.755.4">For example, PaLM can translate between 50 languages, and it can generate code in a variety of </span><span><span class="kobospan" id="kobo.756.1">programming languages.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.757.1">The authors of the </span><a id="_idIndexMarker804" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.758.1">PaLM paper also discuss the ethical considerations related to LLMs, and they discuss potential mitigation strategies. </span><span class="kobospan" id="kobo.758.2">For example, they suggest that it is important to be aware of the potential for bias in LLMs and that it is important to develop techn</span><a id="_idTextAnchor434" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.759.1">iques for detecting and </span><span><span class="kobospan" id="kobo.760.1">mitigating bias.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.761.1">PaLM architecture</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.762.1">PaLM employs the conventional Transformer model architecture in a decoder-exclusive setup, which allows each timestep to attend only to itself and preceding timesteps. </span><span class="kobospan" id="kobo.762.2">Several modifications</span><a id="_idIndexMarker805" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.763.1"> were applied to this setup, including </span><span><span class="kobospan" id="kobo.764.1">the following:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.765.1">SwiGLU activation</span></strong><span class="kobospan" id="kobo.766.1">: Instead of standard ReLU, GeLU, or Swish activations, PaLM utilizes SwiGLU activations (</span><em class="italic"><span class="kobospan" id="kobo.767.1">Swish(xW) · xV</span></em><span class="kobospan" id="kobo.768.1">) for the </span><strong class="bold"><span class="kobospan" id="kobo.769.1">multilayer perceptron</span></strong><span class="kobospan" id="kobo.770.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.771.1">MLP</span></strong><span class="kobospan" id="kobo.772.1">) intermediate activations due to their superior performance in enhancing quality. </span><span class="kobospan" id="kobo.772.2">This approach, however, requires three matrix multiplications in the MLP as opposed to the </span><span><span class="kobospan" id="kobo.773.1">conventional two.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.774.1">Parallel layers</span></strong><span class="kobospan" id="kobo.775.1">: Rather than the typical “serialized” approach, PaLM uses a “parallel” formulation</span><a id="_idIndexMarker806" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.776.1"> for each </span><span><span class="kobospan" id="kobo.777.1">Transformer block.</span></span><p class="calibre6"><span class="kobospan" id="kobo.778.1">The standard structure is given by </span><span><span class="kobospan" id="kobo.779.1">the following:</span></span></p><p class="calibre6"><span class="kobospan" id="kobo.780.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/313.png" class="calibre312"/></span></p><p class="calibre6"><span class="kobospan" id="kobo.781.1">The parallel structure is instead </span><span><span class="kobospan" id="kobo.782.1">the following:</span></span></p><p class="calibre6"><span class="kobospan" id="kobo.783.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/314.png" class="calibre313"/></span></p><p class="calibre6"><span class="kobospan" id="kobo.784.1">This leads to roughly 15% quicker training speed at larger scales due to the fusion of MLP and attention input </span><span><span class="kobospan" id="kobo.785.1">matrix multiplications.</span></span></p></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.786.1">Multi-query attention</span></strong><span class="kobospan" id="kobo.787.1">: In the conventional Transformer formulation, </span><em class="italic"><span class="kobospan" id="kobo.788.1">k</span></em><span class="kobospan" id="kobo.789.1"> attention heads are employed. </span><span class="kobospan" id="kobo.789.2">For each timestep, the input vector is linearly projected into query, key, and value tensors, which have a shape of [</span><em class="italic"><span class="kobospan" id="kobo.790.1">k, h</span></em><span class="kobospan" id="kobo.791.1">]</span><em class="italic"><span class="kobospan" id="kobo.792.1">,</span></em><span class="kobospan" id="kobo.793.1"> where </span><em class="italic"><span class="kobospan" id="kobo.794.1">h</span></em><span class="kobospan" id="kobo.795.1"> denotes the size of the attention head. </span><span class="kobospan" id="kobo.795.2">In the new approach, the projections for “key” and “value” are shared across all heads, meaning “key” and “value” are projected to [</span><em class="italic"><span class="kobospan" id="kobo.796.1">1, h</span></em><span class="kobospan" id="kobo.797.1">], while “query” maintains the shape [</span><em class="italic"><span class="kobospan" id="kobo.798.1">k, h</span></em><span class="kobospan" id="kobo.799.1">]. </span><span class="kobospan" id="kobo.799.2">The authors claimed that this approach doesn’t notably affect model quality or training speed while it does result in significant cost reductions during autoregressive decoding. </span><span class="kobospan" id="kobo.799.3">The reason for this lies in the inefficiency of standard multi-headed attention on accelerator hardware during autoregressive decoding, as the key/value tensors are not shared across examples and only one token is decoded at </span><span><span class="kobospan" id="kobo.800.1">each moment.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.801.1">Rotary Position Embedding (RoPE) embeddings</span></strong><span class="kobospan" id="kobo.802.1">: RoPE embeddings, shown to perform better on longer sequence lengths, are preferred over absolute or relative </span><span><span class="kobospan" id="kobo.803.1">position embeddings.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.804.1">Shared input-output embeddings</span></strong><span class="kobospan" id="kobo.805.1">: The input and output embedding matrices are shared, a practice that is common, though not universal, in </span><span><span class="kobospan" id="kobo.806.1">previous work.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.807.1">No biases</span></strong><span class="kobospan" id="kobo.808.1">: The model abstains from using biases in any dense kernels or layer norms, which enhances training stability for </span><span><span class="kobospan" id="kobo.809.1">larger models.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.810.1">Vocabulary</span></strong><span class="kobospan" id="kobo.811.1">: PaLM uses a 256k-token SentencePiece vocabulary designed for diverse languages in the training corpus, ensuring efficient training without excessive tokenization. </span><span class="kobospan" id="kobo.811.2">This preserves all whitespaces and out-of-vocabulary Unicode characters while splitting numbers into individual digit tokens </span><span><span class="kobospan" id="kobo.812.1">for clarity.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.813.1">Overall, PaLM is a powerful LM that has the potential to be used for a wide variety of applications. </span><span class="kobospan" id="kobo.813.2">It is still under development, but it has already demonstrated the ability to achieve</span><a id="_idIndexMarker807" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.814.1"> bre</span><a id="_idTextAnchor435" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.815.1">akthrough performance on a number </span><span><span class="kobospan" id="kobo.816.1">of tasks.</span></span></p>
<h2 id="_idParaDest-174" class="calibre7"><a id="_idTextAnchor436" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.817.1">Open-source tools for RLHF</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.818.1">OpenAI released the first open-source code to perform RLHF in 2019. </span><span class="kobospan" id="kobo.818.2">They have implemented this approach to improve GPT-2 for different use cases such as summarization. </span><span class="kobospan" id="kobo.818.3">Based on</span><a id="_idIndexMarker808" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.819.1"> human feedback, the model is optimized to have outputs similar to </span><a id="_idIndexMarker809" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.820.1">humans, such as copying parts of the note. </span><span class="kobospan" id="kobo.820.2">More information about this project can be found at the following </span><span><span class="kobospan" id="kobo.821.1">link: </span></span><a href="https://openai.com/research/fine-tuning-gpt-2" class="calibre5 pcalibre1 pcalibre"><span><span class="kobospan" id="kobo.822.1">https://openai.com/research/fine-tuning-gpt-2</span></span></a><span><span class="kobospan" id="kobo.823.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.824.1">The code is also available at the following </span><span><span class="kobospan" id="kobo.825.1">link: </span></span><a href="https://github.com/openai/lm-human-preferences" class="calibre5 pcalibre1 pcalibre"><span><span class="kobospan" id="kobo.826.1">https://github.com/openai/lm-human-preferences</span></span></a><span><span class="kobospan" id="kobo.827.1">.</span></span></p>
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.828.1">Transformers Reinforcement Learning</span></strong><span class="kobospan" id="kobo.829.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.830.1">TRL</span></strong><span class="kobospan" id="kobo.831.1">) is a tool crafted for fine-tuning pretrained LMs using PPO within the </span><a id="_idIndexMarker810" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.832.1">Hugging Face ecosystem. </span><span class="kobospan" id="kobo.832.2">TRLX, an enhanced fork developed by CarperAI, is capable of handling larger models for both online and offline training. </span><span class="kobospan" id="kobo.832.3">Currently, TRLX is equipped with a production-ready API supporting RLHF with PPO and </span><strong class="bold"><span class="kobospan" id="kobo.833.1">implicit language Q-learning</span></strong><span class="kobospan" id="kobo.834.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.835.1">ILQL</span></strong><span class="kobospan" id="kobo.836.1">) for</span><a id="_idIndexMarker811" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.837.1"> deploying LLMs of up to 33 billion parameters. </span><span class="kobospan" id="kobo.837.2">Future versions of TRLX aim to accommodate LMs of up to 200 billion parameters, making it ideal for ML engineers working at </span><span><span class="kobospan" id="kobo.838.1">such scales.</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.839.1">The code for TRL is available at the following </span><span><span class="kobospan" id="kobo.840.1">link: </span></span><a href="https://github.com/lvwerra/trl" class="calibre5 pcalibre1 pcalibre"><span><span class="kobospan" id="kobo.841.1">https://github.com/lvwerra/trl</span></span></a></li>
<li class="calibre15"><span class="kobospan" id="kobo.842.1">The code for TRLX can be found at the following </span><span><span class="kobospan" id="kobo.843.1">link: </span></span><a href="https://github.com/CarperAI/trlx" class="calibre5 pcalibre1 pcalibre"><span><span class="kobospan" id="kobo.844.1">https://github.com/CarperAI/trlx</span></span></a><span><span class="kobospan" id="kobo.845.1">.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.846.1">Another good library is </span><strong class="bold"><span class="kobospan" id="kobo.847.1">Reinforcement Learning for Language Models</span></strong><span class="kobospan" id="kobo.848.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.849.1">RL4LMs</span></strong><span class="kobospan" id="kobo.850.1">). </span><span class="kobospan" id="kobo.850.2">The RL4LMs project addresses the</span><a id="_idIndexMarker812" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.851.1"> challenge of training LLMs to align with human preference metrics. </span><span class="kobospan" id="kobo.851.2">It recognizes that many NLP tasks can be seen as sequence learning problems, but their application is limited due to issues such as reinforcement learning training instability, high variance in automated NLP metrics, and reward hacking. </span><span class="kobospan" id="kobo.851.3">The </span><a id="_idIndexMarker813" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.852.1">project offers solutions by doing </span><span><span class="kobospan" id="kobo.853.1">the following:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.854.1">Giving guidelines on when to use reinforcement learning and suggesting suitable NLP tasks/metrics via a continually updated benchmark </span><span><span class="kobospan" id="kobo.855.1">called GRUE</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.856.1">Introducing a new reinforcement learning algorithm, </span><strong class="bold"><span class="kobospan" id="kobo.857.1">Natural Language Policy Optimization </span></strong><span class="kobospan" id="kobo.858.1">(</span><strong class="bold"><span class="kobospan" id="kobo.859.1">NLPO</span></strong><span class="kobospan" id="kobo.860.1">), designed to handle large language action spaces and reward </span><span><span class="kobospan" id="kobo.861.1">variance better</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.862.1">Offering practical </span><a id="_idIndexMarker814" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.863.1">advice with high-quality implementations and hyperparameters of reinforcement learning, as well as other reinforcement learning algorithms, for training Transformers in the Hugging </span><span><span class="kobospan" id="kobo.864.1">Face library</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.865.1">The code for this project can be found at the </span><a id="_idTextAnchor437" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.866.1">following </span><span><span class="kobospan" id="kobo.867.1">link: </span></span><a href="https://github.com/allenai/RL4LMs" class="calibre5 pcalibre1 pcalibre"><span><span class="kobospan" id="kobo.868.1">https://github.com/allenai/RL4LMs</span></span></a><span><span class="kobospan" id="kobo.869.1">.</span></span></p>
<h1 id="_idParaDest-175" class="calibre4"><a id="_idTextAnchor438" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.870.1">Summary</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.871.1">In this chapter, we’ve delved into the dynamic and complex world of state-of-the-art LLMs. </span><span class="kobospan" id="kobo.871.2">We’ve discussed their remarkable generalization capabilities, making them versatile tools for a wide range of tasks. </span><span class="kobospan" id="kobo.871.3">We also highlighted the crucial aspect of understanding complex contexts, where these models excel by grasping the nuances of language and the intricacies of various </span><span><span class="kobospan" id="kobo.872.1">subject matters.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.873.1">Additionally, we explored the paradigm of RLHF and how it is being employed to enhance LMs. </span><span class="kobospan" id="kobo.873.2">RLHF leverages scalar feedback to improve LMs by mimicking human judgments, thereby helping to mitigate some of the common pitfalls encountered in </span><span><span class="kobospan" id="kobo.874.1">NLP tasks.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.875.1">We discussed technical requirements for working with these models, emphasizing the need for foundational knowledge in areas such as Transformers, reinforcement learning, and </span><span><span class="kobospan" id="kobo.876.1">coding skills.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.877.1">This chapter also touched on some prominent LMs such as GPT-4 and LLaMA, discussing their architecture, methods, and performance. </span><span class="kobospan" id="kobo.877.2">We highlighted the strategies some libraries employ to interpret LM predictions, such as the removal of certain words and analyzing </span><span><span class="kobospan" id="kobo.878.1">gradient changes.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.879.1">To sum up, this chapter offers a comprehensive overview of the current state of LLMs, exploring their capabilities, challenges, the methods used to refine them, and the evolving tools and measures for their evaluation </span><span><span class="kobospan" id="kobo.880.1">and interpretation.</span></span></p>
<h1 id="_idParaDest-176" class="calibre4"><a id="_idTextAnchor439" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.881.1">References</span></h1>
<ul class="calibre14">
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.882.1">Hugging </span></em><span><em class="italic"><span class="kobospan" id="kobo.883.1">Face</span></em></span><span><span class="kobospan" id="kobo.884.1">: </span></span><a href="http://huggingface.co" class="calibre5 pcalibre1 pcalibre"><span><span class="kobospan" id="kobo.885.1">huggingface.co</span></span></a></li>
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.886.1">Large language </span></em><span><em class="italic"><span class="kobospan" id="kobo.887.1">model</span></em></span><span><span class="kobospan" id="kobo.888.1">: </span></span><a href="https://en.m.wikipedia.org/wiki/Large_language_model#" class="calibre5 pcalibre1 pcalibre"><span><span class="kobospan" id="kobo.889.1">https://en.m.wikipedia.org/wiki/Large_language_model#</span></span></a></li>
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.890.1">Zhao, Wayne Xin, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min et al. </span><span class="kobospan" id="kobo.890.2">“A survey of large language models.”</span></em><span class="kobospan" id="kobo.891.1"> arXiv preprint </span><span><span class="kobospan" id="kobo.892.1">arXiv:2303.18223 (2023).</span></span></li>
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.893.1">Introducing LLaMA: A foundational, 65-billion-parameter large language </span></em><span><em class="italic"><span class="kobospan" id="kobo.894.1">model</span></em></span><span><span class="kobospan" id="kobo.895.1">: </span></span><a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/" class="calibre5 pcalibre1 pcalibre"><span><span class="kobospan" id="kobo.896.1">https://ai.facebook.com/blog/large-language-model-llama-meta-ai/</span></span></a></li>
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.897.1">Model </span></em><span><em class="italic"><span class="kobospan" id="kobo.898.1">Details</span></em></span><span><span class="kobospan" id="kobo.899.1">: </span></span><a href="https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md" class="calibre5 pcalibre1 pcalibre"><span><span class="kobospan" id="kobo.900.1">https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md</span></span></a></li>
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.901.1">Touvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière et al. </span><span class="kobospan" id="kobo.901.2">“Llama: Open and efficient foundation language models.” </span><span class="kobospan" id="kobo.901.3">arXiv </span></em><span class="kobospan" id="kobo.902.1">preprint </span><span><span class="kobospan" id="kobo.903.1">arXiv:2302.13971 (2023).</span></span></li>
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.904.1">Elo rating </span></em><span><em class="italic"><span class="kobospan" id="kobo.905.1">system</span></em></span><span><span class="kobospan" id="kobo.906.1">: </span></span><a href="https://en.wikipedia.org/wiki/Elo_rating_system" class="calibre5 pcalibre1 pcalibre"><span><span class="kobospan" id="kobo.907.1">https://en.wikipedia.org/wiki/Elo_rating_system</span></span></a></li>
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.908.1">Chowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham et al. </span><span class="kobospan" id="kobo.908.2">“Palm: Scaling language modeling with pathways.” </span></em><span class="kobospan" id="kobo.909.1">arXiv preprint </span><span><span class="kobospan" id="kobo.910.1">arXiv:2204.02311 (2022).</span></span></li>
<li class="calibre15"><span><em class="italic"><span class="kobospan" id="kobo.911.1">BIG-bench</span></em></span><span><span class="kobospan" id="kobo.912.1">: </span></span><a href="https://github.com/google/BIG-bench" class="calibre5 pcalibre1 pcalibre"><span><span class="kobospan" id="kobo.913.1">https://github.com/google/BIG-bench</span></span></a></li>
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.914.1">Srivastava, Aarohi, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. </span><span class="kobospan" id="kobo.914.2">Brown et al. </span><span class="kobospan" id="kobo.914.3">“Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.” </span></em><span class="kobospan" id="kobo.915.1">arXiv preprint </span><span><span class="kobospan" id="kobo.916.1">arXiv:2206.04615 (2022).</span></span></li>
</ul>
</div>
</body></html>