<html><head></head><body>
<div><p><a id="_idTextAnchor122"/></p>
<h1 class="chapter-number" id="_idParaDest-75"><a id="_idTextAnchor123"/>4</h1>
<h1 id="_idParaDest-76"><a id="_idTextAnchor124"/>Applying Pretrained Generative Models: From Prototype to Production</h1>
<p>In the preceding chapters, we explored the fundamentals of generative AI, explored various generative models, such <a id="_idIndexMarker306"/>as <strong class="bold">generative adversarial networks</strong> (<strong class="bold">GANs</strong>), diffusers, and transformers, and learned about the transformative <a id="_idIndexMarker307"/>impact of <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>). As we transition into the practical aspects of applying generative AI, we should ground our exploration in a practical example. This approach will provide a concrete context, making the technical aspects more relatable and the learning experience more engaging.</p>
<p>We will introduce “StyleSprint,” a clothing shop looking to enhance its online presence. One way to achieve this is by crafting unique and engaging product descriptions for its various products. However, manually creating captivating descriptions for a large inventory is challenging. This situation is prime opportunity for the application of generative AI. By leveraging a pretrained generative model, StyleSprint can automate the crafting of compelling product descriptions, saving considerable time and enriching the online shopping experience for its customers.</p>
<p>As we step into the practical application of a pretrained generative <strong class="bold">large language models</strong> (<strong class="bold">LLM</strong>), the <a id="_idIndexMarker308"/>first order of business is to set up a Python environment conducive to prototyping with generative models. This setup is vital for transitioning the project from a prototype to a production-ready state, setting the stage for StyleSprint to realize its goal of automated content generation.</p>
<p>In <em class="italic">Chapters 2</em> and <em class="italic">3</em>, we used Google Colab for prototyping due to its ease of use and accessible GPU resources. It served as a great platform to test ideas quickly. However, as we shift our focus toward deploying our generative model in a real-world setting, it is essential to understand the transition from a prototyping environment such as Google Colab to a more robust, production-ready setup. This transition will ensure our solution is scalable, reliable, and well-optimized for handling real-world traffic. In this chapter, we will walk through the steps in setting up a production-ready Python environment, underscoring the crucial considerations for a smooth transition from prototype to production.</p>
<p>By the end of this chapter, we will understand the process of taking a generative application from a prototyping environment to a production-ready setup. We will define a reliable and repeatable strategy for evaluating, monitoring, and deploying models to productio<a id="_idTextAnchor125"/>n.</p>
<h1 id="_idParaDest-77"><a id="_idTextAnchor126"/>Prototyping environments</h1>
<p>Jupyter notebooks provide <a id="_idIndexMarker309"/>an interactive computing environment to combine code execution, text, mathematics, plots, and rich media into a single document. They are ideal for prototyping and interactive development, making them a popular choice among data scientists, researchers, and engineers. Here is what they offer:</p>
<ul>
<li><strong class="bold">Kernel</strong>: At the heart of a Jupyter notebook is a kernel, a computational engine that executes the code contained in the notebook. For Python, this is typically an IPython kernel. This kernel remains active and maintains the state of your notebook’s computations while the notebook is open.</li>
<li><strong class="bold">Interactive execution</strong>: Code cells allow you to write and execute code interactively, inspecting the results and tweaking the code as necessary.</li>
<li><code>pip</code> or <code>conda</code> commands.</li>
<li><strong class="bold">Visualization</strong>: You can embed plots, graphs, and other visualizations to explore data and results interactively.</li>
<li><strong class="bold">Documentation</strong>: Combining Markdown cells with code cells allows for well-documented, self-contained notebooks that explain the code and its output.</li>
</ul>
<p>A drawback to Jupyter notebooks is that they typically rely on the computational resources of your personal computer. Most personal laptops and desktops are not optimized or equipped to handle computationally intensive processes. Having adequate computational resources is crucial for managing the computational complexity of experimenting with an LLM. Fortunately, we can extend the capabilities of a Jupyter notebook with cloud-based platforms that offer computational accelerators such as <strong class="bold">graphics processing units</strong> (<strong class="bold">GPUs</strong>) and <strong class="bold">tensor processing units</strong> (<strong class="bold">TPUs</strong>). For example, Google Colab <a id="_idIndexMarker310"/>instantly enhances Jupyter<a id="_idIndexMarker311"/> notebooks, making them conducive to computationally intensive experimentation. Here are some of the key features of a cloud-based notebook environment such as Google Colab:</p>
<ul>
<li><strong class="bold">GPU/TPU access</strong>: Provides<a id="_idIndexMarker312"/> free or affordable access to GPU and TPU resources for accelerated computation, which is crucial when working with demanding machine learning models</li>
<li><strong class="bold">Collaboration</strong>: Permits easy sharing and real-time collaboration, similar to Google Docs</li>
<li><strong class="bold">Integration</strong>: Allows for easy storage and access to notebooks and data</li>
</ul>
<p>Let’s consider our StyleSprint scenario. We will want to explore a few different models to generate product <a id="_idIndexMarker313"/>descriptions before deciding on one that best fits StyleSprint’s goals. We can set up a minimal working prototype in Google Colab to compare models. Again, cloud-based platforms provide an optimal and accessible environment for initial testing, experimentation, and even some lightweight training of models. Here is how we might initially set up a generative model to start experimenting with automated product description generation for StyleSprint:</p>
<pre class="source-code">
# In a Colab or Jupyter notebook
!pip install transformers
# Google Colab Jupyter notebook
from transformers import pipeline
# Initialize a text generation pipeline with a generative model, say GPT-Neo
text_generator = pipeline(
    'text-generation', model='EleutherAI/gpt-neo-2.7B')
# Example prompt for product description generation
prompt = "This high-tech running shoe with advanced cushioning and support"
# Generating the product description
generated_text = text_generator(prompt, max_length=100, do_sample=True)
# Printing the generated product description
print(generated_text[0]['generated_text'])</pre>
<p>Output:</p>
<pre class="source-code">
This high-tech running shoe with advanced cushioning and support combines the best of traditional running shoes and the latest technologies.</pre>
<p>In this simple setup, we’re installing the <code>transformers</code> library, which offers a convenient interface to various pretrained models. We then initialize a text generation pipeline with an open source version of GPT-Neo, capable of generating coherent and contextually relevant text. This setup serves as a starting point for StyleSprint to experiment with generating creative product descriptions on a small scale.</p>
<p>Later in this chapter, we will expand our experiment to evaluate and compare multiple pretrained generative models to determine which best meets our needs. However, before advancing further in our experimentation and prototyping, it is crucial to strategically pause<a id="_idIndexMarker314"/> and project forward. This deliberate forethought allows us to consider the necessary steps for effectively transitioning our experiment into a production environment. By doing so, we ensure a comprehensive view of the project from end to end, to align<a id="_idIndexMarker315"/> with long-term operational goals.</p>
<div><div><img alt="Figure 4.1: Moving from prototyping to production—the stages" src="img/B21773_04_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1: Moving from prototyping to production—the stages</p>
<h1 id="_idParaDest-78"><a id="_idTextAnchor127"/>Transitioning to production</h1>
<p>As we plan for a <a id="_idIndexMarker316"/>production setup, we should first understand the intrinsic benefits and features of the prototyping environment we will want to carry forward to a production setting. Many of the features of prototyping environments such as Google Colab are deeply integrated and can easily go unnoticed, so it is important to dissect and catalog the features we will need in production. For example, the following features are inherent in Google Colab and will be critical in production:</p>
<ul>
<li><code>!pip install library_name</code>. In production, we will have to preinstall libraries or make sure we can install them as needed. We must also ensure that project-specific libraries do not interfere with other projects.</li>
<li><strong class="bold">Dependency isolation</strong>: Google Colab automatically facilitates isolated dependencies, ensuring package installations and updates do not interfere with other projects. In production, we may also want to deploy various projects using the same infrastructure. Dependency isolation will be critical to prevent one project’s dependency updates from impacting other projects.</li>
<li><strong class="bold">Interactive code execution</strong>: The interactive execution of code cells helps in testing individual code snippets, visualizing results, and debugging in real time. This convenience is not necessary in production but could be helpful for quick debugging.</li>
<li><strong class="bold">Resource accessibility</strong>: With Colab, access to GPUs and TPUs is simplified, which is crucial for running computation-intensive tasks. For production, we will want to examine our dynamic computational needs and provision the appropriate infrastructure.</li>
<li><strong class="bold">Data integration</strong>: Colab offers simple connectivity to data sources for analysis and modeling. In production, we can either bootstrap our environment with data (i.e., deploy data directly into the environment) or ensure connectivity to remote data sources as needed.</li>
<li><strong class="bold">Versioning and collaboration</strong>: Tracking versions of your project code with Google Colab can easily be accomplished using notebooks. Additionally, Colab is preconfigured to interact with Git. Git is a distributed version control system that is widely used for tracking changes in source code during software development. In production, we will also want to integrate Git to manage our code and synchronize it with a remote code repository such as GitHub or Bitbucket. Remote versioning ensures that our production environment always reflects the latest changes and enables ongoing collaboration.</li>
<li><strong class="bold">Error handling and debugging</strong>: In Colab, we have direct access to the Python runtime and can typically see error messages and tracebacks in real time to help identify and resolve issues. We will want the same level of visibility in production via adequate logging of system errors. In total, we want to carry over the convenience and simplicity of our Google Colab prototyping environment but provide the robustness and scalability required for production. To do so, we will map each of the<a id="_idIndexMarker318"/> key characteristics we laid out to a corresponding production solution. These key features should ensure a smooth transition for deploying StyleSprint’s <a id="_idIndexMarker319"/>generative model for automated product descriptio<a id="_idTextAnchor128"/>n generation.</li>
</ul>
<h1 id="_idParaDest-79"><a id="_idTextAnchor129"/>Mapping features to production setup</h1>
<p>To ensure we can <a id="_idIndexMarker320"/>seamlessly transition our prototyping environment to production, we can leverage Docker, a leading containerization <a id="_idIndexMarker321"/>tool. <strong class="bold">Containerization</strong> tools package applications with their dependencies for consistent performance across different systems. A containerized approach will help us replicate Google Colab’s isolated, uniform environments, ensuring reliability and reducing potential compatibility issues in production. The table that follows describes how we can map each of the benefits of our prototyping environment to a production <a id="_idIndexMarker322"/>analog:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-1">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Feature</strong></p>
</td>
<td class="No-Table-Style" colspan="2">
<p><strong class="bold">Environment</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><strong class="bold">Prototyping</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Production</strong></p>
</td>
</tr>
</thead>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Package management</p>
</td>
<td class="No-Table-Style">
<p>Inherent through preinstalled package managers</p>
</td>
<td class="No-Table-Style">
<p>Docker streamlines application deployment and consistency across environments including package managers.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Dependency isolation</p>
</td>
<td class="No-Table-Style">
<p>Inherent through notebooks</p>
</td>
<td class="No-Table-Style">
<p>Docker can also ensure projects are cleanly isolated.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Interactive code execution</p>
</td>
<td class="No-Table-Style">
<p>Inherent through notebooks</p>
</td>
<td class="No-Table-Style">
<p>Docker helps to maintain versions of Python that provide interactive code execution by default. However, we may want to connect an <strong class="bold">integrated development environment</strong> (<strong class="bold">IDE</strong>) to our production environment to interact with code remotely as needed.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Resource accessibility</p>
</td>
<td class="No-Table-Style">
<p>Inherent for cloud-based notebooks</p>
</td>
<td class="No-Table-Style">
<p>GPU-enabled Docker containers enhance production by enabling structured GPU utilization, allowing scalable, efficient model performance.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Data integration</p>
</td>
<td class="No-Table-Style">
<p>Not inherent, and requires code-based integration</p>
</td>
<td class="No-Table-Style">
<p>Integrating Docker with a remote data source, such as AWS S3 or Google Cloud Storage, provides secure and scalable solutions for importing and exporting data.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Versioning and collaboration</p>
</td>
<td class="No-Table-Style">
<p>Inherent through notebooks and preconfigured for Git</p>
</td>
<td class="No-Table-Style">
<p>Integrating Docker with platforms such as GitHub or GitLab enables code collaboration and documentation.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Error handling and debugging</p>
</td>
<td class="No-Table-Style">
<p>Inherent through direct interactive access to runtime</p>
</td>
<td class="No-Table-Style">
<p>We can embed Python libraries such as <code>logging</code> or <code>Loguru</code> in Docker deployments for enhanced error tracking in production.</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.1: Transitioning features from Colab to production via Docker</p>
<p>Having mapped out <a id="_idIndexMarker323"/>the features of our prototyping environment to corresponding tools and practices for a production setup, we are now better prepared to implement a generative model for StyleSprint in a production-ready environment. The transition entails setting up a stable, scalable, and reproducible Python environment, a crucial step for deploying our generative model to automate the generation of product descriptions in a real-world setting. As discussed, we can leverage Docker in tandem with GitHub and its <strong class="bold">continuous integration/continuous deployment</strong> (<strong class="bold">CI/CD</strong>) capabilities, providing a robust framework<a id="_idIndexMarker324"/> for this production deployment. A CI pipeline automates the integration of code changes from multiple contributors into a shared repository. We pair CI with CD to automate the deployment of our code to a production environment.<a id="_idTextAnchor130"/></p>
<h1 id="_idParaDest-80"><a id="_idTextAnchor131"/>Setting up a production-ready environment</h1>
<p>So far, we have <a id="_idIndexMarker325"/>discussed how to bridge the gap between prototyping and production environments. Cloud-based environments such as Google Colab provide a wealth of features that are not inherently available in production. Now that we have a better understanding of those characteristics, the next step is to implement a robust production setup to ensure that our application can handle real-world traffic, scale as needed, and remain stable over time.</p>
<p>The tools and practices in a production environment differ significantly from those in a prototyping environment. In production, scalability, reliability, resource management, and security become paramount, whereas, in a prototyping environment, the models are only relied upon by a few users for experimentation. In production, we could expect large-scale consumption from divisions throughout the organization. For example, in the StyleSprint scenario, there may be multiple departments or sub-brands hoping to automate their product descriptions.</p>
<p>In the early stages of our StyleSprint project, we can use free and open source tools such as Docker and GitHub for tasks such as containerization, version control, and CI. These tools are offered and managed by a community of users, giving us a cost-effective solution. As StyleSprint expands, we might consider upgrading to paid or enterprise editions that offer advanced features and professional support. For the moment, our focus is on leveraging the capabilities of the open source versions. Next, we will walk through the practical implementation of these tools step by step. By the end, we will be ready to deploy a <a id="_idIndexMarker326"/>production-ready <strong class="bold">model-as-a-service</strong> (<strong class="bold">MaaS</strong>) for automatic product descriptio<a id="_idTextAnchor132"/>ns.</p>
<h1 id="_idParaDest-81"><a id="_idTextAnchor133"/>Local development setup</h1>
<p>We begin by making sure<a id="_idIndexMarker327"/> we can connect to a production environment remotely. We can leverage an IDE, which is software that enables us to easily organize code and remotely connect to the production<a id="_idIndexMarker328"/> environ<a id="_idTextAnchor134"/>ment.</p>
<h2 id="_idParaDest-82"><a id="_idTextAnchor135"/>Visual Studio Code</h2>
<p>Begin by<a id="_idIndexMarker329"/> installing <strong class="bold">Visual Studio Code</strong> (<strong class="bold">VS Code</strong>), a free code editor by Microsoft. It is preferred <a id="_idIndexMarker330"/>for its integrated Git control, terminal, and marketplace for extensions that enhance its functionality. It provides a conducive environment for writing, testing, and debuggin<a id="_idTextAnchor136"/>g code.</p>
<h2 id="_idParaDest-83"><a id="_idTextAnchor137"/>Project initialization</h2>
<p>Next, we set up a<a id="_idIndexMarker331"/> structured project directory to keep the code modular and organized. We will also initialize our working directory with Git, which enables us to synchronize code with a remote repository. As mentioned, we leverage Git to keep track of code changes and collaborate with others more seamlessly. Using the terminal window in Visual Studio, we can initialize the project using three simple commands. We use <code>mkdir</code> to create or “make” a directory. We use the <code>cd</code> command to change directories. Finally, we use <code>git init</code> to initialize our project with Git. Keep in mind that this assumes Git is installed. Instructions to install Git are made available on its website (<a href="https://git-scm.com/">https://git-scm.com/</a>).</p>
<pre class="console">
mkdir StyleSprint
cd StyleSprint
<a id="_idTextAnchor138"/>git init</pre>
<h2 id="_idParaDest-84">Dock<a id="_idTextAnchor139"/>er setup</h2>
<p>We’ll now move on<a id="_idIndexMarker332"/> to setting <a id="_idIndexMarker333"/>up a Docker container. A Docker container is an isolated environment that encapsulates an application and its dependencies, ensuring consistent operation across different systems. For clarity, we can briefly describe the key aspects of Docker as follows:</p>
<ul>
<li><strong class="bold">Containers</strong>: These are portable units comprising the application and its dependencies.</li>
<li><strong class="bold">Host operating system’s kernel</strong>: When a Docker container is run on a host machine, it utilizes the kernel of the host’s operating system and resources to operate, but it does so in a way that is isolated from both the host system and other containers.</li>
<li><strong class="bold">Dockerfiles</strong>: These are scripts used to create container images. They serve as a blueprint containing everything needed to run the application. This isolation and packaging method prevents application conflicts and promotes efficient resource use, streamlining development and deployment.</li>
</ul>
<p>A containerized approach will help ensure consistency and portability. For example, assume StyleSprint finds a cloud-based hosting provider that is more cost-effective; moving to the new provider is as simple as migrating a few configuration files.</p>
<p>We can install Docker from the official website. Docker provides easy-to-follow installation guides including support for various programming languages.</p>
<p>Once Docker is installed, we can create a Dockerfile in the project directory to specify the environment setup. For GPU support, we will want to start from an NVIDIA CUDA base image. Docker, like many other virtualized systems, operates using a concept called <strong class="bold">images</strong>. Images<a id="_idIndexMarker334"/> are a snapshot of a <a id="_idIndexMarker335"/>preconfigured environment that can be used as a starting point for a new project. In our case, we will want to start with a snapshot that integrates GPU support using the CUDA library, which is a parallel processing library provided by NVIDIA. This library will enable the virtualized environment (or container) to leverage any GPUs installed on the host machine. Leveraging GPUs will accelerate model inferencing.</p>
<p>Now we can go ahead and create a Dockerfile with the specifications for our<a id="_idTextAnchor140"/> application:</p>
<pre class="source-code">
# Use an official NVIDIA CUDA runtime as a base image
FROM nvidia/cuda:11.0-base
# Set the working directory in the container to /app
WORKDIR /app
# Copy the current directory contents into the container at /app
COPY . /app
# Install any needed packages specified in requirements.txt
RUN pip install --no-cache-dir -r requirements.txt
# Make port 80 available to the world outside this container
EXPOSE 80
# Run app.py when the container launches
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "80"]</pre>
<p>This Dockerfile serves as a blueprint that Docker follows to build our container. We initiate the process from an official NVIDIA CUDA base image to ensure GPU support. The working directory in the container is set to <code>/app</code>, where we then copy the contents of our project. Following that, we install the necessary packages listed in the <code>requirements.txt</code> file. <code>Port 80</code> is exposed for external access to our application. Lastly, we specify the command to launch our application, which is running <code>app.py</code> using the Python<a id="_idIndexMarker336"/> interpreter. This <a id="_idIndexMarker337"/>setup encapsulates all the necessary components, including GPU support, to ensure our generative model operates efficiently in a production-lik<a id="_idTextAnchor141"/>e environment.</p>
<h2 id="_idParaDest-85"><a id="_idTextAnchor142"/>Requirements file</h2>
<p>We also need a<a id="_idIndexMarker338"/> method for keeping track of our Python-specific dependencies. The container will include Python but will not have any indication as to what requirements our Python application has. We can specify those dependencies explicitly by defining a <code>requirements.txt</code> file in <a id="_idIndexMarker339"/>our project directory to list all the necessary Python packages:</p>
<pre class="source-code">
fastapi==0.65.2
torch==1.9.0
transformers==4.9.2
uvicorn==0.14.0</pre>
<h2 id="_idParaDest-86"><a id="_idTextAnchor143"/>Application code</h2>
<p>Now we can<a id="_idIndexMarker340"/> create an <code>app.py</code> file for our <a id="_idIndexMarker341"/>application code. This is where we will write the code for our generative model, leveraging libraries such as PyTorch and Transformers. To expose our model as a service, we will use <em class="italic">FastAPI</em>, a modern, high-performance framework for building web APIs. A web API is a protocol that enables different software applications to communicate and exchange data over the internet, allowing them to use each other’s functions and services.</p>
<p>The following snippet creates a minimal API that will serve the model responses whenever another application or software requests the <code>/generate/</code> endpoint. This will enable StyleSprint to host its model as a web service. This means that other applications (e.g., mobile apps, batch processes) can access the model using a simple URL. We can also add exception handling to provide an informative error message should the model produce any kind of error:</p>
<pre class="source-code">
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import pipeline
# Load the pre-trained model
generator = pipeline('text-generation', 
    model='EleutherAI/gpt-neo-2.7B')
# Create the FastAPI app
app = FastAPI()
# Define the request body
class GenerationInput(BaseModel):
prompt: str
# Define the endpoint
@app.post("/generate")
def generate_text(input: GenerationInput):
try:
    # Generate text based on the input prompt
    generated_text = generator(input.prompt, max_length=150)
    return {"generated_text": generated_text}
except:
    raise HTTPException(status_code=500,
        detail="Model failed to generate text")</pre>
<p>Now that we have a Docker setup, the next step is to deploy the application to the host server. We can streamline this process with a CI/CD pipeline. The goal is to fully automate all deployment steps, including a suite of tests to ensure that any code changes do not<a id="_idIndexMarker342"/> introduce <a id="_idIndexMarker343"/>any errors. We then leverage GitHub Actions to create a workflow that is directly integrated with a code repository.</p>
<h2 id="_idParaDest-87"><a id="_idTextAnchor144"/>Creating a code repository</h2>
<p>Before we can leverage<a id="_idIndexMarker344"/> the<a id="_idIndexMarker345"/> automation capabilities of GitHub, we will need a repository. Creating a GitHub repository is straightforward, following these steps:</p>
<ol>
<li><strong class="bold">Sign up/log in to GitHub</strong>: If you don’t have a GitHub account, sign up at <a href="http://github.com">github.com</a>. If you already have an account, just log in.</li>
<li><strong class="bold">Go to the repository creation page</strong>: Click the <strong class="bold">+</strong> icon in the top-right corner of the GitHub home page and select <strong class="bold">New repository</strong>.</li>
<li><strong class="bold">Fill in the </strong><strong class="bold">repository details</strong>:<ul><li><strong class="bold">Repository Name</strong>: Choose a name for your repository</li><li><strong class="bold">Description</strong> (optional): Add a brief description of your repository</li><li><strong class="bold">Visibility</strong>: Select either <strong class="bold">Public</strong> (anyone can see this repository) or <strong class="bold">Private</strong> (only you and the collaborators you invite can see it)</li></ul></li>
<li><code>.gitignore</code> file or choose a license. A <code>gitignore</code> file allows us to add paths or file types that should not be uploaded to the repository. For example, Python creates temporary files that are not critical to the application. Adding <code>`__pycache__/`</code> to the <code>gitignore</code> file will automatically ignore all contents of that directory.</li></ul></li>
<li><strong class="bold">Create repository</strong>: Click the <strong class="bold">Create </strong><strong class="bold">repository</strong> button.</li>
</ol>
<p>With our <a id="_idIndexMarker346"/>repository setup<a id="_idIndexMarker347"/> complete, we can move on to defining our CI/CD pipeline to a<a id="_idTextAnchor145"/>utomate our deployments.</p>
<h2 id="_idParaDest-88"><a id="_idTextAnchor146"/>CI/CD setup</h2>
<p>To create<a id="_idIndexMarker348"/> a pipeline, we <a id="_idIndexMarker349"/>will need a configuration file that outlines the stages of deployment and instructs the automation server to build and deploy our Docker container. Let’s look at the steps:</p>
<ol>
<li>In our GitHub repository, we can create a new file in the <code>.github/workflows</code> directory named <code>ci-cd.yml</code>. GitHub will automatically find any files in this directory to trigger deployments.</li>
<li>Open <code>ci-cd.yml</code> and<a id="_idIndexMarker350"/> define <a id="_idIndexMarker351"/>the following workflow:<pre class="source-code">
name: CI/CD Pipeline</pre><pre class="source-code">
on:</pre><pre class="source-code">
  push:</pre><pre class="source-code">
    branches:</pre><pre class="source-code">
      - main</pre><pre class="source-code">
jobs:</pre><pre class="source-code">
  build-and-test:</pre><pre class="source-code">
    runs-on: ubuntu-latest</pre><pre class="source-code">
  steps:</pre><pre class="source-code">
    - name: Checkout code</pre><pre class="source-code">
      uses: actions/checkout@v4</pre><pre class="source-code">
    - name: Build Docker image</pre><pre class="source-code">
  # assumes the Dockerfile is in the root (.)</pre><pre class="source-code">
      run: docker build -t stylesprint .</pre><pre class="source-code">
    - name: Run tests</pre><pre class="source-code">
  # assumes a set of unit tests were defined</pre><pre class="source-code">
      run: docker run stylesprint python -m unittest discover</pre><pre class="source-code">
deploy:</pre><pre class="source-code">
  needs: build-and-test</pre><pre class="source-code">
  runs-on: ubuntu-latest</pre><pre class="source-code">
  steps:</pre><pre class="source-code">
    - name: Checkout code</pre><pre class="source-code">
      uses: actions/checkout@v4</pre><pre class="source-code">
    - name: Login to DockerHub</pre><pre class="source-code">
      run: echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin</pre><pre class="source-code">
    - name: Push Docker image</pre><pre class="source-code">
      run: |</pre><pre class="source-code">
        docker tag stylesprint:latest ${{ secrets.DOCKER_USERNAME }}/stylesprint:latest</pre><pre class="source-code">
        docker push ${{ secrets.DOCKER_USERNAME }}/stylesprint:latest</pre></li>
</ol>
<p>In this setup, our<a id="_idIndexMarker352"/> workflow consists <a id="_idIndexMarker353"/>of two primary jobs: build-and-test and deploy. The build-and-test job is responsible for checking out the code from the repository, building the Docker image, and executing any tests. On the other hand, the deploy job, which relies on completing build-and-test, handles <em class="italic">DockerHub</em> login and pushes the Docker image there. DockerHub, similar to GitHub, is a repository specifically for Docker images.</p>
<p>For authenticating with DockerHub, it is advised to securely store your DockerHub credentials in your GitHub repository. This can be done by navigating to your repository on GitHub, clicking on <code>DOCKER_USERNAME</code> and <code>DOCKER_PASSWORD</code> as new repository secrets.</p>
<p>Notice that we did not have to perform any additional steps to execute the pipeline. The workflow is designed to trigger automatically upon a push (or upload) to the main branch. Recall that the entire process relies on the Git pattern where new changes are registered through a <code>commit</code> or check-in of code and a <code>push</code> or upload of code changes. Whenever changes are pushed, we can directly observe the entire pipeline in action within the <strong class="bold">Actions</strong> tab of the GitHub repository.</p>
<p>We have now walked through all of the steps necessary to deploy our model to production. With all of this critical setup behind us, we can now return to choosing the best model for our project. The goal is to find a model that can effectively generate captivating product descriptions for StyleSprint. However, the variety of generative models available requires a thoughtful choice based on our project’s needs and constraints.</p>
<p>Moreover, we want to choose the right evaluation metrics and discuss other considerations<a id="_idIndexMarker354"/> that will guide us in <a id="_idIndexMarker355"/>making an informed decision for our project. This exploration will equip us with the knowledge needed to select a model that not only performs well but also aligns with our project objectives and the technical inf<a id="_idTextAnchor147"/>rastructure we have established.</p>
<h1 id="_idParaDest-89"><a id="_idTextAnchor148"/>Model selection – choosing the right pretrained generative model</h1>
<p>Having established <a id="_idIndexMarker356"/>a minimal production environment in the previous section, we now focus on a pivotal aspect of our project – selecting the right generative model for generating engaging product descriptions. The choice of model is crucial as it significantly influences the effectiveness and efficiency of our solution. The objective is to automate the generation of compelling and accurate product descriptions for StyleSprint’s diverse range of retail products. By doing so, we aim to enrich the online shopping experience for customers while alleviating the manual workload of crafting unique product descriptions.</p>
<p>Our objective is to select a generative model that can adeptly handle nuanced and sophisticated text generation to significantly expedite the process of creating unique, engaging product descriptions, saving time and resources for StyleSprint.</p>
<p>In selecting our model, it is important to thoroughly evaluate various factors influencing its performance<a id="_idTextAnchor149"/> and <a id="_idIndexMarker357"/>suitability for the project.</p>
<h2 id="_idParaDest-90"><a id="_idTextAnchor150"/>Meeting project objectives</h2>
<p>Before we can select<a id="_idIndexMarker358"/> and apply evaluation methods to our model selection process, we should first make sure we understand the project objectives. This involves defining the business problem, identifying any technical constraints, identifying any risk associated with the model, including interpretation of model outcomes, and ascertaining considerations for any potential disparate treatment or bias:</p>
<ul>
<li><strong class="bold">Problem definition</strong>: In our scenario, the goal is to create accurate and engaging descriptions for a wide range of retail clothing. As StyleSprint’s product range may expand, the system should scale seamlessly to accommodate a larger inventory without significantly increasing operational costs. Performance expectations include compelling descriptions to attract potential customers, accuracy to avoid misrepresentation, and prompt generation to maintain an up-to-date online catalog. Additionally, StyleSprint may apply personalized content descriptions based on a user’s shopping history. This implies that the model may have to provide product descriptions in near-real-time.</li>
<li><strong class="bold">Technical constraints</strong>: To maximize efficiency, there should not be any noticeable delay (latency) in responses from the model API. The system should be capable of real-time updates to the online catalog (as needed), and the hardware should support quick text generation without compromising quality while remaining cost-effective, especially as the product range expands.</li>
<li><strong class="bold">Transparency and openness</strong>: Generally, pretrained models from developers who disclose architectures and training data sources are preferred, as this level of transparency allows StyleSprint to have a clear understanding of any risks or legal implications associated with model use. Additionally, any usage restrictions imposed by using models provided as APIs, such as request or token limitations, should be understood as they could hinder scalability for a growing catalog.</li>
<li><strong class="bold">Bias and fairness</strong>: Identifying and mitigating biases in model outputs to ensure fair and neutral representations is crucial, especially given StyleSprint’s diverse target audience. Ensuring that the generated descriptions are culturally sensitive is of paramount importance. Fair representation ensures that the descriptions accurately and fairly represent the products to all potential customers, irrespective of their individual characteristics or social backgrounds.</li>
<li><strong class="bold">Suitability of pretraining</strong>: The underlying pretraining of generative models plays a significant role in their ability to generate meaningful and relevant text. Investigating the domains and data on which the models were pretrained or fine-tuned is important. A model pretrained on a broad dataset may be versatile but could lack <a id="_idIndexMarker359"/>domain-specific nuances. For StyleSprint, a model that is fine-tuned on fashion-related data or that has the ability to be fine-tuned on such data would be ideal to ensure the generated descriptions are relevant and appealing.</li>
<li><strong class="bold">Quantitative metrics</strong>: Evaluating the quality of generated product descriptions for StyleSprint necessitates a combination of lexical and semantic metrics. Lexical overlap metrics measure the lexical similarity between generated and reference texts. Specifically, <strong class="bold">Bilingual Evaluation Understudy</strong> (<strong class="bold">BLEU</strong>) emphasizes <a id="_idIndexMarker360"/>n-gram precision, <strong class="bold">Recall-Oriented Understudy for Gisting Evaluation</strong> (<strong class="bold">ROUGE</strong>) focuses<a id="_idIndexMarker361"/> on n-gram <a id="_idIndexMarker362"/>recall, and <strong class="bold">Metric for Evaluation of Translation with Explicit Ordering</strong> (<strong class="bold">METEOR</strong>) aims for a more balanced evaluation by considering synonyms and stemming. For contextual and semantic evaluation, we use similarity metrics to assess the semantic coherence and relevance of the generated descriptions, often utilizing embeddings to represent text in a way that captures its meaning.</li>
</ul>
<p>We can further refine our assessment of the alignment between generated descriptions and product images using models such as <strong class="bold">Contrastive Language-Image Pretraining</strong> (<strong class="bold">CLIP</strong>). Recall that we used CLIP in <a href="B21773_02.xhtml#_idTextAnchor045"><em class="italic">Chapter 2</em></a> to score the compatibility between<a id="_idIndexMarker363"/> captions and a synthesized image. In this case, we can apply CLIP to measure whether our generated descriptions accurately reflect the visual aspects of the products. Collectively, these evaluation techniques provide objective methods for assessing the<a id="_idIndexMarker364"/> performance of the generative model in creating effective product descriptions for StyleSprint:</p>
<ul>
<li><strong class="bold">Qualitative metrics</strong>: We introduce qualitative evaluation to measure nuances such as the engaging and creative nature of descriptions. We also want to ensure we consider equity and inclusivity in the generated content, which is critical to avoid biases or language that could alienate or offend certain groups. Methods for engagement assessment could include customer surveys or A/B testing, a systematic method for testing two competing solutions. Additionally, having a diverse group reviewing the content for equity and inclusivity could provide valuable insights. These steps help StyleSprint create captivating, respectful, and inclusive product descriptions, fostering a welcoming environment for all customers.</li>
<li><strong class="bold">Scalability</strong>: The computational resources required to run a model and the model’s ability to scale with increasing data are vital considerations. Models that demand extensive computational power may not be practical for real-time generation of product descriptions, especially as the product range expands. A balance between computational efficiency and output quality is essential to ensure cost-effectiveness and scalability for StyleSprint.</li>
<li><strong class="bold">Customization and fine-tuning capabilities</strong>: The ability to fine-tune or customize the model on domain-specific data is crucial for better aligning with brand-specific requirements. Exploring the availability and ease of fine-tuning can significantly impact the relevance and quality of generated descriptions, ensuring that they resonate well with the brand identity and product range of StyleSprint. In practice, some models are too large to fine-tune without considerable resources, even when efficient methods are applied. We will explore fine-tuning considerations in detail in the next chapter.</li>
</ul>
<p>Now that we have carefully considered how we might align the model to the project’s goals, we are almost ready to evaluate our initial model selection against a few others to ensure we make <a id="_idIndexMarker365"/>the right choice. However, before benchmarking, we should dedicate time to understanding one vital aspect of the model selection process<a id="_idTextAnchor151"/>: model size and computational complexity.</p>
<h2 id="_idParaDest-91"><a id="_idTextAnchor152"/>Model size and computational complexity</h2>
<p>The size of a <a id="_idIndexMarker366"/>generative model is often described by the number of parameters it has. Parameters in a model are the internal variables that are fine-tuned during the training process based on the training data. In the context of neural networks used in generative models, parameters typically refer to the weights and biases adjusted through training to minimize the discrepancy between predicted outputs and actual targets.</p>
<p>Moreover, a model with more parameters can capture more complex patterns in the data, often leading to better performance on the task at hand. While larger models often perform better in terms of the quality of the generated text, there’s a point of diminishing returns beyond which increasing model size yields marginal improvements. Moreover, the increased size comes with its own set of challenges:</p>
<ul>
<li><strong class="bold">Computational complexity</strong>: Larger<a id="_idIndexMarker367"/> models require more computational power and memory, during both training and inference (the phase where the model is used to make predictions or generate new data based on the learned parameters). This can significantly increase the costs and the time required to train and use the model, making it less suitable for real-time applications or resource-constrained environments.<p class="list-inset">The number of parameters significantly impacts the computational complexity of a model. Each parameter in a model is a variable that must be stored in memory during computation, during both training and inference. Here are some specific considerations for computational requirements:</p><ul><li><strong class="bold">Memory and storage</strong>: The total size of the model in memory is the product of the number of parameters and the size of each parameter (typically a 32-bit or 64-bit float). For instance, a model with 100 million parameters, each represented by a 32-bit float, would require approximately 400 MB of memory (100 million * 32 bits = 400 million bits = 400 MB). Now consider a larger model, say with 10 billion parameters; the memory requirement jumps to 40 GB (10 billion * 32 bits = 40 billion bits = 40 GB). This requirement is just for the parameters and does not account for other data and overheads the model needs for its operations.</li><li><strong class="bold">Loading into memory</strong>: When a model is used for inference, its parameters must be loaded into the RAM of the machine it’s running on. For a large model with 10 billion parameters, you would need a machine with enough RAM to accommodate the entire model, along with additional memory for the operational overhead, the input data, and the generated output. Suppose the model is too large to fit in memory. In that case, it may need to be <strong class="bold">sharded</strong> or distributed across multiple machines or loaded in parts, which can<a id="_idIndexMarker368"/> significantly complicate the deployment and operation of the model and also increase the latency of generating outputs.</li></ul></li>
<li><strong class="bold">Specialized hardware requirements</strong>: Larger models require specialized hardware, such as powerful GPUs or TPUs, which could increase the project costs. As<a id="_idIndexMarker369"/> discussed, models with a large number of parameters require powerful computational resources for both training and inference. Hardware accelerators such as GPUs and TPUs are often employed to meet these demands. These hardware accelerators are designed to handle the parallel computation capabilities needed for the matrix multiplications and other operations inherent in neural network computations, speeding up the processing significantly compared to <a id="_idIndexMarker370"/>traditional <strong class="bold">central processing </strong><strong class="bold">units</strong> (<strong class="bold">CPUs</strong>).<p class="list-inset">Cloud-based infrastructure can alleviate the complexity of setup but often has usage-based pricing. Understanding infrastructure costs on a granular level is vital to ensuring that StyleSprint stays within its budget.</p></li>
<li><strong class="bold">Latency</strong>: We’ve briefly discussed latency, but it is important to reiterate that larger models typically have higher latency, which could be a problem for applications that require real-time responses. In our case, we can process the descriptions as <a id="_idIndexMarker371"/>batches asynchronously. However, StyleSprint may have projects that require fast turnarounds, requiring batches to be completed in hours and not days.</li>
</ul>
<p>In the case of StyleSprint, the trade-off between model performance and size must be carefully evaluated to ensure the final model meets the project’s performance requirements while staying within budget and hardware constraints. StyleSprint was hoping to have near-real-time responses to provide personalized descriptions, which typically translates to a smaller model with less computational complexity. However, it was also important that the model remains highly accurate and aligns with branding standards for tone <a id="_idIndexMarker372"/>and voice, which may require a larger model trained or fine-tuned on a larger dataset. In practice, we can evaluate the performance of models rela<a id="_idTextAnchor153"/>tive to size and complexity through benchmarking.</p>
<h2 id="_idParaDest-92"><a id="_idTextAnchor154"/>Benchmarking</h2>
<p>Benchmarking is a <a id="_idIndexMarker373"/>systematic process used to <a id="_idIndexMarker374"/>evaluate the performance of different generative models against predefined criteria. This process involves comparing the models on various metrics to understand their strengths, weaknesses, and suitability for the project. It is an empirical method (based on observation) to obtain data on how the models perform under similar conditions, providing insights that can inform the decision-making process for model selection.</p>
<p>In the StyleSprint scenario, benchmarking can be an invaluable exercise to navigate the trade-offs between model size, computational complexity, and the accuracy and creativity of generated descriptions.</p>
<p>For our benchmarking exercise, we can return to our Google Colab prototyping environment to quickly load various generative models and run them through tests designed to evaluate their performance based on the considerations outlined in the previous sections, such as computational efficiency and text generation quality. Once we have completed our evaluation and comparison, we can make a few simple changes to our production application code and it will automatically redeploy. Benchmarking will be instrumental in measuring the quality of the descriptions relative to the model size and complexity. Recall that we will measure quality and overall model performance along several dimensions, including lexical and semantic similarity to a “gold standard” of human-written descriptions, and a qualitative assessment performed by a diverse group of reviewers.</p>
<p>The next step<a id="_idIndexMarker375"/> is<a id="_idIndexMarker376"/> to revisit and adapt our original prototyping code to include<a id="_idTextAnchor155"/> a few challenger models and apply evaluation metrics.</p>
<h1 id="_idParaDest-93"><a id="_idTextAnchor156"/>Updating the prototyping environment</h1>
<p>For our evaluation<a id="_idIndexMarker377"/> steps, there are a few key changes to our original experimentation setup in Google Colab. First, we will want to make sure we leverage performance acceleration. Google Colab offers acceleration via GPU or TPU environments. For this experiment, we will leverage GPU. We will also want to transition from the Transformers library to a slightly more versatile library such as Langchain, which allows us to test both open source models such as GPT-Neo and commercial models such as GPT-3.5.</p>
<h2 id="_idParaDest-94"><a id="_idTextAnchor157"/>GPU configuration</h2>
<p>Ensure you have <a id="_idIndexMarker378"/>a GPU enabled for better<a id="_idIndexMarker379"/> performance. Returning to Google Colab, we can follow these steps to enable GPU acceleration:</p>
<ol>
<li>Click on <strong class="bold">Runtime</strong> in the top menu (see <em class="italic">Figure 4</em><em class="italic">.2</em>):</li>
</ol>
<div><div><img alt="Figure 4.2: Runtime drop-down menu" src="img/B21773_04_011.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2: Runtime drop-down menu</p>
<ol>
<li value="2">Select <strong class="bold">Change runtime type</strong> from the drop-down menu, as shown <a id="_idIndexMarker380"/>in the preceding <a id="_idIndexMarker381"/>screenshot.</li>
<li>In the pop-up window, select <strong class="bold">GPU</strong> from the <strong class="bold">Hardware accelerator</strong> drop-down menu (see <em class="italic">Figure 4</em><em class="italic">.3</em>):</li>
</ol>
<div><div><img alt="Figure 4.3: Select GPU and click on Save" src="img/B21773_04_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3: Select GPU and click on Save</p>
<ol>
<li value="4">Click on <strong class="bold">Save</strong>.</li>
</ol>
<p>Now your notebook is set up to use a GPU to significantly speed up the computations needed for the benchmarking process. You can verify the GPU availability using the following code snippet:</p>
<pre class="source-code">
# Verify GPU is available
import torch
torch.cuda.is_available()</pre>
<p>This code snippet will return <code>True</code> if a GPU is available and <code>False</code> otherwise. This setup ensures that you have the necessary computational resources to benchmark various<a id="_idIndexMarker382"/> generative<a id="_idIndexMarker383"/> models. The utilization of a GPU will be crucial when it <a id="_idTextAnchor158"/>comes to handling large models and extensive computations.</p>
<h2 id="_idParaDest-95"><a id="_idTextAnchor159"/>Loading pretrained models with LangChain</h2>
<p>In our first <a id="_idIndexMarker384"/>simple<a id="_idIndexMarker385"/> experiment, we relied on the Transformers library to load an open source version of GPT. However, for our benchmarking exercise, we want to evaluate the retail version of GPT-3 alongside open source models. We can leverage LangChain, a versatile library that provides a streamlined interface, to access both open source models from providers such as Hugging Face and closed source models such as OpenAI’s GPT-3.5. LangChain offers a unified API that simplifies benchmarking and comparison through standardization. Here <a id="_idIndexMarker386"/>are the steps<a id="_idIndexMarker387"/> to do it:</p>
<ol>
<li><strong class="bold">Install necessary libraries</strong>: We begin by installing the required libraries in our Colab environment. LangChain simplifies the interaction with models hosted on OpenAI and Hugging Face.<pre class="source-code">
!pip -q install openai langchain huggingface_hub</pre></li>
<li><strong class="bold">Set up credentials</strong>: We obtain the credentials from OpenAI for accessing GPT-3, GPT-4, or whichever closed source model we select. We also provide credentials for the Hugging Face Hub, which hosts over 350,000 open source models. We must store these credentials securely to prevent any unauthorized access, especially in the case where model usage has an associated cost.<pre class="source-code">
import os</pre><pre class="source-code">
os.environ['OPENAI_API_KEY'] = 'your_openai_api_key_here'</pre><pre class="source-code">
os.environ['HUGGINGFACEHUB_API_TOKEN'] = </pre><pre class="source-code">
    'your_huggingface_token_here'</pre></li>
<li><strong class="bold">Load models</strong>: With LangChain, we can quickly load models and generate responses. The following example demonstrates how to load GPT-3 and GPT-Neo from Hugging Face:<pre class="source-code">
!pip install openai langchain[llms] huggingface_hub</pre><pre class="source-code">
from langchain.llms import OpenAI, HuggingFaceHub</pre><pre class="source-code">
# Loading GPT-3</pre><pre class="source-code">
llm_gpt3 = OpenAI(model_name='text-davinci-003',</pre><pre class="source-code">
                  temperature=0.9,</pre><pre class="source-code">
                  max_tokens = 256)</pre><pre class="source-code">
# Loading Neo from Hugging Face</pre><pre class="source-code">
llm_neo = HuggingFaceHub(repo_id=' EleutherAI/gpt-neo-2.7B',</pre><pre class="source-code">
                         model_kwargs={"temperature":0.9}</pre><pre class="source-code">
)</pre></li>
</ol>
<p>Notice that we have loaded two models that are significantly different in size. As the model signature suggests, GPT-Neo was trained on 2.7 billion parameters. Meanwhile, according to information available from OpenAI, Davinci was trained on 175 billion parameters. As discussed, a model that is significantly larger is expected to have captured much more complex patterns and will likely outperform a smaller model. However, these very large models are typically hosted by major providers and have higher usage costs. We will revisit cost considerations later. For now, we can continue to the next step, which <a id="_idIndexMarker388"/>is to prepare <a id="_idIndexMarker389"/>our testing data. Our test data should provide a baseline for model per<a id="_idTextAnchor160"/>formance that will inform the cost versus performance trade-off.</p>
<h2 id="_idParaDest-96"><a id="_idTextAnchor161"/>Setting up testing data</h2>
<p>In this context, testing <a id="_idIndexMarker390"/>data should comprise <a id="_idIndexMarker391"/>product attributes from the StyleSprint website (e.g., available colors, sizes, materials, etc.) and existing product descriptions written by the StyleSprint team. The human-written descriptions serve as the “ground truth,” or the standard against which to compare the models’ generated descriptions.</p>
<p>We can gather product data from existing datasets by scraping data from e-commerce websites or using a pre-collected dataset from StyleSprint’s database. We should also ensure a varied collection of products to test a model’s capability across different categories and styles. The process of dividing data into distinct groups or segments based on shared characteristics is typically referred to as segmentation. Understanding a model’s behavior across segments should give us an indication of how well it can perform across the entire family of products. For the purposes of this example, product data is made available in the GitHub companion to this book (<a href="https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python">https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python</a>).</p>
<p>Let’s see how<a id="_idIndexMarker392"/> we can extract relevant information <a id="_idIndexMarker393"/>for further processing:</p>
<pre class="source-code">
import pandas as pd
# Assume `product_data.csv` is a CSV file with product data
# The CSV file has two columns: 'product_image' and 
# 'product_description' 
# Load the product data
product_data = pd.read_csv('product_data.csv')
# Split the data into testing and reference sets
test_data = product_data.sample(frac=0.2, random_state=42)
reference_data = product_data.drop(test_data.index)
# Checkpoint the testing and reference data
test_data.to_csv('test_data.csv', index=False)
reference_data.to_csv('reference_data.csv', index=False)
# Extract reference descriptions and image file paths
reference_descriptions = /
    reference_data['product_description'].tolist()
product_images = reference_data['product_image'].tolist()</pre>
<p>We must also format the product data in a way that makes it ready to be input into the models for description generation. This could be just the product title or a combination of product attributes:</p>
<pre class="source-code">
# Assume `product_metadata` is a column in the data that contains the collective information about the product including the title of the product and attributes.
# Format the input data for the models
model_input_data = reference_data['product_metadata].tolist()
reference_descriptions = \
    reference_data['product_description'].tolist()</pre>
<p>Finally, we will ask<a id="_idIndexMarker394"/> the model to generate a batch <a id="_idIndexMarker395"/>of product descriptions using each model.</p>
<pre class="source-code">
from langchain import LLMChain, PromptTemplate
from tqdm.auto import tqdm
template = """
Write a creative product description for the following product: {product_metadata}
"""
PROMPT = PromptTemplate(template=template, 
    input_variables=["product_metadata"])
def generate_descriptions(
    llm: object, 
    prompt: PromptTemplate = PROMPT
) -&gt; list:
    # Initialize the LLM chain
    llm_chain = LLMChain(prompt=prompt, llm=llm)
    descriptions = []
    for i in tqdm(range(len(model_input_data))):
        description = llm_chain.run(model_input_data[i])
        descriptions.append(description)
    return descriptions
gpt3_descriptions = generate_descriptions(llm_gpt3)
gptneo_descriptions = generate_descriptions(llm_neo)</pre>
<p>Now, with the testing data set up, we have a structured dataset of product information, ref<a id="_idTextAnchor162"/>erence<a id="_idIndexMarker396"/> descriptions, and images ready <a id="_idIndexMarker397"/>for use in the evaluation steps.</p>
<h1 id="_idParaDest-97"><a id="_idTextAnchor163"/>Quantitative metrics evaluation</h1>
<p>Now that we have<a id="_idIndexMarker398"/> leveraged Langchain to load multiple models and prepared testing data, we are ready to begin applying evaluation metrics. These metrics capture accuracy and alignment with product images and will help us assess how well the models generate product descriptions compared to humans. As discussed, we focused on two categories of metrics, lexical and semantic similarity, which provide a measure of how many of the same words were used and how much semantic information is common to both the human and AI-generated product descriptions.</p>
<p>In the following code block, we apply <code>BLEU</code>, <code>ROUGE</code>, and <code>METEOR</code> to evaluate the lexical similarity between the generated text and the reference text. Each of these has a reference-based assumption. This means that each metric assumes we are comparing against a human reference. We have already set aside our reference descriptions (or gold standard) for a diverse set of products to compare side-by-side with the generated descriptions.</p>
<pre class="source-code">
!pip install rouge sumeval nltk
# nltk requires an additional package
import nltk
nltk.download('wordnet')
 from nltk.translate.bleu_score import sentence_bleu
from rouge import Rouge
from sumeval.metrics.rouge import RougeCalculator
from nltk.translate.meteor_score import meteor_score
def evaluate(
    reference_descriptions: list, 
    generated_descriptions: list
) -&gt; tuple:
    # Calculating BLEU score
    bleu_scores = [
        sentence_bleu([ref], gen) 
        for ref, gen in zip(reference_descriptions, generated_descriptions)
    ]
    average_bleu = sum(bleu_scores) / len(bleu_scores)
    # Calculating ROUGE score
    rouge = RougeCalculator()
    rouge_scores = [rouge.rouge_n(gen, ref, 2) for ref,
        gen in zip(reference_descriptions,
        generated_descriptions)]
    average_rouge = sum(rouge_scores) / len(rouge_scores)
    # Calculating METEOR score
    meteor_scores = [ meteor_score([ref.split() ],
        gen.split()) for ref,
        gen in zip(reference_descriptions,
        generated_descriptions)]
    average_meteor = sum(meteor_scores) / len(meteor_scores)
    return average_bleu, average_rouge, average_meteor
average_bleu_gpt3, average_rouge_gpt3, average_meteor_gpt3 = \
    evaluate(reference_descriptions, gpt3_descriptions)
print(average_bleu_gpt3, average_rouge_gpt3, average_meteor_gpt3)
average_bleu_neo, average_rouge_neo, average_meteor_neo = \
    evaluate(reference_descriptions, gptneo_descriptions)
print(average_bleu_neo, average_rouge_neo, average_meteor_neo)</pre>
<p>We can evaluate <a id="_idIndexMarker399"/>the semantic coherence and relevance of the generated descriptions using sentence embeddings:</p>
<pre class="source-code">
!pip install sentence-transformers
from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')
def cosine_similarity(reference_descriptions, generated_descriptions):
    # Calculating cosine similarity for generated descriptions
    cosine_scores = [util.pytorch_cos_sim(
        model.encode(ref), model.encode(gen)) for ref,
        gen in zip(reference_descriptions,
        generated_descriptions)]
    average_cosine = sum(cosine_scores) / len(cosine_scores)
    return average_cosine
average_cosine_gpt3 = cosine_similarity(
    reference_descriptions, gpt3_descriptions)
print(average_cosine_gpt3)
average_cosine_neo = cosine_similarity(
    reference_descriptions, gptneo_descriptions)
print(average_cosine_neo)</pre>
<h2 id="_idParaDest-98"><a id="_idTextAnchor164"/>Alignment with CLIP</h2>
<p>We again leverage<a id="_idIndexMarker400"/> the CLIP model to<a id="_idIndexMarker401"/> evaluate the alignment between generated product descriptions and corresponding images, similar to our approach in <a href="B21773_02.xhtml#_idTextAnchor045"><em class="italic">Chapter 2</em></a>. The CLIP model, adept at correlating visual and textual content, scores the congruence between each product image and its associated generated and reference descriptions. The reference description serves as a human baseline for accuracy. These scores provide a quantitative measure of our generative model’s effectiveness at producing descriptions that correspond well to the product image. The following is a snippet from a component that processes the generated descriptions combined with corresponding images to generate a CLIP score. The full component code (including image pre-processing) is available in the <code>chapter 4</code> folder <a id="_idIndexMarker402"/>of this<a id="_idIndexMarker403"/> book’s GitHub repository at <a href="https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python">https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python</a>).</p>
<pre class="source-code">
clip_model = "openai/clip-vit-base-patch32"
def clip_scores(images, descriptions,
                model=clip_model,
                processor=clip_processor
):
    scores = []
    # Process all images and descriptions together
    inputs = process_inputs(processor, descriptions, images)
    # Get model outputs
    outputs = model(**inputs)
    logits_per_image = outputs.logits_per_image # Image-to-text logits
    # Diagonal of the matrix gives the scores for each image-description pair
    for i in range(logits_per_image.size(0)):
        score = logits_per_image[i, i].item()
    scores.append(score)
    return scores
reference_images = [
    load_image_from_path(image_path) 
    for image_path in reference_data.product_image_path
]
gpt3_generated_scores = clip_scores(
    reference_images, gpt3_descriptions
)
reference_scores = clip_scores(
    reference_images, reference_descriptions
)
# Compare the scores
for i, (gen_score, ref_score) in enumerate(
    zip(gpt3_generated_scores, reference_scores)
):
    print(f"Image {i}: Generated Score = {gen_score:.2f}, 
        Reference Score = {ref_score:.2f}")</pre>
<p>In evaluating product descriptions using the CLIP model, the alignment scores generated for each image-description pair are computed relative to other descriptions in the batch. Essentially, CLIP assesses how well a specific description (either generated or reference) aligns with a given image compared to other descriptions within the same batch. For example, a score of 33.79 indicates that the description aligns with the image 33.79% better than the other descriptions in the batch align with that image. In comparing against the reference, we expect that the scores based on the generated descriptions should align closely with the scores based on the reference descriptions.</p>
<p>Now that we have <a id="_idIndexMarker404"/>calculated lexical and semantic similarity to the reference scores, and alignment <a id="_idIndexMarker405"/>between images and generated descriptions relative to reference descriptions, we can evaluate our models holistically<a id="_idTextAnchor165"/> and interpret the outcome of our quantitative evaluation.</p>
<h2 id="_idParaDest-99"><a id="_idTextAnchor166"/>Interpreting outcomes</h2>
<p>We begin with<a id="_idIndexMarker406"/> lexical similarity, which gives us an indication of similarity in phrasing and keywords between the reference and generated descriptions:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table002">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><strong class="bold">BLEU</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">ROUGE</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">METEOR</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>GPT-3.5</p>
</td>
<td class="No-Table-Style">
<p>0.147</p>
</td>
<td class="No-Table-Style">
<p>0.094</p>
</td>
<td class="No-Table-Style">
<p>0.261</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>GPT-Neo</p>
</td>
<td class="No-Table-Style">
<p>0.132</p>
</td>
<td class="No-Table-Style">
<p>0.05</p>
</td>
<td class="No-Table-Style">
<p>0.059</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.2: Lexical similarity</p>
<p>In evaluating text generated by GPT-3.5 and GPT-Neo models, we use several lexical similarity metrics: BLEU, ROUGE, and METEOR. BLEU scores, which assess the precision of matching phrases, show GPT-3.5 (0.147) slightly outperforming GPT-Neo (0.132). ROUGE scores, focusing on the recall of content, indicate that GPT-3.5 (0.094) better captures reference content than GPT-Neo (0.05). METEOR scores, combining both precision and recall with synonym matching, reveal a significant lead for GPT-3.5 (0.261) over GPT-Neo (0.059). Overall, these metrics suggest that GPT-3.5’s generated text aligns more closely with reference standards, both in word choice and content coverage, compared to that of GPT-Neo.</p>
<p>Next, we evaluate semantic similarity, which measures how closely the meanings of the generated text align with the reference text. This assessment goes beyond mere word-to-word matching and considers the context and overall intent of the sentences. Semantic similarity<a id="_idIndexMarker407"/> evaluates the extent to which the generated text captures the nuances, concepts, and themes present in the reference text, providing insight into the model’s ability to understand and replicate deeper semantic meanings:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table003">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Model</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Mean </strong><strong class="bold">cosine similarity</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>GPT-3.5</p>
</td>
<td class="No-Table-Style">
<p>0.8192</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>GPT-Neo</p>
</td>
<td class="No-Table-Style">
<p>0.2289</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.3: Semantic similarity</p>
<p>The mean cosine similarity scores reveal a stark contrast between the two models’ performance in semantic similarity. GPT-3.5 shows a high degree of semantic alignment with the reference text. GPT-Neo’s significantly lower score suggests a relatively poor performance, indicating that the generated descriptions were fundamentally dissimilar to descriptions written by humans.</p>
<p>Finally, we review the CLIP scores, which tell us how well the generated descriptions align visually with the corresponding images. These scores, derived from a model trained to understand and correlate visual and textual data, provide a measure of the relevance and accuracy of the text in representing the visual content. High CLIP scores indicate a strong correlation between the text and the image, suggesting that the generated descriptions are not only textually coherent but also contextually appropriate and visually descriptive:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table004">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Model</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Mean CLIP</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Reference delta</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>GPT-3.5</p>
</td>
<td class="No-Table-Style">
<p>26.195</p>
</td>
<td class="No-Table-Style">
<p>2.815</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>GPT-Neo</p>
</td>
<td class="No-Table-Style">
<p>22.647</p>
</td>
<td class="No-Table-Style">
<p>6.363</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.4: Comparative CLIP score analysis for GPT-3.5 and GPT-Neo models</p>
<p>We calculated the CLIP scores from the reference descriptions, which represent the average alignment <a id="_idIndexMarker408"/>score between a set of benchmark descriptions and the corresponding images. We then calculated CLIP scores for each model and analyzed the delta. In concert with our other metrics, GPT-3.5 has a clear advantage over GPT-Neo, aligning more closely with the reference.</p>
<p>Overall, GPT-3.5 appears to significantly outperform GPT-Neo across all quantitative measures. However, it is worth noting that GPT-3.5 incurs a higher cost and generally has a higher latency than GPT-Neo. In this case, the StyleSprint team would conduct a qualitative analysis to accurately determine whether the GPT-Neo descriptions do not align with brand guidelines and expectations, therefore making the cost of using the better model worthwhile. As discussed, the trade-off here is not clear-cut. StyleSprint must carefully consider that although using a commodity such as GPT-3.5 does not incur computational costs directly, on-demand costs could increase significantly as model usage rises.</p>
<p>The contrasting strengths of the two models pose a decision-making challenge. While one clearly excels in performance metrics and alignment with CLIP, implying higher accuracy and semantic correctness, the other is significantly more resource-efficient and scalable, which is crucial for cost-effectiveness. At this stage, it becomes critical to assess model outcomes qualitatively and to engage stakeholders to help understand organizational priorities.</p>
<p>With these considerations in mind, we’ll revisit qualitative considerations such as transparency, bias, and <a id="_idIndexMarker409"/>fairness and how they play into the broader picture of deploying a responsible and effective AI system<a id="_idTextAnchor167"/>.</p>
<h1 id="_idParaDest-100"><a id="_idTextAnchor168"/>Responsible AI considerations</h1>
<p>Addressing implicit or<a id="_idIndexMarker410"/> covert societal biases in AI systems is crucial to ensure responsible AI deployment. Although it may not seem obvious how a simple product description could introduce bias, the language used can inadvertently reinforce stereotypes or exclude certain groups. For instance, descriptions that consistently associate certain body types or skin tones with certain products or that unnecessarily default to gendered language can unintentionally perpetuate societal biases. However, with a structured mitigation approach, including algorithmic audits, increased model transparency, and stakeholder engagement, StyleSprint can make sure its brand promotes equity and inclusion.</p>
<h2 id="_idParaDest-101"><a id="_idTextAnchor169"/>Addressing and mitigating biases</h2>
<p>We present <a id="_idIndexMarker411"/>several considerations, as suggested by Costanza-Chock et al. in <em class="italic">Who Audits the Auditors? Recommendations from a field scan of the algorithmic </em><em class="italic">auditing ecosystem</em>:</p>
<ul>
<li><strong class="bold">Professional</strong><strong class="bold"> environment examination</strong>: Creating a supportive professional environment is crucial for addressing algorithmic fairness. Implementing whistleblower protections facilitates the safe reporting of biases and unfair practices while establishing processes for individuals to report harms to ensure these concerns are addressed proactively.</li>
<li><strong class="bold">Custom versus standardized audit frameworks</strong>: While custom audit frameworks are expected, considering standardized methods may enhance rigor and transparency in bias mitigation efforts. Engaging with external auditing entities could offer unbiased evaluations of StyleSprint’s AI systems, aligning with the observations by Costanza-Chock et al. (2022).</li>
<li><strong class="bold">Focusing on equity, not just equality</strong>: Equity notions acknowledge differing needs, essential for a comprehensive approach to fairness. Performing intersectional and small population analyses could help you to understand and address biases beyond legally protected classes.</li>
<li><strong class="bold">Disclosure and transparency</strong>: Disclosing audit methods and outcomes can foster a culture of transparency and continuous improvement. Officially released audits could help you establish best practices and gain stakeholder trust.</li>
<li><strong class="bold">Mixed methods analyses</strong>: As presented, a mix of technical and qualitative analyses could provide a holistic view of the system’s fairness. Engaging non-technical stakeholders could emphasize qualitative analyses.</li>
<li><strong class="bold">Community and stakeholder engagement</strong>: Again, involving diverse groups and domain experts in audits could ensure diverse perspectives are considered in bias mitigation efforts. Establishing feedback loops with stakeholders could facilitate continuous improvement.</li>
<li><strong class="bold">Continuous learning and improvement</strong>: Staying updated on emerging standards and best practices regarding AI fairness is crucial for continuous improvement. Fostering a culture of learning could help in adapting to evolving fairness challenges<a id="_idIndexMarker412"/> and regulatory landscapes, thus ensuring StyleSprint’s AI systems remain fair and responsible over t<a id="_idTextAnchor170"/>ime.</li>
</ul>
<h2 id="_idParaDest-102"><a id="_idTextAnchor171"/>Transparency and explainability</h2>
<p>Generally, explainability in <a id="_idIndexMarker413"/>machine learning refers to the ability to understand the internal mechanics of a model, elucidating how it makes decisions or predictions based on given inputs. However, achieving explainability in generative models can be much more complex. As discussed, unlike discriminative machine learning models, generative models do not have the objective of learning a decision boundary, nor do they reflect a clear notion of features or a direct mapping between input features and predictions. This absence of feature-based decision-making makes traditional explainability techniques ineffective for generative foundational models such as GPT-4.</p>
<p>Alternatively, we can adopt some pragmatic transparency practices, such as clear documentation made accessible to all relevant stakeholders, to foster a shared understanding and expectations regarding the model’s capabilities and usage.</p>
<p>The topic of explainability is a critical space to watch, especially as generative models become more complex and their outcomes become increasingly more difficult to rationalize, which may present unknown risk implicati<a id="_idTextAnchor172"/>ons.</p>
<p>Promising research from Anthropic, OpenAI, and others suggests that sparse autoencoders—neural networks that activate only a few neurons at a time—could facilitate the identification of abstract and understandable patterns. This method could help explain the network's behavior by highlighting features that align with human concepts.</p>
<h1 id="_idParaDest-103"><a id="_idTextAnchor173"/>Final deployment</h1>
<p>Assuming we have<a id="_idIndexMarker414"/> carefully gathered quantitative and qualitative feedback regarding the best model for the job, we can select our model and update our production environment to deploy and serve it. We will continue to use FastAPI for creating a web server to serve our model, and Docker to containerize our application. However, now that we have been introduced to the simplicity of LangChain, we will continue to leverage its simplified interface. Our existing CI/CD pipeline will ensure streamlined automatic deployment and continuous application monitoring. This means that deploying our model is as simple as checking-in our latest code. We begin with updating our dependencies list:</p>
<ol>
<li><code>requirements.txt</code> file in your project to include the necessary libraries:<pre class="source-code">
fastapi==0.68.0</pre><pre class="source-code">
uvicorn==0.15.0</pre><pre class="source-code">
openai==0.27.0</pre><pre class="source-code">
langchain==0.1.0</pre></li>
<li><strong class="bold">Update the Dockerfile</strong>: Modify your Dockerfile to ensure it installs the updated requirements <a id="_idIndexMarker415"/>and properly sets up the environment for running LangChain with FastAPI:<pre class="source-code">
# Use an official Python runtime as a base image</pre><pre class="source-code">
FROM python:3.8-slim-buster</pre><pre class="source-code">
# Set the working directory in the container to /app</pre><pre class="source-code">
WORKDIR /app</pre><pre class="source-code">
# Copy the current directory contents into the container at /app</pre><pre class="source-code">
COPY . /app</pre><pre class="source-code">
# Install any needed packages specified in requirements.txt</pre><pre class="source-code">
RUN pip install --no-cache-dir -r requirements.txt</pre><pre class="source-code">
# Make port 80 available to the world outside this container</pre><pre class="source-code">
EXPOSE 80</pre><pre class="source-code">
# Define environment variable</pre><pre class="source-code">
ENV NAME World</pre><pre class="source-code">
# Run app.py when the container launches</pre><pre class="source-code">
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "80"]</pre></li>
<li><strong class="bold">Update the FastAPI application</strong>: Modify your FastAPI application to utilize Langchain for interacting with GPT-3.5. Ensure your OpenAI API key is securely stored and <a id="_idIndexMarker416"/>accessible to your application:<pre class="source-code">
from fastapi import FastAPI, HTTPException, Request</pre><pre class="source-code">
from langchain.llms import OpenAI</pre><pre class="source-code">
import os</pre><pre class="source-code">
# Initialize FastAPI app</pre><pre class="source-code">
app = FastAPI()</pre><pre class="source-code">
# Setup Langchain with GPT-3.5</pre><pre class="source-code">
llm = OpenAI(model_name='text-davinci-003',</pre><pre class="source-code">
             temperature=0.7,</pre><pre class="source-code">
             max_tokens=256,</pre><pre class="source-code">
             api_key=os.environ['OPENAI_API_KEY'])</pre><pre class="source-code">
@app.post("/generate/")</pre><pre class="source-code">
async def generate_text(request: Request):</pre><pre class="source-code">
    data = await request.json()</pre><pre class="source-code">
    prompt = data.get('prompt')</pre><pre class="source-code">
    if not prompt:</pre><pre class="source-code">
        raise HTTPException(status_code=400,</pre><pre class="source-code">
            detail="Prompt is required")</pre><pre class="source-code">
    response = llm(prompt)</pre><pre class="source-code">
    return {"generated_text": response}</pre></li>
</ol>
<h2 id="_idParaDest-104"><a id="_idTextAnchor174"/>Testing and monitoring</h2>
<p>Once the model is deployed, perform<a id="_idIndexMarker417"/> necessary tests to ensure the setup works as expected. Continue to monitor the system’s performance, errors, and other critical metrics to ensure reliable operation.</p>
<p>By this point, we have updated our production environment to deploy and serve GPT-3.5, facilitating the generation of text based on the prompts received via the FastAPI application. This setup ensures a scalable, maintainable, and secure deployment of our new generative model. However, we should also explore some best practices regarding application rel<a id="_idTextAnchor175"/>iability.</p>
<h2 id="_idParaDest-105"><a id="_idTextAnchor176"/>Maintenance and reliability</h2>
<p>Maintaining reliability<a id="_idIndexMarker418"/> in our StyleSprint deployment is critical. As we employ Langchain with FastAPI, Docker, and CI/CD, it’s essential to set up monitoring, alerting, automatic remediation, and failover mechanisms. This section outlines a possible approach to ensure continuous operation and robustness in our production environment:</p>
<ul>
<li><strong class="bold">Monitoring tools</strong>: Integrate monitoring tools within the CI/CD pipeline to continuously track system performance and model metrics. This step is fundamental for identifying and rectifying issues proactively.</li>
<li><strong class="bold">Alerting mechanisms</strong>: Establish alerting mechanisms to notify the maintenance team whenever anomalies or issues are detected. Tuning the alerting thresholds accurately is crucial to catch issues early and minimize false alarms.</li>
<li><strong class="bold">Automatic remediation</strong>: Utilize Kubernetes’ self-healing features and custom scripts triggered by certain alerts for automatic remediation. This setup aims to resolve common issues autonomously, reducing the need for human intervention.</li>
<li><strong class="bold">Failover mechanisms</strong>: Implement a failover mechanism by setting up secondary servers and databases. In case of primary server failure, these secondary setups take over to ensure continuous service availability.</li>
<li><strong class="bold">Regular updates via CI/CD</strong>: Employ the CI/CD pipeline for managing, testing, and deploying updates to LangChain, FastAPI, or other components of the stack. This process keeps the deployment updated and secure, reducing the maintenance burden significantly.</li>
</ul>
<p>By meticulously <a id="_idIndexMarker419"/>addressing each of these areas, you’ll be laying down a solid foundation for a reliable and maintainable StyleSprint <a id="_idTextAnchor177"/>deployment.</p>
<h1 id="_idParaDest-106"><a id="_idTextAnchor178"/>Summary</h1>
<p>This chapter outlined the process of transitioning the StyleSprint generative AI prototype to a production-ready deployment for creating engaging product descriptions on an e-commerce platform. It started with setting up a robust Python environment using Docker, GitHub, and CI/CD pipelines for efficient dependency management, testing, and deployment. The focus then shifted to selecting a suitable pretrained model, emphasizing alignment with project goals, computational considerations, and responsible AI practices. This selection relied on both quantitative benchmarking and qualitative evaluation. We then outlined the deployment of the selected model using FastAPI and LangChain, ensuring a scalable and reliable production environment.</p>
<p>Following the strategies outlined in this chapter will equip teams with the necessary insights and steps to successfully transition their generative AI prototype into a maintainable and value-adding production system. In the next chapter, we will explore fine-tuning and its importance in LLMs. We will also weigh in on the decision-making process, addressing when it is more beneficial to fine-tune versus zero or few-shot prompting.</p>
</div>


<div><h1 id="_idParaDest-107" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor179"/>Part 2: Practical Applications of Generative AI</h1>
<p>This part focuses on the practical applications of generative AI, including fine-tuning models for specific tasks, understanding domain adaptation, mastering prompt engineering, and addressing ethical considerations. It aims to provide hands-on insights and methodologies for effectively implementing and leveraging generative AI in various contexts with a focus on responsible adoption.</p>
<p>This part contains the following chapters:</p>
<ul>
<li><a href="B21773_05.xhtml#_idTextAnchor180"><em class="italic">Chapter 5</em></a>, <em class="italic">Fine-Tuning Generative Models for Specific Tasks</em></li>
<li><a href="B21773_06.xhtml#_idTextAnchor211"><em class="italic">Chapter 6</em></a>, <em class="italic">Understanding Domain Adaptation for Large Language Models</em></li>
<li><a href="B21773_07.xhtml#_idTextAnchor225"><em class="italic">Chapter 7</em></a>, <em class="italic">Mastering the Fundamentals of Prompt Engineering</em></li>
<li><a href="B21773_08.xhtml#_idTextAnchor251"><em class="italic">Chapter 8</em></a>, <em class="italic">Addressing Ethical Considerations and Charting a Path toward Trustworthy Generative AI</em></li>
</ul>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
</body></html>