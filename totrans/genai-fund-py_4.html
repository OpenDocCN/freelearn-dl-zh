<html><head></head><body>
<div id="_idContainer027">
<p><a id="_idTextAnchor122"/></p>
<h1 class="chapter-number" id="_idParaDest-75"><a id="_idTextAnchor123"/><span class="koboSpan" id="kobo.1.1">4</span></h1>
<h1 id="_idParaDest-76"><a id="_idTextAnchor124"/><span class="koboSpan" id="kobo.2.1">Applying Pretrained Generative Models: From Prototype to Production</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In the preceding chapters, we explored the fundamentals of generative AI, explored various generative models, such </span><a id="_idIndexMarker306"/><span class="koboSpan" id="kobo.4.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.5.1">generative adversarial networks</span></strong><span class="koboSpan" id="kobo.6.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.7.1">GANs</span></strong><span class="koboSpan" id="kobo.8.1">), diffusers, and transformers, and learned about the transformative </span><a id="_idIndexMarker307"/><span class="koboSpan" id="kobo.9.1">impact of </span><strong class="bold"><span class="koboSpan" id="kobo.10.1">natural language processing</span></strong><span class="koboSpan" id="kobo.11.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.12.1">NLP</span></strong><span class="koboSpan" id="kobo.13.1">). </span><span class="koboSpan" id="kobo.13.2">As we transition into the practical aspects of applying generative AI, we should ground our exploration in a practical example. </span><span class="koboSpan" id="kobo.13.3">This approach will provide a concrete context, making the technical aspects more relatable and the learning experience </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">more engaging.</span></span></p>
<p><span class="koboSpan" id="kobo.15.1">We will introduce “StyleSprint,” a clothing shop looking to enhance its online presence. </span><span class="koboSpan" id="kobo.15.2">One way to achieve this is by crafting unique and engaging product descriptions for its various products. </span><span class="koboSpan" id="kobo.15.3">However, manually creating captivating descriptions for a large inventory is challenging. </span><span class="koboSpan" id="kobo.15.4">This situation is prime opportunity for the application of generative AI. </span><span class="koboSpan" id="kobo.15.5">By leveraging a pretrained generative model, StyleSprint can automate the crafting of compelling product descriptions, saving considerable time and enriching the online shopping experience for </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">its customers.</span></span></p>
<p><span class="koboSpan" id="kobo.17.1">As we step into the practical application of a pretrained generative </span><strong class="bold"><span class="koboSpan" id="kobo.18.1">large language models</span></strong><span class="koboSpan" id="kobo.19.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.20.1">LLM</span></strong><span class="koboSpan" id="kobo.21.1">), the </span><a id="_idIndexMarker308"/><span class="koboSpan" id="kobo.22.1">first order of business is to set up a Python environment conducive to prototyping with generative models. </span><span class="koboSpan" id="kobo.22.2">This setup is vital for transitioning the project from a prototype to a production-ready state, setting the stage for StyleSprint to realize its goal of automated </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">content generation.</span></span></p>
<p><span class="koboSpan" id="kobo.24.1">In </span><em class="italic"><span class="koboSpan" id="kobo.25.1">Chapters 2</span></em><span class="koboSpan" id="kobo.26.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.27.1">3</span></em><span class="koboSpan" id="kobo.28.1">, we used Google Colab for prototyping due to its ease of use and accessible GPU resources. </span><span class="koboSpan" id="kobo.28.2">It served as a great platform to test ideas quickly. </span><span class="koboSpan" id="kobo.28.3">However, as we shift our focus toward deploying our generative model in a real-world setting, it is essential to understand the transition from a prototyping environment such as Google Colab to a more robust, production-ready setup. </span><span class="koboSpan" id="kobo.28.4">This transition will ensure our solution is scalable, reliable, and well-optimized for handling real-world traffic. </span><span class="koboSpan" id="kobo.28.5">In this chapter, we will walk through the steps in setting up a production-ready Python environment, underscoring the crucial considerations for a smooth transition from prototype </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">to production.</span></span></p>
<p><span class="koboSpan" id="kobo.30.1">By the end of this chapter, we will understand the process of taking a generative application from a prototyping environment to a production-ready setup. </span><span class="koboSpan" id="kobo.30.2">We will define a reliable and repeatable strategy for evaluating, monitoring, and deploying models </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">to productio</span><a id="_idTextAnchor125"/><span class="koboSpan" id="kobo.32.1">n.</span></span></p>
<h1 id="_idParaDest-77"><a id="_idTextAnchor126"/><span class="koboSpan" id="kobo.33.1">Prototyping environments</span></h1>
<p><span class="koboSpan" id="kobo.34.1">Jupyter notebooks provide </span><a id="_idIndexMarker309"/><span class="koboSpan" id="kobo.35.1">an interactive computing environment to combine code execution, text, mathematics, plots, and rich media into a single document. </span><span class="koboSpan" id="kobo.35.2">They are ideal for prototyping and interactive development, making them a popular choice among data scientists, researchers, and engineers. </span><span class="koboSpan" id="kobo.35.3">Here is what </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">they offer:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.37.1">Kernel</span></strong><span class="koboSpan" id="kobo.38.1">: At the heart of a Jupyter notebook is a kernel, a computational engine that executes the code contained in the notebook. </span><span class="koboSpan" id="kobo.38.2">For Python, this is typically an IPython kernel. </span><span class="koboSpan" id="kobo.38.3">This kernel remains active and maintains the state of your notebook’s computations while the notebook </span><span class="No-Break"><span class="koboSpan" id="kobo.39.1">is open.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.40.1">Interactive execution</span></strong><span class="koboSpan" id="kobo.41.1">: Code cells allow you to write and execute code interactively, inspecting the results and tweaking the code </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">as necessary.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.43.1">Dependency management</span></strong><span class="koboSpan" id="kobo.44.1">: You can install and manage libraries and dependencies directly within the notebook using </span><strong class="source-inline"><span class="koboSpan" id="kobo.45.1">pip</span></strong><span class="koboSpan" id="kobo.46.1"> or </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.47.1">conda</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.48.1"> commands.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.49.1">Visualization</span></strong><span class="koboSpan" id="kobo.50.1">: You can embed plots, graphs, and other visualizations to explore data and </span><span class="No-Break"><span class="koboSpan" id="kobo.51.1">results interactively.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.52.1">Documentation</span></strong><span class="koboSpan" id="kobo.53.1">: Combining Markdown cells with code cells allows for well-documented, self-contained notebooks that explain the code and </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">its output.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.55.1">A drawback to Jupyter notebooks is that they typically rely on the computational resources of your personal computer. </span><span class="koboSpan" id="kobo.55.2">Most personal laptops and desktops are not optimized or equipped to handle computationally intensive processes. </span><span class="koboSpan" id="kobo.55.3">Having adequate computational resources is crucial for managing the computational complexity of experimenting with an LLM. </span><span class="koboSpan" id="kobo.55.4">Fortunately, we can extend the capabilities of a Jupyter notebook with cloud-based platforms that offer computational accelerators such as </span><strong class="bold"><span class="koboSpan" id="kobo.56.1">graphics processing units</span></strong><span class="koboSpan" id="kobo.57.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.58.1">GPUs</span></strong><span class="koboSpan" id="kobo.59.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.60.1">tensor processing units</span></strong><span class="koboSpan" id="kobo.61.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.62.1">TPUs</span></strong><span class="koboSpan" id="kobo.63.1">). </span><span class="koboSpan" id="kobo.63.2">For example, Google Colab </span><a id="_idIndexMarker310"/><span class="koboSpan" id="kobo.64.1">instantly enhances Jupyter</span><a id="_idIndexMarker311"/><span class="koboSpan" id="kobo.65.1"> notebooks, making them conducive to computationally intensive experimentation. </span><span class="koboSpan" id="kobo.65.2">Here are some of the key features of a cloud-based notebook environment such as </span><span class="No-Break"><span class="koboSpan" id="kobo.66.1">Google Colab:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.67.1">GPU/TPU access</span></strong><span class="koboSpan" id="kobo.68.1">: Provides</span><a id="_idIndexMarker312"/><span class="koboSpan" id="kobo.69.1"> free or affordable access to GPU and TPU resources for accelerated computation, which is crucial when working with demanding machine </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">learning models</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.71.1">Collaboration</span></strong><span class="koboSpan" id="kobo.72.1">: Permits easy sharing and real-time collaboration, similar to </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">Google Docs</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.74.1">Integration</span></strong><span class="koboSpan" id="kobo.75.1">: Allows for easy storage and access to notebooks </span><span class="No-Break"><span class="koboSpan" id="kobo.76.1">and data</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.77.1">Let’s consider our StyleSprint scenario. </span><span class="koboSpan" id="kobo.77.2">We will want to explore a few different models to generate product </span><a id="_idIndexMarker313"/><span class="koboSpan" id="kobo.78.1">descriptions before deciding on one that best fits StyleSprint’s goals. </span><span class="koboSpan" id="kobo.78.2">We can set up a minimal working prototype in Google Colab to compare models. </span><span class="koboSpan" id="kobo.78.3">Again, cloud-based platforms provide an optimal and accessible environment for initial testing, experimentation, and even some lightweight training of models. </span><span class="koboSpan" id="kobo.78.4">Here is how we might initially set up a generative model to start experimenting with automated product description generation </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">for StyleSprint:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.80.1">
# In a Colab or Jupyter notebook
!pip install transformers
# Google Colab Jupyter notebook
from transformers import pipeline
# Initialize a text generation pipeline with a generative model, say GPT-Neo
text_generator = pipeline(
    'text-generation', model='EleutherAI/gpt-neo-2.7B')
# Example prompt for product description generation
prompt = "This high-tech running shoe with advanced cushioning and support"
# Generating the product description
generated_text = text_generator(prompt, max_length=100, do_sample=True)
# Printing the generated product description
print(generated_text[0]['generated_text'])</span></pre>
<p><span class="No-Break"><span class="koboSpan" id="kobo.81.1">Output:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.82.1">
This high-tech running shoe with advanced cushioning and support combines the best of traditional running shoes and the latest technologies.</span></pre>
<p><span class="koboSpan" id="kobo.83.1">In this simple setup, we’re installing the </span><strong class="source-inline"><span class="koboSpan" id="kobo.84.1">transformers</span></strong><span class="koboSpan" id="kobo.85.1"> library, which offers a convenient interface to various pretrained models. </span><span class="koboSpan" id="kobo.85.2">We then initialize a text generation pipeline with an open source version of GPT-Neo, capable of generating coherent and contextually relevant text. </span><span class="koboSpan" id="kobo.85.3">This setup serves as a starting point for StyleSprint to experiment with generating creative product descriptions on a </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">small scale.</span></span></p>
<p><span class="koboSpan" id="kobo.87.1">Later in this chapter, we will expand our experiment to evaluate and compare multiple pretrained generative models to determine which best meets our needs. </span><span class="koboSpan" id="kobo.87.2">However, before advancing further in our experimentation and prototyping, it is crucial to strategically pause</span><a id="_idIndexMarker314"/><span class="koboSpan" id="kobo.88.1"> and project forward. </span><span class="koboSpan" id="kobo.88.2">This deliberate forethought allows us to consider the necessary steps for effectively transitioning our experiment into a production environment. </span><span class="koboSpan" id="kobo.88.3">By doing so, we ensure a comprehensive view of the project from end to end, to align</span><a id="_idIndexMarker315"/><span class="koboSpan" id="kobo.89.1"> with long-term </span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">operational goals.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer024">
<span class="koboSpan" id="kobo.91.1"><img alt="Figure 4.1: Moving from prototyping to production—the stages" src="image/B21773_04_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.92.1">Figure 4.1: Moving from prototyping to production—the stages</span></p>
<h1 id="_idParaDest-78"><a id="_idTextAnchor127"/><span class="koboSpan" id="kobo.93.1">Transitioning to production</span></h1>
<p><span class="koboSpan" id="kobo.94.1">As we plan for a </span><a id="_idIndexMarker316"/><span class="koboSpan" id="kobo.95.1">production setup, we should first understand the intrinsic benefits and features of the prototyping environment we will want to carry forward to a production setting. </span><span class="koboSpan" id="kobo.95.2">Many of the features of prototyping environments such as Google Colab are deeply integrated and can easily go unnoticed, so it is important to dissect and catalog the features we will need in production. </span><span class="koboSpan" id="kobo.95.3">For example, the following features are inherent in Google Colab and will be critical </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">in production:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.97.1">Package management</span></strong><span class="koboSpan" id="kobo.98.1">: In Colab, installing necessary libraries is as straightforward as </span><a id="_idIndexMarker317"/><span class="koboSpan" id="kobo.99.1">executing a cell with </span><strong class="source-inline"><span class="koboSpan" id="kobo.100.1">!pip install library_name</span></strong><span class="koboSpan" id="kobo.101.1">. </span><span class="koboSpan" id="kobo.101.2">In production, we will have to preinstall libraries or make sure we can install them as needed. </span><span class="koboSpan" id="kobo.101.3">We must also ensure that project-specific libraries do not interfere with </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">other projects.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.103.1">Dependency isolation</span></strong><span class="koboSpan" id="kobo.104.1">: Google Colab automatically facilitates isolated dependencies, ensuring package installations and updates do not interfere with other projects. </span><span class="koboSpan" id="kobo.104.2">In production, we may also want to deploy various projects using the same infrastructure. </span><span class="koboSpan" id="kobo.104.3">Dependency isolation will be critical to prevent one project’s dependency updates from impacting </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">other projects.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.106.1">Interactive code execution</span></strong><span class="koboSpan" id="kobo.107.1">: The interactive execution of code cells helps in testing individual code snippets, visualizing results, and debugging in real time. </span><span class="koboSpan" id="kobo.107.2">This convenience is not necessary in production but could be helpful for </span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">quick debugging.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.109.1">Resource accessibility</span></strong><span class="koboSpan" id="kobo.110.1">: With Colab, access to GPUs and TPUs is simplified, which is crucial for running computation-intensive tasks. </span><span class="koboSpan" id="kobo.110.2">For production, we will want to examine our dynamic computational needs and provision the </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">appropriate infrastructure.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.112.1">Data integration</span></strong><span class="koboSpan" id="kobo.113.1">: Colab offers simple connectivity to data sources for analysis and modeling. </span><span class="koboSpan" id="kobo.113.2">In production, we can either bootstrap our environment with data (i.e., deploy data directly into the environment) or ensure connectivity to remote data sources </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">as needed.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.115.1">Versioning and collaboration</span></strong><span class="koboSpan" id="kobo.116.1">: Tracking versions of your project code with Google Colab can easily be accomplished using notebooks. </span><span class="koboSpan" id="kobo.116.2">Additionally, Colab is preconfigured to interact with Git. </span><span class="koboSpan" id="kobo.116.3">Git is a distributed version control system that is widely used for tracking changes in source code during software development. </span><span class="koboSpan" id="kobo.116.4">In production, we will also want to integrate Git to manage our code and synchronize it with a remote code repository such as GitHub or Bitbucket. </span><span class="koboSpan" id="kobo.116.5">Remote versioning ensures that our production environment always reflects the latest changes and enables </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">ongoing collaboration.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.118.1">Error handling and debugging</span></strong><span class="koboSpan" id="kobo.119.1">: In Colab, we have direct access to the Python runtime and can typically see error messages and tracebacks in real time to help identify and resolve issues. </span><span class="koboSpan" id="kobo.119.2">We will want the same level of visibility in production via adequate logging of system errors. </span><span class="koboSpan" id="kobo.119.3">In total, we want to carry over the convenience and simplicity of our Google Colab prototyping environment but provide the robustness and scalability required for production. </span><span class="koboSpan" id="kobo.119.4">To do so, we will map each of the</span><a id="_idIndexMarker318"/><span class="koboSpan" id="kobo.120.1"> key characteristics we laid out to a corresponding production solution. </span><span class="koboSpan" id="kobo.120.2">These key features should ensure a smooth transition for deploying StyleSprint’s </span><a id="_idIndexMarker319"/><span class="koboSpan" id="kobo.121.1">generative model for automated product </span><span class="No-Break"><span class="koboSpan" id="kobo.122.1">descriptio</span><a id="_idTextAnchor128"/><span class="koboSpan" id="kobo.123.1">n generation.</span></span></li>
</ul>
<h1 id="_idParaDest-79"><a id="_idTextAnchor129"/><span class="koboSpan" id="kobo.124.1">Mapping features to production setup</span></h1>
<p><span class="koboSpan" id="kobo.125.1">To ensure we can </span><a id="_idIndexMarker320"/><span class="koboSpan" id="kobo.126.1">seamlessly transition our prototyping environment to production, we can leverage Docker, a leading containerization </span><a id="_idIndexMarker321"/><span class="koboSpan" id="kobo.127.1">tool. </span><strong class="bold"><span class="koboSpan" id="kobo.128.1">Containerization</span></strong><span class="koboSpan" id="kobo.129.1"> tools package applications with their dependencies for consistent performance across different systems. </span><span class="koboSpan" id="kobo.129.2">A containerized approach will help us replicate Google Colab’s isolated, uniform environments, ensuring reliability and reducing potential compatibility issues in production. </span><span class="koboSpan" id="kobo.129.3">The table that follows describes how we can map each of the benefits of our prototyping environment to a </span><span class="No-Break"><span class="koboSpan" id="kobo.130.1">production </span></span><span class="No-Break"><a id="_idIndexMarker322"/></span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">analog:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-1">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.132.1">Feature</span></strong></span></p>
</td>
<td class="No-Table-Style" colspan="2">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.133.1">Environment</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.134.1">Prototyping</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.135.1">Production</span></strong></span></p>
</td>
</tr>
</thead>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.136.1">Package management</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.137.1">Inherent through preinstalled </span><span class="No-Break"><span class="koboSpan" id="kobo.138.1">package managers</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.139.1">Docker streamlines application deployment and consistency across environments including </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">package managers.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.141.1">Dependency isolation</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.142.1">Inherent through </span><span class="No-Break"><span class="koboSpan" id="kobo.143.1">notebooks</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.144.1">Docker can also ensure projects are </span><span class="No-Break"><span class="koboSpan" id="kobo.145.1">cleanly isolated.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.146.1">Interactive </span><span class="No-Break"><span class="koboSpan" id="kobo.147.1">code execution</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.148.1">Inherent through </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">notebooks</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.150.1">Docker helps to maintain versions of Python that provide interactive code execution by default. </span><span class="koboSpan" id="kobo.150.2">However, we may want to connect an </span><strong class="bold"><span class="koboSpan" id="kobo.151.1">integrated development environment</span></strong><span class="koboSpan" id="kobo.152.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.153.1">IDE</span></strong><span class="koboSpan" id="kobo.154.1">) to our production environment to interact with code remotely </span><span class="No-Break"><span class="koboSpan" id="kobo.155.1">as needed.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.156.1">Resource accessibility</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.157.1">Inherent for cloud-based </span><span class="No-Break"><span class="koboSpan" id="kobo.158.1">notebooks</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.159.1">GPU-enabled Docker containers enhance production by enabling structured GPU utilization, allowing scalable, efficient </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">model performance.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.161.1">Data integration</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.162.1">Not inherent, and requires code-based </span><span class="No-Break"><span class="koboSpan" id="kobo.163.1">integration</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.164.1">Integrating Docker with a remote data source, such as AWS S3 or Google Cloud Storage, provides secure and scalable solutions for importing and </span><span class="No-Break"><span class="koboSpan" id="kobo.165.1">exporting data.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.166.1">Versioning </span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">and collaboration</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.168.1">Inherent through notebooks and preconfigured </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">for Git</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.170.1">Integrating Docker with platforms such as GitHub or GitLab enables code collaboration </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">and documentation.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.172.1">Error handling </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">and debugging</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.174.1">Inherent through direct interactive access </span><span class="No-Break"><span class="koboSpan" id="kobo.175.1">to runtime</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.176.1">We can embed Python libraries such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.177.1">logging</span></strong><span class="koboSpan" id="kobo.178.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.179.1">Loguru</span></strong><span class="koboSpan" id="kobo.180.1"> in Docker deployments for enhanced error tracking </span><span class="No-Break"><span class="koboSpan" id="kobo.181.1">in production.</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.182.1">Table 4.1: Transitioning features from Colab to production via Docker</span></p>
<p><span class="koboSpan" id="kobo.183.1">Having mapped out </span><a id="_idIndexMarker323"/><span class="koboSpan" id="kobo.184.1">the features of our prototyping environment to corresponding tools and practices for a production setup, we are now better prepared to implement a generative model for StyleSprint in a production-ready environment. </span><span class="koboSpan" id="kobo.184.2">The transition entails setting up a stable, scalable, and reproducible Python environment, a crucial step for deploying our generative model to automate the generation of product descriptions in a real-world setting. </span><span class="koboSpan" id="kobo.184.3">As discussed, we can leverage Docker in tandem with GitHub and its </span><strong class="bold"><span class="koboSpan" id="kobo.185.1">continuous integration/continuous deployment</span></strong><span class="koboSpan" id="kobo.186.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.187.1">CI/CD</span></strong><span class="koboSpan" id="kobo.188.1">) capabilities, providing a robust framework</span><a id="_idIndexMarker324"/><span class="koboSpan" id="kobo.189.1"> for this production deployment. </span><span class="koboSpan" id="kobo.189.2">A CI pipeline automates the integration of code changes from multiple contributors into a shared repository. </span><span class="koboSpan" id="kobo.189.3">We pair CI with CD to automate the deployment of our code to a </span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">production environment.</span></span><a id="_idTextAnchor130"/></p>
<h1 id="_idParaDest-80"><a id="_idTextAnchor131"/><span class="koboSpan" id="kobo.191.1">Setting up a production-ready environment</span></h1>
<p><span class="koboSpan" id="kobo.192.1">So far, we have </span><a id="_idIndexMarker325"/><span class="koboSpan" id="kobo.193.1">discussed how to bridge the gap between prototyping and production environments. </span><span class="koboSpan" id="kobo.193.2">Cloud-based environments such as Google Colab provide a wealth of features that are not inherently available in production. </span><span class="koboSpan" id="kobo.193.3">Now that we have a better understanding of those characteristics, the next step is to implement a robust production setup to ensure that our application can handle real-world traffic, scale as needed, and remain stable </span><span class="No-Break"><span class="koboSpan" id="kobo.194.1">over time.</span></span></p>
<p><span class="koboSpan" id="kobo.195.1">The tools and practices in a production environment differ significantly from those in a prototyping environment. </span><span class="koboSpan" id="kobo.195.2">In production, scalability, reliability, resource management, and security become paramount, whereas, in a prototyping environment, the models are only relied upon by a few users for experimentation. </span><span class="koboSpan" id="kobo.195.3">In production, we could expect large-scale consumption from divisions throughout the organization. </span><span class="koboSpan" id="kobo.195.4">For example, in the StyleSprint scenario, there may be multiple departments or sub-brands hoping to automate their </span><span class="No-Break"><span class="koboSpan" id="kobo.196.1">product descriptions.</span></span></p>
<p><span class="koboSpan" id="kobo.197.1">In the early stages of our StyleSprint project, we can use free and open source tools such as Docker and GitHub for tasks such as containerization, version control, and CI. </span><span class="koboSpan" id="kobo.197.2">These tools are offered and managed by a community of users, giving us a cost-effective solution. </span><span class="koboSpan" id="kobo.197.3">As StyleSprint expands, we might consider upgrading to paid or enterprise editions that offer advanced features and professional support. </span><span class="koboSpan" id="kobo.197.4">For the moment, our focus is on leveraging the capabilities of the open source versions. </span><span class="koboSpan" id="kobo.197.5">Next, we will walk through the practical implementation of these tools step by step. </span><span class="koboSpan" id="kobo.197.6">By the end, we will be ready to deploy a </span><a id="_idIndexMarker326"/><span class="koboSpan" id="kobo.198.1">production-ready </span><strong class="bold"><span class="koboSpan" id="kobo.199.1">model-as-a-service</span></strong><span class="koboSpan" id="kobo.200.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.201.1">MaaS</span></strong><span class="koboSpan" id="kobo.202.1">) for automatic </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">product descriptio</span><a id="_idTextAnchor132"/><span class="koboSpan" id="kobo.204.1">ns.</span></span></p>
<h1 id="_idParaDest-81"><a id="_idTextAnchor133"/><span class="koboSpan" id="kobo.205.1">Local development setup</span></h1>
<p><span class="koboSpan" id="kobo.206.1">We begin by making sure</span><a id="_idIndexMarker327"/><span class="koboSpan" id="kobo.207.1"> we can connect to a production environment remotely. </span><span class="koboSpan" id="kobo.207.2">We can leverage an IDE, which is software that enables us to easily organize code and remotely connect to the </span><span class="No-Break"><span class="koboSpan" id="kobo.208.1">production</span></span><span class="No-Break"><a id="_idIndexMarker328"/></span><span class="No-Break"><span class="koboSpan" id="kobo.209.1"> environ</span><a id="_idTextAnchor134"/><span class="koboSpan" id="kobo.210.1">ment.</span></span></p>
<h2 id="_idParaDest-82"><a id="_idTextAnchor135"/><span class="koboSpan" id="kobo.211.1">Visual Studio Code</span></h2>
<p><span class="koboSpan" id="kobo.212.1">Begin by</span><a id="_idIndexMarker329"/><span class="koboSpan" id="kobo.213.1"> installing </span><strong class="bold"><span class="koboSpan" id="kobo.214.1">Visual Studio Code</span></strong><span class="koboSpan" id="kobo.215.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.216.1">VS Code</span></strong><span class="koboSpan" id="kobo.217.1">), a free code editor by Microsoft. </span><span class="koboSpan" id="kobo.217.2">It is preferred </span><a id="_idIndexMarker330"/><span class="koboSpan" id="kobo.218.1">for its integrated Git control, terminal, and marketplace for extensions that enhance its functionality. </span><span class="koboSpan" id="kobo.218.2">It provides a conducive environment for writing, testing, and </span><span class="No-Break"><span class="koboSpan" id="kobo.219.1">debuggin</span><a id="_idTextAnchor136"/><span class="koboSpan" id="kobo.220.1">g code.</span></span></p>
<h2 id="_idParaDest-83"><a id="_idTextAnchor137"/><span class="koboSpan" id="kobo.221.1">Project initialization</span></h2>
<p><span class="koboSpan" id="kobo.222.1">Next, we set up a</span><a id="_idIndexMarker331"/><span class="koboSpan" id="kobo.223.1"> structured project directory to keep the code modular and organized. </span><span class="koboSpan" id="kobo.223.2">We will also initialize our working directory with Git, which enables us to synchronize code with a remote repository. </span><span class="koboSpan" id="kobo.223.3">As mentioned, we leverage Git to keep track of code changes and collaborate with others more seamlessly. </span><span class="koboSpan" id="kobo.223.4">Using the terminal window in Visual Studio, we can initialize the project using three simple commands. </span><span class="koboSpan" id="kobo.223.5">We use </span><strong class="source-inline"><span class="koboSpan" id="kobo.224.1">mkdir</span></strong><span class="koboSpan" id="kobo.225.1"> to create or “make” a directory. </span><span class="koboSpan" id="kobo.225.2">We use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.226.1">cd</span></strong><span class="koboSpan" id="kobo.227.1"> command to change directories. </span><span class="koboSpan" id="kobo.227.2">Finally, we use </span><strong class="source-inline"><span class="koboSpan" id="kobo.228.1">git init</span></strong><span class="koboSpan" id="kobo.229.1"> to initialize our project with Git. </span><span class="koboSpan" id="kobo.229.2">Keep in mind that this assumes Git is installed. </span><span class="koboSpan" id="kobo.229.3">Instructions to install Git are made available on its </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">website (</span></span><a href="https://git-scm.com/"><span class="No-Break"><span class="koboSpan" id="kobo.231.1">https://git-scm.com/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.232.1">).</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.233.1">
mkdir StyleSprint
cd StyleSprint
</span><a id="_idTextAnchor138"/><span class="koboSpan" id="kobo.234.1">git init</span></pre>
<h2 id="_idParaDest-84"><span class="koboSpan" id="kobo.235.1">Dock</span><a id="_idTextAnchor139"/><span class="koboSpan" id="kobo.236.1">er setup</span></h2>
<p><span class="koboSpan" id="kobo.237.1">We’ll now move on</span><a id="_idIndexMarker332"/><span class="koboSpan" id="kobo.238.1"> to setting </span><a id="_idIndexMarker333"/><span class="koboSpan" id="kobo.239.1">up a Docker container. </span><span class="koboSpan" id="kobo.239.2">A Docker container is an isolated environment that encapsulates an application and its dependencies, ensuring consistent operation across different systems. </span><span class="koboSpan" id="kobo.239.3">For clarity, we can briefly describe the key aspects of Docker </span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.241.1">Containers</span></strong><span class="koboSpan" id="kobo.242.1">: These are portable units comprising the application and </span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">its dependencies.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.244.1">Host operating system’s kernel</span></strong><span class="koboSpan" id="kobo.245.1">: When a Docker container is run on a host machine, it utilizes the kernel of the host’s operating system and resources to operate, but it does so in a way that is isolated from both the host system and </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">other containers.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.247.1">Dockerfiles</span></strong><span class="koboSpan" id="kobo.248.1">: These are scripts used to create container images. </span><span class="koboSpan" id="kobo.248.2">They serve as a blueprint containing everything needed to run the application. </span><span class="koboSpan" id="kobo.248.3">This isolation and packaging method prevents application conflicts and promotes efficient resource use, streamlining development </span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">and deployment.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.250.1">A containerized approach will help ensure consistency and portability. </span><span class="koboSpan" id="kobo.250.2">For example, assume StyleSprint finds a cloud-based hosting provider that is more cost-effective; moving to the new provider is as simple as migrating a few </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">configuration files.</span></span></p>
<p><span class="koboSpan" id="kobo.252.1">We can install Docker from the official website. </span><span class="koboSpan" id="kobo.252.2">Docker provides easy-to-follow installation guides including support for various </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">programming languages.</span></span></p>
<p><span class="koboSpan" id="kobo.254.1">Once Docker is installed, we can create a Dockerfile in the project directory to specify the environment setup. </span><span class="koboSpan" id="kobo.254.2">For GPU support, we will want to start from an NVIDIA CUDA base image. </span><span class="koboSpan" id="kobo.254.3">Docker, like many other virtualized systems, operates using a concept called </span><strong class="bold"><span class="koboSpan" id="kobo.255.1">images</span></strong><span class="koboSpan" id="kobo.256.1">. </span><span class="koboSpan" id="kobo.256.2">Images</span><a id="_idIndexMarker334"/><span class="koboSpan" id="kobo.257.1"> are a snapshot of a </span><a id="_idIndexMarker335"/><span class="koboSpan" id="kobo.258.1">preconfigured environment that can be used as a starting point for a new project. </span><span class="koboSpan" id="kobo.258.2">In our case, we will want to start with a snapshot that integrates GPU support using the CUDA library, which is a parallel processing library provided by NVIDIA. </span><span class="koboSpan" id="kobo.258.3">This library will enable the virtualized environment (or container) to leverage any GPUs installed on the host machine. </span><span class="koboSpan" id="kobo.258.4">Leveraging GPUs will accelerate </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">model inferencing.</span></span></p>
<p><span class="koboSpan" id="kobo.260.1">Now we can go ahead and create a Dockerfile with the specifications for </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">our</span><a id="_idTextAnchor140"/><span class="koboSpan" id="kobo.262.1"> application:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.263.1">
# Use an official NVIDIA CUDA runtime as a base image
FROM nvidia/cuda:11.0-base
# Set the working directory in the container to /app
WORKDIR /app
# Copy the current directory contents into the container at /app
COPY . </span><span class="koboSpan" id="kobo.263.2">/app
# Install any needed packages specified in requirements.txt
RUN pip install --no-cache-dir -r requirements.txt
# Make port 80 available to the world outside this container
EXPOSE 80
# Run app.py when the container launches
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "80"]</span></pre>
<p><span class="koboSpan" id="kobo.264.1">This Dockerfile serves as a blueprint that Docker follows to build our container. </span><span class="koboSpan" id="kobo.264.2">We initiate the process from an official NVIDIA CUDA base image to ensure GPU support. </span><span class="koboSpan" id="kobo.264.3">The working directory in the container is set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.265.1">/app</span></strong><span class="koboSpan" id="kobo.266.1">, where we then copy the contents of our project. </span><span class="koboSpan" id="kobo.266.2">Following that, we install the necessary packages listed in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.267.1">requirements.txt</span></strong><span class="koboSpan" id="kobo.268.1"> file. </span><strong class="source-inline"><span class="koboSpan" id="kobo.269.1">Port 80</span></strong><span class="koboSpan" id="kobo.270.1"> is exposed for external access to our application. </span><span class="koboSpan" id="kobo.270.2">Lastly, we specify the command to launch our application, which is running </span><strong class="source-inline"><span class="koboSpan" id="kobo.271.1">app.py</span></strong><span class="koboSpan" id="kobo.272.1"> using the Python</span><a id="_idIndexMarker336"/><span class="koboSpan" id="kobo.273.1"> interpreter. </span><span class="koboSpan" id="kobo.273.2">This </span><a id="_idIndexMarker337"/><span class="koboSpan" id="kobo.274.1">setup encapsulates all the necessary components, including GPU support, to ensure our generative model operates efficiently in a </span><span class="No-Break"><span class="koboSpan" id="kobo.275.1">production-lik</span><a id="_idTextAnchor141"/><span class="koboSpan" id="kobo.276.1">e environment.</span></span></p>
<h2 id="_idParaDest-85"><a id="_idTextAnchor142"/><span class="koboSpan" id="kobo.277.1">Requirements file</span></h2>
<p><span class="koboSpan" id="kobo.278.1">We also need a</span><a id="_idIndexMarker338"/><span class="koboSpan" id="kobo.279.1"> method for keeping track of our Python-specific dependencies. </span><span class="koboSpan" id="kobo.279.2">The container will include Python but will not have any indication as to what requirements our Python application has. </span><span class="koboSpan" id="kobo.279.3">We can specify those dependencies explicitly by defining a </span><strong class="source-inline"><span class="koboSpan" id="kobo.280.1">requirements.txt</span></strong><span class="koboSpan" id="kobo.281.1"> file in </span><a id="_idIndexMarker339"/><span class="koboSpan" id="kobo.282.1">our project directory to list all the necessary </span><span class="No-Break"><span class="koboSpan" id="kobo.283.1">Python packages:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.284.1">
fastapi==0.65.2
torch==1.9.0
transformers==4.9.2
uvicorn==0.14.0</span></pre>
<h2 id="_idParaDest-86"><a id="_idTextAnchor143"/><span class="koboSpan" id="kobo.285.1">Application code</span></h2>
<p><span class="koboSpan" id="kobo.286.1">Now we can</span><a id="_idIndexMarker340"/><span class="koboSpan" id="kobo.287.1"> create an </span><strong class="source-inline"><span class="koboSpan" id="kobo.288.1">app.py</span></strong><span class="koboSpan" id="kobo.289.1"> file for our </span><a id="_idIndexMarker341"/><span class="koboSpan" id="kobo.290.1">application code. </span><span class="koboSpan" id="kobo.290.2">This is where we will write the code for our generative model, leveraging libraries such as PyTorch and Transformers. </span><span class="koboSpan" id="kobo.290.3">To expose our model as a service, we will use </span><em class="italic"><span class="koboSpan" id="kobo.291.1">FastAPI</span></em><span class="koboSpan" id="kobo.292.1">, a modern, high-performance framework for building web APIs. </span><span class="koboSpan" id="kobo.292.2">A web API is a protocol that enables different software applications to communicate and exchange data over the internet, allowing them to use each other’s functions </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">and services.</span></span></p>
<p><span class="koboSpan" id="kobo.294.1">The following snippet creates a minimal API that will serve the model responses whenever another application or software requests the </span><strong class="source-inline"><span class="koboSpan" id="kobo.295.1">/generate/</span></strong><span class="koboSpan" id="kobo.296.1"> endpoint. </span><span class="koboSpan" id="kobo.296.2">This will enable StyleSprint to host its model as a web service. </span><span class="koboSpan" id="kobo.296.3">This means that other applications (e.g., mobile apps, batch processes) can access the model using a simple URL. </span><span class="koboSpan" id="kobo.296.4">We can also add exception handling to provide an informative error message should the model produce any kind </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">of error:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.298.1">
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import pipeline
# Load the pre-trained model
generator = pipeline('text-generation', 
    model='EleutherAI/gpt-neo-2.7B')
# Create the FastAPI app
app = FastAPI()
# Define the request body
class GenerationInput(BaseModel):
prompt: str
# Define the endpoint
@app.post("/generate")
def generate_text(input: GenerationInput):
try:
    # Generate text based on the input prompt
    generated_text = generator(input.prompt, max_length=150)
    return {"generated_text": generated_text}
except:
    raise HTTPException(status_code=500,
        detail="Model failed to generate text")</span></pre>
<p><span class="koboSpan" id="kobo.299.1">Now that we have a Docker setup, the next step is to deploy the application to the host server. </span><span class="koboSpan" id="kobo.299.2">We can streamline this process with a CI/CD pipeline. </span><span class="koboSpan" id="kobo.299.3">The goal is to fully automate all deployment steps, including a suite of tests to ensure that any code changes do not</span><a id="_idIndexMarker342"/><span class="koboSpan" id="kobo.300.1"> introduce </span><a id="_idIndexMarker343"/><span class="koboSpan" id="kobo.301.1">any errors. </span><span class="koboSpan" id="kobo.301.2">We then leverage GitHub Actions to create a workflow that is directly integrated with a </span><span class="No-Break"><span class="koboSpan" id="kobo.302.1">code repository.</span></span></p>
<h2 id="_idParaDest-87"><a id="_idTextAnchor144"/><span class="koboSpan" id="kobo.303.1">Creating a code repository</span></h2>
<p><span class="koboSpan" id="kobo.304.1">Before we can leverage</span><a id="_idIndexMarker344"/><span class="koboSpan" id="kobo.305.1"> the</span><a id="_idIndexMarker345"/><span class="koboSpan" id="kobo.306.1"> automation capabilities of GitHub, we will need a repository. </span><span class="koboSpan" id="kobo.306.2">Creating a GitHub repository is straightforward, following </span><span class="No-Break"><span class="koboSpan" id="kobo.307.1">these steps:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.308.1">Sign up/log in to GitHub</span></strong><span class="koboSpan" id="kobo.309.1">: If you don’t have a GitHub account, sign up at </span><a href="http://github.com"><span class="koboSpan" id="kobo.310.1">github.com</span></a><span class="koboSpan" id="kobo.311.1">. </span><span class="koboSpan" id="kobo.311.2">If you already have an account, just </span><span class="No-Break"><span class="koboSpan" id="kobo.312.1">log in.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.313.1">Go to the repository creation page</span></strong><span class="koboSpan" id="kobo.314.1">: Click the </span><strong class="bold"><span class="koboSpan" id="kobo.315.1">+</span></strong><span class="koboSpan" id="kobo.316.1"> icon in the top-right corner of the GitHub home page and select </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.317.1">New repository</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.319.1">Fill in the </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.320.1">repository details</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.322.1">Repository Name</span></strong><span class="koboSpan" id="kobo.323.1">: Choose a name for </span><span class="No-Break"><span class="koboSpan" id="kobo.324.1">your repository</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.325.1">Description</span></strong><span class="koboSpan" id="kobo.326.1"> (optional): Add a brief description of </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">your repository</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.328.1">Visibility</span></strong><span class="koboSpan" id="kobo.329.1">: Select either </span><strong class="bold"><span class="koboSpan" id="kobo.330.1">Public</span></strong><span class="koboSpan" id="kobo.331.1"> (anyone can see this repository) or </span><strong class="bold"><span class="koboSpan" id="kobo.332.1">Private</span></strong><span class="koboSpan" id="kobo.333.1"> (only you and the collaborators you invite can </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">see it)</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.335.1">Initialize the repository with a </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.336.1">README</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.337.1"> (optional):</span></span><ul><li><span class="koboSpan" id="kobo.338.1">Check </span><strong class="bold"><span class="koboSpan" id="kobo.339.1">Initialize this repository with a README</span></strong><span class="koboSpan" id="kobo.340.1"> if you want to add a simple text file that can be updated later to provide instructions </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">for collaborators.</span></span></li><li><span class="koboSpan" id="kobo.342.1">We can also add a </span><strong class="source-inline"><span class="koboSpan" id="kobo.343.1">.gitignore</span></strong><span class="koboSpan" id="kobo.344.1"> file or choose a license. </span><span class="koboSpan" id="kobo.344.2">A </span><strong class="source-inline"><span class="koboSpan" id="kobo.345.1">gitignore</span></strong><span class="koboSpan" id="kobo.346.1"> file allows us to add paths or file types that should not be uploaded to the repository. </span><span class="koboSpan" id="kobo.346.2">For example, Python creates temporary files that are not critical to the application. </span><span class="koboSpan" id="kobo.346.3">Adding </span><strong class="source-inline"><span class="koboSpan" id="kobo.347.1">`__pycache__/`</span></strong><span class="koboSpan" id="kobo.348.1"> to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.349.1">gitignore</span></strong><span class="koboSpan" id="kobo.350.1"> file will automatically ignore all contents of </span><span class="No-Break"><span class="koboSpan" id="kobo.351.1">that directory.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.352.1">Create repository</span></strong><span class="koboSpan" id="kobo.353.1">: Click the </span><strong class="bold"><span class="koboSpan" id="kobo.354.1">Create </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.355.1">repository</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.356.1"> button.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.357.1">With our </span><a id="_idIndexMarker346"/><span class="koboSpan" id="kobo.358.1">repository setup</span><a id="_idIndexMarker347"/><span class="koboSpan" id="kobo.359.1"> complete, we can move on to defining our CI/CD pipeline to a</span><a id="_idTextAnchor145"/><span class="koboSpan" id="kobo.360.1">utomate </span><span class="No-Break"><span class="koboSpan" id="kobo.361.1">our deployments.</span></span></p>
<h2 id="_idParaDest-88"><a id="_idTextAnchor146"/><span class="koboSpan" id="kobo.362.1">CI/CD setup</span></h2>
<p><span class="koboSpan" id="kobo.363.1">To create</span><a id="_idIndexMarker348"/><span class="koboSpan" id="kobo.364.1"> a pipeline, we </span><a id="_idIndexMarker349"/><span class="koboSpan" id="kobo.365.1">will need a configuration file that outlines the stages of deployment and instructs the automation server to build and deploy our Docker container. </span><span class="koboSpan" id="kobo.365.2">Let’s look at </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">the steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.367.1">In our GitHub repository, we can create a new file in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.368.1">.github/workflows</span></strong><span class="koboSpan" id="kobo.369.1"> directory named </span><strong class="source-inline"><span class="koboSpan" id="kobo.370.1">ci-cd.yml</span></strong><span class="koboSpan" id="kobo.371.1">. </span><span class="koboSpan" id="kobo.371.2">GitHub will automatically find any files in this directory to </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">trigger deployments.</span></span></li>
<li><span class="koboSpan" id="kobo.373.1">Open </span><strong class="source-inline"><span class="koboSpan" id="kobo.374.1">ci-cd.yml</span></strong><span class="koboSpan" id="kobo.375.1"> and</span><a id="_idIndexMarker350"/><span class="koboSpan" id="kobo.376.1"> define </span><a id="_idIndexMarker351"/><span class="koboSpan" id="kobo.377.1">the </span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">following workflow:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.379.1">
name: CI/CD Pipeline</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.380.1">
on:</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.381.1">
  push:</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.382.1">
    branches:</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.383.1">
      - main</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.384.1">
jobs:</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.385.1">
  build-and-test:</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.386.1">
    runs-on: ubuntu-latest</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.387.1">
  steps:</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.388.1">
    - name: Checkout code</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.389.1">
      uses: actions/checkout@v4</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.390.1">
    - name: Build Docker image</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.391.1">
  # assumes the Dockerfile is in the root (.)</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.392.1">
      run: docker build -t stylesprint .</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.393.1">
    - name: Run tests</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.394.1">
  # assumes a set of unit tests were defined</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.395.1">
      run: docker run stylesprint python -m unittest discover</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.396.1">
deploy:</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.397.1">
  needs: build-and-test</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.398.1">
  runs-on: ubuntu-latest</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.399.1">
  steps:</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.400.1">
    - name: Checkout code</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.401.1">
      uses: actions/checkout@v4</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.402.1">
    - name: Login to DockerHub</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.403.1">
      run: echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.404.1">
    - name: Push Docker image</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.405.1">
      run: |</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.406.1">
        docker tag stylesprint:latest ${{ secrets.DOCKER_USERNAME }}/stylesprint:latest</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.407.1">
        docker push ${{ secrets.DOCKER_USERNAME }}/stylesprint:latest</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.408.1">In this setup, our</span><a id="_idIndexMarker352"/><span class="koboSpan" id="kobo.409.1"> workflow consists </span><a id="_idIndexMarker353"/><span class="koboSpan" id="kobo.410.1">of two primary jobs: build-and-test and deploy. </span><span class="koboSpan" id="kobo.410.2">The build-and-test job is responsible for checking out the code from the repository, building the Docker image, and executing any tests. </span><span class="koboSpan" id="kobo.410.3">On the other hand, the deploy job, which relies on completing build-and-test, handles </span><em class="italic"><span class="koboSpan" id="kobo.411.1">DockerHub</span></em><span class="koboSpan" id="kobo.412.1"> login and pushes the Docker image there. </span><span class="koboSpan" id="kobo.412.2">DockerHub, similar to GitHub, is a repository specifically for </span><span class="No-Break"><span class="koboSpan" id="kobo.413.1">Docker images.</span></span></p>
<p><span class="koboSpan" id="kobo.414.1">For authenticating with DockerHub, it is advised to securely store your DockerHub credentials in your GitHub repository. </span><span class="koboSpan" id="kobo.414.2">This can be done by navigating to your repository on GitHub, clicking on </span><strong class="bold"><span class="koboSpan" id="kobo.415.1">Settings</span></strong><span class="koboSpan" id="kobo.416.1">, then </span><strong class="bold"><span class="koboSpan" id="kobo.417.1">Secrets</span></strong><span class="koboSpan" id="kobo.418.1">, and adding </span><strong class="source-inline"><span class="koboSpan" id="kobo.419.1">DOCKER_USERNAME</span></strong><span class="koboSpan" id="kobo.420.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.421.1">DOCKER_PASSWORD</span></strong><span class="koboSpan" id="kobo.422.1"> as new </span><span class="No-Break"><span class="koboSpan" id="kobo.423.1">repository secrets.</span></span></p>
<p><span class="koboSpan" id="kobo.424.1">Notice that we did not have to perform any additional steps to execute the pipeline. </span><span class="koboSpan" id="kobo.424.2">The workflow is designed to trigger automatically upon a push (or upload) to the main branch. </span><span class="koboSpan" id="kobo.424.3">Recall that the entire process relies on the Git pattern where new changes are registered through a </span><strong class="source-inline"><span class="koboSpan" id="kobo.425.1">commit</span></strong><span class="koboSpan" id="kobo.426.1"> or check-in of code and a </span><strong class="source-inline"><span class="koboSpan" id="kobo.427.1">push</span></strong><span class="koboSpan" id="kobo.428.1"> or upload of code changes. </span><span class="koboSpan" id="kobo.428.2">Whenever changes are pushed, we can directly observe the entire pipeline in action within the </span><strong class="bold"><span class="koboSpan" id="kobo.429.1">Actions</span></strong><span class="koboSpan" id="kobo.430.1"> tab of the </span><span class="No-Break"><span class="koboSpan" id="kobo.431.1">GitHub repository.</span></span></p>
<p><span class="koboSpan" id="kobo.432.1">We have now walked through all of the steps necessary to deploy our model to production. </span><span class="koboSpan" id="kobo.432.2">With all of this critical setup behind us, we can now return to choosing the best model for our project. </span><span class="koboSpan" id="kobo.432.3">The goal is to find a model that can effectively generate captivating product descriptions for StyleSprint. </span><span class="koboSpan" id="kobo.432.4">However, the variety of generative models available requires a thoughtful choice based on our project’s needs </span><span class="No-Break"><span class="koboSpan" id="kobo.433.1">and constraints.</span></span></p>
<p><span class="koboSpan" id="kobo.434.1">Moreover, we want to choose the right evaluation metrics and discuss other considerations</span><a id="_idIndexMarker354"/><span class="koboSpan" id="kobo.435.1"> that will guide us in </span><a id="_idIndexMarker355"/><span class="koboSpan" id="kobo.436.1">making an informed decision for our project. </span><span class="koboSpan" id="kobo.436.2">This exploration will equip us with the knowledge needed to select a model that not only performs well but also aligns with our project objectives and the technical inf</span><a id="_idTextAnchor147"/><span class="koboSpan" id="kobo.437.1">rastructure we </span><span class="No-Break"><span class="koboSpan" id="kobo.438.1">have established.</span></span></p>
<h1 id="_idParaDest-89"><a id="_idTextAnchor148"/><span class="koboSpan" id="kobo.439.1">Model selection – choosing the right pretrained generative model</span></h1>
<p><span class="koboSpan" id="kobo.440.1">Having established </span><a id="_idIndexMarker356"/><span class="koboSpan" id="kobo.441.1">a minimal production environment in the previous section, we now focus on a pivotal aspect of our project – selecting the right generative model for generating engaging product descriptions. </span><span class="koboSpan" id="kobo.441.2">The choice of model is crucial as it significantly influences the effectiveness and efficiency of our solution. </span><span class="koboSpan" id="kobo.441.3">The objective is to automate the generation of compelling and accurate product descriptions for StyleSprint’s diverse range of retail products. </span><span class="koboSpan" id="kobo.441.4">By doing so, we aim to enrich the online shopping experience for customers while alleviating the manual workload of crafting unique </span><span class="No-Break"><span class="koboSpan" id="kobo.442.1">product descriptions.</span></span></p>
<p><span class="koboSpan" id="kobo.443.1">Our objective is to select a generative model that can adeptly handle nuanced and sophisticated text generation to significantly expedite the process of creating unique, engaging product descriptions, saving time and resources </span><span class="No-Break"><span class="koboSpan" id="kobo.444.1">for StyleSprint.</span></span></p>
<p><span class="koboSpan" id="kobo.445.1">In selecting our model, it is important to thoroughly evaluate various factors influencing its performance</span><a id="_idTextAnchor149"/><span class="koboSpan" id="kobo.446.1"> and </span><a id="_idIndexMarker357"/><span class="koboSpan" id="kobo.447.1">suitability for </span><span class="No-Break"><span class="koboSpan" id="kobo.448.1">the project.</span></span></p>
<h2 id="_idParaDest-90"><a id="_idTextAnchor150"/><span class="koboSpan" id="kobo.449.1">Meeting project objectives</span></h2>
<p><span class="koboSpan" id="kobo.450.1">Before we can select</span><a id="_idIndexMarker358"/><span class="koboSpan" id="kobo.451.1"> and apply evaluation methods to our model selection process, we should first make sure we understand the project objectives. </span><span class="koboSpan" id="kobo.451.2">This involves defining the business problem, identifying any technical constraints, identifying any risk associated with the model, including interpretation of model outcomes, and ascertaining considerations for any potential disparate treatment </span><span class="No-Break"><span class="koboSpan" id="kobo.452.1">or bias:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.453.1">Problem definition</span></strong><span class="koboSpan" id="kobo.454.1">: In our scenario, the goal is to create accurate and engaging descriptions for a wide range of retail clothing. </span><span class="koboSpan" id="kobo.454.2">As StyleSprint’s product range may expand, the system should scale seamlessly to accommodate a larger inventory without significantly increasing operational costs. </span><span class="koboSpan" id="kobo.454.3">Performance expectations include compelling descriptions to attract potential customers, accuracy to avoid misrepresentation, and prompt generation to maintain an up-to-date online catalog. </span><span class="koboSpan" id="kobo.454.4">Additionally, StyleSprint may apply personalized content descriptions based on a user’s shopping history. </span><span class="koboSpan" id="kobo.454.5">This implies that the model may have to provide product descriptions </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">in near-real-time.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.456.1">Technical constraints</span></strong><span class="koboSpan" id="kobo.457.1">: To maximize efficiency, there should not be any noticeable delay (latency) in responses from the model API. </span><span class="koboSpan" id="kobo.457.2">The system should be capable of real-time updates to the online catalog (as needed), and the hardware should support quick text generation without compromising quality while remaining cost-effective, especially as the product </span><span class="No-Break"><span class="koboSpan" id="kobo.458.1">range expands.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.459.1">Transparency and openness</span></strong><span class="koboSpan" id="kobo.460.1">: Generally, pretrained models from developers who disclose architectures and training data sources are preferred, as this level of transparency allows StyleSprint to have a clear understanding of any risks or legal implications associated with model use. </span><span class="koboSpan" id="kobo.460.2">Additionally, any usage restrictions imposed by using models provided as APIs, such as request or token limitations, should be understood as they could hinder scalability for a </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">growing catalog.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.462.1">Bias and fairness</span></strong><span class="koboSpan" id="kobo.463.1">: Identifying and mitigating biases in model outputs to ensure fair and neutral representations is crucial, especially given StyleSprint’s diverse target audience. </span><span class="koboSpan" id="kobo.463.2">Ensuring that the generated descriptions are culturally sensitive is of paramount importance. </span><span class="koboSpan" id="kobo.463.3">Fair representation ensures that the descriptions accurately and fairly represent the products to all potential customers, irrespective of their individual characteristics or </span><span class="No-Break"><span class="koboSpan" id="kobo.464.1">social backgrounds.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.465.1">Suitability of pretraining</span></strong><span class="koboSpan" id="kobo.466.1">: The underlying pretraining of generative models plays a significant role in their ability to generate meaningful and relevant text. </span><span class="koboSpan" id="kobo.466.2">Investigating the domains and data on which the models were pretrained or fine-tuned is important. </span><span class="koboSpan" id="kobo.466.3">A model pretrained on a broad dataset may be versatile but could lack </span><a id="_idIndexMarker359"/><span class="koboSpan" id="kobo.467.1">domain-specific nuances. </span><span class="koboSpan" id="kobo.467.2">For StyleSprint, a model that is fine-tuned on fashion-related data or that has the ability to be fine-tuned on such data would be ideal to ensure the generated descriptions are relevant </span><span class="No-Break"><span class="koboSpan" id="kobo.468.1">and appealing.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.469.1">Quantitative metrics</span></strong><span class="koboSpan" id="kobo.470.1">: Evaluating the quality of generated product descriptions for StyleSprint necessitates a combination of lexical and semantic metrics. </span><span class="koboSpan" id="kobo.470.2">Lexical overlap metrics measure the lexical similarity between generated and reference texts. </span><span class="koboSpan" id="kobo.470.3">Specifically, </span><strong class="bold"><span class="koboSpan" id="kobo.471.1">Bilingual Evaluation Understudy</span></strong><span class="koboSpan" id="kobo.472.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.473.1">BLEU</span></strong><span class="koboSpan" id="kobo.474.1">) emphasizes </span><a id="_idIndexMarker360"/><span class="koboSpan" id="kobo.475.1">n-gram precision, </span><strong class="bold"><span class="koboSpan" id="kobo.476.1">Recall-Oriented Understudy for Gisting Evaluation</span></strong><span class="koboSpan" id="kobo.477.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.478.1">ROUGE</span></strong><span class="koboSpan" id="kobo.479.1">) focuses</span><a id="_idIndexMarker361"/><span class="koboSpan" id="kobo.480.1"> on n-gram </span><a id="_idIndexMarker362"/><span class="koboSpan" id="kobo.481.1">recall, and </span><strong class="bold"><span class="koboSpan" id="kobo.482.1">Metric for Evaluation of Translation with Explicit Ordering</span></strong><span class="koboSpan" id="kobo.483.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.484.1">METEOR</span></strong><span class="koboSpan" id="kobo.485.1">) aims for a more balanced evaluation by considering synonyms and stemming. </span><span class="koboSpan" id="kobo.485.2">For contextual and semantic evaluation, we use similarity metrics to assess the semantic coherence and relevance of the generated descriptions, often utilizing embeddings to represent text in a way that captures </span><span class="No-Break"><span class="koboSpan" id="kobo.486.1">its meaning.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.487.1">We can further refine our assessment of the alignment between generated descriptions and product images using models such as </span><strong class="bold"><span class="koboSpan" id="kobo.488.1">Contrastive Language-Image Pretraining</span></strong><span class="koboSpan" id="kobo.489.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.490.1">CLIP</span></strong><span class="koboSpan" id="kobo.491.1">). </span><span class="koboSpan" id="kobo.491.2">Recall that we used CLIP in </span><a href="B21773_02.xhtml#_idTextAnchor045"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.492.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.493.1"> to score the compatibility between</span><a id="_idIndexMarker363"/><span class="koboSpan" id="kobo.494.1"> captions and a synthesized image. </span><span class="koboSpan" id="kobo.494.2">In this case, we can apply CLIP to measure whether our generated descriptions accurately reflect the visual aspects of the products. </span><span class="koboSpan" id="kobo.494.3">Collectively, these evaluation techniques provide objective methods for assessing the</span><a id="_idIndexMarker364"/><span class="koboSpan" id="kobo.495.1"> performance of the generative model in creating effective product descriptions </span><span class="No-Break"><span class="koboSpan" id="kobo.496.1">for StyleSprint:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.497.1">Qualitative metrics</span></strong><span class="koboSpan" id="kobo.498.1">: We introduce qualitative evaluation to measure nuances such as the engaging and creative nature of descriptions. </span><span class="koboSpan" id="kobo.498.2">We also want to ensure we consider equity and inclusivity in the generated content, which is critical to avoid biases or language that could alienate or offend certain groups. </span><span class="koboSpan" id="kobo.498.3">Methods for engagement assessment could include customer surveys or A/B testing, a systematic method for testing two competing solutions. </span><span class="koboSpan" id="kobo.498.4">Additionally, having a diverse group reviewing the content for equity and inclusivity could provide valuable insights. </span><span class="koboSpan" id="kobo.498.5">These steps help StyleSprint create captivating, respectful, and inclusive product descriptions, fostering a welcoming environment for </span><span class="No-Break"><span class="koboSpan" id="kobo.499.1">all customers.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.500.1">Scalability</span></strong><span class="koboSpan" id="kobo.501.1">: The computational resources required to run a model and the model’s ability to scale with increasing data are vital considerations. </span><span class="koboSpan" id="kobo.501.2">Models that demand extensive computational power may not be practical for real-time generation of product descriptions, especially as the product range expands. </span><span class="koboSpan" id="kobo.501.3">A balance between computational efficiency and output quality is essential to ensure cost-effectiveness and scalability </span><span class="No-Break"><span class="koboSpan" id="kobo.502.1">for StyleSprint.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.503.1">Customization and fine-tuning capabilities</span></strong><span class="koboSpan" id="kobo.504.1">: The ability to fine-tune or customize the model on domain-specific data is crucial for better aligning with brand-specific requirements. </span><span class="koboSpan" id="kobo.504.2">Exploring the availability and ease of fine-tuning can significantly impact the relevance and quality of generated descriptions, ensuring that they resonate well with the brand identity and product range of StyleSprint. </span><span class="koboSpan" id="kobo.504.3">In practice, some models are too large to fine-tune without considerable resources, even when efficient methods are applied. </span><span class="koboSpan" id="kobo.504.4">We will explore fine-tuning considerations in detail in the </span><span class="No-Break"><span class="koboSpan" id="kobo.505.1">next chapter.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.506.1">Now that we have carefully considered how we might align the model to the project’s goals, we are almost ready to evaluate our initial model selection against a few others to ensure we make </span><a id="_idIndexMarker365"/><span class="koboSpan" id="kobo.507.1">the right choice. </span><span class="koboSpan" id="kobo.507.2">However, before benchmarking, we should dedicate time to understanding one vital aspect of the model selection process</span><a id="_idTextAnchor151"/><span class="koboSpan" id="kobo.508.1">: model size and </span><span class="No-Break"><span class="koboSpan" id="kobo.509.1">computational complexity.</span></span></p>
<h2 id="_idParaDest-91"><a id="_idTextAnchor152"/><span class="koboSpan" id="kobo.510.1">Model size and computational complexity</span></h2>
<p><span class="koboSpan" id="kobo.511.1">The size of a </span><a id="_idIndexMarker366"/><span class="koboSpan" id="kobo.512.1">generative model is often described by the number of parameters it has. </span><span class="koboSpan" id="kobo.512.2">Parameters in a model are the internal variables that are fine-tuned during the training process based on the training data. </span><span class="koboSpan" id="kobo.512.3">In the context of neural networks used in generative models, parameters typically refer to the weights and biases adjusted through training to minimize the discrepancy between predicted outputs and </span><span class="No-Break"><span class="koboSpan" id="kobo.513.1">actual targets.</span></span></p>
<p><span class="koboSpan" id="kobo.514.1">Moreover, a model with more parameters can capture more complex patterns in the data, often leading to better performance on the task at hand. </span><span class="koboSpan" id="kobo.514.2">While larger models often perform better in terms of the quality of the generated text, there’s a point of diminishing returns beyond which increasing model size yields marginal improvements. </span><span class="koboSpan" id="kobo.514.3">Moreover, the increased size comes with its own set </span><span class="No-Break"><span class="koboSpan" id="kobo.515.1">of challenges:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.516.1">Computational complexity</span></strong><span class="koboSpan" id="kobo.517.1">: Larger</span><a id="_idIndexMarker367"/><span class="koboSpan" id="kobo.518.1"> models require more computational power and memory, during both training and inference (the phase where the model is used to make predictions or generate new data based on the learned parameters). </span><span class="koboSpan" id="kobo.518.2">This can significantly increase the costs and the time required to train and use the model, making it less suitable for real-time applications or </span><span class="No-Break"><span class="koboSpan" id="kobo.519.1">resource-constrained environments.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.520.1">The number of parameters significantly impacts the computational complexity of a model. </span><span class="koboSpan" id="kobo.520.2">Each parameter in a model is a variable that must be stored in memory during computation, during both training and inference. </span><span class="koboSpan" id="kobo.520.3">Here are some specific considerations for </span><span class="No-Break"><span class="koboSpan" id="kobo.521.1">computational requirements:</span></span></p><ul><li><strong class="bold"><span class="koboSpan" id="kobo.522.1">Memory and storage</span></strong><span class="koboSpan" id="kobo.523.1">: The total size of the model in memory is the product of the number of parameters and the size of each parameter (typically a 32-bit or 64-bit float). </span><span class="koboSpan" id="kobo.523.2">For instance, a model with 100 million parameters, each represented by a 32-bit float, would require approximately 400 MB of memory (100 million * 32 bits = 400 million bits = 400 MB). </span><span class="koboSpan" id="kobo.523.3">Now consider a larger model, say with 10 billion parameters; the memory requirement jumps to 40 GB (10 billion * 32 bits = 40 billion bits = 40 GB). </span><span class="koboSpan" id="kobo.523.4">This requirement is just for the parameters and does not account for other data and overheads the model needs for </span><span class="No-Break"><span class="koboSpan" id="kobo.524.1">its operations.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.525.1">Loading into memory</span></strong><span class="koboSpan" id="kobo.526.1">: When a model is used for inference, its parameters must be loaded into the RAM of the machine it’s running on. </span><span class="koboSpan" id="kobo.526.2">For a large model with 10 billion parameters, you would need a machine with enough RAM to accommodate the entire model, along with additional memory for the operational overhead, the input data, and the generated output. </span><span class="koboSpan" id="kobo.526.3">Suppose the model is too large to fit in memory. </span><span class="koboSpan" id="kobo.526.4">In that case, it may need to be </span><strong class="bold"><span class="koboSpan" id="kobo.527.1">sharded</span></strong><span class="koboSpan" id="kobo.528.1"> or distributed across multiple machines or loaded in parts, which can</span><a id="_idIndexMarker368"/><span class="koboSpan" id="kobo.529.1"> significantly complicate the deployment and operation of the model and also increase the latency of </span><span class="No-Break"><span class="koboSpan" id="kobo.530.1">generating outputs.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.531.1">Specialized hardware requirements</span></strong><span class="koboSpan" id="kobo.532.1">: Larger models require specialized hardware, such as powerful GPUs or TPUs, which could increase the project costs. </span><span class="koboSpan" id="kobo.532.2">As</span><a id="_idIndexMarker369"/><span class="koboSpan" id="kobo.533.1"> discussed, models with a large number of parameters require powerful computational resources for both training and inference. </span><span class="koboSpan" id="kobo.533.2">Hardware accelerators such as GPUs and TPUs are often employed to meet these demands. </span><span class="koboSpan" id="kobo.533.3">These hardware accelerators are designed to handle the parallel computation capabilities needed for the matrix multiplications and other operations inherent in neural network computations, speeding up the processing significantly compared to </span><a id="_idIndexMarker370"/><span class="koboSpan" id="kobo.534.1">traditional </span><strong class="bold"><span class="koboSpan" id="kobo.535.1">central processing </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.536.1">units</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.537.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.538.1">CPUs</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.539.1">).</span></span><p class="list-inset"><span class="koboSpan" id="kobo.540.1">Cloud-based infrastructure can alleviate the complexity of setup but often has usage-based pricing. </span><span class="koboSpan" id="kobo.540.2">Understanding infrastructure costs on a granular level is vital to ensuring that StyleSprint stays within </span><span class="No-Break"><span class="koboSpan" id="kobo.541.1">its budget.</span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.542.1">Latency</span></strong><span class="koboSpan" id="kobo.543.1">: We’ve briefly discussed latency, but it is important to reiterate that larger models typically have higher latency, which could be a problem for applications that require real-time responses. </span><span class="koboSpan" id="kobo.543.2">In our case, we can process the descriptions as </span><a id="_idIndexMarker371"/><span class="koboSpan" id="kobo.544.1">batches asynchronously. </span><span class="koboSpan" id="kobo.544.2">However, StyleSprint may have projects that require fast turnarounds, requiring batches to be completed in hours and </span><span class="No-Break"><span class="koboSpan" id="kobo.545.1">not days.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.546.1">In the case of StyleSprint, the trade-off between model performance and size must be carefully evaluated to ensure the final model meets the project’s performance requirements while staying within budget and hardware constraints. </span><span class="koboSpan" id="kobo.546.2">StyleSprint was hoping to have near-real-time responses to provide personalized descriptions, which typically translates to a smaller model with less computational complexity. </span><span class="koboSpan" id="kobo.546.3">However, it was also important that the model remains highly accurate and aligns with branding standards for tone </span><a id="_idIndexMarker372"/><span class="koboSpan" id="kobo.547.1">and voice, which may require a larger model trained or fine-tuned on a larger dataset. </span><span class="koboSpan" id="kobo.547.2">In practice, we can evaluate the performance of models rela</span><a id="_idTextAnchor153"/><span class="koboSpan" id="kobo.548.1">tive to size and complexity </span><span class="No-Break"><span class="koboSpan" id="kobo.549.1">through benchmarking.</span></span></p>
<h2 id="_idParaDest-92"><a id="_idTextAnchor154"/><span class="koboSpan" id="kobo.550.1">Benchmarking</span></h2>
<p><span class="koboSpan" id="kobo.551.1">Benchmarking is a </span><a id="_idIndexMarker373"/><span class="koboSpan" id="kobo.552.1">systematic process used to </span><a id="_idIndexMarker374"/><span class="koboSpan" id="kobo.553.1">evaluate the performance of different generative models against predefined criteria. </span><span class="koboSpan" id="kobo.553.2">This process involves comparing the models on various metrics to understand their strengths, weaknesses, and suitability for the project. </span><span class="koboSpan" id="kobo.553.3">It is an empirical method (based on observation) to obtain data on how the models perform under similar conditions, providing insights that can inform the decision-making process for </span><span class="No-Break"><span class="koboSpan" id="kobo.554.1">model selection.</span></span></p>
<p><span class="koboSpan" id="kobo.555.1">In the StyleSprint scenario, benchmarking can be an invaluable exercise to navigate the trade-offs between model size, computational complexity, and the accuracy and creativity of </span><span class="No-Break"><span class="koboSpan" id="kobo.556.1">generated descriptions.</span></span></p>
<p><span class="koboSpan" id="kobo.557.1">For our benchmarking exercise, we can return to our Google Colab prototyping environment to quickly load various generative models and run them through tests designed to evaluate their performance based on the considerations outlined in the previous sections, such as computational efficiency and text generation quality. </span><span class="koboSpan" id="kobo.557.2">Once we have completed our evaluation and comparison, we can make a few simple changes to our production application code and it will automatically redeploy. </span><span class="koboSpan" id="kobo.557.3">Benchmarking will be instrumental in measuring the quality of the descriptions relative to the model size and complexity. </span><span class="koboSpan" id="kobo.557.4">Recall that we will measure quality and overall model performance along several dimensions, including lexical and semantic similarity to a “gold standard” of human-written descriptions, and a qualitative assessment performed by a diverse group </span><span class="No-Break"><span class="koboSpan" id="kobo.558.1">of reviewers.</span></span></p>
<p><span class="koboSpan" id="kobo.559.1">The next step</span><a id="_idIndexMarker375"/><span class="koboSpan" id="kobo.560.1"> is</span><a id="_idIndexMarker376"/><span class="koboSpan" id="kobo.561.1"> to revisit and adapt our original prototyping code to include</span><a id="_idTextAnchor155"/><span class="koboSpan" id="kobo.562.1"> a few challenger models and apply </span><span class="No-Break"><span class="koboSpan" id="kobo.563.1">evaluation metrics.</span></span></p>
<h1 id="_idParaDest-93"><a id="_idTextAnchor156"/><span class="koboSpan" id="kobo.564.1">Updating the prototyping environment</span></h1>
<p><span class="koboSpan" id="kobo.565.1">For our evaluation</span><a id="_idIndexMarker377"/><span class="koboSpan" id="kobo.566.1"> steps, there are a few key changes to our original experimentation setup in Google Colab. </span><span class="koboSpan" id="kobo.566.2">First, we will want to make sure we leverage performance acceleration. </span><span class="koboSpan" id="kobo.566.3">Google Colab offers acceleration via GPU or TPU environments. </span><span class="koboSpan" id="kobo.566.4">For this experiment, we will leverage GPU. </span><span class="koboSpan" id="kobo.566.5">We will also want to transition from the Transformers library to a slightly more versatile library such as Langchain, which allows us to test both open source models such as GPT-Neo and commercial models such </span><span class="No-Break"><span class="koboSpan" id="kobo.567.1">as GPT-3.5.</span></span></p>
<h2 id="_idParaDest-94"><a id="_idTextAnchor157"/><span class="koboSpan" id="kobo.568.1">GPU configuration</span></h2>
<p><span class="koboSpan" id="kobo.569.1">Ensure you have </span><a id="_idIndexMarker378"/><span class="koboSpan" id="kobo.570.1">a GPU enabled for better</span><a id="_idIndexMarker379"/><span class="koboSpan" id="kobo.571.1"> performance. </span><span class="koboSpan" id="kobo.571.2">Returning to Google Colab, we can follow these steps to enable </span><span class="No-Break"><span class="koboSpan" id="kobo.572.1">GPU acceleration:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.573.1">Click on </span><strong class="bold"><span class="koboSpan" id="kobo.574.1">Runtime</span></strong><span class="koboSpan" id="kobo.575.1"> in the top menu (see </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.576.1">Figure 4</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.577.1">.2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.578.1">):</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer025">
<span class="koboSpan" id="kobo.579.1"><img alt="Figure 4.2: Runtime drop-down menu" src="image/B21773_04_011.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.580.1">Figure 4.2: Runtime drop-down menu</span></p>
<ol>
<li value="2"><span class="koboSpan" id="kobo.581.1">Select </span><strong class="bold"><span class="koboSpan" id="kobo.582.1">Change runtime type</span></strong><span class="koboSpan" id="kobo.583.1"> from the drop-down menu, as shown </span><a id="_idIndexMarker380"/><span class="koboSpan" id="kobo.584.1">in the </span><span class="No-Break"><span class="koboSpan" id="kobo.585.1">preceding </span></span><span class="No-Break"><a id="_idIndexMarker381"/></span><span class="No-Break"><span class="koboSpan" id="kobo.586.1">screenshot.</span></span></li>
<li><span class="koboSpan" id="kobo.587.1">In the pop-up window, select </span><strong class="bold"><span class="koboSpan" id="kobo.588.1">GPU</span></strong><span class="koboSpan" id="kobo.589.1"> from the </span><strong class="bold"><span class="koboSpan" id="kobo.590.1">Hardware accelerator</span></strong><span class="koboSpan" id="kobo.591.1"> drop-down menu (see </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.592.1">Figure 4</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.593.1">.3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.594.1">):</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer026">
<span class="koboSpan" id="kobo.595.1"><img alt="Figure 4.3: Select GPU and click on Save" src="image/B21773_04_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.596.1">Figure 4.3: Select GPU and click on Save</span></p>
<ol>
<li value="4"><span class="koboSpan" id="kobo.597.1">Click </span><span class="No-Break"><span class="koboSpan" id="kobo.598.1">on </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.599.1">Save</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.600.1">.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.601.1">Now your notebook is set up to use a GPU to significantly speed up the computations needed for the benchmarking process. </span><span class="koboSpan" id="kobo.601.2">You can verify the GPU availability using the following </span><span class="No-Break"><span class="koboSpan" id="kobo.602.1">code snippet:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.603.1">
# Verify GPU is available
import torch
torch.cuda.is_available()</span></pre>
<p><span class="koboSpan" id="kobo.604.1">This code snippet will return </span><strong class="source-inline"><span class="koboSpan" id="kobo.605.1">True</span></strong><span class="koboSpan" id="kobo.606.1"> if a GPU is available and </span><strong class="source-inline"><span class="koboSpan" id="kobo.607.1">False</span></strong><span class="koboSpan" id="kobo.608.1"> otherwise. </span><span class="koboSpan" id="kobo.608.2">This setup ensures that you have the necessary computational resources to benchmark various</span><a id="_idIndexMarker382"/><span class="koboSpan" id="kobo.609.1"> generative</span><a id="_idIndexMarker383"/><span class="koboSpan" id="kobo.610.1"> models. </span><span class="koboSpan" id="kobo.610.2">The utilization of a GPU will be crucial when it </span><a id="_idTextAnchor158"/><span class="koboSpan" id="kobo.611.1">comes to handling large models and </span><span class="No-Break"><span class="koboSpan" id="kobo.612.1">extensive computations.</span></span></p>
<h2 id="_idParaDest-95"><a id="_idTextAnchor159"/><span class="koboSpan" id="kobo.613.1">Loading pretrained models with LangChain</span></h2>
<p><span class="koboSpan" id="kobo.614.1">In our first </span><a id="_idIndexMarker384"/><span class="koboSpan" id="kobo.615.1">simple</span><a id="_idIndexMarker385"/><span class="koboSpan" id="kobo.616.1"> experiment, we relied on the Transformers library to load an open source version of GPT. </span><span class="koboSpan" id="kobo.616.2">However, for our benchmarking exercise, we want to evaluate the retail version of GPT-3 alongside open source models. </span><span class="koboSpan" id="kobo.616.3">We can leverage LangChain, a versatile library that provides a streamlined interface, to access both open source models from providers such as Hugging Face and closed source models such as OpenAI’s GPT-3.5. </span><span class="koboSpan" id="kobo.616.4">LangChain offers a unified API that simplifies benchmarking and comparison through standardization. </span><span class="koboSpan" id="kobo.616.5">Here </span><a id="_idIndexMarker386"/><span class="koboSpan" id="kobo.617.1">are the steps</span><a id="_idIndexMarker387"/><span class="koboSpan" id="kobo.618.1"> to </span><span class="No-Break"><span class="koboSpan" id="kobo.619.1">do it:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.620.1">Install necessary libraries</span></strong><span class="koboSpan" id="kobo.621.1">: We begin by installing the required libraries in our Colab environment. </span><span class="koboSpan" id="kobo.621.2">LangChain simplifies the interaction with models hosted on OpenAI and </span><span class="No-Break"><span class="koboSpan" id="kobo.622.1">Hugging Face.</span></span><pre class="source-code"><span class="koboSpan" id="kobo.623.1">
!pip -q install openai langchain huggingface_hub</span></pre></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.624.1">Set up credentials</span></strong><span class="koboSpan" id="kobo.625.1">: We obtain the credentials from OpenAI for accessing GPT-3, GPT-4, or whichever closed source model we select. </span><span class="koboSpan" id="kobo.625.2">We also provide credentials for the Hugging Face Hub, which hosts over 350,000 open source models. </span><span class="koboSpan" id="kobo.625.3">We must store these credentials securely to prevent any unauthorized access, especially in the case where model usage has an </span><span class="No-Break"><span class="koboSpan" id="kobo.626.1">associated cost.</span></span><pre class="source-code"><span class="koboSpan" id="kobo.627.1">
import os</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.628.1">
os.environ['OPENAI_API_KEY'] = 'your_openai_api_key_here'</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.629.1">
os.environ['HUGGINGFACEHUB_API_TOKEN'] = </span></pre><pre class="source-code"><span class="koboSpan" id="kobo.630.1">
    'your_huggingface_token_here'</span></pre></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.631.1">Load models</span></strong><span class="koboSpan" id="kobo.632.1">: With LangChain, we can quickly load models and generate responses. </span><span class="koboSpan" id="kobo.632.2">The following example demonstrates how to load GPT-3 and GPT-Neo from </span><span class="No-Break"><span class="koboSpan" id="kobo.633.1">Hugging Face:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.634.1">
!pip install openai langchain[llms] huggingface_hub</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.635.1">
from langchain.llms import OpenAI, HuggingFaceHub</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.636.1">
# Loading GPT-3</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.637.1">
llm_gpt3 = OpenAI(model_name='text-davinci-003',</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.638.1">
                  temperature=0.9,</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.639.1">
                  max_tokens = 256)</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.640.1">
# Loading Neo from Hugging Face</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.641.1">
llm_neo = HuggingFaceHub(repo_id=' EleutherAI/gpt-neo-2.7B',</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.642.1">
                         model_kwargs={"temperature":0.9}</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.643.1">
)</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.644.1">Notice that we have loaded two models that are significantly different in size. </span><span class="koboSpan" id="kobo.644.2">As the model signature suggests, GPT-Neo was trained on 2.7 billion parameters. </span><span class="koboSpan" id="kobo.644.3">Meanwhile, according to information available from OpenAI, Davinci was trained on 175 billion parameters. </span><span class="koboSpan" id="kobo.644.4">As discussed, a model that is significantly larger is expected to have captured much more complex patterns and will likely outperform a smaller model. </span><span class="koboSpan" id="kobo.644.5">However, these very large models are typically hosted by major providers and have higher usage costs. </span><span class="koboSpan" id="kobo.644.6">We will revisit cost considerations later. </span><span class="koboSpan" id="kobo.644.7">For now, we can continue to the next step, which </span><a id="_idIndexMarker388"/><span class="koboSpan" id="kobo.645.1">is to prepare </span><a id="_idIndexMarker389"/><span class="koboSpan" id="kobo.646.1">our testing data. </span><span class="koboSpan" id="kobo.646.2">Our test data should provide a baseline for model per</span><a id="_idTextAnchor160"/><span class="koboSpan" id="kobo.647.1">formance that will inform the cost versus </span><span class="No-Break"><span class="koboSpan" id="kobo.648.1">performance trade-off.</span></span></p>
<h2 id="_idParaDest-96"><a id="_idTextAnchor161"/><span class="koboSpan" id="kobo.649.1">Setting up testing data</span></h2>
<p><span class="koboSpan" id="kobo.650.1">In this context, testing </span><a id="_idIndexMarker390"/><span class="koboSpan" id="kobo.651.1">data should comprise </span><a id="_idIndexMarker391"/><span class="koboSpan" id="kobo.652.1">product attributes from the StyleSprint website (e.g., available colors, sizes, materials, etc.) and existing product descriptions written by the StyleSprint team. </span><span class="koboSpan" id="kobo.652.2">The human-written descriptions serve as the “ground truth,” or the standard against which to compare the models’ </span><span class="No-Break"><span class="koboSpan" id="kobo.653.1">generated descriptions.</span></span></p>
<p><span class="koboSpan" id="kobo.654.1">We can gather product data from existing datasets by scraping data from e-commerce websites or using a pre-collected dataset from StyleSprint’s database. </span><span class="koboSpan" id="kobo.654.2">We should also ensure a varied collection of products to test a model’s capability across different categories and styles. </span><span class="koboSpan" id="kobo.654.3">The process of dividing data into distinct groups or segments based on shared characteristics is typically referred to as segmentation. </span><span class="koboSpan" id="kobo.654.4">Understanding a model’s behavior across segments should give us an indication of how well it can perform across the entire family of products. </span><span class="koboSpan" id="kobo.654.5">For the purposes of this example, product data is made available in the GitHub companion to this </span><span class="No-Break"><span class="koboSpan" id="kobo.655.1">book (</span></span><a href="https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python"><span class="No-Break"><span class="koboSpan" id="kobo.656.1">https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.657.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.658.1">Let’s see how</span><a id="_idIndexMarker392"/><span class="koboSpan" id="kobo.659.1"> we can extract relevant information </span><a id="_idIndexMarker393"/><span class="koboSpan" id="kobo.660.1">for </span><span class="No-Break"><span class="koboSpan" id="kobo.661.1">further processing:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.662.1">
import pandas as pd
# Assume `product_data.csv` is a CSV file with product data
# The CSV file has two columns: 'product_image' and 
# 'product_description' 
# Load the product data
product_data = pd.read_csv('product_data.csv')
# Split the data into testing and reference sets
test_data = product_data.sample(frac=0.2, random_state=42)
reference_data = product_data.drop(test_data.index)
# Checkpoint the testing and reference data
test_data.to_csv('test_data.csv', index=False)
reference_data.to_csv('reference_data.csv', index=False)
# Extract reference descriptions and image file paths
reference_descriptions = /
    reference_data['product_description'].tolist()
product_images = reference_data['product_image'].tolist()</span></pre>
<p><span class="koboSpan" id="kobo.663.1">We must also format the product data in a way that makes it ready to be input into the models for description generation. </span><span class="koboSpan" id="kobo.663.2">This could be just the product title or a combination of </span><span class="No-Break"><span class="koboSpan" id="kobo.664.1">product attributes:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.665.1">
# Assume `product_metadata` is a column in the data that contains the collective information about the product including the title of the product and attributes.
</span><span class="koboSpan" id="kobo.665.2"># Format the input data for the models
model_input_data = reference_data['product_metadata].tolist()
reference_descriptions = \
    reference_data['product_description'].tolist()</span></pre>
<p><span class="koboSpan" id="kobo.666.1">Finally, we will ask</span><a id="_idIndexMarker394"/><span class="koboSpan" id="kobo.667.1"> the model to generate a batch </span><a id="_idIndexMarker395"/><span class="koboSpan" id="kobo.668.1">of product descriptions using </span><span class="No-Break"><span class="koboSpan" id="kobo.669.1">each model.</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.670.1">
from langchain import LLMChain, PromptTemplate
from tqdm.auto import tqdm
template = """
Write a creative product description for the following product: {product_metadata}
"""
PROMPT = PromptTemplate(template=template, 
    input_variables=["product_metadata"])
def generate_descriptions(
    llm: object, 
    prompt: PromptTemplate = PROMPT
) -&gt; list:
    # Initialize the LLM chain
    llm_chain = LLMChain(prompt=prompt, llm=llm)
    descriptions = []
    for i in tqdm(range(len(model_input_data))):
        description = llm_chain.run(model_input_data[i])
        descriptions.append(description)
    return descriptions
gpt3_descriptions = generate_descriptions(llm_gpt3)
gptneo_descriptions = generate_descriptions(llm_neo)</span></pre>
<p><span class="koboSpan" id="kobo.671.1">Now, with the testing data set up, we have a structured dataset of product information, ref</span><a id="_idTextAnchor162"/><span class="koboSpan" id="kobo.672.1">erence</span><a id="_idIndexMarker396"/><span class="koboSpan" id="kobo.673.1"> descriptions, and images ready </span><a id="_idIndexMarker397"/><span class="koboSpan" id="kobo.674.1">for use in the </span><span class="No-Break"><span class="koboSpan" id="kobo.675.1">evaluation steps.</span></span></p>
<h1 id="_idParaDest-97"><a id="_idTextAnchor163"/><span class="koboSpan" id="kobo.676.1">Quantitative metrics evaluation</span></h1>
<p><span class="koboSpan" id="kobo.677.1">Now that we have</span><a id="_idIndexMarker398"/><span class="koboSpan" id="kobo.678.1"> leveraged Langchain to load multiple models and prepared testing data, we are ready to begin applying evaluation metrics. </span><span class="koboSpan" id="kobo.678.2">These metrics capture accuracy and alignment with product images and will help us assess how well the models generate product descriptions compared to humans. </span><span class="koboSpan" id="kobo.678.3">As discussed, we focused on two categories of metrics, lexical and semantic similarity, which provide a measure of how many of the same words were used and how much semantic information is common to both the human and AI-generated </span><span class="No-Break"><span class="koboSpan" id="kobo.679.1">product descriptions.</span></span></p>
<p><span class="koboSpan" id="kobo.680.1">In the following code block, we apply </span><strong class="source-inline"><span class="koboSpan" id="kobo.681.1">BLEU</span></strong><span class="koboSpan" id="kobo.682.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.683.1">ROUGE</span></strong><span class="koboSpan" id="kobo.684.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.685.1">METEOR</span></strong><span class="koboSpan" id="kobo.686.1"> to evaluate the lexical similarity between the generated text and the reference text. </span><span class="koboSpan" id="kobo.686.2">Each of these has a reference-based assumption. </span><span class="koboSpan" id="kobo.686.3">This means that each metric assumes we are comparing against a human reference. </span><span class="koboSpan" id="kobo.686.4">We have already set aside our reference descriptions (or gold standard) for a diverse set of products to compare side-by-side with the </span><span class="No-Break"><span class="koboSpan" id="kobo.687.1">generated descriptions.</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.688.1">
!pip install rouge sumeval nltk
# nltk requires an additional package
import nltk
nltk.download('wordnet')
 from nltk.translate.bleu_score import sentence_bleu
from rouge import Rouge
from sumeval.metrics.rouge import RougeCalculator
from nltk.translate.meteor_score import meteor_score
def evaluate(
    reference_descriptions: list, 
    generated_descriptions: list
) -&gt; tuple:
    # Calculating BLEU score
    bleu_scores = [
        sentence_bleu([ref], gen) 
        for ref, gen in zip(reference_descriptions, generated_descriptions)
    ]
    average_bleu = sum(bleu_scores) / len(bleu_scores)
    # Calculating ROUGE score
    rouge = RougeCalculator()
    rouge_scores = [rouge.rouge_n(gen, ref, 2) for ref,
        gen in zip(reference_descriptions,
        generated_descriptions)]
    average_rouge = sum(rouge_scores) / len(rouge_scores)
    # Calculating METEOR score
    meteor_scores = [ meteor_score([ref.split() ],
        gen.split()) for ref,
        gen in zip(reference_descriptions,
        generated_descriptions)]
    average_meteor = sum(meteor_scores) / len(meteor_scores)
    return average_bleu, average_rouge, average_meteor
average_bleu_gpt3, average_rouge_gpt3, average_meteor_gpt3 = \
    evaluate(reference_descriptions, gpt3_descriptions)
print(average_bleu_gpt3, average_rouge_gpt3, average_meteor_gpt3)
average_bleu_neo, average_rouge_neo, average_meteor_neo = \
    evaluate(reference_descriptions, gptneo_descriptions)
print(average_bleu_neo, average_rouge_neo, average_meteor_neo)</span></pre>
<p><span class="koboSpan" id="kobo.689.1">We can evaluate </span><a id="_idIndexMarker399"/><span class="koboSpan" id="kobo.690.1">the semantic coherence and relevance of the generated descriptions using </span><span class="No-Break"><span class="koboSpan" id="kobo.691.1">sentence embeddings:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.692.1">
!pip install sentence-transformers
from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')
def cosine_similarity(reference_descriptions, generated_descriptions):
    # Calculating cosine similarity for generated descriptions
    cosine_scores = [util.pytorch_cos_sim(
        model.encode(ref), model.encode(gen)) for ref,
        gen in zip(reference_descriptions,
        generated_descriptions)]
    average_cosine = sum(cosine_scores) / len(cosine_scores)
    return average_cosine
average_cosine_gpt3 = cosine_similarity(
    reference_descriptions, gpt3_descriptions)
print(average_cosine_gpt3)
average_cosine_neo = cosine_similarity(
    reference_descriptions, gptneo_descriptions)
print(average_cosine_neo)</span></pre>
<h2 id="_idParaDest-98"><a id="_idTextAnchor164"/><span class="koboSpan" id="kobo.693.1">Alignment with CLIP</span></h2>
<p><span class="koboSpan" id="kobo.694.1">We again leverage</span><a id="_idIndexMarker400"/><span class="koboSpan" id="kobo.695.1"> the CLIP model to</span><a id="_idIndexMarker401"/><span class="koboSpan" id="kobo.696.1"> evaluate the alignment between generated product descriptions and corresponding images, similar to our approach in </span><a href="B21773_02.xhtml#_idTextAnchor045"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.697.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.698.1">. </span><span class="koboSpan" id="kobo.698.2">The CLIP model, adept at correlating visual and textual content, scores the congruence between each product image and its associated generated and reference descriptions. </span><span class="koboSpan" id="kobo.698.3">The reference description serves as a human baseline for accuracy. </span><span class="koboSpan" id="kobo.698.4">These scores provide a quantitative measure of our generative model’s effectiveness at producing descriptions that correspond well to the product image. </span><span class="koboSpan" id="kobo.698.5">The following is a snippet from a component that processes the generated descriptions combined with corresponding images to generate a CLIP score. </span><span class="koboSpan" id="kobo.698.6">The full component code (including image pre-processing) is available in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.699.1">chapter 4</span></strong><span class="koboSpan" id="kobo.700.1"> folder </span><a id="_idIndexMarker402"/><span class="koboSpan" id="kobo.701.1">of this</span><a id="_idIndexMarker403"/><span class="koboSpan" id="kobo.702.1"> book’s GitHub repository </span><span class="No-Break"><span class="koboSpan" id="kobo.703.1">at </span></span><a href="https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python"><span class="No-Break"><span class="koboSpan" id="kobo.704.1">https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.705.1">)</span></span><span class="No-Break"><span class="koboSpan" id="kobo.706.1">.</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.707.1">
clip_model = "openai/clip-vit-base-patch32"
def clip_scores(images, descriptions,
                model=clip_model,
                processor=clip_processor
):
    scores = []
    # Process all images and descriptions together
    inputs = process_inputs(processor, descriptions, images)
    # Get model outputs
    outputs = model(**inputs)
    logits_per_image = outputs.logits_per_image # Image-to-text logits
    # Diagonal of the matrix gives the scores for each image-description pair
    for i in range(logits_per_image.size(0)):
        score = logits_per_image[i, i].item()
    scores.append(score)
    return scores
reference_images = [
    load_image_from_path(image_path) 
    for image_path in reference_data.product_image_path
]
gpt3_generated_scores = clip_scores(
    reference_images, gpt3_descriptions
)
reference_scores = clip_scores(
    reference_images, reference_descriptions
)
# Compare the scores
for i, (gen_score, ref_score) in enumerate(
    zip(gpt3_generated_scores, reference_scores)
):
    print(f"Image {i}: Generated Score = {gen_score:.2f}, 
        Reference Score = {ref_score:.2f}")</span></pre>
<p><span class="koboSpan" id="kobo.708.1">In evaluating product descriptions using the CLIP model, the alignment scores generated for each image-description pair are computed relative to other descriptions in the batch. </span><span class="koboSpan" id="kobo.708.2">Essentially, CLIP assesses how well a specific description (either generated or reference) aligns with a given image compared to other descriptions within the same batch. </span><span class="koboSpan" id="kobo.708.3">For example, a score of 33.79 indicates that the description aligns with the image 33.79% better than the other descriptions in the batch align with that image. </span><span class="koboSpan" id="kobo.708.4">In comparing against the reference, we expect that the scores based on the generated descriptions should align closely with the scores based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.709.1">reference descriptions.</span></span></p>
<p><span class="koboSpan" id="kobo.710.1">Now that we have </span><a id="_idIndexMarker404"/><span class="koboSpan" id="kobo.711.1">calculated lexical and semantic similarity to the reference scores, and alignment </span><a id="_idIndexMarker405"/><span class="koboSpan" id="kobo.712.1">between images and generated descriptions relative to reference descriptions, we can evaluate our models holistically</span><a id="_idTextAnchor165"/><span class="koboSpan" id="kobo.713.1"> and interpret the outcome of our </span><span class="No-Break"><span class="koboSpan" id="kobo.714.1">quantitative evaluation.</span></span></p>
<h2 id="_idParaDest-99"><a id="_idTextAnchor166"/><span class="koboSpan" id="kobo.715.1">Interpreting outcomes</span></h2>
<p><span class="koboSpan" id="kobo.716.1">We begin with</span><a id="_idIndexMarker406"/><span class="koboSpan" id="kobo.717.1"> lexical similarity, which gives us an indication of similarity in phrasing and keywords between the reference and </span><span class="No-Break"><span class="koboSpan" id="kobo.718.1">generated descriptions:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table002">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.719.1">BLEU</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.720.1">ROUGE</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.721.1">METEOR</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.722.1">GPT-3.5</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.723.1">0.147</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.724.1">0.094</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.725.1">0.261</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.726.1">GPT-Neo</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.727.1">0.132</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.728.1">0.05</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.729.1">0.059</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.730.1">Table 4.2: Lexical similarity</span></p>
<p><span class="koboSpan" id="kobo.731.1">In evaluating text generated by GPT-3.5 and GPT-Neo models, we use several lexical similarity metrics: BLEU, ROUGE, and METEOR. </span><span class="koboSpan" id="kobo.731.2">BLEU scores, which assess the precision of matching phrases, show GPT-3.5 (0.147) slightly outperforming GPT-Neo (0.132). </span><span class="koboSpan" id="kobo.731.3">ROUGE scores, focusing on the recall of content, indicate that GPT-3.5 (0.094) better captures reference content than GPT-Neo (0.05). </span><span class="koboSpan" id="kobo.731.4">METEOR scores, combining both precision and recall with synonym matching, reveal a significant lead for GPT-3.5 (0.261) over GPT-Neo (0.059). </span><span class="koboSpan" id="kobo.731.5">Overall, these metrics suggest that GPT-3.5’s generated text aligns more closely with reference standards, both in word choice and content coverage, compared to that </span><span class="No-Break"><span class="koboSpan" id="kobo.732.1">of GPT-Neo.</span></span></p>
<p><span class="koboSpan" id="kobo.733.1">Next, we evaluate semantic similarity, which measures how closely the meanings of the generated text align with the reference text. </span><span class="koboSpan" id="kobo.733.2">This assessment goes beyond mere word-to-word matching and considers the context and overall intent of the sentences. </span><span class="koboSpan" id="kobo.733.3">Semantic similarity</span><a id="_idIndexMarker407"/><span class="koboSpan" id="kobo.734.1"> evaluates the extent to which the generated text captures the nuances, concepts, and themes present in the reference text, providing insight into the model’s ability to understand and replicate deeper </span><span class="No-Break"><span class="koboSpan" id="kobo.735.1">semantic meanings:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table003">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.736.1">Model</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.737.1">Mean </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.738.1">cosine similarity</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.739.1">GPT-3.5</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.740.1">0.8192</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.741.1">GPT-Neo</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.742.1">0.2289</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.743.1">Table 4.3: Semantic similarity</span></p>
<p><span class="koboSpan" id="kobo.744.1">The mean cosine similarity scores reveal a stark contrast between the two models’ performance in semantic similarity. </span><span class="koboSpan" id="kobo.744.2">GPT-3.5 shows a high degree of semantic alignment with the reference text. </span><span class="koboSpan" id="kobo.744.3">GPT-Neo’s significantly lower score suggests a relatively poor performance, indicating that the generated descriptions were fundamentally dissimilar to descriptions written </span><span class="No-Break"><span class="koboSpan" id="kobo.745.1">by humans.</span></span></p>
<p><span class="koboSpan" id="kobo.746.1">Finally, we review the CLIP scores, which tell us how well the generated descriptions align visually with the corresponding images. </span><span class="koboSpan" id="kobo.746.2">These scores, derived from a model trained to understand and correlate visual and textual data, provide a measure of the relevance and accuracy of the text in representing the visual content. </span><span class="koboSpan" id="kobo.746.3">High CLIP scores indicate a strong correlation between the text and the image, suggesting that the generated descriptions are not only textually coherent but also contextually appropriate and </span><span class="No-Break"><span class="koboSpan" id="kobo.747.1">visually descriptive:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table004">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.748.1">Model</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.749.1">Mean CLIP</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.750.1">Reference delta</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.751.1">GPT-3.5</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.752.1">26.195</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.753.1">2.815</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.754.1">GPT-Neo</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.755.1">22.647</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.756.1">6.363</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.757.1">Table 4.4: Comparative CLIP score analysis for GPT-3.5 and GPT-Neo models</span></p>
<p><span class="koboSpan" id="kobo.758.1">We calculated the CLIP scores from the reference descriptions, which represent the average alignment </span><a id="_idIndexMarker408"/><span class="koboSpan" id="kobo.759.1">score between a set of benchmark descriptions and the corresponding images. </span><span class="koboSpan" id="kobo.759.2">We then calculated CLIP scores for each model and analyzed the delta. </span><span class="koboSpan" id="kobo.759.3">In concert with our other metrics, GPT-3.5 has a clear advantage over GPT-Neo, aligning more closely with </span><span class="No-Break"><span class="koboSpan" id="kobo.760.1">the reference.</span></span></p>
<p><span class="koboSpan" id="kobo.761.1">Overall, GPT-3.5 appears to significantly outperform GPT-Neo across all quantitative measures. </span><span class="koboSpan" id="kobo.761.2">However, it is worth noting that GPT-3.5 incurs a higher cost and generally has a higher latency than GPT-Neo. </span><span class="koboSpan" id="kobo.761.3">In this case, the StyleSprint team would conduct a qualitative analysis to accurately determine whether the GPT-Neo descriptions do not align with brand guidelines and expectations, therefore making the cost of using the better model worthwhile. </span><span class="koboSpan" id="kobo.761.4">As discussed, the trade-off here is not clear-cut. </span><span class="koboSpan" id="kobo.761.5">StyleSprint must carefully consider that although using a commodity such as GPT-3.5 does not incur computational costs directly, on-demand costs could increase significantly as model </span><span class="No-Break"><span class="koboSpan" id="kobo.762.1">usage rises.</span></span></p>
<p><span class="koboSpan" id="kobo.763.1">The contrasting strengths of the two models pose a decision-making challenge. </span><span class="koboSpan" id="kobo.763.2">While one clearly excels in performance metrics and alignment with CLIP, implying higher accuracy and semantic correctness, the other is significantly more resource-efficient and scalable, which is crucial for cost-effectiveness. </span><span class="koboSpan" id="kobo.763.3">At this stage, it becomes critical to assess model outcomes qualitatively and to engage stakeholders to help understand </span><span class="No-Break"><span class="koboSpan" id="kobo.764.1">organizational priorities.</span></span></p>
<p><span class="koboSpan" id="kobo.765.1">With these considerations in mind, we’ll revisit qualitative considerations such as transparency, bias, and </span><a id="_idIndexMarker409"/><span class="koboSpan" id="kobo.766.1">fairness and how they play into the broader picture of deploying a responsible and effective </span><span class="No-Break"><span class="koboSpan" id="kobo.767.1">AI system</span><a id="_idTextAnchor167"/><span class="koboSpan" id="kobo.768.1">.</span></span></p>
<h1 id="_idParaDest-100"><a id="_idTextAnchor168"/><span class="koboSpan" id="kobo.769.1">Responsible AI considerations</span></h1>
<p><span class="koboSpan" id="kobo.770.1">Addressing implicit or</span><a id="_idIndexMarker410"/><span class="koboSpan" id="kobo.771.1"> covert societal biases in AI systems is crucial to ensure responsible AI deployment. </span><span class="koboSpan" id="kobo.771.2">Although it may not seem obvious how a simple product description could introduce bias, the language used can inadvertently reinforce stereotypes or exclude certain groups. </span><span class="koboSpan" id="kobo.771.3">For instance, descriptions that consistently associate certain body types or skin tones with certain products or that unnecessarily default to gendered language can unintentionally perpetuate societal biases. </span><span class="koboSpan" id="kobo.771.4">However, with a structured mitigation approach, including algorithmic audits, increased model transparency, and stakeholder engagement, StyleSprint can make sure its brand promotes equity </span><span class="No-Break"><span class="koboSpan" id="kobo.772.1">and inclusion.</span></span></p>
<h2 id="_idParaDest-101"><a id="_idTextAnchor169"/><span class="koboSpan" id="kobo.773.1">Addressing and mitigating biases</span></h2>
<p><span class="koboSpan" id="kobo.774.1">We present </span><a id="_idIndexMarker411"/><span class="koboSpan" id="kobo.775.1">several considerations, as suggested by Costanza-Chock et al. </span><span class="koboSpan" id="kobo.775.2">in </span><em class="italic"><span class="koboSpan" id="kobo.776.1">Who Audits the Auditors? </span><span class="koboSpan" id="kobo.776.2">Recommendations from a field scan of the algorithmic </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.777.1">auditing ecosystem</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.778.1">:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.779.1">Professional</span></strong><strong class="bold"><span class="koboSpan" id="kobo.780.1"> environment examination</span></strong><span class="koboSpan" id="kobo.781.1">: Creating a supportive professional environment is crucial for addressing algorithmic fairness. </span><span class="koboSpan" id="kobo.781.2">Implementing whistleblower protections facilitates the safe reporting of biases and unfair practices while establishing processes for individuals to report harms to ensure these concerns are </span><span class="No-Break"><span class="koboSpan" id="kobo.782.1">addressed proactively.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.783.1">Custom versus standardized audit frameworks</span></strong><span class="koboSpan" id="kobo.784.1">: While custom audit frameworks are expected, considering standardized methods may enhance rigor and transparency in bias mitigation efforts. </span><span class="koboSpan" id="kobo.784.2">Engaging with external auditing entities could offer unbiased evaluations of StyleSprint’s AI systems, aligning with the observations by Costanza-Chock et </span><span class="No-Break"><span class="koboSpan" id="kobo.785.1">al. </span><span class="koboSpan" id="kobo.785.2">(2022).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.786.1">Focusing on equity, not just equality</span></strong><span class="koboSpan" id="kobo.787.1">: Equity notions acknowledge differing needs, essential for a comprehensive approach to fairness. </span><span class="koboSpan" id="kobo.787.2">Performing intersectional and small population analyses could help you to understand and address biases beyond legally </span><span class="No-Break"><span class="koboSpan" id="kobo.788.1">protected classes.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.789.1">Disclosure and transparency</span></strong><span class="koboSpan" id="kobo.790.1">: Disclosing audit methods and outcomes can foster a culture of transparency and continuous improvement. </span><span class="koboSpan" id="kobo.790.2">Officially released audits could help you establish best practices and gain </span><span class="No-Break"><span class="koboSpan" id="kobo.791.1">stakeholder trust.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.792.1">Mixed methods analyses</span></strong><span class="koboSpan" id="kobo.793.1">: As presented, a mix of technical and qualitative analyses could provide a holistic view of the system’s fairness. </span><span class="koboSpan" id="kobo.793.2">Engaging non-technical stakeholders could emphasize </span><span class="No-Break"><span class="koboSpan" id="kobo.794.1">qualitative analyses.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.795.1">Community and stakeholder engagement</span></strong><span class="koboSpan" id="kobo.796.1">: Again, involving diverse groups and domain experts in audits could ensure diverse perspectives are considered in bias mitigation efforts. </span><span class="koboSpan" id="kobo.796.2">Establishing feedback loops with stakeholders could facilitate </span><span class="No-Break"><span class="koboSpan" id="kobo.797.1">continuous improvement.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.798.1">Continuous learning and improvement</span></strong><span class="koboSpan" id="kobo.799.1">: Staying updated on emerging standards and best practices regarding AI fairness is crucial for continuous improvement. </span><span class="koboSpan" id="kobo.799.2">Fostering a culture of learning could help in adapting to evolving fairness challenges</span><a id="_idIndexMarker412"/><span class="koboSpan" id="kobo.800.1"> and regulatory landscapes, thus ensuring StyleSprint’s AI systems remain fair and responsible </span><span class="No-Break"><span class="koboSpan" id="kobo.801.1">over t</span><a id="_idTextAnchor170"/><span class="koboSpan" id="kobo.802.1">ime.</span></span></li>
</ul>
<h2 id="_idParaDest-102"><a id="_idTextAnchor171"/><span class="koboSpan" id="kobo.803.1">Transparency and explainability</span></h2>
<p><span class="koboSpan" id="kobo.804.1">Generally, explainability in </span><a id="_idIndexMarker413"/><span class="koboSpan" id="kobo.805.1">machine learning refers to the ability to understand the internal mechanics of a model, elucidating how it makes decisions or predictions based on given inputs. </span><span class="koboSpan" id="kobo.805.2">However, achieving explainability in generative models can be much more complex. </span><span class="koboSpan" id="kobo.805.3">As discussed, unlike discriminative machine learning models, generative models do not have the objective of learning a decision boundary, nor do they reflect a clear notion of features or a direct mapping between input features and predictions. </span><span class="koboSpan" id="kobo.805.4">This absence of feature-based decision-making makes traditional explainability techniques ineffective for generative foundational models such </span><span class="No-Break"><span class="koboSpan" id="kobo.806.1">as GPT-4.</span></span></p>
<p><span class="koboSpan" id="kobo.807.1">Alternatively, we can adopt some pragmatic transparency practices, such as clear documentation made accessible to all relevant stakeholders, to foster a shared understanding and expectations regarding the model’s capabilities </span><span class="No-Break"><span class="koboSpan" id="kobo.808.1">and usage.</span></span></p>
<p><span class="koboSpan" id="kobo.809.1">The topic of explainability is a critical space to watch, especially as generative models become more complex and their outcomes become increasingly more difficult to rationalize, which may present unknown </span><span class="No-Break"><span class="koboSpan" id="kobo.810.1">risk implicati</span><a id="_idTextAnchor172"/><span class="koboSpan" id="kobo.811.1">ons.</span></span></p>
<p><span class="koboSpan" id="kobo.812.1">Promising research from Anthropic, OpenAI, and others suggests that sparse autoencoders—neural networks that activate only a few neurons at a time—could facilitate the identification of abstract and understandable patterns. </span><span class="koboSpan" id="kobo.812.2">This method could help explain the network's behavior by highlighting features that align with </span><span class="No-Break"><span class="koboSpan" id="kobo.813.1">human concepts.</span></span></p>
<h1 id="_idParaDest-103"><a id="_idTextAnchor173"/><span class="koboSpan" id="kobo.814.1">Final deployment</span></h1>
<p><span class="koboSpan" id="kobo.815.1">Assuming we have</span><a id="_idIndexMarker414"/><span class="koboSpan" id="kobo.816.1"> carefully gathered quantitative and qualitative feedback regarding the best model for the job, we can select our model and update our production environment to deploy and serve it. </span><span class="koboSpan" id="kobo.816.2">We will continue to use FastAPI for creating a web server to serve our model, and Docker to containerize our application. </span><span class="koboSpan" id="kobo.816.3">However, now that we have been introduced to the simplicity of LangChain, we will continue to leverage its simplified interface. </span><span class="koboSpan" id="kobo.816.4">Our existing CI/CD pipeline will ensure streamlined automatic deployment and continuous application monitoring. </span><span class="koboSpan" id="kobo.816.5">This means that deploying our model is as simple as checking-in our latest code. </span><span class="koboSpan" id="kobo.816.6">We begin with updating our </span><span class="No-Break"><span class="koboSpan" id="kobo.817.1">dependencies list:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.818.1">Update the requirements</span></strong><span class="koboSpan" id="kobo.819.1">: Update the </span><strong class="source-inline"><span class="koboSpan" id="kobo.820.1">requirements.txt</span></strong><span class="koboSpan" id="kobo.821.1"> file in your project to include the </span><span class="No-Break"><span class="koboSpan" id="kobo.822.1">necessary libraries:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.823.1">
fastapi==0.68.0</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.824.1">
uvicorn==0.15.0</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.825.1">
openai==0.27.0</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.826.1">
langchain==0.1.0</span></pre></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.827.1">Update the Dockerfile</span></strong><span class="koboSpan" id="kobo.828.1">: Modify your Dockerfile to ensure it installs the updated requirements </span><a id="_idIndexMarker415"/><span class="koboSpan" id="kobo.829.1">and properly sets up the environment for running LangChain </span><span class="No-Break"><span class="koboSpan" id="kobo.830.1">with FastAPI:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.831.1">
# Use an official Python runtime as a base image</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.832.1">
FROM python:3.8-slim-buster</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.833.1">
# Set the working directory in the container to /app</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.834.1">
WORKDIR /app</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.835.1">
# Copy the current directory contents into the container at /app</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.836.1">
COPY . </span><span class="koboSpan" id="kobo.836.2">/app</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.837.1">
# Install any needed packages specified in requirements.txt</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.838.1">
RUN pip install --no-cache-dir -r requirements.txt</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.839.1">
# Make port 80 available to the world outside this container</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.840.1">
EXPOSE 80</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.841.1">
# Define environment variable</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.842.1">
ENV NAME World</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.843.1">
# Run app.py when the container launches</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.844.1">
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "80"]</span></pre></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.845.1">Update the FastAPI application</span></strong><span class="koboSpan" id="kobo.846.1">: Modify your FastAPI application to utilize Langchain for interacting with GPT-3.5. </span><span class="koboSpan" id="kobo.846.2">Ensure your OpenAI API key is securely stored and </span><a id="_idIndexMarker416"/><span class="koboSpan" id="kobo.847.1">accessible to </span><span class="No-Break"><span class="koboSpan" id="kobo.848.1">your application:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.849.1">
from fastapi import FastAPI, HTTPException, Request</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.850.1">
from langchain.llms import OpenAI</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.851.1">
import os</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.852.1">
# Initialize FastAPI app</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.853.1">
app = FastAPI()</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.854.1">
# Setup Langchain with GPT-3.5</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.855.1">
llm = OpenAI(model_name='text-davinci-003',</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.856.1">
             temperature=0.7,</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.857.1">
             max_tokens=256,</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.858.1">
             api_key=os.environ['OPENAI_API_KEY'])</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.859.1">
@app.post("/generate/")</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.860.1">
async def generate_text(request: Request):</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.861.1">
    data = await request.json()</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.862.1">
    prompt = data.get('prompt')</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.863.1">
    if not prompt:</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.864.1">
        raise HTTPException(status_code=400,</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.865.1">
            detail="Prompt is required")</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.866.1">
    response = llm(prompt)</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.867.1">
    return {"generated_text": response}</span></pre></li>
</ol>
<h2 id="_idParaDest-104"><a id="_idTextAnchor174"/><span class="koboSpan" id="kobo.868.1">Testing and monitoring</span></h2>
<p><span class="koboSpan" id="kobo.869.1">Once the model is deployed, perform</span><a id="_idIndexMarker417"/><span class="koboSpan" id="kobo.870.1"> necessary tests to ensure the setup works as expected. </span><span class="koboSpan" id="kobo.870.2">Continue to monitor the system’s performance, errors, and other critical metrics to ensure </span><span class="No-Break"><span class="koboSpan" id="kobo.871.1">reliable operation.</span></span></p>
<p><span class="koboSpan" id="kobo.872.1">By this point, we have updated our production environment to deploy and serve GPT-3.5, facilitating the generation of text based on the prompts received via the FastAPI application. </span><span class="koboSpan" id="kobo.872.2">This setup ensures a scalable, maintainable, and secure deployment of our new generative model. </span><span class="koboSpan" id="kobo.872.3">However, we should also explore some best practices regarding </span><span class="No-Break"><span class="koboSpan" id="kobo.873.1">application rel</span><a id="_idTextAnchor175"/><span class="koboSpan" id="kobo.874.1">iability.</span></span></p>
<h2 id="_idParaDest-105"><a id="_idTextAnchor176"/><span class="koboSpan" id="kobo.875.1">Maintenance and reliability</span></h2>
<p><span class="koboSpan" id="kobo.876.1">Maintaining reliability</span><a id="_idIndexMarker418"/><span class="koboSpan" id="kobo.877.1"> in our StyleSprint deployment is critical. </span><span class="koboSpan" id="kobo.877.2">As we employ Langchain with FastAPI, Docker, and CI/CD, it’s essential to set up monitoring, alerting, automatic remediation, and failover mechanisms. </span><span class="koboSpan" id="kobo.877.3">This section outlines a possible approach to ensure continuous operation and robustness in our </span><span class="No-Break"><span class="koboSpan" id="kobo.878.1">production environment:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.879.1">Monitoring tools</span></strong><span class="koboSpan" id="kobo.880.1">: Integrate monitoring tools within the CI/CD pipeline to continuously track system performance and model metrics. </span><span class="koboSpan" id="kobo.880.2">This step is fundamental for identifying and rectifying </span><span class="No-Break"><span class="koboSpan" id="kobo.881.1">issues proactively.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.882.1">Alerting mechanisms</span></strong><span class="koboSpan" id="kobo.883.1">: Establish alerting mechanisms to notify the maintenance team whenever anomalies or issues are detected. </span><span class="koboSpan" id="kobo.883.2">Tuning the alerting thresholds accurately is crucial to catch issues early and minimize </span><span class="No-Break"><span class="koboSpan" id="kobo.884.1">false alarms.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.885.1">Automatic remediation</span></strong><span class="koboSpan" id="kobo.886.1">: Utilize Kubernetes’ self-healing features and custom scripts triggered by certain alerts for automatic remediation. </span><span class="koboSpan" id="kobo.886.2">This setup aims to resolve common issues autonomously, reducing the need for </span><span class="No-Break"><span class="koboSpan" id="kobo.887.1">human intervention.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.888.1">Failover mechanisms</span></strong><span class="koboSpan" id="kobo.889.1">: Implement a failover mechanism by setting up secondary servers and databases. </span><span class="koboSpan" id="kobo.889.2">In case of primary server failure, these secondary setups take over to ensure continuous </span><span class="No-Break"><span class="koboSpan" id="kobo.890.1">service availability.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.891.1">Regular updates via CI/CD</span></strong><span class="koboSpan" id="kobo.892.1">: Employ the CI/CD pipeline for managing, testing, and deploying updates to LangChain, FastAPI, or other components of the stack. </span><span class="koboSpan" id="kobo.892.2">This process keeps the deployment updated and secure, reducing the maintenance </span><span class="No-Break"><span class="koboSpan" id="kobo.893.1">burden significantly.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.894.1">By meticulously </span><a id="_idIndexMarker419"/><span class="koboSpan" id="kobo.895.1">addressing each of these areas, you’ll be laying down a solid foundation for a reliable and maintainable </span><span class="No-Break"><span class="koboSpan" id="kobo.896.1">StyleSprint </span><a id="_idTextAnchor177"/><span class="koboSpan" id="kobo.897.1">deployment.</span></span></p>
<h1 id="_idParaDest-106"><a id="_idTextAnchor178"/><span class="koboSpan" id="kobo.898.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.899.1">This chapter outlined the process of transitioning the StyleSprint generative AI prototype to a production-ready deployment for creating engaging product descriptions on an e-commerce platform. </span><span class="koboSpan" id="kobo.899.2">It started with setting up a robust Python environment using Docker, GitHub, and CI/CD pipelines for efficient dependency management, testing, and deployment. </span><span class="koboSpan" id="kobo.899.3">The focus then shifted to selecting a suitable pretrained model, emphasizing alignment with project goals, computational considerations, and responsible AI practices. </span><span class="koboSpan" id="kobo.899.4">This selection relied on both quantitative benchmarking and qualitative evaluation. </span><span class="koboSpan" id="kobo.899.5">We then outlined the deployment of the selected model using FastAPI and LangChain, ensuring a scalable and reliable </span><span class="No-Break"><span class="koboSpan" id="kobo.900.1">production environment.</span></span></p>
<p><span class="koboSpan" id="kobo.901.1">Following the strategies outlined in this chapter will equip teams with the necessary insights and steps to successfully transition their generative AI prototype into a maintainable and value-adding production system. </span><span class="koboSpan" id="kobo.901.2">In the next chapter, we will explore fine-tuning and its importance in LLMs. </span><span class="koboSpan" id="kobo.901.3">We will also weigh in on the decision-making process, addressing when it is more beneficial to fine-tune versus zero or </span><span class="No-Break"><span class="koboSpan" id="kobo.902.1">few-shot prompting.</span></span></p>
</div>


<div class="Content" id="_idContainer028">
<h1 id="_idParaDest-107" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor179"/><span class="koboSpan" id="kobo.1.1">Part 2: Practical Applications of Generative AI</span></h1>
<p><span class="koboSpan" id="kobo.2.1">This part focuses on the practical applications of generative AI, including fine-tuning models for specific tasks, understanding domain adaptation, mastering prompt engineering, and addressing ethical considerations. </span><span class="koboSpan" id="kobo.2.2">It aims to provide hands-on insights and methodologies for effectively implementing and leveraging generative AI in various contexts with a focus on </span><span class="No-Break"><span class="koboSpan" id="kobo.3.1">responsible adoption.</span></span></p>
<p><span class="koboSpan" id="kobo.4.1">This part contains the </span><span class="No-Break"><span class="koboSpan" id="kobo.5.1">following chapters:</span></span></p>
<ul>
<li><a href="B21773_05.xhtml#_idTextAnchor180"><em class="italic"><span class="koboSpan" id="kobo.6.1">Chapter 5</span></em></a><span class="koboSpan" id="kobo.7.1">, </span><em class="italic"><span class="koboSpan" id="kobo.8.1">Fine-Tuning Generative Models for Specific Tasks</span></em></li>
<li><a href="B21773_06.xhtml#_idTextAnchor211"><em class="italic"><span class="koboSpan" id="kobo.9.1">Chapter 6</span></em></a><span class="koboSpan" id="kobo.10.1">, </span><em class="italic"><span class="koboSpan" id="kobo.11.1">Understanding Domain Adaptation for Large Language Models</span></em></li>
<li><a href="B21773_07.xhtml#_idTextAnchor225"><em class="italic"><span class="koboSpan" id="kobo.12.1">Chapter 7</span></em></a><span class="koboSpan" id="kobo.13.1">, </span><em class="italic"><span class="koboSpan" id="kobo.14.1">Mastering the Fundamentals of Prompt Engineering</span></em></li>
<li><a href="B21773_08.xhtml#_idTextAnchor251"><em class="italic"><span class="koboSpan" id="kobo.15.1">Chapter 8</span></em></a><span class="koboSpan" id="kobo.16.1">, </span><em class="italic"><span class="koboSpan" id="kobo.17.1">Addressing Ethical Considerations and Charting a Path toward Trustworthy Generative AI</span></em></li>
</ul>
</div>
<div>
<div id="_idContainer029">
</div>
</div>
<div>
<div id="_idContainer030">
</div>
</div>
<div>
<div id="_idContainer031">
</div>
</div>
<div>
<div id="_idContainer032">
</div>
</div>
<div>
<div id="_idContainer033">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer034">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer035">
</div>
</div>
<div>
<div id="_idContainer036">
</div>
</div>
<div>
<div id="_idContainer037">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer038">
</div>
</div>
</body></html>