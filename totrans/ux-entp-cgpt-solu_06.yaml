- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Defining the Desired Experience
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To define the desired experience, the first decision is which experiences demand
    attention. This is the prioritization discussion started in the last chapter.
    Knowing where these use cases are to appear helps with scope, planning, and follow-through.
    Some require building traditional conversational experiences; shockingly, these
    can be called classic now! Others will use channels (such as Slack, Teams, or
    a web experience) that can handle interactive elements, such as GUI components.
    Some will extend features within existing channels to include LLMs in the workflow
    to solve a limited use case. That last one is a powerful incremental approach.
    Others will build single-purpose bots or voice-only interfaces. Each of these
    experiences has unique and general considerations.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the highest-priority enterprise user experiences one
    can develop with ChatGPT, along with three overarching design elements: accessibility,
    internationalization, and trust. Get ready for the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Designing chat experiences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing hybrid UI/chat experiences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating voice-only experiences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing a recommender and behind-the-scenes experiences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overarching considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing chat experiences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An LLM-powered chat experience comes in a few flavors: a straight chat-only
    experience, such as OpenAI’s chat UI, or a chat window that is part of a more
    extensive GUI experience. Each has unique issues, and both have a collection of
    issues to address. A traditional chat-only experience is an excellent place to
    start.'
  prefs: []
  type: TYPE_NORMAL
- en: Chat-only experiences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many, if not most, readers will have this use case: a conversational experience
    backed with a ChatGPT solution. It might support a traditional conversational
    AI platform (Salesforce, Oracle’s Digital Assistant, Alexa, Google Assistant,
    or one of the dozens of other vendors) or eventually replace them. However, the
    primary interaction is via a chat window with chat responses. This means the channel
    SMS (text messaging), WhatsApp, WeChat, Facebook, LinkedIn, a web experience,
    Slack, Teams, or other services depend on the technology those channels support
    for integrations, backend services, security, and user experience.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This example chat experience, as shown in *Figure 5**.1*, is direct from OpenAI’s
    UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – A traditional chat UI from OpenAI
  prefs: []
  type: TYPE_NORMAL
- en: It is typical to have a message input area with a **Submit** button, a single-threaded
    discussion, the ability to download the discussion, and visual affordance for
    the customer input and the AI response. It is more robust than most because, as
    an authenticated user, it keeps track of previous conversations, which is a nice
    touch. It has room because it is a whole browser window and is mobile-friendly.
    An enterprise experience will likely embed a chat framework into a website, portal,
    or app. Some UIs include features such as file attachment, clearing the field,
    or closing or moving the window.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI’s iPhone client has one nice UX feature similar to what was introduced
    at Oracle: scrolling only a little for long answers. Working with Kevin Mullet,
    we coined the problem “over-scrolling.” The window should **not** scroll to the
    end of the response when the AI returns a message longer than the window size.
    If this happens, the user must scroll backward to find where the message started
    and then scroll forward to read. This is a headache for accessibility and just
    annoying for everyone else. As in *Figure 5**.2,* the first chunk of a message
    in the OpenAI mobile UI is returned with the arrow. The entire message is returned
    once the user clicks the arrow, so the user still has to return to where they
    left off. They almost got it right!'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – The OpenAI chat message is sent (left), then pauses when the window
    is full (center). Clicking the continue arrow shows the entire message (right)
  prefs: []
  type: TYPE_NORMAL
- en: See whether your chat client is smart enough with long messages. Does the scroll
    bar grow, but the screen doesn’t scroll as additional details are still not yet
    exposed? Can the user read messages in one scrolling motion, starting from the
    beginning of the response? *All chat UIs should have support for over-scrolling*.
    Slack, for example, shows a **missed messages** button, so it will take the user
    to where they last left the conversation. This is another good **micro-interaction**
    pattern for solving this problem and relieving users from the stress of not knowing
    what they missed.
  prefs: []
  type: TYPE_NORMAL
- en: When designing a chat experience, consider all these micro-interactions and
    the elements needed to be exposed. Micro-interactions are those small but critical
    elements that make UI moments seamless and enjoyable. It can be as simple as the
    button changing color when hovering the mouse over it, giving an affordance that
    it can be clicked, or the typing indicator to help anticipate the arrival of a
    message from a friend. They can be used to update the user on system status, encourage
    behaviors to avoid error, reinforce brand identity, drive the user to a specific
    task or engagement, or improve the interaction. They are generally very lightweight,
    following a “less is more” approach. Consider these when creating the UIs, recommenders,
    or how ChatGPT responds, provides next steps, or interacts with services. A user
    can enjoy a little delight from a micro-interaction. These are the basics. Here
    is a little deeper journey.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [The Role of Micro-interactions in Modern UX](https://www.interaction-design.org/literature/article/micro-interactions-ux)
    ([https://www.interaction-design.org/literature/article/micro-interactions-ux](https://www.interaction-design.org/literature/article/micro-interactions-ux))'
  prefs: []
  type: TYPE_NORMAL
- en: If the plan is to integrate ChatGPT into an existing chat UI, some basics should
    be in the original chat experience. Covering a few basics next can’t hurt.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating ChatGPT into an existing chat experience
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider whether ChatGPT is the primary mechanism for customer interaction
    and what other services and data would support it. In the enterprise space, there
    is something unique and valuable inside the data. The goal is not to repackage
    the existing ChatGPT service while emphasizing the user experience. What should
    be considered when integrating ChatGPT into chat? How should it be done? Let’s
    explore the options:'
  prefs: []
  type: TYPE_NORMAL
- en: Use a simple chat window with ChatGPT as the primary actor with knowledge indexed
    using **Retrieval Augmented Generation** (**RAG**) (RAG is covered in [*Chapter
    6*](B21964_06_split_000.xhtml#_idTextAnchor134), *Gathering Data – Content is
    King*) or other services linked to database integration or interfaces with other
    company services.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supplement an existing Chat “Bot” experience. Enhance the existing deterministic
    flows (wizards or chat flows) with ChatGPT to improve comprehension, entity recognition,
    world language support, image recognition, or other abilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supplement human agent experiences by gathering data or solving problems before
    being handed off to a human. This is to make the agent more efficient or to provide
    additional services, marketing, or feedback.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ChatGPT is used after a human agent has experience handling confidential information,
    surveys, or feedback.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using ChatGPT as a third party in a human conversation (an AI team assistant,
    supporting an agent, a user, or a team). This can be used as a resource in a group
    setting. Consider any security implications, where the requestor might have permission
    to access some data while other members might not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use different LLMs and prompts to support pieces of workflows. Chaining models
    to improve model output from the previous steps is common and will be covered
    extensively in the following chapters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This list can’t be exhaustive. The growth rate of creative use for LLMs is exploding.
    But all of them, if they are going to be chat-centric, have some basic functionality
    one would expect. The existing chat UI could be ready for some enhanced components.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling components for a chat experience
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With a chat window, there is a decision point for what features make sense
    to support the interactions in the chat window. Most of these elements would be
    the same if the chat were between two humans or a human and an LLM. However, many
    developers need to be guided by a suitable set of features:'
  prefs: []
  type: TYPE_NORMAL
- en: Message input window (the typing window for input)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Threaded response area (the conversation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Submit button (to send a message and attachment)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Visual indicators for the message owner: bubbles, avatars, left/right alignment,
    and so on (something is required)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Accessibility compliance: screen reader support, keyboard navigation, alt text
    for images, high contrast mode, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Optional components to enhance the overall experience:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Download, Print, Export, Clear, and Close (when applicable)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Timestamps (as needed and used intelligently; only timestamp some messages)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A movable window
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Previous message history
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Short-cut suggestions that appear to help finish a thought or action
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Search in the thread (supported by browsers, consider for applications)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for not over-scrolling
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Help, tours, or tutorials
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An emoji picker
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Short-cut panels (useful for professional users to quickly insert popular items
    or fill in common commands and statements)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to escalate to an agent (for support use cases, typically done online
    in the chat or with a button)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer feedback (optional as a UI element; also supported in chat)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Omni-channel integration (so other channels have this context)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Typing indicator (recommended if a multi-user UI is used; less critical if the
    session is only with ChatGPT since it should reply immediately)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A good experience will consider all these options but only implement some. Consider
    the importance of each requirement. Use the **Weighted Shortest Job First** (**WSJF**)
    score introduced in [*Chapter 4*](B21964_04.xhtml#_idTextAnchor085)*, Scoring
    Stories,* to decide what part of these requirements are needed. Look at the use
    case and score these options. A few of these are more valuable than others. And
    other than links and maybe images, there is very little richness from a *chat-only*
    experience. Handling links and images is covered in the next section on hybrid
    UIs.
  prefs: []
  type: TYPE_NORMAL
- en: Designing hybrid UI/chat experiences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Hybrid UIs** combine chat with a traditional GUI. This experience can include
    forms, tables, or interactive elements in a chat stream or merge chat into an
    existing GUI. Sometimes, chat is in its panel using the graphical UX as content
    to interact conversationally with the data. Examples like this have been available
    over the years, but the power of LLMs allows for more robust and accurate interactions.
    These multi-modal experiences can blend GUIs, voice, speech-to-text, text-to-speech,
    and LLMs into one solution.'
  prefs: []
  type: TYPE_NORMAL
- en: This is the latest version of **Rich Internet Application** (**RIA**), coined
    around 2002 at Macromedia. I read a paper by Kevin Mullet (written for Macromedia)
    and found it shockingly consistent with today’s experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [The Essence of Effective Rich Internet Applications](https://darmano.typepad.com/for_blog/essence_of_ria.pdf)
    ([https://darmano.typepad.com/for_blog/essence_of_ria.pdf](https://darmano.typepad.com/for_blog/essence_of_ria.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: In software design, the complexities of interactive control remain the most
    significant impediment to a satisfying user experience. – Kevin Mullet (The Essence
    of Effective Rich Internet Applications)
  prefs: []
  type: TYPE_NORMAL
- en: The images in the article are outdated, but the underlying problems are still
    the same, with a few new concepts (voice and LLMs) added to the mix. While much
    has changed, much has stayed the same.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the functionality of the Microsoft Word **Format** menu is mostly
    unchanged, as shown in *Figure 5**.3*. The underlying problems of finding and
    using the breadth of Word’s features haven’t changed and have barely improved
    in decades. We bring this up because of the well-understood use cases found in
    Word.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Similarity between the circa 2003 Windows (left) and 2024 Macintosh
    (right) Format menu
  prefs: []
  type: TYPE_NORMAL
- en: The exciting part about applying AI to Word is that the user’s needs should
    be well known. The most valuable use cases should be easy to determine (but might
    be hard to realize). It is up to the product people to understand customers’ pain
    points, recognize the LLM’s value, and see a path forward.
  prefs: []
  type: TYPE_NORMAL
- en: Getting an order right in a drive-through ordering system is challenging. I
    am constantly reminded to check the bag before leaving a restaurant. A modern
    version for solving complex auditory conditions and ordering problems is a multi-modal
    solution for a drive-through experience from SoundHound. The customer speaks the
    order; the screen confirms and validates the order with AI audio responses. This
    demo shows the power of multi-modal interactions. At the time of this video, it
    did not use an LLM, but it is impressive because of the speed of multi-modal interactions.
    The display updates at a drive-up kiosk as fast as the user speaks and displays
    order corrections. *Figure 5**.4* is a screenshot from the SoundHound ordering
    demo.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Screenshot from the SoundHound ordering demo
  prefs: []
  type: TYPE_NORMAL
- en: 'Video: [Restaurant ordering via voice](https://www.soundhound.com/voice-ai-solutions/restaurant-food-ordering/)
    ([https://www.soundhound.com/voice-ai-solutions/restaurant-food-ordering/](https://www.soundhound.com/voice-ai-solutions/restaurant-food-ordering/))'
  prefs: []
  type: TYPE_NORMAL
- en: Voice is the input and the display updates with changes. At walk-up kiosks,
    touch can supplement voice interactions, interchangeably interacting via touch
    and speech. At the same time, the conversation goes on without turn-taking (you
    can interject in the speech output and don’t have to wait for a pause in conversation).
    It is a fascinating application because it affords the restaurant more flexibility
    in language understanding and better feedback for the user as they see their order
    and can change it. More direct experiences like this are coming, where typing
    (or voice) directly supports interactions with the interface, including on desktop
    and mobile apps. As voice interactions become faster for the LLM (in the 10s millisecond
    response time range), they are fast enough to be natural, pause, or interrupt,
    just like humans.
  prefs: []
  type: TYPE_NORMAL
- en: Another exciting application of ChatGPT is to supplement and support the interaction
    experience for creating content. Suppose the use cases include writing emails,
    generating marketing materials, or writing job descriptions. In that case, you
    can get a big leg up by gathering and using the material created to develop the
    next piece of content. For example, an intelligent assistant accessing all the
    material on a screen can provide many supportive capabilities. With a wealth of
    quality vetted material, new material can be trained to be in the style and tone
    of the existing material. Or, with one change to a prompt, an entirely new tone
    can be developed, but still based on the expertise and content supplied.
  prefs: []
  type: TYPE_NORMAL
- en: The classic example for defining the design space for an application is the
    integration of generative AI with *deterministic* models to fill in forms or perform
    tasks. The current crop of chat assistants is based on coding for specific tasks,
    with specific responses, predictable behavior, and limited flexibility. These
    straightforward deterministic experiences work well for repetitive tasks with
    predictable inputs and outputs. However, they have gaps, typically with complex
    integrations, interactions not accounted for in the coding, and the need for many
    languages. These platforms are being reworked with LLM integration to address
    these shortcomings. Many assistants faced some real challenges around natural
    language processing and recognition. A design can unlock additional value by combining
    a GUI’s visual nature and precise communication abilities with LLM understanding.
    *Figure 5**.5* shows a simulated example of a chat experience with a GUI.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Filling out a hybrid UX with conversational intelligence
  prefs: []
  type: TYPE_NORMAL
- en: This UI appears to be a simple form, but it is in a chat window. A customer
    interacts with the form or by chatting. In the middle screen, the user typed (or
    used voice) entering input to manipulate the form. The form was updated on the
    right, the error was removed, and a message was returned. An LLM can contribute
    intelligence on how to parse these messages better than traditional chatbot software.
    They can also be used to quickly adapt the UIs to support more languages than
    are typically backed by vendors using their internal training models. The current
    models, such as ChatGPT 4o-mini, continue to improve in these areas.
  prefs: []
  type: TYPE_NORMAL
- en: 'I want to point out a few micro-interactions that appeared in *Figure 5**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: The AI understands the missing or misunderstood information in the submitted
    expense or receipt scan (prior to this screen, it would have triggered this response)
    and presents the errors with a traditional in-line error message. In the future,
    I could see the LLM asking the user to visually mark something on the receipt
    that it didn’t understand (e.g., circling a number it didn’t understand and asking
    for help). This could drive a workflow for improvement without significantly burdening
    the user. Don’t turn customers into testers, but sometimes you need all the help
    you can get.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The form clearly shows what is missing and allows the user to edit with text,
    voice, or interaction with the form (the keyboard is hidden for this mockup).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The AI understood “Saturday” as the previous Saturday and entered the information
    in the correct format, avoiding opening the calendar, viewing, picking the month,
    and clicking on the date.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The UI is dynamically updated, acknowledges the added data, removes missing
    details and errors, and allows users to submit with a button or text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, in this hybrid GUI example, ChatGPT can do a lot to interpret the user input
    format and submit it as the correct fields. Imagine doing this entire interaction
    without receipt scanning and no form. It would be very tedious and slow. The system
    and GUI patterns did some heavy lifting to create the complete experience. Similar
    flows using existing chat platforms might already be in production. Still, they
    could use assistance in understanding human language, distinguishing entities
    (specific elements, such as names, businesses, and values), and expanding language
    support. ChatGPT can enhance and improve these parts of the experience without
    throwing out the existing intelligence and expertise built up in designing these
    business-critical tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, scanning a meal receipt in the flow might be an additional area
    for improvement. Training visual foundation models to understand images of business
    documents and classify and extract critical information from them seems like a
    whole world for organizations to explore. Many companies already use AI for expense
    receipts, invoices, bills, quality control images, counting and determining inventory,
    disaster scenes, and car insurance. The quality of these processes will go up
    with more advanced models. This book focuses on ChatGPT’s textual and written
    abilities. However, I encourage exploring image-based use cases when they apply.
    Models such as ChatGPT 4o-mini can handle images as well (the model can classify
    images uploaded, for example), and by integrating with other services such as
    OpenAI’s Whisper, transcribed audio can be pulled into a workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Integrating ChatGPT into a workflow is fundamental today. In another example,
    the product might support writing emails or content for a website. Adding a **Generate
    Post** or **Write Description** button and having ChatGPT gather the information
    and inject the copy into an existing window is not too hard. One gotcha: *always
    support the editing of this material before submitting it*. It might be inaccurate,
    too specific, or not specific enough based on the content used in generation.
    Consider also capturing the suggestion and the final submitted (and edited) version
    that the human changed. Use this as data to help improve the model. The difference
    between this and the previous marketing content example is that deciding what
    has ground truth is essential. With the marketing content, what is good content
    is known, and it can be supplemented with field data on how well customers responded
    to different campaigns. When writing emails or website content, it might be more
    challenging to identify high-quality writing to train the model. Otherwise, a
    generic model will only be provided based on its original training; sometimes,
    that is fine. You can always ask the model to adapt the language. Even the generic
    40-mini can-do tasks like this.'
  prefs: []
  type: TYPE_NORMAL
- en: Write a thank-you to my
  prefs: []
  type: TYPE_NORMAL
- en: customer in less than 100 words.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Re-write it in a more
  prefs: []
  type: TYPE_NORMAL
- en: casual tone, he is family.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Suppose the use case requires a sidebar experience where AI can supplement a
    traditional UI. It could start as a glorified intelligent search, but this can
    be leveraged to quickly gain previously buried or unavailable insight from current
    content. It might have an **Analyze** button (or generate some recommendations
    automatically), know what has happened in the thread, provide a **Generate a Reply**
    option or do tasks that any human assistant might do to help a team member. Each
    application will have its use cases, just like any enterprise company in automotive,
    education, financials, manufacturing, healthcare, government, real estate, sales,
    service, and a dozen other verticals. Companies that provide enterprise software
    have a head start because they can work with and adapt complex components.
  prefs: []
  type: TYPE_NORMAL
- en: Oracle’s Digital Assistant does an excellent job of advancing the state of the
    art, as shown in *Figure 5**.6*, with a sidebar experience.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – Oracle’s Digital Assistant sidebar inside Oracle Sales
  prefs: []
  type: TYPE_NORMAL
- en: This shows how a traditional web enterprise application can be supplemented
    with a conversational generative AI based on understanding the page’s context
    and all the related data. The demo goes into other details, but the point is that
    existing services can be supplemented with an LLM. Check out the Oracle demo,
    which starts at 46 minutes and shows how the co-pilot pattern can work embedded
    with an existing UI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Video: [A generative AI co-pilot feature in a GUI](https://www.youtube.com/watch?v=9CEfru54Oyw)
    by Oracle ([https://www.youtube.com/watch?v=9CEfru54Oyw](https://www.youtube.com/watch?v=9CEfru54Oyw))'
  prefs: []
  type: TYPE_NORMAL
- en: Besides providing insight, consider interactions with the sidebar that can influence
    or address problems. In the preceding example, the chat is used to supplement
    the experience with issues related to the customer, as shown in the dashboard.
    This page has a wealth of context so that conversational AI could offer proactive
    suggestions based on the evaluation of the data.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the team is building a sales enablement page that reviews customer
    accounts and deals. In that case, the sidebar can allow questions about this customer
    that might be buried in other systems or offer recommendations for the next step
    in the process. Order status, customer care experience, open trouble tickets,
    and additional customer history information offer value when on the sales page.
    However, exposing the generative AI as a resource supplements the common understanding
    with these details. But don’t stop there; use hyperlinks to wrap the text so they
    can drill down into the enterprise data to further their understanding. Allow
    the customer to use the generative AI to enhance their editing, posting updates,
    or drilling down further into insights held by all of this information. Help the
    user understand the scope of the options the user has at this point. Sometimes,
    buttons such as **Edit**, **Open Details**, **Send Email,** or **Change Status**
    are used. Whatever the use cases, consider going deeper into exploiting generative
    AI models to enhance these steps.
  prefs: []
  type: TYPE_NORMAL
- en: This focus on creating the desired experience by addressing ChatGPT’s unique
    complexities can’t cover all UX design guidelines for GUIs. There are dozens of
    books and websites available. Focus on the fact that UX design within a conversational
    experience is slightly different. While you might have a traditional web form
    (with fields and drop menus) in a GUI, what would make that different when placed
    into a conversational experience? For example, in the **Oracle Digital Assistant**
    (**ODA**) example shown in *Figure 5**.6*, notice a small table in the chat and
    a very different table on the main page. It is easy to realize that a chat form
    factor table can’t handle everything a big table can do. Let’s review some essential
    tips to consider when designing hybrid conversational experiences.
  prefs: []
  type: TYPE_NORMAL
- en: Chat window size and location
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chat windows are typically vertically oriented and small on web pages, so the
    main page can still be used. Depending on the target window size, a typical default
    chat window can take up 11% to 25% of the screen real estate (and less for folks
    with two monitors). The example in *Figure 5**.7* is based on standard desktop
    screen sizes for August 2024\. This data changes very slowly. Changes in the last
    year varied by less than one percent.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – Comparison of desktop screen resolutions to a typical chat window
    (August 2024)
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Current Desktop addressable sizes](https://gs.statcounter.com/screen-resolution-stats/desktop/worldwide)
    ([https://gs.statcounter.com/screen-resolution-stats/desktop/worldwide](https://gs.statcounter.com/screen-resolution-stats/desktop/worldwide))'
  prefs: []
  type: TYPE_NORMAL
- en: It would be best to examine customer data to get a more accurate estimate of
    screen resolutions. I used to run this analysis every year or so to influence
    all kinds of web application design decisions. Enterprise customers’ screens were
    always significantly larger than in consumer data.
  prefs: []
  type: TYPE_NORMAL
- en: If the primary experience is chat, such as Slack or Teams, size constraints
    are not an issue on the desktop. There is room for large tables, maps, and good-sized
    images, forms, and other interactive experiences. For those where chat is a secondary
    element, provide enough space to support necessary interactions but not so big
    as to interfere with the primary windows. There is no one-size-fits-all answer.
    Consider the use cases; the less complex the cases, the less space is needed.
  prefs: []
  type: TYPE_NORMAL
- en: Text-only seems to work fine for the phones of the world, with roughly only
    40 characters across (as customers increase font size for accessibility, this
    number goes down). And the context of use is essential. If the customers are mobile,
    the width of typical phones or even their specific phone is known (from web analytics).
    Don’t try to do anything else on the screen along with the chat. Horizontal scroll
    bars should only be used with extreme caution. If it is in a window, consider
    notifying the user that they have unread messages while away from the chat. Review
    examples of unread message interactions on any social media platform (LinkedIn,
    Facebook, Instagram, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: One last example for desktop experiences (web or native). Look at the demographics
    of users and their browser sizes and desktop sizes (web logging tools such as
    Adobe Analytics or Google Analytics report this information). Use this to decide
    how much space the user typically needs for the main application window and how
    much real estate can be shared with the chat window. Consider the responsiveness
    of the application window. The ability of the main content window to support good
    flow and even a mobile-friendly view (even on the desktop) allows support for
    a sidebar view, knowing that the main content can adapt.
  prefs: []
  type: TYPE_NORMAL
- en: A list of objects is one of the most relevant patterns in enterprise applications
    that need a lot of space. Sometimes, in tables or card views. We can now address
    how a chat experience impacts tables.
  prefs: []
  type: TYPE_NORMAL
- en: Tables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider the mobile experience’s design space when considering what is done
    in a typical chat window. Tables in desktop windows need to be adapted to a narrower
    chat width. Either use a different layout or limit the number of columns. For
    example, email on a desktop is in columns, as in *Figure 5**.8*, while on a phone,
    a card view is displayed, as in *Figure 5**.9*. If the model will return cards
    for results or small tables, realize more room is needed to present comprehensive
    information directly.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – A traditional table view won’t work well in a small chat window
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – A card view would work in a chat window
  prefs: []
  type: TYPE_NORMAL
- en: Consider format and content when presenting tables. Models such as ChatGPT 4o-mini
    (the “o” is for Omni, so I guess they think it knows everything) are very good
    at formatting data. However, a mobile user will still have the same window size
    issues as those in pre-ChatGPT experiences. And because this has a scrolling window,
    scroll bars show up. But never have double scroll bars. While a scroll bar would
    be fine on the desktop, use a next function to show more data since there is already
    one scroll bar in the scrolling window. Or find another alternative. If an existing
    table component is mobile-friendly, it might automatically adjust to a card view
    in the tight confines of the chat panel. It all depends on how the model models
    support the user experience. If the chat from OpenAI is the UI, there is no option
    to create richer interactive experiences. If the team owns the client, the world
    is your oyster, and call on OpenAI models to fill in the pieces to create rich
    experiences. Even with recommender UIs, there are places to provide feedback for
    the recommender to make more refined decisions. Prompt the user for information
    to fill in the blanks in the model’s understanding to improve the results. This
    is on your plate, not the AIs. *Figure 5**.10* compares the traditional table
    format with a poorly implemented horizontal scroll bar. By changing the prompt
    to provide the results in a more mobile-friendly format, we get more readable
    (but less comparable) data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – A table view (left and center) compared to a mobile-friendly list
    view (right)
  prefs: []
  type: TYPE_NORMAL
- en: 'The horizontal scroll bar is not even in view (and when scrolling, as shown
    in the middle pane, the player’s name will scroll off-screen; it is not frozen
    or locked like in a spreadsheet), making analysis hard and the right-most columns
    impossible to associate with the player. Try prompts such as this on ChatGPT,
    use a phone, or resize the browser window to the smallest size possible, and try
    to review and use the table data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Consider the use cases, how the data should be best presented, and the channel
    in which the user absorbs the results. Links in the table allow the user to dive
    further into the data. This is typical on a web display of data. Understand that
    they might already have interactive richness on the traditional website.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If more room is needed and there is more room (e.g., if in a small chat window
    in a larger desktop window), build the ability to expand the window. Consider
    opening the table in a new window if a table-heavy use case demands it. It is
    possible to identify that the data isn’t fitting in the space and offer the user
    a way to see it all, such as sending a file, opening the data in a new window,
    or other innovative solutions.
  prefs: []
  type: TYPE_NORMAL
- en: As for the table content, please keep it to a minimum. Summarize tables and
    support drill-downs. To interact with one of the results, consider how to select
    it and expand the details. Does it make sense to bring that object into a main
    browser window, where there is plenty of room, or expand the details in the conversation?
    There is no one correct answer. Let the use case guide you. In the preceding example
    with American football statistics, a robust experience around table data could
    have links on data to drill into details, columns that can be sorted, or access
    to richer analytics on players’ names and teams. There are lots of options to
    explore. Unfortunately, the tools and patterns that the enterprise supports might
    be limited. Always remember the end state so improvements and new patterns can
    be prioritized for the chat channel. Editing in a form (and, more recently, in
    tables) is fundamental to enterprise interactions with data and tasks. A drill
    down into forms helps us understand some of the complexities when done within
    a chat experience.
  prefs: []
  type: TYPE_NORMAL
- en: Forms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fortunately, if created for a mobile-friendly experience, most forms are ready
    for a chat experience. Only a few customizations should be considered.
  prefs: []
  type: TYPE_NORMAL
- en: First, consider how big the form is now. Can it fit in a chat window? Does it
    need many pages of fields to be filled out? A crazy long form might not do well
    in a chat experience. Then, consider what happens when the form is submitted.
    Since the chat should persist, what does the form look like after submission?
    I usually suggest converting it to a card. This transition serves as an affordance
    to the form that was submitted. Also, it is essential to consider when mistakes
    are made and how the user can re-edit the information. Like including an **Edit**
    button or link to return to the form, make the change, and then save, returning
    to the card view. As a little preview of [*Chapter 9*](B21964_09_split_000.xhtml#_idTextAnchor190)*,
    Guidelines and Heuristics*, supporting the edit affordance and undo are two items
    that should be addressed in a heuristic evaluation. That chapterwill dive deep
    into using heuristics to understand and classify user experience issues.
  prefs: []
  type: TYPE_NORMAL
- en: Recall the Starbucks expense in the expense assistant example. After “Submit
    this expense,” as in *Figure 5**.11*, should the user continue to see the editable
    form in the chat’s history?
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – Using visual affordances to turn a submitted expense into a card
  prefs: []
  type: TYPE_NORMAL
- en: A traditional GUI would remove the edited form when submitting the expense item.
    So, persisting this editing view conversationally also doesn’t make sense. Disabling
    the form is one option, but using the **Cancel** option at the top of the image
    on the right and updating the existing form to the submitted expense version is
    better. This ability to revisit and edit existing items on the chat history might
    be a challenge, depending on the ownership of the chat window technology. With
    a card display, the content looks finalized; it will feel complete. To edit it
    further, the AI should allow that conversationally or consider a button. Notice
    the **Edit** icon on the meal in *Figure 5**.11*. Editing conversationally before
    submitting the entire expense report, including this single expense, is allowed.
  prefs: []
  type: TYPE_NORMAL
- en: Also, consider form elements that are robust or complex, such as a search experience.
    One component in the form might want to use space allowed by the width or height
    of the chat window. For example, searching for a product name sometimes product
    names can take a long time in the enterprise. Consider wrapping the results to
    a second line instead of truncating. The problem with truncating on a mobile phone
    or small window is that it is hard to show the full name. And if a few items differ
    only by the last few characters, how would a customer tell them apart? This isn’t
    specific to LLMs; it is just a good design for small windows, such as **Chat**.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if they are looking for a product demo to download and search
    for `Agile Product Lifecycle`, which returns two products, it would be impossible
    to tell them apart if the search results were in a small window or drop menu and
    truncated:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Agile Product** **Lifecycle Management…**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Agile Product** **Lifecycle Management…**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the full text of the items is wrapped, they can select the correct one:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Agile Product** **Lifecycle Management**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration Pack for Lavasoft** **E-Business Suite**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Agile Product** **Lifecycle Management**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration Pack for Fission** **Application Suite**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: So, consider how this small viewing area impacts components. If components are
    already mobile-friendly, then most of the problems are solved. However, there
    is still a difference between components *built* for a small screen and those
    *designed* for a small screen. The truncation problem I just shared is far too
    common.
  prefs: []
  type: TYPE_NORMAL
- en: The last consideration is how to interact with the form or card when conversing.
    Let’s look again at the form and card concept shown in *Figure 5**.12*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – The form and card views when starting to edit conversationally
  prefs: []
  type: TYPE_NORMAL
- en: If the user enters `Change the amount to $9.45`, does the amount element in
    the existing form update (left example), or do they have to re-render a new form
    and hopefully remove or hide the now old and incorrect form or card? *Don’t show
    the wrong information to the user*, and updating the card with a visual micro-interaction
    might be better than scrolling another form onto the screen. The micro-interaction
    could be a twinkle and an animation showing the amount changing and maybe the
    message updating. Then, the user saves scrolling through the entire screen to
    invalidate the old form and create the new form. There is no need to show old
    or multiple editable forms in the history. Showing errors on an old form when
    the information is corrected is a terrible look.
  prefs: []
  type: TYPE_NORMAL
- en: With the form submitted and a card appearing, update the card directly with
    a twinkle and animation and update the text to show the change is accepted. These
    patterns are not readily available on any platform. It is fair to point out that
    conversational interaction patterns are still immature. Many kinds of interactions
    will need to be invented, tested, and matured.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s summarize:'
  prefs: []
  type: TYPE_NORMAL
- en: Make forms sensitive to the edit via text or direct edit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a card to show the completed state of the object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is unnecessary to show all the information from the form on the card. Consider
    how important it is for the user to recognize (and not recall) that this object
    has an invisible element.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider whether the use cases require editing everything in the chat AI. Some
    objects are complex and would be hard to edit conversationally, and the form would
    also be massive. There are use cases here that a larger viewable area might best
    serve.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a mechanism for complete components to work in the tiny chat window confines
    for the parts of use cases that make sense.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allow for conversational edits of the form and how to update the card or form
    with those edits.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t show forms in the wrong state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If making direct changes to a card or form, use an affordance to highlight the
    change.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes, a form is used to generate a chart. Now, let’s cover a few critical
    ideas around using charts in ChatGPT solution.
  prefs: []
  type: TYPE_NORMAL
- en: Charts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A chart presented in enterprise use cases should be interactive when displayed
    for exploration. Selecting and drilling into a data point, viewing more details,
    or exploring data segments to answer a different question is all within reason.
    This is similar to our discussion about tables. The accessible version of a chart
    is typically a table, so it should follow the same conventions. Conversational
    analytics requires charts to be more than static images; not every toolkit can
    support this. Consider the context for the chart so conversational interactions
    might tease new insights from what is presented, such as `Show me the breakdown
    of the 1st quarter by salesperson`. The chart is a steppingstone to further the
    conversation with the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the axes. Don’t default to a scale or unit the customer
    doesn’t want: `Show me the sales in Euros, not dollars`, or `Set the salary axis
    starting at zero and change the range to the last five years`. Conversing with
    data gets complex.'
  prefs: []
  type: TYPE_NORMAL
- en: If the user explicitly asks for a chart, give it to them, but a more innovative
    experience could be providing them with a chart because it makes sense. Suppose
    the LLM knows the user is visually impaired (because it was told or from profile
    information). In that case, showing a chart without the ability to show the data
    in a table is a problem because the screen readers will have an easier time with
    a table than a chart (but I suspect LLMs will be solving this as well). `Change
    the table to a chart` would be a logical response if the AI didn’t realize the
    user’s abilities. This knowledge might be used to drive subsequent interactions.
    Accessibility considerations will also be covered later, but it is helpful to
    keep accessibility concerns in mind.
  prefs: []
  type: TYPE_NORMAL
- en: 'When presenting these charts, one should consider what charts and design elements
    should be used. There is a wealth of bad charts, including not starting an axis
    at zero, rotating labels, using excessive colors, not using redundant coding for
    colored bars, lack of labels, too many pie slices, misusing line charts, and many
    other fails. The basic chart should also be designed, or the LLM should craft
    it from instructions. If this is new to you, ask ChatGPT for advice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Currently, only the paid versions of ChatGPT allow charts to be created, but
    that won’t be an issue for enterprise customers. Interacting with data is more
    interesting than just presenting static charts, which only tend to drive more
    questions than answers. And not to mention asking ChatGPT questions about the
    data that the chart might not reveal. Making data (and thus charts) conversationally
    interactive will be a massive space for enterprise use cases. Knowing how to design
    exceptional visuals is critical to effective communication. And being able to
    generate conversational insights from data is likely more important. The instructions
    provided to the LLM must instill some of that expertise if the results don’t match
    expectations. To explore the fundamentals of effective chart design, read the
    collection of Edward Tufte’s classic books linked on the reference page.
  prefs: []
  type: TYPE_NORMAL
- en: 'Books: [The work of Edward Tufte](https://uxdforai.com/references#C13) ([https://uxdforai.com/references](https://uxdforai.com/references#C13)#Tufte)'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have dabbled in charts, it also makes sense to talk about static
    images.
  prefs: []
  type: TYPE_NORMAL
- en: Graphics and images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The enterprise world has two kinds of images: decorative and functional. Showing
    “pretty” graphics can jazz up the user experience or give it style and character.
    These decorative images are not required and, thus, don’t require any support
    regarding accessibility and might not even impact usability. However, they can
    influence the customer’s mood, affecting the user’s experience. The second, being
    more critical, are images that provide a function. Avatars, background color coding,
    and bubbles help communicate the user’s content in the thread. Icons grab the
    eye to make a feature or status stand out. Given the smaller size of most chat
    windows, this is a valuable application of imagery. Generally, it would be best
    to have some affordance to the conversational back-and-forth elements, but not
    every affordance. Sometimes, images are the application’s primary purpose, such
    as with inventory apps, social media, online ordering, or image generators.'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose the application is an AI image generator. Consider how much space is
    available to render images and the resolutions available to download. In this
    use case, it is better to generate an image in the size that makes sense for the
    display because this can be 10x smaller than a “full size” image and thus faster
    to display. Choose the download size with a drop menu or UI element. The major
    image library sites use this pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Legal caution
  prefs: []
  type: TYPE_NORMAL
- en: Generative images are a new frontier of issues. Legal issues can arise if models
    are trained on copywritten and trademarked material, resulting in images that
    are generated too close to the source. This space will mature, but be aware of
    it. The base model was sourced from the Internet. This will also be an issue with
    text, voice, and music. Avoid famous people’s voices and likenesses. James Earl
    Jones, Christopher Walken, Scarlett Johansson, or any celebrities’ legal counsel
    will put offenders in their crosshairs.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose the application generates an image to include in a workflow, such as
    creating an advertising image of a product or a marketing image. If the model
    is trained on enterprise material and told to use those materials, it might address
    any legal concerns. To constrain the end-user, the instructions for the model
    could be set to constrain the image within a corporate standard style. Users should
    not have to do this every time. Once they approve the image, consider storing
    it with the resulting workflow and the messages used to generate it; this knowledge
    can help further model training. If the user needs to edit, they can quickly restart
    from the same message and tweak it further without retyping and, more importantly,
    having to remember what they typed.
  prefs: []
  type: TYPE_NORMAL
- en: In a chat-only experience, such as SMS messaging or a chat channel that doesn’t
    yet have component support, images are likely supported to some extent. So, look
    at the framework to see whether images can be tagged with a meta-data label for
    accessibility or if an explanation has to be written out in the channel as text.
    An LLM can be used behind the scenes or even in batch mode to generate descriptions
    for images and captions for tables and figures. Use LLMs as tools in a business
    workflow and understand what the channel supports to give customers control and
    usability around images.
  prefs: []
  type: TYPE_NORMAL
- en: Icons are a unique set of images. They are used in the design of hybrid UI,
    have some use in pure chat experiences and recommender, and have no place in a
    back-end UI. I haven’t explored using an LLM to generate icons, but from experience,
    it is excellent for brainstorming but not yet for the fine details needed at the
    pixel level. This conversation will thus be constrained to using icons in hybrid
    experiences.
  prefs: []
  type: TYPE_NORMAL
- en: Icons are used to convey an action or state or for visual feedback. Sometimes,
    they support words (such as a button with an icon and label) and, in some cases,
    are just icons. The label may be available on a mouseover or in a wider-screen
    view. In this case, consider the visual style and tone the collection of icons
    represents and whether there is a chat or conversational capability to interact
    with the icon. For example, if an icon shows the user is online (like a green
    dot in the corner by the username), there is presumably a way to go offline or
    into another state. If this is a support application that thousands of chat and
    phone agents use, there is typically a multiple-step process to take them offline
    or online. Is this something that they can do conversationally? It comes down
    to the use cases. A driver clicks buttons in a car to set the temperature, but
    it is much easier and less distracting to say to the car, “Turn on the A/C to
    68 degrees.” GUIs and voice interaction can work hand in hand.
  prefs: []
  type: TYPE_NORMAL
- en: And since interactive elements such as icons and images, their siblings, buttons,
    menus, and choice lists are worth some explanation.
  prefs: []
  type: TYPE_NORMAL
- en: Buttons, menus, and choice lists
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In some traditional chats with autonomous agents, the experiences could be more
    conversational. They *look* like a chat but only offer clicking buttons; the user
    can’t type. *Avoid this* *use case at all costs*. It is a red flag and will derail
    any goodwill with the customer. They become trapped in a poor experience. However,
    in an actual conversational AI, it is sometimes helpful to offer suggestions or
    hints. The primary issue with conversational AI comes from one of the primary
    heuristics of “recognition over recall,” covered in [*Chapter 9*](B21964_09_split_000.xhtml#_idTextAnchor190)*,
    Guidelines and Heuristics*. This means it is faster for users to pick from choices
    and recognize something than to pull from their memory and recall items.
  prefs: []
  type: TYPE_NORMAL
- en: A quick primer on recognition over recall
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Recognition over recall](https://www.nngroup.com/articles/recognition-and-recall/)
    ([https://www.nngroup.com/articles/recognition-and-recall/](https://www.nngroup.com/articles/recognition-and-recall/))'
  prefs: []
  type: TYPE_NORMAL
- en: This tenet applies to so many places in user experiences, such as menus. Although
    some applications have many features, finding a function takes time and effort.
    It is well known that picking from a list is more accessible than recalling a
    menu command. Now, with ChatGPT, the exact name of a command to activate a function
    is not needed. Questions can be asked without using the precise words of a command
    and still find good results. This helps mitigate some of the problems with text
    entry experiences. This is a beautiful change and makes chat experiences possible
    in the first place. However, it is still helpful to reduce the user’s cognitive
    overhead. So, if there are just a few choices or you have a good idea of what
    is next, it is reasonable to offer suggestions. Sometimes, those appear dynamically
    on a typeahead bar. Like in text messaging, it tries to predict what word will
    be typed, as shown on the left in *Figure 5**.13*. This predictive text is precisely
    how LLMs work, so it is the perfect place to assist users with LLMs as long as
    they can keep up with their typing speed. Sometimes, the responses are more robust
    than word or button choices in the response to submitting the prompt, as in the
    example on the right of the figure.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – Predictive typeahead buttons in Messages on an iPhone (left) versus
    the suggestion buttons in-line after a prompt (right)
  prefs: []
  type: TYPE_NORMAL
- en: 'Use predictive completion in three cases: when what is next is understood and
    when the user is lost or needs guidance. All possible paths are too many. Even
    three or four choices are enough. Consider the data. If the data can predict with
    90% certainty that one of four items is correct, then it makes sense to offer
    suggestions. If four options give a 30% of success, don’t offer them. And don’t
    forget to gather this data to help future models and decide how to improve the
    UI. Generally, it would be best to consider what happens when the menus or buttons
    are selected. The elements should be removed after clicking them. The same style
    of any GUI buttons can be used to create consistently, but in most UIs, buttons
    don’t disappear when clicked. However, in a chat UI, the buttons should not appear
    available, and disabling the buttons is a partial solution, but why waste the
    space? Generally, buttons are removed after selection. Choice lists or drop-down
    menus offer more options in less space but with the additional headache of having
    to open the menu to see the possibilities. If there are a lot of choices, then
    it is more complex to deal with search or scroll bars. Follow existing guidelines
    to use these correctly, then remove them from the UI display when done.'
  prefs: []
  type: TYPE_NORMAL
- en: In a GUI, there is room for buttons with large labels, maybe to draw the user’s
    attention to something on a large screen, but there isn’t room for them in a chat
    window, so keep menu labels short. Buttons are suitable for actions; use links
    for navigation.
  prefs: []
  type: TYPE_NORMAL
- en: Links
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Links are the basis of web applications, but linking is expected for any app.
    Tables of contents in documents are the old-school version, and menus are a version
    of links on a desktop. Links are valuable in hybrid UIs and are among the few
    UI elements available in any traditional chat experience. Hybrid, chat, and recommender
    UIs can use the links, and there are a few tips to consider regarding how the
    links should be handled.
  prefs: []
  type: TYPE_NORMAL
- en: Linking to outside sources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Linking provides URLs to services or content that can’t be offered inline. Links
    can give a reference, provide more details with an article, access a form or service
    not within the control of the chat, or open a new window or panel for the user.
    Determine how to present links to the user. If the experience opens a new window,
    will the user return to the first window to continue the conversation? If the
    chat is in a side window or panel to support what is happening in a main window,
    will the experience update if the links resolve in the main window so any subsequent
    conversations know about the change? Make decisions around these interactions.
  prefs: []
  type: TYPE_NORMAL
- en: Labels on links should be clear and indicative of their purpose. As shown in
    *Table 5.1,* instead of showing long links in the output (left), use a well-defined
    label (center).
  prefs: []
  type: TYPE_NORMAL
- en: '| **Don’t** **Do This** | **Readable &** **Manageable** | **Explanation** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Hi Richard. A new Explanation of Benefits (EOB) is ready. It may include
    time-sensitive or other important information.https://click.edelivery.uhc.com/?qs=7b10282
    356f8afb07d1e82e239f9722d466ca 84134771c3c35ee1b60c735d8940b3 2b169349bc53c30141f6c40140989f75b8589cd2af251
    | 1) Hi Richard. Please review your new Explanation of Benefits (EOB). It may
    include time-sensitive or other important information.2) Hi Richard. I can take
    you to the Explanation of Benefits (EOB)➚. This link is outside the portal, sign
    in is required. | 1) Readable links are more direct and take up less space. Avoid
    “Click here”.2) Alternative text when chat supports navigating the main window,
    but this link opens elsewhere. |'
  prefs: []
  type: TYPE_TB
- en: Table 5.1 – Use well-defined labels for links in the conversational output
  prefs: []
  type: TYPE_NORMAL
- en: Don’t expose URLs if the client can support a label. It is familiar enough for
    users to right-click and copy the URL. Focus on the use case and what makes the
    most sense for links. If this is a co-navigate UI (use links in the chat to control
    the page they are on) *and* has links to outside sources, consider how to communicate
    the difference between these two types of links, as I show in the second example
    output in the middle of *Table 5.1*. Will it make sense and be apparent to users
    that they have co-navigated? They might click multiple times without an affordance,
    not knowing the page behind the chat window has changed. Also, sometimes, the
    legal department gets in the way. There are websites where an intermediate screen
    opens after clicking a link, letting the user know they are leaving the site.
    This warning seems too complex for a UI, likely written by lawyers or because
    customers are confused by the link. Clarifying these interactions without the
    extra page and warning would be nice.
  prefs: []
  type: TYPE_NORMAL
- en: Link color
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I strongly encourage following HTML and web norms with links when building a
    client. Specifically, aging the link color after someone has clicked it. Typically,
    a blue link means unvisited and purple once visited. It is immensely valuable,
    even as subtle as it is, to keep track of which links were clicked, especially
    if a collection of links is being shared in a conversation.
  prefs: []
  type: TYPE_NORMAL
- en: Look carefully at Google search, as shown in *Figure 5**.14*; isn’t it great
    to return to the results list and it shows links that were visited? Use this pattern
    for all links. Customers might not notice it overtly, but it will be more usable.
    This is accessible for screen readers, as it communicates that the link was visited.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 – Purple indicates a visited link and is fundamental to providing
    context
  prefs: []
  type: TYPE_NORMAL
- en: Opening windows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Opening a new window is typical for a web link if it doesn’t replace the current
    page. Opening a new window is required for a chat window because it can’t replace
    the current small window with a web page. But realize that if the user goes off-site
    to return to the experience, they must figure out the window containing this chat.
    If they close the newly opened window, they should be returned to where they started,
    but this doesn’t happen as often as expected.
  prefs: []
  type: TYPE_NORMAL
- en: If the link opens a new window on the same site, there will be a different problem
    to solve. Can the chat window reopen and be reconstructed to pick up where the
    user left off in the previous window? Or there are now two windows open that have
    the same chat. And this can continue, so can the site handle 3, 4, or 10 open
    browser windows, all with the same chat? Can it keep up with the changes across
    windows? As this depends on the chat platform. Chat vendors should provide the
    solution. Just make sure to understand the requirements so either the team manages
    the problem or has the vendor address it.
  prefs: []
  type: TYPE_NORMAL
- en: Since a regular link in a chat window is expected to open in a new window, no
    additional affordance is needed. However, a sidebar must provide an affordance
    to help the user understand that it will open in the same window. Don’t use a
    link in a chat window to start a process; this is why buttons exist. Keep links
    for navigation.
  prefs: []
  type: TYPE_NORMAL
- en: These considerations allow for smoother, more engaging, consistent, and natural
    ChatGPT hybrid and chat-only experiences. The hybrid experience is the most robust
    and complex use case because it has to account for all the traditional user experience
    interactions and then layer in the additional complexity of chat. Stepping away
    from a graphical user experience, another vertical worth our attention is a voice
    experience, such as those found when calling a company and using their phone tree.
    Let’s review some tips to create compelling voice-only experiences.
  prefs: []
  type: TYPE_NORMAL
- en: Creating voice-only experiences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Phone calls or smart-speaker devices without a display are a few voice-only
    experiences. Voice-only was very limiting, and there are still human factors to
    consider, but technology has come a long way. ChatGPT 4o-mini takes empathy and
    tone to a new level, and because it is better at following instructions, prompts
    can control answers to be short to accommodate the capacity of a human to process
    voice in small pieces. There is nothing else to interact with because it is voice
    only. We don’t need to consider voice-only *within* a chat or hybrid experience;
    that is really voice-optional. A smartphone allows you to type or use voice and
    then translate the speech into text. All models behind these services will improve
    with LLM models behind the scenes. But we are not here to talk about enhancing
    speech-to-text. Let’s focus on a voice-only channel.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: When designing for a smart device, the vendors emphasize that only some devices
    have displays, but even if they do, the experience should work with voice only.
    Not all use cases make sense to interact physically with the device, even though
    it is supported. For example, an Alexa device in the kitchen might allow interaction
    with the display to go to the next step of a recipe, but the user might want to
    avoid touching it with messy hands, or it might just be out of reach.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with chat experiences, decide whether a generative solution replaces or
    supplements the experience. Everyone has headaches with phone trees, the hierarchical
    prompts that lead to a leaf on the tree when calling a business, requiring pressing
    *1*, **2**, or **3** to navigate down the *tree* of options. A positive of a well-designed
    phone tree is that it focuses on results based on choices. This is also why desktop
    software has menus. It limits the number of options at each turn: walking down
    some trees is more straightforward than directly recalling or accessing a specific
    node.'
  prefs: []
  type: TYPE_NORMAL
- en: Replacing a tree by starting with an open-ended “ask me anything” approach can
    be tricky. If I ask for “help with batteries” in a phone tree at Costco, a big-box
    retailer, do I want to go to “Automotive,” “Hearing Aids,” or maybe see whether
    household batteries such as AAs are in stock or on sale? In a phone tree, the
    user makes selections focusing on limited areas (Automotive, Hearing Aids, or
    Shopping), and then the correct choice is clearer.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the choice to introduce text-to-speech listening with generative AI
    at the root level (“Tell me how I can help”) or use it at the next level, “What
    is your question for the automotive service team?”. Generative AI will be more
    successful with robust content, knowledge, and APIs for a specific part of the
    problem space. Getting that detail takes time and effort. So, doing it for a particular
    department might be an excellent first step to limit scope. Draw on the first
    few chapters to learn which department would have a use case that is best suited.
    Determine which use case would have the highest WSJF score!
  prefs: []
  type: TYPE_NORMAL
- en: The closer to the top of the call tree ChatGPT is added, the more general and,
    thus, the more complex the solution. Customers who think it can answer anything
    will be more likely to ask. For example, a common question for a big-box retailer
    is a stock check. “*Does my local Costco have any Pampers in size 6?*” Without
    access to APIs to answer this question, the model can still understand this request,
    chat about it, and then transfer it to a human agent. Then, solve the problem
    of requiring the user to ask the human agent the same question. Give the agent
    the recorded question or transcription with a head start while the user is being
    transferred. So now the agent can answer the call and say, “*I am checking on
    the size 6 Pampers; one* *moment, please.*”
  prefs: []
  type: TYPE_NORMAL
- en: Not having to repeat the request would be a breakthrough and an actual omnichannel
    moment, even if the automation couldn’t directly answer the question. Or what
    about a stock check system that *only* interacted with the LLM while humans only
    proved the results to the software, thus reserving additional agent time for more
    pressing and complex interactions? “*I have asked a team member to check on the
    Pampers. It might take a few minutes. Is there anything else I can help you with
    while we wait?*” At least until the company gets the necessary APIs to automate
    stock checks.
  prefs: []
  type: TYPE_NORMAL
- en: The point is to create the best experience within the system while pushing for
    the necessary capabilities at the same time. If there are no stock check APIs
    and it is 15% of inbound call volume, then it is probably a problem worth investigating.
    With a simple phone tree and a team not ready to expand to the questions that
    will come with an open-ended experience, don’t bother adding ChatGPT at the top.
    The value comes from improving customer service and decreasing hold time for human
    agents for those common questions that could be answered by combining data sources
    with ChatGPT. There is a wealth of examples of inadequate implementation of AI.
    Don’t be the company that added ChatGPT, which worsened the experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering that the user’s phone number is known in many cases for existing
    customers (a good thing to have an API for), a lookup around this customer’s profile
    and history can generally reveal various opportunities for a customized experience:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Order status and tracking:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The user is known, so if open orders exist, immediately prompt and assist with
    details on those orders or route the call correctly to a human agent giving the
    order as context. *Are you calling about the online order that included the Shipley
    4-piece bedroom set? It is scheduled to be* *delivered tomorrow.*
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Nearest store details:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting with details about their typical store would be reasonable, especially
    on known holidays. *Happy 4th of July! Your Foster City store is currently open.
    It will close at* *6 p.m.*
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Support for purchases:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If they just bought a TV, use that information when they ask for support. *I
    see you just purchased a 65-inch Vizio Smart TV. Is that the product you need*
    *assistance with?*
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Technical support:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An existing open ticket is a likely candidate for further conversation. Of course,
    they might have dozens of available tickets in the enterprise space, so consider
    the scale of the issue when deciding how to interact with the customers. *I see
    your support ticket from yesterday was updated. Are you here for help with your
    credit card being* *charged twice?*
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Map the customer’s phone number to their company history to enable personalized
    assistance until more is known on the call. You have probably experienced this
    with messages like, “*I see you have called us, Richard, from this number before.
    If this is you, press one or stay on the line.*” Still, I suspect every vendor
    is working to improve. If you use a third-party solution, evaluate its ability
    to engage users, review logs and usage to identify opportunities for enhancing
    routing and responses, and use the methods from the prior chapter to prioritize
    improvements.
  prefs: []
  type: TYPE_NORMAL
- en: Here are a few short bits that can be helpful with voice experiences.
  prefs: []
  type: TYPE_NORMAL
- en: If disambiguation is needed to understand the situation (as in the example asking
    what kind of batteries were in stock), keep prompts short and straightforward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Offer secondary channels for complex interactions that would be difficult to
    conduct on a phone call (such as sending an email to complete a task later on
    a phone or computer).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log and track all errors or navigation issues that the user reports.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor path analytics (Funnel reports) to understand where customers hang up
    or get lost. This can expose opportunities for an LLM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure the voice persona suits the audience. Test and get feedback. Use A/B
    testing to consider alternatives. Advanced users could use a different persona.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If using multiple personas, associate the right persona with specific user groups,
    such as new users, experienced customers, or VIPs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This covers experiences with interaction. What about cases where there are little
    or no interactions? Are there things to do to create high-quality experiences?
    Let’s cover bot-only or behind-the-scenes experiences next.
  prefs: []
  type: TYPE_NORMAL
- en: Designing a recommender and behind-the-scenes experiences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, ChatGPT is used to analyze data, make recommendations, offer suggestions,
    or display results. That is an excellent use of this technology, and because the
    experience is simple, ensure the output is also simple. These experiences are
    not conversationally interactive or hide the use of the LLM. They might be called
    “headless” because only a recommendation is provided, and there is no front-end
    experience, such as a chat window.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some of the examples of bot-only or behind-the-scenes experiences:'
  prefs: []
  type: TYPE_NORMAL
- en: A recommendation flag or marker based on a ChatGPT analysis. This could be an
    icon showing **Urgent** or **Warning** on the cloud service dashboard or a **Call
    Now** flag for a sales lead. The user might not even know that an LLM is now behind
    this notification. Traditionally, this was handled by a simple rule, maybe some
    machine learning, and now an LLM. This is because the LLM can deliver more accuracy,
    address more complex situations, or be more timely.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A call to action: **Call the customer by Thursday; they are 30% more likely
    to close** **this deal.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A process improvement: **Response time dropped 11% last week. To return to
    the previous service level, increase call center staff by three agents during**
    **business hours.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A task recommendation: **Ship this package using DHL for the most** **cost-effective
    solution.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A virtual non-player character in a video game interacts more naturally and
    dynamically with players.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Idea generation. Offer suggestions for new ideas to solve problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Editing: Integrate with built-in authoring tools for marketing or coding that
    offer better suggestions than traditional deterministic spelling and grammar solutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In these cases, there is no direct interaction with the model; at best, you
    accept or don’t accept the options or recommendations. In many cases, UIs already
    had these experiences, but the sophistication of a generation AI solution did
    not power them.
  prefs: []
  type: TYPE_NORMAL
- en: Guidelines for bot-only experiences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There is such a wide range of use cases this is just general guidance:'
  prefs: []
  type: TYPE_NORMAL
- en: Use data logging to understand whether recommendations are being followed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a mechanism to get feedback on use while considering any data from logging.
    Don’t ask questions to which the answer is known. For example, from call logs,
    the system knows that an agent called the customer when recommended, so don’t
    ask whether they made the follow-up call. Use survey expertise to ask the right
    questions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t require feedback.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep suggestions short and to the point.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider secondarily exposing recommendation details, learn more about this
    recommendation feature, or analyze what factors led to the recommendation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t overuse recommendations. If every row of data has the same recommendation,
    then the value of the recommendation will be lost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor and revisit recommendations. Creating metrics on their success or failure
    will help position investment in future improvements. Without monitoring, it is
    unknown if it was a waste to develop it. I know one client uses ChatGPT to update
    resumes to be better suited to get through the automated screening tools that
    review resumes. It was suggested that feedback be incorporated against actual
    vendor tools, not just relying on whether the resume writer edited the resume
    further after getting adjusted by the AI. The correct analytic focuses on whether
    the edited resume passed screeners and resulted in the applicant making it to
    the next step. *Asking the right question and building the right metric* *is key.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes, revealing more details of what is happening behind the scenes is
    helpful.
  prefs: []
  type: TYPE_NORMAL
- en: Exposing ChatGPT when it is working behind the scenes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The exciting thing about ChatGPT is it can appear anywhere. The user typically
    doesn’t need to know the details of a behind-the-scenes solution. Usually, details
    are not exposed, but there can be some exceptions in AI cases. It is generally
    accepted as good behavior not to pretend to be human. In the case of a chat UI,
    the LLM doesn’t impersonate a human. Customers are told they are talking to a
    virtual assistant. It can also benefit the user to know this; they might be more
    forthcoming and will know they will get a quick response. In a hybrid experience
    that includes a GUI, or in the case of a Chat AI participating in a group conversation,
    it should be clear when the AI is participating, and it should be clear whether
    there are any guardrails preventing the AI from listening.
  prefs: []
  type: TYPE_NORMAL
- en: In a bot example, where a simple recommendation is offered, machine learning
    and algorithmic recommendations in UIs appeared more than a decade ago. The behind-the-scenes
    algorithms are not explained and might be proprietary (like credit scores). It
    is unnecessary to clarify whether it uses or includes results from ChatGPT. Decide
    based on the use case. Get feedback on recommendations, and this is encouraged
    to happen with some regularity. It might help and can hurt to explain some details
    if feedback is provided. Even the simple feedback mechanism could be powered by
    AI. Imagine a purpose-built model whose job is only focused on getting feedback.
    Having a direct feedback request with possible follow-up can be sufficient. One
    upside of exposing “**Powered by AI**” is that it might help engage customers
    to provide feedback. Position this as the user helping themselves because their
    feedback will be used to improve results. And folks do like being heard. Customers
    might consider this help-free consulting or be annoyed thinking the AI is in “beta.”
    However, I think this opinion will soften as users recognize the value of a well-trained
    generative AI solution.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, for all the solutions that can be invented, most enterprise ChatGPT
    solutions will be covered within one of the types of experiences discussed. There
    are a few overarching areas to consider for all experiences.
  prefs: []
  type: TYPE_NORMAL
- en: Overarching considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some topics apply more universally, regardless of the type of experience. Accessibility,
    internationalization, and security are three essential items worth discussing.
  prefs: []
  type: TYPE_NORMAL
- en: Accessibility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It should not be surprising that I call out **accessibility**, also called **A11y**
    (because who wants to type all those letters). Large multinationals tend to have
    contracts with government entities in the enterprise space, and they require A11y.
    Each country or region can have its own rules; sometimes, those roles can be broken
    if well documented, but our goal should be to make an accessible experience. Some
    forms of accessibility are more challenging than others. All customers can benefit
    from design thinking applied to A11y. For example, keyboard shortcuts and navigation
    are required for those with visual impairments, but many computer experts use
    them extensively. Closed captioning was introduced in 1979 for people who are
    hard of hearing. Still, bars have them on, and others use closed captioning to
    learn a new language or to help improve understanding because of a wide range
    of audio issues from streaming and cable services. Voice recognition was developed
    for those with mobility impairments. Without that history, Siri, Alexa, and the
    voice assistants of the world would not exist. The examples go on and on. Consider
    the use case and ensure a solid A11y solution. Sometimes, the framework, such
    as an Apple iPhone, has considerable accessibility support. So much is handled
    by the framework; lucky for your team!
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the existing standards and learn how they are applied in AI use cases.
    GUIs, chat, and voice all have A11y concerns. Even if you are in a country where
    they might not apply, you can learn much from them. We don’t want accessible experiences;
    we want *usable,* accessible experiences. The first three references have the
    broadest impact:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Web Content Accessibility** **Guidelines** (**WCAG**):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Article: [WCAG](https://www.w3.org/WAI/WCAG21/quickref/) ([https://www.w3.org/WAI/WCAG21/quickref/](https://www.w3.org/WAI/WCAG21/quickref/))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Developed by the **World Wide Web Consortium** (**W3C**), WCAG (pronounced “double
    u-cag”) is a guide for web content accessibility. The foundational guidelines
    are widely adopted and used as a basis for other standards. For example, California
    mandates WCAG 2.1 Level AA and Section 508 for their public websites. WCAG 2.0
    is also referenced as the standard ISO/IEC 40500:2012.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Section 508:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Article: [Section 508 Government standards](https://www.section508.gov/) ([https://www.section508.gov/](https://www.section508.gov/))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Section 508 outlines accessibility requirements for electronic and information
    technology used by the federal government of the United States. Compliance with
    508 is typical for more extensive enterprise solutions sold to the federal government,
    while sales to state governments might also reference this in their project requirements.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Accessible Rich Internet** **Applications** (**ARIA**):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Article: [ARIA guidelines](https://www.w3.org/WAI/standards-guidelines/aria/)
    ([https://www.w3.org/WAI/standards-guidelines/aria/](https://www.w3.org/WAI/standards-guidelines/aria/))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ARIA is a set of attributes that define ways to make web content and applications
    more accessible, especially for dynamic content and advanced user interfaces.
    This is important when doing intelligent or live updates on a page.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Americans with Disabilities** **Act** (**ADA**):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Article: [ADA website](https://www.ada.gov/) ([https://www.ada.gov/](https://www.ada.gov/))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The ADA mandates that public entities, including digital services, web applications,
    and sites, be accessible to people with disabilities. As the ADA is not a standard,
    compliance often aligns with WCAG guidelines. The guidelines specifically call
    out algorithms and AI regarding hiring practices.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Article: [AI Guidance](https://www.ada.gov/resources/ai-guidance/) ([https://www.ada.gov/resources/ai-guidance/](https://www.ada.gov/resources/ai-guidance/))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**European Standard EN** **301 549:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Article: [European Standards](https://www.etsi.org/deliver/etsi_en/301500_301599/301549/03.02.01_60/en_301549v030201p.pdf)
    ([https://www.etsi.org/deliver/etsi_en/301500_301599/301549/03.02.01_60/en_301549v030201p.pdf](https://www.etsi.org/deliver/etsi_en/301500_301599/301549/03.02.01_60/en_301549v030201p.pdf))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This European standard is based on WCAG and provides additional requirements
    to meet public procurement needs for **Information and Communication Technology**
    (**ICT**) products and services. Some of their standards, such as this one for
    “videotelephony” (video calls), are very technical:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Article: [Human Factors in Videotelephony](https://www.etsi.org/deliver/etsi_etr/200_299/297/01_60/etr_297e01p.pdf)
    ([https://www.etsi.org/deliver/etsi_etr/200_299/297/01_60/etr_297e01p.pdf](https://www.etsi.org/deliver/etsi_etr/200_299/297/01_60/etr_297e01p.pdf))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**ITU-T F.922:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Article: [ITU-T F.922 Guidelines](https://www.itu.int/rec/T-REC-F.922-202008-I/en)
    ([https://www.itu.int/rec/T-REC-F.922-202008-I/en](https://www.itu.int/rec/T-REC-F.922-202008-I/en))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The **International Telecommunication Union** (**ITU**) standard provides guidelines
    for accessible user interfaces on telecommunication devices. This specific one
    is for visually impaired persons, but other standards in the same section might
    also be valuable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It would be best to understand and, when needed, refer to the specific standards
    relevant to the target audience. If this is your first chat UI, then consider
    how a screen reader reads the conversation, how to deal with the LLM coming back
    with a response that might be delayed (how to notify the user if they are in another
    window, for example), and to make sure that any GUI elements have the same support
    as they would if they were on a web page.
  prefs: []
  type: TYPE_NORMAL
- en: If you have been building existing UIs to A11y standards, do test suites already
    exist for the models? Compliance and even exceeding mandates and standards help
    create a more inclusive experience for all users, and speaking of all users leads
    us to think about a worldwide audience.
  prefs: []
  type: TYPE_NORMAL
- en: Internationalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is another one of those long words written as **I18n** (skipping those
    annoying 18 characters between the *I* and the *n*). The world of ChatGPT has
    opened the doors to communicating with the audience in their native language.
    That is fantastic news. Can the enterprise handle working in their language? It
    isn’t just a matter of turning on translation. That might lead to a worse situation
    than if the language wasn’t enabled because it is only one piece of supporting
    the customer in that language, at least to some extent. Define what level of support
    makes sense. If there is a ChatGPT conversational solution for asking knowledge
    questions, the next step would be to support the customer’s follow-up questions
    or escalate to a live agent in that language. Many companies are not good at that.
  prefs: []
  type: TYPE_NORMAL
- en: Internationalization, the overarching development effort for language support,
    includes localizing text translation and considering locale-specific elements.
  prefs: []
  type: TYPE_NORMAL
- en: Example of using in-conversation translation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even the largest support companies sometimes have minimal multi-language support
    outside their native language. In one large multinational, even when another language
    is supported (such as the Asian languages, where English is typically not an option
    for this company), a team translates each service request. Then, the translated
    request is sent to the engineering team, and each response is translated back
    into the customer’s language. It is a game of telephone, and these intermediate
    steps cause noise and confusion.
  prefs: []
  type: TYPE_NORMAL
- en: Could this process be improved by putting a real-time machine or LLM translation
    service into the loop? Would the technical nature of the requests and conversation
    confuse the participants? This technology is coming, but getting there will be
    hard work. It is one thing to ask for directions to the train station. It is quite
    another to get into the details when asking what specific SGA parameters and configurations
    need to be adjusted or optimized to enhance the performance and throughput of
    a highly transactional Oracle database system, considering factors such as concurrent
    transactions, response time, and resource utilization. Will the translation keep
    up with the differences between `BUFFER_POOL_KEEP`, `BUFFER_POOL_RECYCLE`, `JAVA_POOL_SIZE`,
    and the eight other properties? A far cry from, “When is the next train to the
    city?”
  prefs: []
  type: TYPE_NORMAL
- en: If conversations are less technical than this crazy database example, that is
    good news. But this is why enterprise solutions are needed. We aim to build on
    the knowledge within the company and presumably use this to expand communication
    capabilities in customers’ native languages. It makes it evident that monitoring,
    debugging, and improving the machine or human-in-the-loop translation services
    are critical to success.
  prefs: []
  type: TYPE_NORMAL
- en: Translating knowledge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Translating knowledge into other languages is a start. The cost was traditionally
    expensive. Each article goes through an editorial process as part of any automation;
    this allows for a high-quality experience. Technically savvy translators must
    build up experience to translate complex or technical material. Guidelines and
    translation dictionaries are needed to maintain consistency and quality. Even
    with all the advances in automation, deciding the quality and cost trade-off threshold
    is still required. Machine translation might be cheap (or free) and fast. Determine
    if it is good.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: There is an adage in the tech world, “Cheap, fast, and good, choose two.” If
    it is cheap and fast, it is likely not good. As shown in *Figure 5**.15*, it is
    probably not cheap if it is fast and good. And if it is cheap and good, it is
    probably not fast. There is an argument for design that some of these are untenable
    options. There are use cases for all of these, but they might not be best for
    the business.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_05_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 – Cheap, fast, and good, choose two
  prefs: []
  type: TYPE_NORMAL
- en: Everyone wants cheap, fast, and good. It seems to be a unicorn, magical and
    non-existent. Please prove me wrong, gladly. I have always considered this adage
    a truth for product development. Let me share Benek Lisefski’s counterpoint for
    some additional reading on this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [The Big Lie of Good, Fast, and Cheap](https://solowork.co/story/the-big-lie-of-good-fast-cheap)
    ([https://solowork.co/story/the-big-lie-of-good-fast-cheap](https://solowork.co/story/the-big-lie-of-good-fast-cheap))'
  prefs: []
  type: TYPE_NORMAL
- en: Pick your battles. There are fast improvements in LLM-based translation, and
    the cost of quality translation is falling rapidly. Automate or allow some of
    this on the fly, but always check or spot-check the results. At least for popular
    articles, pay for professional review and editing even if the LLM did the primary
    translation. However, for less popular articles or lesser-used languages, could
    it be enough to let the user know this article has been offered in their language
    but still might have errors? And don’t translate an article more than once. If
    the system translates on a pay-as-you-go approach for older articles, cache the
    results so you don’t have to hit the model again the next time the article is
    requested. Automation can help batch translate and reduce costs, but because of
    the complex nature of the company languages, this process requires checks and
    balances. Here is an approach to using LLMs in translation situations.
  prefs: []
  type: TYPE_NORMAL
- en: How to decide what languages to support
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes, the decision is made for you. A salesperson pre-sells the product
    in a specific language. I have seen this happen; the customer is still waiting
    years later. It is easier said than done. This is not as common in large companies
    because no one deal is typically big enough to force this hand. However, this
    can be a deal breaker in smaller companies, or it is mandated to enter a new territory
    because of a strategic partnership or country-specific laws. If you are involved
    in deciding what languages to support and how much to help them, here is an outline
    to get started. Adjust as needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four buckets to define: primary, fully, partially, and not supported.
    The company has a primary language, the base languages for the company; English
    is used for this example. A process is needed to define each resource the customers
    will touch, the cost of support, and the quality goals for that language. It is
    tough to achieve close to native quality. The cost to professionally translate
    technical knowledge might be prohibitive using traditional methods, but this would
    ensure that complex descriptions are explained correctly. Having native-speaker-level
    language quality in more than a few languages is challenging. ChatGPT’s translation
    cost is multiple orders of magnitude less expensive, maybe achieving 80–90% of
    the value with the proper monitoring and fine-tuning. Is that worth being able
    to expand markets?'
  prefs: []
  type: TYPE_NORMAL
- en: Some languages would have partial support; some things won’t be done in those
    languages. And there will always be languages that are not supported.
  prefs: []
  type: TYPE_NORMAL
- en: All of this depends on the languages the LLM supports. This is still a maturing
    space. Although many models talk about supporting dozens of languages, training
    will improve their expertise in those languages. The quality of each language
    varies considerably.
  prefs: []
  type: TYPE_NORMAL
- en: Many pieces of the solution can include design elements for supporting languages.
    What happens when they have a ChatGPT conversational experience in German and
    then escalate to a support center that doesn’t speak German? What is the plan?
    What is the customer’s expectation? Consider how ChatGPT can support language
    needs in the customer’s journey. In *Table 5.2*, I have shown some examples (**in
    bold**) where an LLM can enhance or extend language support.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Primary Language** | **Fully Supported** **Languages** | **Partially
    Supported** **Languages** | **Not Supported** **Languages** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Use Case** **for User** | Expects high-quality responses in their native
    language with the most up-to-date language, details, and tone | Expects good quality
    experiences with localization | The least popular language should get the best
    effort from machine translation |'
  prefs: []
  type: TYPE_TB
- en: '| **Percent of** **User Base** | Primary | Greater than 5% of I18n users |
    From 1% to 5% of I18n users | Less than 1% of I18n users |'
  prefs: []
  type: TYPE_TB
- en: '| **Language** | English | Japanese, French, German, Spanish, Italian, Brazilian-
    Portuguese | Finnish, Dutch, Hebrew, Swedish, Arabic, Chinese, Korean | Thai,
    Icelandic, Slovak, Romanian |'
  prefs: []
  type: TYPE_TB
- en: '| **User Interface &** **Help** | Complete | Complete, Professionally Translated
    | **Complete, LLM translated with a** **fine-tuned model** | **Auto-Translated
    with LLMs, cached** **after translated** |'
  prefs: []
  type: TYPE_TB
- en: '| **Knowledge** | Complete, Active Updates | Key Knowledge, Quickly Updated
    | **LLM-based batch translation,** **with fine-tuning** | **Auto-Translated with
    LLMs, cached** **after translated** |'
  prefs: []
  type: TYPE_TB
- en: '| **Conversational** **Assistant** | Full Support | Full Support | LLM Based
    on Translated Sources | **Real-time LLM translation (****if available)** |'
  prefs: []
  type: TYPE_TB
- en: '| **Tier** **1 Agents** | Native Speakers | Native Speakers | Supported with
    Translation | Not Available |'
  prefs: []
  type: TYPE_TB
- en: '| **Tier** **2 Agents** | Native Speakers | Limited, **LLM Support** | Very
    Limited, **LLM Support** | Not Available |'
  prefs: []
  type: TYPE_TB
- en: '| **Milestones** | Timely | Quickly, by Q2 | Slower, by EOY | Not Supported
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Cost** | N/A | The incremental backlog cost is 1.25M, 100K/Qtr. ongoing
    | The incremental KnowledgeBase cost is 125K and 15K/Qtr for new material | UI
    language cost is new and is about 15K per language |'
  prefs: []
  type: TYPE_TB
- en: '| **Extenuating Circumstances** **and Notes** |  | Contractually obligated
    to Arabic Tier 1 in Dec 2026 | Dutch customers mostly speak English, so defer
    | Romanian agents speak supported European or English languages |'
  prefs: []
  type: TYPE_TB
- en: Table 5.2 – Hypothetical comparison of support levels and strategic plan for
    languages
  prefs: []
  type: TYPE_NORMAL
- en: This is just an example of how to consider the impact of language support. This
    table is hypothetical. Clone it, expand it, and consider the implications for
    each box on the business language strategy. When designing for I18n, there are
    also details to work out. Let’s explore a few considerations.
  prefs: []
  type: TYPE_NORMAL
- en: Accounting for I18n when designing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most significant issues for internationalization likely comes from
    the knowledge base. Is the content ready to be translated? Is it organized in
    a way that lends itself to translation? Will culturally specific idiomatic phrases
    fail automated translation? Consider *garbage in, garbage out* (even that phrase
    is idiomatic). It is incredible how well the current models understand idioms.
    Chat GPT continues to impress me with its results.
  prefs: []
  type: TYPE_NORMAL
- en: What happens if I ask you to translate
  prefs: []
  type: TYPE_NORMAL
- en: an idiom like "Your back is up against
  prefs: []
  type: TYPE_NORMAL
- en: the wall" into German? Will it be
  prefs: []
  type: TYPE_NORMAL
- en: a literal translation, or can you
  prefs: []
  type: TYPE_NORMAL
- en: translate the idiomatic meaning?
  prefs: []
  type: TYPE_NORMAL
- en: If you ask me to translate an idiom
  prefs: []
  type: TYPE_NORMAL
- en: like "Es geht gleich los runter"
  prefs: []
  type: TYPE_NORMAL
- en: into German, I will provide a translation
  prefs: []
  type: TYPE_NORMAL
- en: that conveys the idiomatic meaning rather
  prefs: []
  type: TYPE_NORMAL
- en: than a literal translation. The
  prefs: []
  type: TYPE_NORMAL
- en: idiomatic meaning of "Your back is up
  prefs: []
  type: TYPE_NORMAL
- en: against the wall" is that someone is
  prefs: []
  type: TYPE_NORMAL
- en: in a difficult or desperate situation
  prefs: []
  type: TYPE_NORMAL
- en: with few or no options left.
  prefs: []
  type: TYPE_NORMAL
- en: 'In German, a common equivalent idiom is:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Du bist mit dem Rücken zur Wand."'
  prefs: []
  type: TYPE_NORMAL
- en: This phrase captures the same sense
  prefs: []
  type: TYPE_NORMAL
- en: of being in a difficult, constrained situation.
  prefs: []
  type: TYPE_NORMAL
- en: 'ChatGPT would take something like, “It’s raining cats and dogs.” Instead of
    translating it literally, it understands that it is raining heavily. It translates
    it more accurately using the German word for streams: “It’s raining in streams
    (heavily)” (Es regnet in Strömen). In Knowledge bases, we hope not to see these
    problems, but it is challenging to avoid idioms. I have used dozens in this book,
    and I feel it makes it more fun. But I have done my readers a disservice if they
    don’t translate well. Related to idioms is jargon.'
  prefs: []
  type: TYPE_NORMAL
- en: Jargon doesn’t translate well
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remember that some terms in your world might be well understood in other languages
    because they use the same word and don’t translate it. But don’t count on that
    as a strategy. Consider how technical jargon, shortcuts, acronyms, and initialisms
    play out in other languages. For example, *PIN* is short for *Personal Identification
    Number* for an ATM. In Germany, the Geldautomat still uses a *Persönliche* *Identifikationsnummer*,
    and some form of PIN and ATM are typically understood. However, if the company’s
    vocabulary is not universal, account for it in the models, knowledge, or dictionaries.
    Related to this is how to speak to customers. It would be best to learn how to
    address them. Handling language jargon is slightly different than handling technical
    jargon. It might be well-known in *Database* circles to talk about **DB** or **RDBMS**
    (which stands for **Relational Database Management System**), so those might translate
    just as the letters, but generic language jargon should be avoided. [*Chapter
    7*](B21964_07.xhtml#_idTextAnchor150)*, Prompt Engineering,* helps address this
    to control the LLMs output. It is hard to catch these issues because we are so
    used to language. *Not seeing the forest for the trees* can be a challenge. Oops!
    There I go, using an idiomatic expression without even noticing.
  prefs: []
  type: TYPE_NORMAL
- en: UI issues in LLM solutions can be addressed just like traditional GUIs. Topics
    such as right-to-left language and icon direction, using colors not to offend
    certain groups, and even removing culturally offensive imagery can be addressed.
    Still, thousands or millions of articles could generate a profound amount of rework
    to get them ready for translation. This is a cost hog.
  prefs: []
  type: TYPE_NORMAL
- en: As designers and product leaders, we control the user experience, but we know
    those upstream knowledge sources will likely significantly impact it. So, spend
    the time making those ready for an international audience. Let’s review a few
    more language-related areas, starting with punctuation.
  prefs: []
  type: TYPE_NORMAL
- en: Punctuation and grammar
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A funny story can make a point.
  prefs: []
  type: TYPE_NORMAL
- en: '*A panda walks into a bar. He orders a sandwich, eats it, then draws a gun
    and fires two shots in* *the air.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*“Why? Why are you behaving in this strange, un-panda-like fashion?” asks the
    confused waiter as the panda walks towards the exit. The panda produces a badly
    punctuated wildlife manual and tosses it over* *his shoulder.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*I’m a panda,” he says at the door. “Look it up.” The waiter turns to the relevant
    entry and, sure enough, finds* *an explanation.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*“Panda. Large black-and-white bear-like mammal native to China. Eats, shoots
    and leaves.”* – Lynne Truss (*Eats, Shoots &* *Leaves*)'
  prefs: []
  type: TYPE_NORMAL
- en: We do not control ChatGPT. So how does this help? Of course, each response from
    ChatGPT is not controlled by humans. Quality can be expected by ensuring the resources
    given are well written and monitoring its output to verify that it works as expected.
    We can’t expect ChatGPT to return a quality answer if the content shared is not
    written well. If it wasn’t clear, the statement should have been punctuated to
    explain that the panda “eats (bamboo) shoots and (tree) leaves.”
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If writing is not your strong suit, go beyond the required manuals of style
    reading. The book is also included in the online library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Book: [Eats, Shoots & Leaves: The Zero Tolerance Approach to Punctuation](https://amzn.to/3M7x8DH)
    by Lynne Truss ([https://amzn.to/3M7x8DH](https://amzn.to/3M7x8DH))'
  prefs: []
  type: TYPE_NORMAL
- en: Grammar also applies to recommendations UIs. An LLM can be used behind the scenes
    to generate the data or text to insert into recommendations. Grammar and other
    issues, such as plurality, come into play. Consider the length of the messages
    and communications when writing these statements or messaging prompts to write
    well-written prose.
  prefs: []
  type: TYPE_NORMAL
- en: Length of labels and strings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Test UIs for text expansion. For example, if ChatGPT writes a notification,
    account for the length of the message. It is not unreasonable to see more than
    a 200% increase in word length when coming from English. The button *Edit,* in
    German, uses the word Bearbeiten. It is 150% longer. In Hungarian, Szerkesztése,
    clocks in at 200% longer. With ChatGPT powering a recommendation engine, ensure
    the UI has enough room to write the English and translated versions. Prompt engineering
    can be used to create instructions that limit the size of responses. In *Table
    5.3*, the Hungarian translation is up to 53% longer. That is not too bad.
  prefs: []
  type: TYPE_NORMAL
- en: '| **String** | **Language Details** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Tip: Edit the bills for this Employee ID by the end of the day, or the
    state penalties** **will accrue.** | English100 Characters |'
  prefs: []
  type: TYPE_TB
- en: '| **Tipp: A nap végéig szerkeszd meg ennek az alkalmazott azonosítójához tartozó
    számlákat, különben az állami** **büntetések felhalmozódnak.** | Hungarian133
    Characters |'
  prefs: []
  type: TYPE_TB
- en: '| **Tipp: Szerkeszd meg ennek az alkalmazottnak az azonosítójához tartozó számlákat
    a nap végéig, különben állami** **büntetések keletkeznek.** | Hungarian (Google)136
    Characters |'
  prefs: []
  type: TYPE_TB
- en: '| **Tipp: Szerkessze meg a számlákat ezzel a Munkavállalói Azonosítóval a nap
    végéig, különben az állami** **büntetések felhalmozódnak.** | Hungarian (ChatGPT
    3.5)147 Characters |'
  prefs: []
  type: TYPE_TB
- en: '| **Tipp: Szerkessze meg a számlákat az alábbi Munkavállalói azonosítóhoz a
    nap végéig, különben az állami bírságok** **emelkedni fognak.** | Hungarian (ChatGPT
    4o)153 Characters |'
  prefs: []
  type: TYPE_TB
- en: '| 请注意：请在今天结束之前编辑此雇员ID的账单，否则将会累积州的罚款。 | Chinese34 Characters |'
  prefs: []
  type: TYPE_TB
- en: '| **Hinweis: Bearbeiten Sie die Rechnungen für diese Mitarbeiter-ID bis zum
    Ende des Tages, sonst werden staatliche** **Strafen anfallen.** | German (ChatGPT
    4o)142 Characters |'
  prefs: []
  type: TYPE_TB
- en: Table 5.3 – Comparison of English to Hungarian, Chinese, and German translations
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, the opposite is true, and another language can be shorter. In Chinese,
    as shown, the character count is 34, but the double-byte character set takes up
    more space. Still, the issue with wrapping and truncation typically occurs with
    longer text. For messages that are sentence length or longer, account for at least
    50% growth; for shorter items such as labels, consider 100–200%, depending on
    the languages supported. A reasonable guideline is a minimum of 30% plus two extra
    characters. However, if the design can accommodate larger text, there is no harm
    in most cases. If there are tables where excessive wrapping would result, consider
    creating layouts for the other languages to accommodate their unique situation.
    For A11y, consider customers zooming in and scaling text, which can also make
    wrapping and truncation an issue. This can also impact how fields appear with
    their text. In some languages, such as German, compound words have unique rules
    for where to wrap. For example, in a human resource solution, there are labels
    for absence batch parameters, so when an absence job (vacation, holidays, or sick
    time) was run on a schedule, it knew the properties to use. The translation from
    an AI can vary dramatically:'
  prefs: []
  type: TYPE_NORMAL
- en: Absence batch parameters (English)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abwesenheits-Sammelverarbeitungsparameter (literal in German)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fehlzeitenerfassung im Stapel-Modus Einstellungen (descriptive version)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Einstellungen für die Stapelverarbeitung von Abwesenheitszeiten (descriptive
    focusing on the batch process)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stapelverarbeitung von Abwesenheitszeiträumen (common in HR software)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mitarbeiter-Abwesenheiten Sammelbearbeitung (emphasis on the employee)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A human translator might settle on Abwesenheitsbatchparameter; notice there
    are no spaces, so wrapping in a UI might be an issue. If words are not wrapped
    correctly for the language, the customer will view it as a bug.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Consider whether abbreviations can cause displays to be off. Not all languages
    have the concept of abbreviations. Understand how to translate these as whole
    words and have room for them in Chinese or Arabic. Some languages might not recognize
    a character as a word to translate (“#” for number, “Q1” for quarter one, “ft”
    (feet) might be better calculated in meters outside of the USA, “info” might not
    translate as “information,” etc.). FYI, GPT 4o-mini understands *#* and *Q1* in
    translations and converts them correctly to *number* and *first quarter*.
  prefs: []
  type: TYPE_NORMAL
- en: Leave ample space to avoid using ellipses with truncation. Ellipses must be
    used if a word is truncated, but try to prevent truncation. Be sure the user can
    read the entire label or text. I have seen many UIs that show truncation with
    ellipses (**…**), but there is no way to read what is there; not all components
    have a tooltip. And worse, when translated, the order of words can be different.
    “**Customer Number**” might be truncated in English as “**Customer N..**” leaving
    helpful information. In Spanish, the truncated version of “**Número de cliente**”
    is “**Número de ..**”, leaving nothing of value in the visible label. That is
    a failure for the user experience and frustrating for the customer.
  prefs: []
  type: TYPE_NORMAL
- en: I created a simple Google sheet, as shown in *Figure 5**.16*, to test length
    issues. Enter a string into the spreadsheet, and the tool will highlight the languages
    with much larger lengths. This is good for understanding how small words or phrases
    can grow in length. This is not perfect, as this is Google’s translation, but
    it is fast and cheap. Clone this for your purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tool: [Translation length tester](https://docs.google.com/spreadsheets/d/1P-FLn8Kc4wcOgFbUfGuCamAL7O6IXvF_qkDAhvS8QdA/edit#gid=0)
    ([https://docs.google.com/spreadsheets/d/1P-FLn8Kc4wcOgFbUfGuCamAL7O6IXvF_qkDAhvS8QdA/edit#gid=0](https://docs.google.com/spreadsheets/d/1P-FLn8Kc4wcOgFbUfGuCamAL7O6IXvF_qkDAhvS8QdA/edit#gid=0))'
  prefs: []
  type: TYPE_NORMAL
- en: If you are using an LLM for UI text, make sure there is plenty of room in the
    UI for these long translations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_05_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.16 – Example of the Google sheet you can use for quick length checking
  prefs: []
  type: TYPE_NORMAL
- en: Concatenation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One great trick in ChatGPT could be to combine a defined statement template
    with the output from ChatGPT. In examples such as “`It is recommended to " + <ChatGPT
    response>`, or `"Refill the shipping container by " + <ChatGPT response of a time>
    + " today."`, this will experience translation problems. The structure of a sentence
    in English is not the same for other languages. This static part of the text might
    be at the end of the sentence. Break up the answer into two distinct pieces. Create
    the recommendation title with static text and then generate the *entire* recommendation
    from the model. In these examples, put that recommendation in a box labeled `Recommended
    Actions`. Or label the time `Today's Refill Container Deadline` so the output
    “`3:45 PM`” stands alone.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Ask ChatGPT to translate the entire string. That is better than writing code
    to create the formatting for each language.
  prefs: []
  type: TYPE_NORMAL
- en: Let me explore the complexities of translation a little more. Plurals are one
    of the most well-known language problems.
  prefs: []
  type: TYPE_NORMAL
- en: Plurals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Plurals are one of the more complex issues with translations. With a chat experience,
    ChatGPT handles this, and this topic can be skipped. However, if the bot is recommended
    with *templates* to fill in pieces of text coming from ChatGPT, this situation
    can get complex. In English, there are simple rules; we can use a notification
    alerting the user to their vacation balance as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Zero: You have no days off remaining.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One: You have 1 day off remaining.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have one day off remaining. (alternative)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many: You have 23 days off remaining.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: English has three cases for plurality. A special rule for zero is useful (so
    the cleaner textual *no* can be used instead of a zero). The singular word *day*
    is used for one, and all other numbers use plural *days*.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is easy to write a rule in software to account for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This could work for negative balances, but a good UX person would get one more
    condition added. This gets more complex when doing this logic in other languages
    because many languages have different rules. To accommodate languages, there are
    software and standards to account for zero, one, two, a few, many, and other use
    cases. In addition to one noun form for the singular and another noun form for
    the plural, as in English, different languages have unique noun forms when the
    count of the object is two (“dual”) or a few (“paucal”), or when the number ends
    in one or ends in zero… There are many special rules! As shown in the preceding
    example, spell words for small numbers (1 to 9) in English. This can make every
    solution grammatically correct but more effort to code. To go deeper into the
    complexity of using numbers, see this reference:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [When to spell out numbers](https://www.masterclass.com/articles/when-to-spell-out-numbers-explained)
    ([https://www.masterclass.com/articles/when-to-spell-out-numbers-explained](https://www.masterclass.com/articles/when-to-spell-out-numbers-explained))'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, if ChatGPT outputs a complete thought, it should follow the correct
    rules. Work is needed to create a hybrid experience where the output includes
    numbers in static messages, such as recommendation text. The resources listed
    later in this section provide some more detailed explanations. Here is an example
    of English and Czech, which is not even that complex.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This format shows how the translated strings support the corresponding plural
    forms. Don’t expect to inject numbers directly into ChatGPT responses. Either
    let it do it correctly or write the code. *Table 5.4* shows one end-to-end example,
    including what not to do, what to do, and how it will look when done.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Don’t** **Do This** | **Readable &** **Translatable** | **Example Output**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| I’m ready to submit your invoice with {0} and {1}. What should we name the
    invoice? | I’m ready to submit your report with {NUM_ITEMS, plural, =1 {1 item
    for} one {# items totaling} other {# items totaling}} {AMOUNT} {CURRENCY} and
    {NUM_ITEMS1, plural, one {# foreign currency items} other {# foreign currency
    items}}. What should we name the invoice? | I’m ready to submit your invoice with
    5 items totaling 140 USD and 1 foreign currency item. What should we name the
    invoice?I’m ready to submit your invoice with five items totaling 140 USD and
    one foreign currency item. What should we name the invoice? |'
  prefs: []
  type: TYPE_TB
- en: Table 5.4 – Example of a string to be written in ICU format for translation
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the variables are readable, not just 0s and 1s. Do this to validate
    that the statements are readable quickly. Good design thinking extends into the
    code sometimes. Whether these variables are numbers or words must be known to
    translate or validate. The specific number needs to be known if they are numbers
    so the code can generate the correct format. Notice I provided two example outputs,
    one using numbers and one using number words (the written form). I prefer numerals
    for numbers below 10 in UIs, even though they are not grammatically correct in
    English. It helps these values stand out and prevents the user from having to
    read the entire text. People don’t read!
  prefs: []
  type: TYPE_NORMAL
- en: Additional references for plural resources
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Pluralization](https://lingohub.com/blog/pluralization) ([https://lingohub.com/blog/pluralization](https://lingohub.com/blog/pluralization))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: Plural [Rules for Internationalization](https://cldr.unicode.org/index/cldr-spec/plural-rules)
    ([https://cldr.unicode.org/index/cldr-spec/plural-rules](https://cldr.unicode.org/index/cldr-spec/plural-rules))'
  prefs: []
  type: TYPE_NORMAL
- en: How to handle caps in design may be less exciting and less well-known. This
    is not specific to conversational AI, but it is still helpful to realize when
    someone wants to use them in recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: ALL CAPS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Don’t ask ChatGPT to return output in ALL CAPS. To format for emphasis, use
    **Bold**, for example. Not only are all caps harder to read in English, but there
    are also languages where this is not acceptable (French and Greek) and has no
    equivalent in some languages (Chinese and Arabic). Avoid this problem by not using
    all caps and telling the LLM to avoid it. STRINGS IN UPPERCASE ARE SIGNIFICANTLY
    HARDER TO READ. It is not suggested even for titles, tables, or labels. Use Title
    Case, as used in newspaper headlines. For database fields that are stored in uppercase,
    change it, edit the strings, or, as a last resort, force them to be lowercase.
    The 5–10% improvement in readability is worth it.
  prefs: []
  type: TYPE_NORMAL
- en: Work and letter recognition
  prefs: []
  type: TYPE_NORMAL
- en: 'Learn more about some of the research on word and letter recognition:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [The Science of Word Recognition](https://learn.microsoft.com/en-us/typography/develop/word-recognition)
    ([https://learn.microsoft.com/en-us/typography/develop/word-recognition](https://learn.microsoft.com/en-us/typography/develop/word-recognition))'
  prefs: []
  type: TYPE_NORMAL
- en: Accounting for locales
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Designing for locales involves considering how a specific region or country
    handles something. For example, if building a scheduling service and someone asks,
    “*Book a hotel for this weekend*,” what nights would that booking be for? In the
    US, it would be for Friday and Saturday nights. Makes sense? However, different
    countries have different definitions of the weekend. In some Muslim countries
    in the Middle East, it would be for Thursday and Friday, while in Iran, the weekend
    is only Friday night, as they observe a six-day work week. ChatGPT understands
    these concepts to some extent, but make sure integrations also have this understanding.
    Many ideas and words can be locale-specific. For example, when working on the
    design of an expense assistant, it understands the expense of a car, taxi, Uber,
    Lyft, Limo, etc. But there are Didi, BlaBlaCar, Cabify, and many other services
    in various countries. Ensuring models support the locale-specific concepts will
    make the experience more robust. Sometimes, it is not just a country-specific
    product or service; it could be jargon covered earlier in the chapter. If the
    UI will handle understanding the customer’s work week, it should likely learn
    how to address the customer properly.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing a customer and vocative case
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many UIs start with “Hello, Max!” or “Welcome, Madison.” A direct translation
    will not work because, in some cultures, addressing someone by their first name
    is inappropriate.
  prefs: []
  type: TYPE_NORMAL
- en: My friend shared an excellent example of how formal or complex it is to speak
    to the customer in their name. I have shared this in *Table 5.5*. So be sure to
    have enough information to accommodate their first name, last name, or various
    forms of their names (the fancy term is vocative declension), depending on the
    language.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Language** | **Problem** | **Basic Name** | **Example** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **English** | None | Mark Stevens | Hello, Mark |'
  prefs: []
  type: TYPE_TB
- en: '| **Japanese** | Requires last name + honorific | Yoko OnoAkira Kurosawa |
    Hello, **Ono-san**Welcome, **Kurosawa-san** |'
  prefs: []
  type: TYPE_TB
- en: '| **Arabic****(Latvian,** **Lithuanian, Vietnamese)** | Prefers polite address
    | Mohamed Hassan | Hello Mr. Mohamed (suitable for a welcome screen)Dear **Mr.**
    Mohamed,Alt: Dear **Mr. Hassan** (for email) |'
  prefs: []
  type: TYPE_TB
- en: '| **Czech, Polish****(Greek,** **Latvian, Lithuanian)** | Requires vocative
    declension | Petr**a** LukášMar**ek** Martin | Vitejte, Petr**o**Dobrý den, Lukáš**i**Milý
    Mar**ku**Martin**e**, jak vám mohu pomoci?(notice o, i, u, and e at name ends)
    |'
  prefs: []
  type: TYPE_TB
- en: Table 5.5 – Direct address in various languages that can cause issues
  prefs: []
  type: TYPE_NORMAL
- en: 'If all else fails, *don’t use it at all, or don’t use it in those languages*.
    One workaround is to use an email address. I don’t recommend it. It is better
    to say “Hello” than “Hello, jill@miller.com.” Software can be used to build declensions
    (changes to the ending of words, in this case, for the person’s name) for specific
    languages. I asked ChatGPT to do it for Hungarian, and it generated Python code.
    It doesn’t have to be a design problem; it is something to be aware of when using
    people’s names in the experience:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: One nice thing about a ChatGPT solution is that it can ask how they want to
    be addressed. Even if ChatGPT can’t remember this from one session to the next,
    store it in a setup variable and then remind ChatGPT how to address this customer.
    ChatGPT will see enterprise data with people’s names when helping to compose customer
    emails. If working in an international arena, ensure this is handled correctly.
    Addressing the customers is not the only cultural-specific issue.
  prefs: []
  type: TYPE_NORMAL
- en: Cultural and bi-directional hiccups
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If customers use Arabic or Hebrew, be reminded that responses that include images
    or icons might not work as well or as intended without a flipped version. These
    languages are read from right to left. If an arrow is a pointer to a list of items
    (or any icon representing forward motion), it will look odd, with the arrow pointing
    the wrong way. Icons such as text bubbles, arrows, toggle icons, audio controls,
    pagination, progress indicators, and drop-down menu icons must be orientated for
    reading from right to left. There are issues with some icons in some cultures.
    These issues still apply in conversational AI. Thumbs up in some cultures is offensive.
    I learned that the hard way!
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Cultural design](https://material.io/blog/localization-principles-techniques)
    ([https://material.io/blog/localization-principles-techniques](https://material.io/blog/localization-principles-techniques))'
  prefs: []
  type: TYPE_NORMAL
- en: 'A deeper dive into the bi-directional issues can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Bidirectionality issues](https://m2.material.io/design/usability/bidirectionality.html#mirroring-layout)
    ([https://m2.material.io/design/usability/bidirectionality.html#mirroring-layout](https://m2.material.io/design/usability/bidirectionality.html#mirroring-layout))'
  prefs: []
  type: TYPE_NORMAL
- en: Be aware of cultural differences when generating solutions with ChatGPT. Use
    prompts and verify culturally appropriate answers with native speakers or a trusted
    model. The more the design supports these small things, the more customer trust
    is built.
  prefs: []
  type: TYPE_NORMAL
- en: Trust
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have all heard about trust issues. If the AI provides bad or wrong data,
    this can quickly derail a conversation and cause the customer to lose trust in
    the AI. This is the same as when working with a human agent. As soon as a human
    agent says something so silly and wrong, it derails the conversation, and the
    tone of the interaction will immediately change. The goal is to mitigate these
    errors and issues. This next part might feel like a TV show, but here are the
    top 10 ways to improve trust in a ChatGPT experience, in reverse order.
  prefs: []
  type: TYPE_NORMAL
- en: 10\. Regular updates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When appropriate, let folks know the staleness of results. For example, if they
    ask a time-dependent question that can be easily changed based on recent information,
    tell them when the model was last updated. “*I see that the answer to this question
    has changed repeatedly, but as of July 5, 2024, it is…*”, and “*My information
    is up to date as of last week…*”. Do not let customers think they have the most
    current answer; they will act on that. It would be better not to answer at all.
    Even recommendations might need some supporting info to be clear that the recommendation
    is based on the latest information. This also applies to backend services. Sometimes,
    it is worth showing what data was last ingested or the last update time and name
    of the file. [*Chapter 7*](B21964_07.xhtml#_idTextAnchor150), *Prompt Engineering*,
    can address this. These little things can help build trust.
  prefs: []
  type: TYPE_NORMAL
- en: 9\. Empower users
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With tools to customize the experience, make it apparent this is an option and
    how to adjust. They will trust the system differently if they feel in control
    regarding the UI, privacy, data storage, and other essential factors. This is
    about understanding and then implementing use cases that empower the user. Speaking
    in their language using terms they are comfortable with is an empowering feeling.
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Ethics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned, most enterprise use cases don’t delve into ethical concerns. However,
    in problematic areas around medical decisions, legal ramifications, lending practices,
    driving decisions, government practices, personal privacy, mental health, or other
    answer spaces where ethics is essential, know that the answers might not be the
    right choice. Avoid these topics, when possible, implement extensive safeguards,
    and consistently monitor this. Lawyers will likely get involved in providing lengthy
    disclaimers. That is the way of the world. Prompt chaining, by using one ChatGPT
    to check the work of another one’s output, can help to verify the output.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Regularly monitor and address biases in responses. Implement measures to minimize
    biased outputs and ensure fair treatment of all users, regardless of demographics.
    Consider whether the knowledge base contains these biases. [*Chapter 6*](B21964_06_split_000.xhtml#_idTextAnchor134),
    *Gathering Data – Content is King* is next and will go deeper into this subject.
    Use emerging tools to check and monitor output for bias.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Privacy and security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recall how easy it is for someone to share data with a general public model
    without realizing the implications. The more significant consideration in enterprise
    models might be at the company level. Does company A’s model impact other customers’
    models? Does the business run on a model affected by all customer engagements?
    Product owners must ensure that company-sensitive data is not making its way to
    other companies. By siloing customer data and with independent model updates,
    then a good chance of protection at the company level. This also means either
    a lot more work to help every customer’s model improve, outsourcing this improvement
    to the customer or third parties, automation, or ignoring the problem. The reason
    why so many first-generation chatbots failed is that they ignored the problem.
    Don’t fail. Put checks in place so systems can’t be manipulated to share the wrong
    data. Some platforms, such as Salesforce, cover this extensively.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring that only the right people in the company have the correct access is
    still a challenge. Just because someone works for the same company doesn’t mean
    they deserve access. Sales, acquisitions, financials, and human resources all
    demand security. Salesforce goes out of its way to remind people that “Your data
    is your data. Your data is not our product.” This would seem a pretty clear line.
    The downside can mean less competent models without sharing data across companies.
    It is possible that improvements learned in one data silo might not work in another.
    So, there is more to monitoring and improving. A real challenge to scale for the
    enterprise.
  prefs: []
  type: TYPE_NORMAL
- en: The bigger the company, the more it has to lose and the less risk it is willing
    to take. Security is a big deal, and I suggest spending the extra time and effort
    on security audits, **penetration** (**PEN**) testing, performance tuning, and
    usability testing.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. User feedback
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The general feeling of user feedback is that it goes into a black hole when
    given. Sometimes, feedback is anonymous, so it is hard to communicate directly
    with the user. Occasionally, customers are asked for follow-up or to provide their
    information if further questions are needed. One way to build trust is to listen
    to and communicate feedback. If they provided their email, do more than send automated
    emails that their feedback was sent. That doesn’t build much trust. Even this
    step could be automated with an LLM to provide a more thoughtful version of a
    “we are looking at your issue” response. A model can respond in a few ways:'
  prefs: []
  type: TYPE_NORMAL
- en: That a feature request is in the works by comparing the request to existing
    published plans
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharing existing bugs filed, comparing against existing bugs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide a link to monitor an existing bug or community threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide alternatives based on a knowledge search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All ideas support the idea that there is a loop in the feedback process. Let
    the humans do the more complex work and let the LLM assistant handle minor interactions.
  prefs: []
  type: TYPE_NORMAL
- en: For anonymous feedback, point users to documentation or help sites that collect
    and communicate about the feedback. If they see that people like them are being
    listened to, they may gain some trust in the solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is wonderful to ask users for feedback proactively. Very few customers will
    respond, and it can be unpleasant to ask, especially if the user is in the middle
    of something important. Although covered in [*Chapter 2*](B21964_02_split_000.xhtml#_idTextAnchor031)*,
    Conducting Effective User Research*, it is an excellent time to remind folks with
    some tips for gathering feedback:'
  prefs: []
  type: TYPE_NORMAL
- en: Don’t require feedback. Don’t request feedback at every turn or for every customer
    (use sampling).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep feedback simple, such as “*Did we help solve your problem today?*” Based
    on the answer, probe why not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Someone (the LLM?) must follow up if they were asked for contact information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log the context of feedback. If they are prompted while *creating* an urgent
    service request , "Is your problem solved?", the answer is no. Consider *when*
    to request feedback so relevant questions are asked.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4\. Consistent style and tone
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This book spends a lot of time on style and tone, which impact trust. Maintaining
    a consistent communication style lets customers know what is expected. However,
    this doesn’t mean the style and tone can’t be changed. If the situation calls
    for it, then the tone should be understandable for that context. Again, it becomes
    part of the instructions shared with the LLM, covered starting in [*Chapter 7*](B21964_07.xhtml#_idTextAnchor150)*,*
    *Prompt Engineering*.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Explainability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a conversational UI, this is the way, while in a GUI or bot use case, they
    might have an **Info** button, more details, or some element to *please explain*.
    I used the **Info** (i) icon for a decade next to simple recommendations to link
    to more details or provide it in a window. Sometimes, people want to know more
    details and the reasoning behind a decision. This can build trust to expose the
    reasoning; if the reasoning is sound, it can enhance that trust factor. The UI
    can use this to help guide the customer, building trust again. This is covered
    in [*Chapter 9*](B21964_09_split_000.xhtml#_idTextAnchor190)*, Guidelines* *and
    Heuristics*.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Transparency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is more important for a Chat AI than a bot or recommendation. Be clear
    communication is with an AI when in an actual conversation. When a UI pops up
    a simple recommendation or suggestion, it is a less compelling need to say precisely
    *how* this was generated. And it is likely already a combination of AI and other
    tools. An earlier section in this chapter, *Designing a recommender and behind-the-scenes
    experiences*, details communicating with an AI behind the scenes. See the section
    earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Accuracy and relevance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is a reason why this is number one. And because this is only an ordered
    list, it is not apparent how much more important this is than the nine previous
    items. If you read the previous chapter’s discussion on ranking versus scoring,
    now this might make more sense. Accuracy and relevance could be as important as
    *all the other issues combined*. Without accurate and relevant answers, the solution
    is doomed. There can be some flexibility with other trust issues, but not so much
    with wrong answers. Sometimes, the problem can be much worse if they trust it
    is the correct answer and do not understand enough to recognize it is wrong. That
    is the worst problem to have. An incorrect answer allows the customer to dismiss
    the answer, ignore the answer, and possibly leave disgruntled. However, a wrong
    answer that looks right can cause downstream repercussions based on customer decisions.
    Include the right resources to formulate answers and create test cases to detect
    issues (more on testing in [*Chapter 10*](B21964_10_split_000.xhtml#_idTextAnchor216)*,
    Monitoring and Evaluation*). LLM solutions require care and feeding to uncover
    these concerns rapidly. User feedback can only catch known errors; it won’t catch
    unknown errors. Those are hard. Build expertise to tune your radar to find unknown
    errors. The following chapters on prompt engineering and fine-tuning provide techniques
    to be accurate. Given that the goal is to build trust, security can go a long
    way to help with trust. It is logical to finish this chapter on this important
    topic.
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Security takes many forms. At a minimum, conversations should be end-to-end
    encrypted; the user already gets point-to-point encryption with the typical web
    HTTPS interaction. Custom applications should not do less, while some channels
    like SMS are already inherently insecure. Vendors make a point of promoting security
    in their apps. Consider what happens to the data within the enterprise network
    after the HTTPS connection has landed and information is shared with backend services
    or a database.
  prefs: []
  type: TYPE_NORMAL
- en: Is this data safe from prying eyes? For example, some companies don’t allow
    staff to accept credit card information over the phone. Shield this information
    from insider exploitation using a virtual agent to receive and confirm payment
    information. This might even feel safer to customers; the virtual agent can be
    used to market trust.
  prefs: []
  type: TYPE_NORMAL
- en: Consider how models are being updated and with what data. Customer data must
    be isolated in the enterprise and can’t leak into other customers’ models. This
    book is for applying UX-centric design methods to enterprise solutions; our advice
    in this space is limited to making sure the customer’s data is secure and communicating
    this detail to the customer. This is the same for any prompt engineering efforts
    where instructions must include details so the user’s prompt has context. [*Chapter
    7*](B21964_07.xhtml#_idTextAnchor150)*, Prompt Engineering,* covers this. The
    design also has to account for when this data is unavailable because the user
    is not permitted to access it. For example, create context around sales deals
    so that users can ask more detailed questions about the deal. Some salespeople
    might not have full access to the details, so consider this when framing the prompts.
  prefs: []
  type: TYPE_NORMAL
- en: The takeaway concerning security is to be the customer’s advocate and ensure
    that the conversation is safe and will not fall into the wrong hands. Not all
    data and content will be stored in a secure channel. Integrations with other services,
    or even access to some knowledge, might be limited to specific audiences, and
    LLM integration needs to account for this. Integrations become even more critical
    in the next section, which discusses the hybrid UI/chat experience issues.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learn the capabilities of the tools in your framework, or better yet, help find
    the right tools to move the user experience bar forward. Every tool has capabilities
    and limitations. For any experience, the UX implications of accessibility, trust
    security, language, and internationalization support are likely a given. We hope
    the guidance, tricks, and tips help create a **Functional, Usable, Necessary,
    and Engaging** (**FUN-E**) experience. Strive for all four attributes (remember
    it as *funny*).
  prefs: []
  type: TYPE_NORMAL
- en: It is time to build a ChatGPT solution. Use cases should be ready to develop,
    and the context of use should be known. The context of most experiences is easy
    to figure out. In many cases, the context of use is not a choice. If the company
    provides phone support, a voice-only experience is a given. This is where we change
    gears and go from UI-centric discussions about generative AI solutions to model-centric
    discussions with an eye toward UI practices. Integrating enterprise data, like
    knowledge, databases, and other services, is next.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '| ![](img/B21964_QR.jpg) | The links, book recommendations, and GitHub files
    in this chapter are posted on the reference page.Web Page: [Chapter 5 References](https://uxdforai.com/references#C5)
    ([https://uxdforai.com/references#C5](https://uxdforai.com/references#C5)) |'
  prefs: []
  type: TYPE_TB
