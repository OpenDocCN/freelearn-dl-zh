<html><head></head><body>
<div aria-label="309" epub:type="pagebreak" id="page1-10" role="doc-pagebreak"/>
<div id="_idContainer107">
<h1 class="chapterNumber"><a id="_idTextAnchor390"/><span class="koboSpan" id="kobo.1.1">8</span></h1>
<h1 class="chapterTitle" id="_idParaDest-197"><a id="_idTextAnchor391"/><span class="koboSpan" id="kobo.2.1">Evaluation and Testing</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.3.1">As we’ve discussed so far in this book, LLM agents and systems have diverse applications across industries. </span><span class="koboSpan" id="kobo.3.2">However, taking these complex neural network systems from research to real-world deployment comes with significant challenges and necessitates robust evaluation strategies and testing methodologies.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.4.1">Evaluating LLM agents and apps in LangChain comes with new methods and metrics that can help ensure optimized, reliable, and ethically sound outcomes. </span><span class="koboSpan" id="kobo.4.2">This chapter delves into the intricacies of evaluating LLM agents, covering system-level evaluation, evaluation-driven design, offline and online evaluation methods, and practical examples with Python code.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.5.1">By the end of this chapter, you will have a comprehensive understanding of how to evaluate LLM agents and ensure their alignment with intended goals and governance requirements. </span><span class="koboSpan" id="kobo.5.2">In all, this chapter will cover:</span></p>
<ul>
<li class="b lletList"><span class="koboSpan" id="kobo.6.1">Why evaluations matter</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.7.1">What we evaluate: core agent capabilities</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.8.1">How we evaluate: methodologies and approaches</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.9.1">Evaluating LLM agents in practice</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.10.1">Offline evaluation</span></li>
</ul>
<div>
<div class="note" id="_idContainer100">
<div aria-label="310" epub:type="pagebreak" id="page2-10" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.11.1">You can find the code for this chapter in the </span><code class="inlineCode"><span class="koboSpan" id="kobo.12.1">chapter8/</span></code><span class="koboSpan" id="kobo.13.1"> directory of the book’s GitHub repository. </span><span class="koboSpan" id="kobo.13.2">Given the rapid developments in the field and the updates to the LangChain library, we are committed to keeping the GitHub repository current. </span><span class="koboSpan" id="kobo.13.3">Please visit </span><a href="https://github.com/benman1/generative_ai_with_langchain"><span class="url"><span class="koboSpan" id="kobo.14.1">https://github.com/benman1/generative_ai_with_langchain</span></span></a><span class="koboSpan" id="kobo.15.1"> for the latest updates.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.16.1">See </span><a href="E_Chapter_2.xhtml#_idTextAnchor044"><em class="italic"><span class="koboSpan" id="kobo.17.1">Chapter 2</span></em></a><span class="koboSpan" id="kobo.18.1"> for setup instructions. </span><span class="koboSpan" id="kobo.18.2">If you have any questions or encounter issues while running the code, please create an issue on GitHub or join the discussion on Discord at </span><a href="https://packt.link/lang"><span class="url"><span class="koboSpan" id="kobo.19.1">https://packt.link/lang</span></span></a><span class="koboSpan" id="kobo.20.1">.</span></p>
</div>
</div>
<p class="normal"><span class="koboSpan" id="kobo.21.1">In the realm of developing LLM agents, evaluations play a pivotal role in ensuring these complex systems function reliably and effectively across real-world applications. </span><span class="koboSpan" id="kobo.21.2">Let’s start discussing why rigorous evaluation is indispensable!</span></p>
<h1 class="heading-1" id="_idParaDest-198"><a id="_idTextAnchor392"/><span class="koboSpan" id="kobo.22.1">Why evaluation matters</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.23.1">LLM agents represent a</span><a id="_idIndexMarker660"/><span class="koboSpan" id="kobo.24.1"> new class of AI systems that combine language models with reasoning, decision-making, and tool-using capabilities. </span><span class="koboSpan" id="kobo.24.2">Unlike traditional software with predictable behaviors, these agents operate with greater autonomy and complexity, making thorough evaluation essential before deployment.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.25.1">Consider the real-world consequences: unlike traditional software with deterministic behavior, LLM agents make complex, context-dependent decisions. </span><span class="koboSpan" id="kobo.25.2">If unevaluated before being implemented, an AI agent in customer support might provide misleading information that damages brand reputation, while a healthcare assistant could influence critical treatment decisions—highlighting why thorough evaluation is essential.</span></p>
<div>
<div class="note" id="_idContainer101">
<p class="normal"><span class="koboSpan" id="kobo.26.1">Before diving into specific evaluation techniques, it’s important to distinguish between two fundamentally different types of evaluation:</span></p>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.27.1">LLM model evaluation:</span></strong></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.28.1">Focuses on the raw capabilities of the base language model</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.29.1">Uses controlled prompts and standardized benchmarks</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.30.1">Evaluates inherent abilities like reasoning, knowledge recall, and language generation</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.31.1">Typically conducted by model developers or researchers comparing different models</span></li>
</ul>
</div>
</div>
<div>
<div class="note" id="_idContainer102">
<div aria-label="311" epub:type="pagebreak" id="page3-10" role="doc-pagebreak"/>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.32.1">LLM system/application evaluation:</span></strong></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.33.1">Assesses the complete application that includes the LLM plus additional components</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.34.1">Examines real-world performance with actual user queries and scenarios</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.35.1">Evaluates how components work together (retrieval, tools, memory, etc.)</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.36.1">Measures end-to-end effectiveness at solving user problems</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.37.1">While both types of evaluation are important, this chapter focuses on system-level evaluation, as practitioners building LLM agents with LangChain are concerned with overall application performance rather than comparing base models. </span><span class="koboSpan" id="kobo.37.2">A weaker base model with excellent prompt engineering and system</span><a id="_idIndexMarker661"/><span class="koboSpan" id="kobo.38.1"> design might outperform a stronger model with poor integration in real-world applications</span><a id="_idTextAnchor393"/><span class="koboSpan" id="kobo.39.1">.</span></p>
</div>
</div>
<h2 class="heading-2" id="_idParaDest-199"><a id="_idTextAnchor394"/><span class="koboSpan" id="kobo.40.1">Safety and alignment</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.41.1">Alignment in the context of</span><a id="_idIndexMarker662"/><span class="koboSpan" id="kobo.42.1"> LLMs has a dual meaning: as a process, referring to the post-training techniques used to ensure that models behave according to human expectations and values; and as an outcome, measuring the degree to which a model’s behavior conforms to intended human values and safety guidelines. </span><span class="koboSpan" id="kobo.42.2">Unlike task-related performance which focuses on accuracy and completeness, alignment addresses the fundamental calibration of the system to human behavioral standards. </span><span class="koboSpan" id="kobo.42.3">While fine-tuning improves a model’s performance on specific tasks, alignment specifically targets ethical behavior, safety, and reduction of harmful outputs.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.43.1">This distinction is crucial because a model can be highly capable (well fine-tuned) but poorly aligned, creating sophisticated outputs that violate ethical norms or safety guidelines. </span><span class="koboSpan" id="kobo.43.2">Conversely, a model can be well-aligned but lack task-specific capabilities in certain domains. </span><span class="koboSpan" id="kobo.43.3">Alignment with human values is fundamental to responsible AI deployment. </span><span class="koboSpan" id="kobo.43.4">Evaluation must verify that agents align with human expectations across multiple dimensions: factual accuracy in sensitive domains, ethical boundary recognition, safety in responses, and value consistency.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.44.1">Alignment evaluation methods must be tailored to domain-specific concerns. </span><span class="koboSpan" id="kobo.44.2">In financial services, alignment evaluation focuses on regulatory compliance with frameworks like GDPR and the EU AI Act, particularly regarding automated decision-making. </span><span class="koboSpan" id="kobo.44.3">Financial institutions must evaluate bias in fraud detection systems, implement appropriate human oversight mechanisms, and document these processes to satisfy regulatory requirements. </span><span class="koboSpan" id="kobo.44.4">In retail environments, alignment evaluation centers on ethical personalization practices, balancing recommendation relevance with customer privacy concerns and ensuring transparent data usage policies when generating personalized content.</span></p>
<div aria-label="312" epub:type="pagebreak" id="page4-10" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.45.1">Manufacturing contexts require alignment evaluation focused on safety parameters and operational boundaries. </span><span class="koboSpan" id="kobo.45.2">AI systems must recognize potentially dangerous operations, maintain appropriate human intervention protocols for quality control, and adhere to industry safety standards. </span><span class="koboSpan" id="kobo.45.3">Alignment evaluation includes testing whether predictive maintenance systems appropriately escalate critical safety issues to human technicians rather than autonomously deciding maintenance schedules for safety-critical equipment.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.46.1">In educational settings, alignment evaluation must consider developmental appropriateness across student age groups, fair assessment standards across diverse student populations, and appropriate transparency levels. </span><span class="koboSpan" id="kobo.46.2">Educational AI systems require evaluation of their ability to provide balanced perspectives on complex topics, avoid reinforcing stereotypes in learning examples, and appropriately defer to human educators on sensitive or nuanced issues. </span><span class="koboSpan" id="kobo.46.3">These domain-specific alignment evaluations are essential for ensuring AI systems not only perform well technically but </span><a id="_idIndexMarker663"/><span class="koboSpan" id="kobo.47.1">also operate within appropriate ethical and safety boundaries for their application conte</span><a id="_idTextAnchor395"/><span class="koboSpan" id="kobo.48.1">xt.</span></p>
<h2 class="heading-2" id="_idParaDest-200"><a id="_idTextAnchor396"/><span class="koboSpan" id="kobo.49.1">Performance and efficiency</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.50.1">Like early challenges in</span><a id="_idIndexMarker664"/><span class="koboSpan" id="kobo.51.1"> software testing that were resolved through standardized practices, agent evaluations face similar hurdles. </span><span class="koboSpan" id="kobo.51.2">These include:</span></p>
<ul>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.52.1">Overfitting</span></strong><span class="koboSpan" id="kobo.53.1">: Where systems perform well only on test data but not in real-world situations</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.54.1">Gaming benchmarks</span></strong><span class="koboSpan" id="kobo.55.1">: Optimizing for specific test scenarios rather than general performance</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.56.1">Insufficient diversity in evaluation datasets</span></strong><span class="koboSpan" id="kobo.57.1">: Failing to test performance across the breadth of real-world situations the system will encounter, including edge cases and unexpected inputs</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.58.1">Drawing lessons from software testing and other domains, comprehensive evaluation frameworks need to measure not only the accuracy but also the scalability, resource utilization, and safety of LLM agents.</span></p>
<p class="normal"><em class="italic"><span class="koboSpan" id="kobo.59.1">Performance evaluation</span></em><span class="koboSpan" id="kobo.60.1"> determines whether agents can reliably achieve their intended goals, including:</span></p>
<ul>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.61.1">Accuracy</span></strong><span class="koboSpan" id="kobo.62.1"> in task completion across varied scenarios</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.63.1">Robustness</span></strong><span class="koboSpan" id="kobo.64.1"> when handling novel inputs that differ from evaluation examples</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.65.1">Resistance</span></strong><span class="koboSpan" id="kobo.66.1"> to adversarial inputs or manipulation</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.67.1">Resource efficiency</span></strong><span class="koboSpan" id="kobo.68.1"> in computational and operational costs</span></li>
</ul>
<div aria-label="313" epub:type="pagebreak" id="page5-10" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.69.1">Rigorous evaluations identify potential failure modes and risks in diverse real-world scenarios, as evidenced by modern benchmarks and contests. </span><span class="koboSpan" id="kobo.69.2">Ensuring an agent can operate safely and reliably across variations in real-world conditions is paramount. </span><span class="koboSpan" id="kobo.69.3">Evaluation strategies and methodologies continue to evolve, enhancing agent design effectiveness through iterative improvement.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.70.1">Effective evaluations prevent the adoption of unnecessarily complex and costly solutions by balancing accuracy with resource efficiency. </span><span class="koboSpan" id="kobo.70.2">For example, the DSPy framework optimizes both cost and task performance, highlighting how evaluations can guide resource-effective solutions. </span><span class="koboSpan" id="kobo.70.3">LLM agents benefit from similar optimization strategies, ensuring their computational </span><a id="_idIndexMarker665"/><span class="koboSpan" id="kobo.71.1">demands align with their bene</span><a id="_idTextAnchor397"/><span class="koboSpan" id="kobo.72.1">fits.</span></p>
<h2 class="heading-2" id="_idParaDest-201"><a id="_idTextAnchor398"/><span class="koboSpan" id="kobo.73.1">User and stakeholder value</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.74.1">Evaluations help </span><a id="_idIndexMarker666"/><span class="koboSpan" id="kobo.75.1">quantify the actual impact of LLM agents in practical settings. </span><span class="koboSpan" id="kobo.75.2">During the COVID-19 pandemic, the WHO’s implementation of screening chatbots demonstrated how AI could achieve meaningful practical outcomes, evaluated through metrics like user adherence and information quality. </span><span class="koboSpan" id="kobo.75.3">In financial services, JPMorgan Chase’s COIN (Contract Intelligence) platform for reviewing legal documents showcased value by reducing 360,000 hours of manual review work annually, with evaluations focusing on accuracy rates and cost savings compared to traditional methods. </span><span class="koboSpan" id="kobo.75.4">Similarly, Sephora’s Beauty Bot demonstrated retail value through increased conversion rates (6% higher than traditional channels) and higher average order values, proving stakeholder value across multiple dimensions.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.76.1">User experience is a cornerstone of successful AI deployment. </span><span class="koboSpan" id="kobo.76.2">Systems like Alexa and Siri undergo rigorous evaluations for ease of use and engagement, which inform design improvements. </span><span class="koboSpan" id="kobo.76.3">Similarly, assessing user interaction with LLM agents helps refine interfaces and ensures the agents meet or exceed user expectations, thereby improving overall satisfaction and adoption rates.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.77.1">A critical aspect of modern AI systems includes understanding how human interventions affect outcomes. </span><span class="koboSpan" id="kobo.77.2">In healthcare settings, evaluations show how human feedback enhances the performance of chatbots in therapeutic contexts. </span><span class="koboSpan" id="kobo.77.3">In manufacturing, a predictive maintenance LLM agent deployed at a major automotive manufacturer demonstrated value through reduced downtime (22% improvement), extended equipment lifespan, and positive feedback from maintenance technicians about the system’s interpretability and usefulness. </span><span class="koboSpan" id="kobo.77.4">For LLM agents, incorporating human oversight in evaluations reveals insights into decision-making processes and highlights both strengths and areas needing improvement.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.78.1">Comprehensive agent evaluation requires addressing the distinct perspectives and priorities of multiple stakeholders across the agent lifecycle. </span><span class="koboSpan" id="kobo.78.2">The evaluation methods deployed should reflect this diversity, with metrics tailored to each group’s primary concerns.</span></p>
<div aria-label="314" epub:type="pagebreak" id="page6-10" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.79.1">End users evaluate agents primarily through the lens of practical task completion and interaction quality. </span><span class="koboSpan" id="kobo.79.2">Their assessment revolves around the agent’s ability to understand and fulfill requests accurately (task success rate), respond with relevant information (answer relevancy), maintain conversation coherence, and operate with reasonable speed (response time). </span><span class="koboSpan" id="kobo.79.3">This group values satisfaction metrics most highly, with user satisfaction scores and communication efficiency being particularly important in conversational contexts. </span><span class="koboSpan" id="kobo.79.4">In application-specific domains like web navigation or software engineering, users may prioritize domain-specific success metrics—such as whether an e-commerce agent successfully completed a purchase or a coding agent resolved a software issue correctly.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.80.1">Technical stakeholders require a deeper evaluation of the agent’s internal processes rather than just outcomes. </span><span class="koboSpan" id="kobo.80.2">They focus on the quality of planning (plan feasibility, plan optimality), reasoning coherence, tool selection accuracy, and adherence to technical constraints. </span><span class="koboSpan" id="kobo.80.3">For SWE agents, metrics like code correctness and test case passing rate are critical. </span><span class="koboSpan" id="kobo.80.4">Technical teams also closely monitor computational efficiency metrics such as token consumption, latency, and resource utilization, as these directly impact operating costs and scalability. </span><span class="koboSpan" id="kobo.80.5">Their evaluation extends to the agent’s robustness—measuring how it handles edge cases, recovers from errors, and performs under varying loads.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.81.1">Business stakeholders evaluate agents through metrics connecting directly to organizational value. </span><span class="koboSpan" id="kobo.81.2">Beyond basic ROI calculations, they track domain-specific KPIs that demonstrate tangible impact: reduced call center volume for customer service agents, improved inventory accuracy for retail applications, or decreased downtime for manufacturing agents. </span><span class="koboSpan" id="kobo.81.3">Their evaluation framework includes the agent’s alignment with strategic goals, competitive differentiation, and scalability across the organization. </span><span class="koboSpan" id="kobo.81.4">In sectors like finance, metrics bridging technical performance to business outcomes—such as reduced fraud losses while maintaining customer convenience—are especially valuable.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.82.1">Regulatory stakeholders, particularly in high-stakes domains like healthcare, finance, and legal services, evaluate agents through strict compliance and safety lenses. </span><span class="koboSpan" id="kobo.82.2">Their assessment encompasses the agent’s adherence to domain-specific regulations (like HIPAA in healthcare or financial regulations in banking), bias detection measures, robustness against adversarial inputs, and comprehensive documentation of decision processes. </span><span class="koboSpan" id="kobo.82.3">For these stakeholders, the thoroughness of safety testing and the agent’s consistent performance within defined guardrails outweigh pure efficiency or capability metrics. </span><span class="koboSpan" id="kobo.82.4">As autonomous agents gain wider deployment, this regulatory evaluation dimension becomes increasingly crucial to ensure ethical operation and minimize potential harm.</span></p>
<div aria-label="315" epub:type="pagebreak" id="page7-9" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.83.1">For organizational decision-makers, evaluations should include cost-benefit analyses, especially important at the deployment stage. </span><span class="koboSpan" id="kobo.83.2">In healthcare, comparing the costs and benefits of AI interventions </span><a id="_idIndexMarker667"/><span class="koboSpan" id="kobo.84.1">versus traditional methods ensures economic viability. </span><span class="koboSpan" id="kobo.84.2">Similarly, evaluating the financial sustainability of LLM agent deployments involves analyzing operational costs against achieved efficiencies, ensuring scalability without sacrificing effecti</span><a id="_idTextAnchor399"/><span class="koboSpan" id="kobo.85.1">veness.</span></p>
<h2 class="heading-2" id="_idParaDest-202"><a id="_idTextAnchor400"/><span class="koboSpan" id="kobo.86.1">Building consensus for LLM evaluation</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.87.1">Evaluating LLM </span><a id="_idIndexMarker668"/><span class="koboSpan" id="kobo.88.1">agents presents a significant challenge due to their open-ended nature and the subjective, context-dependent definition of </span><em class="italic"><span class="koboSpan" id="kobo.89.1">good</span></em><span class="koboSpan" id="kobo.90.1"> performance. </span><span class="koboSpan" id="kobo.90.2">Unlike traditional software with clear-cut metrics, LLMs can be convincingly wrong, and human judgment on their quality varies. </span><span class="koboSpan" id="kobo.90.3">This necessitates an evaluation strategy centered on building organizational consensus.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.91.1">The foundation of effective evaluation lies in prioritizing user outcomes. </span><span class="koboSpan" id="kobo.91.2">Instead of starting with technical metrics, developers should identify what constitutes success from the user’s perspective, understanding the value the agent should deliver and the potential risks. </span><span class="koboSpan" id="kobo.91.3">This outcomes-based approach ensures evaluation priorities align with real-world impact.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.92.1">Addressing the subjective nature of LLM evaluation requires establishing robust evaluation governance. </span><span class="koboSpan" id="kobo.92.2">This involves creating cross-functional working groups comprising technical experts, domain specialists, and user representatives to define and document formalized evaluation criteria. </span><span class="koboSpan" id="kobo.92.3">Clear ownership of different evaluation dimensions and decision-making frameworks for resolving disagreements is crucial. </span><span class="koboSpan" id="kobo.92.4">Maintaining version control for evaluation standards ensures transparency as understanding evolves.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.93.1">In organizational contexts, balancing diverse stakeholder perspectives is key. </span><span class="koboSpan" id="kobo.93.2">Evaluation frameworks must accommodate technical performance metrics, domain-specific accuracy, and user-centric helpfulness. </span><span class="koboSpan" id="kobo.93.3">Effective governance facilitates this balance through mechanisms like weighted scoring systems and regular cross-functional reviews, ensuring all viewpoints are considered.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.94.1">Ultimately, evaluation governance serves as a mechanism for organizational learning. </span><span class="koboSpan" id="kobo.94.2">Well-structured frameworks help identify specific failure modes, provide actionable insights for development, enable quantitative comparisons between system versions, and support continuous improvement through integrated feedback loops. </span><span class="koboSpan" id="kobo.94.3">Establishing a “model governance committee” with representatives from all stakeholder groups can help review results, resolve disputes, and guide deployment decisions. </span><span class="koboSpan" id="kobo.94.4">Documenting not just results but the discussions around them captures valuable insights into user needs and system limitations.</span></p>
<div aria-label="316" epub:type="pagebreak" id="page8-9" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.95.1">In conclusion, rigorous and well-governed evaluation is an integral part of the LLM agent development lifecycle. </span><span class="koboSpan" id="kobo.95.2">By implementing structured frameworks that consider technical performance, user value, and organizational alignment, teams can ensure these systems deliver benefits effectively while mitigating risks. </span><span class="koboSpan" id="kobo.95.3">The subsequent sections will delve into evaluation methodologies, including concrete examples relevant to developers working with tools like La</span><a id="_idTextAnchor401"/><a id="_idTextAnchor402"/><a id="_idTextAnchor403"/><a id="_idTextAnchor404"/><a id="_idTextAnchor405"/><a id="_idTextAnchor406"/><a id="_idTextAnchor407"/><a id="_idTextAnchor408"/><a id="_idTextAnchor409"/><a id="_idTextAnchor410"/><a id="_idTextAnchor411"/><a id="_idTextAnchor412"/><a id="_idTextAnchor413"/><a id="_idTextAnchor414"/><span class="koboSpan" id="kobo.96.1">ngChain.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.97.1">Building on the foundational principles of LLM agent evaluation and the importance of establishing robust governance, we now turn to the practical realities of assessment. </span><span class="koboSpan" id="kobo.97.2">Developing reliable agents requires a clear understanding of what aspects of their behavior need to be measured and </span><a id="_idIndexMarker669"/><span class="koboSpan" id="kobo.98.1">how to apply effective techniques to quantify their performance. </span><span class="koboSpan" id="kobo.98.2">The upcoming sections provide a detailed guide on the </span><em class="italic"><span class="koboSpan" id="kobo.99.1">what</span></em><span class="koboSpan" id="kobo.100.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.101.1">how</span></em><span class="koboSpan" id="kobo.102.1"> of evaluating LLM agents, breaking down the core capabilities you should focus on and the diverse methodologies you can employ to build a comprehensive evaluation framework for your applications.</span></p>
<h1 class="heading-1" id="_idParaDest-203"><a id="_idTextAnchor415"/><span class="koboSpan" id="kobo.103.1">What we evaluate: core agent capabilities</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.104.1">At the most</span><a id="_idIndexMarker670"/><span class="koboSpan" id="kobo.105.1"> fundamental level, an LLM agent’s value is tied directly to its ability to successfully accomplish the tasks it was designed for. </span><span class="koboSpan" id="kobo.105.2">If an agent cannot reliably complete its core function, its utility is severely limited, regardless of how sophisticated its underlying model or tools are. </span><span class="koboSpan" id="kobo.105.3">Therefore, this task performance evaluation forms the cornerstone of agent assessment. </span><span class="koboSpan" id="kobo.105.4">In the next subsection, we’ll explore the nuances of measuring task success, looking at considerations relevant to assessing how effectively your agent executes its primary functions in real-world scenarios.</span></p>
<h2 class="heading-2" id="_idParaDest-204"><a id="_idTextAnchor416"/><span class="koboSpan" id="kobo.106.1">Task performance evaluation</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.107.1">Task performance forms the foundation of agent evaluation, measuring how effectively an agent accomplishes its intended goals. </span><span class="koboSpan" id="kobo.107.2">Successful agents demonstrate high task completion rates while producing relevant, factually accurate responses that directly address user requirements. </span><span class="koboSpan" id="kobo.107.3">When evaluating task performance, organizations typically assess both the correctness of the final output and the efficiency of the process used to achieve it.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.108.1">TaskBench (Shen and colleagues., 2023) and AgentBench (Liu and colleagues, 2023) provide standardized multi-stage evaluations of LLM-powered agents. </span><span class="koboSpan" id="kobo.108.2">TaskBench divides tasks into decomposition, tool selection, and parameter prediction, then reports that models like GPT-4 exceed 80% success on single-tool invocations but drop to around 50% on end-to-end task automation. </span><span class="koboSpan" id="kobo.108.3">AgentBench’s eight interactive environments likewise show top proprietary models vastly outperform smaller open-source ones, underscoring cross-domain generalization challenges.</span></p>
<div aria-label="317" epub:type="pagebreak" id="page9-8" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.109.1">Financial services applications demonstrate task performance evaluation in practice, though we should view industry-reported metrics with appropriate skepticism. </span><span class="koboSpan" id="kobo.109.2">While many institutions claim high accuracy rates for document analysis systems, independent academic assessments have documented significantly lower performance in realistic conditions. </span><span class="koboSpan" id="kobo.109.3">A particularly important dimension in regulated industries is an agent’s ability to correctly identify instances where it lacks sufficient information—a critical safety feature that requires specific evaluation protocols beyond simple accuracy measurement.</span></p>
<h2 class="heading-2" id="_idParaDest-205"><a id="_idTextAnchor417"/><span class="koboSpan" id="kobo.110.1">Tool usage evaluation</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.111.1">Tool usage </span><a id="_idIndexMarker671"/><span class="koboSpan" id="kobo.112.1">capability—an agent’s ability to select, configure, and leverage external systems—has emerged as a crucial evaluation dimension that distinguishes advanced agents from simple question-answering systems. </span><span class="koboSpan" id="kobo.112.2">Effective tool usage evaluation encompasses multiple aspects: the agent’s ability to select the appropriate tool for a given subtask, provide correct parameters, interpret tool outputs correctly, and integrate these outputs into a coherent solution strategy.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.113.1">The T-Eval framework, developed by Liu and colleagues (2023), decomposes tool usage into distinct measurable capabilities: planning the sequence of tool calls, reasoning about the next steps, retrieving the correct tool from available options, understanding tool documentation, correctly formatting API calls, and reviewing responses to determine if goals were met. </span><span class="koboSpan" id="kobo.113.2">This granular approach allows organizations to identify specific weaknesses in their agent’s tool-handling capabilities rather than simply observing overall failures.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.114.1">Recent benchmarks like ToolBench and ToolSandbox demonstrate that even state-of-the-art agents struggle with tool usage in dynamic environments. </span><span class="koboSpan" id="kobo.114.2">In production systems, evaluation increasingly focuses on efficiency metrics alongside basic correctness—measuring whether agents avoid redundant tool calls, minimize unnecessary API usage, and select the most direct path to solve user problems. </span><span class="koboSpan" id="kobo.114.3">While industry implementations often claim significant efficiency improvements, peer-reviewed research suggests more modest gains, with optimized tool selection typically reducing computation costs by 15-20% in controlled studies while maintaining outcome quality.</span></p>
<h2 class="heading-2" id="_idParaDest-206"><a id="_idTextAnchor418"/><span class="koboSpan" id="kobo.115.1">RAG evaluation</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.116.1">RAG system evaluation</span><a id="_idIndexMarker672"/><span class="koboSpan" id="kobo.117.1"> represents a specialized but crucial area of agent assessment, focusing on how effectively agents retrieve and incorporate external knowledge. </span><span class="koboSpan" id="kobo.117.2">Four key dimensions form the foundation of comprehensive RAG evaluation: retrieval quality, contextual relevance, faithful generation, and information synthesis.</span></p>
<div aria-label="318" epub:type="pagebreak" id="page10-8" role="doc-pagebreak"/>
<p class="normal"><em class="italic"><span class="koboSpan" id="kobo.118.1">Retrieval quality</span></em><span class="koboSpan" id="kobo.119.1"> measures how well the system finds the most appropriate information from its knowledge base. </span><span class="koboSpan" id="kobo.119.2">Rather</span><a id="_idIndexMarker673"/><span class="koboSpan" id="kobo.120.1"> than using simple relevance scores, modern evaluation approaches assess retrieval through precision and recall at different ranks, considering both the absolute relevance of retrieved documents and their coverage of the information needed to answer user queries. </span><span class="koboSpan" id="kobo.120.2">Academic research has developed standardized test collections with expert annotations to enable systematic comparison across different retrieval methodologies.</span></p>
<p class="normal"><em class="italic"><span class="koboSpan" id="kobo.121.1">Contextual relevance</span></em><span class="koboSpan" id="kobo.122.1">, on the other hand, examines how precisely the retrieved information matches the specific information need expressed in the query. </span><span class="koboSpan" id="kobo.122.2">This involves evaluating whether the system can distinguish between superficially similar but contextually different information requests. </span><span class="koboSpan" id="kobo.122.3">Recent research has developed specialized evaluation methodologies for testing disambiguation capabilities in financial contexts, where similar terminology might apply to fundamentally different products or regulations. </span><span class="koboSpan" id="kobo.122.4">These approaches specifically measure how well retrieval systems can distinguish between queries that use similar language but have distinct informational needs.</span></p>
<p class="normal"><em class="italic"><span class="koboSpan" id="kobo.123.1">Faithful generation</span></em><span class="koboSpan" id="kobo.124.1">—the degree to which the agent’s responses accurately reflect the retrieved information without fabricating details—represents perhaps the most critical aspect of RAG evaluation. </span><span class="koboSpan" id="kobo.124.2">Recent studies have found that even well-optimized RAG systems still show non-trivial hallucination rates, between 3-15% on complex domains, highlighting the ongoing challenge in this area. </span><span class="koboSpan" id="kobo.124.3">Researchers have developed various evaluation protocols for faithfulness, including source attribution tests and contradiction detection mechanisms that systematically compare generated content with the retrieved source material.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.125.1">Finally, </span><em class="italic"><span class="koboSpan" id="kobo.126.1">information synthesis</span></em><span class="koboSpan" id="kobo.127.1"> quality evaluates the agent’s ability to integrate information from multiple sources into coherent, well-structured responses. </span><span class="koboSpan" id="kobo.127.2">Rather than simply concatenating or paraphrasing individual documents, advanced agents must reconcile potentially conflicting information, present balanced perspectives, and organize content logically. </span><span class="koboSpan" id="kobo.127.3">Evaluation here extends beyond automated metrics to include expert assessment of how effectively the agent has synthesized complex</span><a id="_idIndexMarker674"/><span class="koboSpan" id="kobo.128.1"> information into accessible, accurate summaries that maintain appropriate nuance.</span></p>
<h2 class="heading-2" id="_idParaDest-207"><a id="_idTextAnchor419"/><span class="koboSpan" id="kobo.129.1">Planning and reasoning evaluation</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.130.1">Planning and reasoning capabilities form the cognitive foundation that enables agents to solve complex, multi-step problems that cannot be addressed through single operations. </span><span class="koboSpan" id="kobo.130.2">Evaluating these capabilities requires moving beyond simple input-output testing to assess the quality of the agent’s thought process and problem-solving strategy.</span></p>
<div aria-label="319" epub:type="pagebreak" id="page11-7" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.131.1">Plan feasibility gauges whether every action in a proposed plan respects the domain’s preconditions and constraints. </span><span class="koboSpan" id="kobo.131.2">Using the PlanBench suite, Valmeekam and colleagues in their 2023 paper </span><em class="italic"><span class="koboSpan" id="kobo.132.1">PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change</span></em><span class="koboSpan" id="kobo.133.1"> showed that GPT-4 correctly generates fully executable plans in only about 34% of classical IPC-style domains under zero-shot conditions—far below reliable thresholds and </span><a id="_idIndexMarker675"/><span class="koboSpan" id="kobo.134.1">underscoring persistent failures to account for environment dynamics and logical preconditions.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.135.1">Plan optimality extends evaluation beyond basic feasibility to consider efficiency. </span><span class="koboSpan" id="kobo.135.2">This dimension assesses whether agents can identify not just any working solution but the most efficient approach to accomplishing their goals. </span><span class="koboSpan" id="kobo.135.3">The Recipe2Plan benchmark specifically evaluates this by testing whether agents can effectively multitask under time constraints, mirroring real-world efficiency requirements. </span><span class="koboSpan" id="kobo.135.4">Current state-of-the-art models show significant room for improvement, with published research indicating optimal planning rates between 45% and 55% for even the most capable systems.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.136.1">Reasoning coherence evaluates the logical structure of the agent’s problem-solving approach—whether individual reasoning steps connect logically, whether conclusions follow from premises, and whether the agent maintains consistency throughout complex analyses. </span><span class="koboSpan" id="kobo.136.2">Unlike traditional software testing where only the final output matters, agent evaluation increasingly examines intermediate reasoning steps to identify failures in logical progression that might be masked by a correct final answer. </span><span class="koboSpan" id="kobo.136.3">Multiple academic studies have demonstrated the importance of this approach, with several research groups developing standardized methods for reasoning trace analysis.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.137.1">Recent studies (</span><em class="italic"><span class="koboSpan" id="kobo.138.1">CoLadder: Supporting Programmers with Hierarchical Code Generation in Multi-Level Abstraction</span></em><span class="koboSpan" id="kobo.139.1">, 2023, and </span><em class="italic"><span class="koboSpan" id="kobo.140.1">Generating a Low-code Complete Workflow via Task Decomposition and RAG</span></em><span class="koboSpan" id="kobo.141.1">, 2024) show that decomposing code-generation tasks into smaller, well-defined subtasks—often using hierarchical or as-needed planning—leads to substantial gains in code quality, developer productivity, and system reliability across both benchmarks and live engineering settings.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.142.1">Building on the foundational principles of LLM agent evaluation and the importance of establishing robust governance, we now turn to the practical realities of assessment. </span><span class="koboSpan" id="kobo.142.2">Developing reliable agents requires a clear understanding of what aspects of their behavior need to be measured and how to apply effective techniques to quantify their performance.</span></p>
<div aria-label="320" epub:type="pagebreak" id="page12-7" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.143.1">Identifying the core capabilities to evaluate is the first critical step. </span><span class="koboSpan" id="kobo.143.2">The next is determining how to effectively measure them, given the complexities and subjective aspects inherent in LLM agents compared to traditional software. </span><span class="koboSpan" id="kobo.143.3">Relying on a single metric or approach is insufficient. </span><span class="koboSpan" id="kobo.143.4">In the next subsection, we’ll explore the various methodologies and approaches available for evaluating agent performance in a robust, scalable, and insightful manner. </span><span class="koboSpan" id="kobo.143.5">We’ll cover the role of automated metrics for consistency, the necessity of human feedback for subjective assessment, the importance of system-level analysis for integrated agents, and how to combine these techniques</span><a id="_idIndexMarker676"/><span class="koboSpan" id="kobo.144.1"> into a practical evaluation framework that drives improvement.</span></p>
<h1 class="heading-1" id="_idParaDest-208"><a id="_idTextAnchor420"/><span class="koboSpan" id="kobo.145.1">How we evaluate: methodologies and approaches</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.146.1">LLM agents, particularly </span><a id="_idIndexMarker677"/><span class="koboSpan" id="kobo.147.1">those built with flexible frameworks like LangChain or LangGraph, are typically composed of different functional parts or </span><em class="italic"><span class="koboSpan" id="kobo.148.1">skills</span></em><span class="koboSpan" id="kobo.149.1">. </span><span class="koboSpan" id="kobo.149.2">An agent’s overall performance isn’t a single monolithic metric; it’s the result of how well it executes these individual capabilities and how effectively they work together. </span><span class="koboSpan" id="kobo.149.3">In the following subsection, we’ll delve into these core capabilities that distinguish effective agents, outlining the specific dimensions we should assess to understand where our agent excels and where it might be failing.</span></p>
<h2 class="heading-2" id="_idParaDest-209"><a id="_idTextAnchor421"/><span class="koboSpan" id="kobo.150.1">Automated evaluation approaches</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.151.1">Automated evaluation </span><a id="_idIndexMarker678"/><span class="koboSpan" id="kobo.152.1">methods provide scalable, consistent assessment of agent capabilities, enabling systematic comparison across different versions or implementations. </span><span class="koboSpan" id="kobo.152.2">While no single metric can capture all aspects of agent performance, combining complementary approaches allows for comprehensive automated evaluation that complements human assessment.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.153.1">Reference-based evaluation compares each agent output against one or more gold-standard answers or trajectories. </span><span class="koboSpan" id="kobo.153.2">While BLEU/ROUGE and early embedding measures like BERTScore / </span><strong class="keyWord"><span class="koboSpan" id="kobo.154.1">Universal Sentence Encoder</span></strong><span class="koboSpan" id="kobo.155.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.156.1">USE</span></strong><span class="koboSpan" id="kobo.157.1">) were </span><a id="_idIndexMarker679"/><span class="koboSpan" id="kobo.158.1">vital first steps, today’s state-of-the-art relies on learned metrics (BLEURT, COMET, BARTScore), QA-based frameworks (QuestEval), and LLM-powered judges, all backed by large human‐rated datasets to ensure robust, semantically aware evaluation.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.159.1">Rather than using direct string comparison, modern evaluation increasingly employs criterion-based assessment frameworks that evaluate outputs against specific requirements. </span><span class="koboSpan" id="kobo.159.2">For example, the T-Eval framework evaluates tool usage through a multi-stage process examining planning, reasoning, tool selection, parameter formation, and result interpretation. </span><span class="koboSpan" id="kobo.159.3">This structured approach allows precise identification of where in the process an agent might be failing, providing far more actionable insights than simple success/failure metrics.</span></p>
<div aria-label="321" epub:type="pagebreak" id="page13-7" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.160.1">LLM-as-a-judge approaches represent a rapidly evolving evaluation methodology where powerful language models serve as automated evaluators, assessing outputs according to defined rubrics. </span><span class="koboSpan" id="kobo.160.2">Research by Zheng and colleagues (</span><em class="italic"><span class="koboSpan" id="kobo.161.1">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</span></em><span class="koboSpan" id="kobo.162.1">, 2023) demonstrates that with carefully designed prompting, models like GPT-4 can achieve substantial agreement with human evaluators on dimensions like factual accuracy, coherence, and relevance. </span><span class="koboSpan" id="kobo.162.2">This approach can help evaluate subjective qualities that traditional metrics struggle to capture, though researchers emphasize the importance of human verification to mitigate</span><a id="_idIndexMarker680"/><span class="koboSpan" id="kobo.163.1"> potential biases in the evaluator models themselves.</span></p>
<h2 class="heading-2" id="_idParaDest-210"><a id="_idTextAnchor422"/><span class="koboSpan" id="kobo.164.1">Human-in-the-loop evaluation</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.165.1">Human evaluation</span><a id="_idIndexMarker681"/><span class="koboSpan" id="kobo.166.1"> remains essential for assessing subjective dimensions of agent performance that automated metrics cannot fully capture. </span><span class="koboSpan" id="kobo.166.2">Effective human-in-the-loop evaluation requires structured methodologies to ensure consistency and reduce bias while leveraging human judgment where it adds the most value.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.167.1">Expert review provides in-depth qualitative assessment from domain specialists who can identify subtle errors, evaluate reasoning quality, and assess alignment with domain-specific best practices. </span><span class="koboSpan" id="kobo.167.2">Rather than unstructured feedback, modern expert review employs standardized rubrics that decompose</span><a id="_idIndexMarker682"/><span class="koboSpan" id="kobo.168.1"> evaluation into specific dimensions, typically using Likert scales or comparative rankings. </span><span class="koboSpan" id="kobo.168.2">Research in healthcare and financial domains has developed standardized protocols for expert evaluation, particularly for assessing agent responses in complex regulatory contexts.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.169.1">User feedback captures the perspective of end users interacting with the agent in realistic contexts. </span><span class="koboSpan" id="kobo.169.2">Structured feedback collection through embedded rating mechanisms (for example, thumbs up/down, 1-5 star ratings) provides quantitative data on user satisfaction, while free-text comments offer qualitative insights into specific strengths or weaknesses. </span><span class="koboSpan" id="kobo.169.3">Academic studies of conversational agent effectiveness increasingly implement systematic feedback collection protocols where user ratings are analyzed to identify patterns in agent performance across different query types, user segments, or time periods.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.170.1">A/B testing methodologies allow controlled comparison of different agent versions or configurations by randomly routing users to different implementations and measuring performance differences. </span><span class="koboSpan" id="kobo.170.2">This experimental approach is particularly valuable for evaluating changes to agent prompting, tool integration, or retrieval mechanisms. </span><span class="koboSpan" id="kobo.170.3">When implementing A/B testing, researchers typically define primary metrics (like task completion rates or user satisfaction) alongside secondary metrics that help explain observed differences (such as response length, tool usage patterns, or conversation duration). </span></p>
<div aria-label="322" epub:type="pagebreak" id="page14-7" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.171.1">Academic research on conversational agent optimization has demonstrated the effectiveness of controlled experiments in identifying specific improvements to agent configurations.</span></p>
<h2 class="heading-2" id="_idParaDest-211"><a id="_idTextAnchor423"/><span class="koboSpan" id="kobo.172.1">System-level evaluation</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.173.1">System-level evaluation is </span><a id="_idIndexMarker683"/><span class="koboSpan" id="kobo.174.1">crucial for complex LLM agents, particularly RAG systems, because testing individual components isn’t enough. </span><span class="koboSpan" id="kobo.174.2">Research indicates that a significant portion of failures (over 60% in some studies) stem from integration issues between components that otherwise function correctly in isolation. </span><span class="koboSpan" id="kobo.174.3">For example, issues can arise from retrieved documents not being used properly, query reformulation altering original intent, or context windows truncating information during handoffs. </span><span class="koboSpan" id="kobo.174.4">System-level evaluation addresses this by examining how information flows between components and how the agent performs as a unified system.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.175.1">Core approaches to system-level evaluation include using diagnostic frameworks that trace information flow through the entire pipeline to identify breakdown points, like the RAG Diagnostic Tool. </span><span class="koboSpan" id="kobo.175.2">Tracing and observability tools (such as LangSmith, Langfuse, and DeepEval) provide visibility into the agent’s internal workings, allowing developers to visualize reasoning chains and pinpoint where errors occur. </span><span class="koboSpan" id="kobo.175.3">End-to-end testing methodologies use comprehensive scenarios to assess how the entire system handles ambiguity, challenge inputs, and maintain context over multiple</span><a id="_idIndexMarker684"/><span class="koboSpan" id="kobo.176.1"> turns, using frameworks like GAIA.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.177.1">Effective evaluation </span><a id="_idIndexMarker685"/><span class="koboSpan" id="kobo.178.1">of LLM applications requires running multiple assessments. </span><span class="koboSpan" id="kobo.178.2">Rather than presenting abstract concepts, here are a few practical steps!</span></p>
<div aria-label="323" epub:type="pagebreak" id="page15-7" role="doc-pagebreak"/>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.179.1">Define business metrics</span></strong><span class="koboSpan" id="kobo.180.1">: Start by identifying metrics that matter to your organization. </span><span class="koboSpan" id="kobo.180.2">Focus on functional aspects like accuracy and completeness, technical factors such as latency and token usage, and user experience elements including helpfulness and clarity. </span><span class="koboSpan" id="kobo.180.3">Each application should have specific criteria with clear measurement methods.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.181.1">Create diverse test datasets</span></strong><span class="koboSpan" id="kobo.182.1">: Develop comprehensive test datasets covering common user queries, challenging edge cases, and potential compliance issues. </span><span class="koboSpan" id="kobo.182.2">Categorize examples systematically to ensure broad coverage. </span><span class="koboSpan" id="kobo.182.3">Continuously expand your dataset as you discover new usage patterns or failure modes.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.183.1">Combine multiple evaluation methods</span></strong><span class="koboSpan" id="kobo.184.1">: Use a mix of evaluation approaches for thorough assessment. </span><span class="koboSpan" id="kobo.184.2">Automated checks for factual accuracy and correctness should be combined with domain-specific criteria. </span><span class="koboSpan" id="kobo.184.3">Consider both quantitative metrics and qualitative assessments from subject matter experts when evaluating responses.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.185.1">Deploy progressively</span></strong><span class="koboSpan" id="kobo.186.1">: Adopt a staged deployment approach. </span><span class="koboSpan" id="kobo.186.2">Begin with development testing against offline benchmarks, then proceed to limited production release with a small user subset. </span><span class="koboSpan" id="kobo.186.3">Only roll out fully after meeting performance thresholds. </span><span class="koboSpan" id="kobo.186.4">This cautious approach helps identify issues before they affect most users.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.187.1">Monitor production performance</span></strong><span class="koboSpan" id="kobo.188.1">: Implement ongoing monitoring in live environments. </span><span class="koboSpan" id="kobo.188.2">Track key performance indicators like response time, error rates, token usage, and user feedback. </span><span class="koboSpan" id="kobo.188.3">Set up alerts for anomalies that might indicate degraded performance or unexpected behavior.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.189.1">Establish improvement cycles</span></strong><span class="koboSpan" id="kobo.190.1">: Create structured processes to translate evaluation insights into concrete improvements. </span><span class="koboSpan" id="kobo.190.2">When issues are identified, investigate root causes, implement specific solutions, and validate the effectiveness of changes through re-evaluation. </span><span class="koboSpan" id="kobo.190.3">Document patterns of problems and successful solutions for future reference.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.191.1">Foster cross-functional collaboration</span></strong><span class="koboSpan" id="kobo.192.1">: Include diverse perspectives in your evaluation process. </span><span class="koboSpan" id="kobo.192.2">Technical teams, domain experts, business stakeholders, and compliance specialists all bring valuable insights. </span><span class="koboSpan" id="kobo.192.3">Regular review sessions with these cross-functional teams help ensure the comprehensive assessment of LLM applications.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.193.1">Maintain living documentation</span></strong><span class="koboSpan" id="kobo.194.1">: Keep centralized records of evaluation results, improvement actions, and outcomes. </span><span class="koboSpan" id="kobo.194.2">This documentation builds organizational knowledge</span><a id="_idIndexMarker686"/><span class="koboSpan" id="kobo.195.1"> and helps teams learn from past experiences, ultimately accelerating the development of more effective LLM applications.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.196.1">It’s time now to put</span><a id="_idIndexMarker687"/><span class="koboSpan" id="kobo.197.1"> the theory to the test and get into the weeds of evaluating</span><a id="_idTextAnchor424"/><span class="koboSpan" id="kobo.198.1"> LLM agents. </span><span class="koboSpan" id="kobo.198.2">Let’s dive in!</span></p>
<h1 class="heading-1" id="_idParaDest-212"><a id="_idTextAnchor425"/><span class="koboSpan" id="kobo.199.1">Evaluating LLM agents in practice</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.200.1">LangChain provides </span><a id="_idIndexMarker688"/><span class="koboSpan" id="kobo.201.1">several predefined evaluators for different evaluation criteria. </span><span class="koboSpan" id="kobo.201.2">These evaluators can be used to assess outputs based on specific rubrics or criteria sets. </span><span class="koboSpan" id="kobo.201.3">Some common criteria include conciseness, relevance, correctness, coherence, helpfulness, and controversiality.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.202.1">We can also compare results from an LLM or agent against reference results using different methods starting from pairwise string comparisons, string distances, and embedding distances. </span><span class="koboSpan" id="kobo.202.2">The evaluation results can be used to determine the preferred LLM or agent based on the comparison of outputs. </span><span class="koboSpan" id="kobo.202.3">Confidence intervals and p-values can also be calculated to assess the reliability of the evaluation results.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.203.1">Let’s go through a few basics and apply useful evaluation strategies. </span><span class="koboSpan" id="kobo.203.2">We’ll </span><a id="_idTextAnchor426"/><span class="koboSpan" id="kobo.204.1">start with LangChain.</span></p>
<div aria-label="324" epub:type="pagebreak" id="page16-7" role="doc-pagebreak"/>
<h2 class="heading-2" id="_idParaDest-213"><a id="_idTextAnchor427"/><span class="koboSpan" id="kobo.205.1">Evaluating the correctness of results</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.206.1">Let’s think of an example, where we want to verify that an LLM’s answer is correct (or how far it is off). </span><span class="koboSpan" id="kobo.206.2">For example, when asked about the Federal Reserve’s interest rate, you might compare the output against a reference answer using both an exact match and a string distance evaluator.</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.207.1">from</span></span><span class="koboSpan" id="kobo.208.1"> langchain.evaluation </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.209.1">import</span></span><span class="koboSpan" id="kobo.210.1"> load_evaluator, ExactMatchStringEvaluator</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.211.1">prompt = </span><span class="hljs-string"><span class="koboSpan" id="kobo.212.1">"What is the current Federal Reserve interest rate?"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.213.1">reference_answer = </span><span class="hljs-string"><span class="koboSpan" id="kobo.214.1">"0.25%"</span></span> <span class="hljs-comment"><span class="koboSpan" id="kobo.215.1"># Suppose this is the correct answer.</span></span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.216.1"># Example predictions from your LLM:</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.217.1">prediction_correct = </span><span class="hljs-string"><span class="koboSpan" id="kobo.218.1">"0.25%"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.219.1">prediction_incorrect = </span><span class="hljs-string"><span class="koboSpan" id="kobo.220.1">"0.50%"</span></span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.221.1"># Initialize an Exact Match evaluator that ignores case differences.</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.222.1">exact_evaluator = ExactMatchStringEvaluator(ignore_case=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.223.1">True</span></span><span class="koboSpan" id="kobo.224.1">)</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.225.1"># Evaluate the correct prediction.</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.226.1">exact_result_correct = exact_evaluator.evaluate_strings(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.227.1">    prediction=prediction_correct, reference=reference_answer</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.228.1">)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.229.1">print</span></span><span class="koboSpan" id="kobo.230.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.231.1">"Exact match result (correct answer):"</span></span><span class="koboSpan" id="kobo.232.1">, exact_result_correct)</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.233.1"># Expected output: score of 1 (or 'Y') indicating a perfect match.</span></span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.234.1"># Evaluate an incorrect prediction.</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.235.1">exact_result_incorrect = exact_evaluator.evaluate_strings(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.236.1">    prediction=prediction_incorrect, reference=reference_answer</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.237.1">)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.238.1">print</span></span><span class="koboSpan" id="kobo.239.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.240.1">"Exact match result (incorrect answer):"</span></span><span class="koboSpan" id="kobo.241.1">, exact_result_incorrect)</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.242.1"># Expected output: score of 0 (or 'N') indicating a mismatch.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.243.1">Now, obviously this</span><a id="_idIndexMarker689"/><span class="koboSpan" id="kobo.244.1"> won’t be very useful if the output comes in a different format or if we want to gauge how far off the answer is. </span><span class="koboSpan" id="kobo.244.2">In the repository, you can find an implementation of a custom comparison that would parse answers such as “It is 0.50%” and “A quarter percent.”</span></p>
<p class="normal"><span class="koboSpan" id="kobo.245.1">A more generalizable approach is LLM‐as‐a‐judge for evaluating correctness. </span><span class="koboSpan" id="kobo.245.2">In this example, instead of using simple string extraction or an exact match, we call an evaluation LLM (for example, an upper mid-range model such as Mistral) that parses and scores the prompt, the prediction, and a reference answer and then returns a numerical score plus reasoning. </span><span class="koboSpan" id="kobo.245.3">This works in scenarios where the prediction might be phrased differently but still correct.</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.246.1">from</span></span><span class="koboSpan" id="kobo.247.1"> langchain_mistralai </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.248.1">import</span></span><span class="koboSpan" id="kobo.249.1"> ChatMistralAI</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.250.1">from</span></span><span class="koboSpan" id="kobo.251.1"> langchain.evaluation.scoring </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.252.1">import</span></span><span class="koboSpan" id="kobo.253.1"> ScoreStringEvalChain</span></p>
<div aria-label="325" epub:type="pagebreak" id="page17-7" role="doc-pagebreak"/>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.254.1"># Initialize the evaluator LLM</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.255.1">llm = ChatMistralAI(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.256.1">    model=</span><span class="hljs-string"><span class="koboSpan" id="kobo.257.1">"mistral-large-latest"</span></span><span class="koboSpan" id="kobo.258.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.259.1">    temperature=</span><span class="hljs-number"><span class="koboSpan" id="kobo.260.1">0</span></span><span class="koboSpan" id="kobo.261.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.262.1">    max_retries=</span><span class="hljs-number"><span class="koboSpan" id="kobo.263.1">2</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.264.1">)</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.265.1"># Create the ScoreStringEvalChain from the LLM</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.266.1">chain = ScoreStringEvalChain.from_llm(llm=llm)</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.267.1"># Define the finance-related input, prediction, and reference answer</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.268.1">finance_input = </span><span class="hljs-string"><span class="koboSpan" id="kobo.269.1">"What is the current Federal Reserve interest rate?"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.270.1">finance_prediction = </span><span class="hljs-string"><span class="koboSpan" id="kobo.271.1">"The current interest rate is 0.25%."</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.272.1">finance_reference = </span><span class="hljs-string"><span class="koboSpan" id="kobo.273.1">"The Federal Reserve's current interest rate is 0.25%."</span></span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.274.1"># Evaluate the prediction using the scoring chain</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.275.1">result_finance = chain.evaluate_strings(</span></p>
<p class="snippet-code"> <span class="hljs-built_in"><span class="koboSpan" id="kobo.276.1">input</span></span><span class="koboSpan" id="kobo.277.1">=finance_input,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.278.1">    prediction=finance_prediction,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.279.1">)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.280.1">print</span></span><span class="koboSpan" id="kobo.281.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.282.1">"Finance Evaluation Result:"</span></span><span class="koboSpan" id="kobo.283.1">)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.284.1">print</span></span><span class="koboSpan" id="kobo.285.1">(result_finance)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.286.1">The output demonstrates how the LLM evaluator assesses the response quality with nuanced reasoning:</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.287.1">Finance Evaluation Result:</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.288.1">{'reasoning': "The assistant's response is not verifiable as it does not provide a date or source for the information. </span><span class="koboSpan" id="kobo.288.2">The Federal Reserve interest rate changes over time and is not static. </span><span class="koboSpan" id="kobo.288.3">Therefore, without a specific date or source, the information provided could be incorrect. </span><span class="koboSpan" id="kobo.288.4">The assistant should have advised the user to check the Federal Reserve's official website or a reliable financial news source for the most current rate. </span><span class="koboSpan" id="kobo.288.5">The response lacks depth and accuracy. </span><span class="koboSpan" id="kobo.288.6">Rating: [[3]]", 'score': 3}</span></p>
<p class="normal"><span class="koboSpan" id="kobo.289.1">This evaluation</span><a id="_idIndexMarker690"/><span class="koboSpan" id="kobo.290.1"> highlights an important advantage of the LLM-as-a-judge approach: it can identify subtle issues that simple matching would miss. </span><span class="koboSpan" id="kobo.290.2">In this case, the evaluator correctly identified that the response lacked important context. </span><span class="koboSpan" id="kobo.290.3">With a score of 3 out of 5, the LLM judge provides a more nuanced assessment than binary correct/incorrect evaluations, giving developers actionable feedback to improve response quality in financial applications where accuracy and proper attribution are critical.</span></p>
<div aria-label="326" epub:type="pagebreak" id="page18-7" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.291.1">The next example shows how to use Mistral AI to evaluate a model’s prediction against a reference answer. </span><span class="koboSpan" id="kobo.291.2">Please make sure to set your </span><code class="inlineCode"><span class="koboSpan" id="kobo.292.1">MISTRAL_API_KEY</span></code><span class="koboSpan" id="kobo.293.1"> environment variable and install the required package: </span><code class="inlineCode"><span class="koboSpan" id="kobo.294.1">pip install langchain_mistralai</span></code><span class="koboSpan" id="kobo.295.1">. </span><span class="koboSpan" id="kobo.295.2">This should already be installed if you followed the instructions in </span><a href="E_Chapter_2.xhtml#_idTextAnchor044"><em class="italic"><span class="koboSpan" id="kobo.296.1">Chapter 2</span></em></a><span class="koboSpan" id="kobo.297.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.298.1">This approach is more appropriate when you have ground truth responses and want to assess how well the model’s output matches the expected answer. </span><span class="koboSpan" id="kobo.298.2">It’s particularly useful for factual questions with clear, correct answers.</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.299.1">from</span></span><span class="koboSpan" id="kobo.300.1"> langchain_mistralai </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.301.1">import</span></span><span class="koboSpan" id="kobo.302.1"> ChatMistralAI</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.303.1">from</span></span><span class="koboSpan" id="kobo.304.1"> langchain.evaluation.scoring </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.305.1">import</span></span><span class="koboSpan" id="kobo.306.1"> LabeledScoreStringEvalChain</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.307.1"># Initialize the evaluator LLM with deterministic output (temperature 0.)</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.308.1">llm = ChatMistralAI(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.309.1">    model=</span><span class="hljs-string"><span class="koboSpan" id="kobo.310.1">"mistral-large-latest"</span></span><span class="koboSpan" id="kobo.311.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.312.1">    temperature=</span><span class="hljs-number"><span class="koboSpan" id="kobo.313.1">0</span></span><span class="koboSpan" id="kobo.314.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.315.1">    max_retries=</span><span class="hljs-number"><span class="koboSpan" id="kobo.316.1">2</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.317.1">)</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.318.1"># Create the evaluation chain that can use reference answers</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.319.1">labeled_chain = LabeledScoreStringEvalChain.from_llm(llm=llm)</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.320.1"># Define the finance-related input, prediction, and reference answer</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.321.1">finance_input = </span><span class="hljs-string"><span class="koboSpan" id="kobo.322.1">"What is the current Federal Reserve interest rate?"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.323.1">finance_prediction = </span><span class="hljs-string"><span class="koboSpan" id="kobo.324.1">"The current interest rate is 0.25%."</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.325.1">finance_reference = </span><span class="hljs-string"><span class="koboSpan" id="kobo.326.1">"The Federal Reserve's current interest rate is 0.25%."</span></span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.327.1"># Evaluate the prediction against the reference</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.328.1">labeled_result = labeled_chain.evaluate_strings(</span></p>
<p class="snippet-code"> <span class="hljs-built_in"><span class="koboSpan" id="kobo.329.1">input</span></span><span class="koboSpan" id="kobo.330.1">=finance_input,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.331.1">    prediction=finance_prediction,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.332.1">    reference=finance_reference,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.333.1">)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.334.1">print</span></span><span class="koboSpan" id="kobo.335.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.336.1">"Finance Evaluation Result (with reference):"</span></span><span class="koboSpan" id="kobo.337.1">)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.338.1">print</span></span><span class="koboSpan" id="kobo.339.1">(labeled_result)</span></p>
<div aria-label="327" epub:type="pagebreak" id="page19-7" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.340.1">The output shows </span><a id="_idIndexMarker691"/><span class="koboSpan" id="kobo.341.1">how providing a reference answer significantly changes the evaluation results:</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.342.1">{'reasoning': "The assistant's response is helpful, relevant, and correct. </span><span class="koboSpan" id="kobo.342.2">It directly answers the user's question about the current Federal Reserve interest rate. </span><span class="koboSpan" id="kobo.342.3">However, it lacks depth as it does not provide any additional information or context about the interest rate, such as how it is determined or what it means for the economy. </span><span class="koboSpan" id="kobo.342.4">Rating: [[8]]", 'score': 8}</span></p>
<p class="normal"><span class="koboSpan" id="kobo.343.1">Notice how the score increased dramatically from 3 (in the previous example) to 8 when we provided a reference answer. </span><span class="koboSpan" id="kobo.343.2">This demonstrates the importance of ground truth in evaluation. </span><span class="koboSpan" id="kobo.343.3">Without a reference, the evaluator focused on the lack of citation and timestamp. </span><span class="koboSpan" id="kobo.343.4">With a reference confirming the factual accuracy, the evaluator now focuses on assessing completeness and depth instead of verifiability.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.344.1">Both of these approaches leverage Mistral’s LLM as an evaluator, which can provide more nuanced and context-aware assessments than simple string matching or statistical methods. </span><span class="koboSpan" id="kobo.344.2">The results from these evaluations should be consistent when using </span><code class="inlineCode"><span class="koboSpan" id="kobo.345.1">temperature=0</span></code><span class="koboSpan" id="kobo.346.1">, though outputs may differ </span><a id="_idIndexMarker692"/><span class="koboSpan" id="kobo.347.1">from those shown in the book due to changes on the provider side.</span></p>
<div>
<div class="note" id="_idContainer103">
<p class="normal"><span class="koboSpan" id="kobo.348.1">Your output may differ from the book example due to model version differences and inherent variations in LLM responses</span><a id="_idTextAnchor428"/><span class="koboSpan" id="kobo.349.1"> (depending on the temperature).</span></p>
</div>
</div>
<h2 class="heading-2" id="_idParaDest-214"><a id="_idTextAnchor429"/><span class="koboSpan" id="kobo.350.1">Evaluating tone and conciseness</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.351.1">Beyond factual accuracy, many applications require responses that meet certain stylistic criteria. </span><span class="koboSpan" id="kobo.351.2">Healthcare applications, for example, must provide accurate information in a friendly, approachable manner without overwhelming patients with unnecessary details. </span><span class="koboSpan" id="kobo.351.3">The following example demonstrates how to evaluate both conciseness and tone using LangChain’s criteria evaluators, allowing developers to assess these subjective but critical aspects of response quality:</span></p>
<p class="normal"><span class="koboSpan" id="kobo.352.1">We start by importing the evaluator loader and a chat LLM for evaluation (for example GPT-4o):</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.353.1">from</span></span><span class="koboSpan" id="kobo.354.1"> langchain.evaluation </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.355.1">import</span></span><span class="koboSpan" id="kobo.356.1"> load_evaluator</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.357.1">from</span></span><span class="koboSpan" id="kobo.358.1"> langchain.chat_models </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.359.1">import</span></span><span class="koboSpan" id="kobo.360.1"> ChatOpenAI</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.361.1">evaluation_llm = ChatOpenAI(model=</span><span class="hljs-string"><span class="koboSpan" id="kobo.362.1">"gpt-4o"</span></span><span class="koboSpan" id="kobo.363.1">, temperature=</span><span class="hljs-number"><span class="koboSpan" id="kobo.364.1">0</span></span><span class="koboSpan" id="kobo.365.1">)</span></p>
<div aria-label="328" epub:type="pagebreak" id="page20-7" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.366.1">Our example prompt and the answer we’ve obtained is:</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.367.1">prompt_health = "What is a healthy blood pressure range for adults?"</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.368.1"># A sample LLM output from your healthcare assistant:</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.369.1">prediction_health = (</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.370.1">    "A normal blood pressure reading is typically around 120/80 mmHg. </span><span class="koboSpan" id="kobo.370.2">"</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.371.1">    "It's important to follow your doctor's advice for personal health management!"</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.372.1">)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.373.1">Now let’s evaluate conciseness using a built-in </span><code class="inlineCode"><span class="koboSpan" id="kobo.374.1">conciseness</span></code><span class="koboSpan" id="kobo.375.1"> criterion:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.376.1">conciseness_evaluator = load_evaluator(</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.377.1">"criteria"</span></span><span class="koboSpan" id="kobo.378.1">, criteria=</span><span class="hljs-string"><span class="koboSpan" id="kobo.379.1">"conciseness"</span></span><span class="koboSpan" id="kobo.380.1">, llm=evaluation_llm</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.381.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.382.1">conciseness_result = conciseness_evaluator.evaluate_strings(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.383.1">    prediction=prediction_health, </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.384.1">input</span></span><span class="koboSpan" id="kobo.385.1">=prompt_health</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.386.1">)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.387.1">print</span></span><span class="koboSpan" id="kobo.388.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.389.1">"Conciseness evaluation result:"</span></span><span class="koboSpan" id="kobo.390.1">, conciseness_result)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.391.1">The result includes a score (0 or 1), a value (“Y” or “N”), and a reasoning chain of thought:</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.392.1">Conciseness evaluation result: {'reasoning': "The criterion is conciseness. </span><span class="koboSpan" id="kobo.392.2">This means the submission should be brief, to the point, and not contain unnecessary information.\n\nLooking at the submission, it provides a direct answer to the question, stating that a normal blood pressure reading is around 120/80 mmHg. </span><span class="koboSpan" id="kobo.392.3">This is a concise answer to the question.\n\nThe submission also includes an additional sentence advising to follow a doctor's advice for personal health management. </span><span class="koboSpan" id="kobo.392.4">While this information is not directly related to the question, it is still relevant and does not detract from the conciseness of the answer.\n\nTherefore, the submission meets the criterion of conciseness.\n\nY", 'value': 'Y', 'score': 1}</span></p>
<p class="normal"><span class="koboSpan" id="kobo.393.1">As for</span><a id="_idIndexMarker693"/><span class="koboSpan" id="kobo.394.1"> friendliness, let’s define a </span><code class="inlineCode"><span class="koboSpan" id="kobo.395.1">custom</span></code><span class="koboSpan" id="kobo.396.1"> criterion:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.397.1">custom_friendliness = {</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.398.1">"friendliness"</span></span><span class="koboSpan" id="kobo.399.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.400.1">"Is the response written in a friendly and approachable tone?"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.401.1">}</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.402.1"># Load a criteria evaluator with this custom criterion.</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.403.1">friendliness_evaluator = load_evaluator(</span></p>
<div aria-label="329" epub:type="pagebreak" id="page21-7" role="doc-pagebreak"/>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.404.1">"criteria"</span></span><span class="koboSpan" id="kobo.405.1">, criteria=custom_friendliness, llm=evaluation_llm</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.406.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.407.1">friendliness_result = friendliness_evaluator.evaluate_strings(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.408.1">    prediction=prediction_health, </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.409.1">input</span></span><span class="koboSpan" id="kobo.410.1">=prompt_health</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.411.1">)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.412.1">print</span></span><span class="koboSpan" id="kobo.413.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.414.1">"Friendliness evaluation result:"</span></span><span class="koboSpan" id="kobo.415.1">, friendliness_result)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.416.1">The evaluator should return whether the tone is friendly (</span><code class="inlineCode"><span class="koboSpan" id="kobo.417.1">Y</span></code><span class="koboSpan" id="kobo.418.1">/</span><code class="inlineCode"><span class="koboSpan" id="kobo.419.1">N</span></code><span class="koboSpan" id="kobo.420.1">) along with reasoning. </span><span class="koboSpan" id="kobo.420.2">In fact, this is what we get:</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.421.1">Friendliness evaluation result: {'reasoning': "The criterion is to assess whether the response is written in a friendly and approachable tone. </span><span class="koboSpan" id="kobo.421.2">The submission provides the information in a straightforward manner and ends with a suggestion to follow doctor's advice for personal health management. </span><span class="koboSpan" id="kobo.421.3">This suggestion can be seen as a friendly advice, showing concern for the reader's health. </span><span class="koboSpan" id="kobo.421.4">Therefore, the submission can be considered as written in a friendly and approachable tone.\n\nY", 'value': 'Y', 'score': 1}</span></p>
<p class="normal"><span class="koboSpan" id="kobo.422.1">This evaluation approach is particularly valuable for applications in healthcare, customer service, and educational domains where the manner of communication is as important as the factual content. </span><span class="koboSpan" id="kobo.422.2">The explicit reasoning provided by the evaluator helps development teams understand exactly which elements of the response contribute to its tone, making it easier to debug and improve response generation. </span><span class="koboSpan" id="kobo.422.3">While binary </span><code class="inlineCode"><span class="koboSpan" id="kobo.423.1">Y</span></code><span class="koboSpan" id="kobo.424.1">/</span><code class="inlineCode"><span class="koboSpan" id="kobo.425.1">N</span></code><span class="koboSpan" id="kobo.426.1"> scores are useful for automated quality gates, the detailed reasoning offers more nuanced insights for continuous improvement. </span><span class="koboSpan" id="kobo.426.2">For production </span><a id="_idIndexMarker694"/><span class="koboSpan" id="kobo.427.1">systems, consider combining multiple criteria evaluators to create a comprehensive quality score that reflects all aspects of your applica</span><a id="_idTextAnchor430"/><span class="koboSpan" id="kobo.428.1">tion’s communication requirements.</span></p>
<h2 class="heading-2" id="_idParaDest-215"><a id="_idTextAnchor431"/><span class="koboSpan" id="kobo.429.1">Evaluating the output format</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.430.1">When working with LLMs to generate structured data like JSON, XML, or CSV, format validation becomes critical. </span><span class="koboSpan" id="kobo.430.2">Financial applications, reporting tools, and API integrations often depend on correctly formatted data structures. </span><span class="koboSpan" id="kobo.430.3">A technically perfect response that fails to adhere to the expected format can break downstream systems. </span><span class="koboSpan" id="kobo.430.4">LangChain provides specialized evaluators for validating structured outputs, as demonstrated in the following example using JSON validation for a financial report:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.431.1">from</span></span><span class="koboSpan" id="kobo.432.1"> langchain.evaluation </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.433.1">import</span></span><span class="koboSpan" id="kobo.434.1"> JsonValidityEvaluator</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.435.1"># Initialize the JSON validity evaluator.</span></span></p>
<div aria-label="330" epub:type="pagebreak" id="page22-7" role="doc-pagebreak"/>
<p class="snippet-code"><span class="koboSpan" id="kobo.436.1">json_validator = JsonValidityEvaluator()</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.437.1">valid_json_output = </span><span class="hljs-string"><span class="koboSpan" id="kobo.438.1">'{"company": "Acme Corp", "revenue": 1000000, "profit": 200000}'</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.439.1">invalid_json_output = </span><span class="hljs-string"><span class="koboSpan" id="kobo.440.1">'{"company": "Acme Corp", "revenue": 1000000, "profit": 200000,}'</span></span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.441.1"># Evaluate the valid JSON.</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.442.1">valid_result = json_validator.evaluate_strings(prediction=valid_json_output)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.443.1">print</span></span><span class="koboSpan" id="kobo.444.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.445.1">"JSON validity result (valid):"</span></span><span class="koboSpan" id="kobo.446.1">, valid_result)</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.447.1"># Evaluate the invalid JSON.</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.448.1">invalid_result = json_validator.evaluate_strings(prediction=invalid_json_output)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.449.1">print</span></span><span class="koboSpan" id="kobo.450.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.451.1">"JSON validity result (invalid):"</span></span><span class="koboSpan" id="kobo.452.1">, invalid_result)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.453.1">We’ll see a score indicating the JSON is valid:</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.454.1">JSON validity result (valid): {'score': 1}</span></p>
<p class="normal"><span class="koboSpan" id="kobo.455.1">For the invalid JSON, we are getting a score indicating the JSON is invalid:</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.456.1">JSON validity result (invalid): {'score': 0, 'reasoning': 'Expecting property name enclosed in double quotes: line 1 column 63 (char 62)'}</span></p>
<p class="normal"><span class="koboSpan" id="kobo.457.1">This validation approach is particularly valuable in production systems where LLMs interface with other software components. </span><span class="koboSpan" id="kobo.457.2">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.458.1">JsonValidityEvaluator</span></code><span class="koboSpan" id="kobo.459.1"> not only identifies invalid outputs but also provides detailed error messages pinpointing the exact location of formatting errors. </span><span class="koboSpan" id="kobo.459.2">This facilitates rapid debugging and can be incorporated into automated testing pipelines to prevent format-related failures. </span><span class="koboSpan" id="kobo.459.3">Consider implementing similar validators for other formats your application may generate, such as XML, CSV, or domain-specific formats like FIX pr</span><a id="_idTextAnchor432"/><span class="koboSpan" id="kobo.460.1">otocol for financial </span><a id="_idIndexMarker695"/><span class="koboSpan" id="kobo.461.1">transactions.</span></p>
<h2 class="heading-2" id="_idParaDest-216"><a id="_idTextAnchor433"/><span class="koboSpan" id="kobo.462.1">Evaluating agent trajectory</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.463.1">Complex agents require evaluation across three critical dimensions:</span></p>
<ul>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.464.1">Final response evaluation</span></strong><span class="koboSpan" id="kobo.465.1">: Assess the ultimate output provided to the user (factual accuracy, helpfulness, quality, and safety)</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.466.1">Trajectory evaluation</span></strong><span class="koboSpan" id="kobo.467.1">: Examine the path the agent took to reach its conclusion</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.468.1">Single-step evaluation</span></strong><span class="koboSpan" id="kobo.469.1">: Analyze individual decision points in isolation</span></li>
</ul>
<div aria-label="331" epub:type="pagebreak" id="page23-7" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.470.1">While final response evaluation focuses on outcomes, trajectory evaluation examines the process itself. </span><span class="koboSpan" id="kobo.470.2">This approach is particularly valuable for complex agents that employ multiple tools, reasoning steps, or decision points to complete tasks. </span><span class="koboSpan" id="kobo.470.3">By evaluating the path taken, we can identify exactly where and how agents succeed or fail, even when the final answer is incorrect.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.471.1">Trajectory evaluation compares the actual sequence of steps an agent took against an expected sequence, calculating a score based on how many expected steps were completed correctly. </span><span class="koboSpan" id="kobo.471.2">This gives partial credit to agents that follow some correct steps even if they don’t reach the right final answer.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.472.1">Let’s implement a </span><code class="inlineCode"><span class="koboSpan" id="kobo.473.1">custom trajectory</span></code><span class="koboSpan" id="kobo.474.1"> evaluator for a healthcare agent that responds to medication questions:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.475.1">from</span></span><span class="koboSpan" id="kobo.476.1"> langsmith </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.477.1">import</span></span><span class="koboSpan" id="kobo.478.1"> Client</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.479.1"># Custom trajectory subsequence evaluator</span></span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.480.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.481.1">trajectory_subsequence</span></span><span class="koboSpan" id="kobo.482.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.483.1">outputs: </span></span><span class="hljs-built_in"><span class="koboSpan" id="kobo.484.1">dict</span></span><span class="hljs-params"><span class="koboSpan" id="kobo.485.1">, reference_outputs: </span></span><span class="hljs-built_in"><span class="koboSpan" id="kobo.486.1">dict</span></span><span class="koboSpan" id="kobo.487.1">) -&gt; </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.488.1">float</span></span><span class="koboSpan" id="kobo.489.1">:</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.490.1">"""</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.491.1">Check how many of the desired steps the agent took."""</span></span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.492.1">if</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.493.1">len</span></span><span class="koboSpan" id="kobo.494.1">(reference_outputs[</span><span class="hljs-string"><span class="koboSpan" id="kobo.495.1">'trajectory'</span></span><span class="koboSpan" id="kobo.496.1">]) &gt; </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.497.1">len</span></span><span class="koboSpan" id="kobo.498.1">(outputs[</span><span class="hljs-string"><span class="koboSpan" id="kobo.499.1">'trajectory'</span></span><span class="koboSpan" id="kobo.500.1">]):</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.501.1">return</span></span> <span class="hljs-literal"><span class="koboSpan" id="kobo.502.1">False</span></span></p>
<p class="snippet-code"> </p>
<p class="snippet-code"><span class="koboSpan" id="kobo.503.1">    i = j = </span><span class="hljs-number"><span class="koboSpan" id="kobo.504.1">0</span></span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.505.1">while</span></span><span class="koboSpan" id="kobo.506.1"> i &lt; </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.507.1">len</span></span><span class="koboSpan" id="kobo.508.1">(reference_outputs[</span><span class="hljs-string"><span class="koboSpan" id="kobo.509.1">'trajectory'</span></span><span class="koboSpan" id="kobo.510.1">]) </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.511.1">and</span></span><span class="koboSpan" id="kobo.512.1"> j &lt; </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.513.1">len</span></span><span class="koboSpan" id="kobo.514.1">(outputs[</span><span class="hljs-string"><span class="koboSpan" id="kobo.515.1">'trajectory'</span></span><span class="koboSpan" id="kobo.516.1">]):</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.517.1">if</span></span><span class="koboSpan" id="kobo.518.1"> reference_outputs[</span><span class="hljs-string"><span class="koboSpan" id="kobo.519.1">'trajectory'</span></span><span class="koboSpan" id="kobo.520.1">][i] == outputs[</span><span class="hljs-string"><span class="koboSpan" id="kobo.521.1">'trajectory'</span></span><span class="koboSpan" id="kobo.522.1">][j]:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.523.1">            i += </span><span class="hljs-number"><span class="koboSpan" id="kobo.524.1">1</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.525.1">        j += </span><span class="hljs-number"><span class="koboSpan" id="kobo.526.1">1</span></span></p>
<p class="snippet-code"> </p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.527.1">return</span></span><span class="koboSpan" id="kobo.528.1"> i / </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.529.1">len</span></span><span class="koboSpan" id="kobo.530.1">(reference_outputs[</span><span class="hljs-string"><span class="koboSpan" id="kobo.531.1">'trajectory'</span></span><span class="koboSpan" id="kobo.532.1">])</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.533.1"># Create example dataset with expected trajectories</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.534.1">client = Client()</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.535.1">trajectory_dataset = client.create_dataset(</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.536.1">"Healthcare Agent Trajectory Evaluation"</span></span><span class="koboSpan" id="kobo.537.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.538.1">    description=</span><span class="hljs-string"><span class="koboSpan" id="kobo.539.1">"Evaluates agent trajectory for medication queries"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.540.1">)</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.541.1"># Add example with expected trajectory</span></span></p>
<div aria-label="332" epub:type="pagebreak" id="page24-7" role="doc-pagebreak"/>
<p class="snippet-code"><span class="koboSpan" id="kobo.542.1">client.create_example(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.543.1">    inputs={</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.544.1">"question"</span></span><span class="koboSpan" id="kobo.545.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.546.1">"What is the recommended dosage of ibuprofen for an adult?"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.547.1">    },</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.548.1">    outputs={</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.549.1">"trajectory"</span></span><span class="koboSpan" id="kobo.550.1">: [</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.551.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.552.1">intent_classifier"</span></span><span class="koboSpan" id="kobo.553.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.554.1">"healthcare_agent"</span></span><span class="koboSpan" id="kobo.555.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.556.1">"MedicalDatabaseSearch"</span></span><span class="koboSpan" id="kobo.557.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.558.1">"format_response"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.559.1">        ],</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.560.1">"response"</span></span><span class="koboSpan" id="kobo.561.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.562.1">"Typically, 200-400mg every 4-6 hours, not exceeding 3200mg per day."</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.563.1">    },</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.564.1">    dataset_id=trajectory_dataset.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.565.1">id</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.566.1">)</span></p>
<div>
<div class="note" id="_idContainer104">
<p class="normal"><span class="koboSpan" id="kobo.567.1">Please remember to set your </span><code class="inlineCode"><span class="koboSpan" id="kobo.568.1">LANGSMITH_API_KEY</span></code><span class="koboSpan" id="kobo.569.1"> environment variable! </span><span class="koboSpan" id="kobo.569.2">If you get a </span><code class="inlineCode"><span class="koboSpan" id="kobo.570.1">Using legacy API key</span></code><span class="koboSpan" id="kobo.571.1"> error, you might need to generate a new API key from the LangSmith dashboard: </span><a href="https://smith.langchain.com/settings"><span class="url"><span class="koboSpan" id="kobo.572.1">https://smith.langchain.com/settings</span></span></a><span class="koboSpan" id="kobo.573.1">. </span><span class="koboSpan" id="kobo.573.2">You always want to use the latest version of the LangSmith package.</span></p>
</div>
</div>
<p class="normal"><span class="koboSpan" id="kobo.574.1">To evaluate the agent’s </span><a id="_idIndexMarker696"/><span class="koboSpan" id="kobo.575.1">trajectory, we need to capture the actual sequence of steps taken. </span><span class="koboSpan" id="kobo.575.2">With LangGraph, we can use streaming capabilities to record every node and tool invocation:</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.576.1"># Function to run graph with trajectory tracking (example implementation)</span></span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.577.1">async</span></span> <span class="hljs-keyword"><span class="koboSpan" id="kobo.578.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.579.1">run_graph_with_trajectory</span></span><span class="koboSpan" id="kobo.580.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.581.1">inputs: </span></span><span class="hljs-built_in"><span class="koboSpan" id="kobo.582.1">dict</span></span><span class="koboSpan" id="kobo.583.1">) -&gt; </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.584.1">dict</span></span><span class="koboSpan" id="kobo.585.1">:</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.586.1">"""Run graph and track the trajectory it takes along with the final response."""</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.587.1">    trajectory = []</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.588.1">    final_response = </span><span class="hljs-string"><span class="koboSpan" id="kobo.589.1">""</span></span></p>
<p class="snippet-code"> </p>
<p class="snippet-code"> <span class="hljs-comment"><span class="koboSpan" id="kobo.590.1"># Here you would implement your actual graph execution</span></span></p>
<p class="snippet-code"> <span class="hljs-comment"><span class="koboSpan" id="kobo.591.1"># For the example, we'll just return a sample result</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.592.1">    trajectory = [</span><span class="hljs-string"><span class="koboSpan" id="kobo.593.1">"intent_classifier"</span></span><span class="koboSpan" id="kobo.594.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.595.1">"healthcare_agent"</span></span><span class="koboSpan" id="kobo.596.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.597.1">"MedicalDatabaseSearch"</span></span><span class="koboSpan" id="kobo.598.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.599.1">"format_response"</span></span><span class="koboSpan" id="kobo.600.1">]</span></p>
<div aria-label="333" epub:type="pagebreak" id="page25-6" role="doc-pagebreak"/>
<p class="snippet-code"><span class="koboSpan" id="kobo.601.1">    final_response = </span><span class="hljs-string"><span class="koboSpan" id="kobo.602.1">"Typically, 200-400mg every 4-6 hours, not exceeding 3200mg per day."</span></span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.603.1">return</span></span><span class="koboSpan" id="kobo.604.1"> {</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.605.1">"trajectory"</span></span><span class="koboSpan" id="kobo.606.1">: trajectory,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.607.1">"response"</span></span><span class="koboSpan" id="kobo.608.1">: final_response</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.609.1">    }</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.610.1"># Note: This is an async function, so in a notebook you'd need to use await</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.611.1">experiment_results = </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.612.1">await</span></span><span class="koboSpan" id="kobo.613.1"> client.aevaluate(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.614.1">    run_graph_with_trajectory,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.615.1">    data=trajectory_dataset.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.616.1">id</span></span><span class="koboSpan" id="kobo.617.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.618.1">    evaluators=[trajectory_subsequence],</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.619.1">    experiment_prefix=</span><span class="hljs-string"><span class="koboSpan" id="kobo.620.1">"healthcare-agent-trajectory"</span></span><span class="koboSpan" id="kobo.621.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.622.1">    num_repetitions=</span><span class="hljs-number"><span class="koboSpan" id="kobo.623.1">1</span></span><span class="koboSpan" id="kobo.624.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.625.1">    max_concurrency=</span><span class="hljs-number"><span class="koboSpan" id="kobo.626.1">4</span></span><span class="koboSpan" id="kobo.627.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.628.1">)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.629.1">We can also analyze results on the dataset, which we can download from LangSmith:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.630.1">results_df = experiment_results.to_pandas()</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.631.1">print</span></span><span class="koboSpan" id="kobo.632.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.633.1">f"Average trajectory match score: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.634.1">{results_df[</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.635.1">'feedback.trajectory_subsequence'</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.636.1">].mean()}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.637.1">"</span></span><span class="koboSpan" id="kobo.638.1">)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.639.1">In this case, this is nonsensical, but this is to illustrate the idea.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.640.1">The following screenshot visually demonstrates what trajectory evaluation results look like in the LangSmith interface. </span><span class="koboSpan" id="kobo.640.2">It shows the perfect trajectory match score (</span><strong class="screenText"><span class="koboSpan" id="kobo.641.1">1.00</span></strong><span class="koboSpan" id="kobo.642.1">), which validates that the agent followed the expected path:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.643.1"><img alt="Figure 8.1: Trajectory evaluation in LangSmith" src="../Images/B32363_08_01.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.644.1">Figure 8.1: Trajectory evaluation in LangSmith</span></p>
<p class="normal"><span class="koboSpan" id="kobo.645.1">Please note that LangSmith displays the actual trajectory steps side by side with the reference trajectory and that it includes real execution metrics like latency and token usage.</span></p>
<div aria-label="334" epub:type="pagebreak" id="page26-6" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.646.1">Trajectory evaluation provides unique insights beyond simple pass/fail assessments:</span></p>
<ul>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.647.1">Identifying failure points</span></strong><span class="koboSpan" id="kobo.648.1">: Pinpoint exactly where agents deviate from expected paths</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.649.1">Process improvement</span></strong><span class="koboSpan" id="kobo.650.1">: Recognize when agents take unnecessary detours or inefficient routes</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.651.1">Tool usage patterns</span></strong><span class="koboSpan" id="kobo.652.1">: Understand how agents leverage available tools and when they make suboptimal choices</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.653.1">Reasoning quality</span></strong><span class="koboSpan" id="kobo.654.1">: Evaluate the agent’s decision-making process independent of final outcomes</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.655.1">For example, an agent might provide a correct medication dosage but reach it through an inappropriate trajectory (bypassing safety checks or using unreliable data sources). </span><span class="koboSpan" id="kobo.655.2">Trajectory evaluation reveals these process issues that outcome-focused evaluation would miss.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.656.1">Consider using trajectory evaluation in conjunction with other evaluation types for a holistic assessment of your </span><a id="_idIndexMarker697"/><span class="koboSpan" id="kobo.657.1">agent’s performance. </span><span class="koboSpan" id="kobo.657.2">This approach is particularly valuable during development and debugging phases, where understanding the </span><em class="italic"><span class="koboSpan" id="kobo.658.1">why</span></em><span class="koboSpan" id="kobo.659.1"> behind agent behavior is as important as measuring final output quality.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.660.1">By implementing continuous trajectory monitoring, you can track how agent behaviors evolve as you refine prompts, add tools, or modify the underlying model, ensuring improvements in one area don’t cause regressions in the agen</span><a id="_idTextAnchor434"/><span class="koboSpan" id="kobo.661.1">t’s overall decision-making process.</span></p>
<h2 class="heading-2" id="_idParaDest-217"><a id="_idTextAnchor435"/><span class="koboSpan" id="kobo.662.1">Evaluating CoT reasoning</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.663.1">Now suppose we want to evaluate the agent’s reasoning. </span><span class="koboSpan" id="kobo.663.2">For example, going back to our earlier example, the agent must not only answer “What is the current interest rate?” </span><span class="koboSpan" id="kobo.663.3">but also provide reasoning behind its answer. </span><span class="koboSpan" id="kobo.663.4">We can use the </span><code class="inlineCode"><span class="koboSpan" id="kobo.664.1">COT_QA</span></code><span class="koboSpan" id="kobo.665.1"> evaluator for chain-of-thought evaluation.</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.666.1">from</span></span><span class="koboSpan" id="kobo.667.1"> langchain.evaluation </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.668.1">import</span></span><span class="koboSpan" id="kobo.669.1"> load_evaluator</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.670.1"># Simulated chain-of-thought reasoning provided by the agent:</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.671.1">agent_reasoning = (</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.672.1">"The current interest rate is 0.25%. </span><span class="koboSpan" id="kobo.672.2">I determined this by recalling that recent monetary policies have aimed "</span></span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.673.1">"to stimulate economic growth by keeping borrowing costs low. </span><span class="koboSpan" id="kobo.673.2">A rate of 0.25% is consistent with the ongoing "</span></span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.674.1">"trend of low rates, which encourages consumer spending and business investment."</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.675.1">)</span></p>
<div aria-label="335" epub:type="pagebreak" id="page27-6" role="doc-pagebreak"/>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.676.1"># Expected reasoning reference:</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.677.1">expected_reasoning = (</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.678.1">"An ideal reasoning should mention that the Federal Reserve has maintained a low interest rate—around 0.25%—to "</span></span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.679.1">"support economic growth, and it should briefly explain the implications for borrowing costs and consumer spending."</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.680.1">)</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.681.1"># Load the chain-of-thought evaluator.</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.682.1">cot_evaluator = load_evaluator(</span><span class="hljs-string"><span class="koboSpan" id="kobo.683.1">"cot_qa"</span></span><span class="koboSpan" id="kobo.684.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.685.1">result_reasoning = cot_evaluator.evaluate_strings(</span></p>
<p class="snippet-code"> <span class="hljs-built_in"><span class="koboSpan" id="kobo.686.1">input</span></span><span class="koboSpan" id="kobo.687.1">=</span><span class="hljs-string"><span class="koboSpan" id="kobo.688.1">"What is the current Federal Reserve interest rate and why does it matter?"</span></span><span class="koboSpan" id="kobo.689.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.690.1">    prediction=agent_reasoning,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.691.1">    reference=expected_reasoning,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.692.1">)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.693.1">print</span></span><span class="koboSpan" id="kobo.694.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.695.1">"\nChain-of-Thought Reasoning Evaluation:"</span></span><span class="koboSpan" id="kobo.696.1">)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.697.1">print</span></span><span class="koboSpan" id="kobo.698.1">(result_reasoning)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.699.1">The returned </span><a id="_idIndexMarker698"/><span class="koboSpan" id="kobo.700.1">score and reasoning allow us to judge whether the agent’s thought process is sound and comprehensive:</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.701.1">Chain-of-Thought Reasoning Evaluation:</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.702.1">{'reasoning': "The student correctly identified the current Federal Reserve interest rate as 0.25%. </span><span class="koboSpan" id="kobo.702.2">They also correctly explained why this rate matters, stating that it is intended to stimulate economic growth by keeping borrowing costs low, which in turn encourages consumer spending and business investment. </span><span class="koboSpan" id="kobo.702.3">This explanation aligns with the context provided, which asked for a brief explanation of the implications for borrowing costs and consumer spending. </span><span class="koboSpan" id="kobo.702.4">Therefore, the student's answer is factually accurate.\nGRADE: CORRECT", 'value': 'CORRECT', 'score': 1}</span></p>
<p class="normal"><span class="koboSpan" id="kobo.703.1">Please note that in this </span><a id="_idIndexMarker699"/><span class="koboSpan" id="kobo.704.1">evaluation, the agent provides detailed reasoning along with its answer. </span><span class="koboSpan" id="kobo.704.2">The evaluator (using chain-of-thought evaluation) compares the agent’s r</span><a id="_idTextAnchor436"/><span class="koboSpan" id="kobo.705.1">easoning with an expected explanation.</span></p>
<div aria-label="336" epub:type="pagebreak" id="page28-6" role="doc-pagebreak"/>
<h1 class="heading-1" id="_idParaDest-218"><a id="_idTextAnchor437"/><span class="koboSpan" id="kobo.706.1">Offline evaluation</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.707.1">Offline evaluation</span><a id="_idIndexMarker700"/><span class="koboSpan" id="kobo.708.1"> involves assessing the agent’s performance under controlled conditions before deployment. </span><span class="koboSpan" id="kobo.708.2">This includes benchmarking to establish general performance baselines and more targeted testing based on generated test cases. </span><span class="koboSpan" id="kobo.708.3">Offline evaluations provide key metrics, error analyses, and pass/fail summaries from controlled test scenarios, establishing baseline performance.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.709.1">While human assessments are sometimes seen as the gold standard, they are hard to scale and require careful design to avoid bias from subjective preferences or authoritative tones. </span><span class="koboSpan" id="kobo.709.2">Benchmarking involves comparing the performance of LLMs against standardized tests or tasks. </span><span class="koboSpan" id="kobo.709.3">This helps identify the strengths and weaknesses of the models and guides further development and improvement.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.710.1">In the next section, we’ll discuss creating an effective evaluation dataset withi</span><a id="_idTextAnchor438"/><span class="koboSpan" id="kobo.711.1">n the context of RAG system evaluation.</span></p>
<h2 class="heading-2" id="_idParaDest-219"><a id="_idTextAnchor439"/><span class="koboSpan" id="kobo.712.1">Evaluating RAG systems</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.713.1">The dimensions of </span><a id="_idIndexMarker701"/><span class="koboSpan" id="kobo.714.1">RAG evaluation discussed earlier (retrieval quality, contextual relevance, faithful generation, and information synthesis) provided a foundation for understanding how to measure RAG system effectiveness. </span><span class="koboSpan" id="kobo.714.2">Understanding failure patterns of RAG systems helps create more effective evaluation strategies. </span><span class="koboSpan" id="kobo.714.3">Barnett and colleagues in their 2024 paper </span><em class="italic"><span class="koboSpan" id="kobo.715.1">Seven Failure Points When Engineering a Retrieval Augmented Generation System</span></em><span class="koboSpan" id="kobo.716.1"> identified several distinct ways RAG systems fail in production environments:</span></p>
<ul>
<li class="b lletList"><span class="koboSpan" id="kobo.717.1">First, </span><strong class="keyWord"><span class="koboSpan" id="kobo.718.1">missing content</span></strong> <strong class="keyWord"><span class="koboSpan" id="kobo.719.1">failures </span></strong><span class="koboSpan" id="kobo.720.1">occur when the system fails to retrieve relevant information that exists in the knowledge base. </span><span class="koboSpan" id="kobo.720.2">This might happen because of chunking strategies that split related information, embedding models that miss semantic connections, or content gaps in the knowledge base itself.</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.721.1">Second, </span><strong class="keyWord"><span class="koboSpan" id="kobo.722.1">ranking failures</span></strong><span class="koboSpan" id="kobo.723.1"> happen when relevant documents exist but aren’t ranked highly enough to be included in the context window. </span><span class="koboSpan" id="kobo.723.2">This commonly stems from suboptimal embedding models, vocabulary mismatches between queries and documents, or poor chunking granularity.</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.724.1">Context window limitations</span></strong><span class="koboSpan" id="kobo.725.1"> create another failure mode when key information is spread across documents that exceed the model’s context limit. </span><span class="koboSpan" id="kobo.725.2">This forces difficult tradeoffs between including more documents and maintaining sufficient detail from each one.</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.726.1">Perhaps most critically, </span><strong class="keyWord"><span class="koboSpan" id="kobo.727.1">information extraction failures</span></strong><span class="koboSpan" id="kobo.728.1"> occur when relevant information is retrieved but the LLM fails to properly synthesize it. </span><span class="koboSpan" id="kobo.728.2">This might happen due to ineffective prompting, complex information formats, or conflicting information across documents.</span></li>
</ul>
<div aria-label="337" epub:type="pagebreak" id="page29-6" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.729.1">To effectively evaluate and address these specific failure modes, we need a structured and comprehensive evaluation approach. </span><span class="koboSpan" id="kobo.729.2">The following example demonstrates how to build a carefully designed evaluation dataset in LangSmith that allows for testing each of these failure patterns in the context of financial advisory systems. </span><span class="koboSpan" id="kobo.729.3">By creating realistic questions with expected answers and relevant metadata, we can systematically identify which failure modes most </span><a id="_idIndexMarker702"/><span class="koboSpan" id="kobo.730.1">frequently </span><a id="_idIndexMarker703"/><span class="koboSpan" id="kobo.731.1">affect our particular implementation:</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.732.1"># Define structured examples with queries, reference answers, and contexts</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.733.1">financial_examples = [</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.734.1">    {</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.735.1">"inputs"</span></span><span class="koboSpan" id="kobo.736.1">: {</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.737.1">"question"</span></span><span class="koboSpan" id="kobo.738.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.739.1">"What are the tax implications of early 401(k) withdrawal?"</span></span><span class="koboSpan" id="kobo.740.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.741.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.742.1">context_needed"</span></span><span class="koboSpan" id="kobo.743.1">: [</span><span class="hljs-string"><span class="koboSpan" id="kobo.744.1">"retirement"</span></span><span class="koboSpan" id="kobo.745.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.746.1">"taxation"</span></span><span class="koboSpan" id="kobo.747.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.748.1">"penalties"</span></span><span class="koboSpan" id="kobo.749.1">]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.750.1">        },</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.751.1">"outputs"</span></span><span class="koboSpan" id="kobo.752.1">: {</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.753.1">"answer"</span></span><span class="koboSpan" id="kobo.754.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.755.1">"Early withdrawals from a 401(k) typically incur a 10% penalty if you're under 59½ years old, in addition to regular income taxes. </span><span class="koboSpan" id="kobo.755.2">However, certain hardship withdrawals may qualify for penalty exemptions."</span></span><span class="koboSpan" id="kobo.756.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.757.1">"key_points"</span></span><span class="koboSpan" id="kobo.758.1">: [</span><span class="hljs-string"><span class="koboSpan" id="kobo.759.1">"10% penalty"</span></span><span class="koboSpan" id="kobo.760.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.761.1">"income tax"</span></span><span class="koboSpan" id="kobo.762.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.763.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.764.1">hardship exemptions"</span></span><span class="koboSpan" id="kobo.765.1">],</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.766.1">"documents"</span></span><span class="koboSpan" id="kobo.767.1">: [</span><span class="hljs-string"><span class="koboSpan" id="kobo.768.1">"IRS publication 575"</span></span><span class="koboSpan" id="kobo.769.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.770.1">"Retirement plan guidelines"</span></span><span class="koboSpan" id="kobo.771.1">]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.772.1">        }</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.773.1">    },</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.774.1">    {</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.775.1">"inputs"</span></span><span class="koboSpan" id="kobo.776.1">: {</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.777.1">"question"</span></span><span class="koboSpan" id="kobo.778.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.779.1">"How does dollar-cost averaging compare to lump-sum investing?"</span></span><span class="koboSpan" id="kobo.780.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.781.1">"context_needed"</span></span><span class="koboSpan" id="kobo.782.1">: [</span><span class="hljs-string"><span class="koboSpan" id="kobo.783.1">"investment strategy"</span></span><span class="koboSpan" id="kobo.784.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.785.1">"risk management"</span></span><span class="koboSpan" id="kobo.786.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.787.1">"market timing"</span></span><span class="koboSpan" id="kobo.788.1">]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.789.1">        },</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.790.1">"outputs"</span></span><span class="koboSpan" id="kobo.791.1">: {</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.792.1">"answer"</span></span><span class="koboSpan" id="kobo.793.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.794.1">"Dollar-cost averaging spreads investments over time to reduce timing risk, while lump-sum investing typically outperforms in rising markets due to longer market exposure. </span><span class="koboSpan" id="kobo.794.2">DCA may provide psychological benefits through reduced volatility exposure."</span></span><span class="koboSpan" id="kobo.795.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.796.1">"key_points"</span></span><span class="koboSpan" id="kobo.797.1">: [</span><span class="hljs-string"><span class="koboSpan" id="kobo.798.1">"timing risk"</span></span><span class="koboSpan" id="kobo.799.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.800.1">"market exposure"</span></span><span class="koboSpan" id="kobo.801.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.802.1">"psychological benefits"</span></span><span class="koboSpan" id="kobo.803.1">],</span></p>
<div aria-label="338" epub:type="pagebreak" id="page30-6" role="doc-pagebreak"/>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.804.1">"documents"</span></span><span class="koboSpan" id="kobo.805.1">: [</span><span class="hljs-string"><span class="koboSpan" id="kobo.806.1">"Investment strategy comparisons"</span></span><span class="koboSpan" id="kobo.807.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.808.1">"Market timing research"</span></span><span class="koboSpan" id="kobo.809.1">]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.810.1">        }</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.811.1">    },</span></p>
<p class="snippet-code"> <span class="hljs-comment"><span class="koboSpan" id="kobo.812.1"># Additional examples would be added here</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.813.1">]</span></p>
<p class="normal"><span class="koboSpan" id="kobo.814.1">This dataset structure serves multiple evaluation purposes. </span><span class="koboSpan" id="kobo.814.2">First, it identifies specific documents that should be retrieved, allowing evaluation of retrieval accuracy. </span><span class="koboSpan" id="kobo.814.3">It then defines key points that should appear in the response, enabling assessment of information extraction. </span><span class="koboSpan" id="kobo.814.4">Finally, it connects each example to testing objectives, making it easier to diagnose specific system capabilities.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.815.1">When implementing this dataset in practice, organizations typically load these examples into evaluation platforms like LangSmith, allowing automated testing of their RAG systems. </span><span class="koboSpan" id="kobo.815.2">The results reveal specific patterns in system performance—perhaps strong retrieval but weak synthesis, or excellent performance on simple factual questions but struggles with complex perspective inquiries.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.816.1">However, implementing effective RAG evaluation goes beyond simply creating datasets; it requires using diagnostic tools to pinpoint exactly where failures occur within the system pipeline. </span><span class="koboSpan" id="kobo.816.2">Drawing on research, these diagnostics identify specific failure modes, such as poor document ranking (information exists but isn’t prioritized) or poor context utilization (the agent ignores relevant retrieved documents). </span><span class="koboSpan" id="kobo.816.3">By diagnosing these issues, organizations gain actionable insights—for instance, consistent ranking failures might suggest implementing hybrid search, while context utilization problems could lead to refined prompting or structured outputs.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.817.1">The ultimate goal of RAG evaluation is to drive continuous improvement. </span><span class="koboSpan" id="kobo.817.2">Organizations achieving the most success follow an iterative cycle: running comprehensive diagnostics to find specific failure patterns, prioritizing fixes based on their frequency and impact, implementing targeted changes, and </span><a id="_idIndexMarker704"/><span class="koboSpan" id="kobo.818.1">then re-evaluating to measure the </span><a id="_idIndexMarker705"/><span class="koboSpan" id="kobo.819.1">improvement. </span><span class="koboSpan" id="kobo.819.2">By systematically diagnosing issues and using those insights to iterate, teams can build more accurate and reliable RAG systems with fewer common errors.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.820.1">In the next section, we’ll see how we can use </span><strong class="keyWord"><span class="koboSpan" id="kobo.821.1">LangSmith</span></strong><span class="koboSpan" id="kobo.822.1">, a companion project for LangChain, to benchmark and evaluate our system’s performance </span><a id="_idTextAnchor440"/><span class="koboSpan" id="kobo.823.1">on a dataset. </span><span class="koboSpan" id="kobo.823.2">Let’s step through an example!</span></p>
<div aria-label="339" epub:type="pagebreak" id="page31-6" role="doc-pagebreak"/>
<h2 class="heading-2" id="_idParaDest-220"><a id="_idTextAnchor441"/><span class="koboSpan" id="kobo.824.1">Evaluating a benchmark in LangSmith</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.825.1">As we’ve mentioned, comprehensive</span><a id="_idIndexMarker706"/><span class="koboSpan" id="kobo.826.1"> benchmarking and evaluation, including testing, are critical for safety, robustness, and intended behavior. </span><span class="koboSpan" id="kobo.826.2">LangSmith, despite being a platform designed for testing, debugging, monitoring, and improving LLM applications, offers tools for evaluation and dataset management. </span><span class="koboSpan" id="kobo.826.3">LangSmith integrates seamlessly with LangChain Benchmarks, providing a cohesive framework for developing and assessing LLM applications.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.827.1">We can run evaluations against benchmark datasets in LangSmith, as we’ll see now. </span><span class="koboSpan" id="kobo.827.2">First, please make sure you create an account on LangSmith here: </span><a href="https://smith.langchain.com/"><span class="url"><span class="koboSpan" id="kobo.828.1">https://smith.langchain.com/</span></span></a><span class="koboSpan" id="kobo.829.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.830.1">You can obtain an API key and set it as </span><code class="inlineCode"><span class="koboSpan" id="kobo.831.1">LANGCHAIN_API_KEY</span></code><span class="koboSpan" id="kobo.832.1"> in your environment. </span><span class="koboSpan" id="kobo.832.2">We can also set environment variables for project ID and tracing:</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.833.1"># Basic LangSmith Integration Example</span></span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.834.1">import</span></span><span class="koboSpan" id="kobo.835.1"> os</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.836.1"># Set up environment variables for LangSmith tracing</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.837.1">os.environ[</span><span class="hljs-string"><span class="koboSpan" id="kobo.838.1">"LANGCHAIN_TRACING_V2"</span></span><span class="koboSpan" id="kobo.839.1">] = </span><span class="hljs-string"><span class="koboSpan" id="kobo.840.1">"true"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.841.1">os.environ[</span><span class="hljs-string"><span class="koboSpan" id="kobo.842.1">"LANGCHAIN_PROJECT"</span></span><span class="koboSpan" id="kobo.843.1">] = </span><span class="hljs-string"><span class="koboSpan" id="kobo.844.1">"LLM Evaluation Example"</span></span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.845.1">print</span></span><span class="koboSpan" id="kobo.846.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.847.1">"Setting up LangSmith tracing..."</span></span><span class="koboSpan" id="kobo.848.1">)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.849.1">This configuration establishes a connection to LangSmith and directs all traces to a specific project. </span><span class="koboSpan" id="kobo.849.2">When no project ID is explicitly defined, LangChain logs against the default project. </span><span class="koboSpan" id="kobo.849.3">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.850.1">LANGCHAIN_TRACING_V2</span></code><span class="koboSpan" id="kobo.851.1"> flag enables the most recent version of LangSmith’s tracing capabilities.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.852.1">After configuring</span><a id="_idIndexMarker707"/><span class="koboSpan" id="kobo.853.1"> the environment, we can begin logging</span><a id="_idIndexMarker708"/><span class="koboSpan" id="kobo.854.1"> interactions with our LLM applications. </span><span class="koboSpan" id="kobo.854.2">Each interaction creates a traceable record in LangSmith:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.855.1">from</span></span><span class="koboSpan" id="kobo.856.1"> langchain_openai </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.857.1">import</span></span><span class="koboSpan" id="kobo.858.1"> ChatOpenAI</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.859.1">from</span></span><span class="koboSpan" id="kobo.860.1"> langsmith </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.861.1">import</span></span><span class="koboSpan" id="kobo.862.1"> Client</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.863.1"># Create a simple LLM call that will be traced in LangSmith</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.864.1">llm = ChatOpenAI()</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.865.1">response = llm.invoke(</span><span class="hljs-string"><span class="koboSpan" id="kobo.866.1">"Hello, world!"</span></span><span class="koboSpan" id="kobo.867.1">)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.868.1">print</span></span><span class="koboSpan" id="kobo.869.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.870.1">f"Model response: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.871.1">{response.content}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.872.1">"</span></span><span class="koboSpan" id="kobo.873.1">)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.874.1">print</span></span><span class="koboSpan" id="kobo.875.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.876.1">"\nThis run has been logged to LangSmith."</span></span><span class="koboSpan" id="kobo.877.1">)</span></p>
<div aria-label="340" epub:type="pagebreak" id="page32-6" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.878.1">When this code executes, it performs a simple interaction with the ChatOpenAI model and automatically logs the request, response, and performance metrics to LangSmith. </span><span class="koboSpan" id="kobo.878.2">These logs appear in the LangSmith project dashboard at </span><a href="https://smith.langchain.com/projects"><span class="url"><span class="koboSpan" id="kobo.879.1">https://smith.langchain.com/projects</span></span></a><span class="koboSpan" id="kobo.880.1">, allowing for detailed inspection of each interaction.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.881.1">We can create a dataset from existing agent runs with the </span><code class="inlineCode"><span class="koboSpan" id="kobo.882.1">create_example_from_run()</span></code><span class="koboSpan" id="kobo.883.1"> function—or from anything else. </span><span class="koboSpan" id="kobo.883.2">Here’s how to create a dataset with a set of questions:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.884.1">from</span></span><span class="koboSpan" id="kobo.885.1"> langsmith </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.886.1">import</span></span><span class="koboSpan" id="kobo.887.1"> Client</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.888.1">client = Client()</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.889.1"># Create dataset in LangSmith</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.890.1">dataset_name = </span><span class="hljs-string"><span class="koboSpan" id="kobo.891.1">"Financial Advisory RAG Evaluation"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.892.1">dataset = client.create_dataset(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.893.1">    dataset_name=dataset_name,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.894.1">    description=</span><span class="hljs-string"><span class="koboSpan" id="kobo.895.1">"Evaluation dataset for financial advisory RAG systems covering retirement, investments, and tax planning."</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.896.1">)</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.897.1"># Add examples to the dataset</span></span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.898.1">for</span></span><span class="koboSpan" id="kobo.899.1"> example </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.900.1">in</span></span><span class="koboSpan" id="kobo.901.1"> financial_examples:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.902.1">    client.create_example(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.903.1">        inputs=example[</span><span class="hljs-string"><span class="koboSpan" id="kobo.904.1">"inputs"</span></span><span class="koboSpan" id="kobo.905.1">],</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.906.1">        outputs=example[</span><span class="hljs-string"><span class="koboSpan" id="kobo.907.1">"outputs"</span></span><span class="koboSpan" id="kobo.908.1">],</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.909.1">        dataset_id=dataset.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.910.1">id</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.911.1">    )</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.912.1">print</span></span><span class="koboSpan" id="kobo.913.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.914.1">f"Created evaluation dataset with </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.915.1">{</span></span><span class="hljs-built_in"><span class="koboSpan" id="kobo.916.1">len</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.917.1">(financial_examples)}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.918.1"> examples"</span></span><span class="koboSpan" id="kobo.919.1">)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.920.1">This code creates a new evaluation dataset in LangSmith containing financial advisory questions. </span><span class="koboSpan" id="kobo.920.2">Each example includes an input query and an expected output answer, establishing a reference standard against which we can evaluate our LLM application responses.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.921.1">We can now define our RAG system with a function like this:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.922.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.923.1">construct_chain</span></span><span class="koboSpan" id="kobo.924.1">():</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.925.1">return</span></span> <span class="hljs-literal"><span class="koboSpan" id="kobo.926.1">None</span></span></p>
<div aria-label="341" epub:type="pagebreak" id="page33-6" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.927.1">In a complete implementation, you would prepare a vector store with relevant financial documents, create appropriate prompt templates, and configure the retrieval and response generation</span><a id="_idIndexMarker709"/><span class="koboSpan" id="kobo.928.1"> components. </span><span class="koboSpan" id="kobo.928.2">The concepts and techniques for building</span><a id="_idIndexMarker710"/><span class="koboSpan" id="kobo.929.1"> robust RAG systems are covered extensively in </span><a href="E_Chapter_4.xhtml#_idTextAnchor152"><em class="italic"><span class="koboSpan" id="kobo.930.1">Chapter 4</span></em></a><span class="koboSpan" id="kobo.931.1">, which provides step-by-step guidance on document processing, embedding creation, vector store setup, and chain construction.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.932.1">We can make changes to our chain and evaluate changes in the application. </span><span class="koboSpan" id="kobo.932.2">Does the change improve the result or not? </span><span class="koboSpan" id="kobo.932.3">Changes can be in any part of our application, be it a new model, a new prompt template, or a new chain or agent. </span><span class="koboSpan" id="kobo.932.4">We can run two versions of the application with the same input examples and save the results of the runs. </span><span class="koboSpan" id="kobo.932.5">Then we evaluate the results by comparing them side by side.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.933.1">To run an evaluation on a dataset, we can either specify an LLM or—for parallelism—use a constructor function to initialize the model or LLM app for each input. </span><span class="koboSpan" id="kobo.933.2">Now, to evaluate the performance against our dataset, we need to define an evaluator as we saw in the previous section:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.934.1">from</span></span><span class="koboSpan" id="kobo.935.1"> langchain.smith </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.936.1">import</span></span><span class="koboSpan" id="kobo.937.1"> RunEvalConfig</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.938.1"># Define evaluation criteria specific to RAG systems</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.939.1">evaluation_config = RunEvalConfig(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.940.1">    evaluators=[</span></p>
<p class="snippet-code"> <span class="hljs-comment"><span class="koboSpan" id="kobo.941.1"># Correctness: Compare response to reference answer</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.942.1">        RunEvalConfig.LLM(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.943.1">            criteria={</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.944.1">"factual_accuracy"</span></span><span class="koboSpan" id="kobo.945.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.946.1">"Does the response contain only factually correct information consistent with the reference answer?"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.947.1">            }</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.948.1">        ),</span></p>
<p class="snippet-code"> <span class="hljs-comment"><span class="koboSpan" id="kobo.949.1"># Groundedness: Ensure response is supported by retrieved context</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.950.1">        RunEvalConfig.LLM(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.951.1">            criteria={</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.952.1">"groundedness"</span></span><span class="koboSpan" id="kobo.953.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.954.1">"Is the response fully supported by the retrieved documents without introducing unsupported information?"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.955.1">            }</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.956.1">        ),</span></p>
<p class="snippet-code"> <span class="hljs-comment"><span class="koboSpan" id="kobo.957.1"># Retrieval quality: Assess relevance of retrieved documents</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.958.1">        RunEvalConfig.LLM(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.959.1">            criteria={</span></p>
<div aria-label="342" epub:type="pagebreak" id="page34-6" role="doc-pagebreak"/>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.960.1">"retrieval_relevance"</span></span><span class="koboSpan" id="kobo.961.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.962.1">"Are the retrieved documents relevant to answering the question?"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.963.1">            }</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.964.1">        )</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.965.1">    ]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.966.1">)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.967.1">This shows how to configure multi-dimensional evaluation for RAG systems, assessing factual accuracy, groundedness, and</span><a id="_idIndexMarker711"/><span class="koboSpan" id="kobo.968.1"> retrieval quality using LLM-based </span><a id="_idIndexMarker712"/><span class="koboSpan" id="kobo.969.1">judges. </span><span class="koboSpan" id="kobo.969.2">The criteria are defined by a dictionary that includes a criterion as a key and a question to check for as the value.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.970.1">We’ll now pass a dataset together with the evaluation configuration with evaluators to </span><code class="inlineCode"><span class="koboSpan" id="kobo.971.1">run_on_dataset()</span></code><span class="koboSpan" id="kobo.972.1"> to generate metrics and feedback:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.973.1">from</span></span><span class="koboSpan" id="kobo.974.1"> langchain.smith </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.975.1">import</span></span><span class="koboSpan" id="kobo.976.1"> run_on_dataset</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.977.1">results = run_on_dataset(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.978.1">    client=client,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.979.1">    dataset_name=dataset_name,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.980.1">    dataset=dataset,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.981.1">    llm_or_chain_factory=construct_chain,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.982.1">    evaluation=evaluation_config</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.983.1">)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.984.1">In the same way, we could pass a dataset and evaluators to </span><code class="inlineCode"><span class="koboSpan" id="kobo.985.1">run_on_dataset()</span></code><span class="koboSpan" id="kobo.986.1"> to generate metrics and feedback asynchronously.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.987.1">This practical implementation provides a framework you can adapt for your specific domain. </span><span class="koboSpan" id="kobo.987.2">By creating a comprehensive evaluation dataset and assessing your RAG system across multiple dimensions (correctness, groundedness, and retrieval quality), you can identify specific areas for improvement and track progress as you refine your system.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.988.1">When implementing this approach, consider incorporating real user queries from your application logs (appropriately anonymized) to ensure your evaluation dataset reflects actual usage patterns. </span><span class="koboSpan" id="kobo.988.2">Additionally, periodically refreshing your dataset with new queries and updated information helps prevent overfitting and ensures your evaluation remains relevant as user needs evolve.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.989.1">Let’s use the </span><a id="_idIndexMarker713"/><span class="koboSpan" id="kobo.990.1">datasets and evaluate libraries by HuggingFace </span><a id="_idIndexMarker714"/><span class="koboSpan" id="kobo.991.1">to check a</span><a id="_idTextAnchor442"/><span class="koboSpan" id="kobo.992.1"> coding LLM approach to solving programming problems.</span></p>
<div aria-label="343" epub:type="pagebreak" id="page35-6" role="doc-pagebreak"/>
<h2 class="heading-2" id="_idParaDest-221"><a id="_idTextAnchor443"/><span class="koboSpan" id="kobo.993.1">Evaluating a benchmark with HF datasets and Evaluate</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.994.1">As a reminder: the </span><code class="inlineCode"><span class="koboSpan" id="kobo.995.1">pass@k</span></code><span class="koboSpan" id="kobo.996.1"> metric</span><a id="_idIndexMarker715"/><span class="koboSpan" id="kobo.997.1"> is a way to evaluate the performance of an LLM in solving programming exercises. </span><span class="koboSpan" id="kobo.997.2">It measures the proportion of exercises for which the LLM generated at least one correct solution within the top </span><code class="inlineCode"><span class="koboSpan" id="kobo.998.1">k</span></code><span class="koboSpan" id="kobo.999.1"> candidates. </span><span class="koboSpan" id="kobo.999.2">A higher </span><code class="inlineCode"><span class="koboSpan" id="kobo.1000.1">pass@k</span></code><span class="koboSpan" id="kobo.1001.1"> score indicates better performance, as it means the LLM was able to generate a correct solution more often within the top </span><code class="inlineCode"><span class="koboSpan" id="kobo.1002.1">k</span></code><span class="koboSpan" id="kobo.1003.1"> candidates.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1004.1">Hugging Face’s </span><code class="inlineCode"><span class="koboSpan" id="kobo.1005.1">Evaluate</span></code><span class="koboSpan" id="kobo.1006.1"> library makes it very easy to calculate </span><code class="inlineCode"><span class="koboSpan" id="kobo.1007.1">pass@k</span></code><span class="koboSpan" id="kobo.1008.1"> and other metrics. </span><span class="koboSpan" id="kobo.1008.2">Here’s an example:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1009.1">from</span></span><span class="koboSpan" id="kobo.1010.1"> datasets </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1011.1">import</span></span><span class="koboSpan" id="kobo.1012.1"> load_dataset</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1013.1">from</span></span><span class="koboSpan" id="kobo.1014.1"> evaluate </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1015.1">import</span></span><span class="koboSpan" id="kobo.1016.1"> load</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1017.1">from</span></span><span class="koboSpan" id="kobo.1018.1"> langchain_core.messages </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1019.1">import</span></span><span class="koboSpan" id="kobo.1020.1"> HumanMessage</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1021.1">human_eval = load_dataset(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1022.1">"openai_humaneval"</span></span><span class="koboSpan" id="kobo.1023.1">, split=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1024.1">"test"</span></span><span class="koboSpan" id="kobo.1025.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1026.1">code_eval_metric = load(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1027.1">"code_eval"</span></span><span class="koboSpan" id="kobo.1028.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1029.1">test_cases = [</span><span class="hljs-string"><span class="koboSpan" id="kobo.1030.1">"assert add(2,3)==5"</span></span><span class="koboSpan" id="kobo.1031.1">]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1032.1">candidates = [[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1033.1">"def add(a,b): return a*b"</span></span><span class="koboSpan" id="kobo.1034.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1035.1">"def add(a, b): return a+b"</span></span><span class="koboSpan" id="kobo.1036.1">]]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1037.1">pass_at_k, results = code_eval_metric.compute(references=test_cases, predictions=candidates, k=[</span><span class="hljs-number"><span class="koboSpan" id="kobo.1038.1">1</span></span><span class="koboSpan" id="kobo.1039.1">, </span><span class="hljs-number"><span class="koboSpan" id="kobo.1040.1">2</span></span><span class="koboSpan" id="kobo.1041.1">])</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.1042.1">print</span></span><span class="koboSpan" id="kobo.1043.1">(pass_at_k)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1044.1">We should get an output like this:</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1045.1">{'pass@1': 0.5, 'pass@2': 1.0}</span></p>
<div>
<div class="note" id="_idContainer106">
<p class="normal"><span class="koboSpan" id="kobo.1046.1">For this code to run, you need to set the </span><code class="inlineCode"><span class="koboSpan" id="kobo.1047.1">HF_ALLOW_CODE_EVAL</span></code><span class="koboSpan" id="kobo.1048.1"> environment variable to 1. </span><span class="koboSpan" id="kobo.1048.2">Please be cautious: running LLM code on your machine comes with a risk.</span></p>
</div>
</div>
<p class="normal"><span class="koboSpan" id="kobo.1049.1">This shows how to evaluate code generation models using HuggingFace’s </span><code class="inlineCode"><span class="koboSpan" id="kobo.1050.1">code_eval</span></code><span class="koboSpan" id="kobo.1051.1"> metric, which measures a</span><a id="_idIndexMarker716"/><span class="koboSpan" id="kobo.1052.1"> model’s ability to produce functioning co</span><a id="_idTextAnchor444"/><span class="koboSpan" id="kobo.1053.1">de solutions. </span><span class="koboSpan" id="kobo.1053.2">This is great. </span><span class="koboSpan" id="kobo.1053.3">Let’s see another example.</span></p>
<div aria-label="344" epub:type="pagebreak" id="page36-6" role="doc-pagebreak"/>
<h2 class="heading-2" id="_idParaDest-222"><a id="_idTextAnchor445"/><span class="koboSpan" id="kobo.1054.1">Evaluating email extraction</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.1055.1">Let’s show how we can use it </span><a id="_idIndexMarker717"/><span class="koboSpan" id="kobo.1056.1">to evaluate an LLM’s ability to extract structured information from insurance claim texts.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1057.1">We’ll first create a synthetic dataset using LangSmith. </span><span class="koboSpan" id="kobo.1057.2">In this synthetic dataset, each example consists of a raw insurance claim text (input) and its corresponding expected structured output (output). </span><span class="koboSpan" id="kobo.1057.3">We will use this dataset to run extraction chains and evaluate your model’s performance.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1058.1">We assume that you’ve</span><a id="_idIndexMarker718"/><span class="koboSpan" id="kobo.1059.1"> already set up your LangSmith credentials.</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1060.1">from</span></span><span class="koboSpan" id="kobo.1061.1"> langsmith </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1062.1">import</span></span><span class="koboSpan" id="kobo.1063.1"> Client</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1064.1"># Define a list of synthetic insurance claim examples</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1065.1">example_inputs = [</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1066.1">    (</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1067.1">"I was involved in a car accident on 2023-08-15. </span><span class="koboSpan" id="kobo.1067.2">My name is Jane Smith, Claim ID INS78910, "</span></span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1068.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1069.1">Policy Number POL12345, and the damage is estimated at $3500."</span></span><span class="koboSpan" id="kobo.1070.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1071.1">        {</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1072.1">"claimant_name"</span></span><span class="koboSpan" id="kobo.1073.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1074.1">"Jane Smith"</span></span><span class="koboSpan" id="kobo.1075.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1076.1">"claim_id"</span></span><span class="koboSpan" id="kobo.1077.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1078.1">"INS78910"</span></span><span class="koboSpan" id="kobo.1079.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1080.1">"policy_number"</span></span><span class="koboSpan" id="kobo.1081.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1082.1">"POL12345"</span></span><span class="koboSpan" id="kobo.1083.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1084.1">"claim_amount"</span></span><span class="koboSpan" id="kobo.1085.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1086.1">"$3500"</span></span><span class="koboSpan" id="kobo.1087.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1088.1">"accident_date"</span></span><span class="koboSpan" id="kobo.1089.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1090.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1091.1">2023-08-15"</span></span><span class="koboSpan" id="kobo.1092.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1093.1">"accident_description"</span></span><span class="koboSpan" id="kobo.1094.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1095.1">"Car accident causing damage"</span></span><span class="koboSpan" id="kobo.1096.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1097.1">"status"</span></span><span class="koboSpan" id="kobo.1098.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1099.1">"pending"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1100.1">        }</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1101.1">    ),</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1102.1">    (</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1103.1">"My motorcycle was hit in a minor collision on 2023-07-20. </span><span class="koboSpan" id="kobo.1103.2">I am John Doe, with Claim ID INS112233 "</span></span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1104.1">"and Policy Number POL99887. </span><span class="koboSpan" id="kobo.1104.2">The estimated damage is $1500."</span></span><span class="koboSpan" id="kobo.1105.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1106.1">        {</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1107.1">"claimant_name"</span></span><span class="koboSpan" id="kobo.1108.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1109.1">"John Doe"</span></span><span class="koboSpan" id="kobo.1110.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1111.1">"claim_id"</span></span><span class="koboSpan" id="kobo.1112.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1113.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1114.1">INS112233"</span></span><span class="koboSpan" id="kobo.1115.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1116.1">"policy_number"</span></span><span class="koboSpan" id="kobo.1117.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1118.1">"POL99887"</span></span><span class="koboSpan" id="kobo.1119.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1120.1">"claim_amount"</span></span><span class="koboSpan" id="kobo.1121.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1122.1">"$1500"</span></span><span class="koboSpan" id="kobo.1123.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1124.1">"accident_date"</span></span><span class="koboSpan" id="kobo.1125.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1126.1">"2023-07-20"</span></span><span class="koboSpan" id="kobo.1127.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1128.1">"accident_description"</span></span><span class="koboSpan" id="kobo.1129.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1130.1">"Minor motorcycle collision"</span></span><span class="koboSpan" id="kobo.1131.1">,</span></p>
<div aria-label="345" epub:type="pagebreak" id="page37-6" role="doc-pagebreak"/>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1132.1">"status"</span></span><span class="koboSpan" id="kobo.1133.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1134.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1135.1">pending"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1136.1">        }</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1137.1">    )</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1138.1">]</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1139.1">We can upload this dataset to LangSmith:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1140.1">client = Client()</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1141.1">dataset_name = </span><span class="hljs-string"><span class="koboSpan" id="kobo.1142.1">"Insurance Claims"</span></span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1143.1"># Create the dataset in LangSmith</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1144.1">dataset = client.create_dataset(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1145.1">    dataset_name=dataset_name,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1146.1">    description=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1147.1">"Synthetic dataset for insurance claim extraction tasks"</span></span><span class="koboSpan" id="kobo.1148.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1149.1">)</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1150.1"># Store examples in the dataset</span></span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1151.1">for</span></span><span class="koboSpan" id="kobo.1152.1"> input_text, expected_output </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1153.1">in</span></span><span class="koboSpan" id="kobo.1154.1"> example_inputs:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1155.1">    client.create_example(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1156.1">        inputs={</span><span class="hljs-string"><span class="koboSpan" id="kobo.1157.1">"input"</span></span><span class="koboSpan" id="kobo.1158.1">: input_text},</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1159.1">        outputs={</span><span class="hljs-string"><span class="koboSpan" id="kobo.1160.1">"output"</span></span><span class="koboSpan" id="kobo.1161.1">: expected_output},</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1162.1">        metadata={</span><span class="hljs-string"><span class="koboSpan" id="kobo.1163.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1164.1">source"</span></span><span class="koboSpan" id="kobo.1165.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1166.1">"Synthetic"</span></span><span class="koboSpan" id="kobo.1167.1">},</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1168.1">        dataset_id=dataset.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1169.1">id</span></span><span class="koboSpan" id="kobo.1170.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1171.1">    )</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1172.1">Now let’s </span><a id="_idIndexMarker719"/><span class="koboSpan" id="kobo.1173.1">run our </span><code class="inlineCode"><span class="koboSpan" id="kobo.1174.1">InsuranceClaim</span></code><span class="koboSpan" id="kobo.1175.1"> dataset on LangSmith. </span><span class="koboSpan" id="kobo.1175.2">We’ll first</span><a id="_idIndexMarker720"/><span class="koboSpan" id="kobo.1176.1"> define a schema for our claims:</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1177.1"># Define the extraction schema</span></span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1178.1">from</span></span><span class="koboSpan" id="kobo.1179.1"> pydantic </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1180.1">import</span></span><span class="koboSpan" id="kobo.1181.1"> BaseModel, Field</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1182.1">class</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1183.1">InsuranceClaim</span></span><span class="koboSpan" id="kobo.1184.1">(</span><span class="hljs-title"><span class="koboSpan" id="kobo.1185.1">BaseModel</span></span><span class="koboSpan" id="kobo.1186.1">):</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1187.1">    claimant_name: </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1188.1">str</span></span><span class="koboSpan" id="kobo.1189.1"> = Field(..., description=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1190.1">"The name of the claimant"</span></span><span class="koboSpan" id="kobo.1191.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1192.1">    claim_id: </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1193.1">str</span></span><span class="koboSpan" id="kobo.1194.1"> = Field(..., description=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1195.1">"The unique insurance claim identifier"</span></span><span class="koboSpan" id="kobo.1196.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1197.1">    policy_number: </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1198.1">str</span></span><span class="koboSpan" id="kobo.1199.1"> = Field(..., description=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1200.1">"The policy number associated with the claim"</span></span><span class="koboSpan" id="kobo.1201.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1202.1">    claim_amount: </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1203.1">str</span></span><span class="koboSpan" id="kobo.1204.1"> = Field(..., description=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1205.1">"The claimed amount (e.g., '$5000')"</span></span><span class="koboSpan" id="kobo.1206.1">)</span></p>
<div aria-label="346" epub:type="pagebreak" id="page38-6" role="doc-pagebreak"/>
<p class="snippet-code"><span class="koboSpan" id="kobo.1207.1">    accident_date: </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1208.1">str</span></span><span class="koboSpan" id="kobo.1209.1"> = Field(..., description=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1210.1">"The date of the accident (YYYY-MM-DD)"</span></span><span class="koboSpan" id="kobo.1211.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1212.1">    accident_description: </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1213.1">str</span></span><span class="koboSpan" id="kobo.1214.1"> = Field(..., description=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1215.1">"A brief description of the accident"</span></span><span class="koboSpan" id="kobo.1216.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1217.1">    status: </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1218.1">str</span></span><span class="koboSpan" id="kobo.1219.1"> = Field(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1220.1">"pending"</span></span><span class="koboSpan" id="kobo.1221.1">, description=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1222.1">"The current status of the claim"</span></span><span class="koboSpan" id="kobo.1223.1">)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1224.1">Now we’ll define our extraction chain. </span><span class="koboSpan" id="kobo.1224.2">We are keeping it very simple; we’ll just ask for a JSON object that follows the </span><code class="inlineCode"><span class="koboSpan" id="kobo.1225.1">InsuranceClaim</span></code><span class="koboSpan" id="kobo.1226.1"> schema. </span><span class="koboSpan" id="kobo.1226.2">The extraction chain is defined with ChatOpenAI LLM with function calling bound to our schema:</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1227.1"># Create extraction chain</span></span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1228.1">from</span></span><span class="koboSpan" id="kobo.1229.1"> langchain.chat_models </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1230.1">import</span></span><span class="koboSpan" id="kobo.1231.1"> ChatOpenAI</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1232.1">from</span></span><span class="koboSpan" id="kobo.1233.1"> langchain.output_parsers.openai_functions </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1234.1">import</span></span><span class="koboSpan" id="kobo.1235.1"> JsonOutputFunctionsParser</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1236.1">instructions = (</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1237.1">"Extract the following structured information from the insurance claim text: "</span></span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1238.1">"claimant_name, claim_id, policy_number, claim_amount, accident_date, "</span></span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1239.1">"accident_description, and status. </span><span class="koboSpan" id="kobo.1239.2">Return the result as a JSON object following "</span></span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1240.1">"this schema: "</span></span><span class="koboSpan" id="kobo.1241.1"> + InsuranceClaim.schema_json()</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1242.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1243.1">llm = ChatOpenAI(model=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1244.1">"gpt-4"</span></span><span class="koboSpan" id="kobo.1245.1">, temperature=</span><span class="hljs-number"><span class="koboSpan" id="kobo.1246.1">0</span></span><span class="koboSpan" id="kobo.1247.1">).bind_functions(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1248.1">    functions=[InsuranceClaim.schema()],</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1249.1">    function_call=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1250.1">"InsuranceClaim"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1251.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1252.1">output_parser = JsonOutputFunctionsParser()</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1253.1">extraction_chain = instructions | llm | output_parser | (</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1254.1">lambda</span></span><span class="koboSpan" id="kobo.1255.1"> x: {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1256.1">"output"</span></span><span class="koboSpan" id="kobo.1257.1">: x})</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1258.1">Finally, we can run </span><a id="_idIndexMarker721"/><span class="koboSpan" id="kobo.1259.1">the extraction chain on our sample </span><a id="_idIndexMarker722"/><span class="koboSpan" id="kobo.1260.1">insurance claim:</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1261.1"># Test the extraction chain</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1262.1">sample_claim_text = (</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1263.1">"I was involved in a car accident on 2023-08-15. </span><span class="koboSpan" id="kobo.1263.2">My name is Jane Smith, "</span></span></p>
<div aria-label="347" epub:type="pagebreak" id="page39-6" role="doc-pagebreak"/>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1264.1">"Claim ID INS78910, Policy Number POL12345, and the damage is estimated at $3500. </span><span class="koboSpan" id="kobo.1264.2">"</span></span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1265.1">"Please process my claim."</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1266.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1267.1">result = extraction_chain.invoke({</span><span class="hljs-string"><span class="koboSpan" id="kobo.1268.1">"input"</span></span><span class="koboSpan" id="kobo.1269.1">: sample_claim_text})</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.1270.1">print</span></span><span class="koboSpan" id="kobo.1271.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1272.1">"Extraction Result:"</span></span><span class="koboSpan" id="kobo.1273.1">)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.1274.1">print</span></span><span class="koboSpan" id="kobo.1275.1">(result)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1276.1">This showed how to evaluate structured information extraction from insurance claims text, using a Pydantic</span><a id="_idIndexMarker723"/><span class="koboSpan" id="kobo.1277.1"> schema to standardize extraction and</span><a id="_idIndexMarker724"/><span class="koboSpan" id="kobo.1278.1"> LangSmith to assess performance.</span></p>
<h1 class="heading-1" id="_idParaDest-223"><a id="_idTextAnchor446"/><span class="koboSpan" id="kobo.1279.1">Summary</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.1280.1">In this chapter, we outlined critical strategies for evaluating LLM applications, ensuring robust performance before production deployment. </span><span class="koboSpan" id="kobo.1280.2">We provided an overview of the importance of evaluation, architectural challenges, evaluation strategies, and types of evaluation. </span><span class="koboSpan" id="kobo.1280.3">We then demonstrated practical evaluation techniques through code examples, including correctness evaluation using exact matches and LLM-as-a-judge approaches. </span><span class="koboSpan" id="kobo.1280.4">For instance, we showed how to implement the </span><code class="inlineCode"><span class="koboSpan" id="kobo.1281.1">ExactMatchStringEvaluator</span></code><span class="koboSpan" id="kobo.1282.1"> for comparing answers about Federal Reserve interest rates, and how to use </span><code class="inlineCode"><span class="koboSpan" id="kobo.1283.1">ScoreStringEvalChain</span></code><span class="koboSpan" id="kobo.1284.1"> for more nuanced evaluations. </span><span class="koboSpan" id="kobo.1284.2">The examples also covered JSON format validation using </span><code class="inlineCode"><span class="koboSpan" id="kobo.1285.1">JsonValidityEvaluator</span></code><span class="koboSpan" id="kobo.1286.1"> and assessment of agent trajectories in healthcare scenarios.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1287.1">Tools like LangChain provide predefined evaluators for criteria such as conciseness and relevance, while platforms like LangSmith enable comprehensive testing and monitoring. </span><span class="koboSpan" id="kobo.1287.2">The chapter presented code examples using LangSmith to create and evaluate datasets, demonstrating how to assess model performance across multiple criteria. </span><span class="koboSpan" id="kobo.1287.3">The implementation of </span><code class="inlineCode"><span class="koboSpan" id="kobo.1288.1">pass@k</span></code><span class="koboSpan" id="kobo.1289.1"> metrics using Hugging Face’s Evaluate library was shown for assessing code generation capabilities. </span><span class="koboSpan" id="kobo.1289.2">We also walked through an example of evaluating insurance claim text extraction using structured schemas and LangChain’s evaluation capabilities.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1290.1">Now that we’ve evaluated our AI workflows, in the next chapter we’ll look at how we can deploy and monitor them. </span><span class="koboSpan" id="kobo.1290.2">Let’s discuss deployment and observability!</span></p>
<div aria-label="348" epub:type="pagebreak" id="page40-5" role="doc-pagebreak"/>
<h1 class="heading-1" id="_idParaDest-224"><a id="_idTextAnchor447"/><span class="koboSpan" id="kobo.1291.1">Questions</span></h1>
<ol>
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.1292.1">Describe three key metrics used in evaluating AI agents.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1293.1">What’s the difference between online and offline evaluation?</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1294.1">What are system-level and application-level evaluations and how do they differ?</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1295.1">How can LangSmith be used to compare different versions of an LLM application?</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1296.1">How does chain-of-thought evaluation differ from traditional output evaluation?</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1297.1">Why is trajectory evaluation important for understanding agent behavior?</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1298.1">What are the key considerations when evaluating LLM agents for production deployment?</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1299.1">How can bias be mitigated when using language models as evaluators?</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1300.1">What role do standardized benchmarks play, and how can we create benchmark datasets for LLM agent evaluation?</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1301.1">How do you balance automated evaluation metrics with human evaluation in production systems?</span></li>
</ol>
</div>
</body></html>