<html><head></head><body>
<div><div><h1 class="chapterNumber"><a id="_idTextAnchor390"/>8</h1>
<h1 class="chapterTitle" id="_idParaDest-197"><a id="_idTextAnchor391"/>Evaluation and Testing</h1>
<p class="normal">As we’ve discussed so far in this book, LLM agents and systems have diverse applications across industries. However, taking these complex neural network systems from research to real-world deployment comes with significant challenges and necessitates robust evaluation strategies and testing methodologies.</p>
<p class="normal">Evaluating LLM agents and apps in LangChain comes with new methods and metrics that can help ensure optimized, reliable, and ethically sound outcomes. This chapter delves into the intricacies of evaluating LLM agents, covering system-level evaluation, evaluation-driven design, offline and online evaluation methods, and practical examples with Python code.</p>
<p class="normal">By the end of this chapter, you will have a comprehensive understanding of how to evaluate LLM agents and ensure their alignment with intended goals and governance requirements. In all, this chapter will cover:</p>
<ul>
<li class="b lletList">Why evaluations matter</li>
<li class="b lletList">What we evaluate: core agent capabilities</li>
<li class="b lletList">How we evaluate: methodologies and approaches</li>
<li class="b lletList">Evaluating LLM agents in practice</li>
<li class="b lletList">Offline evaluation</li>
</ul>
<div><div><div><p class="normal">You can find the code for this chapter in the <code class="inlineCode">chapter8/</code> directory of the book’s GitHub repository. Given the rapid developments in the field and the updates to the LangChain library, we are committed to keeping the GitHub repository current. Please visit <a href="https://github.com/benman1/generative_ai_with_langchain">https://github.com/benman1/generative_ai_with_langchain</a> for the latest updates.</p>
<p class="normal">See <a href="E_Chapter_2.xhtml#_idTextAnchor044"><em class="italic">Chapter 2</em></a> for setup instructions. If you have any questions or encounter issues while running the code, please create an issue on GitHub or join the discussion on Discord at <a href="https://packt.link/lang">https://packt.link/lang</a>.</p>
</div>
</div>
<p class="normal">In the realm of developing LLM agents, evaluations play a pivotal role in ensuring these complex systems function reliably and effectively across real-world applications. Let’s start discussing why rigorous evaluation is indispensable!</p>
<h1 class="heading-1" id="_idParaDest-198"><a id="_idTextAnchor392"/>Why evaluation matters</h1>
<p class="normal">LLM agents represent a<a id="_idIndexMarker660"/> new class of AI systems that combine language models with reasoning, decision-making, and tool-using capabilities. Unlike traditional software with predictable behaviors, these agents operate with greater autonomy and complexity, making thorough evaluation essential before deployment.</p>
<p class="normal">Consider the real-world consequences: unlike traditional software with deterministic behavior, LLM agents make complex, context-dependent decisions. If unevaluated before being implemented, an AI agent in customer support might provide misleading information that damages brand reputation, while a healthcare assistant could influence critical treatment decisions—highlighting why thorough evaluation is essential.</p>
<div><div><p class="normal">Before diving into specific evaluation techniques, it’s important to distinguish between two fundamentally different types of evaluation:</p>
<p class="normal"><strong class="keyWord">LLM model evaluation:</strong></p>
<ul>
<li class="bulletList">Focuses on the raw capabilities of the base language model</li>
<li class="bulletList">Uses controlled prompts and standardized benchmarks</li>
<li class="bulletList">Evaluates inherent abilities like reasoning, knowledge recall, and language generation</li>
<li class="bulletList">Typically conducted by model developers or researchers comparing different models</li>
</ul>
</div>
</div>
<div><div><div><p class="normal"><strong class="keyWord">LLM system/application evaluation:</strong></p>
<ul>
<li class="bulletList">Assesses the complete application that includes the LLM plus additional components</li>
<li class="bulletList">Examines real-world performance with actual user queries and scenarios</li>
<li class="bulletList">Evaluates how components work together (retrieval, tools, memory, etc.)</li>
<li class="bulletList">Measures end-to-end effectiveness at solving user problems</li>
</ul>
<p class="normal">While both types of evaluation are important, this chapter focuses on system-level evaluation, as practitioners building LLM agents with LangChain are concerned with overall application performance rather than comparing base models. A weaker base model with excellent prompt engineering and system<a id="_idIndexMarker661"/> design might outperform a stronger model with poor integration in real-world applications<a id="_idTextAnchor393"/>.</p>
</div>
</div>
<h2 class="heading-2" id="_idParaDest-199"><a id="_idTextAnchor394"/>Safety and alignment</h2>
<p class="normal">Alignment in the context of<a id="_idIndexMarker662"/> LLMs has a dual meaning: as a process, referring to the post-training techniques used to ensure that models behave according to human expectations and values; and as an outcome, measuring the degree to which a model’s behavior conforms to intended human values and safety guidelines. Unlike task-related performance which focuses on accuracy and completeness, alignment addresses the fundamental calibration of the system to human behavioral standards. While fine-tuning improves a model’s performance on specific tasks, alignment specifically targets ethical behavior, safety, and reduction of harmful outputs.</p>
<p class="normal">This distinction is crucial because a model can be highly capable (well fine-tuned) but poorly aligned, creating sophisticated outputs that violate ethical norms or safety guidelines. Conversely, a model can be well-aligned but lack task-specific capabilities in certain domains. Alignment with human values is fundamental to responsible AI deployment. Evaluation must verify that agents align with human expectations across multiple dimensions: factual accuracy in sensitive domains, ethical boundary recognition, safety in responses, and value consistency.</p>
<p class="normal">Alignment evaluation methods must be tailored to domain-specific concerns. In financial services, alignment evaluation focuses on regulatory compliance with frameworks like GDPR and the EU AI Act, particularly regarding automated decision-making. Financial institutions must evaluate bias in fraud detection systems, implement appropriate human oversight mechanisms, and document these processes to satisfy regulatory requirements. In retail environments, alignment evaluation centers on ethical personalization practices, balancing recommendation relevance with customer privacy concerns and ensuring transparent data usage policies when generating personalized content.</p>
<div><p class="normal">Manufacturing contexts require alignment evaluation focused on safety parameters and operational boundaries. AI systems must recognize potentially dangerous operations, maintain appropriate human intervention protocols for quality control, and adhere to industry safety standards. Alignment evaluation includes testing whether predictive maintenance systems appropriately escalate critical safety issues to human technicians rather than autonomously deciding maintenance schedules for safety-critical equipment.</p>
<p class="normal">In educational settings, alignment evaluation must consider developmental appropriateness across student age groups, fair assessment standards across diverse student populations, and appropriate transparency levels. Educational AI systems require evaluation of their ability to provide balanced perspectives on complex topics, avoid reinforcing stereotypes in learning examples, and appropriately defer to human educators on sensitive or nuanced issues. These domain-specific alignment evaluations are essential for ensuring AI systems not only perform well technically but <a id="_idIndexMarker663"/>also operate within appropriate ethical and safety boundaries for their application conte<a id="_idTextAnchor395"/>xt.</p>
<h2 class="heading-2" id="_idParaDest-200"><a id="_idTextAnchor396"/>Performance and efficiency</h2>
<p class="normal">Like early challenges in<a id="_idIndexMarker664"/> software testing that were resolved through standardized practices, agent evaluations face similar hurdles. These include:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Overfitting</strong>: Where systems perform well only on test data but not in real-world situations</li>
<li class="b lletList"><strong class="keyWord">Gaming benchmarks</strong>: Optimizing for specific test scenarios rather than general performance</li>
<li class="b lletList"><strong class="keyWord">Insufficient diversity in evaluation datasets</strong>: Failing to test performance across the breadth of real-world situations the system will encounter, including edge cases and unexpected inputs</li>
</ul>
<p class="normal">Drawing lessons from software testing and other domains, comprehensive evaluation frameworks need to measure not only the accuracy but also the scalability, resource utilization, and safety of LLM agents.</p>
<p class="normal"><em class="italic">Performance evaluation</em> determines whether agents can reliably achieve their intended goals, including:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Accuracy</strong> in task completion across varied scenarios</li>
<li class="b lletList"><strong class="keyWord">Robustness</strong> when handling novel inputs that differ from evaluation examples</li>
<li class="b lletList"><strong class="keyWord">Resistance</strong> to adversarial inputs or manipulation</li>
<li class="b lletList"><strong class="keyWord">Resource efficiency</strong> in computational and operational costs</li>
</ul>
<div><p class="normal">Rigorous evaluations identify potential failure modes and risks in diverse real-world scenarios, as evidenced by modern benchmarks and contests. Ensuring an agent can operate safely and reliably across variations in real-world conditions is paramount. Evaluation strategies and methodologies continue to evolve, enhancing agent design effectiveness through iterative improvement.</p>
<p class="normal">Effective evaluations prevent the adoption of unnecessarily complex and costly solutions by balancing accuracy with resource efficiency. For example, the DSPy framework optimizes both cost and task performance, highlighting how evaluations can guide resource-effective solutions. LLM agents benefit from similar optimization strategies, ensuring their computational <a id="_idIndexMarker665"/>demands align with their bene<a id="_idTextAnchor397"/>fits.</p>
<h2 class="heading-2" id="_idParaDest-201"><a id="_idTextAnchor398"/>User and stakeholder value</h2>
<p class="normal">Evaluations help <a id="_idIndexMarker666"/>quantify the actual impact of LLM agents in practical settings. During the COVID-19 pandemic, the WHO’s implementation of screening chatbots demonstrated how AI could achieve meaningful practical outcomes, evaluated through metrics like user adherence and information quality. In financial services, JPMorgan Chase’s COIN (Contract Intelligence) platform for reviewing legal documents showcased value by reducing 360,000 hours of manual review work annually, with evaluations focusing on accuracy rates and cost savings compared to traditional methods. Similarly, Sephora’s Beauty Bot demonstrated retail value through increased conversion rates (6% higher than traditional channels) and higher average order values, proving stakeholder value across multiple dimensions.</p>
<p class="normal">User experience is a cornerstone of successful AI deployment. Systems like Alexa and Siri undergo rigorous evaluations for ease of use and engagement, which inform design improvements. Similarly, assessing user interaction with LLM agents helps refine interfaces and ensures the agents meet or exceed user expectations, thereby improving overall satisfaction and adoption rates.</p>
<p class="normal">A critical aspect of modern AI systems includes understanding how human interventions affect outcomes. In healthcare settings, evaluations show how human feedback enhances the performance of chatbots in therapeutic contexts. In manufacturing, a predictive maintenance LLM agent deployed at a major automotive manufacturer demonstrated value through reduced downtime (22% improvement), extended equipment lifespan, and positive feedback from maintenance technicians about the system’s interpretability and usefulness. For LLM agents, incorporating human oversight in evaluations reveals insights into decision-making processes and highlights both strengths and areas needing improvement.</p>
<p class="normal">Comprehensive agent evaluation requires addressing the distinct perspectives and priorities of multiple stakeholders across the agent lifecycle. The evaluation methods deployed should reflect this diversity, with metrics tailored to each group’s primary concerns.</p>
<div><p class="normal">End users evaluate agents primarily through the lens of practical task completion and interaction quality. Their assessment revolves around the agent’s ability to understand and fulfill requests accurately (task success rate), respond with relevant information (answer relevancy), maintain conversation coherence, and operate with reasonable speed (response time). This group values satisfaction metrics most highly, with user satisfaction scores and communication efficiency being particularly important in conversational contexts. In application-specific domains like web navigation or software engineering, users may prioritize domain-specific success metrics—such as whether an e-commerce agent successfully completed a purchase or a coding agent resolved a software issue correctly.</p>
<p class="normal">Technical stakeholders require a deeper evaluation of the agent’s internal processes rather than just outcomes. They focus on the quality of planning (plan feasibility, plan optimality), reasoning coherence, tool selection accuracy, and adherence to technical constraints. For SWE agents, metrics like code correctness and test case passing rate are critical. Technical teams also closely monitor computational efficiency metrics such as token consumption, latency, and resource utilization, as these directly impact operating costs and scalability. Their evaluation extends to the agent’s robustness—measuring how it handles edge cases, recovers from errors, and performs under varying loads.</p>
<p class="normal">Business stakeholders evaluate agents through metrics connecting directly to organizational value. Beyond basic ROI calculations, they track domain-specific KPIs that demonstrate tangible impact: reduced call center volume for customer service agents, improved inventory accuracy for retail applications, or decreased downtime for manufacturing agents. Their evaluation framework includes the agent’s alignment with strategic goals, competitive differentiation, and scalability across the organization. In sectors like finance, metrics bridging technical performance to business outcomes—such as reduced fraud losses while maintaining customer convenience—are especially valuable.</p>
<p class="normal">Regulatory stakeholders, particularly in high-stakes domains like healthcare, finance, and legal services, evaluate agents through strict compliance and safety lenses. Their assessment encompasses the agent’s adherence to domain-specific regulations (like HIPAA in healthcare or financial regulations in banking), bias detection measures, robustness against adversarial inputs, and comprehensive documentation of decision processes. For these stakeholders, the thoroughness of safety testing and the agent’s consistent performance within defined guardrails outweigh pure efficiency or capability metrics. As autonomous agents gain wider deployment, this regulatory evaluation dimension becomes increasingly crucial to ensure ethical operation and minimize potential harm.</p>
<div><p class="normal">For organizational decision-makers, evaluations should include cost-benefit analyses, especially important at the deployment stage. In healthcare, comparing the costs and benefits of AI interventions <a id="_idIndexMarker667"/>versus traditional methods ensures economic viability. Similarly, evaluating the financial sustainability of LLM agent deployments involves analyzing operational costs against achieved efficiencies, ensuring scalability without sacrificing effecti<a id="_idTextAnchor399"/>veness.</p>
<h2 class="heading-2" id="_idParaDest-202"><a id="_idTextAnchor400"/>Building consensus for LLM evaluation</h2>
<p class="normal">Evaluating LLM <a id="_idIndexMarker668"/>agents presents a significant challenge due to their open-ended nature and the subjective, context-dependent definition of <em class="italic">good</em> performance. Unlike traditional software with clear-cut metrics, LLMs can be convincingly wrong, and human judgment on their quality varies. This necessitates an evaluation strategy centered on building organizational consensus.</p>
<p class="normal">The foundation of effective evaluation lies in prioritizing user outcomes. Instead of starting with technical metrics, developers should identify what constitutes success from the user’s perspective, understanding the value the agent should deliver and the potential risks. This outcomes-based approach ensures evaluation priorities align with real-world impact.</p>
<p class="normal">Addressing the subjective nature of LLM evaluation requires establishing robust evaluation governance. This involves creating cross-functional working groups comprising technical experts, domain specialists, and user representatives to define and document formalized evaluation criteria. Clear ownership of different evaluation dimensions and decision-making frameworks for resolving disagreements is crucial. Maintaining version control for evaluation standards ensures transparency as understanding evolves.</p>
<p class="normal">In organizational contexts, balancing diverse stakeholder perspectives is key. Evaluation frameworks must accommodate technical performance metrics, domain-specific accuracy, and user-centric helpfulness. Effective governance facilitates this balance through mechanisms like weighted scoring systems and regular cross-functional reviews, ensuring all viewpoints are considered.</p>
<p class="normal">Ultimately, evaluation governance serves as a mechanism for organizational learning. Well-structured frameworks help identify specific failure modes, provide actionable insights for development, enable quantitative comparisons between system versions, and support continuous improvement through integrated feedback loops. Establishing a “model governance committee” with representatives from all stakeholder groups can help review results, resolve disputes, and guide deployment decisions. Documenting not just results but the discussions around them captures valuable insights into user needs and system limitations.</p>
<div><p class="normal">In conclusion, rigorous and well-governed evaluation is an integral part of the LLM agent development lifecycle. By implementing structured frameworks that consider technical performance, user value, and organizational alignment, teams can ensure these systems deliver benefits effectively while mitigating risks. The subsequent sections will delve into evaluation methodologies, including concrete examples relevant to developers working with tools like La<a id="_idTextAnchor401"/><a id="_idTextAnchor402"/><a id="_idTextAnchor403"/><a id="_idTextAnchor404"/><a id="_idTextAnchor405"/><a id="_idTextAnchor406"/><a id="_idTextAnchor407"/><a id="_idTextAnchor408"/><a id="_idTextAnchor409"/><a id="_idTextAnchor410"/><a id="_idTextAnchor411"/><a id="_idTextAnchor412"/><a id="_idTextAnchor413"/><a id="_idTextAnchor414"/>ngChain.</p>
<p class="normal">Building on the foundational principles of LLM agent evaluation and the importance of establishing robust governance, we now turn to the practical realities of assessment. Developing reliable agents requires a clear understanding of what aspects of their behavior need to be measured and <a id="_idIndexMarker669"/>how to apply effective techniques to quantify their performance. The upcoming sections provide a detailed guide on the <em class="italic">what</em> and <em class="italic">how</em> of evaluating LLM agents, breaking down the core capabilities you should focus on and the diverse methodologies you can employ to build a comprehensive evaluation framework for your applications.</p>
<h1 class="heading-1" id="_idParaDest-203"><a id="_idTextAnchor415"/>What we evaluate: core agent capabilities</h1>
<p class="normal">At the most<a id="_idIndexMarker670"/> fundamental level, an LLM agent’s value is tied directly to its ability to successfully accomplish the tasks it was designed for. If an agent cannot reliably complete its core function, its utility is severely limited, regardless of how sophisticated its underlying model or tools are. Therefore, this task performance evaluation forms the cornerstone of agent assessment. In the next subsection, we’ll explore the nuances of measuring task success, looking at considerations relevant to assessing how effectively your agent executes its primary functions in real-world scenarios.</p>
<h2 class="heading-2" id="_idParaDest-204"><a id="_idTextAnchor416"/>Task performance evaluation</h2>
<p class="normal">Task performance forms the foundation of agent evaluation, measuring how effectively an agent accomplishes its intended goals. Successful agents demonstrate high task completion rates while producing relevant, factually accurate responses that directly address user requirements. When evaluating task performance, organizations typically assess both the correctness of the final output and the efficiency of the process used to achieve it.</p>
<p class="normal">TaskBench (Shen and colleagues., 2023) and AgentBench (Liu and colleagues, 2023) provide standardized multi-stage evaluations of LLM-powered agents. TaskBench divides tasks into decomposition, tool selection, and parameter prediction, then reports that models like GPT-4 exceed 80% success on single-tool invocations but drop to around 50% on end-to-end task automation. AgentBench’s eight interactive environments likewise show top proprietary models vastly outperform smaller open-source ones, underscoring cross-domain generalization challenges.</p>
<div><p class="normal">Financial services applications demonstrate task performance evaluation in practice, though we should view industry-reported metrics with appropriate skepticism. While many institutions claim high accuracy rates for document analysis systems, independent academic assessments have documented significantly lower performance in realistic conditions. A particularly important dimension in regulated industries is an agent’s ability to correctly identify instances where it lacks sufficient information—a critical safety feature that requires specific evaluation protocols beyond simple accuracy measurement.</p>
<h2 class="heading-2" id="_idParaDest-205"><a id="_idTextAnchor417"/>Tool usage evaluation</h2>
<p class="normal">Tool usage <a id="_idIndexMarker671"/>capability—an agent’s ability to select, configure, and leverage external systems—has emerged as a crucial evaluation dimension that distinguishes advanced agents from simple question-answering systems. Effective tool usage evaluation encompasses multiple aspects: the agent’s ability to select the appropriate tool for a given subtask, provide correct parameters, interpret tool outputs correctly, and integrate these outputs into a coherent solution strategy.</p>
<p class="normal">The T-Eval framework, developed by Liu and colleagues (2023), decomposes tool usage into distinct measurable capabilities: planning the sequence of tool calls, reasoning about the next steps, retrieving the correct tool from available options, understanding tool documentation, correctly formatting API calls, and reviewing responses to determine if goals were met. This granular approach allows organizations to identify specific weaknesses in their agent’s tool-handling capabilities rather than simply observing overall failures.</p>
<p class="normal">Recent benchmarks like ToolBench and ToolSandbox demonstrate that even state-of-the-art agents struggle with tool usage in dynamic environments. In production systems, evaluation increasingly focuses on efficiency metrics alongside basic correctness—measuring whether agents avoid redundant tool calls, minimize unnecessary API usage, and select the most direct path to solve user problems. While industry implementations often claim significant efficiency improvements, peer-reviewed research suggests more modest gains, with optimized tool selection typically reducing computation costs by 15-20% in controlled studies while maintaining outcome quality.</p>
<h2 class="heading-2" id="_idParaDest-206"><a id="_idTextAnchor418"/>RAG evaluation</h2>
<p class="normal">RAG system evaluation<a id="_idIndexMarker672"/> represents a specialized but crucial area of agent assessment, focusing on how effectively agents retrieve and incorporate external knowledge. Four key dimensions form the foundation of comprehensive RAG evaluation: retrieval quality, contextual relevance, faithful generation, and information synthesis.</p>
<div><p class="normal"><em class="italic">Retrieval quality</em> measures how well the system finds the most appropriate information from its knowledge base. Rather<a id="_idIndexMarker673"/> than using simple relevance scores, modern evaluation approaches assess retrieval through precision and recall at different ranks, considering both the absolute relevance of retrieved documents and their coverage of the information needed to answer user queries. Academic research has developed standardized test collections with expert annotations to enable systematic comparison across different retrieval methodologies.</p>
<p class="normal"><em class="italic">Contextual relevance</em>, on the other hand, examines how precisely the retrieved information matches the specific information need expressed in the query. This involves evaluating whether the system can distinguish between superficially similar but contextually different information requests. Recent research has developed specialized evaluation methodologies for testing disambiguation capabilities in financial contexts, where similar terminology might apply to fundamentally different products or regulations. These approaches specifically measure how well retrieval systems can distinguish between queries that use similar language but have distinct informational needs.</p>
<p class="normal"><em class="italic">Faithful generation</em>—the degree to which the agent’s responses accurately reflect the retrieved information without fabricating details—represents perhaps the most critical aspect of RAG evaluation. Recent studies have found that even well-optimized RAG systems still show non-trivial hallucination rates, between 3-15% on complex domains, highlighting the ongoing challenge in this area. Researchers have developed various evaluation protocols for faithfulness, including source attribution tests and contradiction detection mechanisms that systematically compare generated content with the retrieved source material.</p>
<p class="normal">Finally, <em class="italic">information synthesis</em> quality evaluates the agent’s ability to integrate information from multiple sources into coherent, well-structured responses. Rather than simply concatenating or paraphrasing individual documents, advanced agents must reconcile potentially conflicting information, present balanced perspectives, and organize content logically. Evaluation here extends beyond automated metrics to include expert assessment of how effectively the agent has synthesized complex<a id="_idIndexMarker674"/> information into accessible, accurate summaries that maintain appropriate nuance.</p>
<h2 class="heading-2" id="_idParaDest-207"><a id="_idTextAnchor419"/>Planning and reasoning evaluation</h2>
<p class="normal">Planning and reasoning capabilities form the cognitive foundation that enables agents to solve complex, multi-step problems that cannot be addressed through single operations. Evaluating these capabilities requires moving beyond simple input-output testing to assess the quality of the agent’s thought process and problem-solving strategy.</p>
<div><p class="normal">Plan feasibility gauges whether every action in a proposed plan respects the domain’s preconditions and constraints. Using the PlanBench suite, Valmeekam and colleagues in their 2023 paper <em class="italic">PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change</em> showed that GPT-4 correctly generates fully executable plans in only about 34% of classical IPC-style domains under zero-shot conditions—far below reliable thresholds and <a id="_idIndexMarker675"/>underscoring persistent failures to account for environment dynamics and logical preconditions.</p>
<p class="normal">Plan optimality extends evaluation beyond basic feasibility to consider efficiency. This dimension assesses whether agents can identify not just any working solution but the most efficient approach to accomplishing their goals. The Recipe2Plan benchmark specifically evaluates this by testing whether agents can effectively multitask under time constraints, mirroring real-world efficiency requirements. Current state-of-the-art models show significant room for improvement, with published research indicating optimal planning rates between 45% and 55% for even the most capable systems.</p>
<p class="normal">Reasoning coherence evaluates the logical structure of the agent’s problem-solving approach—whether individual reasoning steps connect logically, whether conclusions follow from premises, and whether the agent maintains consistency throughout complex analyses. Unlike traditional software testing where only the final output matters, agent evaluation increasingly examines intermediate reasoning steps to identify failures in logical progression that might be masked by a correct final answer. Multiple academic studies have demonstrated the importance of this approach, with several research groups developing standardized methods for reasoning trace analysis.</p>
<p class="normal">Recent studies (<em class="italic">CoLadder: Supporting Programmers with Hierarchical Code Generation in Multi-Level Abstraction</em>, 2023, and <em class="italic">Generating a Low-code Complete Workflow via Task Decomposition and RAG</em>, 2024) show that decomposing code-generation tasks into smaller, well-defined subtasks—often using hierarchical or as-needed planning—leads to substantial gains in code quality, developer productivity, and system reliability across both benchmarks and live engineering settings.</p>
<p class="normal">Building on the foundational principles of LLM agent evaluation and the importance of establishing robust governance, we now turn to the practical realities of assessment. Developing reliable agents requires a clear understanding of what aspects of their behavior need to be measured and how to apply effective techniques to quantify their performance.</p>
<div><p class="normal">Identifying the core capabilities to evaluate is the first critical step. The next is determining how to effectively measure them, given the complexities and subjective aspects inherent in LLM agents compared to traditional software. Relying on a single metric or approach is insufficient. In the next subsection, we’ll explore the various methodologies and approaches available for evaluating agent performance in a robust, scalable, and insightful manner. We’ll cover the role of automated metrics for consistency, the necessity of human feedback for subjective assessment, the importance of system-level analysis for integrated agents, and how to combine these techniques<a id="_idIndexMarker676"/> into a practical evaluation framework that drives improvement.</p>
<h1 class="heading-1" id="_idParaDest-208"><a id="_idTextAnchor420"/>How we evaluate: methodologies and approaches</h1>
<p class="normal">LLM agents, particularly <a id="_idIndexMarker677"/>those built with flexible frameworks like LangChain or LangGraph, are typically composed of different functional parts or <em class="italic">skills</em>. An agent’s overall performance isn’t a single monolithic metric; it’s the result of how well it executes these individual capabilities and how effectively they work together. In the following subsection, we’ll delve into these core capabilities that distinguish effective agents, outlining the specific dimensions we should assess to understand where our agent excels and where it might be failing.</p>
<h2 class="heading-2" id="_idParaDest-209"><a id="_idTextAnchor421"/>Automated evaluation approaches</h2>
<p class="normal">Automated evaluation <a id="_idIndexMarker678"/>methods provide scalable, consistent assessment of agent capabilities, enabling systematic comparison across different versions or implementations. While no single metric can capture all aspects of agent performance, combining complementary approaches allows for comprehensive automated evaluation that complements human assessment.</p>
<p class="normal">Reference-based evaluation compares each agent output against one or more gold-standard answers or trajectories. While BLEU/ROUGE and early embedding measures like BERTScore / <strong class="keyWord">Universal Sentence Encoder</strong> (<strong class="keyWord">USE</strong>) were <a id="_idIndexMarker679"/>vital first steps, today’s state-of-the-art relies on learned metrics (BLEURT, COMET, BARTScore), QA-based frameworks (QuestEval), and LLM-powered judges, all backed by large human‐rated datasets to ensure robust, semantically aware evaluation.</p>
<p class="normal">Rather than using direct string comparison, modern evaluation increasingly employs criterion-based assessment frameworks that evaluate outputs against specific requirements. For example, the T-Eval framework evaluates tool usage through a multi-stage process examining planning, reasoning, tool selection, parameter formation, and result interpretation. This structured approach allows precise identification of where in the process an agent might be failing, providing far more actionable insights than simple success/failure metrics.</p>
<div><p class="normal">LLM-as-a-judge approaches represent a rapidly evolving evaluation methodology where powerful language models serve as automated evaluators, assessing outputs according to defined rubrics. Research by Zheng and colleagues (<em class="italic">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</em>, 2023) demonstrates that with carefully designed prompting, models like GPT-4 can achieve substantial agreement with human evaluators on dimensions like factual accuracy, coherence, and relevance. This approach can help evaluate subjective qualities that traditional metrics struggle to capture, though researchers emphasize the importance of human verification to mitigate<a id="_idIndexMarker680"/> potential biases in the evaluator models themselves.</p>
<h2 class="heading-2" id="_idParaDest-210"><a id="_idTextAnchor422"/>Human-in-the-loop evaluation</h2>
<p class="normal">Human evaluation<a id="_idIndexMarker681"/> remains essential for assessing subjective dimensions of agent performance that automated metrics cannot fully capture. Effective human-in-the-loop evaluation requires structured methodologies to ensure consistency and reduce bias while leveraging human judgment where it adds the most value.</p>
<p class="normal">Expert review provides in-depth qualitative assessment from domain specialists who can identify subtle errors, evaluate reasoning quality, and assess alignment with domain-specific best practices. Rather than unstructured feedback, modern expert review employs standardized rubrics that decompose<a id="_idIndexMarker682"/> evaluation into specific dimensions, typically using Likert scales or comparative rankings. Research in healthcare and financial domains has developed standardized protocols for expert evaluation, particularly for assessing agent responses in complex regulatory contexts.</p>
<p class="normal">User feedback captures the perspective of end users interacting with the agent in realistic contexts. Structured feedback collection through embedded rating mechanisms (for example, thumbs up/down, 1-5 star ratings) provides quantitative data on user satisfaction, while free-text comments offer qualitative insights into specific strengths or weaknesses. Academic studies of conversational agent effectiveness increasingly implement systematic feedback collection protocols where user ratings are analyzed to identify patterns in agent performance across different query types, user segments, or time periods.</p>
<p class="normal">A/B testing methodologies allow controlled comparison of different agent versions or configurations by randomly routing users to different implementations and measuring performance differences. This experimental approach is particularly valuable for evaluating changes to agent prompting, tool integration, or retrieval mechanisms. When implementing A/B testing, researchers typically define primary metrics (like task completion rates or user satisfaction) alongside secondary metrics that help explain observed differences (such as response length, tool usage patterns, or conversation duration). </p>
<div><p class="normal">Academic research on conversational agent optimization has demonstrated the effectiveness of controlled experiments in identifying specific improvements to agent configurations.</p>
<h2 class="heading-2" id="_idParaDest-211"><a id="_idTextAnchor423"/>System-level evaluation</h2>
<p class="normal">System-level evaluation is <a id="_idIndexMarker683"/>crucial for complex LLM agents, particularly RAG systems, because testing individual components isn’t enough. Research indicates that a significant portion of failures (over 60% in some studies) stem from integration issues between components that otherwise function correctly in isolation. For example, issues can arise from retrieved documents not being used properly, query reformulation altering original intent, or context windows truncating information during handoffs. System-level evaluation addresses this by examining how information flows between components and how the agent performs as a unified system.</p>
<p class="normal">Core approaches to system-level evaluation include using diagnostic frameworks that trace information flow through the entire pipeline to identify breakdown points, like the RAG Diagnostic Tool. Tracing and observability tools (such as LangSmith, Langfuse, and DeepEval) provide visibility into the agent’s internal workings, allowing developers to visualize reasoning chains and pinpoint where errors occur. End-to-end testing methodologies use comprehensive scenarios to assess how the entire system handles ambiguity, challenge inputs, and maintain context over multiple<a id="_idIndexMarker684"/> turns, using frameworks like GAIA.</p>
<p class="normal">Effective evaluation <a id="_idIndexMarker685"/>of LLM applications requires running multiple assessments. Rather than presenting abstract concepts, here are a few practical steps!</p>
<div><ul>
<li class="bulletList"><strong class="keyWord">Define business metrics</strong>: Start by identifying metrics that matter to your organization. Focus on functional aspects like accuracy and completeness, technical factors such as latency and token usage, and user experience elements including helpfulness and clarity. Each application should have specific criteria with clear measurement methods.</li>
<li class="bulletList"><strong class="keyWord">Create diverse test datasets</strong>: Develop comprehensive test datasets covering common user queries, challenging edge cases, and potential compliance issues. Categorize examples systematically to ensure broad coverage. Continuously expand your dataset as you discover new usage patterns or failure modes.</li>
<li class="bulletList"><strong class="keyWord">Combine multiple evaluation methods</strong>: Use a mix of evaluation approaches for thorough assessment. Automated checks for factual accuracy and correctness should be combined with domain-specific criteria. Consider both quantitative metrics and qualitative assessments from subject matter experts when evaluating responses.</li>
<li class="bulletList"><strong class="keyWord">Deploy progressively</strong>: Adopt a staged deployment approach. Begin with development testing against offline benchmarks, then proceed to limited production release with a small user subset. Only roll out fully after meeting performance thresholds. This cautious approach helps identify issues before they affect most users.</li>
<li class="bulletList"><strong class="keyWord">Monitor production performance</strong>: Implement ongoing monitoring in live environments. Track key performance indicators like response time, error rates, token usage, and user feedback. Set up alerts for anomalies that might indicate degraded performance or unexpected behavior.</li>
<li class="bulletList"><strong class="keyWord">Establish improvement cycles</strong>: Create structured processes to translate evaluation insights into concrete improvements. When issues are identified, investigate root causes, implement specific solutions, and validate the effectiveness of changes through re-evaluation. Document patterns of problems and successful solutions for future reference.</li>
<li class="bulletList"><strong class="keyWord">Foster cross-functional collaboration</strong>: Include diverse perspectives in your evaluation process. Technical teams, domain experts, business stakeholders, and compliance specialists all bring valuable insights. Regular review sessions with these cross-functional teams help ensure the comprehensive assessment of LLM applications.</li>
<li class="bulletList"><strong class="keyWord">Maintain living documentation</strong>: Keep centralized records of evaluation results, improvement actions, and outcomes. This documentation builds organizational knowledge<a id="_idIndexMarker686"/> and helps teams learn from past experiences, ultimately accelerating the development of more effective LLM applications.</li>
</ul>
<p class="normal">It’s time now to put<a id="_idIndexMarker687"/> the theory to the test and get into the weeds of evaluating<a id="_idTextAnchor424"/> LLM agents. Let’s dive in!</p>
<h1 class="heading-1" id="_idParaDest-212"><a id="_idTextAnchor425"/>Evaluating LLM agents in practice</h1>
<p class="normal">LangChain provides <a id="_idIndexMarker688"/>several predefined evaluators for different evaluation criteria. These evaluators can be used to assess outputs based on specific rubrics or criteria sets. Some common criteria include conciseness, relevance, correctness, coherence, helpfulness, and controversiality.</p>
<p class="normal">We can also compare results from an LLM or agent against reference results using different methods starting from pairwise string comparisons, string distances, and embedding distances. The evaluation results can be used to determine the preferred LLM or agent based on the comparison of outputs. Confidence intervals and p-values can also be calculated to assess the reliability of the evaluation results.</p>
<p class="normal">Let’s go through a few basics and apply useful evaluation strategies. We’ll <a id="_idTextAnchor426"/>start with LangChain.</p>
<div><h2 class="heading-2" id="_idParaDest-213"><a id="_idTextAnchor427"/>Evaluating the correctness of results</h2>
<p class="normal">Let’s think of an example, where we want to verify that an LLM’s answer is correct (or how far it is off). For example, when asked about the Federal Reserve’s interest rate, you might compare the output against a reference answer using both an exact match and a string distance evaluator.</p>
<pre>from langchain.evaluation import load_evaluator, ExactMatchStringEvaluator
prompt = "What is the current Federal Reserve interest rate?"
reference_answer = "0.25%" # Suppose this is the correct answer.
# Example predictions from your LLM:
prediction_correct = "0.25%"
prediction_incorrect = "0.50%"
# Initialize an Exact Match evaluator that ignores case differences.
exact_evaluator = ExactMatchStringEvaluator(ignore_case=True)
# Evaluate the correct prediction.
exact_result_correct = exact_evaluator.evaluate_strings(
    prediction=prediction_correct, reference=reference_answer
)
print("Exact match result (correct answer):", exact_result_correct)
# Expected output: score of 1 (or 'Y') indicating a perfect match.
# Evaluate an incorrect prediction.
exact_result_incorrect = exact_evaluator.evaluate_strings(
    prediction=prediction_incorrect, reference=reference_answer
)
print("Exact match result (incorrect answer):", exact_result_incorrect)
# Expected output: score of 0 (or 'N') indicating a mismatch.</pre>
<p class="normal">Now, obviously this<a id="_idIndexMarker689"/> won’t be very useful if the output comes in a different format or if we want to gauge how far off the answer is. In the repository, you can find an implementation of a custom comparison that would parse answers such as “It is 0.50%” and “A quarter percent.”</p>
<p class="normal">A more generalizable approach is LLM‐as‐a‐judge for evaluating correctness. In this example, instead of using simple string extraction or an exact match, we call an evaluation LLM (for example, an upper mid-range model such as Mistral) that parses and scores the prompt, the prediction, and a reference answer and then returns a numerical score plus reasoning. This works in scenarios where the prediction might be phrased differently but still correct.</p>
<pre>from langchain_mistralai import ChatMistralAI
from langchain.evaluation.scoring import ScoreStringEvalChain</pre>
<div><pre># Initialize the evaluator LLM
llm = ChatMistralAI(
    model="mistral-large-latest",
    temperature=0,
    max_retries=2
)
# Create the ScoreStringEvalChain from the LLM
chain = ScoreStringEvalChain.from_llm(llm=llm)
# Define the finance-related input, prediction, and reference answer
finance_input = "What is the current Federal Reserve interest rate?"
finance_prediction = "The current interest rate is 0.25%."
finance_reference = "The Federal Reserve's current interest rate is 0.25%."
# Evaluate the prediction using the scoring chain
result_finance = chain.evaluate_strings(
 input=finance_input,
    prediction=finance_prediction,
)
print("Finance Evaluation Result:")
print(result_finance)</pre>
<p class="normal">The output demonstrates how the LLM evaluator assesses the response quality with nuanced reasoning:</p>
<pre>Finance Evaluation Result:
{'reasoning': "The assistant's response is not verifiable as it does not provide a date or source for the information. The Federal Reserve interest rate changes over time and is not static. Therefore, without a specific date or source, the information provided could be incorrect. The assistant should have advised the user to check the Federal Reserve's official website or a reliable financial news source for the most current rate. The response lacks depth and accuracy. Rating: [[3]]", 'score': 3}</pre>
<p class="normal">This evaluation<a id="_idIndexMarker690"/> highlights an important advantage of the LLM-as-a-judge approach: it can identify subtle issues that simple matching would miss. In this case, the evaluator correctly identified that the response lacked important context. With a score of 3 out of 5, the LLM judge provides a more nuanced assessment than binary correct/incorrect evaluations, giving developers actionable feedback to improve response quality in financial applications where accuracy and proper attribution are critical.</p>
<div><p class="normal">The next example shows how to use Mistral AI to evaluate a model’s prediction against a reference answer. Please make sure to set your <code class="inlineCode">MISTRAL_API_KEY</code> environment variable and install the required package: <code class="inlineCode">pip install langchain_mistralai</code>. This should already be installed if you followed the instructions in <a href="E_Chapter_2.xhtml#_idTextAnchor044"><em class="italic">Chapter 2</em></a>.</p>
<p class="normal">This approach is more appropriate when you have ground truth responses and want to assess how well the model’s output matches the expected answer. It’s particularly useful for factual questions with clear, correct answers.</p>
<pre>from langchain_mistralai import ChatMistralAI
from langchain.evaluation.scoring import LabeledScoreStringEvalChain
# Initialize the evaluator LLM with deterministic output (temperature 0.)
llm = ChatMistralAI(
    model="mistral-large-latest",
    temperature=0,
    max_retries=2
)
# Create the evaluation chain that can use reference answers
labeled_chain = LabeledScoreStringEvalChain.from_llm(llm=llm)
# Define the finance-related input, prediction, and reference answer
finance_input = "What is the current Federal Reserve interest rate?"
finance_prediction = "The current interest rate is 0.25%."
finance_reference = "The Federal Reserve's current interest rate is 0.25%."
# Evaluate the prediction against the reference
labeled_result = labeled_chain.evaluate_strings(
 input=finance_input,
    prediction=finance_prediction,
    reference=finance_reference,
)
print("Finance Evaluation Result (with reference):")
print(labeled_result)</pre>
<div><p class="normal">The output shows <a id="_idIndexMarker691"/>how providing a reference answer significantly changes the evaluation results:</p>
<pre>{'reasoning': "The assistant's response is helpful, relevant, and correct. It directly answers the user's question about the current Federal Reserve interest rate. However, it lacks depth as it does not provide any additional information or context about the interest rate, such as how it is determined or what it means for the economy. Rating: [[8]]", 'score': 8}</pre>
<p class="normal">Notice how the score increased dramatically from 3 (in the previous example) to 8 when we provided a reference answer. This demonstrates the importance of ground truth in evaluation. Without a reference, the evaluator focused on the lack of citation and timestamp. With a reference confirming the factual accuracy, the evaluator now focuses on assessing completeness and depth instead of verifiability.</p>
<p class="normal">Both of these approaches leverage Mistral’s LLM as an evaluator, which can provide more nuanced and context-aware assessments than simple string matching or statistical methods. The results from these evaluations should be consistent when using <code class="inlineCode">temperature=0</code>, though outputs may differ <a id="_idIndexMarker692"/>from those shown in the book due to changes on the provider side.</p>
<div><div><p class="normal">Your output may differ from the book example due to model version differences and inherent variations in LLM responses<a id="_idTextAnchor428"/> (depending on the temperature).</p>
</div>
</div>
<h2 class="heading-2" id="_idParaDest-214"><a id="_idTextAnchor429"/>Evaluating tone and conciseness</h2>
<p class="normal">Beyond factual accuracy, many applications require responses that meet certain stylistic criteria. Healthcare applications, for example, must provide accurate information in a friendly, approachable manner without overwhelming patients with unnecessary details. The following example demonstrates how to evaluate both conciseness and tone using LangChain’s criteria evaluators, allowing developers to assess these subjective but critical aspects of response quality:</p>
<p class="normal">We start by importing the evaluator loader and a chat LLM for evaluation (for example GPT-4o):</p>
<pre>from langchain.evaluation import load_evaluator
from langchain.chat_models import ChatOpenAI
evaluation_llm = ChatOpenAI(model="gpt-4o", temperature=0)</pre>
<div><p class="normal">Our example prompt and the answer we’ve obtained is:</p>
<pre>prompt_health = "What is a healthy blood pressure range for adults?"
# A sample LLM output from your healthcare assistant:
prediction_health = (
    "A normal blood pressure reading is typically around 120/80 mmHg. "
    "It's important to follow your doctor's advice for personal health management!"
)</pre>
<p class="normal">Now let’s evaluate conciseness using a built-in <code class="inlineCode">conciseness</code> criterion:</p>
<pre>conciseness_evaluator = load_evaluator(
 "criteria", criteria="conciseness", llm=evaluation_llm
)
conciseness_result = conciseness_evaluator.evaluate_strings(
    prediction=prediction_health, input=prompt_health
)
print("Conciseness evaluation result:", conciseness_result)</pre>
<p class="normal">The result includes a score (0 or 1), a value (“Y” or “N”), and a reasoning chain of thought:</p>
<pre>Conciseness evaluation result: {'reasoning': "The criterion is conciseness. This means the submission should be brief, to the point, and not contain unnecessary information.\n\nLooking at the submission, it provides a direct answer to the question, stating that a normal blood pressure reading is around 120/80 mmHg. This is a concise answer to the question.\n\nThe submission also includes an additional sentence advising to follow a doctor's advice for personal health management. While this information is not directly related to the question, it is still relevant and does not detract from the conciseness of the answer.\n\nTherefore, the submission meets the criterion of conciseness.\n\nY", 'value': 'Y', 'score': 1}</pre>
<p class="normal">As for<a id="_idIndexMarker693"/> friendliness, let’s define a <code class="inlineCode">custom</code> criterion:</p>
<pre>custom_friendliness = {
 "friendliness": "Is the response written in a friendly and approachable tone?"
}
# Load a criteria evaluator with this custom criterion.
friendliness_evaluator = load_evaluator(</pre>
<div><pre> "criteria", criteria=custom_friendliness, llm=evaluation_llm
)
friendliness_result = friendliness_evaluator.evaluate_strings(
    prediction=prediction_health, input=prompt_health
)
print("Friendliness evaluation result:", friendliness_result)</pre>
<p class="normal">The evaluator should return whether the tone is friendly (<code class="inlineCode">Y</code>/<code class="inlineCode">N</code>) along with reasoning. In fact, this is what we get:</p>
<pre>Friendliness evaluation result: {'reasoning': "The criterion is to assess whether the response is written in a friendly and approachable tone. The submission provides the information in a straightforward manner and ends with a suggestion to follow doctor's advice for personal health management. This suggestion can be seen as a friendly advice, showing concern for the reader's health. Therefore, the submission can be considered as written in a friendly and approachable tone.\n\nY", 'value': 'Y', 'score': 1}</pre>
<p class="normal">This evaluation approach is particularly valuable for applications in healthcare, customer service, and educational domains where the manner of communication is as important as the factual content. The explicit reasoning provided by the evaluator helps development teams understand exactly which elements of the response contribute to its tone, making it easier to debug and improve response generation. While binary <code class="inlineCode">Y</code>/<code class="inlineCode">N</code> scores are useful for automated quality gates, the detailed reasoning offers more nuanced insights for continuous improvement. For production <a id="_idIndexMarker694"/>systems, consider combining multiple criteria evaluators to create a comprehensive quality score that reflects all aspects of your applica<a id="_idTextAnchor430"/>tion’s communication requirements.</p>
<h2 class="heading-2" id="_idParaDest-215"><a id="_idTextAnchor431"/>Evaluating the output format</h2>
<p class="normal">When working with LLMs to generate structured data like JSON, XML, or CSV, format validation becomes critical. Financial applications, reporting tools, and API integrations often depend on correctly formatted data structures. A technically perfect response that fails to adhere to the expected format can break downstream systems. LangChain provides specialized evaluators for validating structured outputs, as demonstrated in the following example using JSON validation for a financial report:</p>
<pre>from langchain.evaluation import JsonValidityEvaluator
# Initialize the JSON validity evaluator.</pre>
<div><pre>json_validator = JsonValidityEvaluator()
valid_json_output = '{"company": "Acme Corp", "revenue": 1000000, "profit": 200000}'
invalid_json_output = '{"company": "Acme Corp", "revenue": 1000000, "profit": 200000,}'
# Evaluate the valid JSON.
valid_result = json_validator.evaluate_strings(prediction=valid_json_output)
print("JSON validity result (valid):", valid_result)
# Evaluate the invalid JSON.
invalid_result = json_validator.evaluate_strings(prediction=invalid_json_output)
print("JSON validity result (invalid):", invalid_result)</pre>
<p class="normal">We’ll see a score indicating the JSON is valid:</p>
<pre>JSON validity result (valid): {'score': 1}</pre>
<p class="normal">For the invalid JSON, we are getting a score indicating the JSON is invalid:</p>
<pre>JSON validity result (invalid): {'score': 0, 'reasoning': 'Expecting property name enclosed in double quotes: line 1 column 63 (char 62)'}</pre>
<p class="normal">This validation approach is particularly valuable in production systems where LLMs interface with other software components. The <code class="inlineCode">JsonValidityEvaluator</code> not only identifies invalid outputs but also provides detailed error messages pinpointing the exact location of formatting errors. This facilitates rapid debugging and can be incorporated into automated testing pipelines to prevent format-related failures. Consider implementing similar validators for other formats your application may generate, such as XML, CSV, or domain-specific formats like FIX pr<a id="_idTextAnchor432"/>otocol for financial <a id="_idIndexMarker695"/>transactions.</p>
<h2 class="heading-2" id="_idParaDest-216"><a id="_idTextAnchor433"/>Evaluating agent trajectory</h2>
<p class="normal">Complex agents require evaluation across three critical dimensions:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Final response evaluation</strong>: Assess the ultimate output provided to the user (factual accuracy, helpfulness, quality, and safety)</li>
<li class="b lletList"><strong class="keyWord">Trajectory evaluation</strong>: Examine the path the agent took to reach its conclusion</li>
<li class="b lletList"><strong class="keyWord">Single-step evaluation</strong>: Analyze individual decision points in isolation</li>
</ul>
<div><p class="normal">While final response evaluation focuses on outcomes, trajectory evaluation examines the process itself. This approach is particularly valuable for complex agents that employ multiple tools, reasoning steps, or decision points to complete tasks. By evaluating the path taken, we can identify exactly where and how agents succeed or fail, even when the final answer is incorrect.</p>
<p class="normal">Trajectory evaluation compares the actual sequence of steps an agent took against an expected sequence, calculating a score based on how many expected steps were completed correctly. This gives partial credit to agents that follow some correct steps even if they don’t reach the right final answer.</p>
<p class="normal">Let’s implement a <code class="inlineCode">custom trajectory</code> evaluator for a healthcare agent that responds to medication questions:</p>
<pre>from langsmith import Client
# Custom trajectory subsequence evaluator
def trajectory_subsequence(outputs: dict, reference_outputs: dict) -&gt; float:
 """Check how many of the desired steps the agent took."""
 if len(reference_outputs['trajectory']) &gt; len(outputs['trajectory']):
 return False
 
    i = j = 0
 while i &lt; len(reference_outputs['trajectory']) and j &lt; len(outputs['trajectory']):
 if reference_outputs['trajectory'][i] == outputs['trajectory'][j]:
            i += 1
        j += 1
 
 return i / len(reference_outputs['trajectory'])
# Create example dataset with expected trajectories
client = Client()
trajectory_dataset = client.create_dataset(
 "Healthcare Agent Trajectory Evaluation",
    description="Evaluates agent trajectory for medication queries"
)
# Add example with expected trajectory</pre>
<div><pre>client.create_example(
    inputs={
 "question": "What is the recommended dosage of ibuprofen for an adult?"
    },
    outputs={
 "trajectory": [
 "intent_classifier",
 "healthcare_agent",
 "MedicalDatabaseSearch",
 "format_response"
        ],
 "response": "Typically, 200-400mg every 4-6 hours, not exceeding 3200mg per day."
    },
    dataset_id=trajectory_dataset.id
)</pre>
<div><div><p class="normal">Please remember to set your <code class="inlineCode">LANGSMITH_API_KEY</code> environment variable! If you get a <code class="inlineCode">Using legacy API key</code> error, you might need to generate a new API key from the LangSmith dashboard: <a href="https://smith.langchain.com/settings">https://smith.langchain.com/settings</a>. You always want to use the latest version of the LangSmith package.</p>
</div>
</div>
<p class="normal">To evaluate the agent’s <a id="_idIndexMarker696"/>trajectory, we need to capture the actual sequence of steps taken. With LangGraph, we can use streaming capabilities to record every node and tool invocation:</p>
<pre># Function to run graph with trajectory tracking (example implementation)
async def run_graph_with_trajectory(inputs: dict) -&gt; dict:
 """Run graph and track the trajectory it takes along with the final response."""
    trajectory = []
    final_response = ""
 
 # Here you would implement your actual graph execution
 # For the example, we'll just return a sample result
    trajectory = ["intent_classifier", "healthcare_agent", "MedicalDatabaseSearch", "format_response"]</pre>
<div><pre>    final_response = "Typically, 200-400mg every 4-6 hours, not exceeding 3200mg per day."
 return {
 "trajectory": trajectory,
 "response": final_response
    }
# Note: This is an async function, so in a notebook you'd need to use await
experiment_results = await client.aevaluate(
    run_graph_with_trajectory,
    data=trajectory_dataset.id,
    evaluators=[trajectory_subsequence],
    experiment_prefix="healthcare-agent-trajectory",
    num_repetitions=1,
    max_concurrency=4,
)</pre>
<p class="normal">We can also analyze results on the dataset, which we can download from LangSmith:</p>
<pre>results_df = experiment_results.to_pandas()
print(f"Average trajectory match score: {results_df['feedback.trajectory_subsequence'].mean()}")</pre>
<p class="normal">In this case, this is nonsensical, but this is to illustrate the idea.</p>
<p class="normal">The following screenshot visually demonstrates what trajectory evaluation results look like in the LangSmith interface. It shows the perfect trajectory match score (<strong class="screenText">1.00</strong>), which validates that the agent followed the expected path:</p>
<figure class="mediaobject"><img alt="Figure 8.1: Trajectory evaluation in LangSmith" src="img/B32363_08_01.png"/></figure>
<p class="packt_figref">Figure 8.1: Trajectory evaluation in LangSmith</p>
<p class="normal">Please note that LangSmith displays the actual trajectory steps side by side with the reference trajectory and that it includes real execution metrics like latency and token usage.</p>
<div><p class="normal">Trajectory evaluation provides unique insights beyond simple pass/fail assessments:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Identifying failure points</strong>: Pinpoint exactly where agents deviate from expected paths</li>
<li class="b lletList"><strong class="keyWord">Process improvement</strong>: Recognize when agents take unnecessary detours or inefficient routes</li>
<li class="b lletList"><strong class="keyWord">Tool usage patterns</strong>: Understand how agents leverage available tools and when they make suboptimal choices</li>
<li class="b lletList"><strong class="keyWord">Reasoning quality</strong>: Evaluate the agent’s decision-making process independent of final outcomes</li>
</ul>
<p class="normal">For example, an agent might provide a correct medication dosage but reach it through an inappropriate trajectory (bypassing safety checks or using unreliable data sources). Trajectory evaluation reveals these process issues that outcome-focused evaluation would miss.</p>
<p class="normal">Consider using trajectory evaluation in conjunction with other evaluation types for a holistic assessment of your <a id="_idIndexMarker697"/>agent’s performance. This approach is particularly valuable during development and debugging phases, where understanding the <em class="italic">why</em> behind agent behavior is as important as measuring final output quality.</p>
<p class="normal">By implementing continuous trajectory monitoring, you can track how agent behaviors evolve as you refine prompts, add tools, or modify the underlying model, ensuring improvements in one area don’t cause regressions in the agen<a id="_idTextAnchor434"/>t’s overall decision-making process.</p>
<h2 class="heading-2" id="_idParaDest-217"><a id="_idTextAnchor435"/>Evaluating CoT reasoning</h2>
<p class="normal">Now suppose we want to evaluate the agent’s reasoning. For example, going back to our earlier example, the agent must not only answer “What is the current interest rate?” but also provide reasoning behind its answer. We can use the <code class="inlineCode">COT_QA</code> evaluator for chain-of-thought evaluation.</p>
<pre>from langchain.evaluation import load_evaluator
# Simulated chain-of-thought reasoning provided by the agent:
agent_reasoning = (
 "The current interest rate is 0.25%. I determined this by recalling that recent monetary policies have aimed "
 "to stimulate economic growth by keeping borrowing costs low. A rate of 0.25% is consistent with the ongoing "
 "trend of low rates, which encourages consumer spending and business investment."
)</pre>
<div><pre># Expected reasoning reference:
expected_reasoning = (
 "An ideal reasoning should mention that the Federal Reserve has maintained a low interest rate—around 0.25%—to "
 "support economic growth, and it should briefly explain the implications for borrowing costs and consumer spending."
)
# Load the chain-of-thought evaluator.
cot_evaluator = load_evaluator("cot_qa")
result_reasoning = cot_evaluator.evaluate_strings(
 input="What is the current Federal Reserve interest rate and why does it matter?",
    prediction=agent_reasoning,
    reference=expected_reasoning,
)
print("\nChain-of-Thought Reasoning Evaluation:")
print(result_reasoning)</pre>
<p class="normal">The returned <a id="_idIndexMarker698"/>score and reasoning allow us to judge whether the agent’s thought process is sound and comprehensive:</p>
<pre>Chain-of-Thought Reasoning Evaluation:
{'reasoning': "The student correctly identified the current Federal Reserve interest rate as 0.25%. They also correctly explained why this rate matters, stating that it is intended to stimulate economic growth by keeping borrowing costs low, which in turn encourages consumer spending and business investment. This explanation aligns with the context provided, which asked for a brief explanation of the implications for borrowing costs and consumer spending. Therefore, the student's answer is factually accurate.\nGRADE: CORRECT", 'value': 'CORRECT', 'score': 1}</pre>
<p class="normal">Please note that in this <a id="_idIndexMarker699"/>evaluation, the agent provides detailed reasoning along with its answer. The evaluator (using chain-of-thought evaluation) compares the agent’s r<a id="_idTextAnchor436"/>easoning with an expected explanation.</p>
<div><h1 class="heading-1" id="_idParaDest-218"><a id="_idTextAnchor437"/>Offline evaluation</h1>
<p class="normal">Offline evaluation<a id="_idIndexMarker700"/> involves assessing the agent’s performance under controlled conditions before deployment. This includes benchmarking to establish general performance baselines and more targeted testing based on generated test cases. Offline evaluations provide key metrics, error analyses, and pass/fail summaries from controlled test scenarios, establishing baseline performance.</p>
<p class="normal">While human assessments are sometimes seen as the gold standard, they are hard to scale and require careful design to avoid bias from subjective preferences or authoritative tones. Benchmarking involves comparing the performance of LLMs against standardized tests or tasks. This helps identify the strengths and weaknesses of the models and guides further development and improvement.</p>
<p class="normal">In the next section, we’ll discuss creating an effective evaluation dataset withi<a id="_idTextAnchor438"/>n the context of RAG system evaluation.</p>
<h2 class="heading-2" id="_idParaDest-219"><a id="_idTextAnchor439"/>Evaluating RAG systems</h2>
<p class="normal">The dimensions of <a id="_idIndexMarker701"/>RAG evaluation discussed earlier (retrieval quality, contextual relevance, faithful generation, and information synthesis) provided a foundation for understanding how to measure RAG system effectiveness. Understanding failure patterns of RAG systems helps create more effective evaluation strategies. Barnett and colleagues in their 2024 paper <em class="italic">Seven Failure Points When Engineering a Retrieval Augmented Generation System</em> identified several distinct ways RAG systems fail in production environments:</p>
<ul>
<li class="b lletList">First, <strong class="keyWord">missing content</strong> <strong class="keyWord">failures </strong>occur when the system fails to retrieve relevant information that exists in the knowledge base. This might happen because of chunking strategies that split related information, embedding models that miss semantic connections, or content gaps in the knowledge base itself.</li>
<li class="b lletList">Second, <strong class="keyWord">ranking failures</strong> happen when relevant documents exist but aren’t ranked highly enough to be included in the context window. This commonly stems from suboptimal embedding models, vocabulary mismatches between queries and documents, or poor chunking granularity.</li>
<li class="b lletList"><strong class="keyWord">Context window limitations</strong> create another failure mode when key information is spread across documents that exceed the model’s context limit. This forces difficult tradeoffs between including more documents and maintaining sufficient detail from each one.</li>
<li class="b lletList">Perhaps most critically, <strong class="keyWord">information extraction failures</strong> occur when relevant information is retrieved but the LLM fails to properly synthesize it. This might happen due to ineffective prompting, complex information formats, or conflicting information across documents.</li>
</ul>
<div><p class="normal">To effectively evaluate and address these specific failure modes, we need a structured and comprehensive evaluation approach. The following example demonstrates how to build a carefully designed evaluation dataset in LangSmith that allows for testing each of these failure patterns in the context of financial advisory systems. By creating realistic questions with expected answers and relevant metadata, we can systematically identify which failure modes most <a id="_idIndexMarker702"/>frequently <a id="_idIndexMarker703"/>affect our particular implementation:</p>
<pre># Define structured examples with queries, reference answers, and contexts
financial_examples = [
    {
 "inputs": {
 "question": "What are the tax implications of early 401(k) withdrawal?",
 "context_needed": ["retirement", "taxation", "penalties"]
        },
 "outputs": {
 "answer": "Early withdrawals from a 401(k) typically incur a 10% penalty if you're under 59½ years old, in addition to regular income taxes. However, certain hardship withdrawals may qualify for penalty exemptions.",
 "key_points": ["10% penalty", "income tax", "hardship exemptions"],
 "documents": ["IRS publication 575", "Retirement plan guidelines"]
        }
    },
    {
 "inputs": {
 "question": "How does dollar-cost averaging compare to lump-sum investing?",
 "context_needed": ["investment strategy", "risk management", "market timing"]
        },
 "outputs": {
 "answer": "Dollar-cost averaging spreads investments over time to reduce timing risk, while lump-sum investing typically outperforms in rising markets due to longer market exposure. DCA may provide psychological benefits through reduced volatility exposure.",
 "key_points": ["timing risk", "market exposure", "psychological benefits"],</pre>
<div><pre> "documents": ["Investment strategy comparisons", "Market timing research"]
        }
    },
 # Additional examples would be added here
]</pre>
<p class="normal">This dataset structure serves multiple evaluation purposes. First, it identifies specific documents that should be retrieved, allowing evaluation of retrieval accuracy. It then defines key points that should appear in the response, enabling assessment of information extraction. Finally, it connects each example to testing objectives, making it easier to diagnose specific system capabilities.</p>
<p class="normal">When implementing this dataset in practice, organizations typically load these examples into evaluation platforms like LangSmith, allowing automated testing of their RAG systems. The results reveal specific patterns in system performance—perhaps strong retrieval but weak synthesis, or excellent performance on simple factual questions but struggles with complex perspective inquiries.</p>
<p class="normal">However, implementing effective RAG evaluation goes beyond simply creating datasets; it requires using diagnostic tools to pinpoint exactly where failures occur within the system pipeline. Drawing on research, these diagnostics identify specific failure modes, such as poor document ranking (information exists but isn’t prioritized) or poor context utilization (the agent ignores relevant retrieved documents). By diagnosing these issues, organizations gain actionable insights—for instance, consistent ranking failures might suggest implementing hybrid search, while context utilization problems could lead to refined prompting or structured outputs.</p>
<p class="normal">The ultimate goal of RAG evaluation is to drive continuous improvement. Organizations achieving the most success follow an iterative cycle: running comprehensive diagnostics to find specific failure patterns, prioritizing fixes based on their frequency and impact, implementing targeted changes, and <a id="_idIndexMarker704"/>then re-evaluating to measure the <a id="_idIndexMarker705"/>improvement. By systematically diagnosing issues and using those insights to iterate, teams can build more accurate and reliable RAG systems with fewer common errors.</p>
<p class="normal">In the next section, we’ll see how we can use <strong class="keyWord">LangSmith</strong>, a companion project for LangChain, to benchmark and evaluate our system’s performance <a id="_idTextAnchor440"/>on a dataset. Let’s step through an example!</p>
<div><h2 class="heading-2" id="_idParaDest-220"><a id="_idTextAnchor441"/>Evaluating a benchmark in LangSmith</h2>
<p class="normal">As we’ve mentioned, comprehensive<a id="_idIndexMarker706"/> benchmarking and evaluation, including testing, are critical for safety, robustness, and intended behavior. LangSmith, despite being a platform designed for testing, debugging, monitoring, and improving LLM applications, offers tools for evaluation and dataset management. LangSmith integrates seamlessly with LangChain Benchmarks, providing a cohesive framework for developing and assessing LLM applications.</p>
<p class="normal">We can run evaluations against benchmark datasets in LangSmith, as we’ll see now. First, please make sure you create an account on LangSmith here: <a href="https://smith.langchain.com/">https://smith.langchain.com/</a>.</p>
<p class="normal">You can obtain an API key and set it as <code class="inlineCode">LANGCHAIN_API_KEY</code> in your environment. We can also set environment variables for project ID and tracing:</p>
<pre># Basic LangSmith Integration Example
import os
# Set up environment variables for LangSmith tracing
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "LLM Evaluation Example"
print("Setting up LangSmith tracing...")</pre>
<p class="normal">This configuration establishes a connection to LangSmith and directs all traces to a specific project. When no project ID is explicitly defined, LangChain logs against the default project. The <code class="inlineCode">LANGCHAIN_TRACING_V2</code> flag enables the most recent version of LangSmith’s tracing capabilities.</p>
<p class="normal">After configuring<a id="_idIndexMarker707"/> the environment, we can begin logging<a id="_idIndexMarker708"/> interactions with our LLM applications. Each interaction creates a traceable record in LangSmith:</p>
<pre>from langchain_openai import ChatOpenAI
from langsmith import Client
# Create a simple LLM call that will be traced in LangSmith
llm = ChatOpenAI()
response = llm.invoke("Hello, world!")
print(f"Model response: {response.content}")
print("\nThis run has been logged to LangSmith.")</pre>
<div><p class="normal">When this code executes, it performs a simple interaction with the ChatOpenAI model and automatically logs the request, response, and performance metrics to LangSmith. These logs appear in the LangSmith project dashboard at <a href="https://smith.langchain.com/projects">https://smith.langchain.com/projects</a>, allowing for detailed inspection of each interaction.</p>
<p class="normal">We can create a dataset from existing agent runs with the <code class="inlineCode">create_example_from_run()</code> function—or from anything else. Here’s how to create a dataset with a set of questions:</p>
<pre>from langsmith import Client
client = Client()
# Create dataset in LangSmith
dataset_name = "Financial Advisory RAG Evaluation"
dataset = client.create_dataset(
    dataset_name=dataset_name,
    description="Evaluation dataset for financial advisory RAG systems covering retirement, investments, and tax planning."
)
# Add examples to the dataset
for example in financial_examples:
    client.create_example(
        inputs=example["inputs"],
        outputs=example["outputs"],
        dataset_id=dataset.id
    )
print(f"Created evaluation dataset with {len(financial_examples)} examples")</pre>
<p class="normal">This code creates a new evaluation dataset in LangSmith containing financial advisory questions. Each example includes an input query and an expected output answer, establishing a reference standard against which we can evaluate our LLM application responses.</p>
<p class="normal">We can now define our RAG system with a function like this:</p>
<pre>def construct_chain():
 return None</pre>
<div><p class="normal">In a complete implementation, you would prepare a vector store with relevant financial documents, create appropriate prompt templates, and configure the retrieval and response generation<a id="_idIndexMarker709"/> components. The concepts and techniques for building<a id="_idIndexMarker710"/> robust RAG systems are covered extensively in <a href="E_Chapter_4.xhtml#_idTextAnchor152"><em class="italic">Chapter 4</em></a>, which provides step-by-step guidance on document processing, embedding creation, vector store setup, and chain construction.</p>
<p class="normal">We can make changes to our chain and evaluate changes in the application. Does the change improve the result or not? Changes can be in any part of our application, be it a new model, a new prompt template, or a new chain or agent. We can run two versions of the application with the same input examples and save the results of the runs. Then we evaluate the results by comparing them side by side.</p>
<p class="normal">To run an evaluation on a dataset, we can either specify an LLM or—for parallelism—use a constructor function to initialize the model or LLM app for each input. Now, to evaluate the performance against our dataset, we need to define an evaluator as we saw in the previous section:</p>
<pre>from langchain.smith import RunEvalConfig
# Define evaluation criteria specific to RAG systems
evaluation_config = RunEvalConfig(
    evaluators=[
 # Correctness: Compare response to reference answer
        RunEvalConfig.LLM(
            criteria={
 "factual_accuracy": "Does the response contain only factually correct information consistent with the reference answer?"
            }
        ),
 # Groundedness: Ensure response is supported by retrieved context
        RunEvalConfig.LLM(
            criteria={
 "groundedness": "Is the response fully supported by the retrieved documents without introducing unsupported information?"
            }
        ),
 # Retrieval quality: Assess relevance of retrieved documents
        RunEvalConfig.LLM(
            criteria={</pre>
<div><pre> "retrieval_relevance": "Are the retrieved documents relevant to answering the question?"
            }
        )
    ]
)</pre>
<p class="normal">This shows how to configure multi-dimensional evaluation for RAG systems, assessing factual accuracy, groundedness, and<a id="_idIndexMarker711"/> retrieval quality using LLM-based <a id="_idIndexMarker712"/>judges. The criteria are defined by a dictionary that includes a criterion as a key and a question to check for as the value.</p>
<p class="normal">We’ll now pass a dataset together with the evaluation configuration with evaluators to <code class="inlineCode">run_on_dataset()</code> to generate metrics and feedback:</p>
<pre>from langchain.smith import run_on_dataset
results = run_on_dataset(
    client=client,
    dataset_name=dataset_name,
    dataset=dataset,
    llm_or_chain_factory=construct_chain,
    evaluation=evaluation_config
)</pre>
<p class="normal">In the same way, we could pass a dataset and evaluators to <code class="inlineCode">run_on_dataset()</code> to generate metrics and feedback asynchronously.</p>
<p class="normal">This practical implementation provides a framework you can adapt for your specific domain. By creating a comprehensive evaluation dataset and assessing your RAG system across multiple dimensions (correctness, groundedness, and retrieval quality), you can identify specific areas for improvement and track progress as you refine your system.</p>
<p class="normal">When implementing this approach, consider incorporating real user queries from your application logs (appropriately anonymized) to ensure your evaluation dataset reflects actual usage patterns. Additionally, periodically refreshing your dataset with new queries and updated information helps prevent overfitting and ensures your evaluation remains relevant as user needs evolve.</p>
<p class="normal">Let’s use the <a id="_idIndexMarker713"/>datasets and evaluate libraries by HuggingFace <a id="_idIndexMarker714"/>to check a<a id="_idTextAnchor442"/> coding LLM approach to solving programming problems.</p>
<div><h2 class="heading-2" id="_idParaDest-221"><a id="_idTextAnchor443"/>Evaluating a benchmark with HF datasets and Evaluate</h2>
<p class="normal">As a reminder: the <code class="inlineCode">pass@k</code> metric<a id="_idIndexMarker715"/> is a way to evaluate the performance of an LLM in solving programming exercises. It measures the proportion of exercises for which the LLM generated at least one correct solution within the top <code class="inlineCode">k</code> candidates. A higher <code class="inlineCode">pass@k</code> score indicates better performance, as it means the LLM was able to generate a correct solution more often within the top <code class="inlineCode">k</code> candidates.</p>
<p class="normal">Hugging Face’s <code class="inlineCode">Evaluate</code> library makes it very easy to calculate <code class="inlineCode">pass@k</code> and other metrics. Here’s an example:</p>
<pre>from datasets import load_dataset
from evaluate import load
from langchain_core.messages import HumanMessage
human_eval = load_dataset("openai_humaneval", split="test")
code_eval_metric = load("code_eval")
test_cases = ["assert add(2,3)==5"]
candidates = [["def add(a,b): return a*b", "def add(a, b): return a+b"]]
pass_at_k, results = code_eval_metric.compute(references=test_cases, predictions=candidates, k=[1, 2])
print(pass_at_k)</pre>
<p class="normal">We should get an output like this:</p>
<pre>{'pass@1': 0.5, 'pass@2': 1.0}</pre>
<div><div><p class="normal">For this code to run, you need to set the <code class="inlineCode">HF_ALLOW_CODE_EVAL</code> environment variable to 1. Please be cautious: running LLM code on your machine comes with a risk.</p>
</div>
</div>
<p class="normal">This shows how to evaluate code generation models using HuggingFace’s <code class="inlineCode">code_eval</code> metric, which measures a<a id="_idIndexMarker716"/> model’s ability to produce functioning co<a id="_idTextAnchor444"/>de solutions. This is great. Let’s see another example.</p>
<div><h2 class="heading-2" id="_idParaDest-222"><a id="_idTextAnchor445"/>Evaluating email extraction</h2>
<p class="normal">Let’s show how we can use it <a id="_idIndexMarker717"/>to evaluate an LLM’s ability to extract structured information from insurance claim texts.</p>
<p class="normal">We’ll first create a synthetic dataset using LangSmith. In this synthetic dataset, each example consists of a raw insurance claim text (input) and its corresponding expected structured output (output). We will use this dataset to run extraction chains and evaluate your model’s performance.</p>
<p class="normal">We assume that you’ve<a id="_idIndexMarker718"/> already set up your LangSmith credentials.</p>
<pre>from langsmith import Client
# Define a list of synthetic insurance claim examples
example_inputs = [
    (
 "I was involved in a car accident on 2023-08-15. My name is Jane Smith, Claim ID INS78910, "
 "Policy Number POL12345, and the damage is estimated at $3500.",
        {
 "claimant_name": "Jane Smith",
 "claim_id": "INS78910",
 "policy_number": "POL12345",
 "claim_amount": "$3500",
 "accident_date": "2023-08-15",
 "accident_description": "Car accident causing damage",
 "status": "pending"
        }
    ),
    (
 "My motorcycle was hit in a minor collision on 2023-07-20. I am John Doe, with Claim ID INS112233 "
 "and Policy Number POL99887. The estimated damage is $1500.",
        {
 "claimant_name": "John Doe",
 "claim_id": "INS112233",
 "policy_number": "POL99887",
 "claim_amount": "$1500",
 "accident_date": "2023-07-20",
 "accident_description": "Minor motorcycle collision",</pre>
<div><pre> "status": "pending"
        }
    )
]</pre>
<p class="normal">We can upload this dataset to LangSmith:</p>
<pre>client = Client()
dataset_name = "Insurance Claims"
# Create the dataset in LangSmith
dataset = client.create_dataset(
    dataset_name=dataset_name,
    description="Synthetic dataset for insurance claim extraction tasks",
)
# Store examples in the dataset
for input_text, expected_output in example_inputs:
    client.create_example(
        inputs={"input": input_text},
        outputs={"output": expected_output},
        metadata={"source": "Synthetic"},
        dataset_id=dataset.id,
    )</pre>
<p class="normal">Now let’s <a id="_idIndexMarker719"/>run our <code class="inlineCode">InsuranceClaim</code> dataset on LangSmith. We’ll first<a id="_idIndexMarker720"/> define a schema for our claims:</p>
<pre># Define the extraction schema
from pydantic import BaseModel, Field
class InsuranceClaim(BaseModel):
    claimant_name: str = Field(..., description="The name of the claimant")
    claim_id: str = Field(..., description="The unique insurance claim identifier")
    policy_number: str = Field(..., description="The policy number associated with the claim")
    claim_amount: str = Field(..., description="The claimed amount (e.g., '$5000')")</pre>
<div><pre>    accident_date: str = Field(..., description="The date of the accident (YYYY-MM-DD)")
    accident_description: str = Field(..., description="A brief description of the accident")
    status: str = Field("pending", description="The current status of the claim")</pre>
<p class="normal">Now we’ll define our extraction chain. We are keeping it very simple; we’ll just ask for a JSON object that follows the <code class="inlineCode">InsuranceClaim</code> schema. The extraction chain is defined with ChatOpenAI LLM with function calling bound to our schema:</p>
<pre># Create extraction chain
from langchain.chat_models import ChatOpenAI
from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser
instructions = (
 "Extract the following structured information from the insurance claim text: "
 "claimant_name, claim_id, policy_number, claim_amount, accident_date, "
 "accident_description, and status. Return the result as a JSON object following "
 "this schema: " + InsuranceClaim.schema_json()
)
llm = ChatOpenAI(model="gpt-4", temperature=0).bind_functions(
    functions=[InsuranceClaim.schema()],
    function_call="InsuranceClaim"
)
output_parser = JsonOutputFunctionsParser()
extraction_chain = instructions | llm | output_parser | (lambda x: {"output": x})</pre>
<p class="normal">Finally, we can run <a id="_idIndexMarker721"/>the extraction chain on our sample <a id="_idIndexMarker722"/>insurance claim:</p>
<pre># Test the extraction chain
sample_claim_text = (
 "I was involved in a car accident on 2023-08-15. My name is Jane Smith, "</pre>
<div><pre> "Claim ID INS78910, Policy Number POL12345, and the damage is estimated at $3500. "
 "Please process my claim."
)
result = extraction_chain.invoke({"input": sample_claim_text})
print("Extraction Result:")
print(result)</pre>
<p class="normal">This showed how to evaluate structured information extraction from insurance claims text, using a Pydantic<a id="_idIndexMarker723"/> schema to standardize extraction and<a id="_idIndexMarker724"/> LangSmith to assess performance.</p>
<h1 class="heading-1" id="_idParaDest-223"><a id="_idTextAnchor446"/>Summary</h1>
<p class="normal">In this chapter, we outlined critical strategies for evaluating LLM applications, ensuring robust performance before production deployment. We provided an overview of the importance of evaluation, architectural challenges, evaluation strategies, and types of evaluation. We then demonstrated practical evaluation techniques through code examples, including correctness evaluation using exact matches and LLM-as-a-judge approaches. For instance, we showed how to implement the <code class="inlineCode">ExactMatchStringEvaluator</code> for comparing answers about Federal Reserve interest rates, and how to use <code class="inlineCode">ScoreStringEvalChain</code> for more nuanced evaluations. The examples also covered JSON format validation using <code class="inlineCode">JsonValidityEvaluator</code> and assessment of agent trajectories in healthcare scenarios.</p>
<p class="normal">Tools like LangChain provide predefined evaluators for criteria such as conciseness and relevance, while platforms like LangSmith enable comprehensive testing and monitoring. The chapter presented code examples using LangSmith to create and evaluate datasets, demonstrating how to assess model performance across multiple criteria. The implementation of <code class="inlineCode">pass@k</code> metrics using Hugging Face’s Evaluate library was shown for assessing code generation capabilities. We also walked through an example of evaluating insurance claim text extraction using structured schemas and LangChain’s evaluation capabilities.</p>
<p class="normal">Now that we’ve evaluated our AI workflows, in the next chapter we’ll look at how we can deploy and monitor them. Let’s discuss deployment and observability!</p>
<div><h1 class="heading-1" id="_idParaDest-224"><a id="_idTextAnchor447"/>Questions</h1>
<ol>
<li class="numberedList" value="1">Describe three key metrics used in evaluating AI agents.</li>
<li class="numberedList">What’s the difference between online and offline evaluation?</li>
<li class="numberedList">What are system-level and application-level evaluations and how do they differ?</li>
<li class="numberedList">How can LangSmith be used to compare different versions of an LLM application?</li>
<li class="numberedList">How does chain-of-thought evaluation differ from traditional output evaluation?</li>
<li class="numberedList">Why is trajectory evaluation important for understanding agent behavior?</li>
<li class="numberedList">What are the key considerations when evaluating LLM agents for production deployment?</li>
<li class="numberedList">How can bias be mitigated when using language models as evaluators?</li>
<li class="numberedList">What role do standardized benchmarks play, and how can we create benchmark datasets for LLM agent evaluation?</li>
<li class="numberedList">How do you balance automated evaluation metrics with human evaluation in production systems?</li>
</ol>
</div>
</body></html>