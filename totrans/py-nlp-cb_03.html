<html><head></head><body>
		<div id="_idContainer011" class="calibre2">
			<h1 id="_idParaDest-66" class="chapter-number"><a id="_idTextAnchor067" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1.1">3</span></h1>
			<h1 id="_idParaDest-67" class="calibre7"><a id="_idTextAnchor068" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.2.1">Representing Text  – Capturing Semantics</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.3.1">Representing the meaning of words, phrases, and sentences in a form that’s understandable to computers is one of the pillars of NLP processing. </span><span class="kobospan" id="kobo.3.2">Machine learning, for example, represents each data point as a list of numbers (a fixed-size vector), and we are faced with the question of how to turn words and sentences into these vectors. </span><span class="kobospan" id="kobo.3.3">Most NLP tasks start by representing the text in some numeric form, and in this chapter, we show several ways to </span><span><span class="kobospan" id="kobo.4.1">do that.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.5.1">First, we will create a simple classifier to demonstrate the effectiveness of each method of encoding, and then we will use it to test the different encoding methods. </span><span class="kobospan" id="kobo.5.2">We will also learn how to turn phrases such as </span><em class="italic"><span class="kobospan" id="kobo.6.1">fried chicken</span></em><span class="kobospan" id="kobo.7.1"> into vectors – that is, how to train a </span><strong class="source-inline"><span class="kobospan" id="kobo.8.1">word2vec</span></strong><span class="kobospan" id="kobo.9.1"> model for phrases. </span><span class="kobospan" id="kobo.9.2">Finally, we will see how to use </span><span><span class="kobospan" id="kobo.10.1">vector-based search.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.11.1">For a theoretical background on some of the concepts discussed in this section, refer to </span><em class="italic"><span class="kobospan" id="kobo.12.1">Building Machine Learning Systems with Python</span></em><span class="kobospan" id="kobo.13.1"> by Coelho et al. </span><span class="kobospan" id="kobo.13.2">This book will explain the basics of building a machine learning project, such as training and test sets, as well as metrics used to evaluate such projects, including precision, recall, F1, </span><span><span class="kobospan" id="kobo.14.1">and accuracy.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.15.1">Here are the recipes that are covered in </span><span><span class="kobospan" id="kobo.16.1">this chapter:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.17.1">Creating a </span><span><span class="kobospan" id="kobo.18.1">simple classifier</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.19.1">Putting documents into a bag </span><span><span class="kobospan" id="kobo.20.1">of words</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.21.1">Constructing an </span><span><em class="italic"><span class="kobospan" id="kobo.22.1">N</span></em></span><span><span class="kobospan" id="kobo.23.1">-gram model</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.24.1">Representing texts </span><span><span class="kobospan" id="kobo.25.1">with TF-IDF</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.26.1">Using </span><span><span class="kobospan" id="kobo.27.1">word embeddings</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.28.1">Training your own </span><span><span class="kobospan" id="kobo.29.1">embeddings model</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.30.1">Using BERT and OpenAI embeddings instead of </span><span><span class="kobospan" id="kobo.31.1">word embeddings</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.32.1">Using </span><strong class="bold"><span class="kobospan" id="kobo.33.1">retrieval augmented </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.34.1">generation</span></strong></span><span><span class="kobospan" id="kobo.35.1"> (</span></span><span><strong class="bold"><span class="kobospan" id="kobo.36.1">RAG</span></strong></span><span><span class="kobospan" id="kobo.37.1">)</span></span></li>
			</ul>
			<h1 id="_idParaDest-68" class="calibre7"><a id="_idTextAnchor069" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.38.1">Technical requirements</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.39.1">The code for this chapter is located at </span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter03" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.40.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter03</span></a><span class="kobospan" id="kobo.41.1">. </span><span class="kobospan" id="kobo.41.2">Packages that are required for this chapter should be installed automatically via the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.42.1">poetry</span></strong></span><span><span class="kobospan" id="kobo.43.1"> environment.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.44.1">In addition, we will use models and datasets located at the following URLs. </span><span class="kobospan" id="kobo.44.2">The Google </span><strong class="source-inline"><span class="kobospan" id="kobo.45.1">word2vec</span></strong><span class="kobospan" id="kobo.46.1"> model is a model that represents words as vectors, and the IMDB dataset contains movie titles, genres, and descriptions. </span><span class="kobospan" id="kobo.46.2">Download them into the </span><strong class="source-inline"><span class="kobospan" id="kobo.47.1">data</span></strong><span class="kobospan" id="kobo.48.1"> folder inside the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.49.1">root</span></strong></span><span><span class="kobospan" id="kobo.50.1"> directory:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.51.1">The Google</span></strong> <strong class="source-inline1"><span class="kobospan" id="kobo.52.1">word2vec</span></strong> <span><strong class="bold"><span class="kobospan" id="kobo.53.1">model</span></strong></span><span><span class="kobospan" id="kobo.54.1">: </span></span><a href="https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.55.1">https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g</span></span></a></li>
				<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.56.1">The IMDB movie dataset</span></strong><span class="kobospan" id="kobo.57.1">: </span><a href="https://github.com/venusanvi/imdb-movies/blob/main/IMDB-Movie-Data.csv" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.58.1">https://github.com/venusanvi/imdb-movies/blob/main/IMDB-Movie-Data.csv</span></a><span class="kobospan" id="kobo.59.1"> (also available in the book’s </span><span><span class="kobospan" id="kobo.60.1">GitHub repo)</span></span></li>
			</ul>
			<p class="calibre3"><a id="_idTextAnchor070" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.61.1">In addition to the preceding files, we will use various functions from a simple classifier that we will create in the first recipe. </span><span class="kobospan" id="kobo.61.2">This file is available </span><span><span class="kobospan" id="kobo.62.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/util_simple_classifier.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.63.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/util_simple_classifier.ipynb</span></span></a><span><span class="kobospan" id="kobo.64.1">.</span></span></p>
			<h1 id="_idParaDest-69" class="calibre7"><a id="_idTextAnchor071" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.65.1">Creating a simple classifier</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.66.1">The reason why we need to </span><a id="_idIndexMarker098" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.67.1">represent text as vectors is to make it into a computer-readable form. </span><span class="kobospan" id="kobo.67.2">Computers can’t understand words but are good at manipulating numbers. </span><span class="kobospan" id="kobo.67.3">One of the main NLP tasks is the classification of texts, and we are going to create a classifier for movie reviews. </span><span class="kobospan" id="kobo.67.4">We will use the same classifier code but with different methods of creating vectors </span><span><span class="kobospan" id="kobo.68.1">from text.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.69.1">In this section, we will create the classifier that will assign either negative or positive sentiment to </span><em class="italic"><span class="kobospan" id="kobo.70.1">Rotten Tomatoes </span></em><span class="kobospan" id="kobo.71.1">reviews, a dataset available through Hugging Face, a large repository of open source models and datasets. </span><span class="kobospan" id="kobo.71.2">We will then use a baseline method, where we encode the text by counting the number of different parts of speech present in it (verbs, nouns, proper nouns, adjectives, adverbs, auxiliary verbs, pronouns, numbers, </span><span><span class="kobospan" id="kobo.72.1">and punctuation).</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.73.1">By the end of this recipe, we will have created a separate file with functions that create the dataset and train the classifier. </span><span class="kobospan" id="kobo.73.2">We will use this file throughout the chapter to test different </span><span><span class="kobospan" id="kobo.74.1">encoding methods.</span></span></p>
			<h2 id="_idParaDest-70" class="calibre5"><a id="_idTextAnchor072" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.75.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.76.1">In this recipe, we will create </span><a id="_idIndexMarker099" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.77.1">a simple classifier for movie reviews. </span><span class="kobospan" id="kobo.77.2">It will be a </span><strong class="bold"><span class="kobospan" id="kobo.78.1">logistic regression classifier</span></strong><span class="kobospan" id="kobo.79.1"> from the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.80.1">sklearn</span></strong></span><span><span class="kobospan" id="kobo.81.1"> package.</span></span></p>
			<h2 id="_idParaDest-71" class="calibre5"><a id="_idTextAnchor073" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.82.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.83.1">We will load the Rotten Tomatoes dataset from Hugging Face. </span><span class="kobospan" id="kobo.83.2">We will use just part of the dataset so that the training time is not </span><span><span class="kobospan" id="kobo.84.1">very long:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.85.1">Import the file and language </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.86.1">util</span></strong></span><span><span class="kobospan" id="kobo.87.1"> notebooks:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.88.1">
%run -i "../util/file_utils.ipynb"
%run -i "../util/lang_utils.ipynb"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.89.1">Load the train and test datasets from Hugging Face (the </span><strong class="source-inline1"><span class="kobospan" id="kobo.90.1">datasets</span></strong><span class="kobospan" id="kobo.91.1"> package). </span><span class="kobospan" id="kobo.91.2">For both the training and test sets, we will select the first and last 15% of the data instead of loading the full datasets. </span><span class="kobospan" id="kobo.91.3">The full dataset is large, and it takes a long time to train </span><span><span class="kobospan" id="kobo.92.1">the model:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.93.1">
from datasets import load_dataset
train_dataset = load_dataset("rotten_tomatoes",
    split="train[:15%]+train[-15%:]")
test_dataset = load_dataset("rotten_tomatoes",
    split="test[:15%]+test[-15%:]")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.94.1">Print out the length of </span><span><span class="kobospan" id="kobo.95.1">each dataset:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.96.1">
print(len(train_dataset))
print(len(test_dataset))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.97.1">The output should be </span><span><span class="kobospan" id="kobo.98.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.99.1">2560
320</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.100.1">Here, we create the </span><strong class="source-inline1"><span class="kobospan" id="kobo.101.1">POS_vectorizer</span></strong><span class="kobospan" id="kobo.102.1"> class. </span><span class="kobospan" id="kobo.102.2">This class has a method, </span><strong class="source-inline1"><span class="kobospan" id="kobo.103.1">vectorize</span></strong><span class="kobospan" id="kobo.104.1">, that processes the text and counts the number of verbs, nouns, proper nouns, adjectives, adverbs, auxiliary verbs, pronouns, numbers, and punctuation marks. </span><span class="kobospan" id="kobo.104.2">The class needs a </span><strong class="source-inline1"><span class="kobospan" id="kobo.105.1">spaCy</span></strong><span class="kobospan" id="kobo.106.1"> model to process the text. </span><span class="kobospan" id="kobo.106.2">Each piece of text is turned into </span><a id="_idIndexMarker100" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.107.1">a vector of size 10. </span><span class="kobospan" id="kobo.107.2">The first element of the vector is the length of the text, and the other numbers indicate the number of words in the text of that particular part </span><span><span class="kobospan" id="kobo.108.1">of speech:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.109.1">
class POS_vectorizer:
    def __init__(self, spacy_model):
        self.model = spacy_model
    def vectorize(self, input_text):
        doc = self.model(input_text)
        vector = []
        vector.append(len(doc))
        pos = {"VERB":0, "NOUN":0, "PROPN":0, "ADJ":0,
            "ADV":0, "AUX":0, "PRON":0, "NUM":0, "PUNCT":0}
        for token in doc:
            if token.pos_ in pos:
                pos[token.pos_] += 1
        vector_values = list(pos.values())
        vector = vector + vector_values
        return vector</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.110.1">Now, we can test out the </span><strong class="source-inline1"><span class="kobospan" id="kobo.111.1">POS_vectorizer</span></strong><span class="kobospan" id="kobo.112.1"> class. </span><span class="kobospan" id="kobo.112.2">We take the first review’s text to process and</span><a id="_idIndexMarker101" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.113.1"> create the vectorizer using the small </span><strong class="source-inline1"><span class="kobospan" id="kobo.114.1">spaCy</span></strong><span class="kobospan" id="kobo.115.1"> model. </span><span class="kobospan" id="kobo.115.2">We then vectorize the text using the newly </span><span><span class="kobospan" id="kobo.116.1">created class:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.117.1">
sample_text = train_dataset[0]["text"]
vectorizer = POS_vectorizer(small_model)
vector = vectorizer.vectorize(sample_text)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.118.1">Let’s print the text and </span><span><span class="kobospan" id="kobo.119.1">the vector:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.120.1">
print(sample_text)
print(vector)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.121.1">The result should look like this. </span><span class="kobospan" id="kobo.121.2">We can see that the vector correctly counts the parts of speech. </span><span class="kobospan" id="kobo.121.3">For example, there are five punctuation marks (two quotes, one comma, one dot, and </span><span><span class="kobospan" id="kobo.122.1">one dash):</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.123.1">the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .
</span><span class="kobospan1" id="kobo.123.2">[38, 3, 8, 3, 4, 1, 3, 1, 0, 5]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.124.1">We will now prepare the data for training our classifier. </span><span class="kobospan" id="kobo.124.2">We first import the </span><strong class="source-inline1"><span class="kobospan" id="kobo.125.1">pandas</span></strong><span class="kobospan" id="kobo.126.1"> and the </span><strong class="source-inline1"><span class="kobospan" id="kobo.127.1">numpy</span></strong><span class="kobospan" id="kobo.128.1"> packages and then create two dataframes, one for training and the other one for testing. </span><span class="kobospan" id="kobo.128.2">In each dataset, we create a new column called </span><strong class="source-inline1"><span class="kobospan" id="kobo.129.1">vector</span></strong><span class="kobospan" id="kobo.130.1"> that contains the vector for this piece of text. </span><span class="kobospan" id="kobo.130.2">We use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.131.1">apply</span></strong><span class="kobospan" id="kobo.132.1"> method to turn the text into vectors and store them in the new column. </span><span class="kobospan" id="kobo.132.2">In this method, we pass in a lambda function that takes a piece of text and applies the </span><strong class="source-inline1"><span class="kobospan" id="kobo.133.1">vectorize</span></strong><span class="kobospan" id="kobo.134.1"> method of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.135.1">POS_vectorizer</span></strong><span class="kobospan" id="kobo.136.1"> class to that piece of text. </span><span class="kobospan" id="kobo.136.2">We then turn the vector and the label columns into </span><strong class="source-inline1"><span class="kobospan" id="kobo.137.1">numpy</span></strong><span class="kobospan" id="kobo.138.1"> arrays to have the data in the right format for the classifier. </span><span class="kobospan" id="kobo.138.2">We use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.139.1">np.stack</span></strong><span class="kobospan" id="kobo.140.1"> method for the vector, since it’s </span><a id="_idIndexMarker102" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.141.1">already a list, and the </span><strong class="source-inline1"><span class="kobospan" id="kobo.142.1">to_numpy</span></strong><span class="kobospan" id="kobo.143.1"> method for the review labels, since they are </span><span><span class="kobospan" id="kobo.144.1">just numbers:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.145.1">
import pandas as pd
import numpy as np
train_df = train_dataset.to_pandas()
train_df.sample(frac=1)
test_df = test_dataset.to_pandas()
train_df["vector"] = train_df["text"].apply(
    lambda x: vectorizer.vectorize(x))
test_df["vector"] = test_df["text"].apply(
    lambda x: vectorizer.vectorize(x))
X_train = np.stack(train_df["vector"].values, axis=0)
X_test = np.stack(test_df["vector"].values, axis=0)
y_train = train_df["label"].to_numpy()
y_test = test_df["label"].to_numpy()</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.146.1">Now, we will train the classifier. </span><span class="kobospan" id="kobo.146.2">We will choose the logistic regression algorithm, since it is one of the simplest algorithms, as well as one of the fastest. </span><span class="kobospan" id="kobo.146.3">First, we import the </span><strong class="source-inline1"><span class="kobospan" id="kobo.147.1">LogisticRegression</span></strong><span class="kobospan" id="kobo.148.1"> class and the </span><strong class="source-inline1"><span class="kobospan" id="kobo.149.1">classification_report</span></strong><span class="kobospan" id="kobo.150.1"> methods from </span><strong class="source-inline1"><span class="kobospan" id="kobo.151.1">sklearn</span></strong><span class="kobospan" id="kobo.152.1">. </span><span class="kobospan" id="kobo.152.2">Then, we create the </span><strong class="source-inline1"><span class="kobospan" id="kobo.153.1">LogisticRegression</span></strong><span class="kobospan" id="kobo.154.1"> object, and finally, train it on the data from the </span><span><span class="kobospan" id="kobo.155.1">previous step:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.156.1">
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
clf = LogisticRegression(C=0.1)
clf = clf.fit(X_train, y_train)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.157.1">We can test the classifier on the test data by applying the </span><strong class="source-inline1"><span class="kobospan" id="kobo.158.1">predict</span></strong><span class="kobospan" id="kobo.159.1"> method to the vectors in the test data and print out the classification report. </span><span class="kobospan" id="kobo.159.2">We can see that the overall accuracy is</span><a id="_idIndexMarker103" class="calibre6 pcalibre pcalibre1"/><a id="_idIndexMarker104" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.160.1"> low, slightly above chance. </span><span class="kobospan" id="kobo.160.2">This is because the vector representation we used is very crude. </span><span class="kobospan" id="kobo.160.3">We will use other vectors in the next sections and see how they affect the </span><span><span class="kobospan" id="kobo.161.1">classifier results:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.162.1">
test_df["prediction"] = test_df["vector"].apply(
    lambda x: clf.predict([x])[0])
print(classification_report(test_df["label"], 
    test_df["prediction"]))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.163.1">The output should be similar </span><span><span class="kobospan" id="kobo.164.1">to this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.165.1">              precision    recall  f1-score   support
           0       0.59      0.54      0.56       160
           1       0.57      0.62      0.60       160
    accuracy                           0.58       320
   macro avg       0.58      0.58      0.58       320
weighted avg       0.58      0.58      0.58       320</span></pre></li>			</ol>
			<h2 id="_idParaDest-72" class="calibre5"><a id="_idTextAnchor074" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.166.1">There’s more…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.167.1">We will now turn the preceding code into several functions so that we can only vary the vectorizer being used in the construction of the dataset. </span><span class="kobospan" id="kobo.167.2">The resulting file is located at </span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/util_simple_classifier.ipynb" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.168.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/util_simple_classifier.ipynb</span></a><span class="kobospan" id="kobo.169.1">. </span><span class="kobospan" id="kobo.169.2">The resulting code will look </span><span><span class="kobospan" id="kobo.170.1">as follows:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.171.1">Import the </span><span><span class="kobospan" id="kobo.172.1">necessary packages:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.173.1">
from datasets import load_dataset
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.174.1">Define the function that will</span><a id="_idIndexMarker105" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.175.1"> create and return the training and test dataframes. </span><span class="kobospan" id="kobo.175.2">It will create them from the Rotten Tomatoes dataset from </span><span><span class="kobospan" id="kobo.176.1">Hugging Face:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.177.1">
def load_train_test_dataset_pd():
    train_dataset = load_dataset("rotten_tomatoes",
        split="train[:15%]+train[-15%:]")
    test_dataset = load_dataset("rotten_tomatoes",
        split="test[:15%]+test[-15%:]")
    train_df = train_dataset.to_pandas()
    train_df.sample(frac=1)
    test_df = test_dataset.to_pandas()
    return (train_df, test_df)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.178.1">This function takes the dataframes and the </span><strong class="source-inline1"><span class="kobospan" id="kobo.179.1">vectorize</span></strong><span class="kobospan" id="kobo.180.1"> method and creates the </span><strong class="source-inline1"><span class="kobospan" id="kobo.181.1">numpy</span></strong><span class="kobospan" id="kobo.182.1"> arrays for the training and test data. </span><span class="kobospan" id="kobo.182.2">This will allow us to train the logistic regression classifier using the </span><span><span class="kobospan" id="kobo.183.1">created vectors:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.184.1">
def create_train_test_data(train_df, test_df, vectorize):
    train_df["vector"] = train_df["text"].apply(
        lambda x: vectorize(x))
    test_df["vector"] = test_df["text"].apply(
        lambda x: vectorize(x))
    X_train = np.stack(train_df["vector"].values, axis=0)
    X_test = np.stack(test_df["vector"].values, axis=0)
    y_train = train_df["label"].to_numpy()
    y_test = test_df["label"].to_numpy()
    return (X_train, X_test, y_train, y_test)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.185.1">This function trains a </span><a id="_idIndexMarker106" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.186.1">logistic regression classifier on the given </span><span><span class="kobospan" id="kobo.187.1">training data:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.188.1">
def train_classifier(X_train, y_train):
    clf = LogisticRegression(C=0.1)
    clf = clf.fit(X_train, y_train)
    return clf</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.189.1">This final function takes in the test data and the trained classifier and prints out the </span><span><span class="kobospan" id="kobo.190.1">classification report:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.191.1">
def test_classifier(test_df, clf):
    test_df["prediction"] = test_df["vector"].apply(
        lambda x: clf.predict([x])[0])
    print(classification_report(test_df["label"],         test_df["prediction"]))</span></pre></li>			</ol>
			<p class="calibre3"><span class="kobospan" id="kobo.192.1">In each succeeding section that demonstrates a new vectorizing method, we will use this file to pre-load the necessary functions to test the classification result. </span><span class="kobospan" id="kobo.192.2">This will allow us to evaluate the different vectorizing methods. </span><span class="kobospan" id="kobo.192.3">We will only vary the vectorizer while keeping the classifier the same. </span><span class="kobospan" id="kobo.192.4">When the classifier performs better, it reflects how well the underlying vectorizer represents </span><span><span class="kobospan" id="kobo.193.1">the text.</span></span></p>
			<h1 id="_idParaDest-73" class="calibre7"><a id="_idTextAnchor075" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.194.1">Putting documents into a bag of words</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.195.1">A </span><strong class="bold"><span class="kobospan" id="kobo.196.1">bag of words</span></strong><span class="kobospan" id="kobo.197.1"> is the simplest way of representing a text. </span><span class="kobospan" id="kobo.197.2">We treat our text as a collection of </span><em class="italic"><span class="kobospan" id="kobo.198.1">documents</span></em><span class="kobospan" id="kobo.199.1">, where documents are anything from sentences to scientific articles to blog posts or whole books. </span><span class="kobospan" id="kobo.199.2">Since we usually compare different documents to each other or use them in a larger </span><a id="_idIndexMarker107" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.200.1">context of other documents, we work with a collection of documents, not just a </span><span><span class="kobospan" id="kobo.201.1">single document.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.202.1">The bag of words method uses a “training” text that provides it with a list of words that it should consider. </span><span class="kobospan" id="kobo.202.2">When encoding new sentences, it counts the number of occurrences each word makes in the document, and the final vector includes those counts for each word in the vocabulary. </span><span class="kobospan" id="kobo.202.3">This representation can then be fed into a machine </span><span><span class="kobospan" id="kobo.203.1">learning algorithm.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.204.1">The reason this vectorizing method is called a </span><em class="italic"><span class="kobospan" id="kobo.205.1">bag of words</span></em><span class="kobospan" id="kobo.206.1"> is that it does not take into account the relationships of words between themselves and only counts the number of occurrences of each word. </span><span class="kobospan" id="kobo.206.2">The decision on what represents a document lies with the engineer and, in many cases, will be obvious. </span><span class="kobospan" id="kobo.206.3">For example, if you are working on classifying tweets that belong to a particular topic, a single tweet will be your document. </span><span class="kobospan" id="kobo.206.4">If, conversely, you would like to find out which chapters of a book are most similar to a book you already have, then chapters </span><span><span class="kobospan" id="kobo.207.1">are documents.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.208.1">In this recipe, we will create a bag of words for the Rotten Tomatoes reviews. </span><span class="kobospan" id="kobo.208.2">Our documents will be the reviews. </span><span class="kobospan" id="kobo.208.3">We then test the encoding using a bag of words by building a logistic regression classifier, using code from the </span><span><span class="kobospan" id="kobo.209.1">previous recipe.</span></span></p>
			<h2 id="_idParaDest-74" class="calibre5"><a id="_idTextAnchor076" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.210.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.211.1">For this recipe, we will use the </span><strong class="source-inline"><span class="kobospan" id="kobo.212.1">CountVectorizer</span></strong><span class="kobospan" id="kobo.213.1"> class from the </span><strong class="source-inline"><span class="kobospan" id="kobo.214.1">sklearn</span></strong><span class="kobospan" id="kobo.215.1"> package. </span><span class="kobospan" id="kobo.215.2">It is included in the </span><strong class="source-inline"><span class="kobospan" id="kobo.216.1">poetry</span></strong><span class="kobospan" id="kobo.217.1"> environment. </span><span class="kobospan" id="kobo.217.2">The </span><strong class="source-inline"><span class="kobospan" id="kobo.218.1">CountVectorizer</span></strong><span class="kobospan" id="kobo.219.1"> class is specifically designed to count the number of occurrences of each word in </span><span><span class="kobospan" id="kobo.220.1">a text.</span></span></p>
			<h2 id="_idParaDest-75" class="calibre5"><a id="_idTextAnchor077" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.221.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.222.1">Our code will take a set of documents – in this case, reviews – and represent them as a matrix of vectors. </span><span class="kobospan" id="kobo.222.2">We will use the Rotten Tomatoes reviews dataset from Hugging Face for </span><span><span class="kobospan" id="kobo.223.1">this task:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.224.1">Run the simple classifier </span><strong class="source-inline1"><span class="kobospan" id="kobo.225.1">utility</span></strong><span class="kobospan" id="kobo.226.1"> file, and then import the </span><strong class="source-inline1"><span class="kobospan" id="kobo.227.1">CountVectorizer</span></strong><span class="kobospan" id="kobo.228.1"> object and the </span><strong class="source-inline1"><span class="kobospan" id="kobo.229.1">sys</span></strong><span class="kobospan" id="kobo.230.1"> package. </span><span class="kobospan" id="kobo.230.2">We will need the </span><strong class="source-inline1"><span class="kobospan" id="kobo.231.1">sys</span></strong><span class="kobospan" id="kobo.232.1"> package to change the </span><a id="_idIndexMarker108" class="calibre6 pcalibre pcalibre1"/><span><span class="kobospan" id="kobo.233.1">printing options:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.234.1">
%run -i "../util/util_simple_classifier.ipynb"
from sklearn.feature_extraction.text import CountVectorizer
import sys</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.235.1">Load the training and testing dataframes by using the function from the </span><strong class="source-inline1"><span class="kobospan" id="kobo.236.1">util_simple_classifier.ipynb</span></strong><span class="kobospan" id="kobo.237.1"> file. </span><span class="kobospan" id="kobo.237.2">We created this function in the previous recipe, </span><em class="italic"><span class="kobospan" id="kobo.238.1">Creating a simple classifier</span></em><span class="kobospan" id="kobo.239.1">. </span><span class="kobospan" id="kobo.239.2">The function loads 15% of the Rotten Tomatoes dataset into a </span><strong class="source-inline1"><span class="kobospan" id="kobo.240.1">pandas</span></strong><span class="kobospan" id="kobo.241.1"> dataframe and randomizes its order. </span><span class="kobospan" id="kobo.241.2">It might take a few minutes </span><span><span class="kobospan" id="kobo.242.1">to run:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.243.1">
(train_df, test_df) = load_train_test_dataset_pd()</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.244.1">Create the vectorizer, fit it on the training data, and print out the resulting matrix. </span><span class="kobospan" id="kobo.244.2">We will use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.245.1">max_df</span></strong><span class="kobospan" id="kobo.246.1"> parameter to specify which words should be used as stop words. </span><span class="kobospan" id="kobo.246.2">In this case, we specify that words that appear in more than 40% of the documents should be ignored when constructing the vectorizer. </span><span class="kobospan" id="kobo.246.3">You should experiment and see exactly which value of </span><strong class="source-inline1"><span class="kobospan" id="kobo.247.1">max_df</span></strong><span class="kobospan" id="kobo.248.1"> would suit your use case. </span><span class="kobospan" id="kobo.248.2">We then fit the vectorizer on the </span><strong class="source-inline1"><span class="kobospan" id="kobo.249.1">text</span></strong><span class="kobospan" id="kobo.250.1"> column of the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.251.1">train_df</span></strong></span><span><span class="kobospan" id="kobo.252.1"> dataframe:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.253.1">
vectorizer = CountVectorizer(max_df=0.4)
X = vectorizer.fit_transform(train_df["text"])
print(X)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.254.1">The resulting matrix is a </span><strong class="source-inline"><span class="kobospan" id="kobo.255.1">scipy.sparse._csr.csr_matrix</span></strong><span class="kobospan" id="kobo.256.1"> object, and the beginning of its printout looks like this. </span><span class="kobospan" id="kobo.256.2">The format of a sparse matrix is </span><strong class="source-inline"><span class="kobospan" id="kobo.257.1">(row, column) value</span></strong><span class="kobospan" id="kobo.258.1">. </span><span class="kobospan" id="kobo.258.2">In our case, this means (the document index, word index) followed by the frequency. </span><span class="kobospan" id="kobo.258.3">In our example, the first review, which is the first document, is document number </span><strong class="source-inline"><span class="kobospan" id="kobo.259.1">0</span></strong><span class="kobospan" id="kobo.260.1">, and it contains words with indices </span><strong class="source-inline"><span class="kobospan" id="kobo.261.1">6578</span></strong><span class="kobospan" id="kobo.262.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.263.1">4219</span></strong><span class="kobospan" id="kobo.264.1">, and others. </span><span class="kobospan" id="kobo.264.2">The frequencies of these words are </span><strong class="source-inline"><span class="kobospan" id="kobo.265.1">1</span></strong><span class="kobospan" id="kobo.266.1"> and </span><span><strong class="source-inline"><span class="kobospan" id="kobo.267.1">2</span></strong></span><span><span class="kobospan" id="kobo.268.1">, respectively.</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.269.1">  (0, 6578)  1
  (0, 4219)  1
  (0, 2106)  1
  (0, 8000)  2
  (0, 717)  1
  (0, 42)  1
  (0, 1280)  1
  (0, 5260)  1
  (0, 1607)  1
  (0, 7889)  1
  (0, 3630)  1
…</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.270.1">In most cases, we use a different format to represent vectors, a dense matrix that is easier to use in practice. </span><span class="kobospan" id="kobo.270.2">Instead of specifying rows and columns with numbers, they are</span><a id="_idIndexMarker109" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.271.1"> inferred from the position of the value. </span><span class="kobospan" id="kobo.271.2">We will now create a dense matrix and </span><span><span class="kobospan" id="kobo.272.1">print it:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.273.1">
dense_matrix = X.todense()
print(dense_matrix)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.274.1">The resulting matrix is a NumPy matrix object, where each review is a vector. </span><span class="kobospan" id="kobo.274.2">You see that most values in the matrix are zeroes, as expected, since each review only uses a handful of words, while the vector collects counts for each word in the vocabulary, or each word in all of the reviews. </span><span class="kobospan" id="kobo.274.3">Any words that are not in the vectorizer’s vocabulary will not be included in </span><span><span class="kobospan" id="kobo.275.1">the vector:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.276.1">[[0 0 0 ... </span><span class="kobospan1" id="kobo.276.2">0 0 0]
 [0 0 0 ... </span><span class="kobospan1" id="kobo.276.3">0 0 0]
 [0 0 0 ... </span><span class="kobospan1" id="kobo.276.4">0 0 0]
 ...
 </span><span class="kobospan1" id="kobo.276.5">[0 0 0 ... </span><span class="kobospan1" id="kobo.276.6">0 0 0]
 [0 0 0 ... </span><span class="kobospan1" id="kobo.276.7">0 0 0]
 [0 0 0 ... </span><span class="kobospan1" id="kobo.276.8">0 0 0]]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.277.1">We can see all the words used in the document set and the length of the vocabulary. </span><span class="kobospan" id="kobo.277.2">This can </span><a id="_idIndexMarker110" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.278.1">be used as a sanity check and to see whether there are any irregularities in </span><span><span class="kobospan" id="kobo.279.1">the vocabulary:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.280.1">
print(vectorizer.get_feature_names_out())
print(len(vectorizer.get_feature_names_out()))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.281.1">The result will be as follows. </span><span class="kobospan" id="kobo.281.2">If you want to see the full, non-truncated list, use the </span><strong class="source-inline"><span class="kobospan" id="kobo.282.1">set_printoptions</span></strong><span class="kobospan" id="kobo.283.1"> function used in </span><span><em class="italic"><span class="kobospan" id="kobo.284.1">step 8</span></em></span><span><span class="kobospan" id="kobo.285.1">:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.286.1">['10' '100' '101' ... </span><span class="kobospan1" id="kobo.286.2">'zone' 'ótimo' 'últimos']
8856</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.287.1">We can also see all the stop words used by </span><span><span class="kobospan" id="kobo.288.1">the vectorizer:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.289.1">
print(vectorizer.stop_words_)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.290.1">The result is three words, </span><strong class="source-inline"><span class="kobospan" id="kobo.291.1">and</span></strong><span class="kobospan" id="kobo.292.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.293.1">the</span></strong><span class="kobospan" id="kobo.294.1">, and </span><strong class="source-inline"><span class="kobospan" id="kobo.295.1">of</span></strong><span class="kobospan" id="kobo.296.1">, that appear in more than 40% </span><span><span class="kobospan" id="kobo.297.1">of reviews:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.298.1">{'and', 'the', 'of'}</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.299.1">We can now also use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.300.1">CountVectorizer</span></strong><span class="kobospan" id="kobo.301.1"> object to represent new reviews that were not in the original document set. </span><span class="kobospan" id="kobo.301.2">This is done when we have a trained model and want to test it on new, unseen samples. </span><span class="kobospan" id="kobo.301.3">We will use the first review in the test dataset. </span><span class="kobospan" id="kobo.301.4">To get the first review in the test set, we will use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.302.1">pandas</span></strong> <span><strong class="source-inline1"><span class="kobospan" id="kobo.303.1">iat</span></strong></span><span><span class="kobospan" id="kobo.304.1"> function.</span></span><pre class="source-code"><span class="kobospan1" id="kobo.305.1">
first_review = test_df['text'].iat[0]
print(first_review)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.306.1">The first review looks </span><span><span class="kobospan" id="kobo.307.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.308.1">lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.309.1">Now, we will create both a sparse and a dense vector from the first review. </span><span class="kobospan" id="kobo.309.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.310.1">transform</span></strong><span class="kobospan" id="kobo.311.1"> method of the</span><a id="_idIndexMarker111" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.312.1"> vectorizer expects a list of strings, so we will create a list. </span><span class="kobospan" id="kobo.312.2">We also set the </span><strong class="source-inline1"><span class="kobospan" id="kobo.313.1">print</span></strong><span class="kobospan" id="kobo.314.1"> option to print out the whole vector instead of just part </span><span><span class="kobospan" id="kobo.315.1">of it:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.316.1">
sparse_vector = vectorizer.transform([first_review])
print(sparse_vector)
dense_vector = sparse_vector.todense()
np.set_printoptions(threshold=sys.maxsize)
print(dense_vector)
np.set_printoptions(threshold=False)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.317.1">The dense vector is very long and is mostly zeroes, </span><span><span class="kobospan" id="kobo.318.1">as expected:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.319.1">  (0, 955)  1
  (0, 3968)  1
  (0, 4451)  1
  (0, 4562)  1
  (0, 4622)  1
  (0, 4688)  1
  (0, 4779)  1
  (0, 4792)  1
  (0, 5764)  1
  (0, 7547)  1
  (0, 7715)  1
  (0, 8000)  1
  (0, 8734)  1
[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
…]]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.320.1">We can use a different method to calculate stop words. </span><span class="kobospan" id="kobo.320.2">Here, stop words are calculated by setting </span><a id="_idIndexMarker112" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.321.1">an absolute threshold on the word frequency. </span><span class="kobospan" id="kobo.321.2">In this case, we use all words whose frequency is lower than 300 across documents. </span><span class="kobospan" id="kobo.321.3">You can see that the stop-word list is </span><span><span class="kobospan" id="kobo.322.1">now larger.</span></span><pre class="source-code"><span class="kobospan1" id="kobo.323.1">
vectorizer = CountVectorizer(max_df=300)
X = vectorizer.fit_transform(train_df["text"])
print(vectorizer.stop_words_)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.324.1">The result will be </span><span><span class="kobospan" id="kobo.325.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.326.1">{'but', 'this', 'its', 'as', 'to', 'and', 'the', 'is', 'film', 'for', 'it', 'an', 'of', 'that', 'movie', 'with', 'in'}</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.327.1">Finally, we can provide our own list of stop words to the vectorizer. </span><span class="kobospan" id="kobo.327.2">These words will be ignored by the vectorizer and not represented in the vector. </span><span class="kobospan" id="kobo.327.3">This is useful if you have very specific words you would like </span><span><span class="kobospan" id="kobo.328.1">to ignore:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.329.1">
vectorizer = CountVectorizer(stop_words=['the', 'this',
    'these', 'in', 'at', 'for'])
X = vectorizer.fit_transform(train_df["text"])</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.330.1">We will now test the effect of this bag-of-words vectorizer on the simple classifier, using the functions we defined in the previous recipe. </span><span class="kobospan" id="kobo.330.2">First, we create the vectorizer, specifying to use only words that appear in less than 80% of the documents. </span><span class="kobospan" id="kobo.330.3">Then, we</span><a id="_idIndexMarker113" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.331.1"> load the training and test dataframes. </span><span class="kobospan" id="kobo.331.2">We fit the vectorizer on the training set reviews. </span><span class="kobospan" id="kobo.331.3">We create a vectorize function using the vectorizer and pass it on to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.332.1">create_train_test_data</span></strong><span class="kobospan" id="kobo.333.1"> function, along with the training and test data frames. </span><span class="kobospan" id="kobo.333.2">We then train the classifier and test it on the testing data. </span><span class="kobospan" id="kobo.333.3">We can see that this vectorizing method gives us much better results than the simple part-of-speech count vector we used in the </span><span><span class="kobospan" id="kobo.334.1">previous section:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.335.1">
vectorizer = CountVectorizer(max_df=0.8)
(train_df, test_df) = load_train_test_dataset_pd()
X = vectorizer.fit_transform(train_df["text"])
vectorize = lambda x: vectorizer.transform([x]).toarray()[0]
(X_train, X_test, y_train, y_test) = create_train_test_data(
    train_df, test_df, vectorize)
clf = train_classifier(X_train, y_train)
test_classifier(test_df, clf)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.336.1">The result will be similar </span><span><span class="kobospan" id="kobo.337.1">to this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.338.1">              precision    recall  f1-score   support
           0       0.74      0.72      0.73       160
           1       0.73      0.75      0.74       160
    accuracy                           0.74       320
   macro avg       0.74      0.74      0.74       320
weighted avg       0.74      0.74      0.74       320</span></pre></li>			</ol>
			<h1 id="_idParaDest-76" class="calibre7"><a id="_idTextAnchor078" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.339.1">Constructing an N-gram model</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.340.1">Representing a document as a bag of words is useful, but semantics is about more than just words in isolation. </span><span class="kobospan" id="kobo.340.2">To </span><a id="_idIndexMarker114" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.341.1">capture word combinations, an </span><strong class="bold"><span class="kobospan" id="kobo.342.1">n-gram model</span></strong><span class="kobospan" id="kobo.343.1"> is useful. </span><span class="kobospan" id="kobo.343.2">Its vocabulary consists of not just words but also word sequences, </span><span><span class="kobospan" id="kobo.344.1">or </span></span><span><em class="italic"><span class="kobospan" id="kobo.345.1">n</span></em></span><span><span class="kobospan" id="kobo.346.1">-grams.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.347.1">We will build a </span><strong class="bold"><span class="kobospan" id="kobo.348.1">bigram model</span></strong><span class="kobospan" id="kobo.349.1"> in this recipe, where</span><a id="_idIndexMarker115" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.350.1"> bigrams are sequences of </span><span><span class="kobospan" id="kobo.351.1">two words.</span></span></p>
			<h2 id="_idParaDest-77" class="calibre5"><a id="_idTextAnchor079" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.352.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.353.1">The </span><strong class="source-inline"><span class="kobospan" id="kobo.354.1">CountVectorizer</span></strong><span class="kobospan" id="kobo.355.1"> class is very versatile and allows us to construct </span><em class="italic"><span class="kobospan" id="kobo.356.1">n</span></em><span class="kobospan" id="kobo.357.1">-gram models. </span><span class="kobospan" id="kobo.357.2">We will use it in this recipe and test it with a </span><span><span class="kobospan" id="kobo.358.1">simple classifier.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.359.1">In this recipe, I make comparisons of the code and its results to the ones in the </span><em class="italic"><span class="kobospan" id="kobo.360.1">Putting documents into a bag of words</span></em><span class="kobospan" id="kobo.361.1"> recipe, since the two are very similar, but they have a few </span><span><span class="kobospan" id="kobo.362.1">differing characteristics.</span></span></p>
			<h2 id="_idParaDest-78" class="calibre5"><a id="_idTextAnchor080" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.363.1">How to do it…</span></h2>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.364.1">Run the simple classifier notebook and import the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.365.1">CountVectorizer</span></strong></span><span><span class="kobospan" id="kobo.366.1"> class:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.367.1">
%run -i "../util/util_simple_classifier.ipynb"
from sklearn.feature_extraction.text import CountVectorizer</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.368.1">Create the training and test dataframes using code from the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.369.1">util_simple_classifier.ipynb</span></strong></span><span><span class="kobospan" id="kobo.370.1"> notebook:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.371.1">
(train_df, test_df) = load_train_test_dataset_pd()</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.372.1">Create a new vectorizer class. </span><span class="kobospan" id="kobo.372.2">In this case, we will use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.373.1">ngram_range</span></strong><span class="kobospan" id="kobo.374.1"> argument. </span><span class="kobospan" id="kobo.374.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.375.1">CountVectorizer</span></strong><span class="kobospan" id="kobo.376.1"> class, when the </span><strong class="source-inline1"><span class="kobospan" id="kobo.377.1">ngram_range</span></strong><span class="kobospan" id="kobo.378.1"> argument is set, counts not only individual words but also word combinations, where the number of words in the combinations depends on the numbers provided to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.379.1">ngram_range</span></strong><span class="kobospan" id="kobo.380.1"> argument. </span><span class="kobospan" id="kobo.380.2">We provided </span><strong class="source-inline1"><span class="kobospan" id="kobo.381.1">ngram_range=(1,2)</span></strong><span class="kobospan" id="kobo.382.1"> as the argument, which means that the number of words in the combinations ranges from 1 to 2, so unigrams</span><a id="_idIndexMarker116" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.383.1"> and bigrams </span><span><span class="kobospan" id="kobo.384.1">are counted:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.385.1">
bigram_vectorizer = CountVectorizer(
    ngram_range=(1, 2), max_df=0.8)
X = bigram_vectorizer.fit_transform(train_df["text"])</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.386.1">Print the vocabulary of the vectorizer and its length. </span><span class="kobospan" id="kobo.386.2">As you can see, the length of the vocabulary is much larger than the length of the unigram vectorizer, since we use two-word combinations in addition to </span><span><span class="kobospan" id="kobo.387.1">single words:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.388.1">
print(bigram_vectorizer.get_feature_names_out())
print(len(bigram_vectorizer.get_feature_names_out()))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.389.1">The result should look </span><span><span class="kobospan" id="kobo.390.1">like this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.391.1">['10' '10 inch' '10 set' ... </span><span class="kobospan1" id="kobo.391.2">'ótimo esforço' 'últimos' 'últimos tiempos']
40552</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.392.1">Now, we take the first review in the testing dataframe and get its dense vector. </span><span class="kobospan" id="kobo.392.2">The result looks very similar to the vector output in the </span><em class="italic"><span class="kobospan" id="kobo.393.1">Putting documents into a bag of words</span></em><span class="kobospan" id="kobo.394.1"> recipe, with the only difference that now the output is longer, as it includes not just individual words but also bigrams, or sequences of </span><span><span class="kobospan" id="kobo.395.1">two words:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.396.1">
first_review = test_df['text'].iat[0]
dense_vector = bigram_vectorizer.transform(
    [first_review]).todense()
print(dense_vector)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.397.1">The printout looks </span><span><span class="kobospan" id="kobo.398.1">like this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.399.1">[[0 0 0 ... </span><span class="kobospan1" id="kobo.399.2">0 0 0]]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.400.1">Finally, we train a simple classifier using the new bigram vectorizer. </span><span class="kobospan" id="kobo.400.2">The resulting accuracy is slightly worse than the accuracy of the classifier that uses a unigram vectorizer from the previous section. </span><span class="kobospan" id="kobo.400.3">There are several possible reasons for this. </span><span class="kobospan" id="kobo.400.4">One is that the vectors</span><a id="_idIndexMarker117" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.401.1"> are now much longer and still mostly zeroes. </span><span class="kobospan" id="kobo.401.2">The other is that we can actually see that not all reviews are in English, so it is hard for the classifier to generalize the </span><span><span class="kobospan" id="kobo.402.1">incoming data:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.403.1">
vectorize = \
    lambda x: bigram_vectorizer.transform([x]).toarray()[0]
(X_train, X_test, y_train, y_test) = create_train_test_data(
    train_df, test_df, vectorize)
clf = train_classifier(X_train, y_train)
test_classifier(test_df, clf)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.404.1">The output will be </span><span><span class="kobospan" id="kobo.405.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.406.1">              precision    recall  f1-score   support
           0       0.72      0.75      0.73       160
           1       0.74      0.71      0.72       160
    accuracy                           0.73       320
   macro avg       0.73      0.73      0.73       320
weighted avg       0.73      0.73      0.73       320</span></pre></li>			</ol>
			<h2 id="_idParaDest-79" class="calibre5"><a id="_idTextAnchor081" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.407.1">There’s more…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.408.1">We can use trigrams, quadrigrams, and so on in the vectorizer by providing the corresponding tuple to the </span><strong class="source-inline"><span class="kobospan" id="kobo.409.1">ngram_range</span></strong><span class="kobospan" id="kobo.410.1"> argument. </span><span class="kobospan" id="kobo.410.2">The downside of this is the ever-expanding vocabulary and the growth of sentence vectors, since each sentence vector has to have an entry for each word in the </span><span><span class="kobospan" id="kobo.411.1">input vocabulary.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.412.1">It is also possible to</span><a id="_idIndexMarker118" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.413.1"> represent character </span><em class="italic"><span class="kobospan" id="kobo.414.1">n</span></em><span class="kobospan" id="kobo.415.1">-grams using the </span><strong class="source-inline"><span class="kobospan" id="kobo.416.1">CountVectorizer</span></strong><span class="kobospan" id="kobo.417.1"> class. </span><span class="kobospan" id="kobo.417.2">In this case, you would count the occurrence of character sequences instead of </span><span><span class="kobospan" id="kobo.418.1">word sequences.</span></span></p>
			<h1 id="_idParaDest-80" class="calibre7"><a id="_idTextAnchor082" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.419.1">Representing texts with TF-IDF</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.420.1">We can go one step further and use the TF-IDF algorithm to count words and </span><em class="italic"><span class="kobospan" id="kobo.421.1">n</span></em><span class="kobospan" id="kobo.422.1">-grams in incoming documents. </span><strong class="bold"><span class="kobospan" id="kobo.423.1">TF-IDF</span></strong><span class="kobospan" id="kobo.424.1"> stands for </span><strong class="bold"><span class="kobospan" id="kobo.425.1">term frequency-inverse document frequency</span></strong><span class="kobospan" id="kobo.426.1"> and gives more weight to words that are unique to a document than to words that are frequent but repeated</span><a id="_idIndexMarker119" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.427.1"> throughout most documents. </span><span class="kobospan" id="kobo.427.2">This allows us to give more weight to words uniquely characteristic </span><a id="_idIndexMarker120" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.428.1">of </span><span><span class="kobospan" id="kobo.429.1">particular documents.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.430.1">In this recipe, we will use a different type of vectorizer that can apply the TF-IDF algorithm to the input text and build a </span><span><span class="kobospan" id="kobo.431.1">small classifier.</span></span></p>
			<h2 id="_idParaDest-81" class="calibre5"><a id="_idTextAnchor083" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.432.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.433.1">We will use the </span><strong class="source-inline"><span class="kobospan" id="kobo.434.1">TfidfVectorizer</span></strong><span class="kobospan" id="kobo.435.1"> class from the </span><strong class="source-inline"><span class="kobospan" id="kobo.436.1">sklearn</span></strong><span class="kobospan" id="kobo.437.1"> package. </span><span class="kobospan" id="kobo.437.2">The features of the </span><strong class="source-inline"><span class="kobospan" id="kobo.438.1">TfidfVectorizer</span></strong><span class="kobospan" id="kobo.439.1"> class should be familiar from the two previous recipes, </span><em class="italic"><span class="kobospan" id="kobo.440.1">Putting documents into a bag of words</span></em><span class="kobospan" id="kobo.441.1"> and </span><em class="italic"><span class="kobospan" id="kobo.442.1">Constructing an N-gram model</span></em><span class="kobospan" id="kobo.443.1">. </span><span class="kobospan" id="kobo.443.2">We will again use the Rotten Tomatoes review dataset from </span><span><span class="kobospan" id="kobo.444.1">Hugging Face.</span></span></p>
			<h2 id="_idParaDest-82" class="calibre5"><a id="_idTextAnchor084" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.445.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.446.1">Here are the steps to build and use the </span><span><span class="kobospan" id="kobo.447.1">TF-IDF vectorizer:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.448.1">Run the small classifier notebook and import the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.449.1">TfidfVectorizer</span></strong></span><span><span class="kobospan" id="kobo.450.1"> class:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.451.1">
%run -i "../util/util_simple_classifier.ipynb"
from sklearn.feature_extraction.text import TfidfVectorizer</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.452.1">Create the training and test dataframes using the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.453.1">load_train_test_dataset_pd()</span></strong></span><span><span class="kobospan" id="kobo.454.1"> function:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.455.1">
(train_df, test_df) = load_train_test_dataset_pd()</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.456.1">Create the vectorizer </span><a id="_idIndexMarker121" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.457.1">and fit it on the training text. </span><span class="kobospan" id="kobo.457.2">We will use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.458.1">max_df</span></strong><span class="kobospan" id="kobo.459.1"> parameter to </span><a id="_idIndexMarker122" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.460.1">exclude stop words – in this case, words that are more frequent </span><span><span class="kobospan" id="kobo.461.1">than 300:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.462.1">
vectorizer = TfidfVectorizer(max_df=300)
vectorizer.fit(train_df["text"])</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.463.1">To make sure the result makes sense, we will print the vectorizer vocabulary and its length. </span><span class="kobospan" id="kobo.463.2">Since we are just using unigrams, the size of the vocabulary should be the same as the one in the </span><span><span class="kobospan" id="kobo.464.1">bag-of-words recipe:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.465.1">
print(vectorizer.get_feature_names_out())
print(len(vectorizer.get_feature_names_out()))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.466.1">The result should be as follows. </span><span class="kobospan" id="kobo.466.2">The length of the vocabulary should be the same as the one we get in the bag-of-words recipe, since we are not </span><span><span class="kobospan" id="kobo.467.1">using </span></span><span><em class="italic"><span class="kobospan" id="kobo.468.1">n</span></em></span><span><span class="kobospan" id="kobo.469.1">-grams:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.470.1">['10' '100' '101' ... </span><span class="kobospan1" id="kobo.470.2">'zone' 'ótimo' 'últimos']
8842</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.471.1">Now, let’s take the first review in the test dataframe and vectorizer it. </span><span class="kobospan" id="kobo.471.2">We then print the dense vector. </span><span class="kobospan" id="kobo.471.3">To learn more about the difference between sparse and dense vectors, see the </span><em class="italic"><span class="kobospan" id="kobo.472.1">Putting documents into a bag of words</span></em><span class="kobospan" id="kobo.473.1"> recipe. </span><span class="kobospan" id="kobo.473.2">Note that the values in the vector are now floats and not integers. </span><span class="kobospan" id="kobo.473.3">This is because the individual values are now ratios and </span><span><span class="kobospan" id="kobo.474.1">not counts:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.475.1">
first_review = test_df['text'].iat[0]
dense_vector = vectorizer.transform([first_review]).todense()
print(dense_vector)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.476.1">The result should be </span><span><span class="kobospan" id="kobo.477.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.478.1">[[0. </span><span class="kobospan1" id="kobo.478.2">0. </span><span class="kobospan1" id="kobo.478.3">0. </span><span class="kobospan1" id="kobo.478.4">... </span><span class="kobospan1" id="kobo.478.5">0. </span><span class="kobospan1" id="kobo.478.6">0. </span><span class="kobospan1" id="kobo.478.7">0.]]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.479.1">Now, let’s train the classifier. </span><span class="kobospan" id="kobo.479.2">We</span><a id="_idIndexMarker123" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.480.1"> can see that the scores are slightly </span><a id="_idIndexMarker124" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.481.1">higher than those for a bag-of-words classifier, both the unigram and </span><span><em class="italic"><span class="kobospan" id="kobo.482.1">n</span></em></span><span><span class="kobospan" id="kobo.483.1">-gram versions:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.484.1">
vectorize = lambda x: vectorizer.transform([x]).toarray()[0]
(X_train, X_test, y_train, y_test) = create_train_test_data(
    train_df, test_df, vectorize)
clf = train_classifier(X_train, y_train)
test_classifier(test_df, clf)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.485.1">The printout of the test scores will look similar </span><span><span class="kobospan" id="kobo.486.1">to this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.487.1">              precision    recall  f1-score   support
           0       0.76      0.72      0.74       160
           1       0.74      0.78      0.76       160
    accuracy                           0.75       320
   macro avg       0.75      0.75      0.75       320
weighted avg       0.75      0.75      0.75       320</span></pre></li>			</ol>
			<h2 id="_idParaDest-83" class="calibre5"><a id="_idTextAnchor085" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.488.1">How it works…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.489.1">The </span><strong class="source-inline"><span class="kobospan" id="kobo.490.1">TfidfVectorizer</span></strong><span class="kobospan" id="kobo.491.1"> class works almost exactly like the </span><strong class="source-inline"><span class="kobospan" id="kobo.492.1">CountVectorizer</span></strong><span class="kobospan" id="kobo.493.1"> class, differing only in </span><a id="_idIndexMarker125" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.494.1">the way the </span><strong class="bold"><span class="kobospan" id="kobo.495.1">word frequencies</span></strong><span class="kobospan" id="kobo.496.1"> are calculated, so most of the steps should be familiar. </span><span class="kobospan" id="kobo.496.2">Word frequencies are calculated as follows. </span><span class="kobospan" id="kobo.496.3">For each word, the overall frequency</span><a id="_idIndexMarker126" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.497.1"> is a product of the </span><strong class="bold"><span class="kobospan" id="kobo.498.1">term frequency</span></strong><span class="kobospan" id="kobo.499.1"> and the </span><strong class="bold"><span class="kobospan" id="kobo.500.1">inverse document frequency</span></strong><span class="kobospan" id="kobo.501.1">. </span><span class="kobospan" id="kobo.501.2">Term frequency is the number of times a word occurs in the </span><a id="_idIndexMarker127" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.502.1">document. </span><span class="kobospan" id="kobo.502.2">Inverse document frequency is the total number of documents divided by the number of documents where the word</span><a id="_idIndexMarker128" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.503.1"> occurs. </span><span class="kobospan" id="kobo.503.2">Usually, these frequencies are</span><a id="_idIndexMarker129" class="calibre6 pcalibre pcalibre1"/> <span><span class="kobospan" id="kobo.504.1">logarithmically scaled.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.505.1">This is done using the </span><span><span class="kobospan" id="kobo.506.1">following formulas:</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.507.1"><img src="image/1.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" class="calibre16"/></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.508.1"><img src="image/2.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" class="calibre17"/></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.509.1"><img src="image/3.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;I&lt;/mml:mi&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;mml:mi&gt;*&lt;/mml:mi&gt;&lt;mml:mi&gt;I&lt;/mml:mi&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:math&gt;" class="calibre18"/></span></p>
			<h2 id="_idParaDest-84" class="calibre5"><a id="_idTextAnchor086" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.510.1">There’s more…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.511.1">We can build </span><strong class="source-inline"><span class="kobospan" id="kobo.512.1">TfidfVectorizer</span></strong><span class="kobospan" id="kobo.513.1"> and use </span><strong class="bold"><span class="kobospan" id="kobo.514.1">character n-grams</span></strong><span class="kobospan" id="kobo.515.1"> instead of </span><strong class="bold"><span class="kobospan" id="kobo.516.1">word n-grams</span></strong><span class="kobospan" id="kobo.517.1">. </span><span class="kobospan" id="kobo.517.2">Character n-grams use the</span><a id="_idIndexMarker130" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.518.1"> character, not the word, as their basic unit. </span><span class="kobospan" id="kobo.518.2">For </span><a id="_idIndexMarker131" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.519.1">example, if we were to build character </span><em class="italic"><span class="kobospan" id="kobo.520.1">n</span></em><span class="kobospan" id="kobo.521.1">-grams for the phrase </span><em class="italic"><span class="kobospan" id="kobo.522.1">the woman</span></em><span class="kobospan" id="kobo.523.1"> with the </span><em class="italic"><span class="kobospan" id="kobo.524.1">n</span></em><span class="kobospan" id="kobo.525.1">-gram range (1, 3), it would be the </span><strong class="source-inline"><span class="kobospan" id="kobo.526.1">[t, h, e, w, o, m, a, n, th, he, wo, om, ma, an, the, wom, oma, man]</span></strong><span class="kobospan" id="kobo.527.1"> set. </span><span class="kobospan" id="kobo.527.2">In some experimental settings, models based on character </span><em class="italic"><span class="kobospan" id="kobo.528.1">n</span></em><span class="kobospan" id="kobo.529.1">-grams perform better than word-based </span><span><em class="italic"><span class="kobospan" id="kobo.530.1">n</span></em></span><span><span class="kobospan" id="kobo.531.1">-gram models.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.532.1">We will use the small Sherlock Holmes text file, </span><strong class="source-inline"><span class="kobospan" id="kobo.533.1">sherlock_holmes_1.txt</span></strong><span class="kobospan" id="kobo.534.1">, found at </span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.535.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt</span></a><span class="kobospan" id="kobo.536.1">, and the same class, </span><strong class="source-inline"><span class="kobospan" id="kobo.537.1">TfidfVectorizer</span></strong><span class="kobospan" id="kobo.538.1">. </span><span class="kobospan" id="kobo.538.2">We will not need a tokenizer function or a stop-word list, since the unit of analysis is the character and not the word. </span><span class="kobospan" id="kobo.538.3">The steps to create the vectorizer and analyze a sentence are </span><span><span class="kobospan" id="kobo.539.1">as follows:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.540.1">Create a new vectorizer object that uses the </span><strong class="source-inline1"><span class="kobospan" id="kobo.541.1">char_wb</span></strong><span class="kobospan" id="kobo.542.1"> analyzer, and then fit it on the </span><span><span class="kobospan" id="kobo.543.1">training text:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.544.1">
tfidf_char_vectorizer = TfidfVectorizer(
    analyzer='char_wb', ngram_range=(1,5))
tfidf_char_vectorizer = tfidf_char_vectorizer.fit(
    train_df["text"])</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.545.1">Print the vectorizer vocabulary and </span><span><span class="kobospan" id="kobo.546.1">its length:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.547.1">
print(list(tfidf_char_vectorizer.get_feature_names_out()))
print(len(tfidf_char_vectorizer.get_feature_names_out()))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.548.1">The partial result </span><a id="_idIndexMarker132" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.549.1">will look </span><span><span class="kobospan" id="kobo.550.1">like this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.551.1">[' ', ' !', ' ! </span><span class="kobospan1" id="kobo.551.2">', ' "', ' " ', ' $', ' $5', ' $50', ' $50-', ' $9', ' $9 ', ' &amp;', ' &amp; ', " '", " ' ", " '5", " '50", " '50'", " '6", " '60", " '60s", " '7", " '70", " '70'", " '70s", " '[", " '[h", " '[ho", " 'a", " 'a ", " 'a'", " 'a' ", " 'ab", " 'aba", " 'ah", " 'ah ", " 'al", " 'alt", " 'an", " 'ana", " 'ar", " 'are", " 'b", " 'ba", " 'bar", " 'be", " 'bee", " 'bes", " 'bl", " 'blu", " 'br", " 'bra", " 'bu", " 'but", " 'c", " 'ch", " 'cha", " 'co", " 'co-", " 'com", " 'd", " 'di", " 'dif", " 'do", " 'dog", " 'du", " 'dum", " 'e", " 'ed", " 'edg", " 'em", " 'em ", " 'ep", " 'epi", " 'f", " 'fa", " 'fac", " 'fat", " 'fu", " 'fun", " 'g", " 'ga", " 'gar", " 'gi", " 'gir", " 'gr", " 'gra", " 'gu", " 'gue", " 'guy", " 'h", " 'ha", " 'hav", " 'ho", " 'hos", " 'how", " 'i", " 'i ", " 'if", " 'if ", " 'in", " 'in ", " 'is",…]
51270</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.552.1">Create the </span><strong class="source-inline1"><span class="kobospan" id="kobo.553.1">vectorize</span></strong><span class="kobospan" id="kobo.554.1"> method using the new vectorizer, and then create the training and test data. </span><span class="kobospan" id="kobo.554.2">Train the </span><a id="_idIndexMarker133" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.555.1">classifier and then </span><span><span class="kobospan" id="kobo.556.1">test it:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.557.1">
vectorize = lambda x: tfidf_char_vectorizer.transform([
    x]).toarray()[0]
(X_train, X_test, y_train, y_test) = create_train_test_data(
    train_df, test_df, vectorize)
clf = train_classifier(X_train, y_train)
test_classifier(test_df, clf)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.558.1">The result will be similar </span><span><span class="kobospan" id="kobo.559.1">to this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.560.1">              precision    recall  f1-score   support
           0       0.74      0.74      0.74       160
           1       0.74      0.74      0.74       160
    accuracy                           0.74       320
   macro avg       0.74      0.74      0.74       320
weighted avg       0.74      0.74      0.74       320</span></pre></li>			</ol>
			<h2 id="_idParaDest-85" class="calibre5"><a id="_idTextAnchor087" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.561.1">See also</span></h2>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.562.1">You can find out more </span><a id="_idIndexMarker134" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.563.1">about term weighting </span><span><span class="kobospan" id="kobo.564.1">at </span></span><a href="https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.565.1">https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting</span></span></a></li>
				<li class="calibre14"><span class="kobospan" id="kobo.566.1">For more information</span><a id="_idIndexMarker135" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.567.1"> about </span><strong class="source-inline1"><span class="kobospan" id="kobo.568.1">TfidfVectorizer</span></strong><span class="kobospan" id="kobo.569.1">, </span><span><span class="kobospan" id="kobo.570.1">see </span></span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.571.1">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</span></span></a></li>
			</ul>
			<h1 id="_idParaDest-86" class="calibre7"><a id="_idTextAnchor088" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.572.1">Using word embeddings</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.573.1">In this recipe, we will switch gears and learn how to represent </span><em class="italic"><span class="kobospan" id="kobo.574.1">words</span></em><span class="kobospan" id="kobo.575.1"> using word embeddings, which are powerful because they are a result of training a neural network that predicts a word from all other</span><a id="_idIndexMarker136" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.576.1"> words in the sentence. </span><span class="kobospan" id="kobo.576.2">Embeddings are also vectors, but usually of a much smaller size, 200 or 300. </span><span class="kobospan" id="kobo.576.3">The resulting vector embeddings are similar for words that occur in similar contexts. </span><span class="kobospan" id="kobo.576.4">Similarity is usually measured by calculating the cosine of the angle between two vectors in the hyperplane, with 200 or 300 dimensions. </span><span class="kobospan" id="kobo.576.5">We will use the embeddings to show </span><span><span class="kobospan" id="kobo.577.1">these similarities.</span></span></p>
			<h2 id="_idParaDest-87" class="calibre5"><a id="_idTextAnchor089" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.578.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.579.1">In this recipe, we will use a pretrained </span><strong class="source-inline"><span class="kobospan" id="kobo.580.1">word2vec</span></strong><span class="kobospan" id="kobo.581.1"> model, which can be found at </span><a href="https://github.com/mmihaltz/word2vec-GoogleNews-vectors" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.582.1">https://github.com/mmihaltz/word2vec-GoogleNews-vectors</span></a><span class="kobospan" id="kobo.583.1">. </span><span class="kobospan" id="kobo.583.2">Download the model and unzip it in the data directory. </span><span class="kobospan" id="kobo.583.3">You should now have a file with the </span><strong class="source-inline"><span class="kobospan" id="kobo.584.1">…/</span></strong><span><strong class="source-inline"><span class="kobospan" id="kobo.585.1">data/GoogleNews-vectors-negative300.bin.gz</span></strong></span><span><span class="kobospan" id="kobo.586.1"> path.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.587.1">We will also use the </span><strong class="source-inline"><span class="kobospan" id="kobo.588.1">gensim</span></strong><span class="kobospan" id="kobo.589.1"> package to load and use the model. </span><span class="kobospan" id="kobo.589.2">It should be installed within the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.590.1">poetry</span></strong></span><span><span class="kobospan" id="kobo.591.1"> environment.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.592.1">The notebook is located </span><span><span class="kobospan" id="kobo.593.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.5_word_embeddings.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.594.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.5_word_embeddings.ipynb</span></span></a><span><span class="kobospan" id="kobo.595.1">.</span></span></p>
			<h2 id="_idParaDest-88" class="calibre5"><a id="_idTextAnchor090" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.596.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.597.1">We will load the model, demonstrate </span><a id="_idIndexMarker137" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.598.1">some features of the </span><strong class="source-inline"><span class="kobospan" id="kobo.599.1">gensim</span></strong><span class="kobospan" id="kobo.600.1"> package, and then compute a sentence vector using the </span><span><span class="kobospan" id="kobo.601.1">word embeddings:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.602.1">Run the simple </span><span><span class="kobospan" id="kobo.603.1">classifier file:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.604.1">
%run -i "../util/simple_classifier.ipynb"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.605.1">Import the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.606.1">gensim</span></strong></span><span><span class="kobospan" id="kobo.607.1"> package:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.608.1">
import gensim</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.609.1">Load the pretrained model. </span><span class="kobospan" id="kobo.609.2">If you get an error at this step, make sure you have downloaded the model in the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.610.1">data</span></strong></span><span><span class="kobospan" id="kobo.611.1"> directory:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.612.1">
model = gensim.models.KeyedVectors.load_word2vec_format(
    '../data/GoogleNews-vectors-negative300.bin.gz',
    binary=True)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.613.1">Using the pretrained model, we can now load individual word vectors. </span><span class="kobospan" id="kobo.613.2">Here, we load the word vector for the word </span><em class="italic"><span class="kobospan" id="kobo.614.1">king</span></em><span class="kobospan" id="kobo.615.1">. </span><span class="kobospan" id="kobo.615.2">We have to lowercase it, since all the words in the model are lowercase. </span><span class="kobospan" id="kobo.615.3">The result is a long vector that represents this word in the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.616.1">word2vec</span></strong></span><span><span class="kobospan" id="kobo.617.1"> model:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.618.1">
vec_king = model['king']
print(vec_king)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.619.1">The result will be </span><span><span class="kobospan" id="kobo.620.1">as</span></span><span><a id="_idIndexMarker138" class="calibre6 pcalibre pcalibre1"/></span><span><span class="kobospan" id="kobo.621.1"> follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.622.1">[ 1.25976562e-01  2.97851562e-02  8.60595703e-03  1.39648438e-01
 -2.56347656e-02 -3.61328125e-02  1.11816406e-01 -1.98242188e-01
  5.12695312e-02  3.63281250e-01 -2.42187500e-01 -3.02734375e-01
 -1.77734375e-01 -2.49023438e-02 -1.67968750e-01 -1.69921875e-01
  3.46679688e-02  5.21850586e-03  4.63867188e-02  1.28906250e-01
  1.36718750e-01  1.12792969e-01  5.95703125e-02  1.36718750e-01
  1.01074219e-01 -1.76757812e-01 -2.51953125e-01  5.98144531e-02
  3.41796875e-01 -3.11279297e-02  1.04492188e-01  6.17675781e-02  …]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.623.1">We can also get words that are most similar to a given word. </span><span class="kobospan" id="kobo.623.2">For example, let’s print out the words most similar to </span><em class="italic"><span class="kobospan" id="kobo.624.1">apple</span></em><span class="kobospan" id="kobo.625.1"> and </span><em class="italic"><span class="kobospan" id="kobo.626.1">tomato</span></em><span class="kobospan" id="kobo.627.1">. </span><span class="kobospan" id="kobo.627.2">The output prints out the words that are the most similar (i.e., occur in similar contexts) and the similarity score. </span><span class="kobospan" id="kobo.627.3">The score is the cosine distance between a pair of vectors – in this case, representing a pair of words. </span><span class="kobospan" id="kobo.627.4">The larger the score, the more similar the two words. </span><span class="kobospan" id="kobo.627.5">The results make sense, since the words most similar to </span><em class="italic"><span class="kobospan" id="kobo.628.1">apple</span></em><span class="kobospan" id="kobo.629.1"> are mostly fruits, and the words most similar to </span><em class="italic"><span class="kobospan" id="kobo.630.1">tomato</span></em><span class="kobospan" id="kobo.631.1"> are </span><span><span class="kobospan" id="kobo.632.1">mostly vegetables:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.633.1">
print(model.most_similar(['apple'], topn=15))
print(model.most_similar(['tomato'], topn=15))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.634.1">The result is </span><span><span class="kobospan" id="kobo.635.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.636.1">[('apples', 0.720359742641449), ('pear', 0.6450697183609009), ('fruit', 0.6410146355628967), ('berry', 0.6302295327186584), ('pears', 0.613396167755127), ('strawberry', 0.6058260798454285), ('peach', 0.6025872826576233), ('potato', 0.5960935354232788), ('grape', 0.5935863852500916), ('blueberry', 0.5866668224334717), ('cherries', 0.5784382820129395), ('mango', 0.5751855969429016), ('apricot', 0.5727777481079102), ('melon', 0.5719985365867615), ('almond', 0.5704829692840576)]
[('tomatoes', 0.8442263007164001), ('lettuce', 0.7069936990737915), ('asparagus', 0.7050934433937073), ('peaches', 0.6938520669937134), ('cherry_tomatoes', 0.6897529363632202), ('strawberry', 0.6888598799705505), ('strawberries', 0.6832595467567444), ('bell_peppers', 0.6813562512397766), ('potato', 0.6784172058105469), ('cantaloupe', 0.6780219078063965), ('celery', 0.675195574760437), ('onion', 0.6740139722824097), ('cucumbers', 0.6706333160400391), ('spinach', 0.6682621240615845), ('cauliflower', 0.6681587100028992)]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.637.1">In the next two steps, we compute a sentence vector by averaging all the word vectors in the sentence. </span><span class="kobospan" id="kobo.637.2">One of the challenges of this method is representing words that are not</span><a id="_idIndexMarker139" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.638.1"> present in the model, and here, we simply skip such words. </span><span class="kobospan" id="kobo.638.2">Let’s define a function that will take a sentence and a model and return a list of the sentence word vectors. </span><span class="kobospan" id="kobo.638.3">Words that are not present in the model will return </span><strong class="source-inline1"><span class="kobospan" id="kobo.639.1">KeyError</span></strong><span class="kobospan" id="kobo.640.1">, and in such a case, we catch the error </span><span><span class="kobospan" id="kobo.641.1">and continue:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.642.1">
def get_word_vectors(sentence, model):
    word_vectors = []
    for word in sentence:
        try:
            word_vector = model[word.lower()]
            word_vectors.append(word_vector)
        except KeyError:
            continue
    return word_vectors</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.643.1">Let’s now define a function that will take the word vector list and compute the sentence vector. </span><span class="kobospan" id="kobo.643.2">In order to compute the average, we represent the matrix as a </span><strong class="source-inline1"><span class="kobospan" id="kobo.644.1">numpy</span></strong><span class="kobospan" id="kobo.645.1"> array and use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.646.1">numpy</span></strong> <strong class="source-inline1"><span class="kobospan" id="kobo.647.1">mean</span></strong><span class="kobospan" id="kobo.648.1"> function to get the </span><span><span class="kobospan" id="kobo.649.1">average vector:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.650.1">
def get_sentence_vector(word_vectors):
    matrix = np.array(word_vectors)
    centroid = np.mean(matrix[:,:], axis=0)
    return centroid</span></pre></li>			</ol>
			<p class="callout-heading"><span class="kobospan" id="kobo.651.1">Note</span></p>
			<p class="callout"><span class="kobospan" id="kobo.652.1">Averaging the word vectors to get the sentence vector is only one way of approaching this task and is not without its problems. </span><span class="kobospan" id="kobo.652.2">One other alternative is to train a </span><strong class="source-inline1"><span class="kobospan" id="kobo.653.1">doc2vec</span></strong><span class="kobospan" id="kobo.654.1"> model, where sentences, paragraphs, and whole documents can all be units instead </span><span><span class="kobospan" id="kobo.655.1">of words.</span></span></p>
			<ol class="calibre13">
				<li value="8" class="calibre14"><span class="kobospan" id="kobo.656.1">We can now test the</span><a id="_idIndexMarker140" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.657.1"> average word embedding as a vectorizer.  </span><span class="kobospan" id="kobo.657.2">Our vectorizer takes in a string input, gets the word vectors for each word, and then returns the sentence vector that we compute in the </span><strong class="source-inline1"><span class="kobospan" id="kobo.658.1">get_sentence_vector</span></strong><span class="kobospan" id="kobo.659.1"> function. </span><span class="kobospan" id="kobo.659.2">We then load the training and test data and create the datasets. </span><span class="kobospan" id="kobo.659.3">We train the logistic regression classifier and </span><span><span class="kobospan" id="kobo.660.1">test it:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.661.1">
vectorize = lambda x: get_sentence_vector(
    get_word_vectors(x, model))
(train_df, test_df) = load_train_test_dataset_pd()
(X_train, X_test, y_train, y_test) = create_train_test_data(
    train_df, test_df, vectorize)
clf = train_classifier(X_train, y_train)
test_classifier(test_df, clf)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.662.1">We can see that the scores are much lower than those in previous sections. </span><span class="kobospan" id="kobo.662.2">There might be several reasons </span><a id="_idIndexMarker141" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.663.1">for this; one of them is that the </span><strong class="source-inline"><span class="kobospan" id="kobo.664.1">word2vec</span></strong><span class="kobospan" id="kobo.665.1"> model is English-only, and the data is multilingual. </span><span class="kobospan" id="kobo.665.2">As an exercise, you can write a script to filter English-only reviews and see whether that improves </span><span><span class="kobospan" id="kobo.666.1">the score:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.667.1">              precision    recall  f1-score   support
           0       0.54      0.57      0.55       160
           1       0.54      0.51      0.53       160
    accuracy                           0.54       320
   macro avg       0.54      0.54      0.54       320
weighted avg       0.54      0.54      0.54       320</span></pre></li>			</ol>
			<h2 id="_idParaDest-89" class="calibre5"><a id="_idTextAnchor091" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.668.1">There’s more…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.669.1">There are some other fun things that </span><strong class="source-inline"><span class="kobospan" id="kobo.670.1">gensim</span></strong><span class="kobospan" id="kobo.671.1"> can do with a pretrained model. </span><span class="kobospan" id="kobo.671.2">For example, it can find an outlier word from a list of words and find the word that is most similar to the given word from a list. </span><span class="kobospan" id="kobo.671.3">Let’s look </span><span><span class="kobospan" id="kobo.672.1">at these:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.673.1">Compile a list of words with one that doesn’t match, apply the </span><strong class="source-inline1"><span class="kobospan" id="kobo.674.1">doesnt_match</span></strong><span class="kobospan" id="kobo.675.1"> function to the list, and print </span><span><span class="kobospan" id="kobo.676.1">the results:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.677.1">
words = ['banana', 'apple', 'computer', 'strawberry']
print(model.doesnt_match(words))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.678.1">The result will be </span><span><span class="kobospan" id="kobo.679.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.680.1">computer</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.681.1">Now, let’s find a word that’s most similar to </span><span><span class="kobospan" id="kobo.682.1">another word.</span></span><pre class="source-code"><span class="kobospan1" id="kobo.683.1">
word = "cup"
words = ['glass', 'computer', 'pencil', 'watch']
print(model.most_similar_to_given(word, words))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.684.1">The result will be </span><span><span class="kobospan" id="kobo.685.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.686.1">glass</span></pre></li>			</ol>
			<h2 id="_idParaDest-90" class="calibre5"><a id="_idTextAnchor092" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.687.1">See also</span></h2>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.688.1">There are</span><a id="_idIndexMarker142" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.689.1"> many other pretrained models available, including some in other languages; </span><span><span class="kobospan" id="kobo.690.1">see </span></span><a href="http://vectors.nlpl.eu/repository/" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.691.1">http://vectors.nlpl.eu/repository/</span></span></a><span><span class="kobospan" id="kobo.692.1">.</span></span><p class="calibre3"><span class="kobospan" id="kobo.693.1">Some pretrained models include part-of-speech information, which can be helpful when disambiguating </span><a id="_idIndexMarker143" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.694.1">words. </span><span class="kobospan" id="kobo.694.2">These models concatenate words with their </span><strong class="bold"><span class="kobospan" id="kobo.695.1">part of speech</span></strong><span class="kobospan" id="kobo.696.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.697.1">POS</span></strong><span class="kobospan" id="kobo.698.1">) (e.g. </span><strong class="source-inline"><span class="kobospan" id="kobo.699.1">cat_NOUN</span></strong><span class="kobospan" id="kobo.700.1">), so keep that in mind when </span><span><span class="kobospan" id="kobo.701.1">using them.</span></span></p></li>
				<li class="calibre14"><span class="kobospan" id="kobo.702.1">To learn more about the theory behind </span><strong class="source-inline1"><span class="kobospan" id="kobo.703.1">word2vec</span></strong><span class="kobospan" id="kobo.704.1">, you can start </span><span><span class="kobospan" id="kobo.705.1">here: </span></span><a href="https://jalammar.github.io/illustrated-word2vec/" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.706.1">https://jalammar.github.io/illustrated-word2vec/</span></span></a><span><span class="kobospan" id="kobo.707.1">.</span></span></li>
			</ul>
			<h1 id="_idParaDest-91" class="calibre7"><a id="_idTextAnchor093" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.708.1">Training your own embeddings model</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.709.1">We can now train our own </span><strong class="source-inline"><span class="kobospan" id="kobo.710.1">word2vec</span></strong><span class="kobospan" id="kobo.711.1"> model on a corpus. </span><span class="kobospan" id="kobo.711.2">This model is a neural network that predicts a word </span><a id="_idIndexMarker144" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.712.1">when given a sentence with words blanked out. </span><span class="kobospan" id="kobo.712.2">The byproduct of the neural network training is the vector representation for each word in the training vocabulary. </span><span class="kobospan" id="kobo.712.3">For this task, we will continue using the Rotten Tomatoes reviews. </span><span class="kobospan" id="kobo.712.4">The dataset is not very large, so the results are not as good as they could be with a </span><span><span class="kobospan" id="kobo.713.1">larger collection.</span></span></p>
			<h2 id="_idParaDest-92" class="calibre5"><a id="_idTextAnchor094" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.714.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.715.1">We will use the </span><strong class="source-inline"><span class="kobospan" id="kobo.716.1">gensim</span></strong><span class="kobospan" id="kobo.717.1"> package for this task. </span><span class="kobospan" id="kobo.717.2">It should be installed as part of the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.718.1">poetry</span></strong></span><span><span class="kobospan" id="kobo.719.1"> environment.</span></span></p>
			<h2 id="_idParaDest-93" class="calibre5"><a id="_idTextAnchor095" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.720.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.721.1">We will create the dataset </span><a id="_idIndexMarker145" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.722.1">and then train the model on the data. </span><span class="kobospan" id="kobo.722.2">We will then test how </span><span><span class="kobospan" id="kobo.723.1">it performs:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.724.1">Import the necessary packages </span><span><span class="kobospan" id="kobo.725.1">and functions:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.726.1">
import gensim
from gensim.models import Word2Vec
from datasets import load_dataset
from gensim import utils</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.727.1">Load the training data and check </span><span><span class="kobospan" id="kobo.728.1">its length:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.729.1">
train_dataset = load_dataset("rotten_tomatoes", split="train")
print(len(train_dataset))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.730.1">The result should be </span><span><span class="kobospan" id="kobo.731.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.732.1">8530</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.733.1">Create the </span><strong class="source-inline1"><span class="kobospan" id="kobo.734.1">RottenTomatoesCorpus</span></strong><span class="kobospan" id="kobo.735.1"> class. </span><span class="kobospan" id="kobo.735.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.736.1">word2vec</span></strong><span class="kobospan" id="kobo.737.1"> training algorithm requires a class with a defined </span><strong class="source-inline1"><span class="kobospan" id="kobo.738.1">__iter__</span></strong><span class="kobospan" id="kobo.739.1"> function that allows you to iterate through the data, so that is why we need </span><span><span class="kobospan" id="kobo.740.1">this class:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.741.1">
class RottenTomatoesCorpus:
    def __init__(self, sentences):
        self.sentences = sentences
    def __iter__(self):
        for review in self.sentences:
            yield utils.simple_preprocess(
                gensim.parsing.preprocessing.remove_stopwords(
                    review))</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.742.1">Create an instance of </span><strong class="source-inline1"><span class="kobospan" id="kobo.743.1">RottenTomatoesCorpus</span></strong><span class="kobospan" id="kobo.744.1"> using the loaded training dataset. </span><span class="kobospan" id="kobo.744.2">Since </span><strong class="source-inline1"><span class="kobospan" id="kobo.745.1">word2vec</span></strong><span class="kobospan" id="kobo.746.1"> models are trained on text only (they are self-supervised models), we </span><a id="_idIndexMarker146" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.747.1">don’t need the </span><span><span class="kobospan" id="kobo.748.1">review score:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.749.1">
sentences = train_dataset["text"]
corpus = RottenTomatoesCorpus(sentences)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.750.1">In this step, we initialize the </span><strong class="source-inline1"><span class="kobospan" id="kobo.751.1">word2vec</span></strong><span class="kobospan" id="kobo.752.1"> model, train it, and then save it to disk. </span><span class="kobospan" id="kobo.752.2">The only required argument is the list of words; some of the other important ones are </span><strong class="source-inline1"><span class="kobospan" id="kobo.753.1">min_count</span></strong><span class="kobospan" id="kobo.754.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.755.1">size</span></strong><span class="kobospan" id="kobo.756.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.757.1">window</span></strong><span class="kobospan" id="kobo.758.1">, and </span><strong class="source-inline1"><span class="kobospan" id="kobo.759.1">workers</span></strong><span class="kobospan" id="kobo.760.1">. </span><span class="kobospan" id="kobo.760.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.761.1">min_count</span></strong><span class="kobospan" id="kobo.762.1"> parameter refers to the minimum number of times a word has to occur in the training corpus, the default being 5. </span><span class="kobospan" id="kobo.762.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.763.1">size</span></strong><span class="kobospan" id="kobo.764.1"> parameter sets the size of the word vector. </span><strong class="source-inline1"><span class="kobospan" id="kobo.765.1">window</span></strong><span class="kobospan" id="kobo.766.1"> restricts the maximum number of words between the predicted and current words in a sentence. </span><strong class="source-inline1"><span class="kobospan" id="kobo.767.1">workers</span></strong><span class="kobospan" id="kobo.768.1"> refers to the number of working threads; the more there are, the quicker the training will proceed. </span><span class="kobospan" id="kobo.768.2">When training the model, the </span><strong class="source-inline1"><span class="kobospan" id="kobo.769.1">epoch</span></strong><span class="kobospan" id="kobo.770.1"> parameter will determine the number of training iterations the model will go through. </span><span class="kobospan" id="kobo.770.2">After initializing the model object, we train it on our corpus for 100 epochs and, finally, save it </span><span><span class="kobospan" id="kobo.771.1">to disk:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.772.1">
model = Word2Vec(sentences=corpus, vector_size=100,
    window=5, min_count=1, workers=4)
model.train(corpus_iterable=corpus,
    total_examples=model.corpus_count, epochs=100)
model.save("../data/rotten_tomato_word2vec.model")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.773.1">Get 10 words similar to the word </span><em class="italic"><span class="kobospan" id="kobo.774.1">movie</span></em><span class="kobospan" id="kobo.775.1">. </span><span class="kobospan" id="kobo.775.2">The words </span><em class="italic"><span class="kobospan" id="kobo.776.1">sequels</span></em><span class="kobospan" id="kobo.777.1"> and </span><em class="italic"><span class="kobospan" id="kobo.778.1">film</span></em><span class="kobospan" id="kobo.779.1"> make sense with this word; the rest are not that related. </span><span class="kobospan" id="kobo.779.2">This is due to the small size of the training corpus. </span><span class="kobospan" id="kobo.779.3">The</span><a id="_idIndexMarker147" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.780.1"> words you get will be different, since the results are different every time the model </span><span><span class="kobospan" id="kobo.781.1">is trained:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.782.1">
w1 = "movie"
words = model.wv.most_similar(w1, topn=10)
print(words)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.783.1">This is a </span><span><span class="kobospan" id="kobo.784.1">possible output:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.785.1">[('sequels', 0.38357362151145935), ('film', 0.33577531576156616), ('stuffed', 0.2925359606742859), ('quirkily', 0.28789234161376953), ('convict', 0.2810690104961395), ('worse', 0.2789292335510254), ('churn', 0.27702808380126953), ('hellish', 0.27698105573654175), ('hey', 0.27566075325012207), ('happens', 0.27498629689216614)]</span></pre></li>			</ol>
			<h2 id="_idParaDest-94" class="calibre5"><a id="_idTextAnchor096" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.786.1">There’s more…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.787.1">There are tools to evaluate a </span><strong class="source-inline"><span class="kobospan" id="kobo.788.1">word2vec</span></strong><span class="kobospan" id="kobo.789.1"> model, although its creation is unsupervised. </span><strong class="source-inline"><span class="kobospan" id="kobo.790.1">gensim</span></strong><span class="kobospan" id="kobo.791.1"> comes with a file that lists word analogies, such as what </span><em class="italic"><span class="kobospan" id="kobo.792.1">Athens</span></em><span class="kobospan" id="kobo.793.1"> is to </span><em class="italic"><span class="kobospan" id="kobo.794.1">Greece</span></em><span class="kobospan" id="kobo.795.1"> being the same as what </span><em class="italic"><span class="kobospan" id="kobo.796.1">Moscow</span></em><span class="kobospan" id="kobo.797.1"> is to </span><em class="italic"><span class="kobospan" id="kobo.798.1">Russia</span></em><span class="kobospan" id="kobo.799.1">. </span><span class="kobospan" id="kobo.799.2">The </span><strong class="source-inline"><span class="kobospan" id="kobo.800.1">evaluate_word_analogies</span></strong><span class="kobospan" id="kobo.801.1"> function runs the analogies through the model and calculates how many </span><span><span class="kobospan" id="kobo.802.1">were correct.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.803.1">Here is how to </span><span><span class="kobospan" id="kobo.804.1">do this:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.805.1">Use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.806.1">evaluate_word_analogies</span></strong><span class="kobospan" id="kobo.807.1"> function to evaluate our trained model. </span><span class="kobospan" id="kobo.807.2">We need the </span><strong class="source-inline1"><span class="kobospan" id="kobo.808.1">analogies</span></strong><span class="kobospan" id="kobo.809.1"> file, which is available in the book GitHub repository </span><span><span class="kobospan" id="kobo.810.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/questions-words.txt" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.811.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/questions-words.txt</span></span></a><pre class="source-code"><span class="kobospan1" id="kobo.812.1">
(analogy_score, word_list) = model.wv.evaluate_word_analogies(
    '../data/questions-words.txt')
print(analogy_score)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.813.1">The result should be similar </span><span><span class="kobospan" id="kobo.814.1">to this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.815.1">0.0015881418740074113</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.816.1">Let’s now evaluate the pretrained model. </span><span class="kobospan" id="kobo.816.2">These commands might take longer </span><span><span class="kobospan" id="kobo.817.1">to run:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.818.1">
pretrained_model = \
    gensim.models.KeyedVectors.load_word2vec_format(
        '../data/GoogleNews-vectors-negative300.bin.gz',
        binary=True)
(analogy_score, word_list) = \
    pretrained_model.evaluate_word_analogies(
        '../data/questions-words.txt')
print(analogy_score)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.819.1">The result should be similar </span><span><span class="kobospan" id="kobo.820.1">to this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.821.1">0.7401448525607863</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.822.1">We use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.823.1">evaluate_word_analogies</span></strong><span class="kobospan" id="kobo.824.1"> function differently in the pretrained model </span><a id="_idIndexMarker148" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.825.1">and our model case because they are different types. </span><span class="kobospan" id="kobo.825.2">With the pretrained model, we just load the vectors (a </span><strong class="source-inline1"><span class="kobospan" id="kobo.826.1">KeyedVectors</span></strong><span class="kobospan" id="kobo.827.1"> class, where each word, represented by a key, is mapped to a vector), and our model is a full </span><strong class="source-inline1"><span class="kobospan" id="kobo.828.1">word2vec</span></strong><span class="kobospan" id="kobo.829.1"> model object. </span><span class="kobospan" id="kobo.829.2">We can check the types by using </span><span><span class="kobospan" id="kobo.830.1">these commands:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.831.1">
print(type(pretrained_model))
print(type(model))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.832.1">The result will be </span><span><span class="kobospan" id="kobo.833.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.834.1">&lt;class 'gensim.models.keyedvectors.KeyedVectors'&gt;
&lt;class 'gensim.models.word2vec.Word2Vec'&gt;</span></pre><p class="calibre3"><span class="kobospan" id="kobo.835.1">The pretrained model was trained on a much larger corpus and, predictably, performs better. </span><span class="kobospan" id="kobo.835.2">You can also construct your own evaluation file with analogies that your </span><span><span class="kobospan" id="kobo.836.1">data requires.</span></span></p></li>			</ol>
			<p class="callout-heading"><span class="kobospan" id="kobo.837.1">Note</span></p>
			<p class="callout"><span class="kobospan" id="kobo.838.1">Make sure your evaluation is based on the type of data that you are going to use in your application; otherwise, you risk having misleading </span><span><span class="kobospan" id="kobo.839.1">evaluation results.</span></span></p>
			<h2 id="_idParaDest-95" class="calibre5"><a id="_idTextAnchor097" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.840.1">See also</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.841.1">There is an additional way </span><a id="_idIndexMarker149" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.842.1">of evaluating model performance, by comparing the similarity between word pairs assigned by a model to the human-assigned judgments. </span><span class="kobospan" id="kobo.842.2">You can do this by using the </span><strong class="source-inline"><span class="kobospan" id="kobo.843.1">evaluate_word_pairs</span></strong><span class="kobospan" id="kobo.844.1"> function. </span><span class="kobospan" id="kobo.844.2">See more </span><span><span class="kobospan" id="kobo.845.1">at </span></span><a href="https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_pairs" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.846.1">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_pairs</span></span></a><span><span class="kobospan" id="kobo.847.1">.</span></span></p>
			<h1 id="_idParaDest-96" class="calibre7"><a id="_idTextAnchor098" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.848.1">Using BERT and OpenAI embeddings instead of 
word embeddings</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.849.1">Instead of word embeddings, we can use </span><strong class="bold"><span class="kobospan" id="kobo.850.1">Bidirectional Encoder Representations from Transformer</span></strong><span class="kobospan" id="kobo.851.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.852.1">BERT</span></strong><span class="kobospan" id="kobo.853.1">) embeddings. </span><span class="kobospan" id="kobo.853.2">A BERT model, like word embeddings, is a pretrained model and</span><a id="_idIndexMarker150" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.854.1"> gives a vector representation, but it takes context into account and can</span><a id="_idIndexMarker151" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.855.1"> represent a whole sentence instead of </span><span><span class="kobospan" id="kobo.856.1">individual words.</span></span></p>
			<h2 id="_idParaDest-97" class="calibre5"><a id="_idTextAnchor099" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.857.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.858.1">For this recipe, we can use the Hugging Face </span><strong class="source-inline"><span class="kobospan" id="kobo.859.1">sentence_transformers</span></strong><span class="kobospan" id="kobo.860.1"> package to represent sentences as vectors. </span><span class="kobospan" id="kobo.860.2">We need </span><strong class="source-inline"><span class="kobospan" id="kobo.861.1">PyTorch</span></strong><span class="kobospan" id="kobo.862.1">, which is installed as part of the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.863.1">poetry</span></strong></span><span><span class="kobospan" id="kobo.864.1"> environment.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.865.1">To get the vectors, we will use the </span><strong class="source-inline"><span class="kobospan" id="kobo.866.1">all-MiniLM-L6-v2</span></strong><span class="kobospan" id="kobo.867.1"> model for </span><span><span class="kobospan" id="kobo.868.1">this recipe.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.869.1">We can also use the embeddings from </span><a id="_idIndexMarker152" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.870.1">OpenAI that come from their </span><strong class="bold"><span class="kobospan" id="kobo.871.1">large language </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.872.1">models</span></strong></span><span><span class="kobospan" id="kobo.873.1"> (</span></span><span><strong class="bold"><span class="kobospan" id="kobo.874.1">LLMs</span></strong></span><span><span class="kobospan" id="kobo.875.1">).</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.876.1">To use the OpenAI embeddings, you will need to create an account and get an API key from OpenAI. </span><span class="kobospan" id="kobo.876.2">You can </span><a id="_idIndexMarker153" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.877.1"> create an account </span><span><span class="kobospan" id="kobo.878.1">at </span></span><a href="https://platform.openai.com/signup" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.879.1">https://platform.openai.com/signup</span></span></a><span><span class="kobospan" id="kobo.880.1">.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.881.1">The notebook is located </span><span><span class="kobospan" id="kobo.882.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.6_train_own_word2vec.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.883.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.6_train_own_word2vec.ipynb</span></span></a><span><span class="kobospan" id="kobo.884.1">.</span></span></p>
			<h2 id="_idParaDest-98" class="calibre5"><a id="_idTextAnchor100" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.885.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.886.1">Hugging Face code makes using</span><a id="_idIndexMarker154" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.887.1"> BERT very easy. </span><span class="kobospan" id="kobo.887.2">The first time the code runs, it will download the necessary model, which might take some time. </span><span class="kobospan" id="kobo.887.3">After the download, it’s just a matter of encoding the sentences using the model. </span><span class="kobospan" id="kobo.887.4">We will test the simple classifier with </span><span><span class="kobospan" id="kobo.888.1">these embeddings:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.889.1">Run the simple classifier notebook to import </span><span><span class="kobospan" id="kobo.890.1">its functions:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.891.1">
%run -i "../util/util_simple_classifier.ipynb"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.892.1">Import the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.893.1">SentenceTransformer</span></strong></span><span><span class="kobospan" id="kobo.894.1"> class:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.895.1">
from sentence_transformers import SentenceTransformer</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.896.1">Load the sentence transformer model, retrieve the embedding of the sentence </span><em class="italic"><span class="kobospan" id="kobo.897.1">I love jazz</span></em><span class="kobospan" id="kobo.898.1">, and print </span><span><span class="kobospan" id="kobo.899.1">it out.</span></span><pre class="source-code"><span class="kobospan1" id="kobo.900.1">
model = SentenceTransformer('all-MiniLM-L6-v2')
embedding = model.encode(["I love jazz"])
print(embedding)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.901.1">As we can see, it is a vector similar to the word embeddings vector from the </span><span><span class="kobospan" id="kobo.902.1">previous recipe:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.903.1">[[ 2.94217980e-03 -7.93536603e-02 -2.82228496e-02 -5.13779782e-02
  -6.44981042e-02  9.83557850e-02  1.09671958e-01 -3.26390602e-02
   4.96566631e-02  2.56580133e-02 -1.08482063e-01  1.88441798e-02
   2.70963665e-02 -3.80690470e-02  2.42502335e-02 -3.65605950e-03
   1.29364491e-01  4.32255343e-02 -6.64561391e-02 -6.93060979e-02
  -1.39410645e-01  4.36719768e-02 -7.85463024e-03  1.68625098e-02
  -1.01160072e-02  1.07926019e-02 -1.05814040e-02  2.57284809e-02
  -1.51516097e-02 -4.53920700e-02  7.12087378e-03  1.17573030e-01… ]]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.904.1">Now, we can test our classifier using the BERT embeddings. </span><span class="kobospan" id="kobo.904.2">First, let’s define a function that will return a </span><a id="_idIndexMarker155" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.905.1">sentence vector. </span><span class="kobospan" id="kobo.905.2">This function takes the input text and a model. </span><span class="kobospan" id="kobo.905.3">It then uses the model to encode the text and returns the resulting embedding. </span><span class="kobospan" id="kobo.905.4">We</span><a id="_idIndexMarker156" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.906.1">  need to pass in the text inside of a list to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.907.1">encode</span></strong><span class="kobospan" id="kobo.908.1"> method, as it expects an iterable. </span><span class="kobospan" id="kobo.908.2">Similarly, we return the first element of the result, since it returns a list </span><span><span class="kobospan" id="kobo.909.1">of embeddings.</span></span><pre class="source-code"><span class="kobospan1" id="kobo.910.1">
def get_sentence_vector(text, model):
    sentence_embeddings = model.encode([text])
    return sentence_embeddings[0]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.911.1">Now, we define the </span><strong class="source-inline1"><span class="kobospan" id="kobo.912.1">vectorize</span></strong><span class="kobospan" id="kobo.913.1"> function, create the training and test data using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.914.1">load_train_test_dataset_pd</span></strong><span class="kobospan" id="kobo.915.1"> function we created in the </span><em class="italic"><span class="kobospan" id="kobo.916.1">Creating a simple classifier</span></em><span class="kobospan" id="kobo.917.1"> recipe, train the classifier, and test it. </span><span class="kobospan" id="kobo.917.2">We will time the dataset creation step, hence the inclusion of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.918.1">time</span></strong><span class="kobospan" id="kobo.919.1"> package commands. </span><span class="kobospan" id="kobo.919.2">We see that it takes about 11 seconds to vectorize the whole dataset (about 85,000 entries). </span><span class="kobospan" id="kobo.919.3">We </span><a id="_idIndexMarker157" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.920.1">then train the model and </span><span><span class="kobospan" id="kobo.921.1">test it:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.922.1">
import time
vectorize = lambda x: get_sentence_vector(x, model)
(train_df, test_df) = load_train_test_dataset_pd()
start = time.time()
(X_train, X_test, y_train, y_test) = create_train_test_data(
    train_df, test_df, vectorize)
print(f"BERT embeddings: {time.time() - start} s")
clf = train_classifier(X_train, y_train)
test_classifier(test_df, clf)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.923.1">The result is our best </span><a id="_idIndexMarker158" class="calibre6 pcalibre pcalibre1"/><span><span class="kobospan" id="kobo.924.1">one yet:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.925.1">BERT embeddings: 11.410213232040405 s
              precision    recall  f1-score   support
           0       0.77      0.79      0.78       160
           1       0.79      0.76      0.77       160
    accuracy                           0.78       320
   macro avg       0.78      0.78      0.78       320
weighted avg       0.78      0.78      0.78       320</span></pre></li>			</ol>
			<h2 id="_idParaDest-99" class="calibre5"><a id="_idTextAnchor101" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.926.1">There’s more…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.927.1">We can now use the</span><a id="_idIndexMarker159" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.928.1">  OpenAI embeddings to see how </span><span><span class="kobospan" id="kobo.929.1">they perform:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.930.1">Import the </span><strong class="source-inline1"><span class="kobospan" id="kobo.931.1">openai</span></strong><span class="kobospan" id="kobo.932.1"> package and assign the </span><span><span class="kobospan" id="kobo.933.1">API key:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.934.1">
import openai
openai.api_key = OPEN_AI_KEY</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.935.1">Assign the model we are going to use, the sentence and create the embedding. </span><span class="kobospan" id="kobo.935.2">The model that we</span><a id="_idIndexMarker160" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.936.1"> will use is specifically an embeddings model, so it returns an embeddings vector for a </span><span><span class="kobospan" id="kobo.937.1">text input:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.938.1">
model = "text-embedding-ada-002"
text = "I love jazz"
response = openai.Embedding.create(
    input=text,
    model=model
)
embeddings = response['data'][0]['embedding']
print(embeddings)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.939.1">The partial result will be </span><span><span class="kobospan" id="kobo.940.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.941.1">[-0.028350897133350372, -0.011136125773191452, -0.0021299426443874836, -0.014453398995101452, -0.012048527598381042, 0.018223850056529045, -0.010247894562780857, -0.01806674897670746, -0.014308380894362926, 0.0007220656843855977, -9.998268797062337e-05, 0.010078707709908485,…]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.942.1">Let’s now test our classifier using the OpenAI embeddings. </span><span class="kobospan" id="kobo.942.2">This is the function that will return a </span><span><span class="kobospan" id="kobo.943.1">sentence vector:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.944.1">
def get_sentence_vector(text, model):
    text = "I love jazz"
    response = openai.Embedding.create(
        input=text,
        model=model
    )
    embeddings = response['data'][0]['embedding']
    return embeddings</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.945.1">Now, define the </span><strong class="source-inline1"><span class="kobospan" id="kobo.946.1">vectorize</span></strong><span class="kobospan" id="kobo.947.1"> function, create</span><a id="_idIndexMarker161" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.948.1"> the training and test data, train the classifier, and</span><a id="_idIndexMarker162" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.949.1">  test it. </span><span class="kobospan" id="kobo.949.2">We will time the </span><span><span class="kobospan" id="kobo.950.1">vectorizing step:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.951.1">
import time
vectorize = lambda x: get_sentence_vector(x, model)
(train_df, test_df) = load_train_test_dataset_pd()
start = time.time()
(X_train, X_test, y_train, y_test) = create_train_test_data(
    train_df, test_df, vectorize)
print(f"OpenAI embeddings: {time.time() - start} s")
clf = train_classifier(X_train, y_train)
test_classifier(test_df, clf)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.952.1">The result will be </span><span><span class="kobospan" id="kobo.953.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.954.1">OpenAI embeddings: 704.3250799179077 s
              precision    recall  f1-score   support
           0       0.49      0.82      0.62       160
           1       0.47      0.16      0.23       160
    accuracy                           0.49       320
   macro avg       0.48      0.49      0.43       320
weighted avg       0.48      0.49      0.43       320</span></pre><p class="calibre3"><span class="kobospan" id="kobo.955.1">Note that the result is quite poor in terms of the score, and it takes more than 10 minutes to process the whole dataset. </span><span class="kobospan" id="kobo.955.2">Here, we only use the LLM embeddings and then </span><a id="_idIndexMarker163" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.956.1"> train a logistic regression classifier on </span><a id="_idIndexMarker164" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.957.1">those embeddings. </span><span class="kobospan" id="kobo.957.2">This is different from using the LLM itself to do </span><span><span class="kobospan" id="kobo.958.1">the classification.</span></span></p></li>			</ol>
			<h2 id="_idParaDest-100" class="calibre5"><a id="_idTextAnchor102" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.959.1">See also</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.960.1">For more pretrained models, </span><span><span class="kobospan" id="kobo.961.1">see </span></span><a href="https://www.sbert.net/docs/pretrained_models.html" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.962.1">https://www.sbert.net/docs/pretrained_models.html</span></span></a><span><span class="kobospan" id="kobo.963.1">.</span></span></p>
			<h1 id="_idParaDest-101" class="calibre7"><a id="_idTextAnchor103" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.964.1">Retrieval augmented generation (RAG)</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.965.1">In this recipe, we will see vector </span><a id="_idIndexMarker165" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.966.1">embeddings in action. </span><span class="kobospan" id="kobo.966.2">RAG is a popular method of working with LLMs. </span><span class="kobospan" id="kobo.966.3">Since these models are pretrained on widely available internet data, they do not have access to our personal data, and we cannot use the model as it is to ask questions about it. </span><span class="kobospan" id="kobo.966.4">A way to overcome this is to use vector embeddings to represent our data. </span><span class="kobospan" id="kobo.966.5">Then, we can compute cosine similarity between our data and the question and include the most similar piece of our data, together with the question – hence the name “retrieval augmented generation,” since we first retrieve relevant data by using cosine similarity and then generate text using </span><span><span class="kobospan" id="kobo.967.1">the LLM.</span></span></p>
			<h2 id="_idParaDest-102" class="calibre5"><a id="_idTextAnchor104" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.968.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.969.1">We will use an IMDB dataset </span><a id="_idIndexMarker166" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.970.1">from </span><strong class="bold"><span class="kobospan" id="kobo.971.1">Kaggle</span></strong><span class="kobospan" id="kobo.972.1">, which can be downloaded from </span><a href="https://www.kaggle.com/PromptCloudHQ/imdb-data" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.973.1">https://www.kaggle.com/PromptCloudHQ/imdb-data</span></a><span class="kobospan" id="kobo.974.1"> and is also included in the book GitHub repo at </span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/IMDB-Movie-Data.csv" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.975.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/IMDB-Movie-Data.csv</span></a><span class="kobospan" id="kobo.976.1">. </span><span class="kobospan" id="kobo.976.2">Download the dataset and unzip the </span><span><span class="kobospan" id="kobo.977.1">CSV file.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.978.1">We will also use the OpenAI embeddings, as well as the </span><strong class="source-inline"><span class="kobospan" id="kobo.979.1">llama_index</span></strong><span class="kobospan" id="kobo.980.1"> package, which is included </span><a id="_idIndexMarker167" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.981.1">within the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.982.1">poetry</span></strong></span><span><span class="kobospan" id="kobo.983.1"> environment.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.984.1">The notebook is located </span><span><span class="kobospan" id="kobo.985.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.9_vector_search.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.986.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.9_vector_search.ipynb</span></span></a><span><span class="kobospan" id="kobo.987.1">.</span></span></p>
			<h2 id="_idParaDest-103" class="calibre5"><a id="_idTextAnchor105" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.988.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.989.1">We will load the IMDB dataset and then create a vector store, using its first 10 entries. </span><span class="kobospan" id="kobo.989.2">We will then use the </span><strong class="source-inline"><span class="kobospan" id="kobo.990.1">llama_index</span></strong><span class="kobospan" id="kobo.991.1"> package to query the </span><span><span class="kobospan" id="kobo.992.1">vector store:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.993.1">Run the file </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.994.1">utilities</span></strong></span><span><span class="kobospan" id="kobo.995.1"> notebook:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.996.1">
%run -i "../util/file_utils.ipynb"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.997.1">Import the necessary classes </span><span><span class="kobospan" id="kobo.998.1">and packages:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.999.1">
import csv
import openai
from llama_index import VectorStoreIndex
from llama_index import Document
openai.api_key = OPEN_AI_KEY</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.1000.1">Read in the CSV data. </span><span class="kobospan" id="kobo.1000.2">We will skip the first header row of </span><span><span class="kobospan" id="kobo.1001.1">the data:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.1002.1">
with open('../data/IMDB-Movie-Data.csv') as f:
    reader = csv.reader(f)
    data = list(reader)
    movies = data[1:]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.1003.1">In this step, we use the first 10 rows of the data we just read in to first create a list of </span><strong class="source-inline1"><span class="kobospan" id="kobo.1004.1">Document</span></strong><span class="kobospan" id="kobo.1005.1"> objects, and then a </span><strong class="source-inline1"><span class="kobospan" id="kobo.1006.1">VectorStoreIndex</span></strong><span class="kobospan" id="kobo.1007.1"> object with these </span><strong class="source-inline1"><span class="kobospan" id="kobo.1008.1">Document</span></strong><span class="kobospan" id="kobo.1009.1"> objects. </span><span class="kobospan" id="kobo.1009.2">An index is an object used for search, where each record contains certain information. </span><span class="kobospan" id="kobo.1009.3">A vector store index stores metadata as well as the vector representation of each record. </span><span class="kobospan" id="kobo.1009.4">For each movie, we assign the description as the text that will be </span><a id="_idIndexMarker168" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1010.1">embedded and the rest as metadata. </span><span class="kobospan" id="kobo.1010.2">We print out </span><strong class="source-inline1"><span class="kobospan" id="kobo.1011.1">document</span></strong><span class="kobospan" id="kobo.1012.1"> objects and can see that a unique ID has been assigned </span><span><span class="kobospan" id="kobo.1013.1">to each:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.1014.1">
documents = []
for movie in movies[0:10]:
    doc_id = movie[0]
    title = movie[1]
    genres = movie[2].split(",")
    description = movie[3]
    director = movie[4]
    actors = movie[5].split(",")
    year = movie[6]
    duration = movie[7]
    rating = movie[8]
    revenue = movie[10]
    document = Document(
        text=description,
        metadata={
            "title": title,
            "genres": genres,
            "director": director,
            "actors": actors,
            "year": year,
            "duration": duration,
            "rating": rating,
            "revenue": revenue
        }
    )
    print(document)
    documents.append(document)
index = VectorStoreIndex.from_documents(documents)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.1015.1">The partial output will look</span><a id="_idIndexMarker169" class="calibre6 pcalibre pcalibre1"/> <span><span class="kobospan" id="kobo.1016.1">like this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.1017.1">id_='6e1ef633-f10b-44e3-9b77-f5f7b08dcedd' embedding=None metadata={'title': 'Guardians of the Galaxy', 'genres': ['Action', 'Adventure', 'Sci-Fi'], 'director': 'James Gunn', 'actors': ['Chris Pratt', ' Vin Diesel', ' Bradley Cooper', ' Zoe Saldana'], 'year': '2014', 'duration': '121', 'rating': '8.1', 'revenue': '333.13'} excluded_embed_metadata_keys=[] excluded_llm_metadata_keys=[] relationships={} hash='e18bdce3a36c69d8c1e55a7eb56f05162c68c97151cbaf40
91814ae3df42dfe8' text='A group of intergalactic criminals are forced to work together to stop a fanatical warrior from taking control of the universe.' </span><span class="kobospan1" id="kobo.1017.2">start_char_idx=None end_char_idx=None text_template='{metadata_str}\n\n{content}' metadata_template='{key}: {value}' metadata_seperator='\n'</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.1018.1">Create the query engine from the index we just created. </span><span class="kobospan" id="kobo.1018.2">The query engine will allow us to send in questions about the documents loaded in </span><span><span class="kobospan" id="kobo.1019.1">the index:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.1020.1">
query_engine = index.as_query_engine()</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.1021.1">Use the engine to </span><a id="_idIndexMarker170" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1022.1">answer </span><span><span class="kobospan" id="kobo.1023.1">a question:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.1024.1">
response = query_engine.query("""Which movies talk about something gigantic?""")
print(response.response)
The answer seems to make sense grammatically, and arguably the Great Wall of China is gigantic. </span><span class="kobospan1" id="kobo.1024.2">However, it is not clear what is gigantic in the movie Prometheus. </span><span class="kobospan1" id="kobo.1024.3">So here we have a partially correct answer. </span><span class="kobospan1" id="kobo.1024.4">The Great Wall and Prometheus both talk about something gigantic. </span><span class="kobospan1" id="kobo.1024.5">In The Great Wall, the protagonists become embroiled in the defense of the Great Wall of China against a horde of monstrous creatures. </span><span class="kobospan1" id="kobo.1024.6">In Prometheus, the protagonists find a structure on a distant moon.</span></pre></li>			</ol>
		</div>
	</body></html>