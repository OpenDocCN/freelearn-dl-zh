<html><head></head><body>
		<div><h1 id="_idParaDest-66" class="chapter-number"><a id="_idTextAnchor067" class="calibre6 pcalibre pcalibre1"/>3</h1>
			<h1 id="_idParaDest-67" class="calibre7"><a id="_idTextAnchor068" class="calibre6 pcalibre pcalibre1"/>Representing Text  – Capturing Semantics</h1>
			<p class="calibre3">Representing the meaning of words, phrases, and sentences in a form that’s understandable to computers is one of the pillars of NLP processing. Machine learning, for example, represents each data point as a list of numbers (a fixed-size vector), and we are faced with the question of how to turn words and sentences into these vectors. Most NLP tasks start by representing the text in some numeric form, and in this chapter, we show several ways to do that.</p>
			<p class="calibre3">First, we will create a simple classifier to demonstrate the effectiveness of each method of encoding, and then we will use it to test the different encoding methods. We will also learn how to turn phrases such as <em class="italic">fried chicken</em> into vectors – that is, how to train a <code>word2vec</code> model for phrases. Finally, we will see how to use vector-based search.</p>
			<p class="calibre3">For a theoretical background on some of the concepts discussed in this section, refer to <em class="italic">Building Machine Learning Systems with Python</em> by Coelho et al. This book will explain the basics of building a machine learning project, such as training and test sets, as well as metrics used to evaluate such projects, including precision, recall, F1, and accuracy.</p>
			<p class="calibre3">Here are the recipes that are covered in this chapter:</p>
			<ul class="calibre15">
				<li class="calibre14">Creating a simple classifier</li>
				<li class="calibre14">Putting documents into a bag of words</li>
				<li class="calibre14">Constructing an <em class="italic">N</em>-gram model</li>
				<li class="calibre14">Representing texts with TF-IDF</li>
				<li class="calibre14">Using word embeddings</li>
				<li class="calibre14">Training your own embeddings model</li>
				<li class="calibre14">Using BERT and OpenAI embeddings instead of word embeddings</li>
				<li class="calibre14">Using <strong class="bold">retrieval augmented </strong><strong class="bold">generation</strong> (<strong class="bold">RAG</strong>)</li>
			</ul>
			<h1 id="_idParaDest-68" class="calibre7"><a id="_idTextAnchor069" class="calibre6 pcalibre pcalibre1"/>Technical requirements</h1>
			<p class="calibre3">The code for this chapter is located at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter03" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter03</a>. Packages that are required for this chapter should be installed automatically via the <code>poetry</code> environment.</p>
			<p class="calibre3">In addition, we will use models and datasets located at the following URLs. The Google <code>word2vec</code> model is a model that represents words as vectors, and the IMDB dataset contains movie titles, genres, and descriptions. Download them into the <code>data</code> folder inside the <code>root</code> directory:</p>
			<ul class="calibre15">
				<li class="calibre14"><strong class="bold">The Google</strong> <strong class="source-inline1">word2vec</strong> <strong class="bold">model</strong>: <a href="https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g" class="calibre6 pcalibre pcalibre1">https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g</a></li>
				<li class="calibre14"><strong class="bold">The IMDB movie dataset</strong>: <a href="https://github.com/venusanvi/imdb-movies/blob/main/IMDB-Movie-Data.csv" class="calibre6 pcalibre pcalibre1">https://github.com/venusanvi/imdb-movies/blob/main/IMDB-Movie-Data.csv</a> (also available in the book’s GitHub repo)</li>
			</ul>
			<p class="calibre3"><a id="_idTextAnchor070" class="calibre6 pcalibre pcalibre1"/>In addition to the preceding files, we will use various functions from a simple classifier that we will create in the first recipe. This file is available at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/util_simple_classifier.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/util_simple_classifier.ipynb</a>.</p>
			<h1 id="_idParaDest-69" class="calibre7"><a id="_idTextAnchor071" class="calibre6 pcalibre pcalibre1"/>Creating a simple classifier</h1>
			<p class="calibre3">The reason why we need to <a id="_idIndexMarker098" class="calibre6 pcalibre pcalibre1"/>represent text as vectors is to make it into a computer-readable form. Computers can’t understand words but are good at manipulating numbers. One of the main NLP tasks is the classification of texts, and we are going to create a classifier for movie reviews. We will use the same classifier code but with different methods of creating vectors from text.</p>
			<p class="calibre3">In this section, we will create the classifier that will assign either negative or positive sentiment to <em class="italic">Rotten Tomatoes </em>reviews, a dataset available through Hugging Face, a large repository of open source models and datasets. We will then use a baseline method, where we encode the text by counting the number of different parts of speech present in it (verbs, nouns, proper nouns, adjectives, adverbs, auxiliary verbs, pronouns, numbers, and punctuation).</p>
			<p class="calibre3">By the end of this recipe, we will have created a separate file with functions that create the dataset and train the classifier. We will use this file throughout the chapter to test different encoding methods.</p>
			<h2 id="_idParaDest-70" class="calibre5"><a id="_idTextAnchor072" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">In this recipe, we will create <a id="_idIndexMarker099" class="calibre6 pcalibre pcalibre1"/>a simple classifier for movie reviews. It will be a <code>sklearn</code> package.</p>
			<h2 id="_idParaDest-71" class="calibre5"><a id="_idTextAnchor073" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">We will load the Rotten Tomatoes dataset from Hugging Face. We will use just part of the dataset so that the training time is not very long:</p>
			<ol class="calibre13">
				<li class="calibre14">Import the file and language <strong class="source-inline1">util</strong> notebooks:<pre class="source-code">
%run -i "../util/file_utils.ipynb"
%run -i "../util/lang_utils.ipynb"</pre></li>				<li class="calibre14">Load the train and test datasets from Hugging Face (the <strong class="source-inline1">datasets</strong> package). For both the training and test sets, we will select the first and last 15% of the data instead of loading the full datasets. The full dataset is large, and it takes a long time to train the model:<pre class="source-code">
from datasets import load_dataset
train_dataset = load_dataset("rotten_tomatoes",
    split="train[:15%]+train[-15%:]")
test_dataset = load_dataset("rotten_tomatoes",
    split="test[:15%]+test[-15%:]")</pre></li>				<li class="calibre14">Print out the length of each dataset:<pre class="source-code">
print(len(train_dataset))
print(len(test_dataset))</pre><p class="calibre3">The output should be as follows:</p><pre class="source-code">2560
320</pre></li>				<li class="calibre14">Here, we create the <strong class="source-inline1">POS_vectorizer</strong> class. This class has a method, <strong class="source-inline1">vectorize</strong>, that processes the text and counts the number of verbs, nouns, proper nouns, adjectives, adverbs, auxiliary verbs, pronouns, numbers, and punctuation marks. The class needs a <strong class="source-inline1">spaCy</strong> model to process the text. Each piece of text is turned into <a id="_idIndexMarker100" class="calibre6 pcalibre pcalibre1"/>a vector of size 10. The first element of the vector is the length of the text, and the other numbers indicate the number of words in the text of that particular part of speech:<pre class="source-code">
class POS_vectorizer:
    def __init__(self, spacy_model):
        self.model = spacy_model
    def vectorize(self, input_text):
        doc = self.model(input_text)
        vector = []
        vector.append(len(doc))
        pos = {"VERB":0, "NOUN":0, "PROPN":0, "ADJ":0,
            "ADV":0, "AUX":0, "PRON":0, "NUM":0, "PUNCT":0}
        for token in doc:
            if token.pos_ in pos:
                pos[token.pos_] += 1
        vector_values = list(pos.values())
        vector = vector + vector_values
        return vector</pre></li>				<li class="calibre14">Now, we can test out the <strong class="source-inline1">POS_vectorizer</strong> class. We take the first review’s text to process and<a id="_idIndexMarker101" class="calibre6 pcalibre pcalibre1"/> create the vectorizer using the small <strong class="source-inline1">spaCy</strong> model. We then vectorize the text using the newly created class:<pre class="source-code">
sample_text = train_dataset[0]["text"]
vectorizer = POS_vectorizer(small_model)
vector = vectorizer.vectorize(sample_text)</pre></li>				<li class="calibre14">Let’s print the text and the vector:<pre class="source-code">
print(sample_text)
print(vector)</pre><p class="calibre3">The result should look like this. We can see that the vector correctly counts the parts of speech. For example, there are five punctuation marks (two quotes, one comma, one dot, and one dash):</p><pre class="source-code">the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .
[38, 3, 8, 3, 4, 1, 3, 1, 0, 5]</pre></li>				<li class="calibre14">We will now prepare the data for training our classifier. We first import the <strong class="source-inline1">pandas</strong> and the <strong class="source-inline1">numpy</strong> packages and then create two dataframes, one for training and the other one for testing. In each dataset, we create a new column called <strong class="source-inline1">vector</strong> that contains the vector for this piece of text. We use the <strong class="source-inline1">apply</strong> method to turn the text into vectors and store them in the new column. In this method, we pass in a lambda function that takes a piece of text and applies the <strong class="source-inline1">vectorize</strong> method of the <strong class="source-inline1">POS_vectorizer</strong> class to that piece of text. We then turn the vector and the label columns into <strong class="source-inline1">numpy</strong> arrays to have the data in the right format for the classifier. We use the <strong class="source-inline1">np.stack</strong> method for the vector, since it’s <a id="_idIndexMarker102" class="calibre6 pcalibre pcalibre1"/>already a list, and the <strong class="source-inline1">to_numpy</strong> method for the review labels, since they are just numbers:<pre class="source-code">
import pandas as pd
import numpy as np
train_df = train_dataset.to_pandas()
train_df.sample(frac=1)
test_df = test_dataset.to_pandas()
train_df["vector"] = train_df["text"].apply(
    lambda x: vectorizer.vectorize(x))
test_df["vector"] = test_df["text"].apply(
    lambda x: vectorizer.vectorize(x))
X_train = np.stack(train_df["vector"].values, axis=0)
X_test = np.stack(test_df["vector"].values, axis=0)
y_train = train_df["label"].to_numpy()
y_test = test_df["label"].to_numpy()</pre></li>				<li class="calibre14">Now, we will train the classifier. We will choose the logistic regression algorithm, since it is one of the simplest algorithms, as well as one of the fastest. First, we import the <strong class="source-inline1">LogisticRegression</strong> class and the <strong class="source-inline1">classification_report</strong> methods from <strong class="source-inline1">sklearn</strong>. Then, we create the <strong class="source-inline1">LogisticRegression</strong> object, and finally, train it on the data from the previous step:<pre class="source-code">
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
clf = LogisticRegression(C=0.1)
clf = clf.fit(X_train, y_train)</pre></li>				<li class="calibre14">We can test the classifier on the test data by applying the <strong class="source-inline1">predict</strong> method to the vectors in the test data and print out the classification report. We can see that the overall accuracy is<a id="_idIndexMarker103" class="calibre6 pcalibre pcalibre1"/><a id="_idIndexMarker104" class="calibre6 pcalibre pcalibre1"/> low, slightly above chance. This is because the vector representation we used is very crude. We will use other vectors in the next sections and see how they affect the classifier results:<pre class="source-code">
test_df["prediction"] = test_df["vector"].apply(
    lambda x: clf.predict([x])[0])
print(classification_report(test_df["label"], 
    test_df["prediction"]))</pre><p class="calibre3">The output should be similar to this:</p><pre class="source-code">              precision    recall  f1-score   support
           0       0.59      0.54      0.56       160
           1       0.57      0.62      0.60       160
    accuracy                           0.58       320
   macro avg       0.58      0.58      0.58       320
weighted avg       0.58      0.58      0.58       320</pre></li>			</ol>
			<h2 id="_idParaDest-72" class="calibre5"><a id="_idTextAnchor074" class="calibre6 pcalibre pcalibre1"/>There’s more…</h2>
			<p class="calibre3">We will now turn the preceding code into several functions so that we can only vary the vectorizer being used in the construction of the dataset. The resulting file is located at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/util_simple_classifier.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/util_simple_classifier.ipynb</a>. The resulting code will look as follows:</p>
			<ol class="calibre13">
				<li class="calibre14">Import the necessary packages:<pre class="source-code">
from datasets import load_dataset
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report</pre></li>				<li class="calibre14">Define the function that will<a id="_idIndexMarker105" class="calibre6 pcalibre pcalibre1"/> create and return the training and test dataframes. It will create them from the Rotten Tomatoes dataset from Hugging Face:<pre class="source-code">
def load_train_test_dataset_pd():
    train_dataset = load_dataset("rotten_tomatoes",
        split="train[:15%]+train[-15%:]")
    test_dataset = load_dataset("rotten_tomatoes",
        split="test[:15%]+test[-15%:]")
    train_df = train_dataset.to_pandas()
    train_df.sample(frac=1)
    test_df = test_dataset.to_pandas()
    return (train_df, test_df)</pre></li>				<li class="calibre14">This function takes the dataframes and the <strong class="source-inline1">vectorize</strong> method and creates the <strong class="source-inline1">numpy</strong> arrays for the training and test data. This will allow us to train the logistic regression classifier using the created vectors:<pre class="source-code">
def create_train_test_data(train_df, test_df, vectorize):
    train_df["vector"] = train_df["text"].apply(
        lambda x: vectorize(x))
    test_df["vector"] = test_df["text"].apply(
        lambda x: vectorize(x))
    X_train = np.stack(train_df["vector"].values, axis=0)
    X_test = np.stack(test_df["vector"].values, axis=0)
    y_train = train_df["label"].to_numpy()
    y_test = test_df["label"].to_numpy()
    return (X_train, X_test, y_train, y_test)</pre></li>				<li class="calibre14">This function trains a <a id="_idIndexMarker106" class="calibre6 pcalibre pcalibre1"/>logistic regression classifier on the given training data:<pre class="source-code">
def train_classifier(X_train, y_train):
    clf = LogisticRegression(C=0.1)
    clf = clf.fit(X_train, y_train)
    return clf</pre></li>				<li class="calibre14">This final function takes in the test data and the trained classifier and prints out the classification report:<pre class="source-code">
def test_classifier(test_df, clf):
    test_df["prediction"] = test_df["vector"].apply(
        lambda x: clf.predict([x])[0])
    print(classification_report(test_df["label"],         test_df["prediction"]))</pre></li>			</ol>
			<p class="calibre3">In each succeeding section that demonstrates a new vectorizing method, we will use this file to pre-load the necessary functions to test the classification result. This will allow us to evaluate the different vectorizing methods. We will only vary the vectorizer while keeping the classifier the same. When the classifier performs better, it reflects how well the underlying vectorizer represents the text.</p>
			<h1 id="_idParaDest-73" class="calibre7"><a id="_idTextAnchor075" class="calibre6 pcalibre pcalibre1"/>Putting documents into a bag of words</h1>
			<p class="calibre3">A <strong class="bold">bag of words</strong> is the simplest way of representing a text. We treat our text as a collection of <em class="italic">documents</em>, where documents are anything from sentences to scientific articles to blog posts or whole books. Since we usually compare different documents to each other or use them in a larger <a id="_idIndexMarker107" class="calibre6 pcalibre pcalibre1"/>context of other documents, we work with a collection of documents, not just a single document.</p>
			<p class="calibre3">The bag of words method uses a “training” text that provides it with a list of words that it should consider. When encoding new sentences, it counts the number of occurrences each word makes in the document, and the final vector includes those counts for each word in the vocabulary. This representation can then be fed into a machine learning algorithm.</p>
			<p class="calibre3">The reason this vectorizing method is called a <em class="italic">bag of words</em> is that it does not take into account the relationships of words between themselves and only counts the number of occurrences of each word. The decision on what represents a document lies with the engineer and, in many cases, will be obvious. For example, if you are working on classifying tweets that belong to a particular topic, a single tweet will be your document. If, conversely, you would like to find out which chapters of a book are most similar to a book you already have, then chapters are documents.</p>
			<p class="calibre3">In this recipe, we will create a bag of words for the Rotten Tomatoes reviews. Our documents will be the reviews. We then test the encoding using a bag of words by building a logistic regression classifier, using code from the previous recipe.</p>
			<h2 id="_idParaDest-74" class="calibre5"><a id="_idTextAnchor076" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">For this recipe, we will use the <code>CountVectorizer</code> class from the <code>sklearn</code> package. It is included in the <code>poetry</code> environment. The <code>CountVectorizer</code> class is specifically designed to count the number of occurrences of each word in a text.</p>
			<h2 id="_idParaDest-75" class="calibre5"><a id="_idTextAnchor077" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">Our code will take a set of documents – in this case, reviews – and represent them as a matrix of vectors. We will use the Rotten Tomatoes reviews dataset from Hugging Face for this task:</p>
			<ol class="calibre13">
				<li class="calibre14">Run the simple classifier <strong class="source-inline1">utility</strong> file, and then import the <strong class="source-inline1">CountVectorizer</strong> object and the <strong class="source-inline1">sys</strong> package. We will need the <strong class="source-inline1">sys</strong> package to change the <a id="_idIndexMarker108" class="calibre6 pcalibre pcalibre1"/>printing options:<pre class="source-code">
%run -i "../util/util_simple_classifier.ipynb"
from sklearn.feature_extraction.text import CountVectorizer
import sys</pre></li>				<li class="calibre14">Load the training and testing dataframes by using the function from the <strong class="source-inline1">util_simple_classifier.ipynb</strong> file. We created this function in the previous recipe, <em class="italic">Creating a simple classifier</em>. The function loads 15% of the Rotten Tomatoes dataset into a <strong class="source-inline1">pandas</strong> dataframe and randomizes its order. It might take a few minutes to run:<pre class="source-code">
(train_df, test_df) = load_train_test_dataset_pd()</pre></li>				<li class="calibre14">Create the vectorizer, fit it on the training data, and print out the resulting matrix. We will use the <strong class="source-inline1">max_df</strong> parameter to specify which words should be used as stop words. In this case, we specify that words that appear in more than 40% of the documents should be ignored when constructing the vectorizer. You should experiment and see exactly which value of <strong class="source-inline1">max_df</strong> would suit your use case. We then fit the vectorizer on the <strong class="source-inline1">text</strong> column of the <strong class="source-inline1">train_df</strong> dataframe:<pre class="source-code">
vectorizer = CountVectorizer(max_df=0.4)
X = vectorizer.fit_transform(train_df["text"])
print(X)</pre><p class="calibre3">The resulting matrix is a <code>scipy.sparse._csr.csr_matrix</code> object, and the beginning of its printout looks like this. The format of a sparse matrix is <code>(row, column) value</code>. In our case, this means (the document index, word index) followed by the frequency. In our example, the first review, which is the first document, is document number <code>0</code>, and it contains words with indices <code>6578</code>, <code>4219</code>, and others. The frequencies of these words are <code>1</code> and <code>2</code>, respectively.</p><pre class="source-code">  (0, 6578)  1
  (0, 4219)  1
  (0, 2106)  1
  (0, 8000)  2
  (0, 717)  1
  (0, 42)  1
  (0, 1280)  1
  (0, 5260)  1
  (0, 1607)  1
  (0, 7889)  1
  (0, 3630)  1
…</pre></li>				<li class="calibre14">In most cases, we use a different format to represent vectors, a dense matrix that is easier to use in practice. Instead of specifying rows and columns with numbers, they are<a id="_idIndexMarker109" class="calibre6 pcalibre pcalibre1"/> inferred from the position of the value. We will now create a dense matrix and print it:<pre class="source-code">
dense_matrix = X.todense()
print(dense_matrix)</pre><p class="calibre3">The resulting matrix is a NumPy matrix object, where each review is a vector. You see that most values in the matrix are zeroes, as expected, since each review only uses a handful of words, while the vector collects counts for each word in the vocabulary, or each word in all of the reviews. Any words that are not in the vectorizer’s vocabulary will not be included in the vector:</p><pre class="source-code">[[0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]</pre></li>				<li class="calibre14">We can see all the words used in the document set and the length of the vocabulary. This can <a id="_idIndexMarker110" class="calibre6 pcalibre pcalibre1"/>be used as a sanity check and to see whether there are any irregularities in the vocabulary:<pre class="source-code">
print(vectorizer.get_feature_names_out())
print(len(vectorizer.get_feature_names_out()))</pre><p class="calibre3">The result will be as follows. If you want to see the full, non-truncated list, use the <code>set_printoptions</code> function used in <em class="italic">step 8</em>:</p><pre class="source-code">['10' '100' '101' ... 'zone' 'ótimo' 'últimos']
8856</pre></li>				<li class="calibre14">We can also see all the stop words used by the vectorizer:<pre class="source-code">
print(vectorizer.stop_words_)</pre><p class="calibre3">The result is three words, <code>and</code>, <code>the</code>, and <code>of</code>, that appear in more than 40% of reviews:</p><pre class="source-code">{'and', 'the', 'of'}</pre></li>				<li class="calibre14">We can now also use the <strong class="source-inline1">CountVectorizer</strong> object to represent new reviews that were not in the original document set. This is done when we have a trained model and want to test it on new, unseen samples. We will use the first review in the test dataset. To get the first review in the test set, we will use the <strong class="source-inline1">pandas</strong> <strong class="source-inline1">iat</strong> function.<pre class="source-code">
first_review = test_df['text'].iat[0]
print(first_review)</pre><p class="calibre3">The first review looks as follows:</p><pre class="source-code">lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .</pre></li>				<li class="calibre14">Now, we will create both a sparse and a dense vector from the first review. The <strong class="source-inline1">transform</strong> method of the<a id="_idIndexMarker111" class="calibre6 pcalibre pcalibre1"/> vectorizer expects a list of strings, so we will create a list. We also set the <strong class="source-inline1">print</strong> option to print out the whole vector instead of just part of it:<pre class="source-code">
sparse_vector = vectorizer.transform([first_review])
print(sparse_vector)
dense_vector = sparse_vector.todense()
np.set_printoptions(threshold=sys.maxsize)
print(dense_vector)
np.set_printoptions(threshold=False)</pre><p class="calibre3">The dense vector is very long and is mostly zeroes, as expected:</p><pre class="source-code">  (0, 955)  1
  (0, 3968)  1
  (0, 4451)  1
  (0, 4562)  1
  (0, 4622)  1
  (0, 4688)  1
  (0, 4779)  1
  (0, 4792)  1
  (0, 5764)  1
  (0, 7547)  1
  (0, 7715)  1
  (0, 8000)  1
  (0, 8734)  1
[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
…]]</pre></li>				<li class="calibre14">We can use a different method to calculate stop words. Here, stop words are calculated by setting <a id="_idIndexMarker112" class="calibre6 pcalibre pcalibre1"/>an absolute threshold on the word frequency. In this case, we use all words whose frequency is lower than 300 across documents. You can see that the stop-word list is now larger.<pre class="source-code">
vectorizer = CountVectorizer(max_df=300)
X = vectorizer.fit_transform(train_df["text"])
print(vectorizer.stop_words_)</pre><p class="calibre3">The result will be as follows:</p><pre class="source-code">{'but', 'this', 'its', 'as', 'to', 'and', 'the', 'is', 'film', 'for', 'it', 'an', 'of', 'that', 'movie', 'with', 'in'}</pre></li>				<li class="calibre14">Finally, we can provide our own list of stop words to the vectorizer. These words will be ignored by the vectorizer and not represented in the vector. This is useful if you have very specific words you would like to ignore:<pre class="source-code">
vectorizer = CountVectorizer(stop_words=['the', 'this',
    'these', 'in', 'at', 'for'])
X = vectorizer.fit_transform(train_df["text"])</pre></li>				<li class="calibre14">We will now test the effect of this bag-of-words vectorizer on the simple classifier, using the functions we defined in the previous recipe. First, we create the vectorizer, specifying to use only words that appear in less than 80% of the documents. Then, we<a id="_idIndexMarker113" class="calibre6 pcalibre pcalibre1"/> load the training and test dataframes. We fit the vectorizer on the training set reviews. We create a vectorize function using the vectorizer and pass it on to the <strong class="source-inline1">create_train_test_data</strong> function, along with the training and test data frames. We then train the classifier and test it on the testing data. We can see that this vectorizing method gives us much better results than the simple part-of-speech count vector we used in the previous section:<pre class="source-code">
vectorizer = CountVectorizer(max_df=0.8)
(train_df, test_df) = load_train_test_dataset_pd()
X = vectorizer.fit_transform(train_df["text"])
vectorize = lambda x: vectorizer.transform([x]).toarray()[0]
(X_train, X_test, y_train, y_test) = create_train_test_data(
    train_df, test_df, vectorize)
clf = train_classifier(X_train, y_train)
test_classifier(test_df, clf)</pre><p class="calibre3">The result will be similar to this:</p><pre class="source-code">              precision    recall  f1-score   support
           0       0.74      0.72      0.73       160
           1       0.73      0.75      0.74       160
    accuracy                           0.74       320
   macro avg       0.74      0.74      0.74       320
weighted avg       0.74      0.74      0.74       320</pre></li>			</ol>
			<h1 id="_idParaDest-76" class="calibre7"><a id="_idTextAnchor078" class="calibre6 pcalibre pcalibre1"/>Constructing an N-gram model</h1>
			<p class="calibre3">Representing a document as a bag of words is useful, but semantics is about more than just words in isolation. To <a id="_idIndexMarker114" class="calibre6 pcalibre pcalibre1"/>capture word combinations, an <strong class="bold">n-gram model</strong> is useful. Its vocabulary consists of not just words but also word sequences, or <em class="italic">n</em>-grams.</p>
			<p class="calibre3">We will build a <strong class="bold">bigram model</strong> in this recipe, where<a id="_idIndexMarker115" class="calibre6 pcalibre pcalibre1"/> bigrams are sequences of two words.</p>
			<h2 id="_idParaDest-77" class="calibre5"><a id="_idTextAnchor079" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">The <code>CountVectorizer</code> class is very versatile and allows us to construct <em class="italic">n</em>-gram models. We will use it in this recipe and test it with a simple classifier.</p>
			<p class="calibre3">In this recipe, I make comparisons of the code and its results to the ones in the <em class="italic">Putting documents into a bag of words</em> recipe, since the two are very similar, but they have a few differing characteristics.</p>
			<h2 id="_idParaDest-78" class="calibre5"><a id="_idTextAnchor080" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<ol class="calibre13">
				<li class="calibre14">Run the simple classifier notebook and import the <strong class="source-inline1">CountVectorizer</strong> class:<pre class="source-code">
%run -i "../util/util_simple_classifier.ipynb"
from sklearn.feature_extraction.text import CountVectorizer</pre></li>				<li class="calibre14">Create the training and test dataframes using code from the <strong class="source-inline1">util_simple_classifier.ipynb</strong> notebook:<pre class="source-code">
(train_df, test_df) = load_train_test_dataset_pd()</pre></li>				<li class="calibre14">Create a new vectorizer class. In this case, we will use the <strong class="source-inline1">ngram_range</strong> argument. The <strong class="source-inline1">CountVectorizer</strong> class, when the <strong class="source-inline1">ngram_range</strong> argument is set, counts not only individual words but also word combinations, where the number of words in the combinations depends on the numbers provided to the <strong class="source-inline1">ngram_range</strong> argument. We provided <strong class="source-inline1">ngram_range=(1,2)</strong> as the argument, which means that the number of words in the combinations ranges from 1 to 2, so unigrams<a id="_idIndexMarker116" class="calibre6 pcalibre pcalibre1"/> and bigrams are counted:<pre class="source-code">
bigram_vectorizer = CountVectorizer(
    ngram_range=(1, 2), max_df=0.8)
X = bigram_vectorizer.fit_transform(train_df["text"])</pre></li>				<li class="calibre14">Print the vocabulary of the vectorizer and its length. As you can see, the length of the vocabulary is much larger than the length of the unigram vectorizer, since we use two-word combinations in addition to single words:<pre class="source-code">
print(bigram_vectorizer.get_feature_names_out())
print(len(bigram_vectorizer.get_feature_names_out()))</pre><p class="calibre3">The result should look like this:</p><pre class="source-code">['10' '10 inch' '10 set' ... 'ótimo esforço' 'últimos' 'últimos tiempos']
40552</pre></li>				<li class="calibre14">Now, we take the first review in the testing dataframe and get its dense vector. The result looks very similar to the vector output in the <em class="italic">Putting documents into a bag of words</em> recipe, with the only difference that now the output is longer, as it includes not just individual words but also bigrams, or sequences of two words:<pre class="source-code">
first_review = test_df['text'].iat[0]
dense_vector = bigram_vectorizer.transform(
    [first_review]).todense()
print(dense_vector)</pre><p class="calibre3">The printout looks like this:</p><pre class="source-code">[[0 0 0 ... 0 0 0]]</pre></li>				<li class="calibre14">Finally, we train a simple classifier using the new bigram vectorizer. The resulting accuracy is slightly worse than the accuracy of the classifier that uses a unigram vectorizer from the previous section. There are several possible reasons for this. One is that the vectors<a id="_idIndexMarker117" class="calibre6 pcalibre pcalibre1"/> are now much longer and still mostly zeroes. The other is that we can actually see that not all reviews are in English, so it is hard for the classifier to generalize the incoming data:<pre class="source-code">
vectorize = \
    lambda x: bigram_vectorizer.transform([x]).toarray()[0]
(X_train, X_test, y_train, y_test) = create_train_test_data(
    train_df, test_df, vectorize)
clf = train_classifier(X_train, y_train)
test_classifier(test_df, clf)</pre><p class="calibre3">The output will be as follows:</p><pre class="source-code">              precision    recall  f1-score   support
           0       0.72      0.75      0.73       160
           1       0.74      0.71      0.72       160
    accuracy                           0.73       320
   macro avg       0.73      0.73      0.73       320
weighted avg       0.73      0.73      0.73       320</pre></li>			</ol>
			<h2 id="_idParaDest-79" class="calibre5"><a id="_idTextAnchor081" class="calibre6 pcalibre pcalibre1"/>There’s more…</h2>
			<p class="calibre3">We can use trigrams, quadrigrams, and so on in the vectorizer by providing the corresponding tuple to the <code>ngram_range</code> argument. The downside of this is the ever-expanding vocabulary and the growth of sentence vectors, since each sentence vector has to have an entry for each word in the input vocabulary.</p>
			<p class="calibre3">It is also possible to<a id="_idIndexMarker118" class="calibre6 pcalibre pcalibre1"/> represent character <em class="italic">n</em>-grams using the <code>CountVectorizer</code> class. In this case, you would count the occurrence of character sequences instead of word sequences.</p>
			<h1 id="_idParaDest-80" class="calibre7"><a id="_idTextAnchor082" class="calibre6 pcalibre pcalibre1"/>Representing texts with TF-IDF</h1>
			<p class="calibre3">We can go one step further and use the TF-IDF algorithm to count words and <em class="italic">n</em>-grams in incoming documents. <strong class="bold">TF-IDF</strong> stands for <strong class="bold">term frequency-inverse document frequency</strong> and gives more weight to words that are unique to a document than to words that are frequent but repeated<a id="_idIndexMarker119" class="calibre6 pcalibre pcalibre1"/> throughout most documents. This allows us to give more weight to words uniquely characteristic <a id="_idIndexMarker120" class="calibre6 pcalibre pcalibre1"/>of particular documents.</p>
			<p class="calibre3">In this recipe, we will use a different type of vectorizer that can apply the TF-IDF algorithm to the input text and build a small classifier.</p>
			<h2 id="_idParaDest-81" class="calibre5"><a id="_idTextAnchor083" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will use the <code>TfidfVectorizer</code> class from the <code>sklearn</code> package. The features of the <code>TfidfVectorizer</code> class should be familiar from the two previous recipes, <em class="italic">Putting documents into a bag of words</em> and <em class="italic">Constructing an N-gram model</em>. We will again use the Rotten Tomatoes review dataset from Hugging Face.</p>
			<h2 id="_idParaDest-82" class="calibre5"><a id="_idTextAnchor084" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">Here are the steps to build and use the TF-IDF vectorizer:</p>
			<ol class="calibre13">
				<li class="calibre14">Run the small classifier notebook and import the <strong class="source-inline1">TfidfVectorizer</strong> class:<pre class="source-code">
%run -i "../util/util_simple_classifier.ipynb"
from sklearn.feature_extraction.text import TfidfVectorizer</pre></li>				<li class="calibre14">Create the training and test dataframes using the <strong class="source-inline1">load_train_test_dataset_pd()</strong> function:<pre class="source-code">
(train_df, test_df) = load_train_test_dataset_pd()</pre></li>				<li class="calibre14">Create the vectorizer <a id="_idIndexMarker121" class="calibre6 pcalibre pcalibre1"/>and fit it on the training text. We will use the <strong class="source-inline1">max_df</strong> parameter to <a id="_idIndexMarker122" class="calibre6 pcalibre pcalibre1"/>exclude stop words – in this case, words that are more frequent than 300:<pre class="source-code">
vectorizer = TfidfVectorizer(max_df=300)
vectorizer.fit(train_df["text"])</pre></li>				<li class="calibre14">To make sure the result makes sense, we will print the vectorizer vocabulary and its length. Since we are just using unigrams, the size of the vocabulary should be the same as the one in the bag-of-words recipe:<pre class="source-code">
print(vectorizer.get_feature_names_out())
print(len(vectorizer.get_feature_names_out()))</pre><p class="calibre3">The result should be as follows. The length of the vocabulary should be the same as the one we get in the bag-of-words recipe, since we are not using <em class="italic">n</em>-grams:</p><pre class="source-code">['10' '100' '101' ... 'zone' 'ótimo' 'últimos']
8842</pre></li>				<li class="calibre14">Now, let’s take the first review in the test dataframe and vectorizer it. We then print the dense vector. To learn more about the difference between sparse and dense vectors, see the <em class="italic">Putting documents into a bag of words</em> recipe. Note that the values in the vector are now floats and not integers. This is because the individual values are now ratios and not counts:<pre class="source-code">
first_review = test_df['text'].iat[0]
dense_vector = vectorizer.transform([first_review]).todense()
print(dense_vector)</pre><p class="calibre3">The result should be as follows:</p><pre class="source-code">[[0. 0. 0. ... 0. 0. 0.]]</pre></li>				<li class="calibre14">Now, let’s train the classifier. We<a id="_idIndexMarker123" class="calibre6 pcalibre pcalibre1"/> can see that the scores are slightly <a id="_idIndexMarker124" class="calibre6 pcalibre pcalibre1"/>higher than those for a bag-of-words classifier, both the unigram and <em class="italic">n</em>-gram versions:<pre class="source-code">
vectorize = lambda x: vectorizer.transform([x]).toarray()[0]
(X_train, X_test, y_train, y_test) = create_train_test_data(
    train_df, test_df, vectorize)
clf = train_classifier(X_train, y_train)
test_classifier(test_df, clf)</pre><p class="calibre3">The printout of the test scores will look similar to this:</p><pre class="source-code">              precision    recall  f1-score   support
           0       0.76      0.72      0.74       160
           1       0.74      0.78      0.76       160
    accuracy                           0.75       320
   macro avg       0.75      0.75      0.75       320
weighted avg       0.75      0.75      0.75       320</pre></li>			</ol>
			<h2 id="_idParaDest-83" class="calibre5"><a id="_idTextAnchor085" class="calibre6 pcalibre pcalibre1"/>How it works…</h2>
			<p class="calibre3">The <code>TfidfVectorizer</code> class works almost exactly like the <code>CountVectorizer</code> class, differing only in <a id="_idIndexMarker125" class="calibre6 pcalibre pcalibre1"/>the way the <strong class="bold">word frequencies</strong> are calculated, so most of the steps should be familiar. Word frequencies are calculated as follows. For each word, the overall frequency<a id="_idIndexMarker126" class="calibre6 pcalibre pcalibre1"/> is a product of the <strong class="bold">term frequency</strong> and the <strong class="bold">inverse document frequency</strong>. Term frequency is the number of times a word occurs in the <a id="_idIndexMarker127" class="calibre6 pcalibre pcalibre1"/>document. Inverse document frequency is the total number of documents divided by the number of documents where the word<a id="_idIndexMarker128" class="calibre6 pcalibre pcalibre1"/> occurs. Usually, these frequencies are<a id="_idIndexMarker129" class="calibre6 pcalibre pcalibre1"/> logarithmically scaled.</p>
			<p class="calibre3">This is done using the following formulas:</p>
			<p class="calibre3"><img src="img/1.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" class="calibre16"/></p>
			<p class="calibre3"><img src="img/2.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" class="calibre17"/></p>
			<p class="calibre3"><img src="img/3.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;I&lt;/mml:mi&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;mml:mi&gt;*&lt;/mml:mi&gt;&lt;mml:mi&gt;I&lt;/mml:mi&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:math&gt;" class="calibre18"/></p>
			<h2 id="_idParaDest-84" class="calibre5"><a id="_idTextAnchor086" class="calibre6 pcalibre pcalibre1"/>There’s more…</h2>
			<p class="calibre3">We can build <code>TfidfVectorizer</code> and use <code>[t, h, e, w, o, m, a, n, th, he, wo, om, ma, an, the, wom, oma, man]</code> set. In some experimental settings, models based on character <em class="italic">n</em>-grams perform better than word-based <em class="italic">n</em>-gram models.</p>
			<p class="calibre3">We will use the small Sherlock Holmes text file, <code>sherlock_holmes_1.txt</code>, found at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt</a>, and the same class, <code>TfidfVectorizer</code>. We will not need a tokenizer function or a stop-word list, since the unit of analysis is the character and not the word. The steps to create the vectorizer and analyze a sentence are as follows:</p>
			<ol class="calibre13">
				<li class="calibre14">Create a new vectorizer object that uses the <strong class="source-inline1">char_wb</strong> analyzer, and then fit it on the training text:<pre class="source-code">
tfidf_char_vectorizer = TfidfVectorizer(
    analyzer='char_wb', ngram_range=(1,5))
tfidf_char_vectorizer = tfidf_char_vectorizer.fit(
    train_df["text"])</pre></li>				<li class="calibre14">Print the vectorizer vocabulary and its length:<pre class="source-code">
print(list(tfidf_char_vectorizer.get_feature_names_out()))
print(len(tfidf_char_vectorizer.get_feature_names_out()))</pre><p class="calibre3">The partial result <a id="_idIndexMarker132" class="calibre6 pcalibre pcalibre1"/>will look like this:</p><pre class="source-code">[' ', ' !', ' ! ', ' "', ' " ', ' $', ' $5', ' $50', ' $50-', ' $9', ' $9 ', ' &amp;', ' &amp; ', " '", " ' ", " '5", " '50", " '50'", " '6", " '60", " '60s", " '7", " '70", " '70'", " '70s", " '[", " '[h", " '[ho", " 'a", " 'a ", " 'a'", " 'a' ", " 'ab", " 'aba", " 'ah", " 'ah ", " 'al", " 'alt", " 'an", " 'ana", " 'ar", " 'are", " 'b", " 'ba", " 'bar", " 'be", " 'bee", " 'bes", " 'bl", " 'blu", " 'br", " 'bra", " 'bu", " 'but", " 'c", " 'ch", " 'cha", " 'co", " 'co-", " 'com", " 'd", " 'di", " 'dif", " 'do", " 'dog", " 'du", " 'dum", " 'e", " 'ed", " 'edg", " 'em", " 'em ", " 'ep", " 'epi", " 'f", " 'fa", " 'fac", " 'fat", " 'fu", " 'fun", " 'g", " 'ga", " 'gar", " 'gi", " 'gir", " 'gr", " 'gra", " 'gu", " 'gue", " 'guy", " 'h", " 'ha", " 'hav", " 'ho", " 'hos", " 'how", " 'i", " 'i ", " 'if", " 'if ", " 'in", " 'in ", " 'is",…]
51270</pre></li>				<li class="calibre14">Create the <strong class="source-inline1">vectorize</strong> method using the new vectorizer, and then create the training and test data. Train the <a id="_idIndexMarker133" class="calibre6 pcalibre pcalibre1"/>classifier and then test it:<pre class="source-code">
vectorize = lambda x: tfidf_char_vectorizer.transform([
    x]).toarray()[0]
(X_train, X_test, y_train, y_test) = create_train_test_data(
    train_df, test_df, vectorize)
clf = train_classifier(X_train, y_train)
test_classifier(test_df, clf)</pre><p class="calibre3">The result will be similar to this:</p><pre class="source-code">              precision    recall  f1-score   support
           0       0.74      0.74      0.74       160
           1       0.74      0.74      0.74       160
    accuracy                           0.74       320
   macro avg       0.74      0.74      0.74       320
weighted avg       0.74      0.74      0.74       320</pre></li>			</ol>
			<h2 id="_idParaDest-85" class="calibre5"><a id="_idTextAnchor087" class="calibre6 pcalibre pcalibre1"/>See also</h2>
			<ul class="calibre15">
				<li class="calibre14">You can find out more <a id="_idIndexMarker134" class="calibre6 pcalibre pcalibre1"/>about term weighting at <a href="https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting" class="calibre6 pcalibre pcalibre1">https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting</a></li>
				<li class="calibre14">For more information<a id="_idIndexMarker135" class="calibre6 pcalibre pcalibre1"/> about <strong class="source-inline1">TfidfVectorizer</strong>, see <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" class="calibre6 pcalibre pcalibre1">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</a></li>
			</ul>
			<h1 id="_idParaDest-86" class="calibre7"><a id="_idTextAnchor088" class="calibre6 pcalibre pcalibre1"/>Using word embeddings</h1>
			<p class="calibre3">In this recipe, we will switch gears and learn how to represent <em class="italic">words</em> using word embeddings, which are powerful because they are a result of training a neural network that predicts a word from all other<a id="_idIndexMarker136" class="calibre6 pcalibre pcalibre1"/> words in the sentence. Embeddings are also vectors, but usually of a much smaller size, 200 or 300. The resulting vector embeddings are similar for words that occur in similar contexts. Similarity is usually measured by calculating the cosine of the angle between two vectors in the hyperplane, with 200 or 300 dimensions. We will use the embeddings to show these similarities.</p>
			<h2 id="_idParaDest-87" class="calibre5"><a id="_idTextAnchor089" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">In this recipe, we will use a pretrained <code>word2vec</code> model, which can be found at <a href="https://github.com/mmihaltz/word2vec-GoogleNews-vectors" class="calibre6 pcalibre pcalibre1">https://github.com/mmihaltz/word2vec-GoogleNews-vectors</a>. Download the model and unzip it in the data directory. You should now have a file with the <code>…/</code><code>data/GoogleNews-vectors-negative300.bin.gz</code> path.</p>
			<p class="calibre3">We will also use the <code>gensim</code> package to load and use the model. It should be installed within the <code>poetry</code> environment.</p>
			<p class="calibre3">The notebook is located at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.5_word_embeddings.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.5_word_embeddings.ipynb</a>.</p>
			<h2 id="_idParaDest-88" class="calibre5"><a id="_idTextAnchor090" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">We will load the model, demonstrate <a id="_idIndexMarker137" class="calibre6 pcalibre pcalibre1"/>some features of the <code>gensim</code> package, and then compute a sentence vector using the word embeddings:</p>
			<ol class="calibre13">
				<li class="calibre14">Run the simple classifier file:<pre class="source-code">
%run -i "../util/simple_classifier.ipynb"</pre></li>				<li class="calibre14">Import the <strong class="source-inline1">gensim</strong> package:<pre class="source-code">
import gensim</pre></li>				<li class="calibre14">Load the pretrained model. If you get an error at this step, make sure you have downloaded the model in the <strong class="source-inline1">data</strong> directory:<pre class="source-code">
model = gensim.models.KeyedVectors.load_word2vec_format(
    '../data/GoogleNews-vectors-negative300.bin.gz',
    binary=True)</pre></li>				<li class="calibre14">Using the pretrained model, we can now load individual word vectors. Here, we load the word vector for the word <em class="italic">king</em>. We have to lowercase it, since all the words in the model are lowercase. The result is a long vector that represents this word in the <strong class="source-inline1">word2vec</strong> model:<pre class="source-code">
vec_king = model['king']
print(vec_king)</pre><p class="calibre3">The result will be as<a id="_idIndexMarker138" class="calibre6 pcalibre pcalibre1"/> follows:</p><pre class="source-code">[ 1.25976562e-01  2.97851562e-02  8.60595703e-03  1.39648438e-01
 -2.56347656e-02 -3.61328125e-02  1.11816406e-01 -1.98242188e-01
  5.12695312e-02  3.63281250e-01 -2.42187500e-01 -3.02734375e-01
 -1.77734375e-01 -2.49023438e-02 -1.67968750e-01 -1.69921875e-01
  3.46679688e-02  5.21850586e-03  4.63867188e-02  1.28906250e-01
  1.36718750e-01  1.12792969e-01  5.95703125e-02  1.36718750e-01
  1.01074219e-01 -1.76757812e-01 -2.51953125e-01  5.98144531e-02
  3.41796875e-01 -3.11279297e-02  1.04492188e-01  6.17675781e-02  …]</pre></li>				<li class="calibre14">We can also get words that are most similar to a given word. For example, let’s print out the words most similar to <em class="italic">apple</em> and <em class="italic">tomato</em>. The output prints out the words that are the most similar (i.e., occur in similar contexts) and the similarity score. The score is the cosine distance between a pair of vectors – in this case, representing a pair of words. The larger the score, the more similar the two words. The results make sense, since the words most similar to <em class="italic">apple</em> are mostly fruits, and the words most similar to <em class="italic">tomato</em> are mostly vegetables:<pre class="source-code">
print(model.most_similar(['apple'], topn=15))
print(model.most_similar(['tomato'], topn=15))</pre><p class="calibre3">The result is as follows:</p><pre class="source-code">[('apples', 0.720359742641449), ('pear', 0.6450697183609009), ('fruit', 0.6410146355628967), ('berry', 0.6302295327186584), ('pears', 0.613396167755127), ('strawberry', 0.6058260798454285), ('peach', 0.6025872826576233), ('potato', 0.5960935354232788), ('grape', 0.5935863852500916), ('blueberry', 0.5866668224334717), ('cherries', 0.5784382820129395), ('mango', 0.5751855969429016), ('apricot', 0.5727777481079102), ('melon', 0.5719985365867615), ('almond', 0.5704829692840576)]
[('tomatoes', 0.8442263007164001), ('lettuce', 0.7069936990737915), ('asparagus', 0.7050934433937073), ('peaches', 0.6938520669937134), ('cherry_tomatoes', 0.6897529363632202), ('strawberry', 0.6888598799705505), ('strawberries', 0.6832595467567444), ('bell_peppers', 0.6813562512397766), ('potato', 0.6784172058105469), ('cantaloupe', 0.6780219078063965), ('celery', 0.675195574760437), ('onion', 0.6740139722824097), ('cucumbers', 0.6706333160400391), ('spinach', 0.6682621240615845), ('cauliflower', 0.6681587100028992)]</pre></li>				<li class="calibre14">In the next two steps, we compute a sentence vector by averaging all the word vectors in the sentence. One of the challenges of this method is representing words that are not<a id="_idIndexMarker139" class="calibre6 pcalibre pcalibre1"/> present in the model, and here, we simply skip such words. Let’s define a function that will take a sentence and a model and return a list of the sentence word vectors. Words that are not present in the model will return <strong class="source-inline1">KeyError</strong>, and in such a case, we catch the error and continue:<pre class="source-code">
def get_word_vectors(sentence, model):
    word_vectors = []
    for word in sentence:
        try:
            word_vector = model[word.lower()]
            word_vectors.append(word_vector)
        except KeyError:
            continue
    return word_vectors</pre></li>				<li class="calibre14">Let’s now define a function that will take the word vector list and compute the sentence vector. In order to compute the average, we represent the matrix as a <strong class="source-inline1">numpy</strong> array and use the <strong class="source-inline1">numpy</strong> <strong class="source-inline1">mean</strong> function to get the average vector:<pre class="source-code">
def get_sentence_vector(word_vectors):
    matrix = np.array(word_vectors)
    centroid = np.mean(matrix[:,:], axis=0)
    return centroid</pre></li>			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">Averaging the word vectors to get the sentence vector is only one way of approaching this task and is not without its problems. One other alternative is to train a <strong class="source-inline1">doc2vec</strong> model, where sentences, paragraphs, and whole documents can all be units instead of words.</p>
			<ol class="calibre13">
				<li value="8" class="calibre14">We can now test the<a id="_idIndexMarker140" class="calibre6 pcalibre pcalibre1"/> average word embedding as a vectorizer.  Our vectorizer takes in a string input, gets the word vectors for each word, and then returns the sentence vector that we compute in the <strong class="source-inline1">get_sentence_vector</strong> function. We then load the training and test data and create the datasets. We train the logistic regression classifier and test it:<pre class="source-code">
vectorize = lambda x: get_sentence_vector(
    get_word_vectors(x, model))
(train_df, test_df) = load_train_test_dataset_pd()
(X_train, X_test, y_train, y_test) = create_train_test_data(
    train_df, test_df, vectorize)
clf = train_classifier(X_train, y_train)
test_classifier(test_df, clf)</pre><p class="calibre3">We can see that the scores are much lower than those in previous sections. There might be several reasons <a id="_idIndexMarker141" class="calibre6 pcalibre pcalibre1"/>for this; one of them is that the <code>word2vec</code> model is English-only, and the data is multilingual. As an exercise, you can write a script to filter English-only reviews and see whether that improves the score:</p><pre class="source-code">              precision    recall  f1-score   support
           0       0.54      0.57      0.55       160
           1       0.54      0.51      0.53       160
    accuracy                           0.54       320
   macro avg       0.54      0.54      0.54       320
weighted avg       0.54      0.54      0.54       320</pre></li>			</ol>
			<h2 id="_idParaDest-89" class="calibre5"><a id="_idTextAnchor091" class="calibre6 pcalibre pcalibre1"/>There’s more…</h2>
			<p class="calibre3">There are some other fun things that <code>gensim</code> can do with a pretrained model. For example, it can find an outlier word from a list of words and find the word that is most similar to the given word from a list. Let’s look at these:</p>
			<ol class="calibre13">
				<li class="calibre14">Compile a list of words with one that doesn’t match, apply the <strong class="source-inline1">doesnt_match</strong> function to the list, and print the results:<pre class="source-code">
words = ['banana', 'apple', 'computer', 'strawberry']
print(model.doesnt_match(words))</pre><p class="calibre3">The result will be as follows:</p><pre class="source-code">computer</pre></li>				<li class="calibre14">Now, let’s find a word that’s most similar to another word.<pre class="source-code">
word = "cup"
words = ['glass', 'computer', 'pencil', 'watch']
print(model.most_similar_to_given(word, words))</pre><p class="calibre3">The result will be as follows:</p><pre class="source-code">glass</pre></li>			</ol>
			<h2 id="_idParaDest-90" class="calibre5"><a id="_idTextAnchor092" class="calibre6 pcalibre pcalibre1"/>See also</h2>
			<ul class="calibre15">
				<li class="calibre14">There are<a id="_idIndexMarker142" class="calibre6 pcalibre pcalibre1"/> many other pretrained models available, including some in other languages; see <a href="http://vectors.nlpl.eu/repository/" class="calibre6 pcalibre pcalibre1">http://vectors.nlpl.eu/repository/</a>.<p class="calibre3">Some pretrained models include part-of-speech information, which can be helpful when disambiguating <a id="_idIndexMarker143" class="calibre6 pcalibre pcalibre1"/>words. These models concatenate words with their <code>cat_NOUN</code>), so keep that in mind when using them.</p></li>
				<li class="calibre14">To learn more about the theory behind <strong class="source-inline1">word2vec</strong>, you can start here: <a href="https://jalammar.github.io/illustrated-word2vec/" class="calibre6 pcalibre pcalibre1">https://jalammar.github.io/illustrated-word2vec/</a>.</li>
			</ul>
			<h1 id="_idParaDest-91" class="calibre7"><a id="_idTextAnchor093" class="calibre6 pcalibre pcalibre1"/>Training your own embeddings model</h1>
			<p class="calibre3">We can now train our own <code>word2vec</code> model on a corpus. This model is a neural network that predicts a word <a id="_idIndexMarker144" class="calibre6 pcalibre pcalibre1"/>when given a sentence with words blanked out. The byproduct of the neural network training is the vector representation for each word in the training vocabulary. For this task, we will continue using the Rotten Tomatoes reviews. The dataset is not very large, so the results are not as good as they could be with a larger collection.</p>
			<h2 id="_idParaDest-92" class="calibre5"><a id="_idTextAnchor094" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will use the <code>gensim</code> package for this task. It should be installed as part of the <code>poetry</code> environment.</p>
			<h2 id="_idParaDest-93" class="calibre5"><a id="_idTextAnchor095" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">We will create the dataset <a id="_idIndexMarker145" class="calibre6 pcalibre pcalibre1"/>and then train the model on the data. We will then test how it performs:</p>
			<ol class="calibre13">
				<li class="calibre14">Import the necessary packages and functions:<pre class="source-code">
import gensim
from gensim.models import Word2Vec
from datasets import load_dataset
from gensim import utils</pre></li>				<li class="calibre14">Load the training data and check its length:<pre class="source-code">
train_dataset = load_dataset("rotten_tomatoes", split="train")
print(len(train_dataset))</pre><p class="calibre3">The result should be as follows:</p><pre class="source-code">8530</pre></li>				<li class="calibre14">Create the <strong class="source-inline1">RottenTomatoesCorpus</strong> class. The <strong class="source-inline1">word2vec</strong> training algorithm requires a class with a defined <strong class="source-inline1">__iter__</strong> function that allows you to iterate through the data, so that is why we need this class:<pre class="source-code">
class RottenTomatoesCorpus:
    def __init__(self, sentences):
        self.sentences = sentences
    def __iter__(self):
        for review in self.sentences:
            yield utils.simple_preprocess(
                gensim.parsing.preprocessing.remove_stopwords(
                    review))</pre></li>				<li class="calibre14">Create an instance of <strong class="source-inline1">RottenTomatoesCorpus</strong> using the loaded training dataset. Since <strong class="source-inline1">word2vec</strong> models are trained on text only (they are self-supervised models), we <a id="_idIndexMarker146" class="calibre6 pcalibre pcalibre1"/>don’t need the review score:<pre class="source-code">
sentences = train_dataset["text"]
corpus = RottenTomatoesCorpus(sentences)</pre></li>				<li class="calibre14">In this step, we initialize the <strong class="source-inline1">word2vec</strong> model, train it, and then save it to disk. The only required argument is the list of words; some of the other important ones are <strong class="source-inline1">min_count</strong>, <strong class="source-inline1">size</strong>, <strong class="source-inline1">window</strong>, and <strong class="source-inline1">workers</strong>. The <strong class="source-inline1">min_count</strong> parameter refers to the minimum number of times a word has to occur in the training corpus, the default being 5. The <strong class="source-inline1">size</strong> parameter sets the size of the word vector. <strong class="source-inline1">window</strong> restricts the maximum number of words between the predicted and current words in a sentence. <strong class="source-inline1">workers</strong> refers to the number of working threads; the more there are, the quicker the training will proceed. When training the model, the <strong class="source-inline1">epoch</strong> parameter will determine the number of training iterations the model will go through. After initializing the model object, we train it on our corpus for 100 epochs and, finally, save it to disk:<pre class="source-code">
model = Word2Vec(sentences=corpus, vector_size=100,
    window=5, min_count=1, workers=4)
model.train(corpus_iterable=corpus,
    total_examples=model.corpus_count, epochs=100)
model.save("../data/rotten_tomato_word2vec.model")</pre></li>				<li class="calibre14">Get 10 words similar to the word <em class="italic">movie</em>. The words <em class="italic">sequels</em> and <em class="italic">film</em> make sense with this word; the rest are not that related. This is due to the small size of the training corpus. The<a id="_idIndexMarker147" class="calibre6 pcalibre pcalibre1"/> words you get will be different, since the results are different every time the model is trained:<pre class="source-code">
w1 = "movie"
words = model.wv.most_similar(w1, topn=10)
print(words)</pre><p class="calibre3">This is a possible output:</p><pre class="source-code">[('sequels', 0.38357362151145935), ('film', 0.33577531576156616), ('stuffed', 0.2925359606742859), ('quirkily', 0.28789234161376953), ('convict', 0.2810690104961395), ('worse', 0.2789292335510254), ('churn', 0.27702808380126953), ('hellish', 0.27698105573654175), ('hey', 0.27566075325012207), ('happens', 0.27498629689216614)]</pre></li>			</ol>
			<h2 id="_idParaDest-94" class="calibre5"><a id="_idTextAnchor096" class="calibre6 pcalibre pcalibre1"/>There’s more…</h2>
			<p class="calibre3">There are tools to evaluate a <code>word2vec</code> model, although its creation is unsupervised. <code>gensim</code> comes with a file that lists word analogies, such as what <em class="italic">Athens</em> is to <em class="italic">Greece</em> being the same as what <em class="italic">Moscow</em> is to <em class="italic">Russia</em>. The <code>evaluate_word_analogies</code> function runs the analogies through the model and calculates how many were correct.</p>
			<p class="calibre3">Here is how to do this:</p>
			<ol class="calibre13">
				<li class="calibre14">Use the <strong class="source-inline1">evaluate_word_analogies</strong> function to evaluate our trained model. We need the <strong class="source-inline1">analogies</strong> file, which is available in the book GitHub repository at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/questions-words.txt" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/questions-words.txt</a><pre class="source-code">
(analogy_score, word_list) = model.wv.evaluate_word_analogies(
    '../data/questions-words.txt')
print(analogy_score)</pre><p class="calibre3">The result should be similar to this:</p><pre class="source-code">0.0015881418740074113</pre></li>				<li class="calibre14">Let’s now evaluate the pretrained model. These commands might take longer to run:<pre class="source-code">
pretrained_model = \
    gensim.models.KeyedVectors.load_word2vec_format(
        '../data/GoogleNews-vectors-negative300.bin.gz',
        binary=True)
(analogy_score, word_list) = \
    pretrained_model.evaluate_word_analogies(
        '../data/questions-words.txt')
print(analogy_score)</pre><p class="calibre3">The result should be similar to this:</p><pre class="source-code">0.7401448525607863</pre></li>				<li class="calibre14">We use the <strong class="source-inline1">evaluate_word_analogies</strong> function differently in the pretrained model <a id="_idIndexMarker148" class="calibre6 pcalibre pcalibre1"/>and our model case because they are different types. With the pretrained model, we just load the vectors (a <strong class="source-inline1">KeyedVectors</strong> class, where each word, represented by a key, is mapped to a vector), and our model is a full <strong class="source-inline1">word2vec</strong> model object. We can check the types by using these commands:<pre class="source-code">
print(type(pretrained_model))
print(type(model))</pre><p class="calibre3">The result will be as follows:</p><pre class="source-code">&lt;class 'gensim.models.keyedvectors.KeyedVectors'&gt;
&lt;class 'gensim.models.word2vec.Word2Vec'&gt;</pre><p class="calibre3">The pretrained model was trained on a much larger corpus and, predictably, performs better. You can also construct your own evaluation file with analogies that your data requires.</p></li>			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">Make sure your evaluation is based on the type of data that you are going to use in your application; otherwise, you risk having misleading evaluation results.</p>
			<h2 id="_idParaDest-95" class="calibre5"><a id="_idTextAnchor097" class="calibre6 pcalibre pcalibre1"/>See also</h2>
			<p class="calibre3">There is an additional way <a id="_idIndexMarker149" class="calibre6 pcalibre pcalibre1"/>of evaluating model performance, by comparing the similarity between word pairs assigned by a model to the human-assigned judgments. You can do this by using the <code>evaluate_word_pairs</code> function. See more at <a href="https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_pairs" class="calibre6 pcalibre pcalibre1">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_pairs</a>.</p>
			<h1 id="_idParaDest-96" class="calibre7"><a id="_idTextAnchor098" class="calibre6 pcalibre pcalibre1"/>Using BERT and OpenAI embeddings instead of 
word embeddings</h1>
			<p class="calibre3">Instead of word embeddings, we can use <strong class="bold">Bidirectional Encoder Representations from Transformer</strong> (<strong class="bold">BERT</strong>) embeddings. A BERT model, like word embeddings, is a pretrained model and<a id="_idIndexMarker150" class="calibre6 pcalibre pcalibre1"/> gives a vector representation, but it takes context into account and can<a id="_idIndexMarker151" class="calibre6 pcalibre pcalibre1"/> represent a whole sentence instead of individual words.</p>
			<h2 id="_idParaDest-97" class="calibre5"><a id="_idTextAnchor099" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">For this recipe, we can use the Hugging Face <code>sentence_transformers</code> package to represent sentences as vectors. We need <code>PyTorch</code>, which is installed as part of the <code>poetry</code> environment.</p>
			<p class="calibre3">To get the vectors, we will use the <code>all-MiniLM-L6-v2</code> model for this recipe.</p>
			<p class="calibre3">We can also use the embeddings from <a id="_idIndexMarker152" class="calibre6 pcalibre pcalibre1"/>OpenAI that come from their <strong class="bold">large language </strong><strong class="bold">models</strong> (<strong class="bold">LLMs</strong>).</p>
			<p class="calibre3">To use the OpenAI embeddings, you will need to create an account and get an API key from OpenAI. You can <a id="_idIndexMarker153" class="calibre6 pcalibre pcalibre1"/> create an account at <a href="https://platform.openai.com/signup" class="calibre6 pcalibre pcalibre1">https://platform.openai.com/signup</a>.</p>
			<p class="calibre3">The notebook is located at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.6_train_own_word2vec.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.6_train_own_word2vec.ipynb</a>.</p>
			<h2 id="_idParaDest-98" class="calibre5"><a id="_idTextAnchor100" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">Hugging Face code makes using<a id="_idIndexMarker154" class="calibre6 pcalibre pcalibre1"/> BERT very easy. The first time the code runs, it will download the necessary model, which might take some time. After the download, it’s just a matter of encoding the sentences using the model. We will test the simple classifier with these embeddings:</p>
			<ol class="calibre13">
				<li class="calibre14">Run the simple classifier notebook to import its functions:<pre class="source-code">
%run -i "../util/util_simple_classifier.ipynb"</pre></li>				<li class="calibre14">Import the <strong class="source-inline1">SentenceTransformer</strong> class:<pre class="source-code">
from sentence_transformers import SentenceTransformer</pre></li>				<li class="calibre14">Load the sentence transformer model, retrieve the embedding of the sentence <em class="italic">I love jazz</em>, and print it out.<pre class="source-code">
model = SentenceTransformer('all-MiniLM-L6-v2')
embedding = model.encode(["I love jazz"])
print(embedding)</pre><p class="calibre3">As we can see, it is a vector similar to the word embeddings vector from the previous recipe:</p><pre class="source-code">[[ 2.94217980e-03 -7.93536603e-02 -2.82228496e-02 -5.13779782e-02
  -6.44981042e-02  9.83557850e-02  1.09671958e-01 -3.26390602e-02
   4.96566631e-02  2.56580133e-02 -1.08482063e-01  1.88441798e-02
   2.70963665e-02 -3.80690470e-02  2.42502335e-02 -3.65605950e-03
   1.29364491e-01  4.32255343e-02 -6.64561391e-02 -6.93060979e-02
  -1.39410645e-01  4.36719768e-02 -7.85463024e-03  1.68625098e-02
  -1.01160072e-02  1.07926019e-02 -1.05814040e-02  2.57284809e-02
  -1.51516097e-02 -4.53920700e-02  7.12087378e-03  1.17573030e-01… ]]</pre></li>				<li class="calibre14">Now, we can test our classifier using the BERT embeddings. First, let’s define a function that will return a <a id="_idIndexMarker155" class="calibre6 pcalibre pcalibre1"/>sentence vector. This function takes the input text and a model. It then uses the model to encode the text and returns the resulting embedding. We<a id="_idIndexMarker156" class="calibre6 pcalibre pcalibre1"/>  need to pass in the text inside of a list to the <strong class="source-inline1">encode</strong> method, as it expects an iterable. Similarly, we return the first element of the result, since it returns a list of embeddings.<pre class="source-code">
def get_sentence_vector(text, model):
    sentence_embeddings = model.encode([text])
    return sentence_embeddings[0]</pre></li>				<li class="calibre14">Now, we define the <strong class="source-inline1">vectorize</strong> function, create the training and test data using the <strong class="source-inline1">load_train_test_dataset_pd</strong> function we created in the <em class="italic">Creating a simple classifier</em> recipe, train the classifier, and test it. We will time the dataset creation step, hence the inclusion of the <strong class="source-inline1">time</strong> package commands. We see that it takes about 11 seconds to vectorize the whole dataset (about 85,000 entries). We <a id="_idIndexMarker157" class="calibre6 pcalibre pcalibre1"/>then train the model and test it:<pre class="source-code">
import time
vectorize = lambda x: get_sentence_vector(x, model)
(train_df, test_df) = load_train_test_dataset_pd()
start = time.time()
(X_train, X_test, y_train, y_test) = create_train_test_data(
    train_df, test_df, vectorize)
print(f"BERT embeddings: {time.time() - start} s")
clf = train_classifier(X_train, y_train)
test_classifier(test_df, clf)</pre><p class="calibre3">The result is our best <a id="_idIndexMarker158" class="calibre6 pcalibre pcalibre1"/>one yet:</p><pre class="source-code">BERT embeddings: 11.410213232040405 s
              precision    recall  f1-score   support
           0       0.77      0.79      0.78       160
           1       0.79      0.76      0.77       160
    accuracy                           0.78       320
   macro avg       0.78      0.78      0.78       320
weighted avg       0.78      0.78      0.78       320</pre></li>			</ol>
			<h2 id="_idParaDest-99" class="calibre5"><a id="_idTextAnchor101" class="calibre6 pcalibre pcalibre1"/>There’s more…</h2>
			<p class="calibre3">We can now use the<a id="_idIndexMarker159" class="calibre6 pcalibre pcalibre1"/>  OpenAI embeddings to see how they perform:</p>
			<ol class="calibre13">
				<li class="calibre14">Import the <strong class="source-inline1">openai</strong> package and assign the API key:<pre class="source-code">
import openai
openai.api_key = OPEN_AI_KEY</pre></li>				<li class="calibre14">Assign the model we are going to use, the sentence and create the embedding. The model that we<a id="_idIndexMarker160" class="calibre6 pcalibre pcalibre1"/> will use is specifically an embeddings model, so it returns an embeddings vector for a text input:<pre class="source-code">
model = "text-embedding-ada-002"
text = "I love jazz"
response = openai.Embedding.create(
    input=text,
    model=model
)
embeddings = response['data'][0]['embedding']
print(embeddings)</pre><p class="calibre3">The partial result will be as follows:</p><pre class="source-code">[-0.028350897133350372, -0.011136125773191452, -0.0021299426443874836, -0.014453398995101452, -0.012048527598381042, 0.018223850056529045, -0.010247894562780857, -0.01806674897670746, -0.014308380894362926, 0.0007220656843855977, -9.998268797062337e-05, 0.010078707709908485,…]</pre></li>				<li class="calibre14">Let’s now test our classifier using the OpenAI embeddings. This is the function that will return a sentence vector:<pre class="source-code">
def get_sentence_vector(text, model):
    text = "I love jazz"
    response = openai.Embedding.create(
        input=text,
        model=model
    )
    embeddings = response['data'][0]['embedding']
    return embeddings</pre></li>				<li class="calibre14">Now, define the <strong class="source-inline1">vectorize</strong> function, create<a id="_idIndexMarker161" class="calibre6 pcalibre pcalibre1"/> the training and test data, train the classifier, and<a id="_idIndexMarker162" class="calibre6 pcalibre pcalibre1"/>  test it. We will time the vectorizing step:<pre class="source-code">
import time
vectorize = lambda x: get_sentence_vector(x, model)
(train_df, test_df) = load_train_test_dataset_pd()
start = time.time()
(X_train, X_test, y_train, y_test) = create_train_test_data(
    train_df, test_df, vectorize)
print(f"OpenAI embeddings: {time.time() - start} s")
clf = train_classifier(X_train, y_train)
test_classifier(test_df, clf)</pre><p class="calibre3">The result will be as follows:</p><pre class="source-code">OpenAI embeddings: 704.3250799179077 s
              precision    recall  f1-score   support
           0       0.49      0.82      0.62       160
           1       0.47      0.16      0.23       160
    accuracy                           0.49       320
   macro avg       0.48      0.49      0.43       320
weighted avg       0.48      0.49      0.43       320</pre><p class="calibre3">Note that the result is quite poor in terms of the score, and it takes more than 10 minutes to process the whole dataset. Here, we only use the LLM embeddings and then <a id="_idIndexMarker163" class="calibre6 pcalibre pcalibre1"/> train a logistic regression classifier on <a id="_idIndexMarker164" class="calibre6 pcalibre pcalibre1"/>those embeddings. This is different from using the LLM itself to do the classification.</p></li>			</ol>
			<h2 id="_idParaDest-100" class="calibre5"><a id="_idTextAnchor102" class="calibre6 pcalibre pcalibre1"/>See also</h2>
			<p class="calibre3">For more pretrained models, see <a href="https://www.sbert.net/docs/pretrained_models.html" class="calibre6 pcalibre pcalibre1">https://www.sbert.net/docs/pretrained_models.html</a>.</p>
			<h1 id="_idParaDest-101" class="calibre7"><a id="_idTextAnchor103" class="calibre6 pcalibre pcalibre1"/>Retrieval augmented generation (RAG)</h1>
			<p class="calibre3">In this recipe, we will see vector <a id="_idIndexMarker165" class="calibre6 pcalibre pcalibre1"/>embeddings in action. RAG is a popular method of working with LLMs. Since these models are pretrained on widely available internet data, they do not have access to our personal data, and we cannot use the model as it is to ask questions about it. A way to overcome this is to use vector embeddings to represent our data. Then, we can compute cosine similarity between our data and the question and include the most similar piece of our data, together with the question – hence the name “retrieval augmented generation,” since we first retrieve relevant data by using cosine similarity and then generate text using the LLM.</p>
			<h2 id="_idParaDest-102" class="calibre5"><a id="_idTextAnchor104" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will use an IMDB dataset <a id="_idIndexMarker166" class="calibre6 pcalibre pcalibre1"/>from <strong class="bold">Kaggle</strong>, which can be downloaded from <a href="https://www.kaggle.com/PromptCloudHQ/imdb-data" class="calibre6 pcalibre pcalibre1">https://www.kaggle.com/PromptCloudHQ/imdb-data</a> and is also included in the book GitHub repo at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/IMDB-Movie-Data.csv" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/IMDB-Movie-Data.csv</a>. Download the dataset and unzip the CSV file.</p>
			<p class="calibre3">We will also use the OpenAI embeddings, as well as the <code>llama_index</code> package, which is included <a id="_idIndexMarker167" class="calibre6 pcalibre pcalibre1"/>within the <code>poetry</code> environment.</p>
			<p class="calibre3">The notebook is located at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.9_vector_search.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter03/3.9_vector_search.ipynb</a>.</p>
			<h2 id="_idParaDest-103" class="calibre5"><a id="_idTextAnchor105" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">We will load the IMDB dataset and then create a vector store, using its first 10 entries. We will then use the <code>llama_index</code> package to query the vector store:</p>
			<ol class="calibre13">
				<li class="calibre14">Run the file <strong class="source-inline1">utilities</strong> notebook:<pre class="source-code">
%run -i "../util/file_utils.ipynb"</pre></li>				<li class="calibre14">Import the necessary classes and packages:<pre class="source-code">
import csv
import openai
from llama_index import VectorStoreIndex
from llama_index import Document
openai.api_key = OPEN_AI_KEY</pre></li>				<li class="calibre14">Read in the CSV data. We will skip the first header row of the data:<pre class="source-code">
with open('../data/IMDB-Movie-Data.csv') as f:
    reader = csv.reader(f)
    data = list(reader)
    movies = data[1:]</pre></li>				<li class="calibre14">In this step, we use the first 10 rows of the data we just read in to first create a list of <strong class="source-inline1">Document</strong> objects, and then a <strong class="source-inline1">VectorStoreIndex</strong> object with these <strong class="source-inline1">Document</strong> objects. An index is an object used for search, where each record contains certain information. A vector store index stores metadata as well as the vector representation of each record. For each movie, we assign the description as the text that will be <a id="_idIndexMarker168" class="calibre6 pcalibre pcalibre1"/>embedded and the rest as metadata. We print out <strong class="source-inline1">document</strong> objects and can see that a unique ID has been assigned to each:<pre class="source-code">
documents = []
for movie in movies[0:10]:
    doc_id = movie[0]
    title = movie[1]
    genres = movie[2].split(",")
    description = movie[3]
    director = movie[4]
    actors = movie[5].split(",")
    year = movie[6]
    duration = movie[7]
    rating = movie[8]
    revenue = movie[10]
    document = Document(
        text=description,
        metadata={
            "title": title,
            "genres": genres,
            "director": director,
            "actors": actors,
            "year": year,
            "duration": duration,
            "rating": rating,
            "revenue": revenue
        }
    )
    print(document)
    documents.append(document)
index = VectorStoreIndex.from_documents(documents)</pre><p class="calibre3">The partial output will look<a id="_idIndexMarker169" class="calibre6 pcalibre pcalibre1"/> like this:</p><pre class="source-code">id_='6e1ef633-f10b-44e3-9b77-f5f7b08dcedd' embedding=None metadata={'title': 'Guardians of the Galaxy', 'genres': ['Action', 'Adventure', 'Sci-Fi'], 'director': 'James Gunn', 'actors': ['Chris Pratt', ' Vin Diesel', ' Bradley Cooper', ' Zoe Saldana'], 'year': '2014', 'duration': '121', 'rating': '8.1', 'revenue': '333.13'} excluded_embed_metadata_keys=[] excluded_llm_metadata_keys=[] relationships={} hash='e18bdce3a36c69d8c1e55a7eb56f05162c68c97151cbaf40
91814ae3df42dfe8' text='A group of intergalactic criminals are forced to work together to stop a fanatical warrior from taking control of the universe.' start_char_idx=None end_char_idx=None text_template='{metadata_str}\n\n{content}' metadata_template='{key}: {value}' metadata_seperator='\n'</pre></li>				<li class="calibre14">Create the query engine from the index we just created. The query engine will allow us to send in questions about the documents loaded in the index:<pre class="source-code">
query_engine = index.as_query_engine()</pre></li>				<li class="calibre14">Use the engine to <a id="_idIndexMarker170" class="calibre6 pcalibre pcalibre1"/>answer a question:<pre class="source-code">
response = query_engine.query("""Which movies talk about something gigantic?""")
print(response.response)
The answer seems to make sense grammatically, and arguably the Great Wall of China is gigantic. However, it is not clear what is gigantic in the movie Prometheus. So here we have a partially correct answer. The Great Wall and Prometheus both talk about something gigantic. In The Great Wall, the protagonists become embroiled in the defense of the Great Wall of China against a horde of monstrous creatures. In Prometheus, the protagonists find a structure on a distant moon.</pre></li>			</ol>
		</div>
	</body></html>