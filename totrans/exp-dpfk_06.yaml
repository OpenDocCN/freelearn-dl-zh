- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Training a Deepfake Model
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练Deepfake模型
- en: Training a deepfake model is the most important part of creating a deepfake.
    It is where the AI actually learns about the faces from your data and where the
    most interesting neural network operations take place.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 训练deepfake模型是创建deepfake最重要的部分。这是AI从你的数据中学习面部的地方，也是最有趣的神经网络操作发生的地方。
- en: In this chapter, we’ll look into the training code and the code that actually
    creates the AI models. We’ll look at the submodules of the neural network and
    how they’re put together to create a complete neural network. Then we’ll go over
    everything needed to train the network and end up with a model ready to swap two
    faces.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入研究训练代码以及实际创建AI模型的代码。我们将查看神经网络子模块以及它们如何组合成一个完整的神经网络。然后我们将介绍训练网络所需的一切，最终得到一个可以交换两个面部图像的模型。
- en: 'We’ll cover the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Understanding convolutional layers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解卷积层
- en: Getting hands-on with AI
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亲身体验AI
- en: Exploring the training code
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索训练代码
- en: By the end of this chapter, we’ll have designed our neural networks and built
    a training pipeline capable of teaching them to swap faces.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我们将设计我们的神经网络，并构建一个能够教会它们交换面部图像的训练管道。
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To run any of the code in this chapter, we recommend downloading our official
    repository at [https://github.com/PacktPublishing/Exploring-Deepfakes](https://github.com/PacktPublishing/Exploring-Deepfakes)
    and following the instructions for setting up an Anaconda environment with all
    of the required libraries.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本章中的任何代码，我们建议您下载我们的官方仓库[https://github.com/PacktPublishing/Exploring-Deepfakes](https://github.com/PacktPublishing/Exploring-Deepfakes)，并按照设置Anaconda环境所需的库的说明进行操作。
- en: In order to train, you must have two sets of extracted faces. You’ll feed both
    sets into the model and it will learn both faces separately. It’s important that
    you get sufficient data for both faces and that there be a good variety. If you’re
    in doubt, please check [*Chapter 3*](B17535_03.xhtml#_idTextAnchor054), *Mastering
    Data*, for advice on getting the best data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行训练，你必须拥有两组提取的面部图像。你需要将这两组图像都输入到模型中，它将分别学习这两张脸。确保你为这两张脸都获得了足够的数据，并且种类丰富是很重要的。如果你有疑问，请查阅[*第3章*](B17535_03.xhtml#_idTextAnchor054)，*掌握数据*，以获取获取最佳数据的建议。
- en: Understanding convolutional layers
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解卷积层
- en: In this chapter, we’ll finally get into the meat of the neural networks behind
    deepfakes. A big part of how networks such as these work is a technique called
    convolutional layers. These layers are extremely important in effectively working
    with image data and form an important cornerstone of most neural networks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将最终深入探讨深伪影背后的神经网络的核心。这些网络如何工作的很大一部分是一个称为卷积层的技术。这些层在有效处理图像数据方面极为重要，并且是大多数神经网络的重要基石。
- en: A **convolution** is an operation that changes the shape of an object. In the
    case of neural networks, we use **convolutional layers**, which iterate a convolution
    over a matrix and create a new (generally smaller) output matrix. Convolutions
    are a way to reduce an image in size while simultaneously searching for patterns.
    The more convolutional layers you stack, the more complicated the patterns that
    can be encoded from the original image.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积**是一种改变物体形状的操作。在神经网络的情况下，我们使用**卷积层**，它在一个矩阵上迭代卷积并创建一个新的（通常是较小的）输出矩阵。卷积是一种在同时搜索模式的同时减小图像尺寸的方法。你堆叠的卷积层越多，从原始图像中可以编码的复杂模式就越多。'
- en: '![Figure 6.1 – An example of a convolution downscaling a full image](img/B17535_06_001.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – 一个卷积缩小完整图像的示例](img/B17535_06_001.jpg)'
- en: Figure 6.1 – An example of a convolution downscaling a full image
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 一个卷积缩小完整图像的示例
- en: There are several details that define a convolutional layer. The first is dimensionality.
    In our case, we’re using 2D convolutions, which work in 2D space. This means that
    the convolution works on the x and y axes for each of the channels. This means
    for the first convolution, each color channel is processed separately.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 定义卷积层有几个细节。第一个是维度。在我们的例子中，我们使用2D卷积，它在2D空间中工作。这意味着卷积在每个通道的x和y轴上操作。这意味着对于第一次卷积，每个颜色通道都是单独处理的。
- en: Next is the **kernel**, which defines how big an area each convolution takes
    into account. The amount of kernels going across affects the output as well. For
    example, if you had a matrix of 3x9 and kernel size of 3x3, you’d get a 1x3 matrix
    output.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是**内核**，它定义了每个卷积考虑的区域大小。内核的数量也会影响输出。例如，如果你有一个3x9的矩阵和3x3的内核大小，你会得到一个1x3的矩阵输出。
- en: '![Figure 6.2 – An example of a 3x3 convolutional process turning a 3x9 into
    a 1x3 matrix output](img/B17535_06_002.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 – 将3x9转换为1x3矩阵输出的3x3卷积过程的示例](img/B17535_06_002.jpg)'
- en: Figure 6.2 – An example of a 3x3 convolutional process turning a 3x9 into a
    1x3 matrix output
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 将3x9转换为1x3矩阵输出的3x3卷积过程的示例
- en: Next is **stride** which defines how big a step each iteration of the convolution
    takes as it travels the matrix. A stride of 2, for example, would make our 3x3
    kernel overlap by a single **entry** of the matrix. Stride is duplicated in every
    dimension, so if you extended the example input matrix to the left or right, you’d
    also get overlap in that direction.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是**步长**，它定义了卷积在矩阵中移动时每次迭代的步长大小。例如，步长为2会使我们的3x3内核重叠矩阵的一个**条目**。步长在每个维度上都是重复的，所以如果你将示例输入矩阵向左或向右扩展，你也会得到那个方向的重叠。
- en: '![Figure 6.3 – An example of a stride smaller than the kernel size causing
    the elements in between to be shared on the output](img/B17535_06_003.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3 – 步长小于内核大小导致输出之间的元素共享的示例](img/B17535_06_003.jpg)'
- en: Figure 6.3 – An example of a stride smaller than the kernel size causing the
    elements in between to be shared on the output
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – 步长小于内核大小导致输出之间的元素共享的示例
- en: Last is `padding_mode` you can specify different types of padding, such as reflect,
    which will make the padding equal to the entry that it mirrors along the padding
    axis like a mirror at the edge of the input matrix, reflecting each entry back.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最后是`padding_mode`，你可以指定不同的填充类型，例如反射，这将使填充等于它沿填充轴反射的条目，就像输入矩阵边缘的镜子，将每个条目反射回来。
- en: '![Figure 6.4 – Example of padding a 1x7 into a 3x9 before convolution into
    a 1x3](img/B17535_06_004.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4 – 将1x7填充为3x9的示例，在卷积到1x3之前](img/B17535_06_004.jpg)'
- en: Figure 6.4 – Example of padding a 1x7 into a 3x9 before convolution into a 1x3
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – 将1x7填充为3x9的示例，在卷积到1x3之前
- en: We stack multiple convolutional layers because we’re looking for bigger patterns.
    To find all the appropriate patterns, we increase the depth of the convolutional
    layers as we add them to the tower. We start with a convolution 128 kernels deep,
    then double them to 256, 512, and finally 1,024\. Each layer also has a kernel
    size of 5, a stride of 2, and a padding of 2\. This effectively shrinks the width
    and height by half of each layer. So, the first layer takes in a 3x64x64 image
    and outputs a 128x32x32 matrix. The next layers turn that to 256x16x16, then 512x8x8,
    and finally 1024x4x4.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们堆叠多个卷积层，因为我们正在寻找更大的模式。为了找到所有适当的模式，我们在添加到塔中时增加卷积层的深度。我们从一个有128个内核的卷积开始，然后增加到256、512，最后到1,024。每一层也有5个内核大小、2个步长和2个填充。这有效地将每一层的宽度和高度减半。因此，第一层接收一个3x64x64的图像，并输出一个128x32x32的矩阵。下一层将其转换为256x16x16，然后是512x8x8，最后是1024x4x4。
- en: Next, we’ll finally get into the code at the heart of deepfakes – the neural
    network itself.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将最终深入到deepfakes的核心代码——神经网络本身。
- en: Tip
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'It can be confusing to track how a convolution layer will change a matrix’s
    size. The equation to calculate the output matrix’s size is actually quite simple
    but non-intuitive: `(input_size+2*padding-stride+1)/2`. If you have square matrices,
    this calculation will match for either dimension, but if you have a non-square
    matrix, you’ll have to calculate this for both dimensions separately.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪卷积层如何改变矩阵的大小可能会令人困惑。计算输出矩阵大小的公式实际上相当简单，但不太直观：`(input_size+2*padding-stride+1)/2`。如果你有正方形矩阵，这个计算将适用于任何维度，但如果你有非正方形矩阵，你必须分别计算这两个维度。
- en: Getting hands-on with AI
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 亲身体验AI
- en: The first code we’ll examine here is the actual model itself. This code defines
    the neural network and how it’s structured, as well as how it’s called. All of
    this is stored in the `lib/models.py` library file.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先检查的代码是实际的模型本身。这段代码定义了神经网络的结构以及如何调用它。所有这些都被存储在`lib/models.py`库文件中。
- en: 'First, we load any libraries we’re using:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载我们使用的任何库：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this case, we only import PyTorch and its `nn` submodule. This is because
    we only include the model code in this file and any other libraries will be called
    in the file that uses those functions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们只导入了PyTorch及其`nn`子模块。这是因为我们只在这个文件中包含了模型代码，而任何其他库都将调用使用这些函数的文件。
- en: Defining our upscaler
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义我们的上采样器
- en: 'One of the most important parts of our model is the upscaling layers. Because
    this is used multiple times in both the encoder and decoder, we’ve broken it out
    into its own definition, and we’ll cover that here:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型最重要的部分之一是上采样层。因为这个在上采样器和解码器中都被多次使用，所以我们将其独立出来定义，我们将在下面介绍：
- en: 'First, we define our class:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们定义我们的类：
- en: '[PRE1]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that, like our encoder, this inherits from `nn.Module`. This means we have
    to make a call to the initialization from the parent class in this class’s initialization.
    This gives our class a lot of useful abilities from PyTorch, including the backpropagation
    algorithms that make neural networks work.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，像我们的编码器一样，它继承自`nn.Module`。这意味着我们必须在这个类的初始化中调用父类的初始化。这使我们的类从PyTorch获得了许多有用的能力，包括使神经网络工作的反向传播算法。
- en: 'Next, we define our layers:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义我们的层：
- en: '[PRE2]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The upscaler only uses two layers. The first is a convolutional layer, which
    has an input size double that of the initialization function. We do this because
    the upscale class takes in an output size, and since it increases the width and
    height by halving the depth, it needs the input depth to be twice the output.
    In this case, padding is `same` instead of a number. This is a special way to
    make the `nn.Conv2d` layer output a matrix with the same width and height as the
    input. For a kernel size of `3` this creates a padding of `1`.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 上采样器只使用两层。第一层是一个卷积层，其输入大小是初始化函数的两倍。我们这样做是因为上采样类接受一个输出大小，并且由于它通过减半深度来增加宽度和高度，它需要输入深度是输出深度的一半。在这种情况下，填充是`same`而不是一个数字。这是一种特殊的方法，使`nn.Conv2d`层输出与输入具有相同宽度和高度的矩阵。对于`3`的核大小，这会创建一个`1`的填充。
- en: The `nn.PixelShuffle` is a layer that takes an input matrix and, by moving the
    entries around, takes depth layers and converts them into width and height. Together
    with the earlier convolutional layer, this effectively “upscales” the image in
    a learnable and efficient way. We pass `2` since we want it to double the width
    and height. Other numbers can be used for different scaling factors but would
    require adjustments of the convolutional layers and the models that call the class.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.PixelShuffle`是一个层，它接受一个输入矩阵，通过移动条目，将深度层转换为宽度和高度。与前面的卷积层一起，这有效地以可学习和有效的方式“上采样”了图像。我们传递`2`，因为我们希望它将宽度和高度加倍。其他数字可以用于不同的缩放因子，但需要调整卷积层和调用该类的模型。'
- en: 'Finally, we have our forward function:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们有了我们的前向函数：
- en: '[PRE3]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This forward function simply takes the input, then runs it through the convolutional
    and `PixelShuffle` layers and returns the result.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个前向函数只是接受输入，然后通过卷积层和`PixelShuffle`层运行它，然后返回结果。
- en: Creating the encoder
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建编码器
- en: 'Let’s create the encoder next:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建编码器：
- en: 'First, we declare the encoder class:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们声明编码器类：
- en: '[PRE4]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here we’ve defined and provided a short comment on the encoder. We declare it
    as a child class of the `nn.Module` class. This gives our class a lot of useful
    abilities from PyTorch, including the backpropagation algorithms that make neural
    networks work.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了编码器并提供了一段简短的注释。我们将其声明为`nn.Module`类的子类。这使我们的类从PyTorch获得了许多有用的能力，包括使神经网络工作的反向传播算法。
- en: Author’s note
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 作者注记
- en: In this book, we’ve included only the basic, original model. This was the first
    deepfake model and has been surpassed in pretty much every way, but it’s easy
    to understand, so it works well for this book. If you’d like to explore other
    models, we recommend that you check out Faceswap at [https://Faceswap.dev](https://Faceswap.dev),
    which is constantly updated with the newest models.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们只包括了基本的、原始模型。这是第一个深度伪造模型，几乎在各个方面都被超越，但它很容易理解，所以它非常适合这本书。如果您想探索其他模型，我们建议您查看[https://Faceswap.dev](https://Faceswap.dev)上的Faceswap，它不断更新最新的模型。
- en: 'Next, we’ll define the initialization function:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义初始化函数：
- en: '[PRE5]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This function is the one that actually builds the neural network's layers. Each
    layer is defined in this function so that PyTorch can automatically handle the
    details of the weights. We also call the `__init__` function from the parent class
    to prepare any variables or functionality that is necessary.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数实际上是构建神经网络层的函数。每个层都在这个函数中定义，以便PyTorch可以自动处理权重的细节。我们还调用了父类的`__init__`函数，以准备任何必要的变量或功能。
- en: 'Next, we’ll start defining our activation function:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将开始定义我们的激活函数：
- en: '[PRE6]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We use `LeakyReLU` or **Leaky Rectified Linear Unit** as an **activation function**
    for our model. An activation function takes the output of a layer and brings it
    into a standardized range.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`LeakyReLU`或**Leaky Rectified Linear Unit**作为我们模型的**激活函数**。激活函数接收一个层的输出并将其带入一个标准化的范围。
- en: What a Leaky Rectified Linear Unit *is* is pretty easy to understand if you
    break down the words from last to first. *Unit*, in this case, means the same
    as function; it takes an input and provides an output. *Linear* means a line,
    one that doesn’t change directions as it moves; in this case, it’s a 1:1, where
    the output matches the input (an input of 1 leads to an output of 1, an input
    of 2 leads to an output of 2, and so on). *Rectified* just means it has been made
    positive, so negative numbers become 0\. *Leaky* actually makes that last sentence
    a bit of a lie. It’s been found that neural networks really don’t work very well
    when the entire negative space becomes 0\. So leaky here means that negative numbers
    get scaled to a range barely below 0.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果从后往前分解Leaky Rectified Linear Unit的单词，*Leaky Rectified Linear Unit*是什么就很容易理解。*Unit*在这里意味着与函数相同；它接收一个输入并提供一个输出。*Linear*意味着一条线，它在移动时不会改变方向；在这种情况下，它是一个1:1的关系，输出与输入匹配（输入为1导致输出为1，输入为2导致输出为2，依此类推）。*Rectified*只是意味着它已经被转换为正数，所以负数变成了0。*Leaky*实际上使最后一句话有点误导。研究发现，当整个负空间变成0时，神经网络实际上工作得并不好。所以这里的leaky实际上意味着负数会被缩放到一个几乎低于0的范围。
- en: We use 0.1 here so that any numbers below 0 get multiplied by 0.1, scaling them
    smaller by 10 times. Many different values can be used here, and various projects
    make different decisions. Standard values typically sit somewhere in the range
    of 0.005 to 0.2.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用0.1，这样任何小于0的数字都会乘以0.1，将其缩小10倍。这里可以使用许多不同的值，不同的项目会做出不同的决定。标准值通常位于0.005到0.2的范围内。
- en: 'Next, we’ll define our convolution tower:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义我们的卷积塔：
- en: '[PRE7]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The convolution tower is exactly what it sounds like, a stack of convolution
    layers. After each of the convolution layers, we include an activation function.
    This is helpful to ensure that the model stays on track and makes the convolutions
    more effective. The activation is identical in each case and doesn’t do any “learning”
    but just works like a function, so we don’t need to make separate layers for each
    one and can use the same activation function we already initialized.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积塔正如其名，是一系列卷积层的堆叠。在每个卷积层之后，我们都包含一个激活函数。这有助于确保模型保持正确的方向，并使卷积更有效。每个情况下的激活都是相同的，并不进行任何“学习”，只是作为一个函数工作，因此我们不需要为每个单独的层创建单独的层，可以使用我们已初始化的相同激活函数。
- en: We use `nn.Sequential` here to combine the stack of layers into a single layer.
    The sequential layer is actually a very powerful tool in PyTorch, allowing you
    to make simple neural networks without having to write a whole class for the model.
    We use it here to combine all the convolutional layers since the input in one
    end goes all the way through in every case. This makes it easier to use later
    in our `forward` function. But a sequential model runs each of its constituent
    layers in sequence and can’t handle conditional `if` statements or functions that
    aren’t written for PyTorch.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用`nn.Sequential`来将层堆叠组合成一个单一的层。在PyTorch中，序列层实际上是一个非常强大的工具，允许你创建简单的神经网络，而无需为模型编写整个类。我们在这里使用它来组合所有的卷积层，因为每个情况中输入的一端都会穿过所有层。这使得在后续的`forward`函数中使用它更容易。但序列模型会按顺序运行其构成层，并且无法处理条件`if`语句或为PyTorch编写的函数。
- en: 'Next, we’ll define a `flatten` layer:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个`flatten`层：
- en: '[PRE8]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: A `flatten` layer does exactly what it sounds like; it flattens a previous layer
    to just one axis. This is used in the forward pass to turn the 1024x4x4 matrix
    that comes out of the convolution tower into a 4,096-element wide single-dimension
    layer.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`flatten`层正是其名称所描述的那样；它将前一个层展平到只有一个轴。这在正向传播中用于将卷积塔产生的1024x4x4矩阵转换成一个4,096个元素的宽单维度层。'
- en: 'Next, we’ll define our dense layers:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义我们的密集层：
- en: '[PRE9]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Dense layers are called dense because they’re fully connected. Unlike convolutional
    layers, every single entry in the matrix is connected to every single input of
    the previous layer. Dense layers were the original neural network layer types
    and are very powerful, but they’re also very memory intensive. In fact, these
    two layers account for most of the memory of the entire deepfake model!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 密集层被称为密集层，因为它们是完全连接的。与卷积层不同，矩阵中的每个条目都与前一层的每个输入相连。密集层是原始神经网络层类型，非常强大，但它们也非常占用内存。实际上，这两个层占用了整个深度伪造模型的大部分内存！
- en: 'We generate two separate dense layers. The first layer takes in an input of
    4,096 entries wide and outputs a 1,024-wide output. This is the **bottleneck**
    of the model: the part of the model that has the least amount of data, which then
    needs to be rebuilt. The second layer takes a 1024 one-dimensional matrix input
    and outputs a matrix with one dimension of 4,096\. This is the first layer that
    starts rebuilding a face from encoded details.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成了两个独立的密集层。第一层接受4,096个条目的输入并输出一个1,024列宽的输出。这是模型的**瓶颈**：模型中数据最少的部分，然后需要重建。第二层接受一个1,024维的矩阵输入并输出一个具有4,096维的矩阵。这是第一个开始从编码细节重建面部的层。
- en: 'The last initialization step is to define our first upscale layer:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后的初始化步骤是定义我们的第一个上采样层：
- en: '[PRE10]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This layer is our first upscaler. This layer will take a 1024x4x4 matrix and
    upscale it back to a 512x8x8 matrix. All other upscalers will exist in the decoder.
    This one was originally put in the encoder, probably as a memory-saving attempt
    since the first upscale was unlikely to need to match a particular person at all,
    as it only had the most general of face patterns.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层是我们的第一个上采样器。这一层将接受一个1024x4x4的矩阵并将其上采样回512x8x8矩阵。所有其他上采样器都将存在于解码器中。这个上采样器最初被放在编码器中，可能是因为作为一个节省内存的尝试，因为第一次上采样很可能根本不需要匹配特定的人，因为它只有最一般的面部模式。
- en: The upscale layer is given an output size of 512\. This means that the output
    will be 512 deep but does not define the width or height. These come naturally
    from the input, with each call to upscale doubling the width and height.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 上采样层被赋予了一个输出大小为512。这意味着输出将深度为512，但没有定义宽度或高度。这些自然地从输入中得出，每次调用上采样都会使宽度和高度加倍。
- en: 'Next, we’ll go over our forward function:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将回顾我们的前向函数：
- en: '[PRE11]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The forward function is what actually applies the network to a given matrix.
    This is used both for training and for inference of the trained model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 前向函数是实际将网络应用于给定矩阵的操作。这既用于训练，也用于训练模型的推理。
- en: 'First, we get the batch size:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们获取批大小：
- en: '[PRE12]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We need the batch size that we started with later in the process, so we save
    it here immediately.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在后续过程中保存我们开始的批大小，所以我们立即保存它。
- en: 'Finally, we run the data through the whole model:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将数据通过整个模型：
- en: '[PRE13]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this code, we run the input matrix through each layer in turn. The only new
    surprise here is the `torch.reshape` call after the final `dense`, which is effectively
    the opposite of the `flatten` call from right before the first `dense`. It takes
    the 4096-wide matrix and changes the shape so that it’s a 1024x4x4 matrix again.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们依次将输入矩阵通过每一层。这里唯一的惊喜是在最后的`dense`之后调用的`torch.reshape`，这实际上是第一层`dense`之前`flatten`调用的反操作。它将4096列宽的矩阵改变形状，使其再次成为1024x4x4矩阵。
- en: We then run the data through the upscale layer and then the activation function
    before we return the result.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在返回结果之前，将数据通过上采样层和激活函数。
- en: Building the decoders
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建解码器
- en: The decoder is responsible for taking the encoded face data and re-creating
    a face as accurately as it can. To do this, it will iterate over thousands or
    even millions of faces to get better at turning encodings into faces. At the same
    time, the encoder will be getting better at encoding faces.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器负责接收编码的面部数据并尽可能准确地重新创建一个面部。为此，它将遍历成千上万甚至数百万个面部，以更好地将编码转换为面部。同时，编码器也将变得擅长编码面部。
- en: We used the plural *decoders* here, but this code only actually defines a single
    decoder. That’s because the training code creates two copies of this decoder class.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用了复数的“decoders”，但实际上这段代码只定义了一个单一的解码器。这是因为训练代码创建了该解码器类的两个副本。
- en: 'First, we define and initialize our model:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们定义并初始化我们的模型：
- en: '[PRE14]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This code, just like the encoder and upscaler, is an instance of `nn.Module`
    and needs an initialization function that also calls the parent’s initializer.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码，就像编码器和上采样器一样，是`nn.Module`的一个实例，需要一个初始化函数，该函数也会调用父类的初始化器。
- en: 'Next, we define our activation function:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义我们的激活函数：
- en: '[PRE15]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Just like our encoder’s activation, we use `LeakeReLu` with a negative scaling
    of `0.1`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们的编码器的激活一样，我们使用 `LeakeReLu` 并带有 `0.1` 的负缩放。
- en: 'Next, we define our upscaling tower:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义我们的上采样塔：
- en: '[PRE16]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The upscale tower is much like the convolution tower of the encoder but uses
    upscale blocks instead of shrinking convolutions. Because there was one upscaler
    in the encoder, we actually have one fewer upscales in this decoder. Just like
    the convolution tower, there are also activation functions after each upscale
    to keep the range trending positive.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 上采样塔与编码器的卷积塔非常相似，但使用上采样块而不是缩小卷积。因为编码器中有一个上采样器，所以实际上这个解码器中上采样更少。就像卷积塔一样，每个上采样之后也有激活函数，以保持范围趋势为正。
- en: 'Next, we define our output layer:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义我们的输出层：
- en: '[PRE17]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The output layer is special. While each of the previous layers’ outputs was
    half the depth of the previous layer’s, this one takes the 64-deep output from
    the convolution layer and converts it back to a three-channel image. There is
    nothing special about the three-channel dimension, but due to how the training
    process works, each is correlated to one of the color channels of the training
    image.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层是特殊的。虽然之前每一层的输出都是前一层深度的一半，但这一层将卷积层的 64 个深度输出转换回一个三通道图像。三通道维度并没有什么特殊之处，但由于训练过程的工作方式，每个都与训练图像的一个颜色通道相关联。
- en: 'Now, we define the forward function of the decoder:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们定义解码器的正向函数：
- en: '[PRE18]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This forward function is familiar, being very similar to those in the encoder
    and the upscale layer. The major difference here is that after we pass the input
    through the upscale tower and the output layer, we use a `torch.sigmoid` layer.
    This is another type of activation layer.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这个正向函数很熟悉，与编码器和上采样层的那些非常相似。这里的主要区别在于，我们在通过上采样塔和输出层传递输入之后，使用了一个 `torch.sigmoid`
    层。这是一种另一种类型的激活层。
- en: Sigmoid works differently from LeakyReLu in that it is not linear. Instead,
    it computes the logistic sigmoid of the input. This is an s-shaped output where
    negative inputs approach `0`, and positive inputs approach `1` with a `0` input
    coming out as `0.5`. The precise equation is `1/(1*e^-input)`. This basically
    puts the results between `0` and `1` with extremes being more compressed, which
    matches how the multiplication of high numbers leads to higher numbers faster.
    This effectively turns the output of the model into a range of `0-1`, which we
    can easily turn into an image.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 与 LeakyReLu 不同，因为它不是线性的。相反，它计算输入的逻辑 sigmoid。这是一个 s 形的输出，其中负输入接近 `0`，正输入接近
    `1`，而 `0` 输入的结果为 `0.5`。精确的方程是 `1/(1*e^-input)`。这基本上将结果放在 `0` 和 `1` 之间，极端值被更紧密地压缩，这与高数乘法导致数值更快增长的方式相匹配。这有效地将模型的输出转换为一个
    `0-1` 的范围，我们可以轻松地将它转换成图像。
- en: '![Figure 6.5 – An example of the sigmoid curve](img/B17535_06_005.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.5 – sigmoid 曲线的示例](img/B17535_06_005.jpg)'
- en: Figure 6.5 – An example of the sigmoid curve
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – sigmoid 曲线的示例
- en: Next, we’ll examine the training code.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将检查训练代码。
- en: Exploring the training code
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索训练代码
- en: Now that we have defined our models, we can go ahead with the process of training
    a neural network on our data. This is the part where we actually have AI learn
    the different faces so that it can later swap between them.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们的模型，我们可以继续在数据上训练神经网络的过程。这是人工智能实际学习不同面部特征以便之后可以在它们之间切换的部分。
- en: 'First, we import our libraries:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入我们的库：
- en: '[PRE19]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Like all Python programs, we import our libraries. We also import our encoder
    and decoders from our model file. This loads the AI model code from earlier in
    this chapter and lets us use those to define our models in this code. Python really
    makes it easy to import code we’ve already written, as every Python file can be
    called directly or imported into another file.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 就像所有的 Python 程序一样，我们导入我们的库。我们还从我们的模型文件中导入编码器和解码器。这加载了本章前面提到的 AI 模型代码，并允许我们使用这些代码来定义本代码中的模型。Python
    真的让导入我们之前编写的代码变得很容易，因为每个 Python 文件都可以直接调用或导入到另一个文件中。
- en: Note that Python uses a strange syntax for folder paths. Python treats this
    syntax exactly the same as a module, so you use a period to tell it to look in
    a folder and then give it the file you want. In this case, we’re pulling the `OriginalEncoder`
    and `OriginalDecoder` classes from the `models.py` file located in the `lib` folder.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Python使用一种奇怪的语法来表示文件夹路径。Python将此语法视为与模块完全相同，因此您使用点来告诉它查看文件夹，然后给出您想要的文件。在这种情况下，我们从位于`lib`文件夹中的`models.py`文件中获取`OriginalEncoder`和`OriginalDecoder`类。
- en: 'Next, we define our arguments and call our main function:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义我们的参数并调用我们的主函数：
- en: '[PRE20]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we define our arguments:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义我们的参数：
- en: '[PRE21]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here we define our arguments. These give us the ability to change our settings,
    files, or details without having to modify the source code directly.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们定义我们的参数。这些参数使我们能够更改设置、文件或细节，而无需直接修改源代码。
- en: 'Then, we parse all the arguments and call our main function:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们解析所有参数并调用我们的主函数：
- en: '[PRE22]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We parse our arguments and pass them into our main function. The main function
    will handle all the training processes, and we need to give it all the arguments.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解析我们的参数并将它们传递给我们的主函数。主函数将处理所有训练过程，我们需要给它所有参数。
- en: 'Next, we start our main function:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们开始我们的主函数：
- en: '[PRE23]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Here we start our main function and check whether we’re supposed to use `cuda`.
    If so, we enable `cuda` so that we can use the **graphics processing unit** (**GPU**)
    to accelerate training. Then we create our export folder if that isn’t already
    created. This is where we’ll save copies of our models and any training previews
    we generate later.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们开始我们的主函数并检查是否应该使用`cuda`。如果是的话，我们启用`cuda`以便我们可以使用**图形处理单元**（**GPU**）来加速训练。然后我们创建我们的导出文件夹，如果尚未创建。这是我们保存模型副本和稍后生成的任何训练预览的地方。
- en: Author’s note
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 作者注记
- en: While it’s possible to run the other parts of the process without a GPU, training
    is far more intensive, and running a training session on a **central processing
    unit** (**CPU**) will take a very large amount of time. Because of this, it’s
    recommended that at least this part be run with a GPU. If you don’t have one locally,
    you can rent one at any number of online services.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可以在没有GPU的情况下运行过程的其它部分，但训练要复杂得多，在**中央处理单元**（**CPU**）上运行训练会话将花费非常长的时间。因此，建议至少这部分使用GPU运行。如果您本地没有，您可以在任何数量的在线服务中租用。
- en: Creating our models
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建我们的模型
- en: 'Here we’ll create our neural models and fill them with weights:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们将创建我们的神经网络模型并将它们填充上权重：
- en: 'First, we’ll create instances of our previous models:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将创建之前模型的实例：
- en: '[PRE24]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In this chunk of code, we create our AI models. We create one instance of the
    encoder and two separate decoders. We call them `a` and `b` here, but that’s entirely
    an arbitrary choice with no effect on the results. By default, we assume that
    you want to put the second face onto the first so in the case of this code, we’d
    be putting the face from `b` onto the frame from `a`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们创建我们的AI模型。我们创建一个编码器实例和两个独立的解码器。在这里我们称它们为`a`和`b`，但这完全是一个任意的选择，对结果没有影响。默认情况下，我们假设您想要将第二个面部放在第一个上，因此在这种情况下，我们将从`b`中提取的面部放在`a`的框架上。
- en: 'Next, we load any previously saved models:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们加载任何先前保存的模型：
- en: '[PRE25]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Here we check whether any models already exist in the given output folder. If
    they do, we load those model weights into the models we instantiated in the last
    section. To do this, we have PyTorch load the weights from the disk and then assign
    the weights to the model’s state dictionary. This lets PyTorch load the weights
    into the model and get it ready for training.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们检查给定输出文件夹中是否存在任何模型。如果存在，我们将加载这些模型权重到我们在上一部分实例化的模型中。为此，我们让PyTorch从磁盘加载权重，然后将权重分配给模型的状态字典。这使得PyTorch将权重加载到模型中，并使其准备好训练。
- en: If there are no weights, then we skip this step. This means that the models
    will be initialized with random weights, ready to start a new training session.
    This lets you get started easily without having to generate any random weights
    yourself.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有权重，则跳过此步骤。这意味着模型将使用随机权重初始化，准备好开始新的训练会话。这使得您可以轻松开始，无需自己生成任何随机权重。
- en: 'Next, we get a list of the images to train with:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们获取用于训练的图像列表：
- en: '[PRE26]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This chunk gets a list of all the images from the folders to train. We load
    only the aligned face images from the folders since we can create the filenames
    for the other images from the filenames for the aligned images.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分从文件夹中获取所有用于训练的图像列表。我们只从文件夹中加载对齐的图像，因为我们可以从对齐图像的文件名中创建其他图像的文件名。
- en: 'Next, we create the tensors for the images and the masks:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们为图像和掩码创建张量：
- en: '[PRE27]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Here we create the tensors that will hold the images we will use for training.
    For the image tensors, we create a tensor that is 64x64 pixels wide with 3 channels
    to handle the red, green, and blue color channels. We also add a **batch size**
    dimension to the tensor so we can store that many images at once. A batch is simply
    how many images we’ll process at the same time. Larger batch sizes help the training
    process run more efficiently as we’re able to benefit from hardware that can do
    multiple tasks simultaneously as well as benefit from PyTorch grouping the tasks
    in the best order.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了将用于训练的图像的张量。对于图像张量，我们创建了一个64x64像素宽、3个通道的张量，用于处理红色、绿色和蓝色通道。我们还向张量中添加了一个**批量大小**维度，这样我们就可以一次性存储那么多图像。批量就是我们将同时处理的图像数量。较大的批量大小有助于训练过程更高效地运行，因为我们能够从能够同时执行多个任务的硬件中受益，同时也能够从PyTorch以最佳顺序对任务进行分组中受益。
- en: Author’s note
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 作者注记
- en: Larger batch sizes lead to more efficiencies in the training, so why don’t we
    set the batch size to 256 or 1024 instead of defaulting to 16? It’s because batch
    size is not a magic bullet. First, larger batches take more memory as the system
    must store every item of the batch at the same time. With large models, this can
    be prohibitive. Additionally, there is a side effect to large batch sizes. It’s
    the classic “forest for the trees,” meaning that larger batch sizes can help generalize
    over large sets of data but perform worse at learning specific details. So, picking
    the ideal batch size can be as important a question as anything else. A good rule
    of thumb for deepfakes is to keep batch size in double digits with 100+ tending
    to be too big and <10 to be avoided unless you have specific plans.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 较大的批量大小可以提高训练效率，那么我们为什么不将批量大小设置为256或1024，而不是默认的16呢？这是因为批量大小并不是万能的子弹。首先，较大的批量需要更多的内存，因为系统必须同时存储批量的每一项。对于大型模型来说，这可能是不可行的。此外，大批量大小还有一个副作用。这就是经典的“只见树木不见森林”，意味着较大的批量大小可以帮助在大数据集上泛化，但在学习特定细节方面表现较差。因此，选择理想的批量大小可能和任何其他问题一样重要。对于深度伪造来说，一个好的经验法则是将批量大小保持在两位数，100+通常太大，而<10则应避免，除非你有具体的计划。
- en: 'Next, we define and set up our optimizers and loss function:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义并设置我们的优化器和损失函数：
- en: '[PRE28]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Optimizers are the part of PyTorch responsible for the most important part
    of training: backpropagation. This process is what changes the weights of the
    model and allows AI to “learn” and get better at re-creating the images we’re
    using for training. It’s responsible for more than just changing the weights but
    actually calculates how much to change them as well.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器是PyTorch负责训练最重要的部分：反向传播。这个过程是改变模型权重并允许AI“学习”并更好地重新创建我们用于训练的图像的过程。它不仅负责改变权重，而且还计算应该改变多少。
- en: In this case, we’re using the `torch.optim.Adam` optimizer. This is part of
    a family of optimizers proven to be very effective and flexible. We use it here
    because it’s what the original deepfake model used, but it’s still one of the
    most reliable and useful optimizers even today.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用`torch.optim.Adam`优化器。这是经过证明非常有效和灵活的优化器家族的一部分。我们在这里使用它，因为它就是原始深度伪造模型使用的，但即使今天，它仍然是其中最可靠和有用的优化器之一。
- en: We pass each model the **learning rate** from our options. The learning rate
    is basically a scaling value of how much the optimizer should change the weights.
    Higher numbers change the weights more, which can lead to faster training at the
    cost of difficulty in fine-tuning since the changes being made are large. Lower
    learning rates can get better accuracy but cause training to take longer by being
    slower. We cut the learning rate of the encoder by half because we will actually
    be training it twice as often since it is being used to encode both faces.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的选项中的**学习率**传递给每个模型。学习率基本上是优化器应该改变权重的缩放值。较大的数字会改变权重更多，这可能导致训练更快，但代价是微调困难，因为所做的改变很大。较低的学习率可以得到更好的精度，但会导致训练时间更长，因为速度较慢。我们将编码器的学习率减半，因为我们实际上会训练它两次，因为它被用来编码两个面孔。
- en: The last thing we do here is to define our `torch.nn.MSEloss` provided by PyTorch.
    This is a loss called **mean squared error** (**MSE**). Let’s look at this word
    by word again.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们做的最后一件事是定义由PyTorch提供的`torch.nn.MSEloss`。这是一个称为**均方误差**（**MSE**）的损失。让我们再次逐字分析这个词。
- en: '*Error* in mathematics is how far off the function is from a perfectly correct
    result. In our case, we are re-creating a face, so the loss function will compare
    the generated face to the original face and count how far off each pixel is from
    the correct answer. This gives a nice easy number for each pixel. Looking at each
    pixel is a bit too difficult, so next, our loss will take the average (mean) of
    all the pixels together. This gives a single number of how far off the AI was
    as a whole. Finally, that number is squared. This makes large differences stand
    out even more and has been shown to help the model reach a good result faster.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 数学中的*错误*是指函数与完美正确结果之间的差距。在我们的情况下，我们正在重新创建一个面部，因此损失函数将比较生成的面部与原始面部，并计算每个像素与正确答案的差距。这为每个像素提供了一个简单易读的数字。查看每个像素有点困难，所以接下来，我们的损失将所有像素的平均值（均值）计算出来。这给出了AI整体偏离程度的一个单一数字。最后，这个数字被平方。这使得大的差异更加突出，并且已经证明这有助于模型更快地达到良好的结果。
- en: There are other loss functions such as **mean absolute error** (**MAE**), which
    gets rid of the squaring in the MSE, or **structural similarity**, which uses
    the similarity of the structure as a measure. In fact, **generative adversarial
    networks** (**GANs**), which are a buzzword of the machine learning field, simply
    replace the static loss of an auto-encoder with another model that provides a
    trainable loss function and pits the two models against each other in a competition
    of which model can do their job better.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他损失函数，如**平均绝对误差**（**MAE**），它去除了MSE中的平方，或者**结构相似性**，它使用结构的相似性作为衡量标准。实际上，**生成对抗网络**（**GANs**），这是机器学习领域的热门词汇，只是用一个提供可训练损失函数的另一个模型替换了自编码器的静态损失，并将两个模型放在一个竞争中，看哪个模型能更好地完成其工作。
- en: 'Next, we move everything to the GPU if enabled:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，如果启用，我们将所有内容移动到GPU上：
- en: '[PRE29]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: If `cuda` was enabled earlier, we need to move the models and the variables
    to the GPU so we can process them. So here, we check whether `cuda` was enabled,
    and if so, we move each of them to the GPU.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果之前启用了`cuda`，我们需要将模型和变量移动到GPU上，以便我们可以处理它们。因此，在这里，我们检查是否启用了`cuda`，如果是，我们将它们中的每一个移动到GPU上。
- en: Looping over the training
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 遍历训练
- en: In order to train the model, we need to loop over all the data. We call this
    the training loop.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练模型，我们需要遍历所有数据。我们称这个为训练循环。
- en: 'First, we create a progress bar and start the training loop:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建一个进度条并开始训练循环：
- en: '[PRE30]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We use `tqdm` again for a progress bar. Here we pass a range of how many iterations
    we want to `tqdm` so it can update our progress bar automatically and assign the
    progress bar to a variable. We then start our loop from that variable to provide
    more information in the progress bar by calling functions that `tqdm` exposes
    in the variable.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次使用`tqdm`来显示进度条。在这里，我们传递一个迭代次数的范围给`tqdm`，以便它可以自动更新进度条，并将进度条分配给一个变量。然后，我们从该变量开始循环，通过调用`tqdm`在变量中暴露的函数来提供更多进度条信息。
- en: 'Next, we load a random set of images:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们加载一组随机图像：
- en: '[PRE31]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This chunk loads a set of images from the `a` set for the model to train with.
    To do this, we get a random sample the same size as our batch size from the list
    of files we generated earlier.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这个块从`a`集中加载一系列图像，以便模型进行训练。为此，我们从我们之前生成的文件列表中获取与批处理大小相同的随机样本。
- en: Next, we go over a loop for each of those images. The loop first reads in the
    face image and resizes it down to 64x64\. Then it does the same for the mask image
    by replacing the `"aligned"` word in the filename with `"mask"`, which matches
    the mask filenames. The mask is also resized to match the training image.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为每个图像遍历一个循环。循环首先读取面部图像并将其调整大小到64x64。然后，它通过将文件名中的`"aligned"`单词替换为`"mask"`来对掩码图像执行相同的操作，这与掩码文件名相匹配。掩码也被调整大小以匹配训练图像。
- en: Next, we randomly get a 50% chance to flip the images horizontally. This is
    an extremely common way to get more variety out of the dataset. Since faces are
    generally pretty symmetrical, we can usually flip them. We use a 50% chance here,
    which gives us an equal chance of the image being flipped or not. Since we have
    a mask, we have to flip it, too, if we flip the image.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们随机以50%的概率水平翻转图像。这是从数据集中获得更多多样性的极其常见的方法。由于人脸通常非常对称，我们通常可以翻转它们。在这里我们使用50%的概率，这给了图像翻转或不翻转相等的可能性。由于我们有掩码，如果我们翻转图像，我们也必须翻转掩码。
- en: Next, we convert both the image and masks from image arrays into tensors. To
    do this to the image, we convert from `[...,::-1]`. This can also be done again
    to get it back to the BGR order (which we’ll do later). The mask is simpler since
    we don’t care about color data for it, so we just see if the pixel data is greater
    than 200; if it is, we put `1` into the tensor; if not, we put `0`.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将图像和掩码从图像数组转换为张量。为此，我们将图像从`[...,::-1]`转换。这也可以再次执行以将其转换回BGR顺序（我们稍后将会这样做）。掩码比较简单，因为我们不关心它的颜色数据，所以我们只需检查像素数据是否大于200；如果是，我们在张量中放入`1`；如果不是，我们放入`0`。
- en: Next, we check whether `cuda` is enabled; if it is, we move the tensors we just
    created to the GPU. This puts everything onto the same device.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们检查`cuda`是否启用；如果是，我们将我们刚刚创建的张量移动到GPU上。这把所有东西都放在了同一个设备上。
- en: Finally, we move the image into the tensor we’ll be using to train the model.
    This lets us batch the images together for efficiency.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将图像移动到我们将用于训练模型的张量中。这使得我们可以批量处理图像以提高效率。
- en: 'Then, we do the same for the other set of images:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们对另一组图像做同样的处理：
- en: '[PRE32]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This code is identical to the last code but is repeated for the `b` set of images.
    This creates our second set of images ready to train.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码与上一段代码相同，但重复用于`b`集的图像。这为我们创建了第二组图像，准备进行训练。
- en: Teaching the network
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 教导网络
- en: 'Now it’s time to actually perform the steps that train the network:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是执行训练网络的步骤的时候了：
- en: 'First, we clear the optimizers:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们清除优化器：
- en: '[PRE33]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: PyTorch is very flexible in how it lets you build your models and training process.
    Because of this, we need to tell PyTorch to clear the `a` side decoder.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch在如何让你构建模型和训练过程方面非常灵活。正因为如此，我们需要告诉PyTorch清除`a`侧解码器。
- en: 'Then, we pass the images through the encoder and decoder:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将图像通过编码器和解码器：
- en: '[PRE34]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This chunk sends the image tensors we created earlier through the encoder and
    then through the decoder and stores the results for comparison. This is actually
    the AI model, and we give it a tensor of images and get a tensor of images back
    out.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将我们之前创建的图像张量通过编码器，然后通过解码器，并将结果存储以供比较。这实际上是AI模型，我们给它一个图像张量，并得到一个图像张量作为输出。
- en: 'Next, we calculate our loss and send it through the optimizers:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们计算损失并将其传递给优化器：
- en: '[PRE35]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This chunk does the rest of the training, calculating the loss and then having
    the optimizers perform backpropagation on the models to update the weights. We
    start by passing the output images and the original images to the loss function.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码执行剩余的训练，计算损失，然后让优化器在模型上执行反向传播以更新权重。我们首先将输出图像和原始图像传递给损失函数。
- en: To apply the masks, we multiply the images by the masks. We do this here instead
    of before we pass the images to the model because we might not have the best masks,
    and it’s better to train the neural network on the whole image and apply the mask
    later.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应用掩码，我们用掩码乘以图像。我们在这里这样做而不是在将图像传递给模型之前，因为我们可能没有最好的掩码，而且最好是在整个图像上训练神经网络，然后再应用掩码。
- en: Next, we call `backward` on the loss variable. We can do this because the variable
    is actually still a tensor, and tensors keep track of all actions that happened
    to them while in training mode. This lets the loss be carried back over all the
    steps back to the original image.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在损失变量上调用`backward`。我们可以这样做，因为变量实际上仍然是一个张量，张量会跟踪在训练模式下对其发生的所有操作。这使得损失可以回传到所有步骤，直到原始图像。
- en: The last step is to call the `step` function of our optimizers. This goes back
    over the model weights, updating them so that the next iteration should be closer
    to the correct results.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是调用优化器的`step`函数。这会回过头来更新模型权重，以便下一次迭代应该更接近正确的结果。
- en: 'Next, we do the same thing, but for the `b` decoder instead:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们做同样的事情，但针对`b`解码器：
- en: '[PRE36]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We go through the same process again with the `b` images and decoder. Remember
    that we’re using the same encoder for both models, so it actually gets trained
    a second time along with the `b` decoder. This is a key part of how deepfakes
    can swap faces. The two decoders share a single encoder, which eventually gives
    both the decoders the information to re-create their individual faces.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次使用 `b` 图像和解码器进行相同的过程。记住，我们使用相同的编码器为两个模型，所以它实际上在 `b` 解码器训练的同时再次进行训练。这是 deepfakes
    可以交换面部的一个关键部分。两个解码器共享一个编码器，这最终为两个解码器提供了重新创建各自面部所需的信息。
- en: 'Next, we update the progress bar with information about this iteration’s loss
    of data:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用关于这次迭代数据损失的进度条信息更新进度条：
- en: '[PRE37]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Since the loss function outputs a number for the optimizers, we can also display
    this number for the user. Sometimes loss is used by deepfakers as an estimate
    of how finished the model is training. Unfortunately, this cannot actually measure
    how good a model is at converting one face into another; it only scores how good
    it is at re-creating the same face it was given. For this reason, it’s an imperfect
    measure and shouldn’t be relied on. Instead, we recommend that the previews we’ll
    be generating later be used for this purpose.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 由于损失函数为优化器输出一个数字，我们也可以向用户显示这个数字。有时损失被深度伪造者用作模型训练完成程度的估计。不幸的是，这实际上不能衡量模型将一个面部转换成另一个面部的好坏；它只评估它重新创建给定相同面部的好坏。因此，这是一个不完美的衡量标准，不应该依赖。相反，我们建议稍后生成的预览用于此目的。
- en: Saving results
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存结果
- en: 'Finally, we will save our results:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将保存我们的结果：
- en: 'First, we’ll check to see whether we should trigger a save:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将检查是否应该触发保存：
- en: '[PRE38]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We want to save regularly – an iteration may take less than a second, but a
    save could take several seconds of writing to disk. Because of this, we don’t
    want to save every iteration; instead, we want to trigger a save on a regular
    basis after a set number of iterations. Different computers will run at different
    speeds, so we let you set the save frequency with an argument.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望定期保存 – 一个迭代可能不到一秒，但保存可能需要几秒钟的磁盘写入时间。因此，我们不想保存每个迭代；相反，我们希望在设置数量的迭代后定期触发保存。不同的计算机运行速度不同，所以我们允许您使用参数设置保存频率。
- en: One thing we want to save along with the current weights is a preview image
    so we can get a good idea of how the model is doing at each save state. For this
    reason, we’ll be using the neural networks, but we don’t want to train while we’re
    doing this step. That’s the exact reason that `torch` has the `torch.no_grad`
    context. By calling our model from inside this context, we won’t be training and
    just getting the results from the network.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要与当前权重一起保存的一件事是预览图像，这样我们就可以了解模型在每次保存状态下的表现。因此，我们将使用神经网络，但我们不希望在执行此步骤时进行训练。这正是
    `torch` 有 `torch.no_grad` 上下文的原因。通过在这个上下文中调用我们的模型，我们不会进行训练，只是从网络中获取结果。
- en: We call each decoder with samples of images from both faces. This lets us compare
    the re-created faces along with the generated swaps. Since we only want a preview
    image, we can throw out all but the first image to be used as a sample of the
    current stage of training.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用来自两个面部的图像样本调用每个解码器。这使我们能够比较重新创建的面部以及生成的交换。由于我们只想预览图像，我们可以丢弃除了第一个图像之外的所有图像，将其用作当前训练阶段的样本。
- en: 'Next, we create the sample image:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建样本图像：
- en: '[PRE39]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We need to create our sample image from all the parts. To do this, we need to
    convert all the image tensors into a single image. We use `np.concatenate` to
    join them all into a single array along the width axis. To do this, we need to
    get them all into image order and convert them to NumPy arrays. The first thing
    we do is drop the batch dimension by selecting the first one. Then we use `permute`
    to reorder each tensor, so the channels are last. Then we use `detach` to remove
    any gradients from the tensors. We can then use `cpu` to bring the weights back
    onto the CPU. Finally, we use `numpy` to finish converting them into NumPy arrays.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要从所有部分创建我们的样本图像。为此，我们需要将所有图像张量转换成一个单独的图像。我们使用 `np.concatenate` 沿着宽度轴将它们全部连接成一个数组。为此，我们需要将它们全部按图像顺序排列并转换为
    NumPy 数组。我们首先通过选择第一个来丢弃批处理维度。然后我们使用 `permute` 来重新排列每个张量，使得通道是最后一个。然后我们使用 `detach`
    从张量中移除任何梯度。然后我们可以使用 `cpu` 将权重带回 CPU。最后，我们使用 `numpy` 完成将它们转换为 NumPy 数组的转换。
- en: 'Next, we write the preview image:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们写入预览图像：
- en: '[PRE40]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This chunk uses `cv2.imwrite` from OpenCV to write out the preview image as
    a PNG file. We put it in the output path and give it a name based on what iteration
    this is. This lets us save each iteration’s preview together and track the progress
    of the network over time. To actually write out a usable image, we have to convert
    the color space back to the BGR that OpenCV expects, and then we multiply by `255`
    to get a result that fits into the integer space.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分代码使用OpenCV的`cv2.imwrite`来将预览图像写入PNG文件。我们将其放在输出路径中，并根据这是哪个迭代来命名。这样，我们可以保存每个迭代的预览，并跟踪网络随时间的变化。要实际写入可用的图像，我们必须将颜色空间转换回OpenCV期望的BGR，然后乘以`255`以得到适合整数空间的输出。
- en: 'Next, we save the weights to a file:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将权重保存到文件中：
- en: '[PRE41]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Here we save the weights for our encoder and both decoders by calling `torch.save`
    with the output path and the filenames we want to use to save the weights. PyTorch
    automatically saves them to the file in its native `pth` format.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过调用`torch.save`并指定输出路径和想要用于保存权重的文件名，来保存编码器和两个解码器的权重。PyTorch会自动以其本地的`pth`格式将它们保存到文件中。
- en: 'Finally, we save again:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们再次保存：
- en: '[PRE42]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: For our last step, we repeat the save code. But, actually, this is done outside
    of the training loop. This is here just in case the training loop ends on an iteration
    number that doesn’t trigger the save there. This way, the model is definitely
    saved at least once, no matter what arguments are chosen by the user.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的最后一步，我们重复保存代码。但实际上，这是在训练循环之外完成的。这里之所以这样做，只是为了以防训练循环在不会触发保存的迭代次数上结束。这样，无论用户选择了什么参数，模型都至少会被保存一次。
- en: Author’s note
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 作者注记
- en: This may seem obvious to anyone who has spent any time coding, but it’s worth
    repeating. You should always consider how choices made in the development process
    might lead to bad or unexpected outcomes and account for those in your design.
    It’s obviously impossible to consider everything, but sometimes even something
    as simple as duplicating a save right before exiting, “just in case,” can save
    someone’s day (or in the case of a very long training session, even more time).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何花过时间编码的人来说，这可能看起来很明显，但重复一遍是值得的。你应该始终考虑在开发过程中做出的选择可能会导致的坏结果或意外结果，并在设计中考虑到这些因素。显然，考虑一切是不可能的，但有时即使是在退出前简单复制保存操作，“以防万一”，也能拯救某人的日子（或者在一个非常长的训练会话中，甚至更多时间）。
- en: Summary
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we trained a neural network to swap faces. To do this, we had
    to explore what convolutional layers are and then build a foundational upscaler
    layer. Then we built the three networks. We built the encoder, then two decoders.
    Finally, we trained the model itself, including loading and preparing images,
    and made sure we saved previews and the final weights.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们训练了一个神经网络来交换面部。为此，我们必须探索卷积层是什么，然后构建一个基础的上采样层。然后我们构建了三个网络。我们构建了编码器，然后是两个解码器。最后，我们训练了模型本身，包括加载和准备图像，并确保我们保存了预览和最终的权重。
- en: First, we built the models of the neural networks that we were going to train
    to perform the face-swapping process. This was broken down into the upscaler,
    the shared encoder, and the two decoders. The upscaler is used to increase the
    size of the image by turning depth into a larger image. The encoder is used to
    encode the face image down into a smaller encoded space that we then pass to the
    decoders, which are responsible for re-creating the original image. We also looked
    at activation layers to understand why they’re helpful.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们构建了将要训练以执行面部交换过程的神经网络的模型。这被分解为上采样器、共享编码器和两个解码器。上采样器用于通过将深度转换为更大的图像来增加图像的大小。编码器用于将面部图像编码到更小的编码空间中，然后我们将其传递给解码器，解码器负责重新创建原始图像。我们还研究了激活层，以了解为什么它们是有帮助的。
- en: Next, we covered the training code. We created instances of the network models,
    loaded weights into the models, and put them on the GPU if one was available.
    We explored optimizers and loss functions to understand the roles that they play
    in the training process. We loaded and processed images so that they were ready
    to go through the model to assist in training. Then we covered the training itself,
    including how to get the loss and apply it to the model using the optimizers.
    Finally, we saved preview images and the model weights themselves so we could
    load them again.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们介绍了训练代码。我们创建了网络模型的实例，将权重加载到模型中，并在有可用的情况下将它们放在GPU上。我们探讨了优化器和损失函数，以了解它们在训练过程中的作用。我们加载并处理图像，以便它们可以通过模型进行训练。然后，我们介绍了训练本身，包括如何获取损失并使用优化器将其应用于模型。最后，我们保存了预览图像和模型权重，以便我们可以再次加载它们。
- en: In the next chapter, we’ll take our trained model and use it to “convert” a
    video, swapping the faces.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将使用我们的训练模型并将其用于“转换”视频，交换面部。
- en: Exercises
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Choosing learning rate is not a solved problem. There is no one “right” learning
    rate. What happens if you make the learning rate 10 times bigger? 10 times smaller?
    What if you start with a large learning rate and then reduce it after some training?
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择学习率并不是一个已经解决的问题。没有一种“正确”的学习率。如果你将学习率增加到10倍，或者减少到原来的1/10会发生什么？如果你从一个大的学习率开始，然后在训练一段时间后减少它呢？
- en: We used the same loss function and optimizer as the original code, which was
    first released back in 2018, but there are a lot of options now that weren’t available
    then. Try replacing the loss function with others from PyTorch’s extensive collection
    ([https://pytorch.org/docs/stable/nn.html#loss-functions](https://pytorch.org/docs/stable/nn.html#loss-functions)).
    Some of them will work without any change, but some won’t work for our situation
    at all. Try different ones, or even try combinations of loss functions!
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用了与原始代码相同的损失函数和优化器，该代码最初于2018年发布，但现在有很多选项当时并不可用。尝试用PyTorch广泛的损失函数集合中的其他函数替换损失函数（[https://pytorch.org/docs/stable/nn.html#loss-functions](https://pytorch.org/docs/stable/nn.html#loss-functions)）。其中一些可能不需要任何更改就能工作，但有些可能根本不适合我们的情况。尝试不同的函数，甚至尝试损失函数的组合！
- en: We defined a model that downscaled from a 64x64 pixel image and re-created that
    same image. But with some tweaks, this same architecture can instead create a
    128x128 or 256x256 pixel image. How would you make changes to the model to do
    this? Should you increase the number (and size) of layers in the convolutional
    tower and keep the bottleneck the same, increase the size of the bottleneck but
    keep the layers the same, or change the convolutional layer’s kernel sizes and
    strides? It’s even possible to send in a 64x64 pixel image and get out a 128x128
    pixel image. All of these techniques have their advantages and drawbacks. Try
    each out and see how they differ.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了一个模型，它从64x64像素的图像下采样并重新创建了相同的图像。但通过一些调整，这个相同的架构可以创建128x128或256x256像素的图像。你将如何修改模型来实现这一点？你应该增加（和大小）卷积塔的层数并保持瓶颈不变，增加瓶颈的大小但保持层数不变，还是改变卷积层的内核大小和步长？甚至可能输入一个64x64像素的图像，输出一个128x128像素的图像。所有这些技术都有其优点和缺点。尝试每一种，看看它们有何不同。
- en: We’re training on just two faces, but you could potentially do more. Try modifying
    the training code to use three different faces instead of just two. What changes
    would you need to make? What modifications would you make so that you can train
    an arbitrary number of faces at once?
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们目前只训练了两个面部，但理论上可以做得更多。尝试修改训练代码，使用三个不同的面部而不是两个。你需要做出哪些改变？你将进行哪些修改，以便可以一次性训练任意数量的面部？
- en: In the years since deepfakes were first released, there have been a lot of different
    models created. Faceswap has implementations for many newer and more advanced
    models, but they’re written in Keras for Tensorflow and can’t work in this PyTorch
    fork without being ported. Check the Faceswap models at the GitHub repo ([https://github.com/deepfakes/Faceswap/tree/master/plugins/train/model](https://github.com/deepfakes/Faceswap/tree/master/plugins/train/model)).
    Compare this model to the one in `Original.py`, which implements the same model.
    Now use that to see how `dfaker.py` differs. The residual layer works by adding
    an extra convolution layer and an `add` layer, which just adds two layers together.
    Can you duplicate the `dfaker` model in this code base? What about the others?
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自从深度伪造首次发布以来，已经创建了众多不同的模型。Faceswap实现了许多较新和更高级的模型，但它们是用Keras为Tensorflow编写的，如果不进行移植，则无法在这个PyTorch分支中运行。请查看GitHub仓库中的Faceswap模型（[https://github.com/deepfakes/Faceswap/tree/master/plugins/train/model](https://github.com/deepfakes/Faceswap/tree/master/plugins/train/model)）。将此模型与`Original.py`中的模型进行比较，后者实现了相同的模型。现在使用它来查看`dfaker.py`的不同之处。残差层通过添加一个额外的卷积层和一个`add`层来工作，这个`add`层只是将两个层组合在一起。你能在本代码库中复制`dfaker`模型吗？其他模型又如何呢？
- en: EBSCOhost - printed on 11/27/2023 6:20 AM via . All use subject to [https://www.ebsco.com/terms-of-use](https://www.ebsco.com/terms-of-use)
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: EBSCOhost - 2023年11月27日早上6:20打印。所有使用均受[https://www.ebsco.com/terms-of-use](https://www.ebsco.com/terms-of-use)条款约束。
