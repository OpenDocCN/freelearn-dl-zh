- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training a Deepfake Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training a deepfake model is the most important part of creating a deepfake.
    It is where the AI actually learns about the faces from your data and where the
    most interesting neural network operations take place.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll look into the training code and the code that actually
    creates the AI models. We’ll look at the submodules of the neural network and
    how they’re put together to create a complete neural network. Then we’ll go over
    everything needed to train the network and end up with a model ready to swap two
    faces.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding convolutional layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting hands-on with AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the training code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, we’ll have designed our neural networks and built
    a training pipeline capable of teaching them to swap faces.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To run any of the code in this chapter, we recommend downloading our official
    repository at [https://github.com/PacktPublishing/Exploring-Deepfakes](https://github.com/PacktPublishing/Exploring-Deepfakes)
    and following the instructions for setting up an Anaconda environment with all
    of the required libraries.
  prefs: []
  type: TYPE_NORMAL
- en: In order to train, you must have two sets of extracted faces. You’ll feed both
    sets into the model and it will learn both faces separately. It’s important that
    you get sufficient data for both faces and that there be a good variety. If you’re
    in doubt, please check [*Chapter 3*](B17535_03.xhtml#_idTextAnchor054), *Mastering
    Data*, for advice on getting the best data.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding convolutional layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll finally get into the meat of the neural networks behind
    deepfakes. A big part of how networks such as these work is a technique called
    convolutional layers. These layers are extremely important in effectively working
    with image data and form an important cornerstone of most neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: A **convolution** is an operation that changes the shape of an object. In the
    case of neural networks, we use **convolutional layers**, which iterate a convolution
    over a matrix and create a new (generally smaller) output matrix. Convolutions
    are a way to reduce an image in size while simultaneously searching for patterns.
    The more convolutional layers you stack, the more complicated the patterns that
    can be encoded from the original image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – An example of a convolution downscaling a full image](img/B17535_06_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – An example of a convolution downscaling a full image
  prefs: []
  type: TYPE_NORMAL
- en: There are several details that define a convolutional layer. The first is dimensionality.
    In our case, we’re using 2D convolutions, which work in 2D space. This means that
    the convolution works on the x and y axes for each of the channels. This means
    for the first convolution, each color channel is processed separately.
  prefs: []
  type: TYPE_NORMAL
- en: Next is the **kernel**, which defines how big an area each convolution takes
    into account. The amount of kernels going across affects the output as well. For
    example, if you had a matrix of 3x9 and kernel size of 3x3, you’d get a 1x3 matrix
    output.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – An example of a 3x3 convolutional process turning a 3x9 into
    a 1x3 matrix output](img/B17535_06_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – An example of a 3x3 convolutional process turning a 3x9 into a
    1x3 matrix output
  prefs: []
  type: TYPE_NORMAL
- en: Next is **stride** which defines how big a step each iteration of the convolution
    takes as it travels the matrix. A stride of 2, for example, would make our 3x3
    kernel overlap by a single **entry** of the matrix. Stride is duplicated in every
    dimension, so if you extended the example input matrix to the left or right, you’d
    also get overlap in that direction.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – An example of a stride smaller than the kernel size causing
    the elements in between to be shared on the output](img/B17535_06_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – An example of a stride smaller than the kernel size causing the
    elements in between to be shared on the output
  prefs: []
  type: TYPE_NORMAL
- en: Last is `padding_mode` you can specify different types of padding, such as reflect,
    which will make the padding equal to the entry that it mirrors along the padding
    axis like a mirror at the edge of the input matrix, reflecting each entry back.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Example of padding a 1x7 into a 3x9 before convolution into
    a 1x3](img/B17535_06_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Example of padding a 1x7 into a 3x9 before convolution into a 1x3
  prefs: []
  type: TYPE_NORMAL
- en: We stack multiple convolutional layers because we’re looking for bigger patterns.
    To find all the appropriate patterns, we increase the depth of the convolutional
    layers as we add them to the tower. We start with a convolution 128 kernels deep,
    then double them to 256, 512, and finally 1,024\. Each layer also has a kernel
    size of 5, a stride of 2, and a padding of 2\. This effectively shrinks the width
    and height by half of each layer. So, the first layer takes in a 3x64x64 image
    and outputs a 128x32x32 matrix. The next layers turn that to 256x16x16, then 512x8x8,
    and finally 1024x4x4.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll finally get into the code at the heart of deepfakes – the neural
    network itself.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be confusing to track how a convolution layer will change a matrix’s
    size. The equation to calculate the output matrix’s size is actually quite simple
    but non-intuitive: `(input_size+2*padding-stride+1)/2`. If you have square matrices,
    this calculation will match for either dimension, but if you have a non-square
    matrix, you’ll have to calculate this for both dimensions separately.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting hands-on with AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first code we’ll examine here is the actual model itself. This code defines
    the neural network and how it’s structured, as well as how it’s called. All of
    this is stored in the `lib/models.py` library file.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we load any libraries we’re using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we only import PyTorch and its `nn` submodule. This is because
    we only include the model code in this file and any other libraries will be called
    in the file that uses those functions.
  prefs: []
  type: TYPE_NORMAL
- en: Defining our upscaler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the most important parts of our model is the upscaling layers. Because
    this is used multiple times in both the encoder and decoder, we’ve broken it out
    into its own definition, and we’ll cover that here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define our class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that, like our encoder, this inherits from `nn.Module`. This means we have
    to make a call to the initialization from the parent class in this class’s initialization.
    This gives our class a lot of useful abilities from PyTorch, including the backpropagation
    algorithms that make neural networks work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define our layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The upscaler only uses two layers. The first is a convolutional layer, which
    has an input size double that of the initialization function. We do this because
    the upscale class takes in an output size, and since it increases the width and
    height by halving the depth, it needs the input depth to be twice the output.
    In this case, padding is `same` instead of a number. This is a special way to
    make the `nn.Conv2d` layer output a matrix with the same width and height as the
    input. For a kernel size of `3` this creates a padding of `1`.
  prefs: []
  type: TYPE_NORMAL
- en: The `nn.PixelShuffle` is a layer that takes an input matrix and, by moving the
    entries around, takes depth layers and converts them into width and height. Together
    with the earlier convolutional layer, this effectively “upscales” the image in
    a learnable and efficient way. We pass `2` since we want it to double the width
    and height. Other numbers can be used for different scaling factors but would
    require adjustments of the convolutional layers and the models that call the class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we have our forward function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This forward function simply takes the input, then runs it through the convolutional
    and `PixelShuffle` layers and returns the result.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s create the encoder next:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we declare the encoder class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here we’ve defined and provided a short comment on the encoder. We declare it
    as a child class of the `nn.Module` class. This gives our class a lot of useful
    abilities from PyTorch, including the backpropagation algorithms that make neural
    networks work.
  prefs: []
  type: TYPE_NORMAL
- en: Author’s note
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we’ve included only the basic, original model. This was the first
    deepfake model and has been surpassed in pretty much every way, but it’s easy
    to understand, so it works well for this book. If you’d like to explore other
    models, we recommend that you check out Faceswap at [https://Faceswap.dev](https://Faceswap.dev),
    which is constantly updated with the newest models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll define the initialization function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This function is the one that actually builds the neural network's layers. Each
    layer is defined in this function so that PyTorch can automatically handle the
    details of the weights. We also call the `__init__` function from the parent class
    to prepare any variables or functionality that is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll start defining our activation function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use `LeakyReLU` or **Leaky Rectified Linear Unit** as an **activation function**
    for our model. An activation function takes the output of a layer and brings it
    into a standardized range.
  prefs: []
  type: TYPE_NORMAL
- en: What a Leaky Rectified Linear Unit *is* is pretty easy to understand if you
    break down the words from last to first. *Unit*, in this case, means the same
    as function; it takes an input and provides an output. *Linear* means a line,
    one that doesn’t change directions as it moves; in this case, it’s a 1:1, where
    the output matches the input (an input of 1 leads to an output of 1, an input
    of 2 leads to an output of 2, and so on). *Rectified* just means it has been made
    positive, so negative numbers become 0\. *Leaky* actually makes that last sentence
    a bit of a lie. It’s been found that neural networks really don’t work very well
    when the entire negative space becomes 0\. So leaky here means that negative numbers
    get scaled to a range barely below 0.
  prefs: []
  type: TYPE_NORMAL
- en: We use 0.1 here so that any numbers below 0 get multiplied by 0.1, scaling them
    smaller by 10 times. Many different values can be used here, and various projects
    make different decisions. Standard values typically sit somewhere in the range
    of 0.005 to 0.2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll define our convolution tower:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The convolution tower is exactly what it sounds like, a stack of convolution
    layers. After each of the convolution layers, we include an activation function.
    This is helpful to ensure that the model stays on track and makes the convolutions
    more effective. The activation is identical in each case and doesn’t do any “learning”
    but just works like a function, so we don’t need to make separate layers for each
    one and can use the same activation function we already initialized.
  prefs: []
  type: TYPE_NORMAL
- en: We use `nn.Sequential` here to combine the stack of layers into a single layer.
    The sequential layer is actually a very powerful tool in PyTorch, allowing you
    to make simple neural networks without having to write a whole class for the model.
    We use it here to combine all the convolutional layers since the input in one
    end goes all the way through in every case. This makes it easier to use later
    in our `forward` function. But a sequential model runs each of its constituent
    layers in sequence and can’t handle conditional `if` statements or functions that
    aren’t written for PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll define a `flatten` layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A `flatten` layer does exactly what it sounds like; it flattens a previous layer
    to just one axis. This is used in the forward pass to turn the 1024x4x4 matrix
    that comes out of the convolution tower into a 4,096-element wide single-dimension
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll define our dense layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Dense layers are called dense because they’re fully connected. Unlike convolutional
    layers, every single entry in the matrix is connected to every single input of
    the previous layer. Dense layers were the original neural network layer types
    and are very powerful, but they’re also very memory intensive. In fact, these
    two layers account for most of the memory of the entire deepfake model!
  prefs: []
  type: TYPE_NORMAL
- en: 'We generate two separate dense layers. The first layer takes in an input of
    4,096 entries wide and outputs a 1,024-wide output. This is the **bottleneck**
    of the model: the part of the model that has the least amount of data, which then
    needs to be rebuilt. The second layer takes a 1024 one-dimensional matrix input
    and outputs a matrix with one dimension of 4,096\. This is the first layer that
    starts rebuilding a face from encoded details.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The last initialization step is to define our first upscale layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This layer is our first upscaler. This layer will take a 1024x4x4 matrix and
    upscale it back to a 512x8x8 matrix. All other upscalers will exist in the decoder.
    This one was originally put in the encoder, probably as a memory-saving attempt
    since the first upscale was unlikely to need to match a particular person at all,
    as it only had the most general of face patterns.
  prefs: []
  type: TYPE_NORMAL
- en: The upscale layer is given an output size of 512\. This means that the output
    will be 512 deep but does not define the width or height. These come naturally
    from the input, with each call to upscale doubling the width and height.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll go over our forward function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The forward function is what actually applies the network to a given matrix.
    This is used both for training and for inference of the trained model.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we get the batch size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We need the batch size that we started with later in the process, so we save
    it here immediately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we run the data through the whole model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this code, we run the input matrix through each layer in turn. The only new
    surprise here is the `torch.reshape` call after the final `dense`, which is effectively
    the opposite of the `flatten` call from right before the first `dense`. It takes
    the 4096-wide matrix and changes the shape so that it’s a 1024x4x4 matrix again.
  prefs: []
  type: TYPE_NORMAL
- en: We then run the data through the upscale layer and then the activation function
    before we return the result.
  prefs: []
  type: TYPE_NORMAL
- en: Building the decoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The decoder is responsible for taking the encoded face data and re-creating
    a face as accurately as it can. To do this, it will iterate over thousands or
    even millions of faces to get better at turning encodings into faces. At the same
    time, the encoder will be getting better at encoding faces.
  prefs: []
  type: TYPE_NORMAL
- en: We used the plural *decoders* here, but this code only actually defines a single
    decoder. That’s because the training code creates two copies of this decoder class.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define and initialize our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code, just like the encoder and upscaler, is an instance of `nn.Module`
    and needs an initialization function that also calls the parent’s initializer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define our activation function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Just like our encoder’s activation, we use `LeakeReLu` with a negative scaling
    of `0.1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define our upscaling tower:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The upscale tower is much like the convolution tower of the encoder but uses
    upscale blocks instead of shrinking convolutions. Because there was one upscaler
    in the encoder, we actually have one fewer upscales in this decoder. Just like
    the convolution tower, there are also activation functions after each upscale
    to keep the range trending positive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define our output layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output layer is special. While each of the previous layers’ outputs was
    half the depth of the previous layer’s, this one takes the 64-deep output from
    the convolution layer and converts it back to a three-channel image. There is
    nothing special about the three-channel dimension, but due to how the training
    process works, each is correlated to one of the color channels of the training
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we define the forward function of the decoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This forward function is familiar, being very similar to those in the encoder
    and the upscale layer. The major difference here is that after we pass the input
    through the upscale tower and the output layer, we use a `torch.sigmoid` layer.
    This is another type of activation layer.
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid works differently from LeakyReLu in that it is not linear. Instead,
    it computes the logistic sigmoid of the input. This is an s-shaped output where
    negative inputs approach `0`, and positive inputs approach `1` with a `0` input
    coming out as `0.5`. The precise equation is `1/(1*e^-input)`. This basically
    puts the results between `0` and `1` with extremes being more compressed, which
    matches how the multiplication of high numbers leads to higher numbers faster.
    This effectively turns the output of the model into a range of `0-1`, which we
    can easily turn into an image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – An example of the sigmoid curve](img/B17535_06_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – An example of the sigmoid curve
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll examine the training code.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the training code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have defined our models, we can go ahead with the process of training
    a neural network on our data. This is the part where we actually have AI learn
    the different faces so that it can later swap between them.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import our libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Like all Python programs, we import our libraries. We also import our encoder
    and decoders from our model file. This loads the AI model code from earlier in
    this chapter and lets us use those to define our models in this code. Python really
    makes it easy to import code we’ve already written, as every Python file can be
    called directly or imported into another file.
  prefs: []
  type: TYPE_NORMAL
- en: Note that Python uses a strange syntax for folder paths. Python treats this
    syntax exactly the same as a module, so you use a period to tell it to look in
    a folder and then give it the file you want. In this case, we’re pulling the `OriginalEncoder`
    and `OriginalDecoder` classes from the `models.py` file located in the `lib` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define our arguments and call our main function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we define our arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here we define our arguments. These give us the ability to change our settings,
    files, or details without having to modify the source code directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we parse all the arguments and call our main function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We parse our arguments and pass them into our main function. The main function
    will handle all the training processes, and we need to give it all the arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we start our main function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here we start our main function and check whether we’re supposed to use `cuda`.
    If so, we enable `cuda` so that we can use the **graphics processing unit** (**GPU**)
    to accelerate training. Then we create our export folder if that isn’t already
    created. This is where we’ll save copies of our models and any training previews
    we generate later.
  prefs: []
  type: TYPE_NORMAL
- en: Author’s note
  prefs: []
  type: TYPE_NORMAL
- en: While it’s possible to run the other parts of the process without a GPU, training
    is far more intensive, and running a training session on a **central processing
    unit** (**CPU**) will take a very large amount of time. Because of this, it’s
    recommended that at least this part be run with a GPU. If you don’t have one locally,
    you can rent one at any number of online services.
  prefs: []
  type: TYPE_NORMAL
- en: Creating our models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here we’ll create our neural models and fill them with weights:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll create instances of our previous models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this chunk of code, we create our AI models. We create one instance of the
    encoder and two separate decoders. We call them `a` and `b` here, but that’s entirely
    an arbitrary choice with no effect on the results. By default, we assume that
    you want to put the second face onto the first so in the case of this code, we’d
    be putting the face from `b` onto the frame from `a`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we load any previously saved models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here we check whether any models already exist in the given output folder. If
    they do, we load those model weights into the models we instantiated in the last
    section. To do this, we have PyTorch load the weights from the disk and then assign
    the weights to the model’s state dictionary. This lets PyTorch load the weights
    into the model and get it ready for training.
  prefs: []
  type: TYPE_NORMAL
- en: If there are no weights, then we skip this step. This means that the models
    will be initialized with random weights, ready to start a new training session.
    This lets you get started easily without having to generate any random weights
    yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we get a list of the images to train with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This chunk gets a list of all the images from the folders to train. We load
    only the aligned face images from the folders since we can create the filenames
    for the other images from the filenames for the aligned images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create the tensors for the images and the masks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here we create the tensors that will hold the images we will use for training.
    For the image tensors, we create a tensor that is 64x64 pixels wide with 3 channels
    to handle the red, green, and blue color channels. We also add a **batch size**
    dimension to the tensor so we can store that many images at once. A batch is simply
    how many images we’ll process at the same time. Larger batch sizes help the training
    process run more efficiently as we’re able to benefit from hardware that can do
    multiple tasks simultaneously as well as benefit from PyTorch grouping the tasks
    in the best order.
  prefs: []
  type: TYPE_NORMAL
- en: Author’s note
  prefs: []
  type: TYPE_NORMAL
- en: Larger batch sizes lead to more efficiencies in the training, so why don’t we
    set the batch size to 256 or 1024 instead of defaulting to 16? It’s because batch
    size is not a magic bullet. First, larger batches take more memory as the system
    must store every item of the batch at the same time. With large models, this can
    be prohibitive. Additionally, there is a side effect to large batch sizes. It’s
    the classic “forest for the trees,” meaning that larger batch sizes can help generalize
    over large sets of data but perform worse at learning specific details. So, picking
    the ideal batch size can be as important a question as anything else. A good rule
    of thumb for deepfakes is to keep batch size in double digits with 100+ tending
    to be too big and <10 to be avoided unless you have specific plans.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define and set up our optimizers and loss function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Optimizers are the part of PyTorch responsible for the most important part
    of training: backpropagation. This process is what changes the weights of the
    model and allows AI to “learn” and get better at re-creating the images we’re
    using for training. It’s responsible for more than just changing the weights but
    actually calculates how much to change them as well.'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we’re using the `torch.optim.Adam` optimizer. This is part of
    a family of optimizers proven to be very effective and flexible. We use it here
    because it’s what the original deepfake model used, but it’s still one of the
    most reliable and useful optimizers even today.
  prefs: []
  type: TYPE_NORMAL
- en: We pass each model the **learning rate** from our options. The learning rate
    is basically a scaling value of how much the optimizer should change the weights.
    Higher numbers change the weights more, which can lead to faster training at the
    cost of difficulty in fine-tuning since the changes being made are large. Lower
    learning rates can get better accuracy but cause training to take longer by being
    slower. We cut the learning rate of the encoder by half because we will actually
    be training it twice as often since it is being used to encode both faces.
  prefs: []
  type: TYPE_NORMAL
- en: The last thing we do here is to define our `torch.nn.MSEloss` provided by PyTorch.
    This is a loss called **mean squared error** (**MSE**). Let’s look at this word
    by word again.
  prefs: []
  type: TYPE_NORMAL
- en: '*Error* in mathematics is how far off the function is from a perfectly correct
    result. In our case, we are re-creating a face, so the loss function will compare
    the generated face to the original face and count how far off each pixel is from
    the correct answer. This gives a nice easy number for each pixel. Looking at each
    pixel is a bit too difficult, so next, our loss will take the average (mean) of
    all the pixels together. This gives a single number of how far off the AI was
    as a whole. Finally, that number is squared. This makes large differences stand
    out even more and has been shown to help the model reach a good result faster.'
  prefs: []
  type: TYPE_NORMAL
- en: There are other loss functions such as **mean absolute error** (**MAE**), which
    gets rid of the squaring in the MSE, or **structural similarity**, which uses
    the similarity of the structure as a measure. In fact, **generative adversarial
    networks** (**GANs**), which are a buzzword of the machine learning field, simply
    replace the static loss of an auto-encoder with another model that provides a
    trainable loss function and pits the two models against each other in a competition
    of which model can do their job better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we move everything to the GPU if enabled:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If `cuda` was enabled earlier, we need to move the models and the variables
    to the GPU so we can process them. So here, we check whether `cuda` was enabled,
    and if so, we move each of them to the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Looping over the training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to train the model, we need to loop over all the data. We call this
    the training loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a progress bar and start the training loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use `tqdm` again for a progress bar. Here we pass a range of how many iterations
    we want to `tqdm` so it can update our progress bar automatically and assign the
    progress bar to a variable. We then start our loop from that variable to provide
    more information in the progress bar by calling functions that `tqdm` exposes
    in the variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we load a random set of images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This chunk loads a set of images from the `a` set for the model to train with.
    To do this, we get a random sample the same size as our batch size from the list
    of files we generated earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we go over a loop for each of those images. The loop first reads in the
    face image and resizes it down to 64x64\. Then it does the same for the mask image
    by replacing the `"aligned"` word in the filename with `"mask"`, which matches
    the mask filenames. The mask is also resized to match the training image.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we randomly get a 50% chance to flip the images horizontally. This is
    an extremely common way to get more variety out of the dataset. Since faces are
    generally pretty symmetrical, we can usually flip them. We use a 50% chance here,
    which gives us an equal chance of the image being flipped or not. Since we have
    a mask, we have to flip it, too, if we flip the image.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we convert both the image and masks from image arrays into tensors. To
    do this to the image, we convert from `[...,::-1]`. This can also be done again
    to get it back to the BGR order (which we’ll do later). The mask is simpler since
    we don’t care about color data for it, so we just see if the pixel data is greater
    than 200; if it is, we put `1` into the tensor; if not, we put `0`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we check whether `cuda` is enabled; if it is, we move the tensors we just
    created to the GPU. This puts everything onto the same device.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we move the image into the tensor we’ll be using to train the model.
    This lets us batch the images together for efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we do the same for the other set of images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code is identical to the last code but is repeated for the `b` set of images.
    This creates our second set of images ready to train.
  prefs: []
  type: TYPE_NORMAL
- en: Teaching the network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now it’s time to actually perform the steps that train the network:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we clear the optimizers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: PyTorch is very flexible in how it lets you build your models and training process.
    Because of this, we need to tell PyTorch to clear the `a` side decoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we pass the images through the encoder and decoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This chunk sends the image tensors we created earlier through the encoder and
    then through the decoder and stores the results for comparison. This is actually
    the AI model, and we give it a tensor of images and get a tensor of images back
    out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we calculate our loss and send it through the optimizers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This chunk does the rest of the training, calculating the loss and then having
    the optimizers perform backpropagation on the models to update the weights. We
    start by passing the output images and the original images to the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: To apply the masks, we multiply the images by the masks. We do this here instead
    of before we pass the images to the model because we might not have the best masks,
    and it’s better to train the neural network on the whole image and apply the mask
    later.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we call `backward` on the loss variable. We can do this because the variable
    is actually still a tensor, and tensors keep track of all actions that happened
    to them while in training mode. This lets the loss be carried back over all the
    steps back to the original image.
  prefs: []
  type: TYPE_NORMAL
- en: The last step is to call the `step` function of our optimizers. This goes back
    over the model weights, updating them so that the next iteration should be closer
    to the correct results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we do the same thing, but for the `b` decoder instead:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We go through the same process again with the `b` images and decoder. Remember
    that we’re using the same encoder for both models, so it actually gets trained
    a second time along with the `b` decoder. This is a key part of how deepfakes
    can swap faces. The two decoders share a single encoder, which eventually gives
    both the decoders the information to re-create their individual faces.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we update the progress bar with information about this iteration’s loss
    of data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Since the loss function outputs a number for the optimizers, we can also display
    this number for the user. Sometimes loss is used by deepfakers as an estimate
    of how finished the model is training. Unfortunately, this cannot actually measure
    how good a model is at converting one face into another; it only scores how good
    it is at re-creating the same face it was given. For this reason, it’s an imperfect
    measure and shouldn’t be relied on. Instead, we recommend that the previews we’ll
    be generating later be used for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Saving results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, we will save our results:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll check to see whether we should trigger a save:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We want to save regularly – an iteration may take less than a second, but a
    save could take several seconds of writing to disk. Because of this, we don’t
    want to save every iteration; instead, we want to trigger a save on a regular
    basis after a set number of iterations. Different computers will run at different
    speeds, so we let you set the save frequency with an argument.
  prefs: []
  type: TYPE_NORMAL
- en: One thing we want to save along with the current weights is a preview image
    so we can get a good idea of how the model is doing at each save state. For this
    reason, we’ll be using the neural networks, but we don’t want to train while we’re
    doing this step. That’s the exact reason that `torch` has the `torch.no_grad`
    context. By calling our model from inside this context, we won’t be training and
    just getting the results from the network.
  prefs: []
  type: TYPE_NORMAL
- en: We call each decoder with samples of images from both faces. This lets us compare
    the re-created faces along with the generated swaps. Since we only want a preview
    image, we can throw out all but the first image to be used as a sample of the
    current stage of training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create the sample image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We need to create our sample image from all the parts. To do this, we need to
    convert all the image tensors into a single image. We use `np.concatenate` to
    join them all into a single array along the width axis. To do this, we need to
    get them all into image order and convert them to NumPy arrays. The first thing
    we do is drop the batch dimension by selecting the first one. Then we use `permute`
    to reorder each tensor, so the channels are last. Then we use `detach` to remove
    any gradients from the tensors. We can then use `cpu` to bring the weights back
    onto the CPU. Finally, we use `numpy` to finish converting them into NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we write the preview image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This chunk uses `cv2.imwrite` from OpenCV to write out the preview image as
    a PNG file. We put it in the output path and give it a name based on what iteration
    this is. This lets us save each iteration’s preview together and track the progress
    of the network over time. To actually write out a usable image, we have to convert
    the color space back to the BGR that OpenCV expects, and then we multiply by `255`
    to get a result that fits into the integer space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we save the weights to a file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here we save the weights for our encoder and both decoders by calling `torch.save`
    with the output path and the filenames we want to use to save the weights. PyTorch
    automatically saves them to the file in its native `pth` format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we save again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For our last step, we repeat the save code. But, actually, this is done outside
    of the training loop. This is here just in case the training loop ends on an iteration
    number that doesn’t trigger the save there. This way, the model is definitely
    saved at least once, no matter what arguments are chosen by the user.
  prefs: []
  type: TYPE_NORMAL
- en: Author’s note
  prefs: []
  type: TYPE_NORMAL
- en: This may seem obvious to anyone who has spent any time coding, but it’s worth
    repeating. You should always consider how choices made in the development process
    might lead to bad or unexpected outcomes and account for those in your design.
    It’s obviously impossible to consider everything, but sometimes even something
    as simple as duplicating a save right before exiting, “just in case,” can save
    someone’s day (or in the case of a very long training session, even more time).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we trained a neural network to swap faces. To do this, we had
    to explore what convolutional layers are and then build a foundational upscaler
    layer. Then we built the three networks. We built the encoder, then two decoders.
    Finally, we trained the model itself, including loading and preparing images,
    and made sure we saved previews and the final weights.
  prefs: []
  type: TYPE_NORMAL
- en: First, we built the models of the neural networks that we were going to train
    to perform the face-swapping process. This was broken down into the upscaler,
    the shared encoder, and the two decoders. The upscaler is used to increase the
    size of the image by turning depth into a larger image. The encoder is used to
    encode the face image down into a smaller encoded space that we then pass to the
    decoders, which are responsible for re-creating the original image. We also looked
    at activation layers to understand why they’re helpful.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we covered the training code. We created instances of the network models,
    loaded weights into the models, and put them on the GPU if one was available.
    We explored optimizers and loss functions to understand the roles that they play
    in the training process. We loaded and processed images so that they were ready
    to go through the model to assist in training. Then we covered the training itself,
    including how to get the loss and apply it to the model using the optimizers.
    Finally, we saved preview images and the model weights themselves so we could
    load them again.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll take our trained model and use it to “convert” a
    video, swapping the faces.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Choosing learning rate is not a solved problem. There is no one “right” learning
    rate. What happens if you make the learning rate 10 times bigger? 10 times smaller?
    What if you start with a large learning rate and then reduce it after some training?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We used the same loss function and optimizer as the original code, which was
    first released back in 2018, but there are a lot of options now that weren’t available
    then. Try replacing the loss function with others from PyTorch’s extensive collection
    ([https://pytorch.org/docs/stable/nn.html#loss-functions](https://pytorch.org/docs/stable/nn.html#loss-functions)).
    Some of them will work without any change, but some won’t work for our situation
    at all. Try different ones, or even try combinations of loss functions!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We defined a model that downscaled from a 64x64 pixel image and re-created that
    same image. But with some tweaks, this same architecture can instead create a
    128x128 or 256x256 pixel image. How would you make changes to the model to do
    this? Should you increase the number (and size) of layers in the convolutional
    tower and keep the bottleneck the same, increase the size of the bottleneck but
    keep the layers the same, or change the convolutional layer’s kernel sizes and
    strides? It’s even possible to send in a 64x64 pixel image and get out a 128x128
    pixel image. All of these techniques have their advantages and drawbacks. Try
    each out and see how they differ.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We’re training on just two faces, but you could potentially do more. Try modifying
    the training code to use three different faces instead of just two. What changes
    would you need to make? What modifications would you make so that you can train
    an arbitrary number of faces at once?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the years since deepfakes were first released, there have been a lot of different
    models created. Faceswap has implementations for many newer and more advanced
    models, but they’re written in Keras for Tensorflow and can’t work in this PyTorch
    fork without being ported. Check the Faceswap models at the GitHub repo ([https://github.com/deepfakes/Faceswap/tree/master/plugins/train/model](https://github.com/deepfakes/Faceswap/tree/master/plugins/train/model)).
    Compare this model to the one in `Original.py`, which implements the same model.
    Now use that to see how `dfaker.py` differs. The residual layer works by adding
    an extra convolution layer and an `add` layer, which just adds two layers together.
    Can you duplicate the `dfaker` model in this code base? What about the others?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: EBSCOhost - printed on 11/27/2023 6:20 AM via . All use subject to [https://www.ebsco.com/terms-of-use](https://www.ebsco.com/terms-of-use)
  prefs: []
  type: TYPE_NORMAL
