- en: <title>Ingesting Data into Our RAG Workflow</title>
  prefs: []
  type: TYPE_NORMAL
- en: Ingesting Data into Our RAG Workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve taken a good look at the overall structure of LlamaIndex from afar. It’s
    now time to get much closer and understand the small details of this framework.
    It’s bound to get more technical but also more intriguing as we go further.
  prefs: []
  type: TYPE_NORMAL
- en: Ready to go deeper down the rabbit hole? Follow me!
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the LlamaHub connectors to ingest our data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking advantage of the many text-chunking tools in LlamaIndex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infusing our nodes with metadata and relationships
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keeping our data private and our budget safe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating ingestion pipelines for better efficiency and lower costs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <title>Technical requirements</title>
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will need to install the following Python libraries in your environment
    to be able to run the examples included in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LangChain** : https://www.langchain.com/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Py-Tree-Sitter** : https://pypi.org/project/tree-sitter/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition, several LlamaIndex Integration packages will be required:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Entity** **extractor** : https://pypi.org/project/llama-index-extractors-entity/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hugging Face** **LLMs** : https://pypi.org/project/llama-index-llms-huggingface/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Database** **reader** : https://pypi.org/project/llama-index-readers-database/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Web** **reader** : https://pypi.org/project/llama-index-readers-web/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All the code examples in this chapter can be found in the *ch4* subfolder of
    this book’s GitHub repository: https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex
    .'
  prefs: []
  type: TYPE_NORMAL
- en: <title>Ingesting data via LlamaHub</title>
  prefs: []
  type: TYPE_NORMAL
- en: Ingesting data via LlamaHub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we saw in *Chapter 3* , *Kickstarting Your Journey with LlamaIndex* , one
    of the first steps in a RAG workflow is to ingest and process our proprietary
    data. We already discovered the concepts of documents and nodes, which are used
    to organize the data and prepare it for indexing. I’ve also briefly introduced
    the LlamaHub data loaders as a way to easily ingest data into LlamaIndex. It’s
    time to examine these steps in more detail and gradually learn how to infuse LLM
    applications with our own, proprietary knowledge. Before we continue, though,
    I’d like to emphasize some very common challenges encountered at this step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'No matter how effective our RAG pipeline is, at the end of the day, the quality
    of the final result will largely depend on the quality of the initial data. To
    overcome this challenge, make sure you start by cleaning up your data first. Eliminate
    potential duplicates and errors. While not exactly duplicates, redundant information
    can also clutter your knowledge base and confuse the RAG system. Be on the lookout
    for ambiguous, biased, incomplete, or outdated information. I’ve seen many cases
    of poorly structured and insufficiently maintained knowledge repositories that
    were completely useless for users looking for quick and accurate answers. Ask
    yourself this question: *If I were to manually search through this data, how easy
    would it be to find the information I need?* Before moving on with building the
    pipeline, do yourself a favor and prepare your data thoroughly until you’re satisfied
    with the answer to that question.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our data is dynamic. An organizational knowledge repository is rarely a static,
    permanent data source. It evolves with the business, reflecting new insights,
    discoveries, and changes in the external environment. Recognizing this fluid nature
    is key to maintaining a relevant and effective system. To overcome this challenge,
    in a production RAG application, you’ll have to implement a systematic method
    for periodically reviewing and updating the content, ensuring that new information
    is incorporated and outdated or incorrect data is removed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data comes in many flavors, shapes, and sizes. Sometimes, it’s structured, sometimes
    not. A well-built RAG system should be able to properly ingest all kinds of formats
    and document types. While LlamaIndex provides a huge number of data loaders for
    many different APIs, databases, and document types, building an automated ingestion
    system can still prove to be challenging. To overcome this particular challenge,
    later in this section, we’ll cover **LlamaParse** – an innovative hosted service
    designed to automatically ingest and process data from different data sources.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we know what kind of problems await along the way, let’s start our
    journey by first discussing the simplest ways of ingesting the data into the RAG
    pipeline – by using the available LlamaHub data loaders.
  prefs: []
  type: TYPE_NORMAL
- en: <title>An overview of LlamaHub</title>
  prefs: []
  type: TYPE_NORMAL
- en: An overview of LlamaHub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LlamaHub is an extensive library of integrations that augments the capabilities
    of the core framework. Among many other types of integrations, LlamaHub contains
    numerous **connectors** – also known as **data readers** or **data loaders** –
    specially built to allow seamless integration of external data with LlamaIndex.
    There are over 180 readily available data readers spanning a wide range of data
    sources and formats, and the list is constantly increasing.
  prefs: []
  type: TYPE_NORMAL
- en: These connectors act as a standard way to ingest data, extracting data from
    sources such as databases, APIs, files, and websites and converting it into LlamaIndex
    `Document` objects. This relieves you from the burden of writing customized parsers
    and connectors for every new data source. But of course, if you’re not satisfied
    with the existing connectors, you can always build your own and contribute to
    the collection.
  prefs: []
  type: TYPE_NORMAL
- en: LlamaHub empowers you to tap into diverse data sources with just a few lines
    of code. The resulting Document objects can then be parsed into nodes and indexed
    as required by your application. The unified output as LlamaIndex `Document` objects
    means your core business logic does not have to worry about handling various data
    types. The complexity is abstracted by the framework.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need so many integrations?
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Chapter 2* , *LlamaIndex: The Hidden Jewel - An Introduction to the LlamaIndex
    Ecosystem* , in the *Familiarizing ourselves with the structure of the LlamaIndex
    code repository* section, I explained the motives behind the framework’s modular
    architecture. Because of this modular architecture, many RAG components provided
    by LlamaIndex are not included in the core elements that are installed together
    with the rest of the framework. This means that before using any data loader for
    the first time, we have to install the corresponding integration package. Once
    the package has been installed, we’ll be able to import the reader into our code
    and use its functionality. Some readers also utilize specialized libraries and
    tools tailored to each data type. For example, `PDFReader` leverages Camelot and
    Tika for parsing PDF content. `AirbyteSalesforceReader` uses the Salesforce API
    client, and so on. This allows us to efficiently adapt to the format and interface
    of each source but may require us to install additional packages in our development
    environment.'
  prefs: []
  type: TYPE_NORMAL
- en: All available readers are listed on the LlamaHub website and usually come with
    detailed documentation and usage samples. Therefore, I’ll briefly cover just a
    few examples to give a general idea of how you can use them in your applications.
  prefs: []
  type: TYPE_NORMAL
- en: I strongly encourage you to take your time and go through the entire list of
    data readers when building your LlamaIndex apps instead of spending valuable time
    building one from scratch. Chances are you’ll just be reinventing the wheel.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re looking to consult the source code for the readers, you’ll find them
    all included in the Llama-index GitHub repository, under the `llama-index-integrations/readers`
    subfolder: https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/readers
    .'
  prefs: []
  type: TYPE_NORMAL
- en: The LlamaHub documentation for each data reader lists its installation requirements
    and usage guidance, so before trying to use them, make sure you also install any
    additional dependencies required by specific connectors you want to use.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Using the LlamaHub data loaders to ingest content</title>
  prefs: []
  type: TYPE_NORMAL
- en: Using the LlamaHub data loaders to ingest content
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'pip install llama-index-readers-web from llama_index.readers.web import SimpleWebPageReader
    urls = ["https://docs.llamaindex.ai"] documents = SimpleWebPageReader().load_data(urls)
    for doc in documents:     print(doc.text) pip install llama-index-readers-database
    from llama_index.readers.database import DatabaseReader reader = DatabaseReader(
        uri="sqlite:///files/db/example.db" ) query = " from llama_index.core import
    SimpleDirectoryReader reader = SimpleDirectoryReader(     input_dir="files",     recursive=True
    ) documents = reader.load_data() for doc in documents:     print(doc.metadata)
    files = [" from llama_parse import LlamaParse from llama_index.core import SimpleDirectoryReader
    from llama_index.core import VectorStoreIndex parser = LlamaParse(result_type="text")
    file_extractor = {".pdf": parser} reader = SimpleDirectoryReader(     "./files/pdf",
        file_extractor=file_extractor ) docs = reader.load_data() index = VectorStoreIndex.from_documents(docs)
    qe = index.as_query_engine() response = qe.query(     " Started parsing the file
    under job_id <…>'
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the *Wikipedia* reader that we discussed in the previous chapter,
    to get a better understanding of how data readers work, let’s look at a few more
    examples of LlamaHub readers that we can use to ingest data.
  prefs: []
  type: TYPE_NORMAL
- en: Ingesting data from a web page
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`SimpleWebPageReader` can extract text content from web pages.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To use it, we must first install the corresponding integration:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once installed, it’s really easy to use:'
  prefs: []
  type: TYPE_NORMAL
- en: This loads and displays the text content of the specified web pages into documents.
  prefs: []
  type: TYPE_NORMAL
- en: At its core, `SimpleWebPageReader` serves as a bridge between the vast, unstructured
    world of the internet and the structured environment of the LlamaIndex RAG pipeline.
    To better understand its inner workings, let’s explore what happens under the
    hood when it extracts text content from web pages.
  prefs: []
  type: TYPE_NORMAL
- en: When loading the data, `SimpleWebPageReader` iterates over a list of URLs provided
    by the user. For each URL, it performs a web request to fetch the page content.
    The response, initially in HTML format, can be transformed into plain text if
    the `html_to_text` flag is set to `True` . This transformation strips away the
    HTML tags and converts the web page content into a more digestible text format.
    However, remember what I’ve said about external dependencies for these readers?
    In this case, the HTML-to-text conversion feature requires the `html2text` package,
    which has to be installed first.
  prefs: []
  type: TYPE_NORMAL
- en: Another significant aspect of this reader is its ability to attach metadata
    to the scraped documents. Through the `metadata_fn` parameter, we can pass a custom
    function that takes a URL as input and returns a dictionary of metadata. This
    flexibility allows for the enrichment of documents with additional information
    or any relevant tags that might be useful in categorizing and understanding the
    context of the data better. Should the user provide a `metadata_fn` parameter,
    the reader then applies this function to the current URL to extract metadata,
    enriching the final `Document` object with this additional layer of information.
  prefs: []
  type: TYPE_NORMAL
- en: A practical use case for the metadata_fn function
  prefs: []
  type: TYPE_NORMAL
- en: We could, for example, use a function that simply returns the current date and
    time. That way, we could ingest the same URL at different moments and build a
    chronological timeline highlighting different versions of that page at various
    points in time. This could prove useful in scenarios such as browsing a code repository
    or answering questions about a developing news story.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, each web page’s content, along with its URL and optionally added metadata,
    is encapsulated in a `Document` object. These objects are then collected into
    a list, providing a structured representation of the text content and metadata
    extracted from each web page.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to keep in mind
  prefs: []
  type: TYPE_NORMAL
- en: As its name suggests, this reader is a simple tool. While it can be effective
    for reading simple web pages, for more advanced cases such as pages requiring
    interaction (for example, navigating a login process or handling JavaScript-rendered
    content), `SimpleWebPageReader` might not be sufficient. Websites that dynamically
    generate content based on user interactions or rely heavily on client-side scripting
    can pose challenges that this basic scraper is not designed to handle.
  prefs: []
  type: TYPE_NORMAL
- en: Through `SimpleWebPageReader` , the task of ingesting and structuring basic
    web content is simplified. The great thing about these readers is that they allow
    us to focus on building and enhancing the logic of our RAG applications instead
    of spending precious time on building compatible ingestion tools for each type
    of data in our knowledge base.
  prefs: []
  type: TYPE_NORMAL
- en: Ingesting data from a database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using databases is not only a common practice but also a highly efficient method
    for managing and retrieving structured information. Databases offer a robust platform
    for storing a vast array of data types, from simple text to complex relationships
    between entities, making them an indispensable asset in data management.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `DatabaseReader` connector allows querying many database systems. First,
    we need to install the necessary integration package:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of how you can easily fetch the contents of an SQLite database:'
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, `DatabaseReader` connects to various databases to fetch data
    and transform it into a format usable by the RAG pipeline. It supports connection
    through a `SQLDatabase` instance, a **SQLAlchemy Engine** , a connection URI,
    or a set of database credentials – provided through the `scheme` , `host` , `port`
    , `user` , `password` , and `dbname` arguments. Once set up, it executes a provided
    SQL query to retrieve data. After connecting to the database, the reader executes
    the provided `query` . The resulting rows are then converted into Document objects,
    with each row from the query result forming a single Document. The conversion
    process involves concatenating each column-value pair into a string, which is
    then assigned as the text of a document.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example I have provided executes the SQL query against an SQLite database
    stored in the `ch4/files/db` folder, loads each returned row as a Document, and
    displays the results. You can find a more general example on the official project
    documentation website: https://docs.llamaindex.ai/en/stable/examples/data_connectors/DatabaseReaderDemo.html
    .'
  prefs: []
  type: TYPE_NORMAL
- en: Alright – I think you understand the workflow now. As you’ve probably noticed,
    the approach for using LlamaHub readers is very straightforward. In all the examples,
    first, we install the required integration package, as described on LlamaHub,
    and then use it to import and load data from the reader. Apart from the examples
    I have provided, you’ll find a huge number of data readers available on LlamaHub.
    From Office documents, Gmail accounts, videos and images, YouTube videos, and
    RSS feeds to GitHub repositories and Discord chats, pretty much every popular
    data format is supported.
  prefs: []
  type: TYPE_NORMAL
- en: But apart from reading individual files using dedicated data readers, in the
    next section, we will also explore more efficient methods that can be used for
    ingesting multiple documents at once.
  prefs: []
  type: TYPE_NORMAL
- en: Bulk-ingesting data from sources with multiple file formats
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Loading data into LlamaIndex is a crucial first step. But sifting through the
    wide range of data loaders in LlamaHub and figuring out how to configure each
    one can feel overwhelming early on. That’s why I’m going to show you two different
    methods that can greatly simplify and reduce the burden of data ingestion for
    your RAG systems.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start with the simple method first.
  prefs: []
  type: TYPE_NORMAL
- en: Using SimpleDirectoryReader to ingest multiple data formats
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When you just want to get started fast or have a simple use case, `SimpleDirectoryReader`
    comes to the rescue. Think of this reader as your trusty pocketknife for bulk
    data ingestion. It’s easy to use, requires minimal setup, and automatically adapts
    to different file types. To load data, you simply point the reader to a folder
    or list of files. Loading a folder containing PDFs, Word docs, plain text files,
    and CSVs is very straightforward. Here’s a demonstration:'
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood
  prefs: []
  type: TYPE_NORMAL
- en: '`SimpleDirectoryReader` has built-in methods to determine which reader works
    best for each file type. You don’t need to worry about those details. It will
    automatically detect formats such as PDF, DOCX, CSV, plain text, and others based
    on the file extensions. Then, it chooses the best tool to extract the content
    into Document objects. For plain text files, it simply reads the text content.
    For binary files such as PDFs and Office docs, it uses libraries such as PyPDF
    and Pillow to extract the text.'
  prefs: []
  type: TYPE_NORMAL
- en: '`SimpleDirectoryReader` effortlessly handles the different files and returns
    the parsed content as documents. By default, it only processes files in the directory’s
    top level. To include subdirectories, you can set the `recursive` parameter to
    `True` .'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also pass in a list of specific files to load, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: The result is a batch of Document objects ready for indexing in just a few lines
    of code. No headaches setting up separate data readers for each file type. When
    you want quick and easy data ingestion without the complexity, let `SimpleDirectoryReader`
    handle the hard work! It’s versatile and automated.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing like a pro with the help of LlamaParse
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While `SimpleDirectoryReader` is great for quick and easy data ingestion, sometimes,
    you need more advanced parsing capabilities, especially for complex file formats.
    Most of the time, we have to deal with complex file structures containing a mix
    of data. For example, a PDF file may include images, charts, code snippets, mathematical
    formulas, and other elements alongside its text content. The naive readers included
    in the LlamaHub integration library will be overwhelmed by such cases. They would
    most probably fail to extract the entire content or – even worse – mess up the
    extracted data and complicate its further processing.
  prefs: []
  type: TYPE_NORMAL
- en: This is where LlamaParse shines. Provided through the LlamaCloud enterprise
    platform ( https://cloud.llamaindex.ai/parse ), this reader is implemented through
    a cutting-edge hosted service that integrates seamlessly with the other components
    of the framework. It uses multi-modal capabilities and LLM intelligence under
    the hood to provide industry-leading document parsing, including exceptional support
    for tricky formats such as PDFs containing tables, figures, and equations.
  prefs: []
  type: TYPE_NORMAL
- en: One of the standout features of `LlamaParse` is that it allows you to provide
    natural language instructions to guide the parsing by using the `parsing_instruction`
    parameter. Since you know your documents best, you can tell `LlamaParse` exactly
    what kind of output you need and how that information should be extracted from
    the files.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance:'
  prefs: []
  type: TYPE_NORMAL
- en: When parsing a technical whitepaper, you could instruct it to extract all the
    section headings, ignore the footnotes, and output any code snippets in markdown
    format. `LlamaParse` will follow your instructions to parse the document accurately.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the instruction-guided parsing mode, `LlamaParse` also offers
    a JSON output mode that provides rich structured data about the parsed document,
    including marking tables, headings, extracting images, and more. Also, for bulk-ingesting
    entire folders in one go, `LlamaParse` can be used in combination with `SimpleDirectoryReader`
    , as you will see in the next example. This gives you full flexibility to build
    custom RAG applications over a complex collection of documents. You could also
    accomplish this manually by using specialized data readers for each file format
    in your collection of data. However, using `LlamaParse` will greatly simplify
    this process, improve the overall quality, and save you a lot of time.
  prefs: []
  type: TYPE_NORMAL
- en: '`LlamaParse` supports a wide and expanding range of file types beyond just
    PDFs, including Word docs, PowerPoint, RTF, ePub, and many more. It offers a generous
    free tier to get started.'
  prefs: []
  type: TYPE_NORMAL
- en: The necessary `LlamaParse` integration package should already be installed along
    with the LlamaIndex components, so no additional installation is required to run
    the code example in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to create a free account on https://cloud.llamaindex.ai and
    obtain an API key. Once you have obtained the key, you can use it directly in
    your code, but for a more secure approach, I strongly encourage you to follow
    the same steps we followed in *Chapter 2* , *LlamaIndex: The Hidden Jewel - An
    Introduction to the LlamaIndex Ecosystem* , and add the key as a variable in your
    local environment under the name `LLAMA_CLOUD_API_KEY` . To demonstrate the capabilities
    of this tool, I’ve designed a sample PDF with a more complex structure, as can
    be seen in *Figure 4* *.1* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_04_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – A sample PDF containing multiple articles, images, and tables
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a basic code example that uses `LlamaParse` to ingest this PDF:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part of the code imported the necessary modules. Next, we’ll configure
    `LlamaParse` and pass it to `SimpleDirectoryReader` as a `file_extractor` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the PDF content has been ingested into a new Document object, it’s time
    to build an index and run a query against our data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of this script should be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: One important consideration with using a hosted service such as `LlamaParse`
    is data privacy. Before submitting your proprietary data through the API, be sure
    to carefully review their privacy policy to ensure it aligns with your data protection
    requirements. While the service offers powerful parsing capabilities, it’s crucial
    to safeguard sensitive information.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that this is a paid service. The great news, however, is that you
    can take advantage of their generous **free tier** . For higher volume needs,
    the current pricing can be found on the website. If you want to unlock the full
    potential of `LlamaParse` to build advanced document retrieval systems or deploy
    it on your private cloud for maximal data security, that option is available as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: For professional, production-ready applications, `LlamaParse` is a powerful
    tool that puts you in full control of parsing your data to maximize the quality
    of your knowledge base and RAG applications.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve got the data, let’s make it easier to handle by breaking it down
    into smaller pieces.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Parsing the documents into nodes</title>
  prefs: []
  type: TYPE_NORMAL
- en: Parsing the documents into nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'from llama_index.core.node_parser import < for node in nodes:     print(f"Metadata
    {node.metadata} \nText: {node.text}") splitter = TokenTextSplitter(     chunk_size
    = 70,     chunk_overlap = 2,     separator = " ",     backup_separators = [".",
    "!", "?"] ) nodes = splitter.get_nodes_from_documents(document) pip install tree_sitter
    pip install tree_sitter_languages code_splitter = CodeSplitter.from_defaults(
        language = ''python'',     chunk_lines = 5,     chunk_lines_overlap = 2,     max_chars
    = 150 ) nodes = code_splitter.get_nodes_from_documents(document) parser = SentenceWindowNodeParser.from_defaults(
        window_size=2,     window_metadata_key="text_window",     original_text_metadata_key="original_sentence"
    ) nodes = parser.get_nodes_from_documents(document) pip install langchain from
    langchain.text_splitter import CharacterTextSplitter from llama_index.core.node_parser
    import LangchainNodeParser parser = LangchainNodeParser(CharacterTextSplitter())
    nodes = parser.get_nodes_from_documents(document) parser = SimpleFileNodeParser()
    nodes = parser.get_nodes_from_documents(documents) my_tags = [" parser = MarkdownNodeParser.from_defaults()
    nodes = parser.get_nodes_from_documents(document) json_parser = JSONNodeParser.from_defaults()
    nodes = json_parser.get_nodes_from_documents(document) hierarchical_parser = HierarchicalNodeParser.from_defaults(
        chunk_sizes=[128, 64, 32],     chunk_overlap=0, ) nodes = hierarchical_parser.get_nodes_from_documents(document)
    node_parser = SentenceWindowNodeParser.from_defaults(   include_prev_next_rel=True
    ) node1.relationships[PREVIOUS] = RelatedNodeInfo(node_id=node0.node_id) node2.relationships[NEXT]
    = RelatedNodeInfo(node_id=node3.node_id)'
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in *Chapter 3* , *Kickstarting Your Journey with LlamaIndex* , the
    next step is to split the documents into nodes. In many cases, documents tend
    to be very large, so we need to break them down into smaller units called nodes.
    Working at this granular level allows for better handling of our content while
    maintaining an accurate representation of its internal structure. This is the
    basic mechanism that LlamaIndex uses to manage our proprietary data content more
    easily.
  prefs: []
  type: TYPE_NORMAL
- en: Now is the time to understand how nodes can be generated in LlamaIndex and what
    customization opportunities we have along the way. In the previous chapter, we
    talked about how to manually create nodes. But that was merely a way to simplify
    the explanation and help you better understand their mechanics. In a real application,
    most likely, we will want to use some automatic methods to generate them from
    the ingested documents. So, that’s what we’ll focus on going forward.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will discover different ways of chunking a document. We’ll
    start by understanding simple **text splitters** – which operate on raw text –
    and then we’ll cover the more advanced **node parsers** – which are capable of
    interpreting more complex formats and following the document structure when extracting
    the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the simple text splitters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Text splitters** *break down* the document into smaller pieces operating
    at the raw text level. They are useful when the content has a *flat* structure
    and does not come in a specific format.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the following examples, make sure you add the necessary imports and
    the document reading logic, using `FlatReader` , at the beginning of your code
    for all examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, if you want to see the actual nodes generated by the code, you can add
    something like this *after* running the parsers:'
  prefs: []
  type: TYPE_NORMAL
- en: Alright. Let’s see what’s in store in the *text* *splitter* category.
  prefs: []
  type: TYPE_NORMAL
- en: SentenceSplitter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This one splits text while maintaining sentence boundaries, providing nodes
    containing groups of sentences. You saw an example of using this parser in *Chapter
    3* , *Kickstarting Your Journey with LlamaIndex* , in the *Automatically extracting
    nodes from documents using* *splitters* section.
  prefs: []
  type: TYPE_NORMAL
- en: TokenTextSplitter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This splitter breaks down text while respecting sentence boundaries to create
    suitable nodes for further natural language processing. It operates at the token
    level.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical usage in code would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some notes on the parameters of this splitter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`chunk_size` : This sets the maximum number of tokens for each chunk'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunk_overlap` : This defines the overlap in tokens between consecutive chunks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`separator` : This is used to determine the primary token boundary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`backup_separators` : These can be used for additional splitting points if
    the primary separator doesn’t split the text sufficiently'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CodeSplitter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This smart splitter knows how to interpret source code. It splits text based
    on programming language and is ideal for managing technical documentation or source
    code. Before running the example, make sure you install the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at an example of how to use this splitter in your code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, there are several parameters you can tune with this splitter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`language` : This specifies the language of the code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunk_lines` : This defines the number of lines per chunk'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunk_lines_overlap` : This defines the lines overlap between chunks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_chars` : This defines the maximum characters per chunk'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quick side note on CodeSplitter
  prefs: []
  type: TYPE_NORMAL
- en: This splitter is cleverly built around a concept called **abstract syntax tree**
    ( **AST** ). An AST is a key idea in computer science that’s mainly used in creating
    programs that translate or interpret code. It’s like a branching diagram that
    shows the basic structure of the code written in a programming language. Each
    point on the diagram represents a different part or piece of the code. Because
    of this splitter’s awareness of AST, when you’re splitting code, you keep related
    statements together as much as possible, which is vital when you need to maintain
    the logical flow of code to understand or process it later.
  prefs: []
  type: TYPE_NORMAL
- en: Using more advanced node parsers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text splitters only provide basic logic for breaking down text, mostly by using
    simple rules. We also have more advanced tools for chunking text into nodes. These
    are designed to process various standard file formats or can be used for more
    specific types of content.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we continue, keep in mind that all the node parsers that we will discuss
    here are derived from the generic class called `NodeParser` . Each parser has
    various parameters that can be configured according to the use case, but at the
    base, three common elements can be customized for all parsers:'
  prefs: []
  type: TYPE_NORMAL
- en: '`include_metadata` : This determines whether the parser should take into account
    the metadata or not. By default, this is set to `True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Include_prev_next_rel` : This determines whether the parser should automatically
    include **prev/next** type relationships between nodes. Again, the default value
    is `True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Callback_manager` : This can be used to define a specific **callback function**
    . These functions can be used for debugging, tracing, and cost analysis, among
    other functions. We will talk more about them in *Chapter 10* , *Prompt Engineering
    Guidelines and* *Best Practices*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from these three general options, each parser provides specific parameters
    to customize. You can get a complete list of configurable parameters for each
    parser by consulting the official documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore the node parsers available in LlamaIndex.
  prefs: []
  type: TYPE_NORMAL
- en: SentenceWindowNodeParser
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Based on the simple **SentenceSplitter** , this parser splits text into individual
    sentences and also includes a *window* of surrounding sentences in the metadata
    of each node. It is useful for building more context around each sentence. During
    the querying process, that context will be fed into the LLM and allow for better
    responses. We can use it like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this parser, three specific parameters can be customized:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Window_size` : This defines the number of sentences on each side to include
    in the window'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`window_metadata_key` : This defines the metadata key for the window sentences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`original_text_metadata_key` : This defines the metadata key for the original
    sentence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangchainNodeParser
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you prefer using the LangChain splitters, this parser allows using any text
    splitter from the Langchain collection, extending the parsing options offered
    by LlamaIndex.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a prerequisite for the next example, you’ll have to install the `LangChain`
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simple example of how to use this parser:'
  prefs: []
  type: TYPE_NORMAL
- en: A quick note on LangChain
  prefs: []
  type: TYPE_NORMAL
- en: The LangChain framework is similar in purpose to LlamaIndex and provides a versatile
    toolkit specialized in advanced natural language processing capabilities. Its
    collection of text segmentation, summarization, and language understanding models
    assist in splitting and digesting textual data into coherent chunks ready for
    indexing in a similar way to LlamaIndex. When dealing with large data sources
    requiring nuanced linguistic analysis, LangChain empowers users to finely control
    the breakdown and ingestion of text - ensuring context and clarity are retained
    for downstream retrieval and querying. As you can see, the two can complement
    each other in a RAG scenario. Want to know more? Check out https://www.langchain.com/
    .
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see what other parsers we have available.
  prefs: []
  type: TYPE_NORMAL
- en: SimpleFileNodeParser
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This one automatically decides which of the following three node parsers should
    be used based on file types. It can automatically handle these file formats and
    transform them into nodes, simplifying the process of interacting with various
    types of content:'
  prefs: []
  type: TYPE_NORMAL
- en: You can simply rely on `FlatReader` to load the file into your `Document` object;
    `SimpleFileNodeParser` will know what to do from there.
  prefs: []
  type: TYPE_NORMAL
- en: HTMLNodeParser
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This parser uses **Beautiful Soup** to parse HTML files and convert them into
    nodes based on selected HTML tags. This parser simplifies the HTML file by extracting
    text from standard text elements and merging adjacent nodes of the same type.
    The parser can be used like this:'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, you have the option to customize the HTML `tags` from where
    you want to retrieve content.
  prefs: []
  type: TYPE_NORMAL
- en: MarkdownNodeParser
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This parser processes raw *markdown* text and generates nodes reflecting its
    structure and content. The markdown node parser divides the content into nodes
    for each header encountered in the file and incorporates the header hierarchy
    into the metadata. Here’s how to use `MarkdownNodeParser` :'
  prefs: []
  type: TYPE_NORMAL
- en: JSONNodeParser
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This parser is specialized in processing and querying structured data in JSON
    format. In a similar way to the Markdown parser, the JSON parser can be used like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: Using relational parsers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Relational parsers** parse information into nodes that are linked to each
    other through relationships. Relationships add a whole new dimension to our data
    and allow for more advanced retrieval techniques in our RAG workflow.'
  prefs: []
  type: TYPE_NORMAL
- en: HierarchicalNodeParser
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This parser organizes the nodes into hierarchies across multiple levels. It
    will generate a hierarchy of nodes, starting with top-level nodes with larger
    section sizes, down to child nodes with smaller section sizes, where each child
    node has a parent node with a larger section size ( *Figure 4* *.1* ). By default,
    the parser uses `SentenceSplitter` to chunk text. The node hierarchy looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Level 1* : Section size 2,048'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Level 2* : Section size 512'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Level 3* : Section size 128'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The top-level nodes, with larger sections, can provide high-level summaries,
    while the lower nodes can allow for a more detailed analysis of text sections.
    Have a look at *Figure 4* *.2* for a visual representation of this concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_04_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Hierarchical nodes of 2,048, 512, and 128 chunk sizes
  prefs: []
  type: TYPE_NORMAL
- en: 'In this way, the different node levels can be used to adjust the accuracy and
    depth of search results, allowing users to find information at different granularity
    levels. Here’s an example of how to use this parser in your code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two specific parameters to customize for this parser:'
  prefs: []
  type: TYPE_NORMAL
- en: '`chunk_sizes` : The values in this list define your hierarchy levels based
    on content size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunk_overlap` : This defines the overlap size between chunks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UnstructuredElementNodeParser
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I left this one last because it is used for more special situations. Sometimes,
    our documents may include a mix of text and data tables, which can make parsing
    inefficient by conventional methods.
  prefs: []
  type: TYPE_NORMAL
- en: This parser can process and split these documents into interpretable nodes,
    distinguishing between text sections and other embedded structures such as tables.
    We’ll talk about it in more detail toward the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Confused about node parsers and text splitters?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may have noticed that I use the two terms quite loosely. Categorizing parsing
    modules into these two groups might initially cause some confusion. To simplify,
    a node parser is a more sophisticated mechanism than a simple splitter. While
    both serve the same basic function and operate at different levels of complexity,
    they differ in their implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Text splitters such as `SentenceSplitter` can divide long flat texts into nodes,
    based on certain rules or limitations, such as **chunk_size** or **chunk_overlap**
    . The nodes could represent lines, paragraphs, or sentences, and may also include
    additional metadata or links to the original document.
  prefs: []
  type: TYPE_NORMAL
- en: Node parsers are more sophisticated and can involve additional data processing
    logic. Beyond simply dividing text into nodes, they can perform extra tasks, such
    as analyzing the structure of HTML or JSON files and producing nodes enriched
    with contextual information.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding chunk_size and chunk_overlap
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you have probably understood by now, text splitters are a basic but important
    component. They control how text in documents gets split into nodes during parsing.
    For each text splitter type, LlamaIndex provides several parameters to customize
    the text splitting behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Probably two of the most important parameters for a text splitter are `chunk_size`
    and `chunk_overlap` . The text splitters themselves, such as `SentenceSplitter`
    , `TokenTextSplitter` , `TextSplitter` , and others, take in the `chunk_size`
    and `chunk_overlap` arguments to control how they break text into smaller chunks
    during node creation. `chunk_size` controls the maximum length of text chunks
    in nodes. This is useful for ensuring nodes don’t take long for the LLM to process.
    Note that in LlamaIndex, the default `chunk_size` is 1,024, while the default
    `chunk_overlap` is 20.
  prefs: []
  type: TYPE_NORMAL
- en: '**chunk size** is an important setting when building an RAG system. If chunks
    are too small, important context may be lost and the quality of the LLM response
    will be lower. On the other hand, large chunks increase the size of the prompts,
    increasing both computational cost and response generation time. An experimental
    approach was used when the default values were selected for LlamaIndex: https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5
    .'
  prefs: []
  type: TYPE_NORMAL
- en: '`chunk_overlap` creates overlapping nodes by re-including some tokens from
    the previous Node. This helps provide context so that the LLM can understand the
    continuity of ideas when processing adjacent Nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4* *.3* provides a visual representation of this concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_04_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – chunk_size and chunk_overlap explained
  prefs: []
  type: TYPE_NORMAL
- en: 'The concept is similar to the way `SentenceWindowNodeParser` works – that is,
    it extracts a *window* of context for each sentence. For example, with `chunk_size=100`
    and `chunk_overlap=10` , let’s say we had the following text:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Gardening is not only a relaxing hobby but also an art form. Cultivating plants,
    designing landscapes, and nurturing nature bring a sense of accomplishment. Many
    find it therapeutic and rewarding, especially when they see their* *garden flourish*
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'It would get split into the following areas:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Node 1 (first 100 characters)* : “Gardening is not only a relaxing hobby but
    also an art form. Cultivating plants, designing landscapes, an”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Node 2 (starts from the 75th character, next 100 characters)* : “designing
    landscapes, and nurturing nature bring a sense of accomplishment. Many find it
    therapeutic and re”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Node 3 (starts from the 150th character to the end of the text)* : “Many find
    it therapeutic and rewarding, especially when they see their garden flourish.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this setup, the overlap between node 1 and node 2 is “ `designing` landscapes,
    an,” whereas the overlap between node 2 and node 3 is “Many find it therapeutic
    and re.”
  prefs: []
  type: TYPE_NORMAL
- en: These overlaps mean that one node re-includes parts of the previous node. This
    mechanism ensures continuity and context between the chunks, making each part
    more meaningful when read in sequence. Of course, choosing the right values for
    these two parameters is very important. The biggest impact will be on creating
    vector indexes. We’ll talk about them later, during *Chapter 5* , *Indexing* *with
    LlamaIndex* .
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s have a quick overview of node relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Including relationships with include_prev_next_rel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s talk a bit about another important parameter that can dictate the behavior
    of our parsers: the `include_prev_next_rel` option. When set to `True` , this
    option makes the parser automatically add `NEXT` and `PREVIOUS` relationships
    between consecutive nodes. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: This helps capture the sequencing between nodes. Then, later when querying,
    you can optionally retrieve previous or next nodes for more context using features
    such as `PrevNextNodePostprocessor` . More on that in *Chapter 6* , *Querying
    Our Data, Part 1 –* *Context Retrieval* .
  prefs: []
  type: TYPE_NORMAL
- en: The relationships get added to the `.relationships` dictionary on each node.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, node 1 would now be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Node 2 would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Capturing these sequences helps provide contextual continuity in long documents
    and brings a lot of other benefits that I listed in more detail in the previous
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Among other things, having a previous-next relationship enables *cluster retrieval*
    : you can get a cluster of related nodes by following the relationships to fetch
    nearby connected nodes. This provides a more focused context instead of randomly
    scattered nodes. Maintaining a cohesive narrative thread through the content when
    following a story or a dialogue is another good reason for having these relationships
    between nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s have a look at how to use these parsers and splitters in our workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Practical ways of using these node creation models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'How you implement the node parsers or text splitters in your code depends on
    how much you want to customize the process but, in the end, it all comes down
    to three main options:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using them standalone by calling from llama_index.core import Document from
    llama_index.core.node_parser import SentenceWindowNodeParser doc = Document(     text="Sentence
    1\. Sentence 2\. Sentence 3." ) parser = SentenceWindowNodeParser.from_defaults(
        window_size=2  ,     window_metadata_key="ContextWindow",     original_text_metadata_key="node_text"
    ) nodes = parser.get_nodes_from_documents([doc]) Node ID: 0715876a-61e6-4e77-95ba-b93e10de1c67
    Text: Sentence 2. {''ContextWindow'': ''Sentence 1.  Sentence 2.  Sentence 3.'',
    ''node_text'': ''Sentence 2\. ''} `get_nodes_from_documents()` , like in this
    example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This code will produce three nodes. If we look at the second node, for example,
    by running `print(nodes[1])` , we’ll get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As you can see, the parser extracted the second sentence and allocated a random
    ID to the node. But if we take a peek at the node’s metadata by running `print(nodes[1].metadata)`
    , we’ll also see the context it gathered, using the keys we specified:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This metadata can later be used when building queries to provide more context
    for each sentence and improve the LLM responses.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We’ll explore this in more detail during *Chapter 6* , *Querying Our Data, Part
    1 –* *Context Retrieval* .
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Configuring them from llama_index.core import Settings, Document,     VectorStoreIndex
    from llama_index.core.node_parser import     SentenceWindowNodeParser doc = Document(
        text="Sentence 1\. Sentence 2\. Sentence 3." ) text_splitter = SentenceWindowNodeParser.from_defaults(
        window_size=2  ,     window_metadata_key="ContextWindow",     original_text_metadata_key="node_text"
    ) Settings.text_splitter = text_splitter index = VectorStoreIndex.from_documents([doc])
    in `Settings` .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The second option is a bit more general and convenient when you need to automatically
    use the same parser for multiple purposes in your app:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This time, after we define and configure our custom `text_splitter` , we pre-load
    it in `Settings` . From this point forward, whenever we call any function that
    relies on text splitting, our custom `text_splitter` will be used by default.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Of course, this actual example is a bit of overkill. You’ve probably noticed
    that I’ve used a node parser in place of a simple text splitter. The index we’re
    building with the nodes won’t benefit in any way from the additional context metadata
    created by the parser. I just wanted to emphasize my previous point regarding
    parsers and splitters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Defining the parsers as a **transformation** step in an **ingestion pipeline**
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An ingestion pipeline is an automatic and structured process for ingesting data.
    It’s running the data through a series of steps (called **transformations** )
    one by one.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: I will explain how this works and what it can be used for later in this chapter,
    in the *Using the ingestion pipeline to increase efficiency* section. You’ll also
    get to see the code for implementing the parser as a transformation in the pipeline.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we’ll talk about metadata and how metadata can be used to improve our
    RAG application.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Working with metadata to improve the context</title>
  prefs: []
  type: TYPE_NORMAL
- en: Working with metadata to improve the context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'document.metadata = {     "report_name": "Sales Report April 2022",     "department":
    "Sales",     "author": "Jane Doe" } From llama_index.core import SimpleDirectoryReader
    from llama_index.core.node_parser import SentenceSplitter reader = SimpleDirectoryReader(''files'')
    documents = reader.load_data() parser = SentenceSplitter(include_prev_next_rel=True)
    nodes = parser.get_nodes_from_documents(documents) from llama_index.core.extractors
    import SummaryExtractor summary_extractor = SummaryExtractor(summaries=["prev",
    "self",     "next"]) metadata_list = summary_extractor.extract(nodes) print(metadata_list)
    from llama_index.core.extractors import QuestionsAnsweredExtractor qa_extractor
    = QuestionsAnsweredExtractor(questions=5) metadata_list = qa_extractor.extract(nodes)
    print(metadata_list) from llama_index.core.extractors import TitleExtractor title_extractor
    = TitleExtractor () metadata_list = title_extractor.extract(nodes) print(metadata_list)
    from llama_index.core.extractors import EntityExtractor entity_extractor = EntityExtractor
    (     label_entities = True,     device = "cpu" ) metadata_list = entity_extractor.extract(nodes)
    print(metadata_list) from llama_index.core.extractors import KeywordExtractor
    key_extractor = KeywordExtractor (keywords=3) metadata_list = key_extractor.extract(nodes)
    print(metadata_list) from llama_index.core.extractors import BaseExtractor from
    typing import List, Dict class CustomExtractor(BaseExtractor):     async def aextract(self,
    nodes) -> List[Dict]:         metadata_list = [             {                 "node_length":  str(len(node.text))
                }             for node in nodes         ]         return metadata_list
    document.excluded_llm_metadata_keys = ["file_name"] document.excluded_embed_metadata_keys
    = ["file_name"] document.metadata_template = "{key}::{value}" print(document.get_content(metadata_mode=MetadataMode.LLM))'
  prefs: []
  type: TYPE_NORMAL
- en: What is **metadata** ? It’s simply additional information we can attach to our
    documents and nodes. This extra context helps LlamaIndex better understand our
    data. It provides additional context about data and can be customized in terms
    of visibility and format.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s say you’ve *ingested* some PDF reports as documents. You
    could then simply add some metadata like this:'
  prefs: []
  type: TYPE_NORMAL
- en: This metadata gives vital clues when querying the data later. In this example,
    we can use it to locate reports by department or author. You can store anything
    useful as metadata – categories, timestamps, locations, and more.
  prefs: []
  type: TYPE_NORMAL
- en: And here’s a neat trick – any metadata you set on a document automatically flows
    down to child nodes! So, if I set an `author` field on a document, all nodes derived
    from that document will inherit the `author` metadata. This propagation saves
    time and prevents duplicating metadata across nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple ways of defining metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting the metadata values directly in the document = Document(     text="",
        metadata={"author": "John Doe"} ) `Document` constructor as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Adding the metadata after document.metadata = {"category": "finance"} document
    creation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Automatically setting metadata in the ingestion process, when using data connectors
    such def set_metadata(filename):     return {"file_name": filename} documents
    = SimpleDirectoryReader(     "./data",     file_metadata=set_metadata("file1.txt")
    ).load_data() as `SimpleDirectoryReader` :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using standalone, dedicated extractors provided by LlamaIndex. **Metadata**
    **extractors** are a powerful way to generate relevant metadata from text using
    the power of LLMs. This extracted metadata can then be attached to documents and
    nodes to provide additional context
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining the extractor as a *transformation* step in an ingestion pipeline.
    Just like in the case of node parsers, extractors can also become part of the
    pipeline. We’ll cover this approach later in this chapter in the *Using the ingestion
    pipeline to increase* *efficiency* section
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: But first, let’s put our magnifying glass on these specialized metadata extractors
    to better understand how they work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you go further, if you want to run the following code examples, make
    sure to include the necessary imports, document ingestion, and node parsing logic
    at the beginning of your code by adding the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: This boilerplate code prepares your data – ingested from the `files` subfolder
    – and puts everything you need into `Nodes` . We’ll store our metadata in a variable
    called `metadata_list` . I’ve added `print(metadata_list)` at the end of each
    example so that we’ll see an output of the extracted metadata. Apart from describing
    their logic, I’ve also highlighted practical uses for each one of the extractors.
  prefs: []
  type: TYPE_NORMAL
- en: SummaryExtractor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This extractor generates summaries of the text contained by the node. Optionally,
    it can generate summaries for the previous and next adjacent nodes. Here’s an
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: This extractor generates concise summaries for each node or adjacent node. These
    are essential during the retrieve phase in an RAG architecture. This ensures that
    the search can consider the summary of the documents without having to process
    the entirety of their content.
  prefs: []
  type: TYPE_NORMAL
- en: Practical use case
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a customer support knowledge base on which `SummaryExtractor` can provide
    summaries of customer issues and resolutions. Then, when a new support request
    comes in, our app can retrieve the most relevant past cases to help generate a
    detailed and contextual solution.
  prefs: []
  type: TYPE_NORMAL
- en: You can customize the type of `summaries` to generate by setting the values
    in the summaries list and the actual prompt that will be used with the LLM by
    defining the prompt in the `prompt_template` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: QuestionsAnsweredExtractor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This extractor generates a specified number of questions the node text can answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example should give you a usage guideline:'
  prefs: []
  type: TYPE_NORMAL
- en: This extractor identifies questions that the text is uniquely positioned to
    answer, allowing the retrieval process to focus on nodes that explicitly address
    specific inquiries.
  prefs: []
  type: TYPE_NORMAL
- en: Practical use case
  prefs: []
  type: TYPE_NORMAL
- en: For an FAQ system, the extractor identifies unique questions answered by articles,
    making it easier to find precise answers to user queries.
  prefs: []
  type: TYPE_NORMAL
- en: You can customize the number of questions it generates but also the actual prompt
    that will be used with the LLM – by setting the `prompt_template` parameter. There
    is also an `embedding_only` Boolean parameter that – if set to `True` – will make
    the metadata available only for embeddings. More on that in *Chapter 5* , *Indexing*
    *with LlamaIndex* .
  prefs: []
  type: TYPE_NORMAL
- en: TitleExtractor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This one extracts a title for the text. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '`TitleExtractor` specializes in pulling out meaningful titles from larger texts,
    assisting in the quick identification and retrieval of documents. In digital libraries,
    for example, `TitleExtractor` can help categorize documents by extracting titles
    from untitled texts, making retrieval more efficient when titles are used as search
    keywords. There are several parameters you can tweak for this extractor:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nodes` : This sets the number of nodes to use for title extraction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node_template` : This changes the default prompt template that’s used for
    extracting the titles'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`combine_template` : This changes the prompt template for combining multiple
    node-level titles in a document-wide title'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s look at `EntityExtractor` .
  prefs: []
  type: TYPE_NORMAL
- en: EntityExtractor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This will extract entities such as people, locations, organizations, and more
    from the node text by using the **span-marker** package. This package is installed
    automatically together with the `EntityExtractor` integration, so no additional
    installations are required. It provides the ability to perform **named entity
    recognition** ( **NER** ) and relies on a tokenizer provided by the **Natural
    Language Toolkit** ( **NLTK** ) package: https://www.nltk.org/ .'
  prefs: []
  type: TYPE_NORMAL
- en: A quick note on NER
  prefs: []
  type: TYPE_NORMAL
- en: NER is a technique that’s used by computers to identify and label specific entities
    in text, such as people’s names, company names, places, and dates. This helps
    the computer to better understand the content and provides useful context in an
    RAG scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a code example for using this extractor:'
  prefs: []
  type: TYPE_NORMAL
- en: The extractor identifies named entities from the text, labels them, and adds
    them to the metadata, enabling a retrieval system to focus on nodes with specific
    references.
  prefs: []
  type: TYPE_NORMAL
- en: Practical use case
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a legal document archive having this metadata attached to each node.
    This extractor could ease the retrieval of documents mentioning particular people,
    locations, or organizations, thus providing the best context for our query.
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s a long list of parameters that you can tune for this extractor:'
  prefs: []
  type: TYPE_NORMAL
- en: '`model_name` : This sets the name of the model to be used by `SpanMarker`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prediction_threshold` : This changes the default 0.5 minimum prediction threshold
    for named entities. As you may have guessed, entity recognition is usually not
    a 100% accurate process. However, you can experiment with different values here
    until you find the best compromise'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`span_joiner` : This changes the default string used to join the spans'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label_entities` : If set to `True` , it will make the extractor label every
    entity name with an entity type. This could be useful later, in the retrieval
    and querying phase. By default, this is set to `False`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` : This controls the device on which the model runs. It defaults to
    `cpu` , but if your system allows, it can be set to `cuda`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`entity_map` : This allows you to customize the labels for each entity type.
    The extractor comes with a predefined entity map that includes labels for people,
    organizations, places, events, and many others'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Tokenizer` : This allows you to change the default tokenizer function – which
    defaults to the NLTK tokenizer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s discuss how to extract keywords with `KeywordExtractor` .
  prefs: []
  type: TYPE_NORMAL
- en: KeywordExtractor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This extractor extracts important keywords from the text. Let’s have a look
    at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: This one identifies important words or phrases, making it an invaluable tool
    for retrieving the most relevant nodes based on user queries.
  prefs: []
  type: TYPE_NORMAL
- en: Practical use case
  prefs: []
  type: TYPE_NORMAL
- en: Integrating `KeywordExtractor` into a content recommendation engine can significantly
    enhance its effectiveness. By aligning the keywords extracted from content nodes
    with the terms used in user searches, the engine can more accurately match and
    recommend content that aligns with user interests. This keyword-based matching
    ensures that recommendations are not only relevant but also tailored to the specific
    inquiries or topics users are exploring.
  prefs: []
  type: TYPE_NORMAL
- en: You can customize the number of keywords it generates by changing the `keywords`
    parameter to a specific value.
  prefs: []
  type: TYPE_NORMAL
- en: PydanticProgramExtractor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This extractor extracts metadata using a **Pydantic** structure. Have a look
    here for a complete example of using this extractor: https://docs.llamaindex.ai/en/stable/examples/metadata_extraction/PydanticExtractor.html#pydantic-extractor
    .'
  prefs: []
  type: TYPE_NORMAL
- en: This *Swiss Army knife* enables the creation of complex and structured metadata
    schemas with a single LLM call by making use of Pydantic models. One of the main
    advantages it has over other extractors is that it can pull multiple fields of
    data using a single LLM call making it a very efficient way to extract metadata.
    This data will be nicely organized in a model of our design.
  prefs: []
  type: TYPE_NORMAL
- en: A quick introduction to Pydantic models
  prefs: []
  type: TYPE_NORMAL
- en: A **Pydantic model** is like a blueprint or a set of rules that you define as
    a class in a Python program. It helps you make sure that the data you receive
    or work with follows certain rules and is in the right format. Think of it as
    a way to define how your data should look – Pydantic helps you enforce those rules
    and make sure the data fits in your desired structure.
  prefs: []
  type: TYPE_NORMAL
- en: For example, imagine you have a program that deals with user data such as names,
    ages, and email addresses. You can create a Pydantic model that specifies that
    a user’s name should be a string, their age should be a number, and their email
    address should be a valid email format. If input data doesn’t follow these rules,
    Pydantic will raise an error, telling you that the data is not correct. LlamaIndex
    embraces this mechanism whenever it needs to ensure the consistency and correctness
    of the data it handles, especially as it often works with complex structures and
    interrelated data.
  prefs: []
  type: TYPE_NORMAL
- en: MarvinMetadataExtractor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This extractor extracts metadata using the **Marvin AI engineering framework**
    (https://www.askmarvin.ai/). Taking advantage of the Marvin AI engineering framework,
    this extractor is capable of trustworthy and scalable metadata extraction and
    augmentation. Its sophistication lies in providing type-safe schemas for text
    – similar to Pydantic models - but also supporting business logic transformations.
    You can find a detailed example here: https://docs.llamaindex.ai/en/stable/examples/metadata_extraction/MarvinMetadataExtractorDemo.html
    .'
  prefs: []
  type: TYPE_NORMAL
- en: Defining your custom extractor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Just in case none of these ready-made extractors satisfies your needs, you
    can always define your own extractor function. Here is a simple example of how
    to define a custom extractor:'
  prefs: []
  type: TYPE_NORMAL
- en: This basic extractor measures the length in characters for each node and saves
    these values in the metadata. Of course, you could replace that with any logic
    required by your app.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having so many tools and methods available at our disposal is a great thing.
    But then a new question arises: *do we need that much metadata?* Let’s find out
    the answer.'
  prefs: []
  type: TYPE_NORMAL
- en: Is having all that metadata always a good thing?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Not necessarily. A key detail is that metadata gets injected into the text
    that’s sent to the LLM and embedding model. This can potentially induce some bias
    in the models. This means that sometimes, you may not want all metadata to be
    visible. For example, filenames may help embeddings but may *distract* the LLM
    because the LLM might not understand them as filenames but as other entities instead,
    and also because the filenames may have no relevance in the context of the prompt.
    You can selectively hide metadata with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This hides `file_name` from the LLM. You can also hide metadata from embeddings
    if you want:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, you can customize the metadata format like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a pro tip when dealing with metadata mode. LlamaIndex has an enum called
    `MetadataMode` that controls metadata visibility:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MetadataMode.ALL` : Shows all metadata'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MetadataMode.LLM` : Only metadata visible to the LLM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MetadataMode.EMBED` : Only metadata visible to embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can test the visibility of metadata with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: So, in summary, metadata gives your data much-needed context. You have full
    control over its format and visibility to different models. These customizations
    let you mold metadata to match your use case!
  prefs: []
  type: TYPE_NORMAL
- en: With that topic exhausted, it’s time to talk about money.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Estimating the potential cost of using metadata extractors</title>
  prefs: []
  type: TYPE_NORMAL
- en: Estimating the potential cost of using metadata extractors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'from llama_index.core import ettings from llama_index.core.extractors import
    QuestionsAnsweredExtractor from llama_index.core.llms.mock import MockLLM from
    llama_index.core.schema import TextNode from llama_index.core.callbacks import
    (     CallbackManager,     TokenCountingHandler ) llm = MockLLM(max_tokens=256)
    counter = TokenCountingHandler(verbose=False) callback_manager = CallbackManager([counter])
    Settings.llm = llm Settings.callback_manager = CallbackManager([counter]) sample_text
    = (     "LlamaIndex is a powerful tool used "     "to create efficient indices
    from data." ) nodes= [TextNode(text=sample_text)] extractor = QuestionsAnsweredExtractor(
        show_progress=False ) Questions_metadata = extractor.extract(nodes) print(f"Prompt
    Tokens: {counter.prompt_llm_token_count}") print(f"Completion Tokens: {counter.completion_llm_token_count}")
    print(f"Total Token Count: {counter.total_llm_token_count}")'
  prefs: []
  type: TYPE_NORMAL
- en: A key consideration when utilizing the various metadata extractors in LlamaIndex
    is the associated LLM compute costs. As mentioned earlier, most of these extractors
    rely on LLMs under the hood to analyze text and generate descriptive metadata.
  prefs: []
  type: TYPE_NORMAL
- en: Repeatedly calling LLMs to process large volumes of text can quickly add up
    in charges. For example, if you are extracting summaries and keywords from thousands
    of document nodes using `SummaryExtractor` and `KeywordExtractor` , those constant
    LLM invocations will carry a significant cost.
  prefs: []
  type: TYPE_NORMAL
- en: Follow these simple best practices to minimize your costs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s talk about some common best practices for minimizing your LLM costs:'
  prefs: []
  type: TYPE_NORMAL
- en: Batch content into fewer LLM calls instead of individual calls per node. This
    amortizes the overhead because you consume fewer tokens compared to multiple separate
    calls. Using the Pydantic extractor is very useful for this purpose since it generates
    multiple fields in a single LLM call
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use cheaper LLM models with lower compute requirements if full accuracy is not
    necessary. However, be careful – you may introduce errors in your data, and these
    errors have the bad habit of propagating and amplifying downstream
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cache previous extractions and reuse them without having to re-invoke LLMs every
    time. I’m going to show you how to accomplish that using *ingestion pipelines*
    later in this chapter, in the *Using the ingestion pipeline to increase* *efficiency*
    section
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restrict metadata extraction only to select subsets of critical nodes rather
    than full coverage. This may be difficult to implement in an automated scenario
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider offline LLMs to eliminate cloud costs. Depending on your hardware,
    this may or may not be a solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While these guidelines should help you greatly reduce the extraction costs,
    it’s still a good idea to make sure you run some estimates before processing large
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Estimate your maximal costs before running the actual extractors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is a basic example of how we can estimate LLM costs by using a **MockLLM**
    before running the extractor on the real one:'
  prefs: []
  type: TYPE_NORMAL
- en: You’ll notice that we’re using some specialized tools to run the actual estimation.
    Let’s have a quick overview of the code. `MockLLM` – as its name implies – is
    a stand-in LLM that simulates the behavior of an LLM without any actual API calls.
  prefs: []
  type: TYPE_NORMAL
- en: When you create a `MockLLM` instance, you have the option to set a `max_tokens`
    parameter. This parameter represents the maximum number of tokens that the mock
    model is supposed to generate for any given prompt, mirroring the behavior you’d
    expect from a real language model – but without actually generating any meaningful
    content.
  prefs: []
  type: TYPE_NORMAL
- en: How does the max_token parameter work?
  prefs: []
  type: TYPE_NORMAL
- en: The goal here is to predict a *worst-case* scenario, but your actual cost will
    vary depending on the LLM response size and in most regular scenarios should be
    lower than the `max_tokens` value. It’s still a very useful tool because it helps
    you understand how different metadata extraction strategies applied to different
    datasets can affect your total cost. For metadata extraction, this total cost
    will depend on the prompt and response size multiplied by the total number of
    calls the extractor performs.
  prefs: []
  type: TYPE_NORMAL
- en: '**CallbackManager** is a debugging mechanism that’s implemented in LlamaIndex
    that we will cover in more detail in *Chapter 10* , *Prompt Engineering Guidelines
    and Best Practices* . In our example, `CallbackManager` is used in combination
    with the **TokenCountingHandler** module, which is specialized in counting the
    tokens that are used for various operations involving an LLM. When defining `TokenCountingHandler`
    , you can also specify a `tokenizer` parameter.'
  prefs: []
  type: TYPE_NORMAL
- en: What is the tokenizer and why do we need it?
  prefs: []
  type: TYPE_NORMAL
- en: The **tokenizer** is responsible for *tokenization* of the text – that is, converting
    it into tokens – since LLMs work with tokens and also measure their usage using
    tokens. When running a cost prediction for a specific prompt on a specific LLM,
    it’s important to use a tokenizer that is compatible with that specific LLM. Each
    LLM is often trained with a particular tokenizer, which determines how the text
    is split into tokens. Using the correct tokenizer is important if you want to
    make more accurate cost predictions. By default, LlamaIndex uses the `CL100K`
    tokenizer, which is specific for GPT-4\. So, if you plan on using other LLMs,
    you may want to customize the tokenizer. More on this topic and on how we can
    optimize the costs of our RAG app will be covered in *Chapter 10* , *Prompt Engineering
    Guidelines and* *Best Practices* .
  prefs: []
  type: TYPE_NORMAL
- en: Going back to our example, what happens under the hood is that when we run the
    extractor, it uses `MockLLM` – so, everything stays locally. Then, `TokenCountingHandler`
    *intercepts* both the prompt and the response from this `MockLLM` and counts the
    actual number of tokens used.
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss a similar mechanism that can be used for estimating the costs
    of generating certain types of Indexes and running queries later in *Chapters
    5* and *6* .
  prefs: []
  type: TYPE_NORMAL
- en: In this example, I’ve shown you how to estimate the cost for only one type of
    extractor, `QuestionsAnsweredExtractor` . If you need to estimate the individual
    cost for more than one extractor in the same run, you can use the `token_counter.reset_counts()`
    method to reset the counters to zero before running the next extraction round.
  prefs: []
  type: TYPE_NORMAL
- en: The main lesson of this section
  prefs: []
  type: TYPE_NORMAL
- en: While rich metadata unlocks many capabilities, overuse without conscious optimization
    can negatively impact operating costs and ruin your day. Make sure you take that
    into account. Apply best practices to minimize the costs and always estimate before
    running extractors on large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s talk about another very important aspect to consider data privacy.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Preserving privacy with metadata extractors, and not only</title>
  prefs: []
  type: TYPE_NORMAL
- en: Preserving privacy with metadata extractors, and not only
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'pip install llama-index-llms-huggingface from llama_index.core.postprocessor
    import NERPIINodePostprocessor from llama_index.llms.huggingface import HuggingFaceLLM
    from llama_index.core.schema import NodeWithScore, TextNode original = (     "Dear
    Jane Doe. Your address has been recorded in "     "our database. Please confirm
    it is valid: 8804 Vista "     "Serro Dr. Cabo Robles, California(CA)." ) node
    = TextNode(text=original) processor = NERPIINodePostprocessor() clean_nodes =
    processor.postprocess_nodes(     [NodeWithScore(node=node)] ) print(clean_nodes[0].node.get_text())
    Dear [PER_5]. Your address has been recorded in our database. Please confirm it
    is valid: 8804 [LOC_95] Dr. [LOC_111], [LOC_124]([LOC_135]).'
  prefs: []
  type: TYPE_NORMAL
- en: Augmenting LLMs with your proprietary data – which, by the way, may belong to
    your customers in many instances – can prove to be a challenging task in terms
    of **data privacy** . While a cloud based LLM solution can enrich your proprietary
    data and offer numerous advantages, *uncontrolled data sharing with external parties
    can quickly turn into a legal, security, and* *regulatory nightmare* .
  prefs: []
  type: TYPE_NORMAL
- en: Although the topic of data privacy is more stringent in the case of indexing
    and querying, utilizing metadata extractors can also raise potential privacy concerns
    to be aware of. Therefore, I believe a brief warning is required already.
  prefs: []
  type: TYPE_NORMAL
- en: Since most extractors rely on processing content via LLMs to generate metadata,
    this means your actual data gets transmitted to and analyzed by external cloud
    services.
  prefs: []
  type: TYPE_NORMAL
- en: There is a risk of exposure or mishandling of any personal or confidential information
    contained in this data, whether due to security lapses, insider risks at the LLM
    vendor, or malicious activities.
  prefs: []
  type: TYPE_NORMAL
- en: It’s not just OUR privacy at stake here
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of privacy issues, remember the example LlamaHub connectors we discussed
    earlier? Ingesting messages with `DiscordReader` transfers data from Discord servers.
    Given that Discord messages may contain private conversations, there is a potential
    privacy concern, especially if Discord’s terms of service and the expectations
    of the message senders are not taken into account. So, if your data includes private
    identities, healthcare details, financial information, and so on, allowing unrestrained
    extraction workflows could be problematic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some ways to mitigate privacy risks:'
  prefs: []
  type: TYPE_NORMAL
- en: Scrubbing personal data before ingestion into LlamaIndex using, for example,
    `PIINodePostprocessor` in combination with a local LLM. Check out the next section
    for a simple implementation guideline for this option
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restricting metadata extraction to only non-sensitive subsets of nodes. Of course,
    this assumes that you manually classify the sensitivity of each Node. That would
    be impractical for automated processing pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running LLMs locally instead of in the cloud where possible to limit external
    exposure. That depends, of course, on your available hardware and model choice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Enabling encryption mechanisms if such features are available with certain
    LLM vendors. If privacy is a big concern in your implementation, you might want
    to consider and read more about **fully homomorphic encryption** ( **FHE** ):
    https://huggingface.co/blog/encrypted-llm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These concerns and best practices apply to any type of interaction with an LLM.
    This subject has been discussed and analyzed in many available lectures and articles,
    so I’m not going to go into further detail here. But that doesn’t mean it’s not
    important!
  prefs: []
  type: TYPE_NORMAL
- en: Key message
  prefs: []
  type: TYPE_NORMAL
- en: What you should understand is that using an LLM already poses a privacy risk
    for your data. Augmenting that LLM with an additional framework such as LlamaIndex
    means also augmenting the privacy risks involved.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, additional diligence is needed when dealing with private data to
    ensure convenience does not override security requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Scrubbing personal data and other sensitive information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a world filled with nosy onlookers and data rulebooks, it’s crucial to be
    as cautious with your data as a squirrel guarding its acorns in a crowded park!
    The good news is that there are solutions for ensuring privacy. And a convenient
    one is already provided by the LlamaIndex framework.
  prefs: []
  type: TYPE_NORMAL
- en: '**Node post-processors** can solve this problem for us.'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we discovered how node post-processors are used in
    a query engine. They are applied to the nodes that are returned from a retriever,
    before the response synthesis step, to apply different transformations on the
    nodes or node data itself. This is, at least, their most common use case.
  prefs: []
  type: TYPE_NORMAL
- en: But there’s also another reason to use them
  prefs: []
  type: TYPE_NORMAL
- en: It turns out we can also use node processors outside of the query engine. Among
    other things, they can be used to clean up any sensitive data before extracting
    metadata using external LLMs, for example.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two methods available: `PIINodePostprocessor` and `NERPIINodePostprocessor`
    . The first one is designed to work with any local LLM that you may have on hand,
    while the other is customized for using a specialized NER model. In case you’re
    not familiar with the acronym, **PII** stands for **Personally** **Identifiable
    Information** .'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simple example of using `NERPIINodePostprocessor` to clean up the
    data. This method uses a NER model from **Hugging Face** to do the job. Because
    I wanted to keep it simple, I didn’t specify a particular model. Therefore, you
    may expect a warning and the HuggingFaceLLM will probably default to using the
    `dbmdz/bert-large-cased-finetuned-conll03-english` model, as documented here:
    https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english .'
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure you install the corresponding integration package first:'
  prefs: []
  type: TYPE_NORMAL
- en: Also, on the first run, the code will download the model from Hugging Face and
    you’ll need to make sure you have at least 1.5 GB of free space available on your
    machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output should be similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the results, we can see that the names have been replaced with placeholders
    so that the data can now be safely passed to any external LLM. The beauty of this
    method is that, on return, the answer can be processed back and the placeholders
    can be replaced with the original data, resulting in a seamless user experience.
  prefs: []
  type: TYPE_NORMAL
- en: The actual mapping between placeholders and real data will be stored in `clean_nodes[0].node.metadata`
    . This metadata will not be sent to the LLM and can later be used to produce the
    original names during response synthesis.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll discuss how to improve the efficiency of the ingestion pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Using the ingestion pipeline to increase efficiency</title>
  prefs: []
  type: TYPE_NORMAL
- en: Using the ingestion pipeline to increase efficiency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'from llama_index.core import SimpleDirectoryReader from llama_index.core.extractors
    import SummaryExtractor,QuestionsAnsweredExtractor from llama_index.core.node_parser
    import TokenTextSplitter from llama_index.core.ingestion import IngestionPipeline,
    IngestionCache from llama_index.core.schema import TransformComponent class CustomTransformation(TransformComponent):
      def __call__(self, nodes, **kwargs):     # run any node transformation logic
    here     return nodes reader = SimpleDirectoryReader(''files'') documents = reader.load_data()
    try:     cached_hashes = IngestionCache.from_persist_path( "./ingestion_cache.json"
    )     print("Cache file found. Running using cache") except:     cached_hashes
    = ""     print("No cache file found. Running without cache") pipeline = IngestionPipeline(
        transformations = [         CustomTransformation(),         TokenTextSplitter(
                separator=" ",             chunk_size=512,             chunk_overlap=128),
            SummaryExtractor(),         QuestionsAnsweredExtractor(             questions=3
            )     ],     cache=cached_hashes ) nodes = pipeline.run(     documents=documents,
        show_progress=True, ) pipeline.cache.persist("./ingestion_cache.json") print("All
    documents loaded") from llama_index.core import Settings Settings.transformations
    = [     CustomTransformation(),     TokenTextSplitter(         separator=" ",
            chunk_size=512,         chunk_overlap=128     ),     SummaryExtractor(),
        QuestionsAnsweredExtractor(         questions=3     ) ]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting with `version 0.9` , the LlamaIndex framework introduced a really
    neat concept: the so-called **ingestion pipeline** .'
  prefs: []
  type: TYPE_NORMAL
- en: A simple analogy
  prefs: []
  type: TYPE_NORMAL
- en: An ingestion pipeline is a bit like a conveyor belt in a factory. In the context
    of LlamaIndex, it’s a setup that takes your raw data and gets it ready to be integrated
    into your RAG workflow. It does this by running the data through a series of steps
    – called **transformations** – one by one. The key idea is to break the ingestion
    process into a series of reusable transformations that are applied to input data.
    This helps standardize and customize ingestion flows for different use cases.
    Think of transformations as different workstations along this conveyor belt. As
    your raw data moves along, it hits different stations where something specific
    happens. It might be split into sentences at one station – that’s your `SentenceSplitter`
    – and have a title extracted at another – such as using `TitleExtractor` .
  prefs: []
  type: TYPE_NORMAL
- en: 'If the factory’s default workstations don’t quite cut it for you, no worries!
    Let’s say you have this special tool you want to use on your raw data. LlamaIndex
    makes it easy to plug in your custom tool – **custom transformation** . Just say
    what your tool does – for example, replacing acronyms with complete names using
    a dictionary – and LlamaIndex will happily add it to your pipeline. *Figure 4*
    *.4* provides an ingestion pipeline schematic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_04_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – An ingestion pipeline at work
  prefs: []
  type: TYPE_NORMAL
- en: The most important thing about the ingestion pipeline is that *it remembers
    the data it has already processed* . It runs a hashing function on the combination
    of each node data and each transformation run. On any future runs of the same
    transformation on the same nodes, the hashes will be identical, so the cached,
    already processed data will be used instead of re-running the transformation.
  prefs: []
  type: TYPE_NORMAL
- en: What does this mean for me?
  prefs: []
  type: TYPE_NORMAL
- en: If you send the same document through the pipeline again, it’s like having a
    fast-track lane where it skips the line because it’s already been handled. This
    is cool because it saves you both time and money by avoiding useless multiple
    processing of the same data.
  prefs: []
  type: TYPE_NORMAL
- en: By default, the cache is stored locally but you can customize the storage options
    and use any external database provider you prefer. Let’s cover an example of how
    the pipeline could be implemented. I will explain the code section by section
    to make it easier to follow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the first section of the code:'
  prefs: []
  type: TYPE_NORMAL
- en: After taking care of the required imports, to show you how to customize your
    pipeline, I have defined a class called `CustomTransformation` . This will be
    fed into the pipeline later. In my example, no actual processing takes place,
    so this will return the nodes unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s continue with the second section:'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding code is responsible for fetching all content of the `files` subfolder
    into documents. Next, the code checks if the cache file already exists and attempts
    to load it into memory. Remember, the cache file contains the hashes and the results
    generated by the previous runs. The first time you run the code, there will be
    no file, so the code won’t load any cached values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s move on to the third section:'
  prefs: []
  type: TYPE_NORMAL
- en: This is the part where we define our pipeline. As you can see, it will contain
    four transformations. The first is our `CustomTransformation` , followed by `TokenTextSplitter`
    , which is responsible for breaking each document into smaller chunks and generating
    nodes. The third transformation extracts the summaries metadata and the last one
    extracts a set of questions that each node can answer.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to take a peek at the result, you could add `print(nodes[0])` at
    the end of the entire script. Notice that, in the `cache` parameter, we also specify
    the source of the cache for the pipeline. If that is empty, it will be ignored;
    otherwise, it will be used to avoid any unnecessary processing by retrieving values
    from the cache.
  prefs: []
  type: TYPE_NORMAL
- en: 'And now, the final part:'
  prefs: []
  type: TYPE_NORMAL
- en: This is where we run the pipeline with the `show_progress` option set to `True`
    . This will make the pipeline’s progress visible and help you better understand
    what’s happening in the background. In the end, we save the results in the cache
    file to avoid re-processing in the next run.
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick side note:'
  prefs: []
  type: TYPE_NORMAL
- en: Even if you saved a cache file, any changes you make in your pipeline logic
    will not be cached and will have to be processed at the next run.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should also know there is an alternative to manually defining and running
    the pipeline every time you want to ingest more data. Just like with the node
    parsers, we can define the transformations inside `Settings` like this:'
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, an ingestion pipeline is a super-efficient way to get your data
    automatically prepped and polished by running it through customizable sets of
    transformations until it’s just right for your app or database.
  prefs: []
  type: TYPE_NORMAL
- en: As we build up the PITS tutoring app, we’ll leverage ingestion pipelines and
    you’ll get the opportunity to experiment more with this concept.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s talk about more complex scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Handling documents that contain a mix of text and tabular data</title>
  prefs: []
  type: TYPE_NORMAL
- en: Handling documents that contain a mix of text and tabular data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data is not always simple. Many real-world documents, such as research papers,
    financial reports, and others, contain a mix of unstructured text, as well as
    structured tabular data in tables. Ingesting such heterogeneous documents presents
    an additional challenge - we need to not only extract text but also identify,
    parse, and process tables embedded within the text. Because, sometimes you get
    tables, sometimes you get text and sometimes you have to deal with a mix of both.
  prefs: []
  type: TYPE_NORMAL
- en: LlamaIndex provides `UnstructuredElementNodeParser` to tackle such documents
    containing both free-form text as well as tables and other structured elements.
    It leverages the `Unstructured` library to analyze the document layout and delineate
    text sections from tables.
  prefs: []
  type: TYPE_NORMAL
- en: 'This parser works exclusively on HTML files and can extract two types of nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text nodes** : Containing the text chunks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Table nodes** : Containing the table data and metadata, such as coordinates'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing these elements as separate nodes allows more modular and meaningful
    processing later in the RAG workflow. The text can be indexed and searched normally
    with elements like keywords. The tables can be loaded into a **pandas DataFrame**
    or any structured database for SQL-based access. So, in complex cases involving
    mixed data types, leveraging `UnstructuredElementNodeParser` before ingestion
    enables better data organization.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find a complete demo for using `UnstructuredElementNodeParser` in the
    official LlamaIndex documentation: https://docs.llamaindex.ai/en/stable/examples/query_engine/sec_tables/tesla_10q_table.html
    .'
  prefs: []
  type: TYPE_NORMAL
- en: With these new concepts in our toolbox, let’s continue building our tutoring
    project.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Hands-on – ingesting study materials into our PITS</title>
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on – ingesting study materials into our PITS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s time for some practice. We now have everything we need to continue building
    our project. Let’s write the `documend_uploader.py` module.
  prefs: []
  type: TYPE_NORMAL
- en: This module will take care of ingesting and preparing our available study material.
    The user can upload any available books, technical documentation, or existing
    articles to provide more context to our tutor.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have from global_settings import STORAGE_PATH, CACHE_FILE from logging_functions
    import log_action from llama_index import SimpleDirectoryReader, VectorStoreIndex
    from llama_index.ingestion import IngestionPipeline, IngestionCache from llama_index.text_splitter
    import TokenTextSplitter from llama_index.extractors import SummaryExtractor from
    llama_index.embeddings import OpenAIEmbedding the imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we must define the main function that’s responsible for handling the
    ingestion process. You’ll notice that it uses an ingestion pipeline to both streamline
    the def ingest_documents():     documents = SimpleDirectoryReader(         STORAGE_PATH,
            filename_as_id = True     ).load_data()     for doc in documents:         print(doc.id_)
            log_action(             f"File ''{doc.id_}'' uploaded user",             action_type="UPLOAD"
            ) code but also benefit from caching:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The function loads all readable documents available in `STORAGE_PATH` , which
    was defined in `global_settings.py` .
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For each document processed, a new event is stored in our log file using `log_action`
    from `logging_functions.py` .
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, the function checks if there’s already cached pipeline data     try:
            cached_hashes = IngestionCache.from_persist_path(             CACHE_FILE
                )         print("Cache file found. Running using cache")     except:
            cached_hashes = ""         print("No cache file found. Running without")
    to use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The next step     pipeline = IngestionPipeline(         transformations=[             TokenTextSplitter(
                    chunk_size=1024,                 chunk_overlap=20             ),
                SummaryExtractor(summaries=[''self'']),             OpenAIEmbedding()
            ],         cache=cached_hashes     )     nodes = pipeline.run(documents=documents)
        pipeline.cache.persist(CACHE_FILE)     return nodes is to define and run the
    pipeline. If hashes from the cache file correspond, no operations should be processed;
    instead, the values will be directly loaded from the cache:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We run three transformations in the pipeline:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Basic chunking using `TokenTextSplitter` .
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A metadata extractor that summarizes each node.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Embedding generation using `OpenAIEmbedding` . Don’t worry about this step for
    now. I will explain it thoroughly in *Chapter 5* , *Indexing* *with LlamaIndex*
    .
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In the end, the function saves the current data in the cache file and returns
    the processed nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That’s it for now. We have now uploaded and prepared the study materials for
    future processing. We’ll continue with the indexing part in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Summary</title>
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LlamaHub offers a variety of pre-built data loaders, streamlining the process
    of importing data from various sources as documents. This eliminates the need
    for creating unique parsers for different data formats.
  prefs: []
  type: TYPE_NORMAL
- en: After data is imported, it undergoes further processing into nodes, and we discussed
    various customization options available.
  prefs: []
  type: TYPE_NORMAL
- en: There’s a broad range of options for metadata extraction, and the parsing process
    can be tailored to meet specific requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Developing pipelines for data ingestion is an invaluable tool for enhancing
    the efficiency, both in terms of cost and time, of our RAG applications. It’s
    also vital to keep privacy considerations in mind.
  prefs: []
  type: TYPE_NORMAL
- en: With data ingestion completed, let’s continue our journey and discover the indexing
    powers of LlamaIndex.
  prefs: []
  type: TYPE_NORMAL
