- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Accessing the Power of Large Language Models: Advanced Setup and Integration
    with RAG'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this dynamic era of **Artificial Intelligence** (**AI**) and **Machine Learning**
    (**ML**), understanding the vast assortment of available resources and learning
    how to utilize them effectively is vital. **Large Language Models** (**LLMs**)
    such as GPT-4 have revolutionized the field of **Natural** **Language Processing**
    (**NLP**) by showcasing unprecedented performance in diverse tasks, from content
    generation to complex problem-solving. Their immense potential extends not only
    to understanding and generating human-like text but also to bridging the gap between
    machines and humans, in terms of communication and task automation. Embracing
    the practical applications of LLMs can empower businesses, researchers, and developers
    to create more intuitive, intelligent, and efficient systems that cater to a wide
    range of requirements. This chapter offers a guide to setting up access to LLMs,
    walking you through using them and building pipelines with them.
  prefs: []
  type: TYPE_NORMAL
- en: Our journey begins by delving into closed source models that utilize **Application
    Programming Interfaces** (**APIs**), taking OpenAI’s API as a quintessential example.
    We will walk you through a practical scenario, illustrating how you can interact
    with this API using an API key within your Python code, demonstrating the potential
    applications of such models in real-world contexts.
  prefs: []
  type: TYPE_NORMAL
- en: As we advance, we will shift our focus to the realm of open source tools, giving
    you a rundown of widely employed open source models that can be manipulated via
    Python. We aim to provide a grasp of the power and versatility these models provide,
    emphasizing the community-driven benefits of open source development.
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently, we will introduce you to retrieval-augmented generation and, specifically,
    LangChain, a robust tool specifically engineered for interaction with LLMs. LangChain
    is essential for the practical application of LLMs because it provides a unified
    and abstracted interface to them, as well as a suite of tools and modules that
    simplify the development and deployment of LLM-powered applications. We’ll guide
    you through the foundational concept of LangChain, highlighting its distinctive
    methodology to circumvent the inherent challenges posed by LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: The cornerstone of this methodology is the transformation of data into embeddings.
    We will shed light on the pivotal role that **Language Models** (**LMs**) and
    LLMs play in this transformation, demonstrating how they are engaged in creating
    these embeddings. Following this, we will discuss the process of establishing
    a local vector database, giving you a brief overview of vector databases and their
    crucial role in managing and retrieving these embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we will address the configuration of an LLM for prompting, which could
    potentially be the same LLM used for the embedding process. We will take you through
    the stepwise setup procedure, detailing the advantages and potential applications
    of this strategy.
  prefs: []
  type: TYPE_NORMAL
- en: In the penultimate segment, we will touch upon the topic of deploying LLMs to
    the cloud. The scalability and cost-effectiveness of cloud services have led to
    an increased adoption of hosting AI models. We will provide an overview of the
    leading cloud service providers, including **Microsoft Azure**, **Amazon Web Services**
    (**AWS**), and **Google Cloud Platform** (**GCP**), giving you insights into their
    service offerings and how they can be harnessed for LLM deployment.
  prefs: []
  type: TYPE_NORMAL
- en: As we embark on this exploration of LLMs, it’s crucial to acknowledge the continuously
    evolving data landscape that these models operate within. The dynamic nature of
    data – its growth in volume, diversity, and complexity – necessitates a forward-looking
    approach to how we develop, deploy, and maintain LLMs. In the subsequent chapters,
    particularly [*Chapter 10*](B18949_10.xhtml#_idTextAnchor525), we will delve deeper
    into the strategic implications of these evolving data landscapes, preparing you
    to navigate the challenges and opportunities they present. This foundational understanding
    will not only enhance your immediate work with LLMs but also ensure your projects
    remain resilient and relevant in the face of rapid technological and data-driven
    changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go through the main topics covered in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up an LLM application – API-based closed source models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt engineering and priming GPT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up an LLM application – local open source models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Employing LLMs from Hugging Face via Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring advanced system design – RAG and LangChain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing a simple LangChain setup in a Jupyter notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs in the cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, the following will be necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Programming knowledge**: Familiarity with Python programming is a must, since
    the open source models, OpenAI’s API, and LangChain are all illustrated using
    Python code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Access to OpenAI’s API**: An API key from OpenAI will be required to explore
    closed source models. This can be obtained by creating an account with OpenAI
    and agreeing to their terms of service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open source models**: Access to the specific open source models mentioned
    in this chapter will be necessary. These can be accessed and downloaded from their
    respective repositories or via package managers such as **pip** or **conda**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A local development environment**: A local development environment setup
    with Python installed is required. An **Integrated Development Environment** (**IDE**)
    such as PyCharm, Jupyter Notebook, or a simple text editor can be used. Note that
    we recommend a free Google Colab notebook, as it encapsulates all these requirements
    in a seamless web interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The ability to install libraries**: You must have permission to install the
    required Python libraries, such as NumPy, SciPy, TensorFlow, and PyTorch. Note
    that the code we provide includes the required installations, and you won’t have
    to install them beforehand. We simply stress that you should have permission to
    do so, which we expect you would. Specifically, using a free Google Colab notebook
    would suffice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hardware requirements**: Depending on the complexity and size of the models
    you’re working with, a computer with sufficient processing power (potentially
    including a good GPU for ML tasks) and ample memory will be required. This is
    only relevant when choosing to not use the free Google Colab.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve grasped the transformative potential of LLMs and the variety
    of tools available, let’s delve deeper and explore how to effectively set up LLM
    applications using APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up an LLM application – API-based closed source models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When looking to employ models in general and LLMs in particular, there are various
    design choices and trade-offs. One key choice is whether to host a model locally
    in your local environment or to employ it remotely, accessing it via a communication
    channel. Local development environments would be wherever your code runs, whether
    that’s your personal computer, your on-premises server, your cloud environment,
    and so on. The choice you make will impact many aspects, such as cost, information
    security, maintenance needs, network overload, and inference speed.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will introduce a quick and simple approach to employing
    an LLM remotely via an API. This approach is quick and simple as it rids us of
    the need to allocate unusual computation resources to host the LLM locally. An
    LLM typically requires amounts of memory and computation resources that aren’t
    common in personal environments.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a remote LLM provider
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before diving into implementation, we need to select a suitable LLM provider
    that aligns with our project requirements. OpenAI, for example, offers several
    versions of the GPT-3.5 and GPT-4 models with comprehensive API documentation.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI’s remote GPT access in Python via an API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To gain access to OpenAI’s LLM API, we need to create an account on their website.
    This process involves registration, account verification, and obtaining API credentials.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI’s website provides guidance for these common actions, and you will be
    able to get set up quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Once registered, we should familiarize ourselves with OpenAI’s API documentation.
    This documentation will guide us through the various endpoints, methods, and parameters
    available to interact with the LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: The first hands-on experience we will take on will be employing OpenAI’s LLMs
    via Python. We have put together a notebook that presents the simple steps of
    employing OpenAI’s GPT model via an API. Refer to the `Ch8_Setting_Up_Close_Source_and_Open_Source_LLMs.ipynb`
    notebook. This notebook, called *Setting Up Close Source and Open Source LLMs*,
    will be utilized in the current section about OpenAI’s API, and also in the subsequent
    section about setting up local LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s walk through the code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by installing the required Python libraries. In particular, to communicate
    with the LLM API, we need to install the necessary Python library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Define OpenAI’s API key**: Before making requests to the LLM API, we must
    embed our personal API key in the library’s configuration. The API key is made
    available for you on OpenAI’s website when you register. This can be done by either
    explicitly pasting the key’s string to be hardcoded in our code or reading it
    from a file that holds that string. Note that the former is the simplest way to
    showcase the API, as it doesn’t require an additional file to be set up, but it
    may not be the right choice when working in a shared development environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Settings – set the model’s configurations**. Here, we set the various parameters
    that control the model’s behavior.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As the foundation is set for connecting to LLMs through APIs, it’s valuable
    to turn our attention to an equally important aspect – prompt engineering and
    priming, the art of effectively communicating with these models.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering and priming GPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let us pause and provide some context before returning to discuss the next part
    of the code.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering is a technique used in NLP to design effective prompts or
    instructions when interacting with LLMs. It involves carefully crafting the input
    given to a model to elicit the desired output. By providing specific cues, context,
    or constraints in the prompts, prompt engineering aims to guide the model’s behavior
    and encourage the generation of more accurate, relevant, or targeted responses.
    The process often involves iterative refinement, experimentation, and understanding
    the model’s strengths and limitations to optimize the prompt for improved performance
    in various tasks, such as question-answering summarization or conversation generation.
    Effective prompt engineering plays a vital role in harnessing the capabilities
    of LMs and shaping their output to meet specific user requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s review one of the most impactful tools in prompt engineering, **priming**.
    Priming GPT via an API involves providing initial context to the model before
    generating a response. The priming step helps set the direction and style of the
    generated content. By giving the model relevant information or examples related
    to the desired output, we can guide its understanding and encourage more focused
    and coherent responses. Priming can be done by including specific instructions,
    context, or even partial sentences that align with the desired outcome. Effective
    priming enhances the model’s ability to generate responses that better match the
    user’s intent or specific requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Priming is done by introducing GPT with several types of messages:'
  prefs: []
  type: TYPE_NORMAL
- en: The main message is the **system prompt**. This message instructs the model
    about the *role* it may play, the way it should answer the questions, the constraints
    it may have, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second type of message is a **user prompt**. A user prompt is sent to the
    model in the priming phase, and it represents an example user prompt, much like
    the prompt you may enter in ChatGPT’s web interface. However, when priming, this
    message could be presented to the model as an example of how it should address
    such a prompt. The developer will introduce a user prompt of some sort and will
    then show the model how it is expected to answer that prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, observe this priming code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]\nimport pandas as pd\n[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]python'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: import numpy as np
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can see that we prime the model to provide concise answers in a Markdown
    format. The example that is used to teach the model is in the form of a question
    and an answer. The question is via a user prompt, and the way we tell the model
    what the potential answer is is provided via an assistant prompt. We then provide
    the model with another user prompt; this one is the actual prompt we’d like the
    model to address for us, and as shown in the output, it gets it right.
  prefs: []
  type: TYPE_NORMAL
- en: By looking at OpenAI’s documentation about prompt engineering, you’ll find that
    there are additional types of prompts to prime the GPT models with.
  prefs: []
  type: TYPE_NORMAL
- en: Going back to our notebook and code, in this section, we leverage *GPT-3.5 Turbo*.
    We prime it in the simplest manner, only giving it a system prompt to provide
    directions in order to showcase how additional functionality could stem from the
    system prompt. We tell the model to finish a response by alerting us about typos
    in the prompt and correcting them.
  prefs: []
  type: TYPE_NORMAL
- en: We then provide our desired prompt in the user prompt section, and we insert
    a few typos into it. Run that code and give it a shot.
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with OpenAI’s GPT model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this stage, we send our prompts to the model.
  prefs: []
  type: TYPE_NORMAL
- en: The following simple example code is run once in the *Setting Up Close Source
    and Open Source LLMs* notebook. You can wrap it in a function and call it repeatedly
    in your own code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some aspects worth noticing are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parsing and processing the returned output from the model**: We structure
    the output response in a coherent manner for the user to read:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Error handling**: We designed the code to allow for several failed attempts
    before accepting a failure to use the API:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Rate limits and cost mitigation**: We don’t implement such restrictions here,
    but it would be ideal to have both of these in an experimental setting and perhaps
    in production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The result of the preceding code is demonstrated as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The model provided us with a legitimate, concise response. It then notified
    us about the typos, which are perfectly in line with the system prompt we provided
    it with.
  prefs: []
  type: TYPE_NORMAL
- en: That was an example showcasing the employment of a remote, off-premises, closed
    source LLM. While leveraging the power of paid APIs such as OpenAI offers convenience
    and cutting-edge performance, there’s also immense potential in tapping into free
    open source LLMs. Let’s explore these cost-effective alternatives next.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up an LLM application – local open source models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we shall touch on the complementary approach to a closed source implementation,
    that is, an open source, local implementation.
  prefs: []
  type: TYPE_NORMAL
- en: We will see how you can achieve a similar functional outcome to the one we reviewed
    in the previous section, without having to register for an account, pay, or share
    prompts that contain possibly sensitive information with a third-party vendor,
    such as OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: About the different aspects that distinguish between open source and closed
    source
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When selecting between open source LLMs, such as LLaMA and GPT-J, and closed
    source, API-based models such as OpenAI’s GPT, several critical factors must be
    considered.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, cost is a major factor. Open source LLMs often have no licensing fees,
    but they require significant computational resources for training and inference,
    which can be expensive. Closed source models, while potentially carrying a subscription
    or pay-per-use fee, eliminate the need for substantial hardware investments.
  prefs: []
  type: TYPE_NORMAL
- en: Processing speed and maintenance are closely linked to computational resources.
    Open source LLMs, if deployed on powerful enough systems, can offer high processing
    speeds but require ongoing maintenance and updates by the implementing team. In
    contrast, closed source models managed by the provider ensure continual maintenance
    and model updates, often with better efficiency and reduced downtime, but processing
    speed can be dependent on the provider’s infrastructure and network latency.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding model updates, open source models offer more control but require a
    proactive approach to incorporate the latest research and improvements. Closed
    source models, however, are regularly updated by the provider, ensuring access
    to the latest advancements without additional effort from the user.
  prefs: []
  type: TYPE_NORMAL
- en: Security and privacy are paramount in both scenarios. Open source models can
    be more secure, as they can be run on private servers, ensuring data privacy.
    However, they demand robust in-house security protocols. Closed source models,
    managed by external providers, often come with built-in security measures but
    pose potential privacy risks, due to data handling by third parties.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the choice between open source and closed source LLMs hinges on the
    trade-off between cost, control, and convenience, with each option presenting
    its own set of advantages and challenges.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that in mind, let’s revisit Hugging Face, the company that put together
    the largest and most approachable hub for free LMs. In the following example,
    we will leverage Hugging Face’s easy and free library: transformers.'
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face’s hub of models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When looking to choose an LLM for our task, we recommend referring to Hugging
    Face’s Models online page. They offer an enormous amount of Python-based, open
    source LLMs. Every model has a page dedicated to it, where you can find information
    about it, including the syntax needed to employ that model via Python code in
    your personal environment.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that in order to implement a model locally, you must have
    an internet connection from the machine that runs the Python code. However, as
    this requirement may become a bottleneck in some cases – for instance, when the
    development environment is restricted by a company’s intranet or has limited internet
    access due to firewall restrictions – there are alternative approaches. Our recommended
    approach is to clone the model repository from Hugging Face’s domain. That is
    a less trivial and less-used approach. Hugging Face provides the necessary cloning
    commands on each model’s web page.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When looking to choose a model, there may be several factors that come into
    play. Depending on your intentions, you may care about configuration speed, processing
    speed, storage space, computation resources, legal usage restrictions, and so
    on. Another factor worth noting is the popularity of a model. It attests to how
    frequently that model is chosen by other developers in the community. For instance,
    if you look for LMs that are labeled for zero-shot classification, you will find
    a very large collection of available models. But, if you then narrow the search
    some more so to only be left with models that were trained on data from news articles,
    you would be left with a much smaller set of available models. In which case,
    you may want to refer to the popularity of each model and start your exploration
    with the model that was used the most.
  prefs: []
  type: TYPE_NORMAL
- en: Other factors that may interest you could be publications about the model, the
    model’s developers, the company or university that released the model, the dataset
    that the model was trained on, the architecture the model was designed by, the
    evaluation metrics, and other potential factors that may be available on each
    model’s web page on Hugging Face’s website.
  prefs: []
  type: TYPE_NORMAL
- en: Employing LLMs from Hugging Face via Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will review a code notebook that exemplifies implementing an open source
    LLM locally using Hugging Face’s free resources. We will continue with the same
    notebook from the previous section, *Setting Up Close Source and Open* *Source*
    LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pip` on the Terminal, we will run the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Alternatively, if running directly from a Jupyter notebook, add `!` to the beginning
    of the command.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Experiment with Microsoft’s DialoGPT-medium**: This LLM is dedicated to conversational
    applications. It was generated by Microsoft and achieved high scores when compared
    to other LLMs on common benchmarks. For that reason, it is also quite popular
    on Hugging Face’s platform, in the sense that it is downloaded frequently by ML
    developers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the **Settings** code section in the notebook, we will define the parameters
    for this code and import the model and its tokenizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that this code requires access to the internet. Even though the model is
    deployed locally, an internet connection is required to import it. Again, if you
    wish, you can clone the model’s repo from Hugging Face and then no longer be required
    to have access to the internet.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Define the prompt**: As can be seen in the following code block, we picked
    a straightforward prompt here, much like a user prompt for the GPT-3.5-Turbo model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Experiment with the model**: Here, we have the syntax that suits this code.
    If you want to create a rolling conversation with this model, you wrap this code
    in a function and iterate over it, collecting prompts from the user in real time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**The result**: The resulting prompt is, **If dinosaurs were alive today, would
    they possess a threat** **to people?:**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This section established the tremendous value proposition that LLMs can bring.
    We now have the necessary background to learn and explore a new frontier in efficient
    LLM application development – constructing pipelines using tools such as LangChain.
    Let’s dive into this advanced approach.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring advanced system design – RAG and LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Retrieval-Augmented Generation** (**RAG**) is a development framework designed
    for seamless interaction with LLMs. LLMs, by virtue of their generalist nature,
    are capable of performing a vast array of tasks competently. However, their generality
    often precludes them from delivering detailed, nuanced responses to queries that
    necessitate specialized knowledge or in-depth expertise in a domain. For instance,
    if you aspire to use an LLM to address queries concerning a specific discipline,
    such as law or medicine, it might satisfactorily answer general queries but fail
    to respond accurately to those needing detailed insights or up-to-date knowledge.'
  prefs: []
  type: TYPE_NORMAL
- en: RAG designs offer a comprehensive solution to the limitations typically encountered
    in LLM processing. In a RAG framework, the text corpus undergoes initial preprocessing,
    where it’s segmented into summaries or distinct chunks and then embedded within
    a vector space. When a query is made, the model identifies the most relevant segments
    of this data and utilizes them to form a response. This process involves a combination
    of offline data preprocessing, online information retrieval, and the application
    of the LLM for response generation. It’s a versatile approach that can be adapted
    to a variety of tasks, including code generation and semantic search. RAG models
    function as an abstraction layer that orchestrates these processes. The efficacy
    of this method is continually increasing, with its applications expanding as LLMs
    evolve and require more contextually rich data during prompt processing. In [*Chapter
    10*](B18949_10.xhtml#_idTextAnchor525), we will present a deeper discussion of
    RAG models and their role in the future of LLM solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve introduced the premise and capabilities of RAG models, let’s
    focus on one particular example, called LangChain. We will review the nuts and
    bolts of its design principles and how it interfaces with data sources.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain’s design concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will dissect the core methodologies and architectural decisions
    that make LangChain stand out. This will give us insights into its structural
    framework, the efficiency of data handling, and its innovative approach to integrating
    LLMs with various data sources.
  prefs: []
  type: TYPE_NORMAL
- en: Data sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most significant virtues of LangChain is the ability to connect an
    arbitrary LLM to a defined data source. By arbitrary, we mean that it could be
    any *off-the-shelf* LLM that was designed and trained with no specific regard
    to the data we are looking to connect it to. Employing LangChain allows us to
    customize it to our domain. The data source is to be used for reference when structuring
    the answer to the user prompt. That data may be proprietary data owned by a company
    or local personal information on your personal machine.
  prefs: []
  type: TYPE_NORMAL
- en: However, when it comes to leveraging a given database, LangChain does more than
    point the LLM to the data; it employs a particular processing scheme and makes
    it quick and efficient. It creates a vector database.
  prefs: []
  type: TYPE_NORMAL
- en: Given raw text data, be it free text in a `.txt` file, formatted files, or other
    various data structures of text, a vector database is created by chunking the
    text into appropriate lengths and creating numerical text embeddings, using a
    designated model. Note that if the designated embedding model is chosen to be
    an LLM, it doesn’t have to be the same LLM that is used for prompting. For instance,
    the embedding model could be picked to be a free, sub-optimal, open source LLM,
    and the prompting model could be a paid LLM with optimal performance. Those embeddings
    are then stored in a vector database. You can clearly see that this approach is
    extremely storage-efficient, as we transform text, and perhaps encoded text, into
    a finite set of numerical values, which by its nature is dense.
  prefs: []
  type: TYPE_NORMAL
- en: When a user enters a prompt, a search mechanism identifies the relevant data
    chunks in the embedded data source. The prompt gets embedded with the same designated
    embedding model. Then, the search mechanism applies a similarity metric, such
    as cosine similarity, for example, and finds the most similar text chunks in the
    defined data source. Then, the original text of these chunks is retrieved. The
    original prompt is then sent again, this time to the prompting LLM. The difference
    is that, this time, the prompt consists of more than just the original user’s
    prompt; it also consists of the retrieved text as a reference. This enables the
    LLM to get a question and a rich text supplement for reference. The LLM then can
    refer to the added information as a reference.
  prefs: []
  type: TYPE_NORMAL
- en: If it weren’t for this design, when the user wanted to find an answer to their
    question, they would need to read through the vast material and find the relevant
    section. For instance, the material may be a company’s entire product methodology,
    consisting of many PDF documents. This process leverages an automated smart search
    mechanism that narrows the relevant material down to an amount of text that can
    fit into a prompt. Then, the LLM frames the answer to the question and presents
    it to the user immediately. If you wish, the pipeline can be designed to quote
    the original text that it used to frame the answer, thus allowing for transparency
    and verification.
  prefs: []
  type: TYPE_NORMAL
- en: 'This paradigm is portrayed in *Figure 8**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – The paradigm of a typical LangChain pipeline](img/B18949_08_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – The paradigm of a typical LangChain pipeline
  prefs: []
  type: TYPE_NORMAL
- en: In order to explain the prompt engineering behind the LangChain pipeline, let’s
    review a financial information use case. Your data source is a cohort of **Securities**
    **&** **Exchange** **Commission** (**SEC**) filings of public companies from the
    US. You are looking to identify companies that gave dividends to their stock holders,
    and in what year.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your prompt would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The pipeline then embeds this question and looks for text chunks with similar
    context (e.g., that discuss paid dividends). It identifies many such chunks, such
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The LangChain pipeline then forms a new prompt that includes the text of the
    identified chunks. In this example, we assume the prompted LLM is OpenAI’s GPT.
    LangChain embeds the information in the system prompt sent to OpenAI’s GPT model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the system prompt is used to instruct the model how to act and
    then to provide the context.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an understanding of the foundational approach and benefits
    of LangChain, let’s go deeper into its intricate design concepts, starting with
    how it bridges LLMs to diverse data sources efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Data that is not pre-embedded
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the preceding description is of data that is preprocessed to take the
    form of a vector database, another approach is to set up access to external data
    sources that are not yet processed into an embedding form. For instance, you may
    wish to leverage a SQL database to supplement other data sources. This approach
    is referred to as **multiple** **retrieval sources**.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve now explored the ways LangChain efficiently interfaces with various data
    sources; now, it is essential to grasp the core structural elements that enable
    its functionalities – chains and agents.
  prefs: []
  type: TYPE_NORMAL
- en: Chains
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The atomic building blocks within LangChain are called components. Typical components
    could be a prompt template, access to various data sources, and access to LLMs.
    When combining various components to form a system, we form a chain. A chain can
    represent a complete LLM-driven application.
  prefs: []
  type: TYPE_NORMAL
- en: We will now present the concept of agents and walk through a code example that
    showcases how chains and agents come together, creating a capability that would
    have been quite complex not too long ago.
  prefs: []
  type: TYPE_NORMAL
- en: Agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next layer of complexity over chains is agents. Agents leverage chains by
    employing them and complementing them with additional calculations and decisions.
    While a chain may yield a response to a simple request prompt, an agent would
    process the response and act upon it with further downstream processing based
    on a prescribed logic.
  prefs: []
  type: TYPE_NORMAL
- en: You can view agents as a reasoning mechanism that employs what we call a tool.
    Tools complement LLMs by connecting them with other data or functions.
  prefs: []
  type: TYPE_NORMAL
- en: Given the typical LLM shortcomings that prevent LLMs from being perfect multitaskers,
    agents employ tools in a prescribed and monitored manner, allowing them to retrieve
    necessary information, leverage it as context, and execute actions using designated
    existing solutions. Agents then observe the results and employ the prescribed
    logic for further downstream processes.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, assume we want to calculate the salary trajectory for an average
    entry-level programmer in our area. This task is comprised of three key sub-tasks
    – finding out what that average starting salary is, identifying the factors for
    salary growth (e.g., a change in the cost of living, or a typical merit increase),
    and then projecting onward. An ideal LLM would be able to do the entire process
    by itself, not requiring anything more than a coherent prompt. However, given
    the typical shortcomings, such as hallucinations and limited training data, current
    LLMs would not be able to perform this entire process to a level where it could
    be productionized within a commercial product. A best practice is to break it
    down and monitor the thought process via agents.
  prefs: []
  type: TYPE_NORMAL
- en: 'In its most simple design, this would require the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining an agent that can access the internet and that can calculate future
    values of time series, given growth factors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Providing the agent with a comprehensive prompt
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The agent breaks the prompt down into the different sub-tasks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fetching the average salary from the internet
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Fetching the growth factors
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Employing the calculation tool by applying the growth factors to the starting
    salary and creating a future time series for salary values
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To exemplify the agentic approach, let's review a simple task that involves
    fetching a particular detail from the web, and using it to perform a calculation.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install these packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, run the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is then shown as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that we didn’t apply any method to fix the LLM to reproduce this exact
    response. Running this code again will yield a slightly different answer.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive deeper into several examples with code. In
    particular, we will program a multi-agent framework, where a team of agents is
    working on a joint project.
  prefs: []
  type: TYPE_NORMAL
- en: Long-term memory and referring to prior conversations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another very important concept is long-term memory. We discussed how LangChain
    complements an LLM’s knowledge by appending additional data sources, some of which
    may be proprietary, making it highly customized for a particular use case. However,
    it still lacks a very important function, the ability to refer to prior conversations
    and learn from them. For instance, you can design an assistant for a project manager.
    As the user interacts with it, they would ideally update each day about the progress
    of the work, the interactions, the challenges, and so on. It would be best if
    the assistant could digest all that newly accumulated knowledge and sustain it.
    That would allow for a scenario such as this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**User**: “Where do we stand with regard to Jim’s team’s task?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Assistant**: “According to the original roadmap, Jim’s team is to address
    the client’s feedback to the design of the prototype. Based on the update from
    last week, the client provided only partial feedback, which you felt would not
    yet be sufficient for Jim’s team to start work.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will touch more on the concept of memory in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring continuous relevance through incremental updates and automated monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To maintain the accuracy and relevance of LLM outputs in dynamic information
    environments, it’s imperative to implement strategies for the ongoing update and
    maintenance of vector databases. As the corpus of knowledge continues to expand
    and evolve, so too must the embeddings that serve as the foundation for LLM responses.
    Incorporating techniques for incremental updates allows these databases to refresh
    their embeddings as new information becomes available, ensuring that the LLMs
    can provide the most accurate and up-to-date responses.
  prefs: []
  type: TYPE_NORMAL
- en: Incremental updates involve periodically re-embedding existing data sources
    with the latest information. This process can be automated to scan for updates
    in the data source, re-embed the new or updated content, and then integrate these
    refreshed embeddings into the existing vector database, without the need for a
    complete overhaul. By doing so, we ensure that the database reflects the most
    current knowledge available, enhancing the LLM’s ability to deliver relevant and
    nuanced responses.
  prefs: []
  type: TYPE_NORMAL
- en: Automated monitoring plays a pivotal role in this ecosystem by continually assessing
    the quality and relevance of the LLM’s outputs. This involves setting up systems
    that track the performance of the LLM, identifying areas where responses may be
    falling short due to outdated information or missing contexts. When such gaps
    are identified, the monitoring system can trigger an incremental update process,
    ensuring that the database remains a robust and accurate reflection of the current
    knowledge landscape.
  prefs: []
  type: TYPE_NORMAL
- en: By embracing these strategies, we ensure that LangChain and similar RAG frameworks
    can sustain their effectiveness over time. This approach not only enhances the
    relevance of LLM applications but also ensures that they can adapt to the rapidly
    evolving landscape of information, maintaining their position at the forefront
    of NLP technology.
  prefs: []
  type: TYPE_NORMAL
- en: We can now get hands-on with LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing a simple LangChain setup in a Jupyter notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are now ready to set up a complete pipeline that can later be lent to various
    NLP applications.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the `Ch8_Setting_Up_LangChain_Configurations_and_Pipeline.ipynb` notebook.
    This notebook implements the LangChain framework. We will walk through it step
    by step, explaining the different building blocks. We chose a simple use case
    here, as the main point of this code is to show how to set up a LangChain pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, we are in the healthcare sector. We have many care givers;
    each has many patients they may see. The physician in chief made a request on
    behalf of all the physicians in the hospital to be able to use a smart search
    across their notes. They heard about the new emerging capabilities with LLMs,
    and they would like to have a tool where they can search within the medical reports
    they wrote.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, one physician said the following:'
  prefs: []
  type: TYPE_NORMAL
- en: “*I often come across research that may be relevant to a patient I saw months
    ago, but I don’t recall who that was. I would like to have a tool where I can
    ask, ‘Who was that patient that complained about ear pain and had a family history
    of migraines?’, and it would find me* *that patient.*”
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the business objective here is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: “*The CTO tasked us with putting together a quick prototype in the form of a
    Jupyter notebook. We will collect several clinical reports from the hospital’s
    database, and we will use LangChain to search through them in the manner that
    the physician in the* *example described.”*
  prefs: []
  type: TYPE_NORMAL
- en: Let’s jump right in by designing the solution in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a LangChain pipeline with Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Diving into the practicalities of LangChain, this section will guide you step
    by step in setting up a LangChain pipeline using Python, from installing the necessary
    libraries to executing sophisticated similarity searches.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the required Python libraries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As always, we have a list of libraries that we will need to install. Since
    we are writing the code in a Jupyter notebook, we can install them from within
    the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Load the text files with mock physician notes**: Here, we put together some
    mock physician notes. We load them and process them per the LangChain paradigm.
    We stress that these aren’t real medical notes and that the people described there
    don’t exist.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Process the data so that it can be prepared for embedding**: Here, we split
    the text per the requirements of the embedding model. As we mentioned in previous
    chapters, LMs, such as those used for embedding, have a finite window of input
    text that they can process in a single batch. That size is hardcoded in their
    design architecture and is fixed for each particular model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create the embeddings that would be stored in the vector database**: The
    vector database is one of the key pillars of the LangChain paradigm. Here, we
    take the text and create an embedding for each item. Those embeddings are then
    stored in a dedicated vector database. The LangChain library allows you to work
    with several different vector databases. While we chose one particular database,
    you can refer to the **Vector Store** page to read more about the different choices.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create the vector database**: Here, we create the vector database. This process
    may be slightly different for each database choice. However, the creators of these
    databases make sure to take away all of the hard work and leave you with a simple
    turnkey function that creates the database for you, given the appropriate embeddings
    in vector form. We leverage Meta’s **Facebook AI Similarity Search** (**FAISS**)
    database, as it is simple, quick to deploy, and free.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Perform a similarity search based on our in-house documents**: This is the
    key part of the pipeline. We introduce several questions and use LangChain’s similarity
    search to identify the physician notes that would best answer our question.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we can see, the similarity search function is able to do a good job with
    most of the questions. It embeds the question and looks for reports whose embeddings
    are similar.
  prefs: []
  type: TYPE_NORMAL
- en: However, a similarity search could only go so far when it comes to answering
    the question correctly. It is easy to think of a question that discusses a matter
    that is very similar to one of the notes, yet a minor difference confuses the
    similarity search mechanism. For instance, the similarity search process actually
    makes a mistake in question two, mistaking different months and, thus, providing
    a wrong answer.
  prefs: []
  type: TYPE_NORMAL
- en: In order to overcome this matter, we would want to do more than just a similarity
    search. We would want an LLM to review the results of the similarity search and
    apply its judgment. We will see how that’s done in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: With our foundation set for LangChain’s practical applications in Python, let’s
    now move on to understanding how the cloud plays a pivotal role, especially when
    harnessing the true potential of LLMs in contemporary computational paradigms.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs in the cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this era of big data and computation, cloud platforms have emerged as vital
    tools for managing large-scale computations, providing infrastructure, storage,
    and services that can be rapidly provisioned and released with minimal management
    effort.
  prefs: []
  type: TYPE_NORMAL
- en: This section will focus on computation environments in the cloud. These have
    become the dominant choice for many leading companies and institutions. As an
    organization, having a computation environment in the cloud versus on-premises
    makes a major difference. It impacts the ability to share resources and manage
    allocations, maintenance, and cost. There are many trade-offs for employing cloud
    services instead of owning physical machines. You can learn about them by searching
    online or even asking a chat LLM about them.
  prefs: []
  type: TYPE_NORMAL
- en: One significant difference with cloud computing is the ecosystem that the providers
    have built around it. When you pick a cloud provider as your computation hub,
    you tap into a whole suite of additional products and services, opening up a new
    world of capabilities that would not be as accessible to you otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will focus on the LLM aspect of those services.
  prefs: []
  type: TYPE_NORMAL
- en: The three primary cloud platforms are AWS, Microsoft Azure, and GCP. These platforms
    offer a myriad of services, catering to the varying needs of businesses and developers.
    When it comes to NLP and LLMs, each platform provides dedicated resources and
    services to facilitate experimentation, deployment, and production.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore each of these platforms to see how they cater to our specific
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: AWS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AWS remains a dominant force in the cloud computing landscape, providing a comprehensive
    and evolving suite of services that cater to the needs of ML and AI development.
    AWS is renowned for its robust infrastructure, extensive service offerings, and
    deep integration with ML tools and frameworks, making it a preferred platform
    for developers and data scientists looking to innovate with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with LLMs on AWS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'AWS provides a rich ecosystem of tools and services designed to facilitate
    the development and experimentation with LLMs, ensuring that researchers and developers
    have access to the most advanced ML capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon SageMaker**: The cornerstone of ML on AWS, SageMaker is a fully managed
    service that streamlines the entire ML workflow. It offers Jupyter notebook instances
    for experimentation, broad framework support, including TensorFlow and PyTorch,
    and a range of tools for model building, training, and debugging. SageMaker’s
    capabilities have been continually enhanced to support the complexities of training
    and fine-tuning LLMs, providing scalable compute options and optimized ML environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Deep Learning Containers and Deep Learning AMIs**: For those looking
    to customize their ML environments, AWS offers Deep Learning Containers and **Amazon
    Machine Images** (**AMIs**) pre-installed with popular ML frameworks. These resources
    simplify the setup process for LLM experiments, allowing developers to focus on
    innovation rather than infrastructure configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pre-trained models and SageMaker JumpStart**: AWS has expanded its library
    of pre-trained models accessible through SageMaker JumpStart, facilitating quick
    experimentation with LLMs for a variety of NLP tasks. JumpStart also offers solution
    templates and executable example notebooks, making it easier for developers to
    start and scale their ML projects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying and productionizing LLMs on AWS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'AWS provides a suite of services designed to efficiently deploy and manage
    LLMs at scale, ensuring that models are easily accessible and performant under
    varying loads:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SageMaker endpoints**: To deploy LLMs, SageMaker endpoints offer fully managed
    hosting services with auto-scaling capabilities. This service allows developers
    to deploy trained models into production quickly, with the infrastructure automatically
    adjusting to the demands of the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elastic Inference and Amazon EC2 Inf1 instances**: To optimize inference
    costs, AWS offers Elastic Inference, which adds GPU-powered inference acceleration
    to SageMaker instances. For even greater performance and cost efficiency, Amazon
    EC2 Inf1 instances, powered by AWS Inferentia chips, provide high-throughput and
    low-latency inference for DL models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Lambda and Amazon Bedrock**: For serverless deployment, AWS Lambda supports
    running inference without provisioning or managing servers, ideal for applications
    with variable demand. Amazon Bedrock, represents a significant leap forward, offering
    serverless access to foundational models through APIs, model customization, and
    seamless integration within an organizational network, ensuring data privacy and
    security.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s move on to the next topic, Microsoft Aure.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Azure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Microsoft Azure stands at the forefront of cloud computing services, offering
    a robust platform for the development, deployment, and management of ML and LLMs.
    Leveraging its strategic partnership with OpenAI, Azure provides exclusive cloud
    access to GPT models, positioning itself as a critical resource for developers
    and data scientists aiming to harness the power of advanced NLP technologies.
    Recent enhancements have expanded Azure’s capabilities, making it an even more
    attractive choice for those looking to push the boundaries of AI and ML applications.
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with LLMs on Azure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Azure has significantly enriched its offerings to support research and experimentation
    with LLMs, providing a variety of tools and platforms that cater to the diverse
    needs of the AI development community:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Azure OpenAI Service**: This directly integrates OpenAI’s cutting-edge models,
    including the latest GPT versions, DALL·E, and Codex, into the Azure ecosystem.
    This service enables developers to easily incorporate sophisticated AI functionalities
    into their applications, with the added benefits of Azure’s scalability and management
    tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure Machine Learning (Azure ML)**: This offers an advanced environment
    for the custom training and fine-tuning of LLMs on specific datasets, allowing
    for enhanced model performance on niche tasks. Azure ML Studio’s pre-built and
    customizable Jupyter notebook templates support a wide range of programming languages
    and frameworks, facilitating a seamless experimentation process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure Cognitive Services**: This provides access to a suite of pre-built
    AI services, including text analytics, speech services, and decision-making capabilities
    powered by LLMs. These services enable developers to add complex AI functions
    to applications quickly, without deep ML expertise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying and productionizing LLMs on Azure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Azure’s infrastructure and services offer comprehensive solutions for the deployment
    and productionization of LLM applications, ensuring scalability, performance,
    and security:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deployment options**: Azure supports various deployment scenarios through
    **Azure Container Instances** (**ACI**) for lightweight deployment needs and **Azure
    Kubernetes Service** (**AKS**) for larger, more complex applications requiring
    high scalability. These services allow for the efficient scaling of LLM applications
    to meet user demand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model management**: Through Azure ML, developers can manage the life cycle
    of their models, including version control, auditing, and governance. This ensures
    that deployed models are not only performant but also comply with industry standards
    and regulatory requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security and compliance**: Azure emphasizes security and compliance across
    all its services, providing features such as data encryption, access control,
    and comprehensive compliance certifications. This commitment ensures that applications
    built and deployed on Azure meet the highest standards for data protection and
    privacy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GCP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GCP continues to be a powerhouse in cloud computing, providing an extensive
    suite of services that cater to the evolving needs of AI and ML development. Known
    for its cutting-edge innovations in AI and ML, GCP offers a rich ecosystem of
    tools and services that facilitate the development, deployment, and scaling of
    LLMs, making it an ideal platform for developers and researchers aiming to leverage
    the latest in AI technology.
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with LLMs on GCP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'GCP has further enhanced its capabilities for experimenting with and developing
    LLMs, offering a comprehensive set of tools that support the entire ML workflow,
    from data ingestion and model training to hyperparameter tuning and evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vertex AI**: At the heart of GCP’s ML offerings, Vertex AI provides an integrated
    suite of tools and services that streamline the ML workflow. It offers advanced
    features for training and fine-tuning LLMs, including AutoML capabilities for
    automating the selection of optimal model architectures and hyperparameters. Vertex
    AI’s integration with GCP’s robust data and analytics services makes it easier
    to manage large datasets that are essential for training LLMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**An IDE**: The built-in notebooks service within Vertex AI offers a fully
    managed JupyterLab environment, enabling developers to write, run, and debug ML
    code seamlessly. This environment is optimized for ML development, supporting
    popular frameworks such as TensorFlow and PyTorch, which are crucial for building
    and experimenting with LLMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI and ML libraries**: GCP continues to expand its library of pre-trained
    models and ML APIs, including those specifically designed for NLP and understanding.
    These tools allow developers to integrate advanced NLP capabilities into their
    applications rapidly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying and productionizing LLMs on GCP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'GCP provides robust and scalable solutions for deploying and productionizing
    LLMs, ensuring that applications built on its platform can meet the demands of
    real-world usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vertex AI prediction**: Once an LLM is trained, Vertex AI’s prediction service
    allows for the easy deployment of models as fully managed, auto-scaling endpoints.
    This service simplifies the process of making your LLMs accessible to applications,
    with the infrastructure automatically adjusting to the workload demands.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Kubernetes Engine (GKE)**: For more complex deployment scenarios requiring
    high availability and scalability, GKE offers a managed environment to deploy
    containerized LLM applications. GKE’s global infrastructure ensures that your
    models are highly available and can scale to meet the needs of enterprise-level
    applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concluding cloud services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The landscape of cloud computing continues to evolve rapidly, with AWS, Azure,
    and GCP each offering unique advantages for the development and deployment of
    LLMs. AWS stands out for its broad infrastructure and deep integration with ML
    tools, making it ideal for a wide range of ML and AI projects. Azure, with its
    exclusive access to OpenAI’s models and deep integration within the Microsoft
    ecosystem, offers unparalleled opportunities for enterprises looking to leverage
    the cutting edge of AI technology. GCP, recognized for its innovation in AI and
    ML, provides tools and services that mirror Google’s internal AI advancements,
    appealing to those seeking the latest in AI research and development. As the capabilities
    of these platforms continue to expand, the choice between them will increasingly
    depend on specific project needs, organizational alignment, and strategic partnerships,
    underscoring the importance of a thoughtful evaluation based on the current and
    future landscape of cloud-based AI and ML.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the world of NLP and LLMs continues to grow rapidly, so do the various practices
    of system design. In this chapter, we reviewed the design process of LLM applications
    and pipelines. We discussed the components of these approaches, touching on both
    API-based closed source and local open source solutions. We then gave you hands-on
    experience with code.
  prefs: []
  type: TYPE_NORMAL
- en: We later delved deeper into the system design process and introduced LangChain.
    We reviewed what LangChain comprises and experimented with an example pipeline
    in code.
  prefs: []
  type: TYPE_NORMAL
- en: To complement the system design process, we surveyed leading cloud services
    that allow you to experiment, develop, and deploy LLM-based solutions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll focus on particular practical use cases, accompanied
    with code.
  prefs: []
  type: TYPE_NORMAL
