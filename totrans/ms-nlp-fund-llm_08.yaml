- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: 'Accessing the Power of Large Language Models: Advanced Setup and Integration
    with RAG'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问大型语言模型的力量：高级设置和与RAG的集成
- en: In this dynamic era of **Artificial Intelligence** (**AI**) and **Machine Learning**
    (**ML**), understanding the vast assortment of available resources and learning
    how to utilize them effectively is vital. **Large Language Models** (**LLMs**)
    such as GPT-4 have revolutionized the field of **Natural** **Language Processing**
    (**NLP**) by showcasing unprecedented performance in diverse tasks, from content
    generation to complex problem-solving. Their immense potential extends not only
    to understanding and generating human-like text but also to bridging the gap between
    machines and humans, in terms of communication and task automation. Embracing
    the practical applications of LLMs can empower businesses, researchers, and developers
    to create more intuitive, intelligent, and efficient systems that cater to a wide
    range of requirements. This chapter offers a guide to setting up access to LLMs,
    walking you through using them and building pipelines with them.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个充满活力的**人工智能**（**AI**）和**机器学习**（**ML**）时代，理解可用的各种资源并学习如何有效地利用它们至关重要。**大型语言模型**（**LLMs**）如GPT-4通过在从内容生成到复杂问题解决的各种任务中展现出前所未有的性能，彻底改变了**自然语言处理**（**NLP**）领域。它们的巨大潜力不仅体现在理解和生成类似人类的文本上，还体现在弥合机器与人类之间的差距，无论是在通信还是任务自动化方面。拥抱LLMs的实际应用能够赋予企业、研究人员和开发者创建更直观、智能和高效的系统，以满足广泛的业务需求。本章将为您提供一个指南，介绍如何设置访问LLMs，并指导您如何使用它们以及如何构建与它们相关的管道。
- en: Our journey begins by delving into closed source models that utilize **Application
    Programming Interfaces** (**APIs**), taking OpenAI’s API as a quintessential example.
    We will walk you through a practical scenario, illustrating how you can interact
    with this API using an API key within your Python code, demonstrating the potential
    applications of such models in real-world contexts.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的旅程从深入研究使用**应用程序编程接口**（**APIs**）的封闭源模型开始，以OpenAI的API作为一个典范示例。我们将通过一个实际场景向您展示，如何使用Python代码中的API密钥与该API交互，并展示此类模型在现实世界中的应用潜力。
- en: As we advance, we will shift our focus to the realm of open source tools, giving
    you a rundown of widely employed open source models that can be manipulated via
    Python. We aim to provide a grasp of the power and versatility these models provide,
    emphasizing the community-driven benefits of open source development.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们不断前进，我们将把焦点转向开源工具的领域，向您介绍一些广泛使用的开源模型，这些模型可以通过Python进行操作。我们的目标是让您了解这些模型提供的力量和多功能性，强调开源开发的社区驱动优势。
- en: Subsequently, we will introduce you to retrieval-augmented generation and, specifically,
    LangChain, a robust tool specifically engineered for interaction with LLMs. LangChain
    is essential for the practical application of LLMs because it provides a unified
    and abstracted interface to them, as well as a suite of tools and modules that
    simplify the development and deployment of LLM-powered applications. We’ll guide
    you through the foundational concept of LangChain, highlighting its distinctive
    methodology to circumvent the inherent challenges posed by LLMs.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们将向您介绍检索增强生成，特别是LangChain，这是一个专门为与LLMs交互而设计的强大工具。LangChain对于LLMs的实际应用至关重要，因为它提供了一个统一和抽象的接口，以及一系列工具和模块，这些工具和模块简化了LLM驱动应用程序的开发和部署。我们将引导您了解LangChain的基础概念，突出其独特的解决LLMs固有挑战的方法。
- en: The cornerstone of this methodology is the transformation of data into embeddings.
    We will shed light on the pivotal role that **Language Models** (**LMs**) and
    LLMs play in this transformation, demonstrating how they are engaged in creating
    these embeddings. Following this, we will discuss the process of establishing
    a local vector database, giving you a brief overview of vector databases and their
    crucial role in managing and retrieving these embeddings.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的基石是将数据转换为嵌入。我们将阐明**语言模型**（**LMs**）和LLMs在这次转换中扮演的关键角色，展示它们如何参与创建这些嵌入。随后，我们将讨论建立本地向量数据库的过程，为您简要概述向量数据库及其在管理和检索这些嵌入中的关键作用。
- en: Then, we will address the configuration of an LLM for prompting, which could
    potentially be the same LLM used for the embedding process. We will take you through
    the stepwise setup procedure, detailing the advantages and potential applications
    of this strategy.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将讨论用于提示的LLM的配置，这可能与用于嵌入过程的LLM相同。我们将逐步引导您进行设置过程，详细说明这种策略的优势和潜在应用。
- en: In the penultimate segment, we will touch upon the topic of deploying LLMs to
    the cloud. The scalability and cost-effectiveness of cloud services have led to
    an increased adoption of hosting AI models. We will provide an overview of the
    leading cloud service providers, including **Microsoft Azure**, **Amazon Web Services**
    (**AWS**), and **Google Cloud Platform** (**GCP**), giving you insights into their
    service offerings and how they can be harnessed for LLM deployment.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一部分，我们将涉及将LLM部署到云的话题。云服务的可扩展性和成本效益导致了托管AI模型采用率的增加。我们将概述领先的云服务提供商，包括**Microsoft
    Azure**、**Amazon Web Services**（**AWS**）和**Google Cloud Platform**（**GCP**），并为您提供关于他们的服务提供方式和如何利用它们进行LLM部署的见解。
- en: As we embark on this exploration of LLMs, it’s crucial to acknowledge the continuously
    evolving data landscape that these models operate within. The dynamic nature of
    data – its growth in volume, diversity, and complexity – necessitates a forward-looking
    approach to how we develop, deploy, and maintain LLMs. In the subsequent chapters,
    particularly [*Chapter 10*](B18949_10.xhtml#_idTextAnchor525), we will delve deeper
    into the strategic implications of these evolving data landscapes, preparing you
    to navigate the challenges and opportunities they present. This foundational understanding
    will not only enhance your immediate work with LLMs but also ensure your projects
    remain resilient and relevant in the face of rapid technological and data-driven
    changes.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始探索LLM的过程中，承认这些模型运行在其内的数据景观的持续演变至关重要。数据的动态特性——其体积、多样性和复杂性的增长——需要我们采取前瞻性的方法来开发、部署和维护LLM。在随后的章节中，特别是[第10章](B18949_10.xhtml#_idTextAnchor525)，我们将深入探讨这些演变数据景观的战略影响，为您准备应对它们带来的挑战和机遇。这种基础理解不仅将增强您与LLM的即时工作，还将确保您的项目在面对快速的技术和数据驱动变化时保持韧性和相关性。
- en: 'Let’s go through the main topics covered in the chapter:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾本章涵盖的主要主题：
- en: Setting up an LLM application – API-based closed source models
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置LLM应用 – 基于API的闭源模型
- en: Prompt engineering and priming GPT
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程和初始化GPT
- en: Setting up an LLM application – local open source models
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置LLM应用 – 本地开源模型
- en: Employing LLMs from Hugging Face via Python
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过Python使用Hugging Face的LLM
- en: Exploring advanced system design – RAG and LangChain
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索高级系统设计 – RAG和LangChain
- en: Reviewing a simple LangChain setup in a Jupyter notebook
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Jupyter笔记本中回顾简单的LangChain设置
- en: LLMs in the cloud
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云端LLM
- en: Technical requirements
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, the following will be necessary:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，以下将是必需的：
- en: '**Programming knowledge**: Familiarity with Python programming is a must, since
    the open source models, OpenAI’s API, and LangChain are all illustrated using
    Python code.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编程知识**：熟悉Python编程是必需的，因为开源模型、OpenAI的API和LangChain都是使用Python代码进行说明的。'
- en: '**Access to OpenAI’s API**: An API key from OpenAI will be required to explore
    closed source models. This can be obtained by creating an account with OpenAI
    and agreeing to their terms of service.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问OpenAI的API**：探索闭源模型需要OpenAI的API密钥。这可以通过在OpenAI创建账户并同意他们的服务条款来获得。'
- en: '**Open source models**: Access to the specific open source models mentioned
    in this chapter will be necessary. These can be accessed and downloaded from their
    respective repositories or via package managers such as **pip** or **conda**.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开源模型**：需要访问本章中提到的特定开源模型。这些模型可以通过各自的存储库或通过包管理器如**pip**或**conda**访问和下载。'
- en: '**A local development environment**: A local development environment setup
    with Python installed is required. An **Integrated Development Environment** (**IDE**)
    such as PyCharm, Jupyter Notebook, or a simple text editor can be used. Note that
    we recommend a free Google Colab notebook, as it encapsulates all these requirements
    in a seamless web interface.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地开发环境**：需要一个安装了Python的本地开发环境。可以使用**集成开发环境**（**IDE**）如PyCharm、Jupyter Notebook或简单的文本编辑器。请注意，我们推荐使用免费的Google
    Colab笔记本，因为它将这些要求封装在一个无缝的网页界面中。'
- en: '**The ability to install libraries**: You must have permission to install the
    required Python libraries, such as NumPy, SciPy, TensorFlow, and PyTorch. Note
    that the code we provide includes the required installations, and you won’t have
    to install them beforehand. We simply stress that you should have permission to
    do so, which we expect you would. Specifically, using a free Google Colab notebook
    would suffice.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安装库的能力**：你必须有权安装所需的Python库，例如NumPy、SciPy、TensorFlow和PyTorch。请注意，我们提供的代码中包含了所需的安装，你不需要事先安装它们。我们只是强调你应该有权这样做，我们预期你会这样做。具体来说，使用免费的Google
    Colab笔记本就足够了。'
- en: '**Hardware requirements**: Depending on the complexity and size of the models
    you’re working with, a computer with sufficient processing power (potentially
    including a good GPU for ML tasks) and ample memory will be required. This is
    only relevant when choosing to not use the free Google Colab.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬件要求**：根据你正在处理的模型复杂性和大小，你需要一台具有足够处理能力（可能包括用于机器学习任务的优秀GPU）和充足内存的计算机。这仅在你选择不使用免费的Google
    Colab时相关。'
- en: Now that we’ve grasped the transformative potential of LLMs and the variety
    of tools available, let’s delve deeper and explore how to effectively set up LLM
    applications using APIs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了LLM的变革潜力以及可用的各种工具，让我们更深入地探讨如何有效地使用API设置LLM应用程序。
- en: Setting up an LLM application – API-based closed source models
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置LLM应用程序 - 基于API的闭源模型
- en: When looking to employ models in general and LLMs in particular, there are various
    design choices and trade-offs. One key choice is whether to host a model locally
    in your local environment or to employ it remotely, accessing it via a communication
    channel. Local development environments would be wherever your code runs, whether
    that’s your personal computer, your on-premises server, your cloud environment,
    and so on. The choice you make will impact many aspects, such as cost, information
    security, maintenance needs, network overload, and inference speed.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑使用模型（特别是LLM）时，有各种设计选择和权衡。一个关键的选择是是否在本地环境中托管模型，还是通过通信渠道远程使用它。本地开发环境将是你代码运行的地方，无论是你的个人电脑、本地服务器、云环境，等等。你的选择将影响许多方面，如成本、信息安全、维护需求、网络过载和推理速度。
- en: In this section, we will introduce a quick and simple approach to employing
    an LLM remotely via an API. This approach is quick and simple as it rids us of
    the need to allocate unusual computation resources to host the LLM locally. An
    LLM typically requires amounts of memory and computation resources that aren’t
    common in personal environments.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一种快速简单的方法，通过API远程使用LLM。这种方法快速简单，因为它免除了我们为本地托管LLM而分配不寻常的计算资源的需求。LLM通常需要大量的内存和计算资源，这在个人环境中并不常见。
- en: Choosing a remote LLM provider
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择远程LLM提供商
- en: Before diving into implementation, we need to select a suitable LLM provider
    that aligns with our project requirements. OpenAI, for example, offers several
    versions of the GPT-3.5 and GPT-4 models with comprehensive API documentation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入实施之前，我们需要选择一个与我们的项目需求相匹配的合适的LLM（大型语言模型）提供商。例如，OpenAI提供了GPT-3.5和GPT-4的多个版本，并配有全面的API文档。
- en: OpenAI’s remote GPT access in Python via an API
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过API在Python中访问OpenAI的远程GPT
- en: To gain access to OpenAI’s LLM API, we need to create an account on their website.
    This process involves registration, account verification, and obtaining API credentials.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取OpenAI的LLM API访问权限，我们需要在他们的网站上创建一个账户。这个过程包括注册、账户验证和获取API凭证。
- en: OpenAI’s website provides guidance for these common actions, and you will be
    able to get set up quickly.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的网站提供了这些常见操作的指导，你将能够快速设置好。
- en: Once registered, we should familiarize ourselves with OpenAI’s API documentation.
    This documentation will guide us through the various endpoints, methods, and parameters
    available to interact with the LLMs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注册后，我们应该熟悉OpenAI的API文档。这份文档将指导我们了解与LLM交互的各种端点、方法和参数。
- en: The first hands-on experience we will take on will be employing OpenAI’s LLMs
    via Python. We have put together a notebook that presents the simple steps of
    employing OpenAI’s GPT model via an API. Refer to the `Ch8_Setting_Up_Close_Source_and_Open_Source_LLMs.ipynb`
    notebook. This notebook, called *Setting Up Close Source and Open Source LLMs*,
    will be utilized in the current section about OpenAI’s API, and also in the subsequent
    section about setting up local LLMs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要进行的第一个动手实践将是通过 Python 使用 OpenAI 的 LLMs。我们准备了一个笔记本，展示了通过 API 使用 OpenAI 的
    GPT 模型的简单步骤。请参考 `Ch8_Setting_Up_Close_Source_and_Open_Source_LLMs.ipynb` 笔记本。这个名为
    *设置近源和开源 LLMs* 的笔记本将在当前关于 OpenAI API 的章节中使用，也将在后续关于设置本地 LLMs 的章节中使用。
- en: 'Let’s walk through the code:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步分析代码：
- en: 'We start by installing the required Python libraries. In particular, to communicate
    with the LLM API, we need to install the necessary Python library:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先安装所需的 Python 库。特别是，为了与 LLM API 进行通信，我们需要安装必要的 Python 库：
- en: '[PRE0]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Define OpenAI’s API key**: Before making requests to the LLM API, we must
    embed our personal API key in the library’s configuration. The API key is made
    available for you on OpenAI’s website when you register. This can be done by either
    explicitly pasting the key’s string to be hardcoded in our code or reading it
    from a file that holds that string. Note that the former is the simplest way to
    showcase the API, as it doesn’t require an additional file to be set up, but it
    may not be the right choice when working in a shared development environment:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义 OpenAI 的 API 密钥**：在向 LLM API 发送请求之前，我们必须将我们的个人 API 密钥嵌入到库的配置中。当你注册 OpenAI
    时，API 密钥会在 OpenAI 的网站上提供给你。这可以通过将密钥的字符串明确粘贴到我们的代码中以硬编码，或者从包含该字符串的文件中读取它来完成。请注意，前者是展示
    API 的最简单方式，因为它不需要设置额外的文件，但在共享开发环境中工作时可能不是最佳选择：'
- en: '[PRE1]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Settings – set the model’s configurations**. Here, we set the various parameters
    that control the model’s behavior.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**设置 – 设置模型的配置**。在这里，我们设置控制模型行为的各种参数。'
- en: As the foundation is set for connecting to LLMs through APIs, it’s valuable
    to turn our attention to an equally important aspect – prompt engineering and
    priming, the art of effectively communicating with these models.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在为通过 API 连接到 LLMs 奠定基础后，将我们的注意力转向同样重要的一个方面——提示工程和预处理，这是与这些模型有效沟通的艺术。
- en: Prompt engineering and priming GPT
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示工程和 GPT 预处理
- en: Let us pause and provide some context before returning to discuss the next part
    of the code.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在返回讨论代码的下一部分之前，让我们暂停并提供一些背景信息。
- en: Prompt engineering is a technique used in NLP to design effective prompts or
    instructions when interacting with LLMs. It involves carefully crafting the input
    given to a model to elicit the desired output. By providing specific cues, context,
    or constraints in the prompts, prompt engineering aims to guide the model’s behavior
    and encourage the generation of more accurate, relevant, or targeted responses.
    The process often involves iterative refinement, experimentation, and understanding
    the model’s strengths and limitations to optimize the prompt for improved performance
    in various tasks, such as question-answering summarization or conversation generation.
    Effective prompt engineering plays a vital role in harnessing the capabilities
    of LMs and shaping their output to meet specific user requirements.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程是 NLP 中用于设计有效提示或指令的技术，当与 LLMs 交互时使用。它涉及精心设计提供给模型的输入，以产生期望的输出。通过在提示中提供特定的提示、上下文或约束，提示工程旨在引导模型的行为，并鼓励生成更准确、相关或针对性的响应。这个过程通常涉及迭代优化、实验，并理解模型的优势和局限性，以优化提示，提高在各种任务（如问答、摘要或对话生成）中的性能。有效的提示工程在利用
    LMs 的能力以及塑造其输出以满足特定用户需求方面发挥着至关重要的作用。
- en: Let’s review one of the most impactful tools in prompt engineering, **priming**.
    Priming GPT via an API involves providing initial context to the model before
    generating a response. The priming step helps set the direction and style of the
    generated content. By giving the model relevant information or examples related
    to the desired output, we can guide its understanding and encourage more focused
    and coherent responses. Priming can be done by including specific instructions,
    context, or even partial sentences that align with the desired outcome. Effective
    priming enhances the model’s ability to generate responses that better match the
    user’s intent or specific requirements.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下提示工程中最具影响力的工具之一，**启动**。通过API启动GPT涉及在生成响应之前向模型提供初始上下文。启动步骤有助于设定生成内容的方向和风格。通过提供与所需输出相关的相关信息或示例，我们可以引导其理解并鼓励更专注和连贯的响应。启动可以通过包含特定指令、上下文，甚至与预期结果一致的片段句子来实现。有效的启动可以增强模型生成与用户意图或具体要求更匹配的响应的能力。
- en: 'Priming is done by introducing GPT with several types of messages:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通过引入GPT和几种类型的信息来进行启动：
- en: The main message is the **system prompt**. This message instructs the model
    about the *role* it may play, the way it should answer the questions, the constraints
    it may have, and so on.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主要信息是**系统提示**。这条信息指导模型关于可能扮演的*角色*，它应该如何回答问题，可能面临的约束等等。
- en: The second type of message is a **user prompt**. A user prompt is sent to the
    model in the priming phase, and it represents an example user prompt, much like
    the prompt you may enter in ChatGPT’s web interface. However, when priming, this
    message could be presented to the model as an example of how it should address
    such a prompt. The developer will introduce a user prompt of some sort and will
    then show the model how it is expected to answer that prompt.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二种信息类型是**用户提示**。用户提示在启动阶段发送给模型，它代表一个示例用户提示，就像您可能在ChatGPT的网页界面中输入的提示一样。然而，在启动时，这条消息可以展示给模型作为如何处理此类提示的示例。开发者将引入某种用户提示，然后展示模型如何预期回答该提示。
- en: 'For example, observe this priming code:'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，观察以下启动代码：
- en: '[PRE2]\nimport pandas as pd\n[PRE3]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE2]\nimport pandas as pd\n[PRE3]'
- en: 'This is the output:'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '[PRE4]python'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE4]python'
- en: import numpy as np
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: import numpy as np
- en: '[PRE5]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can see that we prime the model to provide concise answers in a Markdown
    format. The example that is used to teach the model is in the form of a question
    and an answer. The question is via a user prompt, and the way we tell the model
    what the potential answer is is provided via an assistant prompt. We then provide
    the model with another user prompt; this one is the actual prompt we’d like the
    model to address for us, and as shown in the output, it gets it right.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到我们启动模型以提供Markdown格式的简洁答案。用于教导模型的示例是问题和答案的形式。问题是通过用户提示，而我们告诉模型潜在答案的方式是通过助手提示。然后我们向模型提供另一个用户提示；这是我们要模型为我们处理的实际提示，如输出所示，它回答正确。
- en: By looking at OpenAI’s documentation about prompt engineering, you’ll find that
    there are additional types of prompts to prime the GPT models with.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看OpenAI关于提示工程的文档，你会发现还有其他类型的提示可以用来启动GPT模型。
- en: Going back to our notebook and code, in this section, we leverage *GPT-3.5 Turbo*.
    We prime it in the simplest manner, only giving it a system prompt to provide
    directions in order to showcase how additional functionality could stem from the
    system prompt. We tell the model to finish a response by alerting us about typos
    in the prompt and correcting them.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的笔记本和代码，在这个部分，我们利用*GPT-3.5 Turbo*。我们以最简单的方式启动它，只给它一个系统提示来提供方向，以展示系统提示如何产生额外的功能。我们告诉模型通过提醒我们提示中的错误并纠正它们来完成响应。
- en: We then provide our desired prompt in the user prompt section, and we insert
    a few typos into it. Run that code and give it a shot.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们在用户提示部分提供我们想要的提示，并在其中插入一些错误。运行那段代码并尝试一下。
- en: Experimenting with OpenAI’s GPT model
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 尝试OpenAI的GPT模型
- en: At this stage, we send our prompts to the model.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们向模型发送我们的提示。
- en: The following simple example code is run once in the *Setting Up Close Source
    and Open Source LLMs* notebook. You can wrap it in a function and call it repeatedly
    in your own code.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下简单的示例代码在*设置近源和开源LLMs*笔记本中运行一次。您可以将它封装在一个函数中，并在自己的代码中重复调用它。
- en: 'Some aspects worth noticing are as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的一些方面如下：
- en: '**Parsing and processing the returned output from the model**: We structure
    the output response in a coherent manner for the user to read:'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解析和处理模型返回的输出**：我们以连贯的方式结构化输出响应，以便用户阅读：'
- en: '[PRE6]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Error handling**: We designed the code to allow for several failed attempts
    before accepting a failure to use the API:'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**错误处理**：我们设计代码允许在接受API使用失败之前进行多次尝试：'
- en: '[PRE7]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Rate limits and cost mitigation**: We don’t implement such restrictions here,
    but it would be ideal to have both of these in an experimental setting and perhaps
    in production.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速率限制和成本缓解**：我们在这里没有实施此类限制，但在实验设置中，也许在生产环境中，同时拥有这两者将是理想的。'
- en: 'The result of the preceding code is demonstrated as follows:'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下代码的结果展示如下：
- en: '[PRE8]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The model provided us with a legitimate, concise response. It then notified
    us about the typos, which are perfectly in line with the system prompt we provided
    it with.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 模型为我们提供了一个合法、简洁的响应。然后它通知我们有关错别字，这与我们提供给它的系统提示完全一致。
- en: That was an example showcasing the employment of a remote, off-premises, closed
    source LLM. While leveraging the power of paid APIs such as OpenAI offers convenience
    and cutting-edge performance, there’s also immense potential in tapping into free
    open source LLMs. Let’s explore these cost-effective alternatives next.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 那是一个展示远程、场外、闭源LLM应用的例子。虽然利用像OpenAI提供的付费API的强大功能提供了便利和尖端性能，但挖掘免费开源LLM的潜力同样巨大。让我们接下来探索这些成本效益高的替代方案。
- en: Setting up an LLM application – local open source models
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置LLM应用程序 - 本地开源模型
- en: Now, we shall touch on the complementary approach to a closed source implementation,
    that is, an open source, local implementation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将讨论闭源实现的补充方法，即开源、本地实现。
- en: We will see how you can achieve a similar functional outcome to the one we reviewed
    in the previous section, without having to register for an account, pay, or share
    prompts that contain possibly sensitive information with a third-party vendor,
    such as OpenAI.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到您如何在不注册账户、付费或与第三方供应商（如OpenAI）共享可能包含敏感信息的提示的情况下，实现与上一节中我们审查的类似的功能结果。
- en: About the different aspects that distinguish between open source and closed
    source
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于区分开源和闭源的不同方面
- en: When selecting between open source LLMs, such as LLaMA and GPT-J, and closed
    source, API-based models such as OpenAI’s GPT, several critical factors must be
    considered.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择开源LLM（如LLaMA和GPT-J）和闭源、基于API的模型（如OpenAI的GPT）之间时，必须考虑几个关键因素。
- en: Firstly, cost is a major factor. Open source LLMs often have no licensing fees,
    but they require significant computational resources for training and inference,
    which can be expensive. Closed source models, while potentially carrying a subscription
    or pay-per-use fee, eliminate the need for substantial hardware investments.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，成本是一个主要因素。开源LLM通常没有许可费用，但它们需要大量的计算资源进行训练和推理，这可能很昂贵。闭源模型虽然可能收取订阅费或按使用付费的费用，但消除了对大量硬件投资的必要性。
- en: Processing speed and maintenance are closely linked to computational resources.
    Open source LLMs, if deployed on powerful enough systems, can offer high processing
    speeds but require ongoing maintenance and updates by the implementing team. In
    contrast, closed source models managed by the provider ensure continual maintenance
    and model updates, often with better efficiency and reduced downtime, but processing
    speed can be dependent on the provider’s infrastructure and network latency.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 处理速度和维护与计算资源密切相关。如果部署在足够强大的系统上，开源LLM可以提供高速处理，但需要实施团队持续维护和更新。相比之下，由提供商管理的闭源模型确保持续的维护和模型更新，通常效率更高，停机时间更短，但处理速度可能取决于提供商的基础设施和网络延迟。
- en: Regarding model updates, open source models offer more control but require a
    proactive approach to incorporate the latest research and improvements. Closed
    source models, however, are regularly updated by the provider, ensuring access
    to the latest advancements without additional effort from the user.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 关于模型更新，开源模型提供了更多控制权，但需要主动方法来整合最新的研究和改进。然而，闭源模型由提供商定期更新，确保用户无需额外努力即可访问最新的进展。
- en: Security and privacy are paramount in both scenarios. Open source models can
    be more secure, as they can be run on private servers, ensuring data privacy.
    However, they demand robust in-house security protocols. Closed source models,
    managed by external providers, often come with built-in security measures but
    pose potential privacy risks, due to data handling by third parties.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，安全和隐私都是至关重要的。开源模型可能更安全，因为它们可以在私有服务器上运行，确保数据隐私。然而，它们需要强大的内部安全协议。由外部提供商管理的闭源模型通常带有内置的安全措施，但由于第三方处理数据，可能存在潜在的隐私风险。
- en: Overall, the choice between open source and closed source LLMs hinges on the
    trade-off between cost, control, and convenience, with each option presenting
    its own set of advantages and challenges.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，开源和闭源语言模型（LLM）之间的选择取决于成本、控制和便利性之间的权衡，每种选择都有一套自己的优势和挑战。
- en: 'With that in mind, let’s revisit Hugging Face, the company that put together
    the largest and most approachable hub for free LMs. In the following example,
    we will leverage Hugging Face’s easy and free library: transformers.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，让我们重新审视 Hugging Face，这家公司汇集了最大的、最易于访问的免费语言模型（LM）中心。在下面的示例中，我们将利用 Hugging
    Face 简便且免费的库：transformers。
- en: Hugging Face’s hub of models
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hugging Face 的模型中心
- en: When looking to choose an LLM for our task, we recommend referring to Hugging
    Face’s Models online page. They offer an enormous amount of Python-based, open
    source LLMs. Every model has a page dedicated to it, where you can find information
    about it, including the syntax needed to employ that model via Python code in
    your personal environment.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们寻找为我们任务选择一个 LLM 时，我们建议参考 Hugging Face 的在线模型页面。他们提供了大量的基于 Python 的开源 LLM。每个模型都有一个专门的页面，您可以在这里找到有关它的信息，包括在您的个人环境中通过
    Python 代码使用该模型所需的语法。
- en: It should be noted that in order to implement a model locally, you must have
    an internet connection from the machine that runs the Python code. However, as
    this requirement may become a bottleneck in some cases – for instance, when the
    development environment is restricted by a company’s intranet or has limited internet
    access due to firewall restrictions – there are alternative approaches. Our recommended
    approach is to clone the model repository from Hugging Face’s domain. That is
    a less trivial and less-used approach. Hugging Face provides the necessary cloning
    commands on each model’s web page.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，为了在本地实现模型，您必须从运行 Python 代码的机器上拥有互联网连接。然而，由于这个要求在某些情况下可能成为瓶颈——例如，当开发环境受到公司内部网络的限制或由于防火墙限制而有限的互联网访问时——存在其他方法。我们推荐的方法是从
    Hugging Face 的域名克隆模型仓库。这是一个不那么简单且不太常用的方法。Hugging Face 在每个模型的网页上提供了必要的克隆命令。
- en: Choosing a model
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择模型
- en: When looking to choose a model, there may be several factors that come into
    play. Depending on your intentions, you may care about configuration speed, processing
    speed, storage space, computation resources, legal usage restrictions, and so
    on. Another factor worth noting is the popularity of a model. It attests to how
    frequently that model is chosen by other developers in the community. For instance,
    if you look for LMs that are labeled for zero-shot classification, you will find
    a very large collection of available models. But, if you then narrow the search
    some more so to only be left with models that were trained on data from news articles,
    you would be left with a much smaller set of available models. In which case,
    you may want to refer to the popularity of each model and start your exploration
    with the model that was used the most.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当寻找模型时，可能会有几个因素需要考虑。根据您的意图，您可能关心配置速度、处理速度、存储空间、计算资源、法律使用限制等。另一个值得注意的因素是模型的流行度。它证明了该模型在社区中其他开发者中被选择的频率。例如，如果您寻找标记为零样本分类的
    LMs，您将找到一个非常大的可用模型集合。但是，如果您进一步缩小搜索范围，以便只剩下在新闻文章数据上训练的模型，您将只剩下更小的可用模型集合。在这种情况下，您可能需要参考每个模型的流行度，并从使用最频繁的模型开始您的探索。
- en: Other factors that may interest you could be publications about the model, the
    model’s developers, the company or university that released the model, the dataset
    that the model was trained on, the architecture the model was designed by, the
    evaluation metrics, and other potential factors that may be available on each
    model’s web page on Hugging Face’s website.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 其他可能引起你兴趣的因素可能包括关于该模型的出版物、模型的开发者、发布该模型的公司或大学、模型训练所使用的数据集、模型设计的架构、评估指标，以及在每个模型在Hugging
    Face网站上网页上可能存在的其他潜在因素。
- en: Employing LLMs from Hugging Face via Python
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过Python使用Hugging Face的LLMs
- en: 'Now, we will review a code notebook that exemplifies implementing an open source
    LLM locally using Hugging Face’s free resources. We will continue with the same
    notebook from the previous section, *Setting Up Close Source and Open* *Source*
    LLMs:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将回顾一个代码笔记本，它展示了如何使用Hugging Face的免费资源在本地实现开源LLM。我们将继续使用上一节中相同的笔记本，*设置近源和开源LLMs*：
- en: '`pip` on the Terminal, we will run the following:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在终端上使用`pip`，我们将运行以下命令：
- en: '[PRE9]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Alternatively, if running directly from a Jupyter notebook, add `!` to the beginning
    of the command.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果直接从Jupyter笔记本运行，请在命令的开头添加`!`。
- en: '**Experiment with Microsoft’s DialoGPT-medium**: This LLM is dedicated to conversational
    applications. It was generated by Microsoft and achieved high scores when compared
    to other LLMs on common benchmarks. For that reason, it is also quite popular
    on Hugging Face’s platform, in the sense that it is downloaded frequently by ML
    developers.'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**实验Microsoft的DialoGPT-medium**：这个LLM专注于对话应用。它由Microsoft生成，在与其他LLMs的常见基准比较中取得了高分。因此，它在Hugging
    Face平台上也非常受欢迎，从ML开发者的角度来看，它经常被下载。'
- en: 'In the **Settings** code section in the notebook, we will define the parameters
    for this code and import the model and its tokenizer:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本中的**设置**代码部分，我们将定义此代码的参数并导入模型及其分词器：
- en: '[PRE10]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that this code requires access to the internet. Even though the model is
    deployed locally, an internet connection is required to import it. Again, if you
    wish, you can clone the model’s repo from Hugging Face and then no longer be required
    to have access to the internet.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，此代码需要访问互联网。即使模型是本地部署的，也需要互联网连接来导入它。再次提醒，如果你愿意，你可以从Hugging Face克隆模型的repo，然后不再需要访问互联网。
- en: '**Define the prompt**: As can be seen in the following code block, we picked
    a straightforward prompt here, much like a user prompt for the GPT-3.5-Turbo model.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义提示**：如以下代码块所示，我们在这里选择了简单的提示，类似于GPT-3.5-Turbo模型的用户提示。'
- en: '**Experiment with the model**: Here, we have the syntax that suits this code.
    If you want to create a rolling conversation with this model, you wrap this code
    in a function and iterate over it, collecting prompts from the user in real time.'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**实验模型**：在这里，我们有适合此代码的语法。如果你想与该模型进行滚动对话，你可以将此代码包裹在一个函数中，并实时收集用户的提示。'
- en: '**The result**: The resulting prompt is, **If dinosaurs were alive today, would
    they possess a threat** **to people?:**'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**结果**：生成的提示是，**如果恐龙今天还活着，它们会对人类构成威胁吗**？'
- en: '[PRE11]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This section established the tremendous value proposition that LLMs can bring.
    We now have the necessary background to learn and explore a new frontier in efficient
    LLM application development – constructing pipelines using tools such as LangChain.
    Let’s dive into this advanced approach.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 本节确立了LLMs可以带来的巨大价值主张。我们现在有了学习和探索高效LLM应用开发新前沿的必要背景——使用LangChain等工具构建管道。让我们深入了解这种方法。
- en: Exploring advanced system design – RAG and LangChain
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索高级系统设计——RAG和LangChain
- en: '**Retrieval-Augmented Generation** (**RAG**) is a development framework designed
    for seamless interaction with LLMs. LLMs, by virtue of their generalist nature,
    are capable of performing a vast array of tasks competently. However, their generality
    often precludes them from delivering detailed, nuanced responses to queries that
    necessitate specialized knowledge or in-depth expertise in a domain. For instance,
    if you aspire to use an LLM to address queries concerning a specific discipline,
    such as law or medicine, it might satisfactorily answer general queries but fail
    to respond accurately to those needing detailed insights or up-to-date knowledge.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**检索增强生成**（**RAG**）是一个专为与LLMs无缝交互而设计的开发框架。由于LLMs具有通用性，它们能够胜任大量任务。然而，它们的通用性往往阻止它们对需要特定知识或深入领域专业知识的问题提供详细、细微的响应。例如，如果你希望使用LLM来回答有关特定学科（如法律或医学）的查询，它可能能够满意地回答一般性查询，但可能无法准确回答需要详细见解或最新知识的问题。'
- en: RAG designs offer a comprehensive solution to the limitations typically encountered
    in LLM processing. In a RAG framework, the text corpus undergoes initial preprocessing,
    where it’s segmented into summaries or distinct chunks and then embedded within
    a vector space. When a query is made, the model identifies the most relevant segments
    of this data and utilizes them to form a response. This process involves a combination
    of offline data preprocessing, online information retrieval, and the application
    of the LLM for response generation. It’s a versatile approach that can be adapted
    to a variety of tasks, including code generation and semantic search. RAG models
    function as an abstraction layer that orchestrates these processes. The efficacy
    of this method is continually increasing, with its applications expanding as LLMs
    evolve and require more contextually rich data during prompt processing. In [*Chapter
    10*](B18949_10.xhtml#_idTextAnchor525), we will present a deeper discussion of
    RAG models and their role in the future of LLM solutions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: RAG设计为LLM处理中通常遇到的限制提供了一个全面的解决方案。在RAG框架中，文本语料库经过初步预处理，被分割成摘要或独立的块，然后嵌入到向量空间中。当提出查询时，模型识别出这些数据中最相关的部分，并利用它们来形成响应。这个过程涉及离线数据预处理、在线信息检索以及应用LLM进行响应生成的应用。这是一个灵活的方法，可以适应各种任务，包括代码生成和语义搜索。RAG模型作为一个抽象层，协调这些过程。这种方法的有效性正在不断提高，其应用范围随着LLMs的发展而扩大，在提示处理过程中需要更多上下文丰富的数据。在[*第10章*](B18949_10.xhtml#_idTextAnchor525)中，我们将更深入地讨论RAG模型及其在LLM解决方案未来中的作用。
- en: Now that we’ve introduced the premise and capabilities of RAG models, let’s
    focus on one particular example, called LangChain. We will review the nuts and
    bolts of its design principles and how it interfaces with data sources.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了RAG模型的前提和功能，让我们专注于一个特定的例子，称为LangChain。我们将回顾其设计原则的细节以及它是如何与数据源交互的。
- en: LangChain’s design concepts
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LangChain的设计理念
- en: In this section, we will dissect the core methodologies and architectural decisions
    that make LangChain stand out. This will give us insights into its structural
    framework, the efficiency of data handling, and its innovative approach to integrating
    LLMs with various data sources.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将剖析使LangChain脱颖而出的核心方法和架构决策。这将使我们深入了解其结构框架、数据处理效率以及其将LLMs与各种数据源集成的创新方法。
- en: Data sources
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据源
- en: One of the most significant virtues of LangChain is the ability to connect an
    arbitrary LLM to a defined data source. By arbitrary, we mean that it could be
    any *off-the-shelf* LLM that was designed and trained with no specific regard
    to the data we are looking to connect it to. Employing LangChain allows us to
    customize it to our domain. The data source is to be used for reference when structuring
    the answer to the user prompt. That data may be proprietary data owned by a company
    or local personal information on your personal machine.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain最显著的优点之一是能够将任意LLM连接到定义好的数据源。这里的“任意”意味着它可以是任何*现成的*LLM，这些LLM的设计和训练没有针对我们想要连接的数据进行特定的考虑。使用LangChain允许我们根据我们的领域进行定制。数据源在构建用户提示的答案时用作参考。这些数据可能是公司拥有的专有数据，也可能是个人机器上的本地个人信息。
- en: However, when it comes to leveraging a given database, LangChain does more than
    point the LLM to the data; it employs a particular processing scheme and makes
    it quick and efficient. It creates a vector database.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当涉及到利用特定的数据库时，LangChain所做的不仅仅是将LLM指向数据；它采用特定的处理方案，使其快速高效。它创建一个向量数据库。
- en: Given raw text data, be it free text in a `.txt` file, formatted files, or other
    various data structures of text, a vector database is created by chunking the
    text into appropriate lengths and creating numerical text embeddings, using a
    designated model. Note that if the designated embedding model is chosen to be
    an LLM, it doesn’t have to be the same LLM that is used for prompting. For instance,
    the embedding model could be picked to be a free, sub-optimal, open source LLM,
    and the prompting model could be a paid LLM with optimal performance. Those embeddings
    are then stored in a vector database. You can clearly see that this approach is
    extremely storage-efficient, as we transform text, and perhaps encoded text, into
    a finite set of numerical values, which by its nature is dense.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于原始文本数据，无论是`.txt`文件中的自由文本、格式化文件还是其他各种文本数据结构，通过将文本分割成适当长度并使用指定的模型创建数值文本嵌入，创建一个向量数据库。请注意，如果指定的嵌入模型选择为LLM，它不需要与用于提示的LLM相同。例如，嵌入模型可以选择为免费、次优的开源LLM，而提示模型可以是具有最佳性能的付费LLM。这些嵌入随后存储在向量数据库中。您可以看到，这种方法非常节省存储空间，因为我们将文本和可能的编码文本转换成有限的一组数值，其本质上是密集的。
- en: When a user enters a prompt, a search mechanism identifies the relevant data
    chunks in the embedded data source. The prompt gets embedded with the same designated
    embedding model. Then, the search mechanism applies a similarity metric, such
    as cosine similarity, for example, and finds the most similar text chunks in the
    defined data source. Then, the original text of these chunks is retrieved. The
    original prompt is then sent again, this time to the prompting LLM. The difference
    is that, this time, the prompt consists of more than just the original user’s
    prompt; it also consists of the retrieved text as a reference. This enables the
    LLM to get a question and a rich text supplement for reference. The LLM then can
    refer to the added information as a reference.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户输入提示时，搜索机制会在嵌入的数据源中识别相关的数据块。提示使用相同的指定嵌入模型进行嵌入。然后，搜索机制应用相似度度量，例如余弦相似度，并找到定义的数据源中最相似的文字块。然后，检索这些块的原文本。然后将原始提示再次发送，这次发送到提示LLM。区别在于，这次提示不仅包括原始用户的提示，还包括作为参考检索到的文本。这使得LLM能够获得问题和丰富的文本补充作为参考。然后，LLM可以参考添加的信息作为参考。
- en: If it weren’t for this design, when the user wanted to find an answer to their
    question, they would need to read through the vast material and find the relevant
    section. For instance, the material may be a company’s entire product methodology,
    consisting of many PDF documents. This process leverages an automated smart search
    mechanism that narrows the relevant material down to an amount of text that can
    fit into a prompt. Then, the LLM frames the answer to the question and presents
    it to the user immediately. If you wish, the pipeline can be designed to quote
    the original text that it used to frame the answer, thus allowing for transparency
    and verification.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有这个设计，当用户想要找到他们问题的答案时，他们需要阅读大量的材料并找到相关的部分。例如，这些材料可能是一家公司整个产品方法，包括许多PDF文档。这个过程利用了自动智能搜索机制，将相关材料缩小到可以放入提示中的文本量。然后，LLM构建问题的答案并立即呈现给用户。如果您愿意，可以将管道设计为引用构建答案时使用的原始文本，从而实现透明度和验证。
- en: 'This paradigm is portrayed in *Figure 8**.1*:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这个范式在*图8.1*中展示：
- en: .
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: '![Figure 8.1 – The paradigm of a typical LangChain pipeline](img/B18949_08_1.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1 – 典型LangChain管道的范式](img/B18949_08_1.jpg)'
- en: Figure 8.1 – The paradigm of a typical LangChain pipeline
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 典型LangChain管道的范式
- en: In order to explain the prompt engineering behind the LangChain pipeline, let’s
    review a financial information use case. Your data source is a cohort of **Securities**
    **&** **Exchange** **Commission** (**SEC**) filings of public companies from the
    US. You are looking to identify companies that gave dividends to their stock holders,
    and in what year.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释LangChain管道背后的提示工程，让我们回顾一个金融信息用例。您的数据源是美国上市公司的**证券** **&** **交易所** **委员会**（**SEC**）的文件集合。您正在寻找向股东发放股息的公司及其年份。
- en: 'Your prompt would be as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你的提示如下：
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The pipeline then embeds this question and looks for text chunks with similar
    context (e.g., that discuss paid dividends). It identifies many such chunks, such
    as the following:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线随后将这个问题嵌入其中，寻找具有相似上下文的文本块（例如，讨论派息的）。它识别了许多这样的块，如下所示：
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The LangChain pipeline then forms a new prompt that includes the text of the
    identified chunks. In this example, we assume the prompted LLM is OpenAI’s GPT.
    LangChain embeds the information in the system prompt sent to OpenAI’s GPT model:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain流水线随后形成一个新的提示，其中包含已识别块的文字。在这个例子中，我们假设被提示的LLM是OpenAI的GPT。LangChain将信息嵌入到发送给OpenAI
    GPT模型的系统提示中：
- en: '[PRE14]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As we can see, the system prompt is used to instruct the model how to act and
    then to provide the context.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，系统提示用于指导模型如何行动，并提供上下文。
- en: Now that we have an understanding of the foundational approach and benefits
    of LangChain, let’s go deeper into its intricate design concepts, starting with
    how it bridges LLMs to diverse data sources efficiently.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了LangChain的基础方法和好处，让我们更深入地探讨其复杂的设计概念，从它如何高效地将LLMs与各种数据源桥接开始。
- en: Data that is not pre-embedded
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 未预先嵌入的数据
- en: While the preceding description is of data that is preprocessed to take the
    form of a vector database, another approach is to set up access to external data
    sources that are not yet processed into an embedding form. For instance, you may
    wish to leverage a SQL database to supplement other data sources. This approach
    is referred to as **multiple** **retrieval sources**.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前面的描述是关于预处理成向量数据库形式的数据，但另一种方法是设置访问尚未处理成嵌入形式的外部数据源。例如，你可能希望利用SQL数据库来补充其他数据源。这种方法被称为**多个****检索源**。
- en: We’ve now explored the ways LangChain efficiently interfaces with various data
    sources; now, it is essential to grasp the core structural elements that enable
    its functionalities – chains and agents.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经探讨了LangChain如何高效地与各种数据源接口；现在，理解其功能的核心结构元素至关重要——链和代理。
- en: Chains
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 链
- en: The atomic building blocks within LangChain are called components. Typical components
    could be a prompt template, access to various data sources, and access to LLMs.
    When combining various components to form a system, we form a chain. A chain can
    represent a complete LLM-driven application.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain内部的原子构建块被称为组件。典型的组件可能包括提示模板、访问各种数据源以及访问LLMs。当将各种组件组合成系统时，我们形成了一个链。一个链可以代表一个完整的由LLM驱动的应用程序。
- en: We will now present the concept of agents and walk through a code example that
    showcases how chains and agents come together, creating a capability that would
    have been quite complex not too long ago.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将介绍代理的概念，并通过一个代码示例展示链和代理如何结合在一起，创建出不久前相当复杂的功能。
- en: Agents
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理
- en: The next layer of complexity over chains is agents. Agents leverage chains by
    employing them and complementing them with additional calculations and decisions.
    While a chain may yield a response to a simple request prompt, an agent would
    process the response and act upon it with further downstream processing based
    on a prescribed logic.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在链之上，下一层复杂性是代理。代理通过使用链并辅以额外的计算和决策来利用链。虽然链可能对简单的请求提示产生响应，但代理会处理响应并根据规定的逻辑进行进一步的下层处理。
- en: You can view agents as a reasoning mechanism that employs what we call a tool.
    Tools complement LLMs by connecting them with other data or functions.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将代理视为一种推理机制，它使用我们所说的工具。工具通过将LLMs与其他数据或功能连接起来来补充LLMs。
- en: Given the typical LLM shortcomings that prevent LLMs from being perfect multitaskers,
    agents employ tools in a prescribed and monitored manner, allowing them to retrieve
    necessary information, leverage it as context, and execute actions using designated
    existing solutions. Agents then observe the results and employ the prescribed
    logic for further downstream processes.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 由于典型的LLM缺陷阻止了LLMs成为完美的多任务执行者，代理以规定和监控的方式使用工具，使它们能够检索必要的信息，将其作为上下文，并使用指定的现有解决方案执行操作。然后代理观察结果并使用规定的逻辑进行进一步的下层处理。
- en: As an example, assume we want to calculate the salary trajectory for an average
    entry-level programmer in our area. This task is comprised of three key sub-tasks
    – finding out what that average starting salary is, identifying the factors for
    salary growth (e.g., a change in the cost of living, or a typical merit increase),
    and then projecting onward. An ideal LLM would be able to do the entire process
    by itself, not requiring anything more than a coherent prompt. However, given
    the typical shortcomings, such as hallucinations and limited training data, current
    LLMs would not be able to perform this entire process to a level where it could
    be productionized within a commercial product. A best practice is to break it
    down and monitor the thought process via agents.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们想要计算我们地区一名普通初级程序员的薪资轨迹。这个任务由三个关键子任务组成——找出平均起薪是多少，确定薪资增长的因素（例如，生活成本的变动或典型的绩效提升），然后进行预测。一个理想的LLM能够独立完成整个过程，不需要比一个连贯的提示更多的东西。然而，考虑到典型的缺陷，如幻觉和有限的训练数据，当前的LLM无法达到可以在商业产品中商业化的水平。一种最佳实践是将任务分解，并通过代理监控思维过程。
- en: 'In its most simple design, this would require the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最简单的设计中，这需要以下步骤：
- en: Defining an agent that can access the internet and that can calculate future
    values of time series, given growth factors
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个可以访问互联网并能根据增长因子计算时间序列未来值的代理
- en: Providing the agent with a comprehensive prompt
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为代理提供全面的提示
- en: 'The agent breaks the prompt down into the different sub-tasks:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理将提示分解为不同的子任务：
- en: Fetching the average salary from the internet
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从互联网上获取平均工资
- en: Fetching the growth factors
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取增长因子
- en: Employing the calculation tool by applying the growth factors to the starting
    salary and creating a future time series for salary values
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过应用增长因子到起始薪资并创建薪资值的未来时间序列来使用计算工具
- en: To exemplify the agentic approach, let's review a simple task that involves
    fetching a particular detail from the web, and using it to perform a calculation.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明代理方法，让我们回顾一个简单的任务，该任务涉及从网络上获取特定细节，并使用它来进行计算。
- en: 'First, install these packages:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，安装以下包：
- en: '[PRE15]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, run the following code:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，运行以下代码：
- en: '[PRE16]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is then shown as follows:'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '[PRE17]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note that we didn’t apply any method to fix the LLM to reproduce this exact
    response. Running this code again will yield a slightly different answer.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们没有应用任何方法来固定LLM以重现这个确切响应。再次运行此代码将产生略微不同的答案。
- en: In the next chapter, we will dive deeper into several examples with code. In
    particular, we will program a multi-agent framework, where a team of agents is
    working on a joint project.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将更深入地探讨几个带有代码的示例。特别是，我们将编写一个多代理框架，其中一组代理正在共同完成一个项目。
- en: Long-term memory and referring to prior conversations
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 长期记忆和引用先前对话
- en: 'Another very important concept is long-term memory. We discussed how LangChain
    complements an LLM’s knowledge by appending additional data sources, some of which
    may be proprietary, making it highly customized for a particular use case. However,
    it still lacks a very important function, the ability to refer to prior conversations
    and learn from them. For instance, you can design an assistant for a project manager.
    As the user interacts with it, they would ideally update each day about the progress
    of the work, the interactions, the challenges, and so on. It would be best if
    the assistant could digest all that newly accumulated knowledge and sustain it.
    That would allow for a scenario such as this:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常重要的概念是长期记忆。我们讨论了LangChain如何通过附加额外的数据源来补充LLM的知识，其中一些可能属于专有数据，使其针对特定用例高度定制化。然而，它仍然缺少一个非常重要的功能，即引用先前对话并从中学习的能力。例如，你可以为项目经理设计一个助手。随着用户与之互动，他们理想情况下会每天更新工作进度、互动、挑战等内容。如果助手能够消化所有这些新积累的知识并保持其持续性，那就更好了。这将允许出现以下场景：
- en: '**User**: “Where do we stand with regard to Jim’s team’s task?”'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户**：“关于吉姆团队的任务，我们目前处于什么位置？”'
- en: '**Assistant**: “According to the original roadmap, Jim’s team is to address
    the client’s feedback to the design of the prototype. Based on the update from
    last week, the client provided only partial feedback, which you felt would not
    yet be sufficient for Jim’s team to start work.”'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**助手**：“根据原始路线图，吉姆团队应该处理客户对原型设计的反馈。根据上周的更新，客户只提供了部分反馈，您认为这还不足以让吉姆团队开始工作。”'
- en: We will touch more on the concept of memory in the next chapter.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章中更深入地探讨记忆的概念。
- en: Ensuring continuous relevance through incremental updates and automated monitoring
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过增量更新和自动化监控确保持续的相关性
- en: To maintain the accuracy and relevance of LLM outputs in dynamic information
    environments, it’s imperative to implement strategies for the ongoing update and
    maintenance of vector databases. As the corpus of knowledge continues to expand
    and evolve, so too must the embeddings that serve as the foundation for LLM responses.
    Incorporating techniques for incremental updates allows these databases to refresh
    their embeddings as new information becomes available, ensuring that the LLMs
    can provide the most accurate and up-to-date responses.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在动态信息环境中保持 LLM 输出的准确性和相关性，实施持续更新和维护向量数据库的策略至关重要。随着知识库的持续扩展和演变，作为 LLM 响应基础的嵌入也必须如此。采用增量更新技术允许这些数据库在新的信息可用时刷新其嵌入，确保
    LLMs 可以提供最准确和最新的响应。
- en: Incremental updates involve periodically re-embedding existing data sources
    with the latest information. This process can be automated to scan for updates
    in the data source, re-embed the new or updated content, and then integrate these
    refreshed embeddings into the existing vector database, without the need for a
    complete overhaul. By doing so, we ensure that the database reflects the most
    current knowledge available, enhancing the LLM’s ability to deliver relevant and
    nuanced responses.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 增量更新涉及定期重新嵌入现有数据源的最新信息。这个过程可以自动化，以扫描数据源中的更新，重新嵌入新的或更新的内容，然后将这些更新的嵌入集成到现有的向量数据库中，无需进行全面的重构。通过这样做，我们确保数据库反映了最当前的知识，增强了
    LLM 提供相关和细微响应的能力。
- en: Automated monitoring plays a pivotal role in this ecosystem by continually assessing
    the quality and relevance of the LLM’s outputs. This involves setting up systems
    that track the performance of the LLM, identifying areas where responses may be
    falling short due to outdated information or missing contexts. When such gaps
    are identified, the monitoring system can trigger an incremental update process,
    ensuring that the database remains a robust and accurate reflection of the current
    knowledge landscape.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 自动监控在这个生态系统中发挥着关键作用，它通过持续评估 LLM 输出的质量和相关性。这涉及到建立跟踪 LLM 性能的系统，识别由于信息过时或缺失上下文而导致响应可能不足的区域。当发现这些差距时，监控系统可以触发增量更新过程，确保数据库始终是当前知识格局的稳健和准确反映。
- en: By embracing these strategies, we ensure that LangChain and similar RAG frameworks
    can sustain their effectiveness over time. This approach not only enhances the
    relevance of LLM applications but also ensures that they can adapt to the rapidly
    evolving landscape of information, maintaining their position at the forefront
    of NLP technology.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用这些策略，我们确保 LangChain 和类似的 RAG 框架能够随着时间的推移保持其有效性。这种方法不仅增强了 LLM 应用程序的相关性，还确保它们能够适应信息快速发展的格局，保持其在
    NLP 技术前沿的位置。
- en: We can now get hands-on with LangChain.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以亲身体验 LangChain。
- en: Reviewing a simple LangChain setup in a Jupyter notebook
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Jupyter 笔记本中回顾简单的 LangChain 设置
- en: We are now ready to set up a complete pipeline that can later be lent to various
    NLP applications.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好设置一个完整的管道，该管道可以后来用于各种 NLP 应用程序。
- en: Refer to the `Ch8_Setting_Up_LangChain_Configurations_and_Pipeline.ipynb` notebook.
    This notebook implements the LangChain framework. We will walk through it step
    by step, explaining the different building blocks. We chose a simple use case
    here, as the main point of this code is to show how to set up a LangChain pipeline.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅 `Ch8_Setting_Up_LangChain_Configurations_and_Pipeline.ipynb` 笔记本。这个笔记本实现了
    LangChain 框架。我们将一步一步地走过它，解释不同的构建块。在这里，我们选择了一个简单的用例，因为这段代码的主要目的是展示如何设置 LangChain
    管道。
- en: In this scenario, we are in the healthcare sector. We have many care givers;
    each has many patients they may see. The physician in chief made a request on
    behalf of all the physicians in the hospital to be able to use a smart search
    across their notes. They heard about the new emerging capabilities with LLMs,
    and they would like to have a tool where they can search within the medical reports
    they wrote.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个场景中，我们处于医疗保健行业。我们有许多护理者；每位护理者都有许多他们可能看到的病人。首席医师代表医院内的所有医师提出请求，希望能够跨他们的笔记进行智能搜索。他们了解到
    LLMs 的新兴功能，并希望拥有一个工具，可以在他们编写的医疗报告中进行搜索。
- en: 'For instance, one physician said the following:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一位医生说了以下内容：
- en: “*I often come across research that may be relevant to a patient I saw months
    ago, but I don’t recall who that was. I would like to have a tool where I can
    ask, ‘Who was that patient that complained about ear pain and had a family history
    of migraines?’, and it would find me* *that patient.*”
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: “*我经常遇到可能与几个月前看过的患者相关的科研，但我记不起那个人是谁。我希望有一个工具，我可以问，‘那个抱怨耳朵疼痛并且有偏头痛家族史的患者是谁？’，然后它会找到那个患者。’*
- en: 'Thus, the business objective here is as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这里的商业目标是以下内容：
- en: “*The CTO tasked us with putting together a quick prototype in the form of a
    Jupyter notebook. We will collect several clinical reports from the hospital’s
    database, and we will use LangChain to search through them in the manner that
    the physician in the* *example described.”*
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: “*CTO要求我们快速构建一个以Jupyter笔记本形式的原型。我们将从医院的数据库中收集几份临床报告，并使用LangChain以示例中描述的医生的方式进行搜索。”*
- en: Let’s jump right in by designing the solution in Python.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接进入设计解决方案的Python实现。
- en: Setting up a LangChain pipeline with Python
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Python设置LangChain管道
- en: Diving into the practicalities of LangChain, this section will guide you step
    by step in setting up a LangChain pipeline using Python, from installing the necessary
    libraries to executing sophisticated similarity searches.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 深入LangChain的实际应用，本节将逐步指导您使用Python设置LangChain管道，从安装必要的库到执行复杂的相似性搜索。
- en: Installing the required Python libraries
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装所需的Python库
- en: 'As always, we have a list of libraries that we will need to install. Since
    we are writing the code in a Jupyter notebook, we can install them from within
    the code:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们有一个需要安装的库列表。由于我们将在Jupyter笔记本中编写代码，我们可以从代码中安装它们：
- en: '**Load the text files with mock physician notes**: Here, we put together some
    mock physician notes. We load them and process them per the LangChain paradigm.
    We stress that these aren’t real medical notes and that the people described there
    don’t exist.'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**加载带有模拟医生笔记的文本文件**: 在这里，我们准备了一些模拟医生笔记。我们加载并按照LangChain范式处理它们。我们强调，这些不是真实的医疗笔记，那里描述的人并不存在。'
- en: '**Process the data so that it can be prepared for embedding**: Here, we split
    the text per the requirements of the embedding model. As we mentioned in previous
    chapters, LMs, such as those used for embedding, have a finite window of input
    text that they can process in a single batch. That size is hardcoded in their
    design architecture and is fixed for each particular model.'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**处理数据以便准备嵌入**: 在这里，我们根据嵌入模型的要求分割文本。正如我们在前面的章节中提到的，像用于嵌入的LMs这样的模型有一个有限的输入文本窗口，它们可以在单个批次中处理。这个大小是硬编码在其设计架构中，并且对于每个特定模型是固定的。'
- en: '**Create the embeddings that would be stored in the vector database**: The
    vector database is one of the key pillars of the LangChain paradigm. Here, we
    take the text and create an embedding for each item. Those embeddings are then
    stored in a dedicated vector database. The LangChain library allows you to work
    with several different vector databases. While we chose one particular database,
    you can refer to the **Vector Store** page to read more about the different choices.'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建将存储在向量数据库中的嵌入**: 向量数据库是LangChain范式的重要支柱之一。在这里，我们将文本转换为每个项目的嵌入。这些嵌入随后存储在一个专门的向量数据库中。LangChain库允许您使用几个不同的向量数据库。虽然我们选择了一个特定的数据库，但您可以参考**向量存储**页面了解更多关于不同选择的信息。'
- en: '**Create the vector database**: Here, we create the vector database. This process
    may be slightly different for each database choice. However, the creators of these
    databases make sure to take away all of the hard work and leave you with a simple
    turnkey function that creates the database for you, given the appropriate embeddings
    in vector form. We leverage Meta’s **Facebook AI Similarity Search** (**FAISS**)
    database, as it is simple, quick to deploy, and free.'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建向量数据库**: 在这里，我们创建向量数据库。这个过程可能因数据库选择的不同而略有不同。然而，这些数据库的创建者确保移除所有困难的工作，并留下一个简单的现成函数，在给定适当的向量形式嵌入的情况下为您创建数据库。我们利用Meta的**Facebook
    AI相似性搜索**（**FAISS**）数据库，因为它简单、部署快速且免费。'
- en: '**Perform a similarity search based on our in-house documents**: This is the
    key part of the pipeline. We introduce several questions and use LangChain’s similarity
    search to identify the physician notes that would best answer our question.'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**基于我们内部文档进行相似度搜索**：这是流程中的关键部分。我们提出几个问题，并使用LangChain的相似度搜索来识别最能回答我们问题的医生笔记。'
- en: As we can see, the similarity search function is able to do a good job with
    most of the questions. It embeds the question and looks for reports whose embeddings
    are similar.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，相似度搜索功能能够很好地处理大多数问题。它将问题嵌入并寻找嵌入相似的报告。
- en: However, a similarity search could only go so far when it comes to answering
    the question correctly. It is easy to think of a question that discusses a matter
    that is very similar to one of the notes, yet a minor difference confuses the
    similarity search mechanism. For instance, the similarity search process actually
    makes a mistake in question two, mistaking different months and, thus, providing
    a wrong answer.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当正确回答问题时，相似度搜索只能走这么远。很容易想到一个问题，讨论的是与笔记非常相似的事情，但微小的差异却使相似度搜索机制困惑。例如，相似度搜索过程实际上在第二个问题中犯了一个错误，错误地将不同的月份混淆，从而提供了错误的答案。
- en: In order to overcome this matter, we would want to do more than just a similarity
    search. We would want an LLM to review the results of the similarity search and
    apply its judgment. We will see how that’s done in the next chapter.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这个问题，我们不仅想要进行相似度搜索。我们希望一个LLM能够审查相似度搜索的结果并应用其判断。我们将在下一章中看到这是如何实现的。
- en: With our foundation set for LangChain’s practical applications in Python, let’s
    now move on to understanding how the cloud plays a pivotal role, especially when
    harnessing the true potential of LLMs in contemporary computational paradigms.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在为LangChain在Python中的实际应用打下基础后，现在让我们继续了解云在其中的关键作用，尤其是在利用当代计算范式中的LLM真正潜力时。
- en: LLMs in the cloud
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云中的LLM
- en: In this era of big data and computation, cloud platforms have emerged as vital
    tools for managing large-scale computations, providing infrastructure, storage,
    and services that can be rapidly provisioned and released with minimal management
    effort.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据和计算时代，云平台已成为管理大规模计算的重要工具，提供可快速部署和释放的基础设施、存储和服务，同时管理努力最小化。
- en: This section will focus on computation environments in the cloud. These have
    become the dominant choice for many leading companies and institutions. As an
    organization, having a computation environment in the cloud versus on-premises
    makes a major difference. It impacts the ability to share resources and manage
    allocations, maintenance, and cost. There are many trade-offs for employing cloud
    services instead of owning physical machines. You can learn about them by searching
    online or even asking a chat LLM about them.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将专注于云中的计算环境。这些已成为许多领先公司和机构的首选。作为一个组织，拥有云中的计算环境而不是本地环境，会带来重大差异。它影响资源共享和管理分配、维护和成本的能力。使用云服务而不是拥有物理机器有许多权衡之处。你可以通过在线搜索或甚至询问一个聊天LLM来了解它们。
- en: One significant difference with cloud computing is the ecosystem that the providers
    have built around it. When you pick a cloud provider as your computation hub,
    you tap into a whole suite of additional products and services, opening up a new
    world of capabilities that would not be as accessible to you otherwise.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 与云计算相比，一个显著的不同点是提供商围绕其构建的生态系统。当你选择一个云提供商作为你的计算中心时，你将接入一系列额外的产品和服务，开启一个全新的能力世界，这些能力在其他情况下可能无法轻易获得。
- en: In this section, we will focus on the LLM aspect of those services.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点关注这些服务中的LLM（大型语言模型）方面。
- en: The three primary cloud platforms are AWS, Microsoft Azure, and GCP. These platforms
    offer a myriad of services, catering to the varying needs of businesses and developers.
    When it comes to NLP and LLMs, each platform provides dedicated resources and
    services to facilitate experimentation, deployment, and production.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 三个主要的云平台是AWS、Microsoft Azure和GCP。这些平台提供了一系列服务，满足企业和开发者的不同需求。当涉及到NLP和LLM时，每个平台都提供专门资源和服务，以促进实验、部署和生产。
- en: Let’s explore each of these platforms to see how they cater to our specific
    needs.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索这些平台中的每一个，看看它们如何满足我们的特定需求。
- en: AWS
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS
- en: AWS remains a dominant force in the cloud computing landscape, providing a comprehensive
    and evolving suite of services that cater to the needs of ML and AI development.
    AWS is renowned for its robust infrastructure, extensive service offerings, and
    deep integration with ML tools and frameworks, making it a preferred platform
    for developers and data scientists looking to innovate with LLMs.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: AWS在云计算领域仍然是一个主导力量，提供了一套全面且不断发展的服务，以满足机器学习和人工智能开发的需求。AWS以其强大的基础设施、广泛的服务提供以及与机器学习工具和框架的深度集成而闻名，使其成为寻求使用LLM进行创新的开发者和数据科学家的首选平台。
- en: Experimenting with LLMs on AWS
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在AWS上实验LLM
- en: 'AWS provides a rich ecosystem of tools and services designed to facilitate
    the development and experimentation with LLMs, ensuring that researchers and developers
    have access to the most advanced ML capabilities:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: AWS提供了一套丰富的工具和服务，旨在促进LLM的开发和实验，确保研究人员和开发者能够访问最先进的机器学习能力：
- en: '**Amazon SageMaker**: The cornerstone of ML on AWS, SageMaker is a fully managed
    service that streamlines the entire ML workflow. It offers Jupyter notebook instances
    for experimentation, broad framework support, including TensorFlow and PyTorch,
    and a range of tools for model building, training, and debugging. SageMaker’s
    capabilities have been continually enhanced to support the complexities of training
    and fine-tuning LLMs, providing scalable compute options and optimized ML environments.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon SageMaker**：AWS上机器学习的基础，SageMaker是一个全托管服务，它简化了整个机器学习工作流程。它提供Jupyter笔记本实例进行实验，支持广泛的框架，包括TensorFlow和PyTorch，以及一系列用于模型构建、训练和调试的工具。SageMaker的能力不断得到增强，以支持训练和微调LLM的复杂性，提供可扩展的计算选项和优化的机器学习环境。'
- en: '**AWS Deep Learning Containers and Deep Learning AMIs**: For those looking
    to customize their ML environments, AWS offers Deep Learning Containers and **Amazon
    Machine Images** (**AMIs**) pre-installed with popular ML frameworks. These resources
    simplify the setup process for LLM experiments, allowing developers to focus on
    innovation rather than infrastructure configuration.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS深度学习容器和深度学习AMIs**：对于那些希望定制他们的机器学习环境的人来说，AWS提供了预装了流行机器学习框架的深度学习容器和**Amazon
    Machine Images（AMIs**）。这些资源简化了LLM实验的设置过程，使得开发者能够专注于创新而不是基础设施配置。'
- en: '**Pre-trained models and SageMaker JumpStart**: AWS has expanded its library
    of pre-trained models accessible through SageMaker JumpStart, facilitating quick
    experimentation with LLMs for a variety of NLP tasks. JumpStart also offers solution
    templates and executable example notebooks, making it easier for developers to
    start and scale their ML projects.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预训练模型和SageMaker JumpStart**：AWS通过SageMaker JumpStart扩展了其可访问的预训练模型库，简化了各种自然语言处理任务中LLM的快速实验。JumpStart还提供了解决方案模板和可执行的示例笔记本，使得开发者更容易开始和扩展他们的机器学习项目。'
- en: Deploying and productionizing LLMs on AWS
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在AWS上部署和产品化LLM
- en: 'AWS provides a suite of services designed to efficiently deploy and manage
    LLMs at scale, ensuring that models are easily accessible and performant under
    varying loads:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: AWS提供了一套旨在高效部署和管理大规模LLM的服务，确保模型在各种负载下易于访问且性能出色：
- en: '**SageMaker endpoints**: To deploy LLMs, SageMaker endpoints offer fully managed
    hosting services with auto-scaling capabilities. This service allows developers
    to deploy trained models into production quickly, with the infrastructure automatically
    adjusting to the demands of the application.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SageMaker端点**：为了部署LLM，SageMaker端点提供具有自动扩展功能的全托管托管服务。这项服务允许开发者快速将训练好的模型部署到生产环境中，同时基础设施会自动调整以适应应用程序的需求。'
- en: '**Elastic Inference and Amazon EC2 Inf1 instances**: To optimize inference
    costs, AWS offers Elastic Inference, which adds GPU-powered inference acceleration
    to SageMaker instances. For even greater performance and cost efficiency, Amazon
    EC2 Inf1 instances, powered by AWS Inferentia chips, provide high-throughput and
    low-latency inference for DL models.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**弹性推理和Amazon EC2 Inf1实例**：为了优化推理成本，AWS提供了弹性推理服务，它为SageMaker实例增加了GPU驱动的推理加速功能。为了获得更高的性能和成本效率，由AWS
    Inferentia芯片驱动的Amazon EC2 Inf1实例为深度学习模型提供了高吞吐量和低延迟的推理服务。'
- en: '**AWS Lambda and Amazon Bedrock**: For serverless deployment, AWS Lambda supports
    running inference without provisioning or managing servers, ideal for applications
    with variable demand. Amazon Bedrock, represents a significant leap forward, offering
    serverless access to foundational models through APIs, model customization, and
    seamless integration within an organizational network, ensuring data privacy and
    security.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Lambda和Amazon Bedrock**：对于无服务器部署，AWS Lambda支持在无需配置或管理服务器的情况下运行推理，非常适合需求可变的应用程序。Amazon
    Bedrock代表了向前迈出的重要一步，通过API、模型定制和无缝集成到组织网络中，提供无服务器访问基础模型，确保数据隐私和安全。'
- en: Let’s move on to the next topic, Microsoft Aure.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续下一个主题，微软Azure。
- en: Microsoft Azure
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微软Azure
- en: Microsoft Azure stands at the forefront of cloud computing services, offering
    a robust platform for the development, deployment, and management of ML and LLMs.
    Leveraging its strategic partnership with OpenAI, Azure provides exclusive cloud
    access to GPT models, positioning itself as a critical resource for developers
    and data scientists aiming to harness the power of advanced NLP technologies.
    Recent enhancements have expanded Azure’s capabilities, making it an even more
    attractive choice for those looking to push the boundaries of AI and ML applications.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 微软Azure处于云计算服务的最前沿，为ML和LLMs的开发、部署和管理提供了一个强大的平台。借助其与OpenAI的战略合作伙伴关系，Azure提供了对GPT模型的独家云访问，使其成为旨在利用高级NLP技术的开发人员和数据科学家的重要资源。最近的增强功能扩展了Azure的能力，使其成为那些希望推动AI和ML应用边界的人的更具吸引力的选择。
- en: Experimenting with LLMs on Azure
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Azure上实验LLMs
- en: 'Azure has significantly enriched its offerings to support research and experimentation
    with LLMs, providing a variety of tools and platforms that cater to the diverse
    needs of the AI development community:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Azure显著丰富了其产品，以支持LLMs的研究和实验，提供各种工具和平台，满足AI开发社区的多样化需求：
- en: '**Azure OpenAI Service**: This directly integrates OpenAI’s cutting-edge models,
    including the latest GPT versions, DALL·E, and Codex, into the Azure ecosystem.
    This service enables developers to easily incorporate sophisticated AI functionalities
    into their applications, with the added benefits of Azure’s scalability and management
    tools.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure OpenAI服务**：直接将OpenAI的尖端模型，包括最新的GPT版本、DALL·E和Codex，集成到Azure生态系统中。此服务使开发者能够轻松地将复杂的AI功能集成到他们的应用程序中，同时享有Azure的可伸缩性和管理工具的额外好处。'
- en: '**Azure Machine Learning (Azure ML)**: This offers an advanced environment
    for the custom training and fine-tuning of LLMs on specific datasets, allowing
    for enhanced model performance on niche tasks. Azure ML Studio’s pre-built and
    customizable Jupyter notebook templates support a wide range of programming languages
    and frameworks, facilitating a seamless experimentation process.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure机器学习（Azure ML）**：这提供了一个高级环境，用于在特定数据集上对LLMs进行定制培训和微调，从而在特定任务上提高了模型性能。Azure
    ML Studio的预构建和可定制的Jupyter笔记本模板支持广泛的编程语言和框架，简化了实验过程。'
- en: '**Azure Cognitive Services**: This provides access to a suite of pre-built
    AI services, including text analytics, speech services, and decision-making capabilities
    powered by LLMs. These services enable developers to add complex AI functions
    to applications quickly, without deep ML expertise.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure认知服务**：这提供了一系列预构建的AI服务，包括文本分析、语音服务和由LLMs驱动的决策能力。这些服务使开发者能够快速地将复杂的AI功能添加到应用程序中，无需深厚的ML专业知识。'
- en: Deploying and productionizing LLMs on Azure
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Azure上部署和商业化LLMs
- en: 'Azure’s infrastructure and services offer comprehensive solutions for the deployment
    and productionization of LLM applications, ensuring scalability, performance,
    and security:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Azure的基础设施和服务为LLM应用程序的部署和商业化提供了全面的解决方案，确保了可伸缩性、性能和安全性：
- en: '**Deployment options**: Azure supports various deployment scenarios through
    **Azure Container Instances** (**ACI**) for lightweight deployment needs and **Azure
    Kubernetes Service** (**AKS**) for larger, more complex applications requiring
    high scalability. These services allow for the efficient scaling of LLM applications
    to meet user demand.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署选项**：Azure通过**Azure容器实例（ACI**）支持各种部署场景，适用于轻量级部署需求，以及**Azure Kubernetes服务（AKS**）支持更大、更复杂的应用程序，这些应用程序需要高可伸缩性。这些服务允许高效地扩展LLM应用程序以满足用户需求。'
- en: '**Model management**: Through Azure ML, developers can manage the life cycle
    of their models, including version control, auditing, and governance. This ensures
    that deployed models are not only performant but also comply with industry standards
    and regulatory requirements.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型管理**：通过 Azure ML，开发者可以管理其模型的生命周期，包括版本控制、审计和治理。这确保了部署的模型不仅性能出色，而且符合行业标准和管理要求。'
- en: '**Security and compliance**: Azure emphasizes security and compliance across
    all its services, providing features such as data encryption, access control,
    and comprehensive compliance certifications. This commitment ensures that applications
    built and deployed on Azure meet the highest standards for data protection and
    privacy.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全和合规性**：Azure 强调在其所有服务中实现安全和合规性，提供数据加密、访问控制和全面的合规性认证等功能。这种承诺确保在 Azure 上构建和部署的应用程序符合数据保护和隐私的最高标准。'
- en: GCP
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GCP
- en: GCP continues to be a powerhouse in cloud computing, providing an extensive
    suite of services that cater to the evolving needs of AI and ML development. Known
    for its cutting-edge innovations in AI and ML, GCP offers a rich ecosystem of
    tools and services that facilitate the development, deployment, and scaling of
    LLMs, making it an ideal platform for developers and researchers aiming to leverage
    the latest in AI technology.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: GCP 继续在云计算领域保持领先地位，提供了一套广泛的服务，以满足人工智能（AI）和机器学习（ML）发展的不断变化的需求。以其在 AI 和 ML 领域的尖端创新而闻名，GCP
    提供了一个丰富的工具和服务生态系统，简化了 LLMs 的开发、部署和扩展，使其成为开发者和研究人员利用最新 AI 技术的理想平台。
- en: Experimenting with LLMs on GCP
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 GCP 上实验 LLMs
- en: 'GCP has further enhanced its capabilities for experimenting with and developing
    LLMs, offering a comprehensive set of tools that support the entire ML workflow,
    from data ingestion and model training to hyperparameter tuning and evaluation:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: GCP 进一步增强了其用于实验和开发 LLMs 的功能，提供了一套全面的支持整个 ML 工作流程的工具，从数据摄取和模型训练到超参数调整和评估：
- en: '**Vertex AI**: At the heart of GCP’s ML offerings, Vertex AI provides an integrated
    suite of tools and services that streamline the ML workflow. It offers advanced
    features for training and fine-tuning LLMs, including AutoML capabilities for
    automating the selection of optimal model architectures and hyperparameters. Vertex
    AI’s integration with GCP’s robust data and analytics services makes it easier
    to manage large datasets that are essential for training LLMs.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Vertex AI**：作为 GCP 机器学习产品线的核心，Vertex AI 提供了一套集成的工具和服务，简化了机器学习工作流程。它为训练和微调
    LLMs 提供了高级功能，包括用于自动化选择最佳模型架构和超参数的 AutoML 功能。Vertex AI 与 GCP 强大的数据和数据分析服务的集成使得管理对训练
    LLMs 至关重要的大数据集变得更加容易。'
- en: '**An IDE**: The built-in notebooks service within Vertex AI offers a fully
    managed JupyterLab environment, enabling developers to write, run, and debug ML
    code seamlessly. This environment is optimized for ML development, supporting
    popular frameworks such as TensorFlow and PyTorch, which are crucial for building
    and experimenting with LLMs.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成开发环境（IDE）**：Vertex AI 内置的笔记本服务提供了一个完全管理的 JupyterLab 环境，使开发者能够无缝地编写、运行和调试机器学习（ML）代码。此环境针对
    ML 开发进行了优化，支持 TensorFlow 和 PyTorch 等流行框架，这些框架对于构建和实验大型语言模型（LLMs）至关重要。'
- en: '**AI and ML libraries**: GCP continues to expand its library of pre-trained
    models and ML APIs, including those specifically designed for NLP and understanding.
    These tools allow developers to integrate advanced NLP capabilities into their
    applications rapidly.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AI 和 ML 库**：GCP 继续扩展其预训练模型和机器学习 API 的库，包括专门为自然语言处理（NLP）和理解设计的那些。这些工具允许开发者快速将高级
    NLP 功能集成到他们的应用程序中。'
- en: Deploying and productionizing LLMs on GCP
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 GCP 上部署和产品化 LLMs
- en: 'GCP provides robust and scalable solutions for deploying and productionizing
    LLMs, ensuring that applications built on its platform can meet the demands of
    real-world usage:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: GCP 为部署和产品化 LLMs 提供了强大且可扩展的解决方案，确保在其平台上构建的应用程序能够满足现实世界使用的需求：
- en: '**Vertex AI prediction**: Once an LLM is trained, Vertex AI’s prediction service
    allows for the easy deployment of models as fully managed, auto-scaling endpoints.
    This service simplifies the process of making your LLMs accessible to applications,
    with the infrastructure automatically adjusting to the workload demands.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Vertex AI 预测**：一旦 LLM 被训练，Vertex AI 的预测服务允许轻松地将模型作为完全管理的、自动扩展的端点进行部署。此服务简化了使
    LLMs 可用于应用程序的过程，同时基础设施会自动调整以适应工作负载需求。'
- en: '**Google Kubernetes Engine (GKE)**: For more complex deployment scenarios requiring
    high availability and scalability, GKE offers a managed environment to deploy
    containerized LLM applications. GKE’s global infrastructure ensures that your
    models are highly available and can scale to meet the needs of enterprise-level
    applications.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Kubernetes Engine (GKE)**：对于需要高可用性和可扩展性的复杂部署场景，GKE 提供了一个托管环境来部署容器化的
    LLM 应用程序。GKE 的全球基础设施确保您的模型具有高可用性，并且可以扩展以满足企业级应用程序的需求。'
- en: Concluding cloud services
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云服务总结
- en: The landscape of cloud computing continues to evolve rapidly, with AWS, Azure,
    and GCP each offering unique advantages for the development and deployment of
    LLMs. AWS stands out for its broad infrastructure and deep integration with ML
    tools, making it ideal for a wide range of ML and AI projects. Azure, with its
    exclusive access to OpenAI’s models and deep integration within the Microsoft
    ecosystem, offers unparalleled opportunities for enterprises looking to leverage
    the cutting edge of AI technology. GCP, recognized for its innovation in AI and
    ML, provides tools and services that mirror Google’s internal AI advancements,
    appealing to those seeking the latest in AI research and development. As the capabilities
    of these platforms continue to expand, the choice between them will increasingly
    depend on specific project needs, organizational alignment, and strategic partnerships,
    underscoring the importance of a thoughtful evaluation based on the current and
    future landscape of cloud-based AI and ML.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算领域持续快速发展，AWS、Azure 和 GCP 各自为 LLM 的开发和部署提供了独特的优势。AWS 以其广泛的基础设施和与 ML 工具的深度集成而突出，使其成为各种
    ML 和 AI 项目的理想选择。Azure 通过其对 OpenAI 模型的独家访问和在微软生态系统中的深度集成，为企业利用 AI 技术的尖端提供了无与伦比的机会。GCP
    以其在 AI 和 ML 领域的创新而闻名，提供与谷歌内部 AI 进步相匹配的工具和服务，吸引那些寻求最新 AI 研究和开发的人。随着这些平台功能的不断扩展，选择它们之间的差异将越来越多地取决于具体项目需求、组织协调和战略伙伴关系，这突显了基于当前和未来基于云的
    AI 和 ML 环境进行深思熟虑评估的重要性。
- en: Summary
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: As the world of NLP and LLMs continues to grow rapidly, so do the various practices
    of system design. In this chapter, we reviewed the design process of LLM applications
    and pipelines. We discussed the components of these approaches, touching on both
    API-based closed source and local open source solutions. We then gave you hands-on
    experience with code.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 随着自然语言处理和 LLM 领域的快速增长，系统设计的各种实践也在增加。在本章中，我们回顾了 LLM 应用和管道的设计过程。我们讨论了这些方法的组成部分，涉及基于
    API 的封闭源代码和本地开源解决方案。然后，我们通过代码让您亲身体验。
- en: We later delved deeper into the system design process and introduced LangChain.
    We reviewed what LangChain comprises and experimented with an example pipeline
    in code.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们后来更深入地探讨了系统设计过程，并介绍了 LangChain。我们回顾了 LangChain 包含的内容，并在代码中进行了示例管道的实验。
- en: To complement the system design process, we surveyed leading cloud services
    that allow you to experiment, develop, and deploy LLM-based solutions.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 为了补充系统设计过程，我们调查了领先的云服务，这些服务允许您进行实验、开发和部署基于 LLM 的解决方案。
- en: In the next chapter, we’ll focus on particular practical use cases, accompanied
    with code.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将重点关注特定的实际用例，并伴随代码。
