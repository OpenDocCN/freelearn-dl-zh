<html><head></head><body>
		<div><h1 id="_idParaDest-37" class="chapter-number"><a id="_idTextAnchor036"/>2</h1>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor037"/>NLP Evolution and Transformers: Exploring NLPs and LLMs</h1>
			<p>In the previous introductory chapter, you gained a fundamental understanding of generative AI, including a primer on the growing complexity of generative AI applications, along with a brief introduction to cloud computing for scalability and cost-effectiveness and the key components of data storage, security, and collaboration. You also learned one of the more exciting aspects of generative AI, which can also be a hurdle, which is how to stay up to date with cutting-edge AI technologies such as GenAI.</p>
			<p>In this chapter, we will explore the capabilities of ChatGPT, specifically with regard to its conversation input and response abilities. We will delve deeper into how LLMs are able to understand and respond to user queries and learn and adapt to new information. The information provided will be useful for individuals who are looking to understand more about how AI assistants, such as ChatGPT, work and how they can be utilized to help people find information more efficiently and effectively; subsequently, we will be expanding on this topic in relation to the NLP and prompt engineering topics discussed in <a href="B21443_05.xhtml#_idTextAnchor098"><em class="italic">Chapter 5</em></a>. By the end of this chapter, we hope you will get a deeper understanding of the progression of NLP and generative AI techniques by exploring the capabilities of various text-based tasks for prompts and responses, along with conversational flows and integration.</p>
			<p>We will cover the following main topics in the chapter:</p>
			<ul>
				<li>NLP evolution and the rise of transformers</li>
				<li>Conversation prompts and completions – under the covers</li>
				<li>LLMs landscape, progression, and expansion</li>
			</ul>
			<div><div><img src="img/B21443_02_1.jpg" alt="Figure 2.1 – How profound transformers have become"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – How profound transformers have become</p>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor038"/>NLP evolution and the rise of transformers</h1>
			<p>NLP, or natural language processing, is the<a id="_idIndexMarker196"/> field of artificial intelligence that gives computers the ability to understand and manipulate human language using common spoken (or otherwise) language instead of what was traditionally given as input to computers in the past: computer programming language. Over the past several decades, these computer programming languages became more “natural” with fluency:</p>
			<div><div><img src="img/B21443_02_2.jpg" alt="Figure 2.2 – Brief timeline of NLP evolution"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – Brief timeline of NLP evolution</p>
			<p>Over time, there has been significant <a id="_idIndexMarker197"/>advancement in the field of NLP, with computers increasingly improving in their ability for text generation due to the emergence of neural networks. Text generation itself isn’t a novel idea, but earlier language models before 2017 predominantly <a id="_idIndexMarker198"/>utilized<a id="_idIndexMarker199"/> ML architectures known as <strong class="bold">recurrent neural networks</strong> (<strong class="bold">RNNs</strong>) and <strong class="bold">convolutional neural </strong><strong class="bold">networks</strong> (<strong class="bold">CNNs</strong>).</p>
			<p>RNNs are a<a id="_idIndexMarker200"/> type of neural network architecture that excels at processing sequence data. They process input in a sequential manner, carrying information from one step in the sequence to the next. This makes them quite useful for tasks such as text generation, translation, and sentiment analysis.</p>
			<p>A CNN<a id="_idIndexMarker201"/> is a type of deep learning architecture designed to process and analyze visual data, such as images and videos, by using specialized layers called convolutional layers. These layers apply filters to extract relevant features from the input data, capturing patterns and hierarchies of information. CNNs are primarily used for tasks such as image classification, object detection, and image segmentation in computer vision. In <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>), CNNs can also be applied to tasks such as text classification and sentiment analysis, where the input text is transformed into a matrix-like structure to capture local patterns and relationships among words or characters.</p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor039"/>The main drawbacks of RNNs and CNNs</h2>
			<p>Despite the sophistication of <a id="_idIndexMarker202"/>RNNs, their potential could not be fully harnessed due to certain constraints. RNNs often struggle with the “Vanishing gradient problem” during training, which hampers their ability to learn from long sequences and retain long-term dependencies.</p>
			<p>Additionally, the inherently “sequential processing” of RNNs does not allow for efficient parallelization, significantly slowing down training in an age where GPU-based parallel processing is standard for deep learning models.</p>
			<p>RNNs were, thus, limited in computing and memory. In order to predict the next word in a sentence, the models need to know more than the previous few words; they also need to understand the context of the word in a sentence, paragraph, or whole document.</p>
			<p>Let’s explain this concept with an example by using the sentence</p>
			<p>“<em class="italic">The water in the ocean has a lot of salt, it’s a bit choppy and it </em><em class="italic">tastes </em><em class="italic">sweet</em><em class="italic">.</em>”</p>
			<p>In the preceding sentence, the RNN might generate a continuation, indicating the water is sweet instead of salty. The reason for this is because of only taking the last few words into consideration and not the context of the whole sentence. The RNN will have forgotten the context from earlier in the text that might indicate the taste of the water from the ocean.</p>
			<p>Similarly, CNNs have <a id="_idIndexMarker203"/>revolutionized image analysis by automatically learning hierarchical features through layers of convolutions. Despite their success, CNNs are limited in that they have a fixed receptive field size and operate in a local context. This limitation makes it challenging for them to capture global dependencies and the relationships present in sequences of varying lengths. For instance, in image classification, while CNNs excel at recognizing local patterns, they struggle to grasp the overall context of an image, hindering their ability to understand complex relationships between objects or regions. Consider an image of a cat chasing a mouse with a dog watching in the background. A CNN might effectively identify the cat, mouse, and dog based on their local features. However, understanding the intricate relationships, e.g., the cat is chasing the mouse, and the dog is a passive observer, may be challenging for a CNN.</p>
			<p>So, how did we finally overcome the challenges of CNNs? It was done by using a concept known as transformer model architecture and its “self-attention mechanism,” which is described in the next section. This would not only identify the individual animals but also capture the contextual interactions, such as the chase sequence and the dog’s passive stance.</p>
			<p>However, before we really peel back the layers on how transformers work, the following is a reference timeline about the strengths of NLP coupled with LLMs. Once you realize the benefits and the “why,” we can then dive into the “how.”</p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor040"/>NLP and the strengths of generative AI in LLMs</h2>
			<p>This section provides a broad overview of NLP with LLMs before the next section, where we explain more about transformers: the powerful engine behind LLMs.</p>
			<p><strong class="bold">Large language models</strong> (<strong class="bold">LLMs</strong>) are <a id="_idIndexMarker204"/>incredibly potent language models that are transforming our comprehension and creation of human language. But what’s their connection to NLP? It’s rather fundamental. NLP lays out the structure and goals for interpreting and generating human language, whereas LLMs serve as sophisticated tools that facilitate the realization of these goals on a grand scale, handling intricate tasks with remarkable precision.</p>
			<p>As mentioned earlier, NLP<a id="_idIndexMarker205"/> is a branch of machine learning that enables computers to understand, process, and generate human language. It combines computer science and linguistics. For example, there is a massive amount of audio and text data generated by organizations from various communication channels. These data can be processed by NLP models to automatically process data, determine sentiments, summarize, and find answers, key topics, or even respond effectively.</p>
			<p>As a quick, simple example, the audio data generated by call centers can be converted to text and processed by NLP models to determine both the issue the customer is facing and also the sentiment of the customer (whether they are happy, upset, nonchalant, and so on).</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">NLP is <a id="_idIndexMarker206"/>the technology behind search engines, such as Bing and Google, voice assistants, such as Alexa and Siri, and powerful conversational agents, such as ChatGPT.</p>
			<p>From this, it would appear that NLP technology should address all of our needs. So why should we have to deal with LLMs and GenAI at all?</p>
			<p>By taking a step back briefly and looking at the preceding evolution timeline, the inception of <strong class="bold">advanced NLP</strong> can be <a id="_idIndexMarker207"/>traced back to 2013 with the advent of word2vec, a model introduced by Google that transformed words into dense vectors based on contextual relationships. A vector<a id="_idIndexMarker208"/> is defined as an object that has both a magnitude and a direction and is represented in a numerical array format.</p>
			<p>This was revolutionary, as it captured semantic nuances that older models couldn’t grasp. However, they couldn’t focus on different parts of the text to form a larger understanding. For example, various words in a sentence, or multiple sentences, could not be related to one another for a full understanding of a sentence or paragraph. This limitation was tackled by attention mechanisms, which were introduced in the 2017 paper <em class="italic">Attention Is All You Need</em>. These mechanisms led to the transformer architecture, the backbone of the foundational LLM models we see today, which allowed models to form an understanding of text beyond just words and sentences. There will be more on this a bit later, but first, let’s cover why we want to use LLMs and look at some areas where LLMs can enhance NLP.</p>
			<p>NLP plus LLMs equals expanded possibilities:</p>
			<ul>
				<li><strong class="bold">Understanding language</strong>: LLMs are <a id="_idIndexMarker209"/>adept at comprehending and processing a vast array of language inputs, making them useful for a variety of linguistic tasks. LLMs can be used to build advanced chatbots and virtual assistants. They can understand and respond to customer inquiries, provide information, and execute tasks, improving the efficiency and quality of customer service.</li>
				<li><strong class="bold">Text generation</strong>: LLMs can <a id="_idIndexMarker210"/>generate coherent and contextually appropriate text, enabling applications such as chatbots, content creation, copywriting, and more.</li>
				<li>LLMs <a id="_idIndexMarker211"/>can enhance efficiency in internal and external communications by recommending and suggesting words or reviewing your content.</li>
				<li><strong class="bold">Language translation</strong>: LLMs can directly translate text between different languages, aiding <a id="_idIndexMarker212"/>cross-cultural communication and language learning.</li>
				<li>As LLMs can provide translation between multiple languages, this can help businesses operate more efficiently in a globalized world by breaking down language barriers.</li>
				<li><strong class="bold">Sentiment analysis</strong>: LLMs <a id="_idIndexMarker213"/>can analyze text to determine its sentiment (positive, negative, or neutral), providing valuable insights for applications such as customer feedback analysis. LLMs can analyze customer feedback, reviews, or social media posts to assess public sentiment toward a brand, product, or service. This can help with business strategies and decision-making processes.</li>
				<li><strong class="bold">Question answering</strong>: LLMs can <a id="_idIndexMarker214"/>understand and provide accurate answers to a wide range of questions, making it possible to build an organization-specific enterprise search engine.</li>
				<li><strong class="bold">Text summarization</strong>: LLMs can <a id="_idIndexMarker215"/>condense long pieces of text into shorter summaries, aiding in information processing and comprehension. LLMs can summarize long documents, articles, or reports, making it easier to digest large amounts of information quickly while identifying key areas or the next steps.</li>
				<li><strong class="bold">Adaptability</strong>: LLMs can<a id="_idIndexMarker216"/> generate text in various styles, tones, or formats, adapting to specific user needs or application requirements. For example, you can ask ChatGPT to define and describe Photosynthesis in plants for your 6-year-old child in the style of a pirate. In relation to <a id="_idIndexMarker217"/>this, by using data about user behavior and preferences, LLMs can generate personalized content or product recommendations, thus improving user experience and potentially increasing sales for retail businesses.</li>
				<li><strong class="bold">Context maintenance</strong>: Although they only have short-term memory, LLMs can maintain<a id="_idIndexMarker218"/> conversational context over extended interactions with the right prompt engineering techniques, improving the coherence and relevance of their responses. We will cover prompt engineering techniques in <a href="B21443_05.xhtml#_idTextAnchor098"><em class="italic">Chapter 5</em></a> of this book.</li>
				<li><strong class="bold">Creativity</strong>: LLMs<a id="_idIndexMarker219"/> can generate novel text, opening more possibilities for creative applications such as story generation or poetry creation. From writing articles, reports, and marketing copy to generating creative content, LLMs can automate and enhance various content creation tasks.</li>
			</ul>
			<p>Here, we have listed a few of the areas where large language models have enhanced the functionality of natural language processing. Now that you can appreciate the fact LLMs can provide enhancements to any NLP services, and also to our everyday lives, let’s take the next step: a deeper dive into transformers and the attention mechanism, which gives LLMs their power to run generative AI.</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor041"/>How do transformers work?</h2>
			<p>The introduction of <a id="_idIndexMarker220"/>transformer architecture addresses the preceding shortcomings of RNNs and CNNs. Transformers use an attention mechanism, which allows the model to focus on different parts of the input when generating each word in the output. Simply put, the attention mechanism measures how words interrelate in a sentence, paragraph, or section. For LLMs, the underlying transformer is a set of deep learning neural networks that consist of an encoder component and a decoder component that exist within the concept of self-attention capability. During self-attention, an LLM will assign weights to different words based on their relevance to the current word being processed, and this is what gives the model its power. This attention mechanism dynamically enables LLMs to focus on critical contextual information while also disregarding nonrelevant items/words at the same time. In other words, the encoder and decoder components extract meanings from a sequence of text and understand the relationships between the words and phrases in it.</p>
			<p>This allows<a id="_idIndexMarker221"/> transformers to maintain a better sense of <a id="_idIndexMarker222"/>long-term <strong class="bold">context</strong> compared to RNNs and CNNs. Positional encodings allow for the handling of sequence order and transformers allow for <a id="_idIndexMarker223"/>the <strong class="bold">parallel processing</strong> of sequences, making LLMs much faster to train compared to RNNs. The foundational models underpinning ChatGPT, known as GPT models, employ this transformer architecture.</p>
			<p>When first introduced, the transformer architecture was originally designed for translation and is described in the now famous publication by Google: <em class="italic">Attention is All You Need</em> (see <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a> for a deeper look). From this publication, we show the original transformer architecture in the following image, and we have added the encoder on the left and the decoder on the right for your high-level understanding:</p>
			<div><div><img src="img/B21443_02_3.jpg" alt="Figure 2.3 – Transformer model architecture"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – Transformer model architecture</p>
			<p>While the preceding image<a id="_idIndexMarker224"/> can be daunting to some, especially to beginners in the field of generative AI, you do not necessarily need to have a firm understanding of each subcomponent of the transformer model architecture in the same way that most people do not need to know the internal workings of an automobile engine in order to drive a car. We will only cover the main input and outputs of the transformer architecture, and there is a simplified view later in this chapter to describe some of the inner workings and flow. We will continue to emphasize and repeat various aspects of the transformer model, as this can be a difficult concept to grasp, especially for those<a id="_idIndexMarker225"/> new to generative AI and LLMs.</p>
			<p>From the original purpose of language translations in 2017, the transformer model architecture became the underpinning framework for future generative AI models, leading to the emergence of ChatGPT; the letter <strong class="bold">T</strong> in GPT stands for <strong class="bold">transformer</strong> (GPT).</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>Benefits of transformers</h2>
			<p>As mentioned earlier, transformers<a id="_idIndexMarker226"/> are a type of neural network architecture that replaces traditional RNNs and CNNs with an entirely attention-based mechanism.</p>
			<p>But how does the attention mechanism work?</p>
			<p>Attention does this by calculating “soft” weights for each word in the context window and doing this in parallel in the transformer model vs. sequentially in the RNN/CNN models. These “soft” weights can, and often do, change during the runtime of the model.</p>
			<p>The benefits<a id="_idIndexMarker227"/> of transformers are the following:</p>
			<ul>
				<li>They scale efficiently to use multi-core GPUs and parallel processing training data; hence, they can make use of much, much larger datasets.</li>
				<li>They pay attention to the meaning of input.</li>
				<li>They learn the relevance of every word and their context in a sentence/paragraph, not just the neighboring words, as with RNNs and CNNs.</li>
			</ul>
			<p> Let’s take a look at a visual representation of how the words of the sentence, “<em class="italic">The musician taught the student with the piano</em>,” relate to one another from the perspective of a transformer:</p>
			<div><div><img src="img/B21443_02_4.jpg" alt="Figure 2.4 – Sentence context relationships"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – Sentence context relationships</p>
			<p>As stated in the preceding example, transformers are able to link every word, determine the relationships between every word in the input (even if they are immediately preceding or succeeding word(s)), and understand the context of the word in a sentence. In the preceding image, the colored lines represent stronger relationships.</p>
			<p>Thus, transformers<a id="_idIndexMarker228"/> use modern mathematical techniques, such as attention or self-attention, to determine the inter-relationships and dependencies among data elements, even when they are far apart. This gives the model the ability to learn who taught the student and <a id="_idTextAnchor043"/>with what instrument, etc.</p>
			<p>There are multiple layers in the transformer deep learning architecture, such as the embedding layer, self-attention, and multi-headed attention, as well as the multiple encoder models themselves. While a detailed understanding of the transformer architecture isn’t essential for successful prompt engineering or understanding generative AI, having a foundational grasp of the transformer model, a critical aspect of LLM’s and ChatGPT’s underlying architecture, is important for any cloud solution design.</p>
			<p>As we have talked about benefits, let’s also mention a negative aspect of transformers; they can sometimes produce a by-product that also affects LLMs, which we briefly mentioned in the first chapter but will again mention here as we are discussing transformers and that is the concept of “hallucinations.” A hallucination is basically incorrect information returned by an LLM model. This hallucination is response output, which is inconsistent with the prompt and is often due to a few reasons, such as the actual training data used to train the LLM model itself being incomplete or spurious. We wanted to mention it here, but we will discuss hallucinations in later chapters.</p>
			<p>For now, let’s dive into the inner workings of transformer architecture and explore the transformer concept a bit more with some examples.</p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor044"/>Conversation prompts and completions – under the covers</h2>
			<p>Prompts, or the<a id="_idIndexMarker229"/> input entered by you or an application/service, play a crucial role in NLP + LLMs by facilitating the interaction between humans and language models.</p>
			<p>If you have had any experience with GenAI, you may have already entered a prompt into an online service such as chat.bing.com. A prompt is to an LLM what a search term is to a web search engine, but each can take a prompt input and run some action(s) against such input. Just like you would intelligently enter search terms into a search engine to find the content you are looking for, the same can be said about entering prompts intelligently. This concept is known as<a id="_idIndexMarker230"/> prompt engineering, and we devote an entire chapter to prompt engineering later in this book, which will describe the “how” of writing an effective prompt to get the results you need.</p>
			<p>Some of you who are newer to the generative AI space might wonder why we need to understand how to write a prompt at all. Let’s provide a simple analogy: if you think of a <strong class="bold">database administrator</strong> (<strong class="bold">DBA</strong>) who <a id="_idIndexMarker231"/>needs to pull (query) specific data from a vast database with many tables (say, a typical customer sales database) in order to understand the trends and forecasting of sales to ensure there is enough product, you have to analyze the historical data. However, if the DBA cannot put together a proper query to build a report of past sales history, any forecasting and future trends will be completely incorrect.</p>
			<p>Similarly, a poorly constructed prompt is like using a dull knife, you’re unlikely to get great results. Thus, prompt engineering is crucial to generate useful responses.</p>
			<p>For now, let’s take a look at the inputs of the transformer in a bit more detail.</p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor045"/>Prompt and completion flow simplified</h1>
			<p>There<a id="_idIndexMarker232"/> are<a id="_idIndexMarker233"/> already<a id="_idIndexMarker234"/> countless <a id="_idIndexMarker235"/>transformer<a id="_idIndexMarker236"/> models, such<a id="_idIndexMarker237"/> as <strong class="bold">GPT</strong>, <strong class="bold">Llama 2</strong>, <strong class="bold">Dolly</strong>, <strong class="bold">BERT</strong>, <strong class="bold">BART</strong>, <strong class="bold">T5</strong>, and so on. These are essentially LLMs and, as you already know from <a href="B21443_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, they are trained on vast quantities of unstructured text in a self-supervised manner. In this self-supervised learning, the training objective is automatically derived from the model’s inputs, eliminating the need for human-annotated labels or input (more on this later in this section). This allowed the transformer models or LLMs to be massive in terms of their parameters. GPT-4 has more than 1.75 trillion parameters alone. Sam Altman stated that the cost of training GPT-4 alone was more than $100 million (<a href="https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/">https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/</a>)!</p>
			<p>Such models gain a statistical comprehension of the language they are trained on. However, they are not particularly useful for specific practical tasks. To overcome this, the pre-trained model undergoes a process known as transfer learning. In this phase, the model is fine-tuned in a supervised manner, meaning it uses human-annotated labels for a specific task. We will cover fine-tuning in further detail in the next chapter, but for now, let’s<a id="_idIndexMarker238"/> look at the overall flow of a simple task. One such task could be predicting the next word in a sentence after reading the previous <em class="italic">n</em> words. This is referred to as causal language modeling since the output is dependent on past and present inputs but not future ones.</p>
			<p>Let’s take a look at this simplified input/output flow, as mapped to the transformer model architecture, by using a financial news article as input and summarizing the document using a summarization LLM model:</p>
			<div><div><img src="img/B21443_02_5.jpg" alt="Figure 2.5 – Simplified visual of how prompt/completions work in a typical LLM"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5 – Simplified visual of how prompt/completions work in a typical LLM</p>
			<p>In the preceding simplified transformer architecture, the interaction is the input/output described in the white boxes. The larger gray box is the entirety of the processing taking place without user interaction. Some of the phases in the prompt and completion sequence in the preceding image include the following:</p>
			<ul>
				<li><strong class="bold">Input prompt</strong>: The <a id="_idIndexMarker239"/>user interacts with the system by providing input. This input can exist in various forms, such as text, voice, or other modalities. In our example, a financial news article was the input.</li>
				<li><strong class="bold">Additional prompt engineering</strong>: In the case of summarizing a news article, typically, we do not need <a id="_idIndexMarker240"/>additional prompt engineering. Although we have an entire chapter devoted to covering prompt engineering later, it is enough to know that different prompts will generate different outcomes/completions and prompting is a skill in itself.</li>
				<li><strong class="bold">Input text</strong>: This is <a id="_idIndexMarker241"/>the area where the finalized input is taken in human readable form and passed on to computer processing (the tokenizer). For example, this could be a combination of the original user input and any additional inputs such as datasets. For our example, we used a single financial news article to summarize; however, this could have very well included many additional data points, such as the historical datasets of a financial platform, such as the US stock markets.</li>
				<li><strong class="bold">Tokenizer</strong>: In this layer, the news article would be converted into tokens and encoded into a <a id="_idIndexMarker242"/>vectorized service (more on this in <a href="B21443_04.xhtml#_idTextAnchor070"><em class="italic">Chapter 4</em></a>, RAGs to Riches).</li>
				<li><strong class="bold">Encoded input</strong>: The<a id="_idIndexMarker243"/> encoder takes each tokenized section as input and processes and prepares the encoding for the LLM summarization model.</li>
				<li><strong class="bold">Summarization model (an LLM)</strong>: This is the <a id="_idIndexMarker244"/>hardest working layer, where the deep learning neural network of the LLM model resides. The LLM will add relationship weights to each word to generate relevant context and, in our example of a financial news article, will summarize the article into shortened, relevant, contextual<a id="_idIndexMarker245"/> concepts.</li>
				<li><strong class="bold">Encoded output and tokenizer (decoded)</strong>: The decoder takes the processed information <a id="_idIndexMarker246"/>from the encoder and its internal state to formulate a response. This response can manifest as text, audio, or even actions for downstream use. In our example, the output is an encoded text summary of a financial news article that is still in a numerical format.</li>
				<li><strong class="bold">Output/completion</strong>: This is the<a id="_idIndexMarker247"/> information returned to you, also known as the output. In our example of a long financial news article, you now have a summarized, shortened article.</li>
			</ul>
			<p>As you can see in our preceding simple example, taking a longer article (or any other text input) as input leads to a summarized article, with all the salient point(s) highlighted in a shortened and easily digestible format. This has many relevant business and personal scenarios, and I am sure you can think of how you can apply this to your everyday tasks. This is all done due to the transformer architecture!</p>
			<p>Beyond the preceding illustration, as mentioned at the start of this section, prompts can also include outputs from other services or LLM queries, instead of direct user input. In other words, rather than a human interacting with and posing a question or prompt to an LLM model, the input into that LLM model is really just output from another completion. This allows for chaining the output from one model to the input for another model, allowing for the creation of complex and dynamic interactions, tasks, or applications.</p>
			<h1 id="_idParaDest-46"><a id="_idTextAnchor046"/>LLMs landscape, progression, and expansion</h1>
			<p>We can write many chapters on how modern LLMs have leveraged transformer model architecture, along with its explosive expansion and the numerous models being created on almost on a daily basis. However, in this last section, let’s distill the usage of LLMs and their progression thus far and also add an exciting new layer of additional expansion to the functionality of LLMs using AutoGen.</p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor047"/>Exploring the landscape of transformer architectures</h2>
			<p>With their ability to handle a myriad of tasks, transformer models have revolutionized the field of natural language processing. By tweaking their architecture, we can create different types of transformer models, each with its unique applications. Let’s delve into three prevalent types:</p>
			<ul>
				<li><strong class="bold">Models with encoders only</strong>: These <a id="_idIndexMarker248"/>models, equipped solely with an encoder, are typically employed for tasks that involve understanding the context of the input, such as text classification, sentiment analysis, and question answering. A prime example is Google’s bi-directional encoder representations from transformers (BERT). BERT stands out for its ability to understand context in both directions (left to right and right to left), thanks to its pre-training on extensive text corpora. This bi-directional context understanding makes BERT a popular choice for tasks such as sentiment analysis and named entity recognition.</li>
				<li><strong class="bold">Models with decoders only</strong>: These models exclusively utilize a decoder and are primarily used for tasks that involve generating text, such as text generation, machine translation, and summarization. GPT (generative pre-trained transformer) is a notable instance of such models. GPT is celebrated for its creative text generation capabilities, achieved through a uni-directional decoder for autoregressive language modeling. This makes GPT particularly adept at tasks such as story generation and dialogue completion.</li>
				<li><strong class="bold">Models with both encoders and decoders</strong>: These models amalgamate an encoder and a decoder, making them suitable for tasks that necessitate understanding the input and generating output. This includes tasks such as machine translation and dialogue generation. T5 (text-to-text transfer transformer) exemplifies this category. T5 presents a unified framework where every NLP task is treated as a text-to-text problem, employing both encoders and decoders. This endows T5 with remarkable versatility, enabling it to handle a wide array of tasks, from <a id="_idIndexMarker249"/>summarization to translation.</li>
			</ul>
			<p>By understanding these different types of transformer models, we can better appreciate the flexibility and power of the transformer architecture in tackling diverse NLP tasks, and this can help us select which model is best suited for a cloud solution use case.</p>
			<p>As you learn more about LLMs and where they are heading in the future in the subsequent chapters, please keep in mind these models are evolving quickly, and their support services and frameworks are evolving just as quickly. An exciting area where LLM use is both evolving and expanding is around the concept of AutoGen.</p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor048"/>AutoGen</h2>
			<p>At the time of writing, significant work is being done by Microsoft Research on the next major breakthrough: autonomous agents, or<a id="_idIndexMarker250"/> AutoGen. AutoGen hopes to take LLMs and the evolution of the transformer model architecture to the next level. The Microsoft AutoGen framework is an open source platform for building multi-agent systems using large language models; we feel that this will have a significant impact on the generative AI space.</p>
			<p>Thus, later in <a href="B21443_06.xhtml#_idTextAnchor117"><em class="italic">Chapter 6</em></a>, we will describe the concept and potential of autonomous agents driven by large language models and how they can augment human capabilities and solve complex problems. We will also show how LLM models that use AutoGen can perform tasks such as reasoning, planning, perception, self-improvement, self-evaluation, memory, personalization, and communication via the use of various prompt engineering techniques.</p>
			<p>As you might be able to conclude, the possibilities are endless once we understand how multiple large language models + AutoGen can work together in different ways, such as in hierarchies, networks, or swarms, to increase computing and reasoning power and solve more complex problems, including problems that may not even exist today!</p>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor049"/>Summary</h1>
			<p>In this chapter, we introduced the topic of generative AI and its applications, such as ChatGPT, and gave an overview of the main concepts and components involved, such as cloud computing, NLP, and the transformer model. Since its introduction in 2017, the original transformer model has expanded, leading to explosive growth in models and techniques that extend beyond only NLP-type tasks.</p>
			<p>We also briefly traced the development of NLP from RNNs and CNNs to the transformer model and explained how transformers overcome the limitations of the former models by using attention mechanisms and parallel processing. We covered how prompts, or user inputs, are processed by the transformer models to generate responses or completions using various variables and scenarios.</p>
			<p>Finally, we provided a brief overview of the LLM landscape and how various transformer architectures can be used for a variety of tasks and different use cases, along with their progression, touching on their expansion into many different areas outside the LLM models themselves, such as with AutoGen, which we will cover in depth in <a href="B21443_06.xhtml#_idTextAnchor117"><em class="italic">Chapter 6</em></a>.</p>
			<p>In the next chapter, we will discuss building domain-specific LLMs by using the concept of fine-tuning; then, we will discuss the next logical step in LLM model management and another important tool to have in your generative AI toolbox!</p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor050"/>References</h1>
			<ul>
				<li>Transformer publication: <em class="italic">Attention is All You </em><em class="italic">Need</em>; <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></li>
				<li><em class="italic">Training GPT-4 cost over $100 </em><em class="italic">million</em>; <a href="https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/">https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/</a></li>
				<li><em class="italic">Transformer Architecture: The Engine behind </em><em class="italic">ChatGPT</em>; <a href="https://tinyurl.com/6k99bw98">https://tinyurl.com/6k99bw98</a></li>
			</ul>
		</div>
	

		<div><h1 id="_idParaDest-51" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor051"/>Part 2: Techniques for Tailoring LLMs</h1>
		</div>
		<div><p>This section highlights key techniques that have emerged in recent years to customize <strong class="bold">Large Language Models</strong> (<strong class="bold">LLMs</strong>) for specific business needs, such as fine-tuning. It also addresses current challenges, including mitigating hallucinations and extending training cut-off dates, to incorporate up-to-date information through methods such as <strong class="bold">Retrieval Augmented Generation</strong> (<strong class="bold">RAG</strong>). Additionally, we will explore prompt engineering techniques to enhance effective communication with AI.</p>
			<p>This part contains the following chapters:</p>
			<ul>
				<li><a href="B21443_03.xhtml#_idTextAnchor052"><em class="italic">Chapter 3</em></a>, <em class="italic">Fine Tuning: Building Domain-Specific LLM Applications</em></li>
				<li><a href="B21443_04.xhtml#_idTextAnchor070"><em class="italic">Chapter 4</em></a>, <em class="italic">RAGs to Riches: Elevating AI with External Data</em></li>
				<li><a href="B21443_05.xhtml#_idTextAnchor098"><em class="italic">Chapter 5</em></a>, <em class="italic">Effective Prompt Engineering Strategies: Unlocking Wisdom Through AI</em></li>
			</ul>
		</div>
		<div><div></div>
		</div>
		<div><div></div>
		</div>
	</body></html>