["```py\n    java –cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter6.SimpleEditDistance\n\n    ```", "```py\n    Enter the first string:\n    ab\n    Enter the second string:\n    ba\n    Allowing Transposition Distance between: ab and ba is 1.0\n    No Transposition Distance between: ab and ba is 2.0\n\n    ```", "```py\npublic static void main(String[] args) throws IOException {\n\n  EditDistance dmAllowTrans = new EditDistance(true);\n  EditDistance dmNoTrans = new EditDistance(false);\n```", "```py\nBufferedReader reader = new BufferedReader(new InputStreamReader(System.in));\nwhile (true) {\n  System.out.println(\"Enter the first string:\");\n  String text1 = reader.readLine();\n  System.out.println(\"Enter the second string:\");\n  String text2 = reader.readLine();\n  double allowTransDist = dmAllowTrans.distance(text1, text2);\n  double noTransDist = dmNoTrans.distance(text1, text2);\n  System.out.println(\"Allowing Transposition Distance \" +\" between: \" + text1 + \" and \" + text2 + \" is \" + allowTransDist);\n  System.out.println(\"No Transposition Distance between: \" + text1 + \" and \" + text2 + \" is \" + noTransDist);\n}\n}\n```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter6.SimpleWeightedEditDistance\n\n    ```", "```py\ndouble matchWeight = 0;\ndouble deleteWeight = -2;\ndouble insertWeight = -2;\ndouble substituteWeight = -2;\ndouble transposeWeight = Double.NEGATIVE_INFINITY;\nWeightedEditDistance wed = new FixedWeightEditDistance(matchWeight,deleteWeight,insertWeight,substituteWeight,transposeWeight);\nSystem.out.println(\"Fixed Weight Edit Distance: \"+ wed.toString());\n```", "```py\npublic static class CustomWeightedEditDistance extends WeightedEditDistance{\n\n  @Override\n  public double deleteWeight(char arg0) {\n    return (Character.isDigit(arg0)||Character.isLetter(arg0)) ? -1 : 0;\n\n  }\n\n  @Override\n  public double insertWeight(char arg0) {\n    return deleteWeight(arg0);\n  }\n\n  @Override\n  public double matchWeight(char arg0) {\n    return 0;\n  }\n\n  @Override\n  public double substituteWeight(char cDeleted, char cInserted) {\n    return Character.toLowerCase(cDeleted) == Character.toLowerCase(cInserted) ? 0 :-1;\n\n  }\n\n  @Override\n  public double transposeWeight(char arg0, char arg1) {\n    return Double.NEGATIVE_INFINITY;\n  }\n\n}\n```", "```py\nproximity  = count(common tokens)/count(total tokens)\ndistance = 1 – proximity\n```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter6.JaccardDistanceSample\n\n    ```", "```py\n    Enter the first string:\n    Mimsey Were the Borogroves\n    Enter the second string:\n    All mimsy were the borogoves,\n\n    IndoEuropean Tokenizer\n    Text1 Tokens: {'Mimsey''Were''the'}\n    Text2 Tokens: {'All''mimsy''were''the''borogoves'}\n    IndoEuropean Jaccard Distance is 0.8888888888888888\n\n    Character Tokenizer\n    Text1 Tokens: {'M''i''m''s''e''y''W''e''r''e''t''h''e''B''o''r''o''g''r''o''v''e'}\n    Text2 Tokens: {'A''l''l''m''i''m''s''y''w''e''r''e''t''h''e''b''o''r''o''g''o''v''e''s'}\n    Character Jaccard Distance between is 0.42105263157894735\n\n    EnglishStopWord Tokenizer\n    Text1 Tokens: {'Mimsey''Were'}\n    Text2 Tokens: {'All''mimsy''borogoves'}\n    English Stopword Jaccard Distance between is 1.0\n\n    ```", "```py\nTokenizerFactory indoEuropeanTf = IndoEuropeanTokenizerFactory.INSTANCE;\n\nTokenizerFactory characterTf = CharacterTokenizerFactory.INSTANCE;\n\nTokenizerFactory englishStopWordTf = new EnglishStopTokenizerFactory(indoEuropeanTf);\n```", "```py\nJaccardDistance jaccardIndoEuropean = new JaccardDistance(indoEuropeanTf);\nJaccardDistance jaccardCharacter = new JaccardDistance(characterTf);\n\nJaccardDistance jaccardEnglishStopWord = new JaccardDistance(englishStopWordTf);\n```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter6.TfIdfSearch\n    Reading search index from data/disney.csv\n    Getting IDF data from data/connecticut_yankee_king_arthur.txt\n    enter a query:\n\n    ```", "```py\n    I want to go to disney world\n    0.86 : I want to go to Disneyworld\n    0.86 : I want to go to disneyworld\n    0.75 : I just want to go to DisneyWorld...\n    0.75 : I just want to go to Disneyworld ???\n    0.65 : Cause I wanna go to Disneyworld.\n    0.56 : I wanna go to disneyworld with Demi\n    0.50 : I wanna go back to disneyworld\n    0.50 : I so want to go to Disneyland I've never been. I've been to Disneyworld in Florida.\n    0.47 : I want to go to #DisneyWorld again... It's so magical!!\n    0.45 : I want to go to DisneyWorld.. Never been there :( #jadedchildhood\n\n    ```", "```py\npublic static void main(String[] args) throws IOException {\n  String searchableDocs = args.length > 0 ? args[0] : \"data/disneyWorld.csv\";\n  System.out.println(\"Reading search index from \" + searchableDocs);\n\n  String idfFile = args.length > 1 ? args[1] : \"data/connecticut_yankee_king_arthur.txt\";\n  System.out.println(\"Getting IDF data from \" + idfFile);\n```", "```py\nTokenizerFactory tokFact = IndoEuropeanTokenizerFactory.INSTANCE;\nTfIdfDistance tfIdfDist = new TfIdfDistance(tokFact);\n```", "```py\nString training = Files.readFromFile(new File(idfFile), Strings.UTF8);\nfor (String line: training.split(\"\\\\.\")) {\n  tfIdfDist.handle(line);\n}\n```", "```py\nList<String[]> docsToSearch = Util.readCsvRemoveHeader(new File(searchableDocs));\n```", "```py\nBufferedReader reader = new BufferedReader(new InputStreamReader(System.in));\nwhile (true) {\n  System.out.println(\"enter a query: \");\n  String query = reader.readLine();\n```", "```py\nObjectToDoubleMap<String> scoredMatches = new ObjectToDoubleMap<String>();\nfor (String [] line : docsToSearch) {\n  scoredMatches.put(line[Util.TEXT_OFFSET], tfIdfDist.proximity(line[Util.TEXT_OFFSET], query));\n}\n```", "```py\nList<String> rankedDocs = scoredMatches.keysOrderedByValueList();\nfor (int i = 0; i < 10; ++i) {\n  System.out.printf(\"%.2f : \", scoredMatches.get(rankedDocs.get(i)));\n  System.out.println(rankedDocs.get(i));\n}\n}\n```", "```py\ndidYouMean(received) = ArgMaxintended P(intended | received) \n= ArgMaxintended P(intended,received) / P(received) \n= ArgMaxintended P(intended,received) \n= ArgMaxintended P(intended) * P(received | intended)\n```", "```py\n    java -Xmx1g -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter6.SpellCheck \n\n    ```", "```py\n    Enter word, . to quit:\n    >beleive\n    Query Text: beleive\n    Best Alternative: believe\n    Nbest: 0: believe Score:-13.97322991490364\n    Nbest: 1: believed Score:-17.326215342327487\n    Nbest: 2: believes Score:-20.8595682233572\n    Nbest: 3: because Score:-21.468056442099623\n\n    ```", "```py\n    The rain in Spani falls mainly on the plain.\n    Query Text: The rain in Spani falls mainly on the plain.\n    Best Alternative: the rain in spain falls mainly on the plain .\n    Nbest: 0: the rain in spain falls mainly on the plain . Score:-96.30435947472415\n    Nbest: 1: the rain in spain falls mainly on the plan . Score:-100.55447634639404\n    Nbest: 2: the rain in spain falls mainly on the place . Score:-101.32592701496742\n    Nbest: 3: the rain in spain falls mainly on the plain , Score:-101.81294112237359\n\n    ```", "```py\npublic static void main(String[] args) throws IOException, ClassNotFoundException {\n  double matchWeight = -0.0;\n  double deleteWeight = -4.0;\n  double insertWeight = -2.5;\n  double substituteWeight = -2.5;\n  double transposeWeight = -1.0;\n\n  FixedWeightEditDistance fixedEdit = new FixedWeightEditDistance(matchWeight,deleteWeight,insertWeight,substituteWeight,transposeWeight);\n  int NGRAM_LENGTH = 6;\n  NGramProcessLM lm = new NGramProcessLM(NGRAM_LENGTH);\n\n  TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;\n  tokenizerFactory = new com.aliasi.tokenizer.LowerCaseTokenizerFactory(tokenizerFactory);\n```", "```py\nTrainSpellChecker sc = new TrainSpellChecker(lm,fixedEdit,tokenizerFactory);\n```", "```py\nFile inFile = new File(\"data/project_gutenberg_books.txt\");\nString bigEnglish = Files.readFromFile(inFile,Strings.UTF8);\nsc.handle(bigEnglish);\n```", "```py\nFile dict = new File(\"data/websters_words.txt\");\nString webster = Files.readFromFile(dict, Strings.UTF8);\nsc.handle(webster);\n```", "```py\nCompiledSpellChecker csc = (CompiledSpellChecker) AbstractExternalizable.compile(sc);\n```", "```py\nSet<String> dontEdit = new HashSet<String>();\ndontEdit.add(\"lingpipe\");\ncsc.setDoNotEditTokens(dontEdit);\n```", "```py\ncsc.setTokenizerFactory(tokenizerFactory);\nint nBest = 3;\ncsc.setNBest(64);\n```", "```py\nBufferedReader reader = new BufferedReader(new InputStreamReader(System.in));\nString query = \"\";\nwhile (true) {\n  System.out.println(\"Enter word, . to quit:\");\n  query = reader.readLine();\n  if (query.equals(\".\")){\n    break;\n  }\n  String bestAlternative = csc.didYouMean(query);\n  System.out.println(\"Best Alternative: \" + bestAlternative);\n  int i = 0;\n  Iterator<ScoredObject<String>> iterator = csc.didYouMeanNBest(query);\n  while (i < nBest) {\n    ScoredObject<String> so = iterator.next();\n    System.out.println(\"Nbest: \" + i + \": \" + so.getObject() + \" Score:\" + so.score());\n    i++;\n  }\n}\n```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter6.CaseRestore \n\n    ```", "```py\n    Enter input, . to quit:\n    george washington was the first president of the u.s.a\n    Best Alternative: George Washington was the first President of the U.S.A\n    Enter input, . to quit:\n    ITS RUDE TO SHOUT ON THE WEB\n    Best Alternative: its rude to shout on the Web\n\n    ```", "```py\nint NGRAM_LENGTH = 5;\nNGramProcessLM lm = new NGramProcessLM(NGRAM_LENGTH);\nTrainSpellChecker sc = new TrainSpellChecker(lm,CompiledSpellChecker.CASE_RESTORING);\n```", "```py\nString bestAlternative = csc.didYouMean(query);\n```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter6.AutoComplete \n\n    ```", "```py\n    Enter word, . to quit:\n    new\n    |new|\n    -13.39 New York,New York\n    -17.89 New Orleans,Louisiana\n    -18.30 Newark,New Jersey\n    -18.92 Newport News,Virginia\n    -19.39 New Haven,Connecticut\n    If we misspell 'new' and type 'mew' instead, \n    Enter word, . to quit:\n    mew \n\n    |mew |\n    -13.39 New York,New York\n    -17.89 New Orleans,Louisiana\n    -19.39 New Haven,Connecticut\n\n    ```", "```py\n    Enter word, . to quit:\n    Alta,Wyoming\n    |Alta,Wyoming|\n\n    ```", "```py\nFile wordsFile = new File(\"data/city_populations_2012.csv\");\nString[] lines = FileLineReader.readLineArray(wordsFile,\"ISO-8859-1\");\nObjectToCounterMap<String> cityPopMap = new ObjectToCounterMap<String>();\nint lineCount = 0;\nfor (String line : lines) {\nif(lineCount++ <1) continue;\n  int i = line.lastIndexOf(',');\n  if (i < 0) continue;\n  String phrase = line.substring(0,i);\n  String countString = line.substring(i+1);\n  Integer count = Integer.valueOf(countString);\n\n  cityPopMap.set(phrase,count);\n}\n```", "```py\ndouble matchWeight = 0.0;\ndouble insertWeight = -10.0;\ndouble substituteWeight = -10.0;\ndouble deleteWeight = -10.0;\ndouble transposeWeight = Double.NEGATIVE_INFINITY;\nFixedWeightEditDistance editDistance = new FixedWeightEditDistance(matchWeight,deleteWeight,insertWeight,substituteWeight,transposeWeight);\n```", "```py\nint maxResults = 5;\nint maxQueueSize = 10000;\ndouble minScore = -25.0;\nAutoCompleter completer = new AutoCompleter(cityPopMap, editDistance,maxResults, maxQueueSize, minScore);\n```", "```py\n3.0\n 2.0\n 1.0\n aaa\n aa\n aaaaa\n 1.0\n bbbb\n bbb\n\n```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter6.HierarchicalClustering\n\n    ```", "```py\n    public static void main(String[] args) throws UnsupportedEncodingException, IOException {\n\n      Set<String> inputSet = new HashSet<String>();\n      String [] input = { \"aa\", \"aaa\", \"aaaaa\", \"bbb\", \"bbbb\" };\n      inputSet.addAll(Arrays.asList(input));\n    ```", "```py\n    boolean allowTranspositions = false;\n    Distance<CharSequence> editDistance = new EditDistance(allowTranspositions);\n\n    AbstractHierarchicalClusterer<String> slClusterer = new SingleLinkClusterer<String>(editDistance);\n\n    Dendrogram<String> slDendrogram = slClusterer.hierarchicalCluster(inputSet);\n\n    System.out.println(\"\\nSingle Link Dendrogram\");\n    System.out.println(slDendrogram.prettyPrint());\n    ```", "```py\n    Single Link Dendrogram\n\n    3.0\n     2.0\n     1.0\n     aaa\n     aa\n     aaaaa\n     1.0\n     bbbb\n     bbb\n\n    ```", "```py\n    AbstractHierarchicalClusterer<String> clClusterer = new CompleteLinkClusterer<String>(editDistance);\n\n    Dendrogram<String> clDendrogram = clClusterer.hierarchicalCluster(inputSet);\n\n    System.out.println(\"\\nComplete Link Dendrogram\");\n    System.out.println(clDendrogram.prettyPrint());\n    ```", "```py\n    Complete Link Dendrogram\n\n    5.0\n     3.0\n     1.0\n     aaa\n     aa\n     aaaaa\n     1.0\n     bbbb\n     bbb\n\n    ```", "```py\n    System.out.println(\"\\nSingle Link Clusterings with k Clusters\");\n    for (int k = 1; k < 6; ++k ) {\n      Set<Set<String>> slKClustering = slDendrogram.partitionK(k);\n      System.out.println(k + \"  \" + slKClustering);\n    }\n    ```", "```py\n    Single Link Clusterings with k Clusters\n    1  [[bbbb, aaa, aa, aaaaa, bbb]]\n    2  [[aaa, aa, aaaaa], [bbbb, bbb]]\n    3  [[aaaaa], [bbbb, bbb], [aaa, aa]]\n    4  [[bbbb, bbb], [aa], [aaa], [aaaaa]]\n    5  [[bbbb], [aa], [aaa], [aaaaa], [bbb]]\n\n    ```", "```py\n    Set<Set<String>> slClustering = slClusterer.cluster(inputSet);\n    System.out.println(\"\\nComplete Link Clustering No \" + \"Max Distance\");\n    System.out.println(slClustering + \"\\n\");\n    ```", "```py\n    Complete Link Clustering No Max Distance\n    [[bbbb, aaa, aa, aaaaa, bbb]]\n\n    ```", "```py\n    for(int k = 1; k < 6; ++k ){\n      clClusterer.setMaxDistance(k);\n      System.out.println(\"Complete Link Clustering at \" + \"Max Distance= \" + k);\n\n      Set<Set<String>> slClusteringMd = clClusterer.cluster(inputSet);\n      System.out.println(slClusteringMd);\n    }\n    ```", "```py\n    Complete Link Clustering at Max Distance= 1\n    [[bbbb, bbb], [aaa, aa], [aaaaa]]\n    Complete Link Clustering at Max Distance= 2\n    [[bbbb, bbb], [aaa, aa], [aaaaa]]\n    Complete Link Clustering at Max Distance= 3\n    [[bbbb, bbb], [aaa, aa, aaaaa]]\n    Complete Link Clustering at Max Distance= 4\n    [[bbbb, bbb], [aaa, aa, aaaaa]]\n    Complete Link Clustering at Max Distance= 5\n    [[bbbb, aaa, aa, aaaaa, bbb]] \n\n    ```", "```py\n    File corpusFile = new File(args[0]);\n     List<String[]> tweets = Util.readCsvRemoveHeader(corpusFile);\n    ```", "```py\n    int minTokenCount = 5;\n    ```", "```py\n    short numTopics = 10;\n    ```", "```py\n    double documentTopicPrior = 0.1;\n    ```", "```py\n    double topicWordPrior = 0.01;\n    ```", "```py\n    int burninEpochs = 0;\n    int sampleLag = 1;\n    int numSamples = 2000;\n    ```", "```py\n    long randomSeed = 6474835;\n    ```", "```py\n    SymbolTable symbolTable = new MapSymbolTable();\n    ```", "```py\n    TokenzierFactory tokFactory = IndoEuropeanTokenizerFactory.INSTANCE;\n    ```", "```py\n    System.out.println(\"Input file=\" + corpusFile);\n    System.out.println(\"Minimum token count=\" + minTokenCount);\n    System.out.println(\"Number of topics=\" + numTopics);\n    System.out.println(\"Topic prior in docs=\" + documenttopicPrior);\n    System.out.println(\"Word prior in topics=\" + wordPrior);\n    System.out.println(\"Burnin epochs=\" + burninEpochs);\n    System.out.println(\"Sample lag=\" + sampleLag);\n    System.out.println(\"Number of samples=\" + numSamples);\n    ```", "```py\n    int[][] docTokens = LatentDirichletAllocation.tokenizeDocuments(IdaTexts,tokFactory,symbolTable, minTokenCount);\n    System.out.println(\"Number of unique words above count\" + \" threshold=\" + symbolTable.numSymbols());\n    ```", "```py\n    int numTokens = 0;\n    for (int[] tokens : docTokens){\n      numTokens += tokens.length;\n    }\n    System.out.println(\"Tokenized.  #Tokens After Pruning=\" + numTokens);\n    ```", "```py\n    LdaReportingHandler handler = new LdaReportingHandler(symbolTable);\n    ```", "```py\n    public void handle(LatentDirichletAllocation.GibbsSample sample) {\n      System.out.printf(\"Epoch=%3d   elapsed time=%s\\n\", sample.epoch(), Strings.msToString(System.currentTimeMillis() - mStartTime));\n\n      if ((sample.epoch() % 10) == 0) {\n        double corpusLog2Prob = sample.corpusLog2Probability();\n        System.out.println(\"      log2 p(corpus|phi,theta)=\" + corpusLog2Prob + \"   token cross\" + entropy rate=\" + (-corpusLog2Prob/sample.numTokens()));\n      }\n    }\n    ```", "```py\n    LatentDirichletAllocation.GibbsSample sample = LatentDirichletAllocation.gibbsSampler(docTokens, numTopics,documentTopicPrior,wordPrior,burninEpochs,sampleLag,numSamples,new Random(randomSeed),handler);\n    ```", "```py\n    int maxWordsPerTopic = 20;\n    int maxTopicsPerDoc = 10;\n    boolean reportTokens = true;\n    handler.reportTopics(sample,maxWordsPerTopic,maxTopicsPerDoc,reportTokens);\n    ```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter6.LDA\n\n    ```", "```py\n    Input file=data/gravity_tweets.csv\n    Minimum token count=1\n    Number of topics=10\n    Topic prior in docs=0.1\n    Word prior in topics=0.01\n    Burnin epochs=0\n    Sample lag=1\n    Number of samples=2000\n    Number of unique words above count threshold=1652\n    Tokenized.  #Tokens After Pruning=10101\n    Epoch=  0   elapsed time=:00\n     log2 p(corpus|phi,theta)=-76895.71967475882 \n     token cross-entropy rate=7.612683860484983\n    Epoch=  1   elapsed time=:00\n\n    ```", "```py\n    TOPIC 0  (total count=1033)\n               WORD    COUNT        Z\n    --------------------------------------------------\n              movie      109       4.0\n            Gravity       73       1.9\n                  a       72       6.0\n                 is       57       4.9\n                  !       52       3.2\n                was       45       6.0\n                  .       42      -0.4\n                  ?       41       5.8\n               good       39       5.6\n    ```", "```py\n    TOPIC 1  (total count=1334)\n     WORD    COUNT        Z\n    --------------------------------------------------\n     /      144       2.2\n     .      117       2.5\n     #       91       3.5\n     @       73       4.2\n     :       72       1.0\n     !       50       2.7\n     co       49       1.3\n     t       47       0.8\n     http       47       1.2\n\n    ```", "```py\n    OPIC 18  (total count=115)\n     WORD    COUNT        Z\n    --------------------------------------------------\n     movie       24       1.0\n     the       24       1.3\n     of       15       1.7\n     best       10       3.0\n     ever        9       2.8\n     one        9       2.8\n     I've        8       2.7\n     seen        7       1.8\n     most        4       1.4\n     it's        3       0.9\n     had        1       0.2\n     can        1       0.2\n\n    ```", "```py\n    DOC 34\n    TOPIC    COUNT    PROB\n    ----------------------\n     18        3   0.270\n     4        2   0.183\n     3        1   0.096\n     6        1   0.096\n     8        1   0.096\n     19        1   0.096\n\n    Gravity(4) is(6) the(8) best(18) movie(19) I've(18) seen(18) in(3) a(4)\n\n    DOC 50\n    TOPIC    COUNT    PROB\n    ----------------------\n     18        6   0.394\n     17        4   0.265\n     5        2   0.135\n     7        1   0.071\n\n    The(17) movie(18) Gravity(7) has(17) to(17) be(5) one(18) of(18) the(18) best(18) of(18) all(17) time(5)\n\n    ```"]