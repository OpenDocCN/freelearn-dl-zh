- en: <st c="0">7</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2">The Key Role Vectors and Vector Stores Play in RAG</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**<st c="52">Vectors</st>** <st c="60">are a</st> <st c="66">key component
    of</st> **<st c="84">retrieval-augmented generation</st>** <st c="114">(</st>**<st
    c="116">RAG</st>**<st c="119">) to understand, as they are the secret ingredient
    that helps the entire process work well.</st> <st c="212">In this chapter, we
    dive back into our code from previous chapters with an emphasis on how it is impacted
    by vectors.</st> <st c="330">In simplistic terms, this chapter will talk about
    what a vector is, how vectors are created, and</st> <st c="426">then where to
    store them.</st> <st c="453">In more technical terms, we will talk about vectors,</st>
    **<st c="506">vectorization</st>**<st c="519">, and</st> **<st c="525">vector
    stores</st>**<st c="538">. This chapter is all about vector creation and why</st>
    <st c="590">they are important.</st> <st c="610">We are going to focus on how
    vectors relate to RAG, but we encourage you to spend more time and research gaining
    as in-depth of an understanding about vectors as you can.</st> <st c="781">The
    more you understand vectors, the more effective you will be at improving your</st>
    <st c="863">RAG pipelines.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="877">The vector discussion is so important, though, that we will span
    it across two chapters.</st> <st c="967">While this chapter focuses on vectors
    and vector stores,</st> [*<st c="1024">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)
    <st c="1033">will focus on vector searches, which is to say how the vectors are
    used in a</st> <st c="1111">RAG system.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1122">Specifically, we will cover the following topics in</st> <st c="1175">this
    chapter:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1188">Fundamentals of vectors</st> <st c="1213">in RAG</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1219">Where vectors lurk in</st> <st c="1242">your code</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1251">The amount of text you</st> <st c="1275">vectorize matters!</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1293">Not all semantics are</st> <st c="1316">created equal!</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1330">Common</st> <st c="1338">vectorization techniques</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1362">Selecting a</st> <st c="1375">vectorization option</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1395">Getting started with</st> <st c="1417">vector stores</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1430">Vector stores</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1444">Choosing a</st> <st c="1456">vector store</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1468">Technical requirements</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1491">Going back to the code we have discussed over the past chapters,
    this chapter focuses on just this line</st> <st c="1596">of code:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: <st c="1689">The code for this chapter is</st> <st c="1719">here:</st> [<st
    c="1725">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_07</st>](https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_07
    )
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1822">The filename</st> <st c="1836">is</st> `<st c="1839">CHAPTER7-1_COMMON_VECTORIZATION_TECHNIQUES.ipynb</st>`<st
    c="1887">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1888">And</st> [*<st c="1893">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)
    <st c="1902">will focus on just this line</st> <st c="1932">of code:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: <st c="1979">Is that it?</st> <st c="1992">Just those two lines of code for
    two chapters?</st> <st c="2039">Yes!</st> <st c="2044">That shows you how important
    vectors are to the RAG system.</st> <st c="2104">And to thoroughly understand
    vectors, we start with the fundamentals and build up</st> <st c="2186">from there.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2197">Let’s</st> <st c="2204">get started!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2216">Fundamentals of vectors in RAG</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2247">In this</st> <st c="2256">section, we</st> <st c="2267">will cover
    several important topics related to vectors and embeddings in the context of</st>
    **<st c="2356">natural language processing</st>** <st c="2383">(</st>**<st c="2385">NLP</st>**<st
    c="2388">) and RAG.</st> <st c="2400">We will begin by clarifying the relationship
    between vectors and embeddings, explaining that embeddings are a specific type
    of vector representation used in NLP.</st> <st c="2562">We then discuss the properties
    of vectors, such as their dimensions and size, and how these characteristics impact
    the precision and effectiveness of text search and</st> <st c="2728">similarity
    comparisons.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2751">What is the difference between embeddings and vectors?</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="2806">Vectors</st> <st c="2815">and</st> **<st c="2819">embeddings</st>**
    <st c="2829">are key concepts in NLP and play a crucial role in building language
    models and RAG systems.</st> <st c="2923">But what are they and how do they relate
    to each other?</st> <st c="2979">To put it simply, you can think of embeddings
    as a specific type of vector representation.</st> <st c="3070">When we are talking
    about the</st> **<st c="3100">large language models</st>** <st c="3121">(</st>**<st
    c="3123">LLMs</st>**<st c="3127">) we</st> <st c="3133">use in RAG, which are
    part of a larger universe called NLP, the vectors we use are referred to as embeddings.</st>
    <st c="3243">Vectors on the other hand, in general, are used</st> <st c="3291">across
    a broad variety of fields and can represent many other objects beyond just language
    constructs (such as words, sentences, paragraphs, etc.).</st> <st c="3439">When
    talking about RAG, words such as embeddings, vectors, vector embeddings, and embedding
    vectors can be</st> <st c="3546">used interchangeably!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3567">Now that we have that out of the way, let’s talk about what a vector</st>
    <st c="3637">actually is.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3649">What is a vector?</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="3667">What is</st> <st c="3676">the first thing you think of when you
    hear the word</st> *<st c="3728">vector</st>*<st c="3734">? Many people would
    say math.</st> <st c="3764">That would be accurate; vectors are literally mathematical
    representations of the text we work with in our data, and they allow us to apply
    mathematical operations to our data in new and very</st> <st c="3956">useful ways.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3968">The word</st> *<st c="3978">vector</st>* <st c="3984">might also
    make you think of speed.</st> <st c="4021">That is also accurate; with vectors,
    we can conduct text search at significantly faster speeds than with any other
    technology that preceded</st> <st c="4161">vector search.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4175">Another concept that is often associated with the word</st> *<st
    c="4231">vector</st>* <st c="4237">is precision.</st> <st c="4252">By converting
    text into embeddings that have semantic representation, we can significantly improve
    the precision of our search systems in finding what we are</st> <st c="4410">looking
    for.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4422">And of course, if you are a fan of the movie</st> *<st c="4468">Despicable
    Me</st>* <st c="4481">from Illumination, you may think of the villain Vector,
    who describes himself as “</st>*<st c="4564">I go by the name of… Vector.</st>
    <st c="4594">It’s a mathematical term, a quantity represented by an arrow with
    both direction</st>* *<st c="4675">and magnitude</st>*<st c="4688">.”</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4690">He may be a villain doing questionable things, but he is right
    about the meaning behind his name!</st> <st c="4789">The key thing to take away
    from this description is that a vector is not just a bunch of numbers; it is a
    mathematical object that represents both magnitude and direction.</st> <st c="4961">This
    is why it does a better job of representing your text and similarities between
    text, as it captures a more complex form of them than just</st> <st c="5104">simple
    numbers.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5119">This may give you an understanding of what a vector is, but let’s
    next discuss the important aspects of vectors that will have an impact on your
    RAG development, starting with</st> <st c="5296">vector size.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5308">Vector dimensions and size</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="5335">Vector, the</st> <st c="5348">villain from</st> *<st c="5361">Despicable
    Me</st>*<st c="5374">, said that vectors are “</st>*<st c="5399">a quantity represented
    by an arrow</st>*<st c="5434">.” But while thinking of arrows representing vectors
    on a 2D or 3D graph makes it easier to comprehend what a vector is, it is important
    to</st> <st c="5573">understand that the vectors we work with are often represented
    in many more than just two or three dimensions.</st> <st c="5685">The number of
    dimensions in the vector is also referred to as the vector size.</st> <st c="5764">To
    see this in our code, we are going to add a new cell right below where we define
    our variables.</st> <st c="5863">This code will print out a small section of the</st>
    `<st c="5911">embedding</st>` <st c="5920">vector:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: <st c="6153">In this code, we take the question that we have used throughout
    our code examples,</st> `<st c="6237">What are the advantages of using RAG?</st>`<st
    c="6274">, and we use OpenAI’s embedding API to convert it into a vector representation.</st>
    <st c="6354">The</st> `<st c="6358">question_embedding</st>` <st c="6376">variable
    represents this embedding.</st> <st c="6413">Using a slice,</st> `<st c="6428">[0:5]</st>`<st
    c="6433">, we take the first five numbers from</st> `<st c="6471">question_embedding</st>`<st
    c="6489">, which represent the first five dimensions of the vector, and print
    them out.</st> <st c="6568">The full vector is 1,536 float numbers with 17–20
    digits each, so we will minimize how much is printed out to make it a little more
    manageable to read.</st> <st c="6720">The output of this cell will look</st> <st
    c="6754">like this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: <st c="6918">We only print out the first five dimensions here, but the embedding
    is much larger than that.</st> <st c="7013">We will talk about a practical way
    to determine the total number of dimensions in a moment, but first I want to draw
    your attention to the length of</st> <st c="7162">each number.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7174">All numbers in these embeddings will be +/-0 with a decimal point,
    so let’s talk about the number of digits that come after that decimal point.</st>
    <st c="7319">The first number here,</st> `<st c="7342">-0.006319054113595048</st>`<st
    c="7363">, has 18 digits after the decimal point, the second number has 19, and
    the fourth number has 17\.</st> <st c="7460">These digit lengths are related to
    the precision of the floating-point representation used by OpenAI’s embeddings
    model,</st> `<st c="7581">OpenAIEmbeddings</st>`<st c="7597">. This model uses
    what is considered a high-precision floating-point format, providing 64-bit numbers
    (also known as</st> **<st c="7714">double-precision</st>**<st c="7730">).</st>
    <st c="7734">This high-precision results in</st> <st c="7765">the potential for
    very fine-grained distinctions and accurate representation of the semantic information
    captured by the</st> <st c="7886">embedding model.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7902">In addition, let’s revisit a point made in</st> [*<st c="7946">Chapter
    1</st>*](B22475_01.xhtml#_idTextAnchor015)<st c="7955">, that the preceding output
    looks a lot like a Python list of floating points.</st> <st c="8034">It actually
    is a Python list in this case, as that is what OpenAI returns from their embedding
    API.</st> <st c="8134">This is probably a decision to make it more compatible
    with the Python coding world.</st> <st c="8219">But to avoid confusion, it is
    important to understand that, typically in the machine learning world, when you
    see something like this in use that will be used for machine learning-related
    processing, it is typically a NumPy array, even though a list of numbers and a
    NumPy array look the same when printed out as output like we</st> <st c="8547">just
    did.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8556">Fun fact</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8565">You will eventually hear about the</st> <st c="8600">concept called</st>
    **<st c="8616">quantization</st>** <st c="8628">if you work with generative AI.</st>
    <st c="8661">Much like embeddings, quantization deals with high-precision floating
    points.</st> <st c="8739">However, with quantization, the concept is to convert
    model parameters, such as weights and activations, from their original high-precision
    floating-point representation to a lower-precision format.</st> <st c="8938">This
    reduces the memory footprint and computational requirements of the LLM, which
    can be applied to make it more cost-effective to pre-train, train, and fine-tune
    the LLM.</st> <st c="9111">Quantization can also make it more cost-effective to
    perform inference with the LLM, which is what it is called when you use the LLM
    to get responses.</st> <st c="9262">When I say</st> *<st c="9273">cost-effective</st>*
    <st c="9287">in this context, I am referring to being able to do these things
    in a</st> <st c="9357">smaller, less expensive hardware environment.</st> <st
    c="9404">There is a trade-off, though; quantization is a</st> **<st c="9452">lossy
    compression technique</st>**<st c="9479">, which means that some of the information
    is lost during the conversion process.</st> <st c="9561">The reduced precision
    of the quantized LLMs may result in a loss of accuracy compared to the original</st>
    <st c="9663">high-precision LLMs.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9683">When you are using RAG and considering different algorithms to
    convert your text into embeddings, take note of the length of the embedding values
    to make sure you are using a high-precision floating-point format if the accuracy
    and quality of response are of high priority in your</st> <st c="9965">RAG system.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9976">But how many dimensions are represented by these embeddings?</st>
    <st c="10038">We only show five in the preceding example, but we could have printed
    them all out and counted them individually.</st> <st c="10152">This, of course,
    seems impractical.</st> <st c="10188">We will use the</st> `<st c="10204">len()</st>`
    <st c="10209">function to do the counting for us.</st> <st c="10246">In the following
    code, you see that helpful function put to good use, giving us the total size
    of</st> <st c="10344">this embedding:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: <st c="10443">The output of this code is</st> <st c="10471">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: <st c="10503">This indicates that this embedding is 1,536 dimensions!</st> <st
    c="10560">Trying to visualize this in your mind is difficult when we typically
    only think in 3 dimensions at most, but these extra 1,533 dimensions make a significant
    difference in how precise our embedding semantic representations of the related
    text</st> <st c="10802">can be.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10809">When working with vectors across most modern vectorization algorithms,
    there are often hundreds, or thousands, of dimensions.</st> <st c="10936">The
    number of dimensions is equal to the number of floating points that represent
    the embedding, meaning a 1,024-dimension vector is represented by 1,024 floating
    points.</st> <st c="11107">There is no hard limit to how long an embedding can
    be, but some of the modern vectorizing algorithms tend to have preset sizes.</st>
    <st c="11236">The model we are using, OpenAI’s</st> `<st c="11269">ada</st>` <st
    c="11272">embedding model, uses 1,536 by default.</st> <st c="11313">This is because
    it is trained to produce a certain-sized embedding, and if you try to truncate
    that size, it changes the context captured in</st> <st c="11454">the embedding.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11468">However, this is changing.</st> <st c="11496">New vectorizers
    are now available (such as the OpenAI</st> `<st c="11550">text-embedding-3-large</st>`
    <st c="11572">model) that enable you to change vector sizes.</st> <st c="11620">These
    embedding models were trained to provide the same context, relatively speaking
    across the different vector dimension sizes.</st> <st c="11750">This enables a</st>
    <st c="11765">technique called</st> **<st c="11782">adaptive retrieval</st>**<st
    c="11800">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11801">With adaptive retrieval, you generate multiple sets of embeddings
    at different sizes.</st> <st c="11888">You first search the lower-dimension vectors
    to get you</st> *<st c="11944">close</st>* <st c="11949">to the final results,
    because searching lower-dimension vectors is much faster than searching higher-dimension
    vectors.</st> <st c="12070">Once your lower-dimension search gets you into proximity
    of the content most similar to your input inquiry, your search</st> *<st c="12190">adapts</st>*
    <st c="12196">to searching the slower search-speed, higher-dimension embeddings
    to target the most relevant content and finalize the similarity search.</st> <st
    c="12335">Overall, this can increase your search speeds by 30–90%, depending on
    how you set up the search.</st> <st c="12432">The embeddings generated by this
    technique are called</st> **<st c="12486">Matryoshka embeddings</st>**<st c="12507">,
    named</st> <st c="12514">after the Russian nesting dolls, reflecting that the
    embeddings, like the dolls, are all relatively identical to each other while varying
    in size.</st> <st c="12662">If you ever need to optimize a RAG pipeline in a production
    environment for heavy usage, you are going to want to consider</st> <st c="12785">this
    technique.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12800">The next concept that will be important for you to understand
    is where in the code your vectors reside, helping you to apply the concepts you
    are learning about vectors directly to your</st> <st c="12987">RAG efforts.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12999">Where vectors lurk in your code</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="13031">One way to</st> <st c="13043">indicate the value of vectors in
    the RAG system is to show you all the places they are used.</st> <st c="13136">As
    discussed earlier, you start with your text data and convert it to vectors during
    the vectorization process.</st> <st c="13248">This occurs in the indexing stage
    of the RAG system.</st> <st c="13301">But, in most cases, you must have somewhere
    to put those embedding vectors, which brings in the concept of the</st> <st c="13412">vector
    store.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13425">During the retrieval stage of the RAG system, you start with a
    question as input from the user, which is first converted to an embedding vector
    before the retrieval begins.</st> <st c="13599">Lastly, the retrieval process
    uses a similarity algorithm that determines the proximity between the question
    embedding and all the embeddings in the vector store.</st> <st c="13762">There
    is one more potential area in which vectors are common and that is when you want
    to evaluate your RAG responses, but we will cover that in</st> [*<st c="13907">Chapter
    9</st>*](B22475_09.xhtml#_idTextAnchor184) <st c="13916">when we cover evaluation
    techniques.</st> <st c="13954">For now, let’s dive deeper into each of these other
    concepts, starting</st> <st c="14025">with vectorization.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14044">Vectorization occurs in two places</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="14079">At the very front of</st> <st c="14101">the RAG process, you typically
    have a mechanism for a user to enter a question that is passed to the retriever.</st>
    <st c="14213">We see the processing of this occurring in our</st> <st c="14260">code
    here:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: <st c="14406">The</st> <st c="14410">retriever is a LangChain</st> `<st c="14436">retriever</st>`
    <st c="14445">object that facilitates similarity search and retrieval of relevant
    vectors based on the user query.</st> <st c="14547">So when we talk about vectorization,
    it actually occurs in two places in</st> <st c="14620">our code:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14629">First, when we vectorize the original data that will be used in
    the</st> <st c="14698">RAG system</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="14708">Second, when we need to vectorize the</st> <st c="14747">user
    query</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '<st c="14757">The relationship between these two separate steps is that they
    are both used in the similarity search.</st> <st c="14861">Before we talk about
    the search, though, let’s first talk about where the latter group of embeddings,
    the embeddings from the original data, is stored: the</st> <st c="15017">vector
    store.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15030">Vector databases/stores store and contain vectors</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="15080">A vector store</st> <st c="15095">is typically a vector database
    (but not always, see the following note) that is optimized for storing and serving
    vectors, and plays a crucial role in an effective RAG system.</st> <st c="15272">Technically,
    you could build a RAG system without using a vector database, but you would miss
    out on a lot of the optimizations that have been built into these data storage
    tools, impacting your memory, computation requirements, and search</st> <st c="15512">precision
    unnecessarily.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15536">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15541">You often hear the term</st> **<st c="15566">vector databases</st>**
    <st c="15582">when</st> <st c="15587">referring to optimized database-like structures
    for storing vectors.</st> <st c="15657">However, there are tools and other mechanisms
    that are not databases while serving the same or similar purpose as a vector database.</st>
    <st c="15790">For this reason, we will refer to all of them in a group as</st>
    *<st c="15850">vector stores</st>*<st c="15863">. This is consistent with LangChain
    documentation, which also refers to the group in aggregate as vector stores, inclusive
    of all types of mechanisms that store and serve vectors.</st> <st c="16043">But
    you will often hear the terms used interchangeably, and the term</st> *<st c="16112">vector
    database</st>* <st c="16127">is actually the more popular term used to refer to
    all of these mechanisms.</st> <st c="16204">For the sake of accuracy and to align
    our terminology with LangChain documentation, in this book, we will use the term</st>
    *<st c="16323">vector store</st>*<st c="16335">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16336">In terms of</st> *<st c="16349">where vectors lurk in your code</st>*<st
    c="16380">, the vector store is where most of the vectors generated in</st> <st
    c="16441">your code are stored.</st> <st c="16463">When you vectorize your data,
    those embeddings go into your vector store.</st> <st c="16537">When you conduct
    a similarity search, the embeddings used to represent that data get pulled from
    the vector store.</st> <st c="16652">This makes vector stores a key player in
    the RAG system and worthy of</st> <st c="16722">our attention.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16736">Now that we know where the original data embeddings are stored,
    let’s bring this back to how they are used in relation to the user</st> <st c="16868">query
    embeddings.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16885">Vector similarity compares your vectors</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="16925">We have our two primary</st> <st c="16950">vectorization occurrences:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16976">The embedding for our</st> <st c="16999">user query</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="17009">The vector embeddings representing all the data in our</st> <st
    c="17065">vector store</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="17077">Let’s</st> <st c="17084">review how these two occurrences relate
    to each other.</st> <st c="17139">When we conduct the highly important vector
    similarity search that forms the foundation of our retrieval process, we are really
    just performing a mathematical operation that measures the distance between the
    user query embedding and the original</st> <st c="17385">data embeddings.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17401">Multiple mathematical algorithms can be used to perform this distance
    calculation, which we will review later in</st> [*<st c="17515">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="17524">. But for now, it is important to understand that this distance calculation
    identifies the closest original data embeddings to the user query embedding and
    returns the list of those embeddings in the order of their distance (sorted by
    closest to furthest).</st> <st c="17781">Our code is a bit more simplistic, in
    that the embedding represents the data points (the chunks) in a</st> <st c="17883">1:1
    relationship.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17900">But in many applications, such as with a question-and-answer chatbot
    where the questions or answers are very long and broken up into smaller chunks,
    you will likely see those chunks have a foreign key ID that refers back to a larger
    piece of content.</st> <st c="18152">That allows us to retrieve the full piece
    of content, rather than just the chunk.</st> <st c="18234">This will vary depending
    on the problem your RAG system is trying to solve, but it is important to understand
    that the architecture of this retrieval system can vary to meet the needs of</st>
    <st c="18421">the application.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="18437">This covers the most common places you find vectors in your RAG
    system: where they occur, where they are stored, and how they are used in service
    of the RAG system.</st> <st c="18603">In the next section, we talk about how the
    size of the data text we are using in the search for our RAG system can vary.</st>
    <st c="18724">You will ultimately make decisions in your code that dictate that
    size.</st> <st c="18796">But from what you already know about vectors, you may
    start to wonder, if we are vectorizing content of various sizes, how does that
    impact our ability to compare them and ultimately build the most effective retrieval
    process we can build?</st> <st c="19036">And you would be right to wonder!</st>
    <st c="19070">Let’s discuss the impact of the size of the content that we turn
    into</st> <st c="19140">embeddings next.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19156">The amount of text you vectorize matters!</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="19198">The vector we showed earlier</st> <st c="19227">came from the
    text</st> `<st c="19247">What are the advantages of using RAG?</st>`<st c="19284">.
    That is a relatively short amount of text, which means a 1,536-dimension vector
    is going to do a very thorough job representing the context within that text.</st>
    <st c="19444">But if we go back to the code, the content that we vectorize to
    represent our</st> *<st c="19522">data</st>* <st c="19526">comes</st> <st c="19533">from
    here:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: <st c="19749">This pulls in the web page we looked at in previous chapters,
    which is relatively long compared to the question text.</st> <st c="19868">To
    make that data more manageable, we break that content up into chunks using a text
    splitter in</st> <st c="19965">this code:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: <st c="20072">If you were to pull out the third chunk using</st> `<st c="20119">splits[2]</st>`<st
    c="20128">, it would look</st> <st c="20144">like this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: <st c="21348">I chose the third chunk to show because it is a relatively short
    chunk.</st> <st c="21421">Most of the chunks are much larger.</st> <st c="21457">The</st>
    **<st c="21461">Semantic Chunker text splitter</st>** <st c="21491">we use attempts
    to use semantics to</st> <st c="21528">determine how to split up the text, using
    embeddings to determine those semantics.</st> <st c="21611">In theory, this should
    give us chunks that do a better job of breaking up the data based on context,
    rather than just an</st> <st c="21732">arbitrary size.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21747">However, there</st> <st c="21762">is an important concept to understand
    when it comes to embeddings that will impact the splitter you choose and the size
    of your embeddings in general.</st> <st c="21914">This all stems from the fact
    that no matter how large the text that you pass to the vectorization algorithm
    is, it is still going to give you an embedding that is the same size as any of
    the other embeddings.</st> <st c="22123">In this case, that means the user query
    embedding is going to be 1,536 dimensions, but all those long sections of text
    in the vector store are also going to be 1,536 dimensions, even though their actual
    length in text format is quite different.</st> <st c="22368">It may be counter-intuitive,
    but in an amazing turn of events, it</st> <st c="22434">works well!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22445">When conducting a search with the user query of the vector store,
    the mathematical representations of the user query embedding and the other embeddings
    are done in such a way that we are still able to detect the semantic similarities
    between them, despite the large disparity in their size.</st> <st c="22737">This
    aspect of the vector similarity search is the kind of thing that makes mathematicians
    love math so much.</st> <st c="22847">It just seems to defy all logic that you
    can turn text of very different sizes into numbers and be able to detect similarities</st>
    <st c="22974">between them.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22987">But there is</st> <st c="23001">another aspect of this to consider
    as well—when you compare the results across just the chunks that you break your
    data into, the size of those chunks will matter.</st> <st c="23165">In this case,
    the larger the amount of content that is being vectorized, the more diluted the
    embedding will be.</st> <st c="23278">On the other hand, the smaller the amount
    of content that the embedding represents, the less context you will have to match
    up when you perform a vector similarity search.</st> <st c="23450">For each of
    your RAG implementations, you will need to find a delicate balance between chunk
    size and</st> <st c="23552">context representation.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23575">Understanding this will help you make better decisions about how
    you split data and the vectorization algorithms you choose when trying to improve
    your RAG system.</st> <st c="23740">We will cover some other techniques to get
    more out of your splitting/chunking strategy in</st> [*<st c="23831">Chapter 11</st>*](B22475_11.xhtml#_idTextAnchor229)
    <st c="23841">when we talk about LangChain splitters.</st> <st c="23882">Next,
    we will talk about the importance of testing different</st> <st c="23943">vectorization
    models.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23964">Not all semantics are created equal!</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="24001">A common mistake made in RAG applications is choosing the first
    vectorization algorithm that is implemented and just assuming that provides the
    best results.</st> <st c="24160">These algorithms take the semantic meaning of
    text and represent them mathematically.</st> <st c="24246">However, these algorithms
    are generally large NLP models themselves, and they can vary in capabilities and
    quality as much as the LLMs.</st> <st c="24382">Just as we, as humans, often find
    it challenging to comprehend the intricacies and nuances of text, these models
    can grapple with the same challenge, having varying abilities to grasp the complexities
    inherent in written language.</st> <st c="24613">For example, models in the past
    could not decipher the difference between</st> `<st c="24687">bark</st>` <st c="24691">(a
    dog noise) and</st> `<st c="24710">bark</st>` <st c="24714">(the outer part of
    most trees), but newer models can detect this based on the surrounding text and
    the context in which it is used.</st> <st c="24847">This area of the field is
    adapting and evolving just as fast as</st> <st c="24911">other areas.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24923">In some cases, it is possible that a domain-specific vectorization
    model, such as one trained on scientific papers, is going to do better in an app
    that is focused on scientific papers than using a generic vectorization model.</st>
    <st c="25151">Scientists talk in very specific ways, very different from what
    you might see on social media, and so a giant model trained on general web-based
    text may not perform well in this</st> <st c="25330">specific domain.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25346">Fun fact</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25355">You often hear about how you can fine-tune LLMs to improve your
    domain-specific results.</st> <st c="25445">But did you know that you can also
    fine-tune embedding models?</st> <st c="25508">Fine-tuning an embedding model
    has the potential to improve the way the embedding model understands your domain-specific
    data and, therefore, has the potential to improve your similarity search results.</st>
    <st c="25711">This has the potential to improve your entire RAG system substantially
    for</st> <st c="25786">your domain.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25798">To summarize this section on fundamentals, numerous aspects of
    vectors can help you or hurt you when trying to build the most effective RAG application
    for your needs.</st> <st c="25967">Of course, it would be poor manners for me
    to tell you how important the vectorization algorithm is without telling you which
    ones are available!</st> <st c="26113">To address this, in this next section,
    let’s run through a list of some of the most popular vectorization techniques!</st>
    <st c="26231">We will even do this</st> <st c="26252">with code!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26262">Code lab 7.1 – Common vectorization techniques</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="26309">Vectorization algorithms</st> <st c="26335">have evolved significantly
    over the past few decades.</st> <st c="26389">Understanding how these have changed,
    and why, will help you gain more perspective on how to choose the one that fits
    your needs the most.</st> <st c="26528">Let’s walk through some of these vectorization
    algorithms, starting with some of the earliest ones and ending with the most recent,
    more advanced options.</st> <st c="26683">This is nowhere close to an exhaustive
    list, but these select few should be enough to give you a sense of where this
    part of the field came from and where it is going.</st> <st c="26851">Before we
    start, let’s install and import some new Python packages that play important roles
    in our coding journey through</st> <st c="26974">vectorization techniques:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: <st c="27071">This code should go near the top of the previous code in the same
    cell as the other</st> <st c="27156">package installations.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27178">Term frequency-inverse document frequency (TF-IDF)</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="27229">1972 was probably much sooner a time than what you would expect
    in a book about a relatively</st> <st c="27323">brand-new technology like RAG,
    but this is where we find the roots of the vectorization</st> <st c="27411">techniques
    we are going to</st> <st c="27438">talk about.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27449">Karen Ida Boalth Spärck Jones was a self-taught programmer and
    pioneering British computer scientist who worked on several papers focused on
    the field of NLP.</st> <st c="27609">In 1972, she made one of her most important
    contributions, introducing the concept of</st> **<st c="27695">inverse document
    frequency</st>** <st c="27721">(</st>**<st c="27723">IDF</st>**<st c="27726">).</st>
    <st c="27730">The</st> <st c="27733">basic concept as she stated was that “</st>*<st
    c="27772">the specificity of a term can be quantified as an inverse function of
    the number of documents in which</st>* *<st c="27876">it occurs</st>*<st c="27885">.”</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27887">As a real-world example, consider applying the</st> `<st c="27935">df</st>`
    <st c="27937">(document frequency) and</st> `<st c="27963">idf</st>` <st c="27966">(inverse
    document frequency) score to some words in Shakespeare’s 37 plays and you will
    find that the word</st> `<st c="28074">Romeo</st>` <st c="28079">is the highest-scoring
    result.</st> <st c="28111">This is because it appears very frequently, but only
    in one</st> *<st c="28171">document</st>*<st c="28179">, the</st> `<st c="28185">Romeo
    and Juliet</st>` <st c="28201">document.</st> <st c="28212">In this case,</st>
    `<st c="28226">Romeo</st>` <st c="28231">would be scored</st> `<st c="28248">1</st>`
    <st c="28249">for</st> `<st c="28254">df</st>`<st c="28256">, as it appeared in
    1 document.</st> `<st c="28288">Romeo</st>` <st c="28293">would score</st> `<st
    c="28306">1.57</st>` <st c="28310">for</st> `<st c="28315">idf</st>`<st c="28318">,
    higher than any other word because of its high frequency in that one document.</st>
    <st c="28399">Meanwhile, Shakespeare used the word</st> `<st c="28436">sweet</st>`
    <st c="28441">occasionally but in every single play, giving it a low score.</st>
    <st c="28504">This gives</st> `<st c="28515">sweet</st>` <st c="28520">a</st>
    `<st c="28523">df</st>` <st c="28525">score of</st> `<st c="28535">37</st>`<st
    c="28537">, and an</st> `<st c="28546">idf</st>` <st c="28549">score of</st> `<st
    c="28559">0</st>`<st c="28560">. What Karen Jones was saying in her paper was
    that when you see words such as</st> `<st c="28639">Romeo</st>` <st c="28644">appear
    in just a small number of the overall number of plays, you can take the plays
    where those words appear and consider them very important and predictive of what
    that play is about.</st> <st c="28831">In contrast,</st> `<st c="28844">sweet</st>`
    <st c="28849">had the opposite effect, as it is uninformative in terms of the
    importance of the word and of the documents that the word</st> <st c="28972">is
    in.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28978">But that’s enough talk.</st> <st c="29003">Let’s see how this
    algorithm looks in code!</st> <st c="29047">The scikit-learn library has a function
    that can be applied to text to vectorize that text using the TF-IDF method.</st>
    <st c="29163">The following code is where we define the</st> `<st c="29205">splits</st>`
    <st c="29211">variable, which is what we will use as our data to</st> <st c="29262">train
    the</st> <st c="29273">model on:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: <st c="29927">Unlike</st> <st c="29935">the OpenAI embedding model, this model
    requires you to</st> *<st c="29990">train</st>* <st c="29995">on your</st> *<st
    c="30004">corpus</st>* <st c="30010">data, which is a fancy term for all the text
    data you have available to train with.</st> <st c="30095">This code is primarily
    to demonstrate how a TD-IDF model is used compared to our current RAG pipeline
    retriever, so we won’t review it line by line.</st> <st c="30244">But we encourage
    you to try out the code yourself and try</st> <st c="30302">different settings.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30321">It should be noted that the vectors this algorithm produces are</st>
    <st c="30385">called</st> **<st c="30393">sparse vectors</st>**<st c="30407">,
    and the vectors we were previously working with in previous code labs were</st>
    <st c="30484">called</st> **<st c="30491">dense vectors</st>**<st c="30504">.
    This is an important distinction that we will review in detail in</st> [*<st c="30572">Chapter
    8</st>*](B22475_08.xhtml#_idTextAnchor152)<st c="30581">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30582">This model uses the corpus data to set up the environment that
    can then calculate the embeddings for</st> <st c="30684">new content that you
    introduce to it.</st> <st c="30722">The output should look like the</st> <st c="30754">following
    table:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: <st c="30920">In this case, we</st> <st c="30937">see at least a 10-way tie
    for the</st> `<st c="30972">idf</st>` <st c="30975">highest value (we are only
    showing 10, so there are probably more), and all of them are number-based text.</st>
    <st c="31083">This does not seem particularly useful, but this is primarily because
    our corpus data is so small.</st> <st c="31182">Training on more data from the
    same author or domain can help you build a model that has a better contextual
    understanding of the</st> <st c="31312">underlying content.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31331">Now, going back to the original question that we have been using,</st>
    `<st c="31398">What are the advantages of RAG?</st>`<st c="31429">, we want to
    use the TF-IDF embeddings to determine what the most relevant</st> <st c="31504">documents
    are:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: <st c="31840">This replicates</st> <st c="31857">the behavior we see with the
    retriever, where it uses a similarity algorithm to find the nearest embedding
    by distance.</st> <st c="31977">In this case, we use cosine similarity, which</st>
    <st c="32022">we will talk about in</st> [*<st c="32045">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="32054">, but just keep in mind that there are many distance algorithms that
    we can use to calculate this distance.</st> <st c="32162">Our output from this
    code is</st> <st c="32191">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: <st c="32638">If you run our original code, which uses the original vector store
    and retriever, you will see</st> <st c="32734">this output:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: <st c="33181">They match!</st> <st c="33194">A small algorithm from 1972 trained
    on our own data in a fraction of a second is just as good as the massive algorithms
    developed by OpenAI spending billions of dollars to develop them!</st> <st c="33380">Okay,
    let’s slow down, this is definitely NOT the case!</st> <st c="33436">The reality
    is that in real-world scenarios, you will be working with a much larger dataset
    than we are and much more complicated user queries, and this will benefit from
    the use of more sophisticated modern</st> <st c="33644">embedding techniques.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="33665">TF-IDF has</st> <st c="33677">been</st> <st c="33682">very useful
    over the years.</st> <st c="33710">But was it necessary to learn about an algorithm
    from 1972 when we are talking about the most advanced generative AI models ever
    built?</st> <st c="33846">The answer is BM25\.</st> <st c="33866">This is just
    a teaser, but you will learn more about this very popular</st> **<st c="33937">keyword
    search</st>** <st c="33951">algorithm, one</st> <st c="33966">of the most popular
    algorithms in use today, in the next chapter.</st> <st c="34033">And guess what?</st>
    <st c="34049">It is based on TF-IDF!</st> <st c="34072">What TF-IDF has a problem
    with, though, is capturing context and semantics as well as some of the next models
    we will talk about.</st> <st c="34202">Let’s discuss the next major step up: Word2Vec
    and</st> <st c="34253">related algorithms.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34272">Word2Vec, Sentence2Vec, and Doc2Vec</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**<st c="34308">Word2Vec</st>** <st c="34317">and</st> <st c="34322">similar
    models introduced an early application</st> <st c="34368">of unsupervised learning,
    representing a significant step forward in the NLP field.</st> <st c="34453">There
    are multiple</st> *<st c="34472">vec</st>* <st c="34475">models (word, doc, and
    sentence), where their training was focused on words, documents, or sentences,
    respectively.</st> <st c="34592">These models differ in the level of text they
    are</st> <st c="34642">trained on.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34653">Word2Vec focuses on learning vector representations for individual
    words, capturing their semantic meaning and relationships.</st> **<st c="34780">Doc2Vec</st>**<st
    c="34787">, on the</st> <st c="34795">other hand, learns</st> <st c="34814">vector
    representations for entire documents, allowing it to capture the overall context
    and theme of a</st> <st c="34918">document.</st> **<st c="34928">Sentence2Vec</st>**
    <st c="34940">is similar to Doc2Vec but operates at the</st> <st c="34983">sentence
    level, learning vector representations for individual sentences.</st> <st c="35057">While
    Word2Vec is useful for tasks such as word similarity and analogy, Doc2Vec and
    Sentence2Vec are more suitable for document-level tasks such as document similarity,
    classification,</st> <st c="35242">and retrieval.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35256">Because we are working with larger documents, and not just words
    or sentences, we are going to select the Doc2Vec model over Word2Vec or Sentence2Vec
    and train this model to see how it works as our retriever.</st> <st c="35466">Like
    the TD-IDF model, this model can be trained with our data and then we pass the
    user query to it to see whether we can get similar results for the most similar</st>
    <st c="35630">data chunks.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35642">Add this code in a new cell after the TD-IDF</st> <st c="35688">code
    cell:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: <st c="36220">Much like the</st> <st c="36235">TD-IDF model, this code is primarily
    to demonstrate how a</st> <st c="36293">Doc2Vec model is used</st> <st c="36315">compared
    to our current RAG pipeline retriever, so we won’t review it line by line, but
    we encourage you to try out the code yourself and try different settings.</st>
    <st c="36477">This code focuses on training the Doc2Vec model and saving</st>
    <st c="36536">it locally.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36547">Fun fact</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36556">Training language models is a hot topic and can be a well-paid
    profession these days.</st> <st c="36643">Have you ever trained a language model?</st>
    <st c="36683">If your answer was</st> *<st c="36702">no</st>*<st c="36704">, you
    would be wrong.</st> <st c="36726">Not only did you just train a language model
    but you have now trained two!</st> <st c="36801">Both TF-IDF and Doc2Vec are language
    models that you just trained.</st> <st c="36868">These are relatively basic versions
    of model training, but you have to start somewhere, and you</st> <st c="36964">just
    did!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36973">In this next code, we</st> <st c="36996">will use that model on</st>
    <st c="37019">our data:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: <st c="37668">We separated</st> <st c="37682">the code for creating and saving
    the model from the usage of the model so that you can see how this model can be
    saved and referenced later.</st> <st c="37823">Here is the output from</st> <st
    c="37847">this code:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: <st c="38264">Comparing this to the results from our original retriever shown
    previously, this model does not return the same result.</st> <st c="38385">However,
    this model was set up with just 100 dimension vectors in</st> <st c="38451">this
    line:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: <st c="38562">What happens when you change</st> `<st c="38592">vector_size</st>`
    <st c="38603">in this line to use 1,536, the same vector size as the</st> <st
    c="38659">OpenAI model?</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38672">Change the</st> `<st c="38684">doc2vec_model</st>` <st c="38697">variable
    definition</st> <st c="38718">to this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: <st c="38828">The results</st> <st c="38841">will change</st> <st c="38853">to
    this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: <st c="39298">This resulted in the same text as our original results, using
    OpenAI’s embeddings.</st> <st c="39382">However, the results are not consistent.</st>
    <st c="39423">If you trained this model on more data, it will likely improve</st>
    <st c="39486">the results.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39498">In theory, the benefit this type of model has over TF-IDF is that
    it is a neural network-based approach that takes into account surrounding words,
    whereas TF-IDF is simply a statistical measure that evaluates how relevant a word
    is to the document (keyword search).</st> <st c="39765">But as we said about the
    TD-IDF model, there are still more powerful models than the</st> *<st c="39850">vec</st>*
    <st c="39853">models that capture much more context and semantics of the text
    they are fed.</st> <st c="39932">Let’s jump to another generation of</st> <st
    c="39968">models, transformers.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39989">Bidirectional encoder representations from transformers</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="40045">At this point, with</st> **<st c="40066">bidirectional encoder
    representations from transformers</st>** <st c="40121">(</st>**<st c="40123">BERT</st>**<st
    c="40127">), we are</st> <st c="40137">fully</st> <st c="40144">into using neural
    networks to better understand the underlying semantics of the corpus, yet another
    big step forward for NLP algorithms.</st> <st c="40281">BERT is also among the
    first to apply a specific type of neural network, the</st> **<st c="40358">transformer</st>**<st
    c="40369">, which</st> <st c="40376">was a major step in the progression that
    led to the development of the LLMs we are familiar with today.</st> <st c="40481">OpenAI’s
    popular ChatGPT models are also transformers but were trained on a much larger
    corpus and with different techniques</st> <st c="40606">from BERT.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40616">That said, BERT is still a very capable model.</st> <st c="40664">You
    can use BERT as a standalone model that you import, avoiding having to rely on
    APIs such as OpenAI’s embedding service.</st> <st c="40788">Being able to use
    a local model in your code can be a big advantage in certain network-constrained
    environments, instead of relying on an API service such</st> <st c="40943">as
    OpenAI.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40953">One of the defining characteristics of the transformer models
    is the use of a self-attention mechanism to capture dependencies between words
    in a text.</st> <st c="41106">BERT also has multiple layers of transformers, allowing
    it to learn even more complex representations.</st> <st c="41209">Compared to
    our Doc2Vec model, BERT is pre-trained already on large amounts of data, such
    as Wikipedia and BookCorpus with the objective of predicting the</st> <st c="41364">next
    sentence.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41378">Much like</st> <st c="41388">the</st> <st c="41392">previous two
    models, we provide code for you to compare retrieved results</st> <st c="41467">using
    BERT:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: <st c="42859">There is</st> <st c="42869">one</st> <st c="42872">very important
    difference in this code compared to the usage of the last couple of models.</st>
    <st c="42964">Here, we are not tuning the model on our own data.</st> <st c="43015">This
    BERT model has already been trained on a large dataset.</st> <st c="43076">It
    is possible to fine-tune the model further with our data, which is recommended
    if you want to use this model.</st> <st c="43189">The results will reflect this
    lack of training, but we won’t let that prevent us from showing you how</st> <st
    c="43291">it works!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43300">For this code, we are printing out the vector size for comparison
    to the others.</st> <st c="43382">Like the other models, we can see the top retrieved
    result.</st> <st c="43442">Here is</st> <st c="43450">the output:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: <st c="43652">The vector size is a respectable</st> `<st c="43686">768</st>`<st
    c="43689">. I don’t even need metrics to tell you that the top document it found
    is not the best chunk to answer the question</st> `<st c="43805">What are the
    advantages</st>` `<st c="43829">of RAG?</st>`<st c="43836">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43837">This</st> <st c="43842">model is</st> <st c="43852">powerful and
    has the potential to work better than the previous models, but we would need to
    do some extra work (fine-tuning) to get it to do a better job with our data when
    comparing it to the previous types of embedding models we have discussed so far.</st>
    <st c="44107">That may not be the case with all data, but typically, in a specialized
    domain like this, fine-tuning should be considered as an option for your embedding
    model.</st> <st c="44269">This is especially true if you are using a smaller local
    model rather than a large, hosted API such as OpenAI’s</st> <st c="44381">embeddings
    API.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44396">Running through these three different models illustrates how much
    embedding models have changed over the past 50 years.</st> <st c="44517">Hopefully,
    this exercise has shown you how important the decision is for what embedding model
    you select.</st> <st c="44623">We will conclude our discussion of embedding models
    by bringing us full circle back to the original embedding model we were using,
    the OpenAI embedding model from OpenAI’s API service.</st> <st c="44808">We will
    discuss the OpenAI model, as well as its peers on other</st> <st c="44872">cloud
    services.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44887">OpenAI and other similar large-scale embedding services</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="44943">Let’s talk a</st> <st c="44957">little more about the BERT model
    we just used, relative to OpenAI’s embedding model.</st> <st c="45042">This was
    the</st> `<st c="45055">'bert-base-uncased'</st>` <st c="45074">version, which</st>
    <st c="45089">is a pretty robust 110M parameter transformer model, especially
    compared to the previous models we used.</st> <st c="45195">We have come a long
    way since the TD-IDF model.</st> <st c="45243">Depending on the environment you
    are working in, this may test your computational limitations.</st> <st c="45338">This
    was the largest model my computer could run of the BERT options.</st> <st c="45408">But
    if you have a more powerful environment, you can change the model in these two
    lines</st> <st c="45497">to</st> `<st c="45500">'bert-large-uncased'</st>`<st
    c="45520">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: <st c="45641">You can see the full list of BERT options</st> <st c="45684">here:</st>
    [<st c="45690">https://huggingface.co/google-bert/bert-base-uncased</st>](https://huggingface.co/google-bert/bert-base-uncased
    )
  prefs: []
  type: TYPE_NORMAL
- en: <st c="45742">The</st> `<st c="45747">'bert-large-uncased'</st>` <st c="45767">model
    has 340M parameters, more than three times the size of</st> `<st c="45829">'bert-base-uncased'</st>`<st
    c="45848">. If your environment cannot handle this size of a model, it will crash
    your kernel and you will have to reload all your imports and relevant notebook
    cells.</st> <st c="46006">This just tells you how large these models can get.</st>
    <st c="46058">But just to be clear, these two</st> <st c="46090">BERT models are
    110M and 340M parameters, which is in millions,</st> <st c="46154">not billions.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46167">The</st> <st c="46172">OpenAI embedding model that we have been
    using is based on the</st> **<st c="46235">GPT-3</st>** <st c="46240">architecture,
    which has 175 billion parameters.</st> <st c="46289">That is a</st> *<st c="46299">billion</st>*
    <st c="46306">with a</st> *<st c="46314">B</st>*<st c="46315">. We will be talking
    about their newer embedding models later in this chapter, which are based on the</st>
    **<st c="46417">GPT-4</st>** <st c="46422">architecture and have one trillion
    parameters (with a</st> *<st c="46477">T</st>*<st c="46478">!).</st> <st c="46482">Needless
    to say, these models are massive and dwarf any of the other models we have discussed.</st>
    <st c="46577">BERT and OpenAI are both transformers, but BERT was trained on 3.3
    billion words, whereas the full corpus for GPT-3 is estimated to be around 17
    trillion words (45 TB</st> <st c="46744">of text).</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46753">OpenAI currently has three different embedding models available.</st>
    <st c="46819">We have been using the older model to save API costs based on GPT-3,</st>
    `<st c="46888">'text-embedding-ada-002'</st>`<st c="46912">, but it is a very
    capable embedding model.</st> <st c="46956">The other two newer models that are
    based on GPT-4 are</st> `<st c="47011">'text-embedding-3-small'</st>` <st c="47035">and</st>
    `<st c="47040">'text-embedding-3-large'</st>`<st c="47064">. Both of these models
    support the Matryoshka embeddings we talked about earlier, which allow you to
    use an adaptive retrieval approach for</st> <st c="47204">your retrieval.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47219">OpenAI is not the only cloud provider that offers a text-embedding
    API though.</st> `<st c="47422">'text-embedding-preview-0409'</st>`<st c="47451">.
    The</st> `<st c="47457">'text-embedding-preview-0409'</st>` <st c="47486">model
    is the only other large-scale cloud-hosted embedding model that I am aware of
    at this time that supports Matryoshka embeddings, beyond OpenAI’s</st> <st c="47637">newer
    models.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="47650">Amazon Web Services</st>** <st c="47670">(</st>**<st c="47672">AWS</st>**<st
    c="47675">) has</st> <st c="47682">embedding models based</st> <st c="47705">on
    their</st> **<st c="47714">Titan model</st>**<st c="47725">, as</st> <st c="47729">well
    as</st> **<st c="47738">Cohere’s embedding models</st>**<st c="47763">.</st> **<st
    c="47765">Titan Text Embeddings V2</st>** <st c="47789">is</st> <st c="47792">expected
    to launch soon and is also expected to support</st> <st c="47849">Matryoshka embedding.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47870">That concludes our whirlwind adventure through 50 years of embedding
    generation technology!</st> <st c="47963">The models highlighted were selected
    to represent the progression of embedding</st> <st c="48042">capabilities over
    the past 50 years, but these are just a tiny sliver of the actual number</st>
    <st c="48133">of ways there are to generate embeddings.</st> <st c="48175">Now
    that your knowledge about embedding capabilities has been expanded, let’s turn
    to the factors you can consider when making the actual decisions on which model</st>
    <st c="48338">to use.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48345">Factors in selecting a vectorization option</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="48389">Selecting the</st> <st c="48404">right vectorization option is
    a crucial decision when building a RAG system.</st> <st c="48481">Key considerations
    include the quality of the embeddings for your specific application, the associated
    costs, network availability, speed of embedding generation, and compatibility
    between embedding models.</st> <st c="48688">There are numerous other options
    beyond what we shared above that you can explore for your specific needs when
    it comes to selecting an embedding model.</st> <st c="48841">Let’s review</st>
    <st c="48854">these considerations.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48875">Quality of the embedding</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="48900">When considering the quality of your embeddings, you cannot rely
    on just the generic metrics you have</st> <st c="49002">seen for each model.</st>
    <st c="49024">For example, OpenAI has been tested on the</st> `<st c="49134">'text-embedding-ada-002'</st>`
    <st c="49158">model, whereas the</st> `<st c="49178">'text-embedding-3-large'</st>`
    <st c="49202">model scored 64.6%.</st> <st c="49223">The metrics can be useful,
    especially when trying to hone in on a model of a certain quality, but this does
    not mean the model will be 3.6% better for your specific model.</st> <st c="49395">It
    does not even mean it will necessarily be better at all.</st> <st c="49455">Do
    not rely on generic tests completely.</st> <st c="49496">What ultimately matters
    is how well your embeddings work for your specific application of them.</st> <st
    c="49592">This includes embedding models that you train with your own data.</st>
    <st c="49658">If you work on an application that involves a specific domain, such
    as science, legal, or technology, it is very likely you can find or train a model
    that will work better with your specific domain data.</st> <st c="49862">When
    you start your project, try multiple embedding models within your RAG system and
    then use the evaluation techniques we share in</st> [*<st c="49995">Chapter 9</st>*](B22475_09.xhtml#_idTextAnchor184)
    <st c="50004">to compare results from using each model to determine which is best
    for</st> <st c="50077">your application.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50094">Cost</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="50099">The costs for these embedding services vary from free to relatively
    expensive.</st> <st c="50179">OpenAI’s most expensive embedding model costs $0.13
    per million tokens.</st> <st c="50251">This means that for a page that has 800
    tokens, it will cost you $0.000104, or slightly more than 1% of 1 cent.</st> <st
    c="50363">That</st> <st c="50368">may not sound like much, but for most applications
    using embeddings, especially in the enterprise, these costs get multiplied rapidly,
    pushing the costs into the $1,000s or $10,000s for even a small project.</st>
    <st c="50576">But other embedding APIs cost less and may fit your needs just as
    well.</st> <st c="50648">And of course, if you build your own model like I described
    earlier in this chapter, you will only have the costs of the hardware or hosting
    costs for that model.</st> <st c="50811">That can cost much less over time and
    may meet your needs</st> <st c="50869">as well.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50877">Network availability</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="50898">There are a variety of scenarios you will want to consider in
    terms of network availability.</st> <st c="50992">Almost all applications will
    have some scenarios where the network will not be available.</st> <st c="51082">Network
    availability impacts your users’ access to your application interface, but it
    can also impact network calls you make from your application to other services.</st>
    <st c="51248">In this latter case, this could be a situation where your users
    can access your application’s interface but the application cannot reach OpenAI’s
    embedding service to generate an embedding for your user query.</st> <st c="51458">What
    will you do in this case?</st> <st c="51489">If you are using a model that is
    within your environment, this avoids this problem.</st> <st c="51573">This is
    a consideration of availability and the impact it has on</st> <st c="51638">your
    users.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51649">Keep in mind that you cannot just switch the embedding model for
    your user query, just in case you were thinking you could use a</st> *<st c="51779">fallback</st>*
    <st c="51787">mechanism and have a local embedding model available as a secondary
    option when the network is unavailable.</st> <st c="51896">If you use a proprietary
    API-only embedding model to vectorize your embeddings, you are committed to that
    embedding model, and your RAG system will be reliant on the availability of that
    API.</st> <st c="52088">OpenAI does not offer their embedding models to use locally.</st>
    <st c="52149">See the upcoming</st> *<st c="52166">Embedding</st>* *<st c="52176">compatibility</st>*
    <st c="52189">subsection!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52201">Speed</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="52207">The speed of generating embeddings is an important consideration,
    as it can impact the responsiveness and user experience of your application.</st>
    <st c="52351">When using a hosted API service such as OpenAI, you are making network
    calls to generate embeddings.</st> <st c="52452">While these network calls are
    relatively fast, there is still some latency involved compared to generating embeddings
    locally within your own environment.</st> <st c="52607">However, it’s important
    to note that local embedding generation is not always faster, as the speed also
    depends on the specific</st> <st c="52735">model being used.</st> <st c="52753">Some
    models may have slower inference times, negating the benefits of local processing.</st>
    <st c="52841">Key aspects to consider when determining the speed of your embedding
    option(s) are network latency, model inference time, hardware resources, and,
    in some cases where multiple embeddings are involved, the ability to generate
    embeddings in batches and optimize</st> <st c="53101">with that.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53111">Embedding compatibility</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="53135">Pay close attention; this is a very important consideration and
    fact about embeddings!</st> <st c="53223">In any case when you are comparing embeddings,
    such as when you are detecting the similarity between a user query embedding and
    the embeddings stored in the vector store,</st> *<st c="53394">they must be created
    by the same embedding model</st>*<st c="53442">. These models generate unique
    vector signatures specific to only that model.</st> <st c="53520">This is even
    true of models at the same service provider.</st> <st c="53578">With OpenAI, for
    example, all three embedding models are not compatible with each other.</st> <st
    c="53667">If you use any of OpenAI’s embedding models to vectorize your embeddings
    stored in your vector store, you have to call the OpenAI API and use that same
    model when you vectorize the user query to conduct a</st> <st c="53872">vector
    search.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53886">As your application expands in size, changing or updating an embedding
    model has major cost implications, since it means you will have to generate all
    new embeddings to use a new embedding model.</st> <st c="54083">This may even
    drive you to use a local model rather than a hosted API service since generating
    new embeddings with a model you have control over tends to cost</st> <st c="54242">much
    less.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54252">While generic benchmarks can provide guidance, it’s essential
    to evaluate multiple embedding models within your specific domain and application
    to determine the best fit.</st> <st c="54424">Costs can vary significantly, depending
    on the service provider and the volume of embeddings required.</st> <st c="54527">Network
    availability and speed are important factors, especially when using hosted API
    services, as they can impact the responsiveness and user experience of your application.</st>
    <st c="54703">Compatibility between embedding models is also crucial, as embeddings
    generated by different models cannot be</st> <st c="54813">directly compared.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54831">As your application grows, changing or updating vector embedding
    models can have significant cost implications.</st> <st c="54944">Local embedding
    generation can offer more control and potentially lower costs, but the speed depends
    on the specific model and available hardware resources.</st> <st c="55101">Thorough
    testing and benchmarking are necessary to find the optimal balance of quality,
    cost, speed, and other relevant factors for your application.</st> <st c="55251">Now
    that we have explored the considerations for selecting a vectorization option,
    let’s dive into the topic of how they are stored with</st> <st c="55388">vector
    stores.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="55402">Getting started with vector stores</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="55437">Vector stores, combined</st> <st c="55462">with other data stores
    (databases, data warehouses, data lakes, and any other data sources) are the fuel
    for your RAG system engine.</st> <st c="55595">Not to state the obvious, but without
    a place to store your RAG-focused data, which typically involves the creating,
    management, filtering, and search of vectors, you will not be able to build a
    capable RAG system.</st> <st c="55810">What you use and how it is implemented
    will have significant implications for how your entire RAG system performs, making
    it a critical decision and effort.</st> <st c="55967">To start this section, let’s
    first go back to the original concept of</st> <st c="56037">a database.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="56048">Data sources (other than vector)</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="56081">In our</st> <st c="56088">basic RAG example so far, we are keeping
    it simple (for now) and have not connected it to an additional database resource.</st>
    <st c="56212">You could consider the web page that the content is pulled from
    as the database, although the most accurate description in this context is probably
    to call it an unstructured data source.</st> <st c="56400">Regardless, it is very
    likely your application will expand to the point of needing database-like support.</st>
    <st c="56506">This may come in the form of a traditional SQL database, or it may
    be in the form of a giant data lake (large repository of all types of raw, primarily
    unstructured data), where the data is preprocessed into a more usable format representing
    the data source that supports your</st> <st c="56783">RAG system.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="56794">The architecture of your data storage may be based on a</st> **<st
    c="56851">relational database management system</st>** <st c="56888">(</st>**<st
    c="56890">RDBMS</st>**<st c="56895">), a</st> <st c="56901">variety of different
    types of NoSQL, NewSQL (aimed at giving you the best of the two previous approaches),
    or various versions of data warehouses and data lakes.</st> <st c="57063">From
    the perspective of this book, we will approach the data sources these systems
    represent as an abstract concept of the</st> **<st c="57186">data source</st>**<st
    c="57197">. But what is important</st> <st c="57221">to consider here is that
    your decision on what vector store to use will likely be highly influenced by
    the existing architecture of your data source.</st> <st c="57371">The current
    technical skills of your staff will likely play a key role in these decisions</st>
    <st c="57461">as well.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="57469">As an example, you may be</st> <st c="57496">using</st> **<st
    c="57502">PostgreSQL</st>** <st c="57512">for your RDBMS and have a team of expert
    engineers with significant expertise in fully utilizing and optimizing PostgreSQL.</st>
    <st c="57637">In this case, you</st> <st c="57655">will want to consider the</st>
    **<st c="57681">pgvector</st>** <st c="57689">extension for PostgreSQL, which
    turns PostgreSQL tables into vector stores, extending many of the PostgreSQL capabilities
    your team is familiar with into the vector world.</st> <st c="57862">Concepts
    such as indexing and writing SQL specifically for PostgreSQL are already going
    to be familiar, and that will help get your team up to speed quickly with how
    this extends to pgvector.</st> <st c="58054">If you are building your entire data
    infrastructure from scratch, which is rare in enterprise, then you may go a different
    route optimized for speed, cost, accuracy, or all of the above!</st> <st c="58241">But
    for most companies, you will need to consider compatibility with existing infrastructure
    in the vector store selection</st> <st c="58364">decision criteria.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="58382">Fun fact – What about applications such as SharePoint?</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="58437">SharePoint is</st> <st c="58451">typically considered a</st> **<st
    c="58475">content management system</st>** <st c="58500">(</st>**<st c="58502">CMS</st>**<st
    c="58505">) and may not fit strictly into</st> <st c="58537">the definitions of
    the other data sources we mentioned previously.</st> <st c="58605">But SharePoint
    and similar applications contain massive repositories of unstructured data, including
    PDF, Word, Excel, and PowerPoint documents that represent a huge portion of a
    company’s knowledge base, especially in large enterprise environments.</st> <st
    c="58855">Combine this with the fact that generative AI has shown a proclivity
    to tap into unstructured data unlike any other technology preceding it, and you
    have the makings of an incredible data source for RAG systems.</st> <st c="59067">These
    types of applications also have sophisticated APIs that can conduct data extraction
    as you pull the documents, such as pulling text out of a Word document and putting
    it into your database before vectorization.</st> <st c="59284">In many large companies,
    due to the high value of the data in these applications and the relative ease
    of extracting that data using the APIs, this has been one of the first sources
    of data for RAG systems.</st> <st c="59491">So yes, you can definitely include
    SharePoint and similar applications in your list of potential</st> <st c="59588">data
    sources!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="59601">We will talk more about pgvector and other vector store options
    in a moment, but it is important to understand how these decisions can be very
    specific to each situation and that considerations other than just the vector
    store itself will play an important role in what you ultimately decide to</st>
    <st c="59897">work with.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="59907">Regardless of what option you choose, or are starting with, this
    will be a key component that feeds the data to your RAG system.</st> <st c="60037">This
    leads us to the vector stores themselves, which we can</st> <st c="60097">discuss
    next.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="60110">Vector stores</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="60124">Vector stores, also</st> <st c="60144">known as vector databases
    or vector search engines, are specialized storage systems designed to efficiently
    store, manage, and retrieve vector representations of data.</st> <st c="60313">Unlike
    traditional databases that organize data in rows and columns, vector stores are
    optimized for operations in high-dimensional vector spaces.</st> <st c="60460">They
    play a crucial role in an effective RAG system by enabling fast similarity search,
    which is essential for identifying the most relevant pieces of information in
    response to a</st> <st c="60640">vectorized query.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="60657">The architecture of a vector store typically consists of three</st>
    <st c="60721">main components:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="60737">Indexing layer</st>**<st c="60752">: This</st> <st c="60760">layer
    organizes the vectors in a manner that speeds up search queries.</st> <st c="60831">It
    employs indexing techniques such as tree-based partitioning (e.g., KD-trees) or
    hashing (e.g., locality-sensitive hashing) to facilitate fast retrieval of vectors
    that are near each other in the</st> <st c="61029">vector space.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="61042">Storage layer</st>**<st c="61056">: The storage layer</st>
    <st c="61076">efficiently manages the data storage on disk or in memory, ensuring
    optimal performance</st> <st c="61165">and scalability.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="61181">Processing layer (optional)</st>**<st c="61209">: Some vector
    stores include a processing layer to handle</st> <st c="61267">vector transformations,
    similarity computations, and other analytics operations in</st> <st c="61351">real
    time.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="61361">While it is technically possible to build a RAG system without
    using a vector store, doing so would result in suboptimal performance and scalability.</st>
    <st c="61512">Vector stores are specifically designed to handle the unique challenges
    of storing and serving high-dimensional vectors, offering optimizations that significantly
    improve memory usage, computation requirements, and</st> <st c="61727">search
    precision.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="61744">As we’ve mentioned previously, it is important to note that while
    the terms</st> *<st c="61821">vector database</st>* <st c="61836">and</st> *<st
    c="61841">vector store</st>* <st c="61853">are often used interchangeably, not
    all vector stores are necessarily databases.</st> <st c="61935">There are other
    tools and mechanisms that serve the same or similar purpose as a vector database.</st>
    <st c="62033">For the sake of accuracy and consistency with LangChain documentation,
    we will use the term</st> *<st c="62125">vector store</st>* <st c="62137">to refer
    to all mechanisms that store and serve vectors, including vector databases and
    other</st> <st c="62232">non-database solutions.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="62255">Next, let’s discuss the vector store options to give you a better
    grasp of what</st> <st c="62336">is available.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="62349">Common vector store options</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="62377">When</st> <st c="62383">choosing a vector store, consider factors
    such as scalability requirements, ease of setup and maintenance, performance needs,
    budget constraints, and the level of control and flexibility you require over
    the underlying infrastructure.</st> <st c="62618">Additionally, evaluate the integration
    options and supported programming languages to ensure compatibility with your
    existing</st> <st c="62744">technology stack.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="62761">There are quite a few vector stores, some from established database
    companies and communities, many that are new start-ups, many more that are appearing
    each day, and in all likelihood, some that will go out of business by the time
    you are reading this.</st> <st c="63016">It is a very active space!</st> <st c="63043">Stay
    vigilant and use the information in this chapter to understand the aspects that
    are most important to your specific RAG applications and then look at the current
    marketplace to determine which option works best</st> <st c="63259">for you.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="63267">We will focus on the vector stores that have established integration
    with LangChain, and even then, we will pair them down to not overwhelm you while
    also giving you enough options so that you can get a sense of what kinds of options
    are available.</st> <st c="63517">Keep in mind that these vector stores are adding
    features and improvements all the time.</st> <st c="63606">Be sure to look up
    their latest versions before making a selection!</st> <st c="63674">It could make
    all the difference you need to change your mind and make a</st> <st c="63747">better
    choice!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="63761">In the following subsections, we will walk through some common
    vector store options that integrate with LangChain, along with what you should
    consider about each one during the</st> <st c="63939">selection process.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="63957">Chroma</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**<st c="63964">Chroma</st>** <st c="63971">is an</st> <st c="63978">open source
    vector database.</st> <st c="64007">It offers fast search performance and supports
    easy integration with LangChain through its Python SDK.</st> <st c="64110">Chroma</st>
    <st c="64117">stands out for its simplicity and ease of use, with a straightforward
    API and support for dynamic filtering of collections during search.</st> <st c="64255">It
    also offers built-in support for document chunking and indexing, making it convenient
    for working with large text datasets.</st> <st c="64382">Chroma is a good choice
    if you prioritize simplicity and want an open source solution that can be self-hosted.</st>
    <st c="64493">However, it may not have as many advanced features as some other
    options, such as distributed search, support for multiple indexing algorithms,
    and built-in hybrid search capabilities that combine vector similarity with</st>
    <st c="64713">metadata filtering.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="64732">LanceDB</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**<st c="64740">LanceDB</st>** <st c="64748">is a</st> <st c="64754">vector
    database designed for efficient</st> <st c="64792">similarity search and retrieval.</st>
    <st c="64826">It stands out for its hybrid search capabilities, combining vector
    similarity search with traditional keyword-based search.</st> <st c="64950">LanceDB
    supports various distance metrics and indexing algorithms, including</st> **<st
    c="65027">Hierarchical navigable small world</st>** <st c="65061">(</st>**<st
    c="65063">HNSW</st>**<st c="65067">) for efficient approximate nearest neighbor
    search.</st> <st c="65121">It</st> <st c="65124">integrates with LangChain and
    offers fast search performance and support for various indexing techniques.</st>
    <st c="65230">LanceDB is a good choice if you want a dedicated vector database
    with good performance and integration with LangChain.</st> <st c="65349">However,
    it may not have as large of a community or ecosystem compared to some</st> <st
    c="65428">other options.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="65442">Milvus</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**<st c="65449">Milvus</st>** <st c="65456">is an</st> <st c="65463">open source
    vector database that provides</st> <st c="65504">scalable similarity search and
    supports various indexing algorithms.</st> <st c="65574">It provides a cloud-native
    architecture and supports Kubernetes-based deployments for scalability and high
    availability.</st> <st c="65695">Milvus offers features such as multi-vector indexing,
    allowing you to search across multiple vector fields simultaneously, and provides
    a plugin system for extending its functionality.</st> <st c="65880">It integrates
    well with LangChain and offers distributed deployment and horizontal scalability.</st>
    <st c="65976">Milvus is a good fit if you need a scalable and feature-rich open
    source vector store.</st> <st c="66063">However, it may require more setup and
    management compared to</st> <st c="66125">managed services.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="66142">pgvector</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**<st c="66151">pgvector</st>** <st c="66160">is an</st> <st c="66166">extension
    for PostgreSQL that adds support</st> <st c="66209">for vector similarity search
    and integrates with LangChain as a vector store.</st> <st c="66288">It leverages
    the power and reliability of PostgreSQL, the world’s most advanced open source
    relational database, and benefits from PostgreSQL’s mature ecosystem, extensive
    documentation, and strong community support.</st> <st c="66505">pgvector seamlessly
    integrates vector similarity search with traditional relational database features,
    enabling hybrid</st> <st c="66624">search capabilities.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="66644">Recent updates have improved the level of performance for pgvector
    to bring it in line with other dedicated vector database services.</st> <st c="66779">Given
    that PostgreSQL is the most popular database in the world (a battle-tested mature
    technology that has a huge community) and that the vector extension pgvector gives
    you all the capabilities of other vector databases, this combination offers a
    great solution for any company already</st> <st c="67067">using PostgreSQL.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="67084">Pinecone</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**<st c="67093">Pinecone</st>** <st c="67102">is a</st> <st c="67108">fully
    managed vector database service</st> <st c="67145">that offers high performance,
    scalability, and easy integration with LangChain.</st> <st c="67226">It provides
    a fully managed and serverless experience, abstracting away the complexities of
    infrastructure management.</st> <st c="67345">Pinecone offers features such as
    real-time indexing, allowing you to update and search vectors with low latency,
    and supports hybrid search, combining vector similarity with metadata filtering.</st>
    <st c="67539">It also provides features such as distributed search and support
    for multiple indexing algorithms.</st> <st c="67638">Pinecone is a good choice
    if you want a managed solution with good performance and minimal setup.</st> <st
    c="67736">However, it may be more expensive compared to</st> <st c="67782">self-hosted
    options.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="67802">Weaviate</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**<st c="67811">Weaviate</st>** <st c="67820">is an</st> <st c="67827">open
    source vector search engine that supports</st> <st c="67873">various vector indexing
    and similarity search algorithms.</st> <st c="67932">It follows a schema-based
    approach, allowing you to define a semantic data model for your vectors.</st>
    <st c="68031">Weaviate supports CRUD operations, data validation, and authorization
    mechanisms, and offers modules for common machine learning tasks such as text
    classification and image similarity search.</st> <st c="68223">It integrates with
    LangChain and offers features such as schema management, real-time indexing, and
    a GraphQL API.</st> <st c="68338">Weaviate is a good fit if you want an open source
    vector search engine with advanced features and flexibility.</st> <st c="68449">However,
    it may require more setup and configuration compared to</st> <st c="68514">managed
    services.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="68531">In the</st> <st c="68538">preceding subsections, we discussed
    various vector store options that integrate with LangChain, providing an overview
    of their features, strengths, and considerations for selection.</st> <st c="68721">This</st>
    <st c="68726">emphasizes the importance of evaluating factors such as scalability,
    ease of use, performance, budget, and compatibility with existing technology stacks
    when choosing a vector store.</st> <st c="68909">While broad, this list is still
    very short compared to the overall number of options available to integrate with
    LangChain and to use as vector stores</st> <st c="69060">in general.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="69071">The vector stores mentioned span a range of capabilities, including
    fast similarity search, support for various indexing algorithms, distributed architectures,
    hybrid search combining vector similarity with metadata filtering, and integration
    with other services and databases.</st> <st c="69350">Given the rapid evolution
    of the vector store landscape, new options are emerging frequently.</st> <st c="69444">Use
    this information as a base, but when you are ready to build your next RAG system,
    we highly recommend you visit the LangChain documentation on available vector
    stores, and consider which option best suits your needs at</st> <st c="69667">that
    time.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="69677">In the next section, we will talk more in-depth about considerations
    when choosing a vector store for your</st> <st c="69785">RAG system.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="69796">Choosing a vector store</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="69820">Selecting</st> <st c="69830">the right vector store for a RAG
    system involves considering several factors, including the scale of the data,
    the required search performance (speed and accuracy), and the complexity of the
    vector operations.</st> <st c="70041">Scalability is crucial for applications
    dealing with large datasets, requiring a mechanism that can efficiently manage
    and retrieve vectors from a growing corpus.</st> <st c="70204">Performance considerations
    involve evaluating the database’s search speed and its ability to return highly</st>
    <st c="70311">relevant results.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="70328">Moreover, the ease of integration with existing RAG models and
    the flexibility to support various vector operations are also critical.</st> <st
    c="70464">Developers should look for vector stores that offer robust APIs, comprehensive
    documentation, and strong community or vendor support.</st> <st c="70598">As listed
    previously, there are many popular vector stores, each offering unique features
    and optimizations tailored to different use cases and</st> <st c="70742">performance
    needs.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="70760">When choosing a vector store, it’s essential to align the selection
    with the overall architecture and operational requirements of the RAG system.</st>
    <st c="70907">Here are some</st> <st c="70921">key considerations:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="70940">Compatibility with existing infrastructure</st>**<st c="70983">:
    When evaluating vector stores, it’s crucial to consider how well they integrate
    with your existing data infrastructure, such as databases, data warehouses, and
    data lakes.</st> <st c="71158">Assess the compatibility of the vector store with
    your current technology stack and the skills of your development team.</st> <st
    c="71279">For example, if you have strong expertise in a particular database system
    such as PostgreSQL, a vector store extension such as pgvector</st> <st c="71414">might
    be a suitable choice, as it allows for seamless integration and leverages your
    team’s</st> <st c="71507">existing knowledge.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="71526">Scalability and performance</st>**<st c="71554">: What is the
    vector store’s ability to handle the expected growth of your data and the performance
    requirements of your RAG system?</st> <st c="71688">Assess the indexing and search
    capabilities of the vector store, ensuring it can deliver the desired level of
    performance and accuracy.</st> <st c="71824">If you anticipate a large-scale deployment,
    distributed vector databases such as Milvus or Elasticsearch with vector plugins
    might be more appropriate, as they are designed to handle high data volumes and
    provide efficient</st> <st c="72047">search throughput.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="72065">Ease of use and maintenance</st>**<st c="72093">: What is the
    learning curve associated with the vector store, taking into account the available
    documentation, community support, and vendor support?</st> <st c="72245">Understand
    the effort required for setup, configuration, and ongoing maintenance of the vector
    store.</st> <st c="72347">Fully managed services such as Pinecone can simplify
    deployment and management, reducing the operational burden on your team.</st>
    <st c="72473">On the other hand, self-hosted solutions such as Weaviate provide
    more control and flexibility, allowing for customization and fine-tuning to meet
    your</st> <st c="72625">specific requirements.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="72647">Data security and compliance</st>**<st c="72676">: Evaluate
    the security features and access controls provided by the vector store, ensuring
    they align with your industry’s compliance requirements.</st> <st c="72826">If
    you deal with sensitive data, assess the encryption and data protection capabilities
    of the vector store.</st> <st c="72935">Consider the vector store’s ability to
    meet data privacy regulations and standards, such as GDPR or HIPAA, depending
    on your</st> <st c="73060">specific needs.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="73075">Cost and licensing</st>**<st c="73094">: What is the pricing
    model of the vector store?</st> <st c="73144">Is it based on data volume, search
    operations, or a combination of factors?</st> <st c="73220">Consider the long-term
    cost-effectiveness of the vector store, taking into account the scalability and
    growth projections of your RAG system.</st> <st c="73362">Assess the licensing
    fees, infrastructure costs, and maintenance expenses associated with the vector
    store.</st> <st c="73470">Open source solutions may have lower upfront costs but
    require more in-house expertise</st> <st c="73557">and resources for maintenance,
    while managed services may have higher subscription fees but offer simplified
    management</st> <st c="73677">and support.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="73689">Ecosystem and integrations</st>**<st c="73716">: When selecting
    a vector store, it’s important to evaluate the ecosystem and integrations it supports.</st>
    <st c="73821">Consider the availability of client libraries, SDKs, and APIs for
    different programming languages, as this can greatly simplify the development
    process and enable seamless integration with your existing code base.</st> <st
    c="74035">Assess the compatibility of the vector store with other tools and frameworks
    commonly used in RAG systems, such as NLP libraries or machine learning frameworks.</st>
    <st c="74196">The general size of the supporting community is important as well;
    make sure it is at a critical mass to grow and thrive.</st> <st c="74318">A vector
    store with a robust ecosystem and extensive integrations can provide more flexibility
    and opportunities for extending the functionality of your</st> <st c="74471">RAG
    system.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="74482">By carefully evaluating these factors and aligning them with your
    specific requirements, you can make an informed decision when choosing a vector
    store for your RAG system.</st> <st c="74656">It’s important to conduct thorough
    research, benchmark different options, and consider the long-term implications
    of your choice in terms of scalability, performance,</st> <st c="74823">and maintainability.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="74843">Remember that the choice of vector store is not a one-size-fits-all
    decision, and it may evolve as your RAG system grows and your requirements change.</st>
    <st c="74995">It’s crucial to periodically reassess your vector store selection
    and adjust as needed to ensure optimal performance and alignment with your overall</st>
    <st c="75144">system architecture.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="75164">Summary</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="75172">The integration of vectors and vector stores into RAG systems
    is foundational for enhancing the efficiency and accuracy of information retrieval
    and generation tasks.</st> <st c="75340">By carefully selecting and optimizing
    your vectorization approach, as well as your vector store, you can significantly
    improve the performance of your RAG system.</st> <st c="75503">Vectorization techniques
    and vector stores are only part of how vectors play a role in RAG systems; they
    also play a major role in our retrieval stage.</st> <st c="75655">In the next
    chapter, we will address the retrieval role vectors play, going in-depth on the
    subject of vector similarity search algorithms</st> <st c="75794">and services.</st>
  prefs: []
  type: TYPE_NORMAL
