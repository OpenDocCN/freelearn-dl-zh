- en: <st c="0">7</st>
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: <st c="2">The Key Role Vectors and Vector Stores Play in RAG</st>
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量和向量存储在RAG中扮演的关键角色
- en: '**<st c="52">Vectors</st>** <st c="60">are a</st> <st c="66">key component
    of</st> **<st c="84">retrieval-augmented generation</st>** <st c="114">(</st>**<st
    c="116">RAG</st>**<st c="119">) to understand, as they are the secret ingredient
    that helps the entire process work well.</st> <st c="212">In this chapter, we
    dive back into our code from previous chapters with an emphasis on how it is impacted
    by vectors.</st> <st c="330">In simplistic terms, this chapter will talk about
    what a vector is, how vectors are created, and</st> <st c="426">then where to
    store them.</st> <st c="453">In more technical terms, we will talk about vectors,</st>
    **<st c="506">vectorization</st>**<st c="519">, and</st> **<st c="525">vector
    stores</st>**<st c="538">. This chapter is all about vector creation and why</st>
    <st c="590">they are important.</st> <st c="610">We are going to focus on how
    vectors relate to RAG, but we encourage you to spend more time and research gaining
    as in-depth of an understanding about vectors as you can.</st> <st c="781">The
    more you understand vectors, the more effective you will be at improving your</st>
    <st c="863">RAG pipelines.</st>'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**<st c="52">向量</st>** <st c="60">是</st> <st c="66">检索增强生成</st> **<st c="84">（RAG）</st>
    的一个关键组成部分，需要理解，因为它们是帮助整个过程顺利进行的秘密成分。</st> <st c="212">在本章中，我们将重新审视前几章的代码，重点关注向量对其的影响。</st>
    <st c="330">简单来说，本章将讨论向量是什么，向量是如何创建的，以及</st> <st c="426">然后在哪里存储它们。</st> <st c="453">从更技术性的角度来说，我们将讨论向量、</st>
    **<st c="506">向量化</st>**<st c="519">，以及</st> **<st c="525">向量存储</st>**<st c="538">。本章全部关于向量的创建以及为什么</st>
    <st c="590">它们很重要。</st> <st c="610">我们将关注向量与RAG的关系，但我们鼓励你花更多的时间和精力去深入研究向量，尽可能获得深入的理解。</st>
    <st c="781">你对向量的理解越深入，你在改进你的</st> <st c="863">RAG流水线</st> 时就会越有效。'
- en: <st c="877">The vector discussion is so important, though, that we will span
    it across two chapters.</st> <st c="967">While this chapter focuses on vectors
    and vector stores,</st> [*<st c="1024">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)
    <st c="1033">will focus on vector searches, which is to say how the vectors are
    used in a</st> <st c="1111">RAG system.</st>
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 向量讨论的重要性如此之高，以至于我们将它扩展到两个章节中。</st> <st c="967">虽然本章重点讨论向量和向量存储，</st> [*<st c="1024">第8章</st>](B22475_08.xhtml#_idTextAnchor152)
    <st c="1033">将重点讨论向量搜索，也就是说向量如何在RAG系统中被使用。</st>
- en: <st c="1122">Specifically, we will cover the following topics in</st> <st c="1175">this
    chapter:</st>
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将具体涵盖以下主题：<st c="1122">以下：</st>
- en: <st c="1188">Fundamentals of vectors</st> <st c="1213">in RAG</st>
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG中的向量基础
- en: <st c="1219">Where vectors lurk in</st> <st c="1242">your code</st>
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量在你代码中的位置
- en: <st c="1251">The amount of text you</st> <st c="1275">vectorize matters!</st>
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你向量化的文本量很重要！
- en: <st c="1293">Not all semantics are</st> <st c="1316">created equal!</st>
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并非所有语义都是平等的！
- en: <st c="1330">Common</st> <st c="1338">vectorization techniques</st>
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见的向量化技术
- en: <st c="1362">Selecting a</st> <st c="1375">vectorization option</st>
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择向量化选项
- en: <st c="1395">Getting started with</st> <st c="1417">vector stores</st>
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用向量存储
- en: <st c="1430">Vector stores</st>
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量存储
- en: <st c="1444">Choosing a</st> <st c="1456">vector store</st>
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个向量存储
- en: <st c="1468">Technical requirements</st>
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: <st c="1491">Going back to the code we have discussed over the past chapters,
    this chapter focuses on just this line</st> <st c="1596">of code:</st>
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们在过去章节中讨论的代码，本章将重点讨论这一行代码：<st c="1596">代码：</st>
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: <st c="1689">The code for this chapter is</st> <st c="1719">here:</st> [<st
    c="1725">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_07</st>](https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_07
    )
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码在此：<st c="1689">这里：</st> [<st c="1725">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_07</st>](https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_07
    )
- en: <st c="1822">The filename</st> <st c="1836">is</st> `<st c="1839">CHAPTER7-1_COMMON_VECTORIZATION_TECHNIQUES.ipynb</st>`<st
    c="1887">.</st>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 文件名 <st c="1822">是</st> `<st c="1839">CHAPTER7-1_COMMON_VECTORIZATION_TECHNIQUES.ipynb</st>`<st
    c="1887">。</st>
- en: <st c="1888">And</st> [*<st c="1893">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)
    <st c="1902">will focus on just this line</st> <st c="1932">of code:</st>
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="1888">并且</st> [*<st c="1893">第 8 章</st>*](B22475_08.xhtml#_idTextAnchor152)
    <st c="1902">将专注于这一行</st> <st c="1932">代码：</st>
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: <st c="1979">Is that it?</st> <st c="1992">Just those two lines of code for
    two chapters?</st> <st c="2039">Yes!</st> <st c="2044">That shows you how important
    vectors are to the RAG system.</st> <st c="2104">And to thoroughly understand
    vectors, we start with the fundamentals and build up</st> <st c="2186">from there.</st>
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="1979">就这样了吗？</st> <st c="1992">就这两行代码对应两章内容吗？</st> <st c="2039">是的！</st>
    <st c="2044">这显示了向量对 RAG 系统的重要性。</st> <st c="2104">为了彻底理解向量，我们从基础开始，并在此基础上构建。</st>
- en: <st c="2197">Let’s</st> <st c="2204">get started!</st>
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="2197">让我们</st> <st c="2204">开始吧！</st>
- en: <st c="2216">Fundamentals of vectors in RAG</st>
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="2216">RAG 中向量的基础</st>
- en: <st c="2247">In this</st> <st c="2256">section, we</st> <st c="2267">will cover
    several important topics related to vectors and embeddings in the context of</st>
    **<st c="2356">natural language processing</st>** <st c="2383">(</st>**<st c="2385">NLP</st>**<st
    c="2388">) and RAG.</st> <st c="2400">We will begin by clarifying the relationship
    between vectors and embeddings, explaining that embeddings are a specific type
    of vector representation used in NLP.</st> <st c="2562">We then discuss the properties
    of vectors, such as their dimensions and size, and how these characteristics impact
    the precision and effectiveness of text search and</st> <st c="2728">similarity
    comparisons.</st>
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="2247">在本节中，我们</st> <st c="2256">将涵盖与向量、嵌入在自然语言处理</st> **<st c="2356">（NLP</st>**<st
    c="2383">）和 RAG 的相关的重要主题。</st> <st c="2400">我们将首先阐明向量和嵌入之间的关系，解释嵌入是 NLP 中使用的特定类型的向量表示。</st>
    <st c="2562">然后，我们将讨论向量的属性，如它们的维度和大小，以及这些特征如何影响文本搜索和</st> <st c="2728">相似度比较的精度和有效性。</st>
- en: <st c="2751">What is the difference between embeddings and vectors?</st>
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="2751">嵌入和向量之间的区别是什么？</st>
- en: <st c="2806">Vectors</st> <st c="2815">and</st> **<st c="2819">embeddings</st>**
    <st c="2829">are key concepts in NLP and play a crucial role in building language
    models and RAG systems.</st> <st c="2923">But what are they and how do they relate
    to each other?</st> <st c="2979">To put it simply, you can think of embeddings
    as a specific type of vector representation.</st> <st c="3070">When we are talking
    about the</st> **<st c="3100">large language models</st>** <st c="3121">(</st>**<st
    c="3123">LLMs</st>**<st c="3127">) we</st> <st c="3133">use in RAG, which are
    part of a larger universe called NLP, the vectors we use are referred to as embeddings.</st>
    <st c="3243">Vectors on the other hand, in general, are used</st> <st c="3291">across
    a broad variety of fields and can represent many other objects beyond just language
    constructs (such as words, sentences, paragraphs, etc.).</st> <st c="3439">When
    talking about RAG, words such as embeddings, vectors, vector embeddings, and embedding
    vectors can be</st> <st c="3546">used interchangeably!</st>
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="2806">向量和</st> **<st c="2819">嵌入</st>** <st c="2829">是自然语言处理（NLP）和 RAG
    系统中的关键概念，在构建语言模型和 RAG 系统中发挥着至关重要的作用。</st> <st c="2923">但它们是什么，它们之间有什么关系呢？</st>
    <st c="2979">简单来说，你可以将嵌入视为一种特定的向量表示。</st> <st c="3070">当我们谈论 RAG 中使用的</st> **<st
    c="3100">大型语言模型</st>** <st c="3121">（**<st c="3123">LLMs</st>**<st c="3127">）时，它们是
    NLP 这个更大宇宙的一部分，我们使用的向量被称为嵌入。</st> <st c="3243">另一方面，一般来说，向量被广泛应用于各种领域，可以代表许多其他对象，而不仅仅是语言结构（如单词、句子、段落等）。</st>
    <st c="3439">在谈论 RAG 时，嵌入、向量、向量嵌入和嵌入向量这些词可以</st> <st c="3546">互换使用！</st>
- en: <st c="3567">Now that we have that out of the way, let’s talk about what a vector</st>
    <st c="3637">actually is.</st>
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="3567">现在我们已经解决了这个问题，让我们来谈谈向量实际上是什么。</st> <st c="3637">是什么。</st>
- en: <st c="3649">What is a vector?</st>
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="3649">什么是向量？</st>
- en: <st c="3667">What is</st> <st c="3676">the first thing you think of when you
    hear the word</st> *<st c="3728">vector</st>*<st c="3734">? Many people would
    say math.</st> <st c="3764">That would be accurate; vectors are literally mathematical
    representations of the text we work with in our data, and they allow us to apply
    mathematical operations to our data in new and very</st> <st c="3956">useful ways.</st>
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="3667">当你听到单词</st> <st c="3676">向量</st> <st c="3734">时，你首先想到的是什么？许多人会说数学。</st>
    <st c="3764">这将是准确的；向量实际上是我们在数据处理中使用的文本的数学表示，并且它们允许我们以新的和非常</st> <st c="3956">有用的方式对我们的数据进行数学运算。</st>
- en: <st c="3968">The word</st> *<st c="3978">vector</st>* <st c="3984">might also
    make you think of speed.</st> <st c="4021">That is also accurate; with vectors,
    we can conduct text search at significantly faster speeds than with any other
    technology that preceded</st> <st c="4161">vector search.</st>
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="3968">单词</st> *<st c="3978">向量</st> <st c="3984">也可能让你想到速度。</st> <st
    c="4021">这也是准确的；与向量相比，我们可以以比任何先于向量搜索的技术都要快的速度进行文本搜索。</st>
- en: <st c="4175">Another concept that is often associated with the word</st> *<st
    c="4231">vector</st>* <st c="4237">is precision.</st> <st c="4252">By converting
    text into embeddings that have semantic representation, we can significantly improve
    the precision of our search systems in finding what we are</st> <st c="4410">looking
    for.</st>
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="4175">与单词</st> *<st c="4231">向量</st> <st c="4237">经常相关联的另一个概念是精度。</st>
    <st c="4252">通过将文本转换为具有语义表示的嵌入，我们可以显著提高我们搜索系统的精度，以便找到我们正在寻找的内容。</st> <st c="4410">我们正在寻找的内容。</st>
- en: <st c="4422">And of course, if you are a fan of the movie</st> *<st c="4468">Despicable
    Me</st>* <st c="4481">from Illumination, you may think of the villain Vector,
    who describes himself as “</st>*<st c="4564">I go by the name of… Vector.</st>
    <st c="4594">It’s a mathematical term, a quantity represented by an arrow with
    both direction</st>* *<st c="4675">and magnitude</st>*<st c="4688">.”</st>
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="4422">当然，如果你是电影</st> *<st c="4468">《神偷奶爸》</st> <st c="4481">的粉丝，你可能会想到反派角色Vector，他自称是“</st>*<st
    c="4564">我名叫……Vector。</st> <st c="4594">这是一个数学术语，一个由箭头表示的具有方向</st>* *<st c="4675">和大小</st>*<st
    c="4688">。”</st>
- en: <st c="4690">He may be a villain doing questionable things, but he is right
    about the meaning behind his name!</st> <st c="4789">The key thing to take away
    from this description is that a vector is not just a bunch of numbers; it is a
    mathematical object that represents both magnitude and direction.</st> <st c="4961">This
    is why it does a better job of representing your text and similarities between
    text, as it captures a more complex form of them than just</st> <st c="5104">simple
    numbers.</st>
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="4690">他可能是一个做可疑事情的反派，但他对他名字背后的含义是正确的！从这个描述中我们可以得出的关键点是，向量不仅仅是一堆数字；它是一个数学对象，它代表了大小和方向。</st>
    <st c="4789">这也是为什么它比仅仅表示数字更能代表你的文本和文本之间的相似性，因为它捕捉了它们更复杂的形式。</st> <st c="4961">这就是为什么它比简单的数字更能代表你的文本和文本之间的相似性，因为它捕捉了它们更复杂的形式。</st>
    <st c="5104">这是为什么它比简单的数字更能代表你的文本和文本之间的相似性。</st>
- en: <st c="5119">This may give you an understanding of what a vector is, but let’s
    next discuss the important aspects of vectors that will have an impact on your
    RAG development, starting with</st> <st c="5296">vector size.</st>
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="5119">这可能会让你对向量是什么有一个理解，但接下来让我们讨论对RAG开发有影响的重要的向量方面，首先是</st> <st c="5296">向量大小。</st>
- en: <st c="5308">Vector dimensions and size</st>
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="5308">向量维度和大小</st>
- en: <st c="5335">Vector, the</st> <st c="5348">villain from</st> *<st c="5361">Despicable
    Me</st>*<st c="5374">, said that vectors are “</st>*<st c="5399">a quantity represented
    by an arrow</st>*<st c="5434">.” But while thinking of arrows representing vectors
    on a 2D or 3D graph makes it easier to comprehend what a vector is, it is important
    to</st> <st c="5573">understand that the vectors we work with are often represented
    in many more than just two or three dimensions.</st> <st c="5685">The number of
    dimensions in the vector is also referred to as the vector size.</st> <st c="5764">To
    see this in our code, we are going to add a new cell right below where we define
    our variables.</st> <st c="5863">This code will print out a small section of the</st>
    `<st c="5911">embedding</st>` <st c="5920">vector:</st>
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="5335">Vector，</st> <st c="5348">来自</st> *<st c="5361">《神偷奶爸》</st>*<st
    c="5374">的反派角色说，向量是“</st>*<st c="5399">一个由箭头表示的量</st>*<st c="5434">。”但尽管在二维或三维图上思考表示向量的箭头可以更容易地理解向量是什么，重要的是要</st>
    <st c="5573">理解我们处理的向量通常在两个或三个维度以上表示。</st> <st c="5685">向量的维度数也被称为向量大小。</st> <st
    c="5764">为了在我们的代码中看到这一点，我们将在定义我们的变量下方添加一个新的单元。</st> <st c="5863">此代码将打印出嵌入向量</st>
    `<st c="5911">的一部分</st>` <st c="5920">：</st>
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: <st c="6153">In this code, we take the question that we have used throughout
    our code examples,</st> `<st c="6237">What are the advantages of using RAG?</st>`<st
    c="6274">, and we use OpenAI’s embedding API to convert it into a vector representation.</st>
    <st c="6354">The</st> `<st c="6358">question_embedding</st>` <st c="6376">variable
    represents this embedding.</st> <st c="6413">Using a slice,</st> `<st c="6428">[0:5]</st>`<st
    c="6433">, we take the first five numbers from</st> `<st c="6471">question_embedding</st>`<st
    c="6489">, which represent the first five dimensions of the vector, and print
    them out.</st> <st c="6568">The full vector is 1,536 float numbers with 17–20
    digits each, so we will minimize how much is printed out to make it a little more
    manageable to read.</st> <st c="6720">The output of this cell will look</st> <st
    c="6754">like this:</st>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们采用了我们在代码示例中一直使用的**问题**，《<st c="6237">使用RAG的优势是什么？</st>》，并将其使用OpenAI的嵌入API转换为向量表示。</st>
    <st c="6354">问题嵌入</st> <st c="6376">变量代表这个嵌入。</st> <st c="6413">使用切片</st> `<st
    c="6428">[0:5]</st>`<st c="6433">，我们从</st> `<st c="6471">问题嵌入</st>`<st c="6489">中取出前五个数字，这代表向量的前五个维度，并将它们打印出来。</st>
    <st c="6568">完整的向量包含1,536个浮点数，每个数有17-20位数字，因此我们将打印的内容最小化，以便更容易阅读。</st> <st c="6720">这个单元格的输出将看起来</st>
    <st c="6754">像这样：</st>
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: <st c="6918">We only print out the first five dimensions here, but the embedding
    is much larger than that.</st> <st c="7013">We will talk about a practical way
    to determine the total number of dimensions in a moment, but first I want to draw
    your attention to the length of</st> <st c="7162">each number.</st>
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="6918">我们在这里只打印出前五个维度，但嵌入的大小远大于这个。</st> <st c="7013">我们将在稍后讨论确定维度总数的一种实用方法，但首先我想将你的注意力引向每个数字的长度。</st>
- en: <st c="7174">All numbers in these embeddings will be +/-0 with a decimal point,
    so let’s talk about the number of digits that come after that decimal point.</st>
    <st c="7319">The first number here,</st> `<st c="7342">-0.006319054113595048</st>`<st
    c="7363">, has 18 digits after the decimal point, the second number has 19, and
    the fourth number has 17\.</st> <st c="7460">These digit lengths are related to
    the precision of the floating-point representation used by OpenAI’s embeddings
    model,</st> `<st c="7581">OpenAIEmbeddings</st>`<st c="7597">. This model uses
    what is considered a high-precision floating-point format, providing 64-bit numbers
    (also known as</st> **<st c="7714">double-precision</st>**<st c="7730">).</st>
    <st c="7734">This high-precision results in</st> <st c="7765">the potential for
    very fine-grained distinctions and accurate representation of the semantic information
    captured by the</st> <st c="7886">embedding model.</st>
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="7174">这些嵌入中的所有数字都将有 +/-0的小数点，因此让我们谈谈小数点后有多少位数字。</st> <st c="7319">这里的第一个数字</st>
    `<st c="7342">-0.006319054113595048</st>`<st c="7363">，小数点后有18位数字，第二个数字有19位，第四个数字有17位。</st>
    <st c="7460">这些数字长度与OpenAI的嵌入模型</st> `<st c="7581">OpenAIEmbeddings</st>`<st c="7597">使用的浮点数表示精度有关。</st>
    <st c="7734">这个高精度格式提供了64位数字（也称为</st> **<st c="7714">双精度</st>**<st c="7730">）。</st>
    <st c="7734">这种高精度导致</st> <st c="7765">了非常精细的区分和准确表示嵌入模型捕获的语义信息。</st>
- en: <st c="7902">In addition, let’s revisit a point made in</st> [*<st c="7946">Chapter
    1</st>*](B22475_01.xhtml#_idTextAnchor015)<st c="7955">, that the preceding output
    looks a lot like a Python list of floating points.</st> <st c="8034">It actually
    is a Python list in this case, as that is what OpenAI returns from their embedding
    API.</st> <st c="8134">This is probably a decision to make it more compatible
    with the Python coding world.</st> <st c="8219">But to avoid confusion, it is
    important to understand that, typically in the machine learning world, when you
    see something like this in use that will be used for machine learning-related
    processing, it is typically a NumPy array, even though a list of numbers and a
    NumPy array look the same when printed out as output like we</st> <st c="8547">just
    did.</st>
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="7902">此外，让我们回顾一下在</st> [*<st c="7946">第一章</st>*](B22475_01.xhtml#_idTextAnchor015)<st
    c="7955">中提到的一个观点，即前面的输出看起来非常像Python的浮点数列表。</st> <st c="8034">实际上，在这种情况下它确实是一个Python列表，因为这是OpenAI从他们的嵌入API返回的内容。</st>
    <st c="8134">这可能是为了使其与Python编码世界更加兼容而做出的决定。</st> <st c="8219">但为了避免混淆，重要的是要理解，在机器学习领域，当你看到这种用于机器学习相关处理的内容时，它通常是NumPy数组，尽管数字列表和NumPy数组在打印输出时看起来相同，就像我们</st>
    <st c="8547">刚才做的那样。</st>
- en: <st c="8556">Fun fact</st>
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="8556">有趣的事实</st>
- en: <st c="8565">You will eventually hear about the</st> <st c="8600">concept called</st>
    **<st c="8616">quantization</st>** <st c="8628">if you work with generative AI.</st>
    <st c="8661">Much like embeddings, quantization deals with high-precision floating
    points.</st> <st c="8739">However, with quantization, the concept is to convert
    model parameters, such as weights and activations, from their original high-precision
    floating-point representation to a lower-precision format.</st> <st c="8938">This
    reduces the memory footprint and computational requirements of the LLM, which
    can be applied to make it more cost-effective to pre-train, train, and fine-tune
    the LLM.</st> <st c="9111">Quantization can also make it more cost-effective to
    perform inference with the LLM, which is what it is called when you use the LLM
    to get responses.</st> <st c="9262">When I say</st> *<st c="9273">cost-effective</st>*
    <st c="9287">in this context, I am referring to being able to do these things
    in a</st> <st c="9357">smaller, less expensive hardware environment.</st> <st
    c="9404">There is a trade-off, though; quantization is a</st> **<st c="9452">lossy
    compression technique</st>**<st c="9479">, which means that some of the information
    is lost during the conversion process.</st> <st c="9561">The reduced precision
    of the quantized LLMs may result in a loss of accuracy compared to the original</st>
    <st c="9663">high-precision LLMs.</st>
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="8565">如果你与生成式AI一起工作，你最终会听到被称为</st> <st c="8600">量化</st> **<st c="8616">的概念</st>
    <st c="8628">。如果你与生成式AI一起工作。</st> <st c="8661">与嵌入类似，量化处理高精度浮点数。</st> <st c="8739">然而，在量化中，概念是将模型参数，如权重和激活，从它们原始的高精度浮点表示转换为低精度格式。</st>
    <st c="8938">这减少了LLM的内存占用和计算需求，这可以应用于使其在预训练、训练和微调LLM时更具成本效益。</st> <st c="9111">量化还可以使使用LLM进行推理更具成本效益，这就是当你使用LLM获取响应时所说的。</st>
    <st c="9262">当我在这句话中说</st> *<st c="9273">成本效益</st>* <st c="9287">时，我指的是能够在</st>
    <st c="9357">更小、更便宜的硬件环境中完成这些事情。</st> <st c="9404">然而，有一个权衡；量化是一种</st> **<st c="9452">有损压缩技术</st>**<st
    c="9479">，这意味着在转换过程中会丢失一些信息。</st> <st c="9561">量化LLM的降低精度可能与原始</st> <st c="9663">高精度LLM相比导致精度损失。</st>
- en: <st c="9683">When you are using RAG and considering different algorithms to
    convert your text into embeddings, take note of the length of the embedding values
    to make sure you are using a high-precision floating-point format if the accuracy
    and quality of response are of high priority in your</st> <st c="9965">RAG system.</st>
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="9683">当你在使用RAG并考虑将文本转换为嵌入的不同算法时，请注意嵌入值的长度，以确保如果你在RAG系统中对准确性和响应质量有较高要求，你正在使用高精度浮点格式。</st>
    <st c="9965">RAG系统。</st>
- en: <st c="9976">But how many dimensions are represented by these embeddings?</st>
    <st c="10038">We only show five in the preceding example, but we could have printed
    them all out and counted them individually.</st> <st c="10152">This, of course,
    seems impractical.</st> <st c="10188">We will use the</st> `<st c="10204">len()</st>`
    <st c="10209">function to do the counting for us.</st> <st c="10246">In the following
    code, you see that helpful function put to good use, giving us the total size
    of</st> <st c="10344">this embedding:</st>
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="9976">但是这些嵌入表示了多少维度呢？</st> <st c="10038">在前面的例子中，我们只展示了五个，但我们本可以将它们全部打印出来并单独计数。</st>
    <st c="10152">这当然看起来不太实际。</st> <st c="10188">我们将使用</st> `<st c="10204">len()</st>`
    <st c="10209">函数来为我们计数。</st> <st c="10246">在下面的代码中，你可以看到这个有用的函数被很好地利用，给出了这个嵌入的总大小：</st>
    <st c="10344">如下：</st>
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: <st c="10443">The output of this code is</st> <st c="10471">as follows:</st>
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="10443">此代码的输出如下：</st> <st c="10471">如下：</st>
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: <st c="10503">This indicates that this embedding is 1,536 dimensions!</st> <st
    c="10560">Trying to visualize this in your mind is difficult when we typically
    only think in 3 dimensions at most, but these extra 1,533 dimensions make a significant
    difference in how precise our embedding semantic representations of the related
    text</st> <st c="10802">can be.</st>
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="10503">这表明这个嵌入是1,536维度！</st> <st c="10560">当我们通常最多只考虑3维时，在脑海中尝试可视化这很困难，但这些额外的1,533维度在如何精确地表示相关文本的嵌入语义表示方面产生了显著差异。</st>
- en: <st c="10809">When working with vectors across most modern vectorization algorithms,
    there are often hundreds, or thousands, of dimensions.</st> <st c="10936">The
    number of dimensions is equal to the number of floating points that represent
    the embedding, meaning a 1,024-dimension vector is represented by 1,024 floating
    points.</st> <st c="11107">There is no hard limit to how long an embedding can
    be, but some of the modern vectorizing algorithms tend to have preset sizes.</st>
    <st c="11236">The model we are using, OpenAI’s</st> `<st c="11269">ada</st>` <st
    c="11272">embedding model, uses 1,536 by default.</st> <st c="11313">This is because
    it is trained to produce a certain-sized embedding, and if you try to truncate
    that size, it changes the context captured in</st> <st c="11454">the embedding.</st>
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="10809">在大多数现代向量化算法中处理向量时，通常会有数百或数千个维度。</st> <st c="10936">维度的数量等于表示嵌入的浮点数的数量，这意味着一个1,024维度的向量由1,024个浮点数表示。</st>
    <st c="11107">嵌入的长度没有硬性限制，但一些现代向量化算法倾向于预设大小。</st> <st c="11236">我们使用的模型，OpenAI的</st>
    `<st c="11269">ada</st>` <st c="11272">嵌入模型，默认使用1,536。</st> <st c="11313">这是因为它是训练来产生特定大小的嵌入，如果你尝试截断该大小，它将改变嵌入中捕获的上下文。</st>
    <st c="11454">的嵌入。</st>
- en: <st c="11468">However, this is changing.</st> <st c="11496">New vectorizers
    are now available (such as the OpenAI</st> `<st c="11550">text-embedding-3-large</st>`
    <st c="11572">model) that enable you to change vector sizes.</st> <st c="11620">These
    embedding models were trained to provide the same context, relatively speaking
    across the different vector dimension sizes.</st> <st c="11750">This enables a</st>
    <st c="11765">technique called</st> **<st c="11782">adaptive retrieval</st>**<st
    c="11800">.</st>
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="11468">然而，这种情况正在改变。</st> <st c="11496">现在有新的向量化工具可用（例如OpenAI的</st> `<st
    c="11550">text-embedding-3-large</st>` <st c="11572">模型），它允许你更改向量大小。</st> <st
    c="11620">这些嵌入模型被训练来在不同向量维度大小上提供相对相同的内容。</st> <st c="11750">这使一种称为</st> **<st
    c="11782">自适应检索</st>**<st c="11800">的技术成为可能。</st>
- en: <st c="11801">With adaptive retrieval, you generate multiple sets of embeddings
    at different sizes.</st> <st c="11888">You first search the lower-dimension vectors
    to get you</st> *<st c="11944">close</st>* <st c="11949">to the final results,
    because searching lower-dimension vectors is much faster than searching higher-dimension
    vectors.</st> <st c="12070">Once your lower-dimension search gets you into proximity
    of the content most similar to your input inquiry, your search</st> *<st c="12190">adapts</st>*
    <st c="12196">to searching the slower search-speed, higher-dimension embeddings
    to target the most relevant content and finalize the similarity search.</st> <st
    c="12335">Overall, this can increase your search speeds by 30–90%, depending on
    how you set up the search.</st> <st c="12432">The embeddings generated by this
    technique are called</st> **<st c="12486">Matryoshka embeddings</st>**<st c="12507">,
    named</st> <st c="12514">after the Russian nesting dolls, reflecting that the
    embeddings, like the dolls, are all relatively identical to each other while varying
    in size.</st> <st c="12662">If you ever need to optimize a RAG pipeline in a production
    environment for heavy usage, you are going to want to consider</st> <st c="12785">this
    technique.</st>
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="11801">使用自适应检索，你会在不同大小下生成多组嵌入。</st> <st c="11888">你首先搜索低维向量以帮助你</st>
    *<st c="11944">接近</st>* <st c="11949">最终结果，因为搜索低维向量比搜索高维向量快得多。</st> <st c="12070">一旦你的低维搜索将你带入与你的输入查询最相似的内容附近，你的搜索</st>
    *<st c="12190">适应</st>* <st c="12196">到搜索速度较慢、维度较高的嵌入，以定位最相关的内容并最终完成相似度搜索。</st>
    <st c="12335">总的来说，这可以提高你的搜索速度30-90%，具体取决于你如何设置搜索。</st> <st c="12432">这种技术生成的嵌入被称为</st>
    **<st c="12486">套娃嵌入</st>**<st c="12507">，得名于俄罗斯套娃，反映了嵌入，就像娃娃一样，彼此之间相对相同，但在大小上有所不同。</st>
    <st c="12662">如果你需要在生产环境中优化用于重用场景的RAG管道，你将需要考虑</st> <st c="12785">这种技术。</st>
- en: <st c="12800">The next concept that will be important for you to understand
    is where in the code your vectors reside, helping you to apply the concepts you
    are learning about vectors directly to your</st> <st c="12987">RAG efforts.</st>
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="12800">下一个你需要理解的概念是代码中你的向量所在的位置，这有助于你将你正在学习的关于向量的概念直接应用到你的</st> <st c="12987">RAG努力中。</st>
- en: <st c="12999">Where vectors lurk in your code</st>
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="12999">你的代码中向量潜伏之处</st>
- en: <st c="13031">One way to</st> <st c="13043">indicate the value of vectors in
    the RAG system is to show you all the places they are used.</st> <st c="13136">As
    discussed earlier, you start with your text data and convert it to vectors during
    the vectorization process.</st> <st c="13248">This occurs in the indexing stage
    of the RAG system.</st> <st c="13301">But, in most cases, you must have somewhere
    to put those embedding vectors, which brings in the concept of the</st> <st c="13412">vector
    store.</st>
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13425">During the retrieval stage of the RAG system, you start with a
    question as input from the user, which is first converted to an embedding vector
    before the retrieval begins.</st> <st c="13599">Lastly, the retrieval process
    uses a similarity algorithm that determines the proximity between the question
    embedding and all the embeddings in the vector store.</st> <st c="13762">There
    is one more potential area in which vectors are common and that is when you want
    to evaluate your RAG responses, but we will cover that in</st> [*<st c="13907">Chapter
    9</st>*](B22475_09.xhtml#_idTextAnchor184) <st c="13916">when we cover evaluation
    techniques.</st> <st c="13954">For now, let’s dive deeper into each of these other
    concepts, starting</st> <st c="14025">with vectorization.</st>
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14044">Vectorization occurs in two places</st>
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="14079">At the very front of</st> <st c="14101">the RAG process, you typically
    have a mechanism for a user to enter a question that is passed to the retriever.</st>
    <st c="14213">We see the processing of this occurring in our</st> <st c="14260">code
    here:</st>
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: <st c="14406">The</st> <st c="14410">retriever is a LangChain</st> `<st c="14436">retriever</st>`
    <st c="14445">object that facilitates similarity search and retrieval of relevant
    vectors based on the user query.</st> <st c="14547">So when we talk about vectorization,
    it actually occurs in two places in</st> <st c="14620">our code:</st>
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14629">First, when we vectorize the original data that will be used in
    the</st> <st c="14698">RAG system</st>
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="14708">Second, when we need to vectorize the</st> <st c="14747">user
    query</st>
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '<st c="14757">The relationship between these two separate steps is that they
    are both used in the similarity search.</st> <st c="14861">Before we talk about
    the search, though, let’s first talk about where the latter group of embeddings,
    the embeddings from the original data, is stored: the</st> <st c="15017">vector
    store.</st>'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15030">Vector databases/stores store and contain vectors</st>
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="15080">A vector store</st> <st c="15095">is typically a vector database
    (but not always, see the following note) that is optimized for storing and serving
    vectors, and plays a crucial role in an effective RAG system.</st> <st c="15272">Technically,
    you could build a RAG system without using a vector database, but you would miss
    out on a lot of the optimizations that have been built into these data storage
    tools, impacting your memory, computation requirements, and search</st> <st c="15512">precision
    unnecessarily.</st>
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15080">向量存储</st> <st c="15095">通常是一个向量数据库（但并非总是如此，见以下说明），它针对存储和提供向量进行了优化，并在有效的RAG系统中发挥着关键作用。</st>
    <st c="15272">技术上，你可以不使用向量数据库构建RAG系统，但你将错过这些数据存储工具中已经构建的许多优化，这会影响你的内存、计算需求以及搜索</st>
    <st c="15512">精度，这是不必要的。</st>
- en: <st c="15536">Note</st>
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15536">注意</st>
- en: <st c="15541">You often hear the term</st> **<st c="15566">vector databases</st>**
    <st c="15582">when</st> <st c="15587">referring to optimized database-like structures
    for storing vectors.</st> <st c="15657">However, there are tools and other mechanisms
    that are not databases while serving the same or similar purpose as a vector database.</st>
    <st c="15790">For this reason, we will refer to all of them in a group as</st>
    *<st c="15850">vector stores</st>*<st c="15863">. This is consistent with LangChain
    documentation, which also refers to the group in aggregate as vector stores, inclusive
    of all types of mechanisms that store and serve vectors.</st> <st c="16043">But
    you will often hear the terms used interchangeably, and the term</st> *<st c="16112">vector
    database</st>* <st c="16127">is actually the more popular term used to refer to
    all of these mechanisms.</st> <st c="16204">For the sake of accuracy and to align
    our terminology with LangChain documentation, in this book, we will use the term</st>
    *<st c="16323">vector store</st>*<st c="16335">.</st>
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15541">你经常听到</st> **<st c="15566">向量数据库</st>** <st c="15582">这个术语</st>，当<st
    c="15587">提到用于存储向量的优化数据库结构时。</st> <st c="15657">然而，有一些工具和其他机制虽然不是数据库，但它们在功能上与向量数据库相同或类似。</st>
    <st c="15790">因此，我们将它们统称为</st> *<st c="15850">向量存储</st>*<st c="15863">。这与LangChain文档中的表述一致，它也将这一组称为向量存储，包括所有存储和提供向量的机制类型。</st>
    <st c="16043">但你会经常听到这些术语被互换使用，而术语</st> *<st c="16112">向量数据库</st>** <st c="16127">实际上是更常用的术语，用来指代所有这些机制。</st>
    <st c="16204">为了准确起见，并使我们的术语与LangChain文档保持一致，在这本书中，我们将使用术语</st> *<st c="16323">向量存储</st>**<st
    c="16335">。</st>
- en: <st c="16336">In terms of</st> *<st c="16349">where vectors lurk in your code</st>*<st
    c="16380">, the vector store is where most of the vectors generated in</st> <st
    c="16441">your code are stored.</st> <st c="16463">When you vectorize your data,
    those embeddings go into your vector store.</st> <st c="16537">When you conduct
    a similarity search, the embeddings used to represent that data get pulled from
    the vector store.</st> <st c="16652">This makes vector stores a key player in
    the RAG system and worthy of</st> <st c="16722">our attention.</st>
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="16336">在</st> *<st c="16349">向量在你的代码中隐藏的位置</st>**<st c="16380">方面，向量存储是存储你代码中生成的大多数向量的地方。</st>
    <st c="16463">当你将数据向量化时，这些嵌入会进入你的向量存储。</st> <st c="16537">当你进行相似度搜索时，用于表示数据的嵌入会从向量存储中提取。</st>
    <st c="16652">这使得向量存储在RAG系统中扮演着关键角色，值得我们关注。</st>
- en: <st c="16736">Now that we know where the original data embeddings are stored,
    let’s bring this back to how they are used in relation to the user</st> <st c="16868">query
    embeddings.</st>
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="16736">既然我们已经知道了原始数据嵌入的存储位置，让我们将其与用户</st> <st c="16868">查询嵌入的使用联系起来。</st>
- en: <st c="16885">Vector similarity compares your vectors</st>
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="16885">向量相似度比较你的向量</st>
- en: <st c="16925">We have our two primary</st> <st c="16950">vectorization occurrences:</st>
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="16925">我们有我们的两个主要</st> <st c="16950">向量化事件：</st>
- en: <st c="16976">The embedding for our</st> <st c="16999">user query</st>
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="16976">我们</st> <st c="16999">用户查询</st> 的嵌入
- en: <st c="17009">The vector embeddings representing all the data in our</st> <st
    c="17065">vector store</st>
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="17009">代表我们</st> <st c="17065">向量存储中所有数据的</st> 向量嵌入
- en: <st c="17077">Let’s</st> <st c="17084">review how these two occurrences relate
    to each other.</st> <st c="17139">When we conduct the highly important vector
    similarity search that forms the foundation of our retrieval process, we are really
    just performing a mathematical operation that measures the distance between the
    user query embedding and the original</st> <st c="17385">data embeddings.</st>
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="17077">让我们</st> <st c="17084">回顾一下这两次发生的事件是如何相互关联的。</st> <st c="17139">当我们进行高度重要的向量相似度搜索，这是我们的检索过程的基础时，我们实际上只是在执行一个数学运算，该运算测量用户查询嵌入和原始</st>
    <st c="17385">数据嵌入之间的距离。</st>
- en: <st c="17401">Multiple mathematical algorithms can be used to perform this distance
    calculation, which we will review later in</st> [*<st c="17515">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="17524">. But for now, it is important to understand that this distance calculation
    identifies the closest original data embeddings to the user query embedding and
    returns the list of those embeddings in the order of their distance (sorted by
    closest to furthest).</st> <st c="17781">Our code is a bit more simplistic, in
    that the embedding represents the data points (the chunks) in a</st> <st c="17883">1:1
    relationship.</st>
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="17401">可以使用多种数学算法来执行这种距离计算，我们将在后面的</st> [*<st c="17515">第8章</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="17524">中对其进行回顾。但就目前而言，重要的是要理解这种距离计算确定了与用户查询嵌入最接近的原始数据嵌入，并按距离顺序（从最近到最远）返回这些嵌入的列表。</st>
    <st c="17781">我们的代码稍微简单一些，因为嵌入以1:1的关系表示数据点（块）。</st>
- en: <st c="17900">But in many applications, such as with a question-and-answer chatbot
    where the questions or answers are very long and broken up into smaller chunks,
    you will likely see those chunks have a foreign key ID that refers back to a larger
    piece of content.</st> <st c="18152">That allows us to retrieve the full piece
    of content, rather than just the chunk.</st> <st c="18234">This will vary depending
    on the problem your RAG system is trying to solve, but it is important to understand
    that the architecture of this retrieval system can vary to meet the needs of</st>
    <st c="18421">the application.</st>
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="17900">但在许多应用中，例如在与问答聊天机器人一起使用时，问题或答案可能非常长，并被分成更小的块，你可能会看到这些块有一个外键ID，它引用回更大的内容。</st>
    <st c="18152">这使我们能够检索整个内容，而不仅仅是块。</st> <st c="18234">这会根据你的RAG系统试图解决的问题而变化，但重要的是要理解，这个检索系统的架构可以根据应用的需求而变化。</st>
- en: '<st c="18437">This covers the most common places you find vectors in your RAG
    system: where they occur, where they are stored, and how they are used in service
    of the RAG system.</st> <st c="18603">In the next section, we talk about how the
    size of the data text we are using in the search for our RAG system can vary.</st>
    <st c="18724">You will ultimately make decisions in your code that dictate that
    size.</st> <st c="18796">But from what you already know about vectors, you may
    start to wonder, if we are vectorizing content of various sizes, how does that
    impact our ability to compare them and ultimately build the most effective retrieval
    process we can build?</st> <st c="19036">And you would be right to wonder!</st>
    <st c="19070">Let’s discuss the impact of the size of the content that we turn
    into</st> <st c="19140">embeddings next.</st>'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="18437">这涵盖了你在你的RAG系统中找到向量的最常见地方：它们出现的地方，它们存储的地方，以及它们如何在RAG系统的服务中使用。</st>
    <st c="18603">在下一节中，我们将讨论我们在RAG系统的搜索中使用的数据文本的大小是如何变化的。</st> <st c="18724">你最终会在你的代码中做出决定，这将决定那个大小。</st>
    <st c="18796">但根据你对向量的了解，你可能开始想知道，如果我们将各种大小的内容矢量化，这会如何影响我们比较它们的能力，并最终构建我们能够构建的最有效的检索过程？</st>
    <st c="19036">你确实有理由感到好奇！</st> <st c="19070">让我们接下来讨论我们将内容转换为</st> <st c="19140">嵌入时内容大小的影响。</st>
- en: <st c="19156">The amount of text you vectorize matters!</st>
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="19156">你矢量化文本的数量很重要！</st>
- en: <st c="19198">The vector we showed earlier</st> <st c="19227">came from the
    text</st> `<st c="19247">What are the advantages of using RAG?</st>`<st c="19284">.
    That is a relatively short amount of text, which means a 1,536-dimension vector
    is going to do a very thorough job representing the context within that text.</st>
    <st c="19444">But if we go back to the code, the content that we vectorize to
    represent our</st> *<st c="19522">data</st>* <st c="19526">comes</st> <st c="19533">from
    here:</st>
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前展示的向量<st c="19198">来自文本</st> <st c="19227">《</st> <st c="19247">使用RAG的优势是什么？</st>
    <st c="19284">》</st>。这是一段相对较短的文字，这意味着一个1,536维度的向量将能够非常彻底地代表该文本中的上下文。</st> <st
    c="19444">但如果我们回到代码，我们矢量化以表示我们的*<st c="19522">数据</st> <st c="19526">的内容</st> <st
    c="19533">来自这里：</st>
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: <st c="19749">This pulls in the web page we looked at in previous chapters,
    which is relatively long compared to the question text.</st> <st c="19868">To
    make that data more manageable, we break that content up into chunks using a text
    splitter in</st> <st c="19965">this code:</st>
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="19749">这引入了我们在前几章中查看的网页，与问题文本相比，它相对较长。</st> <st c="19868">为了使这些数据更易于管理，我们使用以下代码中的文本分割器将这些内容分割成片段：</st>
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: <st c="20072">If you were to pull out the third chunk using</st> `<st c="20119">splits[2]</st>`<st
    c="20128">, it would look</st> <st c="20144">like this:</st>
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="20072">如果你使用</st> `<st c="20119">splits[2]</st>`<st c="20128">提取第三个片段，它看起来会是这样：</st>
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: <st c="21348">I chose the third chunk to show because it is a relatively short
    chunk.</st> <st c="21421">Most of the chunks are much larger.</st> <st c="21457">The</st>
    **<st c="21461">Semantic Chunker text splitter</st>** <st c="21491">we use attempts
    to use semantics to</st> <st c="21528">determine how to split up the text, using
    embeddings to determine those semantics.</st> <st c="21611">In theory, this should
    give us chunks that do a better job of breaking up the data based on context,
    rather than just an</st> <st c="21732">arbitrary size.</st>
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21348">我选择第三个片段来展示，因为它相对较短。</st> <st c="21421">大多数片段都大得多。</st> <st c="21457">我们使用的**<st
    c="21461">语义块分割文本分割器</st>** <st c="21491">试图使用语义来确定如何分割文本，使用嵌入来确定这些语义。</st> <st
    c="21528">理论上，这应该会给我们提供更好的基于上下文分割数据的片段，而不是仅仅基于任意大小的。</st>
- en: <st c="21747">However, there</st> <st c="21762">is an important concept to understand
    when it comes to embeddings that will impact the splitter you choose and the size
    of your embeddings in general.</st> <st c="21914">This all stems from the fact
    that no matter how large the text that you pass to the vectorization algorithm
    is, it is still going to give you an embedding that is the same size as any of
    the other embeddings.</st> <st c="22123">In this case, that means the user query
    embedding is going to be 1,536 dimensions, but all those long sections of text
    in the vector store are also going to be 1,536 dimensions, even though their actual
    length in text format is quite different.</st> <st c="22368">It may be counter-intuitive,
    but in an amazing turn of events, it</st> <st c="22434">works well!</st>
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21747">然而，有一个重要的概念需要理解，当涉及到嵌入时，它将影响你选择的分割器以及你嵌入的大小。</st> <st c="21762">这一切都源于这样一个事实，即无论你传递给矢量化算法的文本有多大，它仍然会给你一个与任何其他嵌入相同大小的嵌入。</st>
    <st c="21914">在这种情况下，这意味着用户查询嵌入将是1,536维，但向量存储中的所有那些长文本段也将是1,536维，尽管它们的实际文本长度相当不同。</st>
    <st c="22123">这可能看起来有些反直觉，但令人惊讶的是，它</st> <st c="22434">效果很好！</st>
- en: <st c="22445">When conducting a search with the user query of the vector store,
    the mathematical representations of the user query embedding and the other embeddings
    are done in such a way that we are still able to detect the semantic similarities
    between them, despite the large disparity in their size.</st> <st c="22737">This
    aspect of the vector similarity search is the kind of thing that makes mathematicians
    love math so much.</st> <st c="22847">It just seems to defy all logic that you
    can turn text of very different sizes into numbers and be able to detect similarities</st>
    <st c="22974">between them.</st>
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="22445">当使用向量存储的用户查询进行搜索时，用户查询嵌入和其他嵌入的数学表示是以一种方式进行的，这样我们仍然能够检测它们之间的大小差异很大的语义相似性。</st>
    <st c="22737">向量相似性搜索的这一方面是让数学家如此热爱数学的原因之一。</st> <st c="22847">这似乎完全违背了逻辑，你可以将不同大小的文本转换为数字，并且能够检测它们之间的相似性</st>
    <st c="22974">。</st>
- en: <st c="22987">But there is</st> <st c="23001">another aspect of this to consider
    as well—when you compare the results across just the chunks that you break your
    data into, the size of those chunks will matter.</st> <st c="23165">In this case,
    the larger the amount of content that is being vectorized, the more diluted the
    embedding will be.</st> <st c="23278">On the other hand, the smaller the amount
    of content that the embedding represents, the less context you will have to match
    up when you perform a vector similarity search.</st> <st c="23450">For each of
    your RAG implementations, you will need to find a delicate balance between chunk
    size and</st> <st c="23552">context representation.</st>
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="22987">但是，还有</st> <st c="23001">另一个方面需要考虑——当你只比较你将数据分割成块的结果时，这些块的大小将很重要。</st>
    <st c="23165">在这种情况下，被向量化内容量越大，嵌入就会越稀释。</st> <st c="23278">另一方面，嵌入表示的内容量越小，你在执行向量相似度搜索时需要匹配的上下文就越少。</st>
    <st c="23450">对于你的每个RAG实现，你都需要在块大小和</st> <st c="23552">上下文表示之间找到一个微妙的平衡。</st>
- en: <st c="23575">Understanding this will help you make better decisions about how
    you split data and the vectorization algorithms you choose when trying to improve
    your RAG system.</st> <st c="23740">We will cover some other techniques to get
    more out of your splitting/chunking strategy in</st> [*<st c="23831">Chapter 11</st>*](B22475_11.xhtml#_idTextAnchor229)
    <st c="23841">when we talk about LangChain splitters.</st> <st c="23882">Next,
    we will talk about the importance of testing different</st> <st c="23943">vectorization
    models.</st>
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23575">理解这一点将帮助你在尝试改进你的RAG系统时，做出更好的关于如何分割数据和选择向量化算法的决定。</st> <st c="23740">当我们讨论LangChain分割器时，我们将在</st>
    [*<st c="23831">第11章</st>*](B22475_11.xhtml#_idTextAnchor229) <st c="23841">中介绍一些其他技术，以充分利用你的分割/分块策略。</st>
    <st c="23882">接下来，我们将讨论测试不同</st> <st c="23943">向量化模型的重要性。</st>
- en: <st c="23964">Not all semantics are created equal!</st>
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="23964">并非所有语义都是平等的！</st>
- en: <st c="24001">A common mistake made in RAG applications is choosing the first
    vectorization algorithm that is implemented and just assuming that provides the
    best results.</st> <st c="24160">These algorithms take the semantic meaning of
    text and represent them mathematically.</st> <st c="24246">However, these algorithms
    are generally large NLP models themselves, and they can vary in capabilities and
    quality as much as the LLMs.</st> <st c="24382">Just as we, as humans, often find
    it challenging to comprehend the intricacies and nuances of text, these models
    can grapple with the same challenge, having varying abilities to grasp the complexities
    inherent in written language.</st> <st c="24613">For example, models in the past
    could not decipher the difference between</st> `<st c="24687">bark</st>` <st c="24691">(a
    dog noise) and</st> `<st c="24710">bark</st>` <st c="24714">(the outer part of
    most trees), but newer models can detect this based on the surrounding text and
    the context in which it is used.</st> <st c="24847">This area of the field is
    adapting and evolving just as fast as</st> <st c="24911">other areas.</st>
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在RAG应用中常见的错误是选择第一个实现的向量化算法，并仅仅假设这会提供最佳结果。</st> 这些算法将文本的语义意义以数学方式表示。</st> 然而，这些算法本身通常是大型NLP模型，它们的能力和质量可以像LLMs一样有很大的差异。</st>
    正如我们作为人类，常常发现理解文本的复杂性和细微差别具有挑战性一样，这些模型也会面临同样的挑战，它们在把握书面语言固有的复杂性方面具有不同的能力。</st>
    例如，过去的模型无法区分</st> `<st c="24687">bark</st>` <st c="24691">(狗叫声) 和</st> `<st c="24710">bark</st>`
    <st c="24714">(大多数树木的外层)，但新模型可以根据周围的文本和使用的上下文来检测这一点。</st> 这个领域的这个方面正在以与其他领域一样快的速度适应和演变。</st>
- en: <st c="24923">In some cases, it is possible that a domain-specific vectorization
    model, such as one trained on scientific papers, is going to do better in an app
    that is focused on scientific papers than using a generic vectorization model.</st>
    <st c="25151">Scientists talk in very specific ways, very different from what
    you might see on social media, and so a giant model trained on general web-based
    text may not perform well in this</st> <st c="25330">specific domain.</st>
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="24923">在某些情况下，可能一个特定领域的向量化模型，例如在科学论文上训练的模型，在专注于科学论文的应用程序中可能会比使用通用向量化模型表现得更好。</st>
    科学家们的谈话方式非常具体，与你在社交媒体上看到的方式大不相同，因此在一个基于通用网络文本训练的大型模型可能在这个</st> <st c="25330">特定领域表现不佳。</st>
- en: <st c="25346">Fun fact</st>
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25346">有趣的事实</st>
- en: <st c="25355">You often hear about how you can fine-tune LLMs to improve your
    domain-specific results.</st> <st c="25445">But did you know that you can also
    fine-tune embedding models?</st> <st c="25508">Fine-tuning an embedding model
    has the potential to improve the way the embedding model understands your domain-specific
    data and, therefore, has the potential to improve your similarity search results.</st>
    <st c="25711">This has the potential to improve your entire RAG system substantially
    for</st> <st c="25786">your domain.</st>
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25355">您经常听说如何微调LLM以改进您的特定领域结果。</st> <st c="25445">但您知道您也可以微调嵌入模型吗？</st>
    <st c="25508">微调嵌入模型有可能改善嵌入模型理解您特定领域数据的方式，因此有可能改善您的相似度搜索结果。</st> <st c="25711">这有可能显著改善您整个RAG系统在您领域中的表现。</st>
    <st c="25786">您的领域。</st>
- en: <st c="25798">To summarize this section on fundamentals, numerous aspects of
    vectors can help you or hurt you when trying to build the most effective RAG application
    for your needs.</st> <st c="25967">Of course, it would be poor manners for me
    to tell you how important the vectorization algorithm is without telling you which
    ones are available!</st> <st c="26113">To address this, in this next section,
    let’s run through a list of some of the most popular vectorization techniques!</st>
    <st c="26231">We will even do this</st> <st c="26252">with code!</st>
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25798">为了总结本节关于基础知识的部分，向量的多个方面在尝试为您的需求构建最有效的RAG应用时可能会帮助您，也可能伤害您。</st>
    <st c="25967">当然，如果不告诉您有哪些可用的向量化算法，就告诉您向量化算法的重要性，那将是不礼貌的！</st> <st c="26113">为了解决这个问题，在下一节中，让我们列举一些最受欢迎的向量化技术！</st>
    <st c="26231">我们甚至会用代码来做这件事！</st>
- en: <st c="26262">Code lab 7.1 – Common vectorization techniques</st>
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="26262">代码实验室7.1 – 常见的向量化技术</st>
- en: <st c="26309">Vectorization algorithms</st> <st c="26335">have evolved significantly
    over the past few decades.</st> <st c="26389">Understanding how these have changed,
    and why, will help you gain more perspective on how to choose the one that fits
    your needs the most.</st> <st c="26528">Let’s walk through some of these vectorization
    algorithms, starting with some of the earliest ones and ending with the most recent,
    more advanced options.</st> <st c="26683">This is nowhere close to an exhaustive
    list, but these select few should be enough to give you a sense of where this
    part of the field came from and where it is going.</st> <st c="26851">Before we
    start, let’s install and import some new Python packages that play important roles
    in our coding journey through</st> <st c="26974">vectorization techniques:</st>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26309">向量化算法</st> <st c="26335">在过去几十年中已经发生了显著的变化。</st> <st c="26389">了解这些变化的原因，将帮助您获得更多关于如何选择最适合您需求的算法的视角。</st>
    <st c="26528">让我们回顾一些这些向量化算法，从一些最早的算法开始，到最新的、更高级的选项结束。</st> <st c="26683">这远非一个详尽的列表，但这些精选的几个应该足以让您了解这一领域的这一部分是从哪里来的，以及它将走向何方。</st>
    <st c="26851">在我们开始之前，让我们安装并导入一些在通过向量化技术进行编码之旅中扮演重要角色的新的Python包：</st> <st c="26974">这些代码应该放在上一个代码块中，与包安装相同的单元格中。</st>
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: <st c="27071">This code should go near the top of the previous code in the same
    cell as the other</st> <st c="27156">package installations.</st>
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27071">此代码应放在上一个代码块中，与包安装相同的单元格中。</st>
- en: <st c="27178">Term frequency-inverse document frequency (TF-IDF)</st>
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="27178">词频-逆文档频率 (TF-IDF)</st>
- en: <st c="27229">1972 was probably much sooner a time than what you would expect
    in a book about a relatively</st> <st c="27323">brand-new technology like RAG,
    but this is where we find the roots of the vectorization</st> <st c="27411">techniques
    we are going to</st> <st c="27438">talk about.</st>
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27229">1972年可能比您在关于相对较新的技术如RAG的书中预期的要早得多，但这就是我们将要讨论的向量化技术的根源。</st> <st
    c="27323">我们将要讨论的向量化技术。</st> <st c="27411">我们将要讨论的向量化技术。</st> <st c="27438">我们将要讨论的向量化技术。</st>
- en: <st c="27449">Karen Ida Boalth Spärck Jones was a self-taught programmer and
    pioneering British computer scientist who worked on several papers focused on
    the field of NLP.</st> <st c="27609">In 1972, she made one of her most important
    contributions, introducing the concept of</st> **<st c="27695">inverse document
    frequency</st>** <st c="27721">(</st>**<st c="27723">IDF</st>**<st c="27726">).</st>
    <st c="27730">The</st> <st c="27733">basic concept as she stated was that “</st>*<st
    c="27772">the specificity of a term can be quantified as an inverse function of
    the number of documents in which</st>* *<st c="27876">it occurs</st>*<st c="27885">.”</st>
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27449">凯伦·伊达·博尔特·斯帕克·琼斯是一位自学成才的程序员和开创性的英国计算机科学家，她在NLP领域的几篇论文中进行了研究。</st>
    <st c="27609">1972年，她做出了她最重要的贡献之一，引入了</st> **<st c="27695">逆文档频率</st>** <st c="27721">(**<st
    c="27723">IDF</st>**<st c="27726">) 的概念。</st> <st c="27730">她所陈述的基本概念是，“</st>*<st
    c="27772">一个术语的特异性可以通过它在文档中出现的数量的倒数来量化</st>* *<st c="27876">。”</st>
- en: <st c="27887">As a real-world example, consider applying the</st> `<st c="27935">df</st>`
    <st c="27937">(document frequency) and</st> `<st c="27963">idf</st>` <st c="27966">(inverse
    document frequency) score to some words in Shakespeare’s 37 plays and you will
    find that the word</st> `<st c="28074">Romeo</st>` <st c="28079">is the highest-scoring
    result.</st> <st c="28111">This is because it appears very frequently, but only
    in one</st> *<st c="28171">document</st>*<st c="28179">, the</st> `<st c="28185">Romeo
    and Juliet</st>` <st c="28201">document.</st> <st c="28212">In this case,</st>
    `<st c="28226">Romeo</st>` <st c="28231">would be scored</st> `<st c="28248">1</st>`
    <st c="28249">for</st> `<st c="28254">df</st>`<st c="28256">, as it appeared in
    1 document.</st> `<st c="28288">Romeo</st>` <st c="28293">would score</st> `<st
    c="28306">1.57</st>` <st c="28310">for</st> `<st c="28315">idf</st>`<st c="28318">,
    higher than any other word because of its high frequency in that one document.</st>
    <st c="28399">Meanwhile, Shakespeare used the word</st> `<st c="28436">sweet</st>`
    <st c="28441">occasionally but in every single play, giving it a low score.</st>
    <st c="28504">This gives</st> `<st c="28515">sweet</st>` <st c="28520">a</st>
    `<st c="28523">df</st>` <st c="28525">score of</st> `<st c="28535">37</st>`<st
    c="28537">, and an</st> `<st c="28546">idf</st>` <st c="28549">score of</st> `<st
    c="28559">0</st>`<st c="28560">. What Karen Jones was saying in her paper was
    that when you see words such as</st> `<st c="28639">Romeo</st>` <st c="28644">appear
    in just a small number of the overall number of plays, you can take the plays
    where those words appear and consider them very important and predictive of what
    that play is about.</st> <st c="28831">In contrast,</st> `<st c="28844">sweet</st>`
    <st c="28849">had the opposite effect, as it is uninformative in terms of the
    importance of the word and of the documents that the word</st> <st c="28972">is
    in.</st>
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27887">作为一个现实世界的例子，考虑将</st> `<st c="27935">df</st>` <st c="27937">(文档频率)
    和</st> `<st c="27963">idf</st>` <st c="27966">(逆文档频率) 分数应用于莎士比亚的37部戏剧中的某些单词，你会发现单词</st>
    `<st c="28074">Romeo</st>` <st c="28079">是得分最高的结果。</st> <st c="28111">这是因为它出现的频率非常高，但只在一</st>
    *<st c="28171">个文档</st>*<st c="28179">中，即</st> `<st c="28185">罗密欧与朱丽叶</st>` <st
    c="28201">文档。</st> <st c="28212">在这种情况下，</st> `<st c="28226">Romeo</st>` <st c="28231">的</st>
    `<st c="28248">df</st>`<st c="28249">分数将是</st> `<st c="28254">1</st>` <st c="28249">，因为它出现在1个文档中。</st>
    `<st c="28288">Romeo</st>` <st c="28293">的</st> `<st c="28306">idf</st>`<st c="28310">分数将是</st>
    `<st c="28315">1.57</st>` <st c="28310">，高于其他任何单词，因为它在那一个文档中的频率很高。</st> <st c="28399">同时，莎士比亚偶尔使用了单词</st>
    `<st c="28436">sweet</st>` <st c="28441">，但在每一部戏剧中都有出现，给它一个低分。</st> <st c="28504">这使得</st>
    `<st c="28515">sweet</st>` <st c="28520">的</st> `<st c="28523">df</st>` <st c="28525">分数为</st>
    `<st c="28535">37</st>`<st c="28537">，而</st> `<st c="28546">idf</st>` <st c="28549">分数为</st>
    `<st c="28559">0</st>`<st c="28560">。凯伦·琼斯在她的论文中提到的是，当你看到像</st> `<st c="28639">Romeo</st>`
    <st c="28644">这样的单词只出现在总数很少的戏剧中时，你可以将这些单词出现的戏剧视为非常重要，并且可以预测该戏剧的内容。</st> <st c="28831">相比之下，</st>
    `<st c="28844">sweet</st>` <st c="28849">产生了相反的效果，因为它在单词的重要性和单词所在的文档方面都没有提供信息。</st>
- en: <st c="28978">But that’s enough talk.</st> <st c="29003">Let’s see how this
    algorithm looks in code!</st> <st c="29047">The scikit-learn library has a function
    that can be applied to text to vectorize that text using the TF-IDF method.</st>
    <st c="29163">The following code is where we define the</st> `<st c="29205">splits</st>`
    <st c="29211">variable, which is what we will use as our data to</st> <st c="29262">train
    the</st> <st c="29273">model on:</st>
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28978">但话已说够。</st> <st c="29003">让我们看看这个算法在代码中的样子！</st> <st c="29047">scikit-learn库有一个函数可以将TF-IDF方法应用于文本，以将文本向量化。</st>
    <st c="29163">以下代码是我们定义的</st> `<st c="29205">splits</st>` <st c="29211">变量，这是我们用作训练模型的</st>
    <st c="29262">数据：</st>
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: <st c="29927">Unlike</st> <st c="29935">the OpenAI embedding model, this model
    requires you to</st> *<st c="29990">train</st>* <st c="29995">on your</st> *<st
    c="30004">corpus</st>* <st c="30010">data, which is a fancy term for all the text
    data you have available to train with.</st> <st c="30095">This code is primarily
    to demonstrate how a TD-IDF model is used compared to our current RAG pipeline
    retriever, so we won’t review it line by line.</st> <st c="30244">But we encourage
    you to try out the code yourself and try</st> <st c="30302">different settings.</st>
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="29927">与OpenAI嵌入模型不同，此模型要求您在您的</st> *<st c="29990">训练</st> *<st c="29995">语料库</st>
    *<st c="30004">数据</st> *<st c="30010">上训练，这是一个术语，指的是您可用于训练的所有文本数据。</st> <st c="30095">此代码主要用于演示与我们的当前RAG管道检索器相比如何使用TD-IDF模型，因此我们不会逐行审查它。</st>
    <st c="30244">但我们鼓励您亲自尝试代码并尝试不同的设置。</st>
- en: <st c="30321">It should be noted that the vectors this algorithm produces are</st>
    <st c="30385">called</st> **<st c="30393">sparse vectors</st>**<st c="30407">,
    and the vectors we were previously working with in previous code labs were</st>
    <st c="30484">called</st> **<st c="30491">dense vectors</st>**<st c="30504">.
    This is an important distinction that we will review in detail in</st> [*<st c="30572">Chapter
    8</st>*](B22475_08.xhtml#_idTextAnchor152)<st c="30581">.</st>
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="30321">需要注意的是，此算法生成的向量被称为</st> <st c="30385">稀疏向量</st> **<st c="30393">**<st
    c="30407">，而我们之前在之前的代码实验室中使用的向量被称为</st> **<st c="30491">密集向量</st> **<st c="30504">。这是一个重要的区别，我们将在</st>
    [*<st c="30572">第8章</st> *](B22475_08.xhtml#_idTextAnchor152)<st c="30581">中详细讨论。</st>
- en: <st c="30582">This model uses the corpus data to set up the environment that
    can then calculate the embeddings for</st> <st c="30684">new content that you
    introduce to it.</st> <st c="30722">The output should look like the</st> <st c="30754">following
    table:</st>
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="30582">此模型使用语料库数据来设置环境，然后可以计算您向其引入的新内容的嵌入。</st> <st c="30684">输出应类似于以下表格：</st>
- en: '[PRE12]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: <st c="30920">In this case, we</st> <st c="30937">see at least a 10-way tie
    for the</st> `<st c="30972">idf</st>` <st c="30975">highest value (we are only
    showing 10, so there are probably more), and all of them are number-based text.</st>
    <st c="31083">This does not seem particularly useful, but this is primarily because
    our corpus data is so small.</st> <st c="31182">Training on more data from the
    same author or domain can help you build a model that has a better contextual
    understanding of the</st> <st c="31312">underlying content.</st>
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="30920">在这种情况下，我们至少看到有10个文档在</st> `<st c="30972">idf</st>` <st c="30975">最高值上并列（我们只显示了10个，所以可能还有更多），而且所有这些都是基于数字的文本。</st>
    <st c="31083">这似乎并不特别有用，但这主要是因为我们的语料库数据如此之小。</st> <st c="31182">在来自同一作者或领域的更多数据上训练可以帮助您构建一个对底层内容有更好上下文理解的模型。</st>
- en: <st c="31331">Now, going back to the original question that we have been using,</st>
    `<st c="31398">What are the advantages of RAG?</st>`<st c="31429">, we want to
    use the TF-IDF embeddings to determine what the most relevant</st> <st c="31504">documents
    are:</st>
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="31331">现在，回到我们一直在使用的原始问题，</st> `<st c="31398">RAG的优势是什么？</st>`<st c="31429">，我们想使用TF-IDF嵌入来确定哪些是最相关的文档：</st>
- en: '[PRE13]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: <st c="31840">This replicates</st> <st c="31857">the behavior we see with the
    retriever, where it uses a similarity algorithm to find the nearest embedding
    by distance.</st> <st c="31977">In this case, we use cosine similarity, which</st>
    <st c="32022">we will talk about in</st> [*<st c="32045">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="32054">, but just keep in mind that there are many distance algorithms that
    we can use to calculate this distance.</st> <st c="32162">Our output from this
    code is</st> <st c="32191">as follows:</st>
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="31840">这复制了我们在检索器中看到的行为，其中它使用相似性算法通过距离找到最近的嵌入。</st> <st c="31977">在这种情况下，我们使用余弦相似度，我们将在[*<st
    c="32045">第8章</st>*](B22475_08.xhtml#_idTextAnchor152)中讨论，但请记住，我们可以使用许多距离算法来计算这个距离。</st>
    <st c="32022">从这个代码输出的结果是</st> <st c="32191">如下：</st>
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: <st c="32638">If you run our original code, which uses the original vector store
    and retriever, you will see</st> <st c="32734">this output:</st>
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="32638">如果你运行我们原始的代码，该代码使用原始的向量存储和检索器，你会看到</st> <st c="32734">以下输出：</st>
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: <st c="33181">They match!</st> <st c="33194">A small algorithm from 1972 trained
    on our own data in a fraction of a second is just as good as the massive algorithms
    developed by OpenAI spending billions of dollars to develop them!</st> <st c="33380">Okay,
    let’s slow down, this is definitely NOT the case!</st> <st c="33436">The reality
    is that in real-world scenarios, you will be working with a much larger dataset
    than we are and much more complicated user queries, and this will benefit from
    the use of more sophisticated modern</st> <st c="33644">embedding techniques.</st>
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="33181">它们匹配！</st> <st c="33194">一个1972年的小算法，在几秒钟内训练我们自己的数据，和OpenAI花费数十亿美元开发的庞大算法一样好！</st>
    <st c="33380">好吧，让我们放慢速度，这绝对不是情况！</st> <st c="33436">现实是，在现实世界的场景中，你将处理比我们更大的数据集，以及更复杂的用户查询，这将受益于使用更复杂的现代</st>
    <st c="33644">嵌入技术。</st>
- en: '<st c="33665">TF-IDF has</st> <st c="33677">been</st> <st c="33682">very useful
    over the years.</st> <st c="33710">But was it necessary to learn about an algorithm
    from 1972 when we are talking about the most advanced generative AI models ever
    built?</st> <st c="33846">The answer is BM25\.</st> <st c="33866">This is just
    a teaser, but you will learn more about this very popular</st> **<st c="33937">keyword
    search</st>** <st c="33951">algorithm, one</st> <st c="33966">of the most popular
    algorithms in use today, in the next chapter.</st> <st c="34033">And guess what?</st>
    <st c="34049">It is based on TF-IDF!</st> <st c="34072">What TF-IDF has a problem
    with, though, is capturing context and semantics as well as some of the next models
    we will talk about.</st> <st c="34202">Let’s discuss the next major step up: Word2Vec
    and</st> <st c="34253">related algorithms.</st>'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="33665">TF-IDF在过去的几年中非常有用。</st> <st c="33677">但是，当我们谈论有史以来最先进的生成式AI模型时，有必要学习1972年的算法吗？</st>
    <st c="33710">答案是BM25。</st> <st c="33846">这只是个预告，但你将在下一章中了解更多关于这个非常流行的**<st c="33937">关键词搜索</st>**
    <st c="33951">算法，它是目前使用最广泛的算法之一。</st> <st c="33966">而且你知道吗？</st> <st c="34049">它是基于TF-IDF的！</st>
    <st c="34072">然而，TF-IDF的问题在于捕捉上下文和语义，以及我们接下来将要讨论的一些模型。</st> <st c="34202">让我们讨论下一个重大步骤：Word2Vec和相关算法。</st>
- en: <st c="34272">Word2Vec, Sentence2Vec, and Doc2Vec</st>
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="34272">Word2Vec、Sentence2Vec和Doc2Vec</st>
- en: '**<st c="34308">Word2Vec</st>** <st c="34317">and</st> <st c="34322">similar
    models introduced an early application</st> <st c="34368">of unsupervised learning,
    representing a significant step forward in the NLP field.</st> <st c="34453">There
    are multiple</st> *<st c="34472">vec</st>* <st c="34475">models (word, doc, and
    sentence), where their training was focused on words, documents, or sentences,
    respectively.</st> <st c="34592">These models differ in the level of text they
    are</st> <st c="34642">trained on.</st>'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**<st c="34308">Word2Vec</st>** 和 <st c="34317">类似模型引入了无监督学习的早期应用，这代表了自然语言处理领域的一个重要进步。</st>
    <st c="34322">存在多个</st> *<st c="34472">vec</st> <st c="34475">模型（单词、文档和句子），它们的训练分别集中在单词、文档或句子上。</st>
    <st c="34592">这些模型在训练的文本级别上有所不同。</st>'
- en: <st c="34653">Word2Vec focuses on learning vector representations for individual
    words, capturing their semantic meaning and relationships.</st> **<st c="34780">Doc2Vec</st>**<st
    c="34787">, on the</st> <st c="34795">other hand, learns</st> <st c="34814">vector
    representations for entire documents, allowing it to capture the overall context
    and theme of a</st> <st c="34918">document.</st> **<st c="34928">Sentence2Vec</st>**
    <st c="34940">is similar to Doc2Vec but operates at the</st> <st c="34983">sentence
    level, learning vector representations for individual sentences.</st> <st c="35057">While
    Word2Vec is useful for tasks such as word similarity and analogy, Doc2Vec and
    Sentence2Vec are more suitable for document-level tasks such as document similarity,
    classification,</st> <st c="35242">and retrieval.</st>
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="34653">Word2Vec专注于学习单个单词的向量表示，捕捉其语义意义和关系。</st> **<st c="34780">Doc2Vec</st>**<st
    c="34787">，另一方面，学习整个文档的向量表示，使其能够捕捉文档的整体上下文和主题。</st> **<st c="34928">Sentence2Vec</st>**
    <st c="34940">与Doc2Vec类似，但它在句子级别操作，学习单个句子的向量表示。</st> <st c="35057">虽然Word2Vec对于单词相似性和类比等任务很有用，但Doc2Vec和Sentence2Vec更适合文档级别的任务，如文档相似性、分类和检索。</st>
- en: <st c="35256">Because we are working with larger documents, and not just words
    or sentences, we are going to select the Doc2Vec model over Word2Vec or Sentence2Vec
    and train this model to see how it works as our retriever.</st> <st c="35466">Like
    the TD-IDF model, this model can be trained with our data and then we pass the
    user query to it to see whether we can get similar results for the most similar</st>
    <st c="35630">data chunks.</st>
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="35256">因为我们正在处理更大的文档，而不仅仅是单词或句子，我们将选择Doc2Vec模型而不是Word2Vec或Sentence2Vec，并训练此模型以查看它作为我们的检索器的工作方式。</st>
    <st c="35466">像TD-IDF模型一样，此模型可以用我们的数据进行训练，然后我们向它传递用户查询以查看我们是否可以得到最相似数据块的结果。</st>
- en: <st c="35642">Add this code in a new cell after the TD-IDF</st> <st c="35688">code
    cell:</st>
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="35642">在TD-IDF代码单元之后添加此代码：</st> <st c="35688">代码单元：</st>
- en: '[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: <st c="36220">Much like the</st> <st c="36235">TD-IDF model, this code is primarily
    to demonstrate how a</st> <st c="36293">Doc2Vec model is used</st> <st c="36315">compared
    to our current RAG pipeline retriever, so we won’t review it line by line, but
    we encourage you to try out the code yourself and try different settings.</st>
    <st c="36477">This code focuses on training the Doc2Vec model and saving</st>
    <st c="36536">it locally.</st>
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="36220">与TD-IDF模型类似，这段代码主要是为了演示如何使用Doc2Vec模型，与我们的当前RAG管道检索器相比，所以我们将不会逐行审查，但我们鼓励您自己尝试代码并尝试不同的设置。</st>
    <st c="36477">此代码专注于训练Doc2Vec模型并将其本地保存。</st>
- en: <st c="36547">Fun fact</st>
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="36547">有趣的事实</st>
- en: <st c="36556">Training language models is a hot topic and can be a well-paid
    profession these days.</st> <st c="36643">Have you ever trained a language model?</st>
    <st c="36683">If your answer was</st> *<st c="36702">no</st>*<st c="36704">, you
    would be wrong.</st> <st c="36726">Not only did you just train a language model
    but you have now trained two!</st> <st c="36801">Both TF-IDF and Doc2Vec are language
    models that you just trained.</st> <st c="36868">These are relatively basic versions
    of model training, but you have to start somewhere, and you</st> <st c="36964">just
    did!</st>
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="36556">训练语言模型是当今的热门话题，并且可以成为一份收入颇丰的职业。</st> <st c="36643">你曾经训练过语言模型吗？</st>
    <st c="36683">如果你的答案是*<st c="36702">没有</st>*<st c="36704">，你就错了。</st> <st c="36726">你不仅刚刚训练了一个语言模型，你现在已经训练了两个了！</st>
    <st c="36801">TF-IDF和Doc2Vec都是你刚刚训练的语言模型。</st> <st c="36868">这些是相对基本的模型训练版本，但你必须从某个地方开始，而你刚刚就做到了！</st>
- en: <st c="36973">In this next code, we</st> <st c="36996">will use that model on</st>
    <st c="37019">our data:</st>
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="36973">在接下来的代码中，我们将使用该模型在我们的数据上：</st> <st c="36996">进行操作：</st>
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: <st c="37668">We separated</st> <st c="37682">the code for creating and saving
    the model from the usage of the model so that you can see how this model can be
    saved and referenced later.</st> <st c="37823">Here is the output from</st> <st
    c="37847">this code:</st>
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="37668">我们将创建和保存模型的代码与模型的使用分离，这样您就可以看到该模型如何被保存和稍后引用。</st> <st c="37823">以下是此代码的输出：</st>
- en: '[PRE18]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: <st c="38264">Comparing this to the results from our original retriever shown
    previously, this model does not return the same result.</st> <st c="38385">However,
    this model was set up with just 100 dimension vectors in</st> <st c="38451">this
    line:</st>
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="38264">将此与之前展示的我们原始检索器的结果进行比较，此模型不会返回相同的结果。</st> <st c="38385">然而，这个模型在这个</st>
    <st c="38451">行</st>中仅设置了100维向量：</st>
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: <st c="38562">What happens when you change</st> `<st c="38592">vector_size</st>`
    <st c="38603">in this line to use 1,536, the same vector size as the</st> <st
    c="38659">OpenAI model?</st>
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="38562">当您将此行中的`vector_size`改为1,536，与OpenAI模型的相同向量大小时，会发生什么？</st>
- en: <st c="38672">Change the</st> `<st c="38684">doc2vec_model</st>` <st c="38697">variable
    definition</st> <st c="38718">to this:</st>
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="38672">将</st> `<st c="38684">doc2vec_model</st>` <st c="38697">变量定义</st>
    <st c="38718">改为：</st>
- en: '[PRE20]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: <st c="38828">The results</st> <st c="38841">will change</st> <st c="38853">to
    this:</st>
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="38828">结果</st> <st c="38841">将变为：</st>
- en: '[PRE21]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: <st c="39298">This resulted in the same text as our original results, using
    OpenAI’s embeddings.</st> <st c="39382">However, the results are not consistent.</st>
    <st c="39423">If you trained this model on more data, it will likely improve</st>
    <st c="39486">the results.</st>
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="39298">这导致了与我们的原始结果相同的结果，使用了OpenAI的嵌入。</st> <st c="39382">然而，结果并不一致。</st>
    <st c="39423">如果您在更多数据上训练这个模型，它很可能会改善</st> <st c="39486">结果。</st>
- en: <st c="39498">In theory, the benefit this type of model has over TF-IDF is that
    it is a neural network-based approach that takes into account surrounding words,
    whereas TF-IDF is simply a statistical measure that evaluates how relevant a word
    is to the document (keyword search).</st> <st c="39765">But as we said about the
    TD-IDF model, there are still more powerful models than the</st> *<st c="39850">vec</st>*
    <st c="39853">models that capture much more context and semantics of the text
    they are fed.</st> <st c="39932">Let’s jump to another generation of</st> <st
    c="39968">models, transformers.</st>
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="39498">从理论上讲，这种模型相较于TF-IDF的优势在于它是一种基于神经网络的模型，它考虑了周围的词语，而TF-IDF仅仅是一个统计度量，用于评估一个词对文档的相关性（关键词搜索）。</st>
    <st c="39765">但正如我们之前提到的TD-IDF模型，还有比</st> *<st c="39850">vec</st>* <st c="39853">模型更强大的模型，这些模型能够捕捉到更多被输入文本的上下文和语义。</st>
    <st c="39932">让我们跳到另一代的</st> <st c="39968">模型，即transformers。</st>
- en: <st c="39989">Bidirectional encoder representations from transformers</st>
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="39989">从transformers中得到的双向编码器表示</st>
- en: <st c="40045">At this point, with</st> **<st c="40066">bidirectional encoder
    representations from transformers</st>** <st c="40121">(</st>**<st c="40123">BERT</st>**<st
    c="40127">), we are</st> <st c="40137">fully</st> <st c="40144">into using neural
    networks to better understand the underlying semantics of the corpus, yet another
    big step forward for NLP algorithms.</st> <st c="40281">BERT is also among the
    first to apply a specific type of neural network, the</st> **<st c="40358">transformer</st>**<st
    c="40369">, which</st> <st c="40376">was a major step in the progression that
    led to the development of the LLMs we are familiar with today.</st> <st c="40481">OpenAI’s
    popular ChatGPT models are also transformers but were trained on a much larger
    corpus and with different techniques</st> <st c="40606">from BERT.</st>
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="40045">在这个时候，使用**<st c="40066">从transformers中得到的双向编码器表示</st>** <st c="40121">(**<st
    c="40123">BERT</st>**<st c="40127">)，我们已经完全转向使用神经网络来更好地理解语料库的潜在语义，这是NLP算法的又一次重大进步。</st>
    <st c="40281">BERT也是最早应用特定类型的神经网络之一，即**<st c="40358">transformer</st>**<st c="40369">，这是导致我们今天熟悉的LLMs发展的关键步骤之一。</st>
    <st c="40481">OpenAI流行的ChatGPT模型也是transformers，但它们是在一个更大的语料库上用不同的技术训练的，与BERT不同。</st>
- en: <st c="40616">That said, BERT is still a very capable model.</st> <st c="40664">You
    can use BERT as a standalone model that you import, avoiding having to rely on
    APIs such as OpenAI’s embedding service.</st> <st c="40788">Being able to use
    a local model in your code can be a big advantage in certain network-constrained
    environments, instead of relying on an API service such</st> <st c="40943">as
    OpenAI.</st>
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="40616">话虽如此，BERT仍然是一个非常强大的模型。</st> <st c="40664">您可以将BERT作为一个独立的模型导入，避免依赖于OpenAI的嵌入服务等API。</st>
    <st c="40788">能够在您的代码中使用本地模型在某些网络受限的环境中可能是一个很大的优势，而不是依赖于像OpenAI这样的API服务。</st>
    <st c="40943">OpenAI。</st>
- en: <st c="40953">One of the defining characteristics of the transformer models
    is the use of a self-attention mechanism to capture dependencies between words
    in a text.</st> <st c="41106">BERT also has multiple layers of transformers, allowing
    it to learn even more complex representations.</st> <st c="41209">Compared to
    our Doc2Vec model, BERT is pre-trained already on large amounts of data, such
    as Wikipedia and BookCorpus with the objective of predicting the</st> <st c="41364">next
    sentence.</st>
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="40953">变压器模型的一个定义特征是使用自注意力机制来捕捉文本中词语之间的依赖关系。</st> <st c="41106">BERT
    也具有多层变压器，这使得它能够学习更复杂的表示。</st> <st c="41209">与我们的 Doc2Vec 模型相比，BERT 已经在大规模数据上进行了预训练，例如维基百科和
    BookCorpus，目的是预测</st> <st c="41364">下一个句子。</st>
- en: <st c="41378">Much like</st> <st c="41388">the</st> <st c="41392">previous two
    models, we provide code for you to compare retrieved results</st> <st c="41467">using
    BERT:</st>
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="41378">与之前的两个模型类似，我们为您提供了代码来比较使用 BERT 检索的结果</st> <st c="41467">：</st>
- en: '[PRE22]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: <st c="42859">There is</st> <st c="42869">one</st> <st c="42872">very important
    difference in this code compared to the usage of the last couple of models.</st>
    <st c="42964">Here, we are not tuning the model on our own data.</st> <st c="43015">This
    BERT model has already been trained on a large dataset.</st> <st c="43076">It
    is possible to fine-tune the model further with our data, which is recommended
    if you want to use this model.</st> <st c="43189">The results will reflect this
    lack of training, but we won’t let that prevent us from showing you how</st> <st
    c="43291">it works!</st>
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="42859">与之前几个模型的使用相比，这段代码有一个非常重要的区别。</st> <st c="42869">在这里，我们不是在自己的数据上调整模型。</st>
    <st c="42964">这个 BERT 模型已经在大数据集上进行了训练。</st> <st c="43015">我们可以使用我们的数据进一步微调模型，如果您想使用这个模型，这是推荐的。</st>
    <st c="43076">结果将反映这种训练不足，但我们不会让这阻止我们向您展示它是如何工作的！</st> <st c="43189">结果将反映这种训练不足，但我们不会让这阻止我们向您展示它是如何工作的！</st>
- en: <st c="43300">For this code, we are printing out the vector size for comparison
    to the others.</st> <st c="43382">Like the other models, we can see the top retrieved
    result.</st> <st c="43442">Here is</st> <st c="43450">the output:</st>
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="43300">对于这段代码，我们正在打印出向量大小以供与其他进行比较。</st> <st c="43382">与其他模型一样，我们可以看到检索到的最顶部结果。</st>
    <st c="43442">以下是</st> <st c="43450">输出：</st>
- en: '[PRE23]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: <st c="43652">The vector size is a respectable</st> `<st c="43686">768</st>`<st
    c="43689">. I don’t even need metrics to tell you that the top document it found
    is not the best chunk to answer the question</st> `<st c="43805">What are the
    advantages</st>` `<st c="43829">of RAG?</st>`<st c="43836">.</st>
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="43652">向量大小是可尊敬的</st> `<st c="43686">768</st>`<st c="43689">。我不需要指标就能告诉你，它找到的最顶部文档并不是回答问题的最佳片段</st>
    `<st c="43805">RAG 的优势是什么</st>` `<st c="43829">。</st>`
- en: <st c="43837">This</st> <st c="43842">model is</st> <st c="43852">powerful and
    has the potential to work better than the previous models, but we would need to
    do some extra work (fine-tuning) to get it to do a better job with our data when
    comparing it to the previous types of embedding models we have discussed so far.</st>
    <st c="44107">That may not be the case with all data, but typically, in a specialized
    domain like this, fine-tuning should be considered as an option for your embedding
    model.</st> <st c="44269">This is especially true if you are using a smaller local
    model rather than a large, hosted API such as OpenAI’s</st> <st c="44381">embeddings
    API.</st>
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="43837">这个</st> <st c="43842">模型功能强大，有可能比之前的模型表现更好，但我们需要做一些额外的工作（微调）才能让它在我们与之前讨论的嵌入模型类型进行比较时做得更好。</st>
    <st c="44107">这并不适用于所有数据，但在这种专业领域，微调应该被视为嵌入模型的一个选项。</st> <st c="44269">这尤其适用于您使用的是较小的本地模型，而不是像
    OpenAI 的</st> <st c="44381">嵌入 API 这样的大型托管 API。</st>
- en: <st c="44396">Running through these three different models illustrates how much
    embedding models have changed over the past 50 years.</st> <st c="44517">Hopefully,
    this exercise has shown you how important the decision is for what embedding model
    you select.</st> <st c="44623">We will conclude our discussion of embedding models
    by bringing us full circle back to the original embedding model we were using,
    the OpenAI embedding model from OpenAI’s API service.</st> <st c="44808">We will
    discuss the OpenAI model, as well as its peers on other</st> <st c="44872">cloud
    services.</st>
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="44396">运行这三个不同的模型说明了嵌入模型在过去50年中的变化。</st> <st c="44517">希望这次练习已经向你展示了选择嵌入模型的重要性。</st>
    <st c="44623">我们将通过回到我们最初使用的原始嵌入模型，即OpenAI的API服务中的OpenAI嵌入模型，来结束我们对嵌入模型的讨论。</st>
    <st c="44808">我们将讨论OpenAI模型，以及它在其他</st> <st c="44872">云服务中的同类模型。</st>
- en: <st c="44887">OpenAI and other similar large-scale embedding services</st>
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="44887">OpenAI和其他类似的大型嵌入服务</st>
- en: <st c="44943">Let’s talk a</st> <st c="44957">little more about the BERT model
    we just used, relative to OpenAI’s embedding model.</st> <st c="45042">This was
    the</st> `<st c="45055">'bert-base-uncased'</st>` <st c="45074">version, which</st>
    <st c="45089">is a pretty robust 110M parameter transformer model, especially
    compared to the previous models we used.</st> <st c="45195">We have come a long
    way since the TD-IDF model.</st> <st c="45243">Depending on the environment you
    are working in, this may test your computational limitations.</st> <st c="45338">This
    was the largest model my computer could run of the BERT options.</st> <st c="45408">But
    if you have a more powerful environment, you can change the model in these two
    lines</st> <st c="45497">to</st> `<st c="45500">'bert-large-uncased'</st>`<st
    c="45520">:</st>
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="44943">让我们更深入地谈谈我们刚刚使用的BERT模型，相对于OpenAI的嵌入模型。</st> <st c="45042">这是</st>
    `<st c="45055">'bert-base-uncased'</st>` <st c="45074">版本，这是一个相当健壮的1100万个参数的Transformer模型，特别是与之前我们使用的模型相比。</st>
    <st c="45195">自从TD-IDF模型以来，我们已经走了很长的路。</st> <st c="45243">根据你工作的环境，这可能会测试你的计算限制。</st>
    <st c="45338">这是我的电脑能够运行的BERT选项中最大的模型。</st> <st c="45408">但如果你有一个更强大的环境，你可以在这两行</st>
    <st c="45497">中将模型</st> <st c="45500">'bert-large-uncased'</st>`<st c="45520">:</st>
- en: '[PRE24]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: <st c="45641">You can see the full list of BERT options</st> <st c="45684">here:</st>
    [<st c="45690">https://huggingface.co/google-bert/bert-base-uncased</st>](https://huggingface.co/google-bert/bert-base-uncased
    )
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="45641">您可以在</st> <st c="45684">这里</st> <st c="45690">查看BERT选项的完整列表：</st>
    [<st c="45690">https://huggingface.co/google-bert/bert-base-uncased</st>](https://huggingface.co/google-bert/bert-base-uncased
    )
- en: <st c="45742">The</st> `<st c="45747">'bert-large-uncased'</st>` <st c="45767">model
    has 340M parameters, more than three times the size of</st> `<st c="45829">'bert-base-uncased'</st>`<st
    c="45848">. If your environment cannot handle this size of a model, it will crash
    your kernel and you will have to reload all your imports and relevant notebook
    cells.</st> <st c="46006">This just tells you how large these models can get.</st>
    <st c="46058">But just to be clear, these two</st> <st c="46090">BERT models are
    110M and 340M parameters, which is in millions,</st> <st c="46154">not billions.</st>
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="45742">The</st> `<st c="45747">'bert-large-uncased'</st>` <st c="45767">模型有3400万个参数，是</st>
    `<st c="45829">'bert-base-uncased'</st>`<st c="45848">的三倍多。如果你的环境无法处理这么大的模型，它将导致你的内核崩溃，你将不得不重新加载所有导入和相关的笔记本单元格。</st>
    <st c="46006">这仅仅告诉你这些模型可以有多大。</st> <st c="46058">但为了明确起见，这两个</st> <st c="46090">BERT模型分别有1100万个和3400万个参数，这是以百万为单位的，</st>
    <st c="46154">而不是十亿。</st>
- en: <st c="46167">The</st> <st c="46172">OpenAI embedding model that we have been
    using is based on the</st> **<st c="46235">GPT-3</st>** <st c="46240">architecture,
    which has 175 billion parameters.</st> <st c="46289">That is a</st> *<st c="46299">billion</st>*
    <st c="46306">with a</st> *<st c="46314">B</st>*<st c="46315">. We will be talking
    about their newer embedding models later in this chapter, which are based on the</st>
    **<st c="46417">GPT-4</st>** <st c="46422">architecture and have one trillion
    parameters (with a</st> *<st c="46477">T</st>*<st c="46478">!).</st> <st c="46482">Needless
    to say, these models are massive and dwarf any of the other models we have discussed.</st>
    <st c="46577">BERT and OpenAI are both transformers, but BERT was trained on 3.3
    billion words, whereas the full corpus for GPT-3 is estimated to be around 17
    trillion words (45 TB</st> <st c="46744">of text).</st>
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="46167">我们一直在使用的 OpenAI 嵌入模型基于</st> **<st c="46235">GPT-3</st>** <st c="46240">架构，该架构拥有
    1750 亿个参数。</st> <st c="46289">这是一个</st> *<st c="46299">十亿</st>* <st c="46306">，带有</st>
    *<st c="46314">B</st>*<st c="46315">。我们将在本章后面讨论他们的较新嵌入模型，这些模型基于</st> **<st c="46417">GPT-4</st>**
    <st c="46422">架构，并拥有一万亿个参数（带有</st> *<st c="46477">T</st>*<st c="46478">！）。</st>
    <st c="46482">不用说，这些模型规模巨大，远超我们之前讨论过的任何其他模型。</st> <st c="46577">BERT 和 OpenAI
    都是 Transformer，但 BERT 是在 33 亿个单词上训练的，而 GPT-3 的完整语料库估计约为 1700 亿个单词（45 TB 的文本）。</st>
- en: <st c="46753">OpenAI currently has three different embedding models available.</st>
    <st c="46819">We have been using the older model to save API costs based on GPT-3,</st>
    `<st c="46888">'text-embedding-ada-002'</st>`<st c="46912">, but it is a very
    capable embedding model.</st> <st c="46956">The other two newer models that are
    based on GPT-4 are</st> `<st c="47011">'text-embedding-3-small'</st>` <st c="47035">and</st>
    `<st c="47040">'text-embedding-3-large'</st>`<st c="47064">. Both of these models
    support the Matryoshka embeddings we talked about earlier, which allow you to
    use an adaptive retrieval approach for</st> <st c="47204">your retrieval.</st>
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="46753">OpenAI 目前有三种不同的嵌入模型可供选择。</st> <st c="46819">我们一直在使用基于 GPT-3 的较旧模型来节省
    API 成本，即</st> `<st c="46888">'text-embedding-ada-002'</st>`<st c="46912">，但它是一个非常强大的嵌入模型。</st>
    <st c="46956">另外两种基于 GPT-4 的新模型是</st> `<st c="47011">'text-embedding-3-small'</st>`
    <st c="47035">和</st> `<st c="47040">'text-embedding-3-large'</st>`<st c="47064">。这两个模型都支持我们之前讨论过的马赛克嵌入，这允许你为</st>
    <st c="47204">你的检索使用自适应检索方法。</st>
- en: <st c="47219">OpenAI is not the only cloud provider that offers a text-embedding
    API though.</st> `<st c="47422">'text-embedding-preview-0409'</st>`<st c="47451">.
    The</st> `<st c="47457">'text-embedding-preview-0409'</st>` <st c="47486">model
    is the only other large-scale cloud-hosted embedding model that I am aware of
    at this time that supports Matryoshka embeddings, beyond OpenAI’s</st> <st c="47637">newer
    models.</st>
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="47219">虽然 OpenAI 不是唯一提供文本嵌入 API 的云服务提供商。</st> `<st c="47422">'text-embedding-preview-0409'</st>`<st
    c="47451">。这个</st> `<st c="47457">'text-embedding-preview-0409'</st>` <st c="47486">模型是除
    OpenAI 的较新模型之外，我所知唯一支持马赛克嵌入的大型云托管嵌入模型。</st>
- en: '**<st c="47650">Amazon Web Services</st>** <st c="47670">(</st>**<st c="47672">AWS</st>**<st
    c="47675">) has</st> <st c="47682">embedding models based</st> <st c="47705">on
    their</st> **<st c="47714">Titan model</st>**<st c="47725">, as</st> <st c="47729">well
    as</st> **<st c="47738">Cohere’s embedding models</st>**<st c="47763">.</st> **<st
    c="47765">Titan Text Embeddings V2</st>** <st c="47789">is</st> <st c="47792">expected
    to launch soon and is also expected to support</st> <st c="47849">Matryoshka embedding.</st>'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**<st c="47650">亚马逊网络服务</st>** <st c="47670">(</st>**<st c="47672">AWS</st>**<st
    c="47675">) 提供基于他们</st> **<st c="47714">Titan 模型</st>**<st c="47725">的嵌入模型，以及</st>
    **<st c="47738">Cohere 的嵌入模型</st>**<st c="47763">。</st> **<st c="47765">Titan
    Text Embeddings V2</st>** <st c="47789">预计很快将推出，并预计也将支持</st> <st c="47849">马赛克嵌入。</st>'
- en: <st c="47870">That concludes our whirlwind adventure through 50 years of embedding
    generation technology!</st> <st c="47963">The models highlighted were selected
    to represent the progression of embedding</st> <st c="48042">capabilities over
    the past 50 years, but these are just a tiny sliver of the actual number</st>
    <st c="48133">of ways there are to generate embeddings.</st> <st c="48175">Now
    that your knowledge about embedding capabilities has been expanded, let’s turn
    to the factors you can consider when making the actual decisions on which model</st>
    <st c="48338">to use.</st>
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="47870">这就是我们穿越50年嵌入生成技术风驰电掣的冒险之旅的结束！</st> <st c="47963">所强调的模型被选中以代表过去50年嵌入能力的进步，但这些只是实际生成嵌入方式的微小部分。</st>
    <st c="48042">现在，你对嵌入能力的了解已经扩展，让我们转向你在实际决策中考虑的因素，选择哪个模型。</st> <st c="48133">来使用。</st>
- en: <st c="48345">Factors in selecting a vectorization option</st>
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="48345">选择向量化选项的因素</st>
- en: <st c="48389">Selecting the</st> <st c="48404">right vectorization option is
    a crucial decision when building a RAG system.</st> <st c="48481">Key considerations
    include the quality of the embeddings for your specific application, the associated
    costs, network availability, speed of embedding generation, and compatibility
    between embedding models.</st> <st c="48688">There are numerous other options
    beyond what we shared above that you can explore for your specific needs when
    it comes to selecting an embedding model.</st> <st c="48841">Let’s review</st>
    <st c="48854">these considerations.</st>
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="48389">选择</st> <st c="48404">正确的向量化选项是构建RAG系统时的一个关键决策。</st> <st c="48481">关键考虑因素包括特定应用中嵌入的质量、相关成本、网络可用性、嵌入生成速度以及嵌入模型之间的兼容性。</st>
    <st c="48688">在为选择嵌入模型时，还有许多其他选项可供探索，以满足您的特定需求。</st> <st c="48841">让我们回顾</st>
    <st c="48854">这些考虑因素。</st>
- en: <st c="48875">Quality of the embedding</st>
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="48875">嵌入质量</st>
- en: <st c="48900">When considering the quality of your embeddings, you cannot rely
    on just the generic metrics you have</st> <st c="49002">seen for each model.</st>
    <st c="49024">For example, OpenAI has been tested on the</st> `<st c="49134">'text-embedding-ada-002'</st>`
    <st c="49158">model, whereas the</st> `<st c="49178">'text-embedding-3-large'</st>`
    <st c="49202">model scored 64.6%.</st> <st c="49223">The metrics can be useful,
    especially when trying to hone in on a model of a certain quality, but this does
    not mean the model will be 3.6% better for your specific model.</st> <st c="49395">It
    does not even mean it will necessarily be better at all.</st> <st c="49455">Do
    not rely on generic tests completely.</st> <st c="49496">What ultimately matters
    is how well your embeddings work for your specific application of them.</st> <st
    c="49592">This includes embedding models that you train with your own data.</st>
    <st c="49658">If you work on an application that involves a specific domain, such
    as science, legal, or technology, it is very likely you can find or train a model
    that will work better with your specific domain data.</st> <st c="49862">When
    you start your project, try multiple embedding models within your RAG system and
    then use the evaluation techniques we share in</st> [*<st c="49995">Chapter 9</st>*](B22475_09.xhtml#_idTextAnchor184)
    <st c="50004">to compare results from using each model to determine which is best
    for</st> <st c="50077">your application.</st>
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="48900">在考虑嵌入质量时，你不能仅仅依赖你所看到的每个模型的通用指标。</st> <st c="49002">例如，OpenAI在“text-embedding-ada-002”模型上进行了测试，而“text-embedding-3-large”模型的得分是64.6%。</st>
    <st c="49134">'text-embedding-ada-002'</st> <st c="49158">模型，而</st> <st c="49178">'text-embedding-3-large'</st>
    <st c="49202">模型的得分是64.6%。</st> <st c="49223">这些指标可能是有用的，尤其是在尝试专注于特定质量的模型时，但这并不意味着该模型将比你的特定模型好3.6%。</st>
    <st c="49395">甚至不意味着它一定会更好。</st> <st c="49455">不要完全依赖通用测试。</st> <st c="49496">最终重要的是你的嵌入在你的特定应用中表现如何。</st>
    <st c="49592">这包括你用自己数据训练的嵌入模型。</st> <st c="49658">如果你从事一个涉及特定领域（如科学、法律或技术）的应用，你很可能可以找到或训练一个与你的特定领域数据表现更好的模型。</st>
    <st c="49862">当你开始你的项目时，尝试在RAG系统中使用多个嵌入模型，然后使用我们在</st> [*<st c="49995">第9章</st>*](B22475_09.xhtml#_idTextAnchor184)
    <st c="50004">中分享的评估技术来比较每个模型的使用结果，以确定哪个最适合</st> <st c="50077">你的应用。</st>
- en: <st c="50094">Cost</st>
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="50094">成本</st>
- en: <st c="50099">The costs for these embedding services vary from free to relatively
    expensive.</st> <st c="50179">OpenAI’s most expensive embedding model costs $0.13
    per million tokens.</st> <st c="50251">This means that for a page that has 800
    tokens, it will cost you $0.000104, or slightly more than 1% of 1 cent.</st> <st
    c="50363">That</st> <st c="50368">may not sound like much, but for most applications
    using embeddings, especially in the enterprise, these costs get multiplied rapidly,
    pushing the costs into the $1,000s or $10,000s for even a small project.</st>
    <st c="50576">But other embedding APIs cost less and may fit your needs just as
    well.</st> <st c="50648">And of course, if you build your own model like I described
    earlier in this chapter, you will only have the costs of the hardware or hosting
    costs for that model.</st> <st c="50811">That can cost much less over time and
    may meet your needs</st> <st c="50869">as well.</st>
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="50099">这些嵌入服务的成本从免费到相对昂贵不等。</st> <st c="50179">OpenAI最昂贵的嵌入模型每百万个令牌的成本为0.13美元。</st>
    <st c="50251">这意味着对于一个包含800个令牌的页面，它将花费您0.000104美元，或者略多于1美分的1%。</st> <st c="50363">这</st>
    <st c="50368">可能听起来不多，但对于大多数使用嵌入的应用程序，尤其是在企业中，这些成本会迅速增加，即使是小型项目，成本也可能达到1000美元或10000美元。</st>
    <st c="50576">但其他嵌入API的成本较低，也许同样能满足您的需求。</st> <st c="50648">当然，如果您像我在这章前面描述的那样构建自己的模型，您将只需承担该模型的硬件或托管成本。</st>
    <st c="50811">这可能会随着时间的推移而大幅降低成本，并且可能满足您的需求</st> <st c="50869">。</st>
- en: <st c="50877">Network availability</st>
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="50877">网络可用性</st>
- en: <st c="50898">There are a variety of scenarios you will want to consider in
    terms of network availability.</st> <st c="50992">Almost all applications will
    have some scenarios where the network will not be available.</st> <st c="51082">Network
    availability impacts your users’ access to your application interface, but it
    can also impact network calls you make from your application to other services.</st>
    <st c="51248">In this latter case, this could be a situation where your users
    can access your application’s interface but the application cannot reach OpenAI’s
    embedding service to generate an embedding for your user query.</st> <st c="51458">What
    will you do in this case?</st> <st c="51489">If you are using a model that is
    within your environment, this avoids this problem.</st> <st c="51573">This is
    a consideration of availability and the impact it has on</st> <st c="51638">your
    users.</st>
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="50898">在考虑网络可用性方面，您需要考虑各种场景。</st> <st c="50992">几乎所有应用程序都存在网络不可用的情况。</st>
    <st c="51082">网络可用性会影响用户访问您的应用程序接口，但它也可能影响您从应用程序到其他服务的网络调用。</st> <st c="51248">在后一种情况下，这可能是一种用户可以访问您的应用程序接口，但应用程序无法达到OpenAI的嵌入服务以为您用户查询生成嵌入的情况。</st>
    <st c="51458">在这种情况下，您会怎么做呢？</st> <st c="51489">如果您使用的是您环境内的模型，这可以避免这个问题。</st>
    <st c="51573">这是关于可用性和它对</st> <st c="51638">您的用户产生的影响的考虑。</st>
- en: <st c="51649">Keep in mind that you cannot just switch the embedding model for
    your user query, just in case you were thinking you could use a</st> *<st c="51779">fallback</st>*
    <st c="51787">mechanism and have a local embedding model available as a secondary
    option when the network is unavailable.</st> <st c="51896">If you use a proprietary
    API-only embedding model to vectorize your embeddings, you are committed to that
    embedding model, and your RAG system will be reliant on the availability of that
    API.</st> <st c="52088">OpenAI does not offer their embedding models to use locally.</st>
    <st c="52149">See the upcoming</st> *<st c="52166">Embedding</st>* *<st c="52176">compatibility</st>*
    <st c="52189">subsection!</st>
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="51649">请记住，您不能仅仅切换您用户查询的嵌入模型，以防您认为您可以使用</st> *<st c="51779">回退</st>*
    <st c="51787">机制，并在网络不可用时使用一个本地嵌入模型作为次要选项。</st> <st c="51896">如果您使用的是仅提供API的专有嵌入模型来矢量化您的嵌入，您就承诺了该嵌入模型，并且您的RAG系统将依赖于该API的可用性。</st>
    <st c="52088">OpenAI不提供其嵌入模型以供本地使用。</st> <st c="52149">请参阅即将到来的</st> *<st c="52166">嵌入</st>
    * *<st c="52176">兼容性</st> * <st c="52189">子节！</st>
- en: <st c="52201">Speed</st>
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="52201">速度</st>
- en: <st c="52207">The speed of generating embeddings is an important consideration,
    as it can impact the responsiveness and user experience of your application.</st>
    <st c="52351">When using a hosted API service such as OpenAI, you are making network
    calls to generate embeddings.</st> <st c="52452">While these network calls are
    relatively fast, there is still some latency involved compared to generating embeddings
    locally within your own environment.</st> <st c="52607">However, it’s important
    to note that local embedding generation is not always faster, as the speed also
    depends on the specific</st> <st c="52735">model being used.</st> <st c="52753">Some
    models may have slower inference times, negating the benefits of local processing.</st>
    <st c="52841">Key aspects to consider when determining the speed of your embedding
    option(s) are network latency, model inference time, hardware resources, and,
    in some cases where multiple embeddings are involved, the ability to generate
    embeddings in batches and optimize</st> <st c="53101">with that.</st>
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="52207">生成嵌入的速度是一个重要的考虑因素，因为它可能会影响你应用程序的响应性和用户体验。</st> <st c="52351">当使用如OpenAI这样的托管API服务时，你正在通过网络调用生成嵌入。</st>
    <st c="52452">虽然这些网络调用相对较快，但与在本地环境中生成嵌入相比，仍然存在一些延迟。</st> <st c="52607">然而，重要的是要注意，本地嵌入生成并不总是更快，因为速度也取决于所使用的特定</st>
    <st c="52735">模型。</st> <st c="52753">某些模型可能具有较慢的推理时间，从而抵消了本地处理的好处。</st> <st c="52841">在确定你的嵌入选项的速度时需要考虑的关键方面包括网络延迟、模型推理时间、硬件资源，以及在涉及多个嵌入的情况下，批量生成嵌入并优化</st>
    <st c="53101">的能力。</st>
- en: <st c="53111">Embedding compatibility</st>
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="53111">嵌入兼容性</st>
- en: <st c="53135">Pay close attention; this is a very important consideration and
    fact about embeddings!</st> <st c="53223">In any case when you are comparing embeddings,
    such as when you are detecting the similarity between a user query embedding and
    the embeddings stored in the vector store,</st> *<st c="53394">they must be created
    by the same embedding model</st>*<st c="53442">. These models generate unique
    vector signatures specific to only that model.</st> <st c="53520">This is even
    true of models at the same service provider.</st> <st c="53578">With OpenAI, for
    example, all three embedding models are not compatible with each other.</st> <st
    c="53667">If you use any of OpenAI’s embedding models to vectorize your embeddings
    stored in your vector store, you have to call the OpenAI API and use that same
    model when you vectorize the user query to conduct a</st> <st c="53872">vector
    search.</st>
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="53135">请注意；这是关于嵌入的一个非常重要考虑和事实！</st> <st c="53223">在任何比较嵌入的情况下，例如当你检测用户查询嵌入与存储在向量存储中的嵌入之间的相似性时，*<st
    c="53394">它们必须由相同的嵌入模型创建</st>*<st c="53442">。这些模型生成仅针对该模型的独特向量签名。</st> <st c="53520">即使是同一服务提供商的模型也是如此。</st>
    <st c="53578">例如，在OpenAI，所有三个嵌入模型之间都不兼容。</st> <st c="53667">如果你使用OpenAI的任何嵌入模型将你的向量存储中的嵌入向量化，你必须调用OpenAI
    API并在向量化用户查询以进行向量搜索时使用相同的模型。</st> <st c="53872">进行向量搜索。</st>
- en: <st c="53886">As your application expands in size, changing or updating an embedding
    model has major cost implications, since it means you will have to generate all
    new embeddings to use a new embedding model.</st> <st c="54083">This may even
    drive you to use a local model rather than a hosted API service since generating
    new embeddings with a model you have control over tends to cost</st> <st c="54242">much
    less.</st>
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的应用程序规模扩大时，更改或更新嵌入模型会产生重大的成本影响，因为这意味着你需要生成所有新的嵌入以使用新的嵌入模型。<st c="54083">这甚至可能迫使你使用本地模型而不是托管API服务，因为使用你控制的模型生成新的嵌入通常成本</st>
    <st c="54242">要低得多。</st>
- en: <st c="54252">While generic benchmarks can provide guidance, it’s essential
    to evaluate multiple embedding models within your specific domain and application
    to determine the best fit.</st> <st c="54424">Costs can vary significantly, depending
    on the service provider and the volume of embeddings required.</st> <st c="54527">Network
    availability and speed are important factors, especially when using hosted API
    services, as they can impact the responsiveness and user experience of your application.</st>
    <st c="54703">Compatibility between embedding models is also crucial, as embeddings
    generated by different models cannot be</st> <st c="54813">directly compared.</st>
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="54252">虽然通用基准可以提供指导，但评估您特定领域和应用中的多个嵌入模型以确定最佳匹配至关重要。</st> <st c="54424">成本可能会有很大差异，这取决于服务提供商和所需的嵌入量。</st>
    <st c="54527">网络可用性和速度是重要因素，尤其是在使用托管API服务时，因为它们可能会影响您应用程序的响应性和用户体验。</st> <st c="54703">嵌入模型之间的兼容性也非常关键，因为由不同模型生成的嵌入无法直接比较。</st>
- en: <st c="54831">As your application grows, changing or updating vector embedding
    models can have significant cost implications.</st> <st c="54944">Local embedding
    generation can offer more control and potentially lower costs, but the speed depends
    on the specific model and available hardware resources.</st> <st c="55101">Thorough
    testing and benchmarking are necessary to find the optimal balance of quality,
    cost, speed, and other relevant factors for your application.</st> <st c="55251">Now
    that we have explored the considerations for selecting a vectorization option,
    let’s dive into the topic of how they are stored with</st> <st c="55388">vector
    stores.</st>
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="54831">随着您的应用程序增长，更改或更新向量嵌入模型可能会产生重大的成本影响。</st> <st c="54944">本地嵌入生成可以提供更多控制，并可能降低成本，但速度取决于特定模型和可用的硬件资源。</st>
    <st c="55101">彻底的测试和基准测试对于找到适用于您应用程序的最佳质量、成本、速度和其他相关因素的平衡至关重要。</st> <st c="55251">现在我们已经探讨了选择向量化选项的考虑因素，让我们深入了解它们是如何通过</st>
    <st c="55388">向量存储进行存储的。</st>
- en: <st c="55402">Getting started with vector stores</st>
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="55402">开始使用向量存储</st>
- en: <st c="55437">Vector stores, combined</st> <st c="55462">with other data stores
    (databases, data warehouses, data lakes, and any other data sources) are the fuel
    for your RAG system engine.</st> <st c="55595">Not to state the obvious, but without
    a place to store your RAG-focused data, which typically involves the creating,
    management, filtering, and search of vectors, you will not be able to build a
    capable RAG system.</st> <st c="55810">What you use and how it is implemented
    will have significant implications for how your entire RAG system performs, making
    it a critical decision and effort.</st> <st c="55967">To start this section, let’s
    first go back to the original concept of</st> <st c="56037">a database.</st>
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="55437">向量存储，结合</st> <st c="55462">其他数据存储（数据库、数据仓库、数据湖以及任何其他数据源）是您RAG系统引擎的燃料。</st>
    <st c="55595">不言而喻，但没有一个地方来存储您专注于RAG的数据，这通常涉及向量的创建、管理、过滤和搜索，您将无法构建一个有能力的RAG系统。</st>
    <st c="55810">您所使用的内容及其实现方式将对您整个RAG系统的性能产生重大影响，使其成为一个关键的决定和努力。</st> <st c="55967">为了开始本节，让我们首先回顾一下</st>
    <st c="56037">数据库的原始概念。</st>
- en: <st c="56048">Data sources (other than vector)</st>
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="56048">数据源（除了向量之外）
- en: <st c="56081">In our</st> <st c="56088">basic RAG example so far, we are keeping
    it simple (for now) and have not connected it to an additional database resource.</st>
    <st c="56212">You could consider the web page that the content is pulled from
    as the database, although the most accurate description in this context is probably
    to call it an unstructured data source.</st> <st c="56400">Regardless, it is very
    likely your application will expand to the point of needing database-like support.</st>
    <st c="56506">This may come in the form of a traditional SQL database, or it may
    be in the form of a giant data lake (large repository of all types of raw, primarily
    unstructured data), where the data is preprocessed into a more usable format representing
    the data source that supports your</st> <st c="56783">RAG system.</st>
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="56081">在我们的</st> <st c="56088">基本的RAG示例中，我们目前保持简单（目前是这样），并且尚未将其连接到额外的数据库资源。</st>
    <st c="56212">您可以考虑内容提取的网页作为数据库，尽管在这个上下文中最准确的描述可能是将其称为非结构化数据源。</st> <st c="56400">无论如何，您的应用程序很可能发展到需要类似数据库的支持。</st>
    <st c="56506">这可能以传统SQL数据库的形式出现，或者可能是巨大的数据湖（所有类型原始数据的大型存储库），其中数据被预处理成更可用的格式，代表支持您的</st>
    <st c="56783">RAG系统数据源。</st>
- en: <st c="56794">The architecture of your data storage may be based on a</st> **<st
    c="56851">relational database management system</st>** <st c="56888">(</st>**<st
    c="56890">RDBMS</st>**<st c="56895">), a</st> <st c="56901">variety of different
    types of NoSQL, NewSQL (aimed at giving you the best of the two previous approaches),
    or various versions of data warehouses and data lakes.</st> <st c="57063">From
    the perspective of this book, we will approach the data sources these systems
    represent as an abstract concept of the</st> **<st c="57186">data source</st>**<st
    c="57197">. But what is important</st> <st c="57221">to consider here is that
    your decision on what vector store to use will likely be highly influenced by
    the existing architecture of your data source.</st> <st c="57371">The current
    technical skills of your staff will likely play a key role in these decisions</st>
    <st c="57461">as well.</st>
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="56794">您数据存储的架构可能基于一个</st> **<st c="56851">关系型数据库管理系统</st>** <st c="56888">(</st>**<st
    c="56890">RDBMS</st>**<st c="56895">**<st c="56888">)** <st c="56901">，多种不同类型的NoSQL，NewSQL（旨在结合前两种方法的优点），或者各种版本的数据仓库和数据湖。</st>
    <st c="57063">从本书的角度来看，我们将这些系统所代表的数据源视为一个抽象概念，即</st> **<st c="57186">数据源</st>**<st
    c="57197">。但在此需要考虑的重要问题是，您选择使用哪种向量存储的决定可能会受到您现有数据源架构的高度影响。</st> <st c="57221">您的员工当前的技术技能也可能会在这些决策中扮演关键角色。</st>
- en: <st c="57469">As an example, you may be</st> <st c="57496">using</st> **<st
    c="57502">PostgreSQL</st>** <st c="57512">for your RDBMS and have a team of expert
    engineers with significant expertise in fully utilizing and optimizing PostgreSQL.</st>
    <st c="57637">In this case, you</st> <st c="57655">will want to consider the</st>
    **<st c="57681">pgvector</st>** <st c="57689">extension for PostgreSQL, which
    turns PostgreSQL tables into vector stores, extending many of the PostgreSQL capabilities
    your team is familiar with into the vector world.</st> <st c="57862">Concepts
    such as indexing and writing SQL specifically for PostgreSQL are already going
    to be familiar, and that will help get your team up to speed quickly with how
    this extends to pgvector.</st> <st c="58054">If you are building your entire data
    infrastructure from scratch, which is rare in enterprise, then you may go a different
    route optimized for speed, cost, accuracy, or all of the above!</st> <st c="58241">But
    for most companies, you will need to consider compatibility with existing infrastructure
    in the vector store selection</st> <st c="58364">decision criteria.</st>
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="57469">例如，您可能正在</st> <st c="57496">使用</st> **<st c="57502">PostgreSQL</st>**
    <st c="57512">作为您的RDBMS，并且拥有一支在充分利用和优化PostgreSQL方面具有丰富经验的专家工程师团队。</st> <st c="57637">在这种情况下，您</st>
    <st c="57655">将希望考虑</st> **<st c="57681">pgvector</st>** <st c="57689">扩展，它将PostgreSQL表转换为向量存储，将您的团队熟悉的许多PostgreSQL功能扩展到向量世界。</st>
    <st c="57862">诸如索引和针对PostgreSQL特定编写的SQL等概念已经熟悉，这将帮助您的团队快速了解如何扩展到pgvector。</st>
    <st c="58054">如果您正在从头开始构建整个数据基础设施，这在企业中很少见，那么您可能选择一条针对速度、成本、准确性或所有这些优化的不同路线！</st>
    <st c="58241">但对于大多数公司来说，您在选择向量存储时需要考虑与现有基础设施的兼容性</st> <st c="58364">决策标准。</st>
- en: <st c="58382">Fun fact – What about applications such as SharePoint?</st>
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="58382">有趣的事实——那么像SharePoint这样的应用程序呢？</st>
- en: <st c="58437">SharePoint is</st> <st c="58451">typically considered a</st> **<st
    c="58475">content management system</st>** <st c="58500">(</st>**<st c="58502">CMS</st>**<st
    c="58505">) and may not fit strictly into</st> <st c="58537">the definitions of
    the other data sources we mentioned previously.</st> <st c="58605">But SharePoint
    and similar applications contain massive repositories of unstructured data, including
    PDF, Word, Excel, and PowerPoint documents that represent a huge portion of a
    company’s knowledge base, especially in large enterprise environments.</st> <st
    c="58855">Combine this with the fact that generative AI has shown a proclivity
    to tap into unstructured data unlike any other technology preceding it, and you
    have the makings of an incredible data source for RAG systems.</st> <st c="59067">These
    types of applications also have sophisticated APIs that can conduct data extraction
    as you pull the documents, such as pulling text out of a Word document and putting
    it into your database before vectorization.</st> <st c="59284">In many large companies,
    due to the high value of the data in these applications and the relative ease
    of extracting that data using the APIs, this has been one of the first sources
    of data for RAG systems.</st> <st c="59491">So yes, you can definitely include
    SharePoint and similar applications in your list of potential</st> <st c="59588">data
    sources!</st>
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="58437">SharePoint通常被认为是</st> <st c="58451">一个内容管理系统</st> <st c="58475">（</st>**<st
    c="58475">内容管理系统</st>**<st c="58500">）**<st c="58502">（CMS</st>**<st c="58505">）**，可能不完全符合我们之前提到的其他数据源的定义。</st>
    <st c="58537">但SharePoint和类似的应用程序包含大量的非结构化数据存储库，包括PDF、Word、Excel和PowerPoint文档，这些文档代表了一个公司知识库的很大一部分，尤其是在大型企业环境中。</st>
    <st c="58855">结合生成式AI显示出对非结构化数据有前所未有的倾向，这为RAG系统提供了一个令人难以置信的数据来源。</st> <st c="59067">这类应用程序还具有复杂的API，可以在提取文档时进行数据提取，例如在向量化之前从Word文档中提取文本并将其放入数据库中。</st>
    <st c="59284">在许多大型公司中，由于这些应用程序中数据的高价值以及使用API提取数据的相对容易，这已经成为RAG系统数据来源之一。</st>
    <st c="59491">因此，是的，你绝对可以将SharePoint和类似的应用程序列入你的潜在</st> <st c="59588">数据源列表！</st>
- en: <st c="59601">We will talk more about pgvector and other vector store options
    in a moment, but it is important to understand how these decisions can be very
    specific to each situation and that considerations other than just the vector
    store itself will play an important role in what you ultimately decide to</st>
    <st c="59897">work with.</st>
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="59601">我们将在稍后更多地讨论pgvector和其他向量存储选项，但了解这些决策可能非常具体于每种情况，并且除了向量存储本身之外的其他考虑因素将在你最终决定与之合作的内容中扮演重要角色。</st>
    <st c="59897">工作。</st>
- en: <st c="59907">Regardless of what option you choose, or are starting with, this
    will be a key component that feeds the data to your RAG system.</st> <st c="60037">This
    leads us to the vector stores themselves, which we can</st> <st c="60097">discuss
    next.</st>
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="59907">无论你选择什么选项，或者从哪里开始，这都将是一个向你的RAG系统提供数据的关键组件。</st> <st c="60037">这引出了向量存储本身，我们接下来可以</st>
    <st c="60097">讨论。</st>
- en: <st c="60110">Vector stores</st>
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="60110">向量存储</st>
- en: <st c="60124">Vector stores, also</st> <st c="60144">known as vector databases
    or vector search engines, are specialized storage systems designed to efficiently
    store, manage, and retrieve vector representations of data.</st> <st c="60313">Unlike
    traditional databases that organize data in rows and columns, vector stores are
    optimized for operations in high-dimensional vector spaces.</st> <st c="60460">They
    play a crucial role in an effective RAG system by enabling fast similarity search,
    which is essential for identifying the most relevant pieces of information in
    response to a</st> <st c="60640">vectorized query.</st>
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="60124">向量存储，也</st> <st c="60144">被称为向量数据库或向量搜索引擎，是一种专门为高效存储、管理和检索数据向量表示而设计的存储系统。</st>
    <st c="60313">与传统数据库按行和列组织数据不同，向量存储针对高维向量空间中的操作进行了优化。</st> <st c="60460">它们在有效的RAG系统中发挥着关键作用，通过实现快速相似性搜索，这对于在向量查询的响应中识别最相关的信息至关重要。</st>
    <st c="60640">向量化查询。</st>
- en: <st c="60657">The architecture of a vector store typically consists of three</st>
    <st c="60721">main components:</st>
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="60657">向量存储的架构通常由三个</st> <st c="60721">主要组件组成：</st>
- en: '**<st c="60737">Indexing layer</st>**<st c="60752">: This</st> <st c="60760">layer
    organizes the vectors in a manner that speeds up search queries.</st> <st c="60831">It
    employs indexing techniques such as tree-based partitioning (e.g., KD-trees) or
    hashing (e.g., locality-sensitive hashing) to facilitate fast retrieval of vectors
    that are near each other in the</st> <st c="61029">vector space.</st>'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="60737">索引层</st>**<st c="60752">：这一层</st> <st c="60760">以加快搜索查询的方式组织向量。</st>
    <st c="60831">它采用基于树的分区（例如，KD树）或哈希（例如，局部敏感哈希）等技术，以促进在</st> <st c="61029">向量空间中彼此靠近的向量的快速检索。</st>'
- en: '**<st c="61042">Storage layer</st>**<st c="61056">: The storage layer</st>
    <st c="61076">efficiently manages the data storage on disk or in memory, ensuring
    optimal performance</st> <st c="61165">and scalability.</st>'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="61042">存储层</st>**<st c="61056">：存储层</st> <st c="61076">高效管理磁盘或内存中的数据存储，确保最佳性能</st>
    <st c="61165">和可扩展性。</st>'
- en: '**<st c="61181">Processing layer (optional)</st>**<st c="61209">: Some vector
    stores include a processing layer to handle</st> <st c="61267">vector transformations,
    similarity computations, and other analytics operations in</st> <st c="61351">real
    time.</st>'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="61181">处理层（可选）</st>**<st c="61209">：一些向量存储包括一个处理层来处理</st> <st c="61267">向量转换、相似度计算和其他实时分析操作。</st>'
- en: <st c="61361">While it is technically possible to build a RAG system without
    using a vector store, doing so would result in suboptimal performance and scalability.</st>
    <st c="61512">Vector stores are specifically designed to handle the unique challenges
    of storing and serving high-dimensional vectors, offering optimizations that significantly
    improve memory usage, computation requirements, and</st> <st c="61727">search
    precision.</st>
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="61361">虽然在技术上可以构建一个不使用向量存储的RAG系统，但这样做会导致性能和可扩展性不佳。</st> <st c="61512">向量存储专门设计来处理存储和提供高维向量的独特挑战，提供了显著提高内存使用、计算需求和</st>
    <st c="61727">搜索精度</st>的优化。
- en: <st c="61744">As we’ve mentioned previously, it is important to note that while
    the terms</st> *<st c="61821">vector database</st>* <st c="61836">and</st> *<st
    c="61841">vector store</st>* <st c="61853">are often used interchangeably, not
    all vector stores are necessarily databases.</st> <st c="61935">There are other
    tools and mechanisms that serve the same or similar purpose as a vector database.</st>
    <st c="62033">For the sake of accuracy and consistency with LangChain documentation,
    we will use the term</st> *<st c="62125">vector store</st>* <st c="62137">to refer
    to all mechanisms that store and serve vectors, including vector databases and
    other</st> <st c="62232">non-database solutions.</st>
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="61744">正如我们之前提到的，需要注意的是，虽然术语</st> *<st c="61821">向量数据库</st>** <st c="61836">和</st>
    *<st c="61841">向量存储</st>** <st c="61853">经常互换使用，但并非所有向量存储都是数据库。</st> <st c="61935">还有其他工具和机制服务于与向量数据库相同或类似的目的。</st>
    <st c="62033">为了准确性和与LangChain文档的一致性，我们将使用术语</st> *<st c="62125">向量存储</st>** <st
    c="62137">来指代所有存储和提供向量的机制，包括向量数据库和其他</st> <st c="62232">非数据库解决方案。</st>
- en: <st c="62255">Next, let’s discuss the vector store options to give you a better
    grasp of what</st> <st c="62336">is available.</st>
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="62255">接下来，让我们讨论向量存储选项，以便您更好地了解</st> <st c="62336">可用的内容。</st>
- en: <st c="62349">Common vector store options</st>
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="62349">常见的向量存储选项</st>
- en: <st c="62377">When</st> <st c="62383">choosing a vector store, consider factors
    such as scalability requirements, ease of setup and maintenance, performance needs,
    budget constraints, and the level of control and flexibility you require over
    the underlying infrastructure.</st> <st c="62618">Additionally, evaluate the integration
    options and supported programming languages to ensure compatibility with your
    existing</st> <st c="62744">technology stack.</st>
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="62377">在选择向量存储时，考虑因素包括可扩展性要求、设置和维护的简便性、性能需求、预算限制以及您对底层基础设施的控制和灵活性水平。</st>
    <st c="62618">此外，评估集成选项和支持的编程语言，以确保与您现有的</st> <st c="62744">技术栈兼容。</st>
- en: <st c="62761">There are quite a few vector stores, some from established database
    companies and communities, many that are new start-ups, many more that are appearing
    each day, and in all likelihood, some that will go out of business by the time
    you are reading this.</st> <st c="63016">It is a very active space!</st> <st c="63043">Stay
    vigilant and use the information in this chapter to understand the aspects that
    are most important to your specific RAG applications and then look at the current
    marketplace to determine which option works best</st> <st c="63259">for you.</st>
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="62761">目前有相当多的向量存储，一些来自知名数据库公司和社区，许多是新成立的初创公司，每天都有更多出现，而且很可能在你阅读这篇文档时，一些公司可能会倒闭。</st>
    <st c="63016">这是一个非常活跃的领域！</st> <st c="63043">保持警惕，并使用本章中的信息来了解对你特定的RAG应用最重要的方面，然后查看当前市场，以确定哪个选项最适合你。</st>
    <st c="63259">。</st>
- en: <st c="63267">We will focus on the vector stores that have established integration
    with LangChain, and even then, we will pair them down to not overwhelm you while
    also giving you enough options so that you can get a sense of what kinds of options
    are available.</st> <st c="63517">Keep in mind that these vector stores are adding
    features and improvements all the time.</st> <st c="63606">Be sure to look up
    their latest versions before making a selection!</st> <st c="63674">It could make
    all the difference you need to change your mind and make a</st> <st c="63747">better
    choice!</st>
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="63267">我们将重点关注已经与LangChain建立整合的向量存储，即便如此，我们也会将它们筛选到不会让你感到压倒性，同时也会给你足够的选择，以便你能感受到有哪些选项可供选择。</st>
    <st c="63517">请记住，这些向量存储一直在添加功能和改进。</st> <st c="63606">在选择之前，务必查看它们的最新版本！</st>
    <st c="63674">这可能会让你改变主意，做出更好的选择！</st>
- en: <st c="63761">In the following subsections, we will walk through some common
    vector store options that integrate with LangChain, along with what you should
    consider about each one during the</st> <st c="63939">selection process.</st>
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="63761">在以下小节中，我们将介绍一些与LangChain集成的常见向量存储选项，以及在选择过程中你应该考虑的每个选项。</st> <st
    c="63939">选择过程。</st>
- en: <st c="63957">Chroma</st>
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="63957">Chroma</st>
- en: '**<st c="63964">Chroma</st>** <st c="63971">is an</st> <st c="63978">open source
    vector database.</st> <st c="64007">It offers fast search performance and supports
    easy integration with LangChain through its Python SDK.</st> <st c="64110">Chroma</st>
    <st c="64117">stands out for its simplicity and ease of use, with a straightforward
    API and support for dynamic filtering of collections during search.</st> <st c="64255">It
    also offers built-in support for document chunking and indexing, making it convenient
    for working with large text datasets.</st> <st c="64382">Chroma is a good choice
    if you prioritize simplicity and want an open source solution that can be self-hosted.</st>
    <st c="64493">However, it may not have as many advanced features as some other
    options, such as distributed search, support for multiple indexing algorithms,
    and built-in hybrid search capabilities that combine vector similarity with</st>
    <st c="64713">metadata filtering.</st>'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**<st c="63964">Chroma</st>** <st c="63971">是一个</st> <st c="63978">开源向量数据库。</st>
    <st c="64007">它提供快速搜索性能，并通过其Python SDK轻松集成LangChain。</st> <st c="64110">Chroma</st>
    <st c="64117">以其简洁性和易用性而突出，具有直观的API和搜索过程中对集合动态过滤的支持。</st> <st c="64255">它还提供内置的文档分块和索引支持，方便处理大型文本数据集。</st>
    <st c="64382">如果你优先考虑简洁性并希望有一个可以自行托管的开源解决方案，Chroma是一个不错的选择。</st> <st c="64493">然而，它可能没有像一些其他选项（如分布式搜索、支持多种索引算法和内置的将向量相似性与元数据过滤相结合的混合搜索功能）那样多的高级功能。</st>'
- en: <st c="64732">LanceDB</st>
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="64732">LanceDB</st>
- en: '**<st c="64740">LanceDB</st>** <st c="64748">is a</st> <st c="64754">vector
    database designed for efficient</st> <st c="64792">similarity search and retrieval.</st>
    <st c="64826">It stands out for its hybrid search capabilities, combining vector
    similarity search with traditional keyword-based search.</st> <st c="64950">LanceDB
    supports various distance metrics and indexing algorithms, including</st> **<st
    c="65027">Hierarchical navigable small world</st>** <st c="65061">(</st>**<st
    c="65063">HNSW</st>**<st c="65067">) for efficient approximate nearest neighbor
    search.</st> <st c="65121">It</st> <st c="65124">integrates with LangChain and
    offers fast search performance and support for various indexing techniques.</st>
    <st c="65230">LanceDB is a good choice if you want a dedicated vector database
    with good performance and integration with LangChain.</st> <st c="65349">However,
    it may not have as large of a community or ecosystem compared to some</st> <st
    c="65428">other options.</st>'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '**<st c="64740">LanceDB</st>** 是一个专为高效相似搜索和检索设计的向量数据库。<st c="64748">它以其混合搜索能力而突出，结合了向量相似搜索和基于关键词的传统搜索。</st>
    <st c="64754">LanceDB 支持各种距离度量索引算法，包括</st> **<st c="65027">层次可导航小世界</st>** <st
    c="65061">(**<st c="65063">HNSW</st>**<st c="65067">**)，用于高效的近似最近邻搜索。<st c="65121">它</st>
    <st c="65124">与 LangChain 集成，并提供快速搜索性能和对各种索引技术的支持。</st> <st c="65230">如果您想要一个性能良好且与
    LangChain 集成的专用向量数据库，LanceDB 是一个不错的选择。</st> <st c="65349">然而，与一些其他选项相比，它可能没有这么大的社区或生态系统。</st>'
- en: <st c="65442">Milvus</st>
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="65442">Milvus</st>
- en: '**<st c="65449">Milvus</st>** <st c="65456">is an</st> <st c="65463">open source
    vector database that provides</st> <st c="65504">scalable similarity search and
    supports various indexing algorithms.</st> <st c="65574">It provides a cloud-native
    architecture and supports Kubernetes-based deployments for scalability and high
    availability.</st> <st c="65695">Milvus offers features such as multi-vector indexing,
    allowing you to search across multiple vector fields simultaneously, and provides
    a plugin system for extending its functionality.</st> <st c="65880">It integrates
    well with LangChain and offers distributed deployment and horizontal scalability.</st>
    <st c="65976">Milvus is a good fit if you need a scalable and feature-rich open
    source vector store.</st> <st c="66063">However, it may require more setup and
    management compared to</st> <st c="66125">managed services.</st>'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**<st c="65449">Milvus</st>** 是一个提供可扩展相似搜索并支持各种索引算法的开源向量数据库。<st c="65456">它提供了一种云原生架构，并支持基于
    Kubernetes 的部署，以实现可扩展性和高可用性。</st> <st c="65463">Milvus 提供了多向量索引等特性，允许您同时搜索多个向量字段，并提供一个插件系统以扩展其功能。</st>
    <st c="65504">它与 LangChain 集成良好，提供分布式部署和横向可扩展性。</st> <st c="65574">如果您需要一个可扩展且功能丰富的开源向量存储库，Milvus
    是一个不错的选择。</st> <st c="65695">然而，与托管服务相比，它可能需要更多的设置和管理。</st>'
- en: <st c="66142">pgvector</st>
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="66142">pgvector</st>
- en: '**<st c="66151">pgvector</st>** <st c="66160">is an</st> <st c="66166">extension
    for PostgreSQL that adds support</st> <st c="66209">for vector similarity search
    and integrates with LangChain as a vector store.</st> <st c="66288">It leverages
    the power and reliability of PostgreSQL, the world’s most advanced open source
    relational database, and benefits from PostgreSQL’s mature ecosystem, extensive
    documentation, and strong community support.</st> <st c="66505">pgvector seamlessly
    integrates vector similarity search with traditional relational database features,
    enabling hybrid</st> <st c="66624">search capabilities.</st>'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**<st c="66151">pgvector</st>** 是一个为 PostgreSQL 添加向量相似搜索支持并作为向量存储与 LangChain
    集成的扩展。<st c="66160">它利用了世界上功能最强大的开源关系型数据库 PostgreSQL 的力量和可靠性，并从 PostgreSQL 成熟的生态系统、广泛的文档和强大的社区支持中受益。</st>
    <st c="66288">pgvector 无缝地将向量相似搜索与传统的关系型数据库功能集成，实现了混合搜索能力。</st>'
- en: <st c="66644">Recent updates have improved the level of performance for pgvector
    to bring it in line with other dedicated vector database services.</st> <st c="66779">Given
    that PostgreSQL is the most popular database in the world (a battle-tested mature
    technology that has a huge community) and that the vector extension pgvector gives
    you all the capabilities of other vector databases, this combination offers a
    great solution for any company already</st> <st c="67067">using PostgreSQL.</st>
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="66644">最近的更新提高了pgvector的性能水平，使其与其他专门的向量数据库服务保持一致。</st> <st c="66779">鉴于PostgreSQL是世界上最受欢迎的数据库（一种经过实战考验的成熟技术，拥有庞大的社区），以及向量扩展pgvector为您提供了其他向量数据库的所有功能，这种组合为任何已经</st>
    <st c="67067">使用PostgreSQL的公司提供了一个绝佳的解决方案。</st>
- en: <st c="67084">Pinecone</st>
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="67084">Pinecone</st>
- en: '**<st c="67093">Pinecone</st>** <st c="67102">is a</st> <st c="67108">fully
    managed vector database service</st> <st c="67145">that offers high performance,
    scalability, and easy integration with LangChain.</st> <st c="67226">It provides
    a fully managed and serverless experience, abstracting away the complexities of
    infrastructure management.</st> <st c="67345">Pinecone offers features such as
    real-time indexing, allowing you to update and search vectors with low latency,
    and supports hybrid search, combining vector similarity with metadata filtering.</st>
    <st c="67539">It also provides features such as distributed search and support
    for multiple indexing algorithms.</st> <st c="67638">Pinecone is a good choice
    if you want a managed solution with good performance and minimal setup.</st> <st
    c="67736">However, it may be more expensive compared to</st> <st c="67782">self-hosted
    options.</st>'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**<st c="67093">Pinecone</st>** <st c="67102">是一个</st> <st c="67108">完全托管的向量数据库服务</st>
    <st c="67145">，提供高性能、可扩展性和与LangChain的轻松集成。</st> <st c="67226">它提供完全托管和无服务器体验，抽象出基础设施管理的复杂性。</st>
    <st c="67345">Pinecone提供实时索引等特性，允许您以低延迟更新和搜索向量，并支持混合搜索，结合向量相似度和元数据过滤。</st> <st
    c="67539">它还提供分布式搜索和多种索引算法的支持。</st> <st c="67638">如果您需要一个性能良好且设置简单的托管解决方案，Pinecone是一个不错的选择。</st>
    <st c="67736">然而，与自托管选项相比，它可能更昂贵。</st>'
- en: <st c="67802">Weaviate</st>
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="67802">Weaviate</st>
- en: '**<st c="67811">Weaviate</st>** <st c="67820">is an</st> <st c="67827">open
    source vector search engine that supports</st> <st c="67873">various vector indexing
    and similarity search algorithms.</st> <st c="67932">It follows a schema-based
    approach, allowing you to define a semantic data model for your vectors.</st>
    <st c="68031">Weaviate supports CRUD operations, data validation, and authorization
    mechanisms, and offers modules for common machine learning tasks such as text
    classification and image similarity search.</st> <st c="68223">It integrates with
    LangChain and offers features such as schema management, real-time indexing, and
    a GraphQL API.</st> <st c="68338">Weaviate is a good fit if you want an open source
    vector search engine with advanced features and flexibility.</st> <st c="68449">However,
    it may require more setup and configuration compared to</st> <st c="68514">managed
    services.</st>'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**<st c="67811">Weaviate</st>** <st c="67820">是一个</st> <st c="67827">开源向量搜索引擎，支持</st>
    <st c="67873">各种向量索引和相似度搜索算法。</st> <st c="67932">它采用基于模式的方案，允许您为您的向量定义语义数据模型。</st>
    <st c="68031">Weaviate支持CRUD操作、数据验证和授权机制，并提供用于常见机器学习任务的模块，如文本分类和图像相似度搜索。</st>
    <st c="68223">它集成了LangChain，并提供了诸如模式管理、实时索引和GraphQL API等特性。</st> <st c="68338">如果您需要一个具有高级功能和灵活性的开源向量搜索引擎，Weaviate是一个不错的选择。</st>
    <st c="68449">然而，与托管服务相比，它可能需要更多的设置和配置。</st>'
- en: <st c="68531">In the</st> <st c="68538">preceding subsections, we discussed
    various vector store options that integrate with LangChain, providing an overview
    of their features, strengths, and considerations for selection.</st> <st c="68721">This</st>
    <st c="68726">emphasizes the importance of evaluating factors such as scalability,
    ease of use, performance, budget, and compatibility with existing technology stacks
    when choosing a vector store.</st> <st c="68909">While broad, this list is still
    very short compared to the overall number of options available to integrate with
    LangChain and to use as vector stores</st> <st c="69060">in general.</st>
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="68531">在前几节中，我们讨论了与LangChain集成的各种向量存储选项，概述了它们的特性、优势和选择时的考虑因素。</st> <st
    c="68721">这</st> <st c="68726">强调了在选择向量存储时评估可扩展性、易用性、性能、预算以及与现有技术栈兼容性等因素的重要性。</st>
    <st c="68909">虽然这个列表很广泛，但与LangChain可集成和作为一般向量存储选项的总数相比，仍然非常短。</st> <st c="69060">。</st>
- en: <st c="69071">The vector stores mentioned span a range of capabilities, including
    fast similarity search, support for various indexing algorithms, distributed architectures,
    hybrid search combining vector similarity with metadata filtering, and integration
    with other services and databases.</st> <st c="69350">Given the rapid evolution
    of the vector store landscape, new options are emerging frequently.</st> <st c="69444">Use
    this information as a base, but when you are ready to build your next RAG system,
    we highly recommend you visit the LangChain documentation on available vector
    stores, and consider which option best suits your needs at</st> <st c="69667">that
    time.</st>
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="69071">所提到的向量存储涵盖了多种功能，包括快速相似性搜索、支持各种索引算法、分布式架构、结合向量相似性和元数据过滤的混合搜索，以及与其他服务和数据库的集成。</st>
    <st c="69350">鉴于向量存储领域的快速演变，新的选项频繁出现。</st> <st c="69444">请以此信息为基础，但当你准备构建下一个RAG系统时，我们强烈建议你访问LangChain关于可用向量存储的文档，并考虑当时最适合你需求的选项。</st>
    <st c="69667">。</st>
- en: <st c="69677">In the next section, we will talk more in-depth about considerations
    when choosing a vector store for your</st> <st c="69785">RAG system.</st>
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="69677">在下一节中，我们将更深入地讨论选择向量存储时需要考虑的因素。</st> <st c="69785">。</st>
- en: <st c="69796">Choosing a vector store</st>
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="69796">选择向量存储</st>
- en: <st c="69820">Selecting</st> <st c="69830">the right vector store for a RAG
    system involves considering several factors, including the scale of the data,
    the required search performance (speed and accuracy), and the complexity of the
    vector operations.</st> <st c="70041">Scalability is crucial for applications
    dealing with large datasets, requiring a mechanism that can efficiently manage
    and retrieve vectors from a growing corpus.</st> <st c="70204">Performance considerations
    involve evaluating the database’s search speed and its ability to return highly</st>
    <st c="70311">relevant results.</st>
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="69820">选择</st> <st c="69830">RAG系统的正确向量存储需要考虑多个因素，包括数据规模、所需的搜索性能（速度和准确性）以及向量操作的复杂性。</st>
    <st c="70041">对于处理大型数据集的应用，可扩展性至关重要，需要一种能够高效管理和检索从不断增长的语料库中向量的机制。</st> <st c="70204">性能考虑包括评估数据库的搜索速度及其返回高度相关结果的能力。</st>
- en: <st c="70328">Moreover, the ease of integration with existing RAG models and
    the flexibility to support various vector operations are also critical.</st> <st
    c="70464">Developers should look for vector stores that offer robust APIs, comprehensive
    documentation, and strong community or vendor support.</st> <st c="70598">As listed
    previously, there are many popular vector stores, each offering unique features
    and optimizations tailored to different use cases and</st> <st c="70742">performance
    needs.</st>
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="70328">此外，与现有RAG模型的集成简便性以及支持各种向量操作的灵活性也是至关重要的。</st> <st c="70464">开发者应寻找提供强大API、全面文档和强大社区或供应商支持的向量存储。</st>
    <st c="70598">如前所述，有许多流行的向量存储，每个都提供针对不同用例和性能需求定制的独特功能和优化。</st> <st c="70742">。</st>
- en: <st c="70760">When choosing a vector store, it’s essential to align the selection
    with the overall architecture and operational requirements of the RAG system.</st>
    <st c="70907">Here are some</st> <st c="70921">key considerations:</st>
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="70760">在选择向量存储时，确保其选择与RAG系统的整体架构和运营需求相一致。</st> <st c="70907">以下是一些</st>
    <st c="70921">关键考虑因素：</st>
- en: '**<st c="70940">Compatibility with existing infrastructure</st>**<st c="70983">:
    When evaluating vector stores, it’s crucial to consider how well they integrate
    with your existing data infrastructure, such as databases, data warehouses, and
    data lakes.</st> <st c="71158">Assess the compatibility of the vector store with
    your current technology stack and the skills of your development team.</st> <st
    c="71279">For example, if you have strong expertise in a particular database system
    such as PostgreSQL, a vector store extension such as pgvector</st> <st c="71414">might
    be a suitable choice, as it allows for seamless integration and leverages your
    team’s</st> <st c="71507">existing knowledge.</st>'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="70940">与现有基础设施的兼容性</st>**<st c="70983">：在评估向量存储时，考虑它们与您现有的数据基础设施（如数据库、数据仓库和数据湖）的集成程度至关重要。</st>
    <st c="71158">评估向量存储与您当前技术栈和开发团队技能的兼容性。</st> <st c="71279">例如，如果您在特定的数据库系统（如PostgreSQL）方面有很强的专业知识，向量存储扩展（如pgvector）</st>
    <st c="71414">可能是一个合适的选择，因为它允许无缝集成并利用您团队的</st> <st c="71507">现有知识。</st>'
- en: '**<st c="71526">Scalability and performance</st>**<st c="71554">: What is the
    vector store’s ability to handle the expected growth of your data and the performance
    requirements of your RAG system?</st> <st c="71688">Assess the indexing and search
    capabilities of the vector store, ensuring it can deliver the desired level of
    performance and accuracy.</st> <st c="71824">If you anticipate a large-scale deployment,
    distributed vector databases such as Milvus or Elasticsearch with vector plugins
    might be more appropriate, as they are designed to handle high data volumes and
    provide efficient</st> <st c="72047">search throughput.</st>'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="71526">可扩展性和性能</st>**<st c="71554">：向量存储处理您预期数据增长和RAG系统性能要求的能力如何？</st>
    <st c="71688">评估向量存储的索引和搜索能力，确保它能够提供所需的性能和准确性。</st> <st c="71824">如果您预计进行大规模部署，具有向量插件的分布式向量数据库，如Milvus或Elasticsearch，可能更合适，因为它们旨在处理大量数据并提供高效的</st>
    <st c="72047">搜索吞吐量。</st>'
- en: '**<st c="72065">Ease of use and maintenance</st>**<st c="72093">: What is the
    learning curve associated with the vector store, taking into account the available
    documentation, community support, and vendor support?</st> <st c="72245">Understand
    the effort required for setup, configuration, and ongoing maintenance of the vector
    store.</st> <st c="72347">Fully managed services such as Pinecone can simplify
    deployment and management, reducing the operational burden on your team.</st>
    <st c="72473">On the other hand, self-hosted solutions such as Weaviate provide
    more control and flexibility, allowing for customization and fine-tuning to meet
    your</st> <st c="72625">specific requirements.</st>'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="72065">易用性和维护性</st>**<st c="72093">：考虑到可用的文档、社区支持和供应商支持，与向量存储相关的学习曲线是什么？</st>
    <st c="72245">了解设置、配置和向量存储的持续维护所需的工作量。</st> <st c="72347">完全托管服务，如Pinecone，可以简化部署和管理，减轻您团队的操作负担。</st>
    <st c="72473">另一方面，自托管解决方案，如Weaviate，提供更多控制和灵活性，允许您进行定制和微调以满足您的</st> <st c="72625">特定需求。</st>'
- en: '**<st c="72647">Data security and compliance</st>**<st c="72676">: Evaluate
    the security features and access controls provided by the vector store, ensuring
    they align with your industry’s compliance requirements.</st> <st c="72826">If
    you deal with sensitive data, assess the encryption and data protection capabilities
    of the vector store.</st> <st c="72935">Consider the vector store’s ability to
    meet data privacy regulations and standards, such as GDPR or HIPAA, depending
    on your</st> <st c="73060">specific needs.</st>'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="72647">数据安全和合规性</st>**<st c="72676">：评估向量存储提供的安全功能和访问控制，确保它们符合您所在行业的合规性要求。</st>
    <st c="72826">如果您处理敏感数据，评估向量存储的加密和数据保护能力。</st> <st c="72935">考虑向量存储满足数据隐私法规和标准的能力，例如根据您的</st>
    <st c="73060">特定需求，GDPR或HIPAA。</st>'
- en: '**<st c="73075">Cost and licensing</st>**<st c="73094">: What is the pricing
    model of the vector store?</st> <st c="73144">Is it based on data volume, search
    operations, or a combination of factors?</st> <st c="73220">Consider the long-term
    cost-effectiveness of the vector store, taking into account the scalability and
    growth projections of your RAG system.</st> <st c="73362">Assess the licensing
    fees, infrastructure costs, and maintenance expenses associated with the vector
    store.</st> <st c="73470">Open source solutions may have lower upfront costs but
    require more in-house expertise</st> <st c="73557">and resources for maintenance,
    while managed services may have higher subscription fees but offer simplified
    management</st> <st c="73677">and support.</st>'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="73075">成本和许可</st>**<st c="73094">：矢量存储的定价模式是什么？</st> <st c="73144">它是基于数据量、搜索操作，还是多种因素的组合？</st>
    <st c="73220">考虑矢量存储的长期成本效益，同时考虑您RAG系统的可扩展性和增长预测。</st> <st c="73362">评估与矢量存储相关的许可费用、基础设施成本和维护费用。</st>
    <st c="73470">开源解决方案可能具有较低的前期成本，但需要更多的内部专业知识和维护资源，而托管服务可能具有更高的订阅费用，但提供简化的管理和支持。</st>'
- en: '**<st c="73689">Ecosystem and integrations</st>**<st c="73716">: When selecting
    a vector store, it’s important to evaluate the ecosystem and integrations it supports.</st>
    <st c="73821">Consider the availability of client libraries, SDKs, and APIs for
    different programming languages, as this can greatly simplify the development
    process and enable seamless integration with your existing code base.</st> <st
    c="74035">Assess the compatibility of the vector store with other tools and frameworks
    commonly used in RAG systems, such as NLP libraries or machine learning frameworks.</st>
    <st c="74196">The general size of the supporting community is important as well;
    make sure it is at a critical mass to grow and thrive.</st> <st c="74318">A vector
    store with a robust ecosystem and extensive integrations can provide more flexibility
    and opportunities for extending the functionality of your</st> <st c="74471">RAG
    system.</st>'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="73689">生态系统和集成</st>**<st c="73716">：在选择矢量存储时，评估它支持的生态系统和集成非常重要。</st>
    <st c="73821">考虑不同编程语言的客户端库、SDK和API的可用性，因为这可以极大地简化开发过程，并使与现有代码库的无缝集成成为可能。</st>
    <st c="74035">评估矢量存储与其他在RAG系统中常用工具和框架的兼容性，例如NLP库或机器学习框架。</st> <st c="74196">支持社区的一般规模也很重要；确保它具有足够的规模以成长和繁荣。</st>
    <st c="74318">具有强大生态系统和广泛集成的矢量存储可以为您的RAG系统提供更多灵活性和扩展功能的机会。</st>'
- en: <st c="74482">By carefully evaluating these factors and aligning them with your
    specific requirements, you can make an informed decision when choosing a vector
    store for your RAG system.</st> <st c="74656">It’s important to conduct thorough
    research, benchmark different options, and consider the long-term implications
    of your choice in terms of scalability, performance,</st> <st c="74823">and maintainability.</st>
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="74482">通过仔细评估这些因素，并将它们与您的具体需求相匹配，您在选择RAG系统的矢量存储时可以做出明智的决定。</st> <st c="74656">进行彻底的研究，比较不同的选项，并考虑您选择的长期影响，包括可扩展性、性能和可维护性。</st>
- en: <st c="74843">Remember that the choice of vector store is not a one-size-fits-all
    decision, and it may evolve as your RAG system grows and your requirements change.</st>
    <st c="74995">It’s crucial to periodically reassess your vector store selection
    and adjust as needed to ensure optimal performance and alignment with your overall</st>
    <st c="75144">system architecture.</st>
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="74843">请记住，矢量存储的选择不是一刀切的决定，它可能会随着您的RAG系统的发展以及需求的变化而演变。</st> <st c="74995">定期重新评估您的矢量存储选择，并根据需要调整，以确保最佳性能和与整体系统架构的一致性至关重要。</st>
    <st c="75144">系统架构。</st>
- en: <st c="75164">Summary</st>
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="75164">总结</st>
- en: <st c="75172">The integration of vectors and vector stores into RAG systems
    is foundational for enhancing the efficiency and accuracy of information retrieval
    and generation tasks.</st> <st c="75340">By carefully selecting and optimizing
    your vectorization approach, as well as your vector store, you can significantly
    improve the performance of your RAG system.</st> <st c="75503">Vectorization techniques
    and vector stores are only part of how vectors play a role in RAG systems; they
    also play a major role in our retrieval stage.</st> <st c="75655">In the next
    chapter, we will address the retrieval role vectors play, going in-depth on the
    subject of vector similarity search algorithms</st> <st c="75794">and services.</st>
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 将向量和向量存储集成到RAG系统中是提高信息检索和生成任务效率和准确性的基础。<st c="75340">通过仔细选择和优化您的向量化和向量存储方法，您可以显著提高您RAG系统的性能。</st>
    向量化技术和向量存储只是向量在RAG系统中发挥作用的一部分；它们也在我们的检索阶段发挥着重要作用。<st c="75655">在下一章中，我们将讨论向量在检索中扮演的角色，深入探讨向量相似性搜索算法和服务。</st>
