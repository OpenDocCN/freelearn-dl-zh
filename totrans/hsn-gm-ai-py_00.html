<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Preface</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>This book is your one-stop shop for learning how various <strong>reinforcement learning</strong> (<strong>RL</strong>) techniques and algorithms play an important role in game development using Python.</span></p>
<p class="mce-root"><span>The book will start with the basics to provide you with the necessary foundation to understand how RL is playing a major role in game development. Each chapter will help you implement various RL techniques, such as Markov decision processes, Q-learning, the actor-critic method, <strong>state-action-reward-state-action</strong> (<strong>SARSA</strong>), and the deterministic policy gradients algorithm, to build logical self-learning agents. You will use these techniques to enhance your game development skills and add various features to improve your overall productivity. Later in the book, you will learn how deep RL techniques can be used to devise strategies that enable agents to learn from their own actions so that you can build fun and engaging games.</span></p>
<p class="mce-root"><span>By the end of the book, you will be able to use RL techniques to build various projects and contribute to open source applications.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Who this book is for</h1>
                </header>
            
            <article>
                
<p>This book is for game developers who are looking to add to their knowledge by implementing <span>RL</span> techniques to build games from scratch. This book will also appeal to machine learning and deep learning practitioners, and RL researchers who want to understand how self-learning agents can be used in the game domain. Prior knowledge of game development and a working knowledge of Python programming are expected.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What this book covers</h1>
                </header>
            
            <article>
                
<p><a href="5553d896-c079-4404-a41b-c25293c745bb.xhtml">Chapter 1</a>, <em>Understanding Rewards-Based Learning</em>, explores the basics of learning, what it is to learn, and how <span>RL</span> differs from other, more classic learning methods. From there, we explore how the Markov decision process works in code and how it relates to learning. This leads us to the classic multi-armed and contextual bandit problems. Finally, we will learn about Q-learning and quality-based model learning.</p>
<p><a href="8237fd36-1edf-4da0-b271-9a50c5b8deb3.xhtml">Chapter 2</a>, <em>Dynamic Programming and the Bellman Equation</em>, <span>digs deeper into dynamic programming and explores how the Bellman equation can be intertwined into RL. Here, you will learn how the Bellman equation is used to update a policy. We then go further into detail about policy iteration or value iteration methods using our understanding of Q-learning, by training an agent on a new grid-style environment.</span></p>
<p><a href="5f6ea967-ae50-426e-ad18-c8dda835a950.xhtml">Chapter 3</a>, <em>Monte Carlo Methods</em>, <span>explores model-based methods and how they can be used to train agents on more classic board games. </span></p>
<p><a href="bb05e528-e21b-4753-9e4c-372b8ed11e96.xhtml">Chapter 4</a>, <em>Temporal Difference Learning</em>, <span>explores the heart of RL and how it solves the temporal credit assignment problem often discussed in academia. We apply <strong>temporal difference learning</strong> (<strong>TDL</strong>) to Q-learning and use it to solve a grid world environment (such as FrozenLake).</span></p>
<p><a href="3e0c16c5-2145-498c-8ba1-b917745e0ef0.xhtml">Chapter 5</a>, <em>Exploring SARSA</em>, <span>goes deeper into the fundamentals of on-policy methods such as SARSA. We will explore policy-based learning through understanding the partially observable Markov decision process. Then, we'll look at how we can implement SARSA with Q-learning. This will set the stage for the more advanced policy methods that we will explore in later chapters, called PPO and TRPO.</span></p>
<p><a href="a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml">Chapter 6</a>, <em>Going Deep with DQN</em>, t<span>akes the Q-learning model and integrates it with deep learning to create advanced agents known as <strong>deep Q-learning networks</strong> (<strong>DQNs</strong>). From this, we explain how basic deep learning models work for regression or, in this case, to solve the Q equation. We will use DQNs in the CartPole environment. </span></p>
<p><a href="42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml">Chapter 7</a>, <em>Going Deeper with DDQNs</em>, looks at how extensions to <strong>deep learning</strong> (<strong>DL</strong>) called <strong>convolutional neural networks</strong> (<strong>CNNs</strong>) can be used to observe a visual state. We will then use that knowledge to play Atari games and look at further enhancements. </p>
<p><a href="42626cbd-87b8-428c-8f2a-ecc06f5e387c.xhtml">Chapter 8</a>, <em>Policy Gradient Methods</em>, <span>delves into more advanced policy methods and how they integrate into deep RL agents. This is an advanced chapter as it covers higher-level calculus and probability concepts. You will get to experience the MuJoCo animation RL environment in this chapter as a reward for your hard work. </span></p>
<p><a href="2f6812c0-fd1f-4eda-9df2-6c67c8077aec.xhtml">Chapter 9</a>, <em>Optimizing for Continuous Control</em>, looks at improving the policy methods we looked at previously for continuously controlling advanced environments. We start off by setting up and installing the MuJoCo environment. After that, we look at a novel improvement called recurrent networks for capturing context and see how recurrent networks are applied on top of PPO. Then we get back into the actor-critic method and this time look at asynchronous actor-critic in a couple of different configurations, before finally progressing to actor-critic with experience replay.</p>
<p><a href="1fbfb255-7fd9-44ea-8d02-f385e95d88d2.xhtml">Chapter 10</a>, <em>All Together Rainbow DQN</em>, tells us all about Rainbow. <span>Google DeepMind recently explored the combination of a number of RL enhancements all together in an algorithm called Rainbow. Rainbow is another advanced toolkit that you can explore and either borrow from or use to work with more advanced RL environments.</span></p>
<p><a href="ab9a7f4f-60d8-4643-8627-199cf95bcf55.xhtml">Chapter 11</a>, <em>Exploiting ML-Agents</em>, <span>looks at how we can either use elements from the ML-Agents toolkit in our own agents or use the toolkit to get a fully developed agent.</span></p>
<p><a href="6d061d35-176a-421a-9b62-aed35f48a6b7.xhtml">Chapter 12</a>, <em>DRL Frameworks</em>, opens up the possibilities of playing with solo agents in various environments. We will explore various multi-agent environments as well.</p>
<p><a href="e54c6adf-d238-4f1e-8e32-7ba3c5da0f46.xhtml">Chapter 13</a>, <em>3D Worlds</em>, trains us to <span>use RL agents effectively to tackle a variety of 3D environmental challenges.</span></p>
<p><a href="a171ddfa-e639-4b4e-9652-4279b5ac872a.xhtml">Chapter 14</a>, <em>From DRL to AGI</em>, looks beyond DRL and into the realm of AGI, or at least where we hope we are going with AGI. We will also looks at various DRL algorithms that can be applied in the real world.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">To get the most out of this book</h1>
                </header>
            
            <article>
                
<p>A working knowledge of Python and game development is essential. A good PC with a GPU would be beneficial.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Download the example code files</h1>
                </header>
            
            <article>
                
<p>You can download the example code files for this book from your account at <a href="http://www.packt.com" target="_blank" rel="noopener noreferrer">www.packt.com</a>. If you purchased this book elsewhere, you can visit <a href="https://www.packtpub.com/support">www.packtpub.com/support</a> and register to have the files emailed directly to you.</p>
<p>You can download the code files by following these steps:</p>
<ol>
<li>Log in or register at <a href="http://www.packt.com" target="_blank" rel="noopener noreferrer">www.packt.com</a>.</li>
<li>Select the <span class="packt_screen">Support</span> tab.</li>
<li>Click on <span class="packt_screen">Code Downloads</span>.</li>
<li>Enter the name of the book in the <span class="packt_screen">Search</span> box and follow the onscreen instructions.</li>
</ol>
<p>Once the file is downloaded, please make sure that you unzip or extract the folder using the latest version of:</p>
<ul>
<li>WinRAR/7-Zip for Windows</li>
<li>Zipeg/iZip/UnRarX for Mac</li>
<li>7-Zip/PeaZip for Linux</li>
</ul>
<p><span>The code bundle for the book is also hosted on GitHub at</span><span> </span><a href="https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-for-Games"><span class="Object">https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-for-Games</span></a><span>. </span><span>In case there's an update to the code, it will be updated on the existing GitHub repository.</span></p>
<p><span>We also have other code bundles from our rich catalog of books and videos available at</span><span> </span><strong><span class="Object"><a href="https://github.com/PacktPublishing/" target="_blank" rel="noopener noreferrer">https://github.com/PacktPublishing/</a></span></strong><span>. Check them out!</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Download the color images</h1>
                </header>
            
            <article>
                
<p>We also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here: <a href="http://www.packtpub.com/sites/default/files/downloads/9781839214936_ColorImages.pdf">http://www.packtpub.com/sites/default/files/downloads/9781839214936_ColorImages.pdf</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Conventions used</h1>
                </header>
            
            <article>
                
<p>There are a number of text conventions used throughout this book.</p>
<p><kbd>CodeInText</kbd>: <span>Indicates c</span>ode words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. <span>Here is an example:</span> "<span>The three functions </span><kbd>make_atari</kbd><span>, </span><kbd>wrap_deepmind</kbd><span>, and </span><kbd>wrap_pytorch</kbd><span> are all located in the new </span><kbd>wrappers.py</kbd><span> file we imported earlier.</span>"</p>
<p>A block of code is set as follows:</p>
<pre>env_id = 'PongNoFrameskip-v4'<br/>env = make_atari(env_id)<br/>env = wrap_deepmind(env)<br/>env = wrap_pytorch(env)</pre>
<p>When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:</p>
<pre>epsilon_start = 1.0<br/>epsilon_final = 0.01<br/><strong>epsilon_decay = 30000</strong><br/><br/>epsilon_by_episode = lambda episode: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * episode / epsilon_decay)<br/><br/><strong>plt.plot([epsilon_by_episode(i) for i in range(1000000)])</strong><br/>plt.show()</pre>
<p>Any command-line input or output is written as follows:</p>
<pre>pip install mujoco</pre>
<p><strong>Bold</strong>: Indicates a new term, an important word, or w<span>ords that you see onscreen. For example, words in menus or dialog boxes appear in the text like this. Here is an example: "Building on that, we'll look at a variant of the DQN called the <strong>DDQN</strong>, or <strong>double (dueling) DQN</strong>.</span><span>"</span></p>
<div class="packt_infobox">Warnings or important notes appear like this.</div>
<div class="packt_tip">Tips and tricks appear like this.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Get in touch</h1>
                </header>
            
            <article>
                
<p>Feedback from our readers is always welcome.</p>
<p class="mce-root"><strong>General feedback</strong>: If you have questions about any aspect of this book, <span>mention the book title in the subject of your message and</span> email us at <kbd><span>customercare@packtpub.com</span></kbd>.</p>
<p><strong>Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit <a href="https://www.packtpub.com/support/errata" target="_blank" rel="noopener noreferrer">www.packtpub.com/support/errata</a>, selecting your book, clicking on the Errata Submission Form link, and entering the details.</p>
<p><strong>Piracy</strong>: If you come across any illegal copies of our works in any form on the Internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <kbd>copyright@packt.com</kbd> with a link to the material.</p>
<p class="mce-root"><strong>If you are interested in becoming an author</strong>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit <a href="http://authors.packtpub.com/" target="_blank" rel="noopener noreferrer">authors.packtpub.com</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reviews</h1>
                </header>
            
            <article>
                
<p>Please leave a review. Once you have read and used this book, why not leave a review on the site that you purchased it from? Potential readers can then see and use your unbiased opinion to make purchase decisions, we at Packt can understand what you think about our products, and our authors can see your feedback on their book. Thank you!</p>
<p>For more information about Packt, please visit <a href="http://www.packt.com/" target="_blank" rel="noopener noreferrer">packt.com</a>.</p>


            </article>

            
        </section>
    </body></html>