- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Embedding Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Embedding models** are powerful machine learning techniques that simplify
    high-dimensional data into lower-dimensional space, while preserving essential
    features. Crucial in **natural language processing** (**NLP**), they transform
    sparse word representations into dense vectors, capturing semantic similarities
    between words. Embedding models also process images, audio, video, and structured
    data, enhancing applications in recommendation systems, anomaly detection, and
    clustering.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of an embedding model in action. Suppose the full plot in
    a database of movies has been previously embedded using OpenAI’s `text-embedding-ada-002`
    embedding model. Your goal is to find all movies and animations for *Guardians
    of the Galaxy*, but not by traditional phonetic or lexical matching (where you
    would type some of the words in the title). Instead, you will search by semantic
    means, say, the phrase `Awkward team of space defenders`. You will then use the
    same embedding model again to embed this phrase and query the embedded movie plots.
    *Table 4.1* shows an excerpt of the resulting embedding:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Dimension** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.00262913 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.031449784 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.0020321296 |'
  prefs: []
  type: TYPE_TB
- en: '| ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| 1535 | -0.01821267 |'
  prefs: []
  type: TYPE_TB
- en: '| 1536 | 0.0014683881 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.1: Excerpt of embedding'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will help you understand embedding models in depth. You’ll also
    implement an example using the Python language and the `langchain-openai` library.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Differentiation between embedding models and LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of embedding models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to choose an embedding model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector representations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To follow the examples in this chapter, you will need the following prerequisites:'
  prefs: []
  type: TYPE_NORMAL
- en: A MongoDB Atlas cluster. An Atlas `M0` free cluster should be sufficient as
    you will store a small set of documents and create only one vector index.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An OpenAI account and API key with access to the `text-embedding-3-large` model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Python 3 working environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You will also need to have installed Python libraries for MongoDB, LangChain,
    and OpenAI. You can install these libraries in your Python 3 environment as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To successfully execute the example in this chapter, you will need a MongoDB
    Atlas Vector Index created on the MongoDB Atlas cluster. The index name must be
    `text_vector_index`, created on the `embeddings.text` collection as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: What is an embedding model?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Embedding models are a type of tool used in machine learning and artificial
    intelligence that simplifies large and complex data into a more manageable form.
    This process, known as **embedding**, involves reducing the data’s dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine going from a detailed world map with highways, railroads, rivers, trails,
    and so on, to a simpler, summarized version with only country boundaries and capital
    cities. This not only makes computation faster and less resource-intensive, but
    also helps identify and understand relationships within the data. Because embedding
    models streamline the processing and analyzing of large datasets, they are particularly
    useful in areas of language (text) processing, image and sound recognition, and
    recommendation systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a vast library where each book stands for one point in high dimensions.
    Embedding models can help reorganize the library to improve ease of navigation,
    such as by grouping the books on related topics closer together and reducing the
    library’s overall size. *Figure 4**.1* illustrates this concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: An embedding model example for a library use case'
  prefs: []
  type: TYPE_NORMAL
- en: This conversion or reduction from a high-dimensional or original representation
    to a lower-dimensional representation created the basis for advancements in NLP,
    computer vision, and more.
  prefs: []
  type: TYPE_NORMAL
- en: How do embedding models differ from LLMs?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Embedding models are specialized algorithms that reduce high-dimensional data
    (such as text, images, or sound) into a low-dimensional space of dense vectors.
    On the other hand, LLMs are effective artificial neural networks pre-trained on
    gigantic corpora of textual data.
  prefs: []
  type: TYPE_NORMAL
- en: While both are rooted in neural networks, they employ distinct methodologies.
    LLMs are designed for generating coherent and contextually relevant text. LLMs
    leverage massive amounts of data to understand and predict language patterns.
    Their basic building blocks include transformer architectures, attention mechanisms,
    and large-scale pre-training followed by fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, embedding models focus on mapping words, phrases, or even entire
    sentences into dense vector spaces where semantic relationships are preserved.
    They often use techniques such as **contrastive loss**, which helps in distinguishing
    between similar and dissimilar pairs during training. Positive and negative sampling
    is another technique employed by embedding models. **Positive samples** are similar
    items (such as synonyms or related sentences), while **negative samples** are
    dissimilar items (such as unrelated words or sentences). *Figure 4**.2* visualizes
    an example of contrastive loss and positive and negative sampling in 2D space.
    This sampling aids the model in learning meaningful representations by minimizing
    the distance between positive pairs and maximizing the distance between negative
    pairs in the vector space.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: 2D visualization of contrastive loss and positive and negative
    sampling'
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, while LLMs excel in language generation tasks, embedding models
    are optimized for capturing and leveraging semantic similarities. Both enhance
    NLP by enabling machines to grasp and produce human language more effectively.
    Now, let’s look at an example of each.
  prefs: []
  type: TYPE_NORMAL
- en: '**Word2vec** (developed by Google) transforms words into vectors and discerns
    semantic relationships, such as “king” is to “man” as “queen” is to “woman.” It’s
    useful for sentiment analysis, translation, and content recommendations, enhancing
    natural language understanding for machines.'
  prefs: []
  type: TYPE_NORMAL
- en: '**GPT-4** (developed by OpenAI) is an LLM that is characterized by its ability
    to generate human-like text based on the input it receives. GPT-4 excels in a
    range of language-based tasks, including conversation, content generation, summarization,
    and translation. Its architecture allows it to comprehend the intricate details
    and nuances of language, enabling it to perform tasks that require a deep understanding
    of context, humor, irony, and cultural references.'
  prefs: []
  type: TYPE_NORMAL
- en: When to use embedding models versus LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Embedding models are used in scenarios where the goal is to capture and leverage
    the relationships within data. They are the ideal choice for the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Semantic similarity**: Finding or recommending items (such as documents or
    products) that are like a given item.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering**: Grouping entities based on their semantic properties.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Information retrieval**: Enhancing search functionalities by understanding
    the semantic content of queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLMs are the go-to for tasks that require text understanding, generation, or
    both, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Content creation**: Generating text that is coherent, contextually relevant,
    and stylistically appropriate. For example, generating a synopsis from the full
    plot of a movie.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conversational AI**: Building chatbots and virtual assistants that can understand
    and engage in human-like dialogue, such as answering questions about employment
    policies and employee benefits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language translation**: The extensive training on language-diverse datasets
    allows LLMs to handle idiomatic expressions, cultural nuances, and specialized
    terminology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embedding models and LLMs both play crucial roles in AI. Embedding models capture
    and manipulate semantic properties compactly, while LLMs excel in generating and
    interpreting text. Using both, and selecting the right embedding models based
    on your goals, can unlock AI’s full potential in your projects.
  prefs: []
  type: TYPE_NORMAL
- en: Types of embedding models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Word-level models, including **Global Vectors for Word Representation** (**GloVe**)
    and **Bidirectional Encoder Representations from Transformers** (**BERT**), capture
    broader textual meanings. Specialized models such as **fastText** adapt to linguistic
    challenges. All of these reflect the evolving landscape of embedding models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, you will explore many types of embedding models: word, sentence,
    document, contextual, specialized, non-text, and multi-modal.'
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`n` adjacent symbols in a particular order), which helps to better handle prefixes,
    suffixes, and rare words. Word2vec and GloVe are examples of these models.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Word2vec** was the first attempt of embedding models to learn the representation
    of words as vectors based on their contextual similarities. Developed by a team
    from Google, it uses two architectures: **Continuous Bag of Words** (**CBOW**),
    which predicts a word given a context, and **skip-gram**, which predicts a context
    for a given word. Word2vec has been seen to capture the relationship in the syntax
    of words, evidenced by its ability to deduce meanings from arithmetic operations
    performed with word vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: '**GloVe**, developed at Stanford University, merges the benefits of two leading
    word representation approaches: global matrix factorization with co-occurrence
    statistics and context window methods. By constructing a co-occurrence matrix
    from the corpus and applying dimensionality reduction techniques, GloVe captures
    both global statistics and local context, which is invaluable for tasks that require
    a deep understanding of word relationships.'
  prefs: []
  type: TYPE_NORMAL
- en: Sentence and document embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Sentence and document embedding models** capture the overall semantic meaning
    of text blocks by considering word context and arrangement. A common approach
    aggregates word vectors into a coherent vector for the whole text unit. These
    models are useful in document similarity, information retrieval, and text summarization,
    such as synopses versus full movie plots. Notable models include Doc2vec and BERT.'
  prefs: []
  type: TYPE_NORMAL
- en: Building on Word2vec, **Doc2vec**, which is also known as **Paragraph Vector**,
    encapsulates whole sentences or documents as vectors. Introducing a document ID
    token that allows the model to learn document-level embeddings alongside word
    embeddings aids significantly in tasks such as document classification and similarity
    comparison.
  prefs: []
  type: TYPE_NORMAL
- en: Google’s **BERT** employs context-aware embeddings, reading the entire sequence
    of words concurrently, unlike its predecessors that processed text linearly. This
    approach enables BERT to understand a word’s context from all surrounding words,
    resulting in more dynamic and nuanced embeddings and setting new standards across
    various NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Contextual embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Contextual embedding models** are designed to produce word vectors that vary
    according to the context of use in a sentence. These models use deep learning
    architectures by examining the whole sentence, or at times the surrounding sentences.
    The contextual model produces dynamic embeddings that capture nuances based on
    a word’s particular context and linguistic environment. A model architecture of
    this kind uses a bi-directional framework to process text both forward and in
    reverse, thereby capturing fine semantic and syntactic dependencies within the
    preceding and following contexts. They are useful in sentiment analysis (such
    as to interpret the tone of the text in an IT support ticket) and question-answering
    tasks where the exact meaning of words for interpretation is necessary. ELMo and
    GPT are two examples.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Embeddings from Language Models** (**ELMo**) introduced dynamic, context-dependent
    embeddings, producing variable embeddings based on a word’s linguistic context.
    This approach greatly enhances performance on downstream NLP tasks by providing
    a richer language understanding.'
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI’s **GPT series** leverages transformer technology to offer embeddings
    pre-trained on extensive text corpora and fine-tuned for specific tasks. GPT’s
    success underscores the efficacy of combining large-scale language models with
    transformer architectures in NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Specialized embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Specialized embedding models** capture specific linguistic properties, such
    as places, people, tone, and mood, in vector space. Some are language- or dialect-specific,
    while others analyze sentiment and emotional dimensions. Applications include
    legal document analysis, support ticket triage, sentiment analysis in marketing,
    and multilingual content management.'
  prefs: []
  type: TYPE_NORMAL
- en: '**fastText** is an example of a specialized embedding model. Developed by Facebook’s
    AI Research lab, fastText enhances Word2vec by treating words as bags of character
    n-grams, which proves particularly helpful for handling **out-of-vocabulary**
    (**OOV**) words. OOV words are words not seen during training and thus lack pre-learned
    vector representations, posing challenges for traditional models. fastText enables
    embeddings for OOV words through the summation of their sub-word embeddings. This
    makes it especially suitable for handling rare words and morphologically complex
    languages, which are languages with rich and varied word structures that use extensive
    prefixes, suffixes, and inflections to convey different grammatical meanings,
    such as Finnish, Turkish, and Arabic.'
  prefs: []
  type: TYPE_NORMAL
- en: Other non-text embedding models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Embedding models go beyond converting only text to vector representations.
    Images, audio, video, and even JSON data itself can be represented in vector form:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Images**: Models such as **Visual Geometry Group** (**VGG**) and **Residual
    Network** (**ResNet**) set benchmarks for the translation of raw images into dense
    vectors. These models capture important visual features, such as edges, textures,
    and color gradients, which are vital to many computer vision tasks, including
    image classification and object recognition. VGG works well at recognizing visual
    patterns, while ResNet improves accuracy in complex image-processing tasks, such
    as image segmentation or photo tagging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Audio**: OpenL3 and VGGish are models for audio. **OpenL3** is a model adapted
    from the L3-Net architecture that is used in audio event detection and environmental
    sound classification to embed audio into a temporal and spectral context-rich
    space. **VGGish** is born out of the VGG architecture for images, and so follows
    the same principle of converting sound waves into patterns of small, compact vectors.
    This simplifies tasks such as recognition of speech and music genres.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Video**: **3D Convolutional Neural Networks** (**3D CNNs** or **3D ConvNets**)
    and **Inflated 3D** (**I3D**) expand the capabilities of image embeddings in perceiving
    the temporal dynamics paramount to both action recognition and for video content
    analysis. 3D ConvNets apply convolutional filters in three dimensions (height,
    width, time) capturing spatial and temporal dependencies in volumetric data, making
    them particularly effective for spatiotemporal data, such as video analysis, medical
    imaging, and 3D object recognition. I3D uses a spatiotemporal architecture that
    combines the outputs of two 3D ConvNets: one processes RGB frames, while the other
    handles optical flow predictions between consecutive frames. I3D models are useful
    for sports analytics and surveillance systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graph data**: Node2vec and DeepWalk capture connectivity patterns of nodes
    within a graph and are applied in the domains of social network analysis, fraud
    detection, and recommendation systems. **Node2vec** learns continuous vector representations
    for nodes by performing biased random walks on the graph. This captures the diverse
    node relationships and community structures, improving the performance of tasks
    such as node classification and link prediction. **DeepWalk** treats random walks
    as sequences of nodes like sentences in NLP by capturing the structural relationships
    between nodes and encodes them into continuous vector representations, which can
    be used for node classification and clustering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**JSON data**: There are even JSON data embedding models, such as **Tree-LSTM**,
    which is a variation of the traditional **long short-term memory** (**LSTM**)
    networks, adapted specifically to handle data with a hierarchical tree structure,
    such as JSON. Unlike standard LSTM units that process data sequentially, Tree-LSTM
    operates over tree-structured data by incorporating states from multiple child
    nodes into a parent node, effectively capturing the dependencies in nested structures.
    This makes it particularly suitable for tasks such as semantic parsing and sentiment
    analysis, where understanding the hierarchical relationships within data can significantly
    improve performance. **json2vec** is an implementation of this kind of embedding
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After single-mode models, you can explore multi-modal models. These analyze
    multiple data types simultaneously and are crucial for applications such as autonomous
    driving, where merging data from sensors, cameras, and LiDAR builds a comprehensive
    view of the driving environment.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-modal models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Multi-modal embedding models** process and integrate information from many
    types of data sources into a unified embedding space. This approach is incredibly
    useful when different modalities complement or reinforce each other and together
    can lead to better AI applications. Multi-modal models are excellent for in-depth
    comprehension of multisensory input content, such as the tasks of multi-media
    search engines, automated content moderation, and interactive AI systems that
    can engage the user via visual and verbal interaction. Here are a few examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CLIP**: A well-known multi-modal model by OpenAI. It learns how to correlate
    visual images with textual descriptions in such a way that it can recognize images
    it has never seen during training, based on natural language queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LXMERT**: A model that focuses on processing both visual and text inputs.
    It can improve the performance of tasks such as answering questions with a visual
    aspect, which includes object detection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ViLBERT**: **Vision-and-Language BERT** (**ViLBERT**) extends the BERT architecture
    to process both visual and textual inputs simultaneously by using a two-stream
    model where one stream handles visual features extracted from images using a pre-trained
    **convolutional neural network** (**CNN** or **ConvNet**), and the other processes
    textual data with cross-attention layers facilitating interaction between the
    two modalities. ViLBERT is used for tasks such as visual question answering and
    visual commonsense reasoning, where understanding image-text relationships is
    essential.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VisualBERT**: Integrates visual and textual information by combining image
    features with contextualized word embeddings from a BERT-like architecture. It
    is commonly used for tasks such as image-text retrieval and image captioning,
    where aligning and understanding both visual and textual information are essential.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have now explored word, image, and multi-modal embeddings. Next, you’ll
    learn how to choose embedding models based on your application’s needs.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing embedding models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Embedding models impact an application’s performance, its ability to understand
    language and other forms of data, and ultimately, a project’s success. The following
    sections look at the parameters for choosing the right embedding model that aligns
    with the task requirements, characteristics of your dataset, and computational
    resources. This section explains vector dimensionality and model leaderboards
    as additional information to consider when choosing embedding models. For a quick
    overview of this section, you can consult *Table 4.2*.
  prefs: []
  type: TYPE_NORMAL
- en: Task requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each type of task may benefit from different embedding models based on how they
    process and represent text data. For instance, tasks such as text classification
    and sentiment analysis often require a deep understanding of semantic relationships
    at the word level. Word2vec or GloVe are particularly beneficial in these cases,
    as they provide robust word-level embeddings that capture semantic meanings.
  prefs: []
  type: TYPE_NORMAL
- en: For more complex linguistic tasks such as **named entity recognition** (**NER**)
    and **part-of-speech** (**POS**) tagging, the ability to understand the context
    in which a word is used becomes critical. Here, models such as BERT or ELMo show
    their strengths as they generate embeddings that vary dynamically based on the
    surrounding text, providing a richer and more precise understanding of each word’s
    role within a sentence. This deep contextual awareness is essential for accurately
    identifying entities and tagging parts of speech, as it allows the model to differentiate
    between words with multiple meanings based on their usage.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced models such as BERT, GPT, and Doc2vec are ideal for tasks requiring
    nuanced language understanding, such as question answering, machine translation,
    document similarity, and clustering. These models handle complex dependencies
    within text, making them suitable for analyzing entire documents. Doc2vec excels
    in comparing thematic similarities between documents, like finding similar news
    or sports articles.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset characteristics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When choosing an embedding model, consider the dataset’s size and characteristics.
    For morphologically rich languages or datasets with many OOV words, models such
    as fastText, which capture sub-word information, are advantageous. They handle
    new or rare words effectively. For texts with polysemous words (words with multiple
    meanings), contextual embeddings such as ELMo or BERT are essential, as they provide
    dynamic, context-specific representations.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset size influences the choice of embedding model. Larger datasets benefit
    from complex models such as BERT, GPT, and OpenAI’s `text-embedding-3-large`,
    which capture deep linguistic nuances but require substantial computational power.
    Smaller datasets might benefit from simpler models such as `text-embedding-3-small`,
    offering robust performance with less computational demand. This ensures even
    modest datasets can yield significant insights with the appropriate model.
  prefs: []
  type: TYPE_NORMAL
- en: Computational resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Computational cost is crucial when selecting an embedding model due to varying
    resource demands. Larger models such as GPT-4 require extensive computational
    power, making them less accessible to smaller organizations or projects with limited
    budgets.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a lightweight model or fine-tuning one for specific tasks can reduce
    computational needs, speed up development, and improve response times. Efficient
    models are essential for real-time tasks such as translation, speech recognition,
    and instant recommendations in gaming, media streaming, and e-commerce.
  prefs: []
  type: TYPE_NORMAL
- en: Some level of iterative experimentation helps identify the most suitable models.
    Staying updated on the latest developments is critical, as newer models frequently
    supersede older ones. Model leaderboards can help track advancements in the field
    and are covered later in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Vector representations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The size of a vector in an embedding model affects its ability to capture data
    complexity. Large vectors encode more information, allowing finer distinctions,
    but require more computation. Small vectors are more efficient but might miss
    subtle nuances. Choosing a vector size involves balancing detailed representation
    with practical constraints like memory and speed.
  prefs: []
  type: TYPE_NORMAL
- en: Why do vector dimensions matter?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Knowing the relationship between a vector, its size, and the second-last layer
    of a neural network is crucial for understanding the quality of the model’s output.
    The penultimate or second-last layer often serves as a feature extractor, where
    the dimensions of the output vector represent the learned features of the input
    data, as visualized in *Figure 4**.3*. The size of this vector directly influences
    the granularity of the representation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Penultimate layer of a neural network'
  prefs: []
  type: TYPE_NORMAL
- en: To obtain these vectors, the output layer (the last layer) of the neural network
    is removed, and the output from the preceding layer—the penultimate or second-last
    layer—is captured. Typically, the final layer outputs the model’s prediction,
    prompting the use of the output from the layer just before it. The data that is
    fed into the network’s predictive layer is known as **vector embedding**.
  prefs: []
  type: TYPE_NORMAL
- en: The dimensionality of a vector embedding aligns with the size of the penultimate
    layer of the underlying neural network of the model being used, making it synonymous
    with the vector’s size or length. Dimensionalities such as 384 (by SBERT’s `all-MiniLM-L6-v2`),
    768 (by SBERT’s `all-mpnet-base-v2`), 1,536 (by OpenAI’s `text-embedding-ada-002`),
    and 2,048 (from ResNet-50 by Microsoft Research) are common. Larger vectors are
    becoming available now, such as 3,072 by OpenAI’s `text-embedding-3-large`.
  prefs: []
  type: TYPE_NORMAL
- en: What does a vector embedding mean, and how is it typically used?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vector embeddings are the output of an embedding model, expressed as an array
    of floating-point numbers that typically range from –1.0 to +1.0\. Each position
    in the array represents a dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Vector embeddings play a key role in context-retrieval use cases, such as semantic
    search in chatbots. Data is embedded and stored in a vector database upfront,
    and queries must use the same embedding model for accurate results. Each embedding
    model produces unique embeddings based on its training data, making them specific
    to the model’s domain and not interchangeable. For example, the embedding obtained
    from a model trained on full documents of legal text will differ from one trained
    on healthcare data for patient history.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may recall the example of trying to find movies for *Guardians of the Galaxy*
    at the beginning of this chapter. You now understand why you had to embed the
    search string (which is also called the query vector) using the same embedding
    model. This workflow, common in AI applications, is explained in *Figure 4**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Typical data flow for embedding source data into the vector store
    and query vectors'
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow shows the *Transform into embedding* step twice: one for embedding
    existing data into a vector database (on the left) and another for real-time embedding
    of the query (on the right). Both steps must use the same embedding model.'
  prefs: []
  type: TYPE_NORMAL
- en: Embedding model leaderboards
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With such a variety of existing models and new models constantly evolving, how
    can you stay up to date? **Embedding model leaderboards**, such as those offered
    by platforms like Hugging Face, help gauge the performance of various models across
    numerous tasks. They provide transparent and competitive rankings of models based
    on criteria, such as accuracy and efficiency. By measuring models against standardized
    datasets and benchmark tasks, these leaderboards pinpoint state-of-the-art models
    and their trade-offs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Massive Text Embedding Benchmark** (**MTEB**) leaderboard from Hugging
    Face is a critical resource. It offers a comprehensive overview of the performance
    benchmarks of text embedding models. To see which models are setting the standard,
    visit the Hugging Face MTEB leaderboard: [https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard).'
  prefs: []
  type: TYPE_NORMAL
- en: You can also consult other leaderboards as you select the components of your
    AI/ML application architecture. Hugging Face hosts the Open LLM leaderboard ([https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard))
    and language-specific leaderboards, such as the Open Portuguese LLM leaderboard,
    the Open Ko-LLM leaderboard (Korean), and the Spanish Embeddings leaderboard.
    There are even industry-specific leaderboards, such as the Open Medical-LLM leaderboard.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding models overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Table 4.2* provides a quick overview of some of the embedding models covered
    in this chapter, focusing on their quality and ease of use. Each model’s description
    includes the quality of embeddings based on factors such as accuracy in downstream
    tasks and the richness of semantic representation, ease of use, documentation
    quality, and computational requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Embedding model** | **Embedding quality and ease** **of use** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Word2vec | High-quality, contextually rich embeddings. Available on TensorFlow
    and others, but limited availability online. |'
  prefs: []
  type: TYPE_TB
- en: '| GloVe | Robust embeddings, especially for less frequent words. Available
    on TensorFlow and others, but limited availability online. |'
  prefs: []
  type: TYPE_TB
- en: '| BERT | Contextualized embeddings that are rich and adaptable. Available online.
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPT | High-quality embeddings that excel in generative and language understanding
    tasks. Available online. |'
  prefs: []
  type: TYPE_TB
- en: '| Doc2vec | Suitable for document-level tasks; embeddings reflect broader context
    than word-level models. |'
  prefs: []
  type: TYPE_TB
- en: '| fastText | Captures OOV words effectively. Open source and remarkably lightweight.
    Works on standard hardware and can produce models small enough for mobile devices.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `text-embedding-3-large` | High-quality embeddings for sophisticated NLP
    tasks, capturing nuanced context. Replaced OpenAI’s `text-embedding-ada-002`.
    Can produce smaller vectors while maintaining high embedding quality. |'
  prefs: []
  type: TYPE_TB
- en: '| `text-embedding-3-small` | Good-quality embeddings for standard NLP tasks,
    balancing performance and computational requirements. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.2: Embedding quality and ease of use in various embedding models'
  prefs: []
  type: TYPE_NORMAL
- en: While this comparison should serve as a guide to selecting the most suitable
    embedding model for specific needs, the MTEB leaderboard mentioned previously,
    as well as online documentation, should always be consulted given the fast-moving
    development in this space.
  prefs: []
  type: TYPE_NORMAL
- en: Do you always need an embedding model?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: No, you don’t need an embedding model *always*. Not all situations call for
    the intricate details of an embedding model to represent data in the required
    vector form. For some applications, more straightforward vectorization methods
    are entirely adequate.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, complex public embedding models or bespoke models are unnecessary.
    Tasks with narrow focus, clear rules, or structured data can thrive on simple
    vector representations. This approach suits straightforward clustering, precise
    similarity measurements, and situations with limited computing power.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, **one-hot encoding** is a straightforward technique that turns
    categorical data into binary vectors, fitting perfectly for cases where categories
    are nominal without any intrinsic order. Similarly, **term frequency-inverse document
    frequency** (**TF-IDF**) vectors adeptly convey text significance for information
    retrieval and ranking tasks by highlighting the relevance of terms within documents
    in relation to the whole corpus.
  prefs: []
  type: TYPE_NORMAL
- en: These alternatives may lack the semantic depth of embedding models but provide
    computational efficiency and simplicity for tasks where intricate context isn’t
    required. Opting for simple vector representations enhances transparency, reduces
    computational demands or advanced scientific skill, and is ideal for swift performance
    or resource-limited environments, such as embedded systems or mobile devices.
  prefs: []
  type: TYPE_NORMAL
- en: With your understanding of embedding models established, you can now move on
    to a practical demonstration using Python, LangChain, MongoDB Atlas, and OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Executing code from LangChain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that you have explored the diverse types of embedding models, you will
    see what it is like to use them with working code. The following Python script
    (named `semantic_search.py`) uses the `langchain-openai` library to embed textual
    data with OpenAI’s `text-embedding-3-large` model, tailored to produce 1,024 dimensional
    vectors versus 3,072:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The console output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The example sets up the environment, authenticating to OpenAI with API keys,
    and connecting to MongoDB Atlas. Plots for three movies are then embedded and
    stored in MongoDB Atlas (the vector store) and different vector searches are then
    executed to demonstrate semantic search with ranked results.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Selecting the most appropriate embedding models and vector size is not merely
    a technical decision, but a strategic one that aligns with the unique characteristics,
    technical and organizational constraints, and objectives of your project.
  prefs: []
  type: TYPE_NORMAL
- en: Maintaining computational efficiency and cost is another cornerstone of effectively
    using embedding models. As some models can be resource-intensive and have higher
    response times and higher cost, optimizing the computational aspects without sacrificing
    the quality of the output is essential. Designing your system to use different
    embedding models depending on the task at hand will yield a more resilient application
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: It’s imperative to regularly evaluate your embedding model to ensure your AI/ML
    application continues to perform as expected. This involves routinely checking
    performance metrics and making necessary adjustments. Tweaking your model usage
    could mean altering vector sizes to avoid **overfitting**—where the model is too
    finely tuned to training data and performs poorly on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: It is essential to monitor vector search response times versus the embedding
    models being used and vector sizes, as these impact the user experience of AI-driven
    applications. Also consider the costs of maintaining and updating embedding models,
    including monetary, time, and resource expenses for re-embedding data. Planning
    for these helps make informed decisions on when updates are needed and balancing
    performance, cost-efficiency, and technological advancement.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covered the realm of embedding models, which are essential tools
    in AI/ML applications. They facilitate the transformation of high-dimensional
    data into a more manageable, lower-dimensional space. This process, known as embedding,
    significantly boosts computational efficiency and enhances the ability to describe
    and quantify relationships within data. Selecting the right embedding models for
    different types of data, such as text, audio, video, images, and structured data,
    is essential for expanding the reach of use cases and different workloads.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter also highlighted the importance of consulting leaderboards to gauge
    the effectiveness across the vast list of available models and the delicate balance
    necessary when choosing vector sizes, emphasizing the trade-offs between detail,
    efficiency, performance, and cost. While embedding models provide deep, contextual
    insights, simpler vectorization methods might be adequate for certain tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will delve into aspects of vector databases, examining the
    role of vector search in AI/ML applications with use cases.
  prefs: []
  type: TYPE_NORMAL
