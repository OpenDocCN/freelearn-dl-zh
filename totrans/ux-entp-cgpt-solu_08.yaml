- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Gathering Data – Content is King
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集数据 – 内容为王
- en: 'There is an assumption in this book: enterprise ChatGPT solutions are needed
    in almost all cases because a company has something unique to offer its customers,
    and it possesses an exceptional understanding of its products, services, and content.
    This content is private or unique and thus not part of **large language models**
    (**LLMs**) built from scraping the internet. Models are built on crawling the
    2+ billion pages of web content to teach the model. A third party, Commoncrawl.org,
    is commonly cited as a primary source of this material for major models (GPT-3,
    Llama). These models, which are massive collections of text, learn the statistical
    relationships of words and concepts and can be used to predict and respond to
    questions. Creating a model can take months; most have billions of connections
    and words. When customers come to the enterprise for answers, the models must
    include enterprise content that is not part of this crawl to make them unique,
    secure, and more accurate. This is done with the expectation that the solutions
    will be up to date, optimized to be cost-effective, and less prone to hallucinations
    or lying, as some call it.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本书有一个假设：几乎在所有情况下都需要企业ChatGPT解决方案，因为公司有独特的东西可以提供给客户，并且它对其产品、服务和内容的理解非常出色。这些内容是私有的或独特的，因此不属于从网络抓取构建的**大型语言模型**（**LLMs**）。模型是在爬取超过20亿页的网页内容的基础上构建的，以教授模型。第三方，Commoncrawl.org，通常被引用为该材料的主要来源，用于主要模型（GPT-3、Llama）。这些模型是大量文本的集合，学习单词和概念之间的统计关系，可以用来预测和回答问题。创建一个模型可能需要数月时间；大多数模型都有数十亿个连接和单词。当客户来到企业寻求答案时，模型必须包括不属于这次爬取的企业内容，以使其独特、安全且更准确。这是基于预期解决方案将是最新的、优化以具有成本效益，并且不太可能产生幻觉或谎言，正如一些人所说的那样。
- en: 'This chapter addresses gathering data for the LLM and how to include enterprise
    data sources in LLM solutions using a method called **Retrieval Augmented Generation**
    (**RAG**). We’ll discuss the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了为LLM收集数据以及如何使用称为**检索增强生成**（**RAG**）的方法将企业数据源包含在LLM解决方案中。我们将讨论以下主题：
- en: What is in a ChatGPT foundational model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ChatGPT基础模型中包含什么
- en: Incorporating enterprise data using RAG
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RAG整合企业数据
- en: Resources for RAG
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG资源
- en: This chapter and the next few will be more technical for those who have reached
    this point and are focused on user-centered design concepts. The chapter covers
    all the ideas and provides access to additional videos and online resources. The
    book does not require most of these external resources; they are meant to give
    more details.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章以及接下来的几章将更偏向技术性，对于那些已经达到这个水平并且专注于以用户为中心的设计概念的人来说。本章涵盖了所有这些想法，并提供了访问额外视频和在线资源的途径。本书不需要大多数这些外部资源；它们旨在提供更多细节。
- en: What is in a ChatGPT foundational model
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ChatGPT基础模型中包含什么
- en: 'When an LLM is built, it is trained on sources of data from the internet. It
    knows publicly available information about companies and products. If asked typical
    enterprise-like questions, it can get robust answers – sometimes better than what
    is available from some vendors’ websites. For example:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当构建LLM时，它是在互联网数据源上训练的。它了解关于公司和产品的公开信息。如果被问及典型的企业类问题，它可以给出稳健的答案——有时甚至比某些供应商网站上的信息更好。例如：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Try these questions out and notice a trend. Each answer is slightly more generic
    than the previous one, and that generic nature is part of the problem.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试这些问题并注意趋势。每个答案都比上一个答案稍微通用一些，而这种通用性正是问题的一部分。
- en: 'The following applies to most foundational models such as ChatGPT 3.5 or 4o,
    Anthropic’s Claude, Meta’s Llama, or Mistral7B:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下适用于大多数基础模型，如ChatGPT 3.5或4o、Anthropic的Claude、Meta的Llama或Mistral7B：
- en: Don’t understand specific business or use context or complex products
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不理解特定的业务或使用背景或复杂产品
- en: Don’t have customer history or context to consider
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有客户历史或背景信息可供考虑
- en: Can’t access proprietary knowledge sources
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法访问专有知识源
- en: Are not trained on service requests or other service data and won’t know correct
    assumptions from incorrect assumptions and inaccurate solutions
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有在服务请求或其他服务数据上训练，也不会知道正确的假设与错误的假设和不准确解决方案之间的区别
- en: Can’t integrate with databases or APIs for retrieval and task performance
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法与数据库或API集成以进行检索和任务执行
- en: Can’t be scaled or tuned for multi-tenancy
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法进行扩展或调整以支持多租户
- en: 'Now imagine those questions if they were in the context of rich business knowledge:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一下，如果这些问题是在丰富的商业知识背景下提出的：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Integrating data sources with ChatGPT to contextualize the solution to the business
    can address these richer questions. No matter the design pattern used, such as
    a chat UI, a hybrid experience, or a standalone recommender, enterprise data will
    make the solution powerful. Foundational models gain access to knowledge with
    Retrieval Augmented Generation or **RAG**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据源与ChatGPT集成，以将解决方案与业务情境化，可以解决这些更丰富的问题。无论使用哪种设计模式，如聊天UI、混合体验或独立推荐器，企业数据都将使解决方案变得强大。基础模型通过检索增强生成或**RAG**获得访问知识。
- en: Incorporating enterprise data using RAG
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RAG整合企业数据
- en: There are other ways of taking data and making it part of an LLM. It is possible
    to build a foundation model, but as mentioned, the training time and effort are
    extreme. Even with RAG, there are different approaches. Some technical resources
    are shared, but this chapter will focus on and teach RAG understanding and how
    product people can contribute to the development process. First, a RAG explanation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他方法可以将数据纳入LLM。可以构建一个基础模型，但如前所述，训练时间和努力是极端的。即使有RAG，也有不同的方法。一些技术资源是共享的，但本章将重点介绍RAG的理解，以及产品人员如何参与开发过程。首先，一个RAG的解释。
- en: Understanding RAG
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解RAG
- en: RAG supplements LLMs with enterprise data. RAG is a technique for retrieving
    information, such as from a knowledge base, and it can generate responses from
    authoritative knowledge collection with coherent and contextually accurate answers.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: RAG通过企业数据补充LLM。RAG是一种从知识库等信息源检索信息的技术，并且可以从权威知识集合中生成连贯且上下文准确的答案。
- en: 'This methodology allows us to overcome some generic model problems:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法使我们能够克服一些通用的模型问题：
- en: Material is always up to date since it is evaluated when prompted.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 材料总是最新的，因为它在需要时才会被评估。
- en: Tools can reference the document source and, thus, are more trustworthy.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工具可以引用文档源，因此更加可信。
- en: The foundational model is already trained, so supplementing it is inexpensive
    compared to building a model from scratch.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础模型已经训练好了，所以与从头开始构建模型相比，补充它是相对便宜的。
- en: It allows for a robust set of resources (APIs, SQL databases, and various document
    and presentation file formats) to continue to be managed independently (and still
    available to other solutions).
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许一组强大的资源（API、SQL数据库以及各种文档和演示文件格式）继续独立管理（并且仍然可供其他解决方案使用）。
- en: It will *NOT* be used to throw *ALL* data into the LLM. A mechanism will be
    used to send relevant documents to the LLM just in time for processing.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将*不会*用于将*所有*数据投入LLM。将使用一种机制，在处理时及时将相关文档发送到LLM。
- en: It allows for unique, secure answers with multiple customers.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许为多个客户提供独特、安全的答案。
- en: Technical work is needed to create a RAG pipeline. Even if this book isn’t about
    the development effort to create a RAG pipeline, it still stands to reason that
    a basic understanding of how data becomes valuable to the ChatGPT solution is
    needed. First, consider what would happen *if* all the enterprise data is thrown
    into the LLM. It would look something like *Figure 6**.1*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 需要进行技术工作来创建一个RAG管道。即使这本书不是关于创建RAG管道的开发工作，但仍有理由认为，需要有一个基本理解，即数据如何成为ChatGPT解决方案有价值的部分。首先，考虑一下，如果所有企业数据都投入LLM会发生什么。它看起来可能像*图6.1*。
- en: '![](img/B21964_06_01.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21964_06_01.jpg)'
- en: Figure 6.1 – A model where we add all our knowledge directly to the LLM
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 我们直接将所有知识添加到LLM中的模型
- en: The model in *Figure 6**.1* assumes it can handle all company knowledge and
    include it in an OpenAI model, resulting in a custom company model. This sounds
    right, but the cost and *months* it takes to create it are very high. There needs
    to be a way for the LLM to access all of our resources without the cost and complexity.
    Let’s review some limitations to find a solution to this problem.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.1*中的模型假设它可以处理所有公司知识并将其包含在一个OpenAI模型中，从而形成一个定制的公司模型。这听起来很合理，但创建它的成本和*月数*非常高。需要有一种方法让LLM能够访问所有我们的资源，而无需承担成本和复杂性。让我们回顾一些限制，以找到解决这个问题的方法。'
- en: Limitations of ChatGPT and RAG
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ChatGPT和RAG的限制
- en: To be clear, there are two kinds of limitations worth discussing. The first
    is the limitations of knowledge retrieval using OpenAI models or any models. In
    contrast, the second is the limitations of RAG, even when integrating third-party
    solutions to build an enterprise RAG solution.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了明确起见，有两个值得讨论的限制类型。第一个是使用OpenAI模型或任何模型进行知识检索的限制。相比之下，第二个是RAG的限制，即使整合第三方解决方案构建企业RAG解决方案也是如此。
- en: 'Most enterprise solutions will find data integration requirements within OpenAI
    limiting and look elsewhere for scalability, cost, and performance. With OpenAI
    File Search, which is their way of augmenting the LLM with knowledge, there are
    technical limits:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数企业解决方案都会发现 OpenAI 中的数据集成需求有限，并会寻找其他地方以实现可扩展性、成本和性能。使用 OpenAI 文件搜索，这是他们增强
    LLM 知识的方式，存在一些技术限制：
- en: Maximum file size of 512 MB
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大文件大小为 512 MB
- en: A limit of 5M tokens (up from 2M in the spring of 2024)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 5M 个标记的限制（从 2024 年春季的 2M 个标记增加）
- en: ChatGPT Enterprise supports a context length of 128K (up from 32K in the free
    version and the first Enterprise release)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ChatGPT 企业版支持 128K 的上下文长度（从免费版本和第一个企业版中的 32K 增加）
- en: 'Some limits on file formats (**.pdf**, **.md**, **.docx**) – the complete list
    is here:'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对文件格式（**.pdf**、**.md**、**.docx**）的一些限制——完整列表在此：
- en: 'Documentation: [Supported file formats](https://platform.openai.com/docs/assistants/tools/file-search/supported-files)
    ([https://platform.openai.com/docs/assistants/tools/file-search/supported-files](https://platform.openai.com/docs/assistants/tools/file-search/supported-files))'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档：[支持的文件格式](https://platform.openai.com/docs/assistants/tools/file-search/supported-files)
    ([https://platform.openai.com/docs/assistants/tools/file-search/supported-files](https://platform.openai.com/docs/assistants/tools/file-search/supported-files))
- en: Storage fees are $0.20/GB per assistant per day.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储费用为每位助手每天 $0.20/GB。
- en: These limits (as of September 2024) will change with some frequency. These limitations
    mean third parties are needed for solutions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这些限制（截至 2024 年 9 月）会频繁变化。这些限制意味着需要第三方解决方案。
- en: 'There are also some quality limitations:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些质量限制：
- en: Model data is available to everyone who has access to the model. Security barriers
    or limits are not implicit.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型数据对所有有权访问模型的人开放。安全屏障或限制不是隐含的。
- en: It won’t differentiate between general knowledge and internal knowledge. There
    are weights and an ability to prioritize and emphasize material, but it can still
    hallucinate without good reason.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不会区分一般知识和内部知识。存在权重和优先级和强调材料的能力，但它仍然可以没有理由地产生幻觉。
- en: When changes are made to knowledge, retraining is required and expensive. Since
    results need to be accurate and timely, this becomes a show-stopper.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当知识发生变化时，需要重新训练，这很昂贵。由于结果需要准确和及时，这成为了一个障碍。
- en: OpenAI’s File Search for knowledge retrieval handles one part of the process
    and doesn’t have the additional value of RAG around scale and data input types.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 的知识检索文件搜索处理了流程的一部分，并且没有 RAG 在可扩展性和数据输入类型方面的附加价值。
- en: There is a limit to how much can be shared with the LLM at one time. This is
    called the context window, and this chapter covers how to chunk information to
    fit into that context window. The larger the context window, the more knowledge
    and enterprise data can be shared with the LLM at one time to formulate answers.
    As the window grows larger, less RAG is needed to pre-fetch material. RAG is a
    more scalable and cost-effective approach.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一次可以与 LLM 分享的内容有限。这被称为上下文窗口，本章将介绍如何分块信息以适应该上下文窗口。上下文窗口越大，一次可以与 LLM 分享的知识和企业管理数据就越多，以便制定答案。随着窗口变大，预取材料所需的
    RAG 就越少。RAG 是一种更可扩展且成本效益更高的方法。
- en: Third-party solutions help avoid these limitations. To demo and understand the
    space for this book, the OpenAI built-in tools will work for the small demos.
    However, an enterprise solution will work with a third-party app for a production
    instance. The knowledge gained from these chapters is relevant to any LLM.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 第三方解决方案有助于避免这些限制。为了演示和理解本书的空间，OpenAI 内置工具将适用于小型演示。然而，企业解决方案将与第三方应用程序一起用于生产实例。这些章节中获得的知识对任何
    LLM 都相关。
- en: It is good to start with OpenAI’s built-in capabilities using the playground,
    so no coding is needed. No need to go to the documentation right now, but it is
    included anyway. This approach allows us to get a taste of custom models without
    the overhead required by a complete enterprise solution.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用游乐场以 OpenAI 内置功能开始是好的，这样就不需要编码。现在不需要查看文档，但仍然包含在内。这种方法使我们能够尝试自定义模型，而无需完整企业解决方案所需的额外开销。
- en: 'Article: [OpenAI’s File Search Documentation](https://platform.openai.com/docs/assistants/tools/file-search/quickstart)
    ([https://platform.openai.com/docs/assistants/tools/file-search/quickstart](https://platform.openai.com/docs/assistants/tools/file-search/quickstart))'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 文章：[OpenAI 文件搜索文档](https://platform.openai.com/docs/assistants/tools/file-search/quickstart)
    ([https://platform.openai.com/docs/assistants/tools/file-search/quickstart](https://platform.openai.com/docs/assistants/tools/file-search/quickstart))
- en: It takes significant work to go from data (this chapter) to [*Chapter 7*](B21964_07.xhtml#_idTextAnchor150),
    *Prompt Engineering*, and then to the next steps shown in [*Chapter 8*](B21964_08.xhtml#_idTextAnchor172),
    *Fine-Tuning* so that a solution can be reviewed with [*Chapter 9*](B21964_09_split_000.xhtml#_idTextAnchor190),
    *Guidelines and Heuristics*, and then analyzed for success in [*Chapter 10*](B21964_10_split_000.xhtml#_idTextAnchor216),
    *Monitoring and Evaluation*. For more resources, visit the OpenAI cookbook. It
    has a wealth of articles covering the entire LLM lifecycle, gives many good explanations
    and definitions, and lays out the process. Here is one good article.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据（本章）到[*第7章*](B21964_07.xhtml#_idTextAnchor150)，*提示工程*，再到[*第8章*](B21964_08.xhtml#_idTextAnchor172)，*微调*的下一步，需要做大量的工作，以便能够用[*第9章*](B21964_09_split_000.xhtml#_idTextAnchor190)，*指南和启发式方法*来审查解决方案，并在[*第10章*](B21964_10_split_000.xhtml#_idTextAnchor216)，*监控和评估*中进行分析，以确定成功。更多资源，请访问OpenAI食谱。它包含大量涵盖整个LLM生命周期的文章，提供了许多好的解释和定义，并概述了整个过程。这里有一篇很好的文章。
- en: 'Article: [An OpenAI cookbook article on Fine-Tuning for RAG using Qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant)
    ([https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant))'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 文章：[关于使用Qdrant进行RAG微调的OpenAI食谱文章](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant)
    ([https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant))
- en: The article is technical, but the concepts reinforce the learnings from this
    book. There needs to be a way to provide material to the model that doesn’t require
    retraining and can handle the scale of the enterprise problem. This is done by
    implementing a form of RAG that works off of an index of the knowledge and provides
    only relevant material, as needed, to the LLM. **Indexing** is a way to organize
    information for fast retrieval and comparison. There is not only one way to do
    this, but we’ll look at the basic approach to form an understanding of RAG. Some
    of the steps are beyond the scope of this book. Anyone reading is unlikely to
    build an LLM from scratch. Product people, especially those responsible for the
    knowledgebase or database resources, can improve the data coming into the solution
    to provide the indexing and LLM with the best chance of returning high-quality
    results. So, technology is introduced to handle the scale, performance, and quality
    needed. See *Figure 6**.2*. This requires us to focus on getting data into shape
    for indexing. In this approach, only relevant information is shared with the LLM
    to develop answers.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 文章是技术性的，但概念强化了本书的学习内容。需要有一种方法向模型提供材料，而无需重新训练，并且能够处理企业问题的规模。这是通过实现一种基于知识索引的RAG形式来完成的，根据需要只向LLM提供相关材料。**索引**是一种组织信息以便快速检索和比较的方法。做这件事的方法不止一种，但我们将探讨基本方法，以形成对RAG的理解。其中一些步骤超出了本书的范围。阅读本书的人不太可能从头开始构建一个LLM。产品人员，尤其是那些负责知识库或数据库资源的人员，可以通过改进进入解决方案的数据来提高索引和LLM返回高质量结果的最佳机会。因此，引入了技术来处理所需的规模、性能和质量。参见*图6**.2*。这要求我们专注于将数据整理成适合索引的形式。在这种方法中，只有相关信息与LLM共享，以开发答案。
- en: '![](img/B21964_06_02.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21964_06_02.jpg)'
- en: Figure 6.2 – Introducing the RAG solution to assist in the question-to-answer
    process
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 介绍RAG解决方案以协助问答过程
- en: This is dramatically oversimplifying the process. The indexing icon shown is
    a set of processes that result in a *limited* number of documents to share with
    the LLM as context for the question (this context is shared within the prompt
    – this prompt being the instructions shared with the LLM). The ingestion process
    includes cleaning the data, converting it to text, and creating vector representations
    to match the question’s vector representation against the indexed resources. This
    indexing process organizes the data to match like to like. **Vectorization** is
    the process of converting text into numeric vectors. **Embedding** is the process
    of determining similarity and semantic relationships based on the similarities
    of the vectors. All of the processing and matching is based on matching these
    vectors.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对过程的一个极大的简化。显示的索引图标是一系列过程，这些过程会产生一个*有限*数量的文档，这些文档将与LLM一起作为问题的上下文（这个上下文在提示中共享——这个提示是与LLM共享的指令）。摄入过程包括清理数据、将其转换为文本以及创建向量表示，以便将问题的向量表示与索引资源进行匹配。这个索引过程将数据组织得像似。**向量化**是将文本转换为数字向量的过程。**嵌入**是基于向量的相似性确定相似性和语义关系的过程。所有的处理和匹配都是基于匹配这些向量。
- en: To simplify the concept, consider vectors as numbers with a direction, like
    going west for 5 miles versus going north for 12 miles. The direction and magnitude
    in this example are two dimensions used to match results. However, in the case
    of LLMs, there are thousands of dimensions. The embedding process sees that similar
    vectors represent words with similar meanings and usage. They are in the same
    area. The best matches (north-west for 4 miles is roughly similar to going west
    for 5 miles) are then passed to the LLM for processing with the question. This
    means *the LLM is given limited information to generate its answer*. This also
    means there is value in ensuring that the knowledge and resources are ready for
    this process and using tools to return knowledge related to the presented question.
    This doesn’t require months to train a model. All of that work was done for us.
    However, the foundational model can be enhanced with prompt engineering and fine-tuning.
    **Prompt engineering** is the process of giving instructions to the model to tell
    it what to do, while **fine-tuning** is used to provide examples of what is expected
    from the generative output. Both are covered in the following two chapters.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化概念，可以将向量视为具有方向的数字，例如向西走5英里与向北走12英里。在这个例子中，方向和大小是两个用于匹配结果的维度。然而，在LLMs的情况下，有数千个维度。嵌入过程会看到相似的向量代表具有相似意义和用法的单词，它们位于同一区域。然后，最佳匹配（西北方向4英里大致相当于向西走5英里）将被传递给LLM进行处理，与问题一起。这意味着*LLM被赋予了有限的信息来生成其答案*。这也意味着确保知识和资源为这一过程做好准备，并使用工具返回与所提出问题相关的知识是有价值的。这不需要数月时间来训练模型。所有这些工作都为我们完成了。然而，基础模型可以通过提示工程和微调来增强。**提示工程**是向模型提供指令以告诉它要做什么的过程，而**微调**则用于提供对生成输出期望的示例。这两者都在接下来的两个章节中进行了介绍。
- en: Product owners, designers, writers, and those who care about content quality
    can add value to the input and the output. This chapter is about input and getting
    quality out of data sources. The following chapters will focus on the *output*
    to ensure accuracy when answering.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 产品负责人、设计师、作家以及那些关心内容质量的人可以为输入和输出增加价值。本章是关于输入，从数据源中获取高质量的内容。接下来的章节将专注于*输出*，以确保在回答问题时准确性。
- en: Further reading on RAG
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步阅读RAG
- en: There are plenty of good resources to explain RAG in more detail. Here are a
    few deeper dives into the subject. Let me start with Amazon’s introduction to
    RAG.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多很好的资源可以更详细地解释RAG。以下是一些深入探讨该主题的资料。让我从亚马逊对RAG的介绍开始。
- en: 'Article: [Amazon’s RAG Explanation](https://aws.amazon.com/what-is/retrieval-augmented-generation/)
    ([https://aws.amazon.com/what-is/retrieval-augmented-generation/](https://aws.amazon.com/what-is/retrieval-augmented-generation/))'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 文章：[亚马逊的RAG解释](https://aws.amazon.com/what-is/retrieval-augmented-generation/)
    ([https://aws.amazon.com/what-is/retrieval-augmented-generation/](https://aws.amazon.com/what-is/retrieval-augmented-generation/))
- en: This one goes deeper into the issues and technical pieces of the complete solution.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章节更深入地探讨了完整解决方案的问题和技术细节。
- en: 'Article: [Leveraging LLMs on your domain-specific knowledge base](https://www.ml6.eu/blogpost/leveraging-llms-on-your-domain-specific-knowledge-base)
    ([https://www.ml6.eu/blogpost/leveraging-llms-on-your-domain-specific-knowledge-base](https://www.ml6.eu/blogpost/leveraging-llms-on-your-domain-specific-knowledge-base))'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '文章: [在你的领域特定知识库中利用LLM](https://www.ml6.eu/blogpost/leveraging-llms-on-your-domain-specific-knowledge-base)
    ([https://www.ml6.eu/blogpost/leveraging-llms-on-your-domain-specific-knowledge-base](https://www.ml6.eu/blogpost/leveraging-llms-on-your-domain-specific-knowledge-base))'
- en: Databricks hosted an excellent one-hour video session. It covers prompt engineering
    and RAG.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks举办了一场精彩的一小时视频会议。它涵盖了提示工程和RAG。
- en: 'Video: [Accelerate your Generative AI journey](https://vimeo.com/891439013)
    ([https://vimeo.com/891439013](https://vimeo.com/891439013))'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '视频: [加速你的生成式AI之旅](https://vimeo.com/891439013) ([https://vimeo.com/891439013](https://vimeo.com/891439013))'
- en: Finally, to go deeper, review this well-done survey of RAG techniques and methods
    to learn more about how RAG can be implemented. This is my favorite reference
    for explaining the different approaches, and the authors plan on updating the
    article, so it should be current.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了更深入地了解，请回顾这份关于RAG技术和方法的出色调查，以了解更多关于如何实现RAG的信息。这是我解释不同方法的 favorite 参考，作者计划更新这篇文章，所以它应该是最新的。
- en: 'Article: [A survey of RAG for LLMs](https://arxiv.org/pdf/2312.10997.pdf) ([https://arxiv.org/pdf/2312.10997.pdf](https://arxiv.org/pdf/2312.10997.pdf))'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '文章: [LLM的RAG调查](https://arxiv.org/pdf/2312.10997.pdf) ([https://arxiv.org/pdf/2312.10997.pdf](https://arxiv.org/pdf/2312.10997.pdf))'
- en: By the process of elimination, there are only a few places where product people
    can insert themselves to help the process. Few can build an LLM from scratch,
    and the training data used in the base model is from billions of Internet records.
    There is limited ability to coach the customer on what questions to ask (a good
    design might encourage good behavior without forcing the user to adapt, per se).
    Meanwhile, in recommender UIs, there is no interactive UI.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 通过排除法，只有少数地方可以让产品人员介入以帮助这个过程。很少有人能够从头开始构建一个LLM，基础模型中使用的训练数据来自数十亿互联网记录。在指导客户提出什么问题方面能力有限（良好的设计可能会鼓励良好的行为，而无需强迫用户本身进行适应）。同时，在推荐UI中，没有交互式UI。
- en: Thus, the best value for our efforts is to target the proper use cases, create
    quality knowledge, and support robust access to enterprise databases and resources
    that will allow an LLM to generate results to achieve customer goals. Let’s build
    a simple demo incorporating a data source to help understand the limitations and
    capabilities of an LLM supplemented with private data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们努力的最佳价值是针对合适的使用场景，创建高质量的知识，并支持对企业数据库和资源的稳健访问，这将允许一个大型语言模型（LLM）生成结果以实现客户目标。让我们构建一个简单的演示，其中包含数据源，以帮助理解在私有数据支持下LLM的限制和功能。
- en: Building a demo with enterprise data
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于企业数据构建演示
- en: This is a simple example to make a point. We will start with a collection of
    Frequently Asked Questions (FAQs) common to almost all websites and businesses.
    Hundreds of FAQs that could be found on any financial website (a bank or a brokerage
    company) form the basis of the demo. We name this financial company Alligiance
    (All-i… not an e, so as not to run afoul of an actual company called Allegiance).
    The assistant can be called “Alli” (pronounced Ally). Let’s start with a file
    of raw HTML snippets, answering each question in a row in a table. The file is
    on GitHub, so please try it yourself.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的例子，旨在说明问题。我们将从一个几乎适用于所有网站和企业的常见问题解答（FAQs）集合开始。任何金融网站（如银行或经纪公司）上可能找到的数百个FAQ构成了演示的基础。我们称这家金融公司为Alligiance（All-i…不是一个e，以免与名为Allegiance的实际公司发生冲突）。助手可以称为“Alli”（发音为Ally）。让我们从一个包含原始HTML片段的文件开始，在表格中逐行回答每个问题。该文件位于GitHub上，所以请亲自尝试一下。
- en: 'GitHub: [FAQ Collection for Testing](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Example_FAQs_for_Demo.docx)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Example_FAQs_for_Demo.docx](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Example_FAQs_for_Demo.docx))'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 'GitHub: [测试用的FAQ集合](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Example_FAQs_for_Demo.docx)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Example_FAQs_for_Demo.docx](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Example_FAQs_for_Demo.docx))'
- en: To access the OpenAI playground, follow the instructions in [*Chapter 1*](B21964_01.xhtml#_idTextAnchor016),
    *Recognizing the Power of Design* *in ChatGPT*.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问OpenAI的Playground，请遵循[*第1章*](B21964_01.xhtml#_idTextAnchor016)中关于**在ChatGPT中认识设计力量**的说明。
- en: 'Demo: [OpenAI Playground](https://platform.openai.com/playground) ([https://platform.openai.com/playground](https://platform.openai.com/playground))'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 演示：[OpenAI Playground](https://platform.openai.com/playground) ([https://platform.openai.com/playground](https://platform.openai.com/playground))
- en: The demo starts by asking a simple, specific question about browser support
    that might be common for a private website application. The foundational model
    would not expect it to know the answer, as these FAQs for a company might only
    be for authenticated customers. Then we uploaded the file, as shown in *Figure
    6**.3*, and asked again. Play along, will you?
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 演示从询问一个简单、具体的问题关于浏览器支持开始，这个问题可能对私人网站应用来说是常见的。基础模型不会期望它知道答案，因为这些常见问题解答可能只为认证客户提供。然后我们上传了如图*图6.3*所示的文件，并再次提问。你会一起玩吗？
- en: Tip
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Try side-by-side comparisons: Open two browsers and run the LLM with and without
    the context document. There is also a compare button in the Playground. We will
    demonstrate the compare button later.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试并排比较：打开两个浏览器，分别运行带有和不带有上下文文档的LLM。在Playground中也有一个比较按钮。我们稍后会演示这个比较按钮。
- en: '![](img/B21964_06_03.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21964_06_03.jpg)'
- en: Figure 6.3 – OpenAI Playground shows an answer before and after adding the demo
    file
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – OpenAI Playground展示了添加演示文件前后的答案
- en: '[PRE2]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Table 6.1* compares the HTML source to the answer provided.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*表6.1* 比较了HTML源代码与提供的答案。'
- en: '| **Source Answer (HTML** **formatted version)** | **ChatGPT** **3.5 Answer**
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| **源答案（HTML** **格式化版本）** | **ChatGPT** **3.5答案** |'
- en: '| **Alligiance’s website is best viewed, and is most secure, using the latest
    versions of these browsers. Get free** **upgrades here:**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '| **Alligiance的网站最好使用，并且最安全，请使用以下浏览器的最新版本。在此处获取免费** **升级**：|'
- en: '**Microsoft® Internet Explorer (Windows 7 & 8 operating** **systems only)**'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Microsoft® Internet Explorer（仅限Windows 7 & 8操作系统）**'
- en: '**Google® Chrome**'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google® Chrome**'
- en: '**Mozilla® Firefox**'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Mozilla® Firefox**'
- en: '**Apple® Safari® (Mac operating** **systems only)**'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apple® Safari®（仅限Mac操作系统）**'
- en: '**Note: Our latest site features will not work with older, unsupported browser
    versions, specifically Internet Explorer 8, Safar****i 4 and 5, and Netscape.
    Internet Explorer 9 and 10 may provide a less than optimal experience. The use
    of unsupported browsers may also lead to vulnerability because of viruses, spyware,
    malware, and other** **security issues.****Safari no longer supports upgrades
    for Windows operating systems. If you are using a Windows operating system, please
    use one of the supported browsers above, other than Apple® Safari, for the** **best
    experience.**How to upgrade Apple® Safari**If you’re using an older version of
    Safari on a Mac, you’ll need to first check your Apple** **operating system:****From
    the Apple menu, choose “About** **This Mac.”****This continues for pages, see
    GitHub for the** **full text.** | Alligiance.com is best viewed and most secure
    using the latest versions of the following browsers:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 我们最新的网站功能不支持较旧的浏览器版本，特别是Internet Explorer 8、Safari 4和5、以及Netscape。Internet
    Explorer 9和10可能提供低于最佳体验。使用不受支持的浏览器也可能因病毒、间谍软件、恶意软件和其他**安全问题**而导致漏洞。Safari不再支持Windows操作系统的升级。如果您正在使用Windows操作系统，请使用上述受支持的浏览器之一，而不是Apple®
    Safari，以获得**最佳体验**。如何升级Apple® Safari**如果您在Mac上使用较旧的Safari版本，您首先需要检查您的Apple**操作系统**：从Apple菜单中选择“关于**此Mac**”。这会继续到多页，请参阅GitHub上的**完整文本**。
    | Alligiance.com最好使用以下浏览器的最新版本查看，并且最安全：'
- en: Microsoft® Internet Explorer (Windows 7 & 8 operating systems only)
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Microsoft® Internet Explorer（仅限Windows 7 & 8操作系统）
- en: Google® Chrome
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google® Chrome
- en: Mozilla® Firefox
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mozilla® Firefox
- en: Apple® Safari® (Mac operating systems only)
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apple® Safari®（仅限Mac操作系统）
- en: It’s important to note that the latest site features will not work with older
    unsupported browser versions, specifically Internet Explorer 8, Safari 4 and 5,
    and Netscape. Internet Explorer 9 and 10 may provide a less than optimal experience,
    and the use of unsupported browsers may lead to vulnerability due to viruses,
    spyware, malware, and other security issues[1].Additionally, for troubleshooting,
    if images on a page do not display, users can try clicking the Reload/Refresh
    button at the top of the browser or clearing the browser’s cache. It’s also noted
    that if the links to the images are no longer active, they will not be downloadable[2].[1][2]
    |
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，最新的网站功能不会与较旧的不再受支持的浏览器版本兼容，特别是Internet Explorer 8、Safari 4和5以及Netscape。Internet
    Explorer 9和10可能提供低于最佳体验，使用不受支持的浏览器可能导致由于病毒、间谍软件、恶意软件和其他安全问题而导致的漏洞[1]。此外，对于故障排除，如果页面上的图片不显示，用户可以尝试点击浏览器顶部的重新加载/刷新按钮或清除浏览器的缓存。还应注意，如果图片的链接不再活跃，它们将无法下载[2]。[1][2]
    |
- en: Table 6.1 – Comparison of source document text versus the ChatGPT answer
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1 – 源文档文本与ChatGPT答案的比较
- en: 'This example was run with gpt-3.5-turbo-1106\. It looks pretty good. Notice
    that it is not an exact quote of the source material. Here are some usability
    considerations:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例使用gpt-3.5-turbo-1106运行。看起来相当不错。注意，它并不是源材料的直接引用。以下是一些可用性考虑因素：
- en: The paragraph starting with “Additionally, for troubleshooting…” comes from
    a different FAQ and is unnecessary for this answer. Subsequent runs of this same
    question did not contain this spurious content.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以“此外，对于故障排除...”开头的段落来自不同的常见问题解答，对于这个答案是不必要的。对相同问题的后续运行没有包含这种虚假内容。
- en: The source answer should be shorter for a chat, which is common for documentation.
    [*Chapter 7*](B21964_07.xhtml#_idTextAnchor150)*, Prompt Engineering*, explains
    how to control this.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于聊天来说，源答案应该更短，这在文档中很常见。[*第7章*](B21964_07.xhtml#_idTextAnchor150)“提示工程”，解释了如何控制这一点。
- en: The style and tone are good. They are consistent with a business tone. Even
    though the content is in a business tone, style and tone could be adjusted using
    Prompt Engineering and Fine-Tuning. [*Chapter 8*](B21964_08.xhtml#_idTextAnchor172),
    *Fine-Tuning*, explores using examples to train the model.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 风格和语气很好。它们与商业语气保持一致。尽管内容是商业语气，但风格和语气可以通过提示工程和微调进行调整。[*第8章*](B21964_08.xhtml#_idTextAnchor172)“微调”，探讨了使用示例来训练模型。
- en: The list of browsers is returned in a bulleted list. Subsequent runs of this
    same question only sometimes returned this in a bullet list. Prompt engineering
    can also help return items like a bulleted list more consistently.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 浏览器列表以项目符号形式返回。对相同问题的后续运行有时只以项目符号列表返回。提示工程还可以帮助更一致地返回如项目符号列表这样的项目。
- en: Accuracy is critical in support applications. This answer is factually correct,
    but errors can occur. The next few chapters will discuss techniques for getting
    accurate answers.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确性在支持应用中至关重要。这个答案是事实正确的，但可能会发生错误。接下来的几章将讨论获取准确答案的技术。
- en: Hopefully, this analysis is helpful. There may be other items you can see. One
    must be able to evaluate response quality to care and feed the LLM. This is the
    crux of our mission.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这次分析有所帮助。可能还有其他你可以看到的项目。必须能够评估响应质量，以便关心并培养LLM。这是我们使命的核心。
- en: Quality issues
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 质量问题
- en: The following prompt was provided to the OpenAI model with the FAQ files attached.
    These instructions set the stage for any user interactions.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下提示是与附加的常见问题解答文件一起提供给OpenAI模型的。这些说明为任何用户交互设定了场景。
- en: '[PRE4]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The results around a few security questions are interesting. We will show some
    conversations. The convention is to show users messages on the right and the model’s
    response on the left, similar to the format for messages on your phone. As a reminder,
    conversations are never edited for typos or mistakes.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 几个安全问题周围的结果很有趣。我们将展示一些对话。惯例是显示用户消息在右侧，模型的响应在左侧，类似于手机上消息的格式。提醒一下，对话永远不会因为错别字或错误而编辑。
- en: '[PRE5]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The answer is perfectly reasonable and even logical. Except *none* of this comes
    from the corpus of FAQs that were uploaded. It is a good-sounding generic answer.
    The answer is not something it was trained on from our knowledge. This is classified
    as a **hallucination**. A hallucination is a model-generated text that is incorrect,
    nonsensical, or, in this case, not real. Lying to a customer can cause actual
    harm beyond just the apparent failure. Given the litigious nature of people, avoid
    getting sued for lying to customers. The courts in the US can hold the assistant
    liable as a representative of the company.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是完美合理的，甚至是有逻辑的。但是，**没有任何**这些内容来自上传的常见问题解答语料库。这是一个听起来很好的通用答案。这个答案不是我们从知识中训练出来的。这被归类为**幻觉**。幻觉是模型生成的文本，它是不正确的、不合逻辑的，或者在这种情况下，不是真实的。向客户撒谎可能会造成实际伤害，而不仅仅是明显的失败。鉴于人们的诉讼性质，避免因向客户撒谎而被起诉。美国的法院可以持有助手作为公司代表的责任。
- en: Since no data in the files suggests that Alligiance does regular security audits,
    the response doesn’t provide more details because it only refers to “typical”
    measures, not specifics. If the prompt is adapted to “only provide answers from
    the attached document,” then the LLM answers are similar to the browser answer
    because it discusses how unsupported browsers can have security issues. It is
    typical in an enterprise solution to limit the knowledge only to the company knowledge
    provided. This can reduce hallucinations. HTML files were provided, but it returned
    clean, formatted text. Not every system and process would be that fortunate. When
    scaling up, consider what it means to clean the enterprise data. In the end, all
    of these systems expect text as input. So somewhere, some tool is going to do
    that conversion. Time for some context around data cleaning.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 由于文件中的数据没有表明Alligiance公司定期进行安全审计，因此响应没有提供更多细节，因为它只提到了“典型”措施，而不是具体细节。如果提示被调整为“只提供附件文档中的答案”，那么LLM的回答将与浏览器回答相似，因为它讨论了不受支持的浏览器可能存在的安全问题。在企业解决方案中，通常只限制知识在公司提供的企业知识范围内。这可以减少幻觉。提供了HTML文件，但它返回了干净、格式化的文本。并非每个系统和流程都会如此幸运。在扩展规模时，考虑企业数据清洗的含义。最终，所有这些系统都期望以文本作为输入。所以，某个地方，某个工具将进行这种转换。现在是时候围绕数据清洗提供一些背景信息了。
- en: Cleaning data
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据清洗
- en: Cleaning data is tricky, and manually editing files is unreasonable at the enterprise
    scale. First, understand the problem and either work with vendors that provide
    tools to support creating a cleansing pipeline or start small and learn how to
    code tools piece by piece. Review what it takes to clean data and decide where
    to invest a team’s limited resources. One way or the other, most of this has to
    be automated. The reality is that some is manual work, especially early in the
    process.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 清洗数据很棘手，在企业规模上手动编辑文件是不合理的。首先，了解问题，要么与提供支持创建清洗管道工具的供应商合作，要么从小处着手，学习如何逐步编写工具。审查清洗数据所需的内容，并决定在哪里投资团队有限的资源。无论如何，大部分工作都需要自动化。现实是，有些工作需要手动完成，尤其是在流程的早期。
- en: Data cleaning also depends on the types of resources and how they will be used.
    Handling a large corpus of FAQs, knowledge articles, and marketing materials will
    require different tools than handling database queries. These are some generalizable
    issues to be aware of. Let’s start with how to handle documents.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗也取决于资源的类型以及它们将如何被使用。处理大量常见问题解答、知识文章和营销材料所需的工具与处理数据库查询所需的工具不同。以下是需要注意的一些一般性问题。让我们从如何处理文档开始。
- en: Tip
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Find or build tools to help automate this process, but it is real work for many
    use cases. The next sections include details to help understand the process in
    case issues arise with enterprise data. Some data types will require more effort.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找或构建工具以帮助自动化此过程，但对于许多用例来说，这是一项实际的工作。接下来的几节将提供详细信息，以帮助理解流程，以防企业数据出现问题时。某些数据类型可能需要更多努力。
- en: Data augmentation
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据增强
- en: 'Data augmentation addresses the issue of whether there is enough data. Is there
    enough knowledge about product questions? Are there enough data resources and
    historical data to form recommendations? Are language-specific examples available
    (hint: translate it and translate it back)? Or are various forms of training material
    needed to understand more diverse formats? **Augmentation** artificially generates
    this data to help make solutions more robust.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强旨在解决数据是否充足的问题。是否对产品问题有足够的知识？是否有足够的数据资源和历史数据来形成推荐？是否有特定语言的示例（提示：翻译后再翻译回来）？或者是否需要各种形式的训练材料来理解更多样化的格式？**增强**通过人工生成这些数据来帮助使解决方案更加稳健。
- en: Not all data can be easily augmented. An LLM can’t generate novel knowledge
    articles explaining a process it knows nothing about. But suppose you are training
    a model on specialized information, like understanding medical diagnosis and treatments,
    real-time data (like the weather), or any data that might need more recency than
    the model provides. In that case, the augmentation process can provide precise,
    up-to-date, and contextually relevant explanations.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有数据都可以轻松增强。一个大型语言模型（LLM）无法生成解释它一无所知的过程的新颖知识文章。但是，假设你正在训练一个模型，用于处理特定信息，例如理解医疗诊断和治疗、实时数据（如天气）或可能需要比模型提供更近期的任何数据。在这种情况下，增强过程可以提供精确、最新和上下文相关的解释。
- en: There are tricks. For example, there are times when translating material to
    another language using an LLM when there is limited language data and then translating
    it back can help improve the retrieval step. Or incorporate synonyms for product
    names in the text to create variations to train on. For the most part, be aware
    of this and consider whether there is data that can be used to train or test the
    model. This can be a resource once the state of the enterprise data is understood.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些技巧。例如，当使用LLM将材料翻译成另一种语言，在有限的语言数据下，然后再翻译回来时，可以帮助改进检索步骤。或者，在文本中包含产品名称的同义词以创建用于训练的变体。大部分情况下，要意识到这一点，并考虑是否有可用于训练或测试模型的数据。一旦了解了企业数据的状态，这可以成为一种资源。
- en: It is an option to use the LLM itself to generate training data. Use this as
    a resource and then apply common sense to decide what data to give feedback to
    the model to augment the baseline data with good-quality data. OpenAI suggests
    that, by training on augmented data, the model can handle variety and learn to
    handle noise in the system better when addressing new data. Experimenting and
    iterating will be needed to see what best improves results.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LLM本身生成训练数据是一个选项。将其作为资源使用，然后运用常识来决定向模型提供哪些数据以用高质量数据增强基线数据。OpenAI建议，通过在增强数据上训练，模型可以处理多样性并学会在处理新数据时更好地处理系统中的噪声。需要进行实验和迭代以查看什么最能改善结果。
- en: Try this prompt in ChatGPT to learn more about data augmentation.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在ChatGPT中尝试这个提示以了解更多关于数据增强的信息。
- en: '[PRE6]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Data annotation
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据标注
- en: Annotation is work. And it can be monotonous. **Annotation** is the process
    of marking content with notes to explain it. The concepts of tagging or labeling
    are fundamentally the same. Notes or details are associated with the content.
    This is done to help understand and mark up passages, content, tables, or anything
    that needs to be classified. What data to annotate will depend on the data and
    structure. For example, in long passages, annotation can be done for relevance.
    For tables, headers can be labeled better, which would be evident to a human but
    not a computer. Product items can be tagged so the models can learn sizes (S,
    M, L, XL), categories (first class, business class, economy), related products,
    or other essential attributes that help to give context to the material. With
    large documents, provide context to the chunks of data. For example, if a table
    is pages long, do the headers re-appear on every page? Would a human understand
    the headers if the document was broken into smaller manageable pieces? This is
    one example where the annotation is needed. Suppose the header talked about the
    product and product versions, and this header was for multiple pages earlier.
    In that case, if a chunk turns out to be one page in length, this product header
    information needs to cascade into each of the correct pages and chunks.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注释是一项工作。它可能很单调。**注释**是指用笔记标记内容以解释其过程。标签或标记的概念本质上是一样的。笔记或细节与内容相关联。这样做是为了帮助理解和标记段落、内容、表格或任何需要分类的东西。要注释的数据将取决于数据和结构。例如，在长段落中，可以针对相关性进行注释。对于表格，可以更好地标记标题，这对人类来说是显而易见的，但对计算机来说则不然。产品项目可以标记，这样模型就可以学习尺寸（S、M、L、XL）、类别（头等舱、商务舱、经济舱）、相关产品或其他有助于为材料提供背景的必要属性。对于大型文档，为数据块提供背景。例如，如果表格很长，标题会在每一页都重新出现吗？如果文档被分成更小的可管理部分，人类能否理解标题？这就是需要注释的一个例子。假设标题讨论了产品和产品版本，而这个标题在多页之前就已经出现。在这种情况下，如果某个数据块长度为单页，那么这个产品标题信息需要级联到每个正确的页面和数据块中。
- en: The annotation process needs to be of high quality. Product experts are the
    prime candidates to verify that the tags or annotations match the contents of
    the enterprise data. Thus, designers, writers, and PMs can get involved, using
    their product expertise to create an effective annotation process. This ensures
    steps are taken to quality-check the work (as the job might be outsourced or crowdsourced).
    Create metrics to define a quality bar and test against this (spot check or check
    it all). I wrote a metric to account for the kinds of errors and the frequency
    of mistakes our input would tolerate. The metric compared the quality of the crowdsourced
    material to the expectations of an expert. Results were analyzed to spot if specific
    human workers in the crowd were significantly better, worse, or the same as the
    average worker. So, consider the source, and *always test and verify* to validate
    your quality assumptions. Ask ChatGPT about all the errors that can occur when
    annotating data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 注释过程需要高质量。产品专家是验证标签或注释是否与企业数据内容匹配的主要候选人。因此，设计师、作家和项目经理可以参与其中，利用他们的产品专业知识来创建有效的注释流程。这确保了采取步骤来质量检查工作（因为工作可能外包或众包）。创建指标来定义质量标准，并对其进行测试（抽查或全部检查）。我编写了一个指标来考虑我们输入可以容忍的错误类型和错误频率。该指标将众包材料的质量与专家的期望进行了比较。结果被分析以确定人群中特定的人类工作者是否显著优于、劣于或与平均水平相同。因此，考虑来源，并且*始终测试和验证*以验证你的质量假设。向ChatGPT询问在注释数据时可能发生的所有错误。
- en: '[PRE7]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Another part of making data available for the LLM is segmenting it so the most
    valuable and optimal details are shared in the context window. This is called
    chunking.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 使数据对LLM可用的另一部分是将它分割成块，以便在上下文窗口中共享最有价值和最优化细节。这被称为分块。
- en: Chunking
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分块
- en: Not only do large documents need to be tagged, as discussed, but they are likely
    too big for the RAG process. This leads to discussions concerning **chunking**.
    Chunking refers to dividing a large text or dataset into smaller, manageable pieces
    (chunks) that fit within the LLM’s context window, allowing the model to process
    and understand the information more effectively. This isn’t about becoming a chunking
    expert; it is only about being able to recognize the results of poor chunking
    and help resolve issues.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，不仅大型文档需要标记，而且它们可能太大，不适合RAG过程。这导致了对**分块**的讨论。分块是指将大文本或数据集分成更小、更易于管理的块（块），这些块适合LLM的上下文窗口，从而使模型能够更有效地处理和理解信息。这并不是要成为分块专家；这只是为了能够识别不良分块的结果并帮助解决问题。
- en: Imagine customers want answers about mobile phone battery life. The phone company
    has released hundreds of phone models over the last few years, all with different
    specifications. These knowledge articles and details must be broken down into
    manageable, contextually relevant pieces to ensure RAG can process and retrieve
    them accurately. With this, the amount of information will be manageable for the
    system and result in good-quality answers. Segmenting the text into logical sections
    – chapters, paragraphs, and even sentences – ensures chunks have a coherent unit
    of meaning. This way, RAG can understand and retrieve the most pertinent information.
    We don’t want information about memory cards for an Android phone to be conflated
    with iPhones that do not have card slots because of a generic statement about
    memory cards.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 假设客户想要了解手机电池寿命的问题。手机公司在过去几年中发布了数百款手机型号，所有这些型号都有不同的规格。这些知识文章和细节必须被分解成可管理的、上下文相关的部分，以确保RAG可以准确处理和检索它们。这样，信息量对系统来说是可管理的，并会产生高质量的答案。将文本分割成逻辑部分——章节、段落，甚至句子——确保块具有连贯的意义单元。这样，RAG可以理解和检索最相关的信息。我们不希望有关Android手机内存卡的信息与没有卡槽的iPhone的信息混淆，因为这是一般关于内存卡的陈述。
- en: Different chunking strategies exist. We will cover some basics, with semantic
    chunking being the one of interest for our case studies later in this chapter.
    Come back to these references for more exploration.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 存在着不同的分块策略。我们将介绍一些基础知识，其中语义分块是我们在本章后面案例研究中感兴趣的一个。请回到这些参考资料以进行更多探索。
- en: 'Article: [Semantic Chunking for RAG](https://medium.com/the-ai-forum/semantic-chunking-for-rag-f4733025d5f5)
    ([https://medium.com/the-ai-forum/semantic-chunking-for-rag-f4733025d5f5](https://medium.com/the-ai-forum/semantic-chunking-for-rag-f4733025d5f5))'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 文章：[RAG的语义分块](https://medium.com/the-ai-forum/semantic-chunking-for-rag-f4733025d5f5)
    ([https://medium.com/the-ai-forum/semantic-chunking-for-rag-f4733025d5f5](https://medium.com/the-ai-forum/semantic-chunking-for-rag-f4733025d5f5))
- en: 'The second learning opportunity is a KDB.AI best practices video. With RAG,
    a vector database vendor will be needed. Fortunately, our learnings are primarily
    agnostic to the platforms. Here are a few takeaways from the video to give insight
    into chunking:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个学习机会是KDB.AI最佳实践视频。使用RAG时，需要一个向量数据库供应商。幸运的是，我们的经验主要是平台无关的。以下是视频中的几个要点，以提供对分块的一些见解：
- en: Chunk size depends on the model being used. Changing models might require changing
    chunk sizes. This also suggests that chunking should be done in an automation
    process to adapt quickly.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 块大小取决于所使用的模型。更改模型可能需要更改块大小。这也表明，分块应该在一个自动化过程中完成，以便快速适应。
- en: Small chunk sizes for a small amount of content will be accurate but won’t contain
    much context. Large chunks, typically from full documents, are less granular but
    can cost performance.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于少量内容，小块大小的内容将更准确，但不会包含很多上下文。大块，通常来自完整文档，粒度更少，但可能会影响性能。
- en: Prompts, chat history, and other resources might also be included in the context
    window, so allow for this capacity when deciding how many chunks can be allocated
    to the context windows.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示、聊天历史和其他资源也可能包含在上下文窗口中，因此在决定可以分配给上下文窗口的块数量时，请考虑这种容量。
- en: Because context windows are growing (ChatGPT-4’s window is 128K tokens as of
    Fall 2024), it doesn’t mean it should be filled. Performance, cost, and quality
    are relevant. To put it in context, the FAQ document shared earlier has 465K characters
    and 110K tokens. That document alone would be about as much as sharable with ChatGPT.
    That is an insignificant amount of data compared to what is needed at the Enterprise
    level.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于上下文窗口正在增长（截至2024年秋季，ChatGPT-4的窗口为128K个标记），这并不意味着它应该被填满。性能、成本和质量都是相关的。为了说明这一点，之前分享的FAQ文档有465K个字符和110K个标记。仅此文档就几乎与ChatGPT可以分享的内容一样多。与企业级所需的数据量相比，这是一个微不足道的数量。
- en: Chunk overlap can be adjusted when doing code-based chunking. This is the number
    of chunks to include from previous or future chunks, so there is context. However,
    NLP chunking solutions will be more graceful in breaking the content into more
    logical breaks (in a sentence). Examples are **Natural Language Toolkit** (**NLTK**)
    and spaCy, an open-source library.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在进行基于代码的块划分时，可以调整块重叠。这是从先前或未来的块中包含的块的数量，以便有上下文。然而，NLP块划分解决方案在将内容划分为更合理的断点（在句子中）方面将更加优雅。例如，**自然语言工具包**（**NLTK**）和spaCy，这是一个开源库。
- en: Chunk splitters are getting smarter every month. LangChain understands the structure
    of a document and does an excellent job of understanding sentences and paragraphs.
    It tries to optimize size based on document structure.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 块分割器每个月都在变得更智能。LangChain理解文档的结构，并在理解句子和段落方面做得非常出色。它试图根据文档结构来优化大小。
- en: Structural chunkers understand headers and sections. They can tag chunks with
    metadata so the context is maintained.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化块分割器理解标题和部分。它们可以用元数据标记块，以保持上下文。
- en: Different retrievers can be used for different databases. For example, one can
    be used for summaries to treat high-level questions and one for the source chunks
    to treat specific detailed questions.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用不同的检索器来处理不同的数据库。例如，一个可以用于处理高级问题，另一个可以用于处理源块以处理具体详细的问题。
- en: 'The meat of the discussion starts almost 10 minutes in. Start when Ryan Siegler
    starts talking. Video: [Chunking Best Practices for RAG Applications](https://www.youtube.com/watch?v=uhVMFZjUOJI)
    ([https://www.youtube.com/watch?v=uhVMFZjUOJI](https://www.youtube.com/watch?v=uhVMFZjUOJI))
    (KDB.AI)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论的要点几乎在10分钟后开始。当Ryan Siegler开始讲话时开始观看。视频：[RAG应用的最佳块划分实践](https://www.youtube.com/watch?v=uhVMFZjUOJI)
    ([https://www.youtube.com/watch?v=uhVMFZjUOJI](https://www.youtube.com/watch?v=uhVMFZjUOJI))（KDB.AI）
- en: Why should we care about chunk size? Chunk size impacts the accuracy, context,
    and performance of LLM solutions, which are essential factors product leaders
    will want to monitor and improve.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么应该关心块的大小？块的大小会影响LLM解决方案的准确性、上下文和性能，这些是产品领导者希望监控和改进的基本因素。
- en: Note
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You will likely not be the one setting up these chunks, but you will get involved
    in monitoring performance and quality to provide feedback to the data team. Team
    members who understand the content can help create and manage test cases to explore
    exceptions and validate the solution.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能不会是设置这些块的人，但你将参与监控性能和质量，以便向数据团队提供反馈。理解内容的小组成员可以帮助创建和管理测试用例，以探索例外情况并验证解决方案。
- en: For example, does the model understand an exception explained at the beginning
    of a document when discussing something referenced much later? For instance, in
    the Wove case study, later in the chapter, clearly defined notes appear at the
    start of a spreadsheet they want to ingest, but this information applies to material
    much later in the document; it is thus information relevant to that later chunk.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，模型是否理解在讨论到很久以后才引用的内容时，在文档开头解释的例外情况？例如，在Wove案例研究中，在章节的后面，他们想要摄取的电子表格的开始处出现了明确定义的注释，但这些信息适用于文档中更后面的材料；因此，这是与那个后期块相关的信息。
- en: Documents can also have images, charts, and tables. So, additional tools need
    to be used to summarize and get context from these graphics. Tools such as LayoutPDFReader
    and Unstructured are two examples that can help. The process would need to extract
    all of this independently of the text so that the chunks and summarization can
    be applied to the information extracted from the graphics. Depending on the tools,
    sometimes the embedding step can handle images directly. Almost all pictures and
    graphics in the documentation are more than ornamental, so converting these images
    to meaningful, searchable content is essential. Use LLMs to extract context from
    pictures and then use that knowledge to index and search images later. For example,
    a retailer setting up a marketing campaign might need a picture and ask, “Show
    me teens in jeans having fun on the beach.” This can be found without manually
    annotating images with these keywords. Even my iPhone (without an LLM) allows
    me to search for pictures of “cars,” “food,” “airplanes,” people, or locations
    like “Burlingame.” More intelligence and power are coming into this space with
    the inclusion of LLMs. Work on iterating on the data annotation to get content
    in good standing. Since the discussion of Wove’s use of spreadsheets, this data
    source is worth mentioning.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 文档还可以包含图像、图表和表格。因此，需要使用额外的工具来总结并从这些图形中获取上下文。LayoutPDFReader和Unstructured等工具是两个可以帮助的例子。这个过程需要独立于文本提取所有这些内容，以便可以将块和总结应用于从图形中提取的信息。根据工具的不同，有时嵌入步骤可以直接处理图像。几乎所有文档中的图片和图形都不仅仅是装饰性的，因此将这些图像转换为有意义的、可搜索的内容是至关重要的。使用LLM从图片中提取上下文，然后使用这些知识来索引和搜索图像。例如，一个零售商在建立营销活动时可能需要一个图片，并询问：“给我展示穿着牛仔裤在海滩上玩耍的青少年。”这可以通过手动用这些关键词标注图像来找到。即使是我的iPhone（没有LLM）也允许我搜索“汽车”、“食物”、“飞机”、“人”或“伯灵厄姆”等地点的图片。随着LLM的加入，这个空间正在进入更多的智能和力量。在数据标注上进行迭代工作，以使内容处于良好状态。自从讨论了Wove对电子表格的使用以来，这个数据源值得提及。
- en: Spreadsheet cleanup (Excel, Google Sheets)
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 电子表格清理（Excel, Google Sheets）
- en: 'Spreadsheets and databases share some common issues. Data sometimes needs to
    be transformed into different formats to be understood consistently from one service
    to another. There are tools to do these transformations. Be aware of these issues
    and can then apply the tools of the day to solve a problem. Spreadsheet cleanup
    makes a lot of sense in some backend integrations. Spreadsheets and tables can
    appear in many forms of documentation, and if they need to be understood by the
    LLM, they will likely need cleanup. Our second case study extensively uses spreadsheets,
    and we will explore the effort Wove made for their cleanup process. Hint: It involves
    a lot of manual work and evaluations. First, let’s define reality, or what people
    call the ground truth.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 电子表格和数据库存在一些共同问题。数据有时需要转换成不同的格式，以便在不同的服务中保持一致的理解。有一些工具可以完成这些转换。注意这些问题，然后可以应用当天的工具来解决这些问题。在后台集成中，电子表格清理非常有意义。电子表格和表格可以出现在许多形式的文档中，如果需要由LLM理解，它们可能需要进行清理。我们的第二个案例研究广泛使用了电子表格，我们将探讨Wove为清理过程所做的努力。提示：这涉及到大量的手动工作和评估。首先，让我们定义现实，或者人们所说的“事实”。
- en: Documentation and ground truth in sources
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 源文件中的文档和事实
- en: The **ground truth** is the facts needed as a basis for enterprise solutions.
    If documentation contains conflicting or misleading information, the LLM, like
    customers trying to read documentation, will make mistakes. This is a fundamental
    problem for FAQs, technical articles, and marketing communication. The context
    must be precise to clarify the information associated with which products. Tagging
    and annotation can help set this context. For example, if the instructions are
    to hold the power button down for 3 seconds to reset the device, but older models
    require a different answer, that context must be set clearly. Sometimes, articles
    call out the products or releases that a document impacts but also give exclusions
    later or use call-outs to give exceptions. These exclusions need to clearly define
    their scope for a search engine. Do these exceptions apply to the following few
    paragraphs or just the paragraphs where it was first introduced? Iterations of
    editing, tagging, and testing will solve this. Some tagging might be high-level,
    like articles related to finance or health care, while my examples above are specific
    to product releases or versions. Let’s start by compiling this in a simple text
    FAQ case study.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**事实真相**是企业解决方案所需的基础事实。如果文档包含相互矛盾或误导性信息，LLM（类似于试图阅读文档的客户）将会犯错误。这是常见问题解答、技术文章和营销沟通的基本问题。上下文必须精确，以阐明与哪些产品相关的信息。标签和注释可以帮助设定这个上下文。例如，如果说明是按住电源按钮3秒钟以重置设备，但旧型号需要不同的答案，那么这个上下文必须明确设定。有时，文章会指出受影响的文档中的产品或发布版本，但随后又给出排除或使用突出显示来给出例外。这些排除需要明确界定其搜索范围。这些例外是否适用于接下来的几段，还是仅限于首次引入的那段？编辑、标签和测试的迭代将解决这个问题。一些标签可能是高级别的，例如与金融或医疗保健相关的文章，而我上面的例子是针对产品发布或版本的。让我们从编写一个简单的文本常见问题解答案例研究开始。'
- en: FAQ case study
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 常见问题解答案例研究
- en: The Alli case study used File Search in OpenAI, but what about using the same
    data in a competitive LLM and RAG solution? Cohere is an AI company that provides
    enterprise LLM solutions. Why bother with another product in a book about ChatGPT?
    As models mature, there becomes increasing specialization. An enterprise solution
    might use one model for a specific task and a different model for a general task
    (like Wove does in our case study) . Performance, cost, and context size also
    come into play. With a focus on use cases, it is reasonable that different models
    might provide value. Cohere also provides a playground function for uploading
    documents and testing the model. It also exposed a few design elements in the
    chat UI that provide compelling UI elements worth sharing. In this example, the
    FAQs with no HTML – just the basic cleaned text was uploaded.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Alli案例研究使用了OpenAI的文件搜索功能，但使用相同数据在竞争性的LLM和RAG解决方案中会怎样呢？Cohere是一家提供企业级LLM解决方案的AI公司。为什么在关于ChatGPT的书里还要考虑另一个产品呢？随着模型的发展，专业化程度越来越高。企业级解决方案可能会使用一个模型来完成特定任务，而使用不同的模型来完成一般任务（就像我们在案例研究中做的Wove那样）。性能、成本和上下文大小也会发挥作用。专注于用例，不同的模型可能提供价值是合理的。Cohere还提供了一个上传文档并测试模型的游乐场功能。它还暴露了聊天UI中的一些设计元素，这些元素提供了值得分享的引人注目的UI元素。在这个例子中，上传的常见问题解答没有HTML，只是基本的清洁文本。
- en: '[PRE8]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Go to the Coral web page ([https://coral.cohere.com/](https://coral.cohere.com/))
    and select the **Coral with documents** option (see *Figure 6**.4*).
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问Coral网页([https://coral.cohere.com/](https://coral.cohere.com/))并选择**带有文档的Coral**选项（见*图6**.4*）。
- en: 'Note:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：
- en: The current cohere demo uses a very different design for handling documents,
    so these instructions won't work. The latest version allows you to copy and paste
    the information to provide context, or the files must be uploaded using the Dataset
    tools. We don’t ask readers to do that. We will continue with this example because
    of some excellent features in the results, but you can follow along by opening
    the FAQ and copying and pasting.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当前cohere演示在处理文档方面采用了非常不同的设计，因此这些说明将不起作用。最新版本允许您复制粘贴信息以提供上下文，或者必须使用数据集工具上传文件。我们不要求读者这样做。我们将继续使用这个例子，因为结果中有些出色的功能，但你可以通过打开常见问题解答并复制粘贴来跟随。
- en: The most recent releases of Cohere’s Playground are more complex, technical,
    and cluttered than OpenAI’s. When creating solutions, consider the impact of UI
    elements on feature capability and usability.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Cohere的游乐场最新版本比OpenAI的更复杂、技术性更强、更杂乱。在创建解决方案时，请考虑UI元素对功能能力和可用性的影响。
- en: '![](img/B21964_06_04.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21964_06_04.jpg)'
- en: Figure 6.4 – Setting up Cohere’s Coral with documents
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – 设置Cohere的Coral与文档
- en: Upload the FAQ file shared on GitHub using the **Files** feature.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 **文件** 功能上传 GitHub 上共享的 FAQ 文件。
- en: 'GitHub: [FAQ Sample Document](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Example_FAQs_NoHTML_for_Demo.docx)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Example_FAQs_NoHTML_for_Demo.docx](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Example_FAQs_NoHTML_for_Demo.docx))'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'GitHub: [FAQ 示例文档](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Example_FAQs_NoHTML_for_Demo.docx)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Example_FAQs_NoHTML_for_Demo.docx](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Example_FAQs_NoHTML_for_Demo.docx))'
- en: Close the side panel and use the message window to interact (*Figure 6**.5*).
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭侧面板，使用消息窗口进行交互（*图 6.5*）。
- en: '![](img/B21964_06_05.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21964_06_05.jpg)'
- en: Figure 6.5 – Example showing reference usage
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – 显示引用使用的示例
- en: Test the model with questions related to the FAQ (*Figure 6**.6*).
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用与 FAQ 相关的问题测试模型（*图 6.6*）。
- en: '![](img/B21964_06_06.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21964_06_06.jpg)'
- en: Figure 6.6 – Examples of the FAQ document in Cohere
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 – Cohere 中 FAQ 文档的示例
- en: 'Besides the reasons stated, there are some exciting results from this competitive
    model:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 除了所述的原因外，这个竞争模型还有一些令人兴奋的结果：
- en: What is in this book is generalizable to other models.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这本书中的内容可以推广到其他模型。
- en: Some UX elements, like showing the reference panel, could be valuable to a use
    case. There is only one document in this demo, so viewing the one link doesn’t
    help because it repeats itself with every match. Linking to the reference and
    then scrolling and highlighting the relevant passages makes it easy to understand
    and see the context. The relevance-highlighting UX pattern should become popular
    or even a standard.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些 UX 元素，如显示参考面板，可能对用例很有价值。在这个演示中只有一个文档，所以查看一个链接没有帮助，因为它会随着每个匹配重复。链接到参考内容然后滚动并突出显示相关段落，使其易于理解和查看上下文。相关性突出显示的
    UX 模式可能会变得流行，甚至成为标准。
- en: It is an excellent example of a side-by-side pattern showing supplemental information.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个并列模式展示补充信息的优秀示例。
- en: It gives us a feel for the quality of different models and allows us to see
    differences between each version of ChatGPT.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它让我们对不同模型的质量有了感觉，并允许我们看到 ChatGPT 每个版本之间的差异。
- en: Let’s test our FAQs. It is helpful to give some context with this Cohere example
    so that we can explore using FAQs in ChatGPT. Let’s see if the results meet our
    expectations.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试我们的 FAQ。使用 Cohere 示例提供一些上下文是有帮助的，这样我们就可以探索在 ChatGPT 中使用 FAQ。让我们看看结果是否符合我们的预期。
- en: 'GitHub: [Zip of FAQS as unique PDFs](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQ_PDFs.zip)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQ_PDFs.zip](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQ_PDFs.zip))'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 'GitHub: [FAQS 唯一 PDF 压缩包](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQ_PDFs.zip)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQ_PDFs.zip](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQ_PDFs.zip))'
- en: In this case, the zip file contains the cleaned data in individual PDF documents.
    This allows us better to connect the source as a reference and the results. Return
    to the ChatGPT Playground and create the same assistants as before, but try to
    upload this file.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，压缩文件包含在单个 PDF 文档中的清洗数据。这使我们能够更好地将源作为参考并与结果相连接。返回到 ChatGPT 游乐场并创建与之前相同的助手，但尝试上传此文件。
- en: However, recall there are limitations; uploading the file in ChatGPT 3.5 will
    result in a cryptic user error (meaning too many files were uploaded), as shown
    in *Figure 6**.7*.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请记住，存在局限性；在 ChatGPT 3.5 中上传文件会导致一个神秘的用户错误（意味着上传了太多文件），如 *图 6.7* 所示。
- en: '![](img/B21964_06_07.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21964_06_07.jpg)'
- en: Figure 6.7 – ChatGPT has a file limit
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 – ChatGPT 有文件限制
- en: It is a small dataset, just not small enough. There is a workaround to allow
    its use in the free playground. The PDFs are joined into 18 files, and a single
    PDF can be used for other testing and experimentation.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个小型数据集，只是还不够小。有一个解决方案允许它在免费游乐场中使用。PDF 被合并成 18 个文件，单个 PDF 可以用于其他测试和实验。
- en: 'GitHub: [Zip of 18 FAQ Files](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQS18files.zip)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQS18files.zip](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQS18files.zip))
    (each with 25 or so FAQs)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub：[18个FAQ文件的压缩包](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQS18files.zip)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQS18files.zip](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQS18files.zip))（每个包含大约25个FAQ）
- en: 'GitHub: [Single PDF with all 441 FAQs](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQ-ALL.pdf)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQ-ALL.pdf](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQ-ALL.pdf))'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub：[包含所有441个FAQ的单一PDF](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQ-ALL.pdf)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQ-ALL.pdf](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQ-ALL.pdf))
- en: With these 18 files, it only takes a few seconds to upload and scan them, and
    the Playground will be ready to go.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这18个文件，上传和扫描它们只需要几秒钟，Playground就会准备好。
- en: Once uploaded, try out some test cases like *Tables 6.2 and 6.3*. They were
    written without knowing whether they would work (they were not pre-tested). Test
    cases are covered more extensively in the next few chapters; let’s keep it simple
    and do testing manually. Test whether single or multiple files impact quality
    and see what can be learned from the results.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦上传，尝试一些测试案例，如*表6.2和6.3*。它们是在不知道它们是否有效的情况下编写的（它们没有经过预测试）。在接下来的几章中，我们将更详细地介绍测试案例；让我们保持简单，手动进行测试。测试单个或多个文件对质量的影响，并从结果中学习。
- en: Note
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Try different models available. It doesn’t have to be ChatGPT 3.5; try ChatGPT
    4o-mini or compare it to other vendors’ LLMs.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试不同的可用模型。不一定要是ChatGPT 3.5；尝试ChatGPT 4o-mini或与其他供应商的LLMs进行比较。
- en: In both cases, use the cleaned data column from the spreadsheet. There are some
    spelling errors and chaining questions (questions that demand a follow-up question)
    in the test cases. Now, we can learn about the actual results together.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，使用电子表格中的清洗数据列。测试案例中存在一些拼写错误和连锁问题（需要后续问题的提问）。现在，我们可以一起了解实际结果。
- en: '![](img/B21964_Table_6.2.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21964_Table_6.2.jpg)'
- en: Table 6.2 – Questions 1-10 and the results from two test sessions
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.2 – 第1-10题及两次测试结果
- en: '![](img/B21964_Table_6.3.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21964_Table_6.3.jpg)'
- en: Table 6.3 – Questions 11-20 and the results from two test sessions
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.3 – 第11-20题及两次测试结果
- en: 'Here are some high-level analyses of these results:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些对这些结果的初步分析：
- en: Spelling errors did not cause issues.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拼写错误没有造成问题。
- en: Follow-ups that provide a little extra context returned good results.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供额外背景信息的后续操作返回了良好的结果。
- en: The same model returned very different results for some questions.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于一些问题，相同的模型返回了非常不同的结果。
- en: Specific information like addresses was very challenging.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像地址这样的具体信息非常具有挑战性。
- en: It didn’t think it was the bank; it referred to “your financial institution
    or brokerage firm.” Prompt engineering can fix this problem.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它没有认为那是银行；它指的是“你的金融机构或经纪公司。”提示工程可以解决这个问题。
- en: Both needed help with ending sentences with a period. They tended to put a space
    before the period like this .
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们都需要帮助以句号结束句子。他们倾向于在句号前加空格，如下所示。
- en: Let’s create a simple score for these two methods. Five points for a great correct
    answer, 4 points for a good correct answer, 3 points for a close to correct answer,
    and 2 points if a follow-up returned details that should have been in the first
    answer. Scoring shows 47 points for separate files and 74 for a single file model.
    Recognizing a significant difference between these two starting points doesn’t
    have to be perfect. If you watched the OpenAI video in the last chapter (one of
    my favorite video references of this entire book), they had some similar experiences,
    beginning with a poor result, and with fine-tuning and prompt engineering, they
    improved their result, as shown in *Figure 6**.8*.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为这两种方法创建一个简单的评分标准。对于出色的正确答案得5分，对于良好的正确答案得4分，对于接近正确答案得3分，如果后续返回的细节应该包含在第一个答案中，则得2分。评分显示，分别使用文件得47分，而使用单个文件模型得74分。认识到这两个起点之间的显著差异不必完美。如果你观看了上一章中的OpenAI视频（这本书我最喜欢的视频参考资料之一），他们有一些相似的经历，从较差的结果开始，通过微调和提示工程，他们改进了结果，如图*6.8*所示。
- en: 'Video: [A Survey of Techniques for Maximizing LLM Performance](https://www.youtube.com/watch?v=ahnGLM-RC1Y)
    ([https://www.youtube.com/watch?v=ahnGLM-RC1Y](https://www.youtube.com/watch?v=ahnGLM-RC1Y))'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 视频：[最大化LLM性能的技术综述](https://www.youtube.com/watch?v=ahnGLM-RC1Y) ([https://www.youtube.com/watch?v=ahnGLM-RC1Y](https://www.youtube.com/watch?v=ahnGLM-RC1Y))
- en: '![](img/B21964_06_08.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21964_06_08.jpg)'
- en: Figure 6.8 – A RAG success story for OpenAI’s approach in a use case
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – OpenAI在用例中采用RAG的成功故事
- en: It isn’t necessary to understand all of these methods in detail. Toward the
    end of this chapter, there is a section for other techniques to discuss this.
    The point for the moment is to showcase how continuous improvement to your lifecycle
    will help determine what changes improve the experience. Even with this rudimentary
    scoring, there are dramatically different results. I, too, was surprised by the
    dramatic difference. The full transcript of both results is posted.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 并没有必要详细了解所有这些方法。在本章的结尾部分，有一个专门的部分来讨论其他技术。目前的关键是要展示如何通过持续改进你的生命周期来确定哪些变化能改善体验。即使使用这种基本的评分方法，结果差异也很大。我也对这种显著差异感到惊讶。两个结果的完整记录已经发布。
- en: 'GitHub: [Transcripts of FAQ Test](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Transcripts.docx)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Transcripts.docx](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Transcripts.docx))'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 'GitHub: [FAQ测试记录](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Transcripts.docx)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Transcripts.docx](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Transcripts.docx))'
- en: The data brought in through RAG needs to be cleaned, as seen in the forthcoming
    Wove case study. Something simple, like how files are split up, can profoundly
    impact performance. Each improvement can affect the next step. It is better to
    continue to refine, starting from a score of 74 than from 47\. Find tools to handle
    the mundane work so efforts can be focused on actual data and its quality. There
    are other issues to consider when creating a complete lifecycle for a data pipeline.
    Next is a case study from an exciting company that uses a variety of models to
    make its LLM solution successful.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 通过RAG引入的数据需要清理，正如即将到来的Wove案例研究所示。像文件分割这样简单的事情可能会对性能产生深远影响。每一次改进都可能影响下一步。从74分开始继续细化，而不是从47分开始，会更好。找到处理日常工作的工具，以便将精力集中在实际数据和其质量上。在创建数据管道的完整生命周期时，还有其他问题需要考虑。接下来是一个来自一家使用多种模型使其LLM解决方案成功的令人兴奋公司的案例研究。
- en: Spreadsheet cleanup case study
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 电子表格清理案例研究
- en: Here is an excellent example of spreadsheets used behind the scenes to create
    intelligence in the LLM and offer recommendations from [Wove.com](https://wove.com)
    ([https://wove.com](https://wove.com)). Wove helps freight forwarding companies
    optimize rate management operations by using LLMs to parse and normalize complex
    tabular data from rate sheets, ocean contracts, and other spreadsheets.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的例子，展示了如何使用电子表格在幕后创建LLM的智能并提供来自[Wove.com](https://wove.com) ([https://wove.com](https://wove.com))的建议。Wove通过使用LLM解析和标准化来自费率表、海洋合同和其他电子表格的复杂表格数据，帮助货运代理公司优化费率管理操作。
- en: Freight forwarders act as intermediaries who ensure that small shippers can
    get goods from one location to another—for example, shipping 10,000 widgets from
    a factory in China to a warehouse in Nebraska. Because there are hundreds of ways
    to get from point A to point B, there are complexities based on the vendor, distance,
    ports, transport type, time, type of goods, customs, weight, and volume. This
    complexity is buried in published data from each vendor in spreadsheets, PDFs,
    and other data sources. This complexity increases the time to quote and can lead
    to missing reasonable rates. By taking these rate sheets and putting them into
    the model, customer quotes can be generated more accurately and efficiently. This
    a daunting task. To geek out on the rate sheet use case, look at all the standard
    terms one might see in a sheet.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 货运代理作为中介，确保小型托运人可以从一个地点将货物运送到另一个地点——例如，从中国的工厂将10,000个小部件运送到内布拉斯加州的仓库。由于从A点到B点有数百种方式，因此基于供应商、距离、港口、运输类型、时间、货物类型、海关、重量和体积，存在复杂性。这种复杂性隐藏在每个供应商发布的电子表格、PDF和其他数据源中。这种复杂性增加了报价所需的时间，可能导致遗漏合理的费率。通过将这些费率表放入模型中，可以更准确、更高效地生成客户报价。这是一项艰巨的任务。要深入了解费率表用例，请查看在表格中可能会看到的所有标准术语。
- en: 'Article: [Rate Sheet Terms and Introduction](https://www.slideshare.net/logicalmsgh/understanding-the-freight-rate-sheet)
    ([https://www.slideshare.net/logicalmsgh/understanding-the-freight-rate-sheet](https://www.slideshare.net/logicalmsgh/understanding-the-freight-rate-sheet))'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 文章：[费率表术语和简介](https://www.slideshare.net/logicalmsgh/understanding-the-freight-rate-sheet)
    ([https://www.slideshare.net/logicalmsgh/understanding-the-freight-rate-sheet](https://www.slideshare.net/logicalmsgh/understanding-the-freight-rate-sheet))
- en: Terms like BAR, BL Fee, Demurrage, DDC, CYRC, Detention, and dozens of others
    are a lot to digest. It makes it challenging for an LLM to understand a complex
    spreadsheet. This is an excellent example from our friends at Wove, who have created
    a behind-the-scenes use of ChatGPT and other models, like Anthropic’s Claude.
    They focus on ingesting data to preserve data quality and integrity and normalize
    widely different spreadsheets. Indeed, there are opportunities on the UI side
    to use this data to answer questions about finding the correct rate for a job.
    This part of the case study will focus on data ingestion. The Wove case study
    will be completed after more is explained in the prompt engineering and fine-tuning
    chapters.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如BAR、BL费、滞期费、DDC、CYRC、滞留费等术语有很多需要消化。这对一个大型语言模型理解复杂的电子表格来说是一个挑战。这是我们从Wove的朋友那里得到的一个很好的例子，他们创建了一个幕后使用ChatGPT和其他模型，如Anthropic的Claude。他们专注于摄取数据以保持数据质量和完整性，并标准化广泛不同的电子表格。确实，在用户界面方面，可以使用这些数据来回答关于找到正确费率的问题。本案例研究将重点关注数据摄取。Wove案例研究将在在提示工程和微调章节中解释更多内容后完成。
- en: The terms require understanding, and each rate sheet varies in format, labels,
    exceptions, and other factors. As rates change over time, the correct rate periods
    must be understood. *Figure 6**.9* shows a fraction of a rate sheet to expose
    this complexity.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这些术语需要理解，并且每个费率表在格式、标签、例外和其他因素上都有所不同。随着时间的推移，费率发生变化，必须理解正确的费率周期。*图6.9* 展示了费率表的一部分，以揭示这种复杂性。
- en: '![](img/B21964_010_06.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21964_010_06.jpg)'
- en: Figure 6.9 – Samples of rate sheets from two different vendors
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – 来自两个不同供应商的费率表样本
- en: 'A typical forwarder might have to deal with dozens of different rate sheets,
    and with some of them being *hundreds* of pages long, normalizing all of this
    data manually requires the effort of a whole team. The examples show how varied
    the data columns can be. The labels, the values, the use of tabs, how exceptions
    are handled with remarks, and the headers are all different. However, automation,
    or even semi-automation, can reduce this process by more than 90%. Although one
    should test and verify data along the way, there are numerous places in the manual
    lifecycle where human error causes issues. Let’s review the data cleansing steps
    Wove had to do to ingest this data. The expected flow of this information is as
    follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的货运代理可能需要处理数十个不同的费率表，其中一些长达*数百*页，手动规范化所有这些数据需要整个团队的努力。示例显示了数据列的多样性。标签、值、制表符的使用、如何用备注处理例外情况，以及标题都是不同的。然而，自动化或甚至半自动化可以减少这个过程超过90%。虽然应该在过程中测试和验证数据，但在手动生命周期中有许多地方人为错误会导致问题。让我们回顾Wove在摄取这些数据时必须执行的数据清洗步骤。此信息的预期流程如下：
- en: Before getting new rate sheets, they trained and verified the various models
    needed to create high-quality output. This case study will discuss the different
    models used.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在获取新的费率表之前，他们训练和验证了创建高质量输出所需的各种模型。本案例研究将讨论所使用的不同模型。
- en: Typically, they receive a rate sheet in an email and download the file into
    Wove. There is also an automation path with an email listener that picks up the
    file, monitors for new files, and ingests it into the process. These files can
    have multiple tabs and thousands of rows of data, like the small sample shown
    in *Figure 6**.9*. A typical file is likely an update of a previously processed
    file.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通常，他们会通过电子邮件收到一个费率表，并将其文件下载到Wove中。还有一个自动化路径，其中包含一个电子邮件监听器，它会拾取文件，监视新文件，并将其纳入流程。这些文件可能有多个标签和数千行数据，如*图6*所示。9*。一个典型的文件可能是之前处理过的文件的更新。
- en: Their tools parse the XLS file and identify the tables, and it parses the document
    and turns them into property formatted clumps for the model. There are context
    length limits, detecting tables, understanding the tables, and figuring out how
    the tables relate to each other. They refer to this as table detection. As shown
    next, the development team built ten models to understand the spreadsheet. The
    entire proprietary process isn’t shared, but this should give a sense of what
    each model does and what software was used to help the cleaning and organizing
    process. Although this is a technical process, the results are something mere
    mortals can see. They can determine whether they provide the best results for
    the cost involved. This is a business decision and a user experience problem.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 他们的工具解析XLS文件并识别表格，并解析文档，将它们转换为模型可用的属性格式簇。存在上下文长度限制、检测表格、理解表格以及确定表格之间关系的问题。他们将这称为表格检测。如图所示，开发团队构建了十个模型来理解电子表格。整个专有流程并未公开，但这一点应该能让人了解每个模型的作用以及用于帮助清理和组织过程的软件。尽管这是一个技术过程，但结果却是普通人都能看到的东西。他们可以确定这些结果是否物有所值。这是一个商业决策和用户体验问题。
- en: '**Document Segmentation (Single-Shot GPT 4 Turbo)**: This segments documents
    into coherent sections/ideas.'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档分段（单镜头GPT 4 Turbo）**：此功能将文档分段为连贯的章节/思想。'
- en: '**Context Builder (Multi-Shot Claude 3 Haiku)**: This is applied after document
    segmentation. It builds the reading context for understanding the current document.'
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文构建器（多镜头克劳德3俳句）**：这是在文档分段之后应用的。它为理解当前文档构建阅读上下文。'
- en: '**Table Detection (GPT 3.5 Turbo, Fine-tuned)**: This detects tables in spreadsheets,
    documents, or contracts.'
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表格检测（GPT 3.5 Turbo，微调）**：此功能检测电子表格、文档或合同中的表格。'
- en: '**Table Header Range Detection (GPT 3.5 Turbo, Fine-tuned)**: After the table
    is detected, the range of header rows and where the data for the table starts
    are determined.'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表格标题范围检测（GPT 3.5 Turbo，微调）**：在检测到表格后，确定标题行范围和数据开始的位置。'
- en: '**Table End Detection (GPT 3.5 Turbo, Fine-tuned)**: This detects the end of
    the table data.'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表格结束检测（GPT 3.5 Turbo，微调）**：此功能检测表格数据的结束。'
- en: '**Table Understanding (GPT 3.5 Turbo, Fine-tuned)**: This model understands
    a table’s columns and data and determines its purpose.'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表格理解（GPT 3.5 Turbo，微调）**：此模型理解表格的列和数据，并确定其用途。'
- en: '**Schema Mapping (GPT 3.5 Turbo, Fine-tuned)**: This model is applied after
    the table is understood. It determines which columns from a table map to schema
    fields in a database.'
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式映射（GPT 3.5 Turbo，微调）**：在理解表格后应用此模型。它确定表格中的哪些列映射到数据库中的模式字段。'
- en: '**Field Splitter (Single-Shot Claude 3 Haiku)**: The splitter extracts per-field
    information from combined fields. For example, if effective and expiry dates are
    in the same field, this can extract them into **effective_date** and **expiry_date**
    in the schema.'
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字段分割器（单镜头克劳德3俳句）**：分割器从组合字段中提取每个字段的详细信息。例如，如果有效日期和到期日期在同一字段中，则可以将它们提取到模式中的**effective_date**和**expiry_date**。'
- en: '**Location Normalizer (Multi-Shot GPT 3.5 Turbo)**: This takes unstructured
    location information and normalizes each detected location to a UN/LOCODE (normalized
    country codes such as HK for Hong Kong).'
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**位置标准化器（多镜头GPT 3.5 Turbo）**：它将非结构化位置信息标准化，将每个检测到的位置转换为UN/LOCODE（如香港的HK这样的标准化国家代码）。'
- en: '**Commodity Normalizer (GPT 3.5 Turbo + Ada)**: This takes unstructured commodity
    information and normalizes each commodity type to be searched/compared.'
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**商品标准化器（GPT 3.5 Turbo + Ada）**：它将非结构化商品信息标准化，以便进行搜索/比较。'
- en: These models changed multiple times during the creation of this case study,
    and they continue to change as they are currently testing GPT 4o-mini for some
    use cases. Adapt and improve, and sometimes save some money.
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些模型在创建本案例研究的过程中多次更改，并且随着他们目前正在测试GPT 4o-mini的一些用例，它们仍在不断变化。适应和改进，有时还能节省一些费用。
- en: They identify, tag, and train the system to understand where the table is, where
    data starts, where it ends, the header labels, and so on. The challenge is understanding
    tables when LLMs are primarily for text. The spreadsheets become text. Notice
    some of the models used in this process are fine-tuned. Those are the ones that
    need additional understanding and learning by providing examples of what defines
    a table.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 他们识别、标记并训练系统理解表格的位置、数据开始和结束的位置、标题标签等。挑战在于当LLMs主要用于文本时理解表格。电子表格变成了文本。注意，在此过程中使用的某些模型已经进行了微调。这些是需要通过提供定义表格的示例来增加理解和学习的模型。
- en: Diving more into table detection helps to understand the segmenting of data.
    After table detection from *Step 3*, they do semantic chunking to get the right
    context length. Typically, a suitable context length might start at 500 to 1,000
    tokens. Depending on the model, longer context lengths are acceptable if you want
    to pay for them. Wove prompts GPT-4 to chunk the files into *coherent segments*.
    Chunks are essential, as only so much information can be processed at one time.
    Effective chunking strategies are necessary to have the proper context for a chunk.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 深入研究表格检测有助于理解数据的分割。在*步骤3*的表格检测之后，他们进行语义块分割以获取正确的内容长度。通常，合适的内容长度可能从500到1000个标记开始。根据模型的不同，如果你想付费，更长的内容长度是可以接受的。Wove提示GPT-4将文件分割成*连贯的片段*。块是至关重要的，因为一次只能处理这么多信息。有效的块分割策略对于获得正确的块上下文是必要的。
- en: Their prompt is pretty big—it is a page long. It tells ChatGPT 20 different
    rules to parse a segment. Their prompt starts simple… “You’re an expert in doc
    parsing; you’ll be given a chunk of text. Your job is to split it into coherent
    segments.” They don’t have massive chunks, so chunk size is not limited by the
    LLMs. Each model can have a different token limit to allow for the size of the
    prompt and the resulting output. The models range from 4K to 8K tokens for input
    and output. They use a smaller, faster, and less expensive model in the next step.
    If you are unsure of your model’s limitations, ask it.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的提示相当大——长达一页。它告诉ChatGPT 20条不同的规则来解析一个片段。他们的提示从简单开始…“你是文档解析的专家；你会得到一段文本。你的任务是将其分割成连贯的片段。”他们没有大量文本块，因此块大小不受LLMs的限制。每个模型可以有不同的标记限制，以允许提示和输出的尺寸。模型的输入和输出范围从4K到8K个标记。他们在下一步使用了一个更小、更快、更便宜的模型。如果你不确定你模型的限制，就问它。
- en: '[PRE9]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Wove covers the entire lifecycle. **Functional calling**, the method to access
    other resources such as APIs, is essential to Wove’s process and fundamental to
    enterprise applications. Be aware of this capability. Remember, any enterprise
    solution will connect to various resources to enrich the LLM.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Wove覆盖了整个生命周期。**功能调用**，访问其他资源（如API）的方法，对于Wove的过程至关重要，对于企业应用也是基础。注意这个功能。记住，任何企业解决方案都将连接到各种资源以丰富LLM。
- en: 'Documentation: [ChatGPT developer documentation on function calling](https://platform.openai.com/docs/guides/function-calling)
    ([https://platform.openai.com/docs/guides/function-calling](https://platform.openai.com/docs/guides/function-calling))'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 文档：[ChatGPT函数调用开发者文档](https://platform.openai.com/docs/guides/function-calling)
    ([https://platform.openai.com/docs/guides/function-calling](https://platform.openai.com/docs/guides/function-calling))
- en: They use function calling to generate the sections into a structured output.
    A piece of this function is shown in *Figure 6**.10*. The product team needs to
    understand this to ensure the context is complete. Some of this might be generic
    to any spreadsheet, such as a start line, end line, the section’s name, headers,
    and a description, but getting this understanding right is essential. Later, they
    checked that the tables were processed correctly to confirm the correct start
    line, header, or sub-header labels.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 他们使用函数调用将部分生成到结构化的输出中。*图6.10*展示了这个函数的一部分。产品团队需要理解这一点以确保上下文完整。其中一些内容可能适用于任何电子表格，例如起始行、结束行、部分名称、标题和描述，但正确理解这一点是至关重要的。后来，他们检查了表格是否被正确处理，以确认正确的起始行、标题或副标题标签。
- en: '[PRE10]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Figure 6.10 – A snippet of the function calling that is used to help structure
    the output
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 – 用于帮助结构化输出的函数调用片段
- en: They can use the training validation split data, test the models against the
    removed data, and use their data cleaning technique on the data shown in *Figure
    6**.11* by defining the tables. This data tagging defines *what is what* in the
    table and can improve with more refinements over time. Scripts help generate new
    training data from this tagged source.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 他们可以使用训练验证分割数据，测试模型对移除的数据，并使用他们在*图6.11*中显示的数据清理技术来定义表格。这种数据标记定义了表格中的“什么是什么”，并且随着时间的推移可以不断改进。脚本有助于从标记源生成新的训练数据。
- en: '![](img/B21964_06_11.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21964_06_11.jpg)'
- en: Figure 6.11 – A small table of side terms from the rate sheets
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 – 来自费率表的侧术语的小表格
- en: Look at this definition of `Side Terms`, which is used to train table detection;
    it tells the LLM how to understand this data.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 看看这个“侧术语”的定义，它用于训练表格检测；它告诉LLM如何理解这些数据。
- en: '[PRE11]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Product managers, designers, and the team must monitor table definitions to
    ensure high quality. In this example, they identify the “`Side Terms`” start date
    from row 3 (line 2) to row 21 (line 3). In line 5, they identify the spreadsheet
    columns as from `B` to `G` (column `A` is white space), followed by row 6 being
    defined as the header and defining the source data for the table with `(7,20)`
    for rows 7 to 20\. However, in *Figure 6**.11*, notice the `Remark` column (column
    `F`) extends to line 21, so the process involves human validation *to catch this
    error* and change `(7,20)` to `(7,21)`.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 产品经理、设计师和团队必须监控表格定义以确保高质量。在这个例子中，他们从第3行（第2行）到第21行（第3行）识别了“`Side Terms`”的起始日期。在第5行，他们识别电子表格的列从`B`到`G`（列`A`是空白），然后第6行被定义为标题，并使用`(7,20)`为第7行到第20行定义表格的源数据。然而，在*图6.11*中，注意`备注`列（列`F`）延伸到第21行，因此这个过程涉及人工验证*以捕捉这个错误*并将`(7,20)`更改为`(7,21)`。
- en: Multiple models use this one tagging exercise. This effort supports table-end
    detection, headers, and table understanding.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 多个模型使用这个标记练习。这项工作支持表格端检测、标题和表格理解。
- en: It is vital to catch what needs to be tagged. For example, some notes with stars
    are shown at the top of the table in *Figure 6**.9*. LLMs are good at understanding
    text and the reference to this block of text extracted from the table detection,
    so no additional effort was needed to gather this information.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 捕捉需要标记的内容至关重要。例如，一些带有星号的笔记显示在表格顶部的*图6.9*中。LLMs擅长理解文本以及从表格检测中提取的这段文本的引用，因此不需要额外的努力来收集这些信息。
- en: The data must then be normalized for items such as rates and locations. So,
    for Hong Kong, the port HKHKG is displayed consistently, and dozens of other values
    are mapped correctly across different files.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 数据必须对诸如费率和位置等项目进行归一化。因此，对于香港，港口HKHKG是一致的显示，并且数十个其他值在不同文件中正确映射。
- en: There is a data review process, and Wove has tools for doing so. The team reviews
    this clean data, as shown in *Figure 6**.12*. This drill-down shows rates between
    Hong Kong and Atlanta and some data that goes into these rates.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一个数据审查流程，Wove有工具来完成这项工作。团队审查了如图6.12所示的数据，这个深入分析显示了香港和亚特兰大之间的费率以及一些进入这些费率的数据。
- en: '![](img/B21964_06_12-shphigh.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21964_06_12-shphigh.jpg)'
- en: Figure 6.12 – The view of data so now they can view rates in a normalized view
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 – 现在可以以归一化视图查看数据，以便他们可以查看费率
- en: Now that they have ingested and normalized the data, they can access rates from
    many sources. Let’s explore some of the details of this workflow a bit further.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在他们已经摄取并归一化了数据，他们可以访问来自许多来源的费率。让我们进一步探讨这个工作流程的一些细节。
- en: This is not about a single model performing magic; it takes a collection of
    specialized models. They applied different models to solve various problems. It
    is expected to adapt and change over time, especially with models that use fine-tuning.
    Think of it as a modular approach. If a new or much less expensive model comes
    out, swap it to improve one piece of the puzzle at a time. If there are issues
    around one topic, such as poor or missing data, and the model will need help converging
    to a practical solution, focus on that problem. Each piece can experience its
    version of hallucinations.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是关于单个模型施展魔法；它需要一系列专业模型的集合。他们应用不同的模型来解决各种问题。预计随着时间的推移会进行适应和改变，尤其是对于使用微调的模型。把它看作是一种模块化方法。如果出现了一个新的或成本远低于之前的模型，可以逐个替换以改善拼图的一个部分。如果某个主题周围存在问题，例如数据质量差或缺失，并且模型需要帮助收敛到实际解决方案，那么就关注这个问题。每个部分都可能经历其版本的幻觉。
- en: Data cleansing has a specific meaning for these spreadsheets, especially ensuring
    that rows and exceptions are handled. Chunks must be segmented correctly to have
    a good beginning and end so that context is maintained. This gives RAG a clean
    context and retrieves relevant chunks more accurately.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些电子表格来说，数据清洗具有特定的含义，特别是确保行和异常得到处理。数据块必须正确分割，以便有一个良好的开始和结束，从而保持上下文。这为RAG提供了一个干净的环境，并更准确地检索相关片段。
- en: 'Here are the top issues addressed in their cleanup and ingestion process:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的清理和摄取过程中，以下是他们解决的前十大问题：
- en: Process data as text, even coming from spreadsheets.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据作为文本处理，即使数据来自电子表格。
- en: The model segments large documents—some rate sheets can be hundreds of pages
    long—and breaks them up. For example, ocean shipping documents are more complex
    than road trucking documents.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型将大型文档分割成多个部分——一些费率表可能长达数百页——并将它们拆分。例如，海洋运输文件比公路卡车运输文件更复杂。
- en: The challenge is to understand tables as text. It takes considerable work to
    understand tables well, tag them correctly, look for errors, and find a suitable
    model (which they did and didn’t discuss to protect their expertise). This differs
    from reading straight text, but this might impact the experience even if the team
    controls the knowledge base or databases. Documents with tables, images, flow
    charts, and diagrams all contain information that might need to be fully expressed
    in text.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 挑战在于将表格作为文本来理解。要很好地理解表格、正确标记、查找错误并找到合适的模型（他们为了保护他们的专业知识而讨论了但没有详细说明）需要相当多的工作。这与直接阅读纯文本不同，但这可能会影响体验，即使团队控制着知识库或数据库。包含表格、图像、流程图和图表的文档都包含可能需要完全用文本表达的信息。
- en: Based on the prompts Wove establishes, the model writes instructions for extracting
    all the data from the sheet. This multiple-step process is examined in [*Chapter*
    *8*](B21964_08.xhtml#_idTextAnchor172), *Fine-Tuning*.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据Wove建立的提示，模型编写指令以从表格中提取所有数据。这个过程包含多个步骤，在[*第8章*](B21964_08.xhtml#_idTextAnchor172)“微调”中进行了详细探讨。
- en: In *Step 1*, Wove runs version GPT-4 Turbo, while in other steps, it runs ChatGPT
    3.5 and other models. Running tasks sequentially is ten times faster than running
    GPT-4 once. They used GPT-4 Turbo to generate fine-tuning data. By using more
    than one model, they can balance performance and cost.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*步骤1*中，Wove运行GPT-4 Turbo版本，而在其他步骤中，它运行ChatGPT 3.5和其他模型。按顺序运行任务比一次性运行GPT-4快十倍。他们使用GPT-4
    Turbo生成微调数据。通过使用多个模型，他们可以在性能和成本之间取得平衡。
- en: 'Wove leaves out 10 to 20% of the data to test the model. This is standard practice.
    They take out different chunks of data from documents to create a broader and
    likely more effective test set. Tip: Don’t bias the model by always taking the
    first 20% of every document. They use a random seed to pick pieces of documents
    but then maintain that same chunk each time from the same document; this allows
    them to create a reproducible set. So, their validation steps do not differ because
    of the test data.'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wove省略了10%到20%的数据来测试模型。这是标准做法。他们从文档中取出不同的数据块来创建一个更广泛且可能更有效的测试集。提示：不要通过总是取每份文档的前20%来对模型产生偏见。他们使用随机种子来选择文档的片段，但每次都从同一文档中保持相同的块；这使他们能够创建一个可重复的集合。因此，他们的验证步骤不会因为测试数据而有所不同。
- en: All of this hard work is for data cleaning. The first goal is to expose the
    data to those responsible for ensuring they have the correct data. As mentioned,
    this will set up a later conversational experience to help find rates. The FAQ
    and Wove examples should give some understanding of data issues, but there are
    other considerations.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些艰苦的工作都是为了数据清洗。首要目标是让负责确保数据正确性的相关人员接触到这些数据。正如之前提到的，这将有助于设置后续的对话体验，以帮助找到合适的比率。FAQ和Wove示例应该能提供一些关于数据问题的理解，但还有其他需要考虑的因素。
- en: Other considerations for creating a quality data pipeline
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建高质量数据管道的其他考虑因素
- en: Not all designers and product managers will be involved with every step of the
    RAG process. All vendors use the fancy term **pipeline** to represent this flow
    of information from source to customer. Issues can occur before, during, and after
    models are included in the pipeline. Keep an eye on the following areas for issues
    impacting customer experience.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有设计师和产品经理都会参与RAG过程的每个步骤。所有供应商都使用“管道”这个术语来表示从源头到客户的信息流。在模型被纳入管道之前、期间和之后都可能发生问题。请注意以下可能影响客户体验的问题区域。
- en: Computational resources
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算资源
- en: RAG has some real work to do. It has to take an extensive collection of documents
    and resources and create vector data like the original model generation. Doing
    this regularly can be computationally expensive. Watch for any performance issues
    when scaling up. Many third-party solutions will talk about millisecond response
    times. That is wonderful; responses should feel natural. It might be okay for
    results to take a few seconds in some instances, but nominally, a chat response
    should start in 200-300 ms (about 1/4 of a second)..
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: RAG有一些实际的工作要做。它必须从大量的文档和资源中创建类似于原始模型生成的向量数据。定期这样做可能会非常耗费计算资源。在扩展时注意任何性能问题。许多第三方解决方案会谈论毫秒级的响应时间。那很好；响应应该感觉自然。在某些情况下，结果可能需要几秒钟，但通常，聊天响应应在200-300毫秒（大约1/4秒）内开始。
- en: Meanwhile, recommendations might be triggered when data changes (this can get
    expensive if always recalculated and no user needs to be updated) or calculated
    when a page is rendered. Even a trigger to email or message someone about the
    recommendation requires processes to have current information and evaluate for
    issues on a schedule. Each of these events will have a cost. Consider the cost
    of the recommendation if no one can use it.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，当数据发生变化时可能会触发推荐（如果总是重新计算且没有用户需要更新，这可能会变得昂贵）或当页面渲染时计算。即使是一个触发通过电子邮件或消息某人关于推荐的触发器，也需要有当前信息并按计划评估问题。每个事件都会产生成本。考虑如果没有人可以使用它，推荐的成本。
- en: 'Issue: *There is no such thing as a slow, good user experience*. Designers
    and PMs can help performance in a few places.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：*没有慢速但好的用户体验这回事.* 设计师和项目经理可以在几个地方帮助提高性能。
- en: Monitor and verify a solution’s performance and decide what will meet users’
    expectations. Product owners should set performance expectations.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控和验证解决方案的性能，并决定哪些能满足用户期望。产品负责人应设定性能期望。
- en: Monitor whether too much or too little data is sent to the LLM. All data should
    provide value to the LLM; if not, eliminate it.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控发送给LLM的数据是否过多或过少。所有数据都应为LLM提供价值；如果不是，则删除它。
- en: Determine whether the prompt and context sizes provides value for its sizes.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定提示和上下文大小是否提供了与其大小相应的价值。
- en: API requests in an LLM cost money, so optimize or cache information when possible.
    Understand whether customers use recommendations or visible UIs.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在LLM中，API请求需要付费，因此当可能时优化或缓存信息。了解客户是否使用推荐或可见的UI。
- en: Scalability
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可扩展性
- en: This can be managed if the system only deals with hundreds of documents. Still,
    some large enterprises might be looking at a million documents and massive SQL
    databases. Maintaining this large corpus and refining and improving the quality
    of those databases and documents can be a significant investment. Emphasize the
    most helpful and frequently accessed materials. Take advantage of third-party
    pipeline solutions.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如果系统只处理数百份文档，这可以管理。然而，一些大型企业可能正在查看数百万份文档和庞大的SQL数据库。维护这样的大型语料库，以及改进这些数据库和文档的质量可能是一项重大投资。强调最有帮助和最常访问的材料。利用第三方管道解决方案。
- en: 'Issue: *You can only be in so many places simultaneously.* Scalability also
    applies to your time. Consider whether there are places worth your attention,
    like improving the management process, monitoring quality, maintaining documents,
    or improving the time and process it takes to edit and update documents. Consider
    a personal version of the 80/20 rule. If 20% of the time on project C returns
    80% of the value, spend resources there. Even better, use User Needs Scoring.
    If something is for all customers that they use frequently, and it is a critical
    area, then this deserves attention.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：*你不可能同时出现在很多地方.* 可扩展性也适用于你的时间。考虑是否有值得你关注的地方，比如改进管理流程、监控质量、维护文档或改进编辑和更新文档所需的时间和流程。考虑个人版的80/20规则。如果项目C的20%的时间能带来80%的价值，那么就在那里投入资源。更好的是，使用用户需求评分。如果某些东西是所有客户都频繁使用的，并且是一个关键领域，那么这值得注意。
- en: Training data quality
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练数据质量
- en: Fill in the following puzzle.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 填写以下谜题。
- en: Quality in supports quality out.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 输入质量决定输出质量。
- en: Garbage in supports _______ out.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 输入垃圾数据会导致输出垃圾。
- en: A+ if you guessed garbage. The quality of training materials profoundly affects
    the ability to fine-tune. If content is very limited, biased, or has a lot of
    red herrings that could lead customers astray, then there will be ongoing issues.
    The relevance and quality of content is king. This chapter covered the importance
    of clean data, but that process can only have limited fixes for quality. That
    is, removing redundant or conflicting data might be easy to do. It is hard to
    do when writing this book. Did the reader remember or even see something in [*Chapter
    1*](B21964_01.xhtml#_idTextAnchor016), *Recognizing the Power of Design in ChatGPT*
    that is now important in [*Chapter 5*](B21964_05_split_000.xhtml#_idTextAnchor108),
    *Defining the Desired Experience*? Who is the content expert that can determine
    correctness? This gets more challenging as the data grows. Now, think about how
    a model can handle learning something 150 pages ago that now becomes important.
    The more technical the data, the less likely an individual can know if the content
    is high quality. Models can forget, too. They are especially prone to forgetting
    information in the middle. Not to mention problems understanding knowledge for
    specific releases or combinations of products. Rely on content partners, authors,
    and technical experts. It takes a village. Remember that from [*Chapter 1*](B21964_01.xhtml#_idTextAnchor016),
    *Recognizing the Power of Design* *in ChatGPT*?
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你猜到是垃圾，那你就对了。训练材料的质量深刻影响着微调的能力。如果内容非常有限、有偏见或有很多可能导致客户误入歧途的误导性信息，那么将会有持续的问题。内容的关联性和质量是关键。本章讨论了清洁数据的重要性，但这个过程对质量的改进只能有限。也就是说，删除冗余或冲突的数据可能很容易做到。但在写这本书时很难做到。读者是否记得或甚至看到了在[*第一章*](B21964_01.xhtml#_idTextAnchor016)，*认识到ChatGPT中设计的力量*中现在重要的内容，在[*第五章*](B21964_05_split_000.xhtml#_idTextAnchor108)，*定义期望的体验*中？谁是能够确定内容正确性的内容专家？随着数据量的增长，这会变得更加具有挑战性。现在，想想一个模型如何处理150页之前学习的内容现在变得重要的情况。数据越技术性，个人知道内容是否高质量的可能性就越小。模型也会忘记，尤其是中间的信息。更不用说理解特定发布或产品组合的知识理解问题。依靠内容合作伙伴、作者和技术专家。这需要整个村庄。记得在[*第一章*](B21964_01.xhtml#_idTextAnchor016)，*认识到ChatGPT中设计的力量*中提到的这一点吗？
- en: RAG is well suited for responding to specific questions against a wealth of
    content. However, the data must be in the correct format, and this can be some
    heavy lifting with data at scale. Picking suitable chunk sizes when segmenting
    text is more art than science. For the CliffsNotes version (a student study guide
    for popular books in the US) of dealing with chunking and other lessons learned,
    watch the video from Prolego.This video will be discussed at the end of this chapter.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: RAG非常适合针对大量内容回答特定问题。然而，数据必须以正确的格式存在，这在数据规模较大时可能是一项艰巨的任务。在分割文本时选择合适的块大小更多的是艺术而非科学。关于处理块分割和其他经验教训的CliffsNotes版本（美国流行书籍的学生学习指南），请观看Prolego的视频。本章节结束时将讨论此视频。
- en: 'Video: [Prolego tips for RAG development](https://www.youtube.com/watch?v=Y9qn4XGH1TI)
    ([https://www.youtube.com/watch?v=Y9qn4XGH1TI](https://www.youtube.com/watch?v=Y9qn4XGH1TI))'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 视频：[Prolego关于RAG开发的技巧](https://www.youtube.com/watch?v=Y9qn4XGH1TI) ([https://www.youtube.com/watch?v=Y9qn4XGH1TI](https://www.youtube.com/watch?v=Y9qn4XGH1TI))
- en: 'Issue: *Don’t let the models be overwhelmed with garbage and reduce accuracy.*
    Monitor and set improvement goals.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：*不要让模型被垃圾数据淹没，从而降低准确性。* 监控并设定改进目标。
- en: Domain specificity
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 领域特定性
- en: Enterprise models rely on domain-specific content.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 企业模型依赖于特定领域的内 容。
- en: 'Issue: *Gathering and annotating data to improve performance is expensive.*
    Annotation can take many forms, but as with data quality, find experts inside
    or outside your company to take this to the next level. Invest in building personal
    expertise.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：*收集和注释数据以改进性能是昂贵的。* 注释可以有多种形式，但与数据质量一样，在公司内部或外部找到专家将这一过程提升到下一个层次。投资于建立个人专业知识。
- en: Response consistency and coherence
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 响应一致性和连贯性
- en: RAG solutions will be challenging. Enterprise solutions value deterministic
    answers, which will not happen with only a generative solution. Answers will vary,
    even when asked the same question. This can be improved with prompt engineering,
    fine-tuning, and the careful use of the generative models in a larger ecosystem
    of products.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: RAG解决方案将具有挑战性。企业解决方案重视确定性答案，而仅靠生成式解决方案是无法实现的。即使问相同的问题，答案也会有所不同。这可以通过提示工程、微调和在更大产品生态系统中谨慎使用生成模型来改进。
- en: 'Issue: *Don’t throw the baby out with the bathwater.* With existing chatbots
    that provide repeatable solutions, supplement them with a generative solution.
    Focus fine-tuning on consistency. For recommendation engines, look for the places
    with the most value to add by incremental improvements.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：*不要因噎废食*。对于提供可重复解决方案的现有聊天机器人，补充以生成式解决方案。将微调的重点放在一致性上。对于推荐引擎，寻找通过增量改进增加最大价值的领域。
- en: Privacy, security, and data residency
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐私、安全和数据驻留
- en: Because the data is proprietary and is contained in company databases, knowledge
    base, and APIs, its access can be managed when responding to customer questions.
    Since a ChatGPT response will be based on the context passed to it from a RAG
    solution, it makes sense to make sure privacy and security policies limit the
    visibility of this data to the appropriate customers. Be on the front line to
    monitor for issues that appear during a chat or come in via customer feedback.
    One area where designers can add value to the privacy discussion is related to
    the data seen during reviewing interactions.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据是专有的，包含在公司数据库、知识库和API中，因此在回答客户问题时可以管理其访问权限。由于ChatGPT的响应将基于从RAG解决方案传递给它的上下文，因此确保隐私和安全策略限制适当客户的可见性是有意义的。在聊天过程中出现或通过客户反馈出现的问题的前线进行监控。设计师可以在隐私讨论中增加价值的领域之一与审查交互期间看到的数据有关。
- en: Besides ensuring that data from backends doesn’t get into the wrong conversations,
    there are times when masking **Personally Identifiable Information** (**PII**)
    is necessary. In [*Chapter 2*](B21964_02_split_000.xhtml#_idTextAnchor031), *Conducting
    Effective* *User Research*, ways to mask PII were discussed. This is a consideration
    when training models. Training data can contain PII. One approach is not to mask
    it (replacing the text with ****), as that would hinder the model’s comprehension,
    but to *transform* the data into fake data generated by the model itself. For
    example, instruct a model during a data cleaning step to replace customer names
    with customer names it makes up. It is good at this, and this fake data, called
    **synthetic data,** can replace actual data.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 除了确保后端数据不会进入错误的对话之外，有时对**个人身份信息**（**PII**）进行屏蔽也是必要的。在[*第2章*](B21964_02_split_000.xhtml#_idTextAnchor031)
    *进行有效* *用户研究* 中，讨论了屏蔽PII的方法。这是在训练模型时需要考虑的因素。训练数据可能包含PII。一种方法是不对其进行屏蔽（用****替换文本），因为这会阻碍模型的理解，而是将数据*转换*成模型自身生成的假数据。例如，在数据清洗步骤中指导模型用它自己编造的客户姓名替换客户姓名。它在这方面很擅长，这种假数据被称为**合成数据**，可以替代实际数据。
- en: '[PRE12]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ChatGPT is very good at this. Even ChatGPT will reject providing fake Social
    Security numbers, so notice how the prompt asked for a nine-digit number in the
    prompt. It responded with the results in *Table 6.4*.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT在这方面非常出色。即使是ChatGPT也会拒绝提供假的社会安全号码，所以注意提示中要求在提示中输入九位数。它以*表6.4*中的结果进行了响应。
- en: '| **Customer** **Name** | **Country** | **Age** | **Car** **Sharing (Monthly)**
    | **Identifier** **Number** | **Cell** **Phone Number** | **Favorite** **Payment
    Method** |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| **客户** **姓名** | **国家** | **年龄** | **汽车共享（每月）** | **标识符** **号码** | **手机**
    **号码** | **喜欢的** **支付方式** |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| **Emma Smith** | USA | 29 | $150 | 22-333-444 | (555) 123-4567 | Credit Card
    |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| **Emma Smith** | 美国 | 29 | $150 | 22-333-444 | (555) 123-4567 | 信用卡 |'
- en: '| **Hiroshi Tanaka** | Japan | 35 | ¥18,000 | 22-333-445 | 090-1234-5678 |
    Mobile Payment |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| **Hiroshi Tanaka** | 日本 | 35 | ¥18,000 | 22-333-445 | 090-1234-5678 | 移动支付
    |'
- en: '| **Maria Garcia** | Spain | 41 | €120 | 22-333-446 | 612-345-678 | Debit Card
    |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| **Maria Garcia** | 西班牙 | 41 | €120 | 22-333-446 | 612-345-678 | 借记卡 |'
- en: '| **David Osei** | Ghana | 26 | GHS 600 | 22-333-447 | 024-123-4567 | Mobile
    Money |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| **David Osei** | 加纳 | 26 | GHS 600 | 22-333-447 | 024-123-4567 | 移动货币 |'
- en: '| **Anna Müller** | Germany | 32 | €140 | 22-333-448 | 0151-1234567 | PayPal
    |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| **Anna Müller** | 德国 | 32 | €140 | 22-333-448 | 0151-1234567 | PayPal |'
- en: Table 6.4 – Example of using synthetic data to replace PII
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.4 – 使用合成数据替换PII的示例
- en: Notice how the names feel localized; the counties were varied, local currency
    and reasonable amounts were used, and the phone numbers were localized. *Mobile
    Money* is not a term I recognize, but it is common in Ghana. Mobile Money means
    payments made via mobile phone providers. So, it is even possible to learn something
    from synthetic data. Since the subject of other countries came up, there are other
    country-specific issues.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 注意名字的本地化感觉；县份各不相同，使用了当地货币和合理的金额，电话号码也进行了本地化。*移动货币*这个词我不认识，但在加纳很常见。移动货币是指通过手机运营商进行的支付。因此，甚至可以从合成数据中学习到一些东西。既然提到了其他国家的主题，还有其他特定国家的问题。
- en: There are two considerations when discussing country-specific limitations that
    might limit model enrichment. Generally, this will fall to the product manager.
    The first is whether there are export limitations for company data. Some countries
    restrict the export of customer or employee data across borders. They have data
    residency requirements to house data in-country. This is why many vendors provide
    data centers in some regions. The **General Data Protection Regulation** (**GDPR**)
    in the European Union and the Privacy Shield framework come to mind. When dealing
    with personal information that might be common in a human resource chat application,
    for example, safeguards might be required to be in place, and consent might be
    needed. This can impact the user experience. I have had to design examples where
    user permissions are required or policy requirements on what can or should not
    be shared need to be consented to before proceeding.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论特定国家限制时，有两个考虑因素可能会限制模型丰富。通常，这会落到产品经理身上。第一个是公司数据是否有出口限制。一些国家限制跨境出口客户或员工数据。他们有数据居住地要求，即在境内存储数据。这就是为什么许多供应商在特定地区提供数据中心。欧盟的**通用数据保护条例**（**GDPR**）和隐私盾框架就浮现在脑海中。在处理可能常见于人力资源聊天应用中的个人信息时，可能需要采取保护措施，并且可能需要同意。这可能会影响用户体验。我不得不设计需要用户权限的示例，或者需要在继续之前同意可以或不应共享的政策要求。
- en: The second issue is more data-centric and not subject to data residency issues.
    Processing rules might only apply to certain countries or groups within a country.
    It could be a data issue to ensure that the LLM knows that this person is from
    a particular country, and thus, specific documents, policies, or APIs apply. For
    example, expense reimbursement policies vary per country. It is one thing for
    an American to have dinner on a trip to France and be reimbursed when they return
    to the US (US policy applies) versus someone from France going somewhere outside
    the European Union (EU/French policy). Designers and PMs must recognize the necessary
    attributes to filter and support the correct data and resources. This is not unique
    to LLMs. It must be handled in these cases, such as in a GUI or an existing chatbot.
    Another version of the rules is how to communicate with our audience. Sometimes,
    biases in the data or even ethical concerns need to be addressed.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题更加以数据为中心，并且不受数据居住地问题的影响。处理规则可能仅适用于某些国家或国家内的某些群体。可能存在一个数据问题，即确保LLM知道这个人来自特定国家，因此，特定的文件、政策或API适用。例如，差旅报销政策因国家而异。美国人去法国出差并在回到美国时获得报销（适用美国政策）与法国人去欧盟外的某个地方（适用欧盟/法国政策）是两回事。设计师和项目经理必须认识到必要的属性，以过滤和支撑正确的数据和资源。这不仅仅针对LLM。在这些情况下，例如在GUI或现有的聊天机器人中，必须处理这些问题。
- en: Bias and ethical concerns
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 偏见和伦理问题
- en: It might be reasonable to think that enterprise data doesn’t have bias, but
    still watch out for it. There could be tongue-in-cheek content in the knowledge
    base, but it gets regurgitated as truth. It could cause issues in the results.
    Take a look at *Figure 6**.13*. It is a simple interaction that it should be easy
    to discern facts. It is fair to point out that this was *not* ChatGPT. This is
    a continuation of our Cohere example. Remember, the example from earlier extended
    a basic LLM to include the FAQs. Unless it is told otherwise and guardrails are
    put in place, it will still attempt to answer general model questions. It did
    not go as planned. As these models are expected to improve quickly, it is not
    fair to comment on this model’s shortcomings. All models have shortcomings. It
    is used to make a point about all models.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 可能合理地认为企业数据没有偏见，但仍然要提防它。知识库中可能会有讽刺性的内容，但它被当作真理重复。这可能会在结果中引起问题。看看*图6**.13*。这是一个简单的交互，应该很容易辨别事实。指出这一点是公平的，这*并不是*ChatGPT。这是我们对Cohere示例的延续。记住，早些时候的示例扩展了一个基本的LLM以包括常见问题解答。除非有其他说明并设置了护栏，否则它仍然会尝试回答一般的模型问题。它并没有按计划进行。由于预计这些模型会迅速改进，因此对这一模型的不足之处进行评论是不公平的。所有模型都有不足之处。这是用来说明所有模型的一个例子。
- en: '![](img/B21964_06_13.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21964_06_13.jpg)'
- en: Figure 6.13 – Conversational hallucinations can create bias and errors
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 – 对话式幻觉可能产生偏见和错误
- en: 'Issue: *Don’t get caught making (wrong) ethical decisions*. Avoid these discussions
    when possible. Watch data collection, model training, and the monitoring process
    for potential issues. Let’s break down this collection of issues. Hank Greenberg
    (who is Jewish) and Hank Aaron (who was not) got confused somehow. Maybe this
    is a hallucination. But simple questions such as this would be easy to answer.
    Let me point out a few problems.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：*不要陷入做出（错误）道德决策的困境*。尽可能避免这些讨论。观察数据收集、模型训练和监控过程以发现潜在问题。让我们分析这一系列问题。汉克·格林伯格（他是犹太人）和汉克·阿伦（他不是）以某种方式被混淆了。也许这是一种幻觉。但像这样的简单问题很容易回答。让我指出几个问题。
- en: It didn’t get Hank Aaron (presumably because of my spelling error) even when
    the model corrected my mistake and returned it spelled correctly.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使模型纠正了我的错误并返回了正确的拼写，这也并没有让汉克·阿伦（可能是由于我的拼写错误）受到影响。
- en: Hank Aaron was not Jewish.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 汉克·阿伦并非犹太人。
- en: His career lasted 23 years, not 24.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他的职业生涯持续了23年，而不是24年。
- en: He never had close to 209 home runs per season (he had 755 in his entire 23-year
    career; I know because I watched him tie the record).
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他从未在每个赛季接近209个本垒打（在他的整个23年职业生涯中，他总共击出了755个本垒打；我知道是因为我看过他打破记录）。
- en: Jackie Robinson was the first African-American inducted into the Hall of Fame
    in 1962\. Hank Aaron’s induction was *20* years later.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杰基·罗宾逊是1962年第一位被选入名人堂的非洲裔美国人。汉克·阿伦的入选比他晚了*20*年。
- en: Hank Greenberg was well known to be Jewish and faced discrimination.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 汉克·格林伯格众所周知是犹太人，并面临歧视。
- en: We tried this with ChatGPT. It did assume Hank Aaron even when his name was
    misspelled. It accurately explained he was not Jewish, played for 23 years, knew
    his 755 home runs record, and his place in the Baseball Hall of Fame. The ChatGPT
    3.5 model was factually correct. And to be fair, *a newer update to Cohere got
    all of* *this correct*.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试过用ChatGPT来做这个实验。即使他的名字拼写错误，它也假设了汉克·阿伦。它准确地解释说，他不是犹太人，打了23年球，知道他的755个本垒打记录，以及他在棒球名人堂中的位置。ChatGPT
    3.5模型在事实上是正确的。而且公平地说，Cohere的新更新*完全*正确地处理了这一切。
- en: Enterprise data is not expected to talk about the religion of famous baseball
    players. Just recognize that answers for well-known facts can still contain hallucinations,
    lies, or whatever they should be called, and the organization will likely be liable
    for spreading disinformation. It doesn’t mean a lawsuit. It could mean not meeting
    a service-level agreement, upsetting or losing a potential customer, or having
    to compensate the customer. This is not unlike what would happen if a human agent
    provided incorrect information. There can be downstream costs or service interruptions
    due to wrong answers. With AI, we have seen numerous mistakes, errors, or maybe
    a lack of training, causing issues.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 企业数据不应该讨论著名棒球运动员的宗教信仰。只需认识到，对于众所周知的事实，答案仍然可能包含幻觉、谎言或无论它们应该被称为什么，组织可能会因传播虚假信息而负有责任。这并不意味着诉讼。这可能意味着未能满足服务水平协议，激怒或失去潜在客户，或者不得不赔偿客户。这并不像如果人类代理提供了错误信息会发生的情况。由于答案错误，可能会产生下游成本或服务中断。使用AI，我们已经看到了许多错误、错误或可能是缺乏训练，导致问题。
- en: Semi-autonomous cars causing accidents in the automotive industry come to mind.
    This isn’t to say that human drivers are better (they are not by almost an order
    of magnitude). Still, the kinds of accidents caused by training model issues sometimes
    seem obvious and avoidable by a human driver (being able to identify an 18-wheeler
    truck crossing the road in a high-glare situation). At the same time, there are
    far more cases *not* seen in the news, such as semi-autonomous cars *not* getting
    into accidents where a human’s response time and visibility would have resulted
    in tragedy. Ultimately, expect generative AI to be more reliable, consistent,
    and accurate than humans. It should get there in a few years with significant
    effort. Always be aware that bias and ethics come into play within a model. In
    addition, be ethical in how much effort is expended to build and test models.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在汽车行业中，半自动驾驶汽车导致事故的情况值得考虑。这并不是说人类驾驶员更好（他们几乎好一个数量级）。然而，由训练模型问题引起的某些事故似乎很明显，可以通过人类驾驶员避免（在高反光情况下能够识别正在过路的18轮卡车）。同时，还有许多新闻中没有报道的情况，例如半自动驾驶汽车在没有导致悲剧的人类反应时间和可见性的情况下没有发生事故。最终，预计生成式AI将比人类更可靠、更一致、更准确。通过大量的努力，它可能在几年内达到这一点。始终意识到偏见和道德在模型中发挥作用。此外，在构建和测试模型所付出的努力方面要具有道德性。
- en: There will be a benefit/risk analysis; just don’t get caught on the wrong side,
    as Ford did when it refused to fix defective gas tanks in its Pinto model.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 将进行利益/风险分析；只是不要陷入错误的一边，就像福特在拒绝修复其Pinto车型的缺陷油箱时所做的那样。
- en: 'Wikipedia: [Ford Pinto Gas Tank Controversy](https://en.wikibooks.org/wiki/Professionalism/The_Ford_Pinto_Gas_Tank_Controversy)
    ([https://en.wikibooks.org/wiki/Professionalism/The_Ford_Pinto_Gas_Tank_Controversy](https://en.wikibooks.org/wiki/Professionalism/The_Ford_Pinto_Gas_Tank_Controversy))'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 维基百科：[福特Pinto油箱争议](https://en.wikibooks.org/wiki/Professionalism/The_Ford_Pinto_Gas_Tank_Controversy)
    ([https://en.wikibooks.org/wiki/Professionalism/The_Ford_Pinto_Gas_Tank_Controversy](https://en.wikibooks.org/wiki/Professionalism/The_Ford_Pinto_Gas_Tank_Controversy))
- en: One has to invest in creating good models. There will be lawsuits related to
    this as well. Take the time and energy to put quality first. Document your sources;
    in legal terms, this is called the chain of custody. Check the work and refine
    and resolve problems with a cadence that befits the risks understood by the enterprise.
    It won’t be perfect – nor are human agents. Just put a process in place for constant
    improvement. The Silicon Valley mantra about “move fast and break things” sounds
    great at a start-up, but when delivering a paid service to high-valued customers,
    maybe be more pragmatic about investing in quality.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 必须投资于创建良好的模型。这也将涉及相关的诉讼。花时间精力将质量放在首位。记录你的来源；在法律术语中，这被称为保管链。以符合企业理解的风险节奏检查工作，并改进和解决问题。它不会完美——人类代理也是如此。只需建立一个持续改进的流程。硅谷关于“快速行动，打破事物”的口号在初创公司听起来很棒，但在向高价值客户提供付费服务时，可能需要更加务实，投资于质量。
- en: Embedding other techniques
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入其他技术
- en: If you watched the OpenAI discussion on techniques mentioned at the beginning
    of the chapter to learn about a few additional methods, the video discusses optimization
    techniques at the 15-minute mark. This approach to benchmarking quality and applying
    tools and techniques is right. This was done for a typical ChatGPT solution that
    involves searching a knowledge base. They tried a few methods that did not work
    to get the improvements they expected (**Hypothetical Document Embedding** (**HyDE**)
    retrieval and fine-tuning embedding). They found some worthy investments (chunk/embedding
    reranking, classification, prompt engineering, and query expansion). It would
    be way over our heads exploring how to do these. The key is for designers and
    PMs to work with the team to establish a benchmark, find good data to train the
    model, and test and verify results as they are iterated. Consider the goal so
    it is known when the goal is reached. Recognize that as models change and data
    grows, adapt. In reality, a team won’t ever be done, but with a quality bar, the
    organization can allocate resources more wisely.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你观看了OpenAI关于本章开头提到的技术的讨论，以了解一些额外的学习方法，视频在第15分钟讨论了优化技术。这种方法在基准测试质量和应用工具和技术方面是正确的。这是为典型的ChatGPT解决方案进行的，该方案涉及搜索知识库。他们尝试了一些没有达到预期改进的方法（**假设文档嵌入**（**HyDE**）检索和微调嵌入）。他们发现了一些值得投资的地方（块/嵌入重排序、分类、提示工程和查询扩展）。探索如何做这些事情会超出我们的能力范围。关键是设计师和项目经理与团队合作，建立基准，找到好的数据来训练模型，并在迭代过程中测试和验证结果。考虑目标，以便在达到目标时知道。认识到随着模型的变化和数据量的增长，需要适应。实际上，一个团队永远不会完成，但有了质量标准，组织可以更明智地分配资源。
- en: Evaluation metrics
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估指标
- en: '[*Chapter 10*](B21964_10_split_000.xhtml#_idTextAnchor216), *Monitoring and
    Evaluation*, will cover methods for determining performance. Relevance, diversity,
    and coherence are all crucial factors for our datasets. The focus will be to understand
    this from the user’s perspective with accuracy and customer feedback.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第10章*](B21964_10_split_000.xhtml#_idTextAnchor216)，*监控与评估*，将涵盖确定性能的方法。相关性、多样性和连贯性都是我们数据集的关键因素。重点将是从用户的角度以准确性和客户反馈来理解这一点。'
- en: Despite these limitations, RAG holds promise for enhancing the capabilities
    of conversational AI systems such as ChatGPT by enabling more contextually relevant
    and informative responses to user queries. Addressing the challenges above through
    ongoing research and development efforts can help unlock RAG’s full potential
    to improve user engagement and satisfaction in conversational AI applications.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些限制，但RAG通过允许对用户查询做出更具情境相关性和信息性的响应，为增强如ChatGPT等对话式AI系统的能力提供了希望。通过持续的研究和开发工作解决上述挑战，可以帮助释放RAG的全部潜力，以改善对话式AI应用中的用户参与度和满意度。
- en: Resources for RAG
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAG资源
- en: A good RAG solution will use a service dedicated to managing the influx of data,
    processing it, storing it, and retrieving it to share with the LLM. There has
    been so much movement in this space since RAG was invented, and it is hard to
    realize how quickly this has become real.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的RAG解决方案将使用一个专门用于管理数据流入、处理、存储和检索以与LLM共享的服务。自从RAG被发明以来，这个领域已经发生了巨大的变化，很难意识到这一切发生得如此之快。
- en: Because most of the design work is in data quality rather than technology, it
    will be best to provide resources for those who want to explore the more technical
    pieces of the puzzle. The OpenAI resource is the best place to start; it will
    evolve and adapt as technology changes. GPT-4 and newer work directly with RAG.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大部分设计工作集中在数据质量而不是技术上，因此最好为那些想要探索谜团中更技术性部分的人提供资源。OpenAI资源是开始的最佳地点；它将随着技术的发展而演变和适应。GPT-4和更新的版本直接与RAG协同工作。
- en: 'Web article: [OpenAI RAG vs. Customized RAG with Milvus](https://thenewstack.io/openai-rag-vs-your-customized-rag-which-one-is-better)
    ([https://thenewstack.io/openai-rag-vs-your-customized-rag-which-one-is-better](https://thenewstack.io/openai-rag-vs-your-customized-rag-which-one-is-better))'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 网络文章：[OpenAI RAG与定制RAG（Milvus）比较](https://thenewstack.io/openai-rag-vs-your-customized-rag-which-one-is-better)
    ([https://thenewstack.io/openai-rag-vs-your-customized-rag-which-one-is-better](https://thenewstack.io/openai-rag-vs-your-customized-rag-which-one-is-better))
- en: Do not assume linking to these resources implies they are best in class. They
    are all improving rapidly, will diverge in value, and some will disappear. As
    the market evolves, look for tools that can automate the pipeline with high quality.
    Access to a database is the most prominent tool needed in the enterprise after
    knowledge access.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 不要假设链接到这些资源就意味着它们是同类中的最佳选择。它们都在快速发展，价值会分化，有些可能会消失。随着市场的发展，寻找能够以高质量自动化流程的工具。在企业中，访问数据库是知识访问之后最需要的工具。
- en: Databases and SQL
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据库和SQL
- en: Database retrieval presents challenges. Consider how the database thinks about
    its content and how to ask for results. This is typically expressed in SQL, the
    structured query language of most databases. Some databases do not use SQL and
    are called NoSQL databases. Since most enterprise data to inject into tasks and
    prompts is held in SQL databases, we will focus on a SQL example. LLMs have some
    ability to write SQL, but it is still an evolving area. Here is an example highlighting
    the complexities of working with a database.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库检索存在挑战。考虑数据库是如何考虑其内容的，以及如何请求结果。这通常用SQL（大多数数据库的结构化查询语言）来表示。有些数据库不使用SQL，被称为NoSQL数据库。由于大多数企业数据要注入任务和提示中，都是存储在SQL数据库中，我们将重点关注一个SQL示例。LLMs（大型语言模型）有一些编写SQL的能力，但这仍然是一个不断发展的领域。以下是一个突出显示与数据库工作复杂性的示例。
- en: '[PRE13]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'It might already be obvious there are a few issues to address:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 可能已经很明显，有一些问题需要解决：
- en: Which Burlingame (the one near me in the San Francisco Bay Area, San Diego,
    Kansas, Oregon, etc.…)?
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个Burlingame（我在旧金山湾区附近，圣地亚哥，堪萨斯，俄勒冈等……）？
- en: What kinds of jobs? Does it have context for what is needed?
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有哪些工作类型？这有没有对所需内容的上下文描述？
- en: Are more filters for price, types of companies, and working hours needed?
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是否需要更多的价格、公司类型和工作时间的过滤器？
- en: Focusing on the first step, drilling down into understanding the first statement.
    What does near mean? In the San Francisco Bay Area, 10 miles is reasonable, while
    in Oregon, 25 miles is reasonable. If this were New York City and you wanted a
    pizza place, two blocks would be too far.
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 专注于第一步，深入理解第一个陈述。"附近"是什么意思？在旧金山湾区，10英里是合理的，而在俄勒冈州，25英里是合理的。如果这是纽约市，你想找一家披萨店，两个街区就太远了。
- en: How does one translate this into cities that are in my database?
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何将这个翻译成数据库中的城市？
- en: 'My point is that it is more complicated than extracting an entity like the
    name of a city and expecting, even with SQL magic, to be able to scope good content.
    This request needs to be pre-processed to generate a reasonable SQL statement.
    So, manage the input to expect a sensible output. In real estate property management,
    there is a saying: *inspect what you expect*. The same here. The input must be
    checked and broken up into pieces that need to be analyzed and expanded with more
    details to get the expected output. With the output, it might be possible to send
    a collection of results to the LLM, like sending documents or FAQs, so that ChatGPT
    can use this information to form a more refined answer. The LLM has to know about
    the schema to format a good query.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我的观点是，这比提取像城市名称这样的实体要复杂得多，即使有SQL的魔力，也期望能够找到好的内容。这个请求需要预处理以生成一个合理的SQL语句。因此，管理输入以期望一个合理的输出。在房地产物业管理中，有一句话：“检查你所期望的”。这里也是同样的道理。输入必须被检查并分解成需要分析和扩展更多细节以获得预期输出的部分。有了输出，可能可以向LLM发送一组结果，就像发送文档或FAQ一样，这样ChatGPT就可以使用这些信息来形成一个更精确的答案。LLM必须了解模式以格式化一个好的查询。
- en: Another approach I have worked on and that is becoming popular is text-to-SQL.
    This is a way to apply LLM intelligence to create a logical SQL statement that
    returns effective results. This approach has merit but depends on whether SQL
    can support the query. In the preceding example, the solution would need a city
    or location discovery tool based on a range or distance from a center point (and
    center points can be wildly inaccurate regarding where the person is located and
    where to go). A basic LLM would have to understand the related cities correctly.
    These tools are available and must be integrated to generate the correct list
    of towns and locations to pass into SQL. An LLM can also get this information
    and form the SQL queries.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种我一直在研究和逐渐流行起来的方法是文本到SQL。这是一种将LLM智能应用于创建逻辑SQL语句以返回有效结果的方法。这种方法有其优点，但取决于SQL是否支持查询。在先前的例子中，解决方案需要一个基于中心点范围或距离的城市或位置发现工具（中心点在确定人的位置和目的地方面可能非常不准确）。一个基本的LLM必须正确理解相关的城市。这些工具是可用的，并且必须集成以生成正确的城镇和位置列表，以便传递给SQL。LLM也可以获取这些信息并形成SQL查询。
- en: '[PRE14]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Foster City, CA, which is 3 miles away, was included the first time the question
    was asked, but Belmont, CA, or San Bruno, CA, were never included. So, if this
    customer were looking for a job, they would miss many opportunities. This specific
    issue could be fixed with adjustments to the prompt focusing on the exact cities
    and asking to check its work. For example, the results can be seen with a simple
    adjustment and follow-up.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 当第一次提问时，距离3英里的 Foster City, CA 被包括在内，但 Belmont, CA 或 San Bruno, CA 从未包括。因此，如果这位客户在寻找工作，他们会错过很多机会。这个问题可以通过调整提示，专注于确切的城市并要求检查其工作来得到解决。例如，通过简单的调整和后续跟进可以看到结果。
- en: '[PRE15]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: When asked later how close South San Francisco was to Hillsborough, it correctly
    answered 10 miles—much different than its first answer. *Rely on a well-known
    API to get this data rather than constantly dealing with the variability of an
    LLM*. API resources provide more accurate and reliable data when specific details
    are required. Consider incorporating these into LLM responses to provide natural
    interaction. An enterprise is rich with databases containing collections of facts.
    Use this as a competitive advantage. Don’t expect the LLM to do it all by generating
    results. This is why function calling exists – to get the value of specific data
    in generative output.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 当后来被问及南旧金山与希尔兹伯勒的距离时，它正确地回答了10英里——与它的第一个答案大不相同。*依靠一个知名的API来获取这些数据，而不是不断处理LLM的变异性*。当需要特定细节时，API资源提供了更准确和可靠的数据。考虑将这些资源纳入LLM响应中，以提供自然的交互。企业拥有丰富的数据库，其中包含事实集合。利用这一点作为竞争优势。不要期望LLM通过生成结果来完成所有工作。这就是为什么存在函数调用——为了在生成输出中获取特定数据的价值。
- en: Prompt engineering can also help improve the results. But the point should be
    clear—SQL needs some hand-holding to create effective queries and some pre-processing
    to give the database a good chance at returning effective results.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程也可以帮助提高结果。但应该明确的是——SQL需要一些指导来创建有效的查询，并且需要一些预处理来给数据库一个返回有效结果的好机会。
- en: Online suggestions for connecting to databases focus primarily on straightforward
    queries that don’t explore how the user will ask the questions. This is a more
    complex problem than just connecting to a database.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在线建议主要关注直接查询，而不是探索用户如何提问。这比仅仅连接到数据库的问题更复杂。
- en: Extra credit reading on database
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库相关阅读材料
- en: If database connectivity is new for you, read these references
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据库连接对您来说是新的，请阅读这些参考资料
- en: 'Article: [Talk to your](https://medium.com/@shivansh.kaushik/talk-to-your-database-using-rag-and-llms-42eb852d2a3c)
    Database using RAG and LLMS ([https://medium.com/@shivansh.kaushik/talk-to-your-database-using-rag-and-llms-42eb852d2a3c](https://medium.com/@shivansh.kaushik/talk-to-your-database-using-rag-and-llms-42eb852d2a3c))'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 文章：[使用RAG和LLMS与数据库交谈](https://medium.com/@shivansh.kaushik/talk-to-your-database-using-rag-and-llms-42eb852d2a3c)
    ([https://medium.com/@shivansh.kaushik/talk-to-your-database-using-rag-and-llms-42eb852d2a3c](https://medium.com/@shivansh.kaushik/talk-to-your-database-using-rag-and-llms-42eb852d2a3c))
- en: 'Article: [How to connect LLM to SQL database with LlamaIndex](https://medium.com/dataherald/how-to-connect-llm-to-sql-database-with-llamaindex-fae0e54de97c)
    ([https://medium.com/dataherald/how-to-connect-llm-to-sql-database-with-llamaindex-fae0e54de97c](https://medium.com/dataherald/how-to-connect-llm-to-sql-database-with-llamaindex-fae0e54de97c))'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 文章：[如何使用LlamaIndex将LLM连接到SQL数据库](https://medium.com/dataherald/how-to-connect-llm-to-sql-database-with-llamaindex-fae0e54de97c)
    ([https://medium.com/dataherald/how-to-connect-llm-to-sql-database-with-llamaindex-fae0e54de97c](https://medium.com/dataherald/how-to-connect-llm-to-sql-database-with-llamaindex-fae0e54de97c)))
- en: We will explore one example with the Oracle Digital Assistant. This area will
    see significant improvements in the coming years as the intelligence needed to
    interpret the user’s needs before forming the proper SQL queries will improve.
    The chaining necessary to get the correct result will also improve. This chaining
    problem is a function of what the user asks, the assumptions needed to understand
    the question, and the SQL required to return the answer. Chaining is the connecting
    of one answer that feeds the following question and subsequent answer. Sometimes,
    it makes sense to chain thoughts together to resolve a question. Let me finish
    with a use case example paraphrasing this Oracle blog example.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探索一个使用Oracle数字助手的例子。随着在形成适当的SQL查询之前解释用户需求所需的智能的提高，这个领域在接下来的几年中将看到显著的改进。获取正确结果所需的链接也将得到改善。链接问题是用户提出的问题、理解问题所需的假设以及返回答案所需的SQL的功能。链接是将一个答案连接到下一个问题和后续答案的过程。有时，将思想串联起来以解决问题是有意义的。让我用一个用例示例来结束，这个示例是对这篇Oracle博客示例的改写。
- en: 'Article: [Oracle Digital Assistant SLQ Integration](https://blogs.oracle.com/digitalassistant/post/introducing-the-new-oracle-digital-assistant-sql-dialog)
    ([https://blogs.oracle.com/digitalassistant/post/introducing-the-new-oracle-digital-assistant-sql-dialog](https://blogs.oracle.com/digitalassistant/post/introducing-the-new-oracle-digital-assistant-sql-dialog))'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 文章：[Oracle数字助手SQL集成](https://blogs.oracle.com/digitalassistant/post/introducing-the-new-oracle-digital-assistant-sql-dialog)
    ([https://blogs.oracle.com/digitalassistant/post/introducing-the-new-oracle-digital-assistant-sql-dialog](https://blogs.oracle.com/digitalassistant/post/introducing-the-new-oracle-digital-assistant-sql-dialog)))
- en: '[PRE16]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let’s address the issues with this process chain:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来解决这个流程链的问题：
- en: Michael – who is Michael? Look around my hierarchy and determine if Michael
    is known. This is a whole process by itself and fundamental to people searching
    in an organization.
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迈克尔——谁是迈克尔？在我的层级结构中四处看看，确定迈克尔是否为人所知。这是一个完整的过程，对于组织内的人进行搜索来说是基本的。
- en: If needed, disambiguate which Michael the user could be inferring.
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果需要，消除用户可能推断的迈克尔的不确定性。
- en: Map employees to the SQL field (called EMP). The concept of employees will be
    requested in many ways – workers, teams, teammates, underlings, people, etc. It
    is unlikely a user will *ever* use the SQL field name.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将员工映射到SQL字段（称为EMP）。员工的概念将以多种方式被请求——工人、团队、队友、下属、人们等。用户不太可能*永远*使用SQL字段名。
- en: Determine Michael’s department. (Use SQL to get the answer. It is 23.)
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定迈克尔的部门。（使用SQL获取答案。它是23。）
- en: Decide whether the default information to be returned needs to be enhanced based
    on the query (in this case, nothing special was asked of it).
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决定是否需要根据查询增强要返回的默认信息（在这种情况下，没有特别要求）。
- en: Limit query by security implications (for example, don’t show salary).
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 限制查询的安全影响（例如，不要显示薪资）。
- en: 'Perform the search, determine the size of the results, and return the results
    or a chunk of results if #>30.'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行搜索，确定结果的大小，如果结果数量大于30，则返回结果或结果的一部分。
- en: 'The final query should look something like this: **SELECT EMPNO, ENAME, JOB,
    MGR FROM EMP WHERE DEPTNO = 23 FETCH FIRST 30** **ROWS ONLY**.'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终查询应该看起来像这样：**SELECT EMPNO, ENAME, JOB, MGR FROM EMP WHERE DEPTNO = 23 FETCH
    FIRST 30 ROWS ONLY**。
- en: Return the answer using a generative answer, wrapping the specific details from
    the database.
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用生成式回答返回答案，将数据库中的具体细节包装起来。
- en: 'The Oracle article does an excellent job of discussing synonyms. In their example,
    they use the Big Apple for New York City. It serves as a good reminder that language
    is very flexible, and there are many cases where, without this sort of intelligence,
    the natural language feel that customers expect won’t happen. Since the database
    fields don’t match the users’ language, there is some work to help with. The LLM
    can likely help understand terms and tagging concepts, but a product person must
    help it with cryptic field labels. For example, it might not understand that PH2
    is a cell phone field. Use the LLM to extend the understanding of synonyms for
    a cell phone (such as mobile, digits, contact info, wireless #, phone number,
    #).'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 'Oracle文章在讨论同义词方面做得非常出色。在他们给出的例子中，他们使用“大苹果”来指代纽约市。这很好地提醒我们，语言非常灵活，有许多情况下，如果没有这种智能，客户期望的自然语言感觉就不会发生。由于数据库字段与用户的语言不匹配，有一些工作可以帮助解决这个问题。LLM（大型语言模型）可能有助于理解术语和标记概念，但产品人员必须帮助它理解那些晦涩的字段标签。例如，它可能不理解PH2是一个手机字段。使用LLM扩展对手机同义词（如mobile、digits、contact
    info、wireless #、phone number、#）的理解。'
- en: Service requests and other threaded sources
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 服务请求和其他线程化来源
- en: Service requests and other conversational sources, such as community discussions,
    are good data, but the kernels of truth within them must be exposed. Inferior
    results will occur if these sources are used without tagging and annotating correct
    answers. They are filled with wrong answers, half-truths, and misinformation.
    This is especially true for technical answers where the ground truth might be
    particular to a version or subversion of a product. So, confusing the difference
    between the 11.1 and 11.1.2 products can lead to incorrect results. And there
    can be red herrings in the answers, too. That is, there might be information that
    misleads or distracts from the problem and thus identifies the solution. It sometimes
    starts with “I don’t know if this matters, but…”.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 服务请求和其他对话来源，如社区讨论，是很好的数据，但其中的真相内核必须被揭示。如果没有标记和注释正确的答案而使用这些来源，将会产生较差的结果。它们充满了错误的答案、半真半假的信息和不准确的信息。这在技术答案中尤其如此，其中真实情况可能特定于产品的某个版本或子版本。因此，混淆11.1和11.1.2产品的区别可能导致错误的结果。答案中也可能存在误导性的信息。也就是说，可能存在误导或分散对问题注意力从而识别解决方案的信息。这有时会从“我不知道这有没有关系，但……”开始。
- en: 'Most service request systems mark closed service requests as completed and
    require the agent to tag the correct answer for future processing or analysis.
    A more formal structure for SRs will give a better chance of mining this information.
    The wealth of important information in SRs must be addressed, and there are reasons
    to consider these sources:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数服务请求系统会将已关闭的服务请求标记为已完成，并要求代理人为未来的处理或分析标记正确的答案。SRs（服务请求）的更正式结构将更有利于挖掘这些信息。SRs中包含的大量重要信息必须得到处理，并且有理由考虑这些来源：
- en: Customer language is a rich corpus of how customers talk about products, their
    issues, and how they interact in the real world. This domain-specific language
    and terminology are invaluable to training a model. Technical jargon, colloquialisms,
    initialisms, abbreviations, and shortcuts will appear more frequently in these
    sources than in traditional training, marketing, and technical documentation.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户语言是关于客户如何谈论产品、他们的问题以及他们在现实世界中的互动的丰富语料库。这种特定领域的语言和术语对于训练模型非常有价值。在传统培训、营销和技术文档中，这些来源比这些来源中更频繁地出现技术术语、俚语、首字母缩略词、缩写和快捷方式。
- en: Context is helpful in the LLM to create more accurate responses. Product release,
    patch levels, software installs, and operating system versions are typically what
    might be asked about when there is a problem, and this context can be very valuable.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在LLM中，上下文有助于创建更准确的响应。当有问题时，通常会询问产品发布、补丁级别、软件安装和操作系统版本，这种上下文可能非常有价值。
- en: Commonality—the frequency of common questions helps the model understand the
    likelihood of this type of response being useful in the future.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共同性——常见问题的频率有助于模型理解这种类型的响应在未来可能是有用的可能性。
- en: Technical domain training—there might not be another place to find the situations
    being discussed.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 技术领域培训——可能没有其他地方可以找到正在讨论的情况。
- en: Although most companies try to avoid some discussions in SRs and online channels,
    still be aware and avoid including PII in the model that might leak through in
    these forums. The process should support data cleansing and anonymization, as
    discussed in [*Chapter 2*](B21964_02_split_000.xhtml#_idTextAnchor031), *Conducting
    Effective* *User Research*, or synthesizing some data, as discussed earlier in
    this chapter. Doing this all manually is impossible at scale. Ultimately, these
    are just documents with the same issues as a knowledge base. Similar to databases,
    other pieces of software might be needed to access relevant information.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数公司试图避免在SRs和在线渠道中讨论某些问题，但仍需保持警惕，避免在模型中包含可能在这些论坛中泄露的PII。该过程应支持数据清洗和匿名化，如第[*第2章*](B21964_02_split_000.xhtml#_idTextAnchor031)中所述的*有效用户研究*，或如本章前面所述综合一些数据。在规模上手动完成这一切是不可能的。最终，这些只是与知识库有相同问题的文档。类似于数据库，可能还需要其他软件来访问相关信息。
- en: Integrating external content via APIs
  id: totrans-378
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过API集成外部内容
- en: Be ready to call the right service with the right question. Creating effective
    interactions that perform tasks –filling out an expense report, scheduling an
    appointment on a calendar, or booking a holiday or vacation – all require backend
    services.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好用正确的问题调用正确的服务。创建有效的交互以执行任务——填写费用报告、在日历上安排约会，或预订假日或假期——都需要后端服务。
- en: Many resources with advice on creating effective documents and resource retrieval
    were shared, but the solution’s success will still depend on the collection of
    services and software used. A few minutes on integrations is justified.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 分享了许多关于创建有效文档和资源检索的建议资源，但解决方案的成功仍然取决于所使用的服务和软件集合。在集成上花几分钟时间是合理的。
- en: OpenAI can respond with an API call instead of just replying based on knowledge.
    A model can update a support ticket, ask for shipping information, look up prices
    or products, or perform other interactions the business relies on. Unsurprisingly,
    ChatGPT helps explain and write code to connect to several well-known APIs, but
    that is for development. Product people must know what is available and *how*
    to frame this interaction. For fun, try something like this.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI可以通过API调用进行响应，而不仅仅是基于知识进行回复。模型可以更新支持票，请求运输信息，查找价格或产品，或执行业务依赖的其他交互。不出所料，ChatGPT有助于解释和编写代码以连接到几个知名的API，但这仅限于开发。产品人员必须知道有哪些可用内容以及*如何*构建这种交互。为了乐趣，可以尝试类似这样的东西。
- en: '[PRE17]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Enterprise APIs will mostly be proprietary, and ChatGPT won’t help directly.
    However, since most REST work should be similar, it still might be able to help.
    Sometimes, integrations with third parties such as Zoom, Teams, Slack, Jira, Confluence,
    Salesforce, HubSpot, ServiceNow, Oracle, or other vendors are used internally
    or as part of an enterprise offering. Remember that all the work is still needed
    to authenticate, create a security layer, deal with hallucinations, handle error
    cases, and create a consistent experience. It is real work.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 企业API将大多是专有的，ChatGPT无法直接帮助。然而，由于大多数REST工作应该是相似的，它仍然可能有所帮助。有时，与Zoom、Teams、Slack、Jira、Confluence、Salesforce、HubSpot、ServiceNow、Oracle或其他供应商的集成被用于内部或作为企业产品的一部分。记住，所有这些工作都需要进行身份验证、创建安全层、处理幻觉、处理错误情况，并创建一致的用户体验。这是一项真实的工作。
- en: More robust approaches are evolving. This article on ToolLLM describes an approach
    to using ChatGPT to generate instructions for APIs and then figure out how to
    use them.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 更稳健的方法正在演变。本文在ToolLLM中描述了一种使用ChatGPT生成API指令的方法，然后探讨如何使用它们。
- en: 'Article: [How to use thousands of APIS in LLMs](https://arxiv.org/abs/2307.16789?utm_source=tldrai)
    (ToolLLM paper) ([https://arxiv.org/abs/2307.16789?utm_source=tldrai](https://arxiv.org/abs/2307.16789?utm_source=tldrai))'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 文章：[如何在LLMs中使用数千个API](https://arxiv.org/abs/2307.16789?utm_source=tldrai)（ToolLLM论文）([https://arxiv.org/abs/2307.16789?utm_source=tldrai](https://arxiv.org/abs/2307.16789?utm_source=tldrai))
- en: 'Video: AI News: [An LLM that learns how to work with APIs](https://www.youtube.com/watch?v=lGxaE8FU2-Q)
    (ToolLLM paper) ([https://www.youtube.com/watch?v=lGxaE8FU2-Q](https://www.youtube.com/watch?v=lGxaE8FU2-Q))'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 视频：AI新闻：[一个学习如何与API一起工作的LLM](https://www.youtube.com/watch?v=lGxaE8FU2-Q)（ToolLLM论文）([https://www.youtube.com/watch?v=lGxaE8FU2-Q](https://www.youtube.com/watch?v=lGxaE8FU2-Q))
- en: 'Apply our testing and validation process here as it is for *any* input and
    output testing. As designers, PMs, and people who care about usability, try to
    understand whether the APIs provide the right level of service. Here are some
    items to look for when integrating with backend services:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的测试和验证流程应用于任何输入和输出测试。作为设计师、项目经理和关心可用性的人，尝试了解 API 是否提供了正确的服务水平。在集成后端服务时，以下是一些需要寻找的项目：
- en: Can the required data be supplemented automatically? Users should not have to
    supply every piece of data. For example, the API might need five pieces of data
    to submit a valid request. Some can come from the context and focus the user on
    the essential elements.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否可以自动补充所需数据？用户不应需要提供每一条数据。例如，API 可能需要五条数据来提交一个有效的请求。其中一些可以来自上下文，并让用户专注于关键要素。
- en: Will the response time be fast enough to be integrated with the response? Think
    in milliseconds (200 or less would be good, 50 or less would be great, sub-10s
    are world-class).
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 响应时间是否足够快，可以与响应集成？请以毫秒为单位思考（200 毫秒或更少会很好，50 毫秒或更少会非常棒，低于 10 秒是世界级的）。
- en: Can a single API be called instead of two or three? Optimized API calls help
    with cost, performance, and the number of round trips.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否可以调用单个 API 而不是两个或三个？优化的 API 调用有助于成本、性能和往返次数的数量。
- en: Is the data format consistent with the customer’s needs? If not, consider telling
    ChatGPT how to format it or providing conversations or translations in the correct
    format. For example, understand the user’s time zone and don’t use GMT or other
    time zones.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据格式是否与客户需求一致？如果不一致，考虑告诉 ChatGPT 如何格式化，或者提供正确格式的对话或翻译。例如，了解用户的时区，不要使用 GMT 或其他时区。
- en: Integrations and actions
  id: totrans-392
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成和操作
- en: The ChatGPT economy is growing in leaps and bounds. There are dozens of popular
    services and integrations to make processes more seamless and practical.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 经济正在飞速增长。有数十个流行的服务和集成，使流程更加无缝和实用。
- en: The development team might support other tools to help create a complete solution.
    It would be best to get involved to determine how to apply design thinking and
    your expertise to support a more sustainable process.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 开发团队可能支持其他工具来帮助创建完整的解决方案。最好参与其中，以确定如何应用设计思维和您的专业知识来支持更可持续的过程。
- en: 'There are plenty of libraries, tools, and resources online. Comparing and contrasting
    the wealth of options is out of scope, but a few examples that relate to making
    effective, well-designed solutions that OpenAI posted can be worth your time:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 网上有很多库、工具和资源。比较和对比丰富的选项超出了范围，但一些与 OpenAI 发布的关于制作有效、设计良好的解决方案相关的例子可能值得您花时间：
- en: 'Article: [OpenAI Cookbook](https://cookbook.openai.com/articles/related_resources)
    ([https://cookbook.openai.com/articles/related_resources](https://cookbook.openai.com/articles/related_resources))'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 文章：[OpenAI 烹饪书](https://cookbook.openai.com/articles/related_resources) ([https://cookbook.openai.com/articles/related_resources](https://cookbook.openai.com/articles/related_resources))
- en: 'Article: [LangChain home page](https://www.langchain.com/langsmith) ([https://www.langchain.com/langsmith](https://www.langchain.com/langsmith))'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 文章：[LangChain 主页](https://www.langchain.com/langsmith) ([https://www.langchain.com/langsmith](https://www.langchain.com/langsmith))
- en: 'Article: [Milvus Vector database](https://zilliz.com/blog/customizing-openai-built-in-retrieval-using-milvus-vector-database)
    ([https://zilliz.com/blog/customizing-openai-built-in-retrieval-using-milvus-vector-database](https://zilliz.com/blog/customizing-openai-built-in-retrieval-using-milvus-vector-database))'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 文章：[Milvus 向量数据库](https://zilliz.com/blog/customizing-openai-built-in-retrieval-using-milvus-vector-database)
    ([https://zilliz.com/blog/customizing-openai-built-in-retrieval-using-milvus-vector-database](https://zilliz.com/blog/customizing-openai-built-in-retrieval-using-milvus-vector-database))
- en: The LLM can be just *one* of the services within the entire lifecycle or pipeline.
    This means faults can occur before or after the LLM. Look carefully before placing
    blame on the model. It is only as good as the input and instructions provided.
    Improve the quality of what is shared with the model. Design how to share data
    with the LLM and then test and verify how it works. *Be fully committed to an
    iterative lifecycle to make successful generative AI solutions.* Quality is all
    about the care and feeding process. Since improvements are only improvements if
    we measure them, this is explored in [*Chapter 10*](B21964_10_split_000.xhtml#_idTextAnchor216)*,
    Monitoring and Evaluation*. Ragas is one of those tools to consider using to measure
    how the RAG solution is performing. If this excites you, check it out now.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）可以是整个生命周期或流程中的**一个**服务。这意味着错误可能发生在LLM之前或之后。在责怪模型之前仔细检查。它的好坏取决于提供的输入和指令。提高与模型共享的内容的质量。设计如何与LLM共享数据，然后测试和验证其工作情况。**要全身心投入到迭代的生命周期中，以制作成功的生成式AI解决方案**。质量完全关乎照顾和喂养过程。由于只有当我们衡量它们时，改进才算改进，因此这在[*第10章*](B21964_10_split_000.xhtml#_idTextAnchor216)*，监控和评估*中进行了探讨。Ragas是那些可以考虑使用的工具之一，用于衡量RAG解决方案的性能。如果您对此感兴趣，现在就试试看。
- en: 'Link: [Ragas Documentation](https://docs.ragas.io/en/latest/) ([https://docs.ragas.io/en/latest/](https://docs.ragas.io/en/latest/))'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 链接：[Ragas文档](https://docs.ragas.io/en/latest/) ([https://docs.ragas.io/en/latest/](https://docs.ragas.io/en/latest/))
- en: ChatGPT has a concept called actions (formerly called plugins). These allow
    ChatGPT to connect to the rest of the internet. Actions rely on function calling
    to perform these actions. Recall that the Wove example used function calling.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT有一个称为操作（以前称为插件）的概念。这些允许ChatGPT连接到互联网的其余部分。操作依赖于函数调用以执行这些操作。回想一下，Wove示例使用了函数调用。
- en: 'Documentation: [Actions in GPTs (Calling APIs)](https://platform.openai.com/docs/actions/introduction)
    ([https://platform.openai.com/docs/actions/introduction](https://platform.openai.com/docs/actions/introduction))'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 文档：[GPTs中的操作（调用API）](https://platform.openai.com/docs/actions/introduction)
    ([https://platform.openai.com/docs/actions/introduction](https://platform.openai.com/docs/actions/introduction))
- en: What is impressive is that developers do not have to write these API queries
    by hand. ChatGPT has a bespoke LLM tuned to help developers write actions.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 令人印象深刻的是，开发者不必手动编写这些API查询。ChatGPT有一个定制的LLM，专门调整以帮助开发者编写操作。
- en: 'Demo: [ActionsGPT chat](https://chatgpt.com/g/g-TYEliDU6A-actionsgpt) ([https://chatgpt.com/g/g-TYEliDU6A-actionsgpt](https://chatgpt.com/g/g-TYEliDU6A-actionsgpt))'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 演示：[ActionsGPT聊天](https://chatgpt.com/g/g-TYEliDU6A-actionsgpt) ([https://chatgpt.com/g/g-TYEliDU6A-actionsgpt](https://chatgpt.com/g/g-TYEliDU6A-actionsgpt))
- en: Developers can send messages to the LLM to generate the base code. For example,
    they can try something like this.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者可以向LLM发送消息以生成基础代码。例如，他们可以尝试类似以下的内容。
- en: '[PRE18]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Share these resources and this video with developers to help them get started.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些资源和视频与开发者分享，以帮助他们开始使用。
- en: 'Video: [Introduction to ChatGPT Actions](https://www.youtube.com/watch?v=pq34V_V5j18)
    ([https://www.youtube.com/watch?v=pq34V_V5j18](https://www.youtube.com/watch?v=pq34V_V5j18))
    (Actions start at 9:30)'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 视频：[ChatGPT操作简介](https://www.youtube.com/watch?v=pq34V_V5j18) ([https://www.youtube.com/watch?v=pq34V_V5j18](https://www.youtube.com/watch?v=pq34V_V5j18))（操作从9:30开始）
- en: Figuring out these linkages is for development. As a product leader, know that
    a wealth of services is available for integration from enterprise sources to make
    the solution support intelligence that combines these. To create these connections,
    the paid version of ChatGPT, if not the enterprise version, is needed. In the
    ChatGPT video, Nick Turley hooks up his personal to-do list from asana.com to
    the chat instance in the demo. *Figure 6**.14* shows the current actions setup.
    It is a simple UI to name, describe, and define the instructions.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 确定这些联系是开发工作。作为产品领导者，要知道从企业来源到集成这些解决方案以支持结合智能的丰富服务都是可用的。要创建这些连接，需要ChatGPT的付费版本，如果不是企业版本。在ChatGPT视频中，Nick
    Turley将他的个人待办事项列表从asana.com连接到演示中的聊天实例。*图6**.14*显示了当前的操作设置。这是一个简单的UI，用于命名、描述和定义指令。
- en: '![](img/B21964_06_14.jpg)'
  id: totrans-410
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21964_06_14.jpg)'
- en: Figure 6.14 – Setting up actions on the Configure tab
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 – 在配置选项卡上设置操作
- en: The demo goes further with embedding knowledge to help with summarizing. Watch
    to get a good sense of the integrations fundamental to enterprise solutions. At
    the 20-minute mark, it gets a little creative with a mood demo. They do a great
    demo of integrating with physical devices in the demo room and Spotify to play
    music. The point is that enterprise solutions can be more than just software integrations.
    Manufacturing, lighting, HVAC (air conditioning), processes, routing, planning,
    and more can be improved with intelligent integration. This takes us back to our
    chapter on use cases. There are lots of opportunities out there.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 演示进一步通过嵌入知识来帮助总结。观看以获得对企业解决方案基本集成的良好感觉。在20分钟时，它通过情绪演示变得有些创意。他们在演示室中展示了与物理设备的集成，以及Spotify播放音乐。重点是，企业解决方案可以不仅仅是软件集成。制造、照明、HVAC（空调）、流程、路由、规划等等都可以通过智能集成得到改善。这让我们回到了我们的用例章节。那里有很多机会。
- en: This book is intended to be practical even when the tools change—and they will—so
    don’t get hung up on a tool or direction. New and more robust services will be
    introduced frequently. Build a lean process that supports adaptation and change.
    Learn from the LLM community. The number of blogs, posts, and training opportunities
    is expanding daily.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书旨在即使工具发生变化时也能保持实用性——它们肯定会发生变化——因此不要纠结于某个工具或方向。新服务和更强大的服务将频繁推出。构建一个支持适应和变化的精益流程。从LLM社区中学习。博客、帖子和学习机会的数量每天都在增加。
- en: Community resources
  id: totrans-414
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 社区资源
- en: A wealth of resources exists, including in the OpenAI community. Explore these
    resources, the latest videos, and research to get up to speed.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 资源丰富，包括在OpenAI社区中。探索这些资源，最新的视频和研究，以跟上进度。
- en: 'Article: [RAG Community Discussion](https://community.openai.com/t/rag-is-not-really-a-solution/599291/2)
    ([https://community.openai.com/t/rag-is-not-really-a-solution/599291/2](https://community.openai.com/t/rag-is-not-really-a-solution/599291/2))'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 文章：[RAG社区讨论](https://community.openai.com/t/rag-is-not-really-a-solution/599291/2)
    ([https://community.openai.com/t/rag-is-not-really-a-solution/599291/2](https://community.openai.com/t/rag-is-not-really-a-solution/599291/2))
- en: In Ron Parker’s post, he discusses RAG as being brittle.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 在Ron Parker的帖子中，他讨论了RAG的脆弱性。
- en: '"The biggest problem I’ve run into so far is that some query responses are
    not comprehensive enough. End-users can almost always get a complete answer using
    chain-of-thought queries (few-shot). But, the end-users I’ve been working with
    want complete answers to the first question (zero-shot). This may touch on your
    issue.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: “到目前为止，我遇到的最大问题是某些查询响应不够全面。最终用户几乎总是可以通过思维链查询（少样本）获得完整的答案。但是，我一直在合作的最终用户希望对第一个问题（零样本）获得完整的答案。这可能会触及你的问题。”
- en: 'My resolution: Deep Dive. Have the model dig through all the possible responses,
    categorize and analyze those, and then return a complete list of the best responses.
    Since I built my RAG system, I must also develop this feature. So I’m thinking,
    whatever you say this technique is you’re missing, you may have to build it yourself."'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 我的解决方案：深入挖掘。让模型深入挖掘所有可能的响应，对这些进行分类和分析，然后返回最佳响应的完整列表。由于我构建了我的RAG系统，我必须开发这个功能。所以我在想，无论你说的这种技术是什么，你可能必须自己构建它。"
- en: This makes our point. This is about building solutions, and this puzzle has
    many pieces. He also points out a good usability issue. Users don’t want to have
    conversations to get to their answers. They want the complete answer on the first
    try. Even in our case study with Wove, they worked hard to return the best responses
    and iterate on the training to get the correct answers. They had to figure out
    the model and chunk the rate sheets, and then they refined those answers to improve
    the model. Again, it involves honest work by the development team; work with them
    to improve the quality with every step.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了我们的观点。这是关于构建解决方案的，这个谜题有很多拼图。他还指出了一个很好的可用性问题。用户不想通过对话来获取答案，他们希望在第一次尝试就能得到完整的答案。即使在我们的Wove案例研究中，他们也努力返回最佳响应并迭代训练以获得正确答案。他们必须弄清楚模型并分割数据表，然后他们细化这些答案以提高模型。再次强调，这涉及到开发团队的辛勤工作；与他们合作，每一步都提高质量。
- en: Or, check out a nice video showing how Mayo Oshin (**@maywaoshin**) used GPT-4
    to front-end thousands of pages from PDF documents—in this case, the last few
    years of Tesla’s annual reports. He walks through his architecture. Most of that
    will be too much, but he talks about how he converts documents into text and chunks
    of documents. This discussion is right on point for us.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，查看一个展示Mayo Oshin（**@maywaoshin**）如何使用GPT-4将PDF文档的前端转换为数千页的精彩视频——在这种情况下，是特斯拉过去几年的年度报告。他介绍了他的架构。其中大部分内容可能过于复杂，但他谈论了如何将文档转换为文本和文档块。这次讨论与我们非常相关。
- en: 'Video: [Example of 1000+ Pages of PDF](https://www.youtube.com/watch?v=Ix9WIZpArm0)
    ([https://www.youtube.com/watch?v=Ix9WIZpArm0](https://www.youtube.com/watch?v=Ix9WIZpArm0))'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 视频：[1000+ 页PDF的示例](https://www.youtube.com/watch?v=Ix9WIZpArm0) ([https://www.youtube.com/watch?v=Ix9WIZpArm0](https://www.youtube.com/watch?v=Ix9WIZpArm0))
- en: The last resource for our discussion is the previously mentioned video on lessons
    learned around RAG.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的最后一个资源是之前提到的关于RAG经验教训的视频。
- en: 'Video: [Lessons Learned on LLM RAG Solutions](https://www.youtube.com/live/Y9qn4XGH1TI?si=iUs_x3yDL8BK7aUb56)
    ([https://www.youtube.com/live/Y9qn4XGH1TI?si=iUs_x3yDL8BK7aUb56](https://www.youtube.com/live/Y9qn4XGH1TI?si=iUs_x3yDL8BK7aUb56))'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 视频：[LLM RAG 解决方案的经验教训](https://www.youtube.com/live/Y9qn4XGH1TI?si=iUs_x3yDL8BK7aUb56)
    ([https://www.youtube.com/live/Y9qn4XGH1TI?si=iUs_x3yDL8BK7aUb56](https://www.youtube.com/live/Y9qn4XGH1TI?si=iUs_x3yDL8BK7aUb56))
- en: 'They cover a variety of good things about managing documents. Here is a summary:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 他们涵盖了关于文档管理的各种好处。以下是一个总结：
- en: They remind everyone to do good “data science” and make sure to have good data
    going in. Also, that accurate data is messy. Not only are tables tricky (something
    discussed in this chapter), but different document formats can need different
    libraries to help clean them (or the headache of manual cleaning).
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们提醒每个人要做好“数据科学”，并确保输入的数据质量良好。同时，准确的数据可能很杂乱。不仅表格很棘手（本章中讨论过），不同的文档格式可能需要不同的库来帮助清理它们（或者手动清理的头痛）。
- en: Explanations might not be directly linked to information. Comments or notes
    around negation ("does not have", "except for this version", "does not apply")
    can negate some documentation that does not even appear in a chunk that it relates
    to and might only be understood with additional editing or tagging.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释可能不会直接与信息相关联。关于否定（“没有”，“除了这个版本”，“不适用”）的注释或笔记可以否定一些甚至没有出现在相关块中的文档，可能只有通过额外的编辑或标记才能理解。
- en: Maintain structure. Convert documents to a data structure while preserving their
    meaning. For example, when PDFs are converted to text, a model can decide how
    to parse the PDFs, identify headings, and build out and capture information to
    put it into a meaningful structure.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持结构。在保留其意义的同时将文档转换为数据结构。例如，当PDF转换为文本时，模型可以决定如何解析PDF，识别标题，并构建和捕获信息，以便将其放入有意义的结构中。
- en: If the documents are hierarchical, a flat representation is needed, so try to
    get to a list of elements. The key for a part of the element can represent a section.
    This kind of discussion is more for the data ingestion team, but look for these
    issues when testing. This way, test results can verify that context is maintained.
    Help the data scientists maintain quality.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果文档是分层的，则需要一个扁平化的表示，因此尽量获取一个元素列表。元素的一部分的关键可以代表一个章节。这类讨论更多的是针对数据摄入团队，但在测试时寻找这些问题。这样，测试结果可以验证上下文是否得到保持。帮助数据科学家保持数据质量。
- en: Different methods result in different quality. As discussed, sentence-by-sentence
    embedding will lose some context.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的方法会导致不同的质量。正如讨论的那样，逐句嵌入会丢失一些上下文。
- en: As discussed, get the proper context in chunks. It should not be too narrow
    or too broad; it should be *just right*. The approach in *Figure 6**.15* reminds
    us of what was discussed at the start of the chapter.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如讨论的那样，以块的形式获取适当的上文。它不应该太窄或太宽；应该是“恰到好处”。图6.15中的方法提醒我们本章开头讨论的内容。
- en: '![](img/B21964_06_15.jpg)'
  id: totrans-432
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21964_06_15.jpg)'
- en: Figure 6.15 – Prolego’s approach is similar to our discussion earlier in the
    chapter
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 – Prolego的方法与本章早些时候的讨论类似
- en: Several videos from Prolego (besides the one shared) are easy to digest and
    well-paced. This is just one example of the wealth of video and article resources
    that can help on a RAG journey. Don’t build everything yourself; the LLM vendors
    are only one piece of a more extensive solution that includes tools, documentation,
    databases, and APIs.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 除了分享的之外，Prolego的几个视频易于消化，节奏适中。这只是众多视频和文章资源的一个例子，这些资源可以帮助你在RAG（Retrieval-Augmented
    Generation，检索增强生成）的旅程中。不要自己构建一切；LLM供应商只是更广泛解决方案的一部分，该解决方案包括工具、文档、数据库和API。
- en: Besides RAG, there are wonderful posts on every imaginable topic around LLMs
    on LinkedIn, shared in mailing lists, posted on YouTube, classes from universities,
    and vendor websites.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 除了RAG之外，LinkedIn上关于LLMs（大型语言模型）的每个可想象的主题都有精彩的帖子，这些帖子在邮件列表中共享，发布在YouTube上，来自大学的课程，以及供应商网站上。
- en: Summary
  id: totrans-436
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: It is a big step to prepare an existing knowledge base and data sources to make
    them available within a generative AI solution. It’s likely the most significant
    step because the hard work of creating the ChatGPT model was done for you. For
    many enterprise solutions, this can be an overwhelming task. Just start small.
    Learn from the use cases to prioritize solutions that provide the most significant
    value with the least cost (recall our scoring discussion in [*Chapter 4*](B21964_04.xhtml#_idTextAnchor085),
    *Scoring Stories*). Over time, land grabs can expand into other data sources and,
    thus, new use cases. All of this has to be done with quality in mind. Measuring
    and monitoring are critical. Newer doesn’t mean better. Mix and match ChatGPT
    models to perform specific tasks or optimize cost or performance by using one
    model over another. Use a collection of third-party resources—possibly even other
    models tuned to a particular problem space—to refine results, make data available
    to the model, or do additional integrations. Be aware of the impact of data cleaning
    and how the knowledge in the base model might impact the solution’s decision-making
    ability. Recognize that bias isn’t just about social or political positions; it
    can simply be about having too much data about one product, and this causes the
    model to miss smaller products. Getting all of this right with the enterprise
    data is a challenge.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 准备现有的知识库和数据源，使其在生成式AI解决方案中可用是一个巨大的步骤。这可能是最重要的步骤，因为创建ChatGPT模型的辛勤工作已经为你完成了。对于许多企业解决方案来说，这可能是一项艰巨的任务。只需从小处着手。从用例中学习，优先考虑那些以最低成本提供最大价值的解决方案（回想一下我们在[*第4章*](B21964_04.xhtml#_idTextAnchor085)，*评分故事*）中的评分讨论）。随着时间的推移，土地抢夺可以扩展到其他数据源，从而产生新的用例。所有这些都必须以质量为前提。衡量和监控至关重要。新不一定意味着更好。混合使用ChatGPT模型来执行特定任务，或者通过使用一个模型而不是另一个模型来优化成本或性能。使用第三方资源集合——甚至可能是针对特定问题空间调整的其他模型——来细化结果，使数据可供模型使用，或进行额外的集成。要意识到数据清洗的影响以及基础模型中的知识如何可能影响解决方案的决策能力。认识到偏见不仅仅是关于社会或政治立场；它可能只是关于对某一产品有太多数据，这导致模型错过了较小的产品。在企业数据中正确完成所有这些是一个挑战。
- en: This chapter mainly focused on awareness and considered how techniques can influence
    the quality of inbound data sources. On the outbound side, testing and completing
    the feedback loop is a great way to improve the solution. There should be many
    opportunities to contribute before moving on to the next steps around prompt engineering
    and fine-tuning.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 本章主要关注意识，并考虑了技术如何影响输入数据源的质量。在输出方面，测试和完成反馈循环是提高解决方案的绝佳方式。在进入提示工程和微调的下一步之前，应该有众多贡献的机会。
- en: References
  id: totrans-439
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '| ![](img/B21964_06_16.jpg) | The links, book recommendations, and GitHub files
    in this chapter are posted on the reference page.Web page: [Chapter 6 References](https://uxdforai.com/references#C6)
    ([https://uxdforai.com/references#C6](https://uxdforai.com/references#C6)) |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| ![图片](img/B21964_06_16.jpg) | 本章中提到的链接、书籍推荐和GitHub文件都发布在参考文献页面上。网页：[第6章参考文献](https://uxdforai.com/references#C6)
    ([https://uxdforai.com/references#C6](https://uxdforai.com/references#C6)) |'
