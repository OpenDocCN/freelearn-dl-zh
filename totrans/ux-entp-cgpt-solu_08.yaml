- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gathering Data – Content is King
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is an assumption in this book: enterprise ChatGPT solutions are needed
    in almost all cases because a company has something unique to offer its customers,
    and it possesses an exceptional understanding of its products, services, and content.
    This content is private or unique and thus not part of **large language models**
    (**LLMs**) built from scraping the internet. Models are built on crawling the
    2+ billion pages of web content to teach the model. A third party, Commoncrawl.org,
    is commonly cited as a primary source of this material for major models (GPT-3,
    Llama). These models, which are massive collections of text, learn the statistical
    relationships of words and concepts and can be used to predict and respond to
    questions. Creating a model can take months; most have billions of connections
    and words. When customers come to the enterprise for answers, the models must
    include enterprise content that is not part of this crawl to make them unique,
    secure, and more accurate. This is done with the expectation that the solutions
    will be up to date, optimized to be cost-effective, and less prone to hallucinations
    or lying, as some call it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter addresses gathering data for the LLM and how to include enterprise
    data sources in LLM solutions using a method called **Retrieval Augmented Generation**
    (**RAG**). We’ll discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is in a ChatGPT foundational model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporating enterprise data using RAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resources for RAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter and the next few will be more technical for those who have reached
    this point and are focused on user-centered design concepts. The chapter covers
    all the ideas and provides access to additional videos and online resources. The
    book does not require most of these external resources; they are meant to give
    more details.
  prefs: []
  type: TYPE_NORMAL
- en: What is in a ChatGPT foundational model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When an LLM is built, it is trained on sources of data from the internet. It
    knows publicly available information about companies and products. If asked typical
    enterprise-like questions, it can get robust answers – sometimes better than what
    is available from some vendors’ websites. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Try these questions out and notice a trend. Each answer is slightly more generic
    than the previous one, and that generic nature is part of the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following applies to most foundational models such as ChatGPT 3.5 or 4o,
    Anthropic’s Claude, Meta’s Llama, or Mistral7B:'
  prefs: []
  type: TYPE_NORMAL
- en: Don’t understand specific business or use context or complex products
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t have customer history or context to consider
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can’t access proprietary knowledge sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are not trained on service requests or other service data and won’t know correct
    assumptions from incorrect assumptions and inaccurate solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can’t integrate with databases or APIs for retrieval and task performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can’t be scaled or tuned for multi-tenancy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now imagine those questions if they were in the context of rich business knowledge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Integrating data sources with ChatGPT to contextualize the solution to the business
    can address these richer questions. No matter the design pattern used, such as
    a chat UI, a hybrid experience, or a standalone recommender, enterprise data will
    make the solution powerful. Foundational models gain access to knowledge with
    Retrieval Augmented Generation or **RAG**.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating enterprise data using RAG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are other ways of taking data and making it part of an LLM. It is possible
    to build a foundation model, but as mentioned, the training time and effort are
    extreme. Even with RAG, there are different approaches. Some technical resources
    are shared, but this chapter will focus on and teach RAG understanding and how
    product people can contribute to the development process. First, a RAG explanation.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RAG supplements LLMs with enterprise data. RAG is a technique for retrieving
    information, such as from a knowledge base, and it can generate responses from
    authoritative knowledge collection with coherent and contextually accurate answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'This methodology allows us to overcome some generic model problems:'
  prefs: []
  type: TYPE_NORMAL
- en: Material is always up to date since it is evaluated when prompted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools can reference the document source and, thus, are more trustworthy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The foundational model is already trained, so supplementing it is inexpensive
    compared to building a model from scratch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It allows for a robust set of resources (APIs, SQL databases, and various document
    and presentation file formats) to continue to be managed independently (and still
    available to other solutions).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It will *NOT* be used to throw *ALL* data into the LLM. A mechanism will be
    used to send relevant documents to the LLM just in time for processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It allows for unique, secure answers with multiple customers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical work is needed to create a RAG pipeline. Even if this book isn’t about
    the development effort to create a RAG pipeline, it still stands to reason that
    a basic understanding of how data becomes valuable to the ChatGPT solution is
    needed. First, consider what would happen *if* all the enterprise data is thrown
    into the LLM. It would look something like *Figure 6**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – A model where we add all our knowledge directly to the LLM
  prefs: []
  type: TYPE_NORMAL
- en: The model in *Figure 6**.1* assumes it can handle all company knowledge and
    include it in an OpenAI model, resulting in a custom company model. This sounds
    right, but the cost and *months* it takes to create it are very high. There needs
    to be a way for the LLM to access all of our resources without the cost and complexity.
    Let’s review some limitations to find a solution to this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of ChatGPT and RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To be clear, there are two kinds of limitations worth discussing. The first
    is the limitations of knowledge retrieval using OpenAI models or any models. In
    contrast, the second is the limitations of RAG, even when integrating third-party
    solutions to build an enterprise RAG solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most enterprise solutions will find data integration requirements within OpenAI
    limiting and look elsewhere for scalability, cost, and performance. With OpenAI
    File Search, which is their way of augmenting the LLM with knowledge, there are
    technical limits:'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum file size of 512 MB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A limit of 5M tokens (up from 2M in the spring of 2024)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ChatGPT Enterprise supports a context length of 128K (up from 32K in the free
    version and the first Enterprise release)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some limits on file formats (**.pdf**, **.md**, **.docx**) – the complete list
    is here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Documentation: [Supported file formats](https://platform.openai.com/docs/assistants/tools/file-search/supported-files)
    ([https://platform.openai.com/docs/assistants/tools/file-search/supported-files](https://platform.openai.com/docs/assistants/tools/file-search/supported-files))'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage fees are $0.20/GB per assistant per day.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These limits (as of September 2024) will change with some frequency. These limitations
    mean third parties are needed for solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also some quality limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: Model data is available to everyone who has access to the model. Security barriers
    or limits are not implicit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It won’t differentiate between general knowledge and internal knowledge. There
    are weights and an ability to prioritize and emphasize material, but it can still
    hallucinate without good reason.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When changes are made to knowledge, retraining is required and expensive. Since
    results need to be accurate and timely, this becomes a show-stopper.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI’s File Search for knowledge retrieval handles one part of the process
    and doesn’t have the additional value of RAG around scale and data input types.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a limit to how much can be shared with the LLM at one time. This is
    called the context window, and this chapter covers how to chunk information to
    fit into that context window. The larger the context window, the more knowledge
    and enterprise data can be shared with the LLM at one time to formulate answers.
    As the window grows larger, less RAG is needed to pre-fetch material. RAG is a
    more scalable and cost-effective approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third-party solutions help avoid these limitations. To demo and understand the
    space for this book, the OpenAI built-in tools will work for the small demos.
    However, an enterprise solution will work with a third-party app for a production
    instance. The knowledge gained from these chapters is relevant to any LLM.
  prefs: []
  type: TYPE_NORMAL
- en: It is good to start with OpenAI’s built-in capabilities using the playground,
    so no coding is needed. No need to go to the documentation right now, but it is
    included anyway. This approach allows us to get a taste of custom models without
    the overhead required by a complete enterprise solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [OpenAI’s File Search Documentation](https://platform.openai.com/docs/assistants/tools/file-search/quickstart)
    ([https://platform.openai.com/docs/assistants/tools/file-search/quickstart](https://platform.openai.com/docs/assistants/tools/file-search/quickstart))'
  prefs: []
  type: TYPE_NORMAL
- en: It takes significant work to go from data (this chapter) to [*Chapter 7*](B21964_07.xhtml#_idTextAnchor150),
    *Prompt Engineering*, and then to the next steps shown in [*Chapter 8*](B21964_08.xhtml#_idTextAnchor172),
    *Fine-Tuning* so that a solution can be reviewed with [*Chapter 9*](B21964_09_split_000.xhtml#_idTextAnchor190),
    *Guidelines and Heuristics*, and then analyzed for success in [*Chapter 10*](B21964_10_split_000.xhtml#_idTextAnchor216),
    *Monitoring and Evaluation*. For more resources, visit the OpenAI cookbook. It
    has a wealth of articles covering the entire LLM lifecycle, gives many good explanations
    and definitions, and lays out the process. Here is one good article.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [An OpenAI cookbook article on Fine-Tuning for RAG using Qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant)
    ([https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant))'
  prefs: []
  type: TYPE_NORMAL
- en: The article is technical, but the concepts reinforce the learnings from this
    book. There needs to be a way to provide material to the model that doesn’t require
    retraining and can handle the scale of the enterprise problem. This is done by
    implementing a form of RAG that works off of an index of the knowledge and provides
    only relevant material, as needed, to the LLM. **Indexing** is a way to organize
    information for fast retrieval and comparison. There is not only one way to do
    this, but we’ll look at the basic approach to form an understanding of RAG. Some
    of the steps are beyond the scope of this book. Anyone reading is unlikely to
    build an LLM from scratch. Product people, especially those responsible for the
    knowledgebase or database resources, can improve the data coming into the solution
    to provide the indexing and LLM with the best chance of returning high-quality
    results. So, technology is introduced to handle the scale, performance, and quality
    needed. See *Figure 6**.2*. This requires us to focus on getting data into shape
    for indexing. In this approach, only relevant information is shared with the LLM
    to develop answers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Introducing the RAG solution to assist in the question-to-answer
    process
  prefs: []
  type: TYPE_NORMAL
- en: This is dramatically oversimplifying the process. The indexing icon shown is
    a set of processes that result in a *limited* number of documents to share with
    the LLM as context for the question (this context is shared within the prompt
    – this prompt being the instructions shared with the LLM). The ingestion process
    includes cleaning the data, converting it to text, and creating vector representations
    to match the question’s vector representation against the indexed resources. This
    indexing process organizes the data to match like to like. **Vectorization** is
    the process of converting text into numeric vectors. **Embedding** is the process
    of determining similarity and semantic relationships based on the similarities
    of the vectors. All of the processing and matching is based on matching these
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: To simplify the concept, consider vectors as numbers with a direction, like
    going west for 5 miles versus going north for 12 miles. The direction and magnitude
    in this example are two dimensions used to match results. However, in the case
    of LLMs, there are thousands of dimensions. The embedding process sees that similar
    vectors represent words with similar meanings and usage. They are in the same
    area. The best matches (north-west for 4 miles is roughly similar to going west
    for 5 miles) are then passed to the LLM for processing with the question. This
    means *the LLM is given limited information to generate its answer*. This also
    means there is value in ensuring that the knowledge and resources are ready for
    this process and using tools to return knowledge related to the presented question.
    This doesn’t require months to train a model. All of that work was done for us.
    However, the foundational model can be enhanced with prompt engineering and fine-tuning.
    **Prompt engineering** is the process of giving instructions to the model to tell
    it what to do, while **fine-tuning** is used to provide examples of what is expected
    from the generative output. Both are covered in the following two chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Product owners, designers, writers, and those who care about content quality
    can add value to the input and the output. This chapter is about input and getting
    quality out of data sources. The following chapters will focus on the *output*
    to ensure accuracy when answering.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading on RAG
  prefs: []
  type: TYPE_NORMAL
- en: There are plenty of good resources to explain RAG in more detail. Here are a
    few deeper dives into the subject. Let me start with Amazon’s introduction to
    RAG.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Amazon’s RAG Explanation](https://aws.amazon.com/what-is/retrieval-augmented-generation/)
    ([https://aws.amazon.com/what-is/retrieval-augmented-generation/](https://aws.amazon.com/what-is/retrieval-augmented-generation/))'
  prefs: []
  type: TYPE_NORMAL
- en: This one goes deeper into the issues and technical pieces of the complete solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Leveraging LLMs on your domain-specific knowledge base](https://www.ml6.eu/blogpost/leveraging-llms-on-your-domain-specific-knowledge-base)
    ([https://www.ml6.eu/blogpost/leveraging-llms-on-your-domain-specific-knowledge-base](https://www.ml6.eu/blogpost/leveraging-llms-on-your-domain-specific-knowledge-base))'
  prefs: []
  type: TYPE_NORMAL
- en: Databricks hosted an excellent one-hour video session. It covers prompt engineering
    and RAG.
  prefs: []
  type: TYPE_NORMAL
- en: 'Video: [Accelerate your Generative AI journey](https://vimeo.com/891439013)
    ([https://vimeo.com/891439013](https://vimeo.com/891439013))'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to go deeper, review this well-done survey of RAG techniques and methods
    to learn more about how RAG can be implemented. This is my favorite reference
    for explaining the different approaches, and the authors plan on updating the
    article, so it should be current.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [A survey of RAG for LLMs](https://arxiv.org/pdf/2312.10997.pdf) ([https://arxiv.org/pdf/2312.10997.pdf](https://arxiv.org/pdf/2312.10997.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: By the process of elimination, there are only a few places where product people
    can insert themselves to help the process. Few can build an LLM from scratch,
    and the training data used in the base model is from billions of Internet records.
    There is limited ability to coach the customer on what questions to ask (a good
    design might encourage good behavior without forcing the user to adapt, per se).
    Meanwhile, in recommender UIs, there is no interactive UI.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the best value for our efforts is to target the proper use cases, create
    quality knowledge, and support robust access to enterprise databases and resources
    that will allow an LLM to generate results to achieve customer goals. Let’s build
    a simple demo incorporating a data source to help understand the limitations and
    capabilities of an LLM supplemented with private data.
  prefs: []
  type: TYPE_NORMAL
- en: Building a demo with enterprise data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a simple example to make a point. We will start with a collection of
    Frequently Asked Questions (FAQs) common to almost all websites and businesses.
    Hundreds of FAQs that could be found on any financial website (a bank or a brokerage
    company) form the basis of the demo. We name this financial company Alligiance
    (All-i… not an e, so as not to run afoul of an actual company called Allegiance).
    The assistant can be called “Alli” (pronounced Ally). Let’s start with a file
    of raw HTML snippets, answering each question in a row in a table. The file is
    on GitHub, so please try it yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'GitHub: [FAQ Collection for Testing](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Example_FAQs_for_Demo.docx)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Example_FAQs_for_Demo.docx](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Example_FAQs_for_Demo.docx))'
  prefs: []
  type: TYPE_NORMAL
- en: To access the OpenAI playground, follow the instructions in [*Chapter 1*](B21964_01.xhtml#_idTextAnchor016),
    *Recognizing the Power of Design* *in ChatGPT*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Demo: [OpenAI Playground](https://platform.openai.com/playground) ([https://platform.openai.com/playground](https://platform.openai.com/playground))'
  prefs: []
  type: TYPE_NORMAL
- en: The demo starts by asking a simple, specific question about browser support
    that might be common for a private website application. The foundational model
    would not expect it to know the answer, as these FAQs for a company might only
    be for authenticated customers. Then we uploaded the file, as shown in *Figure
    6**.3*, and asked again. Play along, will you?
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: 'Try side-by-side comparisons: Open two browsers and run the LLM with and without
    the context document. There is also a compare button in the Playground. We will
    demonstrate the compare button later.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – OpenAI Playground shows an answer before and after adding the demo
    file
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*Table 6.1* compares the HTML source to the answer provided.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Source Answer (HTML** **formatted version)** | **ChatGPT** **3.5 Answer**
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Alligiance’s website is best viewed, and is most secure, using the latest
    versions of these browsers. Get free** **upgrades here:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Microsoft® Internet Explorer (Windows 7 & 8 operating** **systems only)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google® Chrome**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mozilla® Firefox**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apple® Safari® (Mac operating** **systems only)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Note: Our latest site features will not work with older, unsupported browser
    versions, specifically Internet Explorer 8, Safar****i 4 and 5, and Netscape.
    Internet Explorer 9 and 10 may provide a less than optimal experience. The use
    of unsupported browsers may also lead to vulnerability because of viruses, spyware,
    malware, and other** **security issues.****Safari no longer supports upgrades
    for Windows operating systems. If you are using a Windows operating system, please
    use one of the supported browsers above, other than Apple® Safari, for the** **best
    experience.**How to upgrade Apple® Safari**If you’re using an older version of
    Safari on a Mac, you’ll need to first check your Apple** **operating system:****From
    the Apple menu, choose “About** **This Mac.”****This continues for pages, see
    GitHub for the** **full text.** | Alligiance.com is best viewed and most secure
    using the latest versions of the following browsers:'
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft® Internet Explorer (Windows 7 & 8 operating systems only)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google® Chrome
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mozilla® Firefox
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apple® Safari® (Mac operating systems only)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s important to note that the latest site features will not work with older
    unsupported browser versions, specifically Internet Explorer 8, Safari 4 and 5,
    and Netscape. Internet Explorer 9 and 10 may provide a less than optimal experience,
    and the use of unsupported browsers may lead to vulnerability due to viruses,
    spyware, malware, and other security issues[1].Additionally, for troubleshooting,
    if images on a page do not display, users can try clicking the Reload/Refresh
    button at the top of the browser or clearing the browser’s cache. It’s also noted
    that if the links to the images are no longer active, they will not be downloadable[2].[1][2]
    |
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.1 – Comparison of source document text versus the ChatGPT answer
  prefs: []
  type: TYPE_NORMAL
- en: 'This example was run with gpt-3.5-turbo-1106\. It looks pretty good. Notice
    that it is not an exact quote of the source material. Here are some usability
    considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: The paragraph starting with “Additionally, for troubleshooting…” comes from
    a different FAQ and is unnecessary for this answer. Subsequent runs of this same
    question did not contain this spurious content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The source answer should be shorter for a chat, which is common for documentation.
    [*Chapter 7*](B21964_07.xhtml#_idTextAnchor150)*, Prompt Engineering*, explains
    how to control this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The style and tone are good. They are consistent with a business tone. Even
    though the content is in a business tone, style and tone could be adjusted using
    Prompt Engineering and Fine-Tuning. [*Chapter 8*](B21964_08.xhtml#_idTextAnchor172),
    *Fine-Tuning*, explores using examples to train the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The list of browsers is returned in a bulleted list. Subsequent runs of this
    same question only sometimes returned this in a bullet list. Prompt engineering
    can also help return items like a bulleted list more consistently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy is critical in support applications. This answer is factually correct,
    but errors can occur. The next few chapters will discuss techniques for getting
    accurate answers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hopefully, this analysis is helpful. There may be other items you can see. One
    must be able to evaluate response quality to care and feed the LLM. This is the
    crux of our mission.
  prefs: []
  type: TYPE_NORMAL
- en: Quality issues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following prompt was provided to the OpenAI model with the FAQ files attached.
    These instructions set the stage for any user interactions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The results around a few security questions are interesting. We will show some
    conversations. The convention is to show users messages on the right and the model’s
    response on the left, similar to the format for messages on your phone. As a reminder,
    conversations are never edited for typos or mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The answer is perfectly reasonable and even logical. Except *none* of this comes
    from the corpus of FAQs that were uploaded. It is a good-sounding generic answer.
    The answer is not something it was trained on from our knowledge. This is classified
    as a **hallucination**. A hallucination is a model-generated text that is incorrect,
    nonsensical, or, in this case, not real. Lying to a customer can cause actual
    harm beyond just the apparent failure. Given the litigious nature of people, avoid
    getting sued for lying to customers. The courts in the US can hold the assistant
    liable as a representative of the company.
  prefs: []
  type: TYPE_NORMAL
- en: Since no data in the files suggests that Alligiance does regular security audits,
    the response doesn’t provide more details because it only refers to “typical”
    measures, not specifics. If the prompt is adapted to “only provide answers from
    the attached document,” then the LLM answers are similar to the browser answer
    because it discusses how unsupported browsers can have security issues. It is
    typical in an enterprise solution to limit the knowledge only to the company knowledge
    provided. This can reduce hallucinations. HTML files were provided, but it returned
    clean, formatted text. Not every system and process would be that fortunate. When
    scaling up, consider what it means to clean the enterprise data. In the end, all
    of these systems expect text as input. So somewhere, some tool is going to do
    that conversion. Time for some context around data cleaning.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cleaning data is tricky, and manually editing files is unreasonable at the enterprise
    scale. First, understand the problem and either work with vendors that provide
    tools to support creating a cleansing pipeline or start small and learn how to
    code tools piece by piece. Review what it takes to clean data and decide where
    to invest a team’s limited resources. One way or the other, most of this has to
    be automated. The reality is that some is manual work, especially early in the
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Data cleaning also depends on the types of resources and how they will be used.
    Handling a large corpus of FAQs, knowledge articles, and marketing materials will
    require different tools than handling database queries. These are some generalizable
    issues to be aware of. Let’s start with how to handle documents.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Find or build tools to help automate this process, but it is real work for many
    use cases. The next sections include details to help understand the process in
    case issues arise with enterprise data. Some data types will require more effort.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Data augmentation addresses the issue of whether there is enough data. Is there
    enough knowledge about product questions? Are there enough data resources and
    historical data to form recommendations? Are language-specific examples available
    (hint: translate it and translate it back)? Or are various forms of training material
    needed to understand more diverse formats? **Augmentation** artificially generates
    this data to help make solutions more robust.'
  prefs: []
  type: TYPE_NORMAL
- en: Not all data can be easily augmented. An LLM can’t generate novel knowledge
    articles explaining a process it knows nothing about. But suppose you are training
    a model on specialized information, like understanding medical diagnosis and treatments,
    real-time data (like the weather), or any data that might need more recency than
    the model provides. In that case, the augmentation process can provide precise,
    up-to-date, and contextually relevant explanations.
  prefs: []
  type: TYPE_NORMAL
- en: There are tricks. For example, there are times when translating material to
    another language using an LLM when there is limited language data and then translating
    it back can help improve the retrieval step. Or incorporate synonyms for product
    names in the text to create variations to train on. For the most part, be aware
    of this and consider whether there is data that can be used to train or test the
    model. This can be a resource once the state of the enterprise data is understood.
  prefs: []
  type: TYPE_NORMAL
- en: It is an option to use the LLM itself to generate training data. Use this as
    a resource and then apply common sense to decide what data to give feedback to
    the model to augment the baseline data with good-quality data. OpenAI suggests
    that, by training on augmented data, the model can handle variety and learn to
    handle noise in the system better when addressing new data. Experimenting and
    iterating will be needed to see what best improves results.
  prefs: []
  type: TYPE_NORMAL
- en: Try this prompt in ChatGPT to learn more about data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Data annotation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Annotation is work. And it can be monotonous. **Annotation** is the process
    of marking content with notes to explain it. The concepts of tagging or labeling
    are fundamentally the same. Notes or details are associated with the content.
    This is done to help understand and mark up passages, content, tables, or anything
    that needs to be classified. What data to annotate will depend on the data and
    structure. For example, in long passages, annotation can be done for relevance.
    For tables, headers can be labeled better, which would be evident to a human but
    not a computer. Product items can be tagged so the models can learn sizes (S,
    M, L, XL), categories (first class, business class, economy), related products,
    or other essential attributes that help to give context to the material. With
    large documents, provide context to the chunks of data. For example, if a table
    is pages long, do the headers re-appear on every page? Would a human understand
    the headers if the document was broken into smaller manageable pieces? This is
    one example where the annotation is needed. Suppose the header talked about the
    product and product versions, and this header was for multiple pages earlier.
    In that case, if a chunk turns out to be one page in length, this product header
    information needs to cascade into each of the correct pages and chunks.
  prefs: []
  type: TYPE_NORMAL
- en: The annotation process needs to be of high quality. Product experts are the
    prime candidates to verify that the tags or annotations match the contents of
    the enterprise data. Thus, designers, writers, and PMs can get involved, using
    their product expertise to create an effective annotation process. This ensures
    steps are taken to quality-check the work (as the job might be outsourced or crowdsourced).
    Create metrics to define a quality bar and test against this (spot check or check
    it all). I wrote a metric to account for the kinds of errors and the frequency
    of mistakes our input would tolerate. The metric compared the quality of the crowdsourced
    material to the expectations of an expert. Results were analyzed to spot if specific
    human workers in the crowd were significantly better, worse, or the same as the
    average worker. So, consider the source, and *always test and verify* to validate
    your quality assumptions. Ask ChatGPT about all the errors that can occur when
    annotating data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Another part of making data available for the LLM is segmenting it so the most
    valuable and optimal details are shared in the context window. This is called
    chunking.
  prefs: []
  type: TYPE_NORMAL
- en: Chunking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Not only do large documents need to be tagged, as discussed, but they are likely
    too big for the RAG process. This leads to discussions concerning **chunking**.
    Chunking refers to dividing a large text or dataset into smaller, manageable pieces
    (chunks) that fit within the LLM’s context window, allowing the model to process
    and understand the information more effectively. This isn’t about becoming a chunking
    expert; it is only about being able to recognize the results of poor chunking
    and help resolve issues.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine customers want answers about mobile phone battery life. The phone company
    has released hundreds of phone models over the last few years, all with different
    specifications. These knowledge articles and details must be broken down into
    manageable, contextually relevant pieces to ensure RAG can process and retrieve
    them accurately. With this, the amount of information will be manageable for the
    system and result in good-quality answers. Segmenting the text into logical sections
    – chapters, paragraphs, and even sentences – ensures chunks have a coherent unit
    of meaning. This way, RAG can understand and retrieve the most pertinent information.
    We don’t want information about memory cards for an Android phone to be conflated
    with iPhones that do not have card slots because of a generic statement about
    memory cards.
  prefs: []
  type: TYPE_NORMAL
- en: Different chunking strategies exist. We will cover some basics, with semantic
    chunking being the one of interest for our case studies later in this chapter.
    Come back to these references for more exploration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Semantic Chunking for RAG](https://medium.com/the-ai-forum/semantic-chunking-for-rag-f4733025d5f5)
    ([https://medium.com/the-ai-forum/semantic-chunking-for-rag-f4733025d5f5](https://medium.com/the-ai-forum/semantic-chunking-for-rag-f4733025d5f5))'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second learning opportunity is a KDB.AI best practices video. With RAG,
    a vector database vendor will be needed. Fortunately, our learnings are primarily
    agnostic to the platforms. Here are a few takeaways from the video to give insight
    into chunking:'
  prefs: []
  type: TYPE_NORMAL
- en: Chunk size depends on the model being used. Changing models might require changing
    chunk sizes. This also suggests that chunking should be done in an automation
    process to adapt quickly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Small chunk sizes for a small amount of content will be accurate but won’t contain
    much context. Large chunks, typically from full documents, are less granular but
    can cost performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompts, chat history, and other resources might also be included in the context
    window, so allow for this capacity when deciding how many chunks can be allocated
    to the context windows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because context windows are growing (ChatGPT-4’s window is 128K tokens as of
    Fall 2024), it doesn’t mean it should be filled. Performance, cost, and quality
    are relevant. To put it in context, the FAQ document shared earlier has 465K characters
    and 110K tokens. That document alone would be about as much as sharable with ChatGPT.
    That is an insignificant amount of data compared to what is needed at the Enterprise
    level.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chunk overlap can be adjusted when doing code-based chunking. This is the number
    of chunks to include from previous or future chunks, so there is context. However,
    NLP chunking solutions will be more graceful in breaking the content into more
    logical breaks (in a sentence). Examples are **Natural Language Toolkit** (**NLTK**)
    and spaCy, an open-source library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chunk splitters are getting smarter every month. LangChain understands the structure
    of a document and does an excellent job of understanding sentences and paragraphs.
    It tries to optimize size based on document structure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structural chunkers understand headers and sections. They can tag chunks with
    metadata so the context is maintained.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different retrievers can be used for different databases. For example, one can
    be used for summaries to treat high-level questions and one for the source chunks
    to treat specific detailed questions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The meat of the discussion starts almost 10 minutes in. Start when Ryan Siegler
    starts talking. Video: [Chunking Best Practices for RAG Applications](https://www.youtube.com/watch?v=uhVMFZjUOJI)
    ([https://www.youtube.com/watch?v=uhVMFZjUOJI](https://www.youtube.com/watch?v=uhVMFZjUOJI))
    (KDB.AI)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why should we care about chunk size? Chunk size impacts the accuracy, context,
    and performance of LLM solutions, which are essential factors product leaders
    will want to monitor and improve.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You will likely not be the one setting up these chunks, but you will get involved
    in monitoring performance and quality to provide feedback to the data team. Team
    members who understand the content can help create and manage test cases to explore
    exceptions and validate the solution.
  prefs: []
  type: TYPE_NORMAL
- en: For example, does the model understand an exception explained at the beginning
    of a document when discussing something referenced much later? For instance, in
    the Wove case study, later in the chapter, clearly defined notes appear at the
    start of a spreadsheet they want to ingest, but this information applies to material
    much later in the document; it is thus information relevant to that later chunk.
  prefs: []
  type: TYPE_NORMAL
- en: Documents can also have images, charts, and tables. So, additional tools need
    to be used to summarize and get context from these graphics. Tools such as LayoutPDFReader
    and Unstructured are two examples that can help. The process would need to extract
    all of this independently of the text so that the chunks and summarization can
    be applied to the information extracted from the graphics. Depending on the tools,
    sometimes the embedding step can handle images directly. Almost all pictures and
    graphics in the documentation are more than ornamental, so converting these images
    to meaningful, searchable content is essential. Use LLMs to extract context from
    pictures and then use that knowledge to index and search images later. For example,
    a retailer setting up a marketing campaign might need a picture and ask, “Show
    me teens in jeans having fun on the beach.” This can be found without manually
    annotating images with these keywords. Even my iPhone (without an LLM) allows
    me to search for pictures of “cars,” “food,” “airplanes,” people, or locations
    like “Burlingame.” More intelligence and power are coming into this space with
    the inclusion of LLMs. Work on iterating on the data annotation to get content
    in good standing. Since the discussion of Wove’s use of spreadsheets, this data
    source is worth mentioning.
  prefs: []
  type: TYPE_NORMAL
- en: Spreadsheet cleanup (Excel, Google Sheets)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Spreadsheets and databases share some common issues. Data sometimes needs to
    be transformed into different formats to be understood consistently from one service
    to another. There are tools to do these transformations. Be aware of these issues
    and can then apply the tools of the day to solve a problem. Spreadsheet cleanup
    makes a lot of sense in some backend integrations. Spreadsheets and tables can
    appear in many forms of documentation, and if they need to be understood by the
    LLM, they will likely need cleanup. Our second case study extensively uses spreadsheets,
    and we will explore the effort Wove made for their cleanup process. Hint: It involves
    a lot of manual work and evaluations. First, let’s define reality, or what people
    call the ground truth.'
  prefs: []
  type: TYPE_NORMAL
- en: Documentation and ground truth in sources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **ground truth** is the facts needed as a basis for enterprise solutions.
    If documentation contains conflicting or misleading information, the LLM, like
    customers trying to read documentation, will make mistakes. This is a fundamental
    problem for FAQs, technical articles, and marketing communication. The context
    must be precise to clarify the information associated with which products. Tagging
    and annotation can help set this context. For example, if the instructions are
    to hold the power button down for 3 seconds to reset the device, but older models
    require a different answer, that context must be set clearly. Sometimes, articles
    call out the products or releases that a document impacts but also give exclusions
    later or use call-outs to give exceptions. These exclusions need to clearly define
    their scope for a search engine. Do these exceptions apply to the following few
    paragraphs or just the paragraphs where it was first introduced? Iterations of
    editing, tagging, and testing will solve this. Some tagging might be high-level,
    like articles related to finance or health care, while my examples above are specific
    to product releases or versions. Let’s start by compiling this in a simple text
    FAQ case study.
  prefs: []
  type: TYPE_NORMAL
- en: FAQ case study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Alli case study used File Search in OpenAI, but what about using the same
    data in a competitive LLM and RAG solution? Cohere is an AI company that provides
    enterprise LLM solutions. Why bother with another product in a book about ChatGPT?
    As models mature, there becomes increasing specialization. An enterprise solution
    might use one model for a specific task and a different model for a general task
    (like Wove does in our case study) . Performance, cost, and context size also
    come into play. With a focus on use cases, it is reasonable that different models
    might provide value. Cohere also provides a playground function for uploading
    documents and testing the model. It also exposed a few design elements in the
    chat UI that provide compelling UI elements worth sharing. In this example, the
    FAQs with no HTML – just the basic cleaned text was uploaded.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Go to the Coral web page ([https://coral.cohere.com/](https://coral.cohere.com/))
    and select the **Coral with documents** option (see *Figure 6**.4*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note:'
  prefs: []
  type: TYPE_NORMAL
- en: The current cohere demo uses a very different design for handling documents,
    so these instructions won't work. The latest version allows you to copy and paste
    the information to provide context, or the files must be uploaded using the Dataset
    tools. We don’t ask readers to do that. We will continue with this example because
    of some excellent features in the results, but you can follow along by opening
    the FAQ and copying and pasting.
  prefs: []
  type: TYPE_NORMAL
- en: The most recent releases of Cohere’s Playground are more complex, technical,
    and cluttered than OpenAI’s. When creating solutions, consider the impact of UI
    elements on feature capability and usability.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Setting up Cohere’s Coral with documents
  prefs: []
  type: TYPE_NORMAL
- en: Upload the FAQ file shared on GitHub using the **Files** feature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'GitHub: [FAQ Sample Document](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Example_FAQs_NoHTML_for_Demo.docx)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Example_FAQs_NoHTML_for_Demo.docx](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Example_FAQs_NoHTML_for_Demo.docx))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Close the side panel and use the message window to interact (*Figure 6**.5*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21964_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Example showing reference usage
  prefs: []
  type: TYPE_NORMAL
- en: Test the model with questions related to the FAQ (*Figure 6**.6*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21964_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Examples of the FAQ document in Cohere
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the reasons stated, there are some exciting results from this competitive
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: What is in this book is generalizable to other models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some UX elements, like showing the reference panel, could be valuable to a use
    case. There is only one document in this demo, so viewing the one link doesn’t
    help because it repeats itself with every match. Linking to the reference and
    then scrolling and highlighting the relevant passages makes it easy to understand
    and see the context. The relevance-highlighting UX pattern should become popular
    or even a standard.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is an excellent example of a side-by-side pattern showing supplemental information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It gives us a feel for the quality of different models and allows us to see
    differences between each version of ChatGPT.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s test our FAQs. It is helpful to give some context with this Cohere example
    so that we can explore using FAQs in ChatGPT. Let’s see if the results meet our
    expectations.
  prefs: []
  type: TYPE_NORMAL
- en: 'GitHub: [Zip of FAQS as unique PDFs](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQ_PDFs.zip)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQ_PDFs.zip](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQ_PDFs.zip))'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the zip file contains the cleaned data in individual PDF documents.
    This allows us better to connect the source as a reference and the results. Return
    to the ChatGPT Playground and create the same assistants as before, but try to
    upload this file.
  prefs: []
  type: TYPE_NORMAL
- en: However, recall there are limitations; uploading the file in ChatGPT 3.5 will
    result in a cryptic user error (meaning too many files were uploaded), as shown
    in *Figure 6**.7*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – ChatGPT has a file limit
  prefs: []
  type: TYPE_NORMAL
- en: It is a small dataset, just not small enough. There is a workaround to allow
    its use in the free playground. The PDFs are joined into 18 files, and a single
    PDF can be used for other testing and experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'GitHub: [Zip of 18 FAQ Files](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQS18files.zip)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQS18files.zip](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQS18files.zip))
    (each with 25 or so FAQs)'
  prefs: []
  type: TYPE_NORMAL
- en: 'GitHub: [Single PDF with all 441 FAQs](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQ-ALL.pdf)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQ-ALL.pdf](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-FAQ-ALL.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: With these 18 files, it only takes a few seconds to upload and scan them, and
    the Playground will be ready to go.
  prefs: []
  type: TYPE_NORMAL
- en: Once uploaded, try out some test cases like *Tables 6.2 and 6.3*. They were
    written without knowing whether they would work (they were not pre-tested). Test
    cases are covered more extensively in the next few chapters; let’s keep it simple
    and do testing manually. Test whether single or multiple files impact quality
    and see what can be learned from the results.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Try different models available. It doesn’t have to be ChatGPT 3.5; try ChatGPT
    4o-mini or compare it to other vendors’ LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: In both cases, use the cleaned data column from the spreadsheet. There are some
    spelling errors and chaining questions (questions that demand a follow-up question)
    in the test cases. Now, we can learn about the actual results together.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_Table_6.2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 6.2 – Questions 1-10 and the results from two test sessions
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_Table_6.3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 6.3 – Questions 11-20 and the results from two test sessions
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some high-level analyses of these results:'
  prefs: []
  type: TYPE_NORMAL
- en: Spelling errors did not cause issues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Follow-ups that provide a little extra context returned good results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same model returned very different results for some questions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specific information like addresses was very challenging.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It didn’t think it was the bank; it referred to “your financial institution
    or brokerage firm.” Prompt engineering can fix this problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both needed help with ending sentences with a period. They tended to put a space
    before the period like this .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s create a simple score for these two methods. Five points for a great correct
    answer, 4 points for a good correct answer, 3 points for a close to correct answer,
    and 2 points if a follow-up returned details that should have been in the first
    answer. Scoring shows 47 points for separate files and 74 for a single file model.
    Recognizing a significant difference between these two starting points doesn’t
    have to be perfect. If you watched the OpenAI video in the last chapter (one of
    my favorite video references of this entire book), they had some similar experiences,
    beginning with a poor result, and with fine-tuning and prompt engineering, they
    improved their result, as shown in *Figure 6**.8*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Video: [A Survey of Techniques for Maximizing LLM Performance](https://www.youtube.com/watch?v=ahnGLM-RC1Y)
    ([https://www.youtube.com/watch?v=ahnGLM-RC1Y](https://www.youtube.com/watch?v=ahnGLM-RC1Y))'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – A RAG success story for OpenAI’s approach in a use case
  prefs: []
  type: TYPE_NORMAL
- en: It isn’t necessary to understand all of these methods in detail. Toward the
    end of this chapter, there is a section for other techniques to discuss this.
    The point for the moment is to showcase how continuous improvement to your lifecycle
    will help determine what changes improve the experience. Even with this rudimentary
    scoring, there are dramatically different results. I, too, was surprised by the
    dramatic difference. The full transcript of both results is posted.
  prefs: []
  type: TYPE_NORMAL
- en: 'GitHub: [Transcripts of FAQ Test](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Transcripts.docx)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Transcripts.docx](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter6-Transcripts.docx))'
  prefs: []
  type: TYPE_NORMAL
- en: The data brought in through RAG needs to be cleaned, as seen in the forthcoming
    Wove case study. Something simple, like how files are split up, can profoundly
    impact performance. Each improvement can affect the next step. It is better to
    continue to refine, starting from a score of 74 than from 47\. Find tools to handle
    the mundane work so efforts can be focused on actual data and its quality. There
    are other issues to consider when creating a complete lifecycle for a data pipeline.
    Next is a case study from an exciting company that uses a variety of models to
    make its LLM solution successful.
  prefs: []
  type: TYPE_NORMAL
- en: Spreadsheet cleanup case study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here is an excellent example of spreadsheets used behind the scenes to create
    intelligence in the LLM and offer recommendations from [Wove.com](https://wove.com)
    ([https://wove.com](https://wove.com)). Wove helps freight forwarding companies
    optimize rate management operations by using LLMs to parse and normalize complex
    tabular data from rate sheets, ocean contracts, and other spreadsheets.
  prefs: []
  type: TYPE_NORMAL
- en: Freight forwarders act as intermediaries who ensure that small shippers can
    get goods from one location to another—for example, shipping 10,000 widgets from
    a factory in China to a warehouse in Nebraska. Because there are hundreds of ways
    to get from point A to point B, there are complexities based on the vendor, distance,
    ports, transport type, time, type of goods, customs, weight, and volume. This
    complexity is buried in published data from each vendor in spreadsheets, PDFs,
    and other data sources. This complexity increases the time to quote and can lead
    to missing reasonable rates. By taking these rate sheets and putting them into
    the model, customer quotes can be generated more accurately and efficiently. This
    a daunting task. To geek out on the rate sheet use case, look at all the standard
    terms one might see in a sheet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Rate Sheet Terms and Introduction](https://www.slideshare.net/logicalmsgh/understanding-the-freight-rate-sheet)
    ([https://www.slideshare.net/logicalmsgh/understanding-the-freight-rate-sheet](https://www.slideshare.net/logicalmsgh/understanding-the-freight-rate-sheet))'
  prefs: []
  type: TYPE_NORMAL
- en: Terms like BAR, BL Fee, Demurrage, DDC, CYRC, Detention, and dozens of others
    are a lot to digest. It makes it challenging for an LLM to understand a complex
    spreadsheet. This is an excellent example from our friends at Wove, who have created
    a behind-the-scenes use of ChatGPT and other models, like Anthropic’s Claude.
    They focus on ingesting data to preserve data quality and integrity and normalize
    widely different spreadsheets. Indeed, there are opportunities on the UI side
    to use this data to answer questions about finding the correct rate for a job.
    This part of the case study will focus on data ingestion. The Wove case study
    will be completed after more is explained in the prompt engineering and fine-tuning
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: The terms require understanding, and each rate sheet varies in format, labels,
    exceptions, and other factors. As rates change over time, the correct rate periods
    must be understood. *Figure 6**.9* shows a fraction of a rate sheet to expose
    this complexity.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_010_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Samples of rate sheets from two different vendors
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical forwarder might have to deal with dozens of different rate sheets,
    and with some of them being *hundreds* of pages long, normalizing all of this
    data manually requires the effort of a whole team. The examples show how varied
    the data columns can be. The labels, the values, the use of tabs, how exceptions
    are handled with remarks, and the headers are all different. However, automation,
    or even semi-automation, can reduce this process by more than 90%. Although one
    should test and verify data along the way, there are numerous places in the manual
    lifecycle where human error causes issues. Let’s review the data cleansing steps
    Wove had to do to ingest this data. The expected flow of this information is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Before getting new rate sheets, they trained and verified the various models
    needed to create high-quality output. This case study will discuss the different
    models used.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Typically, they receive a rate sheet in an email and download the file into
    Wove. There is also an automation path with an email listener that picks up the
    file, monitors for new files, and ingests it into the process. These files can
    have multiple tabs and thousands of rows of data, like the small sample shown
    in *Figure 6**.9*. A typical file is likely an update of a previously processed
    file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Their tools parse the XLS file and identify the tables, and it parses the document
    and turns them into property formatted clumps for the model. There are context
    length limits, detecting tables, understanding the tables, and figuring out how
    the tables relate to each other. They refer to this as table detection. As shown
    next, the development team built ten models to understand the spreadsheet. The
    entire proprietary process isn’t shared, but this should give a sense of what
    each model does and what software was used to help the cleaning and organizing
    process. Although this is a technical process, the results are something mere
    mortals can see. They can determine whether they provide the best results for
    the cost involved. This is a business decision and a user experience problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Document Segmentation (Single-Shot GPT 4 Turbo)**: This segments documents
    into coherent sections/ideas.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context Builder (Multi-Shot Claude 3 Haiku)**: This is applied after document
    segmentation. It builds the reading context for understanding the current document.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Table Detection (GPT 3.5 Turbo, Fine-tuned)**: This detects tables in spreadsheets,
    documents, or contracts.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Table Header Range Detection (GPT 3.5 Turbo, Fine-tuned)**: After the table
    is detected, the range of header rows and where the data for the table starts
    are determined.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Table End Detection (GPT 3.5 Turbo, Fine-tuned)**: This detects the end of
    the table data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Table Understanding (GPT 3.5 Turbo, Fine-tuned)**: This model understands
    a table’s columns and data and determines its purpose.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Schema Mapping (GPT 3.5 Turbo, Fine-tuned)**: This model is applied after
    the table is understood. It determines which columns from a table map to schema
    fields in a database.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Field Splitter (Single-Shot Claude 3 Haiku)**: The splitter extracts per-field
    information from combined fields. For example, if effective and expiry dates are
    in the same field, this can extract them into **effective_date** and **expiry_date**
    in the schema.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Location Normalizer (Multi-Shot GPT 3.5 Turbo)**: This takes unstructured
    location information and normalizes each detected location to a UN/LOCODE (normalized
    country codes such as HK for Hong Kong).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Commodity Normalizer (GPT 3.5 Turbo + Ada)**: This takes unstructured commodity
    information and normalizes each commodity type to be searched/compared.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: These models changed multiple times during the creation of this case study,
    and they continue to change as they are currently testing GPT 4o-mini for some
    use cases. Adapt and improve, and sometimes save some money.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: They identify, tag, and train the system to understand where the table is, where
    data starts, where it ends, the header labels, and so on. The challenge is understanding
    tables when LLMs are primarily for text. The spreadsheets become text. Notice
    some of the models used in this process are fine-tuned. Those are the ones that
    need additional understanding and learning by providing examples of what defines
    a table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Diving more into table detection helps to understand the segmenting of data.
    After table detection from *Step 3*, they do semantic chunking to get the right
    context length. Typically, a suitable context length might start at 500 to 1,000
    tokens. Depending on the model, longer context lengths are acceptable if you want
    to pay for them. Wove prompts GPT-4 to chunk the files into *coherent segments*.
    Chunks are essential, as only so much information can be processed at one time.
    Effective chunking strategies are necessary to have the proper context for a chunk.
  prefs: []
  type: TYPE_NORMAL
- en: Their prompt is pretty big—it is a page long. It tells ChatGPT 20 different
    rules to parse a segment. Their prompt starts simple… “You’re an expert in doc
    parsing; you’ll be given a chunk of text. Your job is to split it into coherent
    segments.” They don’t have massive chunks, so chunk size is not limited by the
    LLMs. Each model can have a different token limit to allow for the size of the
    prompt and the resulting output. The models range from 4K to 8K tokens for input
    and output. They use a smaller, faster, and less expensive model in the next step.
    If you are unsure of your model’s limitations, ask it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Wove covers the entire lifecycle. **Functional calling**, the method to access
    other resources such as APIs, is essential to Wove’s process and fundamental to
    enterprise applications. Be aware of this capability. Remember, any enterprise
    solution will connect to various resources to enrich the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Documentation: [ChatGPT developer documentation on function calling](https://platform.openai.com/docs/guides/function-calling)
    ([https://platform.openai.com/docs/guides/function-calling](https://platform.openai.com/docs/guides/function-calling))'
  prefs: []
  type: TYPE_NORMAL
- en: They use function calling to generate the sections into a structured output.
    A piece of this function is shown in *Figure 6**.10*. The product team needs to
    understand this to ensure the context is complete. Some of this might be generic
    to any spreadsheet, such as a start line, end line, the section’s name, headers,
    and a description, but getting this understanding right is essential. Later, they
    checked that the tables were processed correctly to confirm the correct start
    line, header, or sub-header labels.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Figure 6.10 – A snippet of the function calling that is used to help structure
    the output
  prefs: []
  type: TYPE_NORMAL
- en: They can use the training validation split data, test the models against the
    removed data, and use their data cleaning technique on the data shown in *Figure
    6**.11* by defining the tables. This data tagging defines *what is what* in the
    table and can improve with more refinements over time. Scripts help generate new
    training data from this tagged source.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – A small table of side terms from the rate sheets
  prefs: []
  type: TYPE_NORMAL
- en: Look at this definition of `Side Terms`, which is used to train table detection;
    it tells the LLM how to understand this data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Product managers, designers, and the team must monitor table definitions to
    ensure high quality. In this example, they identify the “`Side Terms`” start date
    from row 3 (line 2) to row 21 (line 3). In line 5, they identify the spreadsheet
    columns as from `B` to `G` (column `A` is white space), followed by row 6 being
    defined as the header and defining the source data for the table with `(7,20)`
    for rows 7 to 20\. However, in *Figure 6**.11*, notice the `Remark` column (column
    `F`) extends to line 21, so the process involves human validation *to catch this
    error* and change `(7,20)` to `(7,21)`.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple models use this one tagging exercise. This effort supports table-end
    detection, headers, and table understanding.
  prefs: []
  type: TYPE_NORMAL
- en: It is vital to catch what needs to be tagged. For example, some notes with stars
    are shown at the top of the table in *Figure 6**.9*. LLMs are good at understanding
    text and the reference to this block of text extracted from the table detection,
    so no additional effort was needed to gather this information.
  prefs: []
  type: TYPE_NORMAL
- en: The data must then be normalized for items such as rates and locations. So,
    for Hong Kong, the port HKHKG is displayed consistently, and dozens of other values
    are mapped correctly across different files.
  prefs: []
  type: TYPE_NORMAL
- en: There is a data review process, and Wove has tools for doing so. The team reviews
    this clean data, as shown in *Figure 6**.12*. This drill-down shows rates between
    Hong Kong and Atlanta and some data that goes into these rates.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_06_12-shphigh.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – The view of data so now they can view rates in a normalized view
  prefs: []
  type: TYPE_NORMAL
- en: Now that they have ingested and normalized the data, they can access rates from
    many sources. Let’s explore some of the details of this workflow a bit further.
  prefs: []
  type: TYPE_NORMAL
- en: This is not about a single model performing magic; it takes a collection of
    specialized models. They applied different models to solve various problems. It
    is expected to adapt and change over time, especially with models that use fine-tuning.
    Think of it as a modular approach. If a new or much less expensive model comes
    out, swap it to improve one piece of the puzzle at a time. If there are issues
    around one topic, such as poor or missing data, and the model will need help converging
    to a practical solution, focus on that problem. Each piece can experience its
    version of hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: Data cleansing has a specific meaning for these spreadsheets, especially ensuring
    that rows and exceptions are handled. Chunks must be segmented correctly to have
    a good beginning and end so that context is maintained. This gives RAG a clean
    context and retrieves relevant chunks more accurately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the top issues addressed in their cleanup and ingestion process:'
  prefs: []
  type: TYPE_NORMAL
- en: Process data as text, even coming from spreadsheets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model segments large documents—some rate sheets can be hundreds of pages
    long—and breaks them up. For example, ocean shipping documents are more complex
    than road trucking documents.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The challenge is to understand tables as text. It takes considerable work to
    understand tables well, tag them correctly, look for errors, and find a suitable
    model (which they did and didn’t discuss to protect their expertise). This differs
    from reading straight text, but this might impact the experience even if the team
    controls the knowledge base or databases. Documents with tables, images, flow
    charts, and diagrams all contain information that might need to be fully expressed
    in text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on the prompts Wove establishes, the model writes instructions for extracting
    all the data from the sheet. This multiple-step process is examined in [*Chapter*
    *8*](B21964_08.xhtml#_idTextAnchor172), *Fine-Tuning*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In *Step 1*, Wove runs version GPT-4 Turbo, while in other steps, it runs ChatGPT
    3.5 and other models. Running tasks sequentially is ten times faster than running
    GPT-4 once. They used GPT-4 Turbo to generate fine-tuning data. By using more
    than one model, they can balance performance and cost.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Wove leaves out 10 to 20% of the data to test the model. This is standard practice.
    They take out different chunks of data from documents to create a broader and
    likely more effective test set. Tip: Don’t bias the model by always taking the
    first 20% of every document. They use a random seed to pick pieces of documents
    but then maintain that same chunk each time from the same document; this allows
    them to create a reproducible set. So, their validation steps do not differ because
    of the test data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All of this hard work is for data cleaning. The first goal is to expose the
    data to those responsible for ensuring they have the correct data. As mentioned,
    this will set up a later conversational experience to help find rates. The FAQ
    and Wove examples should give some understanding of data issues, but there are
    other considerations.
  prefs: []
  type: TYPE_NORMAL
- en: Other considerations for creating a quality data pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Not all designers and product managers will be involved with every step of the
    RAG process. All vendors use the fancy term **pipeline** to represent this flow
    of information from source to customer. Issues can occur before, during, and after
    models are included in the pipeline. Keep an eye on the following areas for issues
    impacting customer experience.
  prefs: []
  type: TYPE_NORMAL
- en: Computational resources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RAG has some real work to do. It has to take an extensive collection of documents
    and resources and create vector data like the original model generation. Doing
    this regularly can be computationally expensive. Watch for any performance issues
    when scaling up. Many third-party solutions will talk about millisecond response
    times. That is wonderful; responses should feel natural. It might be okay for
    results to take a few seconds in some instances, but nominally, a chat response
    should start in 200-300 ms (about 1/4 of a second)..
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, recommendations might be triggered when data changes (this can get
    expensive if always recalculated and no user needs to be updated) or calculated
    when a page is rendered. Even a trigger to email or message someone about the
    recommendation requires processes to have current information and evaluate for
    issues on a schedule. Each of these events will have a cost. Consider the cost
    of the recommendation if no one can use it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Issue: *There is no such thing as a slow, good user experience*. Designers
    and PMs can help performance in a few places.'
  prefs: []
  type: TYPE_NORMAL
- en: Monitor and verify a solution’s performance and decide what will meet users’
    expectations. Product owners should set performance expectations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor whether too much or too little data is sent to the LLM. All data should
    provide value to the LLM; if not, eliminate it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine whether the prompt and context sizes provides value for its sizes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: API requests in an LLM cost money, so optimize or cache information when possible.
    Understand whether customers use recommendations or visible UIs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This can be managed if the system only deals with hundreds of documents. Still,
    some large enterprises might be looking at a million documents and massive SQL
    databases. Maintaining this large corpus and refining and improving the quality
    of those databases and documents can be a significant investment. Emphasize the
    most helpful and frequently accessed materials. Take advantage of third-party
    pipeline solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Issue: *You can only be in so many places simultaneously.* Scalability also
    applies to your time. Consider whether there are places worth your attention,
    like improving the management process, monitoring quality, maintaining documents,
    or improving the time and process it takes to edit and update documents. Consider
    a personal version of the 80/20 rule. If 20% of the time on project C returns
    80% of the value, spend resources there. Even better, use User Needs Scoring.
    If something is for all customers that they use frequently, and it is a critical
    area, then this deserves attention.'
  prefs: []
  type: TYPE_NORMAL
- en: Training data quality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fill in the following puzzle.
  prefs: []
  type: TYPE_NORMAL
- en: Quality in supports quality out.
  prefs: []
  type: TYPE_NORMAL
- en: Garbage in supports _______ out.
  prefs: []
  type: TYPE_NORMAL
- en: A+ if you guessed garbage. The quality of training materials profoundly affects
    the ability to fine-tune. If content is very limited, biased, or has a lot of
    red herrings that could lead customers astray, then there will be ongoing issues.
    The relevance and quality of content is king. This chapter covered the importance
    of clean data, but that process can only have limited fixes for quality. That
    is, removing redundant or conflicting data might be easy to do. It is hard to
    do when writing this book. Did the reader remember or even see something in [*Chapter
    1*](B21964_01.xhtml#_idTextAnchor016), *Recognizing the Power of Design in ChatGPT*
    that is now important in [*Chapter 5*](B21964_05_split_000.xhtml#_idTextAnchor108),
    *Defining the Desired Experience*? Who is the content expert that can determine
    correctness? This gets more challenging as the data grows. Now, think about how
    a model can handle learning something 150 pages ago that now becomes important.
    The more technical the data, the less likely an individual can know if the content
    is high quality. Models can forget, too. They are especially prone to forgetting
    information in the middle. Not to mention problems understanding knowledge for
    specific releases or combinations of products. Rely on content partners, authors,
    and technical experts. It takes a village. Remember that from [*Chapter 1*](B21964_01.xhtml#_idTextAnchor016),
    *Recognizing the Power of Design* *in ChatGPT*?
  prefs: []
  type: TYPE_NORMAL
- en: RAG is well suited for responding to specific questions against a wealth of
    content. However, the data must be in the correct format, and this can be some
    heavy lifting with data at scale. Picking suitable chunk sizes when segmenting
    text is more art than science. For the CliffsNotes version (a student study guide
    for popular books in the US) of dealing with chunking and other lessons learned,
    watch the video from Prolego.This video will be discussed at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Video: [Prolego tips for RAG development](https://www.youtube.com/watch?v=Y9qn4XGH1TI)
    ([https://www.youtube.com/watch?v=Y9qn4XGH1TI](https://www.youtube.com/watch?v=Y9qn4XGH1TI))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Issue: *Don’t let the models be overwhelmed with garbage and reduce accuracy.*
    Monitor and set improvement goals.'
  prefs: []
  type: TYPE_NORMAL
- en: Domain specificity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Enterprise models rely on domain-specific content.
  prefs: []
  type: TYPE_NORMAL
- en: 'Issue: *Gathering and annotating data to improve performance is expensive.*
    Annotation can take many forms, but as with data quality, find experts inside
    or outside your company to take this to the next level. Invest in building personal
    expertise.'
  prefs: []
  type: TYPE_NORMAL
- en: Response consistency and coherence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RAG solutions will be challenging. Enterprise solutions value deterministic
    answers, which will not happen with only a generative solution. Answers will vary,
    even when asked the same question. This can be improved with prompt engineering,
    fine-tuning, and the careful use of the generative models in a larger ecosystem
    of products.
  prefs: []
  type: TYPE_NORMAL
- en: 'Issue: *Don’t throw the baby out with the bathwater.* With existing chatbots
    that provide repeatable solutions, supplement them with a generative solution.
    Focus fine-tuning on consistency. For recommendation engines, look for the places
    with the most value to add by incremental improvements.'
  prefs: []
  type: TYPE_NORMAL
- en: Privacy, security, and data residency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because the data is proprietary and is contained in company databases, knowledge
    base, and APIs, its access can be managed when responding to customer questions.
    Since a ChatGPT response will be based on the context passed to it from a RAG
    solution, it makes sense to make sure privacy and security policies limit the
    visibility of this data to the appropriate customers. Be on the front line to
    monitor for issues that appear during a chat or come in via customer feedback.
    One area where designers can add value to the privacy discussion is related to
    the data seen during reviewing interactions.
  prefs: []
  type: TYPE_NORMAL
- en: Besides ensuring that data from backends doesn’t get into the wrong conversations,
    there are times when masking **Personally Identifiable Information** (**PII**)
    is necessary. In [*Chapter 2*](B21964_02_split_000.xhtml#_idTextAnchor031), *Conducting
    Effective* *User Research*, ways to mask PII were discussed. This is a consideration
    when training models. Training data can contain PII. One approach is not to mask
    it (replacing the text with ****), as that would hinder the model’s comprehension,
    but to *transform* the data into fake data generated by the model itself. For
    example, instruct a model during a data cleaning step to replace customer names
    with customer names it makes up. It is good at this, and this fake data, called
    **synthetic data,** can replace actual data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: ChatGPT is very good at this. Even ChatGPT will reject providing fake Social
    Security numbers, so notice how the prompt asked for a nine-digit number in the
    prompt. It responded with the results in *Table 6.4*.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Customer** **Name** | **Country** | **Age** | **Car** **Sharing (Monthly)**
    | **Identifier** **Number** | **Cell** **Phone Number** | **Favorite** **Payment
    Method** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Emma Smith** | USA | 29 | $150 | 22-333-444 | (555) 123-4567 | Credit Card
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Hiroshi Tanaka** | Japan | 35 | ¥18,000 | 22-333-445 | 090-1234-5678 |
    Mobile Payment |'
  prefs: []
  type: TYPE_TB
- en: '| **Maria Garcia** | Spain | 41 | €120 | 22-333-446 | 612-345-678 | Debit Card
    |'
  prefs: []
  type: TYPE_TB
- en: '| **David Osei** | Ghana | 26 | GHS 600 | 22-333-447 | 024-123-4567 | Mobile
    Money |'
  prefs: []
  type: TYPE_TB
- en: '| **Anna Müller** | Germany | 32 | €140 | 22-333-448 | 0151-1234567 | PayPal
    |'
  prefs: []
  type: TYPE_TB
- en: Table 6.4 – Example of using synthetic data to replace PII
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the names feel localized; the counties were varied, local currency
    and reasonable amounts were used, and the phone numbers were localized. *Mobile
    Money* is not a term I recognize, but it is common in Ghana. Mobile Money means
    payments made via mobile phone providers. So, it is even possible to learn something
    from synthetic data. Since the subject of other countries came up, there are other
    country-specific issues.
  prefs: []
  type: TYPE_NORMAL
- en: There are two considerations when discussing country-specific limitations that
    might limit model enrichment. Generally, this will fall to the product manager.
    The first is whether there are export limitations for company data. Some countries
    restrict the export of customer or employee data across borders. They have data
    residency requirements to house data in-country. This is why many vendors provide
    data centers in some regions. The **General Data Protection Regulation** (**GDPR**)
    in the European Union and the Privacy Shield framework come to mind. When dealing
    with personal information that might be common in a human resource chat application,
    for example, safeguards might be required to be in place, and consent might be
    needed. This can impact the user experience. I have had to design examples where
    user permissions are required or policy requirements on what can or should not
    be shared need to be consented to before proceeding.
  prefs: []
  type: TYPE_NORMAL
- en: The second issue is more data-centric and not subject to data residency issues.
    Processing rules might only apply to certain countries or groups within a country.
    It could be a data issue to ensure that the LLM knows that this person is from
    a particular country, and thus, specific documents, policies, or APIs apply. For
    example, expense reimbursement policies vary per country. It is one thing for
    an American to have dinner on a trip to France and be reimbursed when they return
    to the US (US policy applies) versus someone from France going somewhere outside
    the European Union (EU/French policy). Designers and PMs must recognize the necessary
    attributes to filter and support the correct data and resources. This is not unique
    to LLMs. It must be handled in these cases, such as in a GUI or an existing chatbot.
    Another version of the rules is how to communicate with our audience. Sometimes,
    biases in the data or even ethical concerns need to be addressed.
  prefs: []
  type: TYPE_NORMAL
- en: Bias and ethical concerns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It might be reasonable to think that enterprise data doesn’t have bias, but
    still watch out for it. There could be tongue-in-cheek content in the knowledge
    base, but it gets regurgitated as truth. It could cause issues in the results.
    Take a look at *Figure 6**.13*. It is a simple interaction that it should be easy
    to discern facts. It is fair to point out that this was *not* ChatGPT. This is
    a continuation of our Cohere example. Remember, the example from earlier extended
    a basic LLM to include the FAQs. Unless it is told otherwise and guardrails are
    put in place, it will still attempt to answer general model questions. It did
    not go as planned. As these models are expected to improve quickly, it is not
    fair to comment on this model’s shortcomings. All models have shortcomings. It
    is used to make a point about all models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Conversational hallucinations can create bias and errors
  prefs: []
  type: TYPE_NORMAL
- en: 'Issue: *Don’t get caught making (wrong) ethical decisions*. Avoid these discussions
    when possible. Watch data collection, model training, and the monitoring process
    for potential issues. Let’s break down this collection of issues. Hank Greenberg
    (who is Jewish) and Hank Aaron (who was not) got confused somehow. Maybe this
    is a hallucination. But simple questions such as this would be easy to answer.
    Let me point out a few problems.'
  prefs: []
  type: TYPE_NORMAL
- en: It didn’t get Hank Aaron (presumably because of my spelling error) even when
    the model corrected my mistake and returned it spelled correctly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hank Aaron was not Jewish.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: His career lasted 23 years, not 24.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He never had close to 209 home runs per season (he had 755 in his entire 23-year
    career; I know because I watched him tie the record).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jackie Robinson was the first African-American inducted into the Hall of Fame
    in 1962\. Hank Aaron’s induction was *20* years later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hank Greenberg was well known to be Jewish and faced discrimination.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We tried this with ChatGPT. It did assume Hank Aaron even when his name was
    misspelled. It accurately explained he was not Jewish, played for 23 years, knew
    his 755 home runs record, and his place in the Baseball Hall of Fame. The ChatGPT
    3.5 model was factually correct. And to be fair, *a newer update to Cohere got
    all of* *this correct*.
  prefs: []
  type: TYPE_NORMAL
- en: Enterprise data is not expected to talk about the religion of famous baseball
    players. Just recognize that answers for well-known facts can still contain hallucinations,
    lies, or whatever they should be called, and the organization will likely be liable
    for spreading disinformation. It doesn’t mean a lawsuit. It could mean not meeting
    a service-level agreement, upsetting or losing a potential customer, or having
    to compensate the customer. This is not unlike what would happen if a human agent
    provided incorrect information. There can be downstream costs or service interruptions
    due to wrong answers. With AI, we have seen numerous mistakes, errors, or maybe
    a lack of training, causing issues.
  prefs: []
  type: TYPE_NORMAL
- en: Semi-autonomous cars causing accidents in the automotive industry come to mind.
    This isn’t to say that human drivers are better (they are not by almost an order
    of magnitude). Still, the kinds of accidents caused by training model issues sometimes
    seem obvious and avoidable by a human driver (being able to identify an 18-wheeler
    truck crossing the road in a high-glare situation). At the same time, there are
    far more cases *not* seen in the news, such as semi-autonomous cars *not* getting
    into accidents where a human’s response time and visibility would have resulted
    in tragedy. Ultimately, expect generative AI to be more reliable, consistent,
    and accurate than humans. It should get there in a few years with significant
    effort. Always be aware that bias and ethics come into play within a model. In
    addition, be ethical in how much effort is expended to build and test models.
  prefs: []
  type: TYPE_NORMAL
- en: There will be a benefit/risk analysis; just don’t get caught on the wrong side,
    as Ford did when it refused to fix defective gas tanks in its Pinto model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Wikipedia: [Ford Pinto Gas Tank Controversy](https://en.wikibooks.org/wiki/Professionalism/The_Ford_Pinto_Gas_Tank_Controversy)
    ([https://en.wikibooks.org/wiki/Professionalism/The_Ford_Pinto_Gas_Tank_Controversy](https://en.wikibooks.org/wiki/Professionalism/The_Ford_Pinto_Gas_Tank_Controversy))'
  prefs: []
  type: TYPE_NORMAL
- en: One has to invest in creating good models. There will be lawsuits related to
    this as well. Take the time and energy to put quality first. Document your sources;
    in legal terms, this is called the chain of custody. Check the work and refine
    and resolve problems with a cadence that befits the risks understood by the enterprise.
    It won’t be perfect – nor are human agents. Just put a process in place for constant
    improvement. The Silicon Valley mantra about “move fast and break things” sounds
    great at a start-up, but when delivering a paid service to high-valued customers,
    maybe be more pragmatic about investing in quality.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding other techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you watched the OpenAI discussion on techniques mentioned at the beginning
    of the chapter to learn about a few additional methods, the video discusses optimization
    techniques at the 15-minute mark. This approach to benchmarking quality and applying
    tools and techniques is right. This was done for a typical ChatGPT solution that
    involves searching a knowledge base. They tried a few methods that did not work
    to get the improvements they expected (**Hypothetical Document Embedding** (**HyDE**)
    retrieval and fine-tuning embedding). They found some worthy investments (chunk/embedding
    reranking, classification, prompt engineering, and query expansion). It would
    be way over our heads exploring how to do these. The key is for designers and
    PMs to work with the team to establish a benchmark, find good data to train the
    model, and test and verify results as they are iterated. Consider the goal so
    it is known when the goal is reached. Recognize that as models change and data
    grows, adapt. In reality, a team won’t ever be done, but with a quality bar, the
    organization can allocate resources more wisely.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B21964_10_split_000.xhtml#_idTextAnchor216), *Monitoring and
    Evaluation*, will cover methods for determining performance. Relevance, diversity,
    and coherence are all crucial factors for our datasets. The focus will be to understand
    this from the user’s perspective with accuracy and customer feedback.'
  prefs: []
  type: TYPE_NORMAL
- en: Despite these limitations, RAG holds promise for enhancing the capabilities
    of conversational AI systems such as ChatGPT by enabling more contextually relevant
    and informative responses to user queries. Addressing the challenges above through
    ongoing research and development efforts can help unlock RAG’s full potential
    to improve user engagement and satisfaction in conversational AI applications.
  prefs: []
  type: TYPE_NORMAL
- en: Resources for RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A good RAG solution will use a service dedicated to managing the influx of data,
    processing it, storing it, and retrieving it to share with the LLM. There has
    been so much movement in this space since RAG was invented, and it is hard to
    realize how quickly this has become real.
  prefs: []
  type: TYPE_NORMAL
- en: Because most of the design work is in data quality rather than technology, it
    will be best to provide resources for those who want to explore the more technical
    pieces of the puzzle. The OpenAI resource is the best place to start; it will
    evolve and adapt as technology changes. GPT-4 and newer work directly with RAG.
  prefs: []
  type: TYPE_NORMAL
- en: 'Web article: [OpenAI RAG vs. Customized RAG with Milvus](https://thenewstack.io/openai-rag-vs-your-customized-rag-which-one-is-better)
    ([https://thenewstack.io/openai-rag-vs-your-customized-rag-which-one-is-better](https://thenewstack.io/openai-rag-vs-your-customized-rag-which-one-is-better))'
  prefs: []
  type: TYPE_NORMAL
- en: Do not assume linking to these resources implies they are best in class. They
    are all improving rapidly, will diverge in value, and some will disappear. As
    the market evolves, look for tools that can automate the pipeline with high quality.
    Access to a database is the most prominent tool needed in the enterprise after
    knowledge access.
  prefs: []
  type: TYPE_NORMAL
- en: Databases and SQL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Database retrieval presents challenges. Consider how the database thinks about
    its content and how to ask for results. This is typically expressed in SQL, the
    structured query language of most databases. Some databases do not use SQL and
    are called NoSQL databases. Since most enterprise data to inject into tasks and
    prompts is held in SQL databases, we will focus on a SQL example. LLMs have some
    ability to write SQL, but it is still an evolving area. Here is an example highlighting
    the complexities of working with a database.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'It might already be obvious there are a few issues to address:'
  prefs: []
  type: TYPE_NORMAL
- en: Which Burlingame (the one near me in the San Francisco Bay Area, San Diego,
    Kansas, Oregon, etc.…)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What kinds of jobs? Does it have context for what is needed?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are more filters for price, types of companies, and working hours needed?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Focusing on the first step, drilling down into understanding the first statement.
    What does near mean? In the San Francisco Bay Area, 10 miles is reasonable, while
    in Oregon, 25 miles is reasonable. If this were New York City and you wanted a
    pizza place, two blocks would be too far.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does one translate this into cities that are in my database?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'My point is that it is more complicated than extracting an entity like the
    name of a city and expecting, even with SQL magic, to be able to scope good content.
    This request needs to be pre-processed to generate a reasonable SQL statement.
    So, manage the input to expect a sensible output. In real estate property management,
    there is a saying: *inspect what you expect*. The same here. The input must be
    checked and broken up into pieces that need to be analyzed and expanded with more
    details to get the expected output. With the output, it might be possible to send
    a collection of results to the LLM, like sending documents or FAQs, so that ChatGPT
    can use this information to form a more refined answer. The LLM has to know about
    the schema to format a good query.'
  prefs: []
  type: TYPE_NORMAL
- en: Another approach I have worked on and that is becoming popular is text-to-SQL.
    This is a way to apply LLM intelligence to create a logical SQL statement that
    returns effective results. This approach has merit but depends on whether SQL
    can support the query. In the preceding example, the solution would need a city
    or location discovery tool based on a range or distance from a center point (and
    center points can be wildly inaccurate regarding where the person is located and
    where to go). A basic LLM would have to understand the related cities correctly.
    These tools are available and must be integrated to generate the correct list
    of towns and locations to pass into SQL. An LLM can also get this information
    and form the SQL queries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Foster City, CA, which is 3 miles away, was included the first time the question
    was asked, but Belmont, CA, or San Bruno, CA, were never included. So, if this
    customer were looking for a job, they would miss many opportunities. This specific
    issue could be fixed with adjustments to the prompt focusing on the exact cities
    and asking to check its work. For example, the results can be seen with a simple
    adjustment and follow-up.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: When asked later how close South San Francisco was to Hillsborough, it correctly
    answered 10 miles—much different than its first answer. *Rely on a well-known
    API to get this data rather than constantly dealing with the variability of an
    LLM*. API resources provide more accurate and reliable data when specific details
    are required. Consider incorporating these into LLM responses to provide natural
    interaction. An enterprise is rich with databases containing collections of facts.
    Use this as a competitive advantage. Don’t expect the LLM to do it all by generating
    results. This is why function calling exists – to get the value of specific data
    in generative output.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering can also help improve the results. But the point should be
    clear—SQL needs some hand-holding to create effective queries and some pre-processing
    to give the database a good chance at returning effective results.
  prefs: []
  type: TYPE_NORMAL
- en: Online suggestions for connecting to databases focus primarily on straightforward
    queries that don’t explore how the user will ask the questions. This is a more
    complex problem than just connecting to a database.
  prefs: []
  type: TYPE_NORMAL
- en: Extra credit reading on database
  prefs: []
  type: TYPE_NORMAL
- en: If database connectivity is new for you, read these references
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Talk to your](https://medium.com/@shivansh.kaushik/talk-to-your-database-using-rag-and-llms-42eb852d2a3c)
    Database using RAG and LLMS ([https://medium.com/@shivansh.kaushik/talk-to-your-database-using-rag-and-llms-42eb852d2a3c](https://medium.com/@shivansh.kaushik/talk-to-your-database-using-rag-and-llms-42eb852d2a3c))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [How to connect LLM to SQL database with LlamaIndex](https://medium.com/dataherald/how-to-connect-llm-to-sql-database-with-llamaindex-fae0e54de97c)
    ([https://medium.com/dataherald/how-to-connect-llm-to-sql-database-with-llamaindex-fae0e54de97c](https://medium.com/dataherald/how-to-connect-llm-to-sql-database-with-llamaindex-fae0e54de97c))'
  prefs: []
  type: TYPE_NORMAL
- en: We will explore one example with the Oracle Digital Assistant. This area will
    see significant improvements in the coming years as the intelligence needed to
    interpret the user’s needs before forming the proper SQL queries will improve.
    The chaining necessary to get the correct result will also improve. This chaining
    problem is a function of what the user asks, the assumptions needed to understand
    the question, and the SQL required to return the answer. Chaining is the connecting
    of one answer that feeds the following question and subsequent answer. Sometimes,
    it makes sense to chain thoughts together to resolve a question. Let me finish
    with a use case example paraphrasing this Oracle blog example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Oracle Digital Assistant SLQ Integration](https://blogs.oracle.com/digitalassistant/post/introducing-the-new-oracle-digital-assistant-sql-dialog)
    ([https://blogs.oracle.com/digitalassistant/post/introducing-the-new-oracle-digital-assistant-sql-dialog](https://blogs.oracle.com/digitalassistant/post/introducing-the-new-oracle-digital-assistant-sql-dialog))'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s address the issues with this process chain:'
  prefs: []
  type: TYPE_NORMAL
- en: Michael – who is Michael? Look around my hierarchy and determine if Michael
    is known. This is a whole process by itself and fundamental to people searching
    in an organization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If needed, disambiguate which Michael the user could be inferring.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Map employees to the SQL field (called EMP). The concept of employees will be
    requested in many ways – workers, teams, teammates, underlings, people, etc. It
    is unlikely a user will *ever* use the SQL field name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine Michael’s department. (Use SQL to get the answer. It is 23.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decide whether the default information to be returned needs to be enhanced based
    on the query (in this case, nothing special was asked of it).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Limit query by security implications (for example, don’t show salary).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Perform the search, determine the size of the results, and return the results
    or a chunk of results if #>30.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The final query should look something like this: **SELECT EMPNO, ENAME, JOB,
    MGR FROM EMP WHERE DEPTNO = 23 FETCH FIRST 30** **ROWS ONLY**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return the answer using a generative answer, wrapping the specific details from
    the database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The Oracle article does an excellent job of discussing synonyms. In their example,
    they use the Big Apple for New York City. It serves as a good reminder that language
    is very flexible, and there are many cases where, without this sort of intelligence,
    the natural language feel that customers expect won’t happen. Since the database
    fields don’t match the users’ language, there is some work to help with. The LLM
    can likely help understand terms and tagging concepts, but a product person must
    help it with cryptic field labels. For example, it might not understand that PH2
    is a cell phone field. Use the LLM to extend the understanding of synonyms for
    a cell phone (such as mobile, digits, contact info, wireless #, phone number,
    #).'
  prefs: []
  type: TYPE_NORMAL
- en: Service requests and other threaded sources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Service requests and other conversational sources, such as community discussions,
    are good data, but the kernels of truth within them must be exposed. Inferior
    results will occur if these sources are used without tagging and annotating correct
    answers. They are filled with wrong answers, half-truths, and misinformation.
    This is especially true for technical answers where the ground truth might be
    particular to a version or subversion of a product. So, confusing the difference
    between the 11.1 and 11.1.2 products can lead to incorrect results. And there
    can be red herrings in the answers, too. That is, there might be information that
    misleads or distracts from the problem and thus identifies the solution. It sometimes
    starts with “I don’t know if this matters, but…”.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most service request systems mark closed service requests as completed and
    require the agent to tag the correct answer for future processing or analysis.
    A more formal structure for SRs will give a better chance of mining this information.
    The wealth of important information in SRs must be addressed, and there are reasons
    to consider these sources:'
  prefs: []
  type: TYPE_NORMAL
- en: Customer language is a rich corpus of how customers talk about products, their
    issues, and how they interact in the real world. This domain-specific language
    and terminology are invaluable to training a model. Technical jargon, colloquialisms,
    initialisms, abbreviations, and shortcuts will appear more frequently in these
    sources than in traditional training, marketing, and technical documentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context is helpful in the LLM to create more accurate responses. Product release,
    patch levels, software installs, and operating system versions are typically what
    might be asked about when there is a problem, and this context can be very valuable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Commonality—the frequency of common questions helps the model understand the
    likelihood of this type of response being useful in the future.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical domain training—there might not be another place to find the situations
    being discussed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although most companies try to avoid some discussions in SRs and online channels,
    still be aware and avoid including PII in the model that might leak through in
    these forums. The process should support data cleansing and anonymization, as
    discussed in [*Chapter 2*](B21964_02_split_000.xhtml#_idTextAnchor031), *Conducting
    Effective* *User Research*, or synthesizing some data, as discussed earlier in
    this chapter. Doing this all manually is impossible at scale. Ultimately, these
    are just documents with the same issues as a knowledge base. Similar to databases,
    other pieces of software might be needed to access relevant information.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating external content via APIs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Be ready to call the right service with the right question. Creating effective
    interactions that perform tasks –filling out an expense report, scheduling an
    appointment on a calendar, or booking a holiday or vacation – all require backend
    services.
  prefs: []
  type: TYPE_NORMAL
- en: Many resources with advice on creating effective documents and resource retrieval
    were shared, but the solution’s success will still depend on the collection of
    services and software used. A few minutes on integrations is justified.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI can respond with an API call instead of just replying based on knowledge.
    A model can update a support ticket, ask for shipping information, look up prices
    or products, or perform other interactions the business relies on. Unsurprisingly,
    ChatGPT helps explain and write code to connect to several well-known APIs, but
    that is for development. Product people must know what is available and *how*
    to frame this interaction. For fun, try something like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Enterprise APIs will mostly be proprietary, and ChatGPT won’t help directly.
    However, since most REST work should be similar, it still might be able to help.
    Sometimes, integrations with third parties such as Zoom, Teams, Slack, Jira, Confluence,
    Salesforce, HubSpot, ServiceNow, Oracle, or other vendors are used internally
    or as part of an enterprise offering. Remember that all the work is still needed
    to authenticate, create a security layer, deal with hallucinations, handle error
    cases, and create a consistent experience. It is real work.
  prefs: []
  type: TYPE_NORMAL
- en: More robust approaches are evolving. This article on ToolLLM describes an approach
    to using ChatGPT to generate instructions for APIs and then figure out how to
    use them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [How to use thousands of APIS in LLMs](https://arxiv.org/abs/2307.16789?utm_source=tldrai)
    (ToolLLM paper) ([https://arxiv.org/abs/2307.16789?utm_source=tldrai](https://arxiv.org/abs/2307.16789?utm_source=tldrai))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Video: AI News: [An LLM that learns how to work with APIs](https://www.youtube.com/watch?v=lGxaE8FU2-Q)
    (ToolLLM paper) ([https://www.youtube.com/watch?v=lGxaE8FU2-Q](https://www.youtube.com/watch?v=lGxaE8FU2-Q))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply our testing and validation process here as it is for *any* input and
    output testing. As designers, PMs, and people who care about usability, try to
    understand whether the APIs provide the right level of service. Here are some
    items to look for when integrating with backend services:'
  prefs: []
  type: TYPE_NORMAL
- en: Can the required data be supplemented automatically? Users should not have to
    supply every piece of data. For example, the API might need five pieces of data
    to submit a valid request. Some can come from the context and focus the user on
    the essential elements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will the response time be fast enough to be integrated with the response? Think
    in milliseconds (200 or less would be good, 50 or less would be great, sub-10s
    are world-class).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can a single API be called instead of two or three? Optimized API calls help
    with cost, performance, and the number of round trips.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the data format consistent with the customer’s needs? If not, consider telling
    ChatGPT how to format it or providing conversations or translations in the correct
    format. For example, understand the user’s time zone and don’t use GMT or other
    time zones.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrations and actions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ChatGPT economy is growing in leaps and bounds. There are dozens of popular
    services and integrations to make processes more seamless and practical.
  prefs: []
  type: TYPE_NORMAL
- en: The development team might support other tools to help create a complete solution.
    It would be best to get involved to determine how to apply design thinking and
    your expertise to support a more sustainable process.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are plenty of libraries, tools, and resources online. Comparing and contrasting
    the wealth of options is out of scope, but a few examples that relate to making
    effective, well-designed solutions that OpenAI posted can be worth your time:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [OpenAI Cookbook](https://cookbook.openai.com/articles/related_resources)
    ([https://cookbook.openai.com/articles/related_resources](https://cookbook.openai.com/articles/related_resources))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [LangChain home page](https://www.langchain.com/langsmith) ([https://www.langchain.com/langsmith](https://www.langchain.com/langsmith))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Milvus Vector database](https://zilliz.com/blog/customizing-openai-built-in-retrieval-using-milvus-vector-database)
    ([https://zilliz.com/blog/customizing-openai-built-in-retrieval-using-milvus-vector-database](https://zilliz.com/blog/customizing-openai-built-in-retrieval-using-milvus-vector-database))'
  prefs: []
  type: TYPE_NORMAL
- en: The LLM can be just *one* of the services within the entire lifecycle or pipeline.
    This means faults can occur before or after the LLM. Look carefully before placing
    blame on the model. It is only as good as the input and instructions provided.
    Improve the quality of what is shared with the model. Design how to share data
    with the LLM and then test and verify how it works. *Be fully committed to an
    iterative lifecycle to make successful generative AI solutions.* Quality is all
    about the care and feeding process. Since improvements are only improvements if
    we measure them, this is explored in [*Chapter 10*](B21964_10_split_000.xhtml#_idTextAnchor216)*,
    Monitoring and Evaluation*. Ragas is one of those tools to consider using to measure
    how the RAG solution is performing. If this excites you, check it out now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Link: [Ragas Documentation](https://docs.ragas.io/en/latest/) ([https://docs.ragas.io/en/latest/](https://docs.ragas.io/en/latest/))'
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT has a concept called actions (formerly called plugins). These allow
    ChatGPT to connect to the rest of the internet. Actions rely on function calling
    to perform these actions. Recall that the Wove example used function calling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Documentation: [Actions in GPTs (Calling APIs)](https://platform.openai.com/docs/actions/introduction)
    ([https://platform.openai.com/docs/actions/introduction](https://platform.openai.com/docs/actions/introduction))'
  prefs: []
  type: TYPE_NORMAL
- en: What is impressive is that developers do not have to write these API queries
    by hand. ChatGPT has a bespoke LLM tuned to help developers write actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Demo: [ActionsGPT chat](https://chatgpt.com/g/g-TYEliDU6A-actionsgpt) ([https://chatgpt.com/g/g-TYEliDU6A-actionsgpt](https://chatgpt.com/g/g-TYEliDU6A-actionsgpt))'
  prefs: []
  type: TYPE_NORMAL
- en: Developers can send messages to the LLM to generate the base code. For example,
    they can try something like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Share these resources and this video with developers to help them get started.
  prefs: []
  type: TYPE_NORMAL
- en: 'Video: [Introduction to ChatGPT Actions](https://www.youtube.com/watch?v=pq34V_V5j18)
    ([https://www.youtube.com/watch?v=pq34V_V5j18](https://www.youtube.com/watch?v=pq34V_V5j18))
    (Actions start at 9:30)'
  prefs: []
  type: TYPE_NORMAL
- en: Figuring out these linkages is for development. As a product leader, know that
    a wealth of services is available for integration from enterprise sources to make
    the solution support intelligence that combines these. To create these connections,
    the paid version of ChatGPT, if not the enterprise version, is needed. In the
    ChatGPT video, Nick Turley hooks up his personal to-do list from asana.com to
    the chat instance in the demo. *Figure 6**.14* shows the current actions setup.
    It is a simple UI to name, describe, and define the instructions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – Setting up actions on the Configure tab
  prefs: []
  type: TYPE_NORMAL
- en: The demo goes further with embedding knowledge to help with summarizing. Watch
    to get a good sense of the integrations fundamental to enterprise solutions. At
    the 20-minute mark, it gets a little creative with a mood demo. They do a great
    demo of integrating with physical devices in the demo room and Spotify to play
    music. The point is that enterprise solutions can be more than just software integrations.
    Manufacturing, lighting, HVAC (air conditioning), processes, routing, planning,
    and more can be improved with intelligent integration. This takes us back to our
    chapter on use cases. There are lots of opportunities out there.
  prefs: []
  type: TYPE_NORMAL
- en: This book is intended to be practical even when the tools change—and they will—so
    don’t get hung up on a tool or direction. New and more robust services will be
    introduced frequently. Build a lean process that supports adaptation and change.
    Learn from the LLM community. The number of blogs, posts, and training opportunities
    is expanding daily.
  prefs: []
  type: TYPE_NORMAL
- en: Community resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A wealth of resources exists, including in the OpenAI community. Explore these
    resources, the latest videos, and research to get up to speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [RAG Community Discussion](https://community.openai.com/t/rag-is-not-really-a-solution/599291/2)
    ([https://community.openai.com/t/rag-is-not-really-a-solution/599291/2](https://community.openai.com/t/rag-is-not-really-a-solution/599291/2))'
  prefs: []
  type: TYPE_NORMAL
- en: In Ron Parker’s post, he discusses RAG as being brittle.
  prefs: []
  type: TYPE_NORMAL
- en: '"The biggest problem I’ve run into so far is that some query responses are
    not comprehensive enough. End-users can almost always get a complete answer using
    chain-of-thought queries (few-shot). But, the end-users I’ve been working with
    want complete answers to the first question (zero-shot). This may touch on your
    issue.'
  prefs: []
  type: TYPE_NORMAL
- en: 'My resolution: Deep Dive. Have the model dig through all the possible responses,
    categorize and analyze those, and then return a complete list of the best responses.
    Since I built my RAG system, I must also develop this feature. So I’m thinking,
    whatever you say this technique is you’re missing, you may have to build it yourself."'
  prefs: []
  type: TYPE_NORMAL
- en: This makes our point. This is about building solutions, and this puzzle has
    many pieces. He also points out a good usability issue. Users don’t want to have
    conversations to get to their answers. They want the complete answer on the first
    try. Even in our case study with Wove, they worked hard to return the best responses
    and iterate on the training to get the correct answers. They had to figure out
    the model and chunk the rate sheets, and then they refined those answers to improve
    the model. Again, it involves honest work by the development team; work with them
    to improve the quality with every step.
  prefs: []
  type: TYPE_NORMAL
- en: Or, check out a nice video showing how Mayo Oshin (**@maywaoshin**) used GPT-4
    to front-end thousands of pages from PDF documents—in this case, the last few
    years of Tesla’s annual reports. He walks through his architecture. Most of that
    will be too much, but he talks about how he converts documents into text and chunks
    of documents. This discussion is right on point for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'Video: [Example of 1000+ Pages of PDF](https://www.youtube.com/watch?v=Ix9WIZpArm0)
    ([https://www.youtube.com/watch?v=Ix9WIZpArm0](https://www.youtube.com/watch?v=Ix9WIZpArm0))'
  prefs: []
  type: TYPE_NORMAL
- en: The last resource for our discussion is the previously mentioned video on lessons
    learned around RAG.
  prefs: []
  type: TYPE_NORMAL
- en: 'Video: [Lessons Learned on LLM RAG Solutions](https://www.youtube.com/live/Y9qn4XGH1TI?si=iUs_x3yDL8BK7aUb56)
    ([https://www.youtube.com/live/Y9qn4XGH1TI?si=iUs_x3yDL8BK7aUb56](https://www.youtube.com/live/Y9qn4XGH1TI?si=iUs_x3yDL8BK7aUb56))'
  prefs: []
  type: TYPE_NORMAL
- en: 'They cover a variety of good things about managing documents. Here is a summary:'
  prefs: []
  type: TYPE_NORMAL
- en: They remind everyone to do good “data science” and make sure to have good data
    going in. Also, that accurate data is messy. Not only are tables tricky (something
    discussed in this chapter), but different document formats can need different
    libraries to help clean them (or the headache of manual cleaning).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explanations might not be directly linked to information. Comments or notes
    around negation ("does not have", "except for this version", "does not apply")
    can negate some documentation that does not even appear in a chunk that it relates
    to and might only be understood with additional editing or tagging.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintain structure. Convert documents to a data structure while preserving their
    meaning. For example, when PDFs are converted to text, a model can decide how
    to parse the PDFs, identify headings, and build out and capture information to
    put it into a meaningful structure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the documents are hierarchical, a flat representation is needed, so try to
    get to a list of elements. The key for a part of the element can represent a section.
    This kind of discussion is more for the data ingestion team, but look for these
    issues when testing. This way, test results can verify that context is maintained.
    Help the data scientists maintain quality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different methods result in different quality. As discussed, sentence-by-sentence
    embedding will lose some context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As discussed, get the proper context in chunks. It should not be too narrow
    or too broad; it should be *just right*. The approach in *Figure 6**.15* reminds
    us of what was discussed at the start of the chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B21964_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – Prolego’s approach is similar to our discussion earlier in the
    chapter
  prefs: []
  type: TYPE_NORMAL
- en: Several videos from Prolego (besides the one shared) are easy to digest and
    well-paced. This is just one example of the wealth of video and article resources
    that can help on a RAG journey. Don’t build everything yourself; the LLM vendors
    are only one piece of a more extensive solution that includes tools, documentation,
    databases, and APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Besides RAG, there are wonderful posts on every imaginable topic around LLMs
    on LinkedIn, shared in mailing lists, posted on YouTube, classes from universities,
    and vendor websites.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is a big step to prepare an existing knowledge base and data sources to make
    them available within a generative AI solution. It’s likely the most significant
    step because the hard work of creating the ChatGPT model was done for you. For
    many enterprise solutions, this can be an overwhelming task. Just start small.
    Learn from the use cases to prioritize solutions that provide the most significant
    value with the least cost (recall our scoring discussion in [*Chapter 4*](B21964_04.xhtml#_idTextAnchor085),
    *Scoring Stories*). Over time, land grabs can expand into other data sources and,
    thus, new use cases. All of this has to be done with quality in mind. Measuring
    and monitoring are critical. Newer doesn’t mean better. Mix and match ChatGPT
    models to perform specific tasks or optimize cost or performance by using one
    model over another. Use a collection of third-party resources—possibly even other
    models tuned to a particular problem space—to refine results, make data available
    to the model, or do additional integrations. Be aware of the impact of data cleaning
    and how the knowledge in the base model might impact the solution’s decision-making
    ability. Recognize that bias isn’t just about social or political positions; it
    can simply be about having too much data about one product, and this causes the
    model to miss smaller products. Getting all of this right with the enterprise
    data is a challenge.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter mainly focused on awareness and considered how techniques can influence
    the quality of inbound data sources. On the outbound side, testing and completing
    the feedback loop is a great way to improve the solution. There should be many
    opportunities to contribute before moving on to the next steps around prompt engineering
    and fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '| ![](img/B21964_06_16.jpg) | The links, book recommendations, and GitHub files
    in this chapter are posted on the reference page.Web page: [Chapter 6 References](https://uxdforai.com/references#C6)
    ([https://uxdforai.com/references#C6](https://uxdforai.com/references#C6)) |'
  prefs: []
  type: TYPE_TB
