- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Extracting Semantic Representations with spaCy Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will apply what we learned in [*Chapter 4*](B22441_04.xhtml#_idTextAnchor056)
    *,* to the **Airline Travel Information System** ( **ATIS** ), a well-known airplane
    ticket reservation system dataset. The data consists of utterances – sentences
    of users asking for information. First, we will extract the named entities, creating
    our own extraction patterns with **SpanRuler** . Then we will determine the intent
    of the user utterance with **DepedencyMatcher** patterns. We will also use the
    code to extract the intent and create our own custom spaCy component and use it
    to process large datasets faster with the **Language.pipe()** method.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting named entities with **SpanRuler**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting dependency relations with **DependencyMatcher**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a pipeline component using extension attributes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the pipeline with large datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll process a dataset. The dataset and the chapter code can
    be found at [https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition](https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition)
    .
  prefs: []
  type: TYPE_NORMAL
- en: We will use the **pandas** library to manipulate our dataset. We'll also used
    the **wget** command-line tool. **pandas** can be installed via **pip** and **wget**
    is preinstalled in many Linux distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting named entities with SpanRuler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many NLP applications, including semantic parsing, we start looking for meaning
    in a text by examining the entity types and placing an entity extraction component
    into our NLP pipelines. Named entities play a key role in understanding the meaning
    of user text.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll also start a semantic parsing pipeline by extracting the named entities
    from our corpus. To understand what sort of entities we want to extract, first,
    we’ll get to know the ATIS dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting to know the ATIS dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughout this chapter, we’ll work with the ATIS corpus. ATIS is a well-known
    dataset; it’s one of the standard benchmark datasets for intent classification.
    The dataset consists of utterances from customers who want to book a flight and/or
    get information about flights, including flight costs, destinations, and timetables.
  prefs: []
  type: TYPE_NORMAL
- en: 'No matter what the NLP task is, you should always go over your corpus with
    the naked eye. We want to get to know our corpus to integrate our observations
    of the corpus into our code. While viewing our text data, we usually keep an eye
    on the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What kinds of utterances are there? Is it a short text corpus or does the corpus
    consist of long documents or medium-length paragraphs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What sort of entities does the corpus include? Examples include people’s names,
    city names, country names, organization names, and so on. Which ones do we want
    to extract?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How is punctuation used? Is the text correctly punctuated, or is no punctuation
    used at all?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Did users follow the grammatical rules? How are the grammatical rules followed?
    Is the capitalization correct? Are there misspelled words?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before starting any processing, we’ll examine our corpus. Here’s how:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go ahead and download the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The dataset is a two-column CSV file.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If you are running the code on a Jupyter notebook, you can add a **!** before
    the commands to run them there.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll get some insights into the dataset statistics with **pandas** .
    **pandas** is a popular data manipulation library that is frequently used by data
    scientists. You can read more at [https://pandas.pydata.org/docs/getting_started/intro_tutorials/](https://pandas.pydata.org/docs/getting_started/intro_tutorials/)
    :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The **shape** attribute returns a tuple representing the dimensionality of **DataFrame**
    . We can see that the dataset has two columns and 4,978 rows.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s create a bar plot to see the number of utterances in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The **value_counts()** method returns a series containing counts of unique
    values. The pandas library uses Matplotlib under the hood to plot this in a bar
    chart; this is the result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Bar chart with utterance frequencies](img/B22441_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Bar chart with utterance frequencies
  prefs: []
  type: TYPE_NORMAL
- en: Most user requests are for information about flights, followed by requests about
    airfares. However, before extracting the utterances, we will learn how to extract
    named entities. Let’s do this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Defining LOCATION entities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, our goal is to extract **LOCATION** entities. The pipeline
    of the **en_core_web_sm** model already has an **NER** component. Let’s see what
    entities the default NER model extracts from the **"i want to fly from boston
    at 838 am and arrive in denver at 1110 in the** **morning"** utterance:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then we load the spaCy model and process a sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we display the entities with **displacy** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see the result in *Figure 5* *.2* :'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Entities extracted by the NER component](img/B22441_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – Entities extracted by the NER component
  prefs: []
  type: TYPE_NORMAL
- en: The NER model finds the **Global Political Entity** ( **GPE** ) for **boston**
    and **denver** , but knowing the cities is not sufficient. We want to know **from**
    and **where** they want to fly. This means that in this case, the **adpositions**
    (a cover term for prepositions and postpositions) are important. spaCy uses the
    Universal **Parts-of-Speech** ( **POS** ) tags, so the adposition tag is named
    **"ADP"** . You can see all the descriptions for the POS tags, dependency label,
    or entity type of spaCy in the glossary ( [https://github.com/explosion/spaCy/blob/master/spacy/glossary.py](https://github.com/explosion/spaCy/blob/master/spacy/glossary.py)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: Getting back to the utterance example, **from boston** and **in denver** are
    the entities we want to extract. Since we know exactly what POS tags and GPE entities
    we need to create the new entities, a good way of implementing this extraction
    is to rely on the **Tagger** and **EntityRecognizer** components of the NLP pipeline.
    We’ll do this by creating rules to extract the tokens based on the tags. spaCy
    makes this easy to do with the **SpanRuler** component.
  prefs: []
  type: TYPE_NORMAL
- en: Adding the SpanRuler component to our processing pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Customizing NLP pipelines is very straightforward with spaCy. Each pipeline
    is created with a combination of spaCy components. It might not be very clear
    at first, but when we load an out-of-the-box spaCy model, it already comes with
    several different components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The **nlp.pipe_names** attribute returns the component names, in order. *Figure
    5* *.3* shows all of them.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – The default components of the en_core_web_sm model](img/B22441_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – The default components of the en_core_web_sm model
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the **en_core_web_sm** model comes with the **['tok2vec', 'tagger',
    'parser', 'attribute_ruler', 'lemmatizer', 'ner']** components by default. Each
    component returns a processed **Doc** object, which is then passed on to the next
    component.
  prefs: []
  type: TYPE_NORMAL
- en: You can add components to the processing pipeline using the **Language.add_pipe()**
    method (here nlp is the object of the Language class, which we will be using to
    call **add_pipe()** ). The **.add_pipe()** method expects a string with the name
    of the component. Under the hood, this method takes care of creating the component,
    adds it to the pipeline, and then returns the component object.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **SpanRuler** component is an out-of-the-box component for rule-based span
    and named entity recognition. The component lets you add spans to **Doc.spans**
    and/or **Doc.ents** . Let’s try it for the first time:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we call the **add_pipe()** method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then we add the patterns using the **add_patterns()** method. They should be
    defined using a list of dictionaries containing the **"label"** and **"** **pattern"**
    keys:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: spaCy uses *Thinc* ( [https://thinc.ai/docs/api-config#registry](https://thinc.ai/docs/api-config#registry)
    ) registry, a system that maps string keys to functions. The **"span_ruler"**
    string name is the string to reference the **SpanRuler** component. We then define
    a pattern named **LOCATION** and add it to the component using the **add_patterns()**
    method.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Unlike in **doc.ents** , overlapping matches are allowed in **doc.spans** .
    By default, **SpanRuler** adds the matches as **spans** to the **doc.spans["ruler"]**
    group of spans. Let’s process the text again and check whether **SpanRuler** did
    its job. Since the component adds the Span to the **"ruler"** key, we need to
    specify this to render the spans with **displacy** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let’s see the results in *Figure 5* *.4* .
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Spans extracted with SpanRuler](img/B22441_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Spans extracted with SpanRuler
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the component identified the **"from boston"** and **"in denver"**
    Span. **SpanRuler** has some settings that you can change. This can be done via
    the config argument on the **nlp.pipe()** method or using the **config.cfg** file.
    Let’s add the spans to **Doc.ents** instead of **doc.spans["ruler"]** :'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we remove the component of the pipeline because we only have one component
    with the same name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then we set the **"annotate_ents"** parameter of the component to **True**
    . The entities added by **EntityRecognizer** are needed in our pattern, so we
    also need to set the overwrite parameter to **False** so we don’t overwrite them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now we create the component again with this config, add the previously created
    patterns and process the text again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: By doing all that, the matches become entities, not spans.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s see the new entities in *Figure 5* *.5* :'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Entities extracted using SpanRuler](img/B22441_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Entities extracted using SpanRuler
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the **{'GPE', 'boston'}** and **{'GPE', 'denver'}** entities
    don’t exist anymore; they’re now **{'from boston', 'LOCATION'}** and **{'in denver',
    'LOCATION'}** , respectively. Overlapping entities are not allowed in **Doc.ents**
    , so they are filtered using the **util.filter_spans** function by default. This
    function keeps the first longest span over shorter spans.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can override most of the **SpanRuler** settings. Some of the available
    settings are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**spans_filter** : A method to filter spans before they are assigned to **doc.spans**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ents_filter** : A method to filter spans before they are assigned to **doc.ents**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**validate** : A method to set whether patterns should be validated or passed
    to **Matcher** and **PhraseMatcher** as validate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we’ve learned how to create and extract entities using **SpanRuler**
    . Having the pattern to extract **LOCATION** entities, we can now move on and
    extract the intention of utterances. Let’s do this in the next section using **DependencyMatcher**
    .
  prefs: []
  type: TYPE_NORMAL
- en: Extracting dependency relations with DependencyMatcher
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To extract the *intent* of the utterances, we need to match tokens based on
    their syntax relationship with each other. The goal is to find out what sort of
    intent the user carries – to book a flight, purchase a meal on their already booked
    flight, cancel their flight, and so on. Every intent includes a verb (to book)
    and an object that the web acts on (flight, hotel, meal, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll extract transitive verbs and their direct objects from
    utterances. We’ll begin our intent recognition section by extracting the transitive
    verb and the direct object of the verb. Before we move on to extracting transitive
    verbs and their direct objects, let’s first quickly go over the concepts of transitive
    verbs and direct/indirect objects.
  prefs: []
  type: TYPE_NORMAL
- en: Linguistic primer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s explore some linguistic concepts related to sentence structure, including
    verbs and verb-object relations. A verb is a very important component of the sentence
    as it indicates the *action* in the sentence. The object of the sentence is the
    *thing or person* that is affected by the action of the verb. Hence, there’s a
    natural connection between the sentence verb and objects. The concept of transitivity
    captures verb-object relations. A transitive verb is a verb that needs an object
    to act upon. Let’s see some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In these example sentences, **bought** , **loved** , and **borrowed** are transitive
    verbs. In the first sentence, **bought** is the transitive verb and **flowers**
    is its object, the thing that has been bought by the sentence subject. **I Loved**
    – **his cat** and **borrowed** – **my book** are transitive verb-object examples.
    We’ll focus on the first sentence again – what happens if we erase the **flowers**
    object? Let’s see that here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You bought *what* ? Without an object, this sentence doesn’t carry any meaning
    at all. In the preceding sentences, each of the objects completes the meaning
    of the verb. This is a way of understanding whether a verb is transitive or not
    – erase the object and check whether the sentence remains semantically intact.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some verbs are transitive and some verbs are intransitive. An intransitive
    verb is the opposite of a transitive verb; it doesn’t need an object to act upon.
    Let’s see some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In all the preceding sentences, the verbs make sense without an object. If
    we erase all the words other than the subject and object, these sentences are
    still meaningful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Pairing an intransitive verb with an object doesn’t make sense. You can’t run
    someone or something, you can’t shine something or someone, and you certainly
    cannot die something or someone.
  prefs: []
  type: TYPE_NORMAL
- en: Sentence object
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we remarked before, the object is the thing or person that is affected by
    the verb’s action. The action stated by the verb is committed by the sentence
    subject and the sentence object is affected. A sentence can be direct or indirect.
    A direct object answers the questions *who* and *what* . You can find the direct
    object by asking **The subject {verb} what/who?** . Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'An *indirect object* answers the questions **for what** , **for whom** , and/or
    **to whom** . Let’s see some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Indirect objects are often preceded by the prepositions **to** , **for** , **from**
    , and so on. As you can see from these examples, an indirect object is also an
    object and is affected by the verb’s action, but its role in the sentence is a
    bit different. An indirect object is sometimes viewed as the recipient of the
    direct object.
  prefs: []
  type: TYPE_NORMAL
- en: This is all you need to know about transitive/intransitive verbs and direct/indirect
    objects to digest this chapter’s material. If you want to learn more about sentence
    syntax, you can read the great book *Linguistic Fundamentals for Natural Language
    Processing* by Emily Bender ( [https://dl.acm.org/doi/book/10.5555/2534456](https://dl.acm.org/doi/book/10.5555/2534456)
    ). We have covered the basics of sentence syntax, but this is still a great resource
    to learn about syntax in depth.
  prefs: []
  type: TYPE_NORMAL
- en: Matching patterns with the DependencyMatcher component
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **DependencyMatcher** component lets us match patterns to extract information,
    but instead of defining a list of adjacent tokens as with **SpanRuler** patterns,
    the **DependencyMatcher** patterns match tokens *specifying the relations between
    them* . The component works with the dependencies extracted by the **DependencyParser**
    component. Let’s see the kind of information this component extracts in an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let´s see the results in *Figure 5* *.6* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – The dependency arcs of the sentence (the rest of the sentence
    is omitted)](img/B22441_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – The dependency arcs of the sentence (the rest of the sentence is
    omitted)
  prefs: []
  type: TYPE_NORMAL
- en: The extracted dependency labels are the ones below the arcs. *Show* is a transitive
    verb, and in this sentence, its direct object is *flight* . This dependency is
    extracted by the **DependencyParser** component and is labeled as **dobj** ( direct
    object).
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to extract the intent, so we’ll define the pattern always looking
    for a **verb** and its **dobj** dependencies. **DependencyMatcher** uses Semgrex
    operators to define the patterns. The **Semgrex syntax** might be confusing at
    first, so let’s take it step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **DependencyMatcher** patterns consist of a list of dictionaries. The first
    dictionary defines an anchor token using **RIGHT_ID** and **RIGHT_ATTRS** . **RIGHT_ID**
    is a unique name for the right-hand node in the relation and **RIGHT_ATTRS** are
    the token attributes to match. The pattern format is the same pattern used with
    **SpanRuler** . In our pattern, the anchor token will be the **dobj** token, so
    the first dictionary is defined like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'As spaCy’s documentation says ( [https://spacy.io/usage/rule-based-matching/#dependencymatcher](https://spacy.io/usage/rule-based-matching/#dependencymatcher)
    ), after the first dictionary, the following dictionaries of the pattern should
    have the following keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEFT_ID** : The name of the left-hand node in the relation, which has been
    defined in an earlier node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**REL_OP** : An operator that describes how the two nodes are related'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RIGHT_ID** : A unique name for the right-hand node in the relation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RIGHT_ATTRS** : The token attributes to match for the right-hand node in
    the same format as patterns provided to the regular token-based as in **SpanRuler**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given these keys, we construct the pattern by indicating the left-hand node
    of the relation, defining a name for the new right-hand node, and indicating the
    operator to describe the relation between the two nodes. Getting back to our example,
    after defining **direct_object_token** as the anchor we will set **RIGHT_ID**
    of the next dictionary to be the **VERB** token and define the operator as **direct_object_token
    < verb_token** because the direct object is *the immediate dependent* of the verb.
    Here are some other operators that are supported by the **DependencyMatcher**
    (you can check the full list of operators here [https://spacy.io/usage/rule-based-matching/#dependencymatcher-operators](https://spacy.io/usage/rule-based-matching/#dependencymatcher-operators)
    ):'
  prefs: []
  type: TYPE_NORMAL
- en: '**A < B** : A is the immediate dependent of B'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A > B** : A is the immediate head of B'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A << B** : A is the dependent in a chain to B following dep → head paths'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A >> B** : A is the head in a chain to B following head → dep paths'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Don’t worry if these operations gave you a little headache; it also happened
    to me. Those are just a few of them, you can check the full list of operators
    at [https://spacy.io/usage/rule-based-matching#dependencymatcher-operators](https://spacy.io/usage/rule-based-matching#dependencymatcher-operators)
    . All right, let’s get back to our example and define the full pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are good to create **DependencyMatcher** :'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to pass the **vocabulary** object (the vocabulary is shared
    with the documents the matcher operates on):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we need to define a callback function that will take the following arguments:
    **matcher** , **doc** , **i** , and **matches** . The **matcher** argument refers
    to the matcher instance, **doc** is the document being analyzed, **i** is the
    index of the current match, and **matches** is a list detailing the matches found.
    We will create a **callback** function to show the intent in a single word, such
    as **bookFlight** , **cancelFlight** , **bookMeal** , and so on. The function
    will take the tokens of the match and print their lemma:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To add a rule to the **matcher** , we specify an ID key, one or more patterns,
    and the optional callback function to act on the matches. Finally, we process
    the text again and call the **matcher** object, passing this **doc** as the parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Great! The code prints **Intent: showFlightIntent** , so the recognition was
    successful. Here, we recognized a single intent, but some utterances may carry
    multiple intents. For example, consider the following utterance from the corpus:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the user wants to list all the flights and, at the same time, see the
    fare info. One way of processing is considering these intents as a single and
    complex intent. A common way of processing this sort of utterance is to label
    the utterance with multiple intents.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s see the **DEP** dependencies extracted by **DependencyParser** :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – The dependency arcs of the new sentence (the rest of the sentence
    is omitted)](img/B22441_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – The dependency arcs of the new sentence (the rest of the sentence
    is omitted)
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 5* *.7* , we see that the **dobj** arc connects **show** and **flights**
    . The **conj** (conjunction) arc connects **flights** and **fares** to indicate
    the conjunction relation. This relation is built by a conjunction such as **and**
    or **or** and indicates that a noun is joined to another noun by this conjunction.
    Now let’s write the code to recognize these two intents:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Converting the arc relationship to a **REL_OP** operator, **direct_object_token**
    will be the head of the relationship this time, so we will use the **>** operator
    since **direct_object_token** is *the immediate head* of the new **conjunction_token**
    . This is the new pattern to match two intents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also need to update the callback function so it can print the two intents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we just need to add this new rule to the matcher. Since the pattern ID
    already exists, the patterns will be extended:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With all of that set, we can now find the matches again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now the matcher finds the tokens for the two patterns, the first one and this
    new one, which matches two intents. Until now, we have just been *printing the
    intent* , but in a real setting, it’s a good idea to store this information on
    the **Doc** object. To do that, we’ll create our own spaCy component. Let’s learn
    how to do it in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a pipeline component using extension attributes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To create our component, we will use the **@Language.factory** decorator. A
    component factory is a callable that takes settings and returns a **pipeline component
    function** . The **@Language.factory** decorator also adds the name of the custom
    component to the registry, making it possible to use the **.add_pipe()** method
    to add the component to the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: spaCy allows you to set any custom attributes and methods on the **Doc** , **Span**
    , and **Token** objects, which become available as **Doc._.** , **Span._.** ,
    and **Token._.** . In our case, we will add **Doc._.intent** to **Doc** , taking
    advantage of spaCy’s data structures to store our data.
  prefs: []
  type: TYPE_NORMAL
- en: We will implement the component logic inside a Python class. spaCy expects the
    **__init__()** method to take the **nlp** and **name** arguments (spaCy fills
    then automatically), and the **__call__()** method should receive and return **Doc**
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create the **IntentComponent** class:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create the class. Inside the **__init__()** method, we create the
    **DependencyMatcher** instance, add the patterns to the matcher, and set the **intent**
    extension attribute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, inside the **__call__()** method, we find the matches and check whether
    it’s a **"TWO_INTENTS"** match. If so, we extract the tokens for this pattern
    and set the **doc._.intent** attribute; if not, in the **else** block, we extract
    the tokens for the **"** **INTENT"** match:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With this code, we register the custom extension in **Doc** by setting **doc._.intent
    = intent** on the **__call__()** method, where we find the matches and save the
    intent.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have the class for the custom component, the next step is to register
    it using the decorator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If you’re using a Jupyter Notebook and need to re-create the component, you’ll
    need to restart the kernel. If not, spaCy will give us an error since the component
    name was already registered.
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s it, that’s your first custom component! Congratulations! Now, to extract
    the intent, we just need to add the component to the pipeline. If we want to see
    the intent, we can access it with **doc._.intent** . Here’s how you can do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Cool, right? If you don’t remember, the dataset has 4,978 utterances. That’s
    not a very large number, but what if it was bigger? Could spaCy help us make it
    faster? Yes! In the next section, we will learn how to run our pipelines using
    the **Language.pipe()** method.
  prefs: []
  type: TYPE_NORMAL
- en: Running the pipeline with large datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **Language.pipe()** method processes texts as a stream and yields **Doc**
    objects in order. It buffers the texts in batches instead of one-by-one, since
    this is usually more efficient. If we want to get a specific doc, we need to call
    **list()** first because the method returns a Python generator that yields **Doc**
    objects. This is how you can do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we are getting a list of text utterances from the DataFrame
    we loaded at the beginning of the chapter and processing it in batches using **.pipe()**
    . Let’s compare the time difference by using and not using the **.** **pipe()**
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us a period of **27.12** seconds. Now, let’s use the following method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Using **nlp.pipe()** , we get the same results in 5.90 seconds. That’s a huge
    difference. We can also specify **batch_size** and the **n_process** to set the
    number of processors to use. There is also an option to disable components if
    you need to run **.pipe()** just to get the result of the text processed by specific
    components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Awesome, we’ve finished our first pipeline with our own custom component! Congratulations!
    Here is the full code of the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: spaCy makes the pipeline code tidy and organized, two qualities that are essential
    if we want to maintain our codebase.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to generate a complete semantic parse of utterances.
    First, you added a **SpanRuler** component to extract an NER entity that is significant
    to the use case context. Then, you learned how to use **DependencyMatcher** to
    perform intent recognition by analyzing sentence structure. Next, you also learned
    how to create your own custom spaCy component to extract the intent of the utterances.
    Finally, you saw how to process large datasets faster with the **Language.pipe()**
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Both **SpanRuler** and **DependencyMatcher** rely on the patterns we create.
    The process of creating these patterns is a back-and-forth process. We analyze
    the results, then test out new patterns, then analyze the results again, and so
    on. The goal of this chapter was to teach you how to use these tools so you can
    perform this process in your own projects.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapters, we will shift more toward machine learning methods. [*Chapter
    6*](B22441_06.xhtml#_idTextAnchor087) will cover how to use spaCy with Transformers.
  prefs: []
  type: TYPE_NORMAL
