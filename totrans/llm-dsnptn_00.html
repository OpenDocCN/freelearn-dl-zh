<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div>&#13;
			<div id="_idContainer004" epub:type="preface">&#13;
			</div>&#13;
		</div>&#13;
		<div id="_idContainer006">&#13;
			<h1 id="_idParaDest-5"><a id="_idTextAnchor004"/>Preface</h1>&#13;
			<p>Imagine building a skyscraper without blueprints—every floor constructed on the fly, with no clear plan to ensure stability, efficiency, or even functionality. Developing <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>) without a structured approach can feel much the same. These powerful models, capable of transforming industries and redefining human–computer interactions, are intricate structures that demand meticulous planning and execution. Without a framework to navigate their complexities, practitioners risk creating systems that are inefficient, unreliable, or unable to meet <span class="No-Break">their potential.</span></p>&#13;
			<p>This book, <em class="italic">LLM Design Patterns</em>, provides the blueprints you need. It is a practical guide for engineers, researchers, and innovators seeking to design, build, and implement LLMs effectively. It focuses on four critical pillars: preparing and preprocessing data, training and optimizing models, evaluating and interpreting their behavior, and integrating them seamlessly with advanced knowledge retrieval techniques. These domains are explored through the lens of design patterns, offering proven solutions to recurring challenges in <span class="No-Break">LLM development.</span></p>&#13;
			<p>The rapid evolution of LLMs brings both extraordinary opportunities and daunting challenges. Issues such as data quality, scalability, and interpretability demand adaptive methodologies and innovative strategies. This book equips practitioners at all levels with the design patterns to address these challenges head-on, providing actionable insights and frameworks to not only build models but excel in the rapidly advancing world of LLMs. Whether you’re constructing your first model or refining a cutting-edge application, this book ensures that your approach is as robust as the technology you seek <span class="No-Break">to harness.</span></p>&#13;
			<h1 id="_idParaDest-6"><a id="_idTextAnchor005"/>Who this book is for</h1>&#13;
			<p>This book is for anyone involved in the development, deployment, or application of LLMs, including <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">AI engineers and researchers</strong>: Individuals implementing LLM techniques in their projects</li>&#13;
				<li><strong class="bold">Data scientists and machine learning practitioners</strong>: Professionals seeking guidance on data preparation, model training, and optimization for LLMs</li>&#13;
				<li><strong class="bold">Software architects and project managers</strong>: Those aiming to structure and manage LLM-based projects, ensuring alignment with business and technical objectives</li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-7"><a id="_idTextAnchor006"/>What this book covers</h1>&#13;
			<p><a href="B31249_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, <em class="italic">Introduction to LLM Design Patterns</em>, provides a foundational understanding of LLMs and introduces the critical role of design patterns in <span class="No-Break">their development.</span></p>&#13;
			<p><a href="B31249_02.xhtml#_idTextAnchor035"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, <em class="italic">Data Cleaning for LLM Training</em>, equips you with practical tools and techniques that allow you to effectively clean your data for <span class="No-Break">LLM training.</span></p>&#13;
			<p><a href="B31249_03.xhtml#_idTextAnchor049"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Data Augmentation</em>, helps you understand the data augmentation pattern in depth, from increasing the diversity of your training dataset to maintaining <span class="No-Break">its integrity.</span></p>&#13;
			<p><a href="B31249_04.xhtml#_idTextAnchor072"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Handling Large Datasets for LLM Training</em>, allows you to learn advanced techniques for managing and processing massive datasets essential for training <span class="No-Break">state-of-the-art LLMs.</span></p>&#13;
			<p><a href="B31249_05.xhtml#_idTextAnchor084"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Data Versioning</em>, shows you how to implement effective data versioning strategies for <span class="No-Break">LLM development.</span></p>&#13;
			<p><a href="B31249_06.xhtml#_idTextAnchor095"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Dataset Annotation and Labeling</em>, lets you explore advanced techniques for creating well-annotated datasets that can significantly impact your LLM’s performance across <span class="No-Break">various tasks.</span></p>&#13;
			<p><a href="B31249_07.xhtml#_idTextAnchor108"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Training Pipeline</em>, helps you understand the key components of an LLM training pipeline, from data ingestion and preprocessing to model architecture and <span class="No-Break">optimization strategies.</span></p>&#13;
			<p><a href="B31249_08.xhtml#_idTextAnchor120"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Hyperparameter Tuning</em>, demonstrates what the hyperparameters in LLMs are and strategies for optimizing <span class="No-Break">them efficiently.</span></p>&#13;
			<p><a href="B31249_09.xhtml#_idTextAnchor141"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Regularization</em>, shows you different regularization techniques that are specifically tailored <span class="No-Break">to LLMs.</span></p>&#13;
			<p><a href="B31249_10.xhtml#_idTextAnchor162"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Checkpointing and Recovery</em>, outlines strategies for determining optimal checkpoint frequency, efficient storage formats for large models, and techniques for recovering from various types <span class="No-Break">of failures.</span></p>&#13;
			<p><a href="B31249_11.xhtml#_idTextAnchor181"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, <em class="italic">Fine-Tuning</em>, teaches you effective strategies for fine-tuning pre-trained <span class="No-Break">language models.</span></p>&#13;
			<p><a href="B31249_12.xhtml#_idTextAnchor191"><span class="No-Break"><em class="italic">Chapter 12</em></span></a>, <em class="italic">Model Pruning</em>, lets you explore model pruning techniques, designed to reduce model size while <span class="No-Break">maintaining performance.</span></p>&#13;
			<p><a href="B31249_13.xhtml#_idTextAnchor209"><span class="No-Break"><em class="italic">Chapter 13</em></span></a>, <em class="italic">Quantization</em>, gives you a look into quantization methods that can optimize LLMs for deployment on <span class="No-Break">resource-constrained devices.</span></p>&#13;
			<p><a href="B31249_14.xhtml#_idTextAnchor230"><span class="No-Break"><em class="italic">Chapter 14</em></span></a>, <em class="italic">Evaluation Metrics</em>, explores the most recent and commonly used benchmarks for evaluating LLMs across <span class="No-Break">various domains.</span></p>&#13;
			<p><a href="B31249_15.xhtml#_idTextAnchor247"><span class="No-Break"><em class="italic">Chapter 15</em></span></a>, <em class="italic">Cross-Validation</em>, shows you how to explore cross-validation strategies specifically designed <span class="No-Break">for LLMs.</span></p>&#13;
			<p><a href="B31249_16.xhtml#_idTextAnchor265"><span class="No-Break"><em class="italic">Chapter 16</em></span></a>, <em class="italic">Interpretability</em>, helps you understand how interpretability in LLMs refers to the model’s ability to understand and explain how the model processes inputs and <span class="No-Break">generates outputs.</span></p>&#13;
			<p><a href="B31249_17.xhtml#_idTextAnchor276"><span class="No-Break"><em class="italic">Chapter 17</em></span></a>, <em class="italic">Fairness and Bias Detection</em>, demonstrates that fairness in LLMs involves ensuring that the model’s outputs and decisions do not discriminate against or unfairly treat individuals or groups based on <span class="No-Break">protected attributes.</span></p>&#13;
			<p><a href="B31249_18.xhtml#_idTextAnchor286"><span class="No-Break"><em class="italic">Chapter 18</em></span></a>, <em class="italic">Adversarial Robustness</em>, helps you understand that adversarial attacks on LLMs are designed to manipulate the model’s output by making small, often imperceptible changes to <span class="No-Break">the input.</span></p>&#13;
			<p><a href="B31249_19.xhtml#_idTextAnchor295"><span class="No-Break"><em class="italic">Chapter 19</em></span></a>, <em class="italic">Reinforcement Learning from Human Feedback</em>, takes you through a powerful technique for aligning LLMs with <span class="No-Break">human preferences.</span></p>&#13;
			<p><a href="B31249_20.xhtml#_idTextAnchor305"><span class="No-Break"><em class="italic">Chapter 20</em></span></a>, <em class="italic">Chain-of-Thought Prompting</em>, demonstrates how you can leverage chain-of-thought prompting to improve your LLM’s performance on complex <span class="No-Break">reasoning tasks.</span></p>&#13;
			<p><a href="B31249_21.xhtml#_idTextAnchor315"><span class="No-Break"><em class="italic">Chapter 21</em></span></a>, <em class="italic">Tree-of-Thoughts Prompting</em>, allows you to implement tree-of-thoughts prompting to tackle complex reasoning tasks with <span class="No-Break">your LLMs.</span></p>&#13;
			<p><a href="B31249_22.xhtml#_idTextAnchor325"><span class="No-Break"><em class="italic">Chapter 22</em></span></a>, <em class="italic">Reasoning and Acting</em>, teaches you about the ReAct framework, a powerful technique for prompting your LLMs to not only reason through complex scenarios but also plan and simulate the execution of actions, similar to how humans operate in the <span class="No-Break">real world.</span></p>&#13;
			<p><a href="B31249_23.xhtml#_idTextAnchor339"><span class="No-Break"><em class="italic">Chapter 23</em></span></a>, <em class="italic">Reasoning </em><em class="italic">WithOut</em><em class="italic"> Observation</em>, teaches you the framework for providing LLMs with the ability to reason about hypothetical situations and leverage external <span class="No-Break">tools effectively.</span></p>&#13;
			<p><a href="B31249_24.xhtml#_idTextAnchor346"><span class="No-Break"><em class="italic">Chapter 24</em></span></a>, <em class="italic">Reflection Techniques</em>, demonstrates reflection in LLMs, which refers to a model’s ability to analyze, evaluate, and improve its <span class="No-Break">own outputs.</span></p>&#13;
			<p><a href="B31249_25.xhtml#_idTextAnchor355"><span class="No-Break"><em class="italic">Chapter 25</em></span></a>, <em class="italic">Automatic Multi-Step Reasoning and Tool Use</em>, helps you understand how automatic multi-step reasoning and tool use significantly expand the problem-solving capabilities of LLMs, enabling them to tackle complex, <span class="No-Break">real-world tasks.</span></p>&#13;
			<p><a href="B31249_26.xhtml#_idTextAnchor366"><span class="No-Break"><em class="italic">Chapter 26</em></span></a>, <em class="italic">Retrieval-Augmented Generation</em>, takes you through a technique that enhances the performance of Al models, particularly in tasks that require knowledge or data not contained within the model’s <span class="No-Break">pre-trained parameters.</span></p>&#13;
			<p><a href="B31249_27.xhtml#_idTextAnchor378"><span class="No-Break"><em class="italic">Chapter 27</em></span></a>, <em class="italic">Graph-Based RAG</em>, shows how to leverage graph-structured knowledge in RAG <span class="No-Break">for LLMs.</span></p>&#13;
			<p><a href="B31249_28.xhtml#_idTextAnchor389"><span class="No-Break"><em class="italic">Chapter 28</em></span></a>, <em class="italic">Advanced RAG</em>, demonstrates how you can move beyond these basic RAG methods and explore more sophisticated techniques designed to enhance LLM performance across a wide range <span class="No-Break">of tasks.</span></p>&#13;
			<p><a href="B31249_29.xhtml#_idTextAnchor400"><span class="No-Break"><em class="italic">Chapter 29</em></span></a>, <em class="italic">Evaluating RAG Systems</em>, equips you with the knowledge necessary to assess the ability of RAG systems to produce accurate, relevant, and factually <span class="No-Break">grounded responses.</span></p>&#13;
			<p><a href="B31249_30.xhtml#_idTextAnchor469"><span class="No-Break"><em class="italic">Chapter 30</em></span></a>, <em class="italic">Agentic Patterns</em>, shows you how agentic Al systems using LLMs can be designed to operate autonomously, make decisions, and take actions to achieve <span class="No-Break">specified goals.</span></p>&#13;
			<h1 id="_idParaDest-8"><a id="_idTextAnchor007"/>To get the most out of this book</h1>&#13;
			<p>To get the most out of this book, you should ideally have a foundational understanding of machine learning concepts and basic proficiency in Python programming. These prerequisites will help in grasping the technical methodologies and implementation strategies discussed throughout the chapters. Machine learning knowledge is essential for understanding key aspects of LLM development, such as model training, hyperparameter tuning, regularization techniques, and optimization processes. Python programming skills are particularly valuable as they enable you to implement and experiment with the design patterns, workflows, and algorithms presented in <span class="No-Break">the book.</span></p>&#13;
			<p>Familiarity with natural language processing (NLP) frameworks and tools, such as Hugging Face Transformers, spaCy, or NLTK, will further enhance the learning experience. These frameworks are commonly used in LLM development and provide a practical means to work with pre-trained models, tokenize text, and process linguistic data. Understanding how these tools function will enable you to focus on the higher-level concepts and design patterns without being bogged down by foundational programming or <span class="No-Break">NLP operations.</span></p>&#13;
			<p>For those less familiar with these areas, supplementary resources on machine learning basics, Python programming, and NLP tools are recommended. This book’s approach ensures that with some effort to bridge knowledge gaps, you can successfully navigate its concepts and apply them effectively in <span class="No-Break">real-world projects.</span></p>&#13;
			<p class="callout-heading">Note</p>&#13;
			<p class="callout">This book provides code snippets to illustrate LLM design patterns and implementation concepts. The code is intentionally focused on demonstrating ideas in a concise and readable way, rather than offering complete, executable programs. It is not intended for direct deployment or integration into production environments. You are encouraged to study and adapt the code to your own context, rather than copying and pasting it as is. For this reason, there is no accompanying GitHub repository; the examples presented are self-contained within the book and sufficient for understanding the intended concepts without requiring external code bases.</p>&#13;
			<h1 id="_idParaDest-9"><a id="_idTextAnchor008"/>Download the color images</h1>&#13;
			<p>We also provide a PDF file that has color images of the diagrams used in this book. You can download it <span class="No-Break">here: </span><a href="https://packt.link/gbp/9781836207030"><span class="No-Break">https://packt.link/gbp/9781836207030</span></a><span class="No-Break">.</span></p>&#13;
			<h1 id="_idParaDest-10"><a id="_idTextAnchor009"/>Conventions used</h1>&#13;
			<p>There are a number of text conventions used throughout <span class="No-Break">this book.</span></p>&#13;
			<p>Code in text: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles/X usernames. Here is an example: “Mount the downloaded WebStorm-10*.dmg disk image file as another disk in <span class="No-Break">your system.”</span></p>&#13;
			<p>A block of code is set <span class="No-Break">as follows:</span></p>&#13;
			<pre class="source-code">&#13;
# Model Architecture&#13;
model = AutoModelForCausalLM.from_pretrained(“gpt2”)&#13;
# Optimization&#13;
optimizer = AdamW(model.parameters(), lr=5e-5)</pre>			<p>Any command-line input or output is written <span class="No-Break">as follows:</span></p>&#13;
			<pre class="console">&#13;
pip install faiss-cpu sentence-transformers</pre>			<p>Bold: Indicates a new term, an important word, or words that you <span class="No-Break">see onscreen.</span></p>&#13;
			<p>Tips or <span class="No-Break">important notes</span></p>&#13;
			<p>Appear <span class="No-Break">like this.</span></p>&#13;
			<h1 id="_idParaDest-11"><a id="_idTextAnchor010"/>Get in touch</h1>&#13;
			<p>Feedback from our readers is <span class="No-Break">always welcome.</span></p>&#13;
			<p>General feedback: If you have questions about any aspect of this book, email us at <a href="mailto:customercare@packtpub.com">customercare@packtpub.com</a> and mention the book title in the subject of <span class="No-Break">your message.</span></p>&#13;
			<p>Errata: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit <a href="http://www.packtpub.com/support/errata">www.packtpub.com/support/errata</a> and fill in <span class="No-Break">the form.</span></p>&#13;
			<p>Piracy: If you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <a href="mailto:copyright@packt.com">copyright@packt.com</a> with a link to <span class="No-Break">the material.</span></p>&#13;
			<p>If you are interested in becoming an author: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please <span class="No-Break">visit </span><a href="http://authors.packtpub.com"><span class="No-Break">authors.packtpub.com</span></a><span class="No-Break">.</span></p>&#13;
			<h1 id="_idParaDest-12"><a id="_idTextAnchor011"/>Share Your Thoughts</h1>&#13;
			<p>Once you’ve read <em class="italic">LLM Design Patterns</em>, we’d love to hear your thoughts! Please <a href="https://packt.link/r/1-836-20703-4">click here to go straight to the Amazon review page for this book</a> and share <span class="No-Break">your feedback.</span></p>&#13;
			<p>Your review is important to us and the tech community and will help us make sure we’re delivering excellent <span class="No-Break">quality content.</span></p>&#13;
			<h1 id="_idParaDest-13"><a id="_idTextAnchor012"/>Download a free PDF copy of this book</h1>&#13;
			<p>Thanks for purchasing <span class="No-Break">this book!</span></p>&#13;
			<p>Do you like to read on the go but are unable to carry your print <span class="No-Break">books everywhere?</span></p>&#13;
			<p>Is your eBook purchase not compatible with the device of <span class="No-Break">your choice?</span></p>&#13;
			<p>Don’t worry, now with every Packt book you get a DRM-free PDF version of that book at <span class="No-Break">no cost.</span></p>&#13;
			<p>Read anywhere, any place, on any device. Search, copy, and paste code from your favorite technical books directly into your application. </p>&#13;
			<p>The perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free content in your <span class="No-Break">inbox daily</span></p>&#13;
			<p>Follow these simple steps to get <span class="No-Break">the benefits:</span></p>&#13;
			<ol>&#13;
				<li>Scan the QR code or visit the <span class="No-Break">link below</span></li>&#13;
			</ol>&#13;
			<p class="IMG---Figure"> </p>&#13;
			<div>&#13;
				<div id="_idContainer005" class="IMG---Figure">&#13;
					<img src="image/B31249_QR_Free_PDF.jpg" alt="" role="presentation" width="200" height="200"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a href="https://packt.link/free-ebook/978-1-83620-703-0">https://packt.link/free-ebook/978-1-83620-703-0</a></p>&#13;
			<ol>&#13;
				<li value="2">Submit your proof <span class="No-Break">of purchase</span></li>&#13;
				<li>That’s it! We’ll send your free PDF and other benefits to your <span class="No-Break">email directly</span></li>&#13;
			</ol>&#13;
		</div>&#13;
	</div></div>
<div id="book-content"><div id="sbo-rt-content"><div id="_idContainer007" class="Content" epub:type="part">&#13;
			<h1 id="_idParaDest-14" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor013"/>Part 1: Introduction and Data Preparation</h1>&#13;
			<p>We begin this book by introducing the foundational concepts necessary to understand and work with large language models (LLMs). In this part, you will explore the critical role of data preparation in building high-quality LLMs. From understanding the significance of design patterns in model development to handling the immense datasets required for training, we guide you through the initial steps of the LLM pipeline. The chapters in this part will help you master data cleaning techniques to improve data quality, data augmentation methods to enhance dataset diversity, and dataset versioning strategies to ensure reproducibility. You will also learn how to efficiently handle large datasets and create well-annotated corpora for specific tasks. By the end of this part, you will have the skills to prepare robust and scalable datasets, providing a solid foundation for advanced <span class="No-Break">LLM development.</span></p>&#13;
			<p>This part has the <span class="No-Break">following chapters:</span></p>&#13;
			<ul>&#13;
				<li><a href="B31249_01.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to LLM Design Patterns</em></li>&#13;
				<li><a href="B31249_02.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a>, <em class="italic">Data Cleaning for LLM Training</em></li>&#13;
				<li><a href="B31249_03.xhtml#_idTextAnchor049"><em class="italic">Chapter 3</em></a>, <em class="italic">Data Augmentation</em></li>&#13;
				<li><a href="B31249_04.xhtml#_idTextAnchor072"><em class="italic">Chapter 4</em></a>, <em class="italic">Handling Large Datasets for LLM Training</em></li>&#13;
				<li><a href="B31249_05.xhtml#_idTextAnchor084"><em class="italic">Chapter 5</em></a>, <em class="italic">Data Versioning</em></li>&#13;
				<li><a href="B31249_06.xhtml#_idTextAnchor095"><em class="italic">Chapter 6</em></a>, <em class="italic">Dataset Annotation and Labeling</em></li>&#13;
			</ul>&#13;
		</div>&#13;
		<div>&#13;
			<div id="_idContainer008">&#13;
			</div>&#13;
		</div>&#13;
		<div>&#13;
			<div id="_idContainer009" class="Basic-Graphics-Frame">&#13;
			</div>&#13;
		</div>&#13;
	</div></div></body></html>