<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Time Series Prediction and LSTM Using CNTK</h1>
                </header>
            
            <article>
                
<p>This chapter is dedicated to helping you understand more of the Microsoft Cognitive Toolkit, or CNTK. The inspiration for the examples contained within this chapter comes from the Python version of <strong>CNTK 106</strong>: <strong>Part A – Time Series prediction with LSTM (Basics)</strong>. As C# developers, the Python code is not what we will be using (although there are several ways in which we could) so we made our own C# example to mirror that tutorial. To make our example easy and intuitive, we will use the Sine function to predict future time-series data. Specifically, and more concretely, we will be using a <strong>long short-term memory recurrent neural network</strong>, sometimes called an <strong>LSTM-RNN</strong> or just <strong>LSTM</strong>. There are many variants of the LSTM; we will be working with the original.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>LSTM</li>
<li>Tensors</li>
<li>Static and dynamic axis</li>
<li>Loading datasets</li>
<li>Plotting data</li>
<li>Creating models</li>
<li>Creating mini-batches</li>
<li>And more…</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You will be required to have basic knowledge of .NET development using Microsoft Visual Studio and C#. You will need to download the code for this chapter from the book's website.</p>
<p><span>Check out the following video to see Code in Action: <a href="http://bit.ly/2xtDTto">http://bit.ly/2xtDTto</a>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Long short-term memory</h1>
                </header>
            
            <article>
                
<p><strong>Long short-term memory</strong> (<strong>LSTM</strong>) networks are a specialized form of recurrent neural network. They have the ability to retain long-term memory of things they have encountered in the past. In an LSTM, each neuron is replaced by what is known as a <strong>memory unit</strong>. This memory unit is activated and deactivated at the appropriate time, and is actually what is known as a <strong>recurrent self-connection</strong>.</p>
<p>If we step back for a second and look at the back-propagation phase of a regular recurrent network, the gradient signal can end up being multiplied many times by the weight matrix of the synapses between the neurons within the hidden layer. What does this mean exactly? Well, it means that the magnitude of those weights can then have a stronger impact on the learning process. This can be both good and bad.</p>
<p>If the weights are small they can lead to what is known as <strong>vanishing gradients</strong>, a scenario where the signal gets so small that learning slows to an unbearable pace or even, worse, comes to a complete stop. On the other hand, if the weights are large, this can lead to a situation where the signal is so large that learning diverges rather than converges. Both scenarios are undesirable but are handled by an item within the LSTM model known as a <strong>memory cell</strong>. Let's talk a little about this memory cell now.</p>
<p>A memory cell has four different parts. They are:</p>
<ul>
<li>Input gate, with a constant weight of 1.0</li>
<li>Self-recurrent connection neuron</li>
<li>Forget gate, allowing cells to remember or forget its previous state</li>
<li>Output gate, allowing the memory cell state to have an effect (or no effect) on other neurons</li>
</ul>
<p>Let’s take a look at this and try and make it all come together:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1268 image-border" src="assets/6d17878d-0f64-4358-b7b9-562bd00030b9.png" style="width:30.75em;height:13.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Memory Cell</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">LSTM variants</h1>
                </header>
            
            <article>
                
<p>There are many variants of the LSTM network. Some of these variants include:</p>
<ul>
<li>Gated recurrent neural network</li>
<li>LSTM4</li>
<li>LSTM4a</li>
<li>LSTM5</li>
<li>LSTM5a</li>
<li>LSMT6</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-606 image-border" src="assets/d0a9a772-368f-483e-8d30-60b111bedc35.jpg" style="width:28.25em;height:28.17em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Training and Test accuracy, σ = relu, η = 1e −4</div>
<p>One of those variants, a slightly more dramatic version of the LSTM, is called the gated recurrent unit, or GRU/GRNN. It combines the forget and input gates of the LSTM into a single gate called an <strong>update gate</strong>. This makes it simpler than the standard LSTM and has been increasingly growing in popularity.</p>
<p>Here is what a LSTM looks like:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-607 image-border" src="assets/91bf90ea-2e9b-4f36-92b3-bddbc00a713f.jpg" style="width:41.75em;height:16.08em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">LSTM</div>
<p>As you can see, there are various memory <em>gates</em> in the LSTM that the RNN does not have. This allows it to effortlessly retain both long-and short-term memory. So, if we want to understand text and need to look ahead or behind in time, LSTM is made for just such a scenario. Let’s talk about the different gates for a moment. As we mentioned, there are 3 of them. Let’s use the following phrase to explain how each of these works.</p>
<p><em>Bob lives in New York City. John talks to people on the phone all day, and commutes on the train.</em></p>
<p><strong>Forget Gate</strong>: As soon as we get to the period after the word City, the forget gate realizes that there may be a change of context in the works. As a result, the subject Bob is forgotten and the place where the subject was is now empty. As soon as the sentence turns to John, the subject is now John. This process is caused by the forget gate.</p>
<p><strong>Input Gate</strong>: So the important facts are that Bob lives in New York City, and that John commutes on the train and talks to people all day. However, the fact that he talks to people over the phone is not as important and can be ignored. The process of adding new information is done via the input gate.</p>
<p><strong>Output Gate</strong>: If we were to have a sentence <em>Bob was a great man. We salute ____</em>. In this sentence we have an empty space with many possibilities. What we do know is that we are going to salute whatever is in this empty space, and this is a verb describing a noun. Therefore, we would be safe in assuming that the empty space will be filled with a noun. So, a good candidate could be <em>Bob</em>. The job of selecting what information is useful from the current cell state and showing it as an output is the job of the output gate.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applications of LSTM</h1>
                </header>
            
            <article>
                
<p>The following are just a few of the applications of LSTM stacked networks:</p>
<ul>
<li>Speech recognition</li>
<li>Handwriting recognition</li>
<li>Time series prediction and anomaly detection</li>
<li>Business process management</li>
<li>Robotic control</li>
<li>And many more...</li>
</ul>
<p>Time series prediction itself can have a dramatic effect on the business's bottom line. We may need to predict in which day of the month, which quarter, or which year certain large expenses will occur. We may also have concerns over the Consumer Price Index over the course of time relative to our business. Increased prediction accuracy can lead to definite improvements to our bottom line.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CNTK terminology</h1>
                </header>
            
            <article>
                
<p>It is important that we understand some of the terminology used within the Microsoft CNTK toolkit. Let’s look at some of that terminology now:</p>
<ul>
<li><strong>Tensors</strong>: All CNTK inputs, outputs and parameters are organized as tensors. It should also be noted that <em>minibatches</em> are tensors as well.</li>
<li><strong>Rank</strong>: Each tensor has a rank. Scalars are tensors with a rank of 0, vectors are tensors and have a rank of 1, and matrices are tensors with a rank of 2.</li>
<li><strong>Static axis</strong>: The dimensions listed in 2 are referred to as <strong>axes</strong>. Every tensor has <em>static</em> and <em>dynamic</em> axes. A Static axis has the same length throughout its entire life.</li>
<li><strong>Dynamic axis</strong>: Dynamic axes, however, can vary their length from instance to instance. Their length is typically not known before each minibatch is presented. Additionally, they may be ordered.</li>
<li><strong>Minibatch</strong>: A minibatch is also a tensor. It has a dynamic axis, which is called the <strong>batch axis</strong>. The length of this axis can change from minibatch to minibatch.</li>
</ul>
<p>At the time of writing of CNTK supports one additional single additional dynamic axis, also known as the <strong>sequence axis</strong>. This axis allows the user to work with sequences in a more abstract, higher-level manner. The beauty of sequence is that whenever an operation is performed on a sequence, the CNTK toolkit does a type-checking operation to determine safety:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-608 image-border" src="assets/6fd1d49a-0dba-4156-8d7b-1b9a7a32a281.jpg" style="width:35.08em;height:18.17em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Sequence Axis</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Our example</h1>
                </header>
            
            <article>
                
<p>So now that we have covered some basics for both CNTK and LSTM, it’s time to dive into our example application. You can find this project with the code that accompanies this book. Make sure you have it open in Microsoft Visual Studio before proceeding. You can follow the instructions in the upcoming <em>The Code</em> section if you need further instructions.</p>
<p>The example we are creating uses Microsoft CNTK as a back-end and will use a simple sine wave as our function. The sine wave was plotted earlier and is used due to it’s being widely familiar to most individuals.</p>
<p>Here are screenshots of what our example application looks like:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1269 image-border" src="assets/fce45926-766b-46ad-8232-31c95178763f.png" style="width:51.75em;height:29.33em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Main page – training data</span></div>
<p>The preceding screenshot shows our main screen, displaying our sine wave data points, our training data. The goal is to get our training data (blue) to match the red as closely as possible in shape, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1270 image-border" src="assets/5766a404-4601-41f8-99f9-1f060fd0edd1.png" style="width:51.83em;height:29.25em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Main Page - Charts</div>
<p>The following screen allows us to plot our loss function:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1271 image-border" src="assets/d613e562-4775-485c-964a-8e6dca6215ac.png" style="width:51.58em;height:29.17em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Plotting the loss value (Loss Value tab)</span></div>
<p>The following screen allows us to plot our observed versus predicted values. The goal is to have the predicted values (blue) match as closely as possible the actual values (shown in red):</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1273 image-border" src="assets/bd00e35b-8e22-4eda-b904-39fca2b0d31e.png" style="width:51.67em;height:29.42em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Plotting Observed versus Predicted (Test Data tab)</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Coding our application</h1>
                </header>
            
            <article>
                
<p>Let’s now look at our code. For this you will need to reference the <kbd>LSTMTimeSeriesDemo</kbd> project that accompanies this book. Open the project in Microsoft Visual Studio. All the required CNTK libraries are already referenced in this project for you and included in the Debug/bin directory. The main library we will be using is <kbd>Cntk.Core.Managed</kbd>. We are using version 2.5.1 for this example, in case you were wondering!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading data and graphs</h1>
                </header>
            
            <article>
                
<p>To load our data and graphs, we have two main functions that we will<br/>
use; <kbd>LoadTrainingData()</kbd>and <kbd>PopulateGraphs()</kbd>. Pretty straightforward, right?:</p>
<pre>private void Example_Load(object sender, EventArgs e)<br/>{<br/> LoadTrainingData(DataSet?["features"].train, DataSet?["label"].train);<br/>PopulateGraphs(DataSet?["label"].train, DataSet?["label"].test);<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading training data</h1>
                </header>
            
            <article>
                
<p>For this example, we are simply making up our test and training data on-the-fly. The <kbd>LoadTrainingData()</kbd> function does just that:</p>
<pre>private void LoadTrainingData(float[][] X, float[][] Y)<br/> {<br/> //clear the list first<br/> listView1.Clear();<br/> listView1.GridLines = true;<br/> listView1.HideSelection = false;<br/> if (X == null || Y == null )<br/> return;<br/> <br/> //add features<br/> listView1.Columns.Add(new ColumnHeader() {Width=20});<br/> for (int i=0; i &lt; inDim ;i++)<br/> {<br/> var col1 = new ColumnHeader<br/> {<br/> Text = $"x{i + 1}",<br/> Width = 70<br/> };<br/> listView1.Columns.Add(col1);<br/> }<br/> <br/> //Add label<br/> var col = new ColumnHeader<br/> {<br/> Text = $"y",<br/> Width = 70<br/> };<br/> listView1.Columns.Add(col);<br/>for (int i = 0; i &lt; 100; i++)<br/> {<br/> var itm = listView1.Items.Add($"{(i+1).ToString()}");<br/> for (int j = 0; j &lt; X[i].Length; j++)<br/> itm.SubItems.Add(X[i][j].ToString(CultureInfo.InvariantCulture));<br/> itm.SubItems.Add(Y[i][0].ToString(CultureInfo.InvariantCulture));<br/> }<br/> }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Populating the graphs</h1>
                </header>
            
            <article>
                
<p>This function populates the graphs with training and test data:</p>
<pre>private void PopulateGraphs(float[][] train, float[][] test)<br/> {<br/> if (train == null)<br/> throw new ArgumentException("TrainNetwork parameter cannot be null");<br/> if (test == null)<br/> throw new ArgumentException("test parameter cannot be null");<br/>for (int i=0; i&lt;train.Length; i++)<br/>trainingDataLine?.AddPoint(new PointPair(i + 1, train[i][0]));<br/>for (int i = 0; i &lt; test.Length; i++)<br/>testDataLine?.AddPoint(new PointPair(i + 1, test[i][0]));<br/>zedGraphControl1?.RestoreScale(zedGraphControl1.GraphPane);<br/>zedGraphControl3?.RestoreScale(zedGraphControl3.GraphPane);<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Splitting data</h1>
                </header>
            
            <article>
                
<p>With this function we mirror frameworks such as Python, which make it very easy to split training and testing data from the main dataset. We have created our own function to do the same thing:</p>
<pre>static (float[][] train, float[][] valid, float[][] test) SplitDataForTrainingAndTesting(float[][] data, float valSize = 0.1f, float testSize = 0.1f)<br/>{<br/> if (data == null)<br/> throw new ArgumentException("data parameter cannot be null");<br/><span>//Calculate the data needed<br/></span>var posTest = (int)(data.Length * (1 - testSize));<br/> var posVal = (int)(posTest * (1 - valSize));<br/> return (<br/> data.Skip(0).Take(posVal).ToArray(), <br/> data.Skip(posVal).Take(posTest - posVal).ToArray(), <br/>data.Skip(posTest).ToArray());<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running the application</h1>
                </header>
            
            <article>
                
<p>Once we click the <strong>Run</strong> button, we will execute the function outlined as follows. We first determine the number of iterations the user wants to use, as well as the batch size. After setting up our progress bar and some internal variables, we call our <kbd>TrainNetwork()</kbd> function:</p>
<pre>private void btnStart_Click(object sender, EventArgs e)<br/>{<br/>int iteration = int.Parse(textBox1.Text);<br/> batchSize = int.Parse(textBox2.Text);<br/>progressBar1.Maximum = iteration;<br/>progressBar1.Value = 1;<br/>inDim = 5;<br/> ouDim = 1;<br/> int hiDim = 1;<br/> int cellDim = inDim;<br/>Task.Run(() =&gt; TrainNetwork(DataSet, hiDim, cellDim, iteration, batchSize, ReportProgress));<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the network</h1>
                </header>
            
            <article>
                
<p>In every neural network we must train the network in order for it to recognize whatever we are providing it. Our <kbd>TrainNetwork()</kbd> function does just that:</p>
<pre>private void TrainNetwork(Dictionary&lt;string, (float[][] train, float[][] valid, float[][] test)&gt; dataSet, int hiDim, int cellDim, int iteration, int batchSize, Action&lt;Trainer, Function, int, DeviceDescriptor&gt; progressReport)<br/>{<br/>Split the dataset on TrainNetwork into validate and test parts<br/>var featureSet = dataSet["features"];<br/>var labelSet = dataSet["label"];</pre>
<p>Create the model, as follows:</p>
<pre>var feature = Variable.InputVariable(new int[] { inDim }, DataType.Float, featuresName, null, false /*isSparse*/);<br/> var label = Variable.InputVariable(new int[] { ouDim }, DataType.Float, labelsName, new List&lt;CNTK.Axis&gt;() { CNTK.Axis.DefaultBatchAxis() }, false);<br/> var lstmModel = LSTMHelper.CreateModel(feature, ouDim, hiDim, cellDim, DeviceDescriptor.CPUDevice, "timeSeriesOutput");<br/> Function trainingLoss = CNTKLib.SquaredError(lstmModel, label, "squarederrorLoss");<br/> Function prediction = CNTKLib.SquaredError(lstmModel, label, "squarederrorEval");</pre>
<p>Prepare for training:</p>
<pre>TrainingParameterScheduleDouble learningRatePerSample = new TrainingParameterScheduleDouble(0.0005, 1);<br/>TrainingParameterScheduleDouble momentumTimeConstant = CNTKLib.MomentumAsTimeConstantSchedule(256);<br/>IList&lt;Learner&gt; parameterLearners = new List&lt;Learner&gt;()<br/>{<br/>Learner.MomentumSGDLearner(lstmModel?.Parameters(), learningRatePerSample, momentumTimeConstant, /*unitGainMomentum = */true)<br/>};</pre>
<p>Create the trainer, as follows:</p>
<pre>       var trainer = Trainer.CreateTrainer(lstmModel, trainingLoss, prediction, parameterLearners);</pre>
<p>Train the model, as follows:</p>
<pre>for (int <strong>i</strong> = 1; <strong>i</strong> &lt;= iteration; <strong>i</strong>++)<br/>{</pre>
<p>Get the next minibatch amount of data, as follows:</p>
<pre>foreach (var batchData infrom miniBatchData in GetNextDataBatch(featureSet.train, labelSet.train, batchSize)<br/>let xValues = Value.CreateBatch(new NDShape(1, inDim), miniBatchData.X, DeviceDescriptor.CPUDevice)<br/>let yValues = Value.CreateBatch(new NDShape(1, ouDim), miniBatchData.Y, DeviceDescriptor.CPUDevice)<br/>select new Dictionary&lt;Variable, Value&gt;<br/>{<br/>{ feature, xValues },<br/>{ label, yValues }})<br/>{</pre>
<p>Train, as follows:</p>
<pre>trainer?.TrainMinibatch(batchData, DeviceDescriptor.CPUDevice);<br/>} <br/>if (InvokeRequired)<br/>{<br/>Invoke(new Action(() =&gt; progressReport?.Invoke(trainer, lstmModel.Clone(), <strong>i</strong><span>, DeviceDescriptor.CPUDevice)));<br/></span>}<br/>else<br/>{<br/>progressReport?.Invoke(trainer, lstmModel.Clone(), <strong>i</strong><span>, DeviceDescriptor.CPUDevice);<br/></span>}<br/>}<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a model</h1>
                </header>
            
            <article>
                
<p>To create a model, we are going to build a one-directional recurrent neural network that contains <strong>long short-term memory</strong> (<strong>LSTM</strong>) cells as shown in the following:</p>
<pre>public static Function CreateModel(Variable input, int outDim, int LSTMDim, int cellDim, DeviceDescriptor device, string outputName)<br/>{<br/>Func&lt;Variable, Function&gt; pastValueRecurrenceHook = (x) =&gt; CNTKLib.PastValue(x);</pre>
<p>Create a LSTM cell for each input variable, as follows:</p>
<pre>Function LSTMFunction = LSTMPComponentWithSelfStabilization&lt;float&gt;(input,  new[] { LSTMDim }, new[] { cellDim }, pastValueRecurrenceHook, pastValueRecurrenceHook, device)?.Item1;</pre>
<p><span>After the LSTM sequence is created, return the last cell in order to continue generating the network, as follows:</span></p>
<pre class="mce-root">pre&gt;       Function lastCell = CNTKLib.SequenceLast(LSTMFunction);</pre>
<p>Implement dropout for 20%, as follows:</p>
<pre>       var dropOut = CNTKLib.Dropout(lastCell,0.2, 1);</pre>
<p>Create the last dense layer before the output, as follows:</p>
<pre> return FullyConnectedLinearLayer(dropOut, outDim, device, outputName);<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting the next data batch</h1>
                </header>
            
            <article>
                
<p>We get our next batch of data in an enumerable fashion. We first verify parameters, and then call the <kbd>CreateBatch()</kbd> function, which is listed after this function:</p>
<pre>private static IEnumerable&lt;(float[] X, float[] Y)&gt; GetNextDataBatch(float[][] X, float[][] Y, int mMSize)<br/>{<br/>if (X == null)<br/> throw new ArgumentException("X parameter cannot be null");<br/> if (Y == null)<br/> throw new ArgumentException("Y parameter cannot be null");<br/>for (int i = 0; i &lt;= X.Length - 1; i += mMSize)<br/> {<br/> var size = X.Length - i;<br/> if (size &gt; 0 &amp;&amp; size &gt; mMSize)<br/> size = mMSize;<br/>var x = CreateBatch(X, i, size);<br/> var y = CreateBatch(Y, i, size);<br/>yield return (x, y);<br/> }<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a batch of data</h1>
                </header>
            
            <article>
                
<p>Given a dataset, this function will create <em>batches</em> of data for use in traversing the total data set in more manageable segments. You can see how it is called from the <kbd>GetNextDataBatch()</kbd> function shown previously:</p>
<pre>internal static float[] CreateBatch(float[][] data, int start, int count)<br/>{<br/> var lst = new List&lt;float&gt;();<br/> for (int i = start; i &lt; start + count; i++)<br/> {<br/> if (i &gt;= data.Length)<br/> break;<br/>lst.AddRange(data[i]);<br/>}<br/>return lst.ToArray();<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How well do LSTMs perform?</h1>
                </header>
            
            <article>
                
<p>We tested LSTMs against predictions and historical values for sunspot data prediction, a very famous test in deep learning. As you can see, the red plot, which is our predictions, melded into the trend exactly, which is a tremendously encouraging sign:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-613 image-border" src="assets/e010f68f-0234-4e2f-b147-9b1904874f80.jpg" style="width:36.83em;height:14.50em;"/></p>
<p class="CDPAlignCenter CDPAlign">Prediction versus Performance</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned about long short-term memory recurrent neural networks. We programmed an example application showing how to use them, and we learned some basic terminology along the way. We covered topics such as LSTMs, tensors, static and dynamic axes, loading datasets, plotting data, creating models, and creating minibatches. In our next chapter, we will visit a very close cousin of LSTM networks: gated recurrent units.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li>Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.</li>
<li>Gers, F. A., Schmidhuber, J., &amp; Cummins, F. (2000). Learning to forget: Continual prediction with LSTM. Neural computation, 12(10), 2451-2471.</li>
<li>Graves, Alex. Supervised sequence labeling with recurrent neural networks. Vol. 385. Springer, 2012.</li>
</ul>
<ul>
<li>Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. IEE TRANSACTIONS ON NEURAL NETWORKS, 5, 1994.</li>
<li>F. Chollet. Keras github.</li>
<li>J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling, 2014.</li>
<li>S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9:1735–1780, 1997.</li>
<li>Q. V. Le, N. Jaitly, and H. G. E. A simple way to initialize recurrent networks of rectified linear units. 2015.</li>
<li>Y. Lu and F. Salem. Simplified gating in long short-term memory (lstm) recurrent neural networks. arXiv:1701.03441, 2017.</li>
<li>F. M. Salem. A basic recurrent neural network model. arXiv preprint arXiv:1612.09022, 2016.</li>
<li>F. M. Salem. Reduced parameterization of gated recurrent neural networks. MSU Memorandum, 7.11.2016.</li>
</ul>


            </article>

            
        </section>
    </body></html>