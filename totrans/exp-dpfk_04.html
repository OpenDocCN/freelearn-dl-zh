<html><head></head><body>
		<div><h1 id="_idParaDest-72" class="chapter-number"><a id="_idTextAnchor071"/>4</h1>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor072"/>The Deepfake Workflow</h1>
			<p>Creating a deepfake is an involved process. The tools within the various software applications help to significantly reduce the amount of manual work required; however, they do not eliminate this requirement entirely. Most of this manual work involves collecting and curating source material, as well as cleaning up data for the final swap.</p>
			<p>Whilst there are various applications available for creating deepfakes this chapter will use the open source software Faceswap (<a href="http://www.Faceswap.dev">http://www.Faceswap.dev</a>). The general workflow for creating a deepfake is the same from application to application, but you will find the nuances and available options vary between packages.</p>
			<p>It is also worth noting that Faceswap, at its core, is a command-line application. However, it also comes with a GUI that acts as a wrapper to launch the various processes. Within this chapter, the GUI will be used to illustrate the workflow; however, most of the tasks performed here can also be run from the command-line.</p>
			<p>In this chapter, the deepfake workflow will be covered from the inception of the swap to the final product. Specifically, the following topics will be covered:</p>
			<ul>
				<li>Identifying suitable candidates for a swap</li>
				<li>Preparing the training images</li>
				<li>Training a model</li>
				<li>Applying a trained model to perform a swap</li>
			</ul>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor073"/>Technical requirements</h1>
			<p>As with all machine learning techniques, deepfakes can be created on any PC with a minimum of 4 GB of RAM. However, a machine with 8 GB of RAM or higher and a GPU (a graphics card) is strongly recommended. Training a model on a CPU is likely to take months to complete, which does not make it a realistic endeavor. Graphics cards are built specifically to perform matrix calculations, which makes them ideal for machine learning tasks.</p>
			<p>Faceswap will run on Linux, Windows, and Intel-based macOS systems. At a minimum, Faceswap should be run on a system with 4 GB of VRAM (GPU memory). Ideally, an NVIDIA GPU should be used, as AMD GPUs are not as fully featured as their Nvidia counterparts and run considerably slower. Some features that are available for NVIDIA users are not available for AMD users, due to NVIDIA’s proprietary CUDA library being accepted as an industry standard for machine learning. GPUs with more VRAM will be able to run more of the larger Faceswap models than smaller GPUs.</p>
			<p>It is also possible to rent cloud services (such as Google’s Cloud Compute or Amazon’s AWS) to run Faceswap remotely.</p>
			<p>Installing and setting up the software will not be covered in this chapter, as the method will vary between OSs, and detailed installation instructions can be found on the Faceswap website (<a href="https://forum.Faceswap.dev/viewforum.php?f=4">https://forum.Faceswap.dev/viewforum.php?f=4</a>).</p>
			<h1 id="_idParaDest-75"><a id="_idTextAnchor074"/>Identifying suitable candidates for a swap</h1>
			<p>While it is technically<a id="_idIndexMarker121"/> possible to swap any face with another, creating a convincing deepfake requires paying some attention to the attributes of your source and destination faces. Depending on what you hope to achieve from your deepfake, this may be more or less important to you, but assuming that you wish to create a convincing swap, you should pay attention to the following attributes.</p>
			<ul>
				<li><strong class="bold">Face/head shape</strong>: Are the<a id="_idIndexMarker122"/> shapes of the faces similar to one another? If one face is quite narrow and the other quite round, then while the facial features will be correct, the final swap is unlikely to be particularly convincing if the final swap contains a head shape that is significantly different from the individual you are attempting to target.</li>
				<li><strong class="bold">Hairline/hairstyles</strong>: While it is possible to do full head swaps, these are generally harder to pull off, as hair is complex, and hairstyles can change significantly. You will generally be swapping the face but keeping the hair from the original material, so you need to keep hairline and hairstyles in mind when considering your swap.</li>
				<li><strong class="bold">Skin tone</strong>: The neural network will do some work in matching the skin tone between the faces that you train the model on; however, this will only work to a certain extent. As some attributes of the original face are likely to still exist within the final<a id="_idIndexMarker123"/> swap when it is blended into the original frame, it is important to ensure that the natural skin tone between the faces is not significantly different.</li>
			</ul>
			<p>Once you have<a id="_idIndexMarker124"/> identified your candidates for creating a deepfake, it is time to collect data to train the model.</p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor075"/>Preparing the training images</h1>
			<p>In this section, we <a id="_idIndexMarker125"/>will be collecting, extracting, and curating the images to train our model. Far and away the best sources for collecting face data are video files. Videos are just a series of still images, but as you can obtain 25 still images for every second of video in a standard 25 FPS file, they are a valuable and plentiful resource. Video is also likely to contain a lot more natural and varied poses than photographs, which tend to be posed and contain limited expressions.</p>
			<p>Video sources should be of a high quality. The absolute best source of data is HD content encoded at a high bitrate. You should be wary of video content acquired from online streaming platforms, as these tend to be of a low bitrate, even if the resolution is high. For similar reasons, JPEG images can also be problematic. The neural network will learn to recreate what it sees, and this will include learning compression artifacts from low-bitrate/highly compressed sources. Footage filmed on a modern-day smartphone or better, or extracted from Blu-ray or DVD sources, is ideal. One caveat to this is that you should avoid using HDR footage at all costs. HDR, by its very nature, contains images within a dynamic range. Neural networks expect the data they receive to be within a consistent range, so they struggle with HDR data and, quite often, cannot learn at all when provided with this kind of data.</p>
			<p>You are looking to collect material from as many different sources as possible. It is a misconception that a model is trained for a specific scene. Neural networks of the type that deepfakes use benefit from highly varied data. As the neural network is looking to encode important features for each of the faces it sees, giving it as much varied data as possible will enable it to generate better encodings and better feature maps. This includes variety in poses, expressions, and lighting conditions. While the neural network will do some work to simulate different lighting conditions, this is to help augment already varied data rather than act as a replacement for missing data. The neural network will not <a id="_idIndexMarker126"/>be able to create poses and expressions that are significantly different from anything it has seen before.</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor076"/>Extracting faces from your source data</h2>
			<p>Now that you<a id="_idIndexMarker127"/> have a variety of sources, the next <a id="_idIndexMarker128"/>step is to extract <a id="_idIndexMarker129"/>and <strong class="bold">align</strong> the faces (a process that normalizes the faces in the images) from these sources to build your training set. You are looking to collect between 500 to 50,000 faces for each side that you intend to train on. The variety of data is more important than the quantity of data. Five-hundred highly varied faces will lead to far superior results than 50,000 near-identical faces.</p>
			<p>During the extraction process, an “alignments file” will be created. This file (with a <code>.fsa</code> extension) contains information about the faces that have been discovered within each of your sources. With this in mind, it is good practice to set up a project folder structure to store your data so that you can easily locate and edit any of the data as required. A reasonable structure could be along the following lines:</p>
			<div><div><img src="img/B17535_04_001.jpg" alt="Figure 4.1 – A suggested Faceswap folder structure"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – A suggested Faceswap folder structure</p>
			<p>For each side of the model (<strong class="bold">A</strong> and <strong class="bold">B</strong>), we are creating a folder to store the source videos in, along with their associated generated alignments files (<strong class="bold">Videos</strong>), and a faces folder to store the extracted faces (<strong class="bold">Faces</strong>). If you are extracting from images as a source for faces, then you can add an <strong class="bold">Images</strong> folder.</p>
			<p>Copy the video and image source files to their associated folders and launch the Faceswap application:</p>
			<div><div><img src="img/Image98335.jpg" alt="Figure 4.2 – The Faceswap GUI"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – The Faceswap GUI</p>
			<p>The application is divided up into separate sections, depending on the task that you are currently performing. At this stage, we are extracting faces, so make sure that the <strong class="bold">Extract</strong> tab is selected.</p>
			<p>There are many options available here, but we will just be focusing on those that are required <a id="_idIndexMarker130"/>to<a id="_idIndexMarker131"/> generate a training set. Optional or unnecessary options will be skipped for brevity, but you can view the tooltip for more information in the GUI.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Faceswap comes with <a id="_idIndexMarker132"/>built-in <strong class="bold">tooltips</strong> that explain what each option does. Hover over the entries to access the corresponding ToolTip.</p>
			<h3>Data</h3>
			<p>This section is where<a id="_idIndexMarker133"/> the location of the source material that we intend to extract faces from is entered, as well as the location that we wish to extract the faces to:</p>
			<ul>
				<li><strong class="bold">Input Dir</strong>: The location of the source video file or folder of images that contain the faces you wish to extract. Clicking the buttons on the right will launch a file browser to easily navigate to the correct location. Select the left-hand icon if extracting faces from a video file or the right-hand icon if extracting faces from a folder of images.</li>
				<li><strong class="bold">Output Dir</strong>: The location that identified faces should be extracted to. This should be a new folder within the <strong class="bold">Faces</strong> folder that you set up when creating your project folder structure.</li>
			</ul>
			<div><div><img src="img/B17535_04_003.jpg" alt="Figure 4.3 – The Data section for face extraction"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – The Data section for face extraction</p>
			<p>Next, let’s look at the plugins we can use.</p>
			<h3>Plugins</h3>
			<p>The plugins section is <a id="_idIndexMarker134"/>where the neural networks that will we use to identify faces within the images are chosen, as well as those plugins that identify the key landmarks within the face and any neural network-based masks to apply to the extracted faces:</p>
			<ul>
				<li><strong class="bold">Detector</strong>: The detector identifies faces within each of the images. The most robust detector at the time of <a id="_idIndexMarker135"/>writing is <strong class="bold">S3Fd</strong> (based on the paper Single Shot Scale-Invariant Face Detector: https://arxiv.org/abs/1708.05237). It is, however, resource-intensive, requiring at least 3 GB of GPU memory to run. It also runs incredibly slowly on a CPU due to its complexity. However, if you have the resources available, this is the detector to use. Otherwise, the <strong class="bold">MTCneural network</strong> (<a href="https://arxiv.org/abs/1604.02878">https://arxiv.org/abs/1604.02878</a>) detector should be used, which will <a id="_idIndexMarker136"/>run on far fewer resources and runs a lot quicker on a CPU.</li>
				<li><strong class="bold">Aligner</strong>: Responsible for identifying key facial landmarks. These landmarks are used for aligning any faces detected so that they are consistent for feeding the model during<a id="_idIndexMarker137"/> training. <strong class="bold">FAN</strong> (<a href="https://arxiv.org/pdf/1703.07332.pdf">https://arxiv.org/pdf/1703.07332.pdf</a>) is the best aligner and, if at all possible, should be the option you select here. It is, however, slow on a CPU, in which case, the <strong class="bold">CV2-D </strong>neural <a id="_idIndexMarker138"/>network aligner is available. While this will run a lot quicker on a CPU, it is far inferior to FAN.</li>
				<li><strong class="bold">Masker</strong>: When extracting faces, it is also possible to use neural networks for masking an area of interest. Specifically, we are only interested in training our model on the face area of the extracted image. The background to the face that is included in the extracted images just adds noise to the model, which it is advantageous to exclude. Two masks are always included by default, based on the landmark data generated by the aligner. The landmark-based masks are fine for a lot of use <a id="_idIndexMarker139"/>cases, but they are limited insofar as they do not include the forehead within the masked area (they crop just above the eyebrows). The neural network-based masks attempt to address this issue by using AI to generate masks on extracted faces. Generally, the <strong class="bold">BiSeNet-FP</strong> mask<a id="_idIndexMarker140"/> works best.</li>
			</ul>
			<p>It is worth noting that masks do not need to be created at extract time. Faceswap includes a tool to add masks to training sets after extraction has been performed. This can be beneficial, as often you will not want to spend time generating neural network-based masks until you are happy that the extracted faces are correct and properly aligned. Be aware that the more neural network-based masks that are added (multiple masks can be selected), the longer the extraction process will take.</p>
			<div><div><img src="img/B17535_04_004.jpg" alt="Figure 4.4 – A side-by-side comparison of a BiSeNET-FP (neural network-based) mask (top) and a components (landmarks-based) mask (bottom). The left image is the original aligned face, the right image is the generated mask, and the center image is the aligned face with the mask applied"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – A side-by-side comparison of a BiSeNET-FP (neural network-based) mask (top) and a components (landmarks-based) mask (bottom). The left image is the original aligned face, the right image is the generated mask, and the center image is the aligned face with the mask applied</p>
			<ul>
				<li><strong class="bold">Normalization</strong>: When<a id="_idIndexMarker141"/> the aligner is looking to identify key<a id="_idIndexMarker142"/> landmarks, it can help to normalize the image being fed to the plugin. Generally, <strong class="bold">histogram normalization</strong> (<strong class="bold">Hist</strong>) or <strong class="bold">contrast limited adaptive histogram equalization</strong> (<strong class="bold">CLAHE</strong>) works<a id="_idIndexMarker143"/> best, but it will depend on the source material.</li>
				<li><strong class="bold">Re Feed</strong>/<strong class="bold">Rotate Images</strong>: To generate a training set, these options are not necessary and should be left at their default values (<strong class="bold">1</strong> and blank respectively).</li>
			</ul>
			<div><div><img src="img/B17535_04_005.jpg" alt="Figure 4.5 – The Plugins selection options for face extraction"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5 – The Plugins selection options for face extraction</p>
			<h3>Face processing</h3>
			<p>Face processing is any <a id="_idIndexMarker144"/>task that should be performed after faces have been identified within an image. The only option of concern here is <strong class="bold">Min Size</strong>. False positives can be found by the detector (items that the detector considers a face but are not actually so). This option allows you to discard faces that do not meet this minimum threshold (measured in pixels from corner to corner of the detected face). The value specified here will vary, depending on the size of the input frames and how many of the images are taken up by the faces you are interested in. Leaving it set at a low value, regardless, can help with the curation of data within the next phase.</p>
			<p>The other option within this section is the face filter. Generally, it is advised to avoid using the filter, as it can be somewhat hit and miss at correctly identifying faces and will significantly slow down the extraction process.</p>
			<div><div><img src="img/B17535_04_006.jpg" alt="Figure 4.6 – The Face Processing options for face extraction"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.6 – The Face Processing options for face extraction</p>
			<h3>Output</h3>
			<p>The final area of<a id="_idIndexMarker145"/> interest is the <strong class="bold">Output</strong> section. The only option that needs to be amended from default is the <strong class="bold">Extract Every N</strong> option. While you need lots of images to train a model, variety is more important. When using video as a source of training images, each frame is extremely similar to its immediate neighbor. To reduce the number of similar faces that will be extracted, it is possible to skip frames to parse for faces. The number that is set here will depend on the frame rate of your video, as well as the number of sources that you intend to extract faces from. Generally, between 2 to 8 frames per second of video is a good number to aim for (for example, for a 30 fps video, an <strong class="bold">Extract Every N</strong> value of 6 will extract faces from 5 frames for every 1 second of video).</p>
			<div><div><img src="img/B17535_04_007.jpg" alt="Figure 4.7 – The Output options for face extraction"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7 – The Output options for face extraction</p>
			<h3>Extract</h3>
			<p>Now that the options<a id="_idIndexMarker146"/> have all been set, the <strong class="bold">Extract</strong> button can be pressed. This will extract all of the discovered faces into your given folder and generate the corresponding alignments file. The amount of time this will take will depend on the available hardware, the length of the source material, and the plugins that have been chosen.</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor077"/>Curating training images</h2>
			<p>Once faces have been<a id="_idIndexMarker147"/> extracted, the data needs to be curated. The neural networks used to identify, extract, and align faces do a good job, but they are not perfect. Along with the correctly detected and aligned faces, it is most likely that a not insignificant amount of unusable data will also have been collected. This may include faces other than the target, false positives (parts of the image that the neural network considers a face but are not actually so), and misaligned faces (faces that have been correctly identified but the aligner has failed to align them correctly).</p>
			<div><div><img src="img/B17535_04_008.jpg" alt="Figure 4.8 – A concentrated example of misaligned faces"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.8 – A concentrated example of misaligned faces</p>
			<p>Examining a folder that contains a significant number of unusable faces, it may, at first, appear to be a mammoth task to clean up and curate the training images. Fortunately, neural networks can again be leveraged to make this job a lot easier.</p>
			<h3>Sorting the faces</h3>
			<p>The faces will have <a id="_idIndexMarker148"/>been extracted in frame order – that is, they will exist within the output folder in the order that they were discovered within the source video or images. Faceswap includes a number of sorting mechanisms to arrange these faces in an order to enable easier pruning, which can be accessed by selecting the <strong class="bold">Tools</strong> tab, followed by the <strong class="bold">Sort</strong> sub-tab:</p>
			<div><div><img src="img/B17535_04_009.jpg" alt="Figure 4.9 – The Sort tool within the Tools section of Faceswap"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.9 – The Sort tool within the Tools section of Faceswap</p>
			<p>The most powerful sorting method, by some distance, is to sort by <strong class="bold">face</strong>. This uses the neural network <strong class="bold">VGG Face 2</strong> developed by researchers at the Visual Geometry Group at the University of Oxford (<a href="https://arxiv.org/abs/1710.08092">https://arxiv.org/abs/1710.08092</a>). Faceswap utilizes this network to cluster similar faces together, making the data far easier to parse.</p>
			<p>Within the <strong class="bold">Sort</strong> section of Faceswap, the following options should be selected:</p>
			<ul>
				<li><strong class="bold">Input</strong>: The folder that contains the extracted faces that are to be curated</li>
				<li><strong class="bold">Sort By</strong>: Select <strong class="bold">Face</strong></li>
				<li><strong class="bold">Final Process</strong>: Rename (the faces will be sorted in place, with the filenames renamed)</li>
			</ul>
			<p>All other options can be left at their default values, as shown in the following screenshot:</p>
			<div><div><img src="img/B17535_04_010.jpg" alt="Figure 4.10 – The selected options within Faceswap’s Sort tool"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.10 – The selected options within Faceswap’s Sort tool</p>
			<p>Press the <strong class="bold">Sort</strong> button <a id="_idIndexMarker149"/>to launch the sorting process. Depending on your setup and the number of faces to be sorted, this may take some time.</p>
			<h3>Removing faces</h3>
			<p>Once the sorting <a id="_idIndexMarker150"/>process has completed, the OS’s standard file manager can be used to scroll through the folder and remove any incorrect faces. The sorting process will have grouped all of the similar faces together, making this a far simpler task, as all of the misidentified faces can be bulk selected and deleted. Just ensure that the sort order of the folder is by <strong class="bold">filename</strong>.</p>
			<p>It is also a good idea, at this stage, to remove any faces that are not of suitable quality for training. This includes those images that are of the target individual but are sub-standard in some way – for instance, if the face is not aligned correctly within the frame, the face is significantly obstructed, or the quality of the image is too low. Generally, training images should be of high quality. Not all low-quality/blurry images need to be removed from the training set, as the neural network will also need to know how to recreate these lower-quality/resolution images; however, they should form a minority of the training set.</p>
			<p>Now that the folder just contains faces that are suitable for training, it is a best practice to clean the alignments file. This is the process of removing the faces that you have just deleted from that file. This enables us to go back to the video source and alignments file and re-extract the faces, while avoiding the requirement to sort the data again. This action has a second<a id="_idIndexMarker151"/> advantage, insofar as it will also rename all the faces their original filenames.</p>
			<p>Once again, Faceswap has a tool to help with this – specifically, navigate to the <strong class="bold">Tools</strong> tab and then the <strong class="bold">Alignments</strong> sub-tab:</p>
			<div><div><img src="img/B17535_04_011.jpg" alt="Figure 4.11 – The location of the Alignments tool within Faceswap"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.11 – The location of the Alignments tool within Faceswap</p>
			<p>The <strong class="bold">Alignments</strong> tool allows multiple actions to be performed on the alignments file. The job that is of interest is <strong class="bold">Remove-Faces</strong>, which examines a folder of faces and removes the faces that you have deleted from the alignments file. The following options should be selected:</p>
			<ul>
				<li><strong class="bold">Job</strong>: Select <strong class="bold">Remove-Faces</strong></li>
				<li><code>.fsa</code> alignments file</li>
				<li><strong class="bold">Faces Folder</strong>: Browse to the location that contains your curated face set and select the folder</li>
			</ul>
			<p>All other options<a id="_idIndexMarker152"/> can be left at their default values:</p>
			<div><div><img src="img/B17535_04_012.jpg" alt="Figure 4.12 – The selected options within Faceswap’s Alignments tool"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.12 – The selected options within Faceswap’s Alignments tool</p>
			<p>Press the <strong class="bold">Alignments</strong> button. A backup of the alignments file will be taken, then the process will remove all the faces that do not appear in the faces folder from the file. Finally, the<a id="_idIndexMarker153"/> faces will be renamed with their originally extracted names.</p>
			<h3>Collating training images</h3>
			<p>Now that the<a id="_idIndexMarker154"/> training sources have been curated, the images can be collated into the final training sets.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">If the required mask was not generated during extraction, it can also be added to the faces now, using the <strong class="bold">Mask</strong> tool (the <strong class="bold">Tools</strong> tab | the <strong class="bold">Mask</strong> sub-tab). This should be done prior to collating the final training sets.</p>
			<p>This is as simple as taking all of the contents of each source’s extracted faces and placing them all into the same folder (one folder for the <strong class="bold">A</strong> side, and one folder for the <strong class="bold">B</strong> side). All the information required by Faceswap to train on these images is stored within the EXIF header of the PNG images.</p>
			<p>Some final curating may be required to bring the number of training images down to a manageable size, but anywhere in the region of 500 to 50,000 images per side of the model is reasonable.</p>
			<p>Now that the data has been collected and curated, it is time to train the model.</p>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor078"/>Training a model</h1>
			<p>This part of the process<a id="_idIndexMarker155"/> requires the least amount of manual intervention but will take the longest in terms of compute time. Depending on the model chosen and the hardware in use, this can take anywhere from 12 hours to several weeks to complete.</p>
			<p>It is advised to use a relatively lightweight model when creating a deepfake for the first time. Creating swaps is fairly nuanced, and understanding what works and what doesn’t comes with experience. Whilst Faceswap offers several models, starting with the <strong class="bold">Original</strong> or <strong class="bold">Lightweight</strong> model will allow you to gauge the performance of the swap relatively quickly, while not necessarily giving you the best possible final result.</p>
			<p>Faceswap comes with numerous configurable settings for models and training. These are available within the <strong class="bold">Settings</strong> menu of the application. To cover all of these settings is well outside of <a id="_idIndexMarker156"/>the scope of this walk-through, so default settings will be used unless otherwise stated.</p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor079"/>Setting up</h2>
			<p>Navigate to<a id="_idIndexMarker157"/> the <strong class="bold">Train</strong> tab of the Faceswap application:</p>
			<div><div><img src="img/B17535_04_013.jpg" alt="Figure 4.13 – The training options section of Faceswap"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.13 – The training options section of Faceswap</p>
			<h3>Faces</h3>
			<p>This section is where <a id="_idIndexMarker158"/>we tell the process where our training images are stored. If you followed the extraction and curation steps, then these faces will exist within your project folder, with a single folder for each side:</p>
			<ul>
				<li><strong class="bold">Input A</strong>: The path to the folder containing the extracted faces for the <strong class="bold">A</strong> side of the model (that is, the original face that is to be removed from the final video).</li>
				<li><strong class="bold">Input B</strong>: The path to the folder containing the extracted faces for the <strong class="bold">B</strong> side of the model (that is, the face that you wish to transpose to the final video).</li>
			</ul>
			<div><div><img src="img/B17535_04_014.jpg" alt="Figure 4.14 – The Faces options for training a model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.14 – The Faces options for training a model</p>
			<h3>Model</h3>
			<p>The model section is <a id="_idIndexMarker159"/>where Faceswap is instructed on which model to use, where that model should be stored, and any model-specific actions to perform at runtime:</p>
			<ul>
				<li><strong class="bold">Model Dir</strong>: The folder that the model should be stored in, or if you are resuming a pre-existing model, then the folder that contains the model to be resumed.</li>
			</ul>
			<p>If you are starting a new model, then this location should not pre-exist on the hard drive. When the model is created, the folder specified here will be created and populated with the model and associated files required by Faceswap to track training.</p>
			<p>If you are resuming a previously created model, then this should point to the folder that was created when initially setting up the model (the folder created by Faceswap containing the associated model files).</p>
			<ul>
				<li><strong class="bold">Trainer</strong>: Faceswap has multiple models available (named <strong class="bold">Trainer</strong> for legacy reasons). These models are more or less configurable within the <strong class="bold">Settings</strong> menu, depending on the model chosen. As discussed before, if you are just starting out, then it is advisable to use the <strong class="bold">Original</strong> or <strong class="bold">Lightweight</strong> model.</li>
			</ul>
			<p>If you are starting a new model, then the model selected here will be the model used for all future training sessions of it. It is not possible to change a model type once it has been created.</p>
			<p>The other options within this section can be ignored for your first model, although they may become more relevant as you gain experience using the software. As with all the options, the tooltips will tell you what these additional options do.</p>
			<div><div><img src="img/B17535_04_015.jpg" alt="Figure 4.15 – The Model options for training in Faceswap"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.15 – The Model options for training in Faceswap</p>
			<h3>Training</h3>
			<p>These options relate<a id="_idIndexMarker160"/> to how the model should be trained:</p>
			<ul>
				<li><strong class="bold">Batch Size</strong>: The number of faces to be fed through the model at once. Generally, a higher batch size will lead to a higher training speed; however, higher batch sizes will mean the model generalizes more. Increasing batch size is only sensible up to a limit. Anything beyond 128 and the model will start to struggle to obtain useful information for each batch.</li>
			</ul>
			<p>Batch size is also VRAM-limited. For more complicated models, you will have little choice but to use a smaller batch size, and obviously, the less VRAM available on the GPU, the more limited you are.</p>
			<ul>
				<li><strong class="bold">Iterations</strong>: This can be left at default unless it is desired that the model should stop after a certain number of iterations. Knowing when to stop a model comes from experience and is dictated by the quality of output, so it will never be after a “set number of iterations.”</li>
				<li><strong class="bold">Distributed</strong>: This option is for multi-GPU users only. It allows for multiple video cards to be used, speeding up training by splitting batches over multiple devices.</li>
			</ul>
			<p class="callout-heading">Tip</p>
			<p class="callout">It can be beneficial to start training a model at a higher batch size to get the speed benefits, and then reduce it later in training to get the benefits of drilling down for details.</p>
			<div><div><img src="img/B17535_04_016.jpg" alt="Figure 4.16 – The Training options within Faceswap"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.16 – The Training options within Faceswap</p>
			<h3>Augmentation</h3>
			<p>The <strong class="bold">Augmentation</strong> section<a id="_idIndexMarker161"/> allows you to enable or disable certain image augmentations (the way a neural network artificially increases the number of training images). When starting a new model, these should all be disabled (all augmentations are active). Later in the training session (when faces are becoming identifiable and more detailed), it can be desirable to turn some of these augmentations off:</p>
			<ul>
				<li><strong class="bold">Warp to Landmarks</strong>: This is just an alternative warping technique. There is no conclusive evidence that enabling or disabling this option makes any real difference, so it is recommended to leave it disabled at all times.</li>
				<li><strong class="bold">No Flip</strong>: Faces are vaguely symmetrical. The neural network leverages this knowledge by flipping around 50% of the images horizontally. For nearly all use cases, this is fine, and all images can be flipped at all times. However, in cases when there are distinct details on one side of a face (for example, a beauty mark), then this option should be enabled when you see that faces start to take shape within the training preview window.</li>
				<li><strong class="bold">No Augment Color</strong>: The color augmentation helps the neural network to match color and lighting between the A and B sides by artificially coloring and changing other visual attributes of the images it sees. Generally, this is always desirable and should be left on, but for some use cases, it can be desirable to disable this augmentation.</li>
				<li><strong class="bold">No Warp</strong>: This is possibly the most important option within this section. Warping is incredibly important to how a neural network learns. It is absolutely imperative that all models commence with warping enabled (failure to do so will invariably lead to a sub-standard model or, worse, model collapse). However, later in training, particularly when attempting to drill down into finer details, this warp actually becomes detrimental to the model’s training, and so this option to disable the warp should be selected.</li>
			</ul>
			<div><div><img src="img/B17535_04_017.jpg" alt="Figure 4.17 – The Augmentation section to train a model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.17 – The Augmentation section to train a model</p>
			<ul>
				<li>A rule of thumb is that if faces are recognizable and they do not appear to be getting any<a id="_idIndexMarker162"/> sharper over a significant period of time, then it is probably time to disable warping. Seeing clearly defined teeth and eye glare is a good indicator. It is important to note that it is near impossible to disable warping too late, but it is very definitely possible to disable warping too early, so err on the side of caution.</li>
			</ul>
			<p class="callout-heading">Author’s note</p>
			<p class="callout">Color augmentation and warping images are both invaluable ways to get more mileage from your data. Like the other augmentations in this section, they change your image slightly, as a way to effectively get new images that the AI model hasn’t seen before.</p>
			<p class="callout">Color augmentation works by slightly altering the colors of the image. This gives the model new colors to work with. This also helps the model with new lighting situations that might be absent from the data.</p>
			<p class="callout">Warping works by slightly modifying the shape of the face in the image. This helps if certain expressions are less common in your data. It also helps ensure that the decoder builds the face from memory and not just from a copy of the original image.</p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor080"/>Launching and monitoring training</h2>
			<p>Once the model configuration has been entered and all the settings have been adjusted to their appropriate value, the <strong class="bold">Train</strong> button can be pressed to launch the training session. Training can and will take a long time to complete, and there is no mathematical or numerical measure to know when a model has finished training. Knowing when a model is unlikely to improve anymore mostly comes with experience; however, there are some indicators in place that can help us to determine whether it is time to stop training.</p>
			<h3>Previews</h3>
			<p>Probably the most <a id="_idIndexMarker163"/>important measure of the model’s progress is the preview images themselves. At each saved iteration, a series of preview images is generated to enable visualization of how the model is progressing.</p>
			<div><div><img src="img/Image98480.jpg" alt="Figure 4.18 – Faceswap’s GUI with the Preview window on the right"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.18 – Faceswap’s GUI with the Preview window on the right</p>
			<p>This preview consists of 28 faces randomly picked from the training set (14 for each side). For each training image, the original is shown, followed by an image showing the AI’s attempt to recreate the original face. The final image shows the AI’s attempt to swap the original face with the identity from the other side.</p>
			<p>When training commences, these images may look like a solid color or a blurry image that is vaguely face-shaped; however, it will fairly quickly resolve into an identifiable face and slowly improve over time. It is worth noting that model improvement is not linear. While the faces will improve fairly quickly at first, this improvement will slow down until no visible difference will be seen from iteration to iteration. However, over a period of time, the model will improve. When comparing previews, it is not uncommon to compare the<a id="_idIndexMarker164"/> improvements of images that have been taken 10,000 to 100,000 iterations apart.</p>
			<h3>Loss</h3>
			<p>Another measure available <a id="_idIndexMarker165"/>to us is the loss for each iteration. Every time a batch is processed through the model, the neural network scores itself for how well it thinks it has recreated the images. While the loss value can swing wildly from batch to batch, over time the average value will drop. It is important to note that the actual value of the loss is not important. In fact, the value itself is effectively meaningless. The only issue to concern ourselves with is whether the value is dropping. This is for a couple of reasons; firstly, different loss functions will result in different numbers being generated, which are not comparable with each other. Secondly, the loss values given do not actually represent a score for anything that is useful for us to measure. The loss is generated by how well the neural network thinks it is recreating the <strong class="bold">A</strong> and <strong class="bold">B</strong> faces. It does this by looking at the original face and its recreation, and scoring itself based on the quality of the recreation. However, this score is not useful to us. What we would like to see is a score based on how well the neural network is taking a face and swapping it with another face. As there are no real-world examples of people who have had their faces swapped, this is an impossible measure to achieve, so we make do with using the loss values for face reconstruction rather than for face swapping.</p>
			<h3>Graphs</h3>
			<p>While loss on its own <a id="_idIndexMarker166"/><a id="_idIndexMarker167"/>and at any given time is not a useful measure, its trend over time is. Ultimately, if loss is decreasing, the model is learning. The further a model is trained, the harder it is to ascertain whether the loss is actually still decreasing over time. Faceswap collects logging data for each batch passed through the model in the form of <strong class="bold">TensorFlow event logs</strong>. These logs are stored in the same folder as the model and can be used to visualize data in Faceswap’s GUI, or analyzed using <strong class="bold">TensorBoard</strong> (TensorFlow’s visualization toolkit).</p>
			<p>To analyze the learning progress of an existing model, navigate to the <code>state.json</code> file, located within the model folder. Once the data is parsed, session summary statistics will be displayed for each training session carried out:</p>
			<div><div><img src="img/B17535_04_019.jpg" alt="Figure 4.19 – Statistics for a series of Faceswap training sessions"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.19 – Statistics for a series of Faceswap training sessions</p>
			<p>Clicking the green <strong class="bold">Graph</strong> icon next to any of the session rows will bring up the training graph for that session, displaying the loss for each iteration during it:</p>
			<div><div><img src="img/B17535_04_020.jpg" alt="Figure 4.20 – A training graph showing loss over time for a Faceswap training session"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.20 – A training graph showing loss over time for a Faceswap training session</p>
			<p>For long sessions, it can be <a id="_idIndexMarker168"/>hard to ascertain whether the loss is still falling, due to the sheer quantity and range of data. Fortunately, it is possible to zoom into a selected range of the graph to get a better idea, by selecting the <strong class="bold">Zoom to Rectangle</strong> button toward the bottom right of the screen and selecting the area of interest. In this example, we shall zoom in on the last 100,000 iterations trained and make sure that we are viewing the rolling average of the loss values. As we can see, the loss is still clearly improving.</p>
			<div><div><img src="img/B17535_04_021.jpg" alt="Figure 4.21 – A zoomed-in view of the last 100,000 iterations of a training session"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.21 – A zoomed-in view of the last 100,000 iterations of a training session</p>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor081"/>Manual intervention</h2>
			<p>While training a model<a id="_idIndexMarker169"/> is mostly a “fire and forget” task, toward the end of training, it can help to take some steps to improve the final output. These steps are by no means necessary, but they can help with the final product. For any of these actions to take effect, the model will need to be stopped, the relevant settings adjusted, and then the model can be resumed:</p>
			<ul>
				<li><strong class="bold">Disabling Warp</strong>: As previously mentioned, warping is imperative for a model to learn; however, as the model enters the middle to later stages of training, the warping augmentation can actually hurt the model in terms of image fidelity and clarity. Enabling the <strong class="bold">No Warp</strong> option is good practice for a high-quality final swap. As a general rule of thumb, this option should not be selected until you are at least 50% through the total train or you can see clearly defined features, such as individual teeth and eye glare. It is very hard to disable warping too late, but it is very easy to disable it too early. If the previews still look like they appear to be improving, then it is probably too early to disable warping.</li>
				<li><strong class="bold">Lowering Learning Rate</strong>: As the model enters the late stage of training, it can help to lower the learning rate. This can help the model to drill down into the finer details. It is common to lower the learning rate a little, resume training, lower it some more, resume training, and repeat this cycle until you are happy with the results.</li>
				<li><strong class="bold">Fit Training</strong>: A technique that can help is fitting data to the actual scene that is to be swapped. While it is not recommended to fully train a model only using data that will appear within the final swap, using this data can be useful to fine-tune an otherwise fully trained model.</li>
			</ul>
			<p>When you are <a id="_idIndexMarker170"/>happy that you have trained the model as far as you can, it is time to take the trained model and apply it to a source video file to swap the faces.</p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor082"/>Applying a trained model to perform a swap</h1>
			<p>Once the model<a id="_idIndexMarker171"/> has completed training, it can <a id="_idIndexMarker172"/>be used to swap the faces on any video to that contains the individual that is to be swapped out. Three items are required to successfully perform a swap – a video/series of images, a trained model, and an alignments file for the media that is to be converted. The first two items are self-explanatory; the alignments file is the one item we need to create.</p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor083"/>The alignments file</h2>
			<p>The alignments file is a<a id="_idIndexMarker173"/> file bespoke to<a id="_idIndexMarker174"/> Faceswap, with a <code>.fsa</code> extension. This file should exist for every media source that is to be converted. It contains information about the location of faces within a video file, the alignment information (how the faces are orientated within each frame), as well as any associated masks for each frame.</p>
			<p>Generating an alignments file is fairly trivial. In fact, at least one has been generated already when we built a training set. The process for generating training data and generating an alignments file is the same, bar a few changes. For this reason, a lot of the steps will be familiar to you, as they are the same ones we performed within the <em class="italic">Extracting faces from your source data</em> section. The most notable difference in this section is that alignment information needs to be generated for every single frame within the media source, while for generating a training set, it is more common to only extract faces for a subset of frames within the source material.</p>
			<p>Please refer back to <em class="italic">Extracting faces from your source data</em> for a more detailed explanation of the common options between running an extract to generate training data and running an <a id="_idIndexMarker175"/>extract to perform a <a id="_idIndexMarker176"/>swap.</p>
			<p>Within the Faceswap application, select the <strong class="bold">Extract</strong> tab, and then follow the following sections.</p>
			<h3>Data</h3>
			<p>This section is <a id="_idIndexMarker177"/>the location of the source material that we need to generate alignments for, as well as an output folder that faces will be extracted to. The faces generated here are not used by the Faceswap process at all, but they are very useful for cleaning our alignments file:</p>
			<ul>
				<li><strong class="bold">Input Dir</strong>: The location of the media that we intend to swap the faces within</li>
				<li><strong class="bold">Output Dir</strong>: The location that identified faces will be extracted to</li>
			</ul>
			<div><div><img src="img/B17535_04_022.jpg" alt="Figure 4.22 – The Data options within Faceswap’s extraction settings"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.22 – The Data options within Faceswap’s extraction settings</p>
			<h3>Plugins</h3>
			<p>The plugins that<a id="_idIndexMarker178"/> are selected are likely to be the same as those selected when generating a training set, so refer to the previous section for more information on the options. Only one option within this section will likely change when extracting an alignments file:</p>
			<ul>
				<li><strong class="bold">Re Feed</strong>: As the extraction process has no understanding of temporal coherence (that is, how each frame relates to the previous and subsequent frame), it can lead to “jittery” alignments within the final swap. This means that the face in the final swap moves a small amount from frame to frame. While this is not important for training sets, it is important when generating the file for the final swap.</li>
			</ul>
			<p>Re Feed is a mechanism to help prevent this jitter by feeding a detected face into the aligner a set number of times and taking the average of the results. It is worth noting that each increase in the re-feed amount will slow extraction down, as the data needs to be passed through the process multiple times. The higher this number is set, the smoother the final output should be, but it is diminishing returns. Setting the value too high is unlikely to net any visible benefit but<a id="_idIndexMarker179"/> will take significantly longer to run an extract.</p>
			<p>Set this to a value that brings you satisfactory results, while also running at a speed you can live with. Any value above 1 should give improved results over extracting without re-feed. A re-feed value of 8–10 will likely get the output close to as good as it can be.</p>
			<div><div><img src="img/B17535_04_023.jpg" alt="Figure 4.23 – The Plugins options within Faceswap’s extraction settings"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.23 – The Plugins options within Faceswap’s extraction settings</p>
			<h3>Face Processing</h3>
			<p>As with extracting<a id="_idIndexMarker180"/> training faces, <strong class="bold">Min Size</strong> is the only option within this section that may need to be adjusted. The value specified here will generally correspond to the size of the faces within the source material, but leaving it set at a low value, regardless, can help with removing some false positives that are clearly not valid faces.</p>
			<div><div><img src="img/B17535_04_024..jpg" alt="Figure 4.24 – The Face Processing options within Faceswap’s extraction settings"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.24 – The Face Processing options within Faceswap’s extraction settings</p>
			<h3>Output</h3>
			<p>Finally, the <strong class="bold">Output</strong> section<a id="_idIndexMarker181"/> should be reviewed and updated. The only option that needs to be amended from the settings used for extracting a training set is <strong class="bold">Extract Every N</strong>. It is imperative that every frame has a corresponding entry in the generated alignments file, so this should be set to <strong class="bold">1</strong>.</p>
			<div><div><img src="img/B17535_04_025..jpg" alt="Figure 4.25 – The Output options within Faceswap’s extraction settings"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.25 – The Output options within Faceswap’s extraction settings</p>
			<h3>Extract</h3>
			<p>Once the appropriate<a id="_idIndexMarker182"/> settings have been locked in, press the <strong class="bold">Extract</strong> button, to generate the alignments file and extract the found faces into the given folder. The amount of time this will take will depend on the available hardware, the length of the source material, and the re-feed value that has been set.</p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor084"/>Cleaning the alignments file</h2>
			<p>Similarly to<a id="_idIndexMarker183"/> when we collected faces to build a training set, the extraction process will have done a decent job of identifying faces, but it will not have done a perfect job, so some manual processing is now required to clean up the alignments file. The process is the same as that in the <em class="italic">Curating training images</em> section with an additional step, so follow the steps within that section to perform the initial cleansing of the alignments file. Once unwanted faces have been removed from the alignments file, the folder of extracted faces can be deleted. The faces are not actually used by the conversion process; they are just used as a mechanism to clean the alignments file.</p>
			<p>At this point, we should have a source video that is the target for swapping faces and a corresponding alignments file that holds information about the location of faces within that file. It is entirely feasible to run a conversion at this stage, but another step is required for the best results – fixing the alignments file.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor085"/>Fixing the alignments file</h2>
			<p>Whilst we have <a id="_idIndexMarker184"/>removed unwanted faces, false<a id="_idIndexMarker185"/> positives, and any clearly misaligned faces from our alignments file, some further work is required to clean up the file for the final conversion. The main reasons for this are to fix frames where the following scenarios occur:</p>
			<ul>
				<li>Multiple faces have been identified. Sometimes, the detector will find two faces in a frame, but the aligner performs alignment on the same face twice. This often happens when two faces appear close to each other within a frame.</li>
				<li>The face is not aligned correctly. Sometimes, the face may appear aligned correctly when scanning through the folder of images, but examining the landmarks will demonstrate that this is not the case. These faces will sometimes convert correctly, but often this misalignment will lead to a messy swap for those frames (the swap will not look quite correct, it may look blurry, or may flicker between frames).</li>
				<li>A face hasn’t been identified. Ensuring that all faces being swapped have been identified is necessary; otherwise, the original face, rather than the swapped face, will appear in those frames.</li>
				<li>The mask has not been detected correctly. Depending on the conditions of the source frame, some neural network-based masks may not have been detected correctly, so these need to be fixed up. Depending on how the mask has been rendered, an incorrect mask may mean that parts of the original face show through, or parts of the background frame do not render correctly.</li>
			</ul>
			<p>Again, Faceswap provides tools to make this process easier – specifically, the <strong class="bold">Manual tool</strong>, which <a id="_idIndexMarker186"/>enables the visualization and editing of alignments/masks within the context of the original frame.</p>
			<p>To launch the <a id="_idIndexMarker187"/>Manual tool, navigate <a id="_idIndexMarker188"/>to the <strong class="bold">Tools</strong> tab and then the <strong class="bold">Manual</strong> sub-tab:</p>
			<div><div><img src="img/B17535_04_026.jpg" alt="Figure 4.26 – The location of the Manual tool within Faceswap’s GUI"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.26 – The location of the Manual tool within Faceswap’s GUI</p>
			<p>Assuming that the<a id="_idIndexMarker189"/> alignments file for the video to be converted is in the default location, then only one argument needs to be provided to launch the Manual tool the location of the source video/folder of images that is to be converted. Specify this location within the <strong class="bold">Frames</strong> box and hit the <strong class="bold">Manual</strong> button to launch the tool.</p>
			<div><div><img src="img/B17535_04_027.jpg" alt="Figure 4.27 – The Data options for launching Faceswap’s Manual Tool"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.27 – The Data options for launching Faceswap’s Manual Tool</p>
			<p>Once the tool loads, you will be greeted with a main “video” window that shows the source that is being worked on and a secondary window that displays all of the faces that exist within the alignments file.</p>
			<div><div><img src="img/B17535_04_028.jpg" alt="Figure 4.28 – The Faceswap manual tool showing the video window at the top and the faces viewer at the bottom"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.28 – The Faceswap manual tool showing the video window at the top and the faces viewer at the bottom</p>
			<p>The top left <a id="_idIndexMarker190"/>buttons allow you to perform <a id="_idIndexMarker191"/>different actions on the alignments, so hover over the tooltips to see what is available:</p>
			<div><div><img src="img/B17535_04_029.jpg" alt="Figure 4.29 – The Editor selection buttons within Faceswap’s Manual tool"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.29 – The Editor selection buttons within Faceswap’s Manual tool</p>
			<p>Another area of interest is<a id="_idIndexMarker192"/> the <strong class="bold">Filter</strong>. The Filter is a pull-down list that is located between the frames and faces windows and enables you to filter the frames and faces shown in the tool by certain criteria:</p>
			<div><div><img src="img/B17535_04_030.jpg" alt="Figure 4.30 – The face Filter options within Faceswap’s Manual tool"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.30 – The face Filter options within Faceswap’s Manual tool</p>
			<p>Finally, the faces window also has some buttons on the left-hand side, which enable you to toggle the <a id="_idIndexMarker193"/>display of the face landmark mesh <a id="_idIndexMarker194"/>and the selected mask for faces displayed in the faces area:</p>
			<div><div><img src="img/B17535_04_031.jpg" alt="Figure 4.31 – The face viewer buttons within Faceswap’s Manual tool"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.31 – The face viewer buttons within Faceswap’s Manual tool</p>
			<h3>Removing multiple faces</h3>
			<p>To remove any <a id="_idIndexMarker195"/>extra faces from frames that contain multiple faces, select the <strong class="bold">Multiple Faces</strong> filter. The main video window will now only show any frames that contain multiple faces. If no frames show in the top window and no faces show in the bottom window, then there are no frames with multiple faces, and you can move on to the next action.</p>
			<p>Knowing which face to remove is sometimes not obvious, so press the <strong class="bold">Display landmarks mesh</strong> button to the left of the faces window to bring up the landmarks overlay. If none of the faces are obviously misaligned (when the displayed landmarks mesh does not correspond to the underlying facial features), then any of the multiple faces can be deleted; otherwise, aim to delete the face with the landmarks that correspond least with the underlying face.</p>
			<p>There are several ways to delete faces from the frame. They can be deleted from the main video window by hovering over the unwanted face and pressing the <strong class="bold">Del</strong> key, or by right-clicking on the unwanted face in either the video or faces window and selecting <strong class="bold">Delete Face</strong>. When a frame no longer contains multiple faces (just one face remains in the frame), the faces for that frame will be removed from the faces window. Using this mechanism, it is usually quickest to right-click and select <strong class="bold">Delete Face</strong> from the faces window for all those faces that are unwanted until no faces remain.</p>
			<p>Once all frames<a id="_idIndexMarker196"/> with multiple faces have been cleaned, hit the <strong class="bold">Save</strong> icon to save the changes to the alignments file.</p>
			<h3>Fixing misaligned faces</h3>
			<p>The Manual tool <a id="_idIndexMarker197"/>has misalignment detection built in. It is not perfect, but it does help in identifying and fixing the most obviously misaligned faces. While the detection can find faces that are obviously misaligned, it will not find faces where the face landmarks are in the correct location in relation to each other but do not correspond with the underlying face.</p>
			<p>Select the <strong class="bold">Misaligned Faces</strong> filter to only display frames and faces where misaligned faces have been detected. A slider will appear next to the filter list box to control the distance:</p>
			<div><div><img src="img/B17535_04_032.jpg" alt="Figure 4.32 – The Distance slider for selecting misaligned faces"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.32 – The Distance slider for selecting misaligned faces</p>
			<p>This is how far the landmark points within each face have to be from an “average face” to be considered misaligned. Low values will be less restrictive, so are likely to contain faces that are properly aligned but are at more extreme angles/poses. Generally, distances between 6 and <strong class="bold">10</strong> work fairly well. A distance of <strong class="bold">10</strong> should only show misaligned faces. A distance of 6 is likely to show a mixture of misaligned faces and more extreme poses, but it will catch misaligned faces that higher values will miss. It can be easier to set a higher distance (<strong class="bold">10</strong>, for example) and fix the misaligned faces that appear. Then, set the distance to 8 and repeat the process, continuing to step down until the filter is not catching enough misaligned faces in relation to more extremely posed faces.</p>
			<p>Regardless of the distance that has been set, to fix misaligned faces, the following actions should be taken:</p>
			<ol>
				<li>Enable the landmark mesh for the faces viewer by toggling the landmarks button to the left of the faces viewer.</li>
				<li>Click on a face that has misaligned landmarks.</li>
				<li>Within the main frame editor, make sure that the Bounding Box Editor is selected. This<a id="_idIndexMarker198"/> editor allows for control of the blue box around the face. This is the “detected face” box that was picked up by the face detector during the extraction phase. Adjusting this box will update the face that is fed to the aligner, with new landmarks being calculated. Continue to adjust the box until the landmarks align correctly.</li>
				<li>If the landmarks are not aligning, it can help to switch between the different <strong class="bold">Normalization Methods</strong> options in the right-hand settings box. Different methods work better or worse in different situations, but <strong class="bold">Hist</strong> or <strong class="bold">Clahe</strong> tend to return the best results.</li>
				<li>Some faces can be stubborn (difficult angles, obstructions, or bad lighting). In these cases, it can be next to impossible for the aligner to detect the landmarks. A couple of other editors can be used in these situations:<ul><li><strong class="bold">Extract Box Editor</strong>: This editor shows a green box that corresponds to the area of the frame that will be extracted if face extraction is run. It is possible to move, resize, and rotate this extract box, which will impact the location of the landmarks within the extract box. This can be leveraged to quickly align a face by copying the landmarks from the previous or next frame (assuming that the landmarks have not changed too much between frames – for example, a scene change) and quickly adjusting the extract box to fit the current frame.</li><li><strong class="bold">Landmark Point Editor</strong>: This editor allows for the location of each individual point within the 68 landmarks to be manipulated. This level of granular control is rarely necessary, but it exists if it is needed.</li></ul></li>
			</ol>
			<p>Once the obviously<a id="_idIndexMarker199"/> misaligned landmarks have been fixed, hit the <strong class="bold">Save</strong> button to update the changes to the alignments file.</p>
			<h3>Adding missing faces</h3>
			<p>Some frames may<a id="_idIndexMarker200"/> not have had faces identified where they should have been. The most common reason for this is that the detector did find a face, but the aligner failed to align it correctly, and then the face was deleted during the sorting process. Again, the Manual tool has a filter to help with this.</p>
			<p>Select the <strong class="bold">No Faces</strong> filter to filter the top window to only those frames where no faces appear. The bottom window will remain empty for this particular filter. Navigate through the video until a frame that contains a face that has not been detected is reached, and make sure that the Bounding Box Editor is selected.</p>
			<p>Landmarks can be created by clicking over a face within a frame, or copied from the previous or next frame and then amended. The bounding box can then be edited in the same way as in the previous step, with the same caveat about difficult faces.</p>
			<p>When all frames that were missing faces have been fixed, hit the <strong class="bold">Save</strong> button to save the changes to the alignments file.</p>
			<h3>Final alignments fixups</h3>
			<p>Once the obvious<a id="_idIndexMarker201"/> missing and misaligned faces have been fixed, it’s time to perform any final fixes to the alignments file. This is as simple as scrolling through all of the faces in the faces viewer and fixing any faces that remain misaligned. The faces viewer window can be expanded to show more faces within a single screen, and then the page-up/page-down buttons can be used to scroll through the faces a page at a time. When a misaligned face is discovered, it can be clicked on, and then the Bounding Box Editor can be used to re-align the face correctly.</p>
			<p>Finally, once all faces have been reviewed and fixed, press the <strong class="bold">Save</strong> button to save the final changes <a id="_idIndexMarker202"/>to the alignments file.</p>
			<p>Now that the alignments file has been fixed, you can close the Manual tool.</p>
			<h3>Regenerating masks</h3>
			<p>If an neural network-based<a id="_idIndexMarker203"/> mask is to be used for the swap, then these masks will need to be re-generated for any faces where the alignment data has been edited. The reason for this is that the aligned face, generated from the landmarks, is used to generate the face that is fed into the masking model. Once these landmarks have been edited, the mask is invalidated, so the process automatically deletes these invalid masks when the alignments are changed.</p>
			<p>Again, Faceswap provides a tool to add masks to existing alignments files – the appropriately <a id="_idIndexMarker204"/>named <strong class="bold">Mask tool</strong>. This tool can be used to generate masks that did not previously exist in the alignments file, regenerate all masks, or just populate masks for those faces that are missing the specified mask:</p>
			<ol>
				<li>Navigate to the <strong class="bold">Tools</strong> tab and then the <strong class="bold">Mask</strong> sub-tab:</li>
			</ol>
			<div><div><img src="img/B17535_04_033.jpg" alt="Figure 4.33 – The location of the Mask tool within Faceswap’s GUI"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.33 – The location of the Mask tool within Faceswap’s GUI</p>
			<ol>
				<li value="2">Within the <strong class="bold">Data</strong> section, add the path to the video file to regenerate masks for the <strong class="bold">Input</strong> field, as well as the corresponding alignments file for the <strong class="bold">Alignments</strong> field. As the source to be worked on are the final frames to swap onto, make sure that <strong class="bold">Frames</strong> is selected under <strong class="bold">Input Type</strong>:</li>
			</ol>
			<div><div><img src="img/B17535_04_034.jpg" alt="Figure 4.34 – The Data settings of Faceswap’s Mask tool"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.34 – The Data settings of Faceswap’s Mask tool</p>
			<ol>
				<li value="3">In the <strong class="bold">Process</strong> section, select the mask that is to be populated into the alignments file for <strong class="bold">Masker</strong>. Under <strong class="bold">Processing</strong>, select <strong class="bold">Missing</strong> if masks have already been<a id="_idIndexMarker205"/> generated and the goal is to repopulate those masks that are associated with faces that have had their alignments fixed; otherwise, select <strong class="bold">All</strong> to generate masks for every face within the alignments file:</li>
			</ol>
			<div><div><img src="img/B17535_04_035.jpg" alt="Figure 4.35 – The Process settings of Faceswap’s Mask tool"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.35 – The Process settings of Faceswap’s Mask tool</p>
			<ol>
				<li value="4">The <strong class="bold">Output</strong> section is just for visualizing the masks, serving no practical purpose, so it can be ignored.</li>
				<li>Press the <strong class="bold">Mask</strong> button to generate the missing masks and save them to the alignments file.</li>
			</ol>
			<h3>Fixing masks</h3>
			<p>A final optional step<a id="_idIndexMarker206"/> is to fix up the generated masks. Much like face alignment, the neural networks that generate the masks are good, but they are often not perfect. This can be down to a number of reasons, such as lighting conditions and the quality of an image. In particular, obstructions in front of the face are not handled well by any of the maskers, so these will need to be manually edited.</p>
			<p>This should be the absolute last action performed on the alignments file. Any edits performed on landmark data within the alignment file will strip any neural network masks from the file and overwrite any edited landmark-based masks with the latest landmark data, destroying any manual edits that have been performed.</p>
			<p>The Manual tool, used to fix up the alignments, can also be used to fix masks:</p>
			<ol>
				<li>Launch the Manual tool by selecting the <strong class="bold">Tools</strong> tab, followed by the <strong class="bold">Manual</strong> sub-tab, and launch in the same way as before.</li>
				<li>Select the <strong class="bold">Mask Editor</strong> button from the buttons next to the frame viewer, and then select the mask type to be edited from the right-hand side options panel.</li>
				<li>Press the <strong class="bold">Mask Display</strong> toggle button next to the faces viewer to display the selected mask within the faces window.</li>
				<li>Scroll through the faces window, looking for masks that require fixing. If a face is discovered that requires editing, it can be clicked on to bring the relevant frame into the frame viewer. The <strong class="bold">Brush</strong> and <strong class="bold">Eraser</strong> tools can then be used to paint in or out the desired mask areas.</li>
			</ol>
			<p>Once all the masks have been fixed, press the <strong class="bold">Save</strong> button to save the mask edits to the alignments file.</p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor086"/>Using the Preview tool</h2>
			<p>It is possible to <a id="_idIndexMarker207"/>process the swap now and view <a id="_idIndexMarker208"/>the final output. However, some settings will need to be adjusted on a case-by-case basis, specifically various post-processing actions, such as mask erosion/blending, color correction, and sharpening.</p>
			<p>Faceswap includes the <strong class="bold">Preview tool</strong> to help lock these settings in prior to running the final conversion, which can be accessed by selecting the <strong class="bold">Tools</strong> tab and then the <strong class="bold">Preview</strong> sub-tab:</p>
			<div><div><img src="img/B17535_04_036.jpg" alt="Figure 4.36 – The location of the Preview tool within Faceswap’s GUI"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.36 – The location of the Preview tool within Faceswap’s GUI</p>
			<p>To launch the tool, provide the location of the video you intend to swap onto in the <strong class="bold">Input Dir</strong> field, and the folder that contains the trained model in the <strong class="bold">Model Dir</strong> field. The <strong class="bold">Alignments</strong> field can be left blank, unless the alignments file has been moved or renamed, in which case it will need to be explicitly specified:</p>
			<div><div><img src="img/B17535_04_037.jpg" alt="Figure 4.37 – The Data settings for Faceswap’s Preview tool"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.37 – The Data settings for Faceswap’s Preview tool</p>
			<p>Press the <strong class="bold">Preview</strong> button to launch the tool.</p>
			<p>The tool is split into <a id="_idIndexMarker209"/>three sections. The main <a id="_idIndexMarker210"/>window shows the faces from the original frame in the top row, with the swap applied with current settings in the bottom row. As settings are adjusted, the bottom row will update to reflect these changes.</p>
			<div><div><img src="img/B17535_04_038.jpg" alt="Figure 4.38 – Faceswap’s Preview tool"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.38 – Faceswap’s Preview tool</p>
			<p>The bottom-left panel displays command-line choices, while the bottom-right panel displays <a id="_idIndexMarker211"/>plugin <a id="_idIndexMarker212"/>settings.</p>
			<h3>Command-line choices</h3>
			<p>These are parameters<a id="_idIndexMarker213"/> that are chosen each time the conversion process is run (these options are not persistent), so you will need to remember what is set here to replicate it in the main Faceswap conversion process. Specifically, the options that can be set here are as follows:</p>
			<div><div><img src="img/B17535_04_039.jpg" alt="Figure 4.39 – The Preview Tool’s Command Line Choices"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.39 – The Preview Tool’s Command Line Choices</p>
			<ul>
				<li><strong class="bold">Color</strong>: The color-matching methodology to use. The best choice here will depend on the scene being converted. The <strong class="bold">Color Balance</strong>, <strong class="bold">Manual Balance</strong>, and <strong class="bold">Match Hist</strong> options have further configuration options that can be adjusted from the <strong class="bold">Plugin </strong><strong class="bold">Settings</strong> section.</li>
				<li><strong class="bold">Mask Type</strong>: The type of mask to use for overlaying the swapped face onto the original frame. By default, the landmarks-based <strong class="bold">extended</strong> and <strong class="bold">components</strong> masks will be available. Any additional neural network-based masks that exist within the alignments file will also be accessible, as well as the option to entirely disable the<a id="_idIndexMarker214"/> mask (<strong class="bold">None</strong>). The settings that control the blending of the chosen mask into the background frame are adjusted from the <strong class="bold">Plugin </strong><strong class="bold">Settings</strong> section.</li>
			</ul>
			<h3>Plugin settings</h3>
			<p>This section contains <a id="_idIndexMarker215"/>the configuration settings for the various post-processing plugins available in Faceswap. Values selected here, once saved, are persisted for all future conversions. As such, unlike the <strong class="bold">Command Line Choices</strong> options, there is no need to make a note of what is being set within this section.</p>
			<p>The plugin settings are split into three configuration groups:</p>
			<ul>
				<li><strong class="bold">Color</strong>: Configuration options for color-matching plugins. The actual methodology to use is selected within the <strong class="bold">Command Line Choices</strong> section, but some of the choices have additional configuration parameters that are controllable here. Make sure that the correct methodology is set within the <strong class="bold">Command Line Choices</strong> section to observe any changes within the main window.</li>
				<li><strong class="bold">Mask</strong>: Options to control the blending of the swapped image into the background frame. These settings are broken down into two further categories – <strong class="bold">Box Blend</strong>, which controls the settings that blend the extracted square containing the face into the background frame, and <strong class="bold">Mask Blend</strong>, which controls the settings for blending the mask around the face into the background frame. (Note that if <strong class="bold">None</strong> has been selected as the mask type in the <strong class="bold">Command Line Choices</strong> section, then any changes made within the <strong class="bold">Mask Blend</strong> settings will not be visible within the preview window.)</li>
			</ul>
			<p>How much of an impact each of these settings will have on the final output will depend greatly on the coverage and centering options that were selected when training the model. For example, with a low coverage and legacy centering (that is, very closely cropped), it is entirely possible that an extracted face box is contained entirely within the mask, in which case <strong class="bold">Mask Blend</strong> settings would have no visible <a id="_idIndexMarker216"/>effect. Similarly, with high coverage, and face or head centering, it is possible that the full mask exists within the extract box, in which case the <strong class="bold">Box Blend</strong> settings would have no visible effect. In most cases, adjusting a combination of the two blending settings will be necessary.</p>
			<ul>
				<li><strong class="bold">Scaling</strong>: The final configurable plugin controls any artificial sharpening to apply to an image. Quite often, the swapped face will need to be upscaled to fit into the final frame. This section allows you to control any sharpening effects to help better upscale the image.</li>
			</ul>
			<p class="callout-heading">Tip</p>
			<p class="callout">To get a better impression of the effects of adjusting the mask plugin settings, select <strong class="bold">Manual Balance</strong> as the color command-line choice, and then adjust <strong class="bold">Contrast</strong> and <strong class="bold">Brightness</strong> to <strong class="bold">–100</strong> within the <strong class="bold">Manual Balance</strong> plugin setting. This will display the swap area as entirely black, which can make it easier to adjust the mask correctly.</p>
			<h3>Tuning the conversion settings</h3>
			<p>The actual configuration choices to be used will vary on a video-by-video basis; there are no hard and fast rules, so it is just a question of adjusting the settings until a satisfactory result is achieved. Once a plugin is configured correctly, that plugin’s configuration can be saved by clicking the bottom-right <strong class="bold">Save</strong> button. To save the settings for all plugins that have been adjusted, click the bottom-left <strong class="bold">Save</strong> button.</p>
			<p>When appropriate <a id="_idIndexMarker217"/>settings have been locked in, make a note of the <strong class="bold">Command Line Choices</strong> settings and exit the Preview tool.</p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor087"/>Generating the swap</h2>
			<p>Once the model has been<a id="_idIndexMarker218"/> trained, the alignments file has been created, and the swap settings have been locked in, the final product can be created. The process of generating a swap is <a id="_idIndexMarker219"/>called <strong class="bold">converting</strong> – that is, converting the faces in a source video from their original form to the version generated from the trained model.</p>
			<p>Converting is probably the least involved of the main Faceswap processes. Access the <strong class="bold">Convert</strong> section of the Faceswap application by selecting the <strong class="bold">Convert</strong> tab:</p>
			<div><div><img src="img/B17535_04_040.jpg" alt="Figure 4.40 – The location of the Convert settings within Faceswap’s GUI"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.40 – The location of the Convert settings within Faceswap’s GUI</p>
			<h3>Data</h3>
			<p>This section <a id="_idIndexMarker220"/>is used to tell the process where the assets are located to perform the swap, as well as where the final output should be exported to:</p>
			<ul>
				<li><strong class="bold">Input Dir</strong>: The location of the source video or folder of images to be processed.</li>
				<li><strong class="bold">Output Dir</strong>: The location that the converted media should be outputted to. This folder should not pre-exist.</li>
				<li><strong class="bold">Alignments</strong>: Optionally, specify the location of the alignments file. If the alignments file is in its default location (next to the source video) with the default name, then this can be left blank, as the file will be detected.</li>
				<li><strong class="bold">Reference Video</strong>: This option is only required if the source is a folder of individual frames and the desired output is a video file. The reference video would be the original video file that the folder of frames was extracted from, and it provides the conversion process with the audio track and the FPS that should be compiled into the final video.</li>
				<li><strong class="bold">Model Dir</strong>: The <a id="_idIndexMarker221"/>location of the folder that contains the trained Faceswap model.</li>
			</ul>
			<div><div><img src="img/B17535_04_041.jpg" alt="Figure 4.41 – The Data options within Faceswap’s Convert settings"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.41 – The Data options within Faceswap’s Convert settings</p>
			<h3>Plugins</h3>
			<p>There are several plugins <a id="_idIndexMarker222"/>available for the conversion process, which are selectable here. Two of the plugins will have been seen before when we used the Preview tool (<strong class="bold">Color Adjustment</strong> and <strong class="bold">Mask Type</strong>), so ensure that you select the same options here as those selected within the Preview tool for the output to remain consistent:</p>
			<ul>
				<li><strong class="bold">Color Adjustment</strong>: The color correction to use. This will have been previewed and selected using the Preview tool, so select the same plugin here. The actual plugin to use will vary from project to project.</li>
				<li><strong class="bold">Mask Type</strong>: The type of mask to use to overlay the swapped face over the original frame. The chosen mask here must exist within the alignments file (<strong class="bold">Components</strong> and <strong class="bold">Extended</strong> will always exist; other masks need to be generated). Generally, this will be the same mask that the model was trained with and will have been previewed with the Preview tool, so select the same mask that was used to preview.</li>
				<li><strong class="bold">Writer</strong>: The plugin<a id="_idIndexMarker223"/> to use to create the final media. The writer plugins are used to generate the final product. <strong class="bold">Ffmpeg</strong> is used to create video files, <strong class="bold">Gif</strong> is used to create animated GIFs, and <strong class="bold">OpenCV</strong> and <strong class="bold">Pillow</strong> will create a folder of images, with OpenCV being quicker but having a more limited file format choice than Pillow.</li>
			</ul>
			<p>The writers can each be configured by selecting <strong class="bold">Settings</strong> | <strong class="bold">Configure Settings</strong> and selecting the relevant writer plugin under the <strong class="bold">Convert</strong> node:</p>
			<div><div><img src="img/B17535_04_042.jpg" alt="Figure 4.42 – Faceswap’s Convert plugin settings"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.42 – Faceswap’s Convert plugin settings</p>
			<p>It is possible to<a id="_idIndexMarker224"/> create the swap on a separate transparent layer only containing the swapped face and the mask, to overlay over the original frame in various external VFX applications. The <strong class="bold">OpenCV</strong> and <strong class="bold">Pillow</strong> writers both support this, with OpenCV allowing the generation of four-channel PNG images and Pillow allowing the generation of four-channel PNG or TIFF images. This option can be enabled by selecting the <strong class="bold">Draw Transparent</strong> option within either of these plugins’ configuration settings.</p>
			<div><div><img src="img/B17535_04_043.jpg" alt="Figure 4.43 – The Plugins options within Faceswap’s Convert settings"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.43 – The Plugins options within Faceswap’s Convert settings</p>
			<h3>Other settings</h3>
			<p><strong class="bold">Frame Processing</strong> can be<a id="_idIndexMarker225"/> ignored in most cases. The most likely option of interest is <strong class="bold">Output Scale</strong> though, which scales the final output media by the designated amount. For example, setting an output scale of <strong class="bold">50</strong> for a 720p input video will result in a final output at 360p.</p>
			<p>The <strong class="bold">Face Processing</strong> section can be ignored. If the alignments file has been created correctly, then none of the options here are relevant. Similarly, most of the options within the <strong class="bold">Settings</strong> section can be ignored in most cases. The only possible exception to this is the <strong class="bold">Swap Model</strong> option. This can be used to create a swap in the opposite direction to which the model was trained – that is, instead of <strong class="bold">A</strong> &gt; <strong class="bold">B</strong>, the conversion will run <strong class="bold">B</strong> &gt; <strong class="bold">A</strong>. This can be useful if you have a model trained on a face pair and you wish to run conversion in the opposite direction, or if a model has accidentally been trained the wrong way around (the original face has been trained on the <strong class="bold">B</strong> side, with the desired swap trained on the <strong class="bold">A</strong> side).</p>
			<p>Once all of the settings are set correctly, press the <strong class="bold">Convert</strong> button to apply the trained model on your source media and generate the swap in the output destination.</p>
			<h1 id="_idParaDest-89"><a id="_idTextAnchor088"/>Summary</h1>
			<p>In this chapter, we learned the workflow required to create a deepfake using the open source Faceswap software. The importance of data variety was discussed and the steps required to acquire, curate and generate face sets were demonstrated. We learned how to train a model within Faceswap, and how to gauge when a model has been fully trained, as well as learned some tricks to improve the quality of the model. Finally, we learned how to take our trained model and apply it to a source video to swap the faces within the video.</p>
			<p>In the next chapter, we will begin to take a hands-on look at the neural networks available to build a deepfake pipeline from scratch using the PyTorch ML toolkit, starting with the models available for detecting and extracting faces from source images.</p>
		</div>
		<div><div></div>
		</div>
	<div><p>EBSCOhost - printed on 11/27/2023 6:20 AM via . All use subject to <a target="_blank" href="https://www.ebsco.com/terms-of-use">https://www.ebsco.com/terms-of-use</a></p></div>


		<div><h1 id="_idParaDest-90"><a id="_idTextAnchor089"/>Part 2: Getting Hands-On with the Deepfake Process</h1>
			<p>This part of the book is all about getting hands-on with the code. We will look deep into exactly what it takes to make a deepfake from beginning to end, leaving no stone unturned or line of code unexplained. If you’re here for the code, this is the section for you.</p>
			<p>In the first chapter of this section, we’ll examine extraction. This is the process of getting all the faces out of a video so that we can use them in other stages of the process. We’ll look at the process of turning a video into frame images, then we’ll go through all the code necessary to turn the frames into clean, aligned faces with matching mask images ready for training. After that, we’ll look into training, examine the neural network from the bottom up, and then show the entire learning process of the model. Finally, we’ll get into conversion, where we’ll examine the process of going through every image to swap a new face onto the original, including turning it back into a video.</p>
			<p>By the end of this part, you’ll know exactly how to code your own deepfakes from beginning to end.</p>
			<p>This part comprises the following chapters:</p>
			<ul>
				<li><a href="B17535_05.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a>, <em class="italic">Extracting Faces</em></li>
				<li><a href="B17535_06.xhtml#_idTextAnchor107"><em class="italic">Chapter 6</em></a>, <em class="italic">Training a Deepfake Model</em></li>
				<li><a href="B17535_07.xhtml#_idTextAnchor123"><em class="italic">Chapter 7</em></a>, <em class="italic">Swapping the Face back into the Video</em></li>
			</ul>
		</div>
		<div><div></div>
		</div>
	<div><p>EBSCOhost - printed on 11/27/2023 6:20 AM via . All use subject to <a target="_blank" href="https://www.ebsco.com/terms-of-use">https://www.ebsco.com/terms-of-use</a></p></div>
</body></html>