- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working with Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to cover another great capability of Large Language
    Models, that is, working with programming languages. In the previous chapter,
    we’ve already seen a glimpse of this capability, namely, SQL query generation
    in a SQL database. In this chapter, we are going to examine the other ways in
    which LLMs can be used with code, from “simple” code generation to interaction
    with code repositories and, finally, to the possibility of letting an application
    behave as if it were an algorithm. By the end of this chapter, you will be able
    to leverage LLMs to code-related projects, as well as build LLM-powered applications
    with natural language interfaces to work with code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of the main LLMs with top-performing code capabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using LLMs for code understanding and generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building LLM-powered agents to “act as” algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging Code Interpreter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To complete the tasks in this chapter, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A Hugging Face account and user access token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An OpenAI account and user access token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.7.1 or a later version.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Python packages. Make sure you have the following Python packages installed:
    `langchain`, `python-dotenv`, `huggingface_hub`, `streamlit`, `codeinterpreterapi`,
    and `jupyter_kernel_gateway`. Those can be easily installed via `pip` `install`
    in your terminal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find all the code and examples in the book’s GitHub repository at [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_09.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right LLM for code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Chapter 3*, we described a decision framework to use in order to decide
    the proper LLM for a given application. Generally speaking, all LLMs are endowed
    with knowledge of code understanding and generation; however, some of them are
    particularly specialized in doing so. More specifically, there are some evaluation
    benchmarks – such as the HumanEval – that are specifically tailored to assessing
    LLMs’ capabilities of working with code. The leaderboard of HumanEval One is a
    good source for determining the top-performing models, available at [https://paperswithcode.com/sota/code-generation-on-humaneval](https://paperswithcode.com/sota/code-generation-on-humaneval).
    HumanEval is a benchmark introduced by OpenAI to assess the code generation capabilities
    of LLMs, where the model completes Python functions based on their signature and
    docstring. It has been used to evaluate models like Codex, demonstrating its effectiveness
    in measuring functional correctness.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, you can see the situation of the leaderboard as
    of January 2024:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21714_09_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: HumanEval benchmark in January 2024'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the majority of the models are fine-tuned versions of the GPT-4
    (as well as the GPT-4 itself), as it is the state-of-the-art LLM in basically
    all the domains. Nevertheless, there are many open-source models that reached
    stunning results in the field of code understanding and generation, some of which
    will be covered in the next sections. Another benchmark is **Mostly Basic Programming
    Problems** (**MBPP**), a dataset of 974 programming tasks in Python, designed
    to be solvable by entry-level programmers. Henceforth, when choosing your model
    for a code-specific task, it might be useful to have a look at these benchmarks
    as well as other similar code metrics (we will see throughout the chapter some
    further benchmarks for code-specific LLMs).
  prefs: []
  type: TYPE_NORMAL
- en: 'Staying within the scope of coding, below you can find three additional benchmarks
    often used in the market:'
  prefs: []
  type: TYPE_NORMAL
- en: '**MultiPL-E**: An extension of HumanEval to many other languages, such as Java,
    C#, Ruby, and SQL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DS-1000**: A data science benchmark that tests if the model can write code
    for common data analysis tasks in Python.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tech Assistant Prompt**: A prompt that tests if the model can act as a technical
    assistant and answer programming-related requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to test different LLMs: two code-specific (CodeLlama
    and StarCoder) and one general-purpose, yet also with emerging capabilities in
    the field of code generation (Falcon LLM).'
  prefs: []
  type: TYPE_NORMAL
- en: Code understanding and generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first experiment we are going to run will be code understanding and generation
    leveraging LLMs. This simple use case is at the base of the many AI code assistants
    that were developed since the launch of ChatGPT, first among all the GitHub Copilot.
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: GitHub Copilot is an AI-powered tool that assists developers in writing code
    more efficiently. It analyzes code and comments to provide suggestions for individual
    lines and entire functions. The tool is developed by GitHub, OpenAI, and Microsoft
    and supports multiple programming languages. It can perform various tasks such
    as code completion, modification, explanation, and technical assistance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this experiment, we are going to try three different models: Falcon LLM,
    which we already explored in *Chapter 3*; CodeLlama, a fine-tuned version of Meta
    AI’s Llama; and StarCoder, a code-specific model that we are going to investigate
    in the upcoming sections.'
  prefs: []
  type: TYPE_NORMAL
- en: Since those models are pretty heavy to run on a local machine, for this purpose
    I’m going to use a Hugging Face Hub Inference Endpoint, with a GPU-powered virtual
    machine. You can link one model per Inference Endpoint and then embed it in your
    code, or use the convenient library `HuggingFaceEndpoint`, available in LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start using your Inference Endpoint, you can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can copy and paste the Python code provided on your endpoint’s
    webpage at [https://ui.endpoints.huggingface.co/user_name/endpoints/your_endpoint_name](https://ui.endpoints.huggingface.co/user_name/endpoints/your_endpoint_name):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21714_09_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: User interface of the Hugging Face Inference Endpoint'
  prefs: []
  type: TYPE_NORMAL
- en: To create your Hugging Face Inference Endpoint, you can follow the instructions
    at [https://huggingface.co/docs/inference-endpoints/index](https://huggingface.co/docs/inference-endpoints/index).
  prefs: []
  type: TYPE_NORMAL
- en: You can always leverage the free Hugging Face API as described in *Chapter 4*,
    but you have to expect some latency when running the models.
  prefs: []
  type: TYPE_NORMAL
- en: Falcon LLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Falcon LLM is an open-source model developed by Abu Dhabi’s **Technology Innovation
    Institute** (**TII**) and launched on the market in May 2023\. It is an autoregressive,
    decoder-only transformer, trained on 1 trillion tokens, and has 40 billion parameters
    (although it has also been released as a lighter version with 7 billion parameters).
    As discussed in *Chapter 3*, “small” language models are a representation of a
    new trend of LLMs, consisting of building lighter models (with fewer parameters)
    that focus instead on the quality of the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start using Falcon LLM, we can follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can leverage the HuggingFaceHub wrapper available in LangChain (remember
    to set the Hugging Face API in the `.env` file, passing your secrets as `os.environ["HUGGINGFACEHUB_API_TOKEN"]
    = HUGGINGFACEHUB_API_TOKEN`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we’ve initialized the model, let’s ask it to generate the code for
    a simple webpage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the corresponding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If you save it as an HTML file and execute it, the result will look like the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A close up of a text  Description automatically generated](img/B21714_09_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3: Sample webpage generated by FalconLLM'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also try to generate a Python function to generate random passwords:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is our output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We now have a function named `generate_password()`, which uses random functions
    to generate a password as per our prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s do the opposite, asking the model to explain to us the above
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the obtained output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Overall, even if not code-specific, the model was able to correctly perform
    all the tasks. Note also that this is the “light” version of the model (7 billion
    parameters), yet its performance is great.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now investigate the capabilities of CodeLlama.
  prefs: []
  type: TYPE_NORMAL
- en: CodeLlama
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CodeLlama is a family of LLMs for code based on Llama 2, which is a general-purpose
    language model developed by Meta AI (as discussed in *Chapter 3*). CodeLlama can
    generate and discuss code in various programming languages, such as Python, C++,
    Java, PHP, and more. CodeLlama can also perform infilling, which is the ability
    to fill in missing parts of code based on the surrounding context, as well as
    follow instructions given in natural language and produce code that matches the
    desired functionality.
  prefs: []
  type: TYPE_NORMAL
- en: The model comes in three sizes (7B, 13B, and 34B parameters) and three flavors
    (base model, Python fine-tuned, and instruction-tuned) to cover a wide range of
    applications. CodeLlama is trained on sequences of 16k tokens and can handle inputs
    with up to 100k tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the model paper “Code Llama: Open Foundation Models for Code” by Rozière
    Baptiste et al, released in August 2023, the authors describe how the various
    models were tested against some of the most popular evaluation benchmarks in the
    domain of code understanding and generation, including HumanEval and MBPP, according
    to which CodeLlama models achieved a score up to 53% and 55%, respectively. On
    top of those remarkable results, it is stunning that the Python fine-tuned CodeLlama’s
    smallest size (7 billion parameters) outperformed the largest version of Llama
    2 (70 billion parameters) on HumanEval and MBPP.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s run some tests with this model. As per the previous section, we
    can initialize the model leveraging either the Hugging Face Inference API (pay
    per use) or the free Hugging Face API (with the constraint of higher latency).
    You can consume it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now test it with some code tasks. The first task will be that of optimizing
    Python code so that it runs more efficiently. Let’s see how our model performs
    in this task. In the following code snippet, we simply prompt the model to regenerate
    the provided code in a more efficient way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'def factorial(n):'
  prefs: []
  type: TYPE_NORMAL
- en: result = 1
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'for i in range(1, n + 1):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: result *= i
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: return result
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Example usage:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: n = 5
  prefs: []
  type: TYPE_NORMAL
- en: print("Factorial of", n, "is", factorial(n))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is our output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'def factorial(n):'
  prefs: []
  type: TYPE_NORMAL
- en: 'if n == 0:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: return 1
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'else:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: return n * factorial(n - 1)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the model was able to use a recursive approach, which is more
    efficient and “Pythonic.” It also provides a reference for the user to dive deeper
    into the mathematical theory behind the function.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s leverage the model’s completion capabilities by initializing a function
    to remove non-ASCII characters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: '**American Standard Code for Information Interchange** (**ASCII**) is a character
    encoding standard that uses 7 bits to represent 128 characters, such as letters,
    digits, punctuation marks, and control codes.'
  prefs: []
  type: TYPE_NORMAL
- en: Non-ASCII characters are those that are not part of the ASCII standard and use
    more than 7 bits to encode. They include special characters such as letters with
    accents, glyphs, ideograms, and mathematical symbols. Non-ASCII characters can
    be encoded using different standards, such as Unicode, ISO 8859-1, Windows-1252,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the letter é is a non-ASCII character that can be encoded using
    Unicode as U+00E9 or using Windows-1252 as 0xE9.
  prefs: []
  type: TYPE_NORMAL
- en: 'Accordingly, here is the code to generate the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the function that we receive as the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now leverage the model as a bug fixer, prompting it with the wrong function
    and also asking it to provide an explanation of why it is wrong and how it can
    be fixed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#wrong function'
  prefs: []
  type: TYPE_NORMAL
- en: import random
  prefs: []
  type: TYPE_NORMAL
- en: a = random.randint(1, 12)
  prefs: []
  type: TYPE_NORMAL
- en: b = random.randint(1, 12)
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(10):'
  prefs: []
  type: TYPE_NORMAL
- en: question = "What is " + a + " x " + b + "? "
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: answer = input(question)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'if answer = a * b:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: print (Well done!)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'else:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: print("No.")
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is our output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let’s ask the model in natural language to generate specific Python
    code for a given task that, in our example, will be that of writing a function
    that finds the longest substring of a given string containing only unique characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We then get the following function as our output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'As per the Falcon LLM, in this case we used the light version of the model
    (7 billion parameters), still obtaining great results. This is a perfect example
    of how the task you want to address with your application must be a factor in
    deciding what LLM to use: if you are only interested in code generation, completion,
    infilling, debugging, or any other code-related tasks, a light and open-source
    model could be more than enough, rather than 70 billion parameters of a state-of-the-art
    GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to cover the third and last LLM in the context
    of code generation and understanding.
  prefs: []
  type: TYPE_NORMAL
- en: StarCoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The StarCoder model is an LLM for code that can perform various tasks, such
    as code completion, code modification, code explanation, and technical assistance.
    It was trained on permissively licensed data from GitHub, including from 80+ programming
    languages, Git commits, GitHub issues, and Jupyter notebooks. It has a context
    length of over 8,000 tokens, which enables it to process more input than any other
    open-source language model. It also has an improved license that simplifies the
    process for companies to integrate the model into their products.
  prefs: []
  type: TYPE_NORMAL
- en: The StarCoder model was evaluated on several benchmarks that test its ability
    to write and understand code in different languages and domains, including the
    aforementioned HumanEval and MBPP, where the model scored, respectively, 33.6%
    and 52.7%. Additionally, it was tested against MultiPL-E (where the model matched
    or outperformed the code-cushman-001 model from OpenAI on many languages), the
    DS-1000 (where the model clearly beat the code-cushman-001 model as well as all
    other open-access models), and the Tech Assistant Prompt (where the model was
    able to respond to various queries with relevant and accurate information).
  prefs: []
  type: TYPE_NORMAL
- en: 'According to a survey published on May 4 2023 by Hugging Face, StarCoder demonstrated
    great capabilities compared to other models, using HumanEval and MBPP as benchmarks.
    You can see an illustration of this study below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_09_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.4: Results of evaluation benchmarks for various LLMs. Source: [https://huggingface.co/blog/starcoder](https://huggingface.co/blog/starcoder)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start using StarCoder, we can follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can leverage the HuggingFaceHub wrapper available in LangChain (remember
    to set the Hugging Face API in the `.env` file):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s set the `repo_id` for the StarCoder model and initialize it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: StarCoder is a gated model on the Hugging Face Hub, meaning that you will need
    to request access directly from the bigcode/starcoderplus repo before being able
    to connect to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’re set up, let’s start asking our model to compile some code. To
    start with, we will ask it to generate a Python function to generate the nth Fibonacci
    number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: The Fibonacci sequence is a mathematical series that begins with 0 and 1, and
    each subsequent number is the sum of the two preceding numbers. For instance,
    the first 10 numbers of the Fibonacci sequence are 0, 1, 1, 2, 3, 5, 8, 13, 21,
    and 34.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different ways to compute the nth Fibonacci number, which is denoted
    by F(n). One way is to use a recursive formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21714_09_001.png)'
  prefs: []
  type: TYPE_IMG
- en: This means that to find F(n), we need to find F(n-1) and F(n-2) first, and then
    add them together. This works for any n greater than or equal to 2\. For n equal
    to 0 or 1, we simply return n as the answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_09_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5: Example of Fibonacci functions generated by StarCode'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, it also proposed different approaches to solve the problem,
    alongside the explanation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now ask the model to generate a webpage to play tic tac toe against the
    computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the corresponding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: git clone https://github.com/Mohamed-Elhawary/tic-tac-toe.git
  prefs: []
  type: TYPE_NORMAL
- en: cd tic-tac-toe
  prefs: []
  type: TYPE_NORMAL
- en: python3 -m http.server
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly enough, the model in this case didn’t generate the whole code;
    rather, it gave the instructions to clone and run a git repository that can achieve
    this result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, StarCoder is also available as an extension in VS Code to act as your
    code copilot. You can find it as **HF Code Autocomplete**, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_09_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6: Hugging Face Code Autocomplete extension, powered by StarCoder'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once enabled, you can see that, while compiling your code, StarCoder will provide
    suggestions to complete the code. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21714_09_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.7: Screenshot of a suggested completion, given a function description'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, I commented my code, describing a function to generate the nth
    Fibonacci number, and then started defining the function. Automatically, I’ve
    been provided with the StarCoder auto-completion suggestion.
  prefs: []
  type: TYPE_NORMAL
- en: Code understanding and generation are great capabilities of LLMs. On top of
    those capabilities, there are further applications that we can think about, going
    beyond code generation. In fact, the code can be seen also as a backend reasoning
    tool to propose solutions to complex problems, such as an energy optimization
    problem rather than an algorithm task. To do this, we can leverage LangChain to
    create powerful agents that can *act as if they were algorithms*. In the upcoming
    section, we will see how to do so.
  prefs: []
  type: TYPE_NORMAL
- en: Act as an algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some problems are complex by definition and difficult to solve leveraging “only”
    LLMs’ analytical reasoning skills. However, LLMs are still intelligent enough
    to understand the problems overall and leverage their coding capabilities to solve
    them.
  prefs: []
  type: TYPE_NORMAL
- en: In this context, LangChain provides a tool that empowers the LLM to reason “in
    Python,” meaning that the LLM-powered agent will leverage Python to solve complex
    problems. This tool is the Python REPL, which is a simple Python shell that can
    execute Python commands. The Python REPL is important because it allows users
    to perform complex calculations, generate code, and interact with language models
    using Python syntax. In this section, we will cover some examples of the tool’s
    capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first initialize our agent using the `create_python_agent` class in LangChain.
    To do so, we will need to provide this class with an LLM and a tool, which, in
    our example, will be the Python REPL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'As always, before starting to work with the agent, let’s first inspect the
    default prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is our output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer program  Description automatically generated](img/B21714_09_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.8: Default prompt of the Python agent'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s start with an easy query, asking the model to generate a scatter
    plot based on sample attributes of basketball players:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We then get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This output is accompanied by the following graph based on the players’ statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer program  Description automatically generated](img/B21714_09_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.9: Sample plot generated by the Python agent'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at another example. Say we want to predict the price of a house
    based on some features, such as the number of bedrooms or the size of the house.
    To do so, we can ask our agent to design and train a model to give us the result
    of a given house. For example, let’s consider the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we ask the agent to train a regression model on synthetic data (representative
    of houses with various configurations of rooms, bathrooms, and area, each with
    an associated price as a dependent variable) to give us the estimated price of
    a house with the above features. Let’s see the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the agent was able to generate synthetic training data, train
    a proper regression model using the `sklearn` libraries, and predict with the
    model the price of the house we provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this approach, we can program an agent to act as an algorithm in real-time
    scenarios. Imagine, for example, that we want to design an agent that is capable
    of solving optimization problems in a smart building environment. The goal is
    to optimize the **Heating, Ventilation and Air Conditioning** (**HVAC**) setpoints
    in the building to minimize energy costs while ensuring occupant comfort. Let’s
    define the variables and constraints of the problem: the objective is to adjust
    the temperature setpoints within the specified comfort ranges for each of the
    three zones while considering the varying energy costs per degree, per hour.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is to strike a balance between energy efficiency and occupant comfort.
    Below, you can find a description of the problem and also the initialization of
    our variables and constraints (energy cost per zone, initial temperature per zone,
    and comfort range per zone):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We then get the following output (you can find the whole reasoning chain in
    the book’s GitHub repository):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The agent was able to solve the smart building optimization problem, finding
    the minimum total energy cost, given some constraints. Staying in the scope of
    optimization problems, there are further use cases that these models could address
    with a similar approach, including:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supply chain optimization**: Optimize the logistics and distribution of goods
    to minimize transportation costs, reduce inventory, and ensure timely deliveries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Portfolio optimization**: In finance, use algorithms to construct investment
    portfolios that maximize returns while managing risk.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Route planning**: Plan optimal routes for delivery trucks, emergency services,
    or ride-sharing platforms to minimize travel time and fuel consumption.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manufacturing process optimization**: Optimize manufacturing processes to
    minimize waste, energy consumption, and production costs while maintaining product
    quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Healthcare resource allocation**: Allocate healthcare resources like hospital
    beds, medical staff, and equipment efficiently during a pandemic or other healthcare
    crisis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network routing**: Optimize data routing in computer networks to reduce latency,
    congestion, and energy consumption.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fleet management**: Optimize the use of a fleet of vehicles, such as taxis
    or delivery vans, to reduce operating costs and improve service quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inventory management**: Determine optimal inventory levels and reorder points
    to minimize storage costs while preventing stockouts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Agricultural planning**: Optimize crop planting and harvesting schedules
    based on weather patterns and market demand to maximize yield and profits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Telecommunications network design**: Design the layout of telecommunications
    networks to provide coverage while minimizing infrastructure costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Waste management**: Optimize routes for garbage collection trucks to reduce
    fuel consumption and emissions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Airline crew scheduling**: Create efficient flight crew schedules that adhere
    to labor regulations and minimize costs for airlines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Python REPL agent is amazing; however, it comes with some caveats:'
  prefs: []
  type: TYPE_NORMAL
- en: It does not allow for FileIO, meaning that it cannot read and write with your
    local file system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It forgets the variables after every run, meaning that you cannot keep trace
    of your initialized variables after the model’s response.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To bypass these caveats, in the next section, we are going to cover an open-source
    project built on top of the LangChain agent: the Code Interpreter API.'
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging Code Interpreter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The name “Code Interpreter” was coined by OpenAI, referring to the recently
    developed plugin for ChatGPT. The Code Interpreter plugin allows ChatGPT to write
    and execute computer code in various programming languages. This enables ChatGPT
    to perform tasks such as calculations, data analysis, and generating visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: The Code Interpreter plugin is one of the tools designed specifically for language
    models with safety as a core principle. It helps ChatGPT access up-to-date information,
    run computations, or use third-party services. The plugin is currently in private
    beta and is available for selected developers and ChatGPT Plus users.
  prefs: []
  type: TYPE_NORMAL
- en: While OpenAI’s Code Interpreter still doesn’t offer an API, there are some open-source
    projects that adapted the concept of this plugin in an open-source Python library.
    In this section, we are going to leverage the work of Shroominic, available at
    [https://github.com/shroominic/codeinterpreter-api](https://github.com/shroominic/codeinterpreter-api).
    You can install it via `pip install codeinterpreterapi`.
  prefs: []
  type: TYPE_NORMAL
- en: According to the blog post published by Shroominic, the author of the Code Interpreter
    API (which you can read at [https://blog.langchain.dev/code-interpreter-api/](https://blog.langchain.dev/code-interpreter-api/)),
    it is based on the LangChain agent `OpenAIFunctionsAgent`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: OpenAIFunctionsAgent is a type of agent that can use the OpenAI functions’ ability
    to respond to the user’s prompts using an LLM. The agent is driven by a model
    that supports using OpenAI functions, and it has access to a set of tools that
    it can use to interact with the user.
  prefs: []
  type: TYPE_NORMAL
- en: The OpenAIFunctionsAgent can also integrate custom functions. For example, you
    can define custom functions to get the current stock price or stock performance
    using Yahoo Finance. The OpenAIFunctionsAgent can use the ReAct framework to decide
    which tool to use, and it can use memory to remember the previous conversation
    interactions.
  prefs: []
  type: TYPE_NORMAL
- en: The API comes already with some tools, such as the possibility to navigate the
    web to get up-to-date information.
  prefs: []
  type: TYPE_NORMAL
- en: Yet the greatest difference from the Python REPL tool that we covered in the
    previous section is that the Code Interpreter API can actually execute the code
    it generates. In fact, when a Code Interpreter session starts, a miniature of
    a Jupyter Kernel is launched on your device, thanks to the underlying Python execution
    environment called CodeBox.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start using the code interpreter in your notebook, you can install all the
    dependencies as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, I will ask it to generate a plot of COVID-19 cases in a specific
    time range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the generated output, including a graph that shows the number of global
    confirmed cases in the specified time period:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '![A graph with a line going up  Description automatically generated](img/B21714_09_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.10: Line chart generated by the Code Intepreter API'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the Code Interpreter answered the question with an explanation
    as well as a plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try another one, this time also leveraging its real-time capabilities
    of searching for up-to-date information. In the following snippet, we ask the
    model to plot the price of the S&P 500 index over the last 5 days:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We then get the following output, together with a line graph showing the price
    of the S&P 500 index over the last 5 days:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![A graph with numbers and a line  Description automatically generated](img/B21714_09_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.11: S&P 500 index price plotted by the Code Interpreter API'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can provide local files to the Code Interpreter so that it can perform
    some analyses on that specific data. For example, I’ve downloaded the Titanic
    dataset from Kaggle at [https://www.kaggle.com/datasets/brendan45774/test-file](https://www.kaggle.com/datasets/brendan45774/test-file).
    The Titanic dataset is a popular dataset for machine learning that describes the
    survival status of individual passengers on the Titanic. It contains information
    such as age, sex, class, fare, and whether they survived or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the dataset had downloaded, I passed it as a parameter to the model as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We then get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '![A screenshot of a graph  Description automatically generated](img/B21714_09_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.12: Sample plots generated by the Code Interpreter API'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the model was able to generate to bar charts showing the survival
    status grouped by sex (in the first plot) and then by class (in the second plot).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Code Interpreter plugin, together with code-specific LLMs and the Python
    agent, are great examples of how LLMs are having a huge impact on the world of
    software development. This can be summarized in two main capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs can understand and generate code, since they have been trained on a huge
    amount of programming languages, GitHub repos, StackOverflow conversations, and
    so on. Henceforth, along with natural language, programming languages are part
    of their parametric knowledge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can understand a user’s intent and act as a reasoning engine to activate
    tools like Python REPL or Code Interpreter, which are then able to provide a response
    by working with code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Overall, LLMs are going well beyond the elimination of the gap between natural
    language and machine language: rather, they are integrating the two so that they
    can leverage each other to respond to a user’s query.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored multiple ways in which LLMs can be leveraged to
    work with code. Armed with a refresher of how to evaluate LLMs and the specific
    evaluation benchmarks to take into account when choosing an LLM for code-related
    tasks, we delved into practical experimentations.
  prefs: []
  type: TYPE_NORMAL
- en: We started from the “plain vanilla” application that we have all tried at least
    once using ChatGPT, which is code understanding and generation. For this purpose,
    we leveraged three different models – Falcon LLM, CodeLlama, and StarCoder – each
    resulting in very good results.
  prefs: []
  type: TYPE_NORMAL
- en: We then moved forward with the additional applications that LLMs’ coding capabilities
    can have in the real world. In fact, we saw how code-specific knowledge can be
    used as a booster to solve complex problems, such as algorithmic or optimization
    tasks. Furthermore, we covered how code knowledge can not only be used in the
    backend reasoning of an LLM but also actually executed in a working notebook,
    leveraging the open-source version of the Code Interpreter API.
  prefs: []
  type: TYPE_NORMAL
- en: With this chapter, we are getting closer to the end of Part 2\. So far, we have
    covered the multiple capabilities of LLMs, while always handling language data
    (natural or code). In the next chapter, we will see how to go a step further toward
    multi-modality and build powerful multi-modal agents that can handle data in multiple
    formats.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The open-source version of the Code Interpreter API: [https://github.com/shroominic/codeinterpreter-api](https://github.com/shroominic/codeinterpreter-api)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'StarCoder: [https://huggingface.co/blog/starcoder](https://huggingface.co/blog/starcoder)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The LangChain agent for the Python REPL: [https://python.langchain.com/docs/integrations/toolkits/python](https://python.langchain.com/docs/integrations/toolkits/python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A LangChain blog about the Code Interpreter API: [https://blog.langchain.dev/code-interpreter-api/](https://blog.langchain.dev/code-interpreter-api/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Titanic dataset: [https://www.kaggle.com/datasets/brendan45774/test-file](https://www.kaggle.com/datasets/brendan45774/test-file)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The HF Inference Endpoint: [https://huggingface.co/docs/inference-endpoints/index](https://huggingface.co/docs/inference-endpoints/index
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The CodeLlama model card: [https://huggingface.co/codellama/CodeLlama-7b-hf](https://huggingface.co/codellama/CodeLlama-7b-hf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Code Llama: Open Foundation Models for Code, *Rozière. B., et al* (2023): [https://arxiv.org/abs/2308.12950](https://arxiv.org/abs/2308.12950)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Falcon LLM model card: [https://huggingface.co/tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The StarCoder model card: [https://huggingface.co/bigcode/starcoder](https://huggingface.co/bigcode/starcoder)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/llm]( https://packt.link/llm )'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code214329708533108046.png)'
  prefs: []
  type: TYPE_IMG
