<html><head></head><body>
  <div><h1 class="chapterNumber"><a id="_idTextAnchor055"/>2</h1>
    <h1 class="chapterTitle" id="_idParaDest-54"><a id="_idTextAnchor056"/>Building the Generative AI Controller</h1>
    <p class="normal">A <strong class="keyWord">generative AI system</strong> (<strong class="keyWord">GenAISys</strong>)’s controller requires two key components: a <strong class="keyWord">conversational agent</strong> and an <strong class="keyWord">orchestrator</strong>. The conversational agent—powered by a generative <a id="_idIndexMarker115"/>AI model—interacts with human users and system processes. The orchestrator, on the other hand, is a set of generative AI and non-AI functions, such as managing user roles, content generation, activating machine learning algorithms, and running classical queries. We need both to build a functional GenAISys.</p>
    <p class="normal">If we examine this architecture closely, we’ll see that software orchestrators and user interfaces date back to the first computers. Any operating system, with even basic functionality, has orchestrators that trigger disk space alerts, memory usage, and hundreds of other functions. Today’s user interfaces are intuitive and have event-driven functionality, but at a high level, the underlying architecture of a GenAISys still echoes decades of software design principles. So, what sets a classical software controller apart from a GenAISys controller?</p>
    <p class="normal">We can sum up the difference in one word: <em class="italic">adaptability</em>. In a classical software controller, a sequence of tasks is more or less hardcoded. But in a GenAISys, the user interface is a conversational AI agent that is flexible, and the generative AI model behind it is pre-trained to respond to a wide range of requests with no additional coding. Furthermore, the orchestrator isn’t locked into static flows either; it can modify the tasks it triggers based on the user (human or system) prompts.</p>
    <p class="normal">In this chapter, we’ll take a hands-on approach to building a custom GenAISys based on the architecture of a GenAISys defined in the previous chapter. We’ll begin by defining the structure of our AI controller in Python, breaking it into two parts—the conversational agent and the orchestrator—and exploring how the two interact. Then, we’ll build the conversational agent using GPT-4o. We’ll automate the contextual awareness and memory retention features from <a href="Chapter_1.xhtml#_idTextAnchor021"><em class="italic">Chapter 1</em></a>. Our system will support both short-term and long-term memory, as well as multi-user and cross-session capabilities—pushing it beyond what standard copilots typically offer.</p>
    <p class="normal">Finally, we will build the structure of an AI controller to interpret user input and trigger a response scenario. The response will be a sentiment analysis or a semantic (hard science) analysis, depending on the context of what the AI controller will analyze and manage. Our custom GenAISys will lay the groundwork for domain-specific RAG, something a standard ChatGPT-grade system can’t offer when you’re working with large volumes of data, especially in cases of daily dataset updates, such as the daily sales of a product or service. By the end of this chapter, you’ll know how to build the foundations of a GenAISys AI controller that we will enhance throughout the book.</p>
    <p class="normal">To sum up, this chapter covers the following topics:</p>
    <ul>
      <li class="bulletList">Architecture of the AI controller</li>
      <li class="bulletList">Architecture of an AI conversational agent and its workflow</li>
      <li class="bulletList">Implementing the storage of short- and long-term memory sessions in code</li>
      <li class="bulletList">Architecture of an AI orchestrator and the intent functionality</li>
      <li class="bulletList">Creating a GenAI scenario library containing instruction scenarios</li>
      <li class="bulletList">Processing an input with vector search to orchestrate instructions</li>
      <li class="bulletList">Processing an input with a GPT-4o analysis to orchestrate instructions</li>
      <li class="bulletList">Selecting and executing tasks based on the input with the multipurpose orchestrator</li>
    </ul>
    <p class="normal">Let’s begin by defining the architecture of the AI controller.</p>
    <h1 class="heading-1" id="_idParaDest-55"><a id="_idTextAnchor057"/>Architecture of the AI controller</h1>
    <p class="normal">We’ll continue <a id="_idIndexMarker116"/>to implement the architecture of GenAISys as we’ve defined in <em class="italic">Figure 1.1</em> from <a href="Chapter_1.xhtml#_idTextAnchor021"><em class="italic">Chapter 1</em></a>. <em class="italic">Figure 2.1</em>, on the other hand, takes us further into the underlying functions of a GenAISys.</p>
    <figure class="mediaobject"><img alt="Figure 2.1: Defining the functions to build" src="img/B32304_02_1.png"/></figure>
    <p class="packt_figref">Figure 2.1: Defining the functions to build</p>
    <p class="normal">We established in the previous chapter that human roles are essential, and the preceding figure acknowledges that fact. We are the core of a GenAISys, no matter how advanced the building blocks (models or frameworks) are. Our first task is designing using our human creativity to find <a id="_idIndexMarker117"/>effective ways to implement a GenAISys controller. GenAISys needs human creativity, judgment, and technical decision-making. Under the hood of seamless copilots such as ChatGPT, Gemini, and Microsoft Copilot lie intricate layers of AI and non-AI logic. If we want to build our own ChatGPT-like system, we humans need to do the heavy lifting!</p>
    <p class="normal">We will build two separate programs:</p>
    <ul>
      <li class="bulletList">A <strong class="keyWord">conversational agent</strong> implemented <a id="_idIndexMarker118"/>with GPT-4o, which supports both short- and long-term memory. This will help us enforce contextual awareness across multiple exchanges. It aligns with function <strong class="keyWord">F3</strong> in <em class="italic">Figure 2.1</em>.</li>
      <li class="bulletList">An <strong class="keyWord">AI controller orchestrator</strong> that will <a id="_idIndexMarker119"/>also use GPT-4o to analyze the user input, search a library of instructions, augment the input with the appropriate instructions, and run the function(s) in the instructions.</li>
    </ul>
    <p class="normal">In this chapter, we’ll focus on two scenarios: sentiment analysis and semantic (hard science) analysis, which correspond to functions <strong class="keyWord">F1</strong> and <strong class="keyWord">F2</strong> in our architecture. Functions <strong class="keyWord">F4</strong> and <strong class="keyWord">F5</strong> will be added in <a href="Chapter_3.xhtml#_idTextAnchor085"><em class="italic">Chapter 3</em></a>.</p>
    <p class="normal">Although these <a id="_idIndexMarker120"/>examples are built for OpenAI’s API, the logic is model-agnostic. Once you understand how it works, you can adapt the code to use any LLM—such as Meta’s Llama, xAI’s Grok, Google’s Gemini, or Cohere.</p>
    <p class="normal">Once we’ve built the conversational agent and controller orchestrator programs separately, we will merge them into a unified intelligence AI controller, as shown in <em class="italic">Figure 2.2</em>.</p>
    <figure class="mediaobject"><img alt="Figure 2.2: Next steps—integrating the AI controller functions through a Pinecone vector store" src="img/B32304_02_2.png"/></figure>
    <p class="packt_figref">Figure 2.2: Next steps—integrating the AI controller functions through a Pinecone vector store</p>
    <p class="normal">For now, we need to focus on building each component individually so we can fully understand their behavior. Once that foundation is in place, in <a href="Chapter_3.xhtml#_idTextAnchor085"><em class="italic">Chapter 3</em></a>, we will merge them through a Pinecone vector store. Let’s now dive straight down into code and begin developing the conversational agent.</p>
    <h1 class="heading-1" id="_idParaDest-56"><a id="_idTextAnchor058"/>Conversational AI agent</h1>
    <p class="normal">Our two <a id="_idIndexMarker121"/>primary goals for this section are to build a conversational AI agent with the following:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Short-term memory retention</strong> for a full ChatGPT-like conversational loop. The user <a id="_idIndexMarker122"/>and agent can have as many exchanges as they wish; there is no limit to the number of interactions between them.</li>
      <li class="bulletList"><strong class="keyWord">Long-term memory retention</strong> across multiple users and sessions. We’ll store in-memory <a id="_idIndexMarker123"/>sessions and persist them to a memory storage (in this case, a text file). This will enable multi-user contextual awareness for users such as John, Myriam, and Bob. Our conversational agent will move beyond classic one-to-one ChatGPT-style dialogues toward a custom GenAISys capable of handling multi-session, multi-user interactions.</li>
    </ul>
    <p class="normal">To get started, open <code class="inlineCode">Conversational_AI_Agent.ipynb</code> in this chapter’s GitHub directory (<a href="https://github.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/tree/main">https://github.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/tree/main</a>). This notebook will guide you through the environment setup.</p>
    <h2 class="heading-2" id="_idParaDest-57"><a id="_idTextAnchor059"/>Setting up the environment</h2>
    <p class="normal">We’ll reuse <a id="_idIndexMarker124"/>the setup process from the previous chapter. If you need a refresher, feel free to revisit that section. Start by installing OpenAI and downloading the required files:</p>
    <pre class="programlisting code"><code class="hljs-code">!curl -L https://raw.githubusercontent.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/master/commons/grequests.py --output grequests.py
from grequests import download
download("commons","requirements01.py")
download("commons","openai_setup.py")
download("commons","openai_api.py")
</code></pre>
    <p class="normal">We’ll also need to download two additional functions to build our conversational agent:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">download("commons","conversational_agent.py")</code>: This contains the functions to manage a full-turn conversation loop and memorize the dialogue.</li>
      <li class="bulletList"><code class="inlineCode">download("commons", "processing_conversations.py")</code>: This contains tools to load, display, and cleanse past conversations to increase the memory span of the conversational agent across several sessions and users. This custom multisession, multi-user feature goes beyond the scope of standard ChatGPT-like copilots.</li>
    </ul>
    <p class="normal">Let’s now move on to implementing the functions in <code class="inlineCode">conversational_agent.py</code>, which we’ll call throughout our sessions with the conversational AI agent.</p>
    <h2 class="heading-2" id="_idParaDest-58"><a id="_idTextAnchor060"/>Conversational AI agent workflow</h2>
    <p class="normal">The conversation <a id="_idIndexMarker125"/>AI agent contains two main parts:</p>
    <ul>
      <li class="bulletList">Starting the initial conversation to initiate a dialogue with the AI agent</li>
      <li class="bulletList">Running the full-turn conversation loop to continue as many in-memory exchanges as a user wishes with the AI agent. At the end of each session, the dialog is saved so it can be resumed later—by the same user or another.</li>
    </ul>
    <h3 class="heading-3" id="_idParaDest-59"><a id="_idTextAnchor061"/>Starting the initial conversation</h3>
    <p class="normal">The initial <a id="_idIndexMarker126"/>conversation marks the entry point for a new session. It’s handled by the AI controller and illustrated in <em class="italic">Figure 2.3</em>.</p>
    <figure class="mediaobject"><img alt="Figure 2.3: The initial conversation controller" src="img/B32304_02_3.png"/></figure>
    <p class="packt_figref">Figure 2.3: The initial conversation controller</p>
    <p class="normal">We will go through each step of the initial conversation with the generative AI model to understand in detail how a small-scale ChatGPT-like conversational agent works. The 10-step process begins with <em class="italic">Start</em>.</p>
    <h4 class="heading-4">1. Starting the conversation</h4>
    <p class="normal">The <a id="_idIndexMarker127"/>program begins at this entry point through the <code class="inlineCode">run_conversational_agent</code> function in <code class="inlineCode">openai_api.py</code>, which will be called in the notebook by <code class="inlineCode">conversational_agent</code> and its parameters:</p>
    <pre class="programlisting code"><code class="hljs-code"># Start the conversational agent
def run_conversational_agent(
    uinput, mrole, mcontent, user_role, user_name
):
    conversational_agent(uinput, mrole, mcontent, user_role, user_name)
</code></pre>
    <p class="normal">The parameters <a id="_idIndexMarker128"/>the conversational agent will process in this case are the following:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">uinput</code>: Contains the input (user or system), for example, <code class="inlineCode">Where is Hawaii?</code>.</li>
      <li class="bulletList"><code class="inlineCode">mrole</code>: Defines the role of the message. It can be <code class="inlineCode">user</code> or <code class="inlineCode">system</code>. You can also assign other roles that the API will interpret, such as defining the AI’s persona, for example, <code class="inlineCode">You are a geology expert</code>.</li>
      <li class="bulletList"><code class="inlineCode">mcontent</code>: Is what we expect the system to be, for example, <code class="inlineCode">You are a geology expert</code>.</li>
      <li class="bulletList"><code class="inlineCode">user_role</code>: Defines the role of the user, for example, <code class="inlineCode">user</code>.</li>
      <li class="bulletList"><code class="inlineCode">user_name</code>: The name of the user, for example, <code class="inlineCode">John</code>.</li>
    </ul>
    <h4 class="heading-4">2–3. Initializing API variables and the messages object</h4>
    <p class="normal"><code class="inlineCode">messages_obj</code> is initialized <a id="_idIndexMarker129"/>with the parameters of the conversation described in the previous step, <em class="italic">Starting the conversation</em>:</p>
    <pre class="programlisting code"><code class="hljs-code">messages_obj = [{"role": mrole, "content": mcontent}]
</code></pre>
    <p class="normal"><code class="inlineCode">messages_obj</code> is focusing <a id="_idIndexMarker130"/>on the memory of the system. This object will be appended as long as the session lasts with the exchanges with the GPT-4o model. It will be used to log conversations between sessions. The first message contains the role and content for setting up the agent’s context.</p>
    <h4 class="heading-4">4. Printing a welcome message</h4>
    <p class="normal">The system <a id="_idIndexMarker131"/>is now ready to interact with users. The agent first displays a welcome message and explains how to exit the system once the conversation is over:</p>
    <pre class="programlisting code"><code class="hljs-code">print("Welcome to the conversational agent! Type 'q' or 'quit' to end the conversation.")
</code></pre>
    <h4 class="heading-4">5. Handling the initial user input</h4>
    <p class="normal">The user’s initial input is added to <code class="inlineCode">messages_obj</code> to provide the agent with memory and provide <a id="_idIndexMarker132"/>the direction the agent is expected to follow. The initial user input will be sent from the conversational agent:</p>
    <pre class="programlisting code"><code class="hljs-code">if initial_user_input:
    print(f"{user_name}: {initial_user_input}")
    messages_obj.append(
        {"role": user_role, "content": initial_user_input}
    )
</code></pre>
    <h4 class="heading-4">6. Cleansing the initial conversation log</h4>
    <p class="normal"><code class="inlineCode">messages_obj</code> holds the conversation’s history in a structured format. For certain operations within <a id="_idIndexMarker133"/>our application, such as generating a simplified display, creating a consolidated log entry, or preparing input for a text-based function, we need to convert this structured log into a single, continuous string. This makes sure that the data is in the correct format for these specific tasks and helps resolve any potential punctuation or formatting quirks that might arise when combining the different message parts:</p>
    <pre class="programlisting code"><code class="hljs-code">conversation_string = cleanse_conversation_log(messages_obj)
</code></pre>
    <p class="normal">The cleansing function cleans the conversation and returns a string:</p>
    <pre class="programlisting code"><code class="hljs-code">def cleanse_conversation_log(messages_obj):
  conversation_str = " ".join(
        [f"{entry['role']}: {entry['content']}" for entry in messages_obj]
    )
    # Remove problematic punctuations
    return re.sub(r"[^\w\s,.?!:]", "", conversation_str)
</code></pre>
    <h4 class="heading-4">7. Making the initial API call</h4>
    <p class="normal">The cleansed conversation string is sent to the API for processing. The API provides a response based <a id="_idIndexMarker134"/>on the last input and the conversation history. The system now has a memory:</p>
    <pre class="programlisting code"><code class="hljs-code">agent_response = make_openai_api_call(
    input=conversation_string,
    mrole=mrole,
    mcontent=mcontent,
    user_role=user_role
)
</code></pre>
    <h4 class="heading-4">8. Appending the initial API response</h4>
    <p class="normal">The <a id="_idIndexMarker135"/>assistant’s response from the API is processed and appended to <code class="inlineCode">messages_obj</code>. We are continuing to increase the system’s memory and, thus, its contextual awareness:</p>
    <pre class="programlisting code"><code class="hljs-code">messages_obj.append({"role": "assistant", "content": agent_response})
</code></pre>
    <h4 class="heading-4">9. Displaying the initial assistant’s response</h4>
    <p class="normal">The system’s <a id="_idIndexMarker136"/>response is displayed for the user to analyze and decide whether to continue or exit the session:</p>
    <pre class="programlisting code"><code class="hljs-code">print(f"Agent: {agent_response}")
</code></pre>
    <h4 class="heading-4">10. Starting the conversation loop</h4>
    <p class="normal">The <a id="_idIndexMarker137"/>system now enters the conversation loop, where multiple dialogue turns can take place until the user decides to exit the session:</p>
    <pre class="programlisting code"><code class="hljs-code">while True:
    user_input = input(f"{user_name}: ")
    if user_input.lower() in ["q", "quit"]:
        print("Exiting the conversation. Goodbye!")
        break
</code></pre>
    <p class="normal">We are now ready to begin a full-turn conversation loop.</p>
    <h3 class="heading-3" id="_idParaDest-60"><a id="_idTextAnchor062"/>The full-turn conversation loop</h3>
    <p class="normal">The <a id="_idIndexMarker138"/>initial conversation is now initialized. We will enter the full-turn conversation loop starting from <em class="italic">step 11</em> onward, as illustrated in <em class="italic">Figure 2.4</em>.</p>
    <figure class="mediaobject"><img alt="Figure 2.4: The conversation loop starting from step 11" src="img/B32304_02_4.png"/></figure>
    <p class="packt_figref">Figure 2.4: The conversation loop starting from step 11</p>
    <h4 class="heading-4">11. Prompting for the user input</h4>
    <p class="normal">The <a id="_idIndexMarker139"/>conversation continues the initial dialogue and is memorized through the <code class="inlineCode">messages</code> object. The user prompt triggers a full-turn conversation loop. The first step is to enter the user’s name. This custom takes us beyond the standard ChatGPT-like conversational agents that are limited to one user per session. We are initializing a multi-user conversation:</p>
    <pre class="programlisting code"><code class="hljs-code">user_input = input(f"{user_name}: ")
</code></pre>
    <h4 class="heading-4">12. Checking the Exit condition</h4>
    <p class="normal">If <code class="inlineCode">q</code> or <code class="inlineCode">quit</code> is <a id="_idIndexMarker140"/>entered, the session is ended:</p>
    <pre class="programlisting code"><code class="hljs-code">if user_input.lower() in ["q", "quit"]:
    print("Exiting the conversation. Goodbye!")
    break
</code></pre>
    <h4 class="heading-4">13. Appending the user input to the messages object</h4>
    <p class="normal">The <a id="_idIndexMarker141"/>system is now equipped with a memory of a full-turn conversation loop. It uses the generic API format we defined. The user’s input is appended to <code class="inlineCode">messages_obj</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">messages_obj.append({"role": user_role, "content": user_input})
</code></pre>
    <h4 class="heading-4">14. Cleansing the conversation log (loop)</h4>
    <p class="normal">The <a id="_idIndexMarker142"/>updated <code class="inlineCode">messages_obj</code> is cleansed to make sure it complies with the API calls, as in <em class="italic">step 6</em>, <em class="italic">Cleansing the initial conversation log</em>:</p>
    <pre class="programlisting code"><code class="hljs-code">conversation_string = cleanse_conversation_log(messages_obj)
</code></pre>
    <h4 class="heading-4">15. Making the API call in the conversation loop</h4>
    <p class="normal">In <a id="_idIndexMarker143"/>this full-turn conversation loop, the whole conversation is sent to the API. The API will thus return a response based on the context of the whole conversation and the new input:</p>
    <pre class="programlisting code"><code class="hljs-code">agent_response = make_openai_api_call(
    input=conversation_string,
    mrole=mrole,
    mcontent=mcontent,
    user_role=user_role
)
</code></pre>
    <h4 class="heading-4">16. Appending the API response in the conversation loop</h4>
    <p class="normal">The <a id="_idIndexMarker144"/>API’s response is appended to <code class="inlineCode">messages_obj</code> at each conversation turn:</p>
    <pre class="programlisting code"><code class="hljs-code">messages_obj.append({"role": "assistant", "content": agent_response})
</code></pre>
    <h4 class="heading-4">17. Displaying the assistant’s response</h4>
    <p class="normal">The <a id="_idIndexMarker145"/>API response is displayed at each conversation turn in the loop:</p>
    <pre class="programlisting code"><code class="hljs-code">print(f"Agent: {agent_response}")
</code></pre>
    <h4 class="heading-4">18. Exiting and saving the conversation log</h4>
    <p class="normal">When a user exits the loop, the conversation is saved. This feature will replicate a ChatGPT-like <a id="_idIndexMarker146"/>platform that can save dialogue between two sessions with the same user. However, as <a id="_idIndexMarker147"/>we will see in our implementation of a conversational agent in the <em class="italic">Running the conversational agent</em> section, our program will be able to save a multi-user session in a conversation between team members:</p>
    <pre class="programlisting code"><code class="hljs-code">with open("conversation_log.txt", "w") as log_file:
    log_file.write("\n".join([f"{(user_name if entry['role'] == 'user' else entry['role'])}: {entry['content']}" for entry in messages_obj]))
</code></pre>
    <h4 class="heading-4">19. End</h4>
    <p class="normal">The <a id="_idIndexMarker148"/>conversational agent terminates the session after memorizing the conversation:</p>
    <pre class="programlisting code"><code class="hljs-code">print("Conversation saved to 'conversation_log.txt'.")
</code></pre>
    <p class="normal">We have explored the conversational agent’s functionality.</p>
    <p class="normal">Now, let’s move on to the AI conversational agent program that represents an AI controller.</p>
    <h3 class="heading-3" id="_idParaDest-61"><a id="_idTextAnchor063"/>Running the conversational AI agent</h3>
    <p class="normal">The main program, <code class="inlineCode">Conversational_AI_Agent.ipynb</code>, calls the necessary functions from <code class="inlineCode">conversational_agent.py</code> to <a id="_idIndexMarker149"/>handle AI interactions. We will be running a conversation through three user sessions with this scenario:</p>
    <ol>
      <li class="numberedList" value="1">John begins with a short-term memory session with the conversational AI agent.</li>
      <li class="numberedList">John’s conversation will be saved in a log file when the session is over.</li>
      <li class="numberedList">Myriam resumes the session using that same log file.</li>
      <li class="numberedList">Myriam’s conversation will be saved in the same log file as John’s when the session is over.</li>
      <li class="numberedList">Bob will pick up where John and Myriam left off.</li>
      <li class="numberedList">Bob’s conversation will be saved in the same log file as John’s and Myriam’s when the session is over.</li>
    </ol>
    <p class="normal">All three users interact in successive sessions. In <a href="Chapter_3.xhtml#_idTextAnchor085"><em class="italic">Chapter 3</em></a>, we’ll go further by grouping users through <a id="_idIndexMarker150"/>a Pinecone vector store so that multiple users can participate together in a session in real time. For the moment, let’s walk through this multi-user setup step by step and see how the conversational AI agent handles these sessions. Let’s begin with the first step: John’s short-term memory session.</p>
    <h4 class="heading-4">Short-term memory session</h4>
    <p class="normal">The <a id="_idIndexMarker151"/>session begins with the parameters described in <em class="italic">step 1</em>, <em class="italic">Starting the conversation</em>, of the conversational agent:</p>
    <pre class="programlisting code"><code class="hljs-code">uinput = "Hawai is on a geological volcano system. Explain:"
mrole = "system"
mcontent = "You are an expert in geology."
user_role = "user"
</code></pre>
    <p class="normal">We are also adding the name of the user like in a ChatGPT-like session:</p>
    <pre class="programlisting code"><code class="hljs-code">user_name = "John"
</code></pre>
    <p class="normal">This simple addition—<code class="inlineCode">user_name</code>—is what takes our GenAISys beyond standard ChatGPT-like platforms. It allows us to associate memory with specific users and expand into multi-user conversations within a single system.</p>
    <p class="normal">We will now import the first function, the OpenAI API functionality, to make a request to OpenAI’s API, as described in <a href="Chapter_1.xhtml#_idTextAnchor021"><em class="italic">Chapter 1</em></a>:</p>
    <pre class="programlisting code"><code class="hljs-code">from openai_api import make_openai_api_call
</code></pre>
    <p class="normal">The program now imports the second function, the conversational agent, and runs it as described earlier in this section:</p>
    <pre class="programlisting code"><code class="hljs-code">from conversational_agent import run_conversational_agent
run_conversational_agent(uinput, mrole, mcontent, user_role,user_name)
</code></pre>
    <p class="normal">Let’s go through each step of the dialog implemented with our two functions. The agent first welcomes us:</p>
    <pre class="programlisting con"><code class="hljs-con">Welcome to the conversational agent! Type 'q' or 'quit' to end the conversation.
</code></pre>
    <p class="normal">John, the first user, asks for a geological explanation about Hawaii:</p>
    <pre class="programlisting code"><code class="hljs-code">John: Hawai is on a geological volcano system. Explain:
</code></pre>
    <p class="normal">The agent provides a satisfactory answer:</p>
    <pre class="programlisting con"><code class="hljs-con">Agent: Hawaii is part of a geological volcanic system known as a "hotspot"…
</code></pre>
    <p class="normal">John now asks about surfing “there”:</p>
    <pre class="programlisting code"><code class="hljs-code">John: Can we surf there?
</code></pre>
    <p class="normal">Thanks <a id="_idIndexMarker152"/>to the memory we built into the agent, it now has contextual awareness through memory retention. The agent correctly responds about surfing in Hawaii:</p>
    <pre class="programlisting con"><code class="hljs-con">Agent: Yes, you can definitely surf in Hawaii! The Hawaiian Islands are renowned …
</code></pre>
    <p class="normal">John now asks about the best places to stay without mentioning Hawaii:</p>
    <pre class="programlisting code"><code class="hljs-code">John: Where are the best places to stay?
</code></pre>
    <p class="normal">The agent answers correctly using contextual awareness:</p>
    <pre class="programlisting con"><code class="hljs-con">Agent: Hawaii offers a wide range of accommodations …
</code></pre>
    <p class="normal">John then quits the session:</p>
    <pre class="programlisting code"><code class="hljs-code">John: quit
</code></pre>
    <p class="normal">The agent exits the conversation and saves the dialogue in a conversation log:</p>
    <pre class="programlisting con"><code class="hljs-con">Agent:Exiting the conversation. Goodbye!
Conversation saved to 'conversation_log.txt'.
</code></pre>
    <p class="normal">The short-term session ends, but thanks to memory retention via <code class="inlineCode">conversation_log.txt</code>, we can easily pick up from where John left off. We can thus continue the dialogue immediately or at a later time, leveraging memory retention through the <code class="inlineCode">conversation_log.txt</code> file that was automatically generated.</p>
    <h4 class="heading-4">Long-term memory session</h4>
    <p class="normal">The <a id="_idIndexMarker153"/>short-term session is saved. We have three options:</p>
    <ul>
      <li class="bulletList">Stop the program now. In this case, <code class="inlineCode">conversation_log.txt</code> will only contain John’s session, which can be continued or not.</li>
      <li class="bulletList">Decide to initialize a separate <code class="inlineCode">conversation_log.txt</code> for the next user, Myriam.</li>
      <li class="bulletList">Continue with a multi-user session by loading John’s conversation into Myriam’s initial dialog context.</li>
    </ul>
    <p class="normal">The program in this chapter chooses to continue a multi-session, multi-user scenario.</p>
    <p class="normal">The first step to continue the conversation with John is to load and display the conversation log using <a id="_idIndexMarker154"/>the function in <code class="inlineCode">processing_conversations.py</code> that we downloaded in the <em class="italic">Setting up the environment</em> section. We now import and run the function that we need to load and display the conversation log:</p>
    <pre class="programlisting code"><code class="hljs-code">from processing_conversations import load_and_display_conversation_log
conversation_log = load_and_display_conversation_log()
</code></pre>
    <p class="normal">The function is a standard <code class="inlineCode">IPython</code> process using HTML functionality that reads and displays the conversation:</p>
    <pre class="programlisting code"><code class="hljs-code">from IPython.core.display import display, HTML
import re
# Step 1: Load and Display Conversation Log
def load_and_display_conversation_log():
    try:
        with open("conversation_log.txt", "r") as log_file:
            conversation_log = log_file.readlines()
        # Prepare HTML for display
        html_content = "&lt;h3&gt;Loaded Conversation Log&lt;/h3&gt;&lt;table border='1'&gt;"
        for line in conversation_log:
            html_content += f"&lt;tr&gt;&lt;td&gt;{line.strip()}&lt;/td&gt;&lt;/tr&gt;"
        html_content += "&lt;/table&gt;"
        # Display the HTML
        display(HTML(html_content))
        return conversation_log
    except FileNotFoundError:
        print("Error: conversation_log.txt not found. Ensure it exists in the current directory.")
        return []
</code></pre>
    <p class="normal">The output displays each participant in the conversation, beginning with the system’s information, followed by John’s request, and then the GPT-4o assistant’s response at each turn:</p>
    <pre class="programlisting con"><code class="hljs-con">system: You are an expert in geology.
John: Hawai is on a geological volcano system. Explain:
assistant: Hawaii is part of a geological volcanic system…
</code></pre>
    <p class="normal">Before adding <a id="_idIndexMarker155"/>the conversation to the context of the next input, we will clean and prepare it. To achieve this, we successively import <code class="inlineCode">cleanse_conversation_log</code> and import <code class="inlineCode">initialize_uinput</code> from <code class="inlineCode">processing_conversations.py</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">from processing_conversations import cleanse_conversation_log
from processing_conversations import initialize_uinput
</code></pre>
    <p class="normal">Then, we will call the two Python functions that we defined to cleanse and then prepare the new input:</p>
    <pre class="programlisting code"><code class="hljs-code">cleansed_log = cleanse_conversation_log(conversation_log)
nuinput = initialize_uinput(cleansed_log)
</code></pre>
    <p class="normal">The <code class="inlineCode">cleanse</code> function removes <a id="_idIndexMarker156"/>punctuation and potentially problematic characters:</p>
    <pre class="programlisting code"><code class="hljs-code"># Step 2: Clean the conversation log by removing punctuations and special characters
def cleanse_conversation_log(conversation_log):
    cleansed_log = []
    for line in conversation_log:
        # Remove problematic punctuations and special characters
        cleansed_line = re.sub(r"[^\w\s,.?!:]", "", line)
        cleansed_log.append(cleansed_line.strip())
    return " ".join(cleansed_log)  # Combine all lines into a single string
</code></pre>
    <p class="normal">Finally, we initialize the new input:</p>
    <pre class="programlisting code"><code class="hljs-code"># Step 3: Initialize `uinput` with the cleansed conversation log to continue the conversation
def initialize_uinput(cleansed_log):
    if cleansed_log:
        print("\nCleansed conversation log for continuation:")
        print(cleansed_log)
        return cleansed_log  # Use the cleansed log as the new input
    else:
        print("Error: No data available to initialize `uinput`.")
        return ""
</code></pre>
    <p class="normal">The output <a id="_idIndexMarker157"/>confirms that the conversation log has been cleansed:</p>
    <pre class="programlisting con"><code class="hljs-con">Cleansed conversation log for continuation:
system: You are an expert in geology…
</code></pre>
    <p class="normal">Then, the output confirms that <code class="inlineCode">nuinput</code> contains the conversation log for continuation:</p>
    <pre class="programlisting code"><code class="hljs-code"># `nuinput` now contains the cleansed version of the conversation log and can be used
print("\nInitialized `nuinput` for continuation:", nuinput)
</code></pre>
    <h5 class="heading-5">Continuing the previous session</h5>
    <p class="normal">We can now continue the conversation that John began with <code class="inlineCode">nuinput</code> as the memory retention variable for contextual awareness. We will add the context, <code class="inlineCode">nuinput</code>, to Myriam’s request using the message variables as before:</p>
    <pre class="programlisting code"><code class="hljs-code">ninput = nuinput+ "What about surfing in Long Beach"
mrole = "system"
mcontent = "You are an expert in geology."
user_role = "user"
user_name = "Myriam"
</code></pre>
    <p class="normal">The message call contains two key features:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">ninput = nuinput+ [user input]</code>, which shows that the AI controller now has a long-term memory that goes beyond a single session</li>
      <li class="bulletList"><code class="inlineCode">user_name = "Myriam"</code>, which shows the multi-user feature, proving that our custom small-scale ChatGPT-like AI controller has more flexibility than a standard copilot</li>
    </ul>
    <p class="normal">The overall process is the same as with John. Myriam asks a question:</p>
    <pre class="programlisting code"><code class="hljs-code">Myriam: What about surfing in Long Beach
</code></pre>
    <p class="normal">The agent responds:</p>
    <pre class="programlisting con"><code class="hljs-con">Agent:Long Beach, California, offers a different surfing experience compared to Hawai…
</code></pre>
    <p class="normal">Myriam quits:</p>
    <pre class="programlisting code"><code class="hljs-code">Myriam: quit
</code></pre>
    <p class="normal">The agent <a id="_idIndexMarker158"/>confirms that the conversation has ended and is saved to the conversation log:</p>
    <pre class="programlisting con"><code class="hljs-con">Agent:Exiting the conversation. Goodbye!
Conversation saved to 'conversation_log.txt'.
</code></pre>
    <p class="normal">The AI controller now has a log of John’s session and Myriam’s continuation of the session. The controller can take this further and add yet another user to the conversation.</p>
    <h5 class="heading-5">Continuing the long-term multi-user memory</h5>
    <p class="normal">Let’s <a id="_idIndexMarker159"/>add Bob to the mix to continue the conversation. First, display the conversation log again:</p>
    <pre class="programlisting code"><code class="hljs-code"># Run the process
conversation_log = load_and_display_conversation_log()
</code></pre>
    <p class="normal">You’ll see entries for both John and Myriam:</p>
    <pre class="programlisting con"><code class="hljs-con">system: You are an expert in geology.
Myriam: system: You are an expert …
</code></pre>
    <p class="normal">The log is then cleansed and prepared for the next turn of the conversation as previously. <code class="inlineCode">nuinput</code> now contains John and Myriam’s sessions:</p>
    <pre class="programlisting code"><code class="hljs-code">uinput =nuinput+ "Read the whole dialog then choose the best for geology research"
mrole = "system"
mcontent = "You are an expert in geology."
user_role = "user"
user_name = "Bob"
</code></pre>
    <p class="normal">Bob is focused on the geological mission, not leisure:</p>
    <pre class="programlisting code"><code class="hljs-code">Bob:"Read the whole dialog then choose the best for geology research"
</code></pre>
    <p class="normal">The AI agent provides an accurate response:</p>
    <pre class="programlisting con"><code class="hljs-con">Agent: For geology research, the most relevant part of the dialogue is the explanation of Hawaii's geological volcanic system. This section provides detailed insights into the Hawaiian hotspot, mantle plumes, volcanic activity,…
</code></pre>
    <p class="normal">Bob then quits the session:</p>
    <pre class="programlisting con"><code class="hljs-con">Bob: quit
</code></pre>
    <p class="normal">The agent <a id="_idIndexMarker160"/>exits the conversation and saves it in the conversation log:</p>
    <pre class="programlisting con"><code class="hljs-con">Agent:Exiting the conversation. Goodbye!
Conversation saved to 'conversation_log.txt'.
</code></pre>
    <p class="normal">With these three scenarios, we have implemented a conversational agent managed by the AI controller in a multi-user full-turn conversational loop. Let’s examine the next steps for this conversational agent.</p>
    <h2 class="heading-2" id="_idParaDest-62"><a id="_idTextAnchor064"/>Next steps</h2>
    <p class="normal">At this point, we have the basic structure of a conversational agent. We need to integrate it into an AI controller orchestrator. Let’s sum up the work we did for the conversational agent before beginning to build the AI controller orchestrator.</p>
    <figure class="mediaobject"><img alt="Figure 2.5: The cycle of a conversational agent loop" src="img/B32304_02_5.png"/></figure>
    <p class="packt_figref">Figure 2.5: The cycle of a conversational agent loop</p>
    <p class="normal">As illustrated <a id="_idIndexMarker161"/>in the preceding figure, the AI conversation agent does the following:</p>
    <ol>
      <li class="numberedList" value="1">The agent processes the input (system or human user).</li>
      <li class="numberedList">The agent responds.</li>
      <li class="numberedList">The memory retention function is activated.</li>
      <li class="numberedList">The conversation is added to the following input as context.</li>
      <li class="numberedList">The user can quit.</li>
    </ol>
    <p class="normal">However, the entry/exit point is incomplete. We can enter and exit the conversation but cannot call functions to orchestrate tasks such as activating sentiment analysis and semantic analysis. To complete the architecture of the AI controller, we need to begin building the AI controller orchestrator.</p>
    <h1 class="heading-1" id="_idParaDest-63"><a id="_idTextAnchor065"/>AI controller orchestrator</h1>
    <p class="normal">In this section, we will build the first component of our AI controller orchestrator: the ability to select <a id="_idIndexMarker162"/>the right task to perform. We develop this component as a standalone component that we will integrate starting from <a href="Chapter_3.xhtml#_idTextAnchor085"><em class="italic">Chapter 3</em></a>, where we will bridge the conversational agent with the AI controller orchestrator through a Pinecone vector store.</p>
    <p class="normal"><em class="italic">Figure 2.6</em> illustrates the workflow <a id="_idIndexMarker163"/>of the AI controller orchestrator we’ll be developing:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">C1. AI controller entry point input</strong> triggers the process.</li>
      <li class="bulletList"><strong class="keyWord">C2. Analyzes input,</strong> which could be a system or human user prompt.</li>
      <li class="bulletList"><strong class="keyWord">C3. Embeds user input</strong> through GPT-4o’s native functionality.</li>
      <li class="bulletList"><strong class="keyWord">C4. Embeds task scenario</strong> <strong class="keyWord">repository</strong> through GPT-4o’s native functionality.</li>
      <li class="bulletList"><strong class="keyWord">C5. Selects a</strong> <strong class="keyWord">scenario</strong> to execute a task that best matches the input.</li>
      <li class="bulletList"><strong class="keyWord">C6. Executes the scenario</strong> selected by the AI controller orchestrator.</li>
    </ul>
    <figure class="mediaobject"><img alt="Figure 2.6: Workflow of the AI controller orchestrator" src="img/B32304_02_6.png"/></figure>
    <p class="packt_figref">Figure 2.6: Workflow of the AI controller orchestrator</p>
    <p class="normal">We’ll develop this first <a id="_idIndexMarker164"/>component of the AI controller orchestrator with OpenAI’s GPT-4o API and Python. Additionally, since the idea is to leverage the full power of the generative AI model to perform several tasks requested by the AI controller orchestrator, we will thus avoid overloading the orchestrator with additional libraries to focus on the architecture of the GenAISys.</p>
    <p class="normal">In this notebook, GPT-4o will perform three key functions in the program, as shown in <em class="italic">Figure 2.7</em>:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Embedding</strong>: GPT-4o <a id="_idIndexMarker165"/>systematically embeds all the data it receives through a prompt. The input is embedded before going through the layers of the model. In <a href="Chapter_3.xhtml#_idTextAnchor085"><em class="italic">Chapter 3</em></a>, we will take this further by embedding and upserting reusable data such as instruction scenarios into a Pinecone vector store.</li>
      <li class="bulletList"><strong class="keyWord">Similarity search</strong>: GPT-4o <a id="_idIndexMarker166"/>can perform a similarity search with reliable results. GPT-4o doesn’t have a deterministic fixed cosine similarity function. It learns to understand relationships through its complex neural network, mimicking similarity judgments in a much more nuanced, less deterministic way.</li>
      <li class="bulletList"><strong class="keyWord">Task execution</strong>: Once <a id="_idIndexMarker167"/>a scenario is chosen, GPT-4o can execute a number of standard tasks, such as sentiment and semantic analysis.</li>
    </ul>
    <figure class="mediaobject"><img alt="Figure 2.7: Triggering tasks with similarity searches in a list of instructions" src="img/B32304_02_7.png"/></figure>
    <p class="packt_figref">Figure 2.7: Triggering tasks with similarity searches in a list of instructions</p>
    <p class="normal">We have defined the workflow of the orchestrator and the generative AI model’s usage. However, we must examine how a model identifies the task it is expected to perform.</p>
    <h2 class="heading-2" id="_idParaDest-64"><a id="_idTextAnchor066"/>Understanding the intent functionality</h2>
    <p class="normal">No matter how powerful a generative AI model such as GPT-4o is, it cannot guess what a user wants without <a id="_idIndexMarker168"/>a prompt that explicitly expresses <em class="italic">intent</em>. We cannot just say, “The Grand Canyon is a great place to visit in Arizona” and expect the model to guess that we want a sentiment analysis done on our statement. We have to explicitly formulate our intent by entering: “Provide a sentiment analysis of the following text: The Grand Canyon is a great place to visit in Arizona.”</p>
    <p class="normal">To resolve the issue of intent for an AI controller, we have to find a framework for it to orchestrate its <a id="_idIndexMarker169"/>tasks. A good place to start is to study the <strong class="keyWord">Text-to-Text Transfer Transformer </strong>(<strong class="keyWord">T5</strong>), which is a text-to-text model (Raffel et al., 2020). A T5 model uses <em class="italic">task tags</em> or <em class="italic">task-specific prefixes</em> to provide the intent of a prompt to the transformer model. A task tag contains instructions such as summarization, translation, and classification. The model will detect the tag and know what to do, as shown in <em class="italic">Figure 2.8</em>.</p>
    <figure class="mediaobject"><img alt="Figure 2.8: T5 with task tags" src="img/B32304_02_8.png"/></figure>
    <p class="packt_figref">Figure 2.8: T5 with task tags</p>
    <p class="normal">Training a T5 model involves <em class="italic">explicitly</em> adding a task tag when creating an input and then providing <a id="_idIndexMarker170"/>the response. However, OpenAI GPT models learn which task to perform by analyzing billions of sequences of language, not explicit structures, that contain instructions and responses. A generative AI model using GPT-like architectures will thus learn which task to perform <em class="italic">implicitly</em> through the context of the prompt. For example, a well-parsed prompt such as “Provide a sentiment analysis of the following text: The Grand Canyon is a great place to visit in Arizona.” contains enough context for GPT-4o to infer the desired operation—without requiring an explicit tag.</p>
    <p class="normal">Let’s illustrate how a GPT model works by running T5-style examples with GPT-4o’s implicit analysis of which task needs to be performed.</p>
    <h2 class="heading-2" id="_idParaDest-65"><a id="_idTextAnchor067"/>From T5 to GPT models</h2>
    <p class="normal">In this section, we’ll write a program to show how GPT-4o interprets instructions—a capability we’ll <a id="_idIndexMarker171"/>leverage in our orchestrator. The aim is to demonstrate that, although GPT-style models infer intent implicitly, they still need clear instructions.</p>
    <p class="normal">We’ll begin by opening <code class="inlineCode">T52GPT.ipynb</code> in the <code class="inlineCode">Chapter02</code> directory on GitHub. Set up the environment exactly as in the <em class="italic">Setting up the environment</em> subsection of the <em class="italic">Conversational AI agent</em> section, installing only the OpenAI environment:</p>
    <pre class="programlisting code"><code class="hljs-code">download("commons","requirements01.py")
download("commons","openai_setup.py")
download("commons","openai_api.py")
</code></pre>
    <p class="normal">No additional installations are required. Let’s now begin with a CoLA task.</p>
    <h3 class="heading-3" id="_idParaDest-66"><a id="_idTextAnchor068"/>Corpus of Linguistic Acceptability (CoLA)</h3>
    <p class="normal">The <strong class="keyWord">Corpus of Linguistic Acceptability</strong> (<strong class="keyWord">CoLA</strong>) is a public dataset of short English sentences, each tagged <a id="_idIndexMarker172"/>as acceptable (grammatical) or unacceptable (ungrammatical). By testing GPT-4o on these examples, we can show that advanced <a id="_idIndexMarker173"/>generative models can tackle new tasks purely by understanding language, without any task-specific fine-tuning. This means that we can apply advanced generative AI models to a wide range of tasks we didn’t train them for.</p>
    <p class="normal">Let’s first submit the following input to the GPT-4o model to see whether it is acceptable without an explicit task tag:</p>
    <pre class="programlisting code"><code class="hljs-code">input="This aint the right way to talk."
</code></pre>
    <p class="normal">We will provide minimal information to the system:</p>
    <pre class="programlisting code"><code class="hljs-code">mrole = "system"
user_role = "user"
mcontent = "Follow the instructions in the input"
</code></pre>
    <p class="normal">We’ll also make an OpenAI API call with the function we have been using throughout this chapter:</p>
    <pre class="programlisting code"><code class="hljs-code"># API function call
task_response = openai_api.make_openai_api_call(
    input,mrole,mcontent,user_role
)
print(task_response)
</code></pre>
    <p class="normal">The output shows that even one of the most powerful generative AI models doesn’t have a clue about what to do without a task tag:</p>
    <pre class="programlisting con"><code class="hljs-con">I apologize if my previous response didn't meet your expectations. Please let me know how I can assist you better!
</code></pre>
    <p class="normal">Now, let’s write an instruction with a task tag and the same message:</p>
    <pre class="programlisting code"><code class="hljs-code">input="Is the following sentence gramatically correct:This aint the right way to talk."
mrole = "system"
user_role = "user"
mcontent = "Follow the instructions in the input"
# API function call
task_response = openai_api.make_openai_api_call(
    input,mrole,mcontent,user_role
)
print(task_response)
</code></pre>
    <p class="normal">The input <a id="_idIndexMarker174"/>now contains an indication of what is expected of the generative AI model. The output is now accurate:</p>
    <pre class="programlisting con"><code class="hljs-con">The sentence "This aint the right way to talk." is not grammatically correct. The response corrects the sentence:
"This isn't the right way to talk."
Alternatively, if you want to maintain the informal tone, you could write:
"This ain't the right way to talk."
Note that "ain't" is considered informal and nonstandard in formal writing.
</code></pre>
    <p class="normal">Let’s now perform a translation task.</p>
    <h3 class="heading-3" id="_idParaDest-67"><a id="_idTextAnchor069"/>Translation task</h3>
    <p class="normal">The task <a id="_idIndexMarker175"/>begins with a task tag that is expressed in natural language:</p>
    <pre class="programlisting code"><code class="hljs-code">input=”Translate this sentence into French: Paris is quite a city to visit.”
mrole = “system”
user_role = “user”
mcontent = “Follow the instructions in the input”
# API function call
task_response = openai_api.make_openai_api_call(
    input,mrole,mcontent,user_role
)
print(task_response)
</code></pre>
    <p class="normal">The output we get is accurate:</p>
    <pre class="programlisting con"><code class="hljs-con">Paris est vraiment une ville à visiter.
</code></pre>
    <p class="normal">Let’s <a id="_idIndexMarker176"/>now perform a <strong class="keyWord">Semantic Textual Similarity Benchmark</strong> (<strong class="keyWord">STSB</strong>) task.</p>
    <h3 class="heading-3" id="_idParaDest-68"><a id="_idTextAnchor070"/>Semantic Textual Similarity Benchmark (STSB)</h3>
    <p class="normal">STSB-style scoring is an important feature for a GenAISys AI controller, which depends on similarity searches to pick the right instruction scenarios, documents, and other resources. The <a id="_idIndexMarker177"/>orchestrator will rely on this very <a id="_idIndexMarker178"/>capability. In the test that follows, we submit two sentences to the model and ask it to judge their semantic similarity:</p>
    <pre class="programlisting code"><code class="hljs-code">input=”stsb:Sentence 1: This is a big dog. Sentence 2: This dog is very big.”
mrole = “system”
user_role = “user”
mcontent = “Follow the instructions in the input”
# API function call
task_response = openai_api.make_openai_api_call(
    input,mrole,mcontent,user_role)
print(task_response)
</code></pre>
    <p class="normal">The output we get is accurate:</p>
    <pre class="programlisting con"><code class="hljs-con">The sentences "This is a big dog." and "This dog is very big." are semantically similar. Both sentences convey the idea that the dog in question is large in size. The difference in wording does not significantly alter the meaning, as both sentences describe the same characteristic of the dog.
</code></pre>
    <p class="normal">This function will prove to be very useful when we’re searching for data that matches the input in a dataset. Let’s now run a summarization task.</p>
    <h3 class="heading-3" id="_idParaDest-69"><a id="_idTextAnchor071"/>Summarization</h3>
    <p class="normal">In the <a id="_idIndexMarker179"/>following input, GPT-4o can detect the summarization instruction tag and also interpret the maximum length of the response required:</p>
    <pre class="programlisting code"><code class="hljs-code">input="Summarize this text in 10 words maximum: The group walked in the forest on a nice sunny day. The birds were singing and everyone was happy."
mrole = "system"
user_role = "user"
mcontent = "Follow the instructions in the input"
# API function call
task_response = openai_api.make_openai_api_call(
    input,mrole,mcontent,user_role)
print(task_response)
</code></pre>
    <p class="normal">The output is once again accurate:</p>
    <pre class="programlisting con"><code class="hljs-con">Group enjoyed a sunny forest walk with singing birds.
</code></pre>
    <p class="normal">The takeaway <a id="_idIndexMarker180"/>of this exploration is that no matter which generative AI model we implement, it requires task tags to react as we expect. Next, we’ll use this insight to implement semantic textual similarity in our orchestrator for processing task tags.</p>
    <h2 class="heading-2" id="_idParaDest-70"><a id="_idTextAnchor072"/>Implementing the orchestrator for instruction selection</h2>
    <p class="normal">In this section, we will begin building the orchestrator for two instructions based on task tags, as shown in <em class="italic">Figure 2.9</em>: sentiment analysis to determine the sentiment of a sentence and semantic <a id="_idIndexMarker181"/>analysis to analyze the facts in a sentence.</p>
    <p class="normal">We will make the system more complex by asking the generative AI model to find the best task tag scenario (sentiment or semantic analysis) based on the input. In other words, the task tag will not be part of the input. We will use GPT-4o’s semantic textual similarity features to choose the right task tag itself.</p>
    <figure class="mediaobject"><img alt="Figure 2.9: Running tasks with implicit task tags" src="img/B32304_02_9.png"/></figure>
    <p class="packt_figref">Figure 2.9: Running tasks with implicit task tags</p>
    <div><p class="normal"> Eventually, our orchestrator will support any task (see <strong class="keyWord">3. Any Task required</strong> in <em class="italic">Figure 2.9</em>), not just sentiment or semantic analysis.</p>
    </div>
    <p class="normal">Setting up the environment is the same as earlier:</p>
    <pre class="programlisting code"><code class="hljs-code">download("commons","requirements01.py")
download("commons","openai_setup.py")
download("commons","openai_api.py")
</code></pre>
    <p class="normal">No <a id="_idIndexMarker182"/>additional installations are required for the orchestrator. We will begin by implementing an instruction scenario selection.</p>
    <h2 class="heading-2" id="_idParaDest-71"><a id="_idTextAnchor073"/>Selecting a scenario</h2>
    <p class="normal">The core of an AI controller is to decide what to do when it receives an input (system or human user). The selection <a id="_idIndexMarker183"/>of a task opens a world of possible methods that we will explore throughout the book. However, we can classify them into two categories:</p>
    <ul>
      <li class="bulletList">Using an explicit task tag to trigger an instruction. This tag can be a context in a generative AI model and expressed freely in various ways in a prompt.</li>
      <li class="bulletList">The prompt has no task instruction but instead a repository of scenarios from which the AI controller will make decisions based on semantic textual similarity.</li>
    </ul>
    <p class="normal">Here, we’ll explore the second, more proactive approach. We’ll test two prompts with no instructions, no task tag, and no clue as to what is expected of the generative AI model. Although we will implement other, more explicit approaches later with task tags, a GenAISys AI controller orchestrator must be able to be proactive in certain situations.</p>
    <ul>
      <li class="bulletList">The first prompt is an opinion on a movie, implying that a sentiment analysis might interest the user:
        <pre class="programlisting code"><code class="hljs-code">if prompt==1:
    input = "Gladiator II is a great movie although I didn't like some of the scenes. I liked the actors though. Overall I really enjoyed the experience."
</code></pre>
      </li>
      <li class="bulletList">The second prompt is a fact, implying that a semantic analysis might interest the user:
        <pre class="programlisting code"><code class="hljs-code">if prompt==2:
    input = "Generative AI models such as GPT-4o can be built into Generative AI Systems. Provide more information."
</code></pre>
      </li>
    </ul>
    <p class="normal">To provide the AI controller with decision-making capabilities, we will need a repository of instruction scenarios.</p>
    <h3 class="heading-3" id="_idParaDest-72"><a id="_idTextAnchor074"/>Defining task/instruction scenarios</h3>
    <p class="normal">Scenarios are sets of instructions that live in a repository within a GenAISys. While ChatGPT-like models <a id="_idIndexMarker184"/>are trained to process many instructions natively, domain-specific use cases need custom scenarios (we’ll dive into these starting from <a href="Chapter_5.xhtml#_idTextAnchor140"><em class="italic">Chapter 5</em></a>). For example, a GenAISys could receive a message such as <code class="inlineCode">Customer order #9283444 is late</code>. The message could be about a production delay or a delivery delay. By examining the sender’s username and group (production or delivery department), the AI controller can determine the context and, selecting a scenario, take an appropriate decision.</p>
    <div><p class="normal"> In this notebook, the scenarios are stored in memory. In <a href="Chapter_3.xhtml#_idTextAnchor085"><em class="italic">Chapter 3</em></a>, we will organize the storage and retrieval of these instruction sets in Pinecone vector stores.</p>
    </div>
    <p class="normal">In both cases, we begin by creating a repository of structured scenarios (market, sentiment, and semantic analysis):</p>
    <pre class="programlisting code"><code class="hljs-code">scenarios = [
    {
<strong class="hljs-slc">        </strong><strong class="hljs-string-slc">"scenario_number"</strong><strong class="hljs-slc">: </strong><strong class="hljs-number-slc">1</strong><strong class="hljs-slc">,</strong>
        "description": "Market Semantic analysis.You will be provided with a market survey on a give range of products.The term market must be in the user or system input. Your task is provide an analysis."
    },
    {
<strong class="hljs-slc">        </strong><strong class="hljs-string-slc">"scenario_number"</strong><strong class="hljs-slc">: </strong><strong class="hljs-number-slc">2</strong><strong class="hljs-slc">,</strong>
        "description": " Sentiment analysis  Read the content and classify the content as an opinion  If it is not opinion, stop there  If it is an opinion then your task is to perform a sentiment analysis on these statements and provide a score with the label: Analysis score: followed by a numerical value between 0 and 1  with no + or - sign.Add an explanation."
    },
    {
<strong class="hljs-slc">        </strong><strong class="hljs-string-slc">"scenario_number"</strong><strong class="hljs-slc">: </strong><strong class="hljs-number-slc">3</strong><strong class="hljs-slc">,</strong>
        "description": "Semantic analysis.This is not an analysis but a semantic search. Provide more information on the topic."
    }
]
</code></pre>
    <p class="normal">We will also add a dictionary of the same scenarios, containing simple definitions of the scenarios:</p>
    <pre class="programlisting code"><code class="hljs-code"># Original list of dictionaries
scenario_instructions = [
    {
        <strong class="hljs-string-slc">"Market Semantic analysis.You will be provided with a market survey on a give range of products.The term market must be in the user or system input. Your task is provide an analysis."</strong>
    },
    {
        <strong class="hljs-string-slc">"Sentiment analysis  Read the content return a sentiment analysis on this text and provide a score with the label named : Sentiment analysis score followed by a numerical value between 0 and 1  with no + or - sign and  add an explanation to justify the score."</strong>
    },
    {
        <strong class="hljs-string-slc">"Semantic analysis.This is not an analysis but a semantic search. Provide more information on the topic."</strong>
    }
]
</code></pre>
    <p class="normal">We <a id="_idIndexMarker185"/>now extract the strings from the dictionary and store them in a list:</p>
    <pre class="programlisting code"><code class="hljs-code"># Extract the strings from each dictionary
instructions_as_strings = [
    list(entry)[0] for entry in scenario_instructions
]
</code></pre>
    <p class="normal">At this point, our AI controller has everything it needs to recognize intent—matching any incoming prompt to the best-fitting scenario.</p>
    <h3 class="heading-3" id="_idParaDest-73"><a id="_idTextAnchor075"/>Performing intent recognition and scenario selection</h3>
    <p class="normal">We <a id="_idIndexMarker186"/>first define the parameters <a id="_idIndexMarker187"/>of the conversational AI agent just as we did in the <em class="italic">Conversational AI agent</em> section:</p>
    <pre class="programlisting code"><code class="hljs-code"><strong class="hljs-comment-slc"># Define the parameters for the function call</strong>
mrole = "system"
mcontent = "You are an assistant that matches user inputs to predefined scenarios. Select the scenario that best matches the input. Respond with the scenario_number only."
user_role = "user"
</code></pre>
    <p class="normal">The orchestrator’s job is to find the best task for any given input, making the AI controller flexible and adaptive. In some cases, the orchestrator may decide not to apply a scenario and just follow <a id="_idIndexMarker188"/>the user’s <a id="_idIndexMarker189"/>input. In the following example, however, the orchestrator will select a scenario and apply it.</p>
    <p class="normal">We now adjust the input to take the orchestrator’s request into account:</p>
    <pre class="programlisting code"><code class="hljs-code"># Adjust `input` to combine user input with scenarios
selection_input = f"User input: {input}\nScenarios: {scenarios}"
print(selection_input)
</code></pre>
    <p class="normal">GPT-4o will now perform a text semantic similarity search as we ran in the <em class="italic">Semantic Textual Similarity Benchmark (STSB)</em> section. In this case, it doesn’t just perform a plain text comparison, but matches one text (the user input) against a list of texts (our scenario descriptions):</p>
    <pre class="programlisting code"><code class="hljs-code"># Call the function using your standard API call
response = openai_api.make_openai_api_call(
    selection_input, mrole, mcontent, user_role
)
</code></pre>
    <p class="normal">Our user input is as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">User input: Gladiator II is a great movie
</code></pre>
    <p class="normal">Then, the scenario is chosen:</p>
    <pre class="programlisting code"><code class="hljs-code"># Print the response
print("Scenario:",response )
</code></pre>
    <p class="normal">The scenario number is then chosen, stored with the instructions that go with it, and displayed:</p>
    <pre class="programlisting code"><code class="hljs-code">scenario_number=int(response)
instructions=scenario_instructions[scenario_number-1]
print(instructions)
</code></pre>
    <p class="normal">For our <em class="italic">Gladiator II</em> example, the orchestrator correctly picks the sentiment analysis scenario:</p>
    <pre class="programlisting con"><code class="hljs-con">{'Sentiment analysis  Read the content return a sentiment analysis on this text and provide a score with the label named : Sentiment analysis score followed by a numerical value between 0 and 1  with no + or - sign and  add an explanation to justify the score.'}
</code></pre>
    <p class="normal">This <a id="_idIndexMarker190"/>autonomous task-selection <a id="_idIndexMarker191"/>capability—letting GenAISys choose the right analysis without explicit tags—will prove invaluable in real-world deployments (see <a href="Chapter_5.xhtml#_idTextAnchor140"><em class="italic">Chapter 5</em></a>). The program now runs the scenarios with the generative AI agent.</p>
    <h2 class="heading-2" id="_idParaDest-74"><a id="_idTextAnchor076"/>Running scenarios with the generative AI agent</h2>
    <p class="normal">Now <a id="_idIndexMarker192"/>that the AI controller has identified the correct <code class="inlineCode">scenario_number</code>, it’s time to execute the selected task. In this notebook, we’ll walk through that process step by step. We first print the input:</p>
    <pre class="programlisting code"><code class="hljs-code">print(input)
</code></pre>
    <p class="normal">Using the <code class="inlineCode">scenario_number</code> value, we access the scenario description from our <code class="inlineCode">instructions_as_strings</code> list:</p>
    <pre class="programlisting code"><code class="hljs-code"># Accessing by line number (1-based index)
line_number = scenario_number
instruction = instructions_as_strings[line_number - 1]  # Adjusting for 0-based indexing
print(f"Instruction on line {line_number}:\n{instruction}")
mrole = "system"
user_role = "user"
mcontent = instruction
</code></pre>
    <p class="normal">The orchestrator is now ready to run a sentiment analysis.</p>
    <h3 class="heading-3" id="_idParaDest-75"><a id="_idTextAnchor077"/>Sentiment analysis</h3>
    <p class="normal">We append <a id="_idIndexMarker193"/>the description of the scenario to <a id="_idIndexMarker194"/>the original user prompt and send the combined request to GPT-4o:</p>
    <pre class="programlisting con"><code class="hljs-con">Instruction on line 2:
Sentiment analysis  Read the content return a sentiment analysis nalysis on this text and provide a score with the label named : Sentiment analysis score followed by a numerical value between 0 and 1  with no + or - sign and  add an explanation to justify the score.
</code></pre>
    <pre class="programlisting code"><code class="hljs-code"># API function call
sc_input=instruction +" "+ input
print(sc_input)
task_response = openai_api.make_openai_api_call(
    sc_input,mrole,mcontent,user_role
)
print(task_response)
</code></pre>
    <p class="normal">For our <em class="italic">Gladiator II</em> example, the response might look like this:</p>
    <pre class="programlisting con"><code class="hljs-con">Sentiment analysis score 0.75
The text expresses a generally positive sentiment towards the movie "Gladiator II." The use of words like "great movie," "liked the actors," and "really enjoyed the experience" indicates a favorable opinion. However, the mention of not liking some of the scenes introduces a slight negative element. Despite this, the overall enjoyment and positive remarks about the actors and the movie as a whole outweigh the negative aspect, resulting in a sentiment score leaning towards the positive side.
</code></pre>
    <p class="normal">The response <a id="_idIndexMarker195"/>shows that the orchestrator <a id="_idIndexMarker196"/>found a scenario that matches the input and produces an acceptable output. Now, let’s go back, change the prompt, and see whether the orchestrator finds the right scenario.</p>
    <h3 class="heading-3" id="_idParaDest-76"><a id="_idTextAnchor078"/>Semantic analysis</h3>
    <p class="normal">The goal <a id="_idIndexMarker197"/>now is to verify, without changing a single line of code, whether the orchestrator can access another scenario. The orchestrator <a id="_idIndexMarker198"/>will rely on GPT-4o’s native ability to perform semantic text similarity searches.</p>
    <p class="normal">We will now activate prompt 2:</p>
    <pre class="programlisting code"><code class="hljs-code">prompt=2
…
if prompt==2:
    input = "Generative AI models such as GPT-4o can be built into Generative AI Systems. Provide more information."
</code></pre>
    <p class="normal">This input clearly calls for a semantic analysis rather than sentiment analysis. We then reuse the exact same code as our sentiment analysis search:</p>
    <pre class="programlisting code"><code class="hljs-code"># Accessing by line number (1-based index)
line_number = scenario_number
instruction = instructions_as_strings[line_number - 1]  # Adjusting for 0-based indexing
print(f"Instruction on line {line_number}:\n{instruction}")
mrole = "system"
user_role = "user"
mcontent = instruction
</code></pre>
    <p class="normal">The output <a id="_idIndexMarker199"/>shows that the right scenario was found:</p>
    <pre class="programlisting con"><code class="hljs-con">Instruction on line 3:
Semantic analysis.This is not an analysis but a semantic search. Provide more information on the topic.
</code></pre>
    <p class="normal">The task <a id="_idIndexMarker200"/>response is displayed:</p>
    <pre class="programlisting code"><code class="hljs-code">print(task_response)
</code></pre>
    <p class="normal">The output shows that the orchestrator produces a coherent semantic analysis:</p>
    <pre class="programlisting con"><code class="hljs-con">Generative AI models, like GPT-4, are advanced machine learning models designed to generate human-like text based on the input they receive….
</code></pre>
    <p class="normal">This demonstrates that in some cases, the orchestrator will be able to find the right scenarios without task tags. This will prove useful when we tackle more complex workflows, such as advanced production and support.</p>
    <h1 class="heading-1" id="_idParaDest-77"><a id="_idTextAnchor079"/>Summary</h1>
    <p class="normal">The first takeaway from this chapter is the central role of humans in a GenAISys. Human design drove the creation of both our conversational agent and orchestrator. We started developing these two complex components with simply an OpenAI API and Python, yet we <em class="italic">humans</em> designed the initial levels of the AI controller that powers our custom GenAISys. The basic GenAISys rule will always apply: no human roles, no GenAISys. We design AI systems, implement them, maintain them, and evolve them based on ongoing feedback.</p>
    <p class="normal">The second takeaway is how our conversational AI agent goes beyond a small-scale ChatGPT-like structure. We not only built short-term context and memory retention for a full-turn dialogue, but we also added long-term memory across multiple users and multiple topics. Our dialogue included three users (John, Myriam, and Bob) and two topics (geology and surfing). As we progress through the book, we will expand the scope of these multi-user, multi-topic sessions to use cases where team cooperation is essential.</p>
    <p class="normal">The third takeaway concerns our AI controller orchestrator. We gave the orchestrator a small scenario dataset containing custom instructions that we can expand for a domain-specific use case, and then leveraged GPT-4o to both select the appropriate scenario and execute the task itself.</p>
    <p class="normal">At this point, we have a conversational agent and a nascent AI controller orchestrator. When we assemble our AI controller, they will together form a unique multi-user, multi-domain customized GenAISys. To build our multi-user, multi-domain GenAISys AI controller, we will now build a Pinecone vector store in the next chapter.</p>
    <h1 class="heading-1" id="_idParaDest-78"><a id="_idTextAnchor080"/>Questions</h1>
    <ol>
      <li class="numberedList" value="1">A ChatGPT-like GenAISys only needs a generative AI model such as GPT-4o. (True or False)</li>
      <li class="numberedList">A ChatGPT-like GenAISys doesn’t require an AI controller. (True or False)</li>
      <li class="numberedList">Human roles are critical when building an<a id="_idTextAnchor081"/>d running GenAISys. (True or False)</li>
      <li class="numberedList">Generally, not always, a generative AI model such as GPT-4o contains a task tag in one form or the other. (True or False)</li>
      <li class="numberedList">Sometimes, not always, a generative model can find the most probable task to perform without a task tag. (True or False)</li>
      <li class="numberedList">Semantic text similarity cannot be natively performed by GPT-4o. (True or False)</li>
      <li class="numberedList">A full-turn generative AI conversation loop with an OpenAI API AI requires coding. (True or False)</li>
      <li class="numberedList">Long-term memory AI conversation sessions are never necessary. (True or False)</li>
      <li class="numberedList">Summarizing a text can only be done in English by GPT-4o. (True or False)</li>
      <li class="numberedList">An AI controller orchestrator is sentient. (True or False)</li>
    </ol>
    <h1 class="heading-1" id="_idParaDest-79"><a id="_idTextAnchor082"/>References</h1>
    <ul>
      <li class="bulletList">Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., &amp; Liu, P. J. (2020). <em class="italic">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.</em> <a href="https://arxiv.org/abs/1910.10683">https://arxiv.org/abs/1910.10683</a><a href="https://arxiv.org/abs/1910.10683 "/></li>
      <li class="bulletList">Ren, J., Sun, Y., Du, H., Yuan, W., Wang, C., Wang, X., Zhou, Y., Zhu, Z., Wang, F., &amp; Cui, S. (2024). <em class="italic">Generative Semantic Communication: Architectures, Technologies, and Applications.</em> <a href="https://doi.org/10.48550/arXiv.2412.08642 ">https://doi.org/10.48550/arXiv.2412.08642</a></li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-80"><a id="_idTextAnchor083"/>Further reading</h1>
    <ul>
      <li class="bulletList">Koziolek, H., Gruener, S., &amp; Ashiwal, V. (2023). <em class="italic">ChatGPT for PLC/DCS Control Logic Generation.</em> <a href="https://doi.org/10.48550/arXiv.2305.15809">https://doi.org/10.48550/arXiv.2305.15809</a></li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-81"><a id="_idTextAnchor084"/>Subscribe for a Free eBook</h1>
    <p class="normal">New frameworks, evolving architectures, research drops, production breakdowns—<em class="italic">AI_Distilled</em> filters the noise into a weekly briefing for engineers and researchers working hands-on with LLMs and GenAI systems. Subscribe now and receive a free eBook, along with weekly insights that help you stay focused and informed.</p>
    <p class="normal">Subscribe at <a href="https://packt.link/TRO5B">https://packt.link/TRO5B</a> or scan the QR code below.</p>
    <p class="normal"><img alt="" src="img/Newsletter_QR_Code.png"/></p>
  </div>
</body></html>