["```py\n    pip install torch\n    pip install transformers\n    import torch\n    torch) is a powerful deep learning framework that provides dynamic computational graphs, GPU acceleration, and extensive neural network building blocks, making it popular for machine learning research and development. The transformers package, developed by Hugging Face, complements PyTorch by providing a comprehensive library of pre-trained transformer models (such as BER, GPT, and T5) and tools for natural language processing tasks. Together, these packages offer a robust ecosystem in which torch provides the foundational deep learning operations, tensor computations, and automatic differentiation capabilities, while transformers provides high-level abstractions for working with state-of-the-art language models, including functions for tokenization, model fine-tuning, and inference.\n    ```", "```py\n    def calculate_perplexity(model, tokenizer, text):\n        inputs = tokenizer(text, return_tensors=\"pt\")\n        with torch.no_grad():\n        outputs = model(inputs, labels=inputs[\"input_ids\"])\n        return torch.exp(outputs.loss).item()\n    model = GPT4LMHeadModel.from_pretrained(\"GPT4\")\n    tokenizer = GPT4Tokenizer.from_pretrained(\"GPT4\")\n    ```", "```py\n    clean_text = \"The quick brown fox jumps over the lazy dog.\"\n    noisy_text = \"Th3 qu1ck br0wn f0x jumps 0ver th3 l@zy d0g.\"\n    clean_text and noisy_text. clean_text holds a standard English sentence, while noisy_text contains the same sentence with deliberate character substitutions, making it “noisy” or corrupted. The clean_text and noisy_text examples are used to evaluate a language model’s perplexity, where clean_text provides a baseline for ideal text prediction and noisy_text assesses the model’s robustness to real-world data corruption; by comparing the perplexity scores, we determine how well the model handles noisy input and its suitability for applications where text data is not always perfectly formatted.\n    ```", "```py\n    clean_perplexity = calculate_perplexity(model, tokenizer,\n        clean_text)\n    noisy_perplexity = calculate_perplexity(model, tokenizer,\n        noisy_text)\n    print(f\"Clean text perplexity: {clean_perplexity:.2f}\")\n    print(f\"Noisy text perplexity: {noisy_perplexity:.2f}\")\n    ```", "```py\n    import spacy\n    from collections import Counter\n    # Load spaCy model\n    nlp = spacy.load(\"en_core_web_sm\")\n    def analyze_text_quality(text):\n        doc = nlp(text)\n    ```", "```py\n        misspelled = [\n            token.text for token in doc if token._.is_misspelled\n        ]\n    ```", "```py\n    pos_counts = Counter(token.pos_ for token in doc)\n    grammar_score = pos_counts['NOUN'] + pos_counts['VERB'] \n        + pos_counts['ADJ'] + pos_counts['ADV']\n    ```", "```py\n    incomplete_sentences = [\n        sent.text for sent in doc.sents if len(sent) < 3\n    ]\n        return {\n            \"misspelled_words\": misspelled,\n            \"grammar_score\": grammar_score,\n            \"incomplete_sentences\": incomplete_sentences\n        }\n    ```", "```py\n    text = \"This iz a smple txt with sum issues. Incomplet\"\n    quality_report = analyze_text_quality(text)\n    print(quality_report)\n    ```", "```py\n    import unicodedata\n    import re\n    from nltk.tokenize import word_tokenize\n    from nltk.corpus import stopwords\n    import nltk\n    # Download required NLTK data\n    nltk.download('punkt')\n    nltk.download('stopwords')\n    ```", "```py\n    def preprocess_text(text):\n        # Lowercase the text\n        text = text.lower()\n        # Normalize unicode characters\n        text = unicodedata.normalize(\n            'NFKD', text\n        ).encode(\n            'ascii', 'ignore'\n        ).decode('utf-8')\n        # Remove punctuation\n        text = re.sub(r'[^\\w\\s]', '', text)\n        # Normalize whitespace\n        text = ' '.join(text.split())\n        # Tokenize :\n       tokens = word_tokenize(text)\n    ```", "```py\n        stop_words = set(stopwords.words('english'))\n        tokens = [\n            token for token in tokens if token not in stop_words\n        ]\n        # Join tokens back into text\n        preprocessed_text = ' '.join(tokens)\n        return preprocessed_text\n    ```", "```py\n    raw_text = \"This is an EXAMPLE of text preprocessing... It's quite useful!\"\n    cleaned_text = preprocess_text(raw_text)\n    print(f\"Original: {raw_text}\")\n    print(f\"Preprocessed: {cleaned_text}\")\n    ```", "```py\n    from langdetect import detect\n    from unidecode import unidecode\n    from nltk import word_tokenize\n    import nltk\n    # Download required NLTK data\n    nltk.download('punkt')\n    def handle_multilingual_text(text):\n        # Detect language\n        try:\n            lang = detect(text)\n        except:\n            lang = 'unknown'\n        # Transliterate non-ASCII characters\n        transliterated_text = unidecode(text)\n    ```", "```py\n        tokens = word_tokenize(transliterated_text)\n        return {\n            'original': text,\n            'language': lang,\n            'transliterated': transliterated_text,\n            'tokens': tokens\n        }\n    ```", "```py\n    texts = [\n        \"This is English text.\",\n        \"Dies ist deutscher Text.\",\n        \"これは日本語のテキストです。\",\n        \"This is mixed language text avec un peu de français.\"\n    ]\n    for text in texts:\n        result = handle_multilingual_text(text)\n        print(f\"Original: {result['original']}\")\n        print(f\"Detected Language: {result['language']}\")\n        print(f\"Transliterated: {result['transliterated']}\")\n        print(f\"Tokens: {result['tokens']}\\n\")\n    ```", "```py\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn.metrics.pairwise import cosine_similarity\n    def deduplicate_corpus(corpus, similarity_threshold=0.9):\n        # Create TF-IDF vectorizer\n        vectorizer = TfidfVectorizer()\n        tfidf_matrix = vectorizer.fit_transform(corpus)\n        # Compute pairwise similarities\n        similarity_matrix = cosine_similarity(tfidf_matrix)\n    TfidfVectorizer to convert a text corpus into a numerical cosine_similarity to calculate the pairwise similarity between all documents in the corpus, providing a matrix of similarity scores that can be used to identify near-duplicate texts based on a specified threshold.\n    ```", "```py\n        duplicates = set()\n        for i in range(len(corpus)):\n            for j in range(i + 1, len(corpus)):\n                if similarity_matrix[i, j] > similarity_threshold:\n                    duplicates.add(j)\n    ```", "```py\n        deduplicated_corpus = [\n            doc for i, doc in enumerate(corpus) \n            if i not in duplicates\n        ]\n        return deduplicated_corpus\n    ```", "```py\n    corpus = [\n        \"The quick brown fox jumps over the lazy dog.\",\n        \"A fast auburn fox leaps above the sleepy canine.\",\n        \"The quick brown fox jumps over the lazy dog.\",\n        \"An entirely different sentence about cats.\",\n    ]\n    deduplicated = deduplicate_corpus(corpus)\n    print(f\"Original corpus size: {len(corpus)}\")\n    print(f\"Deduplicated corpus size: {len(deduplicated)}\")\n    print(\"Deduplicated corpus:\")\n    for doc in deduplicated:\n        print(f\"- {doc}\")\n    ```", "```py\n    import pandas as pd\n    import re\n    from nltk.corpus import stopwords\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn.metrics.pairwise import cosine_similarity\n    import nltk\n    # Download required NLTK data\n    nltk.download('stopwords')\n    stop_words = set(stopwords.words('english'))\n    class DataCleaningPipeline:\n        def __init__(\n            self, similarity_threshold=0.9, min_length=10,\n            max_length=1000\n        ):\n            self.similarity_threshold = similarity_threshold\n            self.min_length = min_length\n            self.max_length = max_length\n            self.vectorizer = TfidfVectorizer(stop_words='english')\n    DataCleaningPipeline class that encapsulates text preprocessing, length filtering, and near-duplicate removal functionalities. It initializes with configurable parameters such as similarity threshold and text length constraints, leverages NLTK for stop word removal, and employs scikit-learn’s TfidfVectorizer and cosine_similarity to identify and eliminate similar text entries from a pandas DataFrame.\n    ```", "```py\n        def preprocess(self, text):\n            # Basic preprocessing\n            text = text.lower()\n            text = re.sub(r'[^\\w\\s]', '', text)\n            tokens = [\n                word for word in text.split()\n                if word not in stop_words\n            ]\n            return ' '.join(tokens)\n        def filter_by_length(self, df):\n            return df[\n                (df['text'].str.len() >= self.min_length) &\n                (df['text'].str.len() <= self.max_length)\n            ]\n     methods within a class for text processing.\n    ```", "```py\n    def deduplicate(self, df):\n        tfidf_matrix = self.vectorizer.fit_transform(df['text'])\n        similarity_matrix = cosine_similarity(tfidf_matrix)\n\n        duplicates = set()\n        for i in range(len(df)):\n            for j in range(i + 1, len(df)):\n                if similarity_matrix[i, j] > \\\n                        self.similarity_threshold:\n                    duplicates.add(j)\n\n        return df.drop(df.index[list(duplicates)])\n    ```", "```py\n        def clean(self, input_file, output_file):\n            # Read data\n            df = pd.read_csv(input_file)\n            # Preprocess\n            df['text'] = df['text'].apply(self.preprocess)\n            # Filter by length\n            df = self.filter_by_length(df)\n            # Deduplicate\n            df = self.deduplicate(df)\n            # Save cleaned data\n            df.to_csv(output_file, index=False)\n            print(f\"Cleaned data saved to {output_file}\")\n    ```", "```py\n    pipeline = DataCleaningPipeline()\n    pipeline.clean('input_data.csv', 'cleaned_data.csv')\n    ```", "```py\n    def validate_cleaned_data(file_path, sample_size=100):\n        df = pd.read_csv(file_path)\n        # Basic statistics\n        print(f\"Total samples: {len(df)}\")    \n        print(\n            f\"Average text length: \"\n            f\"{df['text'].str.len().mean():.2f}\"\n        )\n\n        print(f\"Unique samples: {df['text'].nunique()}\")\n    ```", "```py\n    short_texts = df[df['text'].str.len() < 10]\n    print(\n        f\"Texts shorter than 10 characters: \"\n        f\"{len(short_texts)}\"\n    )\n    ```", "```py\n        sample = df.sample(n=min(sample_size, len(df)))\n        print(\"\\nSample for manual review:\")\n        print(sample['text'].head())\n        # Check for common issues\n      common_issues = {\n            'special_chars': df['text'].str.contains(\n                r'[^a-zA-Z0-9\\s]'\n            ),\n            'numbers': df['text'].str.contains(r'\\d'),\n            'all_caps': df['text'].str.isupper()\n        }\n        for issue, mask in common_issues.items():\n            print(f\"Samples with {issue}: {mask.sum()}\")\n    ```", "```py\n        model = GPT4LMHeadModel.from_pretrained('GPT4')\n        tokenizer = GPT4Tokenizer.from_pretrained('GPT4')\n        def calculate_perplexity(text):\n            inputs = tokenizer(\n                text, return_tensors='pt', truncation=True, \n                    max_length=1024\n            )\n            with torch.no_grad():\n                outputs = model(inputs, labels=inputs['input_ids'])\n            return torch.exp(outputs.loss).item()\n        sample_perplexities = sample['text'].apply(\n            calculate_perplexity)\n        print(\n            f\"\\nAverage perplexity on sample: \"\n            f\"{sample_perplexities.mean():.2f}\"\n        )\n    ```", "```py\n    validate_cleaned_data('cleaned_data.csv')\n    ```"]