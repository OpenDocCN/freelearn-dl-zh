<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-33"><a id="_idTextAnchor032"/>2</h1>
<h1 id="_idParaDest-34"><a id="_idTextAnchor033"/>Setting Up Your Robot</h1>
<p>This chapter begins with some background on my thoughts on what a robot is, and what robots are made of – a fairly standard list of parts and components. This chapter aims to allow you to duplicate the exercises and use the source code that is found throughout the book. I will describe how I set up my environments for development, what tools I used to <a id="_idIndexMarker123"/>create my code, and how to install the <strong class="bold">Robotic Operating System version 2</strong> (<strong class="bold">ROS 2</strong>). The assembly of Albert, the robot I use for all the examples, is covered in the GitHub repository for this book. There are many other types and configurations of robots that can work with the code in this book with some changes. I’ll try to provide all the shortcuts I can, including a full image of my robot’s SD card, in the Git repo.</p>
<p>In this chapter, we will be covering the following topics:</p>
<ul>
<li>Understanding the anatomy of a robot</li>
<li>Introducing subsumption architecture</li>
<li>A brief introduction to ROS</li>
<li>Software setup: Linux, ROS 2, Jetson Nano, and Arduino</li>
</ul>
<h1 id="_idParaDest-35"><a id="_idTextAnchor034"/>Technical requirements</h1>
<p>To complete the practical exercises in this chapter, you will need the requirements specified in the <em class="italic">Preface</em> at the beginning of this book. The code for this chapter can be found at <a href="https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e/">https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e/</a>.</p>
<h1 id="_idParaDest-36"><a id="_idTextAnchor035"/>Understanding the anatomy of a robot</h1>
<p>A robot is a machine that is capable of carrying out complex actions and behaviors by itself. Most robots <a id="_idIndexMarker124"/>are controlled by a computer or digital programmable device. Some key characteristics of robots are as follows:</p>
<ul>
<li><strong class="bold">Automation</strong>: Robots can operate automatically without direct human input, based on <a id="_idIndexMarker125"/>their programming. This allows them to do repetitive or dangerous tasks consistently.</li>
<li><strong class="bold">Sensors</strong>: Robots use sensors such as cameras, optics, lidar, and pressure sensors to <a id="_idIndexMarker126"/>gather information about their environment so they can navigate and interact. This sensory information is processed to determine what actions the robot should take.</li>
<li><strong class="bold">Programming</strong>: A robot’s <em class="italic">brain</em> consists of an onboard computer or device that runs code <a id="_idIndexMarker127"/>and algorithms that define how it will behave. Robots are programmed by humans to perform desired behaviors.</li>
<li><strong class="bold">Movement</strong>: Most <a id="_idIndexMarker128"/>robots are able to move around to some degree through wheels, legs, propellers, or other locomotion systems. This allows them to travel through environments to perform tasks.</li>
<li><strong class="bold">Interaction</strong>: Advanced <a id="_idIndexMarker129"/>robots can communicate with humans through voice, visual displays, lights, sounds, physical gestures, and more. This allows useful human-robot interaction and work.</li>
<li><strong class="bold">Autonomy</strong>: While <a id="_idIndexMarker130"/>robots are programmed by humans, they have a degree of self-governance and independence in how they meet their objectives. The ability to take action and make decisions without human oversight is their autonomy.</li>
</ul>
<p>In summary, a robot integrates automation, sensing, movement, programming, and autonomy to reliably carry out jobs that may be complex, repetitive, unsafe, or otherwise unsuitable for humans. They come in many shapes and sizes, from industrial robotic arms to social companion robots to autonomous self-driving cars.</p>
<p>There is a fairly standard collection of components and parts that make up the vast majority of robots. Even robots as outwardly different as a self-driving car, the welding robot that built the car, and a Roomba vacuum cleaner have a lot of the same components <a id="_idIndexMarker131"/>or parts. Some will have more, and some will have less, but most mobile robots will have the following categories of parts:</p>
<div><div><img alt="Figure 2.1 – Block diagram of a typical mobile robot" src="img/B19846_02_1.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – Block diagram of a typical mobile robot</p>
<p>Let’s look at these components in greater detail:</p>
<ul>
<li><strong class="bold">Computer</strong>: A unit that runs the programming that controls the robot. This can be a <a id="_idIndexMarker132"/>traditional computer, a microcontroller, a <strong class="bold">single board computer</strong> (<strong class="bold">SBC</strong>) like we have, or some other sort of <a id="_idIndexMarker133"/>processor that sends and receives commands. Robot <a id="_idIndexMarker134"/>arms and some types of industrial robots will use a <strong class="bold">Programmable Logic Controller</strong> (<strong class="bold">PLC</strong>), which is a special type of controller that applies logic (<em class="italic">AND</em>, <em class="italic">OR</em>, <em class="italic">NOT</em>) to various inputs to produce an output. For a computer to send commands to the robot and receive telemetry, we’ll need some sort of sensor interface, such as a USB port, serial <a id="_idIndexMarker135"/>port, <strong class="bold">General Purpose Input/Output</strong> (<strong class="bold">GPIO</strong>) port, or a network interface such as Ethernet or Wi-Fi.</li>
<li><strong class="bold">Control Station</strong> or <strong class="bold">Human/Robot Interface</strong> (<strong class="bold">HRI</strong>): Robots are designed to perform tasks, which requires that the operator must have some means to send and receive <a id="_idIndexMarker136"/>data from the robot and to supervise that the robot is behaving correctly. We will be using a laptop <a id="_idIndexMarker137"/>or desktop computer for this function, and we will talk to the robot via a wireless network. Our control station sends commands to the robot and receives <strong class="bold">telemetry</strong> from the robot in the form of <strong class="bold">data</strong>, <strong class="bold">video</strong>, or <strong class="bold">audio</strong>.</li>
<li><strong class="bold">Radio</strong> or <strong class="bold">Data Link</strong>: Mobile robots such as the one we are designing in this book are <a id="_idIndexMarker138"/>capable of moving and exploring their environment. While it is possible to send commands to a robot over a tether or wire, the preferred way is to use a radio link. The ubiquitous availability <a id="_idIndexMarker139"/>of wireless networks such as Wi-Fi and cellular data services has made creating data links a lot easier. I have had a lot of robot projects where a network link was unavailable or impractical, and a custom radio solution needed to be devised. Other types of radio used in robots include Bluetooth, Zigbee, and various mesh network systems, such as Flutter.</li>
<li><strong class="bold">Motors</strong> or <strong class="bold">Effectors</strong>: Our definition of a robot includes the ability for <strong class="bold">self-propulsion</strong>; that is, the robot is able to move. In order to move, the robot needs a motor <a id="_idIndexMarker140"/>or set of motors. Our robot, Albert, has ten motors, four for driving and six to control the robot arm and hand. Motors <a id="_idIndexMarker141"/>convert electricity into motion. There are many different types, and picking the right motor is a challenge. You must match the torque (how hard the motor can pull), the speed of the motor shaft in revolutions per minute, and voltage. Here are some key factors to consider when selecting a motor for a robot drive system:<ul><li><strong class="bold">Torque</strong>: Consider <a id="_idIndexMarker142"/>the torque required for your robot’s movements and payload handling. More torque allows faster acceleration and the ability to handle heavier loads. If there is insufficient torque, the robot will “bog down” or stall the motor. An electric motor pulls the most current when it is stalled (it is energized but not moving). All that power going nowhere gets turned into heat, which will eventually melt the wires or cause a fire.</li><li><strong class="bold">Speed</strong>: Determine <a id="_idIndexMarker143"/>the speeds your robot needs to operate at. Higher speeds require motors with higher RPM ratings. We only want our robot to go at a modest rate. The toys can’t get away.</li><li><strong class="bold">Duty cycle</strong>: Choose <a id="_idIndexMarker144"/>a motor that can run continuously for the robot’s required duty cycle without overheating. Intermittent duty cycles allow smaller, lighter motors. We will be driving or moving quite a bit – about 50% of the time, but not too fast.</li><li><strong class="bold">Size and weight</strong>: Large, heavy-duty motors provide a lot of power but may <a id="_idIndexMarker145"/>constrain robot design. Consider the full drive system size and weight. Remember the motor also has to move itself.</li><li><strong class="bold">Control</strong>: Brushless DC motors require electronic speed controllers. Stepper motors <a id="_idIndexMarker146"/>allow open-loop position control. Servomotors, such as the ones in the robot’s arm, have integrated encoders and are controlled by a serial interface. The drive motors I used are <a id="_idIndexMarker147"/>brushed motors, which are controlled by varying the voltage, which we control with <strong class="bold">Pulse Width </strong><strong class="bold">Modulation</strong> (<strong class="bold">PWM</strong>).</li><li><strong class="bold">Voltage</strong>: High <a id="_idIndexMarker148"/>voltages allow more power in small motors. Select a voltage that is compatible with other electronics. My battery is 7.2 volts, which matches the motors selected.</li><li><strong class="bold">Noise</strong>: Quiet motors may be required for home/office robots. Brushless, gear-reduced <a id="_idIndexMarker149"/>motors are quiet but expensive. Geared drivetrains are also noisy.</li><li><strong class="bold">Cost</strong>: More <a id="_idIndexMarker150"/>powerful motors cost more. Balance performance needs with budget constraints. Albert’s brushed motors are very inexpensive.</li></ul><p class="list-inset">Some robot motors also feature gearboxes to reduce the motor speed, basically exchanging speed for torque. Albert’s electric motors have reduction gearboxes that let the motor run at a faster speed than the wheels.</p><p class="list-inset">There are many ways to provide motion to a robot. We call these <em class="italic">things that make the robot move</em> <strong class="bold">effectors</strong>. Effectors are only limited by your imagination, and <a id="_idIndexMarker151"/>include <strong class="bold">pneumatics</strong> (things actuated by compressed air), <strong class="bold">hydraulics</strong> (things actuated <a id="_idIndexMarker152"/>by incompressible fluid), <strong class="bold">linear actuators</strong> (things that convert rotary motion into linear <a id="_idIndexMarker153"/>motion), <strong class="bold">revolving joints</strong> or <strong class="bold">revolute joints</strong> (angular <a id="_idIndexMarker154"/>joints like an elbow) and <a id="_idIndexMarker155"/>even exotic <a id="_idIndexMarker156"/>effectors such as <strong class="bold">shape-memory alloy</strong> or <strong class="bold">piezoelectric crystals</strong>, which <a id="_idIndexMarker157"/>change shape when electricity is applied.</p></li>
<li><strong class="bold">Servos</strong>: Some of <a id="_idIndexMarker158"/>the motors in our robot are a <a id="_idIndexMarker159"/>special category of motors called <strong class="bold">servos</strong>. Servo motors feature a feedback mechanism and a control loop, either to maintain a position or a speed. The feedback is provided by some sort of <strong class="bold">sensor</strong>. The servos we are using consist of a small electric motor that drives a gearbox made up of a series of gears that reduce the speed and consequently increase the torque of the motor. The sensor used in our case is a potentiometer (variable resistor) that can measure the angle of the output gear shaft. When we send a command to the servo, it tells the motor to be set to a particular angle. The angle is measured by the sensor, and any difference between the motor’s position and the sensor creates an error signal that moves the motor in the correct direction. You can hear the motor making a lot of noise because the motor turns many times through seven reduction gears to make the arm move. The gearbox lets us get a lot of torque without drawing a lot of current.<p class="list-inset"><em class="italic">Figure 2</em><em class="italic">.2</em> shows how <a id="_idIndexMarker160"/>a servo motor is controlled using <strong class="bold">Pulse Position Modulation</strong> (<strong class="bold">PPM</strong>). To control a servo, you must generate a pulse of a specific width:</p></li>
</ul>
<div><div><img alt="Figure 2.2 – Servo motor control is via PPM signals" src="img/B19846_02_2.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – Servo motor control is via PPM signals</p>
<p class="list-inset">A short <a id="_idIndexMarker161"/>pulse moves the servo to the beginning of its range. A medium pulse (1,500 microseconds) is the center of the servo’s position. A late pulse causes <a id="_idIndexMarker162"/>the servo motor to go to the end of its range. The robot arm I use in this version of my robot has a servo controller that comes with the arm hardware. We will be controlling the robot arm via serial commands to this controller in <a href="B19846_05.xhtml#_idTextAnchor159"><em class="italic">Chapter 5</em></a>.</p>
<ul>
<li><strong class="bold">Motor Controller or Electronic Speed Control</strong>: Motors are not very useful by <a id="_idIndexMarker163"/>themselves – you need the ability <a id="_idIndexMarker164"/>to convert commands from the control computer into motion from the motors. Since motors need more voltage and more current than the control computer (our Jetson Nano) can provide, we need a device to turn small digital signals into large analog voltage and current. This device is called a motor controller. This controller I had <a id="_idIndexMarker165"/>to purchase separately, and is composed of two parts – an Arduino Uno and a motor controller <a id="_idIndexMarker166"/>shield that is attached:</li>
</ul>
<p class="IMG---Figure"> </p>
<div><div><img alt="Figure 2.3 – Motor ﻿controller shield I used for Albert" src="img/B19846_02_3.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – Motor controller shield I used for Albert</p>
<p class="list-inset">As shown in the image, the four motor wires are attached to the lettered connections.</p>
<ul>
<li>Since we <a id="_idIndexMarker167"/>have a tank-drive robot (we steer by running the motors at different speeds, also called <strong class="bold">differential drive</strong>), we also need the motors to be able to run forward or backward. The motor controller takes a special input <a id="_idIndexMarker168"/>signal called a <strong class="bold">Pulse Width Modulation</strong> (<strong class="bold">PWM</strong>). PWM is a repeating signal where the voltage turns on and off. The motor throttle (how fast the motor turns) is proportional to the amount the PWM signal stays in the <em class="italic">ON</em> position.<p class="list-inset">The motor <a id="_idIndexMarker169"/>controller has several<a id="_idIndexMarker170"/> kinds of connections, and has to be wired carefully due to the high voltages and currents provided. This can be done by performing the following steps:</p><ul><li>There are two <strong class="bold">control wire inputs</strong> – one for speed (the PWM signal) and the other is a direction signal. We put the motor in reverse by changing the <strong class="bold">direction signal</strong> – 1 is forward, and 0 is backward.</li><li>The next thing we need is a <strong class="bold">ground</strong> – it is very important that the controller sending the PWM signal (in our case, it is the Ardunio Mega) and the motor control have their ground lines connected.</li><li>Next, the motor controller needs the <strong class="bold">motor voltage</strong> and <strong class="bold">current</strong>, which we get directly from our battery.</li><li>Finally, we connect two wires from each motor to the controller. It is interesting that we don’t care which wire goes to which side of the motor, since we can run both forward and backward. If the motor is turning the wrong way, just switch the two wires. This is the only time you get to say <em class="italic">just reverse the polarity</em> outside of a science fiction movie.</li></ul><p class="list-inset">We will be covering the specific wiring for the example robot – Albert – in the online appendix.</p></li>
<li><strong class="bold">Sensors</strong>: In order <a id="_idIndexMarker171"/>for the robot, which is a machine that can move and react to its environment, to be able to see its surroundings, it needs sensors. Sensors take information from the outside or inside of the robot and convert it into a digital form. If we use a digital <strong class="bold">camera sensor</strong>, it takes <a id="_idIndexMarker172"/>light and turns it into digital pixels (picture elements), recorded as an array of numbers. A <strong class="bold">sonar sensor</strong> measures the <a id="_idIndexMarker173"/>distance to an object, such as a wall, by sending a pulse of energy (sound waves) and listening for the time delay before hearing an echo. Measuring the time delay gives us the distance to an object, since the speed of sound is fairly constant. For our Albert project, the robot has several types of sensors:<ul><li>Our <strong class="bold">primary sensor</strong> is a wide-angle <a id="_idIndexMarker174"/>video camera, which we will use for avoiding <a id="_idIndexMarker175"/>obstacles and detecting objects.</li><li>We will <a id="_idIndexMarker176"/>also use a <strong class="bold">microphone</strong> to listen for sounds <a id="_idIndexMarker177"/>and perform speech recognition.</li><li>We mentioned <a id="_idIndexMarker178"/>servo motors earlier in this list – each servo <a id="_idIndexMarker179"/>motor contains an <strong class="bold">angle sensor</strong> that detects the amount of rotation and allows us to direct the robot arm and hand.</li><li>We have our <strong class="bold">Emergency Stop button</strong>, which is wired to the Arduino, and is a <a id="_idIndexMarker180"/>type of tactile (touch) sensor. When the <a id="_idIndexMarker181"/>button is pressed, a signal is sent that the robot can interpret as a stop command.</li><li>The robot <a id="_idIndexMarker182"/>arm I chose has a handy <strong class="bold">voltage monitor</strong> that we <a id="_idIndexMarker183"/>will use to keep track of the battery life (charge) remaining.</li></ul></li>
</ul>
<p>In the next section, we will discuss robot software architectures, which act as a framework for the autonomy behaviors we will be creating.</p>
<h1 id="_idParaDest-37"><a id="_idTextAnchor036"/>Introducing subsumption architecture</h1>
<p>At this point, I want <a id="_idIndexMarker184"/>to spend a bit of time on the idea behind the <strong class="bold">subsumption architecture</strong>, and point out some specifics of how we will be using this concept in the design of our robot project. Many of you will be familiar with the concept from school or from study, so you can look at my diagram and then move on. For the rest of us, let’s talk a bit about this biologically inspired robot concept.</p>
<p>Subsumption architecture was originally described by Dr. Rodney Brooks, a professor at MIT, who would later help found iRobot Corporation and invent the Baxter robot. Rodney was trying to develop analogs of insect brains in order to understand how to program intelligent robots. Robots before this time (1986) were very much single-threaded machines that pretty much only did one thing at a time. They read sensors, made decisions, and then acted – and only had one goal at any one time. Creatures such as flies or ants have very simple brains but still manage to function in the real world. Brooks reasoned that there were several layers of closed-loop feedback processes going simultaneously.</p>
<p>The basic concept <a id="_idIndexMarker185"/>of subsumption has been around for some time, and it has been adapted, reused, refined, and simplified in the years since it was first introduced. What I am presenting here is my interpretation of how to apply the concept of subsumption to a robot in the context of what we are trying to accomplish.</p>
<p>The first aspect to understand is that we want our robot to act on a series of goals. The robot is not simply reacting to each stimulus in total isolation, but is rather carrying out some sort of goal-oriented behavior. The goal may be to pick up a toy or navigate the room, avoiding obstacles. The paradigm we are creating has the user set goals for the robot and the robot determines how to carry those goals out, even if the goal is simply to move one meter forward.</p>
<p>The problem begins when the robot has to keep more than one goal in mind at a time. The robot is not just driving around, but driving around avoiding obstacles and looking for toys to pick up. How do we arbitrate between different goals, to determine which one has precedence? The answer is found in the following diagram:</p>
<div><div><img alt="Figure 2.4 – An example subsumption architecture" src="img/B19846_02_4.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – An example subsumption architecture</p>
<p>We will divide the robot’s decision-making systems into three layers, each of which has a different level of responsibility and operates on a different time scale.</p>
<p>At the lowest <a id="_idIndexMarker186"/>levels are what we might call the robot’s autonomic nervous system – it contains the robot’s internal health-keeping and monitoring functions. These processes run very fast – 20 times a second or so, or 20 hertz (Hz), and only deal with what is inside the robot. This includes reading internal sensors, checking battery levels, and reading and responding to heartbeat messages. I’ve labeled this level <em class="italic">Take Care </em><em class="italic">of Myself</em>.</p>
<p class="callout-heading">Important note</p>
<p class="callout">What is a <strong class="bold">heartbeat message</strong>? Once a <a id="_idIndexMarker187"/>second, I have the control station send a special heartbeat message to the robot, which has a time tag down to the millisecond, which is the clock time of the host computer. This goes to the control computer and repeats the heartbeat message back to the host. We can see the delay in our message – our command latency – by comparing the time tags. We want to see a less than 25 milliseconds round trip for the heartbeat. If the onboard computer is not working or is locked up, then the time tag won’t come back and we know the robot is having problems.</p>
<p>The next level handles individual tasks, such as driving around or looking for toys. These tasks are short-term and deal with what the sensors can see. The time period for decisions is <a id="_idIndexMarker188"/>in the second range, so these tasks might have 1 or 2 Hz update rates, but slower than the internal checks. I call this level <em class="italic">Complete the Task</em> – you might call it <em class="italic">Drive the Vehicle</em> or <em class="italic">Operate </em><em class="italic">the Payload</em>.</p>
<p>The final and top level is the section devoted to <em class="italic">completing the mission</em>, and it deals with the overall purpose of the robot. This level has the overall state machine for finding toys, picking them up, and then putting them away, which is the mission of this robot. This level also deals with interacting with humans and responding to commands. The top level works on tasks that take minutes, or even hours, to complete.</p>
<p>The rules of the subsumption architecture – and even where it gets its name – have to do with the priority and interaction of the processes in these layers. The rules are as follows (and this is my version):</p>
<ul>
<li>Each layer can only talk to the layers next to it. The top layer talks only to the middle layer, and the bottom layer also talks only to the middle layer. The middle layer can communicate with both the top and the bottom layer.</li>
<li>The layer with the lower level has the highest priority. The lower level has the ability to interrupt or override the commands from higher layers.</li>
</ul>
<p>Think about this for a minute. I’ve given you an example of driving our robot in a room. The lowest level detects obstacles. The middle level is driving the robot in a particular direction, and the top layer is directing the mission. From the top down, the uppermost layer is commanded to <em class="italic">clean up the room</em>, the middle layer is commanded to <em class="italic">drive around</em>, and the bottom layer gets the command <em class="italic">left motor and right motor forward 60% throttle</em>. Now, the bottom level detects an obstacle. It interrupts the <em class="italic">drive around</em> function and overrides the command from the top layer to turn the robot away from the obstacle. Once the obstacle is cleared, the lowest layer returns control to the middle layer for the driving direction.</p>
<p>Another example could be if the lowest layer loses the heartbeat signal, which indicates that something has gone wrong with the software or hardware. The lowest layer causes the motors to halt, overriding any commands from the upper layers. It does not matter what they want; the robot <a id="_idIndexMarker189"/>has a fault and needs to stop. This <strong class="bold">priority inversion</strong> of the lowest layers having the highest priority is the reason we call this a subsumption architecture, since the higher layers subsume – incorporate – the functions of the lower layers to perform their tasks.</p>
<p>The major <a id="_idIndexMarker190"/>benefit of this type of organization is that it keeps procedures clear as to which events, faults, or commands take precedence over others, and prevents the robot from getting stuck in an indecision loop.</p>
<p>Each type of <a id="_idIndexMarker191"/>robot may have different numbers of layers in their architecture. You could even have a <strong class="bold">supervisory layer</strong> that controls a number of other robots and has goals for the robots as a team. The most I have had so far has been five, used in one of my self-driving car projects.</p>
<p>Now let’s take a look at one of the most important concepts you’ll need in this book – ROS.</p>
<h1 id="_idParaDest-38"><a id="_idTextAnchor037"/>A brief introduction to ROS</h1>
<p>OK, before we do all of the work described in the following section to be able to use ROS 2 – the second <a id="_idIndexMarker192"/>version of the Robotic Operating System – let’s answer your questions. What is ROS, and what are its advantages?</p>
<p>The first thing to know is that ROS is not an actual operating system, such as Linux or Windows. Rather it is a middleware layer that serves as a means of connecting different programs to work together to control a robot. It was originally designed to run Willow Garage’s PR2 robot, which was complex indeed. ROS is supported by a very large open source community and is constantly updated.</p>
<p>I used to be a ROS skeptic, and frankly, reading the documentation did not help my first impression that it was cumbersome at best and difficult to use. However, at the insistence of one of my business partners, we started using ROS for a very complex self-guided security guard robot called RAMSEE, designed for Gamma 2 Robotics:</p>
<div><div><img alt="Figure 2.5 – RAMSEE, the security guard robot, designed by the author" src="img/B19846_02_5.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5 – RAMSEE, the security guard robot, designed by the author</p>
<p>I quickly realized that while the initial learning curve with ROS was steep, the payoff was the ability to <a id="_idIndexMarker193"/>create and implement modular, easily portable services that could be developed independently. I did not need to combine everything into one program, or even in one CPU. I could take advantage of my multi-core computers to run independent processes, or even have more than one computer and move things freely from one to the other. RAMSEE has one computer with eight cores and another with four.</p>
<p class="callout-heading">Important note</p>
<p class="callout">ROS can <a id="_idIndexMarker194"/>be described as a <strong class="bold">Modular Open System Software</strong> (<strong class="bold">MOSA</strong>). It provides a standard interface to allow programs to talk to one another through a <em class="italic">Publish-Subscribe</em> paradigm. This means that one program publishes data, making it available to other programs. The programs that need this subscribe to that data and are sent a message whenever new data is available. This lets us develop programs independently and create standardized interfaces between programs. It really makes creating robots much easier and far more flexible.</p>
<p>The other major advantage, and worth all the bother, is that ROS has a very large library of ready-to-go interfaces for sensors, motors, drivers, and effectors, as well as every imaginable type of robot navigation and control tool. For example, we will be using the OAK-D 3D <a id="_idIndexMarker195"/>depth camera, which has a ROS 2 driver available at <a href="https://github.com/luxonis/depthai-ros">https://github.com/luxonis/depthai-ros</a>.</p>
<p>The RViz2 tool provides visualization of all of your sensor data, as well as showing the localization <a id="_idIndexMarker196"/>and navigation process. I greatly appreciated the logging and debugging tools included in ROS. You can log data – anything that <a id="_idIndexMarker197"/>crosses the publish/subscribe interface – to a <strong class="bold">ROSBag</strong> and play it back later to test your code without the robot being attached, which is very useful.</p>
<p>The following illustration below shows the output given by RViz2, showing a map being drawn by one of my robots:</p>
<div><div><img alt="Figure 2.6 – ROS RViz allows you to see what the robot sees, in this case, a map of a warehouse" src="img/B19846_02_6.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6 – ROS RViz allows you to see what the robot sees, in this case, a map of a warehouse</p>
<p>Since this is the second edition of the book, we will be using ROS 2, the new and improved version of ROS. One of the most frustrating things about the old ROS was the use of <strong class="bold">ROSCORE</strong>, a traffic cop that connected all of the parts of the robot via the network. That is now gone, and the various components can find each other via a different sort <a id="_idIndexMarker198"/>of service, called <strong class="bold">Distributed Data Services</strong> (<strong class="bold">DDS</strong>). We will also need to use Python 3 instead of Python 2 for our code since Python 2 has been discontinued and is no longer supported.</p>
<h1 id="_idParaDest-39"><a id="_idTextAnchor038"/>Hardware and software setup</h1>
<p>To match the <a id="_idIndexMarker199"/>examples in this book, and to have access to the same tools that are used in the code samples, you will have to set up three environments:</p>
<ul>
<li><strong class="bold">A laptop or desktop computer</strong>: This will run our control panel, and also be used to <a id="_idIndexMarker200"/>train neural networks. I used a Windows 10 computer with Oracle VirtualBox supporting a virtual machine running Ubuntu 20.04. You may run a computer running Ubuntu or another Linux operating system by itself (without Windows) if you want. Several of the AI packages we will use in the tutorial sections of the book will require Ubuntu to run. We will load ROS 2 on this computer. I will also be using a PlayStation game controller on this computer for teleoperation (remote control) of the robot when we teach the robot how to navigate. I also have ROS 2 for Windows installed, which may obviate running the virtual machine. Either approach will work, since the Python programs we will use for control run in either mode.</li>
<li><strong class="bold">Nvidia Jetson Nano 8GB</strong>: This also runs Ubuntu Linux 20.04 (you can also run <a id="_idIndexMarker201"/>other Linux versions, but you will have to make any adjustments between those versions yourself). The Nano also runs ROS 2. We will cover the additional libraries we need in the following sub-sections.</li>
<li><strong class="bold">Arduino Mega 256</strong>: We need to be able to create code for the Arduino. I’m using the <a id="_idIndexMarker202"/>regular Arduino IDE from the Arduino website. It can be run on Windows or Linux. We will be using the Arduino to control the motors on the robot base and drive it around. It also gives us a lot of expansion to add additional controls, such as an emergency stop button.</li>
</ul>
<h2 id="_idParaDest-40"><a id="_idTextAnchor039"/>Preparing the laptop</h2>
<p>You will <a id="_idIndexMarker203"/>need to install ROS 2 for Windows <a id="_idIndexMarker204"/>for the robot control software to work. To do this, you can follow the instructions provided at <a href="https://docs.ros.org/en/foxy/Installation/Windows-Install-Binary.html">https://docs.ros.org/en/foxy/Installation/Windows-Install-Binary.html</a>.</p>
<p>I also used <strong class="bold">Virtual Network Computing</strong> (<strong class="bold">VNC</strong>) to talk to my Nano from the laptop, which <a id="_idIndexMarker205"/>saves a lot of time and fiddling with cables and keyboards. Otherwise, you would need to connect the Nano to a monitor, keyboard, and mouse to be able to work on your code that is on the robot. I used <strong class="bold">RealVNC</strong>, which can <a id="_idIndexMarker206"/>be found at <a href="https://www.realvnc.com/en/">https://www.realvnc.com/en/</a>. You can <a id="_idIndexMarker207"/>also use <strong class="bold">UltraVNC</strong>, which is free software.</p>
<h3>Installing Python</h3>
<p>The Linux Ubuntu system will come with a default version of Python. I am going to assume that <a id="_idIndexMarker208"/>you are familiar with Python, as we will be using it throughout the book. If you need help with Python, Packt has several fine books on the subject.</p>
<p>Once you log <a id="_idIndexMarker209"/>on to your virtual machine, check which version of Python you have by opening a terminal window and typing <code>python</code> in the command prompt. You should see the Python version, like this:</p>
<pre class="console">
&gt;python
Python 3.8.16 (default, Jan 17 2023, 22:25:28) [MSC v.1916 64 bit (AMD64)]</pre> <p>You can see that I have version 3.8.16 in this case.</p>
<p>We are going to need several add-on libraries that add on to Python and extend its capabilities. The first <a id="_idIndexMarker210"/>thing to check is to see if you have <code>pip</code> installed. This is the <code>pip</code> by typing in the following:</p>
<pre class="console">
pip</pre> <p>If you get the output <code>No command 'pip' found</code>, then you need to install Pip. Enter the following:</p>
<pre class="console">
sudo apt-get install python-pip python-dev build-essential
sudo pip install --upgrade pip</pre> <p>Now we <a id="_idIndexMarker211"/>can install the rest of the packages we need. As a <a id="_idIndexMarker212"/>start, we need the Python math packages <code>numpy</code>, the scientific Python library <code>scipy</code>, and the math plotting library <code>matplotlib</code>. Let’s install them:</p>
<pre class="console">
sudo apt-get install python-numpy python-scipy python-matplotlib python-sympy</pre> <p>I’ll cover the other Python libraries we will use later (OpenCV, scikit-learn, Keras, etc.) as we need them in the appropriate chapters.</p>
<h3>Setting up Nvidia Jetson Nano</h3>
<p>For this setup, we will use an image to run Ubuntu 20.04 on our Jetson Nano, which is required <a id="_idIndexMarker213"/>for ROS 2. One <a id="_idIndexMarker214"/>source for this version is <a href="https://github.com/Qengineering/Jetson-Nano-Ubuntu-20-image">https://github.com/Qengineering/Jetson-Nano-Ubuntu-20-image</a>.</p>
<p>The basic steps, which you can follow in the Git repo, are as follows:</p>
<ol>
<li>The first step is to prepare an SD card with the operating system image on it. I used <strong class="bold">Imager</strong>, but there are several programs available that will do the job. You need an SD card with at least 32 GB of space – and keep in mind you are erasing the SD card in this process. This means that you need a card greater than 32 GB to start with – I used a 64 GB SD card as a 32 GB SD card did not work, contrary to the instructions provided on the website.</li>
<li>Follow the directions with your SD card – the Jetson Nano Ubuntu website (<a href="https://github.com/jetsonhacks/installROS2">https://github.com/jetsonhacks/installROS2</a>) advises us to use a Class 10 memory card with 64 GB of space. Put the SD card in your reader and start up your disk <a id="_idIndexMarker215"/>imager program. Double (and triple) check that you pick the right drive letter – you are erasing the disk in that drive. Select <a id="_idIndexMarker216"/>the disk image you downloaded. Hit the <strong class="bold">Write</strong> button and let the formatter create your disk image on the SD card:</li>
</ol>
<div><div><img alt="Figure 2.7 – Imager program to write out disk images on SD cards" src="img/B19846_02_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.7 – Imager program to write out disk images on SD cards</p>
<ol>
<li value="3">You can follow the usual setup for setting up your language and keyboard, as well as setting up the network. I like to use a static IP address for the robot since we will be using it a lot.</li>
<li>It is always a good idea to set a new user ID and change the default passwords.</li>
</ol>
<p>Now let’s look at how we can install ROS 2.</p>
<h3>Installing ROS 2</h3>
<p>We need <a id="_idIndexMarker217"/>to install ROS 2 on the Jetson Nano. I used the <em class="italic">Foxy</em> version <a id="_idIndexMarker218"/>on my machine. You can follow the instructions at this link: <a href="https://github.com/Razany98/ROS-2-installation-on-Jetson-Nano">https://github.com/Razany98/ROS-2-installation-on-Jetson-Nano</a>.</p>
<p>You will have to set up the sources and point your computer at the ROS 2 repository. To do this, follow these steps:</p>
<ol>
<li>Set <code>locale</code> using <a id="_idIndexMarker219"/>the following code:<pre class="source-code">
<strong class="bold">locale</strong>
<strong class="bold">sudo apt update &amp;&amp; sudo apt install locales</strong>
<strong class="bold">sudo locale-gen en_US en_US.UTF-8</strong>
<strong class="bold">sudo update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8</strong>
<strong class="bold">export LANG=en_US.UTF-8</strong>
<strong class="bold">locale</strong></pre></li> <li>Set up <a id="_idIndexMarker220"/>the source repository to use:<pre class="source-code">
<strong class="bold">apt-cache policy | grep universe or</strong>
<strong class="bold">sudo apt install software-properties-common</strong>
<strong class="bold">sudo add-apt-repository universe</strong>
<strong class="bold">sudo apt update &amp;&amp; sudo apt install curl gnupg2 lsb-release</strong>
<strong class="bold">sudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg</strong>
<strong class="bold">echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(source /etc/os-release &amp;&amp; echo $UBUNTU_CODENAME) main" | sudo tee /etc/apt/sources.list.d/ros2.list &gt; /dev/null</strong></pre></li> <li>Install the ROS packages:<pre class="source-code">
<strong class="bold">sudo apt update</strong>
<strong class="bold">sudo apt upgrade</strong>
<strong class="bold">sudo apt install ros-foxy-desktop</strong>
<strong class="bold">sudo apt install ros-foxy-ros-base</strong></pre></li> <li>Set up the environment:<pre class="source-code">
<strong class="bold">source /opt/ros/foxy/setup.bash</strong></pre></li> <li>Once you <a id="_idIndexMarker221"/>are done, you can check that your <a id="_idIndexMarker222"/>installation comepleted correctly by typing in the following:<pre class="source-code">
<strong class="bold">ros2 topic list</strong>
<strong class="bold">ros2 node list</strong></pre></li> </ol>
<p>Before we proceed, let’s take a look at how exactly ROS works.</p>
<h4>Understanding how ROS works</h4>
<p>You can <a id="_idIndexMarker223"/>think of ROS as a type of <em class="italic">middleware</em> that works to connect different programs together. It provides <strong class="bold">Interprocess Communications</strong> (<strong class="bold">IPC</strong>) between programs so we don’t have to put all of our <a id="_idIndexMarker224"/>functions in one big block of code – we can distribute our robot’s capabilities and develop them independently.</p>
<p>Each individual <a id="_idIndexMarker225"/>part of a ROS robot control system is called a <strong class="bold">node</strong>. A node is a single-purpose programming module. We will have nodes that collect camera images, perform object recognition, or control the robot arm. With ROS, we can isolate these functions and develop and test them independently.</p>
<p>The various <a id="_idIndexMarker226"/>nodes (programs) talk to one another via <code>/image_raw</code>. This standard message type includes data about the image format, as well as the image itself. We also publish camera data on the <code>/camera_info</code> topic, using the <code>sensor_msgs/CameraInfo</code> format, which is described in the following image:</p>
<div><div><img alt="Figure 2.8 – ROS 2 nodes, topics, and message types" src="img/B19846_02_8.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.8 – ROS 2 nodes, topics, and message types</p>
<p>The <code>/camera_info</code> topic has a lot of valuable information about the image, or frame, including the timestamp the data was collected and the frame number. It also provides calibration information to help us understand the geometry of the captured image, which we can use to map pixels to the 3D space around the robot.</p>
<p>There is <a id="_idIndexMarker227"/>generally an existing or ROS standard message format for whatever you need to convey between components. I like to use the generic <code>std_msgs/String</code>) on a topic called <code>RobotCmd</code> to send general commands, such as mode changes, to the robot from a control application.</p>
<p>ROS 2 allows us to set <code>arm_base_lock</code>, define it as a Boolean, and use the following command:</p>
<pre class="console">
ros2 param set /robot_arm arm_base_lock true</pre> <p>This will turn the rotation lock on. Then we can check this setting with this:</p>
<pre class="console">
ros2 param get /robot_arm arm_base_lock</pre> <p>We get the following reply:</p>
<pre class="console">
Boolean value is true</pre> <p>Since our robot will be composed of a number of nodes (programs) that all have to be started together, ROS 2 provides the concept of a <strong class="bold">launch file</strong> that lets us start all of our programs with <a id="_idIndexMarker228"/>one command. In ROS 1, launch files were built in <strong class="bold">YAML</strong> format. YAML stands for <strong class="bold">Yet Another Markup Language</strong>. In ROS 2 we can use YAML, Python, or <strong class="bold">eXtensible Markup Language</strong> (<strong class="bold">XML</strong>) to define <a id="_idIndexMarker229"/>a launch file. I’m used to creating files in YAML <a id="_idIndexMarker230"/>format, so we will stick to that. In our launch file, we can start nodes, change parameters, and create namespaces if we need to launch multiple copies of a node (for instance, if we had three cameras).</p>
<h3>Virtual Network Computing</h3>
<p>One tool <a id="_idIndexMarker231"/>that I have added to my Jetson Nano is <strong class="bold">Virtual Network Computing</strong> (<strong class="bold">VNC</strong>). This utility, if you are not familiar with it, allows you to see and work with the Nano desktop as if you were connected to it using a keyboard, a mouse, and a monitor. Since the Nano is physically installed inside the robot that travels <a id="_idIndexMarker232"/>by itself, attaching a keyboard, mouse, and monitor is not often convenient (or possible). There are many different versions of VNC, which is a standard protocol used amongst many Unix – and non-Unix – operating systems. The one I used is called <strong class="bold">Vino</strong>. You need <a id="_idIndexMarker233"/>two parts: the <strong class="bold">server</strong> and the <strong class="bold">client</strong>. The server runs on the Nano and basically copies all of the pixels appearing on the screen and sends them out to the Ethernet port. The client catches all of this data and displays it to you on another computer. Let’s install the VNC server using the steps on this webpage: <a href="https://developer.nvidia.com/embedded/learn/tutorials/vnc-setup">https://developer.nvidia.com/embedded/learn/tutorials/vnc-setup</a>.</p>
<p>Load the viewer on your Windows PC, or Linux virtual machine, or do like I did, and load VNC on your Apple iPad. You will find the ability to log directly into the robot and use the desktop tools to be very helpful.</p>
<p class="callout-heading">Important note</p>
<p class="callout">In order to get VNC to run on the Nano without a monitor attached, you must set the Nano to automatically log itself on. You can edit the <code>/etc/gdm3/custom.conf</code> file to enable automatic login:</p>
<p class="callout"><code># Enabling </code><code>automatic login</code></p>
<p class="callout"><code>AutomaticLoginEnable=true</code></p>
<p class="callout"><code>AutomaticLogin=[your username]</code></p>
<h3>Setting up the colcon workspace</h3>
<p>We will <a id="_idIndexMarker234"/>need a <code>colcon</code> workspace <a id="_idIndexMarker235"/>on your development machine—laptop or desktop—as well as on the Jetson Nano. Follow the instructions at <a href="https://docs.ros.org/en/foxy/Tutorials/Beginner-Client-Libraries/Colcon-Tutorial.html">https://docs.ros.org/en/foxy/Tutorials/Beginner-Client-Libraries/Colcon-Tutorial.html</a>.</p>
<p>If you are already a user of ROS, then you know what a workspace is, and how it is used to create packages that can be used and deployed as a unit. We are going to keep all of our programs in a package we will call <code>albert</code>.</p>
<h1 id="_idParaDest-41"><a id="_idTextAnchor040"/>Summary</h1>
<p>This chapter covered several important topics. It started with some of the basics of robotics, for readers who needed a bit more background. We talked about common robot parts, such as sensors, computers, and motors/actuators. We discussed the subsumption architecture in more depth and showed how it helps the robot arbitrate between responding to different events and commands. The next section covered the software setup for running the robot, including the offboard development environment and the onboard Jetson Nano computer environments. We set up the ROS and installed the Python tools.</p>
<p>The final section covered ROS 2 and explained what it is and what it does for us. ROS 2 is a middleware layer that lets us build modular components and multiple single-use programs, rather than having to lump everything into one executable. ROS also has logging, visualization, and debugging tools that help our task of designing a complex robot. ROS 2 is also a wonderful repository of additional capabilities that we can add, including sensor drivers, navigation functions, and controls.</p>
<p>In the next chapter, we will discuss how to go from a concept to a working plan for developing complex robot AI-based software using systems engineering practices such as use cases and storyboards.</p>
<h1 id="_idParaDest-42"><a id="_idTextAnchor041"/>Questions</h1>
<ol>
<li>Name three types of robot sensors.</li>
<li>What does the acronym PWM stand for?</li>
<li>What is analog-to-digital conversion? What goes in and what comes out?</li>
<li>Who invented the subsumption architecture?</li>
<li>Compare my diagram of the three-layer subsumption architecture to the Three Laws of Robotics postulated by Isaac Asimov. Is there a correlation? Why is there one, or why not?<p class="list-inset"><strong class="bold">Hint</strong>: Think about how the laws change the behavior of the robot. Which is the lowest level law (from a subsumption perspective)? Which is the highest?</p></li>
<li>Do you think I should have given our robot project – <em class="italic">Albert</em> – a name? Do you name your robots? What about your washing machine? Why not?</li>
<li>What is the importance of the environment variable <code>ROS_ROOT</code>?</li>
</ol>
<h1 id="_idParaDest-43"><a id="_idTextAnchor042"/>Further reading</h1>
<ul>
<li>Scripts to install ROS 2 on Jetson Nano: <a href="https://github.com/jetsonhacks/installROS2">https://github.com/jetsonhacks/installROS2</a></li>
<li>Helpful troubleshooting in case you have problems with your ROS 2 installation can be found at <a href="https://docs.ros.org/en/rolling/How-To-Guides/Installation-Troubleshooting.html">https://docs.ros.org/en/rolling/How-To-Guides/Installation-Troubleshooting.html</a></li>
<li>ROS 2 documentation: <a href="https://docs.ros.org/en/foxy/index.html">https://docs.ros.org/en/foxy/index.html</a></li>
<li>Dr. Rodney Brooks’s paper on the subsumption architecture: <a href="https://people.csail.mit.edu/brooks/papers/AIM-864.pdf">https://people.csail.mit.edu/brooks/papers/AIM-864.pdf</a></li>
</ul>
</div>
</body></html>