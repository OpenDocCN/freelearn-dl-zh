<html><head></head><body>
  <div><h1 class="chapterNumber">8</h1>
    <h1 id="_idParaDest-200" class="chapterTitle">Inference Optimization</h1>
    <p class="normal">Deploying LLMs is challenging due to their significant computational and memory requirements. Efficiently running these models necessitates the use of specialized accelerators, such as GPUs or TPUs, which can parallelize operations and achieve higher throughput. While some tasks, like document generation, can be processed in batches overnight, others require low latency and fast generation, such as code completion. As a result, optimizing the inference process – how these models make predictions based on input data – is critical for many practical applications. This includes reducing the time it takes to generate the first token (latency), increasing the number of tokens generated per second (throughput), and minimizing the memory footprint of LLMs.</p>
    <p class="normal">Indeed, naive deployment approaches lead to poor hardware utilization and underwhelming throughput and latency. Fortunately, a variety of optimization techniques have emerged to dramatically speed up inference. This chapter will explore key methods like speculative decoding, model parallelism, and weight quantization, demonstrating how thoughtful implementations can achieve speedups of 2–4X or more. We will also introduce three popular inference engines (Text Generation Inference, vLLM, and TensorRT-LLM) and compare their features in terms of inference optimization.</p>
    <p class="normal">In this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bulletList">Model optimization strategies</li>
      <li class="bulletList">Model parallelism</li>
      <li class="bulletList">Model quantization</li>
    </ul>
    <p class="normal">By the end of this chapter, you will understand the core challenges in LLM inference and be familiar with state-of-the-art optimization techniques, including model parallelism and weight quantization.</p>
    <div><p class="normal">All the code examples from this chapter can be found on GitHub at <a href="https://github.com/PacktPublishing/LLM-Engineering">https://github.com/PacktPublishing/LLM-Engineering</a>.</p>
    </div>
    <h1 id="_idParaDest-201" class="heading-1">Model optimization strategies</h1>
    <p class="normal">Most <a id="_idIndexMarker749"/>of the LLMs used nowadays, like GPT or Llama, are powered by a decoder-only Transformer architecture. The <em class="italic">decoder-only</em> architecture is designed for text-generation tasks. It predicts the next word in a sequence based on preceding words, making it effective for generating contextually appropriate text continuations.</p>
    <p class="normal">In contrast, an <em class="italic">encoder-only</em> architecture, like BERT, focuses on understanding and representing the input text with detailed embeddings. It excels in tasks that require comprehensive context understanding, such as text classification and named entity recognition. Finally, the encoder-decoder architecture, like T5, combines both functionalities. The encoder processes the input text to generate a context-rich representation, which the decoder then uses to produce the output text. This dual structure is particularly powerful for sequence-to-sequence tasks like translation and summarization, where understanding the input context and generating a relevant output are equally important.</p>
    <p class="normal">In this book, we only focus <a id="_idIndexMarker750"/>on the decoder-only architecture, which dominates the LLM field.</p>
    <figure class="mediaobject"><img src="img/B31105_08_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.1 – Inference process with decoder-only models. We provide “I have a dream” as input and obtain “of” as output.</p>
    <p class="normal">As shown in <em class="italic">Figure 8.1</em>, the basic inference process for a decoder-only model involves:</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">Tokenizing</strong> the<a id="_idIndexMarker751"/> input prompt and passing it through an embedding layer and positional encoding.</li>
      <li class="numberedList"><strong class="keyWord">Computing</strong> key<a id="_idIndexMarker752"/> and value pairs for each input token using the multi-head attention mechanism.</li>
      <li class="numberedList"><strong class="keyWord">Generating</strong> output<a id="_idIndexMarker753"/> tokens sequentially, one at a time, using the computed keys and values.</li>
    </ol>
    <p class="normal">While <em class="italic">Steps 1</em> and <em class="italic">2</em> are computationally expensive, they consist of highly parallelizable matrix multiplication that can achieve high hardware utilization on accelerators like GPUs and TPUs.</p>
    <p class="normal">The real challenge is that the token generation in <em class="italic">Step 3</em> is inherently sequential – to generate the next token, you need to have generated all previous tokens. This leads to an iterative process where the output sequence is grown one token at a time, failing to leverage the parallel computing capabilities of the hardware. Addressing this bottleneck is one of the core focuses of inference optimization.</p>
    <p class="normal">In this section, we will detail several optimization strategies that are commonly used to speed up inference <a id="_idIndexMarker754"/>and reduce <strong class="keyWord">Video Random-Access Memory</strong> (<strong class="keyWord">VRAM</strong>) usage, such as implementing a (static) KV cache, continuous batching, speculative decoding, and optimized attention mechanisms.</p>
    <h2 id="_idParaDest-202" class="heading-2">KV cache</h2>
    <p class="normal">We <a id="_idIndexMarker755"/>saw that LLMs generate text token by token, which is slow because each new prediction depends on the entire previous context. For example, to predict the 100<sup class="superscript">th</sup> token in a sequence, the model needs the context of tokens 1 through 99. When predicting the 101<sup class="superscript">st</sup> token, it again needs the information from tokens 1 through 99, plus token 100. This repeated computation is particularly inefficient.</p>
    <p class="normal">The <strong class="keyWord">key-value</strong> (<strong class="keyWord">KV</strong>) cache<a id="_idIndexMarker756"/> addresses this issue by storing key-value pairs produced by self-attention layers. Instead of recalculating these pairs for each new token, the model retrieves them from the cache, significantly speeding up the generation. </p>
    <p class="normal">You can see an illustration of this technique in <em class="italic">Figure 8.2</em>:</p>
    <figure class="mediaobject"><img src="img/B31105_08_02.png" alt="An illustration of KV caching depicted in Prefill and Decode phases. Prefill is a highly parallelized operation where the KV tensors of all input tokens can be computed simultaneously. During decode, new KV tensors and subsequently the output token at each step is computed autoregressively. "/></figure>
    <p class="packt_figref">Figure 8.2 – Illustration of the KV cache</p>
    <p class="normal">When a new token is generated, only the key and value for that single token need to be computed and added to the cache. The KV cache is an immediate optimization that is implemented in every popular tool and library. Some implementations maintain a separate KV cache for each layer of the model.</p>
    <p class="normal">The size of the KV cache scales with the number of tokens (<img src="img/B31105_08_001.png" alt=""/>) and several model dimensions, like the number of layers (<img src="img/B31105_08_002.png" alt=""/>), the number of attention heads (<img src="img/B31105_08_003.png" alt=""/>), their dimension (<img src="img/B31105_08_004.png" alt=""/>), and the precision of the parameters in bytes (<img src="img/B31105_08_005.png" alt=""/>):</p>
    <p class="center"><img src="img/B31105_08_006.png" alt=""/></p>
    <p class="normal">For a<a id="_idIndexMarker757"/> typical 7B parameter model using 16-bit precision, this exceeds 2 GB for high sequence lengths (higher than 2,048 tokens). Larger models with more layers and higher embedding dimensions will see even greater memory requirements.</p>
    <p class="normal">Since<a id="_idIndexMarker758"/> the KV cache grows with each generation step and is dynamic, it prevents you from taking advantage of torch.compile, a powerful optimization tool that fuses PyTorch code into fast and optimized kernels. The <em class="italic">static KV cache</em> solves this issue by pre-allocating the KV cache size to a maximum value, which allows you to combine it with <code class="inlineCode">torch.compile</code> for up to a 4x speedup in the forward pass.</p>
    <p class="normal">To configure a model to use a static KV cache with the transformers library, follow these steps:</p>
    <ol>
      <li class="numberedList" value="1">We import the tokenizer and the model we want to optimize:
        <pre class="programlisting code-one"><code class="hljs-code">import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
model_id = "google/gemma-2b-it"
tokenizer = AutoTokenizer.from_pretrained(model_id) 
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto")
</code></pre>
      </li>
      <li class="numberedList">To implement the static cache, we change the cache implementation in the model’s generation config to <code class="inlineCode">static</code>:
        <pre class="programlisting code-one"><code class="hljs-code">model.generation_config.cache_implementation = "static"
</code></pre>
      </li>
      <li class="numberedList">Now that our KV cache is static, we can compile the model using torch.compile:
        <pre class="programlisting code-one"><code class="hljs-code">compiled_model = torch.compile(model, mode="reduce-overhead", fullgraph=True)
</code></pre>
      </li>
      <li class="numberedList">We tokenize an input question, “<code class="inlineCode">What is 2+2?</code>", and store it on a GPU if available (if not, we store it on the CPU):
        <pre class="programlisting code-one"><code class="hljs-code">device = "cuda" if torch.cuda.is_available() else "cpu"
inputs = tokenizer("What is 2+2?", return_tensors="pt").to(device)
</code></pre>
      </li>
      <li class="numberedList">Let’s <a id="_idIndexMarker759"/>use the <code class="inlineCode">generate()</code> method to get the model’s output and decode it with <code class="inlineCode">batch_decode()</code> to print its answer:
        <pre class="programlisting code-one"><code class="hljs-code">outputs = model.generate(**inputs, do_sample=True, temperature=0.7, max_new_tokens=64)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
<strong class="hljs-slc">[</strong><strong class="hljs-string-slc">'What is 2+2?\n\nThe answer is 4. 2+2 = 4.'</strong><strong class="hljs-slc">]</strong>
</code></pre>
      </li>
    </ol>
    <p class="normal">This returns a <a id="_idIndexMarker760"/>list containing both the input and output, correctly answering our question.</p>
    <div><p class="normal">Note that the static cache doesn’t work with all architectures. For details on which architectures are supported, check out the transformers documentation.</p>
    </div>
    <p class="normal">Efficiently managing the KV cache is essential, as it can quickly exhaust available GPU memory and limit the batch sizes that can be processed. This has motivated the development of memory-efficient attention mechanisms and other techniques, which we will cover in the last section.</p>
    <h2 id="_idParaDest-203" class="heading-2">Continuous batching</h2>
    <p class="normal">Batching, or <a id="_idIndexMarker761"/>processing multiple<a id="_idIndexMarker762"/> inference requests simultaneously, is a standard approach to achieve high throughput. Larger batch sizes spread out the memory cost of model weights and transfer more data to the GPU at once, better saturating its parallel compute capacity.</p>
    <p class="normal">However, decoder-only models pose a particular challenge due to the high variability in input <a id="_idIndexMarker763"/>prompt lengths and desired output lengths. Some requests may have short prompts and only need a one-word answer, while others may input a lengthy context and expect a multi-paragraph response.</p>
    <p class="normal">With <a id="_idIndexMarker764"/>traditional batching, we would have to wait for the longest request in a batch to complete before starting a new batch. This leads to under-utilization as the accelerator sits partly idle waiting for a straggling request to finish. <em class="italic">Continuous batching</em>, also known as <code class="inlineCode">in-flight</code> batching, aims to prevent idle time by immediately feeding a new request into the batch as soon as one completes.</p>
    <p class="normal">The batching process begins the same – by filling the batch with initial requests. But as soon as a request completes its generation, it is evicted from the batch and a new request takes its place. This way, the accelerator is always processing a full batch, leading to maximally efficient hardware utilization. An additional consideration is the need to periodically pause the generation process to run prefill, or the embedding and encoding of waiting requests. Finding the optimal balance between generation and prefill requires some tuning of the waiting-served ratio hyperparameter.</p>
    <p class="normal">Continuous batching is natively<a id="_idIndexMarker765"/> implemented in most inference frameworks, like Hugging Face’s <strong class="keyWord">Text Generation Inference</strong> (<strong class="keyWord">TGI</strong>), vLLM, and NVIDIA TensorRT-LLM.</p>
    <h2 id="_idParaDest-204" class="heading-2">Speculative decoding</h2>
    <p class="normal">Another <a id="_idIndexMarker766"/>powerful optimization <a id="_idIndexMarker767"/>technique is <em class="italic">speculative decoding</em>, also called assisted generation. The key insight is that even with continuous batching, the token-by-token generation process fails to fully saturate the parallel processing capabilities of the accelerator. Speculative decoding aims to use this spare <a id="_idIndexMarker768"/>compute capacity to predict multiple tokens simultaneously, using a smaller proxy model (see <em class="italic">Figure 8.3</em>).</p>
    <figure class="mediaobject"><img src="img/B31105_08_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.3 – Illustration of traditional decoding (left) and speculative decoding (right)</p>
    <p class="normal">The <a id="_idIndexMarker769"/>general approach is:</p>
    <ul>
      <li class="bulletList">Apply a smaller model, like a distilled or pruned version of the main model, to predict multiple token completions in parallel. This could be 5–10 tokens predicted in a single step.</li>
      <li class="bulletList">Feed these speculative completions into the full model to validate which predictions match what the large model would have generated.</li>
      <li class="bulletList">Retain the longest matching prefix from the speculative completions and discard any incorrect tokens.</li>
    </ul>
    <p class="normal">The result is that, if the small model approximates the large model well, multiple tokens can be generated in a single step. This avoids running the expensive large model for several iterations. The degree of speedup depends on the quality of the small model’s predictions – a 90% match could result in a 3–4X speedup.</p>
    <p class="normal">It is crucial that both models use the same tokenizer. If this is not the case, the tokens generated by the draft model will not align with those produced by the large model, making them incompatible. Let’s implement this using the transformers library. In this example, we will use two Qwen1.5 models from Alibaba Cloud: a 1.8B version as the main model, and a 0.5B version as the draft model. Note that, if you have enough VRAM, you can use much larger models like 14B, 32B, 72B, or 110B as <a id="_idIndexMarker770"/>the main model. </p>
    <p class="normal">Here, we’re limited by the VRAM of the T4 GPU in Google Colab, but to get the maximum speedup, the assistant model should be much smaller than the large model.</p>
    <p class="normal">Here’s a<a id="_idIndexMarker771"/> step-by-step guide to implement speculative decoding:</p>
    <ol>
      <li class="numberedList" value="1">We load the tokenizer and both models:
        <pre class="programlisting code-one"><code class="hljs-code">import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
model_id = "Qwen/Qwen1.5-1.8B-Chat"
tokenizer = AutoTokenizer.from_pretrained(model_id) 
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto")
draft_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen1.5-0.5B-Chat", device_map="auto")
</code></pre>
      </li>
      <li class="numberedList">We then tokenize the same input and store it in the accelerator, if available:
        <pre class="programlisting code-one"><code class="hljs-code">device = "cuda" if torch.cuda.is_available() else "cpu"
inputs = tokenizer("What is 2+2?", return_tensors="pt").to(device)
</code></pre>
      </li>
      <li class="numberedList">We can now use <code class="inlineCode">model.generate()</code> with the argument <code class="inlineCode">assistant_model</code> to enable speculative decoding:
        <pre class="programlisting code-one"><code class="hljs-code">outputs = model.generate(**inputs, do_sample=True, assistant_model=draft_model, temperature=0.7, max_new_tokens=64)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
<strong class="hljs-slc">[</strong><strong class="hljs-string-slc">'What is 2+2? 2 + 2 equals 4!'</strong><strong class="hljs-slc">]</strong>
</code></pre>
      </li>
    </ol>
    <p class="normal">The speedup in this small example is not significant, but it is clearly noticeable with bigger models.</p>
    <p class="normal"><em class="italic">Prompt lookup decoding</em> is a variant of speculative decoding, tailored to input-grounded tasks like summarization where there is often overlap between the prompt and output. Shared n-grams are used as the LLM candidate tokens. We can enable prompt lookup decoding by using the <code class="inlineCode">prompt_lookup_num_tokens</code> parameter in <code class="inlineCode">model.generate()</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">outputs = model.generate(**inputs, prompt_lookup_num_tokens=4)
</code></pre>
    <p class="normal">By <a id="_idIndexMarker772"/>combining <a id="_idIndexMarker773"/>the static KV cache with torch.compile, implementing continuous batching, and leveraging speculative decoding techniques, LLMs can see inference speedups of 2–4x or more with no loss in quality.</p>
    <p class="normal">Another approach to creating a small proxy model consists of jointly fine-tuning a small model alongside a large model for maximum fidelity. A representative technique here is Medusa, which inserts dedicated speculation heads into the main model. The Medusa-1 approach fine-tunes these speculation heads while freezing the large model, while the Medusa-2 approach jointly fine-tunes both the speculation heads and the large model. The Medusa method has demonstrated impressive results, enabling a 70M parameter model to closely approximate the performance of a 7B parameter model on a range of tasks. Speculative decoding is natively supported by TGI.</p>
    <h2 id="_idParaDest-205" class="heading-2">Optimized attention mechanisms</h2>
    <p class="normal">The<a id="_idIndexMarker774"/> Transformer <a id="_idIndexMarker775"/>architecture is based on the attention mechanism, which scales quadratically with the number of input tokens (or sequence length). This is particularly inefficient for longer sequences, where the size of the KV cache can blow up.</p>
    <p class="normal">Introduced by Kwon, Li, et al. (2023), <em class="italic">PagedAttention</em> addresses these memory challenges by drawing inspiration from virtual memory and paging in operating systems. It partitions the KV cache into blocks, eliminating the need for contiguous memory allocation. Each <a id="_idIndexMarker776"/>block contains the keys and values for a fixed number of tokens. During attention computation, the PagedAttention kernel efficiently fetches these blocks, regardless of their physical memory location.</p>
    <p class="normal">This <a id="_idIndexMarker777"/>partitioning allows for near-optimal memory utilization. This is useful for batching more sequences together, which increases throughput and GPU utilization. Moreover, <code class="inlineCode">PagedAttention</code>'s block-based approach naturally supports memory sharing across multiple output sequences generated from the same prompt. This is particularly advantageous in parallel sampling and beam search, where the same prompt is used to generate multiple outputs. The shared memory blocks reduce redundant computations and memory usage, cutting the memory overhead by up to 55% and improving throughput by up to 2.2x, according to the authors. The vLLM library received the first implementation of PagedAttention. Since then, PagedAttention has also been implemented in TGI and TensorRT-LLM.</p>
    <p class="normal">Another popular option is <em class="italic">FlashAttention-2</em>. Developed by Tri Dao (2023), it introduced several key innovations that are designed to address the quadratic runtime and memory constraints in traditional attention. By dividing input and output matrices into smaller blocks, FlashAttention-2 ensures that these blocks can fit into the GPU’s on-chip SRAM, which is much faster than high-bandwidth memory. This approach significantly reduces the frequency of data transfers between the GPU’s main memory and its processing units. </p>
    <p class="normal">This is combined with online softmax, which computes the softmax function independently for each block of the attention scores matrix, rather than for the entire matrix at once. By maintaining a running maximum and a running sum of exponentials, FlashAttention-2 can calculate attention probabilities without needing to store large intermediate matrices.</p>
    <p class="normal">Additionally, FlashAttention-2’s online softmax computation enables block-wise processing, maintaining accuracy while significantly reducing memory requirements. This is<a id="_idIndexMarker778"/> particularly important for training, where the recomputation of intermediate values (instead of storing them) in the backward pass reduces memory usage from quadratic to linear, in relation to sequence length.</p>
    <p class="normal">Unlike PagedAttention, FlashAttention-2 can easily be used with the transformers library through the <code class="inlineCode">attn_implementation</code> parameter:</p>
    <ol>
      <li class="numberedList" value="1">Install the <code class="inlineCode">flash-attn</code> library with <code class="inlineCode">--no-build-isolation</code> so that we don’t install the dependencies:
        <pre class="programlisting con-one"><code class="hljs-con">pip install flash-attn --no-build-isolation
</code></pre>
      </li>
      <li class="numberedList">To use FlashAttention-2 for inference, specify <code class="inlineCode">flash_attention_2</code> in the <code class="inlineCode">attn_implementation</code> parameter when loading a model. For example, this is how to load Mistral-7B-Instruct-v0.3 with FlashAttention-2:
        <pre class="programlisting code-one"><code class="hljs-code">from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-Instruct-v0.3",
    attn_implementation="flash_attention_2",
)
</code></pre>
      </li>
    </ol>
    <p class="normal">The <a id="_idIndexMarker779"/>techniques presented in this section focused on improving the model’s efficiency in processing tokens. In the next section, we will discuss how to distribute our model and calculations across multiple GPUs.</p>
    <h1 id="_idParaDest-206" class="heading-1">Model parallelism</h1>
    <p class="normal">Model parallelism<a id="_idIndexMarker780"/> allows you to distribute the memory and compute requirements of LLMs across multiple GPUs. This enables the training and inference of models too large to fit on a single device, while also improving performance in terms of throughput (tokens per second).</p>
    <p class="normal">There are three main approaches to model parallelism, each involving splitting the model weights and computation in different ways: <em class="italic">data parallelism</em>, <em class="italic">pipeline parallelism</em>, and <em class="italic">tensor parallelism</em>. </p>
    <p class="normal">Although these approaches were originally developed for training, we can reuse them for inference by focusing on the forward pass only.</p>
    <h2 id="_idParaDest-207" class="heading-2">Data parallelism</h2>
    <p class="normal"><strong class="keyWord">Data parallelism</strong> (<strong class="keyWord">DP</strong>) is the <a id="_idIndexMarker781"/>simplest type<a id="_idIndexMarker782"/> of model parallelism. It involves making copies of the model and distributing these replicas across different GPUs (see <em class="italic">Figure 8.4</em>). Each GPU processes a subset of the data simultaneously. During training, the gradients calculated on each GPU are averaged and used to update the model parameters, ensuring that each replica remains synchronized. This approach is particularly beneficial when the batch size is too large to fit into a single machine or when aiming to speed up the training process.</p>
    <figure class="mediaobject"><img src="img/B31105_08_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.4 – Illustration of data parallelism with four GPUs</p>
    <p class="normal">During <a id="_idIndexMarker783"/>inference, DP can be useful for processing <a id="_idIndexMarker784"/>concurrent requests. By distributing the workload across multiple GPUs, this approach helps reduce latency, as multiple requests can be handled simultaneously. This concurrent processing also increases throughput, since a higher number of requests can be processed at the same time.</p>
    <p class="normal">However, the effectiveness of DP is limited by the model size and the communication overhead between GPUs. Indeed, replicating the model’s parameters on each GPU is inefficient. This means that this technique only works when the model is small enough to fit into a single GPU, leaving less room for input data and thus limiting the batch size. For larger models or when memory is a constraint, this can be a significant drawback.</p>
    <p class="normal">Typically, DP is mainly used for training, while pipeline and tensor parallelism are preferred for inference.</p>
    <h2 id="_idParaDest-208" class="heading-2">Pipeline parallelism</h2>
    <p class="normal">Introduced <a id="_idIndexMarker785"/>by Huang et al. in the GPipe paper (2019), <strong class="keyWord">pipeline parallelism</strong> (<strong class="keyWord">PP</strong>) is<a id="_idIndexMarker786"/> a strategy for distributing the computational load of training and running large neural networks across multiple GPUs.</p>
    <p class="normal">Unlike traditional DP, which replicates the entire model on each GPU, pipeline parallelism partitions the model’s layers across different GPUs. This approach allows each GPU to handle a specific portion of the model, thereby reducing the memory burden on individual GPUs.</p>
    <figure class="mediaobject"><img src="img/B31105_08_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.5 – Illustration of pipeline parallelism with four GPUs</p>
    <p class="normal">As shown in <em class="italic">Figure 8.5</em>, in a typical four-way pipeline parallel split, the model is divided into four segments, with each segment assigned to a different GPU. The first 25% of the model’s layers might be processed by GPU 1, the next 25% by GPU 2, and so on. During the forward pass, activations are computed and then passed along to the next GPU. For training, the backward pass follows a similar sequence in reverse, with gradients being propagated back through the GPUs. The number of GPUs is often referred to as the degree of parallelism.</p>
    <p class="normal">The primary advantage of pipeline parallelism is its ability to significantly reduce the memory requirements per GPU. However, this approach introduces new challenges, particularly related to the sequential nature of the pipeline. One of the main<a id="_idIndexMarker787"/> issues is the occurrence of “pipeline bubbles.” These bubbles arise when some GPUs are idle, waiting for activations from preceding layers. This idle time can reduce the overall efficiency of the process.</p>
    <p class="normal">Micro-batching <a id="_idIndexMarker788"/>was developed to mitigate the impact of pipeline bubbles. By splitting the input batch into smaller sub-batches, micro-batching ensures that GPUs remain busier, as the next sub-batch can begin processing before the previous one is fully completed.</p>
    <figure class="mediaobject"><img src="img/B31105_08_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.6 – Illustration of pipeline parallelism with micro-batching.</p>
    <p class="normal"><em class="italic">Figure 8.6</em> shows an example of pipeline parallelism with micro-batching. In this example, the pipeline has four stages (<strong class="keyWord">F0</strong>, <strong class="keyWord">F1</strong>, <strong class="keyWord">F2</strong>, <strong class="keyWord">F3</strong>), and the input batch is divided into four micro-batches. GPU 0 will process forward paths <strong class="keyWord">F0</strong>,0, <strong class="keyWord">F0</strong>,1, <strong class="keyWord">F0</strong>,2, and <strong class="keyWord">F0</strong>,3, sequentially. Once <strong class="keyWord">F0</strong>,0 is complete, GPU 1 can immediately start processing <strong class="keyWord">F1</strong>,0 and so on. After completing these forward passes, GPU 0 waits for the other GPUs to finish their respective forward computations before starting the backward paths (<strong class="keyWord">B0</strong>,3, <strong class="keyWord">B0</strong>,2, <strong class="keyWord">B0</strong>,1, and <strong class="keyWord">B0</strong>,0).</p>
    <p class="normal">Pipeline parallelism is implemented in distributed training frameworks like Megatron-LM, DeepSpeed (ZeRO), and<a id="_idIndexMarker789"/> PyTorch through the dedicated <strong class="keyWord">Pipeline Parallelism for PyTorch </strong>(<strong class="keyWord">PiPPy</strong>) library. At the time of writing, only certain inference frameworks like TensorRT-LLM support pipeline parallelism.</p>
    <h2 id="_idParaDest-209" class="heading-2">Tensor parallelism</h2>
    <p class="normal">Introduced by Shoeby, Patwary, Puri et al. in the Megatron-LM paper (2019), <strong class="keyWord">tensor parallelism</strong> (<strong class="keyWord">TP</strong>) is <a id="_idIndexMarker790"/>another popular<a id="_idIndexMarker791"/> technique to distribute the computation of LLM layers across multiple devices. In contrast to pipeline parallelism, TP splits the weight matrices found in individual layers. This enables simultaneous computations, significantly reducing memory bottlenecks and increasing processing speed.</p>
    <p class="normal">In TP, large matrices, such as the weight matrices in MLPs or the attention heads in self-attention layers, are partitioned across several GPUs. Each GPU holds a portion of these matrices and performs computations on its respective slice.</p>
    <figure class="mediaobject"><img src="img/B31105_08_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.7 – Illustration of column-wise tensor parallelism in an MLP layer (W)</p>
    <p class="normal">For instance, in <a id="_idIndexMarker792"/>an MLP layer, the weight matrix is divided so that each GPU processes only a subset of the weights (see <em class="italic">Figure 8.7</em>). The inputs are broadcast to all GPUs, which then independently compute their respective outputs. The partial results are then aggregated through an all-reduce operation, combining them to form the final output.</p>
    <p class="normal">In the <a id="_idIndexMarker793"/>context of self-attention layers, TP is particularly efficient due to the inherent parallelism of attention heads. Each GPU can compute a subset of these heads independently, allowing the model to process large sequences more effectively. This makes TP more efficient than pipeline parallelism, which requires waiting for the completion of previous layers.</p>
    <p class="normal">Despite its advantages, TP is not universally applicable to all layers of a neural network. Layers like LayerNorm and Dropout, which have dependencies spanning the entire input, cannot be efficiently partitioned and are typically replicated across devices instead. However, these operations can be split on the sequence dimension of the input instead (sequence parallelism). Different GPUs can compute these layers on different slices of the input sequence, avoiding replication of weights. This technique is limited to a few specific layers, but it can provide additional memory savings, especially for very large input sequence lengths.</p>
    <p class="normal">Moreover, TP necessitates high-speed interconnects between devices to minimize communication overhead, making it impractical to implement across nodes with insufficient interconnect bandwidth.</p>
    <p class="normal">TP is also implemented in distributed training frameworks like Megatron-LM, DeepSpeed (ZeRO), and PyTorch (FSDP). It is available in most inference frameworks, like TGI, vLLM, and TensorRT-LLM.</p>
    <h2 id="_idParaDest-210" class="heading-2">Combining approaches</h2>
    <p class="normal">Data, tensor, and pipeline parallelisms are orthogonal techniques that can be combined. <em class="italic">Figure 8.8</em> illustrates how a given model can be split according to each approach:</p>
    <figure class="mediaobject"><img src="img/B31105_08_08.png" alt="A screenshot of a computer  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 8.8 – Illustration of the different model parallelism techniques</p>
    <p class="normal">Combining these<a id="_idIndexMarker794"/> techniques can mitigate their respective issues. Pipeline parallelism provides the greatest memory reduction but sacrifices efficiency, due to pipeline bubbles. This may be ideal if the primary constraint fits the model in the GPU memory. In contrast, if low latency is paramount, then prioritizing tensor parallelism and accepting a larger memory footprint may be the better trade-off. In practice, a model may be split depth-wise into a few pipeline stages, with tensor parallelism used within each stage.</p>
    <p class="normal">Balancing these tradeoffs and mapping a given model architecture onto available hardware accelerators is a key challenge in deploying LLMs.</p>
    <h1 id="_idParaDest-211" class="heading-1">Model quantization</h1>
    <p class="normal">Quantization <a id="_idIndexMarker795"/>refers to the process of representing the weights<a id="_idIndexMarker796"/> and activations of a neural network using lower-precision data types. In the context of LLMs, quantization primarily focuses on reducing the precision of the model’s weights and activations. </p>
    <p class="normal">By default, weights are typically stored in a 16-bit or 32-bit floating-point format (FP16 or FP32), which provides high precision but comes at the cost of increased memory usage and computational complexity. Quantization is a solution to reduce the memory footprint and accelerate the inference of LLMs.</p>
    <p class="normal">In addition to<a id="_idIndexMarker797"/> these benefits, larger models with over 30 billion parameters can outperform smaller models (7B–13B LLMs) in terms of quality when quantized to 2- or 3-bit precision. This means they can achieve superior performance while maintaining a comparable memory footprint.</p>
    <p class="normal">In this section, we will introduce the concepts of quantization, GGUF with <code class="inlineCode">llama.cpp</code>, GPTQ, and EXL2, along with an overview of additional techniques. In addition to the code provided in this section, you can refer to AutoQuant (<a href="https://bit.ly/autoquant">bit.ly/autoquant</a>) to quantize their models using a Google Colab notebook.</p>
    <h2 id="_idParaDest-212" class="heading-2">Introduction to quantization</h2>
    <p class="normal">There are two main <a id="_idIndexMarker798"/>approaches to weight quantization: <strong class="keyWord">Post-Training Quantization</strong> (<strong class="keyWord">PTQ</strong>) and <strong class="keyWord">Quantization-Aware Training</strong> (<strong class="keyWord">QAT</strong>). PTQ is a straightforward <a id="_idIndexMarker799"/>technique where the weights of a <a id="_idIndexMarker800"/>pre-trained model are directly converted to a lower precision format without any retraining. While PTQ is easy to implement, it may result in some performance degradation. Conversely, QAT performs quantization during the training or fine-tuning stage, allowing the model to adapt to the lower precision weights. QAT often yields better performance compared to PTQ but requires additional computational resources and representative training data.</p>
    <p class="normal">The choice of data type plays a crucial role in quantization. Floating-point numbers, such as <strong class="keyWord">FP32</strong>, <strong class="keyWord">FP16</strong> (half-precision), and <strong class="keyWord">BF16</strong> (brain floating-point), are commonly used in deep learning. These formats allocate a fixed number of bits to represent the <code class="inlineCode">sign</code>, <code class="inlineCode">exponent</code>, and <code class="inlineCode">significand</code> (mantissa) of a number.</p>
    <figure class="mediaobject"><img src="img/B31105_08_09.png" alt="A screenshot of a computer program  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 8.9 – Comparison the between FP32, FP16, and BF16 formats</p>
    <p class="normal">A sign of 0 represents <a id="_idIndexMarker801"/>a positive number, while 1 indicates a negative number. Conversely, the exponent controls the range that is represented (big or small). Finally, the significand controls the precision of the number (the number of digits). The formula used to convert these representations into real numbers is:</p>
    <p class="center"><img src="img/B31105_08_007.png" alt=""/></p>
    <p class="normal">The data types shown in <em class="italic">Figure 7.7</em> display different tradeoffs, as illustrated with different representations of <img src="img/B31105_08_008.png" alt=""/> (<img src="img/B31105_08_009.png" alt=""/>). FP32 uses 32 bits, providing high precision but also requiring more memory. Conversely, FP16 and BF16 use 16 bits, lowering the memory footprint at the cost of a lower precision. In general, neural networks prefer a bigger range than better precision, which is why BF16 is the most popular data type when the hardware supports it. For example, NVIDIA’s Ampere architecture (A100, A30, etc.) supports BF16, but previous generations like Turing (T4, T40, etc.) do not.</p>
    <p class="normal">However, we <a id="_idIndexMarker802"/>are not restricted to these three data types. Lower-precision data types, such as INT8 (8-bit integers), can be employed for quantization, further reducing the memory footprint. Naïve quantization techniques, such as <em class="italic">absolute maximum (absmax) quantization</em> and <em class="italic">zero-point quantization</em>, can be applied to convert <code class="inlineCode">FP32</code>, <code class="inlineCode">FP16</code>, or <code class="inlineCode">BF16</code> weights to <code class="inlineCode">INT8</code>, as illustrated in <em class="italic">Figure 8.10</em>:</p>
    <figure class="mediaobject"><img src="img/B31105_08_10.png" alt="A screenshot of a video game  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 8.10 – Quantization of 0.1 in a [-3.0, 3.2] range with absmax quantization and zero-point quantization</p>
    <p class="normal">Absmax quantization maps the original weights <img src="img/B31105_08_010.png" alt=""/> to the range [-127, 127] by dividing them by the absolute maximum value of <img src="img/B31105_08_010.png" alt=""/> and scaling them:</p>
    <p class="center"><img src="img/B31105_08_012.png" alt=""/></p>
    <p class="normal">For example, if our absolute maximum value is 3.2 (see <em class="italic">Figure 8.8</em>), a weight of 0.1 would be quantized to <img src="img/B31105_08_013.png" alt=""/>. To dequantize it, we do the inverse operation:</p>
    <p class="center"><img src="img/B31105_08_014.png" alt=""/></p>
    <p class="normal">This means that if we dequantize our weight, we obtain <img src="img/B31105_08_015.png" alt=""/>. We can see a rounding error of <img src="img/B31105_08_016.png" alt=""/> in this example. In Python, we can implement it as follows with the PyTorch library:</p>
    <pre class="programlisting code"><code class="hljs-code">import torch
def absmax_quantize(X):
    # Calculate scale
    scale = 127 / torch.max(torch.abs(X))
    # Quantize
    X_quant = (scale * X).round()
    return X_quant.to(torch.int8)
</code></pre>
    <p class="normal">Zero-point quantization, on <a id="_idIndexMarker803"/>the other hand, considers asymmetric<a id="_idIndexMarker804"/> input distributions and maps the weights <img src="img/B31105_08_0161.png" alt=""/>to the range [-128, 127] by introducing a zero-point offset:</p>
    <p class="center"><img src="img/B31105_08_018.png" alt=""/></p>
    <p class="normal">Where <img src="img/B31105_08_019.png" alt=""/> and <img src="img/B31105_08_020.png" alt=""/>.</p>
    <p class="normal">If we take the same example with a weight of 0.1, we get a scale of <img src="img/B31105_08_021.png" alt=""/> and a zero-point value of <img src="img/B31105_08_022.png" alt=""/>. The weight of 0.1 would be quantized to <img src="img/B31105_08_023.png" alt=""/>, unlike the value of <img src="img/B31105_08_024.png" alt=""/> provided by absmax.</p>
    <p class="normal">We can easily get the dequantization by applying the inverse operation:</p>
    <p class="center"><img src="img/B31105_08_025.png" alt=""/></p>
    <p class="normal">In Python, zero-point quantization can be implemented as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">def zeropoint_quantize(X):
    # Calculate value range (denominator)
    x_range = torch.max(X) - torch.min(X)
    x_range = 1 if x_range == 0 else x_range
    # Calculate scale
    scale = 255 / x_range
    # Shift by zero-point
    zeropoint = (-scale * torch.min(X) - 128).round()
    # Scale and round the inputs
    X_quant = torch.clip((X * scale + zeropoint).round(), -128, 127)
   
    return X_quant.to(torch.int8)
</code></pre>
    <p class="normal">However, naïve quantization<a id="_idIndexMarker805"/> methods have limitations, particularly when dealing with <em class="italic">outlier features</em> in LLMs. Outlier features are extreme weight values (about 0.1% of total values) that can significantly impact the quantization process, leading to reduced precision for other values.</p>
    <p class="normal">Discarding these outliers is not feasible, as it would degrade a model’s performance. You can see an example of outliers in <em class="italic">Figure 8.11</em>:</p>
    <figure class="mediaobject"><img src="img/B31105_08_11.png" alt="A screenshot of a computer  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 8.11 – Example of outliers in a weight matrix</p>
    <p class="normal">To address the outlier problem, more advanced quantization techniques have been proposed. One notable example is <code class="inlineCode">LLM.int8()</code>, introduced by Dettmers et al. (2022). <code class="inlineCode">LLM.int8()</code> employs a mixed-precision quantization scheme, where outlier features are processed using FP16, while the remaining values are quantized to INT8. This approach effectively reduces the memory footprint of LLMs by nearly 2x while minimizing performance degradation.</p>
    <p class="normal"><code class="inlineCode">LLM.int8()</code> works by <a id="_idIndexMarker806"/>performing matrix multiplication in three steps. First, it extracts columns containing outlier features from the input hidden states using a custom threshold. Second, it performs separate matrix multiplications for the outliers (in <code class="inlineCode">FP16</code>) and non-outliers (in <code class="inlineCode">INT8</code>) using vector-wise quantization. Finally, it dequantizes the non-outlier results and combines them with the outlier results to obtain the final output in <em class="italic">FP16</em>.</p>
    <p class="normal">The effectiveness of <code class="inlineCode">LLM.int8()</code> has been demonstrated empirically, showing negligible performance degradation (&lt;1%) compared to the original <code class="inlineCode">FP32</code> models. However, it does introduce an additional computational overhead, resulting in around 20% slower inference for large models. Models can be directly loaded in 8-bit precision with the transformer library, using <code class="inlineCode">LLM.int8()</code>, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">from transformers import AutoModelForCausalLM
model_name = "meta-llama/Meta-Llama-3-8B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_8bit=True)
</code></pre>
    <p class="normal">Introduced by <em class="italic">Dettmers et al</em>. (2023), NF4 is a 4-bit precision format designed for QLoRA (discussed in <em class="italic">Chapter 5</em>). It is also integrated into the transformers library but requires the bitsandbytes library as a dependency. To load a model in NF4 (4-bit precision), you can use the <code class="inlineCode">load_in_4bit</code> parameter, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">from transformers import AutoModelForCausalLM
model_name = "meta-llama/Meta-Llama-3-8B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_4bit=True)
</code></pre>
    <h2 id="_idParaDest-213" class="heading-2">Quantization with GGUF and llama.cpp</h2>
    <p class="normal">The llama.cpp project<a id="_idIndexMarker807"/> is an open-source C++ software library created by Georgi Gerganov, designed to perform inference with various LLMs. It is the most popular quantization technique, with many quantized models available on the Hugging Face Hub.</p>
    <p class="normal">Compared to<a id="_idIndexMarker808"/> other libraries that rely on hardware-specific closed-source libraries like CUDA, llama.cpp can run on a broader range of hardware. It has gained significant popularity, particularly among users without specialized hardware, as it can operate on CPUs and Android devices. Moreover, llama.cpp can also offload layers to the GPU, accelerating inference speed. It is compatible with different inference optimization techniques, such as FlashAttention-2 and speculative decoding.</p>
    <p class="normal">This project features its own quantization format, GGUF, designed to simplify and speed up model loading. GGUF files store tensors and metadata, supporting various formats, from 1-bit to 8-bit precision. It follows a naming convention based on the number of bits used and specific variants, such as:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">IQ1_S</code> and <code class="inlineCode">IQ1_M</code>: 1-bit precision – very low quality</li>
      <li class="bulletList"><code class="inlineCode">IQ2_XXS/XS/S/M</code> and <code class="inlineCode">Q2_K</code>: 2-bit precision – generally low quality but IQ2 can be usable for large models</li>
      <li class="bulletList"><code class="inlineCode">IQ3_XXS/XS/S/M</code> and <code class="inlineCode">Q3_K_S/M/L</code>: 3-bit precision – low quality but usable for large models</li>
      <li class="bulletList"><code class="inlineCode">IQ4_XS/NL</code> and <code class="inlineCode">Q4_K_S/M, Q4_0/1</code>: 4-bit precision – good quality and usable for most models</li>
      <li class="bulletList"><code class="inlineCode">Q5_K_S/M</code> and <code class="inlineCode">Q5_0/1</code>: 5-bit precision – high quality</li>
      <li class="bulletList"><code class="inlineCode">Q6_K</code>: 6-bit precision –very high quality</li>
      <li class="bulletList"><code class="inlineCode">Q8_0</code>: 8-bit precision – highest quality</li>
    </ul>
    <p class="normal">To provide a brief overview of GGUF quantization, llama.cpp groups values into blocks and rounds them to a lower precision. For instance, the legacy Q4_0 format handles 32 values per block, scaling and quantizing them based on the largest weight value in the block (<img src="img/B31105_08_026.png" alt=""/>). In Q4_1, the smallest Lvalue in the block is also added (<img src="img/B31105_08_027.png" alt=""/> ). In Q4_K, weights are divided into super-blocks, containing 8 blocks with 32 values. Block scales and minimum values are also quantized in higher precision with 6 bits (<img src="img/B31105_08_028.png" alt=""/>). Finally, i-quants like IQ4_XS are inspired by another quantization technique called QuIP#. This ensures an even number of positive (or negative) quant signs in groups of eight and implements the <img src="img/B31105_08_029.png" alt=""/> lattice to store their magnitude.</p>
    <p class="normal">Here is a<a id="_idIndexMarker809"/> practical example of how to quantize a model in the GGUF format. The following steps can be executed on a free T4 GPU in Google Colab:</p>
    <ol>
      <li class="numberedList" value="1">Install llama.cpp and the required libraries:
        <pre class="programlisting con-one"><code class="hljs-con">!git clone https://github.com/ggerganov/llama.cpp
!cd llama.cpp &amp;&amp; git pull &amp;&amp; make clean &amp;&amp; LLAMA_CUBLAS=1 make
!pip install -r llama.cpp/requirements.txt
</code></pre>
      </li>
      <li class="numberedList">Download the model to convert. We will provide the model ID from the Hugging Face Hub – for example, <code class="inlineCode">mistralai/Mistral-7B-Instruct-v0.2</code>:
        <pre class="programlisting con-one"><code class="hljs-con">MODEL_ID = "mlabonne/EvolCodeLlama-7b"
MODEL_NAME = MODEL_ID.split('/')[-1]
!git lfs install
!git clone https://huggingface.co/{MODEL_ID}
</code></pre>
      </li>
      <li class="numberedList">First, we convert the model into FP16. This is an intermediary artifact that will be used for every GGUF quantization type. Note that different conversion scripts exist in llama.cpp and are compatible with different models:
        <pre class="programlisting con-one"><code class="hljs-con">fp16 = f"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin"
!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16}
</code></pre>
      </li>
      <li class="numberedList">We select a format (here, <code class="inlineCode">Q4_K_M</code>) and start the quantization. This process can take an hour on a T4 GPU:
        <pre class="programlisting con-one"><code class="hljs-con">METHOD = "q4_k_m"
qtype = f"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf"
!./llama.cpp/quantize {fp16} {qtype} {METHOD}
</code></pre>
      </li>
      <li class="numberedList">Once<a id="_idIndexMarker810"/> it’s done, your quantized model is ready. You can download it locally, or upload it to the Hugging Face Hub using the following code:
        <pre class="programlisting code-one"><code class="hljs-code">from huggingface_hub import create_repo, HfApi
hf_token = "" # Specify your token
username = "" # Specify your username
api = HfApi()
# Create empty repo
create_repo(
    repo_id = f"{username}/{MODEL_NAME}-GGUF",
    repo_type="model",
    exist_ok=True,
    token=hf_token
)
# Upload gguf files
api.upload_folder(
    folder_path=MODEL_NAME,
    repo_id=f"{username}/{MODEL_NAME}-GGUF",
    allow_patterns=f"*.gguf",
    token=hf_token
)
</code></pre>
      </li>
    </ol>
    <p class="normal">GGUF models can be used with backends such as llama-cpp-python and frameworks like LangChain. This is useful if you want to integrate a quantized model into a broader system. You can also directly chat with the model using frontends, like llama.cpp’s lightweight server, LM Studio, and the Text Generation Web UI. These tools enable easy interaction with the GGUF models, providing an experience similar to ChatGPT.</p>
    <h2 id="_idParaDest-214" class="heading-2">Quantization with GPTQ and EXL2</h2>
    <p class="normal">While <a id="_idIndexMarker811"/>GGUF and llama.cpp offer CPU inference with GPU offloading, GPTQ and EXL2 are two quantization formats dedicated to GPUs. This makes them both faster than llama.cpp during inference. In particular, EXL2 offers the highest throughput with its dedicated library, ExLlamaV2.</p>
    <p class="normal">GPTQ and EXL2 quants are based on the GPTQ algorithm, introduced by Frantar et al. (2023). It optimizes weight quantization for LLMs by refining <strong class="keyWord">the Optimal Brain Quantization</strong> (<strong class="keyWord">OBQ</strong>) approach<a id="_idIndexMarker812"/> to handle extensive matrices efficiently. It begins with a Cholesky decomposition of the Hessian inverse, ensuring numerical stability. Instead of quantizing weights in a strict order, GPTQ processes them in batches, updating columns and associated blocks iteratively. This method leverages lazy batch updates, reducing computational redundancy and memory bottlenecks.</p>
    <p class="normal">While GPTQ is limited to 4-bit precision, EXL2 offers more flexibility with a highly customizable precision that can mix different quantization levels. This allows for precise bitrates between 2 and 8 bits per weight, such as <code class="inlineCode">2.3</code>, <code class="inlineCode">3.5</code>, or <code class="inlineCode">6.0</code>. It can also apply multiple quantization levels to each linear layer, prioritizing more important weights with higher bit quantization. Parameters are selected automatically, by quantizing each matrix multiple times and choosing a combination that minimizes the quantization error while meeting a target bitrate. In practice, this allows 70B models to run on a single 24 GB GPU with 2.55-bit precision.</p>
    <p class="normal">The inference itself is handled by the ExLlamaV2 library, which supports both the GPTQ and EXL2 models.</p>
    <p class="normal">In the following example, let’s quantize a model in the EXL2 format using ExLlamaV2. These steps can be executed on a free T4 GPU in Google Colab:</p>
    <ol>
      <li class="numberedList" value="1">Install the ExLlamaV2 library from source:
        <pre class="programlisting con-one"><code class="hljs-con">!git clone https://github.com/turboderp/exllamav2
!pip install -e exllamav2
</code></pre>
      </li>
      <li class="numberedList">We download the model to quantize by cloning its repo from the Hugging Face Hub:
        <pre class="programlisting con-one"><code class="hljs-con">MODEL_ID = "meta-llama/Llama-2-7b-chat-hf"
MODEL_NAME = MODEL_ID.split('/')[-1]
!git lfs install
!git clone https://huggingface.co/{MODEL_ID}
</code></pre>
      </li>
      <li class="numberedList">Download<a id="_idIndexMarker813"/> the calibration dataset used to measure the quantization error. In this case, we will use WikiText-103, a standard calibration dataset with high-quality articles from Wikipedia:
        <pre class="programlisting con-one"><code class="hljs-con">!wget https://huggingface.co/datasets/wikitext/resolve/9a9e482b5987f9d25b3a9b2883fc6cc9fd8071b3/wikitext-103-v1/wikitext-test.parquet
</code></pre>
      </li>
      <li class="numberedList">Quantize the model at a given precision (for example, 4.5):
        <pre class="programlisting con-one"><code class="hljs-con">!mkdir quant
!python exllamav2/convert.py \
    -i {MODEL_NAME} \
    -o quant \
    -c wikitext-test.parquet \
    -b 4.5
</code></pre>
      </li>
    </ol>
    <p class="normal">The quantized model can then be uploaded to the Hugging Face Hub, as seen previously.</p>
    <p class="normal">GPTQ and EXL2 quants are not as widely supported as GGUF. For example, frontends like LM Studio do not currently integrate them. You can use other tools instead, like oobabooga’s Text Generation Web UI. It is also directly integrated into the transformers library and supported by TGI. GPTQ models are also supported in TensorRT-LLM.</p>
    <p class="normal">While less popular than GGUF, you can find a lot of GPTQ and EXL2 models on the Hugging Face Hub.</p>
    <h2 id="_idParaDest-215" class="heading-2">Other quantization techniques</h2>
    <p class="normal">There is a <a id="_idIndexMarker814"/>variety of quantization techniques beyond GGUF, GPTQ, and EXL2. This subsection will briefly introduce <strong class="keyWord">Activate-aware Weight Quantization</strong> (<strong class="keyWord">AWQ</strong>) as <a id="_idIndexMarker815"/>well as extreme quantization techniques, like QuIP# (Quantization with Incoherence Processing) and HQQ (Half-Quadratic Quantization).</p>
    <p class="normal">Introduced by Lin et al. (2023), AWQ is another popular quantization algorithm. It identifies and protects the most important weights, which are determined based on activation magnitude instead of weight magnitude. This approach involves applying optimal per-channel scaling to these salient weights, without relying on backpropagation or reconstruction, ensuring that the LLM does not overfit the calibration set. While it relies on a different paradigm, AWQ is quite close to the GPTQ and EXL2 versions, although slightly slower. They are well-supported by inference engines and integrated into TGI, vLLM, and TensorRT-LLM.</p>
    <p class="normal">An interesting trend is the quantization of models into 1- or 2-bit precision. While some formats, like EXL2, allow extreme quantization, the quality of the models often suffers significantly. However, recent algorithms like QuIP# and HQQ have targeted this regime and offer quantization methods that better preserve the performance of the original models. This is particularly true for large models (over 30B parameters), which can end up taking less space than 7B or 13B parameter models while providing higher-quality outputs. </p>
    <p class="normal">This trend is expected to continue, further optimizing these quantization methods.</p>
    <p class="normal">To conclude this chapter, here is a table summarizing the features of the three main inference<a id="_idIndexMarker816"/> engines we covered in the previous sections:</p>
    <table id="table001-4" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Technique</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">TGI</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">vLLM</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">TensorRT-LLM</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Continuous batching</p>
          </td>
          <td class="table-cell">
            <p class="normal">✓</p>
          </td>
          <td class="table-cell">
            <p class="normal">✓</p>
          </td>
          <td class="table-cell">
            <p class="normal">✓</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Speculative decoding</p>
          </td>
          <td class="table-cell">
            <p class="normal">✓</p>
          </td>
          <td class="table-cell"/>
          <td class="table-cell"/>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">FlashAttention2</p>
          </td>
          <td class="table-cell">
            <p class="normal">✓</p>
          </td>
          <td class="table-cell">
            <p class="normal">✓</p>
          </td>
          <td class="table-cell">
            <p class="normal">✓</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">PagedAttention</p>
          </td>
          <td class="table-cell">
            <p class="normal">✓</p>
          </td>
          <td class="table-cell">
            <p class="normal">✓</p>
          </td>
          <td class="table-cell">
            <p class="normal">✓</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Pipeline parallelism</p>
          </td>
          <td class="table-cell"/>
          <td class="table-cell"/>
          <td class="table-cell">
            <p class="normal">✓</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Tensor parallelism</p>
          </td>
          <td class="table-cell">
            <p class="normal">✓</p>
          </td>
          <td class="table-cell">
            <p class="normal">✓</p>
          </td>
          <td class="table-cell">
            <p class="normal">✓</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">GPTQ</p>
          </td>
          <td class="table-cell">
            <p class="normal">✓</p>
          </td>
          <td class="table-cell"/>
          <td class="table-cell">
            <p class="normal">✓</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">EXL2</p>
          </td>
          <td class="table-cell">
            <p class="normal">✓</p>
          </td>
          <td class="table-cell"/>
          <td class="table-cell"/>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">AWQ</p>
          </td>
          <td class="table-cell">
            <p class="normal">✓</p>
          </td>
          <td class="table-cell">
            <p class="normal">✓</p>
          </td>
          <td class="table-cell">
            <p class="normal">✓</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 8.1 – Summary of features for TGI, vLLM, and TensorRT-LLM</p>
    <h1 id="_idParaDest-216" class="heading-1">Summary</h1>
    <p class="normal">In summary, inference optimization is a critical aspect of deploying LLMs effectively. This chapter explored various optimization techniques, including optimized generation methods, model parallelism, and weight quantization. Significant speedups can be achieved by leveraging techniques like predicting multiple tokens in parallel with speculative decoding, or using an optimized attention mechanism with FlashAttention-2. Additionally, we discussed how model parallelism methods, including data, pipeline, and tensor parallelism, distribute the computational load across multiple GPUs to increase throughput and reduce latency. Weight quantization, with formats like GGUF and EXL2, further reduces the memory footprint and accelerates inference, with some calculated tradeoff in output quality.</p>
    <p class="normal">Understanding and applying these optimization strategies are essential for achieving high performance in practical applications of LLMs, such as chatbots and code completion. The choice of techniques and tools depends on specific requirements, including available hardware, desired latency, and throughput. By combining various approaches, such as continuous batching and speculative decoding, along with advanced attention mechanisms and model parallelism, users can tailor their deployment strategies to maximize efficiency.</p>
    <p class="normal">Way back in <em class="chapterRef">Chapter 4</em>, we focused only on implementing the ingestion pipeline, which is just one component of a standard RAG application. In the next chapter, we will conclude the RAG system by implementing the retrieval and generation components and integrating them into the inference pipeline. </p>
    <h1 id="_idParaDest-217" class="heading-1">References</h1>
    <ul>
      <li class="bulletList">Hugging Face, Text Generation Inference, <a href="https://github.com/huggingface/text-generation-inference">https://github.com/huggingface/text-generation-inference</a>, 2022.</li>
      <li class="bulletList"><em class="italic">W. Kwon</em>, <em class="italic">Z. Li</em>, <em class="italic">S. Zhuang</em>, <em class="italic">Y. Sheng</em>, <em class="italic">L. Zheng</em>, <em class="italic">C.H. Yu</em>, <em class="italic">J.E. Gonzalez</em>, <em class="italic">H. Zhang</em>, <em class="italic">I. Stoica</em>, <em class="italic">Efficient Memory Management for Large Language Model Serving with PagedAttention</em>, <em class="italic">2023</em>.</li>
      <li class="bulletList">Nvidia, <em class="italic">TensorRT-LLM</em>, <a href="https://github.com/NVIDIA/TensorRT-LLM">https://github.com/NVIDIA/TensorRT-LLM</a>, 2023.</li>
      <li class="bulletList"><em class="italic">Y. Leviathan, M. Kalman, Y. Matias, Fast Inference from Transformers via Speculative Decoding, 2023</em>.</li>
      <li class="bulletList"><em class="italic">T. Cai, Y. Li, Z. Geng, H. Peng, J.D. Lee, D. Chen, T. Dao, Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads, 2024</em>.</li>
      <li class="bulletList"><em class="italic">W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C.H. Yu, J.E. Gonzalez, H. Zhang, I. Stoica, Efficient Memory Management for Large Language Model Serving with PagedAttention, 2023</em>.</li>
      <li class="bulletList"><em class="italic">R.Y. Aminabadi, S. Rajbhandari, M. Zhang, A.A. Awan, C. Li, D. Li, E. Zheng, J. Rasley, S. Smith, O. Ruwase, Y. He, DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale, 2022</em>.</li>
      <li class="bulletList"><em class="italic">Y. Huang, Y. Cheng, A. Bapna, O. Firat, M.X. Chen, D. Chen, H. Lee, J. Ngiam, Q.V. Le, Y. Wu, Z. Chen, GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism, 2019</em>.</li>
      <li class="bulletList"><em class="italic">K. James Reed, PiPPy: Pipeline Parallelism for PyTorch</em>, <a href="https://github.com/pytorch/PiPPy">https://github.com/pytorch/PiPPy</a>, <em class="italic">2022</em>.</li>
      <li class="bulletList"><em class="italic">M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catanzaro, Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism, 2020.</em></li>
      <li class="bulletList"><em class="italic">Verma and Vaidya, Mastering LLM Techniques: Inference Optimization, NVIDIA Developer Technical Blog</em>, <a href="https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/">https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/</a>, <em class="italic">2023</em>.</li>
      <li class="bulletList"><em class="italic">T. Dettmers, M. Lewis, Y. Belkada, L. Zettlemoyer, LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale, 2022.</em></li>
      <li class="bulletList"><em class="italic">G. Gerganov, llama.cpp</em>, <a href="https://github.com/ggerganov/llama.cpp">https://github.com/ggerganov/llama.cpp</a>, <em class="italic">2023</em>.</li>
      <li class="bulletList"><em class="italic">E. Frantar, S. Ashkboos, T. Hoefler, D. Alistarh, GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers, 2023</em>.</li>
      <li class="bulletList"><em class="italic">Tuboderp, exllamav2</em>, <a href="https://github.com/turboderp/exllamav2">https://github.com/turboderp/exllamav2</a>, <em class="italic">2023</em>.</li>
      <li class="bulletList"><em class="italic">J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang, C. Gan, S. Han, AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration, 2024</em>.</li>
    </ul>
    <h1 id="_idParaDest-218" class="heading-1">Join our book’s Discord space</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/llmeng">https://packt.link/llmeng</a></p>
    <p class="normal"><img src="img/QR_Code79969828252392890.png" alt=""/></p>
  </div>
</body></html>