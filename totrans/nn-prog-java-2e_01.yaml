- en: Chapter 1. Getting Started with Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will introduce neural networks and what they are designed
    for. This chapter serves as a foundation layer for the subsequent chapters, while
    presenting the basic concepts for neural networks. In this chapter, we will cover
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Artificial neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weights and biases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layers of neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural network implementation in Java
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By hearing the term **neural networks** we intuitively create a snapshot of
    a brain in our minds, and indeed that's right, if we consider the brain to be
    a big and natural neural network. However, what about **artificial neural networks**
    (**ANNs**)? Well, here comes an opposite word to natural, and the first thing
    now that comes into our head is an image of an artificial brain or a robot, given
    the term *artificial*. In this case, we also deal with creating a structure similar
    to and inspired by the human brain; therefore, this can be called artificial intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Now the reader newly introduced to ANN may be thinking that this book teaches
    how to build intelligent systems, including an artificial brain, capable of emulating
    the human mind using Java codes, isn't it? The amazing answer is yes, but of course,
    we will not cover the creation of artificial thinking machines such as those from
    the Matrix trilogy movies; the reader will be provided a walkthrough on the process
    of designing artificial neural network solutions capable of abstracting knowledge
    in raw data, taking advantage of the entire Java programming language framework.
  prefs: []
  type: TYPE_NORMAL
- en: Why artificial neural networks?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We cannot begin talking about neural networks without understanding their origins,
    including the term as well. The terms neural networks (NN) and ANN are used as
    synonyms in this book, despite NNs being more general, covering the natural neural
    networks as well. So, what actually is an ANN? Let's explore a little of the history
    of this term.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the 1940s, the neurophysiologist Warren McCulloch and the mathematician
    Walter Pitts designed the first mathematical implementation of an artificial neuron
    combining the neuroscience foundations with mathematical operations. At that time,
    the human brain was being studied largely to understand its hidden and mystery
    behaviors, yet within the field of neuroscience. The natural neuron structure
    was known to have a nucleus, dendrites receiving incoming signals from other neurons,
    and an axon activating a signal to other neurons, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Why artificial neural networks?](img/B05964_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The novelty of McCulloch and Pitts was the math component included in the neuron
    model, supposing a neuron as a simple processor summing all incoming signals and
    activating a new signal to other neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Why artificial neural networks?](img/B05964_01_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Furthermore, considering that the brain is composed of billions of neurons,
    each one interconnected with another tens of thousands, resulting in some trillions
    of connections, we are talking about a giant network structure. On the basis of
    this fact, McCulloch and Pitts designed a simple model for a single neuron, initially
    to simulate the human vision. The available calculators or computers at that time
    were very rare, but capable of dealing with mathematical operations quite well;
    on the other hand, even tasks today such as vision and sound recognition are not
    easily programmed without the use of special frameworks, as opposed to the mathematical
    operations and functions. Nevertheless, the human brain can perform sound and
    image recognition more efficiently than complex mathematical calculations, and
    this fact really intrigues scientists and researchers.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, one known fact is that all complex activities that the human brain
    performs are based on learned knowledge, so as a solution to overcome the difficulty
    that conventional algorithmic approaches face in addressing these tasks easily
    solved by humans, an ANN is designed to have the capability to learn how to solve
    some task by itself, based on its stimuli (data):'
  prefs: []
  type: TYPE_NORMAL
- en: '| Tasks Quickly Solvable by Humans | Tasks Quickly Solvable by Computers |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Classification of imagesVoice recognitionFace identificationForecast events
    on the basis of experience | Complex calculationGrammatical error correctionSignal
    processingOperating system management |'
  prefs: []
  type: TYPE_TB
- en: How neural networks are arranged
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By taking into account the human brain characteristics, it can be said that
    the ANN is a nature-inspired approach, and so is its structure. One neuron connects
    to a number of others that connect to another number of neurons, thus being a
    highly interconnected structure. Later in this book, it will be shown that this
    connectivity between neurons accounts for the capability of learning, since every
    connection is configurable according to the stimuli and the desired goal.
  prefs: []
  type: TYPE_NORMAL
- en: The very basic element – artificial neuron
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s explore the most basic artificial neural element – the artificial neuron.
    Natural neurons have proven to be signal processors since they receive micro signals
    in the dendrites that can trigger a signal in the axon depending on their strength
    or magnitude. We can then think of a neuron as having a signal collector in the
    inputs and an activation unit in the output that can trigger a signal that will
    be forwarded to other neurons, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The very basic element – artificial neuron](img/B05964_01_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In natural neurons, there is a threshold potential that when reached, fires
    the axon and propagates the signal to the other neurons. This firing behavior
    is emulated with activation functions, which has proven to be useful in representing
    nonlinear behaviors in the neurons.
  prefs: []
  type: TYPE_NORMAL
- en: Giving life to neurons – activation function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This activation function is what fires the neuron's output, based on the sum
    of all incoming signals. Mathematically it adds nonlinearity to neural network
    processing, thereby providing the artificial neuron nonlinear behaviors, which
    will be very useful in emulating the nonlinear nature of natural neurons. An activation
    function is usually bounded between two values at the output, therefore being
    a nonlinear function, but in some special cases, it can be a linear function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although any function can be used as activation, let''s concentrate on common
    used ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Function | Equation | Chart |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Sigmoid | ![Giving life to neurons – activation function](img/B05964_01_04_01.jpg)
    | ![Giving life to neurons – activation function](img/B05964_01_04.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| Hyperbolic tangent | ![Giving life to neurons – activation function](img/B05964_01_05_01.jpg)
    | ![Giving life to neurons – activation function](img/B05964_01_05.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| Hard limiting threshold | ![Giving life to neurons – activation function](img/b05964_01_06_01.jpg)
    | ![Giving life to neurons – activation function](img/b05964_01_06.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| Linear | ![Giving life to neurons – activation function](img/b05964_01_07_01.jpg)
    | ![Giving life to neurons – activation function](img/b05964_01_07.jpg) |'
  prefs: []
  type: TYPE_TB
- en: In these equations and charts the coefficient a can be chosen as a setting for
    the activation function.
  prefs: []
  type: TYPE_NORMAL
- en: The flexible values – weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the neural network structure can be fixed, weights represent the connections
    between neurons and they have the capability to amplify or attenuate incoming
    neural signals, thus modifying them and having the power to influence a neuron's
    output. Hence a neuron's activation will not be dependent on only the inputs,
    but on the weights too. Provided that the inputs come from other neurons or from
    the external world (stimuli), the weights are considered to be a neural network's
    established connections between its neurons. Since the weights are an internal
    neural network component and influence its outputs, they can be considered as
    neural network knowledge, provided that changing the weights will change the neural
    network's outputs, that is, its answers to external stimuli.
  prefs: []
  type: TYPE_NORMAL
- en: An extra parameter – bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is useful for the artificial neuron to have an independent component that
    adds an extra signal to the activation function: the **bias**. This parameter
    acts like an input, except for the fact that it is stimulated by one fixed value
    (usually 1), which is multiplied by an associated weight. This feature helps in
    the neural network knowledge representation as a more purely nonlinear system,
    provided that when all inputs are zero, that neuron won''t necessarily produce
    a zero at the output, instead it can fire a different value according to the bias
    associated weight.'
  prefs: []
  type: TYPE_NORMAL
- en: The parts forming the whole – layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to abstract levels of processing, as our mind does, neurons are organized
    in layers. The input layer receives direct stimuli from the outside world, and
    the output layers fire actions that will have a direct influence on the outside
    world. Between these layers, there are a number of hidden layers, in the sense
    that they are invisible (hidden) from the outside world. In artificial neural
    networks, a layer has the same inputs and activation function for all its composing
    neurons, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The parts forming the whole – layers](img/B05964_01_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Neural networks can be composed of several linked layers, forming the so-called
    **multilayer networks**. Neural layers can then be classified as *Input*, *Hidden*,
    or *Output*.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, an additional neural layer enhances the neural network's capacity
    to represent more complex knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Every neural network has at least an input/output layer irrespective of the
    number of layers. In the case of a multilayer network, the layers between the
    input and the output are called **hidden**
  prefs: []
  type: TYPE_NORMAL
- en: Learning about neural network architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A neural network can have different layouts, depending on how the neurons or
    layers are connected to each other. Each neural network architecture is designed
    for a specific goal. Neural networks can be applied to a number of problems, and
    depending on the nature of the problem, the neural network should be designed
    in order to address this problem more efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural network architectures classification is two-fold:'
  prefs: []
  type: TYPE_NORMAL
- en: Neuron connections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monolayer networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilayer networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Signal flow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feedforward networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feedback networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monolayer networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this architecture, all neurons are laid out in the same level, forming one
    single layer, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monolayer networks](img/B05964_01_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The neural network receives the input signals and feeds them into the neurons,
    which in turn produce the output signals. The neurons can be highly connected
    to each other with or without recurrence. Examples of these architectures are
    the single-layer perceptron, Adaline, self-organizing map, Elman, and Hopfield
    neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Multilayer networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this category, neurons are divided into multiple layers, each layer corresponding
    to a parallel layout of neurons that shares the same input data, as shown in the
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multilayer networks](img/B05964_01_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Radial basis functions and multilayer perceptrons are good examples of this
    architecture. Such networks are really useful for approximating real data to a
    function especially designed to represent that data. Moreover, because they have
    multiple layers of processing, these networks are adapted to learn from nonlinear
    data, being able to separate it or determine more easily the knowledge that reproduces
    or recognizes this data.
  prefs: []
  type: TYPE_NORMAL
- en: Feedforward networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The flow of the signals in neural networks can be either in only one direction
    or in recurrence. In the first case, we call the neural network architecture feedforward,
    since the input signals are fed into the input layer; then, after being processed,
    they are forwarded to the next layer, just as shown in the figure in the multilayer
    section. Multilayer perceptrons and radial basis functions are also good examples
    of feedforward networks.
  prefs: []
  type: TYPE_NORMAL
- en: Feedback networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When the neural network has some kind of internal recurrence, it means that
    the signals are fed back in a neuron or layer that has already received and processed
    that signal, the network is of the feedback type. See the following figure of
    feedback networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feedback networks](img/B05964_01_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The special reason to add recurrence in the network is the production of a dynamic
    behavior, particularly when the network addresses problems involving time series
    or pattern recognition, which require an internal memory to reinforce the learning
    process. However, such networks are particularly difficult to train, because there
    will eventually be a recursive behavior during the training (for example, a neuron
    whose outputs are fed back into its inputs), in addition to the arrangement of
    data for training. Most of the feedback networks are single layer, such as Elman
    and Hopfield networks, but it is possible to build a recurrent multilayer network,
    such as echo and recurrent multilayer perceptron networks.
  prefs: []
  type: TYPE_NORMAL
- en: From ignorance to knowledge – learning process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Neural networks learn by adjusting the connections between the neurons, namely
    the weights. As mentioned in the neural structure section, weights represent the
    neural network knowledge. Different weights cause the network to produce different
    results for the same inputs. So, a neural network can improve its results by adapting
    its weights according to a learning rule. The general schema of learning is depicted
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![From ignorance to knowledge – learning process](img/B05964_01_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The process depicted in the previous figure is called **supervised learning**
    because there is a desired output, but neural networks can also learn by the input
    data, without any desired output (supervision). In [Chapter 2](ch02.xhtml "Chapter 2. Getting
    Neural Networks to Learn"), *Getting Neural Networks to Learn*, we are going to
    dive deeper into the neural network learning process.
  prefs: []
  type: TYPE_NORMAL
- en: Let the coding begin! Neural networks in practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, we will cover the entire process of implementing a neural network
    by using the Java programming language. Java is an object-oriented programming
    language that was created in the 1990s by a small group of engineers from Sun
    Microsystems, later acquired by Oracle in the 2010s. Nowadays, Java is present
    in many devices that are part of our daily life.
  prefs: []
  type: TYPE_NORMAL
- en: 'In an object-oriented language, such as Java, we deal with classes and objects.
    A class is a blueprint of something in the real world, and an object is an instance
    of this blueprint, something like a car (class referring to all and any car) and
    my car (object referring to a specific car—mine). Java classes are usually composed
    of attributes and methods (or functions), that include **objects-oriented programming**
    (**OOP**) concepts. We are going to briefly review all of these concepts without
    diving deeper into them, since the goal of this book is just to design and create
    neural networks from a practical point of view. Four concepts are relevant and
    need to be considered in this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Abstraction**: The transcription of a real-world problem or rule into a computer
    programming domain, considering only its relevant features and dismissing the
    details that often hinder development.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encapsulation**: Analogous to a product encapsulation by which some relevant
    features are disclosed openly (public methods), while others are kept hidden within
    their domain (private or protected), therefore avoiding misuse or excess of information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inheritance**: In the real world, multiple classes of objects share attributes
    and methods in a hierarchical manner; for example, a vehicle can be a superclass
    for car and truck. So, in OOP, this concept allows one class to inherit all features
    from another one, thereby avoiding the rewriting of code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Polymorphism**: Almost the same as inheritance, but with the difference that
    methods with the same signature present different behaviors on different classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using the neural network concepts presented in this chapter and the OOP concepts,
    we are now going to design the very first class set that implements a neural network.
    As could be seen, a neural network consists of layers, neurons, weights, activation
    functions, and biases. About layers, there are three types of them: input, hidden,
    and output. Each layer may have one or more neurons. Each neuron is connected
    either to a neural input/output or to another neuron, and these connections are
    known as weights.'
  prefs: []
  type: TYPE_NORMAL
- en: It is important to highlight that a neural network may have many hidden layers
    or none, because the number of neurons in each layer may vary. However, the input
    and output layers have the same number of neurons as the number of neural inputs/outputs,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s start implementing. Initially, we are going to define the following
    classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Neuron**: Defines the artificial neuron'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NeuralLayer**: Abstract class that defines a layer of neurons'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**InputLayer**: Defines the neural input layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HiddenLayer**: Defines the layers between input and output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OutputLayer**: Defines the neural output layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**InputNeuron**: Defines the neuron that is present at the neural network input'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NeuralNet**: Combines all previous classes into one ANN structure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition to these classes, we should also define an `IActivationFunction`
    interface for activation functions. This is necessary because `Activation` functions
    will behave like methods, but they will need to be assigned as a neuron property.
    So we are going to define classes for activation functions that implement this
    interface:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sigmoid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HyperTan
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our first chapter coding is almost complete. We need to define two more classes.
    One for handling eventually thrown exceptions (`NeuralException`) and another
    to generate random numbers (`RandomNumberGenerator`). Finally, we are going to
    separate these classes into two packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '`edu.packt.neuralnet`: For the neural network related classes (`NeuralNet`,
    `Neuron`, `NeuralLayer`, and so on)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`edu.packt.neuralnet.math`: For the math related classes (`IActivationFunction`,
    `Linear`, and so on)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To save space, we are not going to write the full description of each class,
    instead we are going to address the key features of most important classes. However,
    the reader is welcomed to take a glance at the Javadoc documentation of the code,
    in order to get more details on the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: The neuron class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is the very foundation class for this chapter''s code. According to the
    theory, an artificial neuron has the following attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: Inputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is also important to define one attribute that will be useful in future
    examples, that is the output before activation function. We then have the implementation
    of the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When instantiating a neuron, we need to specify how many inputs are going to
    feed values to it, and what should be its activation function. So let''s take
    a look on the constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we define one extra weight for the bias. One important step is the
    initialization of the neuron, that is, how the weights receive their first values.
    This is defined in the `init()` method, by which weights receive randomly generated
    values by the `RandomNumberGenerator static` class. Note the need to prevent an
    attempt to set a value outside the bounds of the weight array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s take a look on how the output values are calculated in the
    `calc()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note that first, the products of all inputs and weights are summed (the bias
    multiplies the last weight `– i==numberOfInputs`), and this value is saved in
    the `outputBeforeActivation` property. The activation function calculates the
    neuron's output with this value.
  prefs: []
  type: TYPE_NORMAL
- en: The NeuralLayer class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this class we are going to group the neurons that are aligned in the same
    layer. Also, there is a need to define links between layers, since one layer forwards
    values to another. So the class will have the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that this class is abstract, the layer classes that can be instantiated
    are `InputLayer`, `HiddenLayer`, and `OutputLayer`. In order to create one layer,
    one must use one of these classes'' constructors that work quite similar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Layers are initialized and calculated as well as the neurons, they also implement
    the methods `init()` and `calc()`. The signature protected guarantees that only
    the subclasses can call or override these methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The ActivationFunction interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we define the `NeuralNetwork` class, let''s take a look at an example
    of Java code with interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The `calc()` signature method is used by a specific Activation Function that
    implements this interface, the `Sigmoid` function, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This is one example of polymorphism, whereby a class or method may present different
    behavior, but yet under the same signature, allowing a flexible application.
  prefs: []
  type: TYPE_NORMAL
- en: The neural network class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, let''s define the neural network class. It has been known so far that
    neural networks organize neurons in layers, and every neural network has at least
    two layers, one for gathering the inputs and one for processing the outputs, and
    a variable number of hidden layers. Therefore our `NeuralNet` class will have
    these properties, in addition to other properties similar to the neuron and the
    `NeuralLayer` classes, such as `numberOfInputs`, `numberOfOutputs`, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The constructor of this class has more arguments than the previous classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Provided that the number of hidden layers is variable, we should take into account
    that there may be many hidden layers or none, and in each of them there will be
    a variable number of hidden neurons. So the best way to deal with this variability
    is to represent the quantity of neurons in each hidden layer as a vector of integers
    (argument `numberofhiddenlayers`). Moreover, one needs to define the activation
    functions for each hidden layer, and for the output layer as well, to that goal
    serve the arguments `hiddenActivationFnc` and `outputAcFnc`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To save space in this chapter we are not going to show the full implementation
    of this constructor, but we can show the example for the definition of layers
    and the links between them. First, the input layer is defined observing the number
    of inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'A hidden layer will be defined depending on its position, if it is right after
    the input layer, the definition is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Or else it will get the reference of the previous hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As for the output layer, the definition is very similar to the latter case,
    except for the `OutputLayer` class and the fact that there may be no hidden layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `calc()` method executes the forwarding flow of signals from the input
    to the output end:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In appendix C, we present the reader the full documentation of the classes along
    with their UML class and package diagrams that will surely help as a reference
    for this book.
  prefs: []
  type: TYPE_NORMAL
- en: Time to play!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s apply these classes and get some results. The following code has
    a `test` class, a `main` method with an object of the `NeuralNet` class called
    `nn`. We are going to define a simple neural network with two inputs, one output,
    and one hidden layer containing three neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Still in this code, let''s feed to the neural network two sets of data, and
    let''s see what output it is going to produce:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This code gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Time to play!](img/B05964_01_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s relevant to remember that each time that the code runs, it generates
    new pseudo random weight values, unless you work with the same seed value. If
    you run the code exactly as provided here, the same values will appear in console:'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've seen an introduction to the neural networks, what they
    are, what they are used for, and their basic concepts. We've also seen a very
    basic implementation of a neural network in the Java programming language, wherein
    we applied the theoretical neural network concepts in practice, by coding each
    of the neural network elements. It's important to understand the basic concepts
    before we move on to advanced concepts. The same applies to the code implemented
    with Java.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will delve into the learning process of a neural network
    and explore the different types of leaning with simple examples.
  prefs: []
  type: TYPE_NORMAL
