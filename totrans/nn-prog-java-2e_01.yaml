- en: Chapter 1. Getting Started with Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章. 神经网络入门
- en: 'In this chapter, we will introduce neural networks and what they are designed
    for. This chapter serves as a foundation layer for the subsequent chapters, while
    presenting the basic concepts for neural networks. In this chapter, we will cover
    the following:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍神经网络及其设计目的。本章作为后续章节的基础层，同时介绍了神经网络的基本概念。在本章中，我们将涵盖以下内容：
- en: Artificial neurons
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工神经元
- en: Weights and biases
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重和偏差
- en: Activation functions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数
- en: Layers of neurons
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元层
- en: Neural network implementation in Java
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java中的神经网络实现
- en: Discovering neural networks
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现神经网络
- en: By hearing the term **neural networks** we intuitively create a snapshot of
    a brain in our minds, and indeed that's right, if we consider the brain to be
    a big and natural neural network. However, what about **artificial neural networks**
    (**ANNs**)? Well, here comes an opposite word to natural, and the first thing
    now that comes into our head is an image of an artificial brain or a robot, given
    the term *artificial*. In this case, we also deal with creating a structure similar
    to and inspired by the human brain; therefore, this can be called artificial intelligence.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们听到**神经网络**这个词时，我们会在脑海中自然地浮现出一个大脑的图像，确实如此，如果我们把大脑看作是一个庞大而自然的神经网络的话。然而，关于**人工神经网络**（**ANNs**）又是怎样的呢？好吧，这里有一个与自然相对的词，鉴于“人工”这个词，我们首先想到的可能是人工大脑或机器人的形象。在这种情况下，我们也在处理创建一个类似于并受人类大脑启发的结构；因此，这可以被称为人工智能。
- en: Now the reader newly introduced to ANN may be thinking that this book teaches
    how to build intelligent systems, including an artificial brain, capable of emulating
    the human mind using Java codes, isn't it? The amazing answer is yes, but of course,
    we will not cover the creation of artificial thinking machines such as those from
    the Matrix trilogy movies; the reader will be provided a walkthrough on the process
    of designing artificial neural network solutions capable of abstracting knowledge
    in raw data, taking advantage of the entire Java programming language framework.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现在对于刚刚接触ANN的读者来说，可能会想这本书是教我们如何构建智能系统，包括一个能够使用Java代码模拟人类思维的的人工大脑，不是吗？令人惊讶的答案是肯定的，但当然，我们不会涵盖创造像《黑客帝国》三部曲电影中的人工思考机器这样的东西；读者将获得设计人工神经网络解决方案的流程指导，这些解决方案能够从原始数据中抽象知识，利用整个Java编程语言框架。
- en: Why artificial neural networks?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么是人工神经网络？
- en: We cannot begin talking about neural networks without understanding their origins,
    including the term as well. The terms neural networks (NN) and ANN are used as
    synonyms in this book, despite NNs being more general, covering the natural neural
    networks as well. So, what actually is an ANN? Let's explore a little of the history
    of this term.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在谈论神经网络之前，不能不了解它们的起源，包括这个术语。尽管NN（神经网络）比ANN（人工神经网络）更通用，涵盖了自然神经网络，但在这本书中，NN和ANN被用作同义词。那么，实际上ANN究竟是什么呢？让我们探索一下这个术语的历史。
- en: 'In the 1940s, the neurophysiologist Warren McCulloch and the mathematician
    Walter Pitts designed the first mathematical implementation of an artificial neuron
    combining the neuroscience foundations with mathematical operations. At that time,
    the human brain was being studied largely to understand its hidden and mystery
    behaviors, yet within the field of neuroscience. The natural neuron structure
    was known to have a nucleus, dendrites receiving incoming signals from other neurons,
    and an axon activating a signal to other neurons, as shown in the following figure:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在20世纪40年代，神经生理学家Warren McCulloch和数学家Walter Pitts设计了第一个人工神经元的数学实现，将神经科学基础与数学运算相结合。当时，人类大脑主要被研究以了解其隐藏和神秘的行为，尤其是在神经科学领域。已知的自然神经元结构包括一个细胞核，树突接收来自其他神经元的传入信号，以及轴突激活信号到其他神经元，如下面的图所示：
- en: '![Why artificial neural networks?](img/B05964_01_01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![为什么是人工神经网络？](img/B05964_01_01.jpg)'
- en: 'The novelty of McCulloch and Pitts was the math component included in the neuron
    model, supposing a neuron as a simple processor summing all incoming signals and
    activating a new signal to other neurons:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: McCulloch和Pitts的创新之处在于神经元模型中包含的数学成分，假设神经元是一个简单的处理器，将所有传入的信号求和，并激活一个新信号到其他神经元：
- en: '![Why artificial neural networks?](img/B05964_01_02.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![为什么是人工神经网络？](img/B05964_01_02.jpg)'
- en: Furthermore, considering that the brain is composed of billions of neurons,
    each one interconnected with another tens of thousands, resulting in some trillions
    of connections, we are talking about a giant network structure. On the basis of
    this fact, McCulloch and Pitts designed a simple model for a single neuron, initially
    to simulate the human vision. The available calculators or computers at that time
    were very rare, but capable of dealing with mathematical operations quite well;
    on the other hand, even tasks today such as vision and sound recognition are not
    easily programmed without the use of special frameworks, as opposed to the mathematical
    operations and functions. Nevertheless, the human brain can perform sound and
    image recognition more efficiently than complex mathematical calculations, and
    this fact really intrigues scientists and researchers.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，考虑到大脑由数十亿个神经元组成，每个神经元与其他数万个神经元相互连接，从而形成数万亿个连接，我们正在谈论一个巨大的网络结构。基于这一事实，麦卡洛克和皮茨设计了一个简单的神经元模型，最初是为了模拟人类视觉。当时可用的计算器或计算机非常罕见，但能够很好地处理数学运算；另一方面，即使像视觉和声音识别这样的任务，如果没有使用特殊框架，也难以编程，这与数学运算和函数不同。尽管如此，人脑在声音和图像识别方面的效率比复杂的数学计算要高，这一事实确实让科学家和研究人员感到好奇。
- en: 'However, one known fact is that all complex activities that the human brain
    performs are based on learned knowledge, so as a solution to overcome the difficulty
    that conventional algorithmic approaches face in addressing these tasks easily
    solved by humans, an ANN is designed to have the capability to learn how to solve
    some task by itself, based on its stimuli (data):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一个已知的事实是，人类大脑执行的所有复杂活动都是基于学习知识，因此为了克服传统算法方法在处理人类容易解决的问题时遇到的困难，人工神经网络被设计成具有通过其刺激（数据）学习如何自行解决某些任务的能力：
- en: '| Tasks Quickly Solvable by Humans | Tasks Quickly Solvable by Computers |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 人类快速可解任务 | 计算机快速可解任务 |'
- en: '| --- | --- |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Classification of imagesVoice recognitionFace identificationForecast events
    on the basis of experience | Complex calculationGrammatical error correctionSignal
    processingOperating system management |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 图像分类语音识别面部识别基于经验预测事件 | 复杂计算语法错误纠正信号处理操作系统管理 |'
- en: How neural networks are arranged
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络的排列方式
- en: By taking into account the human brain characteristics, it can be said that
    the ANN is a nature-inspired approach, and so is its structure. One neuron connects
    to a number of others that connect to another number of neurons, thus being a
    highly interconnected structure. Later in this book, it will be shown that this
    connectivity between neurons accounts for the capability of learning, since every
    connection is configurable according to the stimuli and the desired goal.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到人脑的特点，可以说人工神经网络（ANN）是一种受自然界启发的途径，其结构也是如此。一个神经元连接到许多其他神经元，这些神经元又连接到其他神经元，因此形成一个高度互联的结构。本书后面将展示，这种神经元之间的连接性是学习能力的来源，因为每个连接都可以根据刺激和期望的目标进行配置。
- en: The very basic element – artificial neuron
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最基本的元素——人工神经元
- en: 'Let''s explore the most basic artificial neural element – the artificial neuron.
    Natural neurons have proven to be signal processors since they receive micro signals
    in the dendrites that can trigger a signal in the axon depending on their strength
    or magnitude. We can then think of a neuron as having a signal collector in the
    inputs and an activation unit in the output that can trigger a signal that will
    be forwarded to other neurons, as shown in the following figure:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索最基本的人工神经网络元素——人工神经元。自然神经元已被证明是信号处理器，因为它们在树突中接收微弱的信号，这些信号可以根据其强度或大小在轴突中触发信号。因此，我们可以将神经元视为在输入端有一个信号收集器，在输出端有一个激活单元，可以触发将被转发到其他神经元的信号，如图所示：
- en: '![The very basic element – artificial neuron](img/B05964_01_03.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![最基本的元素——人工神经元](img/B05964_01_03.jpg)'
- en: Tip
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: In natural neurons, there is a threshold potential that when reached, fires
    the axon and propagates the signal to the other neurons. This firing behavior
    is emulated with activation functions, which has proven to be useful in representing
    nonlinear behaviors in the neurons.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然神经元中，存在一个阈值电位，当达到这个电位时，轴突会放电并将信号传播到其他神经元。这种放电行为通过激活函数来模拟，这已被证明在表示神经元中的非线性行为方面是有用的。
- en: Giving life to neurons – activation function
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为神经元注入生命——激活函数
- en: This activation function is what fires the neuron's output, based on the sum
    of all incoming signals. Mathematically it adds nonlinearity to neural network
    processing, thereby providing the artificial neuron nonlinear behaviors, which
    will be very useful in emulating the nonlinear nature of natural neurons. An activation
    function is usually bounded between two values at the output, therefore being
    a nonlinear function, but in some special cases, it can be a linear function.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这个激活函数是根据所有输入信号的求和来触发神经元的输出的。从数学上讲，它为神经网络处理添加了非线性，从而为人工神经元提供了非线性行为，这在模拟自然神经元的非线性特性时非常有用。激活函数通常在输出端被限制在两个值之间，因此是一个非线性函数，但在某些特殊情况下，它也可以是一个线性函数。
- en: 'Although any function can be used as activation, let''s concentrate on common
    used ones:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然任何函数都可以用作激活函数，但让我们集中关注常用的那些：
- en: '| Function | Equation | Chart |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 函数 | 公式 | 图表 |'
- en: '| --- | --- | --- |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Sigmoid | ![Giving life to neurons – activation function](img/B05964_01_04_01.jpg)
    | ![Giving life to neurons – activation function](img/B05964_01_04.jpg) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| Sigmoid | ![为神经元注入活力 – 激活函数](img/B05964_01_04_01.jpg) | ![为神经元注入活力 – 激活函数](img/B05964_01_04.jpg)
    |'
- en: '| Hyperbolic tangent | ![Giving life to neurons – activation function](img/B05964_01_05_01.jpg)
    | ![Giving life to neurons – activation function](img/B05964_01_05.jpg) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 双曲正切 | ![为神经元注入活力 – 激活函数](img/B05964_01_05_01.jpg) | ![为神经元注入活力 – 激活函数](img/B05964_01_05.jpg)
    |'
- en: '| Hard limiting threshold | ![Giving life to neurons – activation function](img/b05964_01_06_01.jpg)
    | ![Giving life to neurons – activation function](img/b05964_01_06.jpg) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 硬限制阈值 | ![为神经元注入活力 – 激活函数](img/b05964_01_06_01.jpg) | ![为神经元注入活力 – 激活函数](img/b05964_01_06.jpg)
    |'
- en: '| Linear | ![Giving life to neurons – activation function](img/b05964_01_07_01.jpg)
    | ![Giving life to neurons – activation function](img/b05964_01_07.jpg) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 线性 | ![为神经元注入活力 – 激活函数](img/b05964_01_07_01.jpg) | ![为神经元注入活力 – 激活函数](img/b05964_01_07.jpg)
    |'
- en: In these equations and charts the coefficient a can be chosen as a setting for
    the activation function.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些方程和图表中，系数a可以作为激活函数的设置。
- en: The flexible values – weights
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 灵活的值 – 权重
- en: While the neural network structure can be fixed, weights represent the connections
    between neurons and they have the capability to amplify or attenuate incoming
    neural signals, thus modifying them and having the power to influence a neuron's
    output. Hence a neuron's activation will not be dependent on only the inputs,
    but on the weights too. Provided that the inputs come from other neurons or from
    the external world (stimuli), the weights are considered to be a neural network's
    established connections between its neurons. Since the weights are an internal
    neural network component and influence its outputs, they can be considered as
    neural network knowledge, provided that changing the weights will change the neural
    network's outputs, that is, its answers to external stimuli.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当神经网络结构可以固定时，权重代表神经元之间的连接，并且它们有能力放大或衰减传入的神经信号，从而修改它们，并具有影响神经元输出的能力。因此，神经元的激活不仅依赖于输入，还依赖于权重。假设输入来自其他神经元或外部世界（刺激），权重被认为是神经网络神经元之间建立的联系。由于权重是神经网络的一个内部组件，并影响其输出，因此它们可以被认为是神经网络的知识，前提是改变权重将改变神经网络的输出，即对外部刺激的回答。
- en: An extra parameter – bias
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个额外的参数 – 偏置
- en: 'It is useful for the artificial neuron to have an independent component that
    adds an extra signal to the activation function: the **bias**. This parameter
    acts like an input, except for the fact that it is stimulated by one fixed value
    (usually 1), which is multiplied by an associated weight. This feature helps in
    the neural network knowledge representation as a more purely nonlinear system,
    provided that when all inputs are zero, that neuron won''t necessarily produce
    a zero at the output, instead it can fire a different value according to the bias
    associated weight.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人工神经元来说，拥有一个独立的组件向激活函数添加额外信号是有用的：**偏置**。这个参数的作用像一个输入，除了它是由一个固定的值（通常是1）刺激，并且这个值乘以一个相关的权重。这个特性有助于神经网络知识表示作为一个更纯粹的非线性系统，前提是当所有输入都为零时，那个神经元不一定会产生零的输出，相反，它可以根据相关的偏置权重产生不同的值。
- en: The parts forming the whole – layers
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构成整体的各个部分 – 层
- en: 'In order to abstract levels of processing, as our mind does, neurons are organized
    in layers. The input layer receives direct stimuli from the outside world, and
    the output layers fire actions that will have a direct influence on the outside
    world. Between these layers, there are a number of hidden layers, in the sense
    that they are invisible (hidden) from the outside world. In artificial neural
    networks, a layer has the same inputs and activation function for all its composing
    neurons, as shown in the following figure:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了抽象出与我们的思维相似的处理层次，神经元被组织成层次。输入层接收来自外部世界的直接刺激，输出层触发将直接影响外部世界的动作。在这些层次之间，存在多个隐藏层，从外部世界的角度来看，它们是不可见的（隐藏的）。在人工神经网络中，一个层次的所有组成神经元具有相同的输入和激活函数，如图所示：
- en: '![The parts forming the whole – layers](img/B05964_01_08.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![构成整体的部分 - 层次](img/B05964_01_08.jpg)'
- en: Neural networks can be composed of several linked layers, forming the so-called
    **multilayer networks**. Neural layers can then be classified as *Input*, *Hidden*,
    or *Output*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可以由几个相互连接的层次组成，形成所谓的**多层网络**。神经网络层可以被分类为*输入*、*隐藏*或*输出*。
- en: In practice, an additional neural layer enhances the neural network's capacity
    to represent more complex knowledge.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，增加一个额外的神经网络层次可以增强神经网络表示更复杂知识的能力。
- en: Tip
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Every neural network has at least an input/output layer irrespective of the
    number of layers. In the case of a multilayer network, the layers between the
    input and the output are called **hidden**
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 任何神经网络至少都有一个输入/输出层，无论层数多少。在多层网络的情况下，输入和输出之间的层被称为**隐藏层**
- en: Learning about neural network architectures
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解神经网络架构
- en: A neural network can have different layouts, depending on how the neurons or
    layers are connected to each other. Each neural network architecture is designed
    for a specific goal. Neural networks can be applied to a number of problems, and
    depending on the nature of the problem, the neural network should be designed
    in order to address this problem more efficiently.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可以有不同的布局，这取决于神经元或层次如何相互连接。每个神经网络架构都是为了特定的目标而设计的。神经网络可以应用于许多问题，并且根据问题的性质，神经网络应该被设计得更加高效地解决这个问题。
- en: 'Neural network architectures classification is two-fold:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络架构的分类有两方面：
- en: Neuron connections
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元连接
- en: Monolayer networks
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单层网络
- en: Multilayer networks
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多层网络
- en: Signal flow
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信号流
- en: Feedforward networks
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈网络
- en: Feedback networks
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反馈网络
- en: Monolayer networks
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单层网络
- en: 'In this architecture, all neurons are laid out in the same level, forming one
    single layer, as shown in the following figure:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种架构中，所有神经元都布局在同一级别，形成一个单独的层次，如图所示：
- en: '![Monolayer networks](img/B05964_01_09.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![单层网络](img/B05964_01_09.jpg)'
- en: The neural network receives the input signals and feeds them into the neurons,
    which in turn produce the output signals. The neurons can be highly connected
    to each other with or without recurrence. Examples of these architectures are
    the single-layer perceptron, Adaline, self-organizing map, Elman, and Hopfield
    neural networks.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络接收输入信号并将它们输入到神经元中，神经元随后产生输出信号。神经元可以高度相互连接，有或没有循环。这些架构的例子包括单层感知器、Adaline、自组织映射、Elman和Hopfield神经网络。
- en: Multilayer networks
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多层网络
- en: 'In this category, neurons are divided into multiple layers, each layer corresponding
    to a parallel layout of neurons that shares the same input data, as shown in the
    following figure:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个类别中，神经元被分为多个层次，每个层次对应于一个共享相同输入数据的神经元并行布局，如图所示：
- en: '![Multilayer networks](img/B05964_01_10.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![多层网络](img/B05964_01_10.jpg)'
- en: Radial basis functions and multilayer perceptrons are good examples of this
    architecture. Such networks are really useful for approximating real data to a
    function especially designed to represent that data. Moreover, because they have
    multiple layers of processing, these networks are adapted to learn from nonlinear
    data, being able to separate it or determine more easily the knowledge that reproduces
    or recognizes this data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 径向基函数和多层感知器是这个架构的好例子。这类网络特别适用于将实际数据逼近到专门设计用来表示该数据的函数。此外，由于它们具有多个处理层次，这些网络适合于从非线性数据中学习，能够更容易地分离它或确定再现或识别这些数据的知识。
- en: Feedforward networks
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前馈网络
- en: The flow of the signals in neural networks can be either in only one direction
    or in recurrence. In the first case, we call the neural network architecture feedforward,
    since the input signals are fed into the input layer; then, after being processed,
    they are forwarded to the next layer, just as shown in the figure in the multilayer
    section. Multilayer perceptrons and radial basis functions are also good examples
    of feedforward networks.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中信号的流动可以是单向的，也可以是循环的。在前一种情况下，我们称神经网络架构为前馈，因为输入信号被输入层接收；然后，在处理之后，它们被转发到下一层，正如多层部分中的图所示。多层感知器和径向基函数也是前馈网络的良好例子。
- en: Feedback networks
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反馈网络
- en: 'When the neural network has some kind of internal recurrence, it means that
    the signals are fed back in a neuron or layer that has already received and processed
    that signal, the network is of the feedback type. See the following figure of
    feedback networks:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当神经网络具有某种内部循环时，这意味着信号被反馈到一个已经接收并处理过该信号的神经元或层，这种网络是反馈类型的。参见以下反馈网络图：
- en: '![Feedback networks](img/B05964_01_11.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![反馈网络](img/B05964_01_11.jpg)'
- en: The special reason to add recurrence in the network is the production of a dynamic
    behavior, particularly when the network addresses problems involving time series
    or pattern recognition, which require an internal memory to reinforce the learning
    process. However, such networks are particularly difficult to train, because there
    will eventually be a recursive behavior during the training (for example, a neuron
    whose outputs are fed back into its inputs), in addition to the arrangement of
    data for training. Most of the feedback networks are single layer, such as Elman
    and Hopfield networks, but it is possible to build a recurrent multilayer network,
    such as echo and recurrent multilayer perceptron networks.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络中添加循环的特殊原因是为了产生动态行为，尤其是在网络处理涉及时间序列或模式识别的问题时，这些问题需要内部记忆来强化学习过程。然而，这种网络特别难以训练，因为在训练过程中最终会出现递归行为（例如，一个其输出被反馈到其输入的神经元），除了为训练安排数据之外。大多数反馈网络是单层，例如Elman和Hopfield网络，但也可以构建循环多层网络，例如回声和循环多层感知器网络。
- en: From ignorance to knowledge – learning process
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从无知到知识——学习过程
- en: 'Neural networks learn by adjusting the connections between the neurons, namely
    the weights. As mentioned in the neural structure section, weights represent the
    neural network knowledge. Different weights cause the network to produce different
    results for the same inputs. So, a neural network can improve its results by adapting
    its weights according to a learning rule. The general schema of learning is depicted
    in the following figure:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络通过调整神经元之间的连接来学习，即权重。如神经结构部分所述，权重代表神经网络知识。不同的权重会导致网络对相同的输入产生不同的结果。因此，神经网络可以通过根据学习规则调整其权重来提高其结果。学习的一般方案如图所示：
- en: '![From ignorance to knowledge – learning process](img/B05964_01_12.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![从无知到知识——学习过程](img/B05964_01_12.jpg)'
- en: The process depicted in the previous figure is called **supervised learning**
    because there is a desired output, but neural networks can also learn by the input
    data, without any desired output (supervision). In [Chapter 2](ch02.xhtml "Chapter 2. Getting
    Neural Networks to Learn"), *Getting Neural Networks to Learn*, we are going to
    dive deeper into the neural network learning process.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 上图所示的过程被称为**监督学习**，因为存在一个期望的输出，但神经网络也可以通过输入数据来学习，而不需要任何期望的输出（监督）。在[第2章](ch02.xhtml
    "第2章。让神经网络学习")《让神经网络学习》中，我们将更深入地探讨神经网络的学习过程。
- en: Let the coding begin! Neural networks in practice
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让编码开始！神经网络在实践中的应用
- en: In this book, we will cover the entire process of implementing a neural network
    by using the Java programming language. Java is an object-oriented programming
    language that was created in the 1990s by a small group of engineers from Sun
    Microsystems, later acquired by Oracle in the 2010s. Nowadays, Java is present
    in many devices that are part of our daily life.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将使用Java编程语言来实现神经网络的全过程。Java是一种在20世纪90年代由Sun Microsystems的一小群工程师创建的面向对象编程语言，后来在2010年代被Oracle收购。如今，Java存在于我们日常生活中的许多设备中。
- en: 'In an object-oriented language, such as Java, we deal with classes and objects.
    A class is a blueprint of something in the real world, and an object is an instance
    of this blueprint, something like a car (class referring to all and any car) and
    my car (object referring to a specific car—mine). Java classes are usually composed
    of attributes and methods (or functions), that include **objects-oriented programming**
    (**OOP**) concepts. We are going to briefly review all of these concepts without
    diving deeper into them, since the goal of this book is just to design and create
    neural networks from a practical point of view. Four concepts are relevant and
    need to be considered in this process:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在面向对象的语言，如Java中，我们处理类和对象。类是现实世界中某物的蓝图，而对象是这个蓝图的实例，就像一辆车（类指的是所有和任何车辆）以及我的车（对象指的是特定的车辆——我的车）。Java类通常由属性和方法（或函数）组成，这些方法包括**面向对象编程**（**OOP**）概念。我们将简要回顾所有这些概念，而不会深入探讨它们，因为这本书的目标只是从实际的角度设计和创建神经网络。以下四个概念在此过程中相关且需要考虑：
- en: '**Abstraction**: The transcription of a real-world problem or rule into a computer
    programming domain, considering only its relevant features and dismissing the
    details that often hinder development.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**抽象**：将现实世界问题或规则转录到计算机编程领域，只考虑其相关特征，忽略通常阻碍发展的细节。'
- en: '**Encapsulation**: Analogous to a product encapsulation by which some relevant
    features are disclosed openly (public methods), while others are kept hidden within
    their domain (private or protected), therefore avoiding misuse or excess of information.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**封装**：类似于产品的封装，通过这种方式，一些相关特性被公开披露（公共方法），而其他特性则在其领域内保持隐藏（私有或受保护），从而避免误用或信息过多。'
- en: '**Inheritance**: In the real world, multiple classes of objects share attributes
    and methods in a hierarchical manner; for example, a vehicle can be a superclass
    for car and truck. So, in OOP, this concept allows one class to inherit all features
    from another one, thereby avoiding the rewriting of code.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**继承**：在现实世界中，多个类对象以分层的方式共享属性和方法；例如，车辆可以是汽车和卡车的超类。因此，在OOP中，这个概念允许一个类从另一个类继承所有功能，从而避免代码的重写。'
- en: '**Polymorphism**: Almost the same as inheritance, but with the difference that
    methods with the same signature present different behaviors on different classes.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多态性**：几乎与继承相同，但不同之处在于具有相同签名的方 法在不同的类上表现出不同的行为。'
- en: 'Using the neural network concepts presented in this chapter and the OOP concepts,
    we are now going to design the very first class set that implements a neural network.
    As could be seen, a neural network consists of layers, neurons, weights, activation
    functions, and biases. About layers, there are three types of them: input, hidden,
    and output. Each layer may have one or more neurons. Each neuron is connected
    either to a neural input/output or to another neuron, and these connections are
    known as weights.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用本章中介绍的神经网络概念和OOP概念，我们现在将设计实现神经网络的第一个类集。如所见，神经网络由层、神经元、权重、激活函数和偏差组成。关于层，有三种类型：输入、隐藏和输出。每一层可能有一个或多个神经元。每个神经元要么连接到神经输入/输出，要么连接到另一个神经元，这些连接被称为权重。
- en: It is important to highlight that a neural network may have many hidden layers
    or none, because the number of neurons in each layer may vary. However, the input
    and output layers have the same number of neurons as the number of neural inputs/outputs,
    respectively.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 需要强调的是，神经网络可能有许多隐藏层或没有隐藏层，因为每层的神经元数量可能不同。然而，输入和输出层的神经元数量与神经输入/输出的数量相同。
- en: 'So, let''s start implementing. Initially, we are going to define the following
    classes:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们开始实施。最初，我们将定义以下类：
- en: '**Neuron**: Defines the artificial neuron'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经元**：定义人工神经元'
- en: '**NeuralLayer**: Abstract class that defines a layer of neurons'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经网络层**：抽象类，定义神经元层'
- en: '**InputLayer**: Defines the neural input layer'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：定义神经网络的输入层'
- en: '**HiddenLayer**: Defines the layers between input and output'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**：定义输入和输出之间的层'
- en: '**OutputLayer**: Defines the neural output layer'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**：定义神经网络的输出层'
- en: '**InputNeuron**: Defines the neuron that is present at the neural network input'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入神经元**：定义神经网络输入处的神经元'
- en: '**NeuralNet**: Combines all previous classes into one ANN structure'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经网络**：将所有前面的类组合成一个ANN结构'
- en: 'In addition to these classes, we should also define an `IActivationFunction`
    interface for activation functions. This is necessary because `Activation` functions
    will behave like methods, but they will need to be assigned as a neuron property.
    So we are going to define classes for activation functions that implement this
    interface:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些类之外，我们还应该为激活函数定义一个 `IActivationFunction` 接口。这是必要的，因为 `Activation` 函数将像方法一样行为，但它们需要作为神经元属性被分配。因此，我们将定义实现此接口的激活函数类：
- en: Linear
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linear
- en: Sigmoid
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid
- en: Step
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Step
- en: HyperTan
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HyperTan
- en: 'Our first chapter coding is almost complete. We need to define two more classes.
    One for handling eventually thrown exceptions (`NeuralException`) and another
    to generate random numbers (`RandomNumberGenerator`). Finally, we are going to
    separate these classes into two packages:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一章编码几乎完成了。我们需要定义两个额外的类。一个用于处理可能抛出的异常（`NeuralException`），另一个用于生成随机数（`RandomNumberGenerator`）。最后，我们将这些类分开到两个包中：
- en: '`edu.packt.neuralnet`: For the neural network related classes (`NeuralNet`,
    `Neuron`, `NeuralLayer`, and so on)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`edu.packt.neuralnet`：对于与神经网络相关的类（`NeuralNet`、`Neuron`、`NeuralLayer` 等）'
- en: '`edu.packt.neuralnet.math`: For the math related classes (`IActivationFunction`,
    `Linear`, and so on)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`edu.packt.neuralnet.math`：对于与数学相关的类（`IActivationFunction`、`Linear` 等）'
- en: To save space, we are not going to write the full description of each class,
    instead we are going to address the key features of most important classes. However,
    the reader is welcomed to take a glance at the Javadoc documentation of the code,
    in order to get more details on the implementation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了节省空间，我们不会写出每个类的完整描述，而是将重点放在最重要的类的关键特性上。然而，读者可以查看代码的 Javadoc 文档，以获取更多关于实现的细节。
- en: The neuron class
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经元类
- en: 'This is the very foundation class for this chapter''s code. According to the
    theory, an artificial neuron has the following attributes:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本章代码的非常基础类。根据理论，人工神经元有以下属性：
- en: Inputs
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入
- en: Weights
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重
- en: Bias
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏置
- en: Activation function
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数
- en: Output
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出
- en: 'It is also important to define one attribute that will be useful in future
    examples, that is the output before activation function. We then have the implementation
    of the following properties:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是定义一个将在未来的示例中很有用的属性，即激活函数之前的输出。然后我们有以下属性的实现：
- en: '[PRE0]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When instantiating a neuron, we need to specify how many inputs are going to
    feed values to it, and what should be its activation function. So let''s take
    a look on the constructor:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当实例化一个神经元时，我们需要指定将要为其提供值的输入数量，以及它应该使用的激活函数。因此，让我们看看构造函数：
- en: '[PRE1]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Note that we define one extra weight for the bias. One important step is the
    initialization of the neuron, that is, how the weights receive their first values.
    This is defined in the `init()` method, by which weights receive randomly generated
    values by the `RandomNumberGenerator static` class. Note the need to prevent an
    attempt to set a value outside the bounds of the weight array:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们为偏置定义了一个额外的权重。一个重要的步骤是神经元的初始化，即权重如何获得它们的第一个值。这通过 `init()` 方法定义，其中权重通过 `RandomNumberGenerator
    static` 类随机生成值。注意需要防止尝试设置超出权重数组范围的值：
- en: '[PRE2]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, let''s take a look on how the output values are calculated in the
    `calc()` method:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看在 `calc()` 方法中如何计算输出值：
- en: '[PRE3]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that first, the products of all inputs and weights are summed (the bias
    multiplies the last weight `– i==numberOfInputs`), and this value is saved in
    the `outputBeforeActivation` property. The activation function calculates the
    neuron's output with this value.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，首先，所有输入和权重的乘积（偏置乘以最后一个权重 `– i==numberOfInputs`）被求和，这个值被保存在 `outputBeforeActivation`
    属性中。激活函数使用这个值来计算神经元的输出。
- en: The NeuralLayer class
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经层类
- en: 'In this class we are going to group the neurons that are aligned in the same
    layer. Also, there is a need to define links between layers, since one layer forwards
    values to another. So the class will have the following properties:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个类中，我们将把在同一层中排列的神经元分组。此外，还需要定义层之间的链接，因为一个层将值传递给另一个层。因此，该类将具有以下属性：
- en: '[PRE4]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Note that this class is abstract, the layer classes that can be instantiated
    are `InputLayer`, `HiddenLayer`, and `OutputLayer`. In order to create one layer,
    one must use one of these classes'' constructors that work quite similar:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个类是抽象的，可以实例化的层类有 `InputLayer`、`HiddenLayer` 和 `OutputLayer`。为了创建一个层，必须使用这些类中的一个构造函数，它们的工作方式相当相似：
- en: '[PRE5]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Layers are initialized and calculated as well as the neurons, they also implement
    the methods `init()` and `calc()`. The signature protected guarantees that only
    the subclasses can call or override these methods:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 层以及神经元一样被初始化和计算，它们也实现了`init()`和`calc()`方法。受保护的签名保证只有子类可以调用或覆盖这些方法：
- en: '[PRE6]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The ActivationFunction interface
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数接口
- en: 'Before we define the `NeuralNetwork` class, let''s take a look at an example
    of Java code with interface:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们定义`NeuralNetwork`类之前，让我们看看一个带有接口的Java代码示例：
- en: '[PRE7]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `calc()` signature method is used by a specific Activation Function that
    implements this interface, the `Sigmoid` function, for example:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`calc()`签名方法由实现此接口的特定激活函数使用，例如`Sigmoid`函数：'
- en: '[PRE8]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This is one example of polymorphism, whereby a class or method may present different
    behavior, but yet under the same signature, allowing a flexible application.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这是多态的一个例子，其中类或方法可以呈现不同的行为，但仍然在相同的签名下，允许灵活的应用。
- en: The neural network class
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络类
- en: 'Finally, let''s define the neural network class. It has been known so far that
    neural networks organize neurons in layers, and every neural network has at least
    two layers, one for gathering the inputs and one for processing the outputs, and
    a variable number of hidden layers. Therefore our `NeuralNet` class will have
    these properties, in addition to other properties similar to the neuron and the
    `NeuralLayer` classes, such as `numberOfInputs`, `numberOfOutputs`, and so on:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们定义神经网络类。到目前为止，我们知道神经网络将神经元组织成层，每个神经网络至少有两个层，一个用于收集输入，一个用于处理输出，以及可变数量的隐藏层。因此，我们的`NeuralNet`类将具有这些属性，以及其他与神经元和`NeuralLayer`类类似的属性，例如`numberOfInputs`、`numberOfOutputs`等：
- en: '[PRE9]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The constructor of this class has more arguments than the previous classes:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类的构造函数比之前的类有更多的参数：
- en: '[PRE10]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Provided that the number of hidden layers is variable, we should take into account
    that there may be many hidden layers or none, and in each of them there will be
    a variable number of hidden neurons. So the best way to deal with this variability
    is to represent the quantity of neurons in each hidden layer as a vector of integers
    (argument `numberofhiddenlayers`). Moreover, one needs to define the activation
    functions for each hidden layer, and for the output layer as well, to that goal
    serve the arguments `hiddenActivationFnc` and `outputAcFnc`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 假设隐藏层的数量是可变的，我们应该考虑到可能存在许多隐藏层或没有隐藏层，在每个隐藏层中将有可变数量的隐藏神经元。因此，处理这种可变性的最佳方式是将每个隐藏层中的神经元数量表示为整数向量（参数`numberofhiddenlayers`）。此外，还需要为每个隐藏层以及输出层定义激活函数，为此提供参数`hiddenActivationFnc`和`outputAcFnc`。
- en: 'To save space in this chapter we are not going to show the full implementation
    of this constructor, but we can show the example for the definition of layers
    and the links between them. First, the input layer is defined observing the number
    of inputs:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在本章中节省空间，我们不会展示这个构造函数的完整实现，但我们可以展示定义层及其之间链接的示例。首先，根据输入的数量定义输入层：
- en: '[PRE11]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'A hidden layer will be defined depending on its position, if it is right after
    the input layer, the definition is as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层的定义将取决于其位置，如果它紧接在输入层之后，其定义如下：
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Or else it will get the reference of the previous hidden layer:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，它将获取前一个隐藏层的引用：
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As for the output layer, the definition is very similar to the latter case,
    except for the `OutputLayer` class and the fact that there may be no hidden layers:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输出层，其定义与后一种情况非常相似，只是涉及到`OutputLayer`类，以及可能不存在隐藏层的事实：
- en: '[PRE14]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `calc()` method executes the forwarding flow of signals from the input
    to the output end:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`calc()`方法执行从输入到输出端的信号前向流动：'
- en: '[PRE15]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In appendix C, we present the reader the full documentation of the classes along
    with their UML class and package diagrams that will surely help as a reference
    for this book.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在附录C中，我们向读者展示了类的完整文档，包括它们的UML类图和包图，这无疑将有助于作为本书的参考。
- en: Time to play!
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 是时候玩耍了！
- en: 'Now let''s apply these classes and get some results. The following code has
    a `test` class, a `main` method with an object of the `NeuralNet` class called
    `nn`. We are going to define a simple neural network with two inputs, one output,
    and one hidden layer containing three neurons:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们应用这些类并获取一些结果。以下代码有一个`test`类，一个包含`NeuralNet`类对象`nn`的`main`方法。我们将定义一个简单的神经网络，包含两个输入，一个输出，以及一个包含三个神经元的隐藏层：
- en: '[PRE16]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Still in this code, let''s feed to the neural network two sets of data, and
    let''s see what output it is going to produce:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然在这个代码中，让我们给神经网络输入两组数据，看看它会产生什么输出：
- en: '[PRE17]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This code gives the following output:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码给出了以下输出：
- en: '![Time to play!](img/B05964_01_13.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![玩耍时间！](img/B05964_01_13.jpg)'
- en: 'It''s relevant to remember that each time that the code runs, it generates
    new pseudo random weight values, unless you work with the same seed value. If
    you run the code exactly as provided here, the same values will appear in console:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的是，每次代码运行时，它都会生成新的伪随机权重值，除非你使用相同的种子值。如果你按照这里提供的方式准确运行代码，控制台将显示相同的值：
- en: Summary
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we've seen an introduction to the neural networks, what they
    are, what they are used for, and their basic concepts. We've also seen a very
    basic implementation of a neural network in the Java programming language, wherein
    we applied the theoretical neural network concepts in practice, by coding each
    of the neural network elements. It's important to understand the basic concepts
    before we move on to advanced concepts. The same applies to the code implemented
    with Java.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了神经网络的基本概念，包括它们是什么，它们用于什么，以及它们的基本概念。我们还看到了用Java编程语言实现的一个非常基础的神经网络实例，其中我们通过编码每个神经网络元素来将理论神经网络概念应用于实践。在继续学习高级概念之前，理解基本概念非常重要。这同样适用于用Java实现的代码。
- en: In the next chapter, we will delve into the learning process of a neural network
    and explore the different types of leaning with simple examples.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨神经网络的学习过程，并通过简单的例子来探索不同的学习类型。
