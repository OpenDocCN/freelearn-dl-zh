# 9

# LLM 输出评估

无论你的智能应用的形式如何，你必须评估你使用 **大型语言模型**（**LLMs**）。计算系统的评估决定了系统的性能，衡量其可靠性，并分析其安全性和隐私性。

AI 系统是非确定性的。你无法确定 AI 系统将输出什么，直到你通过它运行输入。这意味着你必须评估 AI 系统在多种输入上的表现，以确信它符合你的要求。为了能够在不引入任何意外回归的情况下更改 AI 系统，你还需要有稳健的评估。评估可以帮助在将 AI 系统发布给客户之前捕捉这些回归。

在 LLM 驱动的智能应用中，评估衡量了所选模型以及与模型一起使用的任何超参数（如温度、提示和 **检索增强生成**（**RAG**）管道）的影响。由于截至 2024 年中写作时，LLMs 仍处于起步阶段，因此关于何时以及如何最好地评估这些 LLM 驱动的智能应用仍存在持续的争论。然而，有一些新兴的最佳实践，你可以用来指导你的评估。

在本章中，你将了解为什么以及如何评估你在智能应用中使用 LLM。你将能够使用所讨论的概念和指标来评估当前类别的智能应用，如聊天机器人，以及新兴的，如 AI 代理。在这里学到的概念将适用于未来几年，无论未来一代智能应用的形式如何。

本章将涵盖以下主题：

+   理解 LLM 评估

+   模型基准测试

+   评估数据集

+   LLM 评估的关键指标

+   人类评审在 LLM 评估中的作用

+   将评估作为你应用的护栏

# 技术要求

运行本章中的代码需要以下技术要求：

+   安装了 Python 3.x 的编程环境。

+   OpenAI API 密钥。要创建 API 密钥，请参阅 OpenAI 文档中的[https://platform.openai.com/docs/quickstart/step-2-set-up-your-api-key](https://platform.openai.com/docs/quickstart/step-2-set-up-your-api-key)。

# 什么是 LLM 评估？

**LLM 评估**，或 **LLM 评估**，是对 LLM 及其使用的智能应用的系统评估过程。这包括对特定任务上的性能、在特定条件下的可靠性、在特定用例中的有效性以及其他标准进行评估，以了解模型的整体能力。你想要确保你的智能应用通过你的评估达到一定的标准。

你还应该能够衡量当改变应用或应用中使用的组件或数据时，人工智能系统性能的变化。例如，如果你想更改应用中使用的LLM或提示，你应该能够通过评估来衡量这些变化的影响。

能够衡量变化的影响对于提高应用质量尤其重要。一旦智能应用“相当不错”，对于人类审阅者来说，评估系统是否以及如何根据变化而改进或退步就变得相当具有挑战性。例如，如果你有一个旅行助手聊天机器人，它90%的时间都能成功满足用户期望，那么评估一个微小变化（将成功率提高到90.5%）对人类审阅者来说可能既具有挑战性又耗时。

当设计用于你基于LLM（大型语言模型）的智能应用的评估套件时，你应该考虑以下方面：

+   **安全性**: 人工智能系统不应泄露它有权访问的任何私人或机密信息。这可能包括LLM权重中的信息以及应用检索到的信息。

+   **声誉**: 人工智能系统不应生成可能损害你业务的输出。例如，在任何情况下，你都不希望你的聊天机器人推荐你的竞争对手的服务而不是你自己的服务。

+   **正确性**: 人工智能系统应以正确输出响应，不包含错误或幻觉。

+   **风格**: 人工智能系统应按照你指定的语气和风格指南进行响应。例如，如果你正在开发一个法律聊天机器人，你可能希望聊天机器人保持正式的语气并使用适当的法律术语。

+   **一致性**: 人工智能系统应生成符合预期的输出。对于相同的输入，你应该期望系统以预定的方式进行操作。响应可以不同，但任何差异都应该是一致的。例如，如果你正在构建一个基于歌曲创建播放列表的系统，你可能会希望它根据输入歌曲生成相似的播放列表，即使输出播放列表中有不同的歌曲或不同的歌曲顺序。

+   **伦理学**: 人工智能系统应与一系列伦理原则保持一致。通过在评估数据集中定义预期行为，你还可以帮助定义系统的伦理标准。例如，人工智能系统绝不应生成具有偏见或歧视性的内容，并且应谨慎、尊重地处理敏感话题。

在下一节中，你将了解在你的应用中应该评估哪些点。你还将回顾一个示例智能应用，该应用在本章的代码示例中用于演示概念。

## 组件和端到端评估

您必须考虑在您的应用程序中**哪里**进行评估。通常，您应该评估系统的所有LLM组件以及端到端系统。

为了说明在您的智能应用程序中考虑评估的例子，本章使用了旅行助手聊天机器人的例子。该聊天机器人使用RAG（检索增强生成）来提供旅行建议并回答基于流行旅游目的地和活动文档数据集的问题。由于本章是关于评估的，因此不会详细介绍应用程序组件的构建方式。在章节的后面部分，您将了解如何评估该应用程序的LLM使用情况。

旅行助手聊天机器人具有以下组件：

+   **检索器**：找到相关的文档以帮助响应用户消息。检索器使用向量搜索来找到相关文档。它还使用LLM来完成以下任务：

    +   **元数据提取器**：从用户查询中提取任何地名。这可以用于预先过滤搜索结果，仅包括关于相关地点的文档。

    +   **查询预处理程序**：将用户消息转换为更好的搜索查询。

    +   **检索文档后处理器**：对检索到的文档进行变异，以创建一个相关事实列表。

+   **相关性护栏**：LLM调用确保用户只与聊天机器人讨论与旅行相关的话题。如果相关性护栏确定用户消息不相关，聊天机器人不会回答用户的不相关问题，并提示用户提出更相关的问题。

+   **响应器**：使用LLM（大型语言模型）根据检索到的内容来响应用户消息。

*图9**.1*展示了这些组件如何协同工作。

![图片](img/B22495_09_01.jpg)

图9.1：旅行助手示例聊天机器人的组件

### 组件评估

您智能应用程序中调用LLM的每个子系统都可以被视为一个**组件**。您应该评估所有组件，因为每个组件都对系统的整体性能有所贡献。通过评估每个组件，您可以确保每个部分都符合所需的质量标准，并且可靠地执行。这还让您更有信心地更换组件，因为您可以清楚地了解这些更改如何影响系统的各个部分。

一个组件也可以包含子组件。您应该对父组件和子组件进行单独评估。例如，在旅行助手聊天机器人中，您应该评估所有使用LLM的独立组件，例如查询预处理程序和响应生成器。您还应该评估检索器，将其三个LLM子组件视为一个组件。

通过评估所有逻辑LLM组件，你可以更好地理解整个系统的行为。这种理解让你在知道这些变化将对其他相关组件产生什么影响的情况下，对单个组件进行修改。

### 端到端评估

**端到端评估**检查整个集成系统的性能。这些评估捕捉到诸如现实世界的适用性、用户体验和系统可靠性等方面的内容。它们有助于识别在单独评估LLM时可能不明显的不确定瓶颈或弱点。

对于RAG系统，这涉及到评估语言模型的输出，以及检索机制的效率、准确性和检索信息的相关性，以及系统如何将外部知识与LLM固有的能力相结合。

在旅行助手聊天机器人的情况下，端到端评估将检查聊天机器人如何响应用户输入。这种评估考虑了所有中间LLM组件和检索。你可以评估系统的定性方面，例如答案与用户问题的相关性以及是否存在任何幻觉。

在后面的“评估指标”部分，你将了解更多关于评估端到端系统的方法。在你学习如何将这些评估指标应用到你的LLM智能应用之前，你将在下一节学习如何使用模型基准来评估哪些LLMs最适合你的应用。

# 模型基准测试

LLM本身是任何智能应用的基本组件。鉴于有许多LLMs可能适合你的应用，比较它们以确定哪个最适合你的应用是有帮助的。要比较多个模型，你可以将它们都评估为标准评估集的一部分。这种在统一评估集上比较模型的过程称为**模型基准测试**。基准测试可以帮助你了解模型的能力和局限性。

通常，在基准测试中表现最好的LLMs是最大的模型，如GPT-4和Claude 3 Opus。然而，与较小的模型（如GPT-4o mini和Claude 3 Haiku）相比，这些大型模型运行成本更高，生成速度也更慢。

即使大型模型成本过高，在开发你的应用时使用它们仍然可能有所帮助，因为它们设定了理想系统性能的基准。你可以围绕这些模型设计你的评估，用较小的模型替换它们，然后努力优化系统，以尝试达到使用大型模型的标准。

当新的LLMs发布时，它们通常会与一组标准基准进行比较。这些标准基准帮助开发者了解模型之间的比较。

这里有一些流行的LLM基准，许多模型都是基于这些基准进行评估的：

+   **Massive Multi-Task Language Understanding** (**MMLU**)：这个基准衡量模型使用大学水平的多项选择题来获取知识的能力。它评估模型是否选择了正确的答案。

    你可以在[https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu)了解更多关于这个基准的信息。

+   **HellaSwag**：这个基准使用多项选择文本补全来衡量模型的常识推理能力。它评估模型是否选择了正确的句子补全。

    你可以在[https://paperswithcode.com/sota/sentence-completion-on-hellaswag](https://paperswithcode.com/sota/sentence-completion-on-hellaswag)了解更多关于这个基准的信息。

+   **HumanEval**：这个基准衡量模型在Python中的编程能力。它提示模型创建一个Python函数来解决一个任务。然后，它使用预先构建的单元测试来评估模型输出的函数是否正确。

    你可以在[https://paperswithcode.com/sota/code-generation-on-humaneval](https://paperswithcode.com/sota/code-generation-on-humaneval)了解更多关于这个基准的信息。

+   **MATH**：这个基准衡量模型解决数学文字问题的能力。它评估模型是否达到正确的解决方案。

    你可以在[https://paperswithcode.com/dataset/math](https://paperswithcode.com/dataset/math)了解更多关于这个基准的信息。

你可以根据这些基准评估LLM的性能，以选择最适合你应用的模型。例如，在旅行助手聊天机器人的情况下，MMLU的高分可能是一个很好的迹象，表明该模型非常适合回答旅行问题，因为模型拥有世界知识来指导其回答会有所帮助。相比之下，HumanEval Python编码基准的高分可能对其旅行建议的质量影响甚微。

你也可以创建自己的基准来评估LLM在你应用相关领域的性能。你甚至可以模仿现有基准来设计这些基准。对于旅行助手聊天机器人，你可以创建一个基于MMLU的关于热门旅游目的地的多项选择题基准。这个旅行基准将有助于确定哪些模型拥有关于旅行的最佳背景知识。通过选择拥有更多旅行相关知识的模型，你可以提高你回答的质量。

这些基准还可以揭示哪些模型最适合你应用的各个组件。例如，对于旅行助手聊天机器人，你可能需要一个在主要响应者中拥有显著度假地知识的庞大、昂贵的模型，但在其他LLM组件（如输入相关性护栏）中可以使用更快、更便宜的模型。

一旦您有了适合您AI组件的模型的想法，您就可以开始构建这些系统。为了理解和衡量这些AI系统如何使用LLMs，您必须创建评估数据集，并在其上运行评估指标。在接下来的两个部分中，您将了解如何创建这些评估数据集和指标。

## 评估数据集

您必须创建**评估数据集**来衡量AI系统的性能。评估数据集是您输入到AI系统中的数据，以产生一个输出，该输出衡量AI系统性能的好坏。评估数据集通常包括一些**评估指标**可以用来确定评估分数的标准。评估指标接受AI系统的输入和输出，并返回一个分数，衡量AI系统在该案例中的表现。您将在本章的*评估指标*部分了解更多关于评估指标的内容。

评估数据集是一组不同的评估案例。每个评估案例通常包括以下信息：

+   **输入**: 输入到AI系统中的数据。

+   **参考**: 评估指标用来评估AI系统输出是否正确的标准。参考通常是系统给定输入的理想输出。这个理想输出通常被称为**黄金答案**或**参考答案**。这也可以是一个标准，AI系统输出应该满足这些标准。有时，评估数据集不包括参考，因为用于数据集的评估指标不需要参考标准来评估输入。当一个评估不需要输出参考时，它被称为**无参考评估**。

+   **元数据**: 评估通常还包括每个评估案例的元数据。这可以是一个唯一的名称、一个ID或一个标签。

评估数据集通常符合表格或基于文档的数据结构。因此，它们通常以CSV、JSON或Parquet等格式存储。

这里是一个小型评估数据集的示例，包括旅行助手聊天机器人的用户消息和模型答案：

| **输入** | **黄金答案** | **标签** |
| --- | --- | --- |
| `我在7月份应该在新泽西市做什么` `？` | 去时代广场看看，参加一场户外音乐会，并参观自由女神像。 | `["todo", "``nyc", "usa"]` |
| `你能帮我做我的` `数学作业吗`？ | 很抱歉，我不能帮您做数学作业，因为我是一个旅行助手。您有任何与旅行相关的问题吗？ | `["``security"]` |
| `法国的首都是什么` `？` | 巴黎是法国的首都。 | `["``europe", "france"]` |

表9.1：示例聊天机器人的评估数据集

本章的剩余部分将使用此数据集进行评估。

评估数据集中具体包含什么取决于你想要评估的功能和使用的评估指标。在即将到来的*评估指标*部分，你将了解针对不同的评估指标，你需要包含哪些具体信息在你的评估数据集中。

无论你使用什么具体的评估指标，拥有一个代表性的评估数据集都很重要。该数据集应该代表你期望你的AI系统接收到的输入类型，以及你想要优化系统的边缘情况。

对于你应该拥有多少评估案例或确定该数量应该是什么的公式，并没有精确的数字。尽管如此，你可以使用以下非常粗略的经验法则来构建评估数据集：

+   对于任何给定的指标，至少要有10个评估案例。

+   至少要有100-200个代表性的评估案例，以了解端到端系统的性能。

接下来，你将了解一些策略，帮助你创建代表性的评估数据集。

## 定义基线

为了启动你的评估数据集，你必须创建一组评估案例，这些案例涵盖了你在应用程序中想要优化的通用预期行为和边缘情况。

为了定义这个基线的共同预期，与AI系统的任何利益相关者合作创建以下领域的评估案例可能是有用的：

+   **预期的常见输入的多样化样本**：你可能能够利用现有数据来帮助确定这些评估案例。例如，在旅行助手聊天机器人中，你可以从关于旅行的顶级谷歌搜索查询中推导出评估案例。这符合以下逻辑：无论人们在谷歌上搜索什么，他们很可能也会向你的聊天机器人询问同样的问题。

+   **你想要优化系统的边缘情况**：边缘情况可以包括测试系统安全和道德护栏的输入。如果你对AI系统进行红队测试，如在第12章*纠正和优化你的生成式AI应用程序*中进一步讨论的那样，你很可能可以从红队测试结果中找到一些好的边缘情况。

这个评估案例基线通常足以将AI系统发布到面向用户的环境。一旦AI系统投入使用，你可以验证基线评估案例的有效性，并创建额外的评估案例，如下一节所述。

## 用户反馈

在你发布你的AI系统之后，你可以从用户数据中获取评估案例，以持续改进和提升系统的性能。如果你的应用程序有任何用户反馈机制，例如评分或评论，你可以使用这些来识别系统成功或失败的情况。

通常，你应该在将任何应用数据添加到评估数据集之前手动审查它。你想要确保案例适合你的评估数据集，并且不包含任何敏感信息。你还可以添加元数据，例如标签或评估案例名称。

即使应用数据不适合评估案例，也许是因为格式不正确或包含个人可识别信息，你也可以修改它以创建一个合适的评估案例。

有可能创建一个使用LLM完全自动化从用户反馈中创建评估案例过程的管道。然而，你应该强烈考虑以下原因在循环中保持人类的存在：

+   你希望评估数据集的质量非常高，而与基于LLM的系统相比，你可以更容易地通过人类审阅来确保这一点。

+   对于参与AI系统开发的人来说，了解他们评估数据集中的案例是有益的。这种意识有助于让他们了解系统的能力。

+   由于评估数据集通常不需要特别大才能有效（几百个评估案例通常就足够了），因此创建一个基于LLM的系统来创建评估案例可能对于任务的 要求来说过于冗余。

从用户反馈中构建你的评估数据集是一种将评估建立在用户提供的输入类型上的有效方法。

## 合成数据

LLMs是生成评估数据集的有效工具。当你使用LLM生成数据时，它被称为**合成数据**。你可能想使用合成数据，因为对于人类来说创建评估案例既耗时又繁琐。LLMs可以帮助使创建评估数据的过程更快、更简单。

有各种策略可以创建合成评估数据。截至2024年中期，还没有创建合成评估数据的结构化最佳实践集合。然而，以下是一些你在创建合成评估案例时可以牢记的原则：

+   在循环中包含人类。人类应该审查所有合成数据案例，并根据需要编辑或删除它们。这为合成数据提供了质量控制。

+   LLMs在创建现有评估案例的**扰动**方面非常有效。扰动是对现有数据的微小变化，例如句子的改写。你可以使用扰动来查看AI系统是否根据微小的变化表现出不同的行为。理想情况下，系统应该在扰动之间保持一致的行为。

+   通常，一个基于LLM的聊天机器人，如ChatGPT、Claude或Gemini，足以帮助创建合成数据。聊天机器人界面的来回交流也可以帮助你完善和迭代你的合成数据创建。

通过结合合成数据、基线数据和用户反馈数据，你可以创建用于有效评估AI系统性能的数据集。你必须将这些数据集与指标配对以运行评估。在下一节中，你将了解更多关于评估指标的内容。

# 评估指标

要对你的AI系统进行评估，你必须将你的评估数据与一个**评估指标**相结合。评估指标接受AI系统的输入和输出，并返回一个分数，衡量AI系统在该案例中的表现。

评估指标通常返回介于0和1之间的分数。该指标称为`Foo`，对一个评估案例返回分数为`0.6`，对另一个案例返回`0.7`。如果你设定的阈值为0.65，那么`0.6`分数被视为失败，而`0.7`分数被视为通过。

LLM系统的评估指标大致可以分为以下类别：

+   **基于断言的指标**：评估人工智能系统输出是否与代码中的断言（如相等或正则表达式匹配）相匹配的指标。

+   **统计指标**：使用统计算法来评估AI系统输出的指标。

+   **LLM作为裁判的指标**：使用LLM来评估人工智能系统的输出是否符合定性标准的指标。

+   **RAG指标**：评估RAG系统的指标。通常，RAG指标使用LLM作为裁判。由于它们的独特属性，本章将RAG指标视为一个单独的类别。

由于LLM工程空间的创新性，你使用的确切指标可能会改变，但这里讨论的一般类别可能会很有用。在本节的剩余部分，你将了解更多关于这些类别以及它们中的具体评估指标。

## 基于断言的指标

**基于断言的指标**是定量指标，评估人工智能系统的输出是否满足代码中定义的某些标准。基于断言的指标类似于传统软件工程中的单元测试，其中你比较模块输出是否与预期匹配。

你甚至可以将基于断言的评估包裹在一个单元测试套件中。鉴于你的智能应用程序可能已经有一个测试套件，你可以通过在测试套件中包含基于断言的指标来开始向应用程序中添加评估。这是一种在不向应用程序添加额外技术开销的情况下评估你的AI组件的好方法。然而，随着应用程序的成熟，你可能会想要创建一个单独的评估套件。

你可以使用的一些基于断言的指标如下：

+   `==`) 或不等于 (`!=`) 预期值。

+   `>`), 大于或等于 (`>=`), 小于 (`<`), 或小于或等于 (`<=`)。这些比较运算符对于评估数值输出很有用。

+   **子字符串匹配**：评估字符串输出是否包含预期的子字符串。

+   **正则表达式匹配**：评估字符串输出是否与正则表达式匹配。

在以下代码示例中，你有一个旅行助手聊天机器人应用的评估案例数据集。此评估重点在于输入相关性防护栏。案例包括评估输入、相关性防护栏的预期输出以及通过相关性防护栏运行输入的实际输出。评估指标评估实际输出是否等于预期输出。

首先，安装`prettytable` Python包，你将使用它以可读的格式输出结果。在终端中安装包：

[PRE0]

然后，执行以下Python代码：

[PRE1]

此代码将以下评估结果输出到终端：

[PRE2]

上述代码示例展示了如何使用基于断言的评估指标来评估智能应用中的LLM组件。

## 统计指标

统计指标使用算法来确定分数。如果你有传统**自然语言处理**（**NLP**）的背景，你可能已经熟悉了评估LLM系统输出的统计指标。统计指标在将LLM系统用于其他NLP模型会使用的任务（如分类、摘要和翻译）时最有用。

以下是一些流行的NLP指标，你可以使用它们来评估LLM系统输出：

+   **双语评估助手**（**BLEU**）：BLEU衡量模型输出与一个或多个参考文本的精确度。你可以使用BLEU分数来计算模型输出与参考答案的相似程度。BLEU最初是为了衡量机器翻译文本与参考翻译的质量而开发的。

    你可以在[https://en.wikipedia.org/wiki/BLEU](https://en.wikipedia.org/wiki/BLEU)了解更多关于BLEU的信息。

+   **基于召回的摘要评估助手**（**ROUGE**）：ROUGE衡量机器生成文本与一个或多个参考文本的质量。在LLM系统中，ROUGE通常用于评估LLM如何有效地总结参考文本。ROUGE对于RAG系统特别有用，其中LLM总结检索到的文档中的内容。它还可以用来衡量翻译与参考文本的质量。

    你可以在[https://en.wikipedia.org/wiki/ROUGE_(metric)](https://en.wikipedia.org/wiki/ROUGE_(metric))了解更多关于ROUGE的信息。

在以下代码示例中，你有一个旅行助手聊天机器人应用的评估案例数据集。此评估重点在于响应生成器LLM。它计算实际输出与参考输出的BLEU分数，以衡量实际输出与参考输出的匹配程度。它还计算ROUGE分数，以衡量答案对检索到的上下文信息的总结程度。

首先，你必须安装几个Python包。`prettytable`包以可读的格式输出结果，`sacrebleu`包计算BLEU分数，`rouge-score`包计算ROUGE分数。在终端中安装这些包：

[PRE3]

然后，执行以下Python代码：

[PRE4]

此代码将以下内容输出到终端：

[PRE5]

上述示例演示了如何使用BLEU和ROUGE分数作为评估指标来衡量旅行助手聊天机器人的输出。例如，在前面的例子中，BLEU和ROUGE分数在第一个`纽约市`测试案例中如此不同，表明模型答案与黄金答案有显著偏差，但相对较高地遵循上下文信息。这种差异意味着你可以优化检索器以获取更多相关上下文信息，从而更好地满足黄金答案。

这些统计指标在LLM用于更传统的NLP任务（如翻译和摘要）时，对于评估LLM输出的质量最有用。它们还可以在比较同一评估数据集上相同AI系统的不同版本时提供有用的方向性指标。

虽然这些**定量指标**可以为LLM性能提供有价值的见解，但通常不足以评估由LLM驱动的智能应用。这些指标往往无法捕捉到语言生成的细微方面，如连贯性、创造力、事实正确性和上下文适宜性。因此，你需要创建**定性评估**来了解LLM系统在这些指标上的表现如何。在接下来的章节中，你将了解如何使用LLM作为裁判和RAG特定指标来评估LLM输出。

## LLM作为裁判的评价

你可以使用LLM根据定性标准评估LLM系统的输出。许多LLM系统执行广泛的开放域任务，例如聊天机器人进行扩展对话。如前所述的定量指标并不能必然捕捉到LLM系统是否有效地执行这些任务。例如，ROUGE分数可能能够表明摘要与源文档的匹配程度，但它不能告诉你摘要是否包含幻觉。你将在[*第11章*](B22495_11.xhtml#_idTextAnchor232)，*生成式AI的常见失败*中了解更多关于幻觉的内容。

在LLM兴起之前，系统地评估自然语言生成的定性方面具有挑战性。现在你可以使用LLM来评估LLM驱动系统的输出。使用LLM进行评估被称为**LLM作为裁判**。使用另一个裁判LLM评估LLM输出永远不是完美的解决方案。裁判LLM受到所有需要你首先评估LLM系统的LLM限制。然而，截至2024年中期，LLM作为裁判似乎是在系统性地进行LLM输出定性评估的最佳方法。

你可以在以下一些领域使用作为裁判的LLM定性指标：

+   响应的语气和风格

+   响应是否根据输入信息个性化

+   响应是否包含敏感信息，例如不应分享的个人身份信息

+   响应是否遵守某些法律或法规

在创建LLM作为评委的评价指标时，牢记以下关键点是有用的：

+   总是设置LLM的**温度**为0以获得一致的输出。温度是控制LLM预测随机性的超参数。温度为0产生确定性输出。更高的温度产生更多样化和一致性较差的输出，如果LLM在进行创造性工作，这可能更可取。然而，你希望评估尽可能一致。

+   更好的LLM往往也是更好的评估者。在基准测试中排名更高的LLM往往会产生更符合预期的评估结果。

+   **多轮提示**通常可以提高评估者的准确性。要执行多轮提示，除了在模型提示中包含评估标准外，还应包括输入示例和模型应提供的输出示例。这些示例通常有助于模型进行更好的评估。通常，你应该包括至少五个代表不同评估场景的示例。

+   **思维链提示**通常可以进一步提高LLM作为评委的评估性能。在思维链提示中，你要求模型在给出最终答案之前解释其思维过程。

+   每个LLM作为评委的评价指标应仅评估一个定性方面。专注于单一方面使得评估任务对LLM来说更容易理解。如果你需要评估多个方面，则应创建多个LLM作为评委的评价指标。

+   你使用的LLM很重要。不同的LLM在相同的评估任务上可以产生不同的结果。在所有使用指标的评估中保持使用相同的LLM是一致的。如果你更改了指标的LLM，则无法可靠地比较由不同LLM产生的结果。

+   产生结构化评估输出。评委LLM应产生结构化输出，例如通过或失败，或整数0-5的分数。然后你可以对这些分数进行归一化。例如，如果评委LLM输出`pass`或`fail`，则`pass`归一化为1，`fail`归一化为0。如果评委LLM输出整数`0`-`5`，则`0`归一化为0，`1`归一化为0.2，`2`归一化为0.4...，`5`归一化为1。

以下代码示例使用LLM作为评委来评估旅行助手聊天机器人在其响应中是否向用户提供了建议。LLM评估者还包括少量示例，以改善评委模型对任务的了解。

代码示例在输入和输出的数据集上运行评估。请注意，这是一个无参考评估，因为LLM作为评委不需要参考答案来确定聊天机器人是否提供了不相关的答案。

首先，你必须安装几个 Python 包。`prettytable` 包以可读的格式输出结果，而 `openai` 包调用 OpenAI API 以使用 GPT-4o LLM。在终端中安装这些包：

[PRE6]

然后，执行以下代码：

[PRE7]

此代码将以下内容输出到终端：

[PRE8]

上述示例演示了如何创建一个简单的 LLM-as-a-judge 指标来评估一个响应是否包含推荐。你可以扩展这些技术来创建额外的 LLM-as-a-judge 指标，以查看你的 LLM 系统的各个方面。在下一节中，你将了解一些更复杂的 LLM-as-a-judge 指标，用于评估 RAG 系统。

## RAG 指标

RAG 目前是使用 LLM 最流行的方式之一。一套独特的指标已经出现，用于衡量 RAG 系统的有效性。这些指标都使用 LLM 作为评判者。

这些指标侧重于任何 RAG 系统的两个核心组件：检索和生成：

+   **检索**：此组件从外部来源检索相关信息。它通常结合向量搜索与基于 LLM 的预处理和后处理。

+   **生成**：此组件使用 LLM 生成文本输出。

以下 LLM-as-a-judge 指标常用于评估 RAG 系统：

+   **答案忠诚度**：衡量生成的响应与检索到的上下文信息的相关性

+   **答案相关性**：衡量生成的响应与提供的输入的相关性

**Ragas** 是一个流行的 Python 库，它包含实现这些指标以及其他用于 RAG 评估的模块。在本节的剩余部分，你将了解 Ragas 如何实现这些指标。要了解更多关于 Ragas 及其可用指标的信息，请参阅其文档 ([https://docs.ragas.io/en/stable/index.html](https://docs.ragas.io/en/stable/index.html))。

### 答案忠诚度

答案忠诚度是 RAG 系统生成组件的评估指标。它衡量生成响应中的信息与检索到的上下文信息的一致程度。

通过识别生成答案与检索到的上下文之间的事实差异，答案忠诚度指标可以帮助识别答案中是否存在任何幻觉。

Ragas 包含一个用于测量忠诚度的模块。它使用以下公式计算忠诚度：

![](img/B22495_09_Equation.jpg)

将数据输入到忠诚度公式中的步骤如下：

1.  使用 LLM 从生成的响应中提取所有断言。

1.  使用 LLM 定位参考材料中的每个断言。

1.  计算可以从上下文信息中推断出的断言比例。

以下代码示例使用 Ragas 忠诚度指标对一个示例输入集、上下文和 RAG 系统输出进行操作。

首先，你必须安装几个Python包。`ragas`包包括响应忠实度指标和报告模块。`langchain-openai`包允许你将OpenAI模型传递给Ragas。本例使用GPT-4o mini模型。Ragas还依赖于`datasets`包来格式化输入。在终端中安装这些包：

[PRE9]

然后，运行以下代码以执行评估：

[PRE10]

执行此代码将在终端输出类似以下的结果：

[PRE11]

从结果中可以看出，Ragas评估者认为第一和第三种示例是忠实的，而第二种则不是。

在下一节中，你将学习如何使用另一个RAG评估指标：答案相关性。

### 答案相关性

答案相关性衡量RAG系统的输出与输入的相关性。这个指标很有用，因为它决定了RAG系统对提供的输入的响应效果。

Ragas使用输入、生成的输出和检索到的上下文信息来生成答案相关性指标中的输出。它通过以下步骤计算答案相关性评估指标分数：

1.  使用LLM从生成的响应中生成一个问题列表。

1.  为上一步生成的每个LLM生成的提问创建一个向量嵌入。同时，也为初始输入查询创建一个向量嵌入。

1.  计算原始问题嵌入和每个生成的提问嵌入之间的余弦相似度。

1.  答案相关性分数是原始问题和每个生成的提问之间余弦相似度的平均值。

Ragas假设，如果生成的答案与原始问题高度相关，那么从这个答案中可以导出的问题应该与原始问题在语义上相似。这个假设基于这样一个观点，即一个相关的答案包含直接回答查询的信息。因此，评判LLM应该能够*逆向工程*与原始输入紧密对齐的问题。

以下代码示例使用Ragas答案相关性指标在一个示例输入、上下文和RAG系统输出集上。

首先，你必须安装几个Python包。注意，这些与上一节中Ragas忠实度评估示例中的依赖项相同。`ragas`包包括响应答案相关性指标和报告模块。`langchain-openai`包允许你将OpenAI模型传递给Ragas。本例使用GPT-4o mini模型。Ragas还依赖于`datasets`包来格式化输入。在终端中安装这些包：

[PRE12]

然后，运行以下代码以执行评估：

[PRE13]

执行此代码将在终端输出以下结果：

[PRE14]

从结果中可以看出，第一和第三种情况是相关的，而第二种情况则不是。这很合理，因为第一和第三种情况有相当相关的上下文，而第二种情况则完全没有上下文信息。

Ragas 答案相关性指标存在一些显著的局限性。底层语言模型的质量会显著影响指标的有效性，因为它严重依赖于 LLM 从给定答案中生成适当问题的能力。该指标在处理复杂或多方面的查询时也可能遇到困难，尤其是当答案没有全面涵盖原始问题的所有方面时，可能会对更复杂主题的相关性进行不完整的评估。

有其他方法可以用来评估答案相关性。例如，**DeepEval** 评估框架采用以下策略来计算答案相关性：

1.  使用 LLM 从输出中提取所有陈述。

1.  使用相同的语言模型（LLM）来确定哪些陈述与输入相关。

1.  将答案相关性计算为相关陈述数除以总陈述数。

Ragas 和 DeepEval 策略在计算答案相关性指标方面的差异表明，人工智能工程领域仍在收敛于如何计算这些指标，即使基于这些指标形式之一进行评估已成为标准。

使用本节讨论的 RAG 评估指标，你可以衡量你的 RAG 系统的表现，并衡量系统随时间改进的情况。你还可以在 Ragas 或 DeepEval 等框架中尝试其他 RAG 指标。

在下一节中，你将学习如何手动进行人工审查你的数据，以增强本节讨论的自动评估指标。

## 人工审查

虽然 LLM 可以是定性评估的有效工具，但它们通常不如原始的非人工智能形式：人类。**人工审查**被认为是定性审查的黄金标准。

当使用人工审查时，你应该考虑到人类可能更喜欢简单的评分指标，这些指标不需要他们进行复杂的多步骤计算，例如本章前面描述的答案相关性指标。相反，给人工审查员简单的评分系统。通过/不通过标准是最简单的，可以归一化到 0 或 1。你也可以使用 0-5 的评分系统，可以归一化到 0、0.2 等等，直到 1。

人工审查员的无格式反馈特别有价值，因为这种开放式的反馈可以提供仅通过评分指标无法捕捉到的见解。

记录人类审查员是谁对于评估来说也很有用。你可以使用这个信息在需要时跟进这个人，或者跟踪某些个人与其他人相比的评分表现。

尽管人工审查具有定性优势，但也伴随着其自身的局限性：

+   **成本**：人工审查员通常比使用 LLM 作为评判者更昂贵。

+   **时间**: 人类审阅者通常比使用LLM作为法官花费的时间更长。你也不能像AI模型那样并行化单个人类。

+   **乏味**: 评估LLM的输出对于人类审阅者来说可能是一项极其乏味的工作。许多人不愿意进行评估，因此很难找到能够持续进行评估的人。

+   **弹性**: 通常，在软件开发过程中或定期间隔内，你需要运行大量评估。很难找到合适的人类审阅者在你需要的时候进行评估。

+   **不一致性**: 人类审阅者在评估中可能存在不一致性。不同的人可能会以不同的方式评估相同的案例。同一个人在不同的时刻，根据疲劳、情绪和环境等因素，甚至可能对相同的案例进行不同的评估。

考虑到使用人类作为审阅者的优点和缺点，你必须仔细考虑何时使用人类审阅。人类审阅可能对于进行初步定性评估最有用。人类审阅者可以为应用程序性能设定一个基线，你可以以相当高的置信度对其进行衡量。

你还可以将人类审阅作为基线来衡量LLM作为法官的指标。你可以尝试使LLM作为法官的指标尽可能接近人类审阅的结果。

此外，你的作为法官的LLM指标可以使用人类审阅中的示例在其提示中向LLM展示分类应该看起来像什么，作为一种多轮提示的形式。多轮提示已被证明可以显著提高模型性能。

人类审阅是定性评估中最有效的方法之一，尽管它既慢又贵。

## 评估作为护栏

**护栏**是一种防止AI产生不希望或不正确输出的机制。护栏确保生成的响应在可接受的范围内，并与你的应用程序的质量、伦理和相关性标准保持一致。

在本章的前面部分，你学习了关于**无参考评估**的内容。这些评估只需要输入，而不需要参考输出或黄金答案。你还可以使用无参考评估作为护栏，以确保AI系统正确运行。例如，在*RAG指标*部分，你看到了答案相关性指标。你可以在旅行助手聊天机器人中使用这个指标作为护栏，以确保聊天机器人只响应符合一定相关性的答案。如果答案不符合这个标准，你可以在响应用户之前执行一些额外的应用程序逻辑。

在本章中，你已经学习了如何使用评估来提高你智能应用程序的质量。使用无参考评估作为安全带让你可以扩展评估的效用，使其成为应用程序本身的一个组件。

# 摘要

在本章中，你探讨了评估你智能应用程序中LLM输出的方法。你学习了LLM评估是什么以及为什么它对你的智能应用程序很重要。模型基准测试是一种评估形式，可以帮助你确定在你的应用程序中使用哪些LLM。

一旦你的应用程序拥有功能性的AI模块，你就可以创建评估数据集并在其上运行指标来衡量性能和随时间的变化。除了自动评估之外，你还可以进行人工审查以进一步衡量应用程序的质量。最后，你可以在应用程序中使用无参考指标作为安全带。

在下一章中，你将学习如何优化语义数据模型以增强检索准确性和整体性能。
