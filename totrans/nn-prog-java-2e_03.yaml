- en: Chapter 3. Perceptrons and Supervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to explore in more detail supervised learning,
    which is very useful in finding relations between two datasets. Also, we introduce
    perceptrons, a very popular neural network architecture that implements supervised
    learning. This chapter also presents their extended generalized version, the so-called
    multi-layer perceptrons, as well as their features, learning algorithms, and parameters.
    Also, the reader will learn how to implement them in Java and how to use them
    in solving some basic problems. This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perceptrons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear separation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Limitations: the XOR problem'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilayer perceptrons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generalized delta rule – backpropagation algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Levenberg–Marquardt algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single hidden layer neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extreme learning machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised learning – teaching the neural net
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we introduced the learning paradigms that apply to
    neural networks, where supervised learning implies that there is a goal or a defined
    target to reach. In practice, we present a set of input data X, and a set of desired
    output data YT, then we evaluate a cost function whose aim is to reduce the error
    between the neural output Y and the target output YT.
  prefs: []
  type: TYPE_NORMAL
- en: 'In supervised learning, there are two major categories of tasks involved, which
    are detailed as follows: classification and regression.'
  prefs: []
  type: TYPE_NORMAL
- en: Classification – finding the appropriate class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Neural networks also work with categorical data. Given a list of classes and
    a dataset, one wishes to classify them according to a historical dataset containing
    records and their respective class. The following table shows an example of this
    dataset, considering the subjects'' average grades between **0** and **10**:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Student Id | Subjects | Profession |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| English | Math | Physics | Chemistry | Geography | History | Literature |
    Biology |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 89543 | 7.82 | 8.82 | 8.35 | 7.45 | 6.55 | 6.39 | 5.90 | 7.03 | **Electrical
    Engineer** |'
  prefs: []
  type: TYPE_TB
- en: '| 93201 | 8.33 | 6.75 | 8.01 | 6.98 | 7.95 | 7.76 | 6.98 | 6.84 | **Marketer**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 95481 | 7.76 | 7.17 | 8.39 | 8.64 | 8.22 | 7.86 | 7.07 | 9.06 | **Doctor**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 94105 | 8.25 | 7.54 | 7.34 | 7.65 | 8.65 | 8.10 | 8.40 | 7.44 | **Lawyer**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 96305 | 8.05 | 6.75 | 6.54 | 7.20 | 7.96 | 7.54 | 8.01 | 7.86 | **School
    Principal** |'
  prefs: []
  type: TYPE_TB
- en: '| 92904 | 6.95 | 8.85 | 9.10 | 7.54 | 7.50 | 6.65 | 5.86 | 6.76 | **Programmer**
    |'
  prefs: []
  type: TYPE_TB
- en: 'One example is the prediction of profession based on scholar grades. Let''s
    consider a dataset of former students who are now working. We compile a data set
    containing each student''s average grade on each subject and his/her current profession.
    Note that the output would be the name of professions, which neural networks are
    not able to give directly. Instead, we need to make one column (one output) for
    each known profession. If that student chose a certain profession, the column
    corresponding to that profession would have the value one, otherwise it would
    be zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification – finding the appropriate class](img/B05964_03_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we want to find a model - based on a neural network - to predict which
    profession a student will be likely to choose based on his/her grades. To that
    end, we structure a neural network containing the number of scholar subjects as
    the input and the number of known professions as the output, and an arbitrary
    number of hidden neurons in the hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification – finding the appropriate class](img/B05964_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For classification problems, there is usually only one class for each data point.
    So in the output layer, the neurons are fired to produce either zero or one, it
    being better to use activation functions that are output bounded between these
    two values. However, we must consider the case in which more than one neuron fires,
    giving two classes for a record. There are a number of mechanisms to prevent this
    case, such as the softmax function or the winner-takes-all algorithm, for example.
    These mechanisms are going to be detailed in the practical application in [Chapter
    6](ch06.xhtml "Chapter 6. Classifying Disease Diagnosis"), *Classifying Disease
    Diagnosis*.
  prefs: []
  type: TYPE_NORMAL
- en: After being trained, the neural network has learned what will be the most probable
    profession for a given student given his/her grades.
  prefs: []
  type: TYPE_NORMAL
- en: Regression – mapping real inputs to outputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Regression consists in finding some function that maps a set of inputs to a
    set of outputs. The following table shows how a dataset containing k records of
    m independent inputs X are known to be bound to n dependent outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Input independent data | Output dependent data |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| X1 | X2 | … | XM | T1 | T2 | … | TN |'
  prefs: []
  type: TYPE_TB
- en: '| x1[0] | x2[0] | … | xm[0] | t1[0] | t2[0] | … | tn[0] |'
  prefs: []
  type: TYPE_TB
- en: '| x1[1] | x2[1] | … | xm[1] | t1[1] | t2[1] | … | tn[1] |'
  prefs: []
  type: TYPE_TB
- en: '| … | … | … | … | … | … | … | … |'
  prefs: []
  type: TYPE_TB
- en: '| x1[k] | x2[k] | … | xm[k] | t1[k] | t2[k] | … | tn[k] |'
  prefs: []
  type: TYPE_TB
- en: 'The preceding table can be compiled in matrix format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Regression – mapping real inputs to outputs](img/B05964_03_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Unlike the classification, the output values are numerical instead of labels
    or classes. There is also a historical database containing records of some behavior
    we would like the neural network to learn. One example is the prediction of bus
    ticket prices between two cities. In this example, we collect information from
    a list of cities and the current ticket prices of buses departing from one and
    arriving to another. We structure the city features as well as the distance and/or
    time between them as the input and the bus ticket price as the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Regression – mapping real inputs to outputs](img/B05964_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '| Features city of origin | Features city of destination | Features of the
    way between | Ticket fare |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Population** | **GDP** | **Routes** | **Population** | **GDP** | **Routes**
    | **Distance** | **Time** | **Stops** |   |'
  prefs: []
  type: TYPE_TB
- en: '| 500,000 | 4.5 | 6 | 45,000 | 1.5 | 5 | 90 | 1,5 | 0 | 15 |'
  prefs: []
  type: TYPE_TB
- en: '| 120,000 | 2.6 | 4 | 500,000 | 4.5 | 6 | 30 | 0,8 | 0 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| 30,000 | 0.8 | 3 | 65,000 | 3.0 | 3 | 103 | 1,6 | 1 | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| 35,000 | 1.4 | 3 | 45,000 | 1.5 | 5 | 7 | 0.4 | 0 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| … |   |   |   |   |   |   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| 120,000 | 2.6 | 4 | 12,000 | 0.3 | 3 | 37 | 0.6 | 0 | 7 |'
  prefs: []
  type: TYPE_TB
- en: Having structured the dataset, we define a neural network containing the exact
    number of features (multiplied by two, provided two cities) plus the route features
    in the input, one output, and an arbitrary number of neurons in the hidden layer.
    In the case presented in the preceding table, there would be nine inputs. Since
    the output is numerical, there is no need to convert output data.
  prefs: []
  type: TYPE_NORMAL
- en: This neural network would give an estimate price for a route between two cities,
    which currently is not served by any bus transportation company.
  prefs: []
  type: TYPE_NORMAL
- en: A basic neural architecture – perceptrons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Perceptron is the most simple neural network architecture. Projected by *Frank
    Rosenblatt* in 1957, it has just one layer of neurons, receiving a set of inputs
    and producing another set of outputs. This was one of the first representations
    of neural networks to gain attention, especially because of their simplicity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A basic neural architecture – perceptrons](img/B05964_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In our Java implementation, this is illustrated with one neural layer (the
    output layer). The following code creates a perceptron with three inputs and two
    outputs, having the linear function at the output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Applications and limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: However, scientists did not take long to conclude that a perceptron neural network
    could only be applied to simple tasks, according to that simplicity. At that time,
    neural networks were being used for simple classification problems, but perceptrons
    usually failed when faced with more complex datasets. Let's illustrate this with
    a very basic example (an `AND` function) to understand better this issue.
  prefs: []
  type: TYPE_NORMAL
- en: Linear separation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The example consists of an AND function that takes two inputs, **x1** and **x2**.
    That function can be plotted in a two-dimensional chart as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear separation](img/B05964_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'And now let''s examine how the neural network evolves the training using the
    perceptron rule, considering a pair of two weights, **w1** and **w2**, initially
    **0.5**, and bias valued **0.5** as well. Assume learning rate η equals **0.2**:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Epoch | x1 | x2 | w1 | w2 | b | y | t | E | Δw1 | Δw2 | Δb |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 0 | 0.5 | 0.5 | 0.5 | 0.5 | 0 | -0.5 | 0 | 0 | -0.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 1 | 0.5 | 0.5 | 0.4 | 0.9 | 0 | -0.9 | 0 | -0.18 | -0.18 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0 | 0.5 | 0.32 | 0.22 | 0.72 | 0 | -0.72 | -0.144 | 0 | -0.144 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 1 | 0.356 | 0.32 | 0.076 | 0.752 | 1 | 0.248 | 0.0496 | 0.0496 |
    0.0496 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0 | 0 | 0.406 | 0.370 | 0.126 | 0.126 | 0 | -0.126 | 0.000 | 0.000 |
    -0.025 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0 | 1 | 0.406 | 0.370 | 0.100 | 0.470 | 0 | -0.470 | 0.000 | -0.094 |
    -0.094 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1 | 0 | 0.406 | 0.276 | 0.006 | 0.412 | 0 | -0.412 | -0.082 | 0.000 |
    -0.082 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1 | 1 | 0.323 | 0.276 | -0.076 | 0.523 | 1 | 0.477 | 0.095 | 0.095 |
    0.095 |'
  prefs: []
  type: TYPE_TB
- en: '| … | … |   |   |   |   |   |   |   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| 89 | 0 | 0 | 0.625 | 0.562 | -0.312 | -0.312 | 0 | 0.312 | 0 | 0 | 0.062
    |'
  prefs: []
  type: TYPE_TB
- en: '| 89 | 0 | 1 | 0.625 | 0.562 | -0.25 | 0.313 | 0 | -0.313 | 0 | -0.063 | -0.063
    |'
  prefs: []
  type: TYPE_TB
- en: '| 89 | 1 | 0 | 0.625 | 0.500 | -0.312 | 0.313 | 0 | -0.313 | -0.063 | 0 | -0.063
    |'
  prefs: []
  type: TYPE_TB
- en: '| 89 | 1 | 1 | 0.562 | 0.500 | -0.375 | 0.687 | 1 | 0.313 | 0.063 | 0.063 |
    0.063 |'
  prefs: []
  type: TYPE_TB
- en: 'After **89** epochs, we find the network to produce values near to the desired
    output. Since in this example the outputs are binary (zero or one), we can assume
    that any value produced by the network that is below **0.5** is considered to
    be **0** and any value above **0.5** is considered to be **1**. So, we can draw
    a function ![Linear separation](img/B05964_03_05_01.jpg), with the final weights
    and bias found by the learning algorithm *w1=0.562*, *w2=0.5* and *b=-0.375*,
    defining the linear boundary in the chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear separation](img/B05964_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This boundary is a definition of all classifications given by the network. You
    can see that the boundary is linear, given that the function is also linear. Thus,
    the perceptron network is really suitable for problems whose patterns are linearly
    separable.
  prefs: []
  type: TYPE_NORMAL
- en: The XOR case
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now let''s analyze the `XOR` case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The XOR case](img/B05964_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We see that in two dimensions, it is impossible to draw a line to separate
    the two patterns. What would happen if we tried to train a single layer perceptron
    to learn this function? Suppose we tried, let''s see what happened in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Epoch | x1 | x2 | w1 | w2 | b | y | t | E | Δw1 | Δw2 | Δb |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 0 | 0.5 | 0.5 | 0.5 | 0.5 | 0 | -0.5 | 0 | 0 | -0.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 1 | 0.5 | 0.5 | 0.4 | 0.9 | 1 | 0.1 | 0 | 0.02 | 0.02 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0 | 0.5 | 0.52 | 0.42 | 0.92 | 1 | 0.08 | 0.016 | 0 | 0.016 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 1 | 0.516 | 0.52 | 0.436 | 1.472 | 0 | -1.472 | -0.294 | -0.294 |
    -0.294 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0 | 0 | 0.222 | 0.226 | 0.142 | 0.142 | 0 | -0.142 | 0.000 | 0.000 |
    -0.028 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0 | 1 | 0.222 | 0.226 | 0.113 | 0.339 | 1 | 0.661 | 0.000 | 0.132 | 0.132
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1 | 0 | 0.222 | 0.358 | 0.246 | 0.467 | 1 | 0.533 | 0.107 | 0.000 | 0.107
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1 | 1 | 0.328 | 0.358 | 0.352 | 1.038 | 0 | -1.038 | -0.208 | -0.208
    | -0.208 |'
  prefs: []
  type: TYPE_TB
- en: '| … | … |   |   |   |   |   |   |   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| 127 | 0 | 0 | -0.250 | -0.125 | 0.625 | 0.625 | 0 | -0.625 | 0.000 | 0.000
    | -0.125 |'
  prefs: []
  type: TYPE_TB
- en: '| 127 | 0 | 1 | -0.250 | -0.125 | 0.500 | 0.375 | 1 | 0.625 | 0.000 | 0.125
    | 0.125 |'
  prefs: []
  type: TYPE_TB
- en: '| 127 | 1 | 0 | -0.250 | 0.000 | 0.625 | 0.375 | 1 | 0.625 | 0.125 | 0.000
    | 0.125 |'
  prefs: []
  type: TYPE_TB
- en: '| 127 | 1 | 1 | -0.125 | 0.000 | 0.750 | 0.625 | 0 | -0.625 | -0.125 | -0.125
    | -0.125 |'
  prefs: []
  type: TYPE_TB
- en: 'The perceptron just could not find any pair of weights that would drive the
    following error 0.625\. This can be explained mathematically as we already perceived
    from the chart that this function cannot be linearly separable in two dimensions.
    So what if we add another dimension? Let''s see the chart in three dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The XOR case](img/B05964_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In three dimensions, it is possible to draw a plane that would separate the
    patterns, provided that this additional dimension could properly transform the
    input data. Okay, but now there is an additional problem: how could we derive
    this additional dimension since we have only two input variables? One obvious,
    but also workaround, answer would be adding a third variable as a derivation from
    the two original ones. And being this third variable a (derivation), our neural
    network would probably get the following shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The XOR case](img/B05964_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Okay, now the perceptron has three inputs, one of them being a composition
    of the other. This also leads to a new question: how should that composition be
    processed? We can see that this component could act as a neuron, so giving the
    neural network a nested architecture. If so, there would another new question:
    how would the weights of this new neuron be trained, since the error is on the
    output neuron?'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-layer perceptrons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we can see, one simple example in which the patterns are not linearly separable
    has led us to more and more issue using the perceptron architecture. That need
    led to the application of multilayer perceptrons. In [Chapter 1](ch01.xhtml "Chapter 1. Getting
    Started with Neural Networks"), *Getting Started with Neural Networks* we dealt
    with the fact that the natural neural network is structured in layers as well,
    and each layer captures pieces of information from a specific environment. In
    artificial neural networks, layers of neurons act in this way, by extracting and
    abstracting information from data, transforming them into another dimension or
    shape.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the XOR example, we found the solution to be the addition of a third component
    that would make possible a linear separation. But there remained a few questions
    regarding how that third component would be computed. Now let''s consider the
    same solution as a two-layer perceptron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-layer perceptrons](img/B05964_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now we have three neurons instead of just one, but in the output the information
    transferred by the previous layer is transformed into another dimension or shape,
    whereby it would be theoretically possible to establish a linear boundary on those
    data points. However, the question on finding the weights for the first layer
    remains unanswered, or can we apply the same training rule to neurons other than
    the output? We are going to deal with this issue in the Generalized delta rule
    section.
  prefs: []
  type: TYPE_NORMAL
- en: MLP properties
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multi-layer perceptrons can have any number of layers and also any number of
    neurons in each layer. The activation functions may be different on any layer.
    An MLP network is usually composed of at least two layers, one for the output
    and one hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are also some references that consider the input layer as the nodes that
    collect input data; therefore, for those cases, the MLP is considered to have
    at least three layers. For the purpose of this book, let's consider the input
    layer as a special type of layer which has no weights, and as the effective layers,
    that is, those enabled to be trained, we'll consider the hidden and output layers.
  prefs: []
  type: TYPE_NORMAL
- en: A hidden layer is called that because it actually `hides` its outputs from the
    external world. Hidden layers can be connected in series in any number, thus forming
    a deep neural network. However, the more layers a neural network has, the slower
    would be both training and running, and according to mathematical foundations,
    a neural network with one or two hidden layers at most may learn as well as deep
    neural networks with dozens of hidden layers. But it depends on several factors.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is really recommended for the activation functions to be nonlinear in the
    hidden layers, especially if in the output layer the activation function is linear.
    According to linear algebra, having a linear activation function in all layers
    is equivalent to having only one output layer, provided that the additional variables
    introduced by the layers would be mere linear combinations of the previous ones
    or the inputs. Usually, activation functions such as hyperbolic tangent or sigmoid
    are used, because they are derivable.
  prefs: []
  type: TYPE_NORMAL
- en: MLP weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In an MLP feedforward network, one particular neuron *i* receives data from
    a neuron j of the previous layer and forwards its output to a neuron k of the
    next layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![MLP weights](img/B05964_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The mathematical description of a neural network is recursive:'
  prefs: []
  type: TYPE_NORMAL
- en: '![MLP weights](img/B05964_03_21_Replace.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *yo* is the network output (should we have multiple outputs, we can replace
    *yo* with Y, representing a vector); *fo* is the activation function of the output;
    *l* is the number of hidden layers; *nhi* is the number of neurons in the hidden
    layer *i*; *wi* is the weight connecting the *i* th neuron of the last hidden
    layer to the output; *fi* is the activation function of the neuron *i*; and *bi*
    is the bias of the neuron *i*. It can be seen that this equation gets larger as
    the number of layers increases. In the last summing operation, there will be the
    inputs *xi*.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent MLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The neurons on an MLP may feed signals not only to neurons in the next layers
    (feedforward network), but also to neurons in the same or previous layers (feedback
    or recurrent). This behavior allows the neural network to maintain state on some
    data sequence, and this feature is especially exploited when dealing with time
    series or handwriting recognition. Recurrent networks are usually harder to train,
    and eventually the computer may run out of memory while executing them. In addition,
    there are recurrent network architectures better than MLPs, such as Elman, Hopfield,
    Echo state, Bidirectional RNNs (recurrent neural networks). But we are not going
    to dive deep into these architectures, because this book focuses on the simplest
    applications for those who have minimal experience in programming. However, we
    recommend good literature on recurrent networks for those who are interested in
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Coding an MLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bringing these concepts into the OOP point of view, we can review the classes
    already designed so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding an MLP](img/B05964_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'One can see that the neural network structure is hierarchical. A neural network
    is composed of layers that are composed of neurons. In the MLP architecture, there
    are three types of layers: input, hidden, and output. So suppose that in Java,
    we would like to define a neural network consisting of three inputs, one output
    (linear activation function) and one hidden layer (`sigmoid` function) containing
    five neurons. The resulting code would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Learning in MLPs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The multi-layer perceptron network learns based on the Delta Rule, which is
    also inspired by the gradient descent optimization method. The gradient method
    is broadly applied to find minima or maxima of a given function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning in MLPs](img/B05964_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This method is applied at *walking* the direction where the function''s output
    is higher or lower, depending on the criteria. This concept is explored in the
    Delta Rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning in MLPs](img/B05964_03_12_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The function that the Delta Rule wants to minimize is the error between the
    neural network output and the target output, and the parameters to be found are
    the neural weights. This is an enhanced learning algorithm compared to the perceptron
    rule, because it takes into account the activation function derivative *g'(h)*,
    which in mathematical terms indicates the direction where the function is decreasing
    most.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although the Delta Rule works well for the neural networks having only output
    and input layers, for the MLP networks, the pure Delta Rule cannot be applied
    because of the hidden layer neurons. To overcome this issue, in the 1980s, *Rummelhart*
    and others proposed a new algorithm, also inspired by the gradient method, called
    backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm is indeed a generalization of the Delta Rule for MLPs. The benefits
    of having additional layers to abstract more data from the environment have motivated
    the development of a training algorithm that could properly adjust the weights
    of the hidden layer. Based on the gradient method, the error from output would
    be (back) propagated to the previous layers, so making possible the weight update
    using the same equation as the Delta Rule.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm runs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Backpropagation algorithm](img/B05964_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The second step is the backpropagation itself. What it does is to find the
    weight variation according to the gradient, which is the base for the Delta Rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Backpropagation algorithm](img/B05964_03_13_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, E is the error, *wji* is the weight between the neurons *i* and *j*, *oi*
    is the output of the *ith* neuron, and *hi* is the weighted sum of that neuron's
    inputs before passing to activation function. Remember that *oi=f(hi)*, *f* being
    the activation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'For updating in the hidden layers, it is a bit more complicated as we consider
    the error as function of all neurons between the weight to be updated and the
    output. To facilitate this process, we should compute the sensibility or backpropagation
    error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Backpropagation algorithm](img/B05964_03_14_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'And the weight update is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Backpropagation algorithm](img/B05964_03_14_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The calculation of the backpropagation error varies for the output and for
    the hidden layers:'
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation for the output layer:![Backpropagation algorithm](img/B05964_03_14_03.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, oi is the *ith* output, *ti* is the desired *ith* output, *f'(hi)* is
    the derivative of the output activation function, and *hi* is the weighted sum
    of the *ith* neuron inputs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Backpropagation for the hidden layer:![Backpropagation algorithm](img/B05964_03_14_04.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, *l* is a neuron of the layer ahead and *wil* is the weight that connects
    the current neuron to the lth neuron of the layer immediately ahead.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For simplicity reasons, we did not demonstrate fully how the backpropagation
    equation was developed. Anyway, if you are interested in the details, you may
    consult the book neural networks – a comprehensive foundation by Simon Haykin.
  prefs: []
  type: TYPE_NORMAL
- en: The momentum
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like any gradient-based method, there is a risk of falling into a local minimum.
    To mitigate this risk, we can add another term to the weight update rule called
    momentum, which takes into consideration the last variation of weight:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The momentum](img/B05964_03_14_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, μ is a momentum rate and ![The momentum](img/B05964_03_14_06.jpg) is the
    last delta weight. This gives an additional step to the update, therefore attenuating
    the oscillations in the error hyperspace.
  prefs: []
  type: TYPE_NORMAL
- en: Coding the backpropagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s define the class backpropagation in the package `edu.packt.neural.learn`.
    Since this learning algorithm is a generalization of the `DeltaRule`, this class
    may inherit and override the features already defined in Delta Rule. Three additional
    attributes included in this class are the momentum rate, the delta neuron, and
    the last delta weight arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The constructor will have the same arguments as for the `DeltaRule` class,
    adding the calls to methods for initialization of the `deltaNeuron` and `lastDeltaWeights`
    arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `train()` method will work in a similar way as in the `DeltaRule` class;
    the additional component is the backward step, whereby the error is `backpropagated`
    throughout the neural layers up to the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The role of the backward step is to determine the delta weights by means of
    error backpropagation, from the output layer down to the first hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The backpropagation step is performed in the method `calcDeltaWeight()`. The
    momentum will be added only before updating the weights because it should recall
    the last delta weight determined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note the calculation of the `_deltaNeuron` is different for the output and the
    hidden layers, but for both of them the derivative is used. To facilitate this
    task, we've added the `derivative()` method to the class `Neuron`. Details can
    be found in *Annex III* documentation. At the end, the input corresponding to
    the weight is multiplied to the delta weight calculated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The weight update is performed by the method `applyNewWeights()`. To save space,
    we are not going to write here the whole method body, but only the core where
    the weight update is performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the code listing, `l` represents the layer, `j` the neuron, and `i` the
    input to the weight. For the output layer, l will be equal to the number of hidden
    layers (exceeding the Java array limits), so the `NeuralLayer` called is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This class can be used exactly the same way as `DeltaRule`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: At the end of this chapter, we will make a comparison of the Delta Rule for
    the perceptrons with the backpropagation with a multi-layer perceptron, trying
    to solve the XOR problem.
  prefs: []
  type: TYPE_NORMAL
- en: Levenberg-Marquardt algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The backpropagation algorithm, like all gradient-based methods, usually presents
    slow convergence, especially when it falls in a zig-zag situation, when the weights
    are changed to almost the same value every two iterations. This drawback was studied
    in problems such as curve-fitting interpolation by Kenneth Levenberg in 1944,
    and later by Donald Marquart in 1963, who developed a method for finding coefficients
    based on the Gauss Newton algorithm and the gradient descent, so therefrom comes
    the name of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The LM algorithm deals with some optimization terms which are beyond the scope
    of this book, but in the references section, the reader will find good resources
    to learn more about these concepts, so we will present the method in a simpler
    way. Let''s suppose we have a list of inputs x and outputs *t*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Levenberg-Marquardt algorithm](img/B05964_03_14_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We have seen that a neural network has the property to map inputs to outputs
    just like a nonlinear function f with coefficients *W* (weights and bias):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Levenberg-Marquardt algorithm](img/B05964_03_14_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The nonlinear function will produce values different than the outputs T; that''s
    because we marked the variable Y in the equation. The Levenberg-Marquardt algorithm
    works over a Jacobian matrix, which is a matrix of all partial derivatives in
    regard to each weight and bias for each data row. So the Jacobian matrix has the
    following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Levenberg-Marquardt algorithm](img/B05964_03_14_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *k* is the total number of data points and p is the total number of weights
    and bias. In the Jacobian matrix, all weights and bias are stored serially in
    a single row. The elements of the Jacobian Matrix are calculated from the gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Levenberg-Marquardt algorithm](img/B05964_03_14_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The partial derivative of the error E in relation to each weight is calculated
    in the backpropagation algorithm, so this algorithm is going to run the backpropagation
    step as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'In every optimization problem, one wishes to minimize the total error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Levenberg-Marquardt algorithm](img/B05964_03_14_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *W* (weights and bias in the NN case) are the variables to optimize.
    The optimization algorithm updates W by adding ΔW. By applying some algebra, the
    last equation can be extended to this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Levenberg-Marquardt algorithm](img/B05964_03_14_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Converting to the vector and notation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Levenberg-Marquardt algorithm](img/B05964_03_14_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, by setting the error E to zero, we get the Levenberg-Marquardt equation
    after some manipulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Levenberg-Marquardt algorithm](img/B05964_03_14_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is the weight update rule. As can be seen, it involves matrix operations
    such as transposition and inversion. The Greek letter λ is the damping factor,
    an equivalent for the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: Coding the Levenberg-Marquardt with matrix algebra
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to effectively implement the LM algorithm, it is very useful to work
    with matrix algebra. To address that, we defined a class called `Matrix` in the
    package `edu.packt.neuralnet.math`, including all the matrix operations, such
    as multiplication, inverse, and LU decomposition, among others. The reader may
    refer to the documentation to find out more about this class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Levenberg-Marquardt algorithm uses many features of the backpropagation
    algorithm; that''s why we inherit this class from backpropagation. Some new attributes
    are included:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Jacobian matrix**: This is the matrix containing the partial derivatives
    to each weight for all training records'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Damping factor**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error backpropagation**: This array has the same function of `deltaNeuron`,
    but its calculation differs a little to each neural output; that''s why we defined
    it in a separate attribute'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error LMA**: The error in the matrix form:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Basically, the train function is the same as that of the backpropagation, except
    for the following calculation of the Jacobian and error matrices and the damping
    update:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The loop where it goes over the training dataset calls the method `calculateJacobian`.
    This method works on the error backpropagation evaluated in the method backward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the code listing, `p` is the input connecting to the neuron (when it is equal
    to the number of neuron inputs, it represents the bias), `k` is the neuron, l
    is the layer, m is the neural output, `i` is a sequential index of the record,
    and `j` is the sequential index of the weight or bias, according to the layer
    and neuron in which it is located. Note that after setting the value in the Jacobian
    matrix, `j` is incremented.
  prefs: []
  type: TYPE_NORMAL
- en: 'The weight update is performed by means of determining the `deltaWeight` matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous code refers to the matrix algebra shown in the section presenting
    the algorithm. The matrix `deltaWeight` contains the steps for each weight in
    the neural network. In the following code, `k` is the neuron, `j` is the input,
    and `l` is the layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note that the weights are saved in the `lastWeights` array, so they can be recovered
    if the error gets worse.
  prefs: []
  type: TYPE_NORMAL
- en: Extreme learning machines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Taking advantage of the matrix algebra, **extreme learning machines** (**ELMs**)
    are able to converge learning very fast. This learning algorithm has one limitation,
    since it is applied only on neural networks containing one single hidden layer.
    In practice, one hidden layer works pretty fine for most applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Representing the neural network in matrix algebra, for the following neural
    network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Extreme learning machines](img/B05964_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We have the corresponding equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Extreme learning machines](img/B05964_03_15_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *H* is the output of the hidden layer, *g()* is the activation function
    of the hidden layer, *Xi* is the *ith* input record, *Wj* is the weight vector
    for the *jth* hidden neuron, *bj* is the bias of the *jth* hidden neuron, βp is
    the weight vector for the output *p*, and *Y* is the output generated by the neural
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the ELM algorithm, the hidden layer weights are generated randomly, while
    the output weights are adjusted according to a least squares approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Extreme learning machines](img/B05964_03_15_02.jpg)![Extreme learning machines](img/B05964_03_15_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, T is the target output training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'This algorithm is implemented in a class called `ELM` in the same package as
    the other training algorithms. This class will inherit from `DeltaRule`, which
    has all the basic properties for supervised learning algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In this class, we define the matrices *H* and *T*, which will be later used
    for output weight calculation. The constructor is similar to the other training
    algorithms, except for the fact that this algorithm works only on batch mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since this training algorithm takes only one epoch, the train method forwards
    all training records to build the *H* matrix. Then, it calculates the output weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `buildMatrices` method only places the output of the hidden layer to its
    corresponding row in the *H* matrix. The output weights are adjusted in the `applyNewWeights`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Practical example 1 – the XOR case with delta rule and backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s see the multilayer perceptron in action. We coded the example `XORTest.java`,
    which basically creates two neural networks with the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Neural Network | Perceptron | Multi-layer Percepetron |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Inputs | 2 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Outputs | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Hidden Layers | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Hidden Neurons in each layer | 0 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Hidden Layer Activation Function | Non | Sigmoid |'
  prefs: []
  type: TYPE_TB
- en: '| Output Layer Activation Function | Linear | Linear |'
  prefs: []
  type: TYPE_TB
- en: '| Training Algorithm | Delta Rule | Backpropagation |'
  prefs: []
  type: TYPE_TB
- en: '| Learning Rate | 0.1 | 0.3Momentum 0.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Max Epochs | 4000 | 4000 |'
  prefs: []
  type: TYPE_TB
- en: '| Min. overall error | 0.1 | 0.01 |'
  prefs: []
  type: TYPE_TB
- en: 'In Java, this is coded as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define the dataset and the learning algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The training is then performed for both algorithms. As expected, the XOR case
    is not linearly separable by one single layer perceptron. The neural network runs
    the training but unsuccessfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![Practical example 1 – the XOR case with delta rule and backpropagation](img/B05964_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'But the backpropagation algorithm for the multilayer perceptron manages to
    learn the XOR function after 39 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![Practical example 1 – the XOR case with delta rule and backpropagation](img/B05964_03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Practical example 2 – predicting enrolment status
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In Brazil, one of the ways for a person to enter university consists of taking
    an exam and if he/she achieves the minimum grade for the course that he/she is
    seeking, then he/she can enroll. To demonstrate the backpropagation algorithm,
    let us consider this scenario. The data shown in the table was collected from
    a university database. The second column represents the person''s gender (one
    means female and zero means male); the third column has grades scaled by 100 and
    the last column is formed by two neurons (1,0 means performed enrollment and 0,1
    means waiver enrollment):'
  prefs: []
  type: TYPE_NORMAL
- en: '| Sample | Gender | Grade | EnrollmentStatus |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0.73 | 1 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1 | 0.81 | 1 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1 | 0.86 | 1 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0 | 0.65 | 1 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0 | 0.45 | 1 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 1 | 0.70 | 0 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 0 | 0.51 | 0 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 1 | 0.89 | 0 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 1 | 0.79 | 0 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0 | 0.54 | 0 1 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a neural network containing three neurons in the hidden layer, as
    shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Practical example 2 – predicting enrolment status](img/B05964_03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ve also set up the learning algorithms Levenberg-Marquardt and extreme
    learning machines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the training, we find that the training was successful. For the Levenberg-Marquardt
    algorithm, the minimum satisfied error was found after nine epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Practical example 2 – predicting enrolment status](img/B05964_03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'And the extreme learning machine found an error near to zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Practical example 2 – predicting enrolment status](img/B05964_03_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ve seen how perceptrons can be applied to solve linear
    separation problems, but also their limitations in classifying nonlinear data.
    To suppress those limitations, we presented **multi-layer perceptrons** (**MLPs**)
    and new training algorithms: backpropagation, Levenberg-Marquardt, and extreme
    learning machines. We''ve also seen some classes of problems which MLPs can be
    applied to, such as classification and regression. The Java implementation explored
    the power of the backpropagation algorithm in updating the weights both in the
    output layer and the hidden layer. Two practical applications were shown to demonstrate
    the MLPs for the solution of problems with the three learning algorithms.'
  prefs: []
  type: TYPE_NORMAL
