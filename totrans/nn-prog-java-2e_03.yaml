- en: Chapter 3. Perceptrons and Supervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to explore in more detail supervised learning,
    which is very useful in finding relations between two datasets. Also, we introduce
    perceptrons, a very popular neural network architecture that implements supervised
    learning. This chapter also presents their extended generalized version, the so-called
    multi-layer perceptrons, as well as their features, learning algorithms, and parameters.
    Also, the reader will learn how to implement them in Java and how to use them
    in solving some basic problems. This chapter will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression tasks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification tasks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perceptrons
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear separation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Limitations: the XOR problem'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilayer perceptrons
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generalized delta rule – backpropagation algorithm
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Levenberg–Marquardt algorithm
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single hidden layer neural networks
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extreme learning machines
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised learning – teaching the neural net
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we introduced the learning paradigms that apply to
    neural networks, where supervised learning implies that there is a goal or a defined
    target to reach. In practice, we present a set of input data X, and a set of desired
    output data YT, then we evaluate a cost function whose aim is to reduce the error
    between the neural output Y and the target output YT.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'In supervised learning, there are two major categories of tasks involved, which
    are detailed as follows: classification and regression.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Classification – finding the appropriate class
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Neural networks also work with categorical data. Given a list of classes and
    a dataset, one wishes to classify them according to a historical dataset containing
    records and their respective class. The following table shows an example of this
    dataset, considering the subjects'' average grades between **0** and **10**:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '| Student Id | Subjects | Profession |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
- en: '| English | Math | Physics | Chemistry | Geography | History | Literature |
    Biology |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
- en: '| 89543 | 7.82 | 8.82 | 8.35 | 7.45 | 6.55 | 6.39 | 5.90 | 7.03 | **Electrical
    Engineer** |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
- en: '| 93201 | 8.33 | 6.75 | 8.01 | 6.98 | 7.95 | 7.76 | 6.98 | 6.84 | **Marketer**
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
- en: '| 95481 | 7.76 | 7.17 | 8.39 | 8.64 | 8.22 | 7.86 | 7.07 | 9.06 | **Doctor**
    |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
- en: '| 94105 | 8.25 | 7.54 | 7.34 | 7.65 | 8.65 | 8.10 | 8.40 | 7.44 | **Lawyer**
    |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
- en: '| 96305 | 8.05 | 6.75 | 6.54 | 7.20 | 7.96 | 7.54 | 8.01 | 7.86 | **School
    Principal** |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
- en: '| 92904 | 6.95 | 8.85 | 9.10 | 7.54 | 7.50 | 6.65 | 5.86 | 6.76 | **Programmer**
    |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
- en: 'One example is the prediction of profession based on scholar grades. Let''s
    consider a dataset of former students who are now working. We compile a data set
    containing each student''s average grade on each subject and his/her current profession.
    Note that the output would be the name of professions, which neural networks are
    not able to give directly. Instead, we need to make one column (one output) for
    each known profession. If that student chose a certain profession, the column
    corresponding to that profession would have the value one, otherwise it would
    be zero:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子是根据学术成绩预测职业。让我们考虑一组现在工作的前学生的数据集。我们编制了一个包含每个学生在每门课程上的平均成绩以及他/她的当前职业的数据集。请注意，输出将是职业名称，神经网络无法直接给出。相反，我们需要为每个已知职业创建一列（一个输出）。如果该学生选择了一个特定的职业，则对应该职业的列将具有值一，否则为零：
- en: '![Classification – finding the appropriate class](img/B05964_03_20.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![分类 – 找到合适的类别](img/B05964_03_20.jpg)'
- en: 'Now we want to find a model - based on a neural network - to predict which
    profession a student will be likely to choose based on his/her grades. To that
    end, we structure a neural network containing the number of scholar subjects as
    the input and the number of known professions as the output, and an arbitrary
    number of hidden neurons in the hidden layer:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想要找到一个基于神经网络来预测学生将可能选择哪个职业的模型，基于他/她的成绩。为此，我们构建了一个神经网络，其中包含学术科目的数量作为输入，已知职业的数量作为输出，以及隐藏层中的任意数量的隐藏神经元：
- en: '![Classification – finding the appropriate class](img/B05964_03_01.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![分类 – 找到合适的类别](img/B05964_03_01.jpg)'
- en: For classification problems, there is usually only one class for each data point.
    So in the output layer, the neurons are fired to produce either zero or one, it
    being better to use activation functions that are output bounded between these
    two values. However, we must consider the case in which more than one neuron fires,
    giving two classes for a record. There are a number of mechanisms to prevent this
    case, such as the softmax function or the winner-takes-all algorithm, for example.
    These mechanisms are going to be detailed in the practical application in [Chapter
    6](ch06.xhtml "Chapter 6. Classifying Disease Diagnosis"), *Classifying Disease
    Diagnosis*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题，通常每个数据点只有一个类别。因此，在输出层，神经元被激活以产生零或一，最好使用输出值介于这两个值之间的激活函数。然而，我们必须考虑这种情况，即多个神经元被激活，给一个记录分配两个类别。有许多机制可以防止这种情况，例如
    softmax 函数或全胜算法，例如。这些机制将在第 6 章[分类疾病诊断]的实践应用中详细说明，*分类疾病诊断*。
- en: After being trained, the neural network has learned what will be the most probable
    profession for a given student given his/her grades.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 经过训练后，神经网络已经学会了给定学生的成绩，最有可能的职业是什么。
- en: Regression – mapping real inputs to outputs
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归 – 将实际输入映射到输出
- en: 'Regression consists in finding some function that maps a set of inputs to a
    set of outputs. The following table shows how a dataset containing k records of
    m independent inputs X are known to be bound to n dependent outputs:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 回归包括找到一些函数，这些函数将一组输入映射到一组输出。以下表格显示了包含 k 条记录的 m 个独立输入 X 的数据集如何与 n 个相关输出绑定：
- en: '| Input independent data | Output dependent data |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 输入独立数据 | 输出相关数据 |'
- en: '| --- | --- |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| X1 | X2 | … | XM | T1 | T2 | … | TN |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| X1 | X2 | … | XM | T1 | T2 | … | TN |'
- en: '| x1[0] | x2[0] | … | xm[0] | t1[0] | t2[0] | … | tn[0] |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| x1[0] | x2[0] | … | xm[0] | t1[0] | t2[0] | … | tn[0] |'
- en: '| x1[1] | x2[1] | … | xm[1] | t1[1] | t2[1] | … | tn[1] |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| x1[1] | x2[1] | … | xm[1] | t1[1] | t2[1] | … | tn[1] |'
- en: '| … | … | … | … | … | … | … | … |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| … | … | … | … | … | … | … | … |'
- en: '| x1[k] | x2[k] | … | xm[k] | t1[k] | t2[k] | … | tn[k] |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| x1[k] | x2[k] | … | xm[k] | t1[k] | t2[k] | … | tn[k] |'
- en: 'The preceding table can be compiled in matrix format:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的表格可以编译成矩阵格式：
- en: '![Regression – mapping real inputs to outputs](img/B05964_03_01_01.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![回归 – 将实际输入映射到输出](img/B05964_03_01_01.jpg)'
- en: 'Unlike the classification, the output values are numerical instead of labels
    or classes. There is also a historical database containing records of some behavior
    we would like the neural network to learn. One example is the prediction of bus
    ticket prices between two cities. In this example, we collect information from
    a list of cities and the current ticket prices of buses departing from one and
    arriving to another. We structure the city features as well as the distance and/or
    time between them as the input and the bus ticket price as the output:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 与分类不同，输出值是数值而不是标签或类别。还有一个包含我们希望神经网络学习的某些行为记录的历史数据库。一个例子是预测两个城市之间的公交车票价。在这个例子中，我们从一系列城市和从一地出发到达另一地的公交车当前票价中收集信息。我们将城市特征以及它们之间的距离和/或时间作为输入，将公交车票价作为输出：
- en: '![Regression – mapping real inputs to outputs](img/B05964_03_02.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![回归 - 将真实输入映射到输出](img/B05964_03_02.jpg)'
- en: '| Features city of origin | Features city of destination | Features of the
    way between | Ticket fare |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 出发城市特征 | 目的地城市特征 | 路线特征 | 票价 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Population** | **GDP** | **Routes** | **Population** | **GDP** | **Routes**
    | **Distance** | **Time** | **Stops** |   |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| **人口** | **GDP** | **路线** | **人口** | **GDP** | **路线** | **距离** | **时间** |
    **停靠站** |   |'
- en: '| 500,000 | 4.5 | 6 | 45,000 | 1.5 | 5 | 90 | 1,5 | 0 | 15 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 500,000 | 4.5 | 6 | 45,000 | 1.5 | 5 | 90 | 1.5 | 0 | 15 |'
- en: '| 120,000 | 2.6 | 4 | 500,000 | 4.5 | 6 | 30 | 0,8 | 0 | 10 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 120,000 | 2.6 | 4 | 500,000 | 4.5 | 6 | 30 | 0.8 | 0 | 10 |'
- en: '| 30,000 | 0.8 | 3 | 65,000 | 3.0 | 3 | 103 | 1,6 | 1 | 20 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 30,000 | 0.8 | 3 | 65,000 | 3.0 | 3 | 103 | 1.6 | 1 | 20 |'
- en: '| 35,000 | 1.4 | 3 | 45,000 | 1.5 | 5 | 7 | 0.4 | 0 | 5 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 35,000 | 1.4 | 3 | 45,000 | 1.5 | 5 | 7 | 0.4 | 0 | 5 |'
- en: '| … |   |   |   |   |   |   |   |   |   |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| … |   |   |   |   |   |   |   |   |'
- en: '| 120,000 | 2.6 | 4 | 12,000 | 0.3 | 3 | 37 | 0.6 | 0 | 7 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 120,000 | 2.6 | 4 | 12,000 | 0.3 | 3 | 37 | 0.6 | 0 | 7 |'
- en: Having structured the dataset, we define a neural network containing the exact
    number of features (multiplied by two, provided two cities) plus the route features
    in the input, one output, and an arbitrary number of neurons in the hidden layer.
    In the case presented in the preceding table, there would be nine inputs. Since
    the output is numerical, there is no need to convert output data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在结构化数据集后，我们定义了一个包含确切数量的特征（由于两个城市而乘以二）加上路线特征的输入，一个输出，以及隐藏层中的任意数量的神经元。在前面表格中展示的案例中，将有九个输入。由于输出是数值的，因此不需要转换输出数据。
- en: This neural network would give an estimate price for a route between two cities,
    which currently is not served by any bus transportation company.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个神经网络将给出两个城市之间路线的估计价格，目前还没有任何公交运输公司提供服务。
- en: A basic neural architecture – perceptrons
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个基本的神经网络架构 – 感知器
- en: 'Perceptron is the most simple neural network architecture. Projected by *Frank
    Rosenblatt* in 1957, it has just one layer of neurons, receiving a set of inputs
    and producing another set of outputs. This was one of the first representations
    of neural networks to gain attention, especially because of their simplicity:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器是最简单的神经网络架构。由 *Frank Rosenblatt* 在 1957 年提出，它只有一个神经元层，接收一组输入并产生另一组输出。这是神经网络首次获得关注的代表之一，尤其是由于它们的简单性：
- en: '![A basic neural architecture – perceptrons](img/B05964_03_03.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![一个基本的神经网络架构 - 感知器](img/B05964_03_03.jpg)'
- en: 'In our Java implementation, this is illustrated with one neural layer (the
    output layer). The following code creates a perceptron with three inputs and two
    outputs, having the linear function at the output layer:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 Java 实现中，这通过一个神经网络层（输出层）来展示。以下代码创建了一个具有三个输入和两个输出，输出层具有线性函数的感知器：
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Applications and limitations
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用和限制
- en: However, scientists did not take long to conclude that a perceptron neural network
    could only be applied to simple tasks, according to that simplicity. At that time,
    neural networks were being used for simple classification problems, but perceptrons
    usually failed when faced with more complex datasets. Let's illustrate this with
    a very basic example (an `AND` function) to understand better this issue.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，科学家们并没有花很长时间就得出结论，感知器神经网络只能应用于简单任务，根据这种简单性。当时，神经网络被用于简单的分类问题，但感知器通常在面对更复杂的数据集时失败。让我们用一个非常基本的例子（一个
    `AND` 函数）来更好地说明这个问题。
- en: Linear separation
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性分离
- en: 'The example consists of an AND function that takes two inputs, **x1** and **x2**.
    That function can be plotted in a two-dimensional chart as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 该示例由一个接受两个输入，**x1** 和 **x2** 的 AND 函数组成。该函数可以绘制在以下二维图表中：
- en: '![Linear separation](img/B05964_03_04.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![线性分离](img/B05964_03_04.jpg)'
- en: 'And now let''s examine how the neural network evolves the training using the
    perceptron rule, considering a pair of two weights, **w1** and **w2**, initially
    **0.5**, and bias valued **0.5** as well. Assume learning rate η equals **0.2**:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考察神经网络如何使用感知器规则进行训练，考虑一对权重，**w1** 和 **w2**，初始值为 **0.5**，以及偏置值为 **0.5**。假设学习率
    η 等于 **0.2**：
- en: '| Epoch | x1 | x2 | w1 | w2 | b | y | t | E | Δw1 | Δw2 | Δb |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 周期 | x1 | x2 | w1 | w2 | b | y | t | E | Δw1 | Δw2 | Δb |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 0 | 0 | 0.5 | 0.5 | 0.5 | 0.5 | 0 | -0.5 | 0 | 0 | -0.1 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0 | 0.5 | 0.5 | 0.5 | 0.5 | 0 | -0.5 | 0 | 0 | -0.1 |'
- en: '| 1 | 0 | 1 | 0.5 | 0.5 | 0.4 | 0.9 | 0 | -0.9 | 0 | -0.18 | -0.18 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 1 | 0.5 | 0.5 | 0.4 | 0.9 | 0 | -0.9 | 0 | -0.18 | -0.18 |'
- en: '| 1 | 1 | 0 | 0.5 | 0.32 | 0.22 | 0.72 | 0 | -0.72 | -0.144 | 0 | -0.144 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 0 | 0.5 | 0.32 | 0.22 | 0.72 | 0 | -0.72 | -0.144 | 0 | -0.144 |'
- en: '| 1 | 1 | 1 | 0.356 | 0.32 | 0.076 | 0.752 | 1 | 0.248 | 0.0496 | 0.0496 |
    0.0496 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 1 | 0.356 | 0.32 | 0.076 | 0.752 | 1 | 0.248 | 0.0496 | 0.0496 |
    0.0496 |'
- en: '| 2 | 0 | 0 | 0.406 | 0.370 | 0.126 | 0.126 | 0 | -0.126 | 0.000 | 0.000 |
    -0.025 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0 | 0 | 0.406 | 0.370 | 0.126 | 0.126 | 0 | -0.126 | 0.000 | 0.000 |
    -0.025 |'
- en: '| 2 | 0 | 1 | 0.406 | 0.370 | 0.100 | 0.470 | 0 | -0.470 | 0.000 | -0.094 |
    -0.094 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0 | 1 | 0.406 | 0.370 | 0.100 | 0.470 | 0 | -0.470 | 0.000 | -0.094 |
    -0.094 |'
- en: '| 2 | 1 | 0 | 0.406 | 0.276 | 0.006 | 0.412 | 0 | -0.412 | -0.082 | 0.000 |
    -0.082 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 | 0 | 0.406 | 0.276 | 0.006 | 0.412 | 0 | -0.412 | -0.082 | 0.000 |
    -0.082 |'
- en: '| 2 | 1 | 1 | 0.323 | 0.276 | -0.076 | 0.523 | 1 | 0.477 | 0.095 | 0.095 |
    0.095 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 | 1 | 0.323 | 0.276 | -0.076 | 0.523 | 1 | 0.477 | 0.095 | 0.095 |
    0.095 |'
- en: '| … | … |   |   |   |   |   |   |   |   |   |   |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| … | … |   |   |   |   |   |   |   |   |   |   |'
- en: '| 89 | 0 | 0 | 0.625 | 0.562 | -0.312 | -0.312 | 0 | 0.312 | 0 | 0 | 0.062
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 89 | 0 | 0 | 0.625 | 0.562 | -0.312 | -0.312 | 0 | 0.312 | 0 | 0 | 0.062
    |'
- en: '| 89 | 0 | 1 | 0.625 | 0.562 | -0.25 | 0.313 | 0 | -0.313 | 0 | -0.063 | -0.063
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 89 | 0 | 1 | 0.625 | 0.562 | -0.25 | 0.313 | 0 | -0.313 | 0 | -0.063 | -0.063
    |'
- en: '| 89 | 1 | 0 | 0.625 | 0.500 | -0.312 | 0.313 | 0 | -0.313 | -0.063 | 0 | -0.063
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 89 | 1 | 0 | 0.625 | 0.500 | -0.312 | 0.313 | 0 | -0.313 | -0.063 | 0 | -0.063
    |'
- en: '| 89 | 1 | 1 | 0.562 | 0.500 | -0.375 | 0.687 | 1 | 0.313 | 0.063 | 0.063 |
    0.063 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 89 | 1 | 1 | 0.562 | 0.500 | -0.375 | 0.687 | 1 | 0.313 | 0.063 | 0.063 |
    0.063 |'
- en: 'After **89** epochs, we find the network to produce values near to the desired
    output. Since in this example the outputs are binary (zero or one), we can assume
    that any value produced by the network that is below **0.5** is considered to
    be **0** and any value above **0.5** is considered to be **1**. So, we can draw
    a function ![Linear separation](img/B05964_03_05_01.jpg), with the final weights
    and bias found by the learning algorithm *w1=0.562*, *w2=0.5* and *b=-0.375*,
    defining the linear boundary in the chart:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 经过 **89** 个周期后，我们发现网络产生的值接近期望的输出。由于在这个例子中，输出是二进制的（零或一），我们可以假设网络产生的任何低于 **0.5**
    的值被认为是 **0**，任何高于 **0.5** 的值被认为是 **1**。因此，我们可以绘制一个函数 ![线性分离](img/B05964_03_05_01.jpg)，其中包含学习算法找到的最终权重和偏置
    *w1=0.562*，*w2=0.5* 和 *b=-0.375*，定义图表中的线性边界：
- en: '![Linear separation](img/B05964_03_05.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![线性分离](img/B05964_03_05.jpg)'
- en: This boundary is a definition of all classifications given by the network. You
    can see that the boundary is linear, given that the function is also linear. Thus,
    the perceptron network is really suitable for problems whose patterns are linearly
    separable.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这个边界是网络给出的所有分类的定义。你可以看到，由于函数也是线性的，边界是线性的。因此，感知器网络非常适合那些模式线性可分的问题。
- en: The XOR case
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XOR 情况
- en: 'Now let''s analyze the `XOR` case:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来分析 `XOR` 情况：
- en: '![The XOR case](img/B05964_03_06.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![XOR 情况](img/B05964_03_06.jpg)'
- en: 'We see that in two dimensions, it is impossible to draw a line to separate
    the two patterns. What would happen if we tried to train a single layer perceptron
    to learn this function? Suppose we tried, let''s see what happened in the following
    table:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，在二维空间中，不可能画出一条线来分离这两种模式。如果我们尝试训练一个单层感知器来学习这个函数，会发生什么？假设我们尝试了，让我们看看以下表格中的结果：
- en: '| Epoch | x1 | x2 | w1 | w2 | b | y | t | E | Δw1 | Δw2 | Δb |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 周期 | x1 | x2 | w1 | w2 | b | y | t | E | Δw1 | Δw2 | Δb |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 0 | 0 | 0.5 | 0.5 | 0.5 | 0.5 | 0 | -0.5 | 0 | 0 | -0.1 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0 | 0.5 | 0.5 | 0.5 | 0.5 | 0 | -0.5 | 0 | 0 | -0.1 |'
- en: '| 1 | 0 | 1 | 0.5 | 0.5 | 0.4 | 0.9 | 1 | 0.1 | 0 | 0.02 | 0.02 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 1 | 0.5 | 0.5 | 0.4 | 0.9 | 1 | 0.1 | 0 | 0.02 | 0.02 |'
- en: '| 1 | 1 | 0 | 0.5 | 0.52 | 0.42 | 0.92 | 1 | 0.08 | 0.016 | 0 | 0.016 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 0 | 0.5 | 0.52 | 0.42 | 0.92 | 1 | 0.08 | 0.016 | 0 | 0.016 |'
- en: '| 1 | 1 | 1 | 0.516 | 0.52 | 0.436 | 1.472 | 0 | -1.472 | -0.294 | -0.294 |
    -0.294 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 1 | 0.516 | 0.52 | 0.436 | 1.472 | 0 | -1.472 | -0.294 | -0.294 |
    -0.294 |'
- en: '| 2 | 0 | 0 | 0.222 | 0.226 | 0.142 | 0.142 | 0 | -0.142 | 0.000 | 0.000 |
    -0.028 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0 | 0 | 0.222 | 0.226 | 0.142 | 0.142 | 0 | -0.142 | 0.000 | 0.000 |
    -0.028 |'
- en: '| 2 | 0 | 1 | 0.222 | 0.226 | 0.113 | 0.339 | 1 | 0.661 | 0.000 | 0.132 | 0.132
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0 | 1 | 0.222 | 0.226 | 0.113 | 0.339 | 1 | 0.661 | 0.000 | 0.132 | 0.132
    |'
- en: '| 2 | 1 | 0 | 0.222 | 0.358 | 0.246 | 0.467 | 1 | 0.533 | 0.107 | 0.000 | 0.107
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 | 0 | 0.222 | 0.358 | 0.246 | 0.467 | 1 | 0.533 | 0.107 | 0.000 | 0.107
    |'
- en: '| 2 | 1 | 1 | 0.328 | 0.358 | 0.352 | 1.038 | 0 | -1.038 | -0.208 | -0.208
    | -0.208 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 | 1 | 0.328 | 0.358 | 0.352 | 1.038 | 0 | -1.038 | -0.208 | -0.208
    | -0.208 |'
- en: '| … | … |   |   |   |   |   |   |   |   |   |   |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| … | … |   |   |   |   |   |   |   |   |   |   |'
- en: '| 127 | 0 | 0 | -0.250 | -0.125 | 0.625 | 0.625 | 0 | -0.625 | 0.000 | 0.000
    | -0.125 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 127 | 0 | 0 | -0.250 | -0.125 | 0.625 | 0.625 | 0 | -0.625 | 0.000 | 0.000
    | -0.125 |'
- en: '| 127 | 0 | 1 | -0.250 | -0.125 | 0.500 | 0.375 | 1 | 0.625 | 0.000 | 0.125
    | 0.125 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 127 | 0 | 1 | -0.250 | -0.125 | 0.500 | 0.375 | 1 | 0.625 | 0.000 | 0.125
    | 0.125 |'
- en: '| 127 | 1 | 0 | -0.250 | 0.000 | 0.625 | 0.375 | 1 | 0.625 | 0.125 | 0.000
    | 0.125 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 127 | 1 | 0 | -0.250 | 0.000 | 0.625 | 0.375 | 1 | 0.625 | 0.125 | 0.000
    | 0.125 |'
- en: '| 127 | 1 | 1 | -0.125 | 0.000 | 0.750 | 0.625 | 0 | -0.625 | -0.125 | -0.125
    | -0.125 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 127 | 1 | 1 | -0.125 | 0.000 | 0.750 | 0.625 | 0 | -0.625 | -0.125 | -0.125
    | -0.125 |'
- en: 'The perceptron just could not find any pair of weights that would drive the
    following error 0.625\. This can be explained mathematically as we already perceived
    from the chart that this function cannot be linearly separable in two dimensions.
    So what if we add another dimension? Let''s see the chart in three dimensions:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机无法找到任何一对权重来驱动以下误差 0.625。这可以从我们已从图表中感知到的数学上解释，即这个函数在二维空间中无法线性可分。那么如果我们增加另一个维度会怎样呢？让我们看看三维空间的图表：
- en: '![The XOR case](img/B05964_03_07.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![XOR 情况](img/B05964_03_07.jpg)'
- en: 'In three dimensions, it is possible to draw a plane that would separate the
    patterns, provided that this additional dimension could properly transform the
    input data. Okay, but now there is an additional problem: how could we derive
    this additional dimension since we have only two input variables? One obvious,
    but also workaround, answer would be adding a third variable as a derivation from
    the two original ones. And being this third variable a (derivation), our neural
    network would probably get the following shape:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在三维空间中，如果这个额外的维度能够正确地转换输入数据，就有可能画出一个平面来分离模式。好吧，但现在有一个额外的问题：我们只有两个输入变量，我们如何推导出这个额外的维度呢？一个明显但也是权宜之计的答案是从两个原始变量中添加一个第三个变量作为导数。而这个第三个变量是一个（导数），我们的神经网络可能具有以下形状：
- en: '![The XOR case](img/B05964_03_08.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![XOR 情况](img/B05964_03_08.jpg)'
- en: 'Okay, now the perceptron has three inputs, one of them being a composition
    of the other. This also leads to a new question: how should that composition be
    processed? We can see that this component could act as a neuron, so giving the
    neural network a nested architecture. If so, there would another new question:
    how would the weights of this new neuron be trained, since the error is on the
    output neuron?'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，现在感知机有三个输入，其中一个是其他输入的组合。这也引出了一个新的问题：这个组合应该如何处理？我们可以看到这个组件可以作为一个神经元，因此给神经网络一个嵌套架构。如果是这样，那么就会有另一个新的问题：由于错误在输出神经元上，我们如何训练这个新神经元的权重？
- en: Multi-layer perceptrons
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知机
- en: As we can see, one simple example in which the patterns are not linearly separable
    has led us to more and more issue using the perceptron architecture. That need
    led to the application of multilayer perceptrons. In [Chapter 1](ch01.xhtml "Chapter 1. Getting
    Started with Neural Networks"), *Getting Started with Neural Networks* we dealt
    with the fact that the natural neural network is structured in layers as well,
    and each layer captures pieces of information from a specific environment. In
    artificial neural networks, layers of neurons act in this way, by extracting and
    abstracting information from data, transforming them into another dimension or
    shape.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，一个简单的例子，其中模式无法线性可分，这导致我们使用感知机架构时遇到了越来越多的问题。这种需求导致了多层感知机的应用。在[第 1 章](ch01.xhtml
    "第 1 章. 开始使用神经网络") *开始使用神经网络* 中，我们处理了这样一个事实，即自然神经网络的结构也是分层的，每一层都从特定的环境中捕获信息片段。在人工神经网络中，神经元层以这种方式行动，通过从数据中提取和抽象信息，将它们转换成另一个维度或形状。
- en: 'In the XOR example, we found the solution to be the addition of a third component
    that would make possible a linear separation. But there remained a few questions
    regarding how that third component would be computed. Now let''s consider the
    same solution as a two-layer perceptron:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在XOR示例中，我们找到了解决方案是添加一个第三组件，使其能够进行线性分离。但关于如何计算这个第三组件仍有一些疑问。现在让我们将相同的解决方案视为一个双层感知器：
- en: '![Multi-layer perceptrons](img/B05964_03_09.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![多层感知器](img/B05964_03_09.jpg)'
- en: Now we have three neurons instead of just one, but in the output the information
    transferred by the previous layer is transformed into another dimension or shape,
    whereby it would be theoretically possible to establish a linear boundary on those
    data points. However, the question on finding the weights for the first layer
    remains unanswered, or can we apply the same training rule to neurons other than
    the output? We are going to deal with this issue in the Generalized delta rule
    section.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有三个神经元而不是只有一个，但在输出层，前一层的传递信息被转换成另一个维度或形状，理论上可以在这些数据点上建立线性边界。然而，关于如何找到第一层的权重的疑问仍未得到解答，或者我们是否可以将相同的训练规则应用于输出层以外的神经元？我们将在广义delta规则部分处理这个问题。
- en: MLP properties
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLP属性
- en: Multi-layer perceptrons can have any number of layers and also any number of
    neurons in each layer. The activation functions may be different on any layer.
    An MLP network is usually composed of at least two layers, one for the output
    and one hidden layer.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知器可以具有任意数量的层，以及每一层任意数量的神经元。激活函数可以在任何层上不同。一个MLP网络通常由至少两层组成，一层用于输出，一层为隐藏层。
- en: Tip
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: There are also some references that consider the input layer as the nodes that
    collect input data; therefore, for those cases, the MLP is considered to have
    at least three layers. For the purpose of this book, let's consider the input
    layer as a special type of layer which has no weights, and as the effective layers,
    that is, those enabled to be trained, we'll consider the hidden and output layers.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 也有一些参考资料将输入层视为收集输入数据的节点；因此，对于这些情况，MLP被认为至少有三级。为了本书的目的，让我们将输入层视为一种特殊的层，它没有权重，并且作为有效层，即那些能够被训练的层，我们将考虑隐藏层和输出层。
- en: A hidden layer is called that because it actually `hides` its outputs from the
    external world. Hidden layers can be connected in series in any number, thus forming
    a deep neural network. However, the more layers a neural network has, the slower
    would be both training and running, and according to mathematical foundations,
    a neural network with one or two hidden layers at most may learn as well as deep
    neural networks with dozens of hidden layers. But it depends on several factors.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一个隐藏层之所以被称为隐藏层，是因为它实际上`隐藏`了其输出对外部世界。隐藏层可以以任意数量串联连接，从而形成一个深度神经网络。然而，神经网络层数越多，训练和运行的速度就越慢，根据数学基础，一个最多只有一两个隐藏层的神经网络可能学习效果与拥有数十个隐藏层的深度神经网络相当。但这取决于几个因素。
- en: Tip
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: It is really recommended for the activation functions to be nonlinear in the
    hidden layers, especially if in the output layer the activation function is linear.
    According to linear algebra, having a linear activation function in all layers
    is equivalent to having only one output layer, provided that the additional variables
    introduced by the layers would be mere linear combinations of the previous ones
    or the inputs. Usually, activation functions such as hyperbolic tangent or sigmoid
    are used, because they are derivable.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 真的强烈建议在隐藏层中使用非线性激活函数，尤其是在输出层激活函数是线性的情况下。根据线性代数，如果所有层的激活函数都是线性的，那么这相当于只有一个输出层，前提是层引入的额外变量将是前一层或输入的线性组合。通常，使用如双曲正切或Sigmoid这样的激活函数，因为它们是可导的。
- en: MLP weights
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLP权重
- en: 'In an MLP feedforward network, one particular neuron *i* receives data from
    a neuron j of the previous layer and forwards its output to a neuron k of the
    next layer:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在MLP前馈网络中，一个特定的神经元*i*从前一层的神经元j接收数据，并将其输出转发到下一层的神经元k：
- en: '![MLP weights](img/B05964_03_10.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![MLP权重](img/B05964_03_10.jpg)'
- en: 'The mathematical description of a neural network is recursive:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的数学描述是递归的：
- en: '![MLP weights](img/B05964_03_21_Replace.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![MLP权重](img/B05964_03_21_Replace.jpg)'
- en: Here, *yo* is the network output (should we have multiple outputs, we can replace
    *yo* with Y, representing a vector); *fo* is the activation function of the output;
    *l* is the number of hidden layers; *nhi* is the number of neurons in the hidden
    layer *i*; *wi* is the weight connecting the *i* th neuron of the last hidden
    layer to the output; *fi* is the activation function of the neuron *i*; and *bi*
    is the bias of the neuron *i*. It can be seen that this equation gets larger as
    the number of layers increases. In the last summing operation, there will be the
    inputs *xi*.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*yo* 是网络输出（如果我们有多个输出，我们可以用 Y 代替 *yo*，表示一个向量）；*fo* 是输出层的激活函数；*l* 是隐藏层的数量；*nhi*
    是隐藏层 *i* 中的神经元数量；*wi* 是连接最后一个隐藏层第 *i* 个神经元到输出的权重；*fi* 是神经元 *i* 的激活函数；*bi* 是神经元
    *i* 的偏置。可以看出，随着层数的增加，这个方程会变得更大。在最后的求和操作中，会有输入 *xi*。
- en: Recurrent MLP
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环MLP
- en: The neurons on an MLP may feed signals not only to neurons in the next layers
    (feedforward network), but also to neurons in the same or previous layers (feedback
    or recurrent). This behavior allows the neural network to maintain state on some
    data sequence, and this feature is especially exploited when dealing with time
    series or handwriting recognition. Recurrent networks are usually harder to train,
    and eventually the computer may run out of memory while executing them. In addition,
    there are recurrent network architectures better than MLPs, such as Elman, Hopfield,
    Echo state, Bidirectional RNNs (recurrent neural networks). But we are not going
    to dive deep into these architectures, because this book focuses on the simplest
    applications for those who have minimal experience in programming. However, we
    recommend good literature on recurrent networks for those who are interested in
    it.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: MLP中的神经元不仅可以向下一层的神经元（前馈网络）发送信号，还可以向同一层或上一层的神经元（反馈或循环）发送信号。这种行为允许神经网络在某个数据序列上维持状态，当处理时间序列或手写识别时，这个特性尤其被利用。循环网络通常更难训练，并且在执行时计算机可能会耗尽内存。此外，还有一些比MLP更好的循环网络架构，如Elman、Hopfield、Echo
    state、双向RNN（循环神经网络）。但我们不会深入探讨这些架构，因为这本书专注于对编程经验最少的人的最简单应用。然而，我们推荐那些对循环网络感兴趣的人阅读有关循环网络的良好文献。
- en: Coding an MLP
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码MLP
- en: 'Bringing these concepts into the OOP point of view, we can review the classes
    already designed so far:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些概念引入面向对象的观点，我们可以回顾到目前为止已经设计的类：
- en: '![Coding an MLP](img/B05964_03_11.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![编码MLP](img/B05964_03_11.jpg)'
- en: 'One can see that the neural network structure is hierarchical. A neural network
    is composed of layers that are composed of neurons. In the MLP architecture, there
    are three types of layers: input, hidden, and output. So suppose that in Java,
    we would like to define a neural network consisting of three inputs, one output
    (linear activation function) and one hidden layer (`sigmoid` function) containing
    five neurons. The resulting code would be as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到，神经网络结构是分层的。神经网络由层组成，而层由神经元组成。在MLP架构中，有三种类型的层：输入、隐藏和输出。所以假设在Java中，我们想要定义一个由三个输入、一个输出（线性激活函数）和一个包含五个神经元的隐藏层（`sigmoid`函数）组成的神经网络。生成的代码如下：
- en: '[PRE1]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Learning in MLPs
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLP中的学习
- en: 'The multi-layer perceptron network learns based on the Delta Rule, which is
    also inspired by the gradient descent optimization method. The gradient method
    is broadly applied to find minima or maxima of a given function:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知器网络基于Delta规则进行学习，该规则也受到梯度下降优化方法的启发。梯度方法广泛应用于寻找给定函数的最小值或最大值：
- en: '![Learning in MLPs](img/B05964_03_12.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![MLP中的学习](img/B05964_03_12.jpg)'
- en: 'This method is applied at *walking* the direction where the function''s output
    is higher or lower, depending on the criteria. This concept is explored in the
    Delta Rule:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法应用于*行走*函数输出更高或更低的方向，具体取决于标准。这个概念在Delta规则中得到了探索：
- en: '![Learning in MLPs](img/B05964_03_12_01.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![MLP中的学习](img/B05964_03_12_01.jpg)'
- en: The function that the Delta Rule wants to minimize is the error between the
    neural network output and the target output, and the parameters to be found are
    the neural weights. This is an enhanced learning algorithm compared to the perceptron
    rule, because it takes into account the activation function derivative *g'(h)*,
    which in mathematical terms indicates the direction where the function is decreasing
    most.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Delta 规则想要最小化的函数是神经网络输出和目标输出之间的误差，要找到的参数是神经权重。与感知器规则相比，这是一个增强的学习算法，因为它考虑了激活函数导数
    *g'(h)*，这在数学上表示函数减少最快的方向。
- en: Backpropagation algorithm
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向传播算法
- en: Although the Delta Rule works well for the neural networks having only output
    and input layers, for the MLP networks, the pure Delta Rule cannot be applied
    because of the hidden layer neurons. To overcome this issue, in the 1980s, *Rummelhart*
    and others proposed a new algorithm, also inspired by the gradient method, called
    backpropagation.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Delta 规则对于只有输出层和输入层的神经网络效果很好，但对于 MLP 网络，由于隐藏层神经元的存在，纯 Delta 规则不能应用。为了克服这个问题，在
    20 世纪 80 年代，*Rummelhart* 和其他人提出了一种新的算法，该算法也受到梯度方法的启发，称为反向传播。
- en: This algorithm is indeed a generalization of the Delta Rule for MLPs. The benefits
    of having additional layers to abstract more data from the environment have motivated
    the development of a training algorithm that could properly adjust the weights
    of the hidden layer. Based on the gradient method, the error from output would
    be (back) propagated to the previous layers, so making possible the weight update
    using the same equation as the Delta Rule.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法实际上是 Delta 规则对于 MLP 的一般化。拥有额外的层来从环境中抽象更多数据的优势激励了开发一个能够正确调整隐藏层权重的训练算法。基于梯度方法，输出层的误差会（反向）传播到前面的层，因此可以使用与
    Delta 规则相同的方程进行权重更新。
- en: 'The algorithm runs as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 算法运行如下：
- en: '![Backpropagation algorithm](img/B05964_03_13.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播算法](img/B05964_03_13.jpg)'
- en: 'The second step is the backpropagation itself. What it does is to find the
    weight variation according to the gradient, which is the base for the Delta Rule:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是反向传播本身。它所做的就是根据梯度找到权重变化，这是 Delta 规则的基础：
- en: '![Backpropagation algorithm](img/B05964_03_13_01.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播算法](img/B05964_03_13_01.jpg)'
- en: Here, E is the error, *wji* is the weight between the neurons *i* and *j*, *oi*
    is the output of the *ith* neuron, and *hi* is the weighted sum of that neuron's
    inputs before passing to activation function. Remember that *oi=f(hi)*, *f* being
    the activation function.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，E 是误差，*wji* 是神经元 *i* 和 *j* 之间的权重，*oi* 是第 *i* 个神经元的输出，而 *hi* 是该神经元输入在传递到激活函数之前的加权和。记住，*oi=f(hi)*，其中
    *f* 是激活函数。
- en: 'For updating in the hidden layers, it is a bit more complicated as we consider
    the error as function of all neurons between the weight to be updated and the
    output. To facilitate this process, we should compute the sensibility or backpropagation
    error:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对于隐藏层的更新，由于我们将误差视为要更新的权重和输出之间所有神经元的函数，因此它稍微复杂一些。为了便于这个过程，我们应该计算敏感性或反向传播误差：
- en: '![Backpropagation algorithm](img/B05964_03_14_01.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播算法](img/B05964_03_14_01.jpg)'
- en: 'And the weight update is as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 权重更新如下：
- en: '![Backpropagation algorithm](img/B05964_03_14_02.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播算法](img/B05964_03_14_02.jpg)'
- en: 'The calculation of the backpropagation error varies for the output and for
    the hidden layers:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播误差的计算对于输出层和隐藏层是不同的：
- en: Backpropagation for the output layer:![Backpropagation algorithm](img/B05964_03_14_03.jpg)
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层的反向传播：![反向传播算法](img/B05964_03_14_03.jpg)
- en: Here, oi is the *ith* output, *ti* is the desired *ith* output, *f'(hi)* is
    the derivative of the output activation function, and *hi* is the weighted sum
    of the *ith* neuron inputs
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这里，oi 是第 *i* 个输出，ti 是期望的第 *i* 个输出，*f'(hi)* 是输出激活函数的导数，而 *hi* 是第 *i* 个神经元输入的加权和
- en: Backpropagation for the hidden layer:![Backpropagation algorithm](img/B05964_03_14_04.jpg)
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层的反向传播：![反向传播算法](img/B05964_03_14_04.jpg)
- en: Here, *l* is a neuron of the layer ahead and *wil* is the weight that connects
    the current neuron to the lth neuron of the layer immediately ahead.
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里，*l* 是前一个层的神经元，*wil* 是连接当前神经元到前一个层第 *l* 个神经元的权重。
- en: For simplicity reasons, we did not demonstrate fully how the backpropagation
    equation was developed. Anyway, if you are interested in the details, you may
    consult the book neural networks – a comprehensive foundation by Simon Haykin.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，我们没有完全展示如何发展反向传播方程。无论如何，如果你对细节感兴趣，你可以查阅Simon Haykin的《神经网络——全面基础》这本书。
- en: The momentum
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动量
- en: 'Like any gradient-based method, there is a risk of falling into a local minimum.
    To mitigate this risk, we can add another term to the weight update rule called
    momentum, which takes into consideration the last variation of weight:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如同任何基于梯度的方法一样，存在陷入局部最小值的风险。为了减轻这种风险，我们可以在权重更新规则中添加另一个称为动量的项，它考虑了权重的最后一个变化：
- en: '![The momentum](img/B05964_03_14_05.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![动量](img/B05964_03_14_05.jpg)'
- en: Here, μ is a momentum rate and ![The momentum](img/B05964_03_14_06.jpg) is the
    last delta weight. This gives an additional step to the update, therefore attenuating
    the oscillations in the error hyperspace.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，μ是动量率，![动量](img/B05964_03_14_06.jpg)是最后一个delta权重。这为更新提供了一个额外的步骤，因此减弱了误差超空间中的振荡。
- en: Coding the backpropagation
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码反向传播
- en: 'Let''s define the class backpropagation in the package `edu.packt.neural.learn`.
    Since this learning algorithm is a generalization of the `DeltaRule`, this class
    may inherit and override the features already defined in Delta Rule. Three additional
    attributes included in this class are the momentum rate, the delta neuron, and
    the last delta weight arrays:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在`edu.packt.neural.learn`包中定义`backpropagation`类。由于这个学习算法是`DeltaRule`的泛化，这个类可能继承并覆盖Delta规则中已经定义的特性。这个类包含的三个附加属性是动量率、delta神经元和最后一个delta权重数组：
- en: '[PRE2]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The constructor will have the same arguments as for the `DeltaRule` class,
    adding the calls to methods for initialization of the `deltaNeuron` and `lastDeltaWeights`
    arrays:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数将具有与`DeltaRule`类相同的参数，并添加对`deltaNeuron`和`lastDeltaWeights`数组初始化方法的调用：
- en: '[PRE3]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `train()` method will work in a similar way as in the `DeltaRule` class;
    the additional component is the backward step, whereby the error is `backpropagated`
    throughout the neural layers up to the input:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`train()`方法将以与`DeltaRule`类类似的方式工作；额外的组件是反向步骤，其中错误在整个神经网络层中`反向传播`到输入：'
- en: '[PRE4]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The role of the backward step is to determine the delta weights by means of
    error backpropagation, from the output layer down to the first hidden layer:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 向后步骤的作用是通过误差反向传播来确定delta权重，从输出层到第一隐藏层：
- en: '[PRE5]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The backpropagation step is performed in the method `calcDeltaWeight()`. The
    momentum will be added only before updating the weights because it should recall
    the last delta weight determined:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播步骤是在`calcDeltaWeight()`方法中执行的。动量只会在更新权重之前添加，因为它应该调用确定的最后一个delta权重：
- en: '[PRE6]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note the calculation of the `_deltaNeuron` is different for the output and the
    hidden layers, but for both of them the derivative is used. To facilitate this
    task, we've added the `derivative()` method to the class `Neuron`. Details can
    be found in *Annex III* documentation. At the end, the input corresponding to
    the weight is multiplied to the delta weight calculated.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到对于输出层和隐藏层，`_deltaNeuron`的计算是不同的，但它们都使用了导数。为了便于这项任务，我们在`Neuron`类中添加了`derivative()`方法。详细信息可以在*附录III*文档中找到。最后，将对应于权重的输入乘以计算出的delta权重。
- en: 'The weight update is performed by the method `applyNewWeights()`. To save space,
    we are not going to write here the whole method body, but only the core where
    the weight update is performed:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 权重更新是通过`applyNewWeights()`方法执行的。为了节省空间，我们不会在这里写出整个方法体，而只写出执行权重更新的核心部分：
- en: '[PRE7]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the code listing, `l` represents the layer, `j` the neuron, and `i` the
    input to the weight. For the output layer, l will be equal to the number of hidden
    layers (exceeding the Java array limits), so the `NeuralLayer` called is as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码列表中，`l`代表层，`j`代表神经元，`i`代表输入到权重的输入。对于输出层，`l`将等于隐藏层的数量（超出Java数组限制），因此调用的`NeuralLayer`如下：
- en: '[PRE8]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This class can be used exactly the same way as `DeltaRule`:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类可以像`DeltaRule`一样使用：
- en: '[PRE9]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: At the end of this chapter, we will make a comparison of the Delta Rule for
    the perceptrons with the backpropagation with a multi-layer perceptron, trying
    to solve the XOR problem.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，我们将对感知器的Delta规则与多层感知器的反向传播进行比较，尝试解决XOR问题。
- en: Levenberg-Marquardt algorithm
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Levenberg-Marquardt算法
- en: The backpropagation algorithm, like all gradient-based methods, usually presents
    slow convergence, especially when it falls in a zig-zag situation, when the weights
    are changed to almost the same value every two iterations. This drawback was studied
    in problems such as curve-fitting interpolation by Kenneth Levenberg in 1944,
    and later by Donald Marquart in 1963, who developed a method for finding coefficients
    based on the Gauss Newton algorithm and the gradient descent, so therefrom comes
    the name of the algorithm.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播算法，像所有基于梯度的方法一样，通常收敛速度较慢，尤其是在它陷入曲折情况时，当权重每两次迭代都改变到几乎相同的值时。这种缺点在1944年由Kenneth
    Levenberg在曲线拟合插值问题中进行了研究，后来在1963年由Donald Marquart进行了研究，他开发了一种基于高斯-牛顿算法和梯度下降法寻找系数的方法，因此算法得名。
- en: 'The LM algorithm deals with some optimization terms which are beyond the scope
    of this book, but in the references section, the reader will find good resources
    to learn more about these concepts, so we will present the method in a simpler
    way. Let''s suppose we have a list of inputs x and outputs *t*:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: LM 算法处理一些超出本书范围的优化项，但在参考文献部分，读者将找到学习这些概念的好资源，因此我们将以更简单的方式介绍这种方法。假设我们有一个输入 x
    和输出 *t* 的列表：
- en: '![Levenberg-Marquardt algorithm](img/B05964_03_14_07.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![Levenberg-Marquardt 算法](img/B05964_03_14_07.jpg)'
- en: 'We have seen that a neural network has the property to map inputs to outputs
    just like a nonlinear function f with coefficients *W* (weights and bias):'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，神经网络具有将输入映射到输出的特性，就像具有系数 *W*（权重和偏置）的非线性函数 f 一样：
- en: '![Levenberg-Marquardt algorithm](img/B05964_03_14_08.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![Levenberg-Marquardt 算法](img/B05964_03_14_08.jpg)'
- en: 'The nonlinear function will produce values different than the outputs T; that''s
    because we marked the variable Y in the equation. The Levenberg-Marquardt algorithm
    works over a Jacobian matrix, which is a matrix of all partial derivatives in
    regard to each weight and bias for each data row. So the Jacobian matrix has the
    following format:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性函数将产生与输出 T 不同的值；这是因为我们在方程中标记了变量 Y。Levenberg-Marquardt 算法在一个雅可比矩阵上工作，这是一个关于每个数据行中每个权重和偏置的所有偏导数的矩阵。因此，雅可比矩阵具有以下格式：
- en: '![Levenberg-Marquardt algorithm](img/B05964_03_14_09.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![Levenberg-Marquardt 算法](img/B05964_03_14_09.jpg)'
- en: 'Here, *k* is the total number of data points and p is the total number of weights
    and bias. In the Jacobian matrix, all weights and bias are stored serially in
    a single row. The elements of the Jacobian Matrix are calculated from the gradients:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*k* 是数据点的总数，p 是权重和偏置的总数。在雅可比矩阵中，所有权重和偏置都按顺序存储在单行中。雅可比矩阵的元素是从梯度计算出来的：
- en: '![Levenberg-Marquardt algorithm](img/B05964_03_14_10.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![Levenberg-Marquardt 算法](img/B05964_03_14_10.jpg)'
- en: The partial derivative of the error E in relation to each weight is calculated
    in the backpropagation algorithm, so this algorithm is going to run the backpropagation
    step as well.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播算法中，计算了误差 E 对每个权重的偏导数，因此这个算法将运行反向传播步骤。
- en: 'In every optimization problem, one wishes to minimize the total error:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一个优化问题中，人们都希望最小化总误差：
- en: '![Levenberg-Marquardt algorithm](img/B05964_03_14_11.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![Levenberg-Marquardt 算法](img/B05964_03_14_11.jpg)'
- en: 'Here, *W* (weights and bias in the NN case) are the variables to optimize.
    The optimization algorithm updates W by adding ΔW. By applying some algebra, the
    last equation can be extended to this one:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*W*（在神经网络情况下为权重和偏置）是需要优化的变量。优化算法通过添加 ΔW 来更新 W。通过应用一些代数运算，最后一个方程可以扩展为以下形式：
- en: '![Levenberg-Marquardt algorithm](img/B05964_03_14_12.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![Levenberg-Marquardt 算法](img/B05964_03_14_12.jpg)'
- en: 'Converting to the vector and notation:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 转换为向量和符号：
- en: '![Levenberg-Marquardt algorithm](img/B05964_03_14_13.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![Levenberg-Marquardt 算法](img/B05964_03_14_13.jpg)'
- en: 'Finally, by setting the error E to zero, we get the Levenberg-Marquardt equation
    after some manipulation:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过将误差 E 设置为零，经过一些操作后，我们得到 Levenberg-Marquardt 方程：
- en: '![Levenberg-Marquardt algorithm](img/B05964_03_14_14.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![Levenberg-Marquardt 算法](img/B05964_03_14_14.jpg)'
- en: This is the weight update rule. As can be seen, it involves matrix operations
    such as transposition and inversion. The Greek letter λ is the damping factor,
    an equivalent for the learning rate.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这是权重更新规则。如所见，它涉及到矩阵运算，如转置和求逆。希腊字母 λ 是阻尼因子，是学习率的等效物。
- en: Coding the Levenberg-Marquardt with matrix algebra
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用矩阵代数编码 Levenberg-Marquardt
- en: In order to effectively implement the LM algorithm, it is very useful to work
    with matrix algebra. To address that, we defined a class called `Matrix` in the
    package `edu.packt.neuralnet.math`, including all the matrix operations, such
    as multiplication, inverse, and LU decomposition, among others. The reader may
    refer to the documentation to find out more about this class.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地实现LM算法，与矩阵代数一起工作非常有用。为了解决这个问题，我们在`edu.packt.neuralnet.math`包中定义了一个名为`Matrix`的类，包括所有矩阵运算，如乘法、逆运算和LU分解等。读者可以参考文档以了解更多关于这个类的信息。
- en: 'The Levenberg-Marquardt algorithm uses many features of the backpropagation
    algorithm; that''s why we inherit this class from backpropagation. Some new attributes
    are included:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Levenberg-Marquardt算法使用了反向传播算法的许多特性；这就是为什么我们从这个类继承。包含了一些新的属性：
- en: '**Jacobian matrix**: This is the matrix containing the partial derivatives
    to each weight for all training records'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**雅可比矩阵**：这是一个包含所有训练记录对每个权重的偏导数的矩阵'
- en: '**Damping factor**'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**阻尼因子**'
- en: '**Error backpropagation**: This array has the same function of `deltaNeuron`,
    but its calculation differs a little to each neural output; that''s why we defined
    it in a separate attribute'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**误差反向传播**：这个数组与`deltaNeuron`具有相同的功能，但它的计算对每个神经网络输出略有不同；这就是为什么我们将其定义为单独的属性'
- en: '**Error LMA**: The error in the matrix form:'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**误差LMA**：矩阵形式的误差：'
- en: '[PRE10]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Basically, the train function is the same as that of the backpropagation, except
    for the following calculation of the Jacobian and error matrices and the damping
    update:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，训练函数与反向传播相同，除了以下雅可比矩阵和误差矩阵的计算以及阻尼更新：
- en: '[PRE11]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The loop where it goes over the training dataset calls the method `calculateJacobian`.
    This method works on the error backpropagation evaluated in the method backward:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历训练数据集的循环调用`calculateJacobian`方法。该方法在`backward`方法中评估误差反向传播：
- en: '[PRE12]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the code listing, `p` is the input connecting to the neuron (when it is equal
    to the number of neuron inputs, it represents the bias), `k` is the neuron, l
    is the layer, m is the neural output, `i` is a sequential index of the record,
    and `j` is the sequential index of the weight or bias, according to the layer
    and neuron in which it is located. Note that after setting the value in the Jacobian
    matrix, `j` is incremented.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码列表中，`p`是连接到神经元的输入（当它等于神经元输入的数量时，它代表偏置），`k`是神经元，`l`是层，`m`是神经网络输出，`i`是记录的顺序索引，`j`是权重或偏置的顺序索引，根据其在层和神经元中的位置。请注意，在雅可比矩阵中设置值后，`j`会增加。
- en: 'The weight update is performed by means of determining the `deltaWeight` matrix:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 权重更新是通过确定`deltaWeight`矩阵来完成的：
- en: '[PRE13]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The previous code refers to the matrix algebra shown in the section presenting
    the algorithm. The matrix `deltaWeight` contains the steps for each weight in
    the neural network. In the following code, `k` is the neuron, `j` is the input,
    and `l` is the layer:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码引用了算法展示部分中所示的矩阵代数。`deltaWeight`矩阵包含神经网络中每个权重的步骤。在以下代码中，`k`是神经元，`j`是输入，`l`是层：
- en: '[PRE14]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that the weights are saved in the `lastWeights` array, so they can be recovered
    if the error gets worse.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，权重保存在`lastWeights`数组中，因此如果误差变差，可以恢复它们。
- en: Extreme learning machines
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 极端学习机
- en: Taking advantage of the matrix algebra, **extreme learning machines** (**ELMs**)
    are able to converge learning very fast. This learning algorithm has one limitation,
    since it is applied only on neural networks containing one single hidden layer.
    In practice, one hidden layer works pretty fine for most applications.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 利用矩阵代数，**极端学习机**（**ELMs**）能够非常快地收敛学习。这个学习算法有一个限制，因为它只应用于包含单个隐藏层的神经网络。在实践中，一个隐藏层对大多数应用来说都相当不错。
- en: 'Representing the neural network in matrix algebra, for the following neural
    network:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 用矩阵代数表示神经网络，对于以下神经网络：
- en: '![Extreme learning machines](img/B05964_03_14.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![极端学习机](img/B05964_03_14.jpg)'
- en: 'We have the corresponding equations:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有相应的方程：
- en: '![Extreme learning machines](img/B05964_03_15_01.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![极端学习机](img/B05964_03_15_01.jpg)'
- en: Here, *H* is the output of the hidden layer, *g()* is the activation function
    of the hidden layer, *Xi* is the *ith* input record, *Wj* is the weight vector
    for the *jth* hidden neuron, *bj* is the bias of the *jth* hidden neuron, βp is
    the weight vector for the output *p*, and *Y* is the output generated by the neural
    network.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*H*是隐藏层的输出，*g()*是隐藏层的激活函数，*Xi*是第*i*个输入记录，*Wj*是第*j*个隐藏神经元的权重向量，*bj*是第*j*个隐藏神经元的偏置，βp是输出*p*的权重向量，*Y*是神经网络生成的输出。
- en: 'In the ELM algorithm, the hidden layer weights are generated randomly, while
    the output weights are adjusted according to a least squares approach:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在ELM算法中，隐藏层权重是随机生成的，而输出权重则根据最小二乘法进行调整：
- en: '![Extreme learning machines](img/B05964_03_15_02.jpg)![Extreme learning machines](img/B05964_03_15_03.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![极端学习机](img/B05964_03_15_02.jpg)![极端学习机](img/B05964_03_15_03.jpg)'
- en: Here, T is the target output training dataset.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，T是目标输出训练数据集。
- en: 'This algorithm is implemented in a class called `ELM` in the same package as
    the other training algorithms. This class will inherit from `DeltaRule`, which
    has all the basic properties for supervised learning algorithms:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法在名为`ELM`的类中实现，该类与其他训练算法位于同一包中。这个类将继承自`DeltaRule`，它具有所有监督学习算法的基本属性：
- en: '[PRE15]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In this class, we define the matrices *H* and *T*, which will be later used
    for output weight calculation. The constructor is similar to the other training
    algorithms, except for the fact that this algorithm works only on batch mode.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个类中，我们定义了*H*和*T*矩阵，这些矩阵将用于后续的输出权重计算。构造函数与其他训练算法类似，只是这个算法只工作在批处理模式下。
- en: 'Since this training algorithm takes only one epoch, the train method forwards
    all training records to build the *H* matrix. Then, it calculates the output weights:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个训练算法只需要一个迭代周期，train方法将所有训练记录转发以构建*H*矩阵。然后，它计算输出权重：
- en: '[PRE16]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `buildMatrices` method only places the output of the hidden layer to its
    corresponding row in the *H* matrix. The output weights are adjusted in the `applyNewWeights`
    method:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '`buildMatrices`方法只将隐藏层的输出放置在*H*矩阵的对应行中。输出权重在`applyNewWeights`方法中调整：'
- en: '[PRE17]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Practical example 1 – the XOR case with delta rule and backpropagation
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际示例 1 – 使用delta规则和反向传播的XOR情况
- en: 'Now let''s see the multilayer perceptron in action. We coded the example `XORTest.java`,
    which basically creates two neural networks with the following features:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看多层感知器的作用。我们编写了示例`XORTest.java`，它基本上创建了两个具有以下特征的神经网络：
- en: '| Neural Network | Perceptron | Multi-layer Percepetron |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 神经网络 | 感知器 | 多层感知器 |'
- en: '| --- | --- | --- |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Inputs | 2 | 2 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | 2 | 2 |'
- en: '| Outputs | 1 | 1 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | 1 | 1 |'
- en: '| Hidden Layers | 0 | 1 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 隐藏层 | 0 | 1 |'
- en: '| Hidden Neurons in each layer | 0 | 2 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 每层的隐藏神经元 | 0 | 2 |'
- en: '| Hidden Layer Activation Function | Non | Sigmoid |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 隐藏层激活函数 | 非 | Sigmoid |'
- en: '| Output Layer Activation Function | Linear | Linear |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 输出层激活函数 | 线性 | 线性 |'
- en: '| Training Algorithm | Delta Rule | Backpropagation |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 训练算法 | Delta规则 | 反向传播 |'
- en: '| Learning Rate | 0.1 | 0.3Momentum 0.6 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 0.1 | 0.3 动量 0.6 |'
- en: '| Max Epochs | 4000 | 4000 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 最大迭代次数 | 4000 | 4000 |'
- en: '| Min. overall error | 0.1 | 0.01 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 总体最小误差 | 0.1 | 0.01 |'
- en: 'In Java, this is coded as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中，这可以这样编码：
- en: '[PRE18]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, we define the dataset and the learning algorithms:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义数据集和学习算法：
- en: '[PRE19]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The training is then performed for both algorithms. As expected, the XOR case
    is not linearly separable by one single layer perceptron. The neural network runs
    the training but unsuccessfully:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 然后对两种算法进行了训练。正如预期的那样，XOR情况不能由单个感知器层线性分离。神经网络运行了训练但未能成功：
- en: '[PRE20]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![Practical example 1 – the XOR case with delta rule and backpropagation](img/B05964_03_15.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![实际示例 1 – 使用delta规则和反向传播的XOR情况](img/B05964_03_15.jpg)'
- en: 'But the backpropagation algorithm for the multilayer perceptron manages to
    learn the XOR function after 39 epochs:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 但多层感知器的反向传播算法在39个迭代周期后成功学习了XOR函数：
- en: '[PRE21]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Practical example 1 – the XOR case with delta rule and backpropagation](img/B05964_03_16.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![实际示例 1 – 使用delta规则和反向传播的XOR情况](img/B05964_03_16.jpg)'
- en: Practical example 2 – predicting enrolment status
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际示例 2 – 预测注册状态
- en: 'In Brazil, one of the ways for a person to enter university consists of taking
    an exam and if he/she achieves the minimum grade for the course that he/she is
    seeking, then he/she can enroll. To demonstrate the backpropagation algorithm,
    let us consider this scenario. The data shown in the table was collected from
    a university database. The second column represents the person''s gender (one
    means female and zero means male); the third column has grades scaled by 100 and
    the last column is formed by two neurons (1,0 means performed enrollment and 0,1
    means waiver enrollment):'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在巴西，一个人进入大学的一种方式是通过参加考试，如果他/她达到了所申请课程的最低分数，那么他/她就可以注册入学。为了演示反向传播算法，让我们考虑这个场景。表中显示的数据是从大学数据库中收集的。第二列代表个人的性别（1表示女性，0表示男性）；第三列是按100分缩放的分数，最后一列由两个神经元组成（1,0表示完成注册，0,1表示放弃注册）：
- en: '| Sample | Gender | Grade | EnrollmentStatus |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 样本 | 性别 | 分数 | 注册状态 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | 1 | 0.73 | 1 0 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 0.73 | 1 0 |'
- en: '| 2 | 1 | 0.81 | 1 0 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 | 0.81 | 1 0 |'
- en: '| 3 | 1 | 0.86 | 1 0 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1 | 0.86 | 1 0 |'
- en: '| 4 | 0 | 0.65 | 1 0 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0 | 0.65 | 1 0 |'
- en: '| 5 | 0 | 0.45 | 1 0 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0 | 0.45 | 1 0 |'
- en: '| 6 | 1 | 0.70 | 0 1 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 1 | 0.70 | 0 1 |'
- en: '| 7 | 0 | 0.51 | 0 1 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0 | 0.51 | 0 1 |'
- en: '| 8 | 1 | 0.89 | 0 1 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 1 | 0.89 | 0 1 |'
- en: '| 9 | 1 | 0.79 | 0 1 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 1 | 0.79 | 0 1 |'
- en: '| 10 | 0 | 0.54 | 0 1 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0 | 0.54 | 0 1 |'
- en: '[PRE22]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We create a neural network containing three neurons in the hidden layer, as
    shown in the following figure:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个包含三个隐藏层神经元的神经网络，如图所示：
- en: '![Practical example 2 – predicting enrolment status](img/B05964_03_17.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![实际示例 2 – 预测注册状态](img/B05964_03_17.jpg)'
- en: '[PRE23]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We''ve also set up the learning algorithms Levenberg-Marquardt and extreme
    learning machines:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还设置了学习算法Levenberg-Marquardt和极端学习机：
- en: '[PRE24]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Running the training, we find that the training was successful. For the Levenberg-Marquardt
    algorithm, the minimum satisfied error was found after nine epochs:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 运行训练后，我们发现训练成功。对于Levenberg-Marquardt算法，在九个epoch后找到了满足最小误差：
- en: '![Practical example 2 – predicting enrolment status](img/B05964_03_18.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![实际示例 2 – 预测注册状态](img/B05964_03_18.jpg)'
- en: 'And the extreme learning machine found an error near to zero:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 极端学习机发现了一个接近零的错误：
- en: '![Practical example 2 – predicting enrolment status](img/B05964_03_19.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![实际示例 2 – 预测注册状态](img/B05964_03_19.jpg)'
- en: Summary
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we''ve seen how perceptrons can be applied to solve linear
    separation problems, but also their limitations in classifying nonlinear data.
    To suppress those limitations, we presented **multi-layer perceptrons** (**MLPs**)
    and new training algorithms: backpropagation, Levenberg-Marquardt, and extreme
    learning machines. We''ve also seen some classes of problems which MLPs can be
    applied to, such as classification and regression. The Java implementation explored
    the power of the backpropagation algorithm in updating the weights both in the
    output layer and the hidden layer. Two practical applications were shown to demonstrate
    the MLPs for the solution of problems with the three learning algorithms.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了感知器如何应用于解决线性分离问题，但也看到了它们在分类非线性数据时的局限性。为了抑制这些局限性，我们介绍了**多层感知器**（**MLPs**）和新的训练算法：反向传播、Levenberg-Marquardt和极端学习机。我们还看到了一些问题类别，MLPs可以应用于，例如分类和回归。Java实现探讨了反向传播算法在更新输出层和隐藏层权重方面的能力。展示了两个实际应用，以演示使用三种学习算法解决问题的MLPs。
