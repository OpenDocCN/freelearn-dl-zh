["```py\n    %run -i \"../util/file_utils.ipynb\"\n    %run -i \"../util/lang_utils.ipynb\"\n    ```", "```py\n    from datasets import load_dataset\n    train_dataset = load_dataset(\"rotten_tomatoes\",\n        split=\"train[:15%]+train[-15%:]\")\n    test_dataset = load_dataset(\"rotten_tomatoes\",\n        split=\"test[:15%]+test[-15%:]\")\n    ```", "```py\n    print(len(train_dataset))\n    print(len(test_dataset))\n    ```", "```py\n    2560\n    320\n    ```", "```py\n    class POS_vectorizer:\n        def __init__(self, spacy_model):\n            self.model = spacy_model\n        def vectorize(self, input_text):\n            doc = self.model(input_text)\n            vector = []\n            vector.append(len(doc))\n            pos = {\"VERB\":0, \"NOUN\":0, \"PROPN\":0, \"ADJ\":0,\n                \"ADV\":0, \"AUX\":0, \"PRON\":0, \"NUM\":0, \"PUNCT\":0}\n            for token in doc:\n                if token.pos_ in pos:\n                    pos[token.pos_] += 1\n            vector_values = list(pos.values())\n            vector = vector + vector_values\n            return vector\n    ```", "```py\n    sample_text = train_dataset[0][\"text\"]\n    vectorizer = POS_vectorizer(small_model)\n    vector = vectorizer.vectorize(sample_text)\n    ```", "```py\n    print(sample_text)\n    print(vector)\n    ```", "```py\n    the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n    [38, 3, 8, 3, 4, 1, 3, 1, 0, 5]\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    train_df = train_dataset.to_pandas()\n    train_df.sample(frac=1)\n    test_df = test_dataset.to_pandas()\n    train_df[\"vector\"] = train_df[\"text\"].apply(\n        lambda x: vectorizer.vectorize(x))\n    test_df[\"vector\"] = test_df[\"text\"].apply(\n        lambda x: vectorizer.vectorize(x))\n    X_train = np.stack(train_df[\"vector\"].values, axis=0)\n    X_test = np.stack(test_df[\"vector\"].values, axis=0)\n    y_train = train_df[\"label\"].to_numpy()\n    y_test = test_df[\"label\"].to_numpy()\n    ```", "```py\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import classification_report\n    clf = LogisticRegression(C=0.1)\n    clf = clf.fit(X_train, y_train)\n    ```", "```py\n    test_df[\"prediction\"] = test_df[\"vector\"].apply(\n        lambda x: clf.predict([x])[0])\n    print(classification_report(test_df[\"label\"], \n        test_df[\"prediction\"]))\n    ```", "```py\n                  precision    recall  f1-score   support\n               0       0.59      0.54      0.56       160\n               1       0.57      0.62      0.60       160\n        accuracy                           0.58       320\n       macro avg       0.58      0.58      0.58       320\n    weighted avg       0.58      0.58      0.58       320\n    ```", "```py\n    from datasets import load_dataset\n    import pandas as pd\n    import numpy as np\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import classification_report\n    ```", "```py\n    def load_train_test_dataset_pd():\n        train_dataset = load_dataset(\"rotten_tomatoes\",\n            split=\"train[:15%]+train[-15%:]\")\n        test_dataset = load_dataset(\"rotten_tomatoes\",\n            split=\"test[:15%]+test[-15%:]\")\n        train_df = train_dataset.to_pandas()\n        train_df.sample(frac=1)\n        test_df = test_dataset.to_pandas()\n        return (train_df, test_df)\n    ```", "```py\n    def create_train_test_data(train_df, test_df, vectorize):\n        train_df[\"vector\"] = train_df[\"text\"].apply(\n            lambda x: vectorize(x))\n        test_df[\"vector\"] = test_df[\"text\"].apply(\n            lambda x: vectorize(x))\n        X_train = np.stack(train_df[\"vector\"].values, axis=0)\n        X_test = np.stack(test_df[\"vector\"].values, axis=0)\n        y_train = train_df[\"label\"].to_numpy()\n        y_test = test_df[\"label\"].to_numpy()\n        return (X_train, X_test, y_train, y_test)\n    ```", "```py\n    def train_classifier(X_train, y_train):\n        clf = LogisticRegression(C=0.1)\n        clf = clf.fit(X_train, y_train)\n        return clf\n    ```", "```py\n    def test_classifier(test_df, clf):\n        test_df[\"prediction\"] = test_df[\"vector\"].apply(\n            lambda x: clf.predict([x])[0])\n        print(classification_report(test_df[\"label\"],         test_df[\"prediction\"]))\n    ```", "```py\n    %run -i \"../util/util_simple_classifier.ipynb\"\n    from sklearn.feature_extraction.text import CountVectorizer\n    import sys\n    ```", "```py\n    (train_df, test_df) = load_train_test_dataset_pd()\n    ```", "```py\n    vectorizer = CountVectorizer(max_df=0.4)\n    X = vectorizer.fit_transform(train_df[\"text\"])\n    print(X)\n    ```", "```py\n      (0, 6578)  1\n      (0, 4219)  1\n      (0, 2106)  1\n      (0, 8000)  2\n      (0, 717)  1\n      (0, 42)  1\n      (0, 1280)  1\n      (0, 5260)  1\n      (0, 1607)  1\n      (0, 7889)  1\n      (0, 3630)  1\n    …\n    ```", "```py\n    dense_matrix = X.todense()\n    print(dense_matrix)\n    ```", "```py\n    [[0 0 0 ... 0 0 0]\n     [0 0 0 ... 0 0 0]\n     [0 0 0 ... 0 0 0]\n     ...\n     [0 0 0 ... 0 0 0]\n     [0 0 0 ... 0 0 0]\n     [0 0 0 ... 0 0 0]]\n    ```", "```py\n    print(vectorizer.get_feature_names_out())\n    print(len(vectorizer.get_feature_names_out()))\n    ```", "```py\n    ['10' '100' '101' ... 'zone' 'ótimo' 'últimos']\n    8856\n    ```", "```py\n    print(vectorizer.stop_words_)\n    ```", "```py\n    {'and', 'the', 'of'}\n    ```", "```py\n    first_review = test_df['text'].iat[0]\n    print(first_review)\n    ```", "```py\n    lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\n    ```", "```py\n    sparse_vector = vectorizer.transform([first_review])\n    print(sparse_vector)\n    dense_vector = sparse_vector.todense()\n    np.set_printoptions(threshold=sys.maxsize)\n    print(dense_vector)\n    np.set_printoptions(threshold=False)\n    ```", "```py\n      (0, 955)  1\n      (0, 3968)  1\n      (0, 4451)  1\n      (0, 4562)  1\n      (0, 4622)  1\n      (0, 4688)  1\n      (0, 4779)  1\n      (0, 4792)  1\n      (0, 5764)  1\n      (0, 7547)  1\n      (0, 7715)  1\n      (0, 8000)  1\n      (0, 8734)  1\n    [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n      0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n    …]]\n    ```", "```py\n    vectorizer = CountVectorizer(max_df=300)\n    X = vectorizer.fit_transform(train_df[\"text\"])\n    print(vectorizer.stop_words_)\n    ```", "```py\n    {'but', 'this', 'its', 'as', 'to', 'and', 'the', 'is', 'film', 'for', 'it', 'an', 'of', 'that', 'movie', 'with', 'in'}\n    ```", "```py\n    vectorizer = CountVectorizer(stop_words=['the', 'this',\n        'these', 'in', 'at', 'for'])\n    X = vectorizer.fit_transform(train_df[\"text\"])\n    ```", "```py\n    vectorizer = CountVectorizer(max_df=0.8)\n    (train_df, test_df) = load_train_test_dataset_pd()\n    X = vectorizer.fit_transform(train_df[\"text\"])\n    vectorize = lambda x: vectorizer.transform([x]).toarray()[0]\n    (X_train, X_test, y_train, y_test) = create_train_test_data(\n        train_df, test_df, vectorize)\n    clf = train_classifier(X_train, y_train)\n    test_classifier(test_df, clf)\n    ```", "```py\n                  precision    recall  f1-score   support\n               0       0.74      0.72      0.73       160\n               1       0.73      0.75      0.74       160\n        accuracy                           0.74       320\n       macro avg       0.74      0.74      0.74       320\n    weighted avg       0.74      0.74      0.74       320\n    ```", "```py\n    %run -i \"../util/util_simple_classifier.ipynb\"\n    from sklearn.feature_extraction.text import CountVectorizer\n    ```", "```py\n    (train_df, test_df) = load_train_test_dataset_pd()\n    ```", "```py\n    bigram_vectorizer = CountVectorizer(\n        ngram_range=(1, 2), max_df=0.8)\n    X = bigram_vectorizer.fit_transform(train_df[\"text\"])\n    ```", "```py\n    print(bigram_vectorizer.get_feature_names_out())\n    print(len(bigram_vectorizer.get_feature_names_out()))\n    ```", "```py\n    ['10' '10 inch' '10 set' ... 'ótimo esforço' 'últimos' 'últimos tiempos']\n    40552\n    ```", "```py\n    first_review = test_df['text'].iat[0]\n    dense_vector = bigram_vectorizer.transform(\n        [first_review]).todense()\n    print(dense_vector)\n    ```", "```py\n    [[0 0 0 ... 0 0 0]]\n    ```", "```py\n    vectorize = \\\n        lambda x: bigram_vectorizer.transform([x]).toarray()[0]\n    (X_train, X_test, y_train, y_test) = create_train_test_data(\n        train_df, test_df, vectorize)\n    clf = train_classifier(X_train, y_train)\n    test_classifier(test_df, clf)\n    ```", "```py\n                  precision    recall  f1-score   support\n               0       0.72      0.75      0.73       160\n               1       0.74      0.71      0.72       160\n        accuracy                           0.73       320\n       macro avg       0.73      0.73      0.73       320\n    weighted avg       0.73      0.73      0.73       320\n    ```", "```py\n    %run -i \"../util/util_simple_classifier.ipynb\"\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    ```", "```py\n    (train_df, test_df) = load_train_test_dataset_pd()\n    ```", "```py\n    vectorizer = TfidfVectorizer(max_df=300)\n    vectorizer.fit(train_df[\"text\"])\n    ```", "```py\n    print(vectorizer.get_feature_names_out())\n    print(len(vectorizer.get_feature_names_out()))\n    ```", "```py\n    ['10' '100' '101' ... 'zone' 'ótimo' 'últimos']\n    8842\n    ```", "```py\n    first_review = test_df['text'].iat[0]\n    dense_vector = vectorizer.transform([first_review]).todense()\n    print(dense_vector)\n    ```", "```py\n    [[0\\. 0\\. 0\\. ... 0\\. 0\\. 0.]]\n    ```", "```py\n    vectorize = lambda x: vectorizer.transform([x]).toarray()[0]\n    (X_train, X_test, y_train, y_test) = create_train_test_data(\n        train_df, test_df, vectorize)\n    clf = train_classifier(X_train, y_train)\n    test_classifier(test_df, clf)\n    ```", "```py\n                  precision    recall  f1-score   support\n               0       0.76      0.72      0.74       160\n               1       0.74      0.78      0.76       160\n        accuracy                           0.75       320\n       macro avg       0.75      0.75      0.75       320\n    weighted avg       0.75      0.75      0.75       320\n    ```", "```py\n    tfidf_char_vectorizer = TfidfVectorizer(\n        analyzer='char_wb', ngram_range=(1,5))\n    tfidf_char_vectorizer = tfidf_char_vectorizer.fit(\n        train_df[\"text\"])\n    ```", "```py\n    print(list(tfidf_char_vectorizer.get_feature_names_out()))\n    print(len(tfidf_char_vectorizer.get_feature_names_out()))\n    ```", "```py\n    [' ', ' !', ' ! ', ' \"', ' \" ', ' $', ' $5', ' $50', ' $50-', ' $9', ' $9 ', ' &', ' & ', \" '\", \" ' \", \" '5\", \" '50\", \" '50'\", \" '6\", \" '60\", \" '60s\", \" '7\", \" '70\", \" '70'\", \" '70s\", \" '[\", \" '[h\", \" '[ho\", \" 'a\", \" 'a \", \" 'a'\", \" 'a' \", \" 'ab\", \" 'aba\", \" 'ah\", \" 'ah \", \" 'al\", \" 'alt\", \" 'an\", \" 'ana\", \" 'ar\", \" 'are\", \" 'b\", \" 'ba\", \" 'bar\", \" 'be\", \" 'bee\", \" 'bes\", \" 'bl\", \" 'blu\", \" 'br\", \" 'bra\", \" 'bu\", \" 'but\", \" 'c\", \" 'ch\", \" 'cha\", \" 'co\", \" 'co-\", \" 'com\", \" 'd\", \" 'di\", \" 'dif\", \" 'do\", \" 'dog\", \" 'du\", \" 'dum\", \" 'e\", \" 'ed\", \" 'edg\", \" 'em\", \" 'em \", \" 'ep\", \" 'epi\", \" 'f\", \" 'fa\", \" 'fac\", \" 'fat\", \" 'fu\", \" 'fun\", \" 'g\", \" 'ga\", \" 'gar\", \" 'gi\", \" 'gir\", \" 'gr\", \" 'gra\", \" 'gu\", \" 'gue\", \" 'guy\", \" 'h\", \" 'ha\", \" 'hav\", \" 'ho\", \" 'hos\", \" 'how\", \" 'i\", \" 'i \", \" 'if\", \" 'if \", \" 'in\", \" 'in \", \" 'is\",…]\n    51270\n    ```", "```py\n    vectorize = lambda x: tfidf_char_vectorizer.transform([\n        x]).toarray()[0]\n    (X_train, X_test, y_train, y_test) = create_train_test_data(\n        train_df, test_df, vectorize)\n    clf = train_classifier(X_train, y_train)\n    test_classifier(test_df, clf)\n    ```", "```py\n                  precision    recall  f1-score   support\n               0       0.74      0.74      0.74       160\n               1       0.74      0.74      0.74       160\n        accuracy                           0.74       320\n       macro avg       0.74      0.74      0.74       320\n    weighted avg       0.74      0.74      0.74       320\n    ```", "```py\n    %run -i \"../util/simple_classifier.ipynb\"\n    ```", "```py\n    import gensim\n    ```", "```py\n    model = gensim.models.KeyedVectors.load_word2vec_format(\n        '../data/GoogleNews-vectors-negative300.bin.gz',\n        binary=True)\n    ```", "```py\n    vec_king = model['king']\n    print(vec_king)\n    ```", "```py\n    [ 1.25976562e-01  2.97851562e-02  8.60595703e-03  1.39648438e-01\n     -2.56347656e-02 -3.61328125e-02  1.11816406e-01 -1.98242188e-01\n      5.12695312e-02  3.63281250e-01 -2.42187500e-01 -3.02734375e-01\n     -1.77734375e-01 -2.49023438e-02 -1.67968750e-01 -1.69921875e-01\n      3.46679688e-02  5.21850586e-03  4.63867188e-02  1.28906250e-01\n      1.36718750e-01  1.12792969e-01  5.95703125e-02  1.36718750e-01\n      1.01074219e-01 -1.76757812e-01 -2.51953125e-01  5.98144531e-02\n      3.41796875e-01 -3.11279297e-02  1.04492188e-01  6.17675781e-02  …]\n    ```", "```py\n    print(model.most_similar(['apple'], topn=15))\n    print(model.most_similar(['tomato'], topn=15))\n    ```", "```py\n    [('apples', 0.720359742641449), ('pear', 0.6450697183609009), ('fruit', 0.6410146355628967), ('berry', 0.6302295327186584), ('pears', 0.613396167755127), ('strawberry', 0.6058260798454285), ('peach', 0.6025872826576233), ('potato', 0.5960935354232788), ('grape', 0.5935863852500916), ('blueberry', 0.5866668224334717), ('cherries', 0.5784382820129395), ('mango', 0.5751855969429016), ('apricot', 0.5727777481079102), ('melon', 0.5719985365867615), ('almond', 0.5704829692840576)]\n    [('tomatoes', 0.8442263007164001), ('lettuce', 0.7069936990737915), ('asparagus', 0.7050934433937073), ('peaches', 0.6938520669937134), ('cherry_tomatoes', 0.6897529363632202), ('strawberry', 0.6888598799705505), ('strawberries', 0.6832595467567444), ('bell_peppers', 0.6813562512397766), ('potato', 0.6784172058105469), ('cantaloupe', 0.6780219078063965), ('celery', 0.675195574760437), ('onion', 0.6740139722824097), ('cucumbers', 0.6706333160400391), ('spinach', 0.6682621240615845), ('cauliflower', 0.6681587100028992)]\n    ```", "```py\n    def get_word_vectors(sentence, model):\n        word_vectors = []\n        for word in sentence:\n            try:\n                word_vector = model[word.lower()]\n                word_vectors.append(word_vector)\n            except KeyError:\n                continue\n        return word_vectors\n    ```", "```py\n    def get_sentence_vector(word_vectors):\n        matrix = np.array(word_vectors)\n        centroid = np.mean(matrix[:,:], axis=0)\n        return centroid\n    ```", "```py\n    vectorize = lambda x: get_sentence_vector(\n        get_word_vectors(x, model))\n    (train_df, test_df) = load_train_test_dataset_pd()\n    (X_train, X_test, y_train, y_test) = create_train_test_data(\n        train_df, test_df, vectorize)\n    clf = train_classifier(X_train, y_train)\n    test_classifier(test_df, clf)\n    ```", "```py\n                  precision    recall  f1-score   support\n               0       0.54      0.57      0.55       160\n               1       0.54      0.51      0.53       160\n        accuracy                           0.54       320\n       macro avg       0.54      0.54      0.54       320\n    weighted avg       0.54      0.54      0.54       320\n    ```", "```py\n    words = ['banana', 'apple', 'computer', 'strawberry']\n    print(model.doesnt_match(words))\n    ```", "```py\n    computer\n    ```", "```py\n    word = \"cup\"\n    words = ['glass', 'computer', 'pencil', 'watch']\n    print(model.most_similar_to_given(word, words))\n    ```", "```py\n    glass\n    ```", "```py\n    import gensim\n    from gensim.models import Word2Vec\n    from datasets import load_dataset\n    from gensim import utils\n    ```", "```py\n    train_dataset = load_dataset(\"rotten_tomatoes\", split=\"train\")\n    print(len(train_dataset))\n    ```", "```py\n    8530\n    ```", "```py\n    class RottenTomatoesCorpus:\n        def __init__(self, sentences):\n            self.sentences = sentences\n        def __iter__(self):\n            for review in self.sentences:\n                yield utils.simple_preprocess(\n                    gensim.parsing.preprocessing.remove_stopwords(\n                        review))\n    ```", "```py\n    sentences = train_dataset[\"text\"]\n    corpus = RottenTomatoesCorpus(sentences)\n    ```", "```py\n    model = Word2Vec(sentences=corpus, vector_size=100,\n        window=5, min_count=1, workers=4)\n    model.train(corpus_iterable=corpus,\n        total_examples=model.corpus_count, epochs=100)\n    model.save(\"../data/rotten_tomato_word2vec.model\")\n    ```", "```py\n    w1 = \"movie\"\n    words = model.wv.most_similar(w1, topn=10)\n    print(words)\n    ```", "```py\n    [('sequels', 0.38357362151145935), ('film', 0.33577531576156616), ('stuffed', 0.2925359606742859), ('quirkily', 0.28789234161376953), ('convict', 0.2810690104961395), ('worse', 0.2789292335510254), ('churn', 0.27702808380126953), ('hellish', 0.27698105573654175), ('hey', 0.27566075325012207), ('happens', 0.27498629689216614)]\n    ```", "```py\n    (analogy_score, word_list) = model.wv.evaluate_word_analogies(\n        '../data/questions-words.txt')\n    print(analogy_score)\n    ```", "```py\n    0.0015881418740074113\n    ```", "```py\n    pretrained_model = \\\n        gensim.models.KeyedVectors.load_word2vec_format(\n            '../data/GoogleNews-vectors-negative300.bin.gz',\n            binary=True)\n    (analogy_score, word_list) = \\\n        pretrained_model.evaluate_word_analogies(\n            '../data/questions-words.txt')\n    print(analogy_score)\n    ```", "```py\n    0.7401448525607863\n    ```", "```py\n    print(type(pretrained_model))\n    print(type(model))\n    ```", "```py\n    <class 'gensim.models.keyedvectors.KeyedVectors'>\n    <class 'gensim.models.word2vec.Word2Vec'>\n    ```", "```py\n    %run -i \"../util/util_simple_classifier.ipynb\"\n    ```", "```py\n    from sentence_transformers import SentenceTransformer\n    ```", "```py\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    embedding = model.encode([\"I love jazz\"])\n    print(embedding)\n    ```", "```py\n    [[ 2.94217980e-03 -7.93536603e-02 -2.82228496e-02 -5.13779782e-02\n      -6.44981042e-02  9.83557850e-02  1.09671958e-01 -3.26390602e-02\n       4.96566631e-02  2.56580133e-02 -1.08482063e-01  1.88441798e-02\n       2.70963665e-02 -3.80690470e-02  2.42502335e-02 -3.65605950e-03\n       1.29364491e-01  4.32255343e-02 -6.64561391e-02 -6.93060979e-02\n      -1.39410645e-01  4.36719768e-02 -7.85463024e-03  1.68625098e-02\n      -1.01160072e-02  1.07926019e-02 -1.05814040e-02  2.57284809e-02\n      -1.51516097e-02 -4.53920700e-02  7.12087378e-03  1.17573030e-01… ]]\n    ```", "```py\n    def get_sentence_vector(text, model):\n        sentence_embeddings = model.encode([text])\n        return sentence_embeddings[0]\n    ```", "```py\n    import time\n    vectorize = lambda x: get_sentence_vector(x, model)\n    (train_df, test_df) = load_train_test_dataset_pd()\n    start = time.time()\n    (X_train, X_test, y_train, y_test) = create_train_test_data(\n        train_df, test_df, vectorize)\n    print(f\"BERT embeddings: {time.time() - start} s\")\n    clf = train_classifier(X_train, y_train)\n    test_classifier(test_df, clf)\n    ```", "```py\n    BERT embeddings: 11.410213232040405 s\n                  precision    recall  f1-score   support\n               0       0.77      0.79      0.78       160\n               1       0.79      0.76      0.77       160\n        accuracy                           0.78       320\n       macro avg       0.78      0.78      0.78       320\n    weighted avg       0.78      0.78      0.78       320\n    ```", "```py\n    import openai\n    openai.api_key = OPEN_AI_KEY\n    ```", "```py\n    model = \"text-embedding-ada-002\"\n    text = \"I love jazz\"\n    response = openai.Embedding.create(\n        input=text,\n        model=model\n    )\n    embeddings = response['data'][0]['embedding']\n    print(embeddings)\n    ```", "```py\n    [-0.028350897133350372, -0.011136125773191452, -0.0021299426443874836, -0.014453398995101452, -0.012048527598381042, 0.018223850056529045, -0.010247894562780857, -0.01806674897670746, -0.014308380894362926, 0.0007220656843855977, -9.998268797062337e-05, 0.010078707709908485,…]\n    ```", "```py\n    def get_sentence_vector(text, model):\n        text = \"I love jazz\"\n        response = openai.Embedding.create(\n            input=text,\n            model=model\n        )\n        embeddings = response['data'][0]['embedding']\n        return embeddings\n    ```", "```py\n    import time\n    vectorize = lambda x: get_sentence_vector(x, model)\n    (train_df, test_df) = load_train_test_dataset_pd()\n    start = time.time()\n    (X_train, X_test, y_train, y_test) = create_train_test_data(\n        train_df, test_df, vectorize)\n    print(f\"OpenAI embeddings: {time.time() - start} s\")\n    clf = train_classifier(X_train, y_train)\n    test_classifier(test_df, clf)\n    ```", "```py\n    OpenAI embeddings: 704.3250799179077 s\n                  precision    recall  f1-score   support\n               0       0.49      0.82      0.62       160\n               1       0.47      0.16      0.23       160\n        accuracy                           0.49       320\n       macro avg       0.48      0.49      0.43       320\n    weighted avg       0.48      0.49      0.43       320\n    ```", "```py\n    %run -i \"../util/file_utils.ipynb\"\n    ```", "```py\n    import csv\n    import openai\n    from llama_index import VectorStoreIndex\n    from llama_index import Document\n    openai.api_key = OPEN_AI_KEY\n    ```", "```py\n    with open('../data/IMDB-Movie-Data.csv') as f:\n        reader = csv.reader(f)\n        data = list(reader)\n        movies = data[1:]\n    ```", "```py\n    documents = []\n    for movie in movies[0:10]:\n        doc_id = movie[0]\n        title = movie[1]\n        genres = movie[2].split(\",\")\n        description = movie[3]\n        director = movie[4]\n        actors = movie[5].split(\",\")\n        year = movie[6]\n        duration = movie[7]\n        rating = movie[8]\n        revenue = movie[10]\n        document = Document(\n            text=description,\n            metadata={\n                \"title\": title,\n                \"genres\": genres,\n                \"director\": director,\n                \"actors\": actors,\n                \"year\": year,\n                \"duration\": duration,\n                \"rating\": rating,\n                \"revenue\": revenue\n            }\n        )\n        print(document)\n        documents.append(document)\n    index = VectorStoreIndex.from_documents(documents)\n    ```", "```py\n    id_='6e1ef633-f10b-44e3-9b77-f5f7b08dcedd' embedding=None metadata={'title': 'Guardians of the Galaxy', 'genres': ['Action', 'Adventure', 'Sci-Fi'], 'director': 'James Gunn', 'actors': ['Chris Pratt', ' Vin Diesel', ' Bradley Cooper', ' Zoe Saldana'], 'year': '2014', 'duration': '121', 'rating': '8.1', 'revenue': '333.13'} excluded_embed_metadata_keys=[] excluded_llm_metadata_keys=[] relationships={} hash='e18bdce3a36c69d8c1e55a7eb56f05162c68c97151cbaf40\n    91814ae3df42dfe8' text='A group of intergalactic criminals are forced to work together to stop a fanatical warrior from taking control of the universe.' start_char_idx=None end_char_idx=None text_template='{metadata_str}\\n\\n{content}' metadata_template='{key}: {value}' metadata_seperator='\\n'\n    ```", "```py\n    query_engine = index.as_query_engine()\n    ```", "```py\n    response = query_engine.query(\"\"\"Which movies talk about something gigantic?\"\"\")\n    print(response.response)\n    The answer seems to make sense grammatically, and arguably the Great Wall of China is gigantic. However, it is not clear what is gigantic in the movie Prometheus. So here we have a partially correct answer. The Great Wall and Prometheus both talk about something gigantic. In The Great Wall, the protagonists become embroiled in the defense of the Great Wall of China against a horde of monstrous creatures. In Prometheus, the protagonists find a structure on a distant moon.\n    ```"]