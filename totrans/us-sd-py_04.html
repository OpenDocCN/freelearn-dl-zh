<html><head></head><body>
		<div><h1 id="_idParaDest-53" class="chapter-number"><a id="_idTextAnchor081"/>4</h1>
			<h1 id="_idParaDest-54"><a id="_idTextAnchor082"/>Understanding the Theory Behind Diffusion Models</h1>
			<p>This chapter will dive into the theory that powers <strong class="bold">diffusion models</strong> and see the internal workings of the system. How could a neural network model generate such realistic images? Curious minds would like to lift the cover and see the internal workings.</p>
			<p>We are going to touch on the foundation of the diffusion model, aiming to figure out how it works internally and pave the foundation to implement a workable pipeline in the next chapter.</p>
			<p>By comprehending the intricacies of diffusion models, we not only enhance our understanding of the <a id="_idIndexMarker107"/>advanced <strong class="bold">Stable Diffusion</strong> (also known as <strong class="bold">latent diffusion models</strong> (<strong class="bold">LDMs</strong>)) but also gain the ability to navigate <a id="_idIndexMarker108"/>the source code of the Diffusers package more effectively.</p>
			<p>This knowledge will enable us to extend the package’s features in line with emerging requirements.</p>
			<p>Specifically, we will go through the following topics:</p>
			<ul>
				<li>Understanding the image-to-noise process</li>
				<li>A more efficient <strong class="bold">forward </strong><strong class="bold">diffusion process</strong></li>
				<li>The noise-to-image training process</li>
				<li>The noise-to-image sampling process</li>
				<li>Understanding Classifier Guidance denoising</li>
			</ul>
			<p>By the end of this chapter, we will have taken a deep dive into the internal workings of the diffusion model initially brought out by Jonathan Ho et al. [4]. We will understand the foundational idea of the diffusion model and learn about the <strong class="bold">forward diffusion process</strong>. We will understand the reverse diffusion process for diffusion model training and sampling and learn to enable a text-guided diffusion model.</p>
			<p>Let’s get started<a id="_idTextAnchor083"/>.</p>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor084"/>Understanding the image-to-noise process</h1>
			<p>The idea of the diffusion model is inspired by the diffusion concept from thermodynamics. Take one<a id="_idIndexMarker109"/> image as a cup of water and add enough noise (ink) to the image (water) to finally turn the image (water) into a complete noise image (ink water).</p>
			<p>As shown in <em class="italic">Figure 4</em><em class="italic">.1</em>, image x 0 can be converted to a nearly Gaussian (normally distributed) noise image x T.</p>
			<div><div><img src="img/B21263_04_01.jpg" alt="Figure 4.1: Forward diffusion and reverse denoising"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1: Forward diffusion and reverse denoising</p>
			<p>We employ a predetermined forward diffusion process, denoted as q, which systematically introduces Gaussian noise to an image until it culminates in pure noise. The process is denoted by q(x t | x t-1). Note that the reverse process p θ(x t-1 | x t) is still unknown.</p>
			<p>One step of the forward diffusion process can be denoted as follows:</p>
			<p>q(x t | x t-1) ≔  𝒩(x t; √ _ 1 − β t x t-1 , β t I)</p>
			<p>Let me explain this formula bit by bit from left to right:</p>
			<ul>
				<li>The notation q(x t | x t-1) is used to denote a conditional probability distribution. In this case, the distribution q represents the probability of observing the noisy image x t given the previous image x t−1.</li>
				<li>The define sign : = is used in the formula instead of the tilde symbol (∼) because the diffusion forward process is a deterministic process. The tilde symbol (∼) is typically used to represent a distribution. In this case, if we used the tilde symbol, the formula would be saying that the noisy image is a complete Gaussian distribution. However, this is not the case. The noisy image in t step is defined by a deterministic<a id="_idIndexMarker110"/> function of the previous image and added noise.</li>
				<li>Then why is 𝒩 used here? The 𝒩 symbol is used to represent a Gaussian distribution. However, in this case, the 𝒩 symbol is being used to represent the functional form of the noisy image.</li>
				<li>On the right side, before the semicolon, x t is the thing we want to have in normal distribution. After the semicolon, those are the parameters of the distribution. A semicolon is usually used to separate the output and parameters.</li>
				<li>β t is the noise variance at step t. √ _ 1 − β t  x t−1 is the mean of the new distribution.</li>
				<li>Why is the big I used in the formula? Because an RGB image can have multiple channels, and the identity matrix can apply the noise variance to different channels independently.</li>
			</ul>
			<p>It is quite easy to add Gaussian noise to an image using Python:</p>
			<pre class="source-code">
import numpy as np
import matplotlib.pyplot as plt
import ipyplot
from PIL import Image
# Load an image
img_path = r"dog.png"
image = plt.imread(img_path)
# Parameters
num_iterations = 16
beta = 0.1              # noise_variance
images = []
steps = ["Step:"+str(i) for i in range(num_iterations)]
# Forward diffusion process
for i in range(num_iterations):
    mean = np.sqrt(1 - beta) * image
    image = np.random.normal(mean, beta, image.shape)
    # convert image to PIL image object
    pil_image = Image.fromarray((image * 255).astype('uint8'), 'RGB')
    # add to image list
    images.append(pil_image)
ipyplot.plot_images(images, labels=steps, img_width=120)</pre>
			<p>To execute the preceding code, you<a id="_idIndexMarker111"/> will also need to install the <code>ipyplot</code> package by <code>pip install ipyplot</code>. The code provided performs a simulation of a forward diffusion process on an image and then visualizes the progression of this process over a number of iterations. Here’s a step-by-step explanation of what each part of the code is doing:</p>
			<ol>
				<li>Importing libraries:<ul><li><code>ipyplot</code> is a library for plotting images in Jupyter notebooks in a more interactive way.</li><li><code>PIL</code> (which <a id="_idIndexMarker112"/>stands for <code>Image</code> module, is used for image <a id="_idIndexMarker113"/>manipulation.</li></ul></li>
				<li>Loading the image:<ul><li><code>img_path</code> is defined as the path to the <code>image</code> file <code>dog.png</code>.</li><li><code>image</code> is loaded using <code>plt.imread(img_path)</code>.</li></ul></li>
				<li>Setting parameters:<ul><li><code>num_iterations</code> defines the number of times the diffusion process will be simulated.</li><li><code>beta</code> is a parameter that simulates noise variance in the diffusion process.</li></ul></li>
				<li>Initializing lists:<ul><li><code>images</code> is initialized as an empty list, which will later hold the PIL image objects that result from each iteration of the diffusion process.</li><li><code>steps</code> is a list of strings that will act as labels for the images when they are plotted, indicating the step number for each image.</li></ul></li>
				<li>Forward diffusion process:<ul><li>A <code>for</code> loop runs for <code>num_iterations</code> times, each time performing a diffusion step. <code>mean</code> is computed by scaling the image with a factor of <code>sqrt(1 - </code><code>beta)</code>.</li><li>A new image is generated by adding Gaussian noise to the mean, where the noise has a standard deviation of <code>beta</code>. This is done using <code>np.random.normal</code>.</li><li>The resulting image array values are scaled to the range 0-255 and converted to an 8-bit unsigned integer format, which is a common format for images.</li><li><code>pil_image</code> is created by converting the image array to a PIL image object in RGB mode.</li></ul></li>
				<li>Plot the image using <code>ipyplot</code> in a grid <a id="_idIndexMarker114"/>as shown in <em class="italic">Figure 4</em><em class="italic">.2</em>.</li>
			</ol>
			<div><div><img src="img/B21263_04_02.jpg" alt="Figure 4.2: Add noise to the image"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2: Add noise to the image</p>
			<p>From the result, we can see that even though every image is from a normal distribution function, not every image is a complete Gaussian distribution, or more strictly speaking, an <code>1000</code>, and later, in Stable Diffusion, the step number is reduced to between <code>20</code> to <code>50</code>.</p>
			<p>If the last image of <em class="italic">Figure 4</em><em class="italic">.2</em> is an isotropic Gaussian distribution, its 2D distribution visualization will appear as a circle; it is characterized by having equal variances in all dimensions. In other words, the spread or width of the distribution is the same along all axes.</p>
			<p>Let’s plot an image pixel distribution after adding 16x times Gaussian noise:</p>
			<pre class="source-code">
sample_img = image  # take the last image from the diffusion process
plt.scatter(sample_img[:, 0], sample_img[:, 1], alpha=0.5)
plt.title("2D Isotropic Gaussian Distribution")
plt.xlabel("X")
plt.ylabel("Y")
plt.axis("equal")
plt.show()</pre>
			<p>The result is shown in <em class="italic">Figure 4</em><em class="italic">.3</em>.</p>
			<div><div><img src="img/B21263_04_03.jpg" alt="Figure 4.3: A nearly isotropic, normally distributed noise image"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3: A nearly isotropic, normally distributed noise image</p>
			<p>The figure shows how the<a id="_idIndexMarker116"/> code efficiently transforms an image into a nearly isotropic, normally distributed noise image in just 16 steps, as illustrated in the last image of <em class="italic">Figur<a id="_idTextAnchor085"/>e 4</em><em class="italic">.2</em>.</p>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor086"/>A more efficient forward diffusion process</h1>
			<p>If we use the chained process to <a id="_idIndexMarker117"/>calculate a noisy image at t step, it first requires calculating the noisy image from 1 to t − 1 steps, which is not efficient. We can leverage a trick called <strong class="bold">reparameterization</strong> [10] to transform the original chained process into a <a id="_idIndexMarker118"/>one-step process. Here is what the trick looks like.</p>
			<p>If we have a Gaussian distribution z with μ as the mean and σ 2 variance:</p>
			<p>z ∼  𝒩(μ, σ 2)</p>
			<p>Then, we can rewrite the distribution as follows:</p>
			<p>ϵ ∼  𝒩(0,1)</p>
			<p>z = μ+ σϵ</p>
			<p>The benefit brought by this trick is that we can now calculate an image at any step with a one-step calculation, which will greatly boost the training performance:</p>
			<p>x t = √ _ 1 − β t  x t−1 + √ _ β t  ϵ t−1</p>
			<p>Now, say we define the following:</p>
			<p>α t = 1 − β t</p>
			<p>We now have the following:</p>
			<p>_ α t = ∏ i=1 t α i</p>
			<p>There is no magic here; define α t and α ‾  t is only for convenience, so that we can calculate a noised image at step t and generate x t from the source un-noised image x 0 using the following equation:</p>
			<p>x t = √ _ _ α t  x 0 + √ _ 1 − _ α t </p>
			<p>What do α t and α ‾  t look like? Here is a simplified sample (<em class="italic">Figure 4</em><em class="italic">.4</em>).</p>
			<div><div><img src="img/B21263_04_04.jpg" alt="Figure 4.4: Implementation of reparameterization"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4: Implementation of reparameterization</p>
			<p>In <em class="italic">Figure 4</em><em class="italic">.4</em>, we have all the same α - 0.1 and β - 0.9. Now, whenever we need to generate a noised image x t, we can quickly calculate α ‾  t from known numbers; the lines show what numbers are <a id="_idIndexMarker119"/>used to calculate α ‾  t.</p>
			<p>The following code can generate a noised image at any step:</p>
			<pre class="source-code">
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from itertools import accumulate
def get_product_accumulate(numbers):
    product_list = list(accumulate(numbers, lambda x, y: x * y))
    return product_list
# Load an image
img_path = r"dog.png"
image = plt.imread(img_path)
image = image * 2 - 1                   # [0,1] to [-1,1]
# Parameters
num_iterations = 16
beta = 0.05                             # noise_variance
betas = [beta]*num_iterations
alpha_list = [1 - beta for beta in betas]
alpha_bar_list = get_product_accumulate(alpha_list)
target_index = 5
x_target = (
    np.sqrt(alpha_bar_list[target_index]) * image
    + np.sqrt(1 - alpha_bar_list[target_index]) * 
    np.random.normal(0,1,image.shape)
)
x_target = (x_target+1)/2
x_target = Image.fromarray((x_target * 255).astype('uint8'), 'RGB')
display(x_target)</pre>
			<p>This code is the implementation <a id="_idIndexMarker120"/>of the previously presented math formula. I present the code here to help build a correlated understanding between the math formula and the real implementation. If you are familiar with Python, you may find that this code makes the underlying subtleties easier to understand. The code can generate a noised image as shown in <em class="italic">Figure 4</em><em class="italic">.5</em>.</p>
			<div><div><img src="img/B21263_04_05.jpg" alt="Figure 4.5: Implementation of reparameterization"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5: Implementation of reparameterization</p>
			<p>Now, let’s think about <a id="_idIndexMarker121"/>how to recover an image by leveraging a neural<a id="_idTextAnchor087"/> network.</p>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor088"/>The noise-to-image training process</h1>
			<p>We have the solution to add noise to the image, which is known as forward diffusion, as shown in <em class="italic">Figure 4</em><em class="italic">.6</em>. To recover an image<a id="_idIndexMarker122"/> from the noise, or <strong class="bold">reverse diffusion</strong>, as shown in <em class="italic">Figure 4</em><em class="italic">.6</em>, we need to<a id="_idIndexMarker123"/> find a way to implement the reverse step p θ(x t−1| x t). However, this step is intractable or uncomputable without additional help.</p>
			<p>Consider that we have the ending Gaussian noise data, and all those noise step data in hand. What if we can train a neural network that can reverse the process? We can use the neural network to provide the mean and variance of a noise image and then remove the generated noise from the previous image data. By doing this, we should be able to use this step to represent p θ(x t−1| x t), and thus recover an image.</p>
			<div><div><img src="img/B21263_04_06.jpg" alt="Figure 4.6: Forward diffusion and reverse process"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.6: Forward diffusion and reverse process</p>
			<p>You may ask how we should calculate the loss and update the weights. The ending image (x T) removes the previously added noise and will provide the ground truth data. After all, we can generate the noise data in the forward diffusion processes on the fly. Next, compare it with the output data from the neural network (usually a UNet). We get the loss data that can be used to calculate the gradient descendant data and update the neural network weights.</p>
			<p>The DDPM paper [4] provided a simplified way to calculate the loss:</p>
			<p>L simple(θ) : = 𝔼 t, x 0,∈[|| ∈ − ∈ θ(√ _ _ α t  x 0 + √ _ 1 − _ α t  ϵ, t) || 2]</p>
			<p>Since x t = √ _ _ α t x 0  + √ _ 1 − _ α t , we can further simplify the formula to the following:</p>
			<p>L simple(θ) ≔ 𝔼 t,x 0,ϵ[||ϵ − ϵ θ(x t, t) || 2]</p>
			<p>The UNet will take a noised<a id="_idIndexMarker124"/> image data: x t and a time step data: t as inputs as shown in <em class="italic">Figure 4</em><em class="italic">.7</em>. Why take t as input? Because all the denoising processes share the same neural network weights, the input t will help train a UNet with a time step in mind.</p>
			<div><div><img src="img/B21263_04_07.jpg" alt="Figure 4.7: UNet training inputs and loss calculation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7: UNet training inputs and loss calculation</p>
			<p>When we say let’s train a neural network to predict the noise distribution that will be removed from the image leading to a clearer image, what is the neural network predicting? In the DDPM paper [4], the original diffusion model uses a fixed variance θ, and sets the Gaussian distribution mean - μ as the only parameter that needs to be learned through a neural network.</p>
			<p>In a PyTorch implementation, the loss data can be calculated like this:</p>
			<pre class="source-code">
import torch
import torch.nn as nn
# code prepare the model object, image and timestep
# ...
# noise is the Ɛ ~ N(0,1) with the shape of the image x_t.
noise = torch.randn_like(x_t)
# x_t is the noised image at step "t", together with the time_step value
predicted_noise = model(x_t, time_step)
loss = nn.MSELoss(noise, predicted_noise)
# backward weight propagation
# ...</pre>
			<p>Now, we should be able to<a id="_idIndexMarker125"/> train a diffusion model and the model should be able to recover an image from a random Gaussian distributed noise. Next, let’s take a look at how the inference or samp<a id="_idTextAnchor089"/>ling works.</p>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor090"/>The noise-to-image sampling process</h1>
			<p>Here are the steps to sample <a id="_idIndexMarker126"/>an image from the model, or, in other words, generate an image from the reverse diffusion process:</p>
			<ol>
				<li>Generate a complete Gaussian noise with a mean of 0 and a variance of 1:</li>
			</ol>
			<p>x T ∼  𝒩(0,1)</p>
			<p class="list-inset">We will use this noise as the starting image.</p>
			<p>2.	Loop through t = T to t = 1. In each step, if t &gt; 1, then generate another Gaussian noise image z:</p>
			<p>z ∼  𝒩(0,1)</p>
			<p class="list-inset">If t = 1, then the following occurs:</p>
			<p>z = 0</p>
			<p class="list-inset">Then, generate a noise from the UNet model, and remove the generated noise from the input noisy image x t:</p>
			<p class="IMG---Figure">x t-1 =  1 _ √ _ α t (x t −  1 − α t _ √ _ 1 − _ α t  ϵ θ(x t, t)) + √ _ 1 − α t  z</p>
			<p class="list-inset">If we take a look at the preceding equation, all those α t and α ‾  t are known numbers sourced from β t. The only thing we need from the UNet is the ϵ θ(x t, t), which is the noise produced by the UNet, as shown in <em class="italic">Figure 4</em><em class="italic">.8</em>.</p>
			<div><div><img src="img/B21263_04_08.jpg" alt="Figure 4.8: Sampling from UNet"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.8: Sampling from UNet</p>
			<p class="list-inset">The added √ _ 1 − α t  z looks a little bit mysterious here. Why add this to the process? The original paper doesn’t explain this added noise, but researchers found that the added <a id="_idIndexMarker127"/>noise in the denoising process will significantly improve the generated image quality!</p>
			<p>3.	Loop end, return the final generated image x 0.</p>
			<p>Now, let’s talk about the image generati<a id="_idTextAnchor091"/>on guidance.</p>
			<h1 id="_idParaDest-59"><a id="_idTextAnchor092"/>Understanding Classifier Guidance denoising</h1>
			<p>Until now, we haven’t talked<a id="_idIndexMarker128"/> about the text guidance yet. The image generation process will take a random Gaussian noise as the only input, and then randomly generate an image based on the training dataset. But we want a guided image generation; for example, input “dog” to ask the diffusion model to generate an image including “dog.”</p>
			<p>In 2021, Dhariwal and Nichol, from OpenAI, proposed classifier guidance in their paper titled <em class="italic">Diffusion Models Beat GANs on Image </em><em class="italic">Synthesis</em> [12].</p>
			<p>Based on the proposed methodology, we can achieve classifier-guided denoising by providing a classification label <a id="_idIndexMarker129"/>during the training stage. Instead of just image or time-step embedding, we also provide text description embeddings as shown in <em class="italic">Figure 4</em><em class="italic">.9</em>.</p>
			<div><div><img src="img/B21263_04_09.jpg" alt="Figure 4.9: Train a diffusion model with conditional text"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.9: Train a diffusion model with conditional text</p>
			<p>In <em class="italic">Figure 4</em><em class="italic">.7</em>, there are two inputs, while in <em class="italic">Figure 4</em><em class="italic">.9</em>, there is one additional input –  <strong class="bold">Text embedding</strong>; it is the embedding data generated from OpenAI’s CLIP model. We will discuss the way more powerful CLIP model guided diffusion model in the <a id="_idTextAnchor093"/>next chapter.</p>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor094"/>Summary</h1>
			<p>In this chapter, we took a deep dive into the internal workings of the diffusion model initially brought out by Jonathan Ho et al. [4]. We learned about the foundational ideas of the diffusion model and learned about the forward diffusion process. We also walked through the reverse diffusion process for diffusion model training and sampling and explored how to enable a text-guided diffusion model.</p>
			<p>Through this chapter, we aimed to explain the core idea of the diffusion model. If you want to implement a diffusion model by yourself, I would recommend reading through the original DDPM paper directly.</p>
			<p>The DDPM diffusion model can generate realistic images, but one of its problems is its performance. Not only is training a model slow, but the image sampling is also slow. In the next chapter, we are going to discuss the Stable Diffusion model, which will boost the speed in <a id="_idTextAnchor095"/>a genius way.</p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor096"/>References</h1>
			<ol>
				<li><em class="italic">The Annotated Diffusion Model</em> – <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb#scrollTo=c5a94671&#13;">https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb#scrollTo=c5a94671</a></li>
				<li><em class="italic">Training with Diffusers</em> – <a href="https://colab.research.google.com/gist/anton-l/f3a8206dae4125b93f05b1f5f703191d/diffusers_training_example.ipynb">https://colab.research.google.com/gist/anton-l/f3a8206dae4125b93f05b1f5f703191d/diffusers_training_example.ipynb</a> </li>
				<li><em class="italic">Diffusers</em> – <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=PzW5ublpBuUt&#13;">https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=PzW5ublpBuUt</a></li>
				<li>Jonathan Ho et al., <em class="italic">Denoising Diffusion Probabilistic Models</em> – <a href="https://arxiv.org/abs/2006.11239">https://arxiv.org/abs/2006.11239</a></li>
				<li>Steins, <em class="italic">Diffusion Model Clearly Explained!</em> – <a href="https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166">https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166</a></li>
				<li>Steins, <em class="italic">Stable Diffusion Clearly Explained!</em> – <a href="https://medium.com/@steinsfu/stable-diffusion-clearly-explained-ed008044e07e">https://medium.com/@steinsfu/stable-diffusion-clearly-explained-ed008044e07e</a></li>
				<li>DeepFindr, <em class="italic">Diffusion models from scratch in PyTorch</em> – <a href="https://www.youtube.com/watch?v=a4Yfz2FxXiY&amp;t=5s&amp;ab_channel=DeepFindr&#13;">https://www.youtube.com/watch?v=a4Yfz2FxXiY&amp;t=5s&amp;ab_channel=DeepFindr</a></li>
				<li>Ari Seff, <em class="italic">What are Diffusion Models?</em> – <a href="https://www.youtube.com/watch?v=fbLgFrlTnGU&amp;ab_channel=AriSeff&#13;">https://www.youtube.com/watch?v=fbLgFrlTnGU&amp;ab_channel=AriSeff</a></li>
				<li>Prafulla Dhariwal, Alex Nichol<em class="italic">, Diffusion Models Beat GANs on Image Synthesis</em> – <a href="https://arxiv.org/abs/2105.05233">https://arxiv.org/abs/2105.05233</a></li>
				<li>Diederik P Kingma, Max Welling, <em class="italic">Auto-Encoding Variational Bayes</em> – <a href="https://arxiv.org/abs/1312.6114">https://arxiv.org/abs/1312.6114</a></li>
				<li>Lilian Weng, <em class="italic">What are Diffusion Models?</em> – <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</a></li>
				<li>Prafulla Dhariwal, Alex Nichol, <em class="italic">Diffusion Models Beat GANs on Image Synthesis</em> – <a href="https://arxiv.org/abs/2105.05233">https://arxiv.org/abs/2105.05233</a></li>
			</ol>
		</div>
	</body></html>