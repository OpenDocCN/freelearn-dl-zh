["```py\nfrom datasets import load_dataset\ndataset = load_dataset(\"imdb\")\ndataset \n```", "```py\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50000\n    })\n}) \n```", "```py\ndataset[\"train\"][100] \n```", "```py\n{'text': \"Terrible movie. Nuff Said.[â€¦]\n 'label': 0} \n```", "```py\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\") \n```", "```py\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding = \"max_length\", truncation=True)\ntokenized_datasets = dataset.map(tokenize_function, batched=True) \n```", "```py\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ntokenized_datasets['train'][100]['input_ids'] \n```", "```py\n[101,\n 12008,\n 27788,\n...\n 0,\n 0,\n 0,\n 0,\n 0] \n```", "```py\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(500))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500)) \n```", "```py\nimport torch\nfrom transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2) \n```", "```py\n    import numpy as np\n    import evaluate\n    metric = evaluate.load(\"accuracy\") \n    ```", "```py\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return metric.compute(predictions=predictions, references=labels) \n    ```", "```py\n    from transformers import TrainingArguments, Trainer\n    training_args = TrainingArguments(output_dir=\"test_trainer\", num_train_epochs = 2\n    evaluation_strategy=\"epoch\") \n    ```", "```py\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=small_train_dataset,\n        eval_dataset=small_eval_dataset,\n        compute_metrics=compute_metrics,\n    ) \n    ```", "```py\n    trainer.train() \n    ```", "```py\n{'eval_loss': 0.6720085144042969, 'eval_accuracy': 0.58, 'eval_runtime': 609.7916, 'eval_samples_per_second': 0.328, 'eval_steps_per_second': 0.041, 'epoch': 1.0}\n{'eval_loss': 0.5366445183753967, 'eval_accuracy': 0.82, 'eval_runtime': 524.186, 'eval_samples_per_second': 0.382, 'eval_steps_per_second': 0.048, 'epoch': 2.0} \n```", "```py\n    trainer.save_model('models/sentiment-classifier') \n    ```", "```py\n    model = AutoModelForSequenceClassification.from_pretrained('models/sentiment-classifier') \n    ```", "```py\n    inputs = tokenizer(\"I cannot stand it anymore!\", return_tensors=\"pt\")\n    outputs = model(**inputs)\n    outputs \n    ```", "```py\nSequenceClassifierOutput(loss=None, logits=tensor([[ 0.6467, -0.0041]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None) \n```", "```py\n    import tensorflow as tf\n    predictions = tf.math.softmax(outputs.logits.detach(), axis=-1)\n    print(predictions) \n    ```", "```py\ntf.Tensor([[0.6571879  0.34281212]], shape=(1, 2), dtype=float32) \n```", "```py\n    from huggingface_hub import notebook_login\n    notebook_login() \n    ```", "```py\n    trainer.push_to_hub('vaalto/sentiment-classifier') \n    ```"]