["```py\nfrom typing_extensions import TypedDict\nclass JobApplicationState(TypedDict):\n job_description: str\n is_suitable: bool\n application: str\n```", "```py\nfrom langgraph.graph import StateGraph, START, END, Graph\ndef analyze_job_description(state):\n   print(\"...Analyzing a provided job description ...\")\n   return {\"is_suitable\": len(state[\"job_description\"]) > 100}\ndef generate_application(state):\n   print(\"...generating application...\")\n   return {\"application\": \"some_fake_application\"}\nbuilder = StateGraph(JobApplicationState)\nbuilder.add_node(\"analyze_job_description\", analyze_job_description)\nbuilder.add_node(\"generate_application\", generate_application)\nbuilder.add_edge(START, \"analyze_job_description\")\nbuilder.add_edge(\"analyze_job_description\", \"generate_application\")\nbuilder.add_edge(\"generate_application\", END)\ngraph = builder.compile()\n```", "```py\nfrom IPython.display import Image, display\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```", "```py\nres = graph.invoke({\"job_description\":\"fake_jd\"})\nprint(res)\n>>...Analyzing a provided job description ...\n...generating application...\n{'job_description': 'fake_jd', 'is_suitable': True, 'application': 'some_fake_application'}\n```", "```py\nfrom typing import Literal\nbuilder = StateGraph(JobApplicationState)\nbuilder.add_node(\"analyze_job_description\", analyze_job_description)\nbuilder.add_node(\"generate_application\", generate_application)\ndef is_suitable_condition(state: StateGraph) -> Literal[\"generate_application\", END]:\n if state.get(\"is_suitable\"):\n return \"generate_application\"\n return END\nbuilder.add_edge(START, \"analyze_job_description\")\nbuilder.add_conditional_edges(\"analyze_job_description\", is_suitable_condition)\nbuilder.add_edge(\"generate_application\", END)\ngraph = builder.compile()\n```", "```py\nfrom IPython.display import Image, display\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```", "```py\nclass JobApplicationState(TypedDict):\n   ...\n   actions: list[str]\n```", "```py\nfrom typing import Annotated, Optional\nfrom operator import add\nclass JobApplicationState(TypedDict):\n   ...\n   actions: Annotated[list[str], add]\n```", "```py\nfrom typing import Annotated, Optional, Union\ndef my_reducer(left: list[str], right: Optional[Union[str, list[str]]]) -> list[str]:\n if right:\n return left + [right] if isinstance(right, str) else left + right\n return left\nclass JobApplicationState(TypedDict):\n   ...\n   actions: Annotated[list[str], my_reducer]\n```", "```py\nfrom langchain_core.messages import AnyMessage\nfrom langgraph.graph.message import add_messages \nclass JobApplicationState(TypedDict): \n  ...\n  messages: Annotated[list[AnyMessage], add_messages]\n```", "```py\nfrom langgraph.graph import MessagesState \nclass JobApplicationState(MessagesState): \n  ...\n```", "```py\nfrom langchain_core.runnables.config import RunnableConfig\ndef generate_application(state: JobApplicationState, config: RunnableConfig):\n   model_provider = config[\"configurable\"].get(\"model_provider\", \"Google\")\n   model_name = config[\"configurable\"].get(\"model_name\", \"gemini-1.5-flash-002\")\n print(f\"...generating application with {model_provider} and {model_name} ...\")\n return {\"application\": \"some_fake_application\", \"actions\": [\"action2\", \"action3\"]}\n```", "```py\nres = graph.invoke({\"job_description\":\"fake_jd\"}, config={\"configurable\": {\"model_provider\": \"OpenAI\", \"model_name\": \"gpt-4o\"}})\nprint(res)\n>> ...Analyzing a provided job description ...\n...generating application with OpenAI and OpenAI ...\n{'job_description': 'fake_jd', 'is_suitable': True, 'application': 'some_fake_application', 'actions': ['action1', 'action2', 'action3']}\n```", "```py\nfrom langchain_google_vertexai import ChatVertexAI\nllm = ChatVertexAI(model=\"gemini-1.5-flash-002\")\n```", "```py\njob_description: str = ...  # put your JD here\nprompt_template = (\n \"Given a job description, decide whether it suits a junior Java developer.\"\n \"\\nJOB DESCRIPTION:\\n{job_description}\\n\"\n)\nresult = llm.invoke(prompt_template.format(job_description=job_description))\nprint(result.content)\n>> No, this job description is not suitable for a junior Java developer.\\n\\nThe key reasons are:\\n\\n* â€¦ (output reduced)\n```", "```py\nprompt_template_enum = (\n \"Given a job description, decide whether it suits a junior Java developer.\"\n \"\\nJOB DESCRIPTION:\\n{job_description}\\n\\nAnswer only YES or NO.\"\n)\nresult = llm.invoke(prompt_template_enum.format(job_description=job_description))\nprint(result.content)\n>> NO\n```", "```py\nfrom enum import Enum\nfrom langchain.output_parsers import EnumOutputParser\nfrom langchain_core.messages import HumanMessage\n```", "```py\nclass IsSuitableJobEnum(Enum):\n   YES = \"YES\"\n   NO = \"NO\"\nparser = EnumOutputParser(enum=IsSuitableJobEnum)\nassert parser.invoke(\"NO\") == IsSuitableJobEnum.NO\nassert parser.invoke(\"YES\\n\") == IsSuitableJobEnum.YES\nassert parser.invoke(\" YES \\n\") == IsSuitableJobEnum.YES\nassert parser.invoke(HumanMessage(content=\"YES\")) == IsSuitableJobEnum.YES\n```", "```py\nchain = llm | parser\nresult = chain.invoke(prompt_template_enum.format(job_description=job_description))\nprint(result)\n>> NO\n```", "```py\nclass JobApplicationState(TypedDict):\n   job_description: str\n   is_suitable: IsSuitableJobEnum\n   application: str\nanalyze_chain = llm | parser\ndef analyze_job_description(state):\n   prompt = prompt_template_enum.format(job_description=state[\"job_description\"])\n```", "```py\n   result = analyze_chain.invoke(prompt)\n return {\"is_suitable\": result}\ndef is_suitable_condition(state: StateGraph):\n return state[\"is_suitable\"] == IsSuitableJobEnum.YES\nbuilder = StateGraph(JobApplicationState)\nbuilder.add_node(\"analyze_job_description\", analyze_job_description)\nbuilder.add_node(\"generate_application\", generate_application)\nbuilder.add_edge(START, \"analyze_job_description\")\nbuilder.add_conditional_edges(\n \"analyze_job_description\", is_suitable_condition,\n    {True: \"generate_application\", False: END})\nbuilder.add_edge(\"generate_application\", END)\n```", "```py\nimport logging\nlogger = logging.getLogger(__name__)\nllms = {\n \"fake\": fake_llm,\n \"Google\": llm\n}\ndef analyze_job_description(state, config: RunnableConfig):\n try:\n     llm = config[\"configurable\"].get(\"model_provider\", \"Google\")\n     llm = llms[model_provider]\n     analyze_chain = llm | parser\n     prompt = prompt_template_enum.format(job_description=job_description)\n     result = analyze_chain.invoke(prompt)\n return {\"is_suitable\": result}\n except Exception as e:\n     logger.error(f\"Exception {e} occurred while executing analyze_job_description\")\n return {\"is_suitable\": False}\n```", "```py\nfrom langchain_core.language_models import GenericFakeChatModel\nfrom langchain_core.messages import AIMessage\nclass MessagesIterator:\n def __init__(self):\n self._count = 0\n def __iter__(self):\n return self\n def __next__(self):\n self._count += 1\n if self._count % 2 == 1:\n raise ValueError(\"Something went wrong\")\n return AIMessage(content=\"False\")\nfake_llm = GenericFakeChatModel(messages=MessagesIterator())\n```", "```py\nres = graph.invoke({\"job_description\":\"fake_jd\"}, config={\"configurable\": {\"model_provider\": \"fake\"}})\nprint(res)\n>> ERROR:__main__:Exception Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'> occured while executing analyze_job_description\n{'job_description': 'fake_jd', 'is_suitable': False}\n```", "```py\nfake_llm_retry = fake_llm.with_retry(\n   retry_if_exception_type=(ValueError,),\n   wait_exponential_jitter=True,\n   stop_after_attempt=2,\n)\nanalyze_chain_fake_retries = fake_llm_retry | parser\n```", "```py\nfrom langgraph.pregel import RetryPolicy\nbuilder.add_node(\n \"analyze_job_description\", analyze_job_description,\n  retry=RetryPolicy(retry_on=ValueError, max_attempts=2))\n```", "```py\nfrom langchain.output_parsers import RetryWithErrorOutputParser\nfix_parser = RetryWithErrorOutputParser.from_llm(\n  llm=llm, # provide llm here\n  parser=parser, # your original parser that failed\n  prompt=retry_prompt, # an optional parameter, you can redefine the default prompt \n)\nfixed_output = fix_parser.parse_with_prompt(\n  completion=original_response, prompt_value=original_prompt)\n```", "```py\nprompt = \"\"\"\nPrompt: {prompt} Completion: {completion} Above, the Completion did not satisfy the constraints given in the Prompt. Details: {error} Please try again:\n\"\"\" \nretry_chain = prompt | llm | StrOutputParser()\n# try to parse a completion with a provided parser\nparser.parse(completion)\n# if it fails, catch an error and try to recover max_retries attempts\ncompletion = retry_chain.invoke(original_prompt, completion, error)\n```", "```py\nfrom langchain_core.runnables import RunnableLambda\nchain_fallback = RunnableLambda(lambda _: print(\"running fallback\"))\nchain = fake_llm | RunnableLambda(lambda _: print(\"running main chain\"))\nchain_with_fb = chain.with_fallbacks([chain_fallback])\nchain_with_fb.invoke(\"test\")\nchain_with_fb.invoke(\"test\")\n>> running fallback\nrunning main chain\n```", "```py\nfrom langchain_core.output_parsers import StrOutputParser\nlc_prompt_template = PromptTemplate.from_template(prompt_template)\nchain = lc_prompt_template | llm | StrOutputParser()\nchain.invoke({\"job_description\": job_description})\n```", "```py\nfrom langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\nfrom langchain_core.messages import SystemMessage, HumanMessage\nmsg_template = HumanMessagePromptTemplate.from_template(\n  prompt_template)\nmsg_example = msg_template.format(job_description=\"fake_jd\")\nchat_prompt_template = ChatPromptTemplate.from_messages([\n  SystemMessage(content=\"You are a helpful assistant.\"),\n  msg_template])\nchain = chat_prompt_template | llm | StrOutputParser()\nchain.invoke({\"job_description\": job_description})\n```", "```py\nchat_prompt_template = ChatPromptTemplate.from_messages(\n   [(\"system\", \"You are a helpful assistant.\"),\n    (\"human\", prompt_template)])\n```", "```py\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nchat_prompt_template = ChatPromptTemplate.from_messages(\n   [(\"system\", \"You are a helpful assistant.\"),\n    (\"placeholder\", \"{history}\"),\n # same as MessagesPlaceholder(\"history\"),\n    (\"human\", prompt_template)])\nlen(chat_prompt_template.invoke({\"job_description\": \"fake\", \"history\": [(\"human\", \"hi!\"), (\"ai\", \"hi!\")]}).messages)\n>> 4\n```", "```py\nsystem_template = PromptTemplate.from_template(\"a: {a} b: {b}\")\nsystem_template_part = system_template.partial(\n   a=\"a\" # you also can provide a function here\n)\nprint(system_template_part.invoke({\"b\": \"b\"}).text)\n>> a: a b: b\n```", "```py\nsystem_template_part1 = PromptTemplate.from_template(\"a: {a}\")\nsystem_template_part2 = PromptTemplate.from_template(\"b: {b}\")\nsystem_template = system_template_part1 + system_template_part2\nprint(system_template_part.invoke({\"a\": \"a\", \"b\": \"b\"}).text)\n>> a: a b: b\n```", "```py\nsystem_prompt_template = PromptTemplate.from_template(\"a: {a} b: {b}\")\nchat_prompt_template = ChatPromptTemplate.from_messages(\n   [(\"system\", system_prompt_template.template),\n    (\"human\", \"hi\"),\n    (\"ai\", \"{c}\")])\nmessages = chat_prompt_template.invoke({\"a\": \"a\", \"b\": \"b\", \"c\": \"c\"}).messages\nprint(len(messages))\nprint(messages[0].content)\n>> 3\na: a b: b\n```", "```py\nfrom langchain import hub\nmath_cot_prompt = hub.pull(\"arietem/math_cot\")\ncot_chain = math_cot_prompt | llm | StrOutputParser()\nprint(cot_chain.invoke(\"Solve equation 2*x+5=15\"))\n>> Answer: Let's think step by step\nSubtract 5 from both sides:\n2x + 5 - 5 = 15 - 5\n2x = 10\nDivide both sides by 2:\n2x / 2 = 10 / 2\nx = 5\n```", "```py\nfrom operator import itemgetter\nparse_prompt_template = (\n \"Given the initial question and a full answer, \"\n \"extract the concise answer. Do not assume anything and \"\n \"only use a provided full answer.\\n\\nQUESTION:\\n{question}\\n\"\n \"FULL ANSWER:\\n{full_answer}\\n\\nCONCISE ANSWER:\\n\"\n)\nparse_prompt = PromptTemplate.from_template(\n   parse_prompt_template\n)\nfinal_chain = (\n {\"full_answer\": itemgetter(\"question\") | cot_chain,\n \"question\": itemgetter(\"question\"),\n }\n | parse_prompt\n```", "```py\n | llm\n | StrOutputParser()\n)\nprint(final_chain.invoke({\"question\": \"Solve equation 2*x+5=15\"}))\n>> 5\n```", "```py\ntemplate = ChatPromptTemplate.from_messages([\n    (\"system\", \"\"\"You are a problem-solving assistant that shows its reasoning process. First, walk through your thought process step by step, labeling this section as 'THINKING:'. After completing your analysis, provide your final answer labeled as 'ANSWER:'.\"\"\"),\n    (\"user\", \"{problem}\")\n])\n```", "```py\ngenerations = []\nfor _ in range(20):\n generations.append(final_chain.invoke({\"question\": \"Solve equation 2*x**2-96*x+1152\"}, temperature=2.0).strip())\nfrom collections import Counter\nprint(Counter(generations).most_common(1)[0][0])\n>> x = 24\n```", "```py\nfrom langgraph.constants import Send\nimport operator\nclass AgentState(TypedDict):\n   video_uri: str\n   chunks: int\n   interval_secs: int\n   summaries: Annotated[list, operator.add]\n   final_summary: str\nclass _ChunkState(TypedDict):\n   video_uri: str\n   start_offset: int\n   interval_secs: int\n```", "```py\nhuman_part = {\"type\": \"text\", \"text\": \"Provide a summary of the video.\"}\nasync def _summarize_video_chunk(state:  _ChunkState):\n   start_offset = state[\"start_offset\"]\n   interval_secs = state[\"interval_secs\"]\n   video_part = {\n \"type\": \"media\", \"file_uri\": state[\"video_uri\"], \"mime_type\": \"video/mp4\",\n \"video_metadata\": {\n \"start_offset\": {\"seconds\": start_offset*interval_secs},\n \"end_offset\": {\"seconds\": (start_offset+1)*interval_secs}}\n   }\n   response = await llm.ainvoke(\n       [HumanMessage(content=[human_part, video_part])])\n return {\"summaries\": [response.content]}\nasync def _generate_final_summary(state: AgentState):\n   summary = _merge_summaries(\n       summaries=state[\"summaries\"], interval_secs=state[\"interval_secs\"])\n   final_summary = await (reduce_prompt | llm | StrOutputParser()).ainvoke({\"summaries\": summary})\n return {\"final_summary\": final_summary}\ndef _map_summaries(state: AgentState):\n   chunks = state[\"chunks\"]\n   payloads = [\n       {\n \"video_uri\": state[\"video_uri\"],\n \"interval_secs\": state[\"interval_secs\"],\n \"start_offset\": i\n       } for i in range(state[\"chunks\"])\n   ] \n return [Send(\"summarize_video_chunk\", payload) for payload in payloads]\n```", "```py\ngraph = StateGraph(AgentState)\ngraph.add_node(\"summarize_video_chunk\", _summarize_video_chunk)\ngraph.add_node(\"generate_final_summary\", _generate_final_summary)\ngraph.add_conditional_edges(START, _map_summaries, [\"summarize_video_chunk\"])\ngraph.add_edge(\"summarize_video_chunk\", \"generate_final_summary\")\ngraph.add_edge(\"generate_final_summary\", END)\napp = graph.compile()\nresult = await app.ainvoke(\n   {\"video_uri\": video_uri, \"chunks\": 5, \"interval_secs\": 600},\n   {\"max_concurrency\": 3}\n)[\"final_summary\"]\n```", "```py\nfrom langchain_core.chat_history import InMemoryChatMessageHistory\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_core.language_models import FakeListChatModel\nfrom langchain.callbacks.base import BaseCallbackHandler\nclass PrintOutputCallback(BaseCallbackHandler):\n def on_chat_model_start(self, serialized, messages, **kwargs):\n print(f\"Amount of input messages: {len(messages)}\")\nsessions = {}\nhandler = PrintOutputCallback()\nllm = FakeListChatModel(responses=[\"ai1\", \"ai2\", \"ai3\"])\ndef get_session_history(session_id: str):\n if session_id not in sessions:\n       sessions[session_id] = InMemoryChatMessageHistory()\n return sessions[session_id]\n```", "```py\ntrimmer = trim_messages(\n   max_tokens=1,\n   strategy=\"last\",\n   token_counter=len,\n   include_system=True,\n   start_on=\"human\",\n)\nraw_chain = trimmer | llm\nchain = RunnableWithMessageHistory(raw_chain, get_session_history)\n```", "```py\nconfig = {\"callbacks\": [PrintOutputCallback()], \"configurable\": {\"session_id\": \"1\"}}\n_ = chain.invoke(\n   [HumanMessage(\"Hi!\")],\n   config=config,\n)\nprint(f\"History length: {len(sessions['1'].messages)}\")\n_ = chain.invoke(\n   [HumanMessage(\"How are you?\")],\n   config=config,\n)\nprint(f\"History length: {len(sessions['1'].messages)}\")\n>> Amount of input messages: 1\nHistory length: 2\nAmount of input messages: 1\nHistory length: 4\n```", "```py\nfrom langgraph.graph import MessageGraph\nfrom langgraph.checkpoint.memory import MemorySaver\n```", "```py\ndef test_node(state):\n # ignore the last message since it's an input one\n print(f\"History length = {len(state[:-1])}\")\n return [AIMessage(content=\"Hello!\")]\nbuilder = MessageGraph()\nbuilder.add_node(\"test_node\", test_node)\nbuilder.add_edge(START, \"test_node\")\nbuilder.add_edge(\"test_node\", END)\nmemory = MemorySaver()\ngraph = builder.compile(checkpointer=memory)\n```", "```py\n_ = graph.invoke([HumanMessage(content=\"test\")],\n  config={\"configurable\": {\"thread_id\": \"thread-a\"}})\n_ = graph.invoke([HumanMessage(content=\"test\")]\n  config={\"configurable\": {\"thread_id\": \"thread-b\"}})\n_ = graph.invoke([HumanMessage(content=\"test\")]\n  config={\"configurable\": {\"thread_id\": \"thread-a\"}})\n>> History length = 0\nHistory length = 0\nHistory length = 2\n```", "```py\ncheckpoints = list(memory.list(config={\"configurable\": {\"thread_id\": \"thread-a\"}}))\nfor check_point in checkpoints:\n print(check_point.config[\"configurable\"][\"checkpoint_id\"])\n```", "```py\ncheckpoint_id = checkpoints[-1].config[\"configurable\"][\"checkpoint_id\"]\n_ = graph.invoke(\n```", "```py\n   [HumanMessage(content=\"test\")],\n   config={\"configurable\": {\"thread_id\": \"thread-a\", \"checkpoint_id\": checkpoint_id}})\n>> History length = 0\n```", "```py\ncheckpoint_id = checkpoints[-3].config[\"configurable\"][\"checkpoint_id\"]\n_ = graph.invoke(\n   [HumanMessage(content=\"test\")],\n   config={\"configurable\": {\"thread_id\": \"thread-a\", \"checkpoint_id\": checkpoint_id}})\n>> History length = 2\n```"]