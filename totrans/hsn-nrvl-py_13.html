<html><head></head><body>
        

                            
                    <h1 class="header-title">Deep Neuroevolution</h1>
                
            
            
                
<p class="p1">In this chapter, you will learn about the deep neuroevolution method, which can be used to train <strong>Deep Neural Networks</strong> (<strong>DNNs</strong>). DNNs are conventionally trained using backpropagation methods based on the descent of the error gradient, which is computed with respect to the weights of the connections between neural nodes. Although gradient-based learning is a powerful technique that conceived the current era of deep machine learning, it has its drawbacks, such as long training times and enormous computing power requirements.</p>
<p class="p1">In this chapter, we will demonstrate how deep neuroevolution methods can be used for reinforcement learning and how they considerably outperform traditional DQN, A3C gradient-based learning methods of training DNNs. By the end of this chapter, you will have a solid understanding of deep neuroevolution methods, and you'll also have practical experience with them. We will learn how to evolve agents so that they can play classic Atari games using deep neuroevolution. Also, you will learn how to use <strong>Visual Inspector for NeuroEvolution</strong> (<strong>VINE</strong>) to examine the results of experiments.</p>
<p class="p1">In this chapter, we'll cover the following topics:</p>
<ul class="ul1">
<li class="li1">Deep neuroevolution for deep reinforcement learning</li>
<li class="li1">Evolving agents to play Frostbite Atari games using deep neuroevolution</li>
<li>Training an agent to play the Frostbite game</li>
<li class="li1">Running the Frostbite Atari experiment</li>
<li class="li1">Examining the results with VINE</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p class="p3">The following technical requirements should be met so that you can complete the experiments described in this chapter:</p>
<ul class="ul1">
<li class="li3">A modern PC with a Nvidia graphics accelerator GeForce GTX 1080Ti or better </li>
<li class="li3">MS Windows 10, Ubuntu Linux 16.04, or macOS 10.14 with a discrete GPU</li>
<li class="li3">Anaconda Distribution version 2019.03 or newer</li>
</ul>
<p>The code for this chapter can be found at <a href="https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/tree/master/Chapter10">https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/tree/master/Chapter10</a></p>


            

            
        
    

        

                            
                    <h1 class="header-title">Deep neuroevolution for deep reinforcement learning</h1>
                
            
            
                
<p class="p3">In this book, we have already covered how the neuroevolution method can be applied to solve simple <strong>reinforcement learning</strong> (<strong>RL</strong>) tasks, such as single- and double-pole balancing in <a href="34913ccd-6aac-412a-8f54-70d1900cef41.xhtml" target="_blank">Chapter 4</a>, <em>Pole-Balancing Experiments</em>. However, while the pole-balancing experiment is exciting and easy to conduct, it is pretty simple and operates with tiny artificial neural networks. In this chapter, we will discuss how to apply neuroevolution to reinforcement learning problems that require immense ANNs to approximate the value function of the RL algorithm.</p>
<p class="p1">The RL algorithm learns through trial and error. Almost all the variants of RL algorithms try to optimize the value function, which maps the current state of the system to the appropriate action that will be performed in the next time step. The most widely used classical version of the RL algorithm uses a Q-learning method that is built around a table of states keyed by actions, which constitute the policy rules to be followed by the algorithm after training is complete. The training consists of updating the cells of the Q-table by iteratively executing specific actions at particular states and collecting the reward signals afterward. The following formula determines the process of updating a particular cell in a Q-table:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/c2b89378-75ed-435b-9129-bb0942dd3642.png" style="width:34.25em;height:2.25em;"/></p>
<p class="p1">Here <img class="fm-editor-equation" src="img/a8ce4769-6a5a-41fa-a659-5774d19f3608.png" style="width:1.00em;height:1.00em;"/> is the reward that's received when the system state changes from state <img class="fm-editor-equation" src="img/fb3f1125-08a8-4221-aec4-8da38d84ef4c.png" style="width:1.00em;height:1.00em;"/> to state <img class="fm-editor-equation" src="img/5af7a49b-2ffc-49c6-9b42-9a2c41d56fc1.png" style="width:2.00em;height:1.00em;"/>, <img class="fm-editor-equation" src="img/d747ca3a-3547-4da6-a5b1-9093ac311bb4.png" style="width:1.17em;height:1.00em;"/> is the action taken at time <img class="fm-editor-equation" src="img/038e002e-5dc9-45cf-8613-e9f369d1dd4e.png" style="width:0.42em;height:0.92em;"/> leading to the state change, <img class="fm-editor-equation" src="img/8d7ff50c-3187-487b-bcd8-44f13acbe3e1.png" style="width:0.92em;height:0.83em;"/> is the learning rate, and <img class="fm-editor-equation" src="img/6d098dd0-38e8-4639-b80d-abdd4c795fe7.png" style="width:0.67em;height:1.00em;"/> is a discount factor that controls the importance of the future rewards. The learning rate determines to what extent the new information overrides existing information in the specific Q-table cell. If we set the learning rate to zero, then nothing will be learned, and if we set it to <em>1</em>, then nothing will be retained. Thus, the learning rate controls how fast the system is able to learn new information while maintaining useful, already-learned data.</p>
<p class="p1">The simple version of the Q-learning algorithm iterates over all possible action-state combinations and updates the Q-values, as we've already discussed. This approach works pretty well for simple tasks with a small number of action-state pairs but quickly fails with an increase in the number of such pairs, that is, with increased dimensionality of the action-state space. Most real-world tasks have profound dimensionality of the action-state space, which makes it infeasible for the classical version of Q-learning.</p>
<p class="p1">The method of Q-value function approximation was proposed to address the problem of increased dimensionality. In this method, the Q-learning policy is defined not by the action-state table we mentioned earlier but is instead approximated by a function. One of the ways to achieve this approximation is to use an ANN as a universal approximation function. By using an ANN, especially a deep ANN for Q-value approximation, it becomes possible to use RL algorithms for very complex problems, even for problems with a continuous state space. Thus, the DQN method was devised, which uses a DNN for Q-value approximation. The RL based on the DNN value-function approximation was named <strong>deep reinforcement learning</strong> (<strong>deep RL</strong>).</p>
<p class="p1">With deep RL, it is possible to learn action policies directly from the pixels of a video stream. This allows us to use a video stream to train agents to play video games, for example. However, the DQN method can be considered a gradient-based method. It uses error (loss) backpropagation in the DNN to optimize the Q-value function approximator. While being a potent technique, it has a significant drawback regarding the computational complexity that's involved, which requires the use of GPUs to perform all the matrix multiplications during the gradient descent-related computations.</p>
<p class="p1">One of the methods that can be used to reduce computational costs is <strong>Genetic Algorithms</strong> (<strong>GA</strong>), such as neuroevolution. Neuroevolution allows us to evolve a DNN for the Q-value function approximation without any gradient-based computations involved. In recent studies, it was shown that gradient-free GA methods show excellent performance when it comes to challenging deep RL tasks and that they can even outperform their conventional counterparts. In the next section, we'll discuss how the deep neuroevolution method can be used to train successful agents to play one of the classic Atari games, just by reading game screen observations.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Evolving an agent to play the Frostbite Atari game using deep neuroevolution</h1>
                
            
            
                
<p class="p3">Recently, classic Atari games were encapsulated by the <strong>Atari Learning Environment</strong> (<strong>ALE</strong>) to become a benchmark for testing different implementations of RL algorithms. Algorithms that are tested against the ALE are required to read the game state from the pixels of the game screen and devise a sophisticated control logic that allows the agent to win the game. Thus, the task of the algorithm is to evolve an understanding of the game situation in terms of the game character and its adversaries. Also, the algorithm needs to understand the reward signal that's received from the game screen in the form of the final game score at the end of a single game run.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The Frostbite Atari game</h1>
                
            
            
                
<p class="p3">Frostbite is a classic Atari game where you control a game character that is building an igloo. The game screen is shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-803 image-border" src="img/9ecfe780-eea0-4e30-b482-9a48c91801a5.png" style="width:27.67em;height:21.58em;"/></p>
<p>The Frostbite game screen</p>
<p class="p1">The bottom part of the screen is water, with floating ice blocks arranged in four rows. The game character jumps from one row to another while trying to avoid various foes. If the game character jumps on a white ice block, this block is collected and used to build an igloo on the shore in the top right of the screen. After that, the white ice block changes its color and cannot be used anymore.</p>
<p class="p1">To build the igloo, the game character must collect 15 ice blocks within 45 seconds. Otherwise, the game ends because the game character becomes frozen. When the igloo is complete, the game character must move inside it to complete the current level. The faster the game character completes a level, the more bonus points are awarded to the player.</p>
<p class="p1">Next, we'll discuss how the game screen state is mapped into input parameters, which can be used by the neuroevolution method.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Game screen mapping into actions</h1>
                
            
            
                
<p class="p3">Deep ANNs can be trained to play Atari games if they can directly map the pixels on the screen to a system to control the game. This means that our algorithm must read the game screen and decide what game action to take to get the highest game score possible.</p>
<p class="p3">This task can be divided into two logical subtasks: </p>
<ul class="ul1">
<li class="li3">The image analysis task, which encodes the state of the current game situation on the screen, including the game character's position, obstacles, and adversaries</li>
<li class="li3">The RL training task, which is used to train the Q-value approximation ANN to build the correct mapping between the specific game state and actions to be performed</li>
</ul>
<p class="p3"><strong>Convolutional Neural Networks</strong> (<strong>CNNs</strong>) are commonly used in tasks related to the analysis of visual imagery or other high-dimensional Euclidean data. The power of CNNs is based on their ability to significantly reduce the number of learning parameters compared to other types of ANN if they're applied to visual recognition. The CNN hierarchy usually has multiple sequential convolutional layers combined with non-linear fully connected layers and ends with a fully connected layer that is followed by the loss layer. The final fully connected and loss layers implement the high-level reasoning in the Neural Network architecture. In the case of deep RL, these layers make the Q-value approximation. Next, we'll consider the details of convolutional layer implementation.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Convolutional layers</h1>
                
            
            
                
<p class="p3">By studying the organization of the visual cortex of higher life forms (including humans), researchers gained inspiration for the design of CNNs. Each neuron of the visual cortex responds to signals that are received from a limited region of the visual field – the neuron's receptive field. The receptive fields of various neurons overlap partially, which allows them to cover the entire visual field, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-804 image-border" src="img/560f658d-543d-4b08-b4d3-269287909c3a.png" style="width:31.92em;height:20.50em;"/></p>
<p>The scheme of connections between the receptive field (on the left) and neurons in the convolutional layer (on the right)</p>
<p class="p1">The convolutional layer consists of a column of neurons, where each neuron in a single column is connected to the same receptive field. This column represents a set of filters (kernels). Each filter is defined by the size of the receptive field and the number of channels. The number of channels defines the depth of the neurons column, while the size of the receptive field determines the number of columns in the convolutional layer. When the receptive field moves over the visual field, in each step, the new column of neurons is activated.</p>
<p class="p1">As we mentioned previously, each convolutional layer is usually combined with a fully connected layer with non-linear activation, such as <strong>Rectified Linear Unit</strong> (<strong>ReLU</strong>). The ReLU activation function has the effect of filtering out negative values, as given by the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/09ad5aa4-e9d6-42f3-b871-e1377f38c153.png" style="width:13.42em;height:1.67em;"/></p>
<p class="p1">Here, <img class="fm-editor-equation" src="img/0b414ba6-373b-4aba-a98a-d9fd256e41f4.png" style="width:0.75em;height:0.75em;"/> is the input to a neuron.</p>
<p class="p1">In the ANN architecture, several convolutional layers are connected to a number of fully connected layers, which performs high-level reasoning. Next, we'll discuss the CNN architecture that's used in our experiment.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The CNN architecture to train the Atari playing agent</h1>
                
            
            
                
<p class="p2">In our experiment, we'll use a CNN architecture consisting of three convolutional layers with 32, 64, and 64 channels, followed by a fully connected layer with 512 units and the output layer with the number of layers corresponding to the number of game actions. The convolutional layers have 8 x 8, 4 x 4, and 3 x 3 kernel sizes and use strides of 4, 2, and 1, respectively. The ReLU non-linearity follows all the convolutional and fully connected layers.</p>
<p class="p2">The source code to create the described network graph model using the TensorFlow framework is defined as follows:</p>
<pre>class LargeModel(Model):<br/>    def _make_net(self, x, num_actions):<br/>        x = self.nonlin(self.conv(x, name='conv1', num_outputs=32, <br/>                                  kernel_size=8, stride=4, std=1.0))<br/>        x = self.nonlin(self.conv(x, name='conv2', num_outputs=64, <br/>                                  kernel_size=4, stride=2, std=1.0))<br/>        x = self.nonlin(self.conv(x, name='conv3', num_outputs=64, <br/>                                  kernel_size=3, stride=1, std=1.0))<br/>        x = self.flattenallbut0(x)<br/>        x = self.nonlin(self.dense(x, 512, 'fc'))<br/><br/>        return self.dense(x, num_actions, 'out', std=0.1)</pre>
<p class="p1">As a result of this architecture, the CNN contains about <strong>4 million trainable parameters</strong>. Next, we'll discuss how RL training is done in our experiment.</p>
<p>For complete implementation details, refer to the <kbd>dqn.py</kbd> Python script at <a href="https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/dqn.py">https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/dqn.py</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The RL training of the game agent</h1>
                
            
            
                
<p class="p1">The RL training in our experiment was implemented using the neuroevolution method. This method is based on a simple genetic algorithm that evolves a population of individuals. The genotype of each individual encodes the vector of the trainable parameters of the controller ANN. By trainable parameters, we mean the connection weights between the network nodes. In every generation, each genotype is evaluated against a test environment by playing Frostbite and produces a specific fitness score. We evaluate each agent (genome) against 20,000 frames of the game. During the evaluation period, the game character can play multiple times, and the final Atari game score is the fitness score, which is a reward signal in terms of RL.</p>
<p class="p1">Next, we'll discuss the genome encoding scheme, which allows us to encode more than 4 million learning parameters of the ANN that's controlling the game-solving agent.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The genome encoding scheme</h1>
                
            
            
                
<p class="p2">The deep RL neural network that we use as the controller of the game agent has about 4 million trainable parameters. Each trainable parameter is the weight of a connection between two nodes of the neural network. Traditionally, training neural networks is about finding the appropriate values of all the connection weights, allowing the neural network to approximate a function that describes the specifics of the modeled process.</p>
<p class="p1">The conventional way to estimate these trainable parameters is to use some form of error backpropagation based on the gradient descent of the loss value, which is very computationally intensive. On the other hand, the neuroevolution algorithm allows us to train ANNs using a nature-inspired genetic algorithm. The neuroevolution algorithm applies a series of mutations and recombinations to the trainable parameters to find the correct configuration of the ANN. However, to use the genetic algorithm, an appropriate scheme of encoding of the phenotype ANN should be devised. After that, the population of individuals (genomes encoding the phenotype ANN) can be created and evolved using a simple genetic algorithm, which we'll discuss later.</p>
<p class="p1">As we mentioned earlier, the encoding scheme should produce compact genomes that can encode values of more than 4 million connection weights between the nodes of the deep RL ANN controlling the game agent. We are looking for compact genomes to reduce the computational costs associated with genetic algorithm evaluation. Next, we'll discuss the definition of the genome encoding scheme, which can be used to encode the large phenotype ANN.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Genome encoding scheme definition</h1>
                
            
            
                
<p class="p1">Researchers at the Uber AI lab have proposed a coding scheme that uses the seed of a pseudo-random number generator to encode the phenotype ANN. In this scheme, the genome is represented as a list of seed values, which is applied sequentially to generate the values for all the weights (trainable parameters) of connections that are expressed between the nodes of a controller ANN.</p>
<p class="p1">In other words, the first seed value in the list represents the policy initialization seed, which is shared between the genealogy of the descendants of a single parent. All the subsequent seed values represent the specific mutations that are acquired during the evolution process by the offspring. Each seed is applied sequentially to produce the ANN parameters vector of a specific phenotype. The following formula defines the estimation of the phenotype parameters vector for a specific individual (<img class="fm-editor-equation" src="img/13909b80-d25b-4184-b366-f96c9f975f53.png" style="width:0.83em;height:1.00em;"/>):</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/776be492-a072-455d-96d6-90bec14f7106.png" style="width:12.08em;height:4.25em;"/></p>
<p class="p1">Here, <img class="fm-editor-equation" src="img/1981550c-2382-4367-8347-2dc4254fdb36.png" style="width:0.67em;height:0.75em;"/> is the encoding of <img class="fm-editor-equation" src="img/56b74879-2d2d-47a1-a0d7-9cdaab645845.png" style="width:1.00em;height:1.00em;"/> and consists of a list of mutation seeds; <img class="fm-editor-equation" src="img/62ff51bb-f7d9-4625-a301-3a06360e0676.png" style="width:6.67em;height:1.25em;"/> is a deterministic Gaussian pseudo-random number generator with an input seed <img class="fm-editor-equation" src="img/8bc7ccc1-51cf-4d57-945b-d8a09fca3658.png" style="width:1.00em;height:0.92em;"/> that produces a vector of length <img class="fm-editor-equation" src="img/b118aca6-1e3a-452c-a1a2-ee497c040b41.png" style="width:0.92em;height:1.08em;"/>; <img class="fm-editor-equation" src="img/41c3bd93-95fa-45d5-8bc6-04ffdbcf35df.png" style="width:0.83em;height:0.92em;"/> is an initial parameters vector that's created during initialization as follows, <img class="fm-editor-equation" src="img/bb4d1685-94bc-4a18-9273-bc2976e7e3ee.png" style="width:3.83em;height:1.08em;"/>, where <img class="fm-editor-equation" src="img/54023dfc-394f-4c25-8c75-c12e92a9650c.png" style="width:0.58em;height:1.08em;"/> is a deterministic initialization function; and <img class="fm-editor-equation" src="img/dae292b2-7070-4859-9119-f5f3a0271ccb.png" style="width:0.75em;height:0.75em;"/> is a mutation power that determines the strength of the influence of all the subsequent parameter vectors on the initial parameters vector <img class="fm-editor-equation" src="img/9fd6b743-557c-43f2-9f3c-14ff0b5d0a4a.png" style="width:0.92em;height:1.00em;"/>.</p>
<p class="p1">In the current implementation, <img class="fm-editor-equation" src="img/2082ad39-de82-43c4-b07e-a9e5c6e0bea7.png" style="width:2.00em;height:1.17em;"/> is a precomputed table with 250 million random vectors indexed using 28-bit seeds. This is done to speed up runtime processing because lookup by index is faster than the generation of new random numbers. Next, we'll discuss how to implement an encoding scheme in the Python source code.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Genome encoding scheme implementation</h1>
                
            
            
                
<p class="p1">The following source code implements ANN parameter estimations, as defined by the formula from the previous section (see the <kbd>compute_weights_from_seeds</kbd> function):</p>
<pre>    idx = seeds[0]<br/>    theta = noise.get(idx, self.num_params).copy() * self.scale_by<br/><br/>    for mutation in seeds[1:]:<br/>        idx, power = mutation<br/>        theta = self.compute_mutation(noise, theta, idx, power)<br/>    return theta</pre>
<p class="p1">The <kbd>compute_mutation</kbd> function implements an estimation of the single step of the ANN parameter estimation, as follows:</p>
<pre>    def compute_mutation(self, noise, parent_theta, idx, mutation_power):<br/>        return parent_theta + mutation_power * noise.get(idx, <br/>                                                        self.num_params)</pre>
<p class="p1">The preceding code takes the vector of the parent's trainable parameters and adds to it a random vector that's produced by a deterministic pseudo-random generator using a specific seed index. The mutation power parameter scales the generated random vector before it is added to the parent's parameters vector.</p>
<p>For more implementation details, refer to the <kbd>base.py</kbd> script at <a href="https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/base.py">https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/base.py</a>.</p>
<p class="p1">Next, we'll discuss the particulars of a simple genetic algorithm that's used to train the Frostbite playing agent.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The simple genetic algorithm</h1>
                
            
            
                
<p class="p2">The simple genetic algorithm that's used in our experiment evolves the population of <em>N</em> individuals over generations of evolution. As we mentioned previously, each individual genome encodes the vector of the trainable ANN parameters. Also, in each generation, we select the top <em>T</em> individuals to become the parents for the next generation.</p>
<p class="p2">The process of producing the next generation is implemented as follows. For <em>N-1</em> repetitions, we do the following:</p>
<ol class="ol1">
<li class="li2">The parent is selected uniformly at random and removed from the selection list. </li>
<li class="li2">The mutation is applied to the selected parent by applying additive Gaussian noise to the parameter vector that's encoded by the individual.</li>
<li class="li2">Next, we add the new organism to the list of individuals for the next generation.</li>
</ol>
<p class="p2">After that, the best individual from the current generation is copied in its unmodified state to the next generation (elitism). To guarantee that the best individual was selected, we evaluate each of the 10 top individuals in the current generation against 30 additional game episodes. The individual with the highest mean fitness score is then selected as an elite to be copied to the next generation.</p>
<p class="p1">The mutation of the parent individual is implemented as follows:</p>
<pre>    def mutate(self, parent, rs, noise, mutation_power):<br/>        parent_theta, parent_seeds = parent<br/>        idx = noise.sample_index(rs, self.num_params)<br/>        seeds = parent_seeds + ((idx, mutation_power), )<br/>        theta = self.compute_mutation(noise, parent_theta, idx, <br/>                                      mutation_power)<br/>        return theta, seeds</pre>
<p class="p1">This function receives the phenotype and genotype of the parent individual, the random source, along with the precomputed noise table (250 million vectors), and the mutation power value. The random source produces the random seed number (<kbd>idx</kbd>), which is used as an index so that we can select the appropriate parameters vector from the noise table. After that, we create the offspring genome by combining the list of parent seeds with a new seed. Finally, we create the phenotype of the offspring by combining the phenotype of the parent with Gaussian noise that's been extracted from the shared noise table using a randomly sampled seed index we obtained earlier (<kbd>idx</kbd>). In the next section, we will look at an experiment we can perform in order to train an agent to play the Frostbite Atari game.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Training an agent to play the Frostbite game</h1>
                
            
            
                
<p class="p3">Now that we have discussed the theory behind the game-playing agent's implementation, we are ready to start working on it. Our implementation is based on the source code provided by the Uber AI Lab on GitHub at <a href="https://github.com/uber-research/deep-neuroevolution">https://github.com/uber-research/deep-neuroevolution</a>. The source code in this repository contains an implementation of two methods to train DNNs: the CPU-based methods for multicore systems (up to 720 cores) and the GPU-based methods. We are interested in the GPU-based implementation because the majority of practitioners don't have access to such behemoths of technology as a PC with 720 CPU cores. At the same time, it is pretty easy to get access to a modern Nvidia GPU.</p>
<p class="p3">Next, we'll discuss the implementation details.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Atari Learning Environment</h1>
                
            
            
                
<p class="p3">During agent training, we need to simulate actual gameplay in the Atari system. This can be done using the ALE, which simulates an Atari system that can run ROM images of the games. The ALE provides an interface that allows us to capture game screen frames and control the game by emulating the game controller. Here, we'll use the ALE modification that's available at <a href="https://github.com/yaricom/atari-py">https://github.com/yaricom/atari-py</a>.</p>
<p class="p3">Our implementation uses the TensorFlow framework to implement ANN models and execute them on the GPU. Thus, the corresponding bridge needs to be implemented between ALE and TensorFlow. This is done by implementing a custom TensorFlow operation using the C++ programming language for efficiency. The corresponding Python interface is also provided as an AtariEnv Python class at <a href="https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/gym_tensorflow/atari/tf_atari.py">https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/gym_tensorflow/atari/tf_atari.py</a>.</p>
<p class="p3">AtariEnv provides functions so that we can execute a single game step, reset the game, and return the current game state (observation). Next, we'll discuss each function.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The game step function</h1>
                
            
            
                
<p class="p2">The game step function executes a single game step using the provided actions. It is implemented as follows:</p>
<pre>    def step(self, action, indices=None, name=None):<br/>        if indices is None:<br/>            indices = np.arange(self.batch_size)<br/>        with tf.variable_scope(name, default_name='AtariStep'):<br/>            rew, done = gym_tensorflow_module.environment_step(<br/>                               self.instances, indices, action)<br/>            return rew, done</pre>
<p class="p1">This function applies the game action that's received from the controller ANN to the current game environment. Please note that this function can execute a single game step simultaneously in multiple game instances. The <kbd>self.batch_size</kbd> parameter or the length of the <kbd>indices</kbd> input tensor determines the number of game instances we'll have. The function returns two tensors: one tensor with rewards (game score) and another with flags indicating whether the current game evaluation is complete after this step (solved or failed). Both tensors have a length equal to <kbd>self.batch_size</kbd> or the length of the <kbd>indices</kbd> input tensor.</p>
<p class="p1">Next, we'll discuss how the game observations are created.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The game observation function</h1>
                
            
            
                
<p class="p2">This function obtains the current game state from the Atari environment as a game screen buffer. This function is implemented as follows:</p>
<pre>    def observation(self, indices=None, name=None):<br/>        if indices is None:<br/>            indices = np.arange(self.batch_size)<br/>        with tf.variable_scope(name, default_name='AtariObservation'):<br/>            with tf.device('/cpu:0'):<br/>                obs = gym_tensorflow_module.environment_observation(<br/>                                   self.instances, indices, T=tf.uint8)<br/><br/>            obs = tf.gather(tf.constant(self.color_pallete), <br/>                                                tf.cast(obs,tf.int32))<br/>            obs = tf.reduce_max(obs, axis=1)<br/>            obs = tf.image.resize_bilinear(obs, self.warp_size, <br/>                                                   align_corners=True)<br/>            obs.set_shape((None,) + self.warp_size + (1,))<br/>            return obs</pre>
<p class="p1">This function acquires a screengrab from the Atari environment and wraps it in a tensor that can be used by the TensorFlow framework. The game observation function also allows us to receive the state from multiple games, which is determined either by the <kbd>self.batch_size</kbd> parameter or the length of the <kbd>indices</kbd> input parameter. The function returns screengrabs from multiple games, wrapped in a tensor.</p>
<p class="p1">We also need to implement the function to reset the Atari environment to the initial random state, which we'll discuss next.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The reset Atari environment function</h1>
                
            
            
                
<p class="p2">To train game agents, we need to implement a function that starts the Atari environment from a particular random state. It is vital to implement a stochastic Atari reset function to guarantee that our agent can play the game from any initial state. The function is implemented as follows:</p>
<pre>    def reset(self, indices=None, max_frames=None, name=None):<br/>        if indices is None:<br/>            indices = np.arange(self.batch_size)<br/>        with tf.variable_scope(name, default_name='AtariReset'):<br/>            noops = tf.random_uniform(tf.shape(indices), minval=1, <br/>                                       maxval=31, dtype=tf.int32)<br/>            if max_frames is None:<br/>                max_frames = tf.ones_like(indices, dtype=tf.int32) * \<br/>                                         (100000 * self.frameskip)<br/>            import collections<br/>            if not isinstance(max_frames, collections.Sequence):<br/>                max_frames = tf.ones_like(indices, dtype=tf.int32) * \<br/>                                          max_frames<br/>            return gym_tensorflow_module.environment_reset(self.instances, <br/>                             indices, noops=noops, max_frames=max_frames)</pre>
<p class="p1">This function uses the <kbd>indices</kbd> input parameter to simultaneously reset multiple instances of Atari games in the random initial states. This function also defines the maximum number of frames for each game instance.</p>
<p class="p1">Next, we'll discuss how RL evaluation is performed on GPU cores.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">RL evaluation on GPU cores</h1>
                
            
            
                
<p class="p3">In our experiment, we'll implement an RL evaluation using the TensorFlow framework on GPU devices. This means that all the calculations related to the propagation of input signals through the controller ANN are performed on the GPU. This allows us to effectively calculate more than 4 million training parameters – the connection weights between control ANN nodes – for every single time step of the game. Furthermore, we can concurrently simulate multiple runs of the game in parallel, each controlled by a different controller ANN.</p>
<p class="p3">The concurrent evaluation of multiple game controller ANNs is implemented by two Python classes: <kbd>RLEvalutionWorker</kbd> and <kbd>ConcurrentWorkers</kbd>. Next, we'll discuss each class.</p>
<p>For complete implementation details, refer to the <kbd>concurrent_worker.py</kbd> class at <a href="https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/concurrent_worker.py">https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/concurrent_worker.py</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The RLEvalutionWorker class</h1>
                
            
            
                
<p class="p1">This class holds the configuration and the network graph of the controller ANN. It provides us with methods so that we can create a network graph of the controller ANN, run an evaluation loop over the created network graph, and put new tasks into the evaluation loop. Next, we'll discuss how the network graph is created from a network model.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating the network graph</h1>
                
            
            
                
<p class="p3">The TensorFlow network graph is created by the <kbd>make_net</kbd> function, which receives the ANN model constructor, the GPU device identifier, and the batch size as input parameters. The network graph is created as follows:</p>
<ol class="ol1">
<li class="li1">We'll start by creating the controller ANN model and the game evaluation environment:</li>
</ol>
<pre style="padding-left: 60px">    self.model = model_constructor()<br/>    …<br/>    with tf.variable_scope(None, default_name='model'):<br/>        with tf.device(‘/cpu:0'):<br/>            self.env = self.make_env_f(self.batch_size)</pre>
<ol start="2">
<li>Next, we'll create placeholders so that we can receive values during network graph evaluation. Also, we'll create an operator to reset the game before the start of the new game episode:</li>
</ol>
<pre style="padding-left: 60px">        self.placeholder_indices = tf.placeholder(tf.int32, <br/>                                                    shape=(None, ))<br/>        self.placeholder_max_frames = tf.placeholder(<br/>                                          tf.int32, shape=(None, ))<br/>        self.reset_op = self.env.reset(<br/>                            indices=self.placeholder_indices, <br/>                            max_frames=self.placeholder_max_frames)</pre>
<ol start="3">
<li>After that, using the context of the provided GPU device, we'll create two operators to receive game state observations and evaluate the game actions that follow:</li>
</ol>
<pre style="padding-left: 60px">        with tf.device(device):<br/>            self.obs_op = self.env.observation(<br/>                            indices=self.placeholder_indices)<br/>            obs = tf.expand_dims(self.obs_op, axis=1)<br/>            self.action_op = self.model.make_net(obs, <br/>                            self.env.action_space, <br/>                            indices=self.placeholder_indices, <br/>                            batch_size=self.batch_size, <br/>                            ref_batch=ref_batch)</pre>
<ol start="4">
<li>The <kbd>action</kbd> operator returns an array of action likelihood values, which needs to be filtered if the action space is discrete:</li>
</ol>
<pre style="padding-left: 60px">        if self.env.discrete_action:<br/>            self.action_op = tf.argmax(<br/>                        self.action_op[:tf.shape(<br/>                        self.placeholder_indices)[0]], <br/>                        axis=-1, output_type=tf.int32)</pre>
<p class="p1" style="padding-left: 60px">The code checks whether the current game environment requires discrete actions and wraps an <kbd>action</kbd> operator using the built-in <kbd>tf.argmax</kbd> operator of the TensorFlow framework. The <kbd>tf.argmax</kbd> operator returns the index of the action with the largest value, which can be used to signal that a specific game action should be executed.</p>
<p>The Atari game environment is a discrete action environment, which means that only one action is accepted at each time step.</p>
<ol start="5">
<li class="p1">Finally, we create the operator to perform a single game step:</li>
</ol>
<pre style="padding-left: 60px">        with tf.device(device):<br/>            self.rew_op, self.done_op = \<br/>                       self.env.step(self.action_op, <br/>                       indices=self.placeholder_indices)</pre>
<p class="p1">Here, we create a single game step operator, which returns operations to obtain rewards, <kbd>self.rew_op</kbd>, and the game completed status, <kbd>self.done_op</kbd>, after the execution of a single game step.</p>
<p class="p1">Next, we'll discuss how the evaluation loop is implemented.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The graph evaluation loop</h1>
                
            
            
                
<p class="p3">This is the loop that we use to evaluate the previously created network graph over multiple games in parallel – the number of games that can be evaluated simultaneously is determined by the <kbd>batch_size</kbd> parameter.</p>
<p class="p3">The evaluation loop is defined in the <kbd>_loop</kbd> function and is implemented as follows:</p>
<ol>
<li class="p5">First, we start with the creation of arrays to hold game evaluation values over multiple episodes:</li>
</ol>
<pre style="padding-left: 90px">running = np.zeros((self.batch_size,), dtype=np.bool)<br/>cumrews = np.zeros((self.batch_size, ), dtype=np.float32)<br/>cumlen = np.zeros((self.batch_size, ), dtype=np.int32)</pre>
<ol start="2">
<li>Next, we start the loop and set the corresponding indices of the running array we just created to <kbd>True</kbd>:</li>
</ol>
<pre style="padding-left: 60px">    while True:<br/>        # nothing loaded, block<br/>        if not any(running):<br/>            idx = self.queue.get()<br/>            if idx is None:<br/>               break<br/>            running[idx] = True<br/>        while not self.queue.empty():<br/>           idx = self.queue.get()<br/>           if idx is None:<br/>                 break<br/>           running[idx] = True</pre>
<ol start="3">
<li>Using the indices array, we are ready to execute a single game step operator and collect the results:</li>
</ol>
<pre style="padding-left: 90px">indices = np.nonzero(running)[0]<br/>rews, is_done, _ = self.sess.run(<br/>          [self.rew_op, self.done_op, self.incr_counter], <br/>          {self.placeholder_indices: indices})<br/>cumrews[running] += rews<br/>cumlen[running] += 1</pre>
<ol start="4">
<li>Finally, we need to test whether any of the evaluated games are done, either by winning or by hitting the maximum game frames limit. For all the completed tasks, we apply a number of operations, as follows:</li>
</ol>
<pre style="padding-left: 90px">if any(is_done):<br/>    for idx in indices[is_done]:<br/>        self.sample_callback[idx](self, idx, <br/>              (self.model.seeds[idx],cumrews[idx], <br/>                                         cumlen[idx]))<br/>    cumrews[indices[is_done]] = 0.<br/>    cumlen[indices[is_done]] = 0.<br/>    running[indices[is_done]] = False</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="p1">The preceding code uses the indices of all the completed tasks and invokes the corresponding registered callbacks before resetting the collector variables at specific indices.</p>
<p class="p1">Now, we are ready to discuss how to add and run the new task using our worker.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The asynchronous task runner</h1>
                
            
            
                
<p class="p3">This function registers a specific task to be evaluated by a worker in the GPU device context. It takes the ID of the task, the task object holder, and the callback to be executed on task completion as input. This function is defined under the name <kbd>run_async</kbd> and is implemented as follows:</p>
<ol>
<li class="p3">First, it extracts the corresponding data from the task object and loads it into the current TensorFlow session:</li>
</ol>
<pre style="padding-left: 30px">    theta, extras, max_frames=task<br/>    self.model.load(self.sess, task_id, theta, extras)<br/>    if max_frames is None:<br/>        max_frames = self.env.env_default_timestep_cutoff</pre>
<p class="p1" style="padding-left: 60px">Here, <kbd>theta</kbd> is an array with all the connection weights in the controller ANN model, <kbd>extras</kbd> holds a random seeds list of the corresponding genome, and <kbd>max_frames</kbd> is the game frame's cutoff value.</p>
<ol start="2">
<li>Next, we run the TensorFlow session with <kbd>self.reset_op</kbd>, which resets a specific game environment at a specified index:</li>
</ol>
<pre style="padding-left: 30px">    self.sess.run(self.reset_op, {self.placeholder_indices:[task_id], <br/>                  self.placeholder_max_frames:[max_frames]})<br/>    self.sample_callback[task_id] = callback<br/>    self.queue.put(task_id)</pre>
<p class="p1">The code runs <kbd>self.reset_op</kbd> within the TensorFlow session. Also, we register the current task identifier with the <kbd>reset</kbd> operator and the maximum game frame's cutoff value for a given task. The task identifier is used in the evaluation loop to associate evaluation results of the network graph with a specific genome in the population. Next, we'll discuss how concurrent asynchronous workers are maintained.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The ConcurrentWorkers class</h1>
                
            
            
                
<p class="p3">The <kbd>ConcurrentWorkers</kbd> class holds the configuration of the concurrent execution environment, which includes several evaluation workers (<kbd>RLEvalutionWorker</kbd> instances) and auxiliary routines to support multiple executions of concurrent tasks.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating the evaluation workers</h1>
                
            
            
                
<p class="p3">One of the primary responsibilities of the <kbd>ConcurrentWorkers</kbd> class is to create and manage <kbd>RLEvalutionWorker</kbd> instances. This is done in the class constructor as follows:</p>
<pre>    self.workers = [RLEvalutionWorker(make_env_f, *args, <br/>         ref_batch=ref_batch, <br/>         **dict(kwargs, device=gpus[i])) for i in range(len(gpus))]<br/>    self.model = self.workers[0].model<br/>    self.steps_counter = sum([w.steps_counter for w in self.workers])<br/>    self.async_hub = AsyncTaskHub()<br/>    self.hub = WorkerHub(self.workers, self.async_hub.input_queue, <br/>                            self.async_hub)</pre>
<p class="p1">Here, we create the number of <kbd>RLEvalutionWorker</kbd> instances that are correlated to the number of GPU devices that are available in the system. After that, we initialize the selected ANN graph model and create auxiliary routines to manage multiple executions of asynchronous tasks. Next, we'll discuss how work tasks are scheduled for execution.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Running work tasks and monitoring results</h1>
                
            
            
                
<p class="p3">To use the RL evaluation mechanism we described earlier, we need a method to schedule the work task for evaluation and to monitor the results. This is implemented in the <kbd>monitor_eval</kbd> function, which receives the list of genomes in the population and evaluates them against the Atari game environment. This function has two essential implementation parts, both of which we'll discuss in this section:</p>
<ol>
<li>First, we iterate over all the genomes in the list and create the asynchronous work task so that each genome can be evaluated against the Atari game environment:</li>
</ol>
<pre style="padding-left: 60px">    tasks = []<br/>    for t in it:<br/>        tasks.append(self.eval_async(*t, max_frames=max_frames, <br/>                                    error_callback=error_callback))<br/>        if time.time() - tstart &gt; logging_interval:<br/>            cur_timesteps = self.sess.run(self.steps_counter)<br/>            tlogger.info('Num timesteps:', cur_timesteps, <br/>             'per second:', <br/>             (cur_timesteps-last_timesteps)//(time.time()-tstart),<br/>             'num episodes finished: {}/{}'.format(<br/>             sum([1 if t.ready() else 0 for t in tasks]), <br/>             len(tasks)))<br/>            tstart = time.time()<br/>            last_timesteps = cur_timesteps</pre>
<p class="p1" style="padding-left: 60px">The preceding code schedules each genome in the list for asynchronous evaluation and saves a reference to each asynchronous task for later use. Also, we periodically output the results of the evaluation process of already scheduled tasks. Now, we'll discuss how to monitor the evaluation results.</p>
<ol start="2">
<li>The following code block is waiting for the completion of asynchronous tasks:</li>
</ol>
<pre style="padding-left: 60px">    while not all([t.ready() for t in tasks]):<br/>        if time.time() - tstart &gt; logging_interval:<br/>            cur_timesteps = self.sess.run(self.steps_counter)<br/>            tlogger.info('Num timesteps:', cur_timesteps, 'per second:', (cur_timesteps-last_timesteps)//(time.time()-tstart), 'num episodes:', sum([1 if t.ready() else 0 for t in tasks]))<br/>            tstart = time.time()<br/>            last_timesteps = cur_timesteps<br/>        time.sleep(0.1)</pre>
<p class="p1" style="padding-left: 60px">Here, we iterate over all the references to the scheduled asynchronous tasks and wait for their completion. Also, we periodically output the evaluation progress. Next, we'll discuss how task evaluation results are collected.</p>
<ol start="3">
<li>Finally, after the completion of all tasks, we collect the results, as follows:</li>
</ol>
<pre style="padding-left: 60px">    tlogger.info(<br/>       'Done evaluating {} episodes in {:.2f} seconds'.format(<br/>                          len(tasks), time.time()-tstart_all))<br/>    return [t.get() for t in tasks]</pre>
<p class="p1">The code iterates through all the references to the scheduled asynchronous tasks and creates a list of the evaluation results. Next, we'll discuss the experiment runner implementation.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Experiment runner</h1>
                
            
            
                
<p class="p1">The experiment runner implementation receives a configuration of the experiment defined in the JSON file and runs the neuroevolution process for the specified number of game time steps. In our experiment, the evaluation stops after reaching 1.5 billion time steps of Frostbite. Next, we'll discuss the experiment configuration details.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Experiment configuration file</h1>
                
            
            
                
<p class="p2">Here is the file that provides configuration parameters for the experiment runner. For our experiment, it has the following content:</p>
<pre>{<br/>    "game": "frostbite",<br/>    "model": "LargeModel",<br/>    "num_validation_episodes": 30,<br/>    "num_test_episodes": 200,<br/>    "population_size": 1000,<br/>    "episode_cutoff_mode": 5000,<br/>    "timesteps": 1.5e9,<br/>    "validation_threshold": 10,<br/>    "mutation_power": 0.002,<br/>    "selection_threshold": 20<br/>}</pre>
<p>The configuration parameters are as follows:</p>
<ul class="ul1">
<li class="li1">The <kbd>game</kbd> parameter is the name of the game, as registered in the ALE. The full list of supported games is available at <a href="https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/gym_tensorflow/atari/tf_atari.py">https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/gym_tensorflow/atari/tf_atari.py</a>.</li>
<li class="li1">The <kbd>model</kbd> parameter designates the name of the network graph model to use for the construction of the controller ANN. The models are defined at <a href="https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/dqn.py">https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/dqn.py</a>.</li>
<li class="li1">The <kbd>num_validation_episodes</kbd> parameter defines how many game episodes are used for the evaluation of the top individuals in the population. After this step, we can select the true elite of the population.</li>
<li class="li1">The <kbd>num_test_episodes</kbd> parameter sets the number of game episodes to use to test the performance of the selected population elite.</li>
<li class="li1">The <kbd>population_size</kbd> parameter determines the number of genomes in the population.</li>
<li class="li1">The <kbd>episode_cutoff_mode</kbd> parameter defines how the game evaluation stops for a particular genome. The game episode can stop either upon the execution of a particular number of time steps or by using the default stop signal of the corresponding game environment.</li>
<li class="li1">The <kbd>timesteps</kbd> parameter sets the total number of time steps of the game to be executed during the neuroevolution process.</li>
<li class="li1">The <kbd>validation_threshold</kbd> parameter sets the number of top individuals that are selected from each generation for additional validation execution. The population elite is selected from these selected individuals.</li>
<li class="li1">The <kbd>mutation_power</kbd> parameter defines how subsequent mutations that are added to the individual influence the training parameters (connection weights).</li>
<li class="li1">The <kbd>selection_threshold</kbd> parameter determines how many parent individuals are allowed to produce offspring in the next generation.</li>
</ul>
<p class="p1">Now, we are ready to discuss the implementation details of the experiment runner.</p>
<p>The experiment configuration file can be found at <a href="https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/configurations/ga_atari_config.json">https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/configurations/ga_atari_config.json</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Experiment runner implementation</h1>
                
            
            
                
<p class="p1">The experiment runner implementation creates the concurrent evaluation environment and runs the evolution loop over the population of individuals. Let's discuss the essential implementation details:</p>
<ol>
<li>We start by setting up the evaluation environment by loading the controller ANN model and creating the concurrent workers to execute the evaluation:</li>
</ol>
<pre style="padding-left: 60px">    Model = neuroevolution.models.__dict__[config['model']]<br/>    all_tstart = time.time()<br/>    def make_env(b):<br/>        return gym_tensorflow.make(game=config["game"], <br/>                                   batch_size=b)<br/>    worker = ConcurrentWorkers(make_env, Model, batch_size=64)</pre>
<ol start="2">
<li>Next, we create a table with random noise values, which will be used as random seeds, and define the function to create offspring for the next generation:</li>
</ol>
<pre style="padding-left: 60px">    noise = SharedNoiseTable()<br/>    rs = np.random.RandomState()<br/><br/>    def make_offspring():<br/>        if len(cached_parents) == 0:<br/>            return worker.model.randomize(rs, noise)<br/>        else:<br/>            assert len(cached_parents) == config['selection_threshold']<br/>            parent = cached_parents[<br/>                    rs.randint(len(cached_parents))]<br/>            theta, seeds = worker.model.mutate( parent, rs, noise, <br/>                   mutation_power=state.sample(<br/>                   state.mutation_power))<br/>            return theta, seeds</pre>
<ol start="3">
<li>After that, the main evolution loop starts. We use the previously defined function to create a population of the offspring for the current generation:</li>
</ol>
<pre style="padding-left: 60px">    tasks = [make_offspring() for _ in range(<br/>                              config['population_size'])]<br/>    for seeds, episode_reward, episode_length in \<br/>        worker.monitor_eval(tasks, max_frames=state.tslimit * 4):<br/>        results.append(Offspring(seeds, <br/>                       [episode_reward], [episode_length]))<br/><br/>    state.num_frames += sess.run(worker.steps_counter) - \<br/>                                frames_computed_so_far</pre>
<p class="p1" style="padding-left: 60px">Here, we create the work tasks for each offspring in the population and schedule each task for evaluation against the game environment.</p>
<ol start="4">
<li>When we finish with the evaluation of each individual in the population, we start the evaluation of the top individuals to select the elite:</li>
</ol>
<pre style="padding-left: 60px">    state.population = sorted(results, <br/>                  key=lambda x:x.fitness, reverse=True)<br/>    …<br/>    validation_population = state.\<br/>                   population[:config['validation_threshold']]<br/>    if state.elite is not None:<br/>        validation_population = [state.elite] + \<br/>                                    validation_population[:-1]<br/><br/>    validation_tasks = [<br/>        (worker.model.compute_weights_from_seeds(noise, <br/>        validation_population[x].seeds, cache=cached_parents), <br/>        validation_population[x].seeds) for x in range(<br/>                             config['validation_threshold'])]<br/>    _,population_validation, population_validation_len =\ <br/>        zip(*worker.monitor_eval_repeated(validation_tasks, <br/>        max_frames=state.tslimit * 4, <br/>        num_episodes=config['num_validation_episodes']))</pre>
<ol start="5">
<li>Using the evaluation results of the top 10 individuals, we select the population's elite and execute the final test runs over it to evaluate its performance:</li>
</ol>
<pre style="padding-left: 60px">    population_elite_idx = np.argmax(population_validation)<br/>    state.elite = validation_population[population_elite_idx]<br/>    elite_theta = worker.model.compute_weights_from_seeds(<br/>              noise, state.elite.seeds, cache=cached_parents)<br/>    _,population_elite_evals,population_elite_evals_timesteps=\<br/>                  worker.monitor_eval_repeated(<br/>                  [(elite_theta, state.elite.seeds)], <br/>                  max_frames=None, <br/>                  num_episodes=config[‘num_test_episodes’])[0]</pre>
<p class="p1" style="padding-left: 60px">The elite individual will be copied to the next generation as is.</p>
<ol start="6">
<li>Finally, we select the top individuals from the current population to become parents for the next generation:</li>
</ol>
<pre style="padding-left: 60px">    if config['selection_threshold'] &gt; 0:<br/>        tlogger.info("Caching parents")<br/>        new_parents = []<br/>        if state.elite in \<br/>            state.population[:config['selection_threshold']]:<br/>            new_parents.extend([<br/>                 (worker.model.compute_weights_from_seeds(<br/>                  noise, o.seeds, cache=cached_parents), o.seeds) for o in state.population[:config['selection_threshold']]])<br/>        else:<br/>            new_parents.append(<br/>                (worker.model.compute_weights_from_seeds(<br/>                 noise, state.elite.seeds, cache=cached_parents), <br/>                 state.elite.seeds))<br/>            new_parents.extend([<br/>                (worker.model.compute_weights_from_seeds(<br/>                 noise, o.seeds, cache=cached_parents), o.seeds) for o in state.population[:config[‘selection_threshold']-1]])</pre>
<p class="p1">The preceding code collects the top individuals from the population to become the parents of the next generation. Also, it appends the current elite to the list of parents if it isn't in the parent list.</p>
<p class="p1">Now, we are ready to discuss how to run the experiment.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Running the Frostbite Atari experiment</h1>
                
            
            
                
<p class="p1">Now that we have discussed all the particulars of the experiment's implementation, it is time to run the experiment. However, the first thing we need to do is create an appropriate work environment, which we'll discuss next.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Setting up the work environment </h1>
                
            
            
                
<p class="p3">The work environment for training the agent to play Atari games assumes that a large controller ANN needs to be trained in the process. We already stated that the controller ANN has more than 4 million training parameters and requires a lot of computational resources to be able to evaluate. Fortunately, modern GPU accelerators allow the execution of massive parallel computations simultaneously. This feature is convenient for our experiment because we need to evaluate each individual against the game environment multiple times during the evolution process. Without GPU acceleration, it would either take a lot of time or require a massive number of processing cores (about 720).</p>
<p class="p3">Let's discuss how to prepare the working environment:</p>
<ol class="ol1">
<li class="li3">The working environment requires a Nvidia video accelerator (such as GeForce 1080Ti) present in the system and the appropriate Nvidia CUDA SDK installed. More details about the CUDA SDK and its installation can be found at <a href="https://developer.nvidia.com/cuda-toolkit">https://developer.nvidia.com/cuda-toolkit</a>.</li>
<li class="li3">Next, we need to make sure that the CMake build tool is installed, as described at <a href="https://cmake.org">https://cmake.org</a>.</li>
<li class="li3">Now, we need to create a new Python environment using Anaconda and install all the dependencies that are used by the experiment's implementation:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ conda create -n deep_ne python=3.5</strong><br/><strong>$ conda activate deep_ne</strong><br/><strong>$ conda install -c anaconda tensorflow-gpu</strong><br/><strong>$ pip install gym</strong><br/><strong>$ pip install Pillow</strong></pre>
<p class="p1" style="padding-left: 60px">These commands create and activate a new Python 3.5 environment. Next, it installs TensorFlow, OpenAI Gym, and the Python Imaging Library as dependencies.</p>
<ol start="4">
<li>After that, you need to clone the repository with the experiment's source code:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ git clone https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python.git</strong><br/><strong>$ cd Hands-on-Neuroevolution-with-Python/Chapter10</strong></pre>
<p class="p1" style="padding-left: 60px">After executing these commands, our current working directory becomes the directory that contains the experiment source code.</p>
<ol start="5">
<li>Now, we need to build the ALE and integrate it into our experiment. We need to clone the ALE repository into the appropriate directory and build it with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cd cd gym_tensorflow/atari/</strong><br/><strong>$ git clone https://github.com/yaricom/atari-py.git</strong><br/><strong>$ cd ./atari-py &amp;&amp; make</strong></pre>
<p class="p1" style="padding-left: 60px">Now, we have a working ALE environment that's been integrated with TensorFlow. We can use it to evaluate the controller ANNs that are produced from a population of genomes against an Atari game (Frostbite, in our experiment).</p>
<ol start="6">
<li>After the ALE integration is complete, we need to build an integration between OpenAI Gym and TensorFlow that is specific to our experiment implementation:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cd ../..gym_tensorflow &amp;&amp; make</strong></pre>
<p class="p1">Now, we have a fully defined work environment and we are ready to start our experiments. Next, we'll discuss how to run the experiment.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Running the experiment</h1>
                
            
            
                
<p class="p1">With an adequately defined work environment, we are ready to start our experiment. You can start an experiment from the <kbd>Chapter10</kbd> directory by executing the following command:</p>
<pre><strong>$ python ga.py -c configurations/ga_atari_config.json -o out</strong></pre>
<p class="p1">The preceding command starts an experiment using the configuration file that was provided as the first parameter. The experiment's output will be stored in the <kbd>out</kbd> directory.</p>
<p class="p1">After completing the experiment, the console's output should look similar to the following:</p>
<pre><strong>...</strong><br/><strong>| PopulationEpRewMax                    | 3.47e+03  |</strong><br/><strong>| PopulationEpRewMean                   | 839       |</strong><br/><strong>| PopulationEpCount                     | 1e+03     |</strong><br/><strong>| PopulationTimesteps                   | 9.29e+05  |</strong><br/><strong>| NumSelectedIndividuals                | 20        |</strong><br/><strong>| TruncatedPopulationRewMean            | 3.24e+03  |</strong><br/><strong>| TruncatedPopulationValidationRewMean  | 2.36e+03  |</strong><br/><strong>| TruncatedPopulationEliteValidationRew | 3.1e+03   |</strong><br/><strong>| TruncatedPopulationEliteIndex         | 0         |</strong><br/><strong>...</strong><br/><strong>| TruncatedPopulationEliteTestRewMean   | 3.06e+03  |</strong><br/><strong>...</strong><br/><strong> Current elite: (47236580, (101514609, 0.002), (147577692, 0.002), (67106649, 0.002), (202520553, 0.002), (230555280, 0.002), (38614601, 0.002), (133511446, 0.002), (27624159, 0.002), (233455358, 0.002), (73372122, 0.002), (32459655, 0.002), (181449271, 0.002), (205743718, 0.002), (114244841, 0.002), (129962094, 0.002), (24016384, 0.002), (77767788, 0.002), (90094370, 0.002), (14090622, 0.002), (171607709, 0.002), (147408008, 0.002), (150151615, 0.002), (224734414, 0.002), (138721819, 0.002), (154735910, 0.002), (172264633, 0.002))</strong> </pre>
<p class="p1">Here, we have the statistics output after a specific generation of evolution. You can see the following results:</p>
<ul>
<li class="p1">The maximum reward score that's achieved during the evaluation of a population is 3,470 (<kbd>PopulationEpRewMax</kbd>).</li>
<li class="p1">The maximum score that's achieved among the top individuals on an additional 30 episodes of validation is 3,240 (<kbd>TruncatedPopulationRewMean</kbd>).</li>
<li class="p1">The mean score of the top individual's evaluation is 2,360 (<kbd>TruncatedPopulationValidationRewMean</kbd>).</li>
<li class="p1">The mean score of the elite individual that's received during an additional 200 test runs is 3,060 (<kbd>TruncatedPopulationEliteTestRewMean</kbd>).</li>
</ul>
<p class="p1">The achieved reward scores are pretty high compared to other training methods if we look at the results that were published at <a href="https://arxiv.org/abs/1712.06567v3">https://arxiv.org/abs/1712.06567v3</a>.</p>
<p class="p1">Also, at the end of the outputs, you can see the genome representation of the population elite. The elite genome can be used to visualize playing Frostbite by the phenotype ANN that was created from it. Next, we'll discuss how to make this visualization possible.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Frostbite visualization</h1>
                
            
            
                
<p class="p3">Now that we have the results of the game agent's training, it will be interesting to see how the solution we've found plays Frostbite in the Atari environment. To run the simulation, you need to copy the current elite genome representation from the output and paste it into the <kbd>seeds</kbd> field of the <kbd>display.py</kbd> file. After that, the simulation can be run using the following command:</p>
<pre><strong>$ python display.py</strong></pre>
<p class="p1">The preceding command uses the provided elite genome to create a phenotype ANN and uses it as the controller of the Frostbite playing agent. It will open the game window, where you can see how the controller ANN is performing. The game will continue until the game character doesn't have any lives left. The following image shows several captured game screens from the execution of <kbd>display.py</kbd> in an Ubuntu 16.04 environment:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-805 image-border" src="img/4591a38f-4a6a-40df-a39a-29179b4edfeb.png" style="width:43.92em;height:16.25em;"/></p>
<p>Frostbite screenshots, all of which have been taken from the elite genome game-playing session</p>
<p class="p1">It is pretty amazing to see how the trained controller ANN can learn the game rules solely from visual observations and is able to demonstrate such smooth gameplay.</p>
<p class="p1">Next, we'll discuss an additional visualization method that allows us to analyze the results.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Visual inspector for neuroevolution</h1>
                
            
            
                
<p class="p3">During the neuroevolution process, we are evolving a population of individuals. Each of the individuals is evaluated against the test environment (such as an Atari game) and reward scores are collected per individual for each generation of evolution. To explore the general dynamics of the neuroevolution process, we need to have a tool that can visualize the cloud of results for each individual in each generation of evolution. Also, it is interesting to see the changes in the fitness score of the elite individual to understand the progress of the evolution process.</p>
<p class="p3">To address these requirements, the researchers from Uber AI developed the VINE tool, which we'll discuss next.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Setting up the work environment</h1>
                
            
            
                
<p class="p3">To use the VINE tool, we need to install additional libraries into our virtual Python environment with the following commands:</p>
<pre><strong>$ pip install click</strong><br/><strong>$ conda install matplotlib</strong><br/><strong>$ pip install colour</strong><br/><strong>$ conda install pandas</strong></pre>
<p class="p1">These commands install all the necessary dependencies into a virtual Python environment that we have created for our experiment. Next, we'll discuss how to use the VINE tool.</p>
<p>Don't forget to activate the appropriate virtual environment with the following command before running the preceding commands: <kbd>conda activate deep_ne</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using VINE for experiment visualization</h1>
                
            
            
                
<p class="p7">Now, when we have all the dependencies installed on the Python virtual environment, we are ready to use the VINE tool. First, you need to clone it from the Git repository with the following commands:</p>
<pre><strong>$ git clone https://github.com/uber-research/deep-neuroevolution.git</strong><br/><strong>$ cd visual_inspector</strong></pre>
<p class="p1">Here, we cloned the deep neuroevolution repository into the current directory and changed the directory to the <kbd>visual_inspector</kbd> folder, where the source code of the VINE tool is present.</p>
<p>Let's discuss how VINE can be used to visualize the results of the neuroevolution experiment using the results of the Mujoco Humanoid experiment provided by the Uber AI Lab. More details about the Mujoco Humanoid experiment can be found at <a href="https://eng.uber.com/deep-neuroevolution/">https://eng.uber.com/deep-neuroevolution/</a>.</p>
<p class="p1">Now, we can run the visualization of the Mujoco Humanoid experiment results, which is supplied in the <kbd>sample_data</kbd> folder, using the following command:</p>
<pre><strong>$ python -m main_mujoco 90 99 sample_data/mujoco/final_xy_bc/</strong></pre>
<p class="p1">The preceding command uses the same data that was supplied by Uber AI Lab from their experiment for training humanoid locomotion and displays the following graphs:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-806 image-border" src="img/d4769f1a-6ceb-4ddd-89d4-ad8f3d360e3e.png" style="width:68.92em;height:27.75em;"/></p>
<p>The VINE tool's visualization of Humanoid locomotion results</p>
<p class="p1">On the left-hand side of the graph, you can see the cloud of results for each individual in the population, starting from generation <kbd>90</kbd> and ending at generation <kbd>99</kbd>. On the right-hand side of the graph, you can see the fitness score of the population's elite per generation. In the right-hand graph, you can see that the evolutionary process demonstrates the positive dynamics from generation to generation as the fitness score of the elite is increasing.</p>
<p class="p1">Each point on the left-hand side graph demonstrates the behavioral characterization points for each individual in a population. The behavioral characterization for the Humanoid locomotion task is the final position of the Humanoid at the end of the trajectory. The farther it is from the origin coordinates (<kbd>0,0</kbd>), the higher the fitness score of the individual. You can see that, with the progress of evolution, the results cloud is moving away from the origin coordinates. This movement of the results cloud is also a signal of the positive learning dynamics because each individual was able to stay balanced for a more extended period.</p>
<p>For more details about the Mujoco Humanoid locomotion experiment, please refer to the article at <a href="https://eng.uber.com/deep-neuroevolution/">https://eng.uber.com/deep-neuroevolution/</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Exercises</h1>
                
            
            
                
<ol class="ol1">
<li class="li1">Try to increase the <kbd>population_size</kbd> parameter in the experiment and see what happens.</li>
<li class="li1">Try to create the experiment results, which can be visualized using VINE. You can use the  <kbd>master_extract_parent_ga</kbd> and <kbd>master_extract_cloud_ga</kbd> helper functions in the <kbd>ga.py</kbd> script to do this.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p class="p2">In this chapter, we discussed how neuroevolution can be used to train large ANNs with more than 4 million trainable parameters. You learned how to apply this learning method to create successful agents that are able to play classic Atari games by learning the game rules solely from observing the game screens. By completing the Atari game-playing experiment that was described in this chapter, you have learned about CNNs and how they can be used to map high-dimensional inputs, such as game screen observations, into the appropriate game actions. You now have a solid understanding of how CNNs can be used for value-function approximations in the deep RL method, which is guided by the deep neuroevolution algorithm.</p>
<p class="p2">With the knowledge that you've acquired from this chapter, you will be able to apply deep neuroevolution methods in domains with high-dimensional input data, such as inputs that have been acquired from cameras or other image sources.</p>
<p class="p2">In the next chapter, we'll summarize what we have covered in this book and provide some hints about where you can continue your self-education.</p>


            

            
        
    </body></html>