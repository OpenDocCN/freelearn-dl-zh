["```py\ngit clone https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition.git\n```", "```py\npoetry install\npoetry shell\n```", "```py\njupyter notebook\n```", "```py\npip install -r requirements.txt\n```", "```py\nconda create --name <env_name> --file requirements.txt\n```", "```py\n    %run -i \"../util/file_utils.ipynb\"\n    ```", "```py\n    sherlock_holmes_part_of_text = read_text_file(\"../data/sherlock_holmes_1.txt\")\n    ```", "```py\n    def read_text_file(filename):\n        file = open(filename, \"r\", encoding=\"utf-8\")\n        return file.read()\n    ```", "```py\n    print(sherlock_holmes_part_of_text)\n    ```", "```py\n    To Sherlock Holmes she is always _the_ woman. I have seldom heard him\n    mention her under any other name. In his eyes she eclipses and\n    predominates the whole of her sex…\n    ```", "```py\n    import nltk\n    ```", "```py\n    nltk.download('punkt')\n    ```", "```py\n    tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n    ```", "```py\n    sentences_nltk = tokenizer.tokenize(\n        sherlock_holmes_part_of_text)\n    ```", "```py\n    print(sentences_nltk)\n    ```", "```py\n    ['To Sherlock Holmes she is always _the_ woman.', 'I have seldom heard him\\nmention her under any other name.', 'In his eyes she eclipses and\\npredominates the whole of her sex.', 'It was not that he felt any emotion\\nakin to love for Irene Adler.', 'All emotions, and that one particularly,\\nwere abhorrent to his cold, precise but admirably balanced mind.', 'He\\nwas, I take it, the most perfect reasoning and observing machine that\\nthe world has seen, but as a lover he would have placed himself in a\\nfalse position.', 'He never spoke of the softer passions, save with a gibe\\nand a sneer.', 'They were admirable things for the observer—excellent for\\ndrawing the veil from men's motives and actions.', 'But for the trained\\nreasoner to admit such intrusions into his own delicate and finely\\nadjusted temperament was to introduce a distracting factor which might\\nthrow a doubt upon all his mental results.', 'Grit in a sensitive\\ninstrument, or a crack in one of his own high-power lenses, would not\\nbe more disturbing than a strong emotion in a nature such as his.', 'And\\nyet there was but one woman to him, and that woman was the late Irene\\nAdler, of dubious and questionable memory.']\n    ```", "```py\n    print(len(sentences_nltk))\n    ```", "```py\n    11\n    ```", "```py\n    import spacy\n    ```", "```py\n    !python -m spacy download en_core_web_sm\n    ```", "```py\n    nlp = spacy.load(\"en_core_web_sm\")\n    ```", "```py\n    doc = nlp(sherlock_holmes_part_of_text)\n    ```", "```py\n    sentences_spacy = [sentence.text for sentence in doc.sents]\n    print(sentences_spacy)\n    print(len(sentences_spacy))\n    ```", "```py\n    ['To Sherlock Holmes she is always _the_ woman.', 'I have seldom heard him\\nmention her under any other name.', 'In his eyes she eclipses and\\npredominates the whole of her sex.', 'It was not that he felt any emotion\\nakin to love for Irene Adler.', 'All emotions, and that one particularly,\\nwere abhorrent to his cold, precise but admirably balanced mind.', 'He\\nwas, I take it, the most perfect reasoning and observing machine that\\nthe world has seen, but as a lover he would have placed himself in a\\nfalse position.', 'He never spoke of the softer passions, save with a gibe\\nand a sneer.', 'They were admirable things for the observer—excellent for\\ndrawing the veil from men's motives and actions.', 'But for the trained\\nreasoner to admit such intrusions into his own delicate and finely\\nadjusted temperament was to introduce a distracting factor which might\\nthrow a doubt upon all his mental results.', 'Grit in a sensitive\\ninstrument, or a crack in one of his own high-power lenses, would not\\nbe more disturbing than a strong emotion in a nature such as his.', 'And\\nyet there was but one woman to him, and that woman was the late Irene\\nAdler, of dubious and questionable memory.']\n    11\n    ```", "```py\nimport time\ndef split_into_sentences_nltk(text):\n    sentences = tokenizer.tokenize(text)\n    return sentences\ndef split_into_sentences_spacy(text):\n    doc = nlp(text)\n    sentences = [sentence.text for sentence in doc.sents]\n    return sentences\nstart = time.time()\nsplit_into_sentences_nltk(sherlock_holmes_part_of_text)\nprint(f\"NLTK: {time.time() - start} s\")\nstart = time.time()\nsplit_into_sentences_spacy(sherlock_holmes_part_of_text)\nprint(f\"spaCy: {time.time() - start} s\")\n```", "```py\ntokenizer = nltk.data.load(\"tokenizers/punkt/spanish.pickle\")\n```", "```py\npython -m spacy download es_core_news_sm\n```", "```py\nnlp = spacy.load(\"es_core_news_sm\")\n```", "```py\n    %run -i \"../util/file_utils.ipynb\"\n    ```", "```py\n    sherlock_holmes_part_of_text = read_text_file(\"../data/sherlock_holmes_1.txt\")\n    print(sherlock_holmes_part_of_text)\n    ```", "```py\n    To Sherlock Holmes she is always _the_ woman. I have seldom heard him\n    mention her under any other name. In his eyes she eclipses and\n    predominates the whole of her sex... [Output truncated]\n    ```", "```py\n    import nltk\n    ```", "```py\n    words = nltk.tokenize.word_tokenize(\n        sherlock_holmes_part_of_text)\n    print(words)\n    print(len(words))\n    ```", "```py\n    ['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', '_the_', 'woman', '.', 'I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', '.', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole', 'of', 'her', 'sex', '.', 'It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', 'akin', 'to', 'love', 'for', 'Irene', 'Adler', '.', 'All', 'emotions', ',', 'and', 'that', 'one', 'particularly', ',', 'were', 'abhorrent', 'to', 'his', 'cold', ',', 'precise', 'but', 'admirably', 'balanced', 'mind', '.', 'He', 'was', ',', 'I', 'take', 'it', ',', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', 'the', 'world', 'has', 'seen', ',', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', 'false', 'position', '.', 'He', 'never', 'spoke', 'of', 'the', 'softer', 'passions', ',', 'save', 'with', 'a', 'gibe', 'and', 'a', 'sneer', '.', 'They', 'were', 'admirable', 'things', 'for', 'the', 'observer—excellent', 'for', 'drawing', 'the', 'veil', 'from', 'men', ''', 's', 'motives', 'and', 'actions', '.', 'But', 'for', 'the', 'trained', 'reasoner', 'to', 'admit', 'such', 'intrusions', 'into', 'his', 'own', 'delicate', 'and', 'finely', 'adjusted', 'temperament', 'was', 'to', 'introduce', 'a', 'distracting', 'factor', 'which', 'might', 'throw', 'a', 'doubt', 'upon', 'all', 'his', 'mental', 'results', '.', 'Grit', 'in', 'a', 'sensitive', 'instrument', ',', 'or', 'a', 'crack', 'in', 'one', 'of', 'his', 'own', 'high-power', 'lenses', ',', 'would', 'not', 'be', 'more', 'disturbing', 'than', 'a', 'strong', 'emotion', 'in', 'a', 'nature', 'such', 'as', 'his', '.', 'And', 'yet', 'there', 'was', 'but', 'one', 'woman', 'to', 'him', ',', 'and', 'that', 'woman', 'was', 'the', 'late', 'Irene', 'Adler', ',', 'of', 'dubious', 'and', 'questionable', 'memory', '.']\n    230\n    ```", "```py\n    from nltk.tokenize import MWETokenizer\n    ```", "```py\n    tokenizer = MWETokenizer([('dim', 'sum', 'dinner')])\n    ```", "```py\n    tokenizer.add_mwe(('best', 'dim', 'sum'))\n    ```", "```py\n    tokens = tokenizer.tokenize('Last night I went for dinner in an Italian restaurant. The pasta was delicious.'.split())\n    print(tokens)\n    ```", "```py\n    ['Last', 'night', 'I', 'went', 'for', 'dinner', 'in', 'an', 'Italian', 'restaurant.', 'The', 'pasta', 'was', 'delicious.']\n    ```", "```py\n    tokens = tokenizer.tokenize('I went out to a dim sum dinner last night. This restaurant has the best dim sum in town.'.split())\n    print(tokens)\n    ```", "```py\n    ['I', 'went', 'out', 'to', 'a', 'dim_sum_dinner', 'last', 'night.', 'This', 'restaurant', 'has', 'the_best_dim_sum', 'in', 'town.']\n    ```", "```py\n    import spacy\n    ```", "```py\n    !python -m spacy download en_core_web_sm\n    ```", "```py\n    nlp = spacy.load(\"en_core_web_sm\")\n    ```", "```py\n    doc = nlp(sherlock_holmes_part_of_text)\n    words = [token.text for token in doc]\n    ```", "```py\n    print(words)\n    print(len(words))\n    ```", "```py\n    ['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', '_', 'the', '_', 'woman', '.', 'I', 'have', 'seldom', 'heard', 'him', '\\n', 'mention', 'her', 'under', 'any', 'other', 'name', '.', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', '\\n', 'predominates', 'the', 'whole', 'of', 'her', 'sex', '.', 'It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', '\\n', 'akin', 'to', 'love', 'for', 'Irene', 'Adler', '.', 'All', 'emotions', ',', 'and', 'that', 'one', 'particularly', ',', '\\n', 'were', 'abhorrent', 'to', 'his', 'cold', ',', 'precise', 'but', 'admirably', 'balanced', 'mind', '.', 'He', '\\n', 'was', ',', 'I', 'take', 'it', ',', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', '\\n', 'the', 'world', 'has', 'seen', ',', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', '\\n', 'false', 'position', '.', 'He', 'never', 'spoke', 'of', 'the', 'softer', 'passions', ',', 'save', 'with', 'a', 'gibe', '\\n', 'and', 'a', 'sneer', '.', 'They', 'were', 'admirable', 'things', 'for', 'the', 'observer', '—', 'excellent', 'for', '\\n', 'drawing', 'the', 'veil', 'from', 'men', ''s', 'motives', 'and', 'actions', '.', 'But', 'for', 'the', 'trained', '\\n', 'reasoner', 'to', 'admit', 'such', 'intrusions', 'into', 'his', 'own', 'delicate', 'and', 'finely', '\\n', 'adjusted', 'temperament', 'was', 'to', 'introduce', 'a', 'distracting', 'factor', 'which', 'might', '\\n', 'throw', 'a', 'doubt', 'upon', 'all', 'his', 'mental', 'results', '.', 'Grit', 'in', 'a', 'sensitive', '\\n', 'instrument', ',', 'or', 'a', 'crack', 'in', 'one', 'of', 'his', 'own', 'high', '-', 'power', 'lenses', ',', 'would', 'not', '\\n', 'be', 'more', 'disturbing', 'than', 'a', 'strong', 'emotion', 'in', 'a', 'nature', 'such', 'as', 'his', '.', 'And', '\\n', 'yet', 'there', 'was', 'but', 'one', 'woman', 'to', 'him', ',', 'and', 'that', 'woman', 'was', 'the', 'late', 'Irene', '\\n', 'Adler', ',', 'of', 'dubious', 'and', 'questionable', 'memory', '.']\n    251\n    ```", "```py\nprint(set(words_spacy)-set(words_nltk))\n```", "```py\n{'high', 'power', 'observer', '-', '_', '—', 'excellent', ''s', '\\n'}\n```", "```py\npython -m spacy download es_core_news_sm\n```", "```py\nnlp = spacy.load(\"es_core_news_sm\")\n```", "```py\n    %run -i \"../util/file_utils.ipynb\"\n    %run -i \"../util/lang_utils.ipynb\"\n    ```", "```py\n    def pos_tag_spacy(text, model):\n        doc = model(text)\n        words = [token.text for token in doc]\n        pos = [token.pos_ for token in doc]\n        return list(zip(words, pos))\n    ```", "```py\n    text = read_text_file(\"../data/sherlock_holmes_1.txt\")\n    ```", "```py\n    words_with_pos = pos_tag_spacy(text, small_model)\n    ```", "```py\n    print(words_with_pos)\n    ```", "```py\n    [('To', 'ADP'),\n     ('Sherlock', 'PROPN'),\n     ('Holmes', 'PROPN'),\n     ('she', 'PRON'),\n     ('is', 'AUX'),\n     ('always', 'ADV'),\n     ('_', 'PUNCT'),\n     ('the', 'DET'),\n     ('_', 'PROPN'),\n     ('woman', 'NOUN'),\n     ('.', 'PUNCT'),\n     ('I', 'PRON'),\n     ('have', 'AUX'),\n     ('seldom', 'ADV'),\n     ('heard', 'VERB'),\n     ('him', 'PRON'),\n     ('\\n', 'SPACE'),\n     ('mention', 'VERB'),\n     ('her', 'PRON'),\n     ('under', 'ADP'),\n     ('any', 'DET'),\n     ('other', 'ADJ'),\n     ('name', 'NOUN'),\n     ('.', 'PUNCT'),…\n    ```", "```py\n    def pos_tag_nltk(text):\n        words = word_tokenize_nltk(text)\n        words_with_pos = nltk.pos_tag(words)\n        return words_with_pos\n    ```", "```py\n    words_with_pos = pos_tag_nltk(text)\n    ```", "```py\n    print(words_with_pos)\n    ```", "```py\n    [('To', 'TO'),\n     ('Sherlock', 'NNP'),\n     ('Holmes', 'NNP'),\n     ('she', 'PRP'),\n     ('is', 'VBZ'),\n     ('always', 'RB'),\n     ('_the_', 'JJ'),\n     ('woman', 'NN'),\n     ('.', '.'),\n     ('I', 'PRP'),\n     ('have', 'VBP'),\n     ('seldom', 'VBN'),\n     ('heard', 'RB'),\n     ('him', 'PRP'),\n     ('mention', 'VB'),\n     ('her', 'PRP'),\n     ('under', 'IN'),\n     ('any', 'DT'),\n     ('other', 'JJ'),\n     ('name', 'NN'),\n     ('.', '.'),…\n    ```", "```py\npython\n>>> import nltk\n>>> nltk.download('tagsets')\n>>> nltk.help.upenn_tagset()\n```", "```py\n    from openai import OpenAI\n    client = OpenAI(api_key=OPEN_AI_KEY)\n    ```", "```py\n    prompt=\"\"\"Decide what the part of speech tags are for a sentence.\n    Preserve original capitalization.\n    Return the list in the format of a python tuple: (word, part of speech).\n    Sentence: In his eyes she eclipses and predominates the whole of her sex.\"\"\"\n    ```", "```py\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        temperature=0,\n        max_tokens=256,\n        top_p=1.0,\n        frequency_penalty=0,\n        presence_penalty=0,\n        messages=[\n            {\"role\": \"system\", \n             \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n    )\n    ```", "```py\n    print(response)\n    ```", "```py\n    ChatCompletion(id='chatcmpl-9hCq34UAzMiNiqNGopt2U8ZmZM5po', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Here are the part of speech tags for the sentence \"In his eyes she eclipses and predominates the whole of her sex\" in the format of a Python tuple:\\n\\n[(\\'In\\', \\'IN\\'), (\\'his\\', \\'PRP$\\'), (\\'eyes\\', \\'NNS\\'), (\\'she\\', \\'PRP\\'), (\\'eclipses\\', \\'VBZ\\'), (\\'and\\', \\'CC\\'), (\\'predominates\\', \\'VBZ\\'), (\\'the\\', \\'DT\\'), (\\'whole\\', \\'JJ\\'), (\\'of\\', \\'IN\\'), (\\'her\\', \\'PRP$\\'), (\\'sex\\', \\'NN\\')]', role='assistant', function_call=None, tool_calls=None))], created=1720084483, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=120, prompt_tokens=74, total_tokens=194))\n    ```", "```py\n    print(response.choices[0].message.content)\n    ```", "```py\n    Here are the part of speech tags for the sentence \"In his eyes she eclipses and predominates the whole of her sex\" in the format of a Python tuple:\n    [('In', 'IN'), ('his', 'PRP$'), ('eyes', 'NNS'), ('she', 'PRP'), ('eclipses', 'VBZ'), ('and', 'CC'), ('predominates', 'VBZ'), ('the', 'DT'), ('whole', 'JJ'), ('of', 'IN'), ('her', 'PRP$'), ('sex', 'NN')]\n    ```", "```py\n    from ast import literal_eval\n    def pos_tag_gpt(text, client):\n        prompt = f\"\"\"Decide what the part of speech tags are for a sentence.\n        Preserve original capitalization.\n        Return the list in the format of a python tuple: (word, part of speech).\n        Do not include any other explanations.\n        Sentence: {text}.\"\"\"\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            temperature=0,\n            max_tokens=256,\n            top_p=1.0,\n            frequency_penalty=0,\n            presence_penalty=0,\n            messages=[\n                {\"role\": \"system\", \n                 \"content\": \"You are a helpful assistant.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n        )\n        result = response.choices[0].message.content\n        result = result.replace(\"\\n\", \"\")\n        result = list(literal_eval(result))\n        return result\n    ```", "```py\n    start = time.time()\n    first_sentence = \"In his eyes she eclipses and predominates the whole of her sex.\"\n    words_with_pos = pos_tag_gpt(first_sentence, OPEN_AI_KEY)\n    print(words_with_pos)\n    print(f\"GPT: {time.time() - start} s\")\n    ```", "```py\n    [('In', 'IN'), ('his', 'PRP$'), ('eyes', 'NNS'), ('she', 'PRP'), ('eclipses', 'VBZ'), ('and', 'CC'), ('predominates', 'VBZ'), ('the', 'DT'), ('whole', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('sex', 'NN'), ('.', '.')]\n    GPT: 2.4942469596862793 s\n    ```", "```py\n    words_with_pos_nltk = pos_tag_nltk(first_sentence)\n    print(words_with_pos == words_with_pos_nltk)\n    ```", "```py\n    False\n    ```", "```py\nnlp = spacy.load(\"es_core_news_sm\")\n```", "```py\n    %run -i \"../util/file_utils.ipynb\"\n    %run -i \"../util/lang_utils.ipynb\"\n    ```", "```py\n    words = [\"leaf\", \"leaves\", \"booking\", \"writing\", \"completed\", \"stemming\"]\n    ```", "```py\n    docs = [small_model(word) for word in words]\n    ```", "```py\n    for doc in docs:\n        for token in doc:\n            print(token, token.lemma_)\n    ```", "```py\n    leaf leaf\n    leaves leave\n    booking book\n    writing write\n    completed complete\n    stemming stem\n    ```", "```py\n    Text = read_text_file(../data/sherlock_holmes_1.txt\")\n    doc = small_model(text)\n    for token in doc:\n        print(token, token.lemma_)\n    ```", "```py\n    To to\n    Sherlock Sherlock\n    Holmes Holmes\n    she she\n    is be\n    always always\n    _ _\n    the the\n    _ _\n    woman woman\n    . ….\n    ```", "```py\n    lemmatizer = None\n    for name, proc in small_model.pipeline:\n        if name == \"lemmatizer\":\n            lemmatizer = proc\n    ```", "```py\n    for token in doc:\n        print(f\"{token} is in its base form: \n            {lemmatizer.is_base_form(token)}\")\n    ```", "```py\n    To is in its base form: False\n    Sherlock is in its base form: False\n    Holmes is in its base form: False\n    she is in its base form: False\n    is is in its base form: False\n    always is in its base form: False\n    _ is in its base form: False\n    the is in its base form: False\n    _ is in its base form: False\n    woman is in its base form: True\n    . is in its base form: False…\n    ```", "```py\n    %run -i \"../util/file_utils.ipynb\"\n    %run -i \"../util/lang_utils.ipynb\"\n    ```", "```py\n    from nltk.corpus import stopwords\n    ```", "```py\n    nltk.download('stopwords')\n    ```", "```py\n    print(stopwords.words('english'))\n    ```", "```py\n    ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n    ```", "```py\n    text = read_text_file(\"../data/sherlock_holmes_1.txt\")\n    ```", "```py\n    words = word_tokenize_nltk(text)\n    print(len(words))\n    ```", "```py\n    230\n    ```", "```py\n    words = [word for word in words if word not in stopwords.words(\"english\")]\n    print(len(words))\n    ```", "```py\n    105\n    ```", "```py\n    stopwords = small_model.Defaults.stop_words\n    ```", "```py\n    words = word_tokenize_nltk(text)\n    print(len(words))\n    ```", "```py\n    230\n    ```", "```py\n    words = [word for word in words if word.lower() not in stopwords]\n    print(len(words))\n    ```", "```py\n    106\n    ```", "```py\n    print(len(stopwords))\n    stopwords.add(\"new\")\n    print(len(stopwords))\n    ```", "```py\n    327\n    328\n    ```", "```py\n    print(len(stopwords))\n    stopwords.remove(\"new\")\n    print(len(stopwords))\n    ```", "```py\n    328\n    327\n    ```", "```py\n    from nltk.probability import FreqDist\n    ```", "```py\n    def compile_stopwords_list_frequency(text, cut_off=0.02):\n        words = word_tokenize_nltk(text)\n        freq_dist = FreqDist(word.lower() for word in words)\n        words_with_frequencies = [\n            (word, freq_dist[word]) for word in freq_dist.keys()]\n        sorted_words = sorted(words_with_frequencies, \n            key=lambda tup: tup[1])\n        stopwords = []\n        if (type(cut_off) is int):\n            # First option: use a frequency cutoff\n            stopwords = [tuple[0] for tuple in sorted_words \n                if tuple[1] > cut_off]\n        elif (type(cut_off) is float):\n            # Second option: use a percentage of the words\n            length_cutoff = int(cut_off*len(sorted_words))\n            stopwords = [tuple[0] for tuple in \n                sorted_words[-length_cutoff:]]\n        else:\n            raise TypeError(\"The cut off needs to be either a float (percentage) or an int (frequency cut off)\")\n        return stopwords\n    ```", "```py\n    text = read_text_file(\"../data/sherlock_holmes.txt\")\n    stopwords = compile_stopwords_list_frequency(text)\n    print(stopwords)\n    print(len(stopwords))\n    ```", "```py\n    ['make', 'myself', 'night', 'until', 'street', 'few', 'why', 'thought', 'take', 'friend', 'lady', 'side', 'small', 'still', 'these', 'find', 'st.', 'every', 'watson', 'too', 'round', 'young', 'father', 'left', 'day', 'yet', 'first', 'once', 'took', 'its', 'eyes', 'long', 'miss', 'through', 'asked', 'most', 'saw', 'oh', 'morning', 'right', 'last', 'like', 'say', 'tell', 't', 'sherlock', 'their', 'go', 'own', 'after', 'away', 'never', 'good', 'nothing', 'case', 'however', 'quite', 'found', 'made', 'house', 'such', 'heard', 'way', 'yes', 'hand', 'much', 'matter', 'where', 'might', 'just', 'room', 'any', 'face', 'here', 'back', 'door', 'how', 'them', 'two', 'other', 'came', 'time', 'did', 'than', 'come', 'before', 'must', 'only', 'know', 'about', 'shall', 'think', 'more', 'over', 'us', 'well', 'am', 'or', 'may', 'they', ';', 'our', 'should', 'now', 'see', 'down', 'can', 'some', 'if', 'will', 'mr.', 'little', 'who', 'into', 'do', 'has', 'could', 'up', 'man', 'out', 'when', 'would', 'an', 'are', 'by', '!', 'were', 's', 'then', 'one', 'all', 'on', 'no', 'what', 'been', 'your', 'very', 'him', 'her', 'she', 'so', ''', 'holmes', 'upon', 'this', 'said', 'from', 'there', 'we', 'me', 'be', 'but', 'not', 'for', '?', 'at', 'which', 'with', 'had', 'as', 'have', 'my', ''', 'is', 'his', 'was', 'you', 'he', 'it', 'that', 'in', '\"', 'a', 'of', 'to', '\"', 'and', 'i', '.', 'the', ',']\n    181\n    ```", "```py\n    text = read_text_file(\"../data/sherlock_holmes.txt\")\n    stopwords = compile_stopwords_list_frequency(text, cut_off=0.05)\n    print(len(stopwords))\n    ```", "```py\n    452\n    ```", "```py\n    stopwords = compile_stopwords_list_frequency(text, cut_off=100)\n    print(stopwords)\n    print(len(stopwords))\n    ```", "```py\n    ['away', 'never', 'good', 'nothing', 'case', 'however', 'quite', 'found', 'made', 'house', 'such', 'heard', 'way', 'yes', 'hand', 'much', 'matter', 'where', 'might', 'just', 'room', 'any', 'face', 'here', 'back', 'door', 'how', 'them', 'two', 'other', 'came', 'time', 'did', 'than', 'come', 'before', 'must', 'only', 'know', 'about', 'shall', 'think', 'more', 'over', 'us', 'well', 'am', 'or', 'may', 'they', ';', 'our', 'should', 'now', 'see', 'down', 'can', 'some', 'if', 'will', 'mr.', 'little', 'who', 'into', 'do', 'has', 'could', 'up', 'man', 'out', 'when', 'would', 'an', 'are', 'by', '!', 'were', 's', 'then', 'one', 'all', 'on', 'no', 'what', 'been', 'your', 'very', 'him', 'her', 'she', 'so', ''', 'holmes', 'upon', 'this', 'said', 'from', 'there', 'we', 'me', 'be', 'but', 'not', 'for', '?', 'at', 'which', 'with', 'had', 'as', 'have', 'my', ''', 'is', 'his', 'was', 'you', 'he', 'it', 'that', 'in', '\"', 'a', 'of', 'to', '\"', 'and', 'i', '.', 'the', ',']\n    131\n    ```"]