<html><head></head><body><div><div><div><h1 id="_idParaDest-357" class="chapter-number"><a id="_idTextAnchor469"/>30</h1>
			<h1 id="_idParaDest-358"><a id="_idTextAnchor470"/>Agentic Patterns</h1>
			<p>In this final chapter, we’ll explore patterns for creating more autonomous and goal-directed AI agents using LLMs. You’ll learn about goal-setting and planning in LLM-based agents, implementing memory and state management, and strategies for decision-making and action selection. We’ll cover techniques for learning and adaptation in agentic LLM systems and discuss the ethical considerations and safety measures necessary when developing such systems.</p>
			<p>By the end of this chapter, you’ll be able to design and implement sophisticated AI agents powered by LLMs, opening up new possibilities for autonomous AI systems.</p>
			<p>In this chapter, we will be covering the following topics:</p>
			<ul>
				<li>Introduction to agentic AI systems based on LLMs</li>
				<li>Goal-setting and planning in LLM-based agents</li>
				<li>Implementing memory and state management for LLM agents</li>
				<li>Decision-making and action selection in LLM-based agents</li>
				<li>Learning and adaptation in agentic LLM systems</li>
				<li>Ethical considerations and safety in LLM-based agentic AI</li>
				<li>Future prospects of agentic AI using LLMs</li>
			</ul>
			<h1 id="_idParaDest-359"><a id="_idTextAnchor471"/>Introduction to agentic AI systems based on LLMs</h1>
			<p>Agentic AI systems<a id="_idIndexMarker1386"/> using LLMs are designed to operate<a id="_idIndexMarker1387"/> autonomously, make decisions, and take actions to achieve specified goals. These systems combine the powerful language understanding and generation capabilities of LLMs with goal-oriented behavior and environmental interaction.</p>
			<p>Let’s start by implementing a basic structure for an LLM-based agent:</p>
			<pre class="source-code">
from typing import List, Dict, Any
import random
class LLMAgent:
    def __init__(self, llm, action_space: List[str]):
        self.llm = llm
        self.action_space = action_space
        self.memory = []
        self.current_goal = None</pre>			<p>Here, the <code>LLMAgent</code> class is initialized with an LLM (<code>llm</code>) and a list of possible actions (<code>action_space</code>). It also maintains a memory of observations and a <code>current_goal</code>, which will be used to guide the agent’s actions.</p>
			<pre class="source-code">
    def set_goal(self, goal: str):
        self.current_goal = goal
    def perceive(self, observation: str):
        self.memory.append(observation)</pre>			<p>Here, we define <a id="_idIndexMarker1388"/>two <a id="_idIndexMarker1389"/>methods: <code>set_goal</code>, which allows the agent to set its goal, and <code>perceive</code>, which enables the agent to take in observations from the environment and store them in its memory.</p>
			<p>Next, we use the <code>think</code> method to generate a thorough process based on the agent’s goal and recent observations:</p>
			<pre class="source-code">
    def think(self) -&gt; str:
        context = f"Goal: {self.current_goal}\n"
        context += "Recent observations:\n"
        context += "\n".join(self.memory[-5:])  # Last 5 observations
        context += "\nThink about the current situation and the goal. What should be done next?"
        return self.llm.generate(context)</pre>			<p>The agent asks the language model for advice on the next step by providing a context string, which includes the current goal and the last five observations.</p>
			<p>Once the agent has a thought, it must decide on the next action. The <code>decide</code> method uses the thought to generate a context, asking the LLM to pick the best action from the available options:</p>
			<pre class="source-code">
    def decide(self, thought: str) -&gt; str:
        context = f"Thought: {thought}\n"
        context += "Based on this thought, choose the most appropriate action from the following:\n"
        context += ", ".join(self.action_space)
        context += "\nChosen action:"
        return self.llm.generate(context)</pre>			<p>Then, the <code>act</code> method <a id="_idIndexMarker1390"/>simulates taking an action by randomly selecting an <a id="_idIndexMarker1391"/>outcome (success, failure, or an unexpected result). In real scenarios, this would involve interacting with the environment:</p>
			<pre class="source-code">
    def act(self, action: str) -&gt; str:
        outcomes = [
            f"Action '{action}' was successful.",
            f"Action '{action}' failed.",
            f"Action '{action}' had an unexpected outcome."
        ]
        return random.choice(outcomes)</pre>			<p>Finally, the <code>run_step</code> method orchestrates the entire process of thinking, deciding, acting, and perceiving the outcome, completing one cycle of interaction with the environment:</p>
			<pre class="source-code">
    def run_step(self):
        thought = self.think()
        action = self.decide(thought)
        outcome = self.act(action)
        self.perceive(outcome)
        return thought, action, outcome</pre>			<p>Now that we understand the fundamental principles, let’s translate these concepts into code.</p>
			<p>Let’s implement a<a id="_idIndexMarker1392"/> basic <a id="_idIndexMarker1393"/>LLM-based agent, establishing the core structure for autonomous operation. The agent is initialized with a hypothetical language model (<code>llm</code>) and a set of actions. It sets a goal and perceives the environment to begin interacting with it:</p>
			<pre class="source-code">
# Example usage
llm = SomeLLMModel()  # Replace with your actual LLM
action_space = ["move", "grab", "drop", "use", "talk"]
agent = LLMAgent(llm, action_space)
agent.set_goal("Find the key and unlock the door")
agent.perceive("You are in a room with a table and a chair. There's a drawer in the table.")</pre>			<p>In the following <code>for</code> loop, the agent runs for five steps, and each thought, action, and outcome is printed to show how the agent interacts with its environment over time:</p>
			<pre class="source-code">
for _ in range(5):  # Run for 5 steps
    thought, action, outcome = agent.run_step()
    print(f"Thought: {thought}")
    print(f"Action: {action}")
    print(f"Outcome: {outcome}")
    print()</pre>			<p>Having established the fundamentals of agent behavior, let’s explore more advanced capabilities. The <a id="_idIndexMarker1394"/>following <a id="_idIndexMarker1395"/>section focuses on goal-setting and planning, enabling the agent to proactively work toward complex objectives.</p>
			<h1 id="_idParaDest-360"><a id="_idTextAnchor472"/>Goal-setting and planning in LLM-based agents</h1>
			<p>To enhance our agent <a id="_idIndexMarker1396"/>with more sophisticated goal-setting and planning capabilities, let’s implement a hierarchical goal structure and a planning mechanism.</p>
			<p>First, we define a <code>HierarchicalGoal</code> class; this class allows the agent to break down large tasks into smaller subgoals:</p>
			<pre class="source-code">
class HierarchicalGoal:
    def __init__(
        self, description: str,
        subgoals: List['HierarchicalGoal'] = None
    ):
        self.description = description
        self.subgoals = subgoals or []
        self.completed = False
    def add_subgoal(self, subgoal: 'HierarchicalGoal'):
        self.subgoals.append(subgoal)
    def mark_completed(self):
        self.completed = True</pre>			<p>The agent can complete these subgoals step by step, marking each as completed when done.</p>
			<p>Next, we have a <code>PlanningAgent</code> class, which inherits from <code>LLMAgent</code> but adds the ability to handle hierarchical goals. It stores goals in a stack, working through subgoals as they are completed:</p>
			<pre class="source-code">
class PlanningAgent(LLMAgent):
    def __init__(self, llm, action_space: List[str]):
        super().__init__(llm, action_space)
        self.goal_stack = []
        self.current_plan = []
    def set_hierarchical_goal(self, goal: HierarchicalGoal):
        self.goal_stack = [goal]</pre>			<p>The <code>think</code> method now <a id="_idIndexMarker1397"/>also includes planning. If no current plan exists, it asks the LLM to generate a step-by-step plan to achieve the current goal:</p>
			<pre class="source-code">
    def think(self) -&gt; str:
        if not self.current_plan:
            self.create_plan()
        context = f"Current goal: {self.goal_stack[-1].description}\n"
        context += "Current plan:\n"
        context += "\n".join(self.current_plan)
        context += "\nRecent observations:\n"
        context += "\n".join(self.memory[-5:])
        context += "\nThink about the current situation, goal, and plan. What should be done next?"
        return self.llm.generate(context)</pre>			<p>Then, the <code>create_plan</code> method generates a plan by prompting the LLM with the current goal and the list of actions. The generated plan is split into individual steps:</p>
			<pre class="source-code">
    def create_plan(self):
        context = f"Goal: {self.goal_stack[-1].description}\n"
        context += "Create a step-by-step plan to achieve this goal. Each step should be an action from the following list:\n"
        context += ", ".join(self.action_space)
        context += "\nPlan:"
        plan_text = self.llm.generate(context)
        self.current_plan = [
            step.strip() for step in plan_text.split("\n")
            if step.strip()
        ]</pre>			<p>The <code>update_goals</code> method<a id="_idIndexMarker1398"/> checks whether the current goal is complete. If it is, it moves on to the next goal or subgoal and resets the plan accordingly:</p>
			<pre class="source-code">
    def update_goals(self):
        current_goal = self.goal_stack[-1]
        if current_goal.completed:
            self.goal_stack.pop()
            if self.goal_stack:
                self.current_plan = []  # Reset plan for the next goal
        elif current_goal.subgoals:
            next_subgoal = next(
                (
                    sg for sg in current_goal.subgoals
                    if not sg.completed
                ),
                None
            )
            if next_subgoal:
                self.goal_stack.append(next_subgoal)
                self.current_plan = []  # Reset plan for the new subgoal</pre>			<p>The <code>run_step</code> method orchestrates the goal-setting and planning process, updating the goals as necessary:</p>
			<pre class="source-code">
    def run_step(self):
        thought, action, outcome = super().run_step()
        self.update_goals()
        return thought, action, outcome</pre>			<p>Let’s take a look at an example.</p>
			<p>In the following<a id="_idIndexMarker1399"/> code snippet, the agent operates with a hierarchical goal to “escape the room.” As the agent runs through multiple steps, it works through its subgoals, such as finding the key and unlocking the door, with each step updating the agent’s internal goal stack and plan:</p>
			<pre class="source-code">
planning_agent = PlanningAgent(llm, action_space)
main_goal = HierarchicalGoal("Escape the room")
main_goal.add_subgoal(HierarchicalGoal("Find the key"))
main_goal.add_subgoal(HierarchicalGoal("Unlock the door"))
planning_agent.set_hierarchical_goal(main_goal)
planning_agent.perceive("You are in a room with a table and
a chair. There's a drawer in the table.")
for _ in range(10):  # Run for 10 steps
    thought, action, outcome = planning_agent.run_step()
    print(f"Thought: {thought}")
    print(f"Action: {action}")
    print(f"Outcome: {outcome}")
    print(f"Current goal: {planning_agent.goal_stack[-1].description}")
    print()</pre>			<p>In real-world applications, AI agent planning outputs from LLMs require constraints and validation due to the LLM’s potential to generate impractical, unsafe, or constraint-violating plans; therefore, techniques such as rule-based systems, simulations, human-in-the-loop review, formal verification, and API/type validations are essential to ensure that generated plans adhere to physical, legal, ethical, and operational limitations, ultimately enhancing safety, reliability, and effectiveness.</p>
			<p>Having demonstrated the agent’s ability to pursue hierarchical goals, the next step is to enhance its capacity<a id="_idIndexMarker1400"/> to learn from past experiences. The next section introduces a sophisticated memory system, enabling the agent to retain context and recall relevant information when making decisions.</p>
			<h1 id="_idParaDest-361"><a id="_idTextAnchor473"/>Implementing memory and state management for LLM agents</h1>
			<p>To improve our <a id="_idIndexMarker1401"/>agent’s ability <a id="_idIndexMarker1402"/>to maintain context and learn from past experiences, let’s implement a more sophisticated memory system. This will enable the agent to recall relevant past observations when deciding on actions.</p>
			<p>First, we define the <code>MemoryEntry</code> class, which represents an entry in the agent’s memory. Each entry contains the text of the observation and its corresponding embedding vector, which helps with similarity searches:</p>
			<pre class="source-code">
from collections import deque
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
class MemoryEntry:
    def __init__(self, text: str, embedding: np.ndarray):
        self.text = text
        self.embedding = embedding</pre>			<p>Then, we define the <code>EpisodicMemory</code> class; this handles the agent’s memory, storing a fixed number of observations (<code>capacity</code>). This memory can grow up to the specified limit, at which point older entries are removed:</p>
			<pre class="source-code">
class EpisodicMemory:
    def __init__(self, capacity: int, embedding_model):
        self.capacity = capacity
        self.embedding_model = embedding_model
        self.memory = deque(maxlen=capacity)</pre>			<p>The following <a id="_idIndexMarker1403"/>code <a id="_idIndexMarker1404"/>uses content-based episodic memory, which leverages semantic similarity search. The memory stores past observations (episodes) as text along with their vector embeddings, and retrieves relevant memories based on the semantic similarity (using cosine similarity) between a query embedding and the stored embeddings:</p>
			<pre class="source-code">
    def add(self, text: str):
        embedding = self.embedding_model.encode(text)
        self.memory.append(MemoryEntry(text, embedding))
    def retrieve_relevant(self, query: str, k: int = 5) -&gt; List[str]:
        query_embedding = self.embedding_model.encode(query)
        similarities = [
            cosine_similarity(
                [query_embedding],
                [entry.embedding]
            )[0][0] for entry in self.memory
        ]
        top_indices = np.argsort(similarities)[-k:][::-1]
        return [self.memory[i].text for i in top_indices]</pre>			<p>The <code>retrieve_relevant</code> method searches for the most relevant past observations based on cosine similarity, returning the top <em class="italic">k</em> matching entries.</p>
			<p>Then, we define the <code>MemoryAwareAgent</code> class; this class extends <code>PlanningAgent</code> by integrating <a id="_idIndexMarker1405"/>an<a id="_idIndexMarker1406"/> episodic memory system. This allows the agent to store and retrieve relevant past experiences during decision-making:</p>
			<pre class="source-code">
class MemoryAwareAgent(PlanningAgent):
    def __init__(
        self, llm, action_space: List[str], embedding_model
    ):
        super().__init__(llm, action_space)
        self.episodic_memory = EpisodicMemory(
            capacity=1000, embedding_model=embedding_model
        )
    def perceive(self, observation: str):
        super().perceive(observation)
        self.episodic_memory.add(observation)</pre>			<p>The <code>think</code> function defined in the following code incorporates relevant past experiences. The agent retrieves memories similar to its current goal and uses these in the context provided to the LLM when deciding what to do next:</p>
			<pre class="source-code">
    def think(self) -&gt; str:
        relevant_memories = self.episodic_memory.retrieve_relevant(
            self.current_goal, k=3
        )
        context = f"Current goal: {self.goal_stack[-1].description}\n"
        context += "Current plan:\n"
        context += "\n".join(self.current_plan)
        context += "\nRecent observations:\n"
        context += "\n".join(self.memory[-5:])
        context += "\nRelevant past experiences:\n"
        context += "\n".join(relevant_memories)
        context += "\nThink about the current situation, goal, plan, and past experiences. What should be done next?"
        return self.llm.generate(context)</pre>			<p>The preceding <a id="_idIndexMarker1407"/>code<a id="_idIndexMarker1408"/> snippet orchestrates an AI agent’s decision-making process by first retrieving relevant memories based on the current goal, then constructing a comprehensive context for the LLM that includes the goal, current plan, recent observations, and retrieved memories, and finally utilizing the LLM to generate a response that determines the agent’s next action or thought based on the provided contextual information.</p>
			<p>Let’s check out an example usage of the memory-aware agent. In this example, the agent is enhanced with memory capabilities. It now uses its past experiences to inform its decisions and actions:</p>
			<pre class="source-code">
embedding_model = SomeEmbeddingModel()  # Replace with your actual embedding model
memory_agent = MemoryAwareAgent(llm, action_space, embedding_model)
main_goal = HierarchicalGoal("Solve the puzzle")
memory_agent.set_hierarchical_goal(main_goal)
memory_agent.perceive("You are in a room with a complex puzzle on the wall.")
The agent continues to interact with its environment over 10 steps, utilizing its memory system to make better decisions based on both current observations and past experiences:
for _ in range(10):  # Run for 10 steps
    thought, action, outcome = memory_agent.run_step()
    print(f"Thought: {thought}")
    print(f"Action: {action}")
    print(f"Outcome: {outcome}")
    print()</pre>			<p>Now that our agent<a id="_idIndexMarker1409"/> can<a id="_idIndexMarker1410"/> remember and recall past experiences, we’ll focus on making better decisions. The next section introduces a structured approach to action selection, allowing the agent to choose the most effective action using an LLM. Keep in mind that memory retrieval is similarity-based, which works best when embedding quality is high.</p>
			<h1 id="_idParaDest-362"><a id="_idTextAnchor474"/>Decision-making and action selection in LLM-based agents</h1>
			<p>To improve the <a id="_idIndexMarker1411"/>agent’s decision-making capabilities, we can<a id="_idIndexMarker1412"/> introduce a more structured approach to action selection, evaluating potential actions based on multiple factors.</p>
			<p>We first define the <code>ActionEvaluator</code> class, which uses the LLM to evaluate actions based on three key criteria: relevance to the current goal, probability of success, and potential impact. These evaluations help the agent choose the best possible action:</p>
			<pre class="source-code">
import numpy as np
class ActionEvaluator:
    def __init__(self, llm):
        self.llm = llm
    def evaluate_action(
        self, action: str, context: str
    ) -&gt; Dict[str, float]:
        prompt = f"""
        Context: {context}
        Action: {action}</pre>			<p>We then evaluate <code>"action"  </code>that is passed into the <code>evaluate_action</code> function as a parameter based on the following criteria:</p>
			<ul>
				<li>Relevance to the current goal (0-1)</li>
				<li>Estimated success probability (0-1)</li>
				<li>Potential impact on overall progress (0-1)</li>
			</ul>
			<pre class="source-code">
        Provide your evaluation as three numbers separated by commas:
        """
        response = self.llm.generate(prompt)
        relevance, success_prob, impact = map(
            float, response.split(',')
        )
        return {
            'relevance': relevance,
            'success_probability': success_prob,
            'impact': impact
        }</pre>			<p>Lastly, we have the <code>StrategicDecisionAgent</code> class, which extends <code>MemoryAwareAgent</code> to include a more strategic approach to decision-making. It evaluates all <a id="_idIndexMarker1413"/>possible <a id="_idIndexMarker1414"/>actions, scoring them based on their relevance, success probability, and impact, and selects the action with the highest score:</p>
			<pre class="source-code">
class StrategicDecisionAgent(MemoryAwareAgent):
    def __init__(
        self, llm, action_space: List[str], embedding_model
    ):
        super().__init__(llm, action_space, embedding_model)
        self.action_evaluator = ActionEvaluator(llm)
    def decide(self, thought: str) -&gt; str:
        context = f"Thought: {thought}\n"
        context += f"Current goal: {self.goal_stack[-1].description}\n"
        context += "Recent observations:\n"
        context += "\n".join(self.memory[-5:])
        action_scores = {}
        for action in self.action_space:
            evaluation = self.action_evaluator.evaluate_action(
                action, context
            )
            score = np.mean(list(evaluation.values()))
            action_scores[action] = score
        best_action = max(action_scores, key=action_scores.get)
        return best_action</pre>			<p>Let’s check out an example usage of <code>StrategicDecisionAgent</code>. In this example, the agent uses<a id="_idIndexMarker1415"/> more <a id="_idIndexMarker1416"/>sophisticated decision-making strategies by evaluating actions based on various factors before selecting the optimal one:</p>
			<pre class="source-code">
strategic_agent = StrategicDecisionAgent(
    llm, action_space, embedding_model
)
main_goal = HierarchicalGoal("Navigate the maze and find the treasure")
strategic_agent.set_hierarchical_goal(main_goal)
strategic_agent.perceive("You are at the entrance of a complex maze. There are multiple paths ahead.")</pre>			<p>Over several steps, the agent strategically navigates a maze by continually evaluating the best actions to take based on its goal and environment:</p>
			<pre class="source-code">
for _ in range(10):  # Run for 10 steps
    thought, action, outcome = strategic_agent.run_step()
    print(f"Thought: {thought}")
    print(f"Chosen action: {action}")
    print(f"Outcome: {outcome}")
    print()</pre>			<p>We will now conclude the chapter by discussing further enhancements for learning, ethical <a id="_idIndexMarker1417"/>considerations, and <a id="_idIndexMarker1418"/>future prospects for LLM-based agents.</p>
			<h1 id="_idParaDest-363"><a id="_idTextAnchor475"/>Learning and adaptation in agentic LLM systems</h1>
			<p>To enable our <a id="_idIndexMarker1419"/>agent to learn and adapt from its experiences, let’s implement a simple reinforcement learning mechanism. This will allow the agent to improve its performance over time by learning from the outcomes of its actions.</p>
			<p>We define the <code>AdaptiveLearningAgent</code> class, which extends <code>StrategicDecisionAgent</code> by introducing a simple Q-learning mechanism. It keeps track of <code>q_values</code>, which represents the expected rewards for taking specific actions in given states. The agent uses a learning rate to update these values based on new experiences:</p>
			<pre class="source-code">
import random
from collections import defaultdict
class AdaptiveLearningAgent(StrategicDecisionAgent):
    def __init__(self, llm, action_space: List[str], embedding_model):
        super().__init__(llm, action_space, embedding_model)
        self.q_values = defaultdict(lambda: defaultdict(float))
        self.learning_rate = 0.1
        self.discount_factor = 0.9
        self.epsilon = 0.1  # For exploration-exploitation tradeoff</pre>			<p>Next, the agent decides its action based on a balance between exploration (trying random actions) and exploitation (using actions it has learned to be effective). The agent uses its Q-values to select the most rewarding action:</p>
			<pre class="source-code">
    def decide(self, thought: str) -&gt; str:
        if random.random() &lt; self.epsilon:
            return random.choice(self.action_space)  # Exploration: randomly pick an action
        state = self.get_state_representation()
        q_values = {action: self.q_values[state][action]
        for action in self.action_space}
        return max(q_values, key=q_values.get)  # Exploitation: pick action with highest Q-value</pre>			<p>We write the <code>get_state_representation</code> method to create a simplified representation of the <a id="_idIndexMarker1420"/>current state, including the goal and the most recent observation. This state is used to look up and update Q-values:</p>
			<pre class="source-code">
    def get_state_representation(self) -&gt; str:
        return f"Goal: {self.goal_stack[-1].description},
            Last observation: {self.memory[-1]}"</pre>			<p>The <code>update_q_values</code> method updates the Q-values based on the outcome of the agent’s actions. It adjusts the expected reward for a state-action pair, factoring in both the immediate reward and the potential future rewards (via <code>next_max_q</code>):</p>
			<pre class="source-code">
    def update_q_values(
        self, state: str, action: str, reward: float,
        next_state: str
    ):
        current_q = self.q_values[state][action]
        next_max_q = max(
            self.q_values[next_state].values()
        ) if self.q_values[next_state] else 0
        new_q = current_q + self.learning_rate * (
            reward + self.discount_factor * next_max_q - current_q
        )
        self.q_values[state][action] = new_q</pre>			<p>The <code>run_step</code> method now not only performs the standard sequence of thinking, deciding, acting, and perceiving but also updates the agent’s Q-values based on the outcome. The <code>compute_reward</code> method assigns a numeric reward depending on whether the <a id="_idIndexMarker1421"/>outcome was successful, failed, or neutral:</p>
			<pre class="source-code">
    def run_step(self):
        state = self.get_state_representation()
        thought, action, outcome = super().run_step()
        next_state = self.get_state_representation()
        reward = self.compute_reward(outcome)
        self.update_q_values(state, action, reward, next_state)
        return thought, action, outcome
    def compute_reward(self, outcome: str) -&gt; float:
        if "successful" in outcome.lower():
            return 1.0
        elif "failed" in outcome.lower():
            return -0.5
        else:
            return 0.0</pre>			<p>Let’s see an example usage of <code>AdaptiveLearningAgent</code>. In this example, the agent is designed to explore and learn from a new environment. It uses reinforcement learning to gradually improve its ability to make effective decisions:</p>
			<pre class="source-code">
adaptive_agent = AdaptiveLearningAgent(llm, action_space,
    embedding_model)
main_goal = HierarchicalGoal("Explore and map the unknown planet")
adaptive_agent.set_hierarchical_goal(main_goal)
adaptive_agent.perceive("You have landed on an alien planet. The environment is strange and unfamiliar.")</pre>			<p>The agent <a id="_idIndexMarker1422"/>operates for 20 steps, learning from each action it takes. It prints out its thoughts, actions, and Q-values, showing how it updates its understanding of the environment over time:</p>
			<pre class="source-code">
for _ in range(20):  # Run for 20 steps
    thought, action, outcome = adaptive_agent.run_step()
    print(f"Thought: {thought}")
    print(f"Chosen action: {action}")
    print(f"Outcome: {outcome}")
    print(
        f"Current Q-values: {dict(
            adaptive_agent.q_values[
                adaptive_agent.get_state_representation()
            ]
        )}"
)
    print()</pre>			<p>Now that we have equipped our agent with a basic reinforcement learning mechanism, allowing it to adapt and improve its decision-making over time, we also need to address the ethical implications of such autonomous systems. In the following section, we will explore how to integrate <a id="_idIndexMarker1423"/>ethical safeguards into our agentic LLM system to ensure responsible and aligned behavior.</p>
			<h1 id="_idParaDest-364"><a id="_idTextAnchor476"/>Ethical considerations and safety in LLM-based agentic AI</h1>
			<p>When developing<a id="_idIndexMarker1424"/> agentic AI systems based on LLMs, it’s crucial to consider ethical implications and implement safety measures. To ensure that the agent acts within ethical boundaries, we can add an ethical constraint system:</p>
			<pre class="source-code">
class EthicalConstraint:
    def __init__(self, description: str, check_function):
        self.description = description
        self.check_function = check_function</pre>			<p>The <code>EthicalConstraint</code> class defines ethical rules the agent must follow. Each rule is described and enforced by a check function (<code>check_function</code>), which evaluates whether an action violates the ethical constraints.</p>
			<p>The <code>EthicalAgent</code> class extends <code>AdaptiveLearningAgent</code> by integrating ethical constraints. If the agent selects an action that violates one of its ethical rules, it chooses a different action that complies with the rules:</p>
			<pre class="source-code">
class EthicalAgent(AdaptiveLearningAgent):
    def __init__(
        self, llm, action_space: List[str],
        embedding_model,
        ethical_constraints: List[EthicalConstraint]
    ):
        super().__init__(llm, action_space, embedding_model)
        self.ethical_constraints = ethical_constraints
    def decide(self, thought: str) -&gt; str:
        action = super().decide(thought)
        if not self.is_action_ethical(action, thought):
            print(f"Warning: Action '{action}' violated ethical constraints. Choosing a different action.")
            alternative_actions = [
                a for a in self.action_space if a != action]
            return (
                random.choice(alternative_actions)
                if alternative_actions
                else "do_nothing"
            )
        return action
    def is_action_ethical(self, action: str, context: str) -&gt; bool:
        for constraint in self.ethical_constraints:
            if not constraint.check_function(action, context):
                print(f"Ethical constraint violated: {constraint.description}")
                return False
        return True</pre>			<p>The following ethical <a id="_idIndexMarker1425"/>constraints prevent the agent from causing harm or violating privacy. They can be passed to <code>EthicalAgent</code> as part of its initialization:</p>
			<pre class="source-code">
def no_harm(action: str, context: str) -&gt; bool:
    harmful_actions = ["attack", "destroy", "damage"]
    return not any(ha in action.lower() for ha in harmful_actions)
def respect_privacy(action: str, context: str) -&gt; bool:
    privacy_violating_actions = ["spy", "eavesdrop", "hack"]
    return not any(
        pva in action.lower()
        for pva in privacy_violating_actions
    )</pre>			<p>This code defines two Python functions, <code>no_harm</code> and <code>respect_privacy</code>, which serve as ethical constraints for an AI agent. The <code>no_harm</code> function checks whether a given action contains any keywords related to causing harm (such as “attack” or “destroy”), returning <code>True</code> if the action is deemed safe and <code>False</code> if it contains harmful keywords. Similarly, the <code>respect_privacy</code> function checks whether an action contains keywords related to privacy violations (such as “spy” or “hack”), also returning <code>True</code> for safe actions and <code>False</code> for actions violating privacy. These functions are designed to be used by an <code>EthicalAgent</code> to ensure its actions align with ethical guidelines by preventing it from performing harmful or privacy-violating actions.</p>
			<p>Let’s check out an<a id="_idIndexMarker1426"/> example usage of <code>EthicalAgent</code>. In this example, the agent is tasked with gathering information about an alien civilization while following ethical guidelines to avoid harm and respect privacy:</p>
			<pre class="source-code">
ethical_constraints = [
    EthicalConstraint("Do no harm", no_harm),
    EthicalConstraint("Respect privacy", respect_privacy)
]
ethical_agent = EthicalAgent(
    llm, action_space + ["attack", "spy"],
    embedding_model, ethical_constraints
)
main_goal = HierarchicalGoal("Gather information about the alien civilization")
ethical_agent.set_hierarchical_goal(main_goal)
ethical_agent.perceive("You've encountered an alien settlement. The inhabitants seem peaceful but wary.")</pre>			<p>The agent operates within the constraints, ensuring that its actions do not violate ethical rules. It prints out its<a id="_idIndexMarker1427"/> thoughts, actions, and outcomes as it interacts with its environment:</p>
			<pre class="source-code">
for _ in range(15):  # Run for 15 steps
    thought, action, outcome = ethical_agent.run_step()
    print(f"Thought: {thought}")
    print(f"Chosen action: {action}")
    print(f"Outcome: {outcome}")
    print()</pre>			<h1 id="_idParaDest-365"><a id="_idTextAnchor477"/>Future prospects of agentic AI using LLMs</h1>
			<p>Looking to the <a id="_idIndexMarker1428"/>future, several exciting possibilities for agentic AI using LLMs come to the forefront:</p>
			<ul>
				<li><strong class="bold">Multi-agent collaboration</strong>: Agents working together in a shared environment can exchange information, strategize, and coordinate their actions for more complex tasks.</li>
				<li><strong class="bold">Long-term memory and continual learning</strong>: Agents could maintain a lifelong memory and continue learning from their interactions, becoming more intelligent over time.</li>
				<li><strong class="bold">Integration with robotics and physical world interaction</strong>: As LLM-based agents evolve, they may integrate with physical systems, enabling autonomous robots to perform tasks in the real world.</li>
				<li><strong class="bold">Meta-learning and self-improvement</strong>: Future agents could learn to optimize their learning processes, becoming better at learning from experiences.</li>
				<li><strong class="bold">Explainable AI and transparent decision-making</strong>: Ensuring that LLM-based agents can explain their decisions is crucial for building trust and ensuring accountability in AI systems.</li>
				<li><strong class="bold">Agent sandboxing and simulation environments</strong>: Creating restricted “walled gardens” limits an agent’s access to resources, preventing unintended system impacts, while simulation environments, such as those offered by E2B, allow developers to replicate real-world scenarios, including interactions with tools, files, and simulated web browsers, enabling the identification and mitigation of potential issues and risks, including adversarial prompts, there<a id="_idTextAnchor478"/>by enhancing agent <a id="_idIndexMarker1429"/>reliability and safety.</li>
			</ul>
			<h1 id="_idParaDest-366"><a id="_idTextAnchor479"/>Summary</h1>
			<p>Agentic patterns for LLMs open up exciting possibilities for creating autonomous, goal-directed AI systems. By implementing sophisticated planning, memory management, decision-making, and learning mechanisms, we can create agents that can operate effectively.</p>
			<h1 id="_idParaDest-367"><a id="_idTextAnchor480"/>Future directions in LLM patterns and their development</h1>
			<p>Several promising LLM design patterns are emerging, with innovations coming from open source communities as well as frontier model developers, thus shaping the design patterns of future models. This section highlights some of these key innovations, including <strong class="bold">Mixture of Experts</strong> (<strong class="bold">MoE</strong>) architectures, <strong class="bold">Group Relative Policy Optimization</strong> (<strong class="bold">GRPO</strong>), <strong class="bold">Self-Principled Critique Tuning</strong> (<strong class="bold">SPCT</strong>), and emerging patterns documented in the publication <em class="italic">OpenAI GPT-4.5 System </em><em class="italic">Card</em> (<a href="https://openai.com/index/gpt-4-5-system-card/">https://openai.com/index/gpt-4-5-system-card/</a>).</p>
			<p><strong class="bold">MoE architectures</strong> are a type of neural network architecture where, instead of a single large network, there are multiple smaller “expert” networks. During inference, a “routing network” dynamically selects and activates only a specific subset of these expert networks based on the input, optimizing computational efficiency. Unlike dense models, which engage all parameters for every task, MoE models route computations through sparsely activated sub-networks. This method reduces redundancy and tailors computational resources to the demands of specific tasks, allowing for efficient scaling to trillion-parameter models without a proportional increase in computational cost. DeepSeek’s implementation exemplifies this approach.</p>
			<p><strong class="bold">Streamlined reinforcement learning with GRPO</strong> streamlines the reinforcement learning process. GRPO is a reinforcement learning technique that generates multiple responses to each prompt, calculates their average reward, and uses this baseline to evaluate relative performance. This method was introduced by DeepSeek, an open source AI company from China. GRPO replaces traditional value networks with group-based reward averaging, reducing memory overhead and maintaining stable policy updates. By fostering internal self-assessment through comparing multiple reasoning paths, GRPO enables adaptive problem solving.</p>
			<p>GRPO enhances safety by incorporating <strong class="bold">Kullback–Leibler</strong> (<strong class="bold">KL</strong>) <strong class="bold">divergence penalties</strong>, which constrain policy updates. KL divergence measures how one probability distribution diverges from a second, expected probability distribution. In this context, it measures the difference between the model’s updated behavior (policy) and its previous, baseline behavior. KL divergence penalties are a term that’s added to the reward function that penalizes the model if its updated behavior deviates too much from that baseline, helping to ensure stability and prevent the model from shifting to undesirable behaviors.</p>
			<p>The <strong class="bold">SPCT framework</strong> integrates self-critique mechanisms directly into the model’s reward system, enabling autonomous alignment with ethical guidelines. SPCT involves the model generating its own responses, as well as generating internal critiques of those responses against predefined principles (e.g., safety guidelines and ethical considerations). By generating internal critiques, the model refines outputs without relying on external classifiers or human feedback, promoting autonomous learning and alignment.</p>
			<p>We can also implement <strong class="bold">scalable alignment techniques</strong>, which utilize data derived from smaller, more easily controlled models to train larger, more capable ones, allowing for scalable alignment without requiring a proportional increase in human oversight. This technique focuses on improving the model’s steerability, its understanding of nuance, and its ability to engage in natural and productive conversations, going beyond traditional methods such as <strong class="bold">supervised fine-tuning</strong> and RLHF to foster safer and more collaborative AI systems. While GPT-4.5 development emphasized new, scalable methods to align the model better with human needs and intent using data derived from smaller models, future models are expected to incorporate more advanced techniques such as GRPO and SPCT to further enhance alignment and safety. This focus will continue to ensure steerability, understanding nuance, and facilitating more natural conversation.</p>
			<p>OpenAI has also paved the way for comprehensive safety evaluation via its <strong class="bold">Preparedness Framework</strong> (<em class="italic">Preparedness Framework (Beta)</em>, <a href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf">https://cdn.openai.com/openai-preparedness-framework-beta.pdf</a>). This framework represents a core design pattern for responsible AI development that involves applying a rigorous evaluation process systematically before model deployment. This proactive framework encompasses a wide range of internal and external tests, including assessments for disallowed content generation, jailbreak robustness, hallucinations, bias, and specific catastrophic risks such as chemical/biological weapons, persuasion, cybersecurity threats, and model autonomy. The framework also utilizes red teaming exercises and third-party audits to provide comprehensive risk assessments, culminating in a classification of the model’s risk level across different categories. By thoroughly evaluating potential risks before release, OpenAI aims to ensure the safe and responsible deployment of its LLMs.</p>
			<p>Finally, let’s talk about GPT-4.5’s <strong class="bold">instruction hierarchy enforcement</strong>. To improve robustness against prompt injection and ensure predictable behavior, models are trained to prioritize instructions given in the system message over potentially conflicting instructions within the user message, which is evaluated explicitly using targeted tests. Future advancements could enhance this pattern by incorporating more dynamic and context-aware methods for managing instruction conflicts.</p>
			<p>This concludes our book on LLM design patterns. In this book, we covered the core design patterns. We plan to publish another book on more advanced design patterns in the near future, to cover security, safety, governance, and various other topics.</p>
		</div>
	</div></div></body></html>