<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer058">
			<h1 id="_idParaDest-357" class="chapter-number"><a id="_idTextAnchor469"/>30</h1>
			<h1 id="_idParaDest-358"><a id="_idTextAnchor470"/>Agentic Patterns</h1>
			<p>In this final chapter, we’ll explore patterns for creating more autonomous and goal-directed AI agents using LLMs. You’ll learn about goal-setting and planning in LLM-based agents, implementing memory and state management, and strategies for decision-making and action selection. We’ll cover techniques for learning and adaptation in agentic LLM systems and discuss the ethical considerations and safety measures necessary when developing <span class="No-Break">such systems.</span></p>
			<p>By the end of this chapter, you’ll be able to design and implement sophisticated AI agents powered by LLMs, opening up new possibilities for autonomous <span class="No-Break">AI systems.</span></p>
			<p>In this chapter, we will be covering the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Introduction to agentic AI systems based <span class="No-Break">on LLMs</span></li>
				<li>Goal-setting and planning in <span class="No-Break">LLM-based agents</span></li>
				<li>Implementing memory and state management for <span class="No-Break">LLM agents</span></li>
				<li>Decision-making and action selection in <span class="No-Break">LLM-based agents</span></li>
				<li>Learning and adaptation in agentic <span class="No-Break">LLM systems</span></li>
				<li>Ethical considerations and safety in LLM-based <span class="No-Break">agentic AI</span></li>
				<li>Future prospects of agentic AI <span class="No-Break">using LLMs</span></li>
			</ul>
			<h1 id="_idParaDest-359"><a id="_idTextAnchor471"/>Introduction to agentic AI systems based on LLMs</h1>
			<p>Agentic AI systems<a id="_idIndexMarker1386"/> using LLMs are designed to operate<a id="_idIndexMarker1387"/> autonomously, make decisions, and take actions to achieve specified goals. These systems combine the powerful language understanding and generation capabilities of LLMs with goal-oriented behavior and <span class="No-Break">environmental interaction.</span></p>
			<p>Let’s start by implementing a basic structure for an <span class="No-Break">LLM-based agent:</span></p>
			<pre class="source-code">
from typing import List, Dict, Any
import random
class LLMAgent:
    def __init__(self, llm, action_space: List[str]):
        self.llm = llm
        self.action_space = action_space
        self.memory = []
        self.current_goal = None</pre>			<p>Here, the <strong class="source-inline">LLMAgent</strong> class is initialized with an LLM (<strong class="source-inline">llm</strong>) and a list of possible actions (<strong class="source-inline">action_space</strong>). It also maintains a memory of observations and a <strong class="source-inline">current_goal</strong>, which will be used to guide the <span class="No-Break">agent’s actions.</span></p>
			<pre class="source-code">
    def set_goal(self, goal: str):
        self.current_goal = goal
    def perceive(self, observation: str):
        self.memory.append(observation)</pre>			<p>Here, we define <a id="_idIndexMarker1388"/>two <a id="_idIndexMarker1389"/>methods: <strong class="source-inline">set_goal</strong>, which allows the agent to set its goal, and <strong class="source-inline">perceive</strong>, which enables the agent to take in observations from the environment and store them in <span class="No-Break">its memory.</span></p>
			<p>Next, we use the <strong class="source-inline">think</strong> method to generate a thorough process based on the agent’s goal and <span class="No-Break">recent observations:</span></p>
			<pre class="source-code">
    def think(self) -&gt; str:
        context = f"Goal: {self.current_goal}\n"
        context += "Recent observations:\n"
        context += "\n".join(self.memory[-5:])  # Last 5 observations
        context += "\nThink about the current situation and the goal. What should be done next?"
        return self.llm.generate(context)</pre>			<p>The agent asks the language model for advice on the next step by providing a context string, which includes the current goal and the last <span class="No-Break">five observations.</span></p>
			<p>Once the agent has a thought, it must decide on the next action. The <strong class="source-inline">decide</strong> method uses the thought to generate a context, asking the LLM to pick the best action from the <span class="No-Break">available options:</span></p>
			<pre class="source-code">
    def decide(self, thought: str) -&gt; str:
        context = f"Thought: {thought}\n"
        context += "Based on this thought, choose the most appropriate action from the following:\n"
        context += ", ".join(self.action_space)
        context += "\nChosen action:"
        return self.llm.generate(context)</pre>			<p>Then, the <strong class="source-inline">act</strong> method <a id="_idIndexMarker1390"/>simulates taking an action by randomly selecting an <a id="_idIndexMarker1391"/>outcome (success, failure, or an unexpected result). In real scenarios, this would involve interacting with <span class="No-Break">the environment:</span></p>
			<pre class="source-code">
    def act(self, action: str) -&gt; str:
        outcomes = [
            f"Action '{action}' was successful.",
            f"Action '{action}' failed.",
            f"Action '{action}' had an unexpected outcome."
        ]
        return random.choice(outcomes)</pre>			<p>Finally, the <strong class="source-inline">run_step</strong> method orchestrates the entire process of thinking, deciding, acting, and perceiving the outcome, completing one cycle of interaction with <span class="No-Break">the environment:</span></p>
			<pre class="source-code">
    def run_step(self):
        thought = self.think()
        action = self.decide(thought)
        outcome = self.act(action)
        self.perceive(outcome)
        return thought, action, outcome</pre>			<p>Now that we understand the fundamental principles, let’s translate these concepts <span class="No-Break">into code.</span></p>
			<p>Let’s implement a<a id="_idIndexMarker1392"/> basic <a id="_idIndexMarker1393"/>LLM-based agent, establishing the core structure for autonomous operation. The agent is initialized with a hypothetical language model (<strong class="source-inline">llm</strong>) and a set of actions. It sets a goal and perceives the environment to begin interacting <span class="No-Break">with it:</span></p>
			<pre class="source-code">
# Example usage
llm = SomeLLMModel()  # Replace with your actual LLM
action_space = ["move", "grab", "drop", "use", "talk"]
agent = LLMAgent(llm, action_space)
agent.set_goal("Find the key and unlock the door")
agent.perceive("You are in a room with a table and a chair. There's a drawer in the table.")</pre>			<p>In the following <strong class="source-inline">for</strong> loop, the agent runs for five steps, and each thought, action, and outcome is printed to show how the agent interacts with its environment <span class="No-Break">over time:</span></p>
			<pre class="source-code">
for _ in range(5):  # Run for 5 steps
    thought, action, outcome = agent.run_step()
    print(f"Thought: {thought}")
    print(f"Action: {action}")
    print(f"Outcome: {outcome}")
    print()</pre>			<p>Having established the fundamentals of agent behavior, let’s explore more advanced capabilities. The <a id="_idIndexMarker1394"/>following <a id="_idIndexMarker1395"/>section focuses on goal-setting and planning, enabling the agent to proactively work toward <span class="No-Break">complex objectives.</span></p>
			<h1 id="_idParaDest-360"><a id="_idTextAnchor472"/>Goal-setting and planning in LLM-based agents</h1>
			<p>To enhance our agent <a id="_idIndexMarker1396"/>with more sophisticated goal-setting and planning capabilities, let’s implement a hierarchical goal structure and a <span class="No-Break">planning mechanism.</span></p>
			<p>First, we define a <strong class="source-inline">HierarchicalGoal</strong> class; this class allows the agent to break down large tasks into <span class="No-Break">smaller subgoals:</span></p>
			<pre class="source-code">
class HierarchicalGoal:
    def __init__(
        self, description: str,
        subgoals: List['HierarchicalGoal'] = None
    ):
        self.description = description
        self.subgoals = subgoals or []
        self.completed = False
    def add_subgoal(self, subgoal: 'HierarchicalGoal'):
        self.subgoals.append(subgoal)
    def mark_completed(self):
        self.completed = True</pre>			<p>The agent can complete these subgoals step by step, marking each as completed <span class="No-Break">when done.</span></p>
			<p>Next, we have a <strong class="source-inline">PlanningAgent</strong> class, which inherits from <strong class="source-inline">LLMAgent</strong> but adds the ability to handle hierarchical goals. It stores goals in a stack, working through subgoals as they <span class="No-Break">are completed:</span></p>
			<pre class="source-code">
class PlanningAgent(LLMAgent):
    def __init__(self, llm, action_space: List[str]):
        super().__init__(llm, action_space)
        self.goal_stack = []
        self.current_plan = []
    def set_hierarchical_goal(self, goal: HierarchicalGoal):
        self.goal_stack = [goal]</pre>			<p>The <strong class="source-inline">think</strong> method now <a id="_idIndexMarker1397"/>also includes planning. If no current plan exists, it asks the LLM to generate a step-by-step plan to achieve the <span class="No-Break">current goal:</span></p>
			<pre class="source-code">
    def think(self) -&gt; str:
        if not self.current_plan:
            self.create_plan()
        context = f"Current goal: {self.goal_stack[-1].description}\n"
        context += "Current plan:\n"
        context += "\n".join(self.current_plan)
        context += "\nRecent observations:\n"
        context += "\n".join(self.memory[-5:])
        context += "\nThink about the current situation, goal, and plan. What should be done next?"
        return self.llm.generate(context)</pre>			<p>Then, the <strong class="source-inline">create_plan</strong> method generates a plan by prompting the LLM with the current goal and the list of actions. The generated plan is split into <span class="No-Break">individual steps:</span></p>
			<pre class="source-code">
    def create_plan(self):
        context = f"Goal: {self.goal_stack[-1].description}\n"
        context += "Create a step-by-step plan to achieve this goal. Each step should be an action from the following list:\n"
        context += ", ".join(self.action_space)
        context += "\nPlan:"
        plan_text = self.llm.generate(context)
        self.current_plan = [
            step.strip() for step in plan_text.split("\n")
            if step.strip()
        ]</pre>			<p>The <strong class="source-inline">update_goals</strong> method<a id="_idIndexMarker1398"/> checks whether the current goal is complete. If it is, it moves on to the next goal or subgoal and resets the <span class="No-Break">plan accordingly:</span></p>
			<pre class="source-code">
    def update_goals(self):
        current_goal = self.goal_stack[-1]
        if current_goal.completed:
            self.goal_stack.pop()
            if self.goal_stack:
                self.current_plan = []  # Reset plan for the next goal
        elif current_goal.subgoals:
            next_subgoal = next(
                (
                    sg for sg in current_goal.subgoals
                    if not sg.completed
                ),
                None
            )
            if next_subgoal:
                self.goal_stack.append(next_subgoal)
                self.current_plan = []  # Reset plan for the new subgoal</pre>			<p>The <strong class="source-inline">run_step</strong> method orchestrates the goal-setting and planning process, updating the goals <span class="No-Break">as necessary:</span></p>
			<pre class="source-code">
    def run_step(self):
        thought, action, outcome = super().run_step()
        self.update_goals()
        return thought, action, outcome</pre>			<p>Let’s take a look at <span class="No-Break">an example.</span></p>
			<p>In the following<a id="_idIndexMarker1399"/> code snippet, the agent operates with a hierarchical goal to “escape the room.” As the agent runs through multiple steps, it works through its subgoals, such as finding the key and unlocking the door, with each step updating the agent’s internal goal stack <span class="No-Break">and plan:</span></p>
			<pre class="source-code">
planning_agent = PlanningAgent(llm, action_space)
main_goal = HierarchicalGoal("Escape the room")
main_goal.add_subgoal(HierarchicalGoal("Find the key"))
main_goal.add_subgoal(HierarchicalGoal("Unlock the door"))
planning_agent.set_hierarchical_goal(main_goal)
planning_agent.perceive("You are in a room with a table and
a chair. There's a drawer in the table.")
for _ in range(10):  # Run for 10 steps
    thought, action, outcome = planning_agent.run_step()
    print(f"Thought: {thought}")
    print(f"Action: {action}")
    print(f"Outcome: {outcome}")
    print(f"Current goal: {planning_agent.goal_stack[-1].description}")
    print()</pre>			<p>In real-world applications, AI agent planning outputs from LLMs require constraints and validation due to the LLM’s potential to generate impractical, unsafe, or constraint-violating plans; therefore, techniques such as rule-based systems, simulations, human-in-the-loop review, formal verification, and API/type validations are essential to ensure that generated plans adhere to physical, legal, ethical, and operational limitations, ultimately enhancing safety, reliability, <span class="No-Break">and effectiveness.</span></p>
			<p>Having demonstrated the agent’s ability to pursue hierarchical goals, the next step is to enhance its capacity<a id="_idIndexMarker1400"/> to learn from past experiences. The next section introduces a sophisticated memory system, enabling the agent to retain context and recall relevant information when <span class="No-Break">making decisions.</span></p>
			<h1 id="_idParaDest-361"><a id="_idTextAnchor473"/>Implementing memory and state management for LLM agents</h1>
			<p>To improve our <a id="_idIndexMarker1401"/>agent’s ability <a id="_idIndexMarker1402"/>to maintain context and learn from past experiences, let’s implement a more sophisticated memory system. This will enable the agent to recall relevant past observations when deciding <span class="No-Break">on actions.</span></p>
			<p>First, we define the <strong class="source-inline">MemoryEntry</strong> class, which represents an entry in the agent’s memory. Each entry contains the text of the observation and its corresponding embedding vector, which helps with <span class="No-Break">similarity searches:</span></p>
			<pre class="source-code">
from collections import deque
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
class MemoryEntry:
    def __init__(self, text: str, embedding: np.ndarray):
        self.text = text
        self.embedding = embedding</pre>			<p>Then, we define the <strong class="source-inline">EpisodicMemory</strong> class; this handles the agent’s memory, storing a fixed number of observations (<strong class="source-inline">capacity</strong>). This memory can grow up to the specified limit, at which point older entries <span class="No-Break">are removed:</span></p>
			<pre class="source-code">
class EpisodicMemory:
    def __init__(self, capacity: int, embedding_model):
        self.capacity = capacity
        self.embedding_model = embedding_model
        self.memory = deque(maxlen=capacity)</pre>			<p>The following <a id="_idIndexMarker1403"/>code <a id="_idIndexMarker1404"/>uses content-based episodic memory, which leverages semantic similarity search. The memory stores past observations (episodes) as text along with their vector embeddings, and retrieves relevant memories based on the semantic similarity (using cosine similarity) between a query embedding and the <span class="No-Break">stored embeddings:</span></p>
			<pre class="source-code">
    def add(self, text: str):
        embedding = self.embedding_model.encode(text)
        self.memory.append(MemoryEntry(text, embedding))
    def retrieve_relevant(self, query: str, k: int = 5) -&gt; List[str]:
        query_embedding = self.embedding_model.encode(query)
        similarities = [
            cosine_similarity(
                [query_embedding],
                [entry.embedding]
            )[0][0] for entry in self.memory
        ]
        top_indices = np.argsort(similarities)[-k:][::-1]
        return [self.memory[i].text for i in top_indices]</pre>			<p>The <strong class="source-inline">retrieve_relevant</strong> method searches for the most relevant past observations based on cosine similarity, returning the top <em class="italic">k</em> <span class="No-Break">matching entries.</span></p>
			<p>Then, we define the <strong class="source-inline">MemoryAwareAgent</strong> class; this class extends <strong class="source-inline">PlanningAgent</strong> by integrating <a id="_idIndexMarker1405"/>an<a id="_idIndexMarker1406"/> episodic memory system. This allows the agent to store and retrieve relevant past experiences <span class="No-Break">during decision-making:</span></p>
			<pre class="source-code">
class MemoryAwareAgent(PlanningAgent):
    def __init__(
        self, llm, action_space: List[str], embedding_model
    ):
        super().__init__(llm, action_space)
        self.episodic_memory = EpisodicMemory(
            capacity=1000, embedding_model=embedding_model
        )
    def perceive(self, observation: str):
        super().perceive(observation)
        self.episodic_memory.add(observation)</pre>			<p>The <strong class="source-inline">think</strong> function defined in the following code incorporates relevant past experiences. The agent retrieves memories similar to its current goal and uses these in the context provided to the LLM when deciding what to <span class="No-Break">do next:</span></p>
			<pre class="source-code">
    def think(self) -&gt; str:
        relevant_memories = self.episodic_memory.retrieve_relevant(
            self.current_goal, k=3
        )
        context = f"Current goal: {self.goal_stack[-1].description}\n"
        context += "Current plan:\n"
        context += "\n".join(self.current_plan)
        context += "\nRecent observations:\n"
        context += "\n".join(self.memory[-5:])
        context += "\nRelevant past experiences:\n"
        context += "\n".join(relevant_memories)
        context += "\nThink about the current situation, goal, plan, and past experiences. What should be done next?"
        return self.llm.generate(context)</pre>			<p>The preceding <a id="_idIndexMarker1407"/>code<a id="_idIndexMarker1408"/> snippet orchestrates an AI agent’s decision-making process by first retrieving relevant memories based on the current goal, then constructing a comprehensive context for the LLM that includes the goal, current plan, recent observations, and retrieved memories, and finally utilizing the LLM to generate a response that determines the agent’s next action or thought based on the provided <span class="No-Break">contextual information.</span></p>
			<p>Let’s check out an example usage of the memory-aware agent. In this example, the agent is enhanced with memory capabilities. It now uses its past experiences to inform its decisions <span class="No-Break">and actions:</span></p>
			<pre class="source-code">
embedding_model = SomeEmbeddingModel()  # Replace with your actual embedding model
memory_agent = MemoryAwareAgent(llm, action_space, embedding_model)
main_goal = HierarchicalGoal("Solve the puzzle")
memory_agent.set_hierarchical_goal(main_goal)
memory_agent.perceive("You are in a room with a complex puzzle on the wall.")
The agent continues to interact with its environment over 10 steps, utilizing its memory system to make better decisions based on both current observations and past experiences:
for _ in range(10):  # Run for 10 steps
    thought, action, outcome = memory_agent.run_step()
    print(f"Thought: {thought}")
    print(f"Action: {action}")
    print(f"Outcome: {outcome}")
    print()</pre>			<p>Now that our agent<a id="_idIndexMarker1409"/> can<a id="_idIndexMarker1410"/> remember and recall past experiences, we’ll focus on making better decisions. The next section introduces a structured approach to action selection, allowing the agent to choose the most effective action using an LLM. Keep in mind that memory retrieval is similarity-based, which works best when embedding quality <span class="No-Break">is high.</span></p>
			<h1 id="_idParaDest-362"><a id="_idTextAnchor474"/>Decision-making and action selection in LLM-based agents</h1>
			<p>To improve the <a id="_idIndexMarker1411"/>agent’s decision-making capabilities, we can<a id="_idIndexMarker1412"/> introduce a more structured approach to action selection, evaluating potential actions based on <span class="No-Break">multiple factors.</span></p>
			<p>We first define the <strong class="source-inline">ActionEvaluator</strong> class, which uses the LLM to evaluate actions based on three key criteria: relevance to the current goal, probability of success, and potential impact. These evaluations help the agent choose the best <span class="No-Break">possible action:</span></p>
			<pre class="source-code">
import numpy as np
class ActionEvaluator:
    def __init__(self, llm):
        self.llm = llm
    def evaluate_action(
        self, action: str, context: str
    ) -&gt; Dict[str, float]:
        prompt = f"""
        Context: {context}
        Action: {action}</pre>			<p>We then evaluate <strong class="source-inline">"action"  </strong>that is passed into the <strong class="source-inline">evaluate_action</strong> function as a parameter based on the <span class="No-Break">following criteria:</span></p>
			<ul>
				<li>Relevance to the current <span class="No-Break">goal (0-1)</span></li>
				<li>Estimated success <span class="No-Break">probability (0-1)</span></li>
				<li>Potential impact on overall <span class="No-Break">progress (0-1)</span></li>
			</ul>
			<pre class="source-code">
        Provide your evaluation as three numbers separated by commas:
        """
        response = self.llm.generate(prompt)
        relevance, success_prob, impact = map(
            float, response.split(',')
        )
        return {
            'relevance': relevance,
            'success_probability': success_prob,
            'impact': impact
        }</pre>			<p>Lastly, we have the <strong class="source-inline">StrategicDecisionAgent</strong> class, which extends <strong class="source-inline">MemoryAwareAgent</strong> to include a more strategic approach to decision-making. It evaluates all <a id="_idIndexMarker1413"/>possible <a id="_idIndexMarker1414"/>actions, scoring them based on their relevance, success probability, and impact, and selects the action with the <span class="No-Break">highest score:</span></p>
			<pre class="source-code">
class StrategicDecisionAgent(MemoryAwareAgent):
    def __init__(
        self, llm, action_space: List[str], embedding_model
    ):
        super().__init__(llm, action_space, embedding_model)
        self.action_evaluator = ActionEvaluator(llm)
    def decide(self, thought: str) -&gt; str:
        context = f"Thought: {thought}\n"
        context += f"Current goal: {self.goal_stack[-1].description}\n"
        context += "Recent observations:\n"
        context += "\n".join(self.memory[-5:])
        action_scores = {}
        for action in self.action_space:
            evaluation = self.action_evaluator.evaluate_action(
                action, context
            )
            score = np.mean(list(evaluation.values()))
            action_scores[action] = score
        best_action = max(action_scores, key=action_scores.get)
        return best_action</pre>			<p>Let’s check out an example usage of <strong class="source-inline">StrategicDecisionAgent</strong>. In this example, the agent uses<a id="_idIndexMarker1415"/> more <a id="_idIndexMarker1416"/>sophisticated decision-making strategies by evaluating actions based on various factors before selecting the <span class="No-Break">optimal one:</span></p>
			<pre class="source-code">
strategic_agent = StrategicDecisionAgent(
    llm, action_space, embedding_model
)
main_goal = HierarchicalGoal("Navigate the maze and find the treasure")
strategic_agent.set_hierarchical_goal(main_goal)
strategic_agent.perceive("You are at the entrance of a complex maze. There are multiple paths ahead.")</pre>			<p>Over several steps, the agent strategically navigates a maze by continually evaluating the best actions to take based on its goal <span class="No-Break">and environment:</span></p>
			<pre class="source-code">
for _ in range(10):  # Run for 10 steps
    thought, action, outcome = strategic_agent.run_step()
    print(f"Thought: {thought}")
    print(f"Chosen action: {action}")
    print(f"Outcome: {outcome}")
    print()</pre>			<p>We will now conclude the chapter by discussing further enhancements for learning, ethical <a id="_idIndexMarker1417"/>considerations, and <a id="_idIndexMarker1418"/>future prospects for <span class="No-Break">LLM-based agents.</span></p>
			<h1 id="_idParaDest-363"><a id="_idTextAnchor475"/>Learning and adaptation in agentic LLM systems</h1>
			<p>To enable our <a id="_idIndexMarker1419"/>agent to learn and adapt from its experiences, let’s implement a simple reinforcement learning mechanism. This will allow the agent to improve its performance over time by learning from the outcomes of <span class="No-Break">its actions.</span></p>
			<p>We define the <strong class="source-inline">AdaptiveLearningAgent</strong> class, which extends <strong class="source-inline">StrategicDecisionAgent</strong> by introducing a simple Q-learning mechanism. It keeps track of <strong class="source-inline">q_values</strong>, which represents the expected rewards for taking specific actions in given states. The agent uses a learning rate to update these values based on <span class="No-Break">new experiences:</span></p>
			<pre class="source-code">
import random
from collections import defaultdict
class AdaptiveLearningAgent(StrategicDecisionAgent):
    def __init__(self, llm, action_space: List[str], embedding_model):
        super().__init__(llm, action_space, embedding_model)
        self.q_values = defaultdict(lambda: defaultdict(float))
        self.learning_rate = 0.1
        self.discount_factor = 0.9
        self.epsilon = 0.1  # For exploration-exploitation tradeoff</pre>			<p>Next, the agent decides its action based on a balance between exploration (trying random actions) and exploitation (using actions it has learned to be effective). The agent uses its Q-values to select the most <span class="No-Break">rewarding action:</span></p>
			<pre class="source-code">
    def decide(self, thought: str) -&gt; str:
        if random.random() &lt; self.epsilon:
            return random.choice(self.action_space)  # Exploration: randomly pick an action
        state = self.get_state_representation()
        q_values = {action: self.q_values[state][action]
        for action in self.action_space}
        return max(q_values, key=q_values.get)  # Exploitation: pick action with highest Q-value</pre>			<p>We write the <strong class="source-inline">get_state_representation</strong> method to create a simplified representation of the <a id="_idIndexMarker1420"/>current state, including the goal and the most recent observation. This state is used to look up and <span class="No-Break">update Q-values:</span></p>
			<pre class="source-code">
    def get_state_representation(self) -&gt; str:
        return f"Goal: {self.goal_stack[-1].description},
            Last observation: {self.memory[-1]}"</pre>			<p>The <strong class="source-inline">update_q_values</strong> method updates the Q-values based on the outcome of the agent’s actions. It adjusts the expected reward for a state-action pair, factoring in both the immediate reward and the potential future rewards (<span class="No-Break">via </span><span class="No-Break"><strong class="source-inline">next_max_q</strong></span><span class="No-Break">):</span></p>
			<pre class="source-code">
    def update_q_values(
        self, state: str, action: str, reward: float,
        next_state: str
    ):
        current_q = self.q_values[state][action]
        next_max_q = max(
            self.q_values[next_state].values()
        ) if self.q_values[next_state] else 0
        new_q = current_q + self.learning_rate * (
            reward + self.discount_factor * next_max_q - current_q
        )
        self.q_values[state][action] = new_q</pre>			<p>The <strong class="source-inline">run_step</strong> method now not only performs the standard sequence of thinking, deciding, acting, and perceiving but also updates the agent’s Q-values based on the outcome. The <strong class="source-inline">compute_reward</strong> method assigns a numeric reward depending on whether the <a id="_idIndexMarker1421"/>outcome was successful, failed, <span class="No-Break">or neutral:</span></p>
			<pre class="source-code">
    def run_step(self):
        state = self.get_state_representation()
        thought, action, outcome = super().run_step()
        next_state = self.get_state_representation()
        reward = self.compute_reward(outcome)
        self.update_q_values(state, action, reward, next_state)
        return thought, action, outcome
    def compute_reward(self, outcome: str) -&gt; float:
        if "successful" in outcome.lower():
            return 1.0
        elif "failed" in outcome.lower():
            return -0.5
        else:
            return 0.0</pre>			<p>Let’s see an example usage of <strong class="source-inline">AdaptiveLearningAgent</strong>. In this example, the agent is designed to explore and learn from a new environment. It uses reinforcement learning to gradually improve its ability to make <span class="No-Break">effective decisions:</span></p>
			<pre class="source-code">
adaptive_agent = AdaptiveLearningAgent(llm, action_space,
    embedding_model)
main_goal = HierarchicalGoal("Explore and map the unknown planet")
adaptive_agent.set_hierarchical_goal(main_goal)
adaptive_agent.perceive("You have landed on an alien planet. The environment is strange and unfamiliar.")</pre>			<p>The agent <a id="_idIndexMarker1422"/>operates for 20 steps, learning from each action it takes. It prints out its thoughts, actions, and Q-values, showing how it updates its understanding of the environment <span class="No-Break">over time:</span></p>
			<pre class="source-code">
for _ in range(20):  # Run for 20 steps
    thought, action, outcome = adaptive_agent.run_step()
    print(f"Thought: {thought}")
    print(f"Chosen action: {action}")
    print(f"Outcome: {outcome}")
    print(
        f"Current Q-values: {dict(
            adaptive_agent.q_values[
                adaptive_agent.get_state_representation()
            ]
        )}"
)
    print()</pre>			<p>Now that we have equipped our agent with a basic reinforcement learning mechanism, allowing it to adapt and improve its decision-making over time, we also need to address the ethical implications of such autonomous systems. In the following section, we will explore how to integrate <a id="_idIndexMarker1423"/>ethical safeguards into our agentic LLM system to ensure responsible and <span class="No-Break">aligned behavior.</span></p>
			<h1 id="_idParaDest-364"><a id="_idTextAnchor476"/>Ethical considerations and safety in LLM-based agentic AI</h1>
			<p>When developing<a id="_idIndexMarker1424"/> agentic AI systems based on LLMs, it’s crucial to consider ethical implications and implement safety measures. To ensure that the agent acts within ethical boundaries, we can add an ethical <span class="No-Break">constraint system:</span></p>
			<pre class="source-code">
class EthicalConstraint:
    def __init__(self, description: str, check_function):
        self.description = description
        self.check_function = check_function</pre>			<p>The <strong class="source-inline">EthicalConstraint</strong> class defines ethical rules the agent must follow. Each rule is described and enforced by a check function (<strong class="source-inline">check_function</strong>), which evaluates whether an action violates the <span class="No-Break">ethical constraints.</span></p>
			<p>The <strong class="source-inline">EthicalAgent</strong> class extends <strong class="source-inline">AdaptiveLearningAgent</strong> by integrating ethical constraints. If the agent selects an action that violates one of its ethical rules, it chooses a different action that complies with <span class="No-Break">the rules:</span></p>
			<pre class="source-code">
class EthicalAgent(AdaptiveLearningAgent):
    def __init__(
        self, llm, action_space: List[str],
        embedding_model,
        ethical_constraints: List[EthicalConstraint]
    ):
        super().__init__(llm, action_space, embedding_model)
        self.ethical_constraints = ethical_constraints
    def decide(self, thought: str) -&gt; str:
        action = super().decide(thought)
        if not self.is_action_ethical(action, thought):
            print(f"Warning: Action '{action}' violated ethical constraints. Choosing a different action.")
            alternative_actions = [
                a for a in self.action_space if a != action]
            return (
                random.choice(alternative_actions)
                if alternative_actions
                else "do_nothing"
            )
        return action
    def is_action_ethical(self, action: str, context: str) -&gt; bool:
        for constraint in self.ethical_constraints:
            if not constraint.check_function(action, context):
                print(f"Ethical constraint violated: {constraint.description}")
                return False
        return True</pre>			<p>The following ethical <a id="_idIndexMarker1425"/>constraints prevent the agent from causing harm or violating privacy. They can be passed to <strong class="source-inline">EthicalAgent</strong> as part of <span class="No-Break">its initialization:</span></p>
			<pre class="source-code">
def no_harm(action: str, context: str) -&gt; bool:
    harmful_actions = ["attack", "destroy", "damage"]
    return not any(ha in action.lower() for ha in harmful_actions)
def respect_privacy(action: str, context: str) -&gt; bool:
    privacy_violating_actions = ["spy", "eavesdrop", "hack"]
    return not any(
        pva in action.lower()
        for pva in privacy_violating_actions
    )</pre>			<p>This code defines two Python functions, <strong class="source-inline">no_harm</strong> and <strong class="source-inline">respect_privacy</strong>, which serve as ethical constraints for an AI agent. The <strong class="source-inline">no_harm</strong> function checks whether a given action contains any keywords related to causing harm (such as “attack” or “destroy”), returning <strong class="source-inline">True</strong> if the action is deemed safe and <strong class="source-inline">False</strong> if it contains harmful keywords. Similarly, the <strong class="source-inline">respect_privacy</strong> function checks whether an action contains keywords related to privacy violations (such as “spy” or “hack”), also returning <strong class="source-inline">True</strong> for safe actions and <strong class="source-inline">False</strong> for actions violating privacy. These functions are designed to be used by an <strong class="source-inline">EthicalAgent</strong> to ensure its actions align with ethical guidelines by preventing it from performing harmful or <span class="No-Break">privacy-violating actions.</span></p>
			<p>Let’s check out an<a id="_idIndexMarker1426"/> example usage of <strong class="source-inline">EthicalAgent</strong>. In this example, the agent is tasked with gathering information about an alien civilization while following ethical guidelines to avoid harm and <span class="No-Break">respect privacy:</span></p>
			<pre class="source-code">
ethical_constraints = [
    EthicalConstraint("Do no harm", no_harm),
    EthicalConstraint("Respect privacy", respect_privacy)
]
ethical_agent = EthicalAgent(
    llm, action_space + ["attack", "spy"],
    embedding_model, ethical_constraints
)
main_goal = HierarchicalGoal("Gather information about the alien civilization")
ethical_agent.set_hierarchical_goal(main_goal)
ethical_agent.perceive("You've encountered an alien settlement. The inhabitants seem peaceful but wary.")</pre>			<p>The agent operates within the constraints, ensuring that its actions do not violate ethical rules. It prints out its<a id="_idIndexMarker1427"/> thoughts, actions, and outcomes as it interacts with <span class="No-Break">its environment:</span></p>
			<pre class="source-code">
for _ in range(15):  # Run for 15 steps
    thought, action, outcome = ethical_agent.run_step()
    print(f"Thought: {thought}")
    print(f"Chosen action: {action}")
    print(f"Outcome: {outcome}")
    print()</pre>			<h1 id="_idParaDest-365"><a id="_idTextAnchor477"/>Future prospects of agentic AI using LLMs</h1>
			<p>Looking to the <a id="_idIndexMarker1428"/>future, several exciting possibilities for agentic AI using LLMs come to <span class="No-Break">the forefront:</span></p>
			<ul>
				<li><strong class="bold">Multi-agent collaboration</strong>: Agents working together in a shared environment can exchange information, strategize, and coordinate their actions for more <span class="No-Break">complex tasks.</span></li>
				<li><strong class="bold">Long-term memory and continual learning</strong>: Agents could maintain a lifelong memory and continue learning from their interactions, becoming more intelligent <span class="No-Break">over time.</span></li>
				<li><strong class="bold">Integration with robotics and physical world interaction</strong>: As LLM-based agents evolve, they may integrate with physical systems, enabling autonomous robots to perform tasks in the <span class="No-Break">real world.</span></li>
				<li><strong class="bold">Meta-learning and self-improvement</strong>: Future agents could learn to optimize their learning processes, becoming better at learning <span class="No-Break">from experiences.</span></li>
				<li><strong class="bold">Explainable AI and transparent decision-making</strong>: Ensuring that LLM-based agents can explain their decisions is crucial for building trust and ensuring accountability in <span class="No-Break">AI systems.</span></li>
				<li><strong class="bold">Agent sandboxing and simulation environments</strong>: Creating restricted “walled gardens” limits an agent’s access to resources, preventing unintended system impacts, while simulation environments, such as those offered by E2B, allow developers to replicate real-world scenarios, including interactions with tools, files, and simulated web browsers, enabling the identification and mitigation of potential issues and risks, including adversarial prompts, there<a id="_idTextAnchor478"/>by enhancing agent <a id="_idIndexMarker1429"/>reliability <span class="No-Break">and safety.</span></li>
			</ul>
			<h1 id="_idParaDest-366"><a id="_idTextAnchor479"/>Summary</h1>
			<p>Agentic patterns for LLMs open up exciting possibilities for creating autonomous, goal-directed AI systems. By implementing sophisticated planning, memory management, decision-making, and learning mechanisms, we can create agents that can <span class="No-Break">operate effectively.</span></p>
			<h1 id="_idParaDest-367"><a id="_idTextAnchor480"/>Future directions in LLM patterns and their development</h1>
			<p>Several promising LLM design patterns are emerging, with innovations coming from open source communities as well as frontier model developers, thus shaping the design patterns of future models. This section highlights some of these key innovations, including <strong class="bold">Mixture of Experts</strong> (<strong class="bold">MoE</strong>) architectures, <strong class="bold">Group Relative Policy Optimization</strong> (<strong class="bold">GRPO</strong>), <strong class="bold">Self-Principled Critique Tuning</strong> (<strong class="bold">SPCT</strong>), and emerging patterns documented in the publication <em class="italic">OpenAI GPT-4.5 System </em><span class="No-Break"><em class="italic">Card</em></span><span class="No-Break"> (</span><a href="https://openai.com/index/gpt-4-5-system-card/"><span class="No-Break">https://openai.com/index/gpt-4-5-system-card/</span></a><span class="No-Break">).</span></p>
			<p><strong class="bold">MoE architectures</strong> are a type of neural network architecture where, instead of a single large network, there are multiple smaller “expert” networks. During inference, a “routing network” dynamically selects and activates only a specific subset of these expert networks based on the input, optimizing computational efficiency. Unlike dense models, which engage all parameters for every task, MoE models route computations through sparsely activated sub-networks. This method reduces redundancy and tailors computational resources to the demands of specific tasks, allowing for efficient scaling to trillion-parameter models without a proportional increase in computational cost. DeepSeek’s implementation exemplifies <span class="No-Break">this approach.</span></p>
			<p><strong class="bold">Streamlined reinforcement learning with GRPO</strong> streamlines the reinforcement learning process. GRPO is a reinforcement learning technique that generates multiple responses to each prompt, calculates their average reward, and uses this baseline to evaluate relative performance. This method was introduced by DeepSeek, an open source AI company from China. GRPO replaces traditional value networks with group-based reward averaging, reducing memory overhead and maintaining stable policy updates. By fostering internal self-assessment through comparing multiple reasoning paths, GRPO enables adaptive <span class="No-Break">problem solving.</span></p>
			<p>GRPO enhances safety by incorporating <strong class="bold">Kullback–Leibler</strong> (<strong class="bold">KL</strong>) <strong class="bold">divergence penalties</strong>, which constrain policy updates. KL divergence measures how one probability distribution diverges from a second, expected probability distribution. In this context, it measures the difference between the model’s updated behavior (policy) and its previous, baseline behavior. KL divergence penalties are a term that’s added to the reward function that penalizes the model if its updated behavior deviates too much from that baseline, helping to ensure stability and prevent the model from shifting to <span class="No-Break">undesirable behaviors.</span></p>
			<p>The <strong class="bold">SPCT framework</strong> integrates self-critique mechanisms directly into the model’s reward system, enabling autonomous alignment with ethical guidelines. SPCT involves the model generating its own responses, as well as generating internal critiques of those responses against predefined principles (e.g., safety guidelines and ethical considerations). By generating internal critiques, the model refines outputs without relying on external classifiers or human feedback, promoting autonomous learning <span class="No-Break">and alignment.</span></p>
			<p>We can also implement <strong class="bold">scalable alignment techniques</strong>, which utilize data derived from smaller, more easily controlled models to train larger, more capable ones, allowing for scalable alignment without requiring a proportional increase in human oversight. This technique focuses on improving the model’s steerability, its understanding of nuance, and its ability to engage in natural and productive conversations, going beyond traditional methods such as <strong class="bold">supervised fine-tuning</strong> and RLHF to foster safer and more collaborative AI systems. While GPT-4.5 development emphasized new, scalable methods to align the model better with human needs and intent using data derived from smaller models, future models are expected to incorporate more advanced techniques such as GRPO and SPCT to further enhance alignment and safety. This focus will continue to ensure steerability, understanding nuance, and facilitating more <span class="No-Break">natural conversation.</span></p>
			<p>OpenAI has also paved the way for comprehensive safety evaluation via its <strong class="bold">Preparedness Framework</strong> (<em class="italic">Preparedness Framework (Beta)</em>, <a href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf">https://cdn.openai.com/openai-preparedness-framework-beta.pdf</a>). This framework represents a core design pattern for responsible AI development that involves applying a rigorous evaluation process systematically before model deployment. This proactive framework encompasses a wide range of internal and external tests, including assessments for disallowed content generation, jailbreak robustness, hallucinations, bias, and specific catastrophic risks such as chemical/biological weapons, persuasion, cybersecurity threats, and model autonomy. The framework also utilizes red teaming exercises and third-party audits to provide comprehensive risk assessments, culminating in a classification of the model’s risk level across different categories. By thoroughly evaluating potential risks before release, OpenAI aims to ensure the safe and responsible deployment of <span class="No-Break">its LLMs.</span></p>
			<p>Finally, let’s talk about GPT-4.5’s <strong class="bold">instruction hierarchy enforcement</strong>. To improve robustness against prompt injection and ensure predictable behavior, models are trained to prioritize instructions given in the system message over potentially conflicting instructions within the user message, which is evaluated explicitly using targeted tests. Future advancements could enhance this pattern by incorporating more dynamic and context-aware methods for managing <span class="No-Break">instruction conflicts.</span></p>
			<p>This concludes our book on LLM design patterns. In this book, we covered the core design patterns. We plan to publish another book on more advanced design patterns in the near future, to cover security, safety, governance, and various <span class="No-Break">other topics.</span></p>
		</div>
	</div></div></body></html>