<html><head></head><body>
		<div id="_idContainer304">
			<h1 class="chapter-number" id="_idParaDest-211"><a id="_idTextAnchor210"/>14</h1>
			<h1 id="_idParaDest-212"><a id="_idTextAnchor211"/>Accelerate Data Engineering on AWS</h1>
			<p>In this chapter, we will look at the following <span class="No-Break">key topics:</span></p>
			<ul>
				<li>Code assistance options with <span class="No-Break">AWS services</span></li>
				<li>Code assistance integration with <span class="No-Break">AWS Glue</span></li>
				<li>Code assistance integration with <span class="No-Break">Amazon EMR</span></li>
				<li>Code assistance integration with <span class="No-Break">AWS Lambda</span></li>
				<li>Code assistance integration with <span class="No-Break">Amazon SageMaker</span></li>
				<li>Code assistance integration with <span class="No-Break">Amazon Redshift</span></li>
			</ul>
			<p>In the previous part of the book, we explored auto-code generation techniques and the integration of a<a id="_idIndexMarker571"/> code companion with <strong class="bold">integrated development environments</strong> (<strong class="bold">IDEs</strong>) and provided examples using JetBrains PyCharm IDE with Amazon Q Developer for different languages that developers use very often. In this chapter, we will specifically focus on how Amazon is expanding in the area of assisting code developers by integrating with core <span class="No-Break">AWS services.</span></p>
			<h1 id="_idParaDest-213"><a id="_idTextAnchor212"/>Code assistance options with AWS services</h1>
			<p>AWS<a id="_idIndexMarker572"/> users select diverse services, considering<a id="_idIndexMarker573"/> factors such as the unique requirements of their projects, use cases, developers’ technical needs, developer preferences, and the characteristics of AWS services. To cater to various developer personas, such as data engineers, data scientists, application developers, and so on, AWS has integrated code assistance with many of its code services. If you are an application builder, software developer, data engineer, or data scientist working with AWS services, you would frequently use builder-friendly tools such as Amazon SageMaker as a platform for <a id="_idIndexMarker574"/>building AI / <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) projects, Amazon EMR as a platform for building big data processing projects, AWS Glue for <a id="_idIndexMarker575"/>building <strong class="bold">extract, transform, and load</strong> (<strong class="bold">ETL</strong>) pipelines, AWS Lambda as a serverless compute service for application development. All these services <a id="_idIndexMarker576"/>provide tools that help builders and <a id="_idIndexMarker577"/>developers <span class="No-Break">write code.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer255">
					<img alt="Figure 14.1 – Code assistance options with AWS services" src="image/B21378_14_1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.1 – Code assistance options with AWS services</p>
			<p>As of the writing of this book, AWS has integrated Amazon Q Developer with AWS Glue, Amazon EMR, AWS Lambda, Amazon SageMaker, and Amazon Redshift. However, we anticipate that the list of services benefiting from code assistance, such as Amazon Q Developer, will continue to expand in <span class="No-Break">the future.</span></p>
			<p>In the following sections, we will dive deep into each of these services, examining their integration with Amazon Q in detail. We will provide examples that will be helpful for data engineers to accelerate development <span class="No-Break">on AWS.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout"><strong class="bold">Large language models</strong> (<strong class="bold">LLMs</strong>), by nature, are <a id="_idIndexMarker578"/>non-deterministic, so you may not get the same code blocks shown in the code snapshots. However, logically, the generated code should meet <span class="No-Break">the requirements.</span></p>
			<p class="callout"><strong class="bold">CodeWhisperer</strong> is <a id="_idIndexMarker579"/>a legacy name from a service that merged with Amazon Q Developer. As of the time of writing this book, some of the integrations are still referred to as CodeWhisperer in the AWS console, which may change in <span class="No-Break">the future.</span></p>
			<h1 id="_idParaDest-214"><a id="_idTextAnchor213"/>Code assistance integration with AWS Glue</h1>
			<p>Before we start diving deep into code assistance support for AWS Glue service, let’s quickly go through an overview of AWS Glue. <strong class="bold">AWS Glue</strong> is a<a id="_idIndexMarker580"/> serverless data integration service designed to simplify the process of discovering, preparing, moving, and integrating data from diverse sources, catering to analytics, ML, and application development needs. At the very high level, AWS Glue has the following major components, and each of them <a id="_idIndexMarker581"/>has multiple features to <a id="_idIndexMarker582"/>support <span class="No-Break">data engineers:</span></p>
			<ul>
				<li><strong class="bold">Glue Data Catalog</strong>: It’s a <a id="_idIndexMarker583"/>centralized technical metadata repository. It stores metadata about data sources, transformations, and targets, providing a unified view of <span class="No-Break">the data.</span></li>
				<li><strong class="bold">Glue Studio</strong>: AWS Glue Studio<a id="_idIndexMarker584"/> offers a graphical interface that facilitates the seamless creation, execution, and monitoring of data integration jobs within AWS Glue. Additionally, it provides Jupyter notebooks for <span class="No-Break">advanced developers.</span></li>
			</ul>
			<p>AWS Glue Studio is seamlessly integrated with Amazon Q Developer. Let’s explore the further functionality by considering a very common use case of <span class="No-Break">data enrichment.</span></p>
			<h2 id="_idParaDest-215"><a id="_idTextAnchor214"/>Use case for AWS Glue</h2>
			<p>Features and functionalities <a id="_idIndexMarker585"/>of any service or tool are best understood when we have a use case to solve. So, let’s start with one of the easy and widely used use cases of data enrichment <span class="No-Break">using lookups.</span></p>
			<p><strong class="bold">Data enrichment using lookup</strong>: In a typical scenario, business analysts often require data enrichment by incorporating details associated with codes/IDs found in a column through a lookup table. The desired result is a comprehensive and denormalized record containing both the code and corresponding details in the same row. To address this specific use case, data engineers develop ETL jobs to join the tables, creating the final structure with a <span class="No-Break">denormalized dataset.</span></p>
			<p>To illustrate this use case, we will use yellow taxi trip records that encompass details such as the date and time of pick-up and drop-off, the locations for pick-up and drop-off, the trip distance, comprehensive fare breakdowns, various rate types, utilized payment methods, and passenger counts reported by the driver. Additionally, trip information incorporates passenger location codes for both pick-up <span class="No-Break">and drop-off.</span></p>
			<p>The business objective is to enhance the dataset with zone information based on the pick-up <span class="No-Break">location code.</span></p>
			<p>To meet this requirement, data engineers must develop a PySpark ETL script. This script should perform a lookup for zone information corresponding to the pick-up location code. Subsequently, the engineers create denormalized/enriched data by amalgamating yellow taxi trip data with detailed pick-up zone information and save the result as <span class="No-Break">a file.</span></p>
			<p>As a code developer / data engineer, you will need to convert the preceding business objectives into <span class="No-Break">technical requirements.</span></p>
			<h2 id="_idParaDest-216"><a id="_idTextAnchor215"/>Solution blueprint</h2>
			<ol>
				<li>Write a <a id="_idIndexMarker586"/>PySpark code to handle <span class="No-Break">technical requirements.</span></li>
				<li>Read the <strong class="source-inline">yellow_tripdata_2023-01.parquet</strong> file from the S3 location in a DataFrame and display a sample of <span class="No-Break">10 records.</span></li>
				<li>Read the <strong class="source-inline">taxi+_zone_lookup.csv</strong> file from the S3 location in a DataFrame and display a sample of <span class="No-Break">10 records.</span></li>
				<li>Perform a left outer join on <strong class="source-inline">yellow_tripdata_2023-01.parquet</strong> and <strong class="source-inline">taxi+_zone_lookup.csv</strong> on <strong class="source-inline">PULocationID = LocationID</strong> to gather pick-up <span class="No-Break">zone information.</span></li>
				<li>Save the preceding dataset as a CSV file in the preceding Amazon S3 bucket in a new <span class="No-Break"><strong class="source-inline">glue_notebook_yellow_pick_up_zone_output</strong></span><span class="No-Break"> folder.</span></li>
				<li>For <a id="_idIndexMarker587"/>verification, download and check the files from the <span class="No-Break"><strong class="source-inline">glue_notebook_yellow_pick_up_zone_output</strong></span><span class="No-Break"> folder.</span></li>
			</ol>
			<p>Now that we have a use case defined, let’s go through the step-by-step solution <span class="No-Break">for it.</span></p>
			<h2 id="_idParaDest-217"><a id="_idTextAnchor216"/>Data preparation</h2>
			<p>The first step will be to<a id="_idIndexMarker588"/> prepare the data. To illustrate its functionality, in the following sections, we will utilize the publicly available NY Taxi dataset from TLC Trip Record <span class="No-Break">Data. </span><a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"><span class="No-Break">https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page</span></a><span class="No-Break">.</span></p>
			<p>Firstly, we will download the required files on a local machine and then upload them in one of Amazon’s <span class="No-Break">S3 buckets:</span></p>
			<ol>
				<li>Download the Yellow Taxi Trip Records data for the Jan 2023 Parquet file (<strong class="source-inline">yellow_tripdata_2023-01.parquet</strong>) on a local machine <span class="No-Break">from </span><a href="https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet"><span class="No-Break">https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet</span></a><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer256">
					<img alt="Figure 14.2 – The Yellow Taxi Trip Records data for Jan 2023 Parquet file" src="image/B21378_14_2.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.2 – The Yellow Taxi Trip Records data for Jan 2023 Parquet file</p>
			<ol>
				<li value="2">Download the Taxi<a id="_idIndexMarker589"/> Zone Lookup Table CSV file (<strong class="source-inline">taxi+_zone_lookup.csv</strong>) on a local machine <span class="No-Break">from </span><a href="https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv"><span class="No-Break">https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv</span></a><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer257">
					<img alt="Figure 14.3 – The Zone Lookup Table CSV file" src="image/B21378_14_3.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.3 – The Zone Lookup Table CSV file</p>
			<ol>
				<li value="3">Create the <a id="_idIndexMarker590"/>two <strong class="source-inline">yellow_taxi_trip_records</strong> and <strong class="source-inline">zone_lookup</strong> folders in Amazon S3, which we can reference in our Glue <span class="No-Break">notebook job.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer258">
					<img alt="Figure 14.4 – S3 folders structure" src="image/B21378_14_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.4 – S3 folders structure</p>
			<ol>
				<li value="4">Upload the <strong class="source-inline">yellow_tripdata_2023-01.parquet</strong> file to the <span class="No-Break"><strong class="source-inline">yellow_taxi_trip_records</strong></span><span class="No-Break"> folder.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer259">
					<img alt="Figure 14.5 – The yellow_taxi_tripdata_record file" src="image/B21378_14_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.5 – The yellow_taxi_tripdata_record file</p>
			<ol>
				<li value="5">Upload <a id="_idIndexMarker591"/>the <strong class="source-inline">taxi+_zone_lookup.csv</strong> file to the <span class="No-Break"><strong class="source-inline">zone_lookup</strong></span><span class="No-Break"> folder.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer260">
					<img alt="Figure 14.6 – The zone_lookup file" src="image/B21378_14_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.6 – The zone_lookup file</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We will use the same dataset and use case to discover solutions using AWS Glue and Amazon EMR. For illustrative purposes, we have prepared the data manually. However, in a production environment, file transfers can be automated by leveraging various AWS services and/or <span class="No-Break">third-party software.</span></p>
			<p>Now, let’s dive deep into a detailed exploration of the solution using the integration of Amazon Q Developer with an AWS Glue Studio notebook for the preceding <span class="No-Break">use case.</span></p>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor217"/>Solution – Amazon Q Developer with an AWS Glue Studio notebook</h2>
			<p>Let’s first enable Amazon Q Developer with an AWS Glue <span class="No-Break">Studio notebook.</span></p>
			<h3>Prerequisites to enable Amazon Q Developer with an AWS Glue Studio notebook</h3>
			<p>The developer is required to modify the <strong class="bold">identity and access management</strong> (<strong class="bold">IAM</strong>) policy<a id="_idIndexMarker592"/> associated with the IAM user or role to <a id="_idIndexMarker593"/>grant permissions for Amazon Q Developer to initiate recommendations in a Glue Studio notebook. Reference <a href="B21378_02.xhtml#_idTextAnchor022"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> for the details to enable Amazon Q Developer with an AWS Glue <span class="No-Break">Studio notebook.</span></p>
			<p>To fulfill the previously mentioned solution blueprint, we will use various auto-code generation techniques that were discussed in <a href="B21378_03.xhtml#_idTextAnchor060"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>. Mainly, we will focus on single-line prompts, multi-line prompts, and chain-of-thought prompts for <span class="No-Break">auto-code generation.</span></p>
			<p>Let’s use Amazon Q Developer to auto-generate an end-to-end script in an AWS Glue Studio notebook. Here is the step-by-step solution walk-through for the previously defined <span class="No-Break">solution blueprint.</span></p>
			<h3>Requirement 1</h3>
			<p>First, you need to write some <span class="No-Break">PySpark code.</span></p>
			<p>While creating a Glue Studio notebook, select the <strong class="bold">Spark (Python)</strong> engine and the role that has the Amazon Q Developer <span class="No-Break">policy attached.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer261">
					<img alt="Figure 14.7 – Create a Glue Studio notebook with PySpark" src="image/B21378_14_7.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.7 – Create a Glue Studio notebook with PySpark</p>
			<p>Once you <a id="_idIndexMarker594"/>create the notebook, observe the kernel named <span class="No-Break"><strong class="source-inline">Glue PySpark</strong></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer262">
					<img alt="Figure 14.8 – A Glue Studio notebook with the Glue PySpark kernel" src="image/B21378_14_08.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.8 – A Glue Studio notebook with the Glue PySpark kernel</p>
			<h3>Requirement 2</h3>
			<p>Read the <strong class="source-inline">yellow_tripdata_2023-01.parquet</strong> file from the S3 location in a DataFrame and display a sample of <span class="No-Break">10 records.</span></p>
			<p>Let’s use a chain-of-thought prompt technique with multiple single-line prompts in different cells to achieve the <span class="No-Break">preceding requirement:</span></p>
			<pre class="source-code">
Prompt # 1:
# Read s3://&lt;your-bucket-name-here&gt;/yellow_taxi_trip_records/yellow_tripdata_2023-01.parquet file in a dataframe
Prompt # 2:
# display a sample of 10 records from dataframe</pre>			<div>
				<div class="IMG---Figure" id="_idContainer263">
					<img alt="Figure 14.9 – PySpark code to read the Yellow Taxi Trip Records data using single-line prompts" src="image/B21378_14_09.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.9 – PySpark code to read the Yellow Taxi Trip Records data using single-line prompts</p>
			<p>Observe that <a id="_idIndexMarker595"/>upon entering the Amazon Q Developer-enabled Glue Studio notebook prompt, it initiates code recommendations. Q Developer recognizes the file format as Parquet and suggests using the <strong class="source-inline">spark.read.parquet</strong> method. You can directly execute each cell/code from the notebook. Furthermore, as you move to the next cell, Q Developer utilizes “line-by-line recommendations” to suggest displaying <span class="No-Break">the schema.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer264">
					<img alt="Figure 14.10 – Line-by-line recommendations to display schema" src="image/B21378_14_10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.10 – Line-by-line recommendations to display schema</p>
			<h3>Requirement 3</h3>
			<p>Read <a id="_idIndexMarker596"/>the <strong class="source-inline">taxi+_zone_lookup.csv</strong> file from the S3 location in a DataFrame and display a sample of <span class="No-Break">10 records.</span></p>
			<p>We already explored the chain-of-thought prompt technique with multiple single-line prompts for <em class="italic">Requirement 2</em>. Now, let’s try with a multi-line prompt to achieve the preceding requirement and we will try to customize the code for the <span class="No-Break">DataFrame name:</span></p>
			<pre class="source-code">
Prompt:
"""
Read s3://&lt;your-bucket-name-here&gt;/zone_lookup/taxi+_zone_lookup.csv in a dataframe name zone_df.
Show sample 10 records from zone_df.
"""</pre>			<div>
				<div class="IMG---Figure" id="_idContainer265">
					<img alt="Figure 14.11 – PySpark code to read the Zone Lookup file using a multi-line prompt" src="image/B21378_14_11.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.11 – PySpark code to read the Zone Lookup file using a multi-line prompt</p>
			<p>Observe that <a id="_idIndexMarker597"/>Amazon Q Developer understood the context behind the multi-line prompt and also the specific DataFrame name instructed in the prompt. It auto-generated multiple lines of code with the DataFrame name as <strong class="source-inline">zone_df</strong> and file format as CSV, suggesting the use of the <strong class="source-inline">spark.read.csv</strong> method to read CSV files. You can directly execute each cell/code from <span class="No-Break">the notebook.</span></p>
			<h3>Requirement 4</h3>
			<p>Perform a left outer join on <strong class="source-inline">yellow_tripdata_2023-01.parquet</strong> and <strong class="source-inline">taxi+_zone_lookup.csv</strong> on <strong class="source-inline">pulocationid = LocationID</strong> to gather pick-up <span class="No-Break">zone information.</span></p>
			<p>We will continue using multi-line prompts and some code customization to achieve the <span class="No-Break">preceding requirement:</span></p>
			<pre class="source-code">
Prompt:
"""
Perform a left outer join on dataframe df and dataframe zone_df on PULocationID = LocationID to save in dataframe name yellow_pu_zone_df.
Show sample 10 records from yellow_pu_zone_df and show schema.
"""</pre>			<div>
				<div class="IMG---Figure" id="_idContainer266">
					<img alt="Figure 14.12 – Left outer join df and dataframe zone_df – multi-line prompt" src="image/B21378_14_12.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.12 – Left outer join df and dataframe zone_df – multi-line prompt</p>
			<p>Now, let’s<a id="_idIndexMarker598"/> review the schema of the DataFrame returns by the <span class="No-Break">code execution.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer267">
					<img alt="Figure 14.13 – Left outer join df and dataframe zone_df – display schema" src="image/B21378_14_13.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.13 – Left outer join df and dataframe zone_df – display schema</p>
			<p>Observe<a id="_idIndexMarker599"/> that, as instructed in the multi-line prompt, Amazon Q Developer understood the context and auto-generated error-free code with the exact specifications we provided related to the DataFrame name of <strong class="source-inline">yellow_pu_zone_df</strong>. You can directly execute each cell/code from <span class="No-Break">the notebook.</span></p>
			<h3>Requirement 5</h3>
			<p>Save the preceding dataset as a CSV file in the preceding Amazon S3 bucket in a new folder <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">glue_notebook_yellow_pick_up_zone_output</strong></span><span class="No-Break">.</span></p>
			<p>Since the preceding requirement is straightforward and can be encapsulated in a single sentence, we will use a single-line prompt to generate the code, and we will also include a header to facilitate <span class="No-Break">easy verification:</span></p>
			<pre class="source-code">
Prompt:
# Save dataframe yellow_pu_zone_df as CSV file at location s3://&lt;your-bucket-name-here&gt;/tlc-dataset-ny-taxi/glue_notebook_yellow_pick_up_zone_output/ with header information</pre>			<div>
				<div class="IMG---Figure" id="_idContainer268">
					<img alt="Figure 14.14 – Save the CSV file with enrichment pick-up location data" src="image/B21378_14_14.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.14 – Save the CSV file with enrichment pick-up location data</p>
			<h3>Requirement 6</h3>
			<p>For <a id="_idIndexMarker600"/>verification, download and check files from the <span class="No-Break"><strong class="source-inline">glue_notebook_yellow_pick_up_zone_output</strong></span><span class="No-Break"> folder.</span></p>
			<p>Let’s go to the Amazon S3 console to verify the files. Select one of the files and <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Download</strong></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer269">
					<img alt="Figure 14.15 – Save a CSV file with enrichment pick-up location data" src="image/B21378_14_15.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.15 – Save a CSV file with enrichment pick-up location data</p>
			<p>After downloading the file, you can use any text editor to review the <span class="No-Break">file contents.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer270">
					<img alt="Figure 14.16 – Verify the CSV file with enrichment pick-up location data" src="image/B21378_14_16.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.16 – Verify the CSV file with enrichment pick-up location data</p>
			<p>Observe that the CSV file has additional columns with zone information based on the pick-up location ID. In the next section, we <a id="_idIndexMarker601"/>will explore Amazon Q Developer integration with AWS Glue and use the chat <span class="No-Break">assistant technique.</span></p>
			<p class="callout-heading">Think challenge</p>
			<p class="callout">To fulfill <em class="italic">Requirement 6</em>, if you are interested, attempt to utilize the same Glue Studio notebook for reading a CSV file, displaying sample records, and adding <span class="No-Break">a header.</span></p>
			<p class="callout"><strong class="bold">Hint</strong>: Use the multi-line prompt technique, similar to the one we used when reading the Zone <span class="No-Break">Lookup file.</span></p>
			<h2 id="_idParaDest-219"><a id="_idTextAnchor218"/>Solution – Amazon Q Developer with AWS Glue</h2>
			<p>Amazon Q <a id="_idIndexMarker602"/>Developer provides a chat-style interface in the AWS Glue console. Now, let’s explore the integration between Amazon Q Developer and AWS Glue for the same use case and solution blueprint that we handled using Amazon Q Developer and an AWS Glue Studio <span class="No-Break">notebook integration.</span></p>
			<p>Let’s now look at the prerequisites to enable Amazon Q with <span class="No-Break">AWS Glue.</span></p>
			<p>To enable Amazon Q Developer integration with AWS Glue, we will need to update the IAM policy. Please refer to <a href="B21378_02.xhtml#_idTextAnchor022"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> for additional details on initiating interaction with Amazon Q in <span class="No-Break">AWS Glue.</span></p>
			<p>Now, let’s dive deep into a detailed exploration of the integration of Amazon Q Developer with AWS Glue Studio for the preceding <span class="No-Break">use case.</span></p>
			<p>To fulfill the mentioned requirements, we will mainly use the chat companion that was discussed in <a href="B21378_03.xhtml#_idTextAnchor060"><span class="No-Break"><em class="italic">Chapter 3</em></span></a><span class="No-Break">.</span></p>
			<p>Here is a <a id="_idIndexMarker603"/>step-by-step solution walk-through that we’ll use as a prompt for all of the <span class="No-Break">preceding requirements:</span></p>
			<pre class="source-code">
Instruction to Amazon Q:
Write a Glue ETL job.
Read the 's3://&lt;your bucket name&gt;/yellow_taxi_trip_records/yellow_tripdata_2023-01.parquet' file in a dataframe and display a sample of 10 records.
Read the 's3://&lt;your bucket name&gt;/zone_lookup/taxi+_zone_lookup.csv' file in a dataframe and display a sample of 10 records.
Perform a left outer join on 'yellow_tripdata_2023-01.parquet' and 'taxi+_zone_lookup.csv' on DOLocationID = LocationID to gather pick-up zone information.
Save the above dataset as a CSV file in above Amazon S3 bucket in a new folder 'glue_notebook_yellow_drop_off_zone_output'.</pre>			<div>
				<div class="IMG---Figure" id="_idContainer271">
					<img alt="Figure 14.17 – The AWS Glue ETL code suggested by Amazon Q Developer – part 1" src="image/B21378_14_17.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.17 – The AWS Glue ETL code suggested by Amazon Q Developer – part 1</p>
			<p>You can see <a id="_idIndexMarker604"/>that, based on the instruction provided to Amazon Q, it generated the skeleton on the ETL code. It generated code structure with Glue-PySpark libraries, a s3node with create dynamic dataframe to read parquet file, and a s3node with write dynamic dataframe to write <span class="No-Break">CSV file.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer272">
					<img alt="Figure 14.18 – AWS Glue ETL code suggested by Amazon Q Developer – part 2" src="image/B21378_14_18.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.18 – AWS Glue ETL code suggested by Amazon Q Developer – part 2</p>
			<p>Observe that Amazon Q also provided technical details to explain the script flow. This can also be used to meet the in-script <span class="No-Break">documentation needs.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer273">
					<img alt="Figure 14.19 – AWS Glue ETL code suggested by Amazon Q Developer – script summary" src="image/B21378_14_19.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.19 – AWS Glue ETL code suggested by Amazon Q Developer – script summary</p>
			<p>Data engineers with coding experience can easily reference the script summary and script skeleton to write end-to-end scripts to meet the solution blueprint. LLMs, by nature, are non-deterministic, so you may not get the same code blocks shown in the <span class="No-Break">code snapshots.</span></p>
			<p>Based on the<a id="_idIndexMarker605"/> preceding use case illustration, AWS Glue integration with Amazon Q Developer with prompting techniques can be used by data engineers at a relatively lower experience level, while AWS Glue integration with Amazon Q Developer using the chat assistant can be utilized by ETL developers with relatively <span class="No-Break">more experience.</span></p>
			<h3>Summary – Amazon Q Developer with an AWS Glue Studio notebook</h3>
			<p>As illustrated, we can automatically generate end-to-end, error-free, and executable code simply by providing prompts with specific requirements. Amazon Q Developer, integrated with an AWS Glue Studio notebook, comprehends the context and automatically generates PySpark code that can be run directly from the notebook without the need to provision any hardware upfront. This marks a significant advancement for many data engineers, relieving them from concerns about the technical intricacies associated with PySpark libraries, methods, <span class="No-Break">and syntax.</span></p>
			<p>Next, we will explore code assistance integration with <span class="No-Break">Amazon EMR.</span></p>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor219"/>Code assistance integration with Amazon EMR</h1>
			<p>Before we dive deep <a id="_idIndexMarker606"/>into the details of code assistance support for Amazon EMR, let’s quickly go through an overview of Amazon <a id="_idIndexMarker607"/>EMR. <strong class="bold">Amazon EMR</strong> is a cloud-based big data platform that simplifies the deployment, management, and scaling of various big data frameworks such as Apache <a id="_idIndexMarker608"/>Hadoop, Apache Spark, Apache Hive, and Apache HBase. At a high level, Amazon EMR comprises the following major components, each with multiple features to support data engineers and <span class="No-Break">data scientists:</span></p>
			<ul>
				<li><strong class="bold">EMR on EC2/EKS</strong>: The<a id="_idIndexMarker609"/> Amazon EMR service provides two options, EMR on EC2 and EMR on EKS, allowing customers to provision clusters. Amazon EMR streamlines the execution of batch jobs and interactive workloads for data analysts <span class="No-Break">and engineers.</span></li>
				<li><strong class="bold">EMR Serverless</strong>: Amazon <a id="_idIndexMarker610"/>EMR Serverless is a serverless alternative within Amazon EMR. With Amazon EMR Serverless, users can access the full suite of features and advantages offered by Amazon EMR, all without requiring specialized expertise for cluster planning <span class="No-Break">and management.</span></li>
				<li><strong class="bold">EMR Studio</strong>: EMR Studio<a id="_idIndexMarker611"/> supports data engineers and data scientists in developing, visualizing, and debugging applications within an IDE. It also provides a Jupyter Notebook environment for <span class="No-Break">interactive coding.</span></li>
			</ul>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor220"/>Use case for Amazon EMR Studio</h2>
			<p>For simplicity and ease of<a id="_idIndexMarker612"/> following Amazon Q Developer integration with Amazon EMR, we will use the same use case and data that we used in this chapter under the <em class="italic">Code assistance integration with AWS Glue</em> section. Refer to the <em class="italic">Use case for AWS Glue</em> section, which covers details related to the solution blueprint and <span class="No-Break">data preparation.</span></p>
			<h2 id="_idParaDest-222"><a id="_idTextAnchor221"/>Solution – Amazon Q Developer with Amazon EMR Studio</h2>
			<p>Let’s first enable Amazon Q Developer with Amazon EMR Studio. To enable Amazon Q Developer integration with Amazon EMR Studio, we will need to update the <span class="No-Break">IAM policy.</span></p>
			<h3>Prerequisite to enable Amazon Q Developer with Amazon EMR Studio</h3>
			<p>The developer is <a id="_idIndexMarker613"/>required to modify the IAM policy associated with the role to grant permissions for Amazon Q Developer to initiate recommendations in EMR Studio. Please reference <a href="B21378_02.xhtml#_idTextAnchor022"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> for additional details on initiating interaction with Amazon Q Developer in Amazon <span class="No-Break">EMR Studio.</span></p>
			<p>To fulfill the mentioned requirements, we will use various auto-code generation techniques that were discussed in <a href="B21378_03.xhtml#_idTextAnchor060"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>. Mainly, we will focus on single-line prompts, multi-line prompts, and chain-of-thought prompts for auto-code <span class="No-Break">generation techniques.</span></p>
			<p>Let’s use Amazon Q Developer to auto-generate end-to-end scripts, which can achieve the following requirements in Amazon EMR Studio. Here is a step-by-step solution walk-through of <span class="No-Break">the solution.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can observe lots of similarities between a Glue Studio notebook and an EMR Studio notebook when it comes to code recommended by Amazon <span class="No-Break">Q Developer.</span></p>
			<h3>Requirement 1</h3>
			<p>You will need<a id="_idIndexMarker614"/> to write a PySpark code to handle <span class="No-Break">technical requirements.</span></p>
			<p>Once you open Amazon EMR Studio, use <strong class="bold">Launcher</strong> to select <strong class="bold">PySpark</strong> from the <span class="No-Break"><strong class="bold">Notebook</strong></span><span class="No-Break"> section.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer274">
					<img alt="Figure 14.20 – Create an EMR Studio notebook with PySpark" src="image/B21378_14_20.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.20 – Create an EMR Studio notebook with PySpark</p>
			<p>Once you create the<a id="_idIndexMarker615"/> notebook, you can see a kernel named <strong class="source-inline">PySpark</strong>. The kernel is a standalone process that runs in the background and executes the code you write in your notebooks. For more information, refer to the <em class="italic">References</em> section at the end of <span class="No-Break">the chapter.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer275">
					<img alt="Figure 14.21 – The EMR Studio notebook with a PySpark kernel" src="image/B21378_14_21.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.21 – The EMR Studio notebook with a PySpark kernel</p>
			<p>I have already attached a cluster to my notebook, but you can explore different options to attach the compute to EMR studio in AWS documentation <span class="No-Break">at </span><a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-use-clusters.html"><span class="No-Break">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-use-clusters.html</span></a><span class="No-Break">.</span></p>
			<h3>Requirement 2</h3>
			<p>Read the <strong class="source-inline">yellow_tripdata_2023-01.parquet</strong> file from the S3 location in a DataFrame and display a sample of <span class="No-Break">10 records.</span></p>
			<p>Let’s use a chain-of-thought prompts technique with multiple single-line prompts in different cells to achieve <span class="No-Break">this requirement:</span></p>
			<pre class="source-code">
Prompt # 1:
# Read s3://&lt;your-bucket-name-here&gt;/yellow_taxi_trip_records/yellow_tripdata_2023-01.parquet file in a dataframe
Prompt # 2:
# Display a sample of 10 records from dataframe</pre>			<div>
				<div class="IMG---Figure" id="_idContainer276">
					<img alt="Figure 14.22 – PySpark code to read the Yellow Taxi Trip Records data using single-line prompts" src="image/B21378_14_22.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.22 – PySpark code to read the Yellow Taxi Trip Records data using single-line prompts</p>
			<p>Observe that upon entering the Amazon Q Developer-enabled EMR Studio notebook prompt, it initiates code recommendations. Amazon Q Developer recognizes the file format as Parquet and suggests using the <strong class="source-inline">spark.read.parquet</strong> method. You can directly execute each cell/code from the notebook. Furthermore, as you move to the next cell, Amazon Q Developer utilizes “line-by-line recommendations” to suggest displaying <span class="No-Break">the schema.</span></p>
			<h3>Requirement 3</h3>
			<p>Read the <strong class="source-inline">taxi+_zone_lookup.csv</strong> file from the S3 location in a DataFrame and display a <a id="_idIndexMarker616"/>sample of <span class="No-Break">10 records.</span></p>
			<p>We already explored the chain-of-thought prompts technique with multiple single-line prompts for <em class="italic">Requirement 2</em>. Now, let’s try a multi-line prompt to achieve this requirement and we will try to customize the code for the <span class="No-Break">DataFrame name:</span></p>
			<pre class="source-code">
Prompt:
"""
Read s3://&lt;your-bucket-name-here&gt;/zone_lookup/taxi+_zone_lookup.csv in a dataframe name zone_df. Show sample 10 records from zone_df.
"""</pre>			<div>
				<div class="IMG---Figure" id="_idContainer277">
					<img alt="Figure 14.23 – PySpark code to read the Zone Lookup file using a multi-line prompt" src="image/B21378_14_23.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.23 – PySpark code to read the Zone Lookup file using a multi-line prompt</p>
			<p>Observe that Amazon Q Developer understood the context behind the multi-line prompt and also the specific DataFrame name instructed in the prompt. It auto-generated multiple lines of code with the DataFrame name of <strong class="source-inline">zone_df</strong> and file format as CSV, suggesting the use of the <strong class="source-inline">spark.read.csv</strong> method to read CSV files. You can directly execute each cell/code from <span class="No-Break">the notebook.</span></p>
			<h3>Requirement 4</h3>
			<p>Perform a<a id="_idIndexMarker617"/> left outer join on <strong class="source-inline">yellow_tripdata_2023-01.parquet</strong> and <strong class="source-inline">taxi+_zone_lookup.csv</strong> on <strong class="source-inline">pulocationid = LocationID</strong> to gather pick-up <span class="No-Break">zone information.</span></p>
			<p>We will continue using multi-line prompts and some code customization to achieve the <span class="No-Break">preceding requirement:</span></p>
			<pre class="source-code">
Prompt:
"""
Perform a left outer join on dataframe df and dataframe zone_df on PULocationID = LocationID to save in dataframe name yellow_pu_zone_df.
Show sample 10 records from yellow_pu_zone_df and show schema.
"""</pre>			<div>
				<div class="IMG---Figure" id="_idContainer278">
					<img alt="Figure 14.24 – Left outer join df and dataframe zone_df – multi-line prompt" src="image/B21378_14_24.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.24 – Left outer join df and dataframe zone_df – multi-line prompt</p>
			<p>Now, let’s <a id="_idIndexMarker618"/>review the schema of the DataFrame printed by <span class="No-Break">the code.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer279">
					<img alt="Figure 14.25 – Left outer join df and dataframe zone_df – display schema" src="image/B21378_14_25.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.25 – Left outer join df and dataframe zone_df – display schema</p>
			<p>Observe that, as instructed in the multi-line prompt, Amazon Q Developer understood the context and auto-generated error-free code with the exact specifications we provided related to the DataFrame named <strong class="source-inline">yellow_pu_zone_df</strong>. You can directly execute each cell/code from <span class="No-Break">the notebook.</span></p>
			<h3>Requirement 5</h3>
			<p>Save the <a id="_idIndexMarker619"/>preceding dataset as a CSV file in the previous Amazon S3 bucket in a new folder <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">glue_notebook_yellow_pick_up_zone_output</strong></span><span class="No-Break">.</span></p>
			<p>Since this requirement is straightforward and can be encapsulated in a single sentence, we will use a single-line prompt to generate the code, and we will also include a header to facilitate <span class="No-Break">easy verification:</span></p>
			<pre class="source-code">
Prompt:
# Save dataframe yellow_pu_zone_df as CSV file at location s3://&lt;your-bucket-name-here&gt;/tlc-dataset-ny-taxi/glue_notebook_yellow_pick_up_zone_output/ with header information</pre>			<div>
				<div class="IMG---Figure" id="_idContainer280">
					<img alt="Figure 14.26 – Save the CSV file with enrichment pick-up location data" src="image/B21378_14_26.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.26 – Save the CSV file with enrichment pick-up location data</p>
			<h3>Requirement 6</h3>
			<p>For verification, download and check files from the <span class="No-Break"><strong class="source-inline">glue_notebook_yellow_pick_up_zone_output</strong></span><span class="No-Break"> folder.</span></p>
			<p> Let’s go to the<a id="_idIndexMarker620"/> Amazon S3 console to verify the files. Select one of the files and <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Download</strong></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer281">
					<img alt="Figure 14.27 – Verify final result set – Amazon Q Developer with Amazon EMR Studio" src="image/B21378_14_27.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.27 – Verify final result set – Amazon Q Developer with Amazon EMR Studio</p>
			<p>After downloading, you can use a text editor to review the <span class="No-Break">file contents.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer282">
					<img alt="Figure 14.28 – Verify the CSV file contents of Amazon Q Developer with Amazon EMR Studio" src="image/B21378_14_28.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.28 – Verify the CSV file contents of Amazon Q Developer with Amazon EMR Studio</p>
			<p>Observe that the CSV file has additional columns with zone information based on the pick-up <span class="No-Break">location ID.</span></p>
			<h3>Summary – Amazon Q Developer with Amazon EMR Studio</h3>
			<p>As illustrated, we can automatically generate end-to-end, error-free, and executable code simply by providing prompts with specific requirements. Amazon Q Developer, integrated with an Amazon EMR Studio notebook, comprehends the context and automatically generates PySpark code that can be run directly from the notebook. This marks a significant advancement for many data engineers, relieving them from concerns about the technical intricacies associated with PySpark libraries, methods, <span class="No-Break">and syntax.</span></p>
			<p class="callout-heading">Think challenge</p>
			<p class="callout">To fulfill <em class="italic">Requirement 6</em>, if you are interested, attempt to utilize the same EMR Studio notebook for reading a CSV file, displaying sample records, and adding <span class="No-Break">a header.</span></p>
			<p class="callout"><strong class="bold">Hint</strong>: Use the multi-line prompt technique, similar to the one we used when reading the Zone <span class="No-Break">Lookup file.</span></p>
			<p>In the next section, we will consider an application developer persona to explore code assistance integration with <span class="No-Break">AWS Lambda.</span></p>
			<h1 id="_idParaDest-223"><a id="_idTextAnchor222"/>Code assistance integration with AWS Lambda</h1>
			<p>Before we start diving deep into code assistance support for the AWS Lambda service, let’s quickly go through an overview of<a id="_idIndexMarker621"/> AWS Lambda. <strong class="bold">AWS Lambda</strong> is a serverless computing service that allows users to run code without provisioning or managing servers. With Lambda, you can upload your code or use the available editor from the Lambda console. During the runtime of the code, based on the provided configurations, the service automatically takes care of the compute resources needed for execution. It is designed to be highly scalable, cost effective, and suitable for <span class="No-Break">event-driven applications.</span></p>
			<p>AWS Lambda <a id="_idIndexMarker622"/>supports multiple programming <a id="_idIndexMarker623"/>languages, including Node.js, Python, Java, Go, and .NET Core, allowing you to choose the language that best fits your application. Lambda can be easily integrated with other AWS services, enabling you to build complex and scalable architectures. It works seamlessly with services such as Amazon S3, DynamoDB, and <span class="No-Break">API Gateway.</span></p>
			<p>The AWS Lambda console is integrated with Amazon Q Developer to make it easy for developers to get <span class="No-Break">coding assistance/recommendations.</span></p>
			<h2 id="_idParaDest-224"><a id="_idTextAnchor223"/>Use case for AWS Lambda</h2>
			<p>Let’s start<a id="_idIndexMarker624"/> with one of the easy and widely used use cases of converting <span class="No-Break">file format.</span></p>
			<p><strong class="bold">File format conversion</strong>: In a typical scenario, once a file is received from an external team and/or source, it may not be in the target location and have the required name expected by the application. In that case, AWS Lambda can be used to quickly copy the file from the source location to the target location and rename the file at the <span class="No-Break">target location.</span></p>
			<p>To illustrate this use case, let’s copy the NY Taxi Zone lookup file from the source location (<strong class="source-inline">s3://&lt;your-bucket-name&gt;/zone_lookup/</strong>) to the target location (<strong class="source-inline">s3://&lt;your-bucket-name&gt;/source_lookup_file/</strong>). Also, remove the special character (<strong class="source-inline">+</strong>) from the filename to save it <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">taxi_zone_lookup.csv</strong></span><span class="No-Break">.</span></p>
			<p>To meet this requirement, application developers must develop a Python script. This script should copy and rename the Zone Lookup file from the source to the <span class="No-Break">target location.</span></p>
			<p>As a code developer / data engineer, you will need to convert the preceding business objectives into the <span class="No-Break">solution blueprint.</span></p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor224"/>Solution blueprint</h2>
			<ol>
				<li>Write a <a id="_idIndexMarker625"/>Python script to handle <span class="No-Break">technical requirements.</span></li>
				<li>Copy the <strong class="source-inline">taxi+_zone_lookup.csv</strong> file from S3 to the <strong class="source-inline">zone_lookup</strong> folder to the <span class="No-Break"><strong class="source-inline">source_lookup_file</strong></span><span class="No-Break"> folder.</span></li>
				<li>During copying, change <strong class="source-inline">taxi+_zone_lookup.csv</strong> to <strong class="source-inline">taxi_zone_lookup.csv</strong> in the target <span class="No-Break"><strong class="source-inline">source_lookup_file</strong></span><span class="No-Break"> folder.</span></li>
				<li>For verification, check the contents of the <span class="No-Break"><strong class="source-inline">source_lookup_file/taxi_zone_lookup.csv</strong></span><span class="No-Break"> file.</span></li>
			</ol>
			<p>Now that we have a use case defined, let’s go through the step-by-step solution <span class="No-Break">for it.</span></p>
			<h2 id="_idParaDest-226"><a id="_idTextAnchor225"/>Data preparation</h2>
			<p>We are using the same lookup file that we provisioned in this chapter under the <em class="italic">Code assistance integration with AWS Glue</em> section. Please refer to the <em class="italic">Use case for AWS Glue</em> section, which covers details related to <span class="No-Break">data preparation.</span></p>
			<h2 id="_idParaDest-227"><a id="_idTextAnchor226"/>Solution – Amazon Q Developer with AWS Lambda</h2>
			<p>Let’s first enable Amazon Q Developer with the AWS Lambda console. To enable Amazon Q Developer integration with AWS Lambda, we will need to update the <span class="No-Break">IAM policy.</span></p>
			<h3>Prerequisite to enable Amazon Q Developer with AWS Lambda</h3>
			<p>The developer is <a id="_idIndexMarker626"/>required to modify the IAM policy associated with the IAM user or role to grant permissions for Amazon Q Developer to initiate recommendations in the AWS Lambda console. Please reference <a href="B21378_02.xhtml#_idTextAnchor022"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> for additional details on initiating interaction with Amazon Q Developer in <span class="No-Break">AWS Lambda.</span></p>
			<p>To let Amazon Q Developer start code suggestions, make sure to choose <strong class="bold">Tools</strong> | <strong class="bold">Amazon CodeWhisperer </strong><span class="No-Break"><strong class="bold">Code Suggestions</strong></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer283">
					<img alt="Figure 14.29 – AWS Lambda console with Amazon Q Developer for Python runtime" src="image/B21378_14_29.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.29 – AWS Lambda console with Amazon Q Developer for Python runtime</p>
			<p>To fulfill<a id="_idIndexMarker627"/> the mentioned requirements, we will use auto-code generation techniques that were discussed in <a href="B21378_03.xhtml#_idTextAnchor060"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>. Mainly, we will focus on the multi-line prompt for auto-code generation. Let’s use Amazon Q Developer to auto-generate an end-to-end script that can achieve the following requirements in AWS Lambda Console and EMR Studio. Here is a step-by-step solution walk-through of <span class="No-Break">the solution.</span></p>
			<h3>Requirement 1</h3>
			<p>You need to write a Python script to handle <span class="No-Break">technical requirements.</span></p>
			<p>Once you open the AWS Lambda console, use the launcher to select a <span class="No-Break">Python runtime.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer284">
					<img alt="Figure 14.30 – Create a Python runtime from AWS Lambda" src="image/B21378_14_30.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.30 – Create a Python runtime from AWS Lambda</p>
			<p>Once you <a id="_idIndexMarker628"/>successfully create a Lambda function, observe that AWS Lambda creates a <strong class="source-inline">lambda_function.py</strong> file with some sample code. We can safely delete the sample code for this exercise, as we will use Amazon Q Developer to generate <span class="No-Break">end-to-end code.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer285">
					<img alt="Figure 14.31 – The AWS Lambda console with Amazon Q Developer for Python runtime" src="image/B21378_14_31.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.31 – The AWS Lambda console with Amazon Q Developer for Python runtime</p>
			<p>Let’s combine <em class="italic">Requirements 2</em> and <em class="italic">3</em>, as we are planning to use a <span class="No-Break">multi-line prompt.</span></p>
			<h3>Requirements 2 and 3</h3>
			<p>Copy the <strong class="source-inline">taxi+_zone_lookup.csv</strong> file from S3 to the <strong class="source-inline">zone_lookup</strong> folder to the <span class="No-Break"><strong class="source-inline">source_lookup_file</strong></span><span class="No-Break"> folder.</span></p>
			<p>During copying, change the<a id="_idIndexMarker629"/> file name from <strong class="source-inline">taxi+_zone_lookup.csv</strong> to <strong class="source-inline">taxi_zone_lookup.csv</strong> in the target <span class="No-Break"><strong class="source-inline">source_lookup_file</strong></span><span class="No-Break"> folder.</span></p>
			<p>Let’s use multi-line prompts to auto-generate <span class="No-Break">the code:</span></p>
			<pre class="source-code">
Prompt:
"""
write a lambda function.
copy s3://&lt;your-bucket-name&gt;/zone_lookup/taxi+_zone_lookup.csv
as s3://&lt;your-bucket-name&gt;/source_lookup_file/taxi_zone_lookup.csv
"""</pre>			<div>
				<div class="IMG---Figure" id="_idContainer286">
					<img alt="Figure 14.32 – Amazon Q Developer generated code for the AWS Lambda console" src="image/B21378_14_32.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.32 – Amazon Q Developer generated code for the AWS Lambda console</p>
			<p>Observe that Amazon Q Developer created a <strong class="source-inline">lambda_handler</strong> function and added <strong class="source-inline">return code of 200</strong> with a <span class="No-Break">success message.</span></p>
			<h3>Requirement 4</h3>
			<p>For verification, check<a id="_idIndexMarker630"/> the contents of the <span class="No-Break"><strong class="source-inline">source_lookup_file/taxi_zone_lookup.csv</strong></span><span class="No-Break"> file.</span></p>
			<p>Let’s deploy and use a test event to run Lambda code generated by Amazon <span class="No-Break">Q Developer.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer287">
					<img alt="Figure 14.33 – Deploy AWS Lambda code" src="image/B21378_14_33.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.33 – Deploy AWS Lambda code</p>
			<p>Now, let’s test the code by going to the <strong class="bold">Test</strong> tab and clicking the <strong class="bold">Test</strong> button. Since we are not passing any values to this Lambda function, the JSON event values from the <strong class="bold">Test</strong> tab do not matter in <span class="No-Break">our case.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer288">
					<img alt="Figure 14.34 – Test AWS Lambda code" src="image/B21378_14_34.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.34 – Test AWS Lambda code</p>
			<p>Once the Lambda code executes successfully, it will provide you with the details of the execution. Observe that the code is executed successfully and displays the returned code with a <span class="No-Break">success message.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer289">
					<img alt="Figure 14.35 – AWS Lambda code execution" src="image/B21378_14_35.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.35 – AWS Lambda code execution</p>
			<p>Let’s use the <a id="_idIndexMarker631"/>Amazon S3 console to download and <span class="No-Break">verify </span><span class="No-Break"><strong class="source-inline">s3://&lt;your-bucket-name&gt;/source_lookup_file/taxi_zone_lookup.csv</strong></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer290">
					<img alt="Figure 14.36 – Target lookup file from Amazon S3" src="image/B21378_14_36.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.36 – Target lookup file from Amazon S3</p>
			<div>
				<div class="IMG---Figure" id="_idContainer291">
					<img alt="Figure 14.37 – The Zone Lookup file" src="image/B21378_14_37.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.37 – The Zone Lookup file</p>
			<h3>Summary – Amazon Q Developer with AWS Lambda</h3>
			<p>As illustrated, we can automatically generate end-to-end, error-free, and executable code simply by providing prompts with specific requirements. Amazon Q Developer, integrated with AWS Lambda, automatically generates the <strong class="source-inline">lambda_handle</strong><strong class="source-inline">r</strong> <strong class="source-inline">()</strong> function with return code based on the Lambda runtime environment selected. This integration can assist application developers with relatively limited coding experience in automatically generating Lambda functions with minor to no <span class="No-Break">code changes.</span></p>
			<p>Continuing with the application developer persona, next, we will explore the data scientist persona to investigate code assistance integration with <span class="No-Break">Amazon SageMaker.</span></p>
			<h1 id="_idParaDest-228"><a id="_idTextAnchor227"/>Code assistance integration with Amazon SageMaker</h1>
			<p>Before we start diving deep into code assistance support for the Amazon SageMaker service, let’s quickly go through an overview of<a id="_idIndexMarker632"/> Amazon SageMaker. <strong class="bold">Amazon SageMaker</strong> is a fully managed service that simplifies the process of building, training, and deploying ML models at scale. It is designed to make it easier for developers and data scientists to build, train, and deploy ML models without the need for extensive<a id="_idIndexMarker633"/> expertise in ML or<a id="_idIndexMarker634"/> deep learning. It has multiple features such as end-to-end workflow, built-in algorithms, custom model training, automatic model tuning, ground truth, edge manager, augmented AI, and managed notebooks, just to name a few. Amazon SageMaker integrates with other AWS services, such as Amazon S3 for data storage, AWS Lambda for serverless inference, and Amazon CloudWatch <span class="No-Break">for monitoring.</span></p>
			<p>Amazon SageMaker Studio hosts the managed notebooks, which are integrated with Amazon <span class="No-Break">Q Developer.</span></p>
			<h2 id="_idParaDest-229"><a id="_idTextAnchor228"/>Use case for Amazon SageMaker</h2>
			<p>Let’s use a very common business use case related to churn prediction for which data scientists use the <span class="No-Break">XGBoost algorithm.</span></p>
			<p><strong class="bold">Churn prediction</strong> in <a id="_idIndexMarker635"/>business involves utilizing data and algorithms to forecast which customers are at risk of discontinuing their usage of a product or service. The term “churn” commonly denotes customers ending subscriptions, discontinuing purchases, or ceasing service utilization. The primary objective of churn prediction is to identify these customers before they churn, enabling businesses to implement proactive measures for <span class="No-Break">customer retention.</span></p>
			<p>We will use publicly available direct marketing bank data to illustrate the support provided by Amazon Q Developer for milestone steps such as data collection, feature engineering, model training, and model deployment using <span class="No-Break">Amazon SageMaker.</span></p>
			<p>Typically, data scientists need to write a complex script to carry out all of the preceding milestone steps from an Amazon SageMaker <span class="No-Break">Studio notebook.</span></p>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor229"/>Solution blueprint</h2>
			<ol>
				<li>Set up an environment <a id="_idIndexMarker636"/>with the required set <span class="No-Break">of libraries.</span></li>
				<li><strong class="bold">Data collection</strong>: Download and unzip direct marketing bank data <span class="No-Break">from </span><a href="https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip"><span class="No-Break">https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip</span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">Feature engineering</strong>: To demonstrate the functionality, we will carry out the following commonly used feature <span class="No-Break">engineering steps:</span><ul><li>Manipulate column data using <span class="No-Break">default values</span></li><li>Drop <span class="No-Break">extra columns</span></li><li>Carry out <span class="No-Break">one-hot encoding</span></li></ul></li>
				<li><strong class="bold">Model training</strong>: Let’s use the <span class="No-Break">XGBoost algorithm:</span><ul><li>Rearrange data to create training, validation, and <span class="No-Break">test datasets/files</span></li><li>Use XGBoost algorithms to train the model using the <span class="No-Break">training dataset</span></li></ul></li>
				<li><strong class="bold">Model deployment</strong>: Deploy<a id="_idIndexMarker637"/> the model as an endpoint to <span class="No-Break">allow inferences.</span></li>
			</ol>
			<p>In the preceding solution blueprint, we illustrate the integration of Amazon Q Developer with Amazon SageMaker by handling commonly used milestone steps. However, based on the complexity of your data and enterprise needs, there might be additional <span class="No-Break">steps required.</span></p>
			<h2 id="_idParaDest-231"><a id="_idTextAnchor230"/>Data preparation</h2>
			<p>We will utilize an AWS dataset <a id="_idIndexMarker638"/>publicly hosted for direct marketing bank data. The complete dataset is available at <a href="https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip">https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip</a>. All the data preparation steps will be conducted in the SageMaker Studio notebook as part of the data <span class="No-Break">collection requirement.</span></p>
			<h2 id="_idParaDest-232"><a id="_idTextAnchor231"/>Solution – Amazon Q with Amazon SageMaker Studio</h2>
			<p>Let’s first enable Amazon Q Developer with Amazon SageMaker Studio. The following prerequisites are needed to allow Amazon Q Developer to auto-generate code inside Amazon <span class="No-Break">SageMaker studio.</span></p>
			<h3>Prerequisite to enable Amazon Q Developer with Amazon SageMaker Studio</h3>
			<p>The developer is required to<a id="_idIndexMarker639"/> modify the IAM policy associated with the IAM user or role to grant permissions for Amazon Q Developer to initiate recommendations in for Amazon SageMaker Studio notebook. Refer <a href="B21378_02.xhtml#_idTextAnchor022"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> for the details to enable Amazon Q Developer with Amazon SageMaker <span class="No-Break">Studio notebook.</span></p>
			<p>Once the Amazon Q Developer is activated for Amazon SageMaker Studio notebook, select <strong class="bold">Create notebook</strong> from the <strong class="bold">Launcher</strong> to verify that Amazon Q Developer <span class="No-Break">is enabled.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer292">
					<img alt="Figure 14.38 –An Amazon Q Developer-enabled notebook from SageMaker Studio" src="image/B21378_14_38.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.38 –An Amazon Q Developer-enabled notebook from SageMaker Studio</p>
			<p>To fulfill the <a id="_idIndexMarker640"/>mentioned requirements, we will use auto-code generation techniques that were discussed in <a href="B21378_04.xhtml#_idTextAnchor081"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>. Mainly, we will focus on single-line prompts, multi-line prompts, and chain-of-thought prompts for auto-code <span class="No-Break">generation techniques.</span></p>
			<h3>Requirement 1</h3>
			<p>Set up an environment with the required set <span class="No-Break">of libs.</span></p>
			<p>Let’s use <span class="No-Break">single-line prompts:</span></p>
			<pre class="source-code">
Prompt 1:
# Fetch this data by importing the SageMaker library
Prompt 2:
# Defining global variables BUCKET and ROLE that point to the bucket associated with the Domain and it's execution role</pre>			<div>
				<div class="IMG---Figure" id="_idContainer293">
					<img alt="Figure 14.39 –Amazon Q Developer – SageMaker Studio setup environment" src="image/B21378_14_39.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.39 –Amazon Q Developer – SageMaker Studio setup environment</p>
			<p>Observe that, based on our prompts, Amazon Q Developer generated code with a default set of libraries and variables. However, based on your needs, and account setup, you may need to<a id="_idIndexMarker641"/> update/add <span class="No-Break">the code.</span></p>
			<h3>Requirement 2</h3>
			<p>For data collection, download and unzip direct marketing bank data <span class="No-Break">from </span><a href="https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip"><span class="No-Break">https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip</span></a><span class="No-Break">.</span></p>
			<p>We will focus on multi-line prompts to achieve <span class="No-Break">this requirement:</span></p>
			<pre class="source-code">
Prompt:
'''Using the requests library download the ZIP file from
the url "https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip"
and save it to current directory and unzip the archive
'''</pre>			<div>
				<div class="IMG---Figure" id="_idContainer294">
					<img alt="Figure 14.40 – Amazon Q Developer – SageMaker Studio data collection" src="image/B21378_14_40.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.40 – Amazon Q Developer – SageMaker Studio data collection</p>
			<h3>Requirement 3</h3>
			<p>To demonstrate the<a id="_idIndexMarker642"/> functionality of feature engineering, we will carry out the following commonly used feature engineering steps, which will help us improve the <span class="No-Break">model accuracy:</span></p>
			<ol>
				<li>Manipulate column data using <span class="No-Break">default values.</span></li>
				<li>Drop <span class="No-Break">extra columns.</span></li>
				<li>Carry out <span class="No-Break">one-hot encoding.</span></li>
			</ol>
			<p>We will focus on multi-line prompts to achieve <span class="No-Break">this requirement:</span></p>
			<pre class="source-code">
Prompt #1:
'''
Create a new dataframe with column no_previous_contact and populates from existing dataframe column pdays using numpy when the condition equals to 999, 1, 0 and show the table
'''
Prompt # 2:
# do one hot encoding for full_data
Prompt # 3:
'''
Drop the columns 'duration', emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m' and 'nr.employed'
from the dataframe and create a new dataframe with name model_data
'''</pre>			<p>We get the <a id="_idIndexMarker643"/><span class="No-Break">following screen.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer295">
					<img alt="Figure 14.41 –Amazon Q Developer – SageMaker Studio feature engineering" src="image/B21378_14_41.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.41 –Amazon Q Developer – SageMaker Studio feature engineering</p>
			<p>Now, let’s proceed with <a id="_idIndexMarker644"/>the generated code for the model training, testing, <span class="No-Break">and validation:</span></p>
			<pre class="source-code">
Prompt # 4:
#split model_data for train, validation, and test
Prompt # 5:
'''
for train_data move y_yes as first column.
Drop y_no and y_yes columns from train_data.
save file as train.csv
'''
Prompt # 6:
'''
for validation_data move y_yes as first column.
Drop y_no and y_yes columns from validation_data.
save file as validation.csv
'''</pre>			<div>
				<div class="IMG---Figure" id="_idContainer296">
					<img alt="Figure 14.42 –Amazon Q Developer – SageMaker Studio feature engineering" src="image/B21378_14_42.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.42 –Amazon Q Developer – SageMaker Studio feature engineering</p>
			<p>Note that for <a id="_idIndexMarker645"/>both single-line and multi-line prompts, we needed to provide much more specific details to generate the code <span class="No-Break">as expected.</span></p>
			<h3>Requirement 4</h3>
			<p><strong class="bold">Model training</strong>: Let’s use the <span class="No-Break">XGBoost algorithm:</span></p>
			<ul>
				<li>Rearrange data to create training, validation, and <span class="No-Break">test datasets/files</span></li>
				<li>Use XGBOOST algorithms to train the model using a <span class="No-Break">training dataset</span></li>
			</ul>
			<p>We will focus <a id="_idIndexMarker646"/>on multi-line prompts to achieve this requirement to start the model <span class="No-Break">training activity:</span></p>
			<pre class="source-code">
Prompt #1:
''' upload train.csv to S3 Bucket train/train.csv prefix.
upload validation.csv to S3 Bucket validation/validation.csv prefix '''
Prompt #2:
# pull latest xgboost model as a CONTAINER
Prompt #3:
# create TrainingInput from s3 train/train.csv and validation/validation.csv
Prompt #3:
# create training job with hyper paramers max_depth=5, eta=0.2, gamma=4, min_child_weight=6, subsample=0.8, objective='binary:logistic', num_round=100</pre>			<div>
				<div class="IMG---Figure" id="_idContainer297">
					<img alt="Figure 14.43 – Amazon Q Developer – SageMaker Studio model training" src="image/B21378_14_43.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.43 – Amazon Q Developer – SageMaker Studio model training</p>
			<h3>Requirement 5</h3>
			<p>Deploy the<a id="_idIndexMarker647"/> model as an endpoint to <span class="No-Break">allow inferences.</span></p>
			<p>We will focus on single-line prompts to achieve <span class="No-Break">this requirement:</span></p>
			<pre class="source-code">
Prompt #1:
# Deploy a model that's hosted behind a real-time endpoint</pre>			<div>
				<div class="IMG---Figure" id="_idContainer298">
					<img alt="Figure 14.44 – Amazon Q Developer -SageMaker studio model training" src="image/B21378_14_44.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.44 – Amazon Q Developer -SageMaker studio model training</p>
			<p>Observe that Amazon Q Developer uses a default configuration for <strong class="source-inline">instance_type</strong> and <strong class="source-inline">initial_instance_count</strong>. You can check the hosted model from the Amazon SageMaker console by clicking the <strong class="bold">Inference</strong> dropdown and selecting the <span class="No-Break"><strong class="bold">Endpoints</strong></span><span class="No-Break"> option.</span></p>
			<p>In the preceding <a id="_idIndexMarker648"/>examples, we extensively used inline prompts with single-line prompting, multi-line prompting, and chain of thought prompting techniques. If you wish to use a chat-style interface, you can leverage the Amazon Q Developer chat-style interface, as shown in the <span class="No-Break">following screenshot.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer299">
					<img alt="Figure 14.45 –Amazon Q Developer -SageMaker studio chat style interface" src="image/B21378_14_45.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.45 –Amazon Q Developer -SageMaker studio chat style interface</p>
			<h3>Summary – Amazon Q Developer with Amazon SageMaker</h3>
			<p>As demonstrated, Amazon Q Developer seamlessly integrated with the Amazon SageMaker Studio notebook IDE, enables the automatic generation of end-to-end, error-free, and executable code. By supplying prompts with specific requirements, Q Developer can auto-generate code for essential milestone steps, including data collection, feature engineering, model training, and model deployment within the SageMaker <span class="No-Break">Studio notebook.</span></p>
			<p>While data scientists can utilize this integration to produce code blocks, customization may be necessary. Specific details must be provided in prompts to tailor the code. In some instances, adjustments may be required to align with enterprise standards, business requirements, and configurations. Users should possess expertise in prompt engineering, familiarity with scripting, and conduct thorough testing to ensure the scripts meet business requirements before deploying them <span class="No-Break">into production.</span></p>
			<p>Now, let’s dive deep to see how data analysts can use code assistance while working with <span class="No-Break">Amazon Redshift.</span></p>
			<h1 id="_idParaDest-233"><a id="_idTextAnchor232"/>Code assistance integration with Amazon Redshift</h1>
			<p>Before we start diving deep into code assistance support for the Amazon Redshift service, let’s quickly go through an overview of AWS Redshift. <strong class="bold">Amazon Redshift</strong> is an AI-powered, fully managed, cloud-based data warehouse <a id="_idIndexMarker649"/>service. It is designed for high-performance analysis and the processing of large datasets using standard <span class="No-Break">SQL queries.</span></p>
			<p>Amazon Redshift is <a id="_idIndexMarker650"/>optimized for data warehousing, providing<a id="_idIndexMarker651"/> a fast and scalable solution for processing and analyzing large volumes of structured data. It uses columnar storage <a id="_idIndexMarker652"/>and <strong class="bold">massively parallel processing</strong> (<strong class="bold">MPP</strong>) architecture, distributing data and queries across multiple nodes to deliver high performance for complex queries. This architecture allows it to easily scale from a few hundred gigabytes to petabytes of data, enabling organizations to grow their data warehouse as their needs evolve. It integrates with various data sources, allowing you to load data from multiple sources, including Amazon S3, Amazon DynamoDB, and <span class="No-Break">Amazon EMR.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">To query the data, Amazon Redshift also provides a query editor. The Redshift query editor v2 has two modes to interact with databases: <strong class="bold">Editor</strong> and <strong class="bold">Notebook</strong>. Code assistance is integrated with the Notebook mode of the Redshift query <span class="No-Break">editor v2.</span></p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor233"/>Use case for Amazon Redshift</h2>
			<p>Let’s start with one of the easy and widely used use cases of converting <span class="No-Break">file format.</span></p>
			<p><strong class="bold">Identifying top performers</strong>: In a<a id="_idIndexMarker653"/> typical business use case, analysts are interested in identifying top performers based on <span class="No-Break">certain criteria.</span></p>
			<p>To illustrate this use case, we will be using the publicly available <strong class="source-inline">tickit</strong> database, which is readily available with Amazon Redshift. For more information about the <strong class="source-inline">tickit</strong> database, refer to the <em class="italic">References</em> section at the end of <span class="No-Break">the chapter.</span></p>
			<p>Analysts want to identify the top state where most of the <span class="No-Break">venues are.</span></p>
			<p>To meet this requirement, analyst developers must develop SQL queries to interact with different tables from the <span class="No-Break"><strong class="source-inline">tickit</strong></span><span class="No-Break"> database.</span></p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor234"/>Solution blueprint</h2>
			<p>As we are considering the <a id="_idIndexMarker654"/>data analyst persona and using code assistance to generate the code, we do not need to further break down the business ask into the solution blueprint. This makes it easy for analysts to interact with databases without getting involved in table structures and <span class="No-Break">relationship details:</span></p>
			<ul>
				<li>Write SQL to identify the top state where most of the <span class="No-Break">venues are</span></li>
			</ul>
			<h2 id="_idParaDest-236"><a id="_idTextAnchor235"/>Data preparation</h2>
			<p>We will be using the publicly<a id="_idIndexMarker655"/> available <strong class="source-inline">tickit</strong> database, which comes with Amazon Redshift. Let’s import the data using Redshift query <span class="No-Break">editor v2:</span></p>
			<ol>
				<li>Connect to your Amazon Redshift cluster or Serverless endpoint from Redshift query <span class="No-Break">editor 2.</span></li>
				<li>Then, choose <strong class="source-inline">sample_data_dev</strong> and <a id="_idIndexMarker656"/>click <span class="No-Break">on </span><span class="No-Break"><strong class="source-inline">tickit</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer300">
					<img alt="Figure 14.46 – Import the tickit database using Amazon Redshift" src="image/B21378_14_46.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.46 – Import the tickit database using Amazon Redshift</p>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor236"/>Solution – Amazon Q with Amazon Redshift</h2>
			<p>Let’s first enable Amazon Q with Amazon Redshift. To allow Amazon Q to generate SQL inside Amazon Redshift, the admin needs to enable the <strong class="bold">Generative SQL</strong> option inside <strong class="bold">Notebook</strong> of Redshift query editor v2. Please reference <a href="B21378_03.xhtml#_idTextAnchor060"><span class="No-Break"><em class="italic">Chapter 3</em></span></a> for additional details on initiating interaction with Amazon Q in <span class="No-Break">Amazon Redshift.</span></p>
			<h3>Prerequisite to enable Amazon Q with Amazon Redshift</h3>
			<p>Let’s walk through<a id="_idIndexMarker657"/> the steps needed to enable the <strong class="bold">Generative SQL</strong> option inside <strong class="bold">Notebook</strong> of the Redshift query <span class="No-Break">editor v2.</span></p>
			<ol>
				<li>Log in with admin privileges to connect to your Amazon Redshift cluster or <span class="No-Break">Serverless endpoint.</span></li>
				<li><span class="No-Break">Choose </span><span class="No-Break"><strong class="bold">Notebook</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer301">
					<img alt="Figure 14.47 – Notebook using Redshift query editor v2" src="image/B21378_14_47.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.47 – Notebook using Redshift query editor v2</p>
			<ol>
				<li value="3">Choose <strong class="bold">Generative SQL</strong>, then <a id="_idIndexMarker658"/>check the <strong class="bold">Generative SQL</strong> box, and <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Save</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer302">
					<img alt="Figure 14.48 – Enable Generative SQL using Redshift query editor v2" src="image/B21378_14_48.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.48 – Enable Generative SQL using Redshift query editor v2</p>
			<p>To fulfill the mentioned requirements, we will use auto-code generation techniques that were discussed in <a href="B21378_04.xhtml#_idTextAnchor081"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>. Mainly, we will focus on the chat companion for <span class="No-Break">auto-code generation.</span></p>
			<h3>Requirement 1</h3>
			<p>Write SQL to identify the top state where most of the <span class="No-Break">venues are.</span></p>
			<p>Use Amazon Q’s interactive session to ask the <span class="No-Break">following question:</span></p>
			<pre class="source-code">
Q:Which state has most venues?</pre>			<p>Observe that we did not provide database details to the Amazon Q Developer, but it was still able to identify the required table, <strong class="source-inline">tickit.venue</strong>. It generated the fully executable end-to-end query with <strong class="source-inline">Group by</strong>, <strong class="source-inline">Order by</strong>, and <strong class="source-inline">Limit</strong> to meet the requirements. To make it easy for analysts to run the queries, code assistance is integrated with the notebook. Just by clicking <strong class="bold">Add to notebook</strong>, the SQL code will be available in a notebook <a id="_idIndexMarker659"/>cell that users can <span class="No-Break">run directly.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer303">
					<img alt="Figure 14.49 – Interact with code assistance from Amazon Redshift" src="image/B21378_14_49.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.49 – Interact with code assistance from Amazon Redshift</p>
			<h3>Summary – Amazon Q with Amazon Redshift</h3>
			<p>As demonstrated, we can effortlessly generate end-to-end, error-free, and executable SQL by interacting with Amazon Q through a chat-style interface. Amazon Q seamlessly integrates with notebooks in the Amazon Redshift query editor v2. Users are not required to provide database and/or table details to the code assistant. It autonomously identifies the necessary tables and generates SQL code to fulfill the specified requirements in the prompt. Furthermore, to facilitate analysts in running queries, it is directly integrated with the notebook. Amazon Q, in conjunction with Amazon Redshift, proves to be a valuable asset for data analysts. In many cases, data analysts do not need to translate business requirements into technical steps. They can leverage the auto-generate SQL feature, bypassing the need to delve deep into database and <span class="No-Break">table details.</span></p>
			<h1 id="_idParaDest-238"><a id="_idTextAnchor237"/>Summary</h1>
			<p>In this chapter, we initially covered the integration of different AWS services with code companions to assist users in auto-code generation. Then, we explored the integration of Amazon Q Developer with some of the core services, such as AWS Glue, Amazon EMR, AWS Lambda, Amazon Redshift, and Amazon SageMaker, commonly used by application developers, data engineers, and <span class="No-Break">data scientists.</span></p>
			<p>We then discussed, in the prerequisites, the in-depth integration with sample common use cases and corresponding solution walk-throughs for <span class="No-Break">various integrations.</span></p>
			<p>AWS Glue integration with Amazon Q Developer, aiding data engineers in generating and executing ETL scripts using the AWS Glue Studio notebook environment. This includes a skeletal outline of a full end-to-end Glue ETL job using AWS <span class="No-Break">Glue Studio.</span></p>
			<p>AWS EMR integration with Amazon Q Developer to assist data engineers in generating and executing ETL scripts using the AWS EMR Studio <span class="No-Break">notebook environment.</span></p>
			<p>AWS Lambda console IDE integration with Amazon Q Developer, supporting application engineers in generating and executing end-to-end Python-based applications for <span class="No-Break">file movement.</span></p>
			<p>Amazon SageMaker studio notebook integration with Amazon Q Developer to help data scientists achieve major milestone steps in data collection, feature engineering, model training, and model deployment using different <span class="No-Break">prompting techniques.</span></p>
			<p>Amazon Redshift integration with Amazon Q to aid business analysts in generating SQL queries by simply providing business requirements. Users are not required to provide database and/or table details to the <span class="No-Break">code assistant.</span></p>
			<p>In the next chapter, we will look at how you can use Amazon Q Developer to get AWS-specific guidance and recommendations, either from the AWS console or from the documentation on a variety of topics such as architecture and best <span class="No-Break">practices support.</span></p>
			<h1 id="_idParaDest-239"><a id="_idTextAnchor238"/>References</h1>
			<ul>
				<li>AWS Prescriptive Guidance - Data <span class="No-Break">engineering: </span><a href="https://docs.aws.amazon.com/prescriptive-guidance/latest/aws-caf-platform-perspective/data-eng.html"><span class="No-Break">https://docs.aws.amazon.com/prescriptive-guidance/latest/aws-caf-platform-perspective/data-eng.html</span></a><a href="https://docs.aws.amazon.com/prescriptive-guidance/latest/aws-caf-platform-perspective/data-eng.html&#13;"/></li>
				<li>Jupyter <span class="No-Break">kernel: </span><a href="https://docs.jupyter.org/en/latest/projects/kernels.html"><span class="No-Break">https://docs.jupyter.org/en/latest/projects/kernels.html</span></a><a href="https://docs.jupyter.org/en/latest/projects/kernels.html&#13;"/></li>
				<li>Amazon Q Developer with AWS Glue <span class="No-Break">Studio: </span><a href="https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/glue-setup.html"><span class="No-Break">https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/glue-setup.html</span></a></li>
				<li>TLC Trip Record <span class="No-Break">Data: </span><a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"><span class="No-Break">https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page</span></a><a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page&#13;"/></li>
				<li>Setting up Amazon Q data integration in AWS <span class="No-Break">Glue: </span><a href="https://docs.aws.amazon.com/glue/latest/dg/q-setting-up.html"><span class="No-Break">https://docs.aws.amazon.com/glue/latest/dg/q-setting-up.html</span></a><a href="https://docs.aws.amazon.com/glue/latest/dg/q-setting-up.html&#13;"/></li>
				<li>Setting up Amazon Q Developer data integration in Amazon <span class="No-Break">EMR: </span><a href="https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/emr-setup.html"><span class="No-Break">https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/emr-setup.html</span></a></li>
				<li>Attach a compute to an EMR Studio <span class="No-Break">Workspace: </span><a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-use-clusters.html"><span class="No-Break">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-use-clusters.html</span></a><a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-use-clusters.html&#13;"/></li>
				<li>Using Amazon Q Developer with AWS <span class="No-Break">Lambda: </span><a href="https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/lambda-setup.html"><span class="No-Break">https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/lambda-setup.html</span></a></li>
				<li>Interacting with query editor v2 generative <span class="No-Break">SQL: </span><a href="https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-generative-ai.html"><span class="No-Break">https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-generative-ai.html</span></a></li>
				<li>Amazon Redshift “tickit” <span class="No-Break">database: </span><a href="https://docs.aws.amazon.com/redshift/latest/dg/c_sampledb.html"><span class="No-Break">https://docs.aws.amazon.com/redshift/latest/dg/c_sampledb.html</span></a><a href="https://docs.aws.amazon.com/redshift/latest/dg/c_sampledb.html&#13;"/></li>
				<li>Direct marketing bank <span class="No-Break">data: </span><a href="https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip"><span class="No-Break">https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip</span></a><a href="https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip&#13;"/></li>
				<li>Amazon SageMaker <span class="No-Break">Studio: </span><a href="https://aws.amazon.com/sagemaker/studio/"><span class="No-Break">https://aws.amazon.com/sagemaker/studio/</span></a></li>
			</ul>
		</div>
	</body></html>