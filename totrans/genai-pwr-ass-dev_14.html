<html><head></head><body>
		<div><h1 class="chapter-number" id="_idParaDest-211"><a id="_idTextAnchor210"/>14</h1>
			<h1 id="_idParaDest-212"><a id="_idTextAnchor211"/>Accelerate Data Engineering on AWS</h1>
			<p>In this chapter, we will look at the following key topics:</p>
			<ul>
				<li>Code assistance options with AWS services</li>
				<li>Code assistance integration with AWS Glue</li>
				<li>Code assistance integration with Amazon EMR</li>
				<li>Code assistance integration with AWS Lambda</li>
				<li>Code assistance integration with Amazon SageMaker</li>
				<li>Code assistance integration with Amazon Redshift</li>
			</ul>
			<p>In the previous part of the book, we explored auto-code generation techniques and the integration of a<a id="_idIndexMarker571"/> code companion with <strong class="bold">integrated development environments</strong> (<strong class="bold">IDEs</strong>) and provided examples using JetBrains PyCharm IDE with Amazon Q Developer for different languages that developers use very often. In this chapter, we will specifically focus on how Amazon is expanding in the area of assisting code developers by integrating with core AWS services.</p>
			<h1 id="_idParaDest-213"><a id="_idTextAnchor212"/>Code assistance options with AWS services</h1>
			<p>AWS<a id="_idIndexMarker572"/> users select diverse services, considering<a id="_idIndexMarker573"/> factors such as the unique requirements of their projects, use cases, developers’ technical needs, developer preferences, and the characteristics of AWS services. To cater to various developer personas, such as data engineers, data scientists, application developers, and so on, AWS has integrated code assistance with many of its code services. If you are an application builder, software developer, data engineer, or data scientist working with AWS services, you would frequently use builder-friendly tools such as Amazon SageMaker as a platform for <a id="_idIndexMarker574"/>building AI / <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) projects, Amazon EMR as a platform for building big data processing projects, AWS Glue for <a id="_idIndexMarker575"/>building <strong class="bold">extract, transform, and load</strong> (<strong class="bold">ETL</strong>) pipelines, AWS Lambda as a serverless compute service for application development. All these services <a id="_idIndexMarker576"/>provide tools that help builders and <a id="_idIndexMarker577"/>developers write code.</p>
			<div><div><img alt="Figure 14.1 – Code assistance options with AWS services" src="img/B21378_14_1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.1 – Code assistance options with AWS services</p>
			<p>As of the writing of this book, AWS has integrated Amazon Q Developer with AWS Glue, Amazon EMR, AWS Lambda, Amazon SageMaker, and Amazon Redshift. However, we anticipate that the list of services benefiting from code assistance, such as Amazon Q Developer, will continue to expand in the future.</p>
			<p>In the following sections, we will dive deep into each of these services, examining their integration with Amazon Q in detail. We will provide examples that will be helpful for data engineers to accelerate development on AWS.</p>
			<p class="callout-heading">Note</p>
			<p class="callout"><strong class="bold">Large language models</strong> (<strong class="bold">LLMs</strong>), by nature, are <a id="_idIndexMarker578"/>non-deterministic, so you may not get the same code blocks shown in the code snapshots. However, logically, the generated code should meet the requirements.</p>
			<p class="callout"><strong class="bold">CodeWhisperer</strong> is <a id="_idIndexMarker579"/>a legacy name from a service that merged with Amazon Q Developer. As of the time of writing this book, some of the integrations are still referred to as CodeWhisperer in the AWS console, which may change in the future.</p>
			<h1 id="_idParaDest-214"><a id="_idTextAnchor213"/>Code assistance integration with AWS Glue</h1>
			<p>Before we start diving deep into code assistance support for AWS Glue service, let’s quickly go through an overview of AWS Glue. <strong class="bold">AWS Glue</strong> is a<a id="_idIndexMarker580"/> serverless data integration service designed to simplify the process of discovering, preparing, moving, and integrating data from diverse sources, catering to analytics, ML, and application development needs. At the very high level, AWS Glue has the following major components, and each of them <a id="_idIndexMarker581"/>has multiple features to <a id="_idIndexMarker582"/>support data engineers:</p>
			<ul>
				<li><strong class="bold">Glue Data Catalog</strong>: It’s a <a id="_idIndexMarker583"/>centralized technical metadata repository. It stores metadata about data sources, transformations, and targets, providing a unified view of the data.</li>
				<li><strong class="bold">Glue Studio</strong>: AWS Glue Studio<a id="_idIndexMarker584"/> offers a graphical interface that facilitates the seamless creation, execution, and monitoring of data integration jobs within AWS Glue. Additionally, it provides Jupyter notebooks for advanced developers.</li>
			</ul>
			<p>AWS Glue Studio is seamlessly integrated with Amazon Q Developer. Let’s explore the further functionality by considering a very common use case of data enrichment.</p>
			<h2 id="_idParaDest-215"><a id="_idTextAnchor214"/>Use case for AWS Glue</h2>
			<p>Features and functionalities <a id="_idIndexMarker585"/>of any service or tool are best understood when we have a use case to solve. So, let’s start with one of the easy and widely used use cases of data enrichment using lookups.</p>
			<p><strong class="bold">Data enrichment using lookup</strong>: In a typical scenario, business analysts often require data enrichment by incorporating details associated with codes/IDs found in a column through a lookup table. The desired result is a comprehensive and denormalized record containing both the code and corresponding details in the same row. To address this specific use case, data engineers develop ETL jobs to join the tables, creating the final structure with a denormalized dataset.</p>
			<p>To illustrate this use case, we will use yellow taxi trip records that encompass details such as the date and time of pick-up and drop-off, the locations for pick-up and drop-off, the trip distance, comprehensive fare breakdowns, various rate types, utilized payment methods, and passenger counts reported by the driver. Additionally, trip information incorporates passenger location codes for both pick-up and drop-off.</p>
			<p>The business objective is to enhance the dataset with zone information based on the pick-up location code.</p>
			<p>To meet this requirement, data engineers must develop a PySpark ETL script. This script should perform a lookup for zone information corresponding to the pick-up location code. Subsequently, the engineers create denormalized/enriched data by amalgamating yellow taxi trip data with detailed pick-up zone information and save the result as a file.</p>
			<p>As a code developer / data engineer, you will need to convert the preceding business objectives into technical requirements.</p>
			<h2 id="_idParaDest-216"><a id="_idTextAnchor215"/>Solution blueprint</h2>
			<ol>
				<li>Write a <a id="_idIndexMarker586"/>PySpark code to handle technical requirements.</li>
				<li>Read the <code>yellow_tripdata_2023-01.parquet</code> file from the S3 location in a DataFrame and display a sample of 10 records.</li>
				<li>Read the <code>taxi+_zone_lookup.csv</code> file from the S3 location in a DataFrame and display a sample of 10 records.</li>
				<li>Perform a left outer join on <code>yellow_tripdata_2023-01.parquet</code> and <code>taxi+_zone_lookup.csv</code> on <code>PULocationID = LocationID</code> to gather pick-up zone information.</li>
				<li>Save the preceding dataset as a CSV file in the preceding Amazon S3 bucket in a new <code>glue_notebook_yellow_pick_up_zone_output</code> folder.</li>
				<li>For <a id="_idIndexMarker587"/>verification, download and check the files from the <code>glue_notebook_yellow_pick_up_zone_output</code> folder.</li>
			</ol>
			<p>Now that we have a use case defined, let’s go through the step-by-step solution for it.</p>
			<h2 id="_idParaDest-217"><a id="_idTextAnchor216"/>Data preparation</h2>
			<p>The first step will be to<a id="_idIndexMarker588"/> prepare the data. To illustrate its functionality, in the following sections, we will utilize the publicly available NY Taxi dataset from TLC Trip Record Data. <a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page">https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page</a>.</p>
			<p>Firstly, we will download the required files on a local machine and then upload them in one of Amazon’s S3 buckets:</p>
			<ol>
				<li>Download the Yellow Taxi Trip Records data for the Jan 2023 Parquet file (<code>yellow_tripdata_2023-01.parquet</code>) on a local machine from <a href="https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet">https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet</a>.</li>
			</ol>
			<div><div><img alt="Figure 14.2 – The Yellow Taxi Trip Records data for Jan 2023 Parquet file" src="img/B21378_14_2.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.2 – The Yellow Taxi Trip Records data for Jan 2023 Parquet file</p>
			<ol>
				<li value="2">Download the Taxi<a id="_idIndexMarker589"/> Zone Lookup Table CSV file (<code>taxi+_zone_lookup.csv</code>) on a local machine from <a href="https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv">https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv</a>.</li>
			</ol>
			<div><div><img alt="Figure 14.3 – The Zone Lookup Table CSV file" src="img/B21378_14_3.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.3 – The Zone Lookup Table CSV file</p>
			<ol>
				<li value="3">Create the <a id="_idIndexMarker590"/>two <code>yellow_taxi_trip_records</code> and <code>zone_lookup</code> folders in Amazon S3, which we can reference in our Glue notebook job.</li>
			</ol>
			<div><div><img alt="Figure 14.4 – S3 folders structure" src="img/B21378_14_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.4 – S3 folders structure</p>
			<ol>
				<li value="4">Upload the <code>yellow_tripdata_2023-01.parquet</code> file to the <code>yellow_taxi_trip_records</code> folder.</li>
			</ol>
			<div><div><img alt="Figure 14.5 – The yellow_taxi_tripdata_record file" src="img/B21378_14_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.5 – The yellow_taxi_tripdata_record file</p>
			<ol>
				<li value="5">Upload <a id="_idIndexMarker591"/>the <code>taxi+_zone_lookup.csv</code> file to the <code>zone_lookup</code> folder.</li>
			</ol>
			<div><div><img alt="Figure 14.6 – The zone_lookup file" src="img/B21378_14_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.6 – The zone_lookup file</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We will use the same dataset and use case to discover solutions using AWS Glue and Amazon EMR. For illustrative purposes, we have prepared the data manually. However, in a production environment, file transfers can be automated by leveraging various AWS services and/or third-party software.</p>
			<p>Now, let’s dive deep into a detailed exploration of the solution using the integration of Amazon Q Developer with an AWS Glue Studio notebook for the preceding use case.</p>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor217"/>Solution – Amazon Q Developer with an AWS Glue Studio notebook</h2>
			<p>Let’s first enable Amazon Q Developer with an AWS Glue Studio notebook.</p>
			<h3>Prerequisites to enable Amazon Q Developer with an AWS Glue Studio notebook</h3>
			<p>The developer is required to modify the <strong class="bold">identity and access management</strong> (<strong class="bold">IAM</strong>) policy<a id="_idIndexMarker592"/> associated with the IAM user or role to <a id="_idIndexMarker593"/>grant permissions for Amazon Q Developer to initiate recommendations in a Glue Studio notebook. Reference <a href="B21378_02.xhtml#_idTextAnchor022"><em class="italic">Chapter 2</em></a> for the details to enable Amazon Q Developer with an AWS Glue Studio notebook.</p>
			<p>To fulfill the previously mentioned solution blueprint, we will use various auto-code generation techniques that were discussed in <a href="B21378_03.xhtml#_idTextAnchor060"><em class="italic">Chapter 3</em></a>. Mainly, we will focus on single-line prompts, multi-line prompts, and chain-of-thought prompts for auto-code generation.</p>
			<p>Let’s use Amazon Q Developer to auto-generate an end-to-end script in an AWS Glue Studio notebook. Here is the step-by-step solution walk-through for the previously defined solution blueprint.</p>
			<h3>Requirement 1</h3>
			<p>First, you need to write some PySpark code.</p>
			<p>While creating a Glue Studio notebook, select the <strong class="bold">Spark (Python)</strong> engine and the role that has the Amazon Q Developer policy attached.</p>
			<div><div><img alt="Figure 14.7 – Create a Glue Studio notebook with PySpark" src="img/B21378_14_7.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.7 – Create a Glue Studio notebook with PySpark</p>
			<p>Once you <a id="_idIndexMarker594"/>create the notebook, observe the kernel named <code>Glue PySpark</code>.</p>
			<div><div><img alt="Figure 14.8 – A Glue Studio notebook with the Glue PySpark kernel" src="img/B21378_14_08.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.8 – A Glue Studio notebook with the Glue PySpark kernel</p>
			<h3>Requirement 2</h3>
			<p>Read the <code>yellow_tripdata_2023-01.parquet</code> file from the S3 location in a DataFrame and display a sample of 10 records.</p>
			<p>Let’s use a chain-of-thought prompt technique with multiple single-line prompts in different cells to achieve the preceding requirement:</p>
			<pre class="source-code">
Prompt # 1:
# Read s3://&lt;your-bucket-name-here&gt;/yellow_taxi_trip_records/yellow_tripdata_2023-01.parquet file in a dataframe
Prompt # 2:
# display a sample of 10 records from dataframe</pre>			<div><div><img alt="Figure 14.9 – PySpark code to read the Yellow Taxi Trip Records data using single-line prompts" src="img/B21378_14_09.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.9 – PySpark code to read the Yellow Taxi Trip Records data using single-line prompts</p>
			<p>Observe that <a id="_idIndexMarker595"/>upon entering the Amazon Q Developer-enabled Glue Studio notebook prompt, it initiates code recommendations. Q Developer recognizes the file format as Parquet and suggests using the <code>spark.read.parquet</code> method. You can directly execute each cell/code from the notebook. Furthermore, as you move to the next cell, Q Developer utilizes “line-by-line recommendations” to suggest displaying the schema.</p>
			<div><div><img alt="Figure 14.10 – Line-by-line recommendations to display schema" src="img/B21378_14_10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.10 – Line-by-line recommendations to display schema</p>
			<h3>Requirement 3</h3>
			<p>Read <a id="_idIndexMarker596"/>the <code>taxi+_zone_lookup.csv</code> file from the S3 location in a DataFrame and display a sample of 10 records.</p>
			<p>We already explored the chain-of-thought prompt technique with multiple single-line prompts for <em class="italic">Requirement 2</em>. Now, let’s try with a multi-line prompt to achieve the preceding requirement and we will try to customize the code for the DataFrame name:</p>
			<pre class="source-code">
Prompt:
"""
Read s3://&lt;your-bucket-name-here&gt;/zone_lookup/taxi+_zone_lookup.csv in a dataframe name zone_df.
Show sample 10 records from zone_df.
"""</pre>			<div><div><img alt="Figure 14.11 – PySpark code to read the Zone Lookup file using a multi-line prompt" src="img/B21378_14_11.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.11 – PySpark code to read the Zone Lookup file using a multi-line prompt</p>
			<p>Observe that <a id="_idIndexMarker597"/>Amazon Q Developer understood the context behind the multi-line prompt and also the specific DataFrame name instructed in the prompt. It auto-generated multiple lines of code with the DataFrame name as <code>zone_df</code> and file format as CSV, suggesting the use of the <code>spark.read.csv</code> method to read CSV files. You can directly execute each cell/code from the notebook.</p>
			<h3>Requirement 4</h3>
			<p>Perform a left outer join on <code>yellow_tripdata_2023-01.parquet</code> and <code>taxi+_zone_lookup.csv</code> on <code>pulocationid = LocationID</code> to gather pick-up zone information.</p>
			<p>We will continue using multi-line prompts and some code customization to achieve the preceding requirement:</p>
			<pre class="source-code">
Prompt:
"""
Perform a left outer join on dataframe df and dataframe zone_df on PULocationID = LocationID to save in dataframe name yellow_pu_zone_df.
Show sample 10 records from yellow_pu_zone_df and show schema.
"""</pre>			<div><div><img alt="Figure 14.12 – Left outer join df and dataframe zone_df – multi-line prompt" src="img/B21378_14_12.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.12 – Left outer join df and dataframe zone_df – multi-line prompt</p>
			<p>Now, let’s<a id="_idIndexMarker598"/> review the schema of the DataFrame returns by the code execution.</p>
			<div><div><img alt="Figure 14.13 – Left outer join df and dataframe zone_df – display schema" src="img/B21378_14_13.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.13 – Left outer join df and dataframe zone_df – display schema</p>
			<p>Observe<a id="_idIndexMarker599"/> that, as instructed in the multi-line prompt, Amazon Q Developer understood the context and auto-generated error-free code with the exact specifications we provided related to the DataFrame name of <code>yellow_pu_zone_df</code>. You can directly execute each cell/code from the notebook.</p>
			<h3>Requirement 5</h3>
			<p>Save the preceding dataset as a CSV file in the preceding Amazon S3 bucket in a new folder called <code>glue_notebook_yellow_pick_up_zone_output</code>.</p>
			<p>Since the preceding requirement is straightforward and can be encapsulated in a single sentence, we will use a single-line prompt to generate the code, and we will also include a header to facilitate easy verification:</p>
			<pre class="source-code">
Prompt:
# Save dataframe yellow_pu_zone_df as CSV file at location s3://&lt;your-bucket-name-here&gt;/tlc-dataset-ny-taxi/glue_notebook_yellow_pick_up_zone_output/ with header information</pre>			<div><div><img alt="Figure 14.14 – Save the CSV file with enrichment pick-up location data" src="img/B21378_14_14.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.14 – Save the CSV file with enrichment pick-up location data</p>
			<h3>Requirement 6</h3>
			<p>For <a id="_idIndexMarker600"/>verification, download and check files from the <code>glue_notebook_yellow_pick_up_zone_output</code> folder.</p>
			<p>Let’s go to the Amazon S3 console to verify the files. Select one of the files and click <strong class="bold">Download</strong>.</p>
			<div><div><img alt="Figure 14.15 – Save a CSV file with enrichment pick-up location data" src="img/B21378_14_15.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.15 – Save a CSV file with enrichment pick-up location data</p>
			<p>After downloading the file, you can use any text editor to review the file contents.</p>
			<div><div><img alt="Figure 14.16 – Verify the CSV file with enrichment pick-up location data" src="img/B21378_14_16.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.16 – Verify the CSV file with enrichment pick-up location data</p>
			<p>Observe that the CSV file has additional columns with zone information based on the pick-up location ID. In the next section, we <a id="_idIndexMarker601"/>will explore Amazon Q Developer integration with AWS Glue and use the chat assistant technique.</p>
			<p class="callout-heading">Think challenge</p>
			<p class="callout">To fulfill <em class="italic">Requirement 6</em>, if you are interested, attempt to utilize the same Glue Studio notebook for reading a CSV file, displaying sample records, and adding a header.</p>
			<p class="callout"><strong class="bold">Hint</strong>: Use the multi-line prompt technique, similar to the one we used when reading the Zone Lookup file.</p>
			<h2 id="_idParaDest-219"><a id="_idTextAnchor218"/>Solution – Amazon Q Developer with AWS Glue</h2>
			<p>Amazon Q <a id="_idIndexMarker602"/>Developer provides a chat-style interface in the AWS Glue console. Now, let’s explore the integration between Amazon Q Developer and AWS Glue for the same use case and solution blueprint that we handled using Amazon Q Developer and an AWS Glue Studio notebook integration.</p>
			<p>Let’s now look at the prerequisites to enable Amazon Q with AWS Glue.</p>
			<p>To enable Amazon Q Developer integration with AWS Glue, we will need to update the IAM policy. Please refer to <a href="B21378_02.xhtml#_idTextAnchor022"><em class="italic">Chapter 2</em></a> for additional details on initiating interaction with Amazon Q in AWS Glue.</p>
			<p>Now, let’s dive deep into a detailed exploration of the integration of Amazon Q Developer with AWS Glue Studio for the preceding use case.</p>
			<p>To fulfill the mentioned requirements, we will mainly use the chat companion that was discussed in <a href="B21378_03.xhtml#_idTextAnchor060"><em class="italic">Chapter 3</em></a>.</p>
			<p>Here is a <a id="_idIndexMarker603"/>step-by-step solution walk-through that we’ll use as a prompt for all of the preceding requirements:</p>
			<pre class="source-code">
Instruction to Amazon Q:
Write a Glue ETL job.
Read the 's3://&lt;your bucket name&gt;/yellow_taxi_trip_records/yellow_tripdata_2023-01.parquet' file in a dataframe and display a sample of 10 records.
Read the 's3://&lt;your bucket name&gt;/zone_lookup/taxi+_zone_lookup.csv' file in a dataframe and display a sample of 10 records.
Perform a left outer join on 'yellow_tripdata_2023-01.parquet' and 'taxi+_zone_lookup.csv' on DOLocationID = LocationID to gather pick-up zone information.
Save the above dataset as a CSV file in above Amazon S3 bucket in a new folder 'glue_notebook_yellow_drop_off_zone_output'.</pre>			<div><div><img alt="Figure 14.17 – The AWS Glue ETL code suggested by Amazon Q Developer – part 1" src="img/B21378_14_17.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.17 – The AWS Glue ETL code suggested by Amazon Q Developer – part 1</p>
			<p>You can see <a id="_idIndexMarker604"/>that, based on the instruction provided to Amazon Q, it generated the skeleton on the ETL code. It generated code structure with Glue-PySpark libraries, a s3node with create dynamic dataframe to read parquet file, and a s3node with write dynamic dataframe to write CSV file.</p>
			<div><div><img alt="Figure 14.18 – AWS Glue ETL code suggested by Amazon Q Developer – part 2" src="img/B21378_14_18.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.18 – AWS Glue ETL code suggested by Amazon Q Developer – part 2</p>
			<p>Observe that Amazon Q also provided technical details to explain the script flow. This can also be used to meet the in-script documentation needs.</p>
			<div><div><img alt="Figure 14.19 – AWS Glue ETL code suggested by Amazon Q Developer – script summary" src="img/B21378_14_19.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.19 – AWS Glue ETL code suggested by Amazon Q Developer – script summary</p>
			<p>Data engineers with coding experience can easily reference the script summary and script skeleton to write end-to-end scripts to meet the solution blueprint. LLMs, by nature, are non-deterministic, so you may not get the same code blocks shown in the code snapshots.</p>
			<p>Based on the<a id="_idIndexMarker605"/> preceding use case illustration, AWS Glue integration with Amazon Q Developer with prompting techniques can be used by data engineers at a relatively lower experience level, while AWS Glue integration with Amazon Q Developer using the chat assistant can be utilized by ETL developers with relatively more experience.</p>
			<h3>Summary – Amazon Q Developer with an AWS Glue Studio notebook</h3>
			<p>As illustrated, we can automatically generate end-to-end, error-free, and executable code simply by providing prompts with specific requirements. Amazon Q Developer, integrated with an AWS Glue Studio notebook, comprehends the context and automatically generates PySpark code that can be run directly from the notebook without the need to provision any hardware upfront. This marks a significant advancement for many data engineers, relieving them from concerns about the technical intricacies associated with PySpark libraries, methods, and syntax.</p>
			<p>Next, we will explore code assistance integration with Amazon EMR.</p>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor219"/>Code assistance integration with Amazon EMR</h1>
			<p>Before we dive deep <a id="_idIndexMarker606"/>into the details of code assistance support for Amazon EMR, let’s quickly go through an overview of Amazon <a id="_idIndexMarker607"/>EMR. <strong class="bold">Amazon EMR</strong> is a cloud-based big data platform that simplifies the deployment, management, and scaling of various big data frameworks such as Apache <a id="_idIndexMarker608"/>Hadoop, Apache Spark, Apache Hive, and Apache HBase. At a high level, Amazon EMR comprises the following major components, each with multiple features to support data engineers and data scientists:</p>
			<ul>
				<li><strong class="bold">EMR on EC2/EKS</strong>: The<a id="_idIndexMarker609"/> Amazon EMR service provides two options, EMR on EC2 and EMR on EKS, allowing customers to provision clusters. Amazon EMR streamlines the execution of batch jobs and interactive workloads for data analysts and engineers.</li>
				<li><strong class="bold">EMR Serverless</strong>: Amazon <a id="_idIndexMarker610"/>EMR Serverless is a serverless alternative within Amazon EMR. With Amazon EMR Serverless, users can access the full suite of features and advantages offered by Amazon EMR, all without requiring specialized expertise for cluster planning and management.</li>
				<li><strong class="bold">EMR Studio</strong>: EMR Studio<a id="_idIndexMarker611"/> supports data engineers and data scientists in developing, visualizing, and debugging applications within an IDE. It also provides a Jupyter Notebook environment for interactive coding.</li>
			</ul>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor220"/>Use case for Amazon EMR Studio</h2>
			<p>For simplicity and ease of<a id="_idIndexMarker612"/> following Amazon Q Developer integration with Amazon EMR, we will use the same use case and data that we used in this chapter under the <em class="italic">Code assistance integration with AWS Glue</em> section. Refer to the <em class="italic">Use case for AWS Glue</em> section, which covers details related to the solution blueprint and data preparation.</p>
			<h2 id="_idParaDest-222"><a id="_idTextAnchor221"/>Solution – Amazon Q Developer with Amazon EMR Studio</h2>
			<p>Let’s first enable Amazon Q Developer with Amazon EMR Studio. To enable Amazon Q Developer integration with Amazon EMR Studio, we will need to update the IAM policy.</p>
			<h3>Prerequisite to enable Amazon Q Developer with Amazon EMR Studio</h3>
			<p>The developer is <a id="_idIndexMarker613"/>required to modify the IAM policy associated with the role to grant permissions for Amazon Q Developer to initiate recommendations in EMR Studio. Please reference <a href="B21378_02.xhtml#_idTextAnchor022"><em class="italic">Chapter 2</em></a> for additional details on initiating interaction with Amazon Q Developer in Amazon EMR Studio.</p>
			<p>To fulfill the mentioned requirements, we will use various auto-code generation techniques that were discussed in <a href="B21378_03.xhtml#_idTextAnchor060"><em class="italic">Chapter 3</em></a>. Mainly, we will focus on single-line prompts, multi-line prompts, and chain-of-thought prompts for auto-code generation techniques.</p>
			<p>Let’s use Amazon Q Developer to auto-generate end-to-end scripts, which can achieve the following requirements in Amazon EMR Studio. Here is a step-by-step solution walk-through of the solution.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can observe lots of similarities between a Glue Studio notebook and an EMR Studio notebook when it comes to code recommended by Amazon Q Developer.</p>
			<h3>Requirement 1</h3>
			<p>You will need<a id="_idIndexMarker614"/> to write a PySpark code to handle technical requirements.</p>
			<p>Once you open Amazon EMR Studio, use <strong class="bold">Launcher</strong> to select <strong class="bold">PySpark</strong> from the <strong class="bold">Notebook</strong> section.</p>
			<div><div><img alt="Figure 14.20 – Create an EMR Studio notebook with PySpark" src="img/B21378_14_20.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.20 – Create an EMR Studio notebook with PySpark</p>
			<p>Once you create the<a id="_idIndexMarker615"/> notebook, you can see a kernel named <code>PySpark</code>. The kernel is a standalone process that runs in the background and executes the code you write in your notebooks. For more information, refer to the <em class="italic">References</em> section at the end of the chapter.</p>
			<div><div><img alt="Figure 14.21 – The EMR Studio notebook with a PySpark kernel" src="img/B21378_14_21.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.21 – The EMR Studio notebook with a PySpark kernel</p>
			<p>I have already attached a cluster to my notebook, but you can explore different options to attach the compute to EMR studio in AWS documentation at <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-use-clusters.html">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-use-clusters.html</a>.</p>
			<h3>Requirement 2</h3>
			<p>Read the <code>yellow_tripdata_2023-01.parquet</code> file from the S3 location in a DataFrame and display a sample of 10 records.</p>
			<p>Let’s use a chain-of-thought prompts technique with multiple single-line prompts in different cells to achieve this requirement:</p>
			<pre class="source-code">
Prompt # 1:
# Read s3://&lt;your-bucket-name-here&gt;/yellow_taxi_trip_records/yellow_tripdata_2023-01.parquet file in a dataframe
Prompt # 2:
# Display a sample of 10 records from dataframe</pre>			<div><div><img alt="Figure 14.22 – PySpark code to read the Yellow Taxi Trip Records data using single-line prompts" src="img/B21378_14_22.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.22 – PySpark code to read the Yellow Taxi Trip Records data using single-line prompts</p>
			<p>Observe that upon entering the Amazon Q Developer-enabled EMR Studio notebook prompt, it initiates code recommendations. Amazon Q Developer recognizes the file format as Parquet and suggests using the <code>spark.read.parquet</code> method. You can directly execute each cell/code from the notebook. Furthermore, as you move to the next cell, Amazon Q Developer utilizes “line-by-line recommendations” to suggest displaying the schema.</p>
			<h3>Requirement 3</h3>
			<p>Read the <code>taxi+_zone_lookup.csv</code> file from the S3 location in a DataFrame and display a <a id="_idIndexMarker616"/>sample of 10 records.</p>
			<p>We already explored the chain-of-thought prompts technique with multiple single-line prompts for <em class="italic">Requirement 2</em>. Now, let’s try a multi-line prompt to achieve this requirement and we will try to customize the code for the DataFrame name:</p>
			<pre class="source-code">
Prompt:
"""
Read s3://&lt;your-bucket-name-here&gt;/zone_lookup/taxi+_zone_lookup.csv in a dataframe name zone_df. Show sample 10 records from zone_df.
"""</pre>			<div><div><img alt="Figure 14.23 – PySpark code to read the Zone Lookup file using a multi-line prompt" src="img/B21378_14_23.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.23 – PySpark code to read the Zone Lookup file using a multi-line prompt</p>
			<p>Observe that Amazon Q Developer understood the context behind the multi-line prompt and also the specific DataFrame name instructed in the prompt. It auto-generated multiple lines of code with the DataFrame name of <code>zone_df</code> and file format as CSV, suggesting the use of the <code>spark.read.csv</code> method to read CSV files. You can directly execute each cell/code from the notebook.</p>
			<h3>Requirement 4</h3>
			<p>Perform a<a id="_idIndexMarker617"/> left outer join on <code>yellow_tripdata_2023-01.parquet</code> and <code>taxi+_zone_lookup.csv</code> on <code>pulocationid = LocationID</code> to gather pick-up zone information.</p>
			<p>We will continue using multi-line prompts and some code customization to achieve the preceding requirement:</p>
			<pre class="source-code">
Prompt:
"""
Perform a left outer join on dataframe df and dataframe zone_df on PULocationID = LocationID to save in dataframe name yellow_pu_zone_df.
Show sample 10 records from yellow_pu_zone_df and show schema.
"""</pre>			<div><div><img alt="Figure 14.24 – Left outer join df and dataframe zone_df – multi-line prompt" src="img/B21378_14_24.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.24 – Left outer join df and dataframe zone_df – multi-line prompt</p>
			<p>Now, let’s <a id="_idIndexMarker618"/>review the schema of the DataFrame printed by the code.</p>
			<div><div><img alt="Figure 14.25 – Left outer join df and dataframe zone_df – display schema" src="img/B21378_14_25.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.25 – Left outer join df and dataframe zone_df – display schema</p>
			<p>Observe that, as instructed in the multi-line prompt, Amazon Q Developer understood the context and auto-generated error-free code with the exact specifications we provided related to the DataFrame named <code>yellow_pu_zone_df</code>. You can directly execute each cell/code from the notebook.</p>
			<h3>Requirement 5</h3>
			<p>Save the <a id="_idIndexMarker619"/>preceding dataset as a CSV file in the previous Amazon S3 bucket in a new folder called <code>glue_notebook_yellow_pick_up_zone_output</code>.</p>
			<p>Since this requirement is straightforward and can be encapsulated in a single sentence, we will use a single-line prompt to generate the code, and we will also include a header to facilitate easy verification:</p>
			<pre class="source-code">
Prompt:
# Save dataframe yellow_pu_zone_df as CSV file at location s3://&lt;your-bucket-name-here&gt;/tlc-dataset-ny-taxi/glue_notebook_yellow_pick_up_zone_output/ with header information</pre>			<div><div><img alt="Figure 14.26 – Save the CSV file with enrichment pick-up location data" src="img/B21378_14_26.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.26 – Save the CSV file with enrichment pick-up location data</p>
			<h3>Requirement 6</h3>
			<p>For verification, download and check files from the <code>glue_notebook_yellow_pick_up_zone_output</code> folder.</p>
			<p> Let’s go to the<a id="_idIndexMarker620"/> Amazon S3 console to verify the files. Select one of the files and click <strong class="bold">Download</strong>.</p>
			<div><div><img alt="Figure 14.27 – Verify final result set – Amazon Q Developer with Amazon EMR Studio" src="img/B21378_14_27.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.27 – Verify final result set – Amazon Q Developer with Amazon EMR Studio</p>
			<p>After downloading, you can use a text editor to review the file contents.</p>
			<div><div><img alt="Figure 14.28 – Verify the CSV file contents of Amazon Q Developer with Amazon EMR Studio" src="img/B21378_14_28.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.28 – Verify the CSV file contents of Amazon Q Developer with Amazon EMR Studio</p>
			<p>Observe that the CSV file has additional columns with zone information based on the pick-up location ID.</p>
			<h3>Summary – Amazon Q Developer with Amazon EMR Studio</h3>
			<p>As illustrated, we can automatically generate end-to-end, error-free, and executable code simply by providing prompts with specific requirements. Amazon Q Developer, integrated with an Amazon EMR Studio notebook, comprehends the context and automatically generates PySpark code that can be run directly from the notebook. This marks a significant advancement for many data engineers, relieving them from concerns about the technical intricacies associated with PySpark libraries, methods, and syntax.</p>
			<p class="callout-heading">Think challenge</p>
			<p class="callout">To fulfill <em class="italic">Requirement 6</em>, if you are interested, attempt to utilize the same EMR Studio notebook for reading a CSV file, displaying sample records, and adding a header.</p>
			<p class="callout"><strong class="bold">Hint</strong>: Use the multi-line prompt technique, similar to the one we used when reading the Zone Lookup file.</p>
			<p>In the next section, we will consider an application developer persona to explore code assistance integration with AWS Lambda.</p>
			<h1 id="_idParaDest-223"><a id="_idTextAnchor222"/>Code assistance integration with AWS Lambda</h1>
			<p>Before we start diving deep into code assistance support for the AWS Lambda service, let’s quickly go through an overview of<a id="_idIndexMarker621"/> AWS Lambda. <strong class="bold">AWS Lambda</strong> is a serverless computing service that allows users to run code without provisioning or managing servers. With Lambda, you can upload your code or use the available editor from the Lambda console. During the runtime of the code, based on the provided configurations, the service automatically takes care of the compute resources needed for execution. It is designed to be highly scalable, cost effective, and suitable for event-driven applications.</p>
			<p>AWS Lambda <a id="_idIndexMarker622"/>supports multiple programming <a id="_idIndexMarker623"/>languages, including Node.js, Python, Java, Go, and .NET Core, allowing you to choose the language that best fits your application. Lambda can be easily integrated with other AWS services, enabling you to build complex and scalable architectures. It works seamlessly with services such as Amazon S3, DynamoDB, and API Gateway.</p>
			<p>The AWS Lambda console is integrated with Amazon Q Developer to make it easy for developers to get coding assistance/recommendations.</p>
			<h2 id="_idParaDest-224"><a id="_idTextAnchor223"/>Use case for AWS Lambda</h2>
			<p>Let’s start<a id="_idIndexMarker624"/> with one of the easy and widely used use cases of converting file format.</p>
			<p><strong class="bold">File format conversion</strong>: In a typical scenario, once a file is received from an external team and/or source, it may not be in the target location and have the required name expected by the application. In that case, AWS Lambda can be used to quickly copy the file from the source location to the target location and rename the file at the target location.</p>
			<p>To illustrate this use case, let’s copy the NY Taxi Zone lookup file from the source location (<code>s3://&lt;your-bucket-name&gt;/zone_lookup/</code>) to the target location (<code>s3://&lt;your-bucket-name&gt;/source_lookup_file/</code>). Also, remove the special character (<code>+</code>) from the filename to save it as <code>taxi_zone_lookup.csv</code>.</p>
			<p>To meet this requirement, application developers must develop a Python script. This script should copy and rename the Zone Lookup file from the source to the target location.</p>
			<p>As a code developer / data engineer, you will need to convert the preceding business objectives into the solution blueprint.</p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor224"/>Solution blueprint</h2>
			<ol>
				<li>Write a <a id="_idIndexMarker625"/>Python script to handle technical requirements.</li>
				<li>Copy the <code>taxi+_zone_lookup.csv</code> file from S3 to the <code>zone_lookup</code> folder to the <code>source_lookup_file</code> folder.</li>
				<li>During copying, change <code>taxi+_zone_lookup.csv</code> to <code>taxi_zone_lookup.csv</code> in the target <code>source_lookup_file</code> folder.</li>
				<li>For verification, check the contents of the <code>source_lookup_file/taxi_zone_lookup.csv</code> file.</li>
			</ol>
			<p>Now that we have a use case defined, let’s go through the step-by-step solution for it.</p>
			<h2 id="_idParaDest-226"><a id="_idTextAnchor225"/>Data preparation</h2>
			<p>We are using the same lookup file that we provisioned in this chapter under the <em class="italic">Code assistance integration with AWS Glue</em> section. Please refer to the <em class="italic">Use case for AWS Glue</em> section, which covers details related to data preparation.</p>
			<h2 id="_idParaDest-227"><a id="_idTextAnchor226"/>Solution – Amazon Q Developer with AWS Lambda</h2>
			<p>Let’s first enable Amazon Q Developer with the AWS Lambda console. To enable Amazon Q Developer integration with AWS Lambda, we will need to update the IAM policy.</p>
			<h3>Prerequisite to enable Amazon Q Developer with AWS Lambda</h3>
			<p>The developer is <a id="_idIndexMarker626"/>required to modify the IAM policy associated with the IAM user or role to grant permissions for Amazon Q Developer to initiate recommendations in the AWS Lambda console. Please reference <a href="B21378_02.xhtml#_idTextAnchor022"><em class="italic">Chapter 2</em></a> for additional details on initiating interaction with Amazon Q Developer in AWS Lambda.</p>
			<p>To let Amazon Q Developer start code suggestions, make sure to choose <strong class="bold">Tools</strong> | <strong class="bold">Amazon CodeWhisperer </strong><strong class="bold">Code Suggestions</strong>.</p>
			<div><div><img alt="Figure 14.29 – AWS Lambda console with Amazon Q Developer for Python runtime" src="img/B21378_14_29.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.29 – AWS Lambda console with Amazon Q Developer for Python runtime</p>
			<p>To fulfill<a id="_idIndexMarker627"/> the mentioned requirements, we will use auto-code generation techniques that were discussed in <a href="B21378_03.xhtml#_idTextAnchor060"><em class="italic">Chapter 3</em></a>. Mainly, we will focus on the multi-line prompt for auto-code generation. Let’s use Amazon Q Developer to auto-generate an end-to-end script that can achieve the following requirements in AWS Lambda Console and EMR Studio. Here is a step-by-step solution walk-through of the solution.</p>
			<h3>Requirement 1</h3>
			<p>You need to write a Python script to handle technical requirements.</p>
			<p>Once you open the AWS Lambda console, use the launcher to select a Python runtime.</p>
			<div><div><img alt="Figure 14.30 – Create a Python runtime from AWS Lambda" src="img/B21378_14_30.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.30 – Create a Python runtime from AWS Lambda</p>
			<p>Once you <a id="_idIndexMarker628"/>successfully create a Lambda function, observe that AWS Lambda creates a <code>lambda_function.py</code> file with some sample code. We can safely delete the sample code for this exercise, as we will use Amazon Q Developer to generate end-to-end code.</p>
			<div><div><img alt="Figure 14.31 – The AWS Lambda console with Amazon Q Developer for Python runtime" src="img/B21378_14_31.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.31 – The AWS Lambda console with Amazon Q Developer for Python runtime</p>
			<p>Let’s combine <em class="italic">Requirements 2</em> and <em class="italic">3</em>, as we are planning to use a multi-line prompt.</p>
			<h3>Requirements 2 and 3</h3>
			<p>Copy the <code>taxi+_zone_lookup.csv</code> file from S3 to the <code>zone_lookup</code> folder to the <code>source_lookup_file</code> folder.</p>
			<p>During copying, change the<a id="_idIndexMarker629"/> file name from <code>taxi+_zone_lookup.csv</code> to <code>taxi_zone_lookup.csv</code> in the target <code>source_lookup_file</code> folder.</p>
			<p>Let’s use multi-line prompts to auto-generate the code:</p>
			<pre class="source-code">
Prompt:
"""
write a lambda function.
copy s3://&lt;your-bucket-name&gt;/zone_lookup/taxi+_zone_lookup.csv
as s3://&lt;your-bucket-name&gt;/source_lookup_file/taxi_zone_lookup.csv
"""</pre>			<div><div><img alt="Figure 14.32 – Amazon Q Developer generated code for the AWS Lambda console" src="img/B21378_14_32.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.32 – Amazon Q Developer generated code for the AWS Lambda console</p>
			<p>Observe that Amazon Q Developer created a <code>lambda_handler</code> function and added <code>return code of 200</code> with a success message.</p>
			<h3>Requirement 4</h3>
			<p>For verification, check<a id="_idIndexMarker630"/> the contents of the <code>source_lookup_file/taxi_zone_lookup.csv</code> file.</p>
			<p>Let’s deploy and use a test event to run Lambda code generated by Amazon Q Developer.</p>
			<div><div><img alt="Figure 14.33 – Deploy AWS Lambda code" src="img/B21378_14_33.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.33 – Deploy AWS Lambda code</p>
			<p>Now, let’s test the code by going to the <strong class="bold">Test</strong> tab and clicking the <strong class="bold">Test</strong> button. Since we are not passing any values to this Lambda function, the JSON event values from the <strong class="bold">Test</strong> tab do not matter in our case.</p>
			<div><div><img alt="Figure 14.34 – Test AWS Lambda code" src="img/B21378_14_34.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.34 – Test AWS Lambda code</p>
			<p>Once the Lambda code executes successfully, it will provide you with the details of the execution. Observe that the code is executed successfully and displays the returned code with a success message.</p>
			<div><div><img alt="Figure 14.35 – AWS Lambda code execution" src="img/B21378_14_35.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.35 – AWS Lambda code execution</p>
			<p>Let’s use the <a id="_idIndexMarker631"/>Amazon S3 console to download and verify <code>s3://&lt;your-bucket-name&gt;/source_lookup_file/taxi_zone_lookup.csv</code>.</p>
			<div><div><img alt="Figure 14.36 – Target lookup file from Amazon S3" src="img/B21378_14_36.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.36 – Target lookup file from Amazon S3</p>
			<div><div><img alt="Figure 14.37 – The Zone Lookup file" src="img/B21378_14_37.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.37 – The Zone Lookup file</p>
			<h3>Summary – Amazon Q Developer with AWS Lambda</h3>
			<p>As illustrated, we can automatically generate end-to-end, error-free, and executable code simply by providing prompts with specific requirements. Amazon Q Developer, integrated with AWS Lambda, automatically generates the <code>lambda_handle</code><code>r</code> <code>()</code> function with return code based on the Lambda runtime environment selected. This integration can assist application developers with relatively limited coding experience in automatically generating Lambda functions with minor to no code changes.</p>
			<p>Continuing with the application developer persona, next, we will explore the data scientist persona to investigate code assistance integration with Amazon SageMaker.</p>
			<h1 id="_idParaDest-228"><a id="_idTextAnchor227"/>Code assistance integration with Amazon SageMaker</h1>
			<p>Before we start diving deep into code assistance support for the Amazon SageMaker service, let’s quickly go through an overview of<a id="_idIndexMarker632"/> Amazon SageMaker. <strong class="bold">Amazon SageMaker</strong> is a fully managed service that simplifies the process of building, training, and deploying ML models at scale. It is designed to make it easier for developers and data scientists to build, train, and deploy ML models without the need for extensive<a id="_idIndexMarker633"/> expertise in ML or<a id="_idIndexMarker634"/> deep learning. It has multiple features such as end-to-end workflow, built-in algorithms, custom model training, automatic model tuning, ground truth, edge manager, augmented AI, and managed notebooks, just to name a few. Amazon SageMaker integrates with other AWS services, such as Amazon S3 for data storage, AWS Lambda for serverless inference, and Amazon CloudWatch for monitoring.</p>
			<p>Amazon SageMaker Studio hosts the managed notebooks, which are integrated with Amazon Q Developer.</p>
			<h2 id="_idParaDest-229"><a id="_idTextAnchor228"/>Use case for Amazon SageMaker</h2>
			<p>Let’s use a very common business use case related to churn prediction for which data scientists use the XGBoost algorithm.</p>
			<p><strong class="bold">Churn prediction</strong> in <a id="_idIndexMarker635"/>business involves utilizing data and algorithms to forecast which customers are at risk of discontinuing their usage of a product or service. The term “churn” commonly denotes customers ending subscriptions, discontinuing purchases, or ceasing service utilization. The primary objective of churn prediction is to identify these customers before they churn, enabling businesses to implement proactive measures for customer retention.</p>
			<p>We will use publicly available direct marketing bank data to illustrate the support provided by Amazon Q Developer for milestone steps such as data collection, feature engineering, model training, and model deployment using Amazon SageMaker.</p>
			<p>Typically, data scientists need to write a complex script to carry out all of the preceding milestone steps from an Amazon SageMaker Studio notebook.</p>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor229"/>Solution blueprint</h2>
			<ol>
				<li>Set up an environment <a id="_idIndexMarker636"/>with the required set of libraries.</li>
				<li><strong class="bold">Data collection</strong>: Download and unzip direct marketing bank data from <a href="https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip">https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip</a>.</li>
				<li><strong class="bold">Feature engineering</strong>: To demonstrate the functionality, we will carry out the following commonly used feature engineering steps:<ul><li>Manipulate column data using default values</li><li>Drop extra columns</li><li>Carry out one-hot encoding</li></ul></li>
				<li><strong class="bold">Model training</strong>: Let’s use the XGBoost algorithm:<ul><li>Rearrange data to create training, validation, and test datasets/files</li><li>Use XGBoost algorithms to train the model using the training dataset</li></ul></li>
				<li><strong class="bold">Model deployment</strong>: Deploy<a id="_idIndexMarker637"/> the model as an endpoint to allow inferences.</li>
			</ol>
			<p>In the preceding solution blueprint, we illustrate the integration of Amazon Q Developer with Amazon SageMaker by handling commonly used milestone steps. However, based on the complexity of your data and enterprise needs, there might be additional steps required.</p>
			<h2 id="_idParaDest-231"><a id="_idTextAnchor230"/>Data preparation</h2>
			<p>We will utilize an AWS dataset <a id="_idIndexMarker638"/>publicly hosted for direct marketing bank data. The complete dataset is available at <a href="https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip">https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip</a>. All the data preparation steps will be conducted in the SageMaker Studio notebook as part of the data collection requirement.</p>
			<h2 id="_idParaDest-232"><a id="_idTextAnchor231"/>Solution – Amazon Q with Amazon SageMaker Studio</h2>
			<p>Let’s first enable Amazon Q Developer with Amazon SageMaker Studio. The following prerequisites are needed to allow Amazon Q Developer to auto-generate code inside Amazon SageMaker studio.</p>
			<h3>Prerequisite to enable Amazon Q Developer with Amazon SageMaker Studio</h3>
			<p>The developer is required to<a id="_idIndexMarker639"/> modify the IAM policy associated with the IAM user or role to grant permissions for Amazon Q Developer to initiate recommendations in for Amazon SageMaker Studio notebook. Refer <a href="B21378_02.xhtml#_idTextAnchor022"><em class="italic">Chapter 2</em></a> for the details to enable Amazon Q Developer with Amazon SageMaker Studio notebook.</p>
			<p>Once the Amazon Q Developer is activated for Amazon SageMaker Studio notebook, select <strong class="bold">Create notebook</strong> from the <strong class="bold">Launcher</strong> to verify that Amazon Q Developer is enabled.</p>
			<div><div><img alt="Figure 14.38 –An Amazon Q Developer-enabled notebook from SageMaker Studio" src="img/B21378_14_38.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.38 –An Amazon Q Developer-enabled notebook from SageMaker Studio</p>
			<p>To fulfill the <a id="_idIndexMarker640"/>mentioned requirements, we will use auto-code generation techniques that were discussed in <a href="B21378_04.xhtml#_idTextAnchor081"><em class="italic">Chapter 4</em></a>. Mainly, we will focus on single-line prompts, multi-line prompts, and chain-of-thought prompts for auto-code generation techniques.</p>
			<h3>Requirement 1</h3>
			<p>Set up an environment with the required set of libs.</p>
			<p>Let’s use single-line prompts:</p>
			<pre class="source-code">
Prompt 1:
# Fetch this data by importing the SageMaker library
Prompt 2:
# Defining global variables BUCKET and ROLE that point to the bucket associated with the Domain and it's execution role</pre>			<div><div><img alt="Figure 14.39 –Amazon Q Developer – SageMaker Studio setup environment" src="img/B21378_14_39.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.39 –Amazon Q Developer – SageMaker Studio setup environment</p>
			<p>Observe that, based on our prompts, Amazon Q Developer generated code with a default set of libraries and variables. However, based on your needs, and account setup, you may need to<a id="_idIndexMarker641"/> update/add the code.</p>
			<h3>Requirement 2</h3>
			<p>For data collection, download and unzip direct marketing bank data from <a href="https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip">https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip</a>.</p>
			<p>We will focus on multi-line prompts to achieve this requirement:</p>
			<pre class="source-code">
Prompt:
'''Using the requests library download the ZIP file from
the url "https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip"
and save it to current directory and unzip the archive
'''</pre>			<div><div><img alt="Figure 14.40 – Amazon Q Developer – SageMaker Studio data collection" src="img/B21378_14_40.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.40 – Amazon Q Developer – SageMaker Studio data collection</p>
			<h3>Requirement 3</h3>
			<p>To demonstrate the<a id="_idIndexMarker642"/> functionality of feature engineering, we will carry out the following commonly used feature engineering steps, which will help us improve the model accuracy:</p>
			<ol>
				<li>Manipulate column data using default values.</li>
				<li>Drop extra columns.</li>
				<li>Carry out one-hot encoding.</li>
			</ol>
			<p>We will focus on multi-line prompts to achieve this requirement:</p>
			<pre class="source-code">
Prompt #1:
'''
Create a new dataframe with column no_previous_contact and populates from existing dataframe column pdays using numpy when the condition equals to 999, 1, 0 and show the table
'''
Prompt # 2:
# do one hot encoding for full_data
Prompt # 3:
'''
Drop the columns 'duration', emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m' and 'nr.employed'
from the dataframe and create a new dataframe with name model_data
'''</pre>			<p>We get the <a id="_idIndexMarker643"/>following screen.</p>
			<div><div><img alt="Figure 14.41 –Amazon Q Developer – SageMaker Studio feature engineering" src="img/B21378_14_41.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.41 –Amazon Q Developer – SageMaker Studio feature engineering</p>
			<p>Now, let’s proceed with <a id="_idIndexMarker644"/>the generated code for the model training, testing, and validation:</p>
			<pre class="source-code">
Prompt # 4:
#split model_data for train, validation, and test
Prompt # 5:
'''
for train_data move y_yes as first column.
Drop y_no and y_yes columns from train_data.
save file as train.csv
'''
Prompt # 6:
'''
for validation_data move y_yes as first column.
Drop y_no and y_yes columns from validation_data.
save file as validation.csv
'''</pre>			<div><div><img alt="Figure 14.42 –Amazon Q Developer – SageMaker Studio feature engineering" src="img/B21378_14_42.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.42 –Amazon Q Developer – SageMaker Studio feature engineering</p>
			<p>Note that for <a id="_idIndexMarker645"/>both single-line and multi-line prompts, we needed to provide much more specific details to generate the code as expected.</p>
			<h3>Requirement 4</h3>
			<p><strong class="bold">Model training</strong>: Let’s use the XGBoost algorithm:</p>
			<ul>
				<li>Rearrange data to create training, validation, and test datasets/files</li>
				<li>Use XGBOOST algorithms to train the model using a training dataset</li>
			</ul>
			<p>We will focus <a id="_idIndexMarker646"/>on multi-line prompts to achieve this requirement to start the model training activity:</p>
			<pre class="source-code">
Prompt #1:
''' upload train.csv to S3 Bucket train/train.csv prefix.
upload validation.csv to S3 Bucket validation/validation.csv prefix '''
Prompt #2:
# pull latest xgboost model as a CONTAINER
Prompt #3:
# create TrainingInput from s3 train/train.csv and validation/validation.csv
Prompt #3:
# create training job with hyper paramers max_depth=5, eta=0.2, gamma=4, min_child_weight=6, subsample=0.8, objective='binary:logistic', num_round=100</pre>			<div><div><img alt="Figure 14.43 – Amazon Q Developer – SageMaker Studio model training" src="img/B21378_14_43.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.43 – Amazon Q Developer – SageMaker Studio model training</p>
			<h3>Requirement 5</h3>
			<p>Deploy the<a id="_idIndexMarker647"/> model as an endpoint to allow inferences.</p>
			<p>We will focus on single-line prompts to achieve this requirement:</p>
			<pre class="source-code">
Prompt #1:
# Deploy a model that's hosted behind a real-time endpoint</pre>			<div><div><img alt="Figure 14.44 – Amazon Q Developer -SageMaker studio model training" src="img/B21378_14_44.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.44 – Amazon Q Developer -SageMaker studio model training</p>
			<p>Observe that Amazon Q Developer uses a default configuration for <code>instance_type</code> and <code>initial_instance_count</code>. You can check the hosted model from the Amazon SageMaker console by clicking the <strong class="bold">Inference</strong> dropdown and selecting the <strong class="bold">Endpoints</strong> option.</p>
			<p>In the preceding <a id="_idIndexMarker648"/>examples, we extensively used inline prompts with single-line prompting, multi-line prompting, and chain of thought prompting techniques. If you wish to use a chat-style interface, you can leverage the Amazon Q Developer chat-style interface, as shown in the following screenshot.</p>
			<div><div><img alt="Figure 14.45 –Amazon Q Developer -SageMaker studio chat style interface" src="img/B21378_14_45.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.45 –Amazon Q Developer -SageMaker studio chat style interface</p>
			<h3>Summary – Amazon Q Developer with Amazon SageMaker</h3>
			<p>As demonstrated, Amazon Q Developer seamlessly integrated with the Amazon SageMaker Studio notebook IDE, enables the automatic generation of end-to-end, error-free, and executable code. By supplying prompts with specific requirements, Q Developer can auto-generate code for essential milestone steps, including data collection, feature engineering, model training, and model deployment within the SageMaker Studio notebook.</p>
			<p>While data scientists can utilize this integration to produce code blocks, customization may be necessary. Specific details must be provided in prompts to tailor the code. In some instances, adjustments may be required to align with enterprise standards, business requirements, and configurations. Users should possess expertise in prompt engineering, familiarity with scripting, and conduct thorough testing to ensure the scripts meet business requirements before deploying them into production.</p>
			<p>Now, let’s dive deep to see how data analysts can use code assistance while working with Amazon Redshift.</p>
			<h1 id="_idParaDest-233"><a id="_idTextAnchor232"/>Code assistance integration with Amazon Redshift</h1>
			<p>Before we start diving deep into code assistance support for the Amazon Redshift service, let’s quickly go through an overview of AWS Redshift. <strong class="bold">Amazon Redshift</strong> is an AI-powered, fully managed, cloud-based data warehouse <a id="_idIndexMarker649"/>service. It is designed for high-performance analysis and the processing of large datasets using standard SQL queries.</p>
			<p>Amazon Redshift is <a id="_idIndexMarker650"/>optimized for data warehousing, providing<a id="_idIndexMarker651"/> a fast and scalable solution for processing and analyzing large volumes of structured data. It uses columnar storage <a id="_idIndexMarker652"/>and <strong class="bold">massively parallel processing</strong> (<strong class="bold">MPP</strong>) architecture, distributing data and queries across multiple nodes to deliver high performance for complex queries. This architecture allows it to easily scale from a few hundred gigabytes to petabytes of data, enabling organizations to grow their data warehouse as their needs evolve. It integrates with various data sources, allowing you to load data from multiple sources, including Amazon S3, Amazon DynamoDB, and Amazon EMR.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To query the data, Amazon Redshift also provides a query editor. The Redshift query editor v2 has two modes to interact with databases: <strong class="bold">Editor</strong> and <strong class="bold">Notebook</strong>. Code assistance is integrated with the Notebook mode of the Redshift query editor v2.</p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor233"/>Use case for Amazon Redshift</h2>
			<p>Let’s start with one of the easy and widely used use cases of converting file format.</p>
			<p><strong class="bold">Identifying top performers</strong>: In a<a id="_idIndexMarker653"/> typical business use case, analysts are interested in identifying top performers based on certain criteria.</p>
			<p>To illustrate this use case, we will be using the publicly available <code>tickit</code> database, which is readily available with Amazon Redshift. For more information about the <code>tickit</code> database, refer to the <em class="italic">References</em> section at the end of the chapter.</p>
			<p>Analysts want to identify the top state where most of the venues are.</p>
			<p>To meet this requirement, analyst developers must develop SQL queries to interact with different tables from the <code>tickit</code> database.</p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor234"/>Solution blueprint</h2>
			<p>As we are considering the <a id="_idIndexMarker654"/>data analyst persona and using code assistance to generate the code, we do not need to further break down the business ask into the solution blueprint. This makes it easy for analysts to interact with databases without getting involved in table structures and relationship details:</p>
			<ul>
				<li>Write SQL to identify the top state where most of the venues are</li>
			</ul>
			<h2 id="_idParaDest-236"><a id="_idTextAnchor235"/>Data preparation</h2>
			<p>We will be using the publicly<a id="_idIndexMarker655"/> available <code>tickit</code> database, which comes with Amazon Redshift. Let’s import the data using Redshift query editor v2:</p>
			<ol>
				<li>Connect to your Amazon Redshift cluster or Serverless endpoint from Redshift query editor 2.</li>
				<li>Then, choose <code>sample_data_dev</code> and <a id="_idIndexMarker656"/>click on <code>tickit</code>.</li>
			</ol>
			<div><div><img alt="Figure 14.46 – Import the tickit database using Amazon Redshift" src="img/B21378_14_46.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.46 – Import the tickit database using Amazon Redshift</p>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor236"/>Solution – Amazon Q with Amazon Redshift</h2>
			<p>Let’s first enable Amazon Q with Amazon Redshift. To allow Amazon Q to generate SQL inside Amazon Redshift, the admin needs to enable the <strong class="bold">Generative SQL</strong> option inside <strong class="bold">Notebook</strong> of Redshift query editor v2. Please reference <a href="B21378_03.xhtml#_idTextAnchor060"><em class="italic">Chapter 3</em></a> for additional details on initiating interaction with Amazon Q in Amazon Redshift.</p>
			<h3>Prerequisite to enable Amazon Q with Amazon Redshift</h3>
			<p>Let’s walk through<a id="_idIndexMarker657"/> the steps needed to enable the <strong class="bold">Generative SQL</strong> option inside <strong class="bold">Notebook</strong> of the Redshift query editor v2.</p>
			<ol>
				<li>Log in with admin privileges to connect to your Amazon Redshift cluster or Serverless endpoint.</li>
				<li>Choose <strong class="bold">Notebook</strong>.</li>
			</ol>
			<div><div><img alt="Figure 14.47 – Notebook using Redshift query editor v2" src="img/B21378_14_47.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.47 – Notebook using Redshift query editor v2</p>
			<ol>
				<li value="3">Choose <strong class="bold">Generative SQL</strong>, then <a id="_idIndexMarker658"/>check the <strong class="bold">Generative SQL</strong> box, and click <strong class="bold">Save</strong>.</li>
			</ol>
			<div><div><img alt="Figure 14.48 – Enable Generative SQL using Redshift query editor v2" src="img/B21378_14_48.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.48 – Enable Generative SQL using Redshift query editor v2</p>
			<p>To fulfill the mentioned requirements, we will use auto-code generation techniques that were discussed in <a href="B21378_04.xhtml#_idTextAnchor081"><em class="italic">Chapter 4</em></a>. Mainly, we will focus on the chat companion for auto-code generation.</p>
			<h3>Requirement 1</h3>
			<p>Write SQL to identify the top state where most of the venues are.</p>
			<p>Use Amazon Q’s interactive session to ask the following question:</p>
			<pre class="source-code">
Q:Which state has most venues?</pre>			<p>Observe that we did not provide database details to the Amazon Q Developer, but it was still able to identify the required table, <code>tickit.venue</code>. It generated the fully executable end-to-end query with <code>Group by</code>, <code>Order by</code>, and <code>Limit</code> to meet the requirements. To make it easy for analysts to run the queries, code assistance is integrated with the notebook. Just by clicking <strong class="bold">Add to notebook</strong>, the SQL code will be available in a notebook <a id="_idIndexMarker659"/>cell that users can run directly.</p>
			<div><div><img alt="Figure 14.49 – Interact with code assistance from Amazon Redshift" src="img/B21378_14_49.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.49 – Interact with code assistance from Amazon Redshift</p>
			<h3>Summary – Amazon Q with Amazon Redshift</h3>
			<p>As demonstrated, we can effortlessly generate end-to-end, error-free, and executable SQL by interacting with Amazon Q through a chat-style interface. Amazon Q seamlessly integrates with notebooks in the Amazon Redshift query editor v2. Users are not required to provide database and/or table details to the code assistant. It autonomously identifies the necessary tables and generates SQL code to fulfill the specified requirements in the prompt. Furthermore, to facilitate analysts in running queries, it is directly integrated with the notebook. Amazon Q, in conjunction with Amazon Redshift, proves to be a valuable asset for data analysts. In many cases, data analysts do not need to translate business requirements into technical steps. They can leverage the auto-generate SQL feature, bypassing the need to delve deep into database and table details.</p>
			<h1 id="_idParaDest-238"><a id="_idTextAnchor237"/>Summary</h1>
			<p>In this chapter, we initially covered the integration of different AWS services with code companions to assist users in auto-code generation. Then, we explored the integration of Amazon Q Developer with some of the core services, such as AWS Glue, Amazon EMR, AWS Lambda, Amazon Redshift, and Amazon SageMaker, commonly used by application developers, data engineers, and data scientists.</p>
			<p>We then discussed, in the prerequisites, the in-depth integration with sample common use cases and corresponding solution walk-throughs for various integrations.</p>
			<p>AWS Glue integration with Amazon Q Developer, aiding data engineers in generating and executing ETL scripts using the AWS Glue Studio notebook environment. This includes a skeletal outline of a full end-to-end Glue ETL job using AWS Glue Studio.</p>
			<p>AWS EMR integration with Amazon Q Developer to assist data engineers in generating and executing ETL scripts using the AWS EMR Studio notebook environment.</p>
			<p>AWS Lambda console IDE integration with Amazon Q Developer, supporting application engineers in generating and executing end-to-end Python-based applications for file movement.</p>
			<p>Amazon SageMaker studio notebook integration with Amazon Q Developer to help data scientists achieve major milestone steps in data collection, feature engineering, model training, and model deployment using different prompting techniques.</p>
			<p>Amazon Redshift integration with Amazon Q to aid business analysts in generating SQL queries by simply providing business requirements. Users are not required to provide database and/or table details to the code assistant.</p>
			<p>In the next chapter, we will look at how you can use Amazon Q Developer to get AWS-specific guidance and recommendations, either from the AWS console or from the documentation on a variety of topics such as architecture and best practices support.</p>
			<h1 id="_idParaDest-239"><a id="_idTextAnchor238"/>References</h1>
			<ul>
				<li>AWS Prescriptive Guidance - Data engineering: <a href="https://docs.aws.amazon.com/prescriptive-guidance/latest/aws-caf-platform-perspective/data-eng.html">https://docs.aws.amazon.com/prescriptive-guidance/latest/aws-caf-platform-perspective/data-eng.html</a><a href="https://docs.aws.amazon.com/prescriptive-guidance/latest/aws-caf-platform-perspective/data-eng.html&#13;"/></li>
				<li>Jupyter kernel: <a href="https://docs.jupyter.org/en/latest/projects/kernels.html">https://docs.jupyter.org/en/latest/projects/kernels.html</a><a href="https://docs.jupyter.org/en/latest/projects/kernels.html&#13;"/></li>
				<li>Amazon Q Developer with AWS Glue Studio: <a href="https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/glue-setup.html">https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/glue-setup.html</a></li>
				<li>TLC Trip Record Data: <a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page">https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page</a><a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page&#13;"/></li>
				<li>Setting up Amazon Q data integration in AWS Glue: <a href="https://docs.aws.amazon.com/glue/latest/dg/q-setting-up.html">https://docs.aws.amazon.com/glue/latest/dg/q-setting-up.html</a><a href="https://docs.aws.amazon.com/glue/latest/dg/q-setting-up.html&#13;"/></li>
				<li>Setting up Amazon Q Developer data integration in Amazon EMR: <a href="https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/emr-setup.html">https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/emr-setup.html</a></li>
				<li>Attach a compute to an EMR Studio Workspace: <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-use-clusters.html">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-use-clusters.html</a><a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-use-clusters.html&#13;"/></li>
				<li>Using Amazon Q Developer with AWS Lambda: <a href="https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/lambda-setup.html">https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/lambda-setup.html</a></li>
				<li>Interacting with query editor v2 generative SQL: <a href="https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-generative-ai.html">https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-generative-ai.html</a></li>
				<li>Amazon Redshift “tickit” database: <a href="https://docs.aws.amazon.com/redshift/latest/dg/c_sampledb.html">https://docs.aws.amazon.com/redshift/latest/dg/c_sampledb.html</a><a href="https://docs.aws.amazon.com/redshift/latest/dg/c_sampledb.html&#13;"/></li>
				<li>Direct marketing bank data: <a href="https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip">https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip</a><a href="https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip&#13;"/></li>
				<li>Amazon SageMaker Studio: <a href="https://aws.amazon.com/sagemaker/studio/">https://aws.amazon.com/sagemaker/studio/</a></li>
			</ul>
		</div>
	</body></html>