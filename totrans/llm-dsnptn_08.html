<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer023">
			<h1 id="_idParaDest-104" class="chapter-number"><a id="_idTextAnchor120"/>8</h1>
			<h1 id="_idParaDest-105"><a id="_idTextAnchor121"/>Hyperparameter Tuning</h1>
			<p>In this chapter, you’ll learn about the hyperparameters in LLMs and strategies for optimizing them efficiently. We’ll explore both manual and automated tuning approaches, including grid search, random search, and more advanced methods, such as Bayesian optimization and population-based training. You’ll also gain insights into handling multi-objective optimization scenarios common in <span class="No-Break">LLM development.</span></p>
			<p>By the end, you’ll be equipped with practical tools and techniques to fine-tune your LLMs for optimal performance across various tasks <span class="No-Break">and domains.</span></p>
			<p>In this chapter, we’ll be covering the <span class="No-Break">following topics:</span></p>
			<ul>
				<li><span class="No-Break">Understanding hyperparameters</span></li>
				<li>Manual versus <span class="No-Break">automated tuning</span></li>
				<li>Grid and <span class="No-Break">random search</span></li>
				<li><span class="No-Break">Bayesian optimization</span></li>
				<li><span class="No-Break">Population-based methods</span></li>
				<li>Multi-objective <span class="No-Break">hyperparameter optimization</span></li>
				<li>Hyperparameter tuning at scale – challenges <span class="No-Break">and solutions</span></li>
			</ul>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor122"/>Understanding hyperparameters</h1>
			<p>Hyperparameters are<a id="_idIndexMarker374"/> settings that are set before the machine learning training process begins and are not learned from the data. They control various aspects of the learning algorithm itself, such as the model’s complexity, learning rate, and the overall training process. Data scientists manually choose and tune these hyperparameters to optimize the <span class="No-Break">model’s performance.</span></p>
			<p>Hyperparameters in LLMs can be broadly categorized into three groups: architectural, optimization, and <span class="No-Break">regularization hyperparameters:</span></p>
			<ul>
				<li><strong class="bold">Architectural hyperparameters</strong>: These<a id="_idIndexMarker375"/> define the<a id="_idIndexMarker376"/> design and structure of the model, determining how it processes and represents data. They are critical because they directly influence the model’s capacity to learn complex patterns and relationships in the data. The right architecture balances computational efficiency with performance, enabling the model to generalize well to <span class="No-Break">unseen data.</span><p class="list-inset">Parameters within this category include <span class="No-Break">the following:</span></p><ul><li>Number <span class="No-Break">of layers</span></li><li><span class="No-Break">Hidden size</span></li><li>Number of <span class="No-Break">attention heads</span></li><li><span class="No-Break">Feed-forward dimension</span></li><li><span class="No-Break">Vocabulary size</span></li></ul></li>
				<li><strong class="bold">Optimization hyperparameters</strong>: These govern how the model learns during training by<a id="_idIndexMarker377"/> adjusting the parameters to minimize the loss function. They are important because they control the rate and manner of updates, affecting convergence speed, stability, and the model’s ability to reach an optimal solution. Proper tuning ensures efficient training without divergence <span class="No-Break">or underfitting.</span><p class="list-inset">Parameters within this category include the following (we covered these in <a href="B31249_07.xhtml#_idTextAnchor108"><span class="No-Break"><em class="italic">Chapter 7</em></span></a><span class="No-Break">):</span></p><ul><li><span class="No-Break">Learning rate</span></li><li><span class="No-Break">Batch size</span></li><li>Number of <span class="No-Break">training steps</span></li><li><span class="No-Break">Warmup steps</span></li><li>Learning <span class="No-Break">rate schedule</span></li></ul></li>
				<li><strong class="bold">Regularization hyperparameters</strong>: These introduce mechanisms to prevent the model <a id="_idIndexMarker378"/>from overfitting to the training data, ensuring it generalizes to new data. They are crucial because models with high capacity can easily memorize the training data, leading to poor performance on unseen data. Regularization techniques enforce constraints that encourage simplicity <span class="No-Break">and robustness.</span><p class="list-inset">Parameters within this category include the following (see <a href="B31249_09.xhtml#_idTextAnchor141"><span class="No-Break"><em class="italic">Chapter 9</em></span></a> <span class="No-Break">for more):</span></p><ul><li><span class="No-Break">Dropout rate</span></li><li><span class="No-Break">Weight decay</span></li><li><span class="No-Break">Label smoothing</span></li></ul></li>
			</ul>
			<p>Let’s implement a<a id="_idIndexMarker379"/> function to create an LLM with <span class="No-Break">configurable hyperparameters:</span></p>
			<pre class="source-code">
from transformers import GPT2Config, GPT2LMHeadModel
def create_llm(
    num_layers, hidden_size, num_heads, ff_dim, vocab_size
):
    config = GPT2Config(
        n_layer=num_layers,
        n_embd=hidden_size,
        n_head=num_heads,
        n_inner=ff_dim,
        vocab_size=vocab_size
    )
    model = GPT2LMHeadModel(config)
    return model
# Example usage
model = create_llm(num_layers=12, hidden_size=768,
    num_heads=12, ff_dim=3072, vocab_size=50257)
print(f"Model parameters: {model.num_parameters():,}")</pre>			<p>In this code, we define a function, <strong class="source-inline">create_llm</strong>, that allows us to easily create LLMs with different <a id="_idIndexMarker380"/>architectural hyperparameters. The function takes the <span class="No-Break">following parameters:</span></p>
			<ul>
				<li><strong class="source-inline">num_layers</strong>: The number of transformer layers in the model. More layers can capture more complex patterns, but they increase <span class="No-Break">computational requirements.</span></li>
				<li><strong class="source-inline">hidden_size</strong>: The dimension of the hidden states throughout the model. This affects the model’s capacity to <span class="No-Break">capture information.</span></li>
				<li><strong class="source-inline">num_heads</strong>: The number of attention heads in each layer. Multiple heads allow the model to focus on different aspects of the <span class="No-Break">input simultaneously.</span></li>
				<li><strong class="source-inline">ff_dim</strong>: The dimension of the feed-forward layer in each transformer block. This is typically set to four times <span class="No-Break">the </span><span class="No-Break"><strong class="source-inline">hidden_size</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">vocab_size</strong>: The size of the model’s vocabulary. This determines the size of the embedding layer and the <span class="No-Break">output layer.</span></li>
			</ul>
			<p>We use these<a id="_idIndexMarker381"/> parameters to create a <strong class="source-inline">GPT2Config</strong> object, which is then used to initialize a <strong class="source-inline">GPT2LMHeadModel</strong>. This approach allows us to easily experiment with different <span class="No-Break">model architectures.</span></p>
			<h1 id="_idParaDest-107"><a id="_idTextAnchor123"/>Manual versus automated tuning</h1>
			<p><strong class="bold">Manual tuning</strong> involves<a id="_idIndexMarker382"/> adjusting hyperparameters<a id="_idIndexMarker383"/> based on intuition, experience, and gradual experimentation. Manual tuning allows you to leverage domain knowledge to explore tailored configurations systematically, but it is time-intensive, prone to suboptimal results, and inefficient in exploring large <span class="No-Break">hyperparameter spaces.</span></p>
			<p><strong class="bold">Automated tuning</strong>, on the<a id="_idIndexMarker384"/> other hand, uses algorithms to<a id="_idIndexMarker385"/> systematically explore the hyperparameter space. Automated tuning efficiently explores large hyperparameter spaces using algorithms to optimize performance, saving time and effort compared to manual tuning, but it can be computationally expensive and may require expertise to <span class="No-Break">configure properly.</span></p>
			<p>Manual tuning is useful when domain knowledge or intuition can guide a small, targeted search space, especially in resource-constrained settings or for simpler models. Automated<a id="_idIndexMarker386"/> tuning<a id="_idIndexMarker387"/> is better for large, complex hyperparameter spaces where systematic<a id="_idIndexMarker388"/> exploration and optimization are<a id="_idIndexMarker389"/> required, as it can find better configurations more efficiently despite higher <span class="No-Break">computational costs.</span></p>
			<p>Let’s implement <span class="No-Break">both approaches.</span></p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor124"/>Manual tuning</h2>
			<p>First, we’ll implement <a id="_idIndexMarker390"/><span class="No-Break">manual tuning:</span></p>
			<ol>
				<li>Start with <span class="No-Break">the imports:</span><pre class="source-code">
import numpy as np
from transformers import Trainer, TrainingArguments
from datasets import load_dataset</pre></li>				<li>Load a <span class="No-Break">sample dataset:</span><pre class="source-code">
dataset = load_dataset(
    "wikitext", "wikitext-2-raw-v1", split="train")
def tokenize_function(examples):
    return tokenizer(
        examples["text"], truncation=True, max_length=512)
tokenized_dataset = dataset.map(tokenize_function,
    batched=True, remove_columns=dataset.column_names)</pre></li>				<li>Set up the manual <span class="No-Break">tuning hyperparameters:</span><pre class="source-code">
manual_hyperparameters = [
    {"num_layers": 6, "hidden_size": 512, "num_heads": 8, "ff_dim": 2048},
    {"num_layers": 12, "hidden_size": 768, "num_heads": 12, "ff_dim": 3072},
    {"num_layers": 24, "hidden_size": 1024, "num_heads": 16, "ff_dim": 4096}
]</pre></li>				<li>Conduct <a id="_idIndexMarker391"/>training <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">manual_hyperparameters</strong></span><span class="No-Break">:</span><pre class="source-code">
for hp in manual_hyperparameters:
    model = create_llm(hp, vocab_size=50257)
    training_args = TrainingArguments(
        output_dir=(
            f"./results_{hp['num_layers']}_"
            f"{hp['hidden_size']}"
        ),
        num_train_epochs=3,
        per_device_train_batch_size=8,
        logging_dir=(
            f"./logs_{hp['num_layers']}_"
            f"{hp['hidden_size']}"
        ),
    )
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
    )
    trainer.train()</pre></li>				<li>Evaluate <span class="No-Break">the model:</span><pre class="source-code">
    eval_results = trainer.evaluate()
    print(f"Hyperparameters: {hp}")
    print(f"Evaluation results: {eval_results}")</pre></li>			</ol>
			<p>In this manual tuning example, we define a list of hyperparameter configurations to try. We then iterate through these configurations, creating a model for each, training it, and evaluating its performance. This approach allows us to systematically explore different model sizes <span class="No-Break">and architectures.</span></p>
			<p>The manual tuning process can be guided by domain knowledge and intuition. For example, we might start with a small model (6 layers, 512 hidden size) and gradually increase the size to see how it affects performance. We choose these specific configurations <a id="_idIndexMarker392"/>based on common practices in <span class="No-Break">transformer-based models:</span></p>
			<ul>
				<li>The smallest configuration (6 layers, 512 hidden size) represents a compact model suitable for faster training <span class="No-Break">and deployment</span></li>
				<li>The medium configuration (12 layers, 768 hidden size) is similar to the base GPT-2 model, known to perform well on <span class="No-Break">many tasks</span></li>
				<li>The largest configuration (24 layers, 1,024 hidden size) represents a more powerful model that might capture more complex patterns but requires more <span class="No-Break">computational resources</span></li>
			</ul>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor125"/>Automated tuning</h2>
			<p>Now, let’s <a id="_idIndexMarker393"/>implement a simple automated tuning approach using random search (we will show a more advanced random search in the <span class="No-Break">next section):</span></p>
			<ol>
				<li>Add the <strong class="source-inline">import</strong> statement and set up the <span class="No-Break">random parameters:</span><pre class="source-code">
import random
def random_hp_search(num_trials=10):
    best_eval_loss = float('inf')
    best_hp = None
    for _ in range(num_trials):
        hp = {
            "num_layers": random.choice([6, 12, 24]),
            "hidden_size": random.choice([512, 768, 1024]),
            "num_heads": random.choice([8, 12, 16]),
            "ff_dim": random.choice([2048, 3072, 4096])
        }</pre></li>				<li><span class="No-Break">Conduct </span><span class="No-Break"><a id="_idIndexMarker394"/></span><span class="No-Break">training:</span><pre class="source-code">
        model = create_llm(hp, vocab_size=50257)
        training_args = TrainingArguments(
            output_dir=f"./results_random_{_}",
            num_train_epochs=3,
            per_device_train_batch_size=8,
            logging_dir=f"./logs_random_{_}",
        )
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
        )
        trainer.train()</pre></li>				<li>Evaluate and print out <span class="No-Break">the results:</span><pre class="source-code">
        eval_results = trainer.evaluate()
        eval_loss = eval_results['eval_loss']
        if eval_loss &lt; best_eval_loss:
            best_eval_loss = eval_loss
            best_hp = hp
        print(
            f"Trial {_ + 1}: "
            f"Hyperparameters: {hp}, "
            f"Eval Loss: {eval_loss}"
        )
    print(
        f"Best Hyperparameters: {best_hp}, "
        f"Best Eval Loss: {best_eval_loss}"
    )
random_hp_search()</pre></li>			</ol>
			<p>This random search<a id="_idIndexMarker395"/> implementation randomly selects hyperparameters from predefined options for each trial (no manual intervention during trials). Predefined options refer to the specified ranges, sets, or distributions from which random values for hyperparameters are sampled during the search process. For example, discrete hyperparameters such as the number of layers might be chosen from a set <strong class="source-inline">[6, 12, 24]</strong>, while continuous hyperparameters such as learning rate could be sampled from a uniform or log-uniform distribution, such as <strong class="source-inline">10^(-5)</strong> to <strong class="source-inline">10^(-3)</strong>. These options define the boundaries and possible values for each hyperparameter, guiding the random <span class="No-Break">sampling process.</span></p>
			<p>We choose to search over a discrete set of values for each hyperparameter to limit the search space and ensure that we’re exploring configurations that are known to work well for transformer models. The number of trials (10 in this case) is a balance between exploration and computational resources. More trials increase the chance of finding a good configuration but also increase the <span class="No-Break">computational cost.</span></p>
			<p>In the subsequent<a id="_idIndexMarker396"/> sections, we will introduce other automated turning techniques such as grid search and more advanced random search, Bayesian optimization, the population-based method, and multi-objective <span class="No-Break">hyperparameter optimization.</span></p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor126"/>Grid and random search</h1>
			<p><strong class="bold">Grid search</strong> and <strong class="bold">random search</strong> are two common methods for hyperparameter tuning. We<a id="_idIndexMarker397"/> covered random search in the <a id="_idIndexMarker398"/>previous <a id="_idIndexMarker399"/>section. In this section, we implement grid search and a more advanced version of <span class="No-Break">random search.</span></p>
			<ol>
				<li>Add the import and set up the grid <span class="No-Break">search parameters:</span><pre class="source-code">
import itertools
def grid_search():
    hp_grid = {
        "num_layers": [6, 12, 24],
        "hidden_size": [512, 768, 1024],
        "num_heads": [8, 12, 16],
        "ff_dim": [2048, 3072, 4096]
    }
    best_eval_loss = float('inf')
    best_hp = None</pre></li>				<li>Train the model with the <span class="No-Break">defined hyperparameters:</span><pre class="source-code">
for hp in itertools.product(*hp_grid.values()):
        hp_dict = dict(zip(hp_grid.keys(),hp))
        model = create_llm(
            hp_dict["num_layers"],
            hp_dict["hidden_size"],
            hp_dict["num_heads"],
            hp_dict["ff_dim"],
            vocab_size=50257
        )
        training_args = TrainingArguments(
            output_dir=(
                f"./results_grid_{hp_dict['num_layers']}_"
                f"{hp_dict['hidden_size']}"
            ),
            num_train_epochs=3,
            per_device_train_batch_size=8,
            logging_dir=(
                f"./logs_grid_{hp_dict['num_layers']}_"
                f"{hp_dict['hidden_size']}"
            ),
        )
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
        )
        trainer.train()</pre></li>				<li>Evaluate<a id="_idIndexMarker400"/> and<a id="_idIndexMarker401"/> print<a id="_idIndexMarker402"/> out <span class="No-Break">the results:</span><pre class="source-code">
        eval_results = trainer.evaluate()
        eval_loss = eval_results['eval_loss']
        if eval_loss &lt; best_eval_loss:
            best_eval_loss = eval_loss
            best_hp = hp_dict
        print(
            f"Hyperparameters: {hp_dict}, "
            f"Eval Loss: {eval_loss}"
        )
    print(
        f"Best Hyperparameters: {best_hp}, "
        f"Best Eval Loss: {best_eval_loss}"
    )
grid_search()</pre></li>			</ol>
			<p>Grid search exhaustively explores all combinations of hyperparameters. This approach is thorough but can be computationally expensive, especially for LLMs with many hyperparameters. In this implementation, we’re exploring <strong class="source-inline">3^4</strong> = <strong class="source-inline">81</strong> different configurations, which could take a significant amount of time <span class="No-Break">and resources.</span></p>
			<p>The hyperparameter ranges are chosen to cover a reasonable space of model sizes, from relatively small (6 layers, 512 hidden size) to quite large (24 layers, 1,024 hidden size). This allows us to explore the trade-off between model size <span class="No-Break">and performance.</span></p>
			<p>Now, let’s<a id="_idIndexMarker403"/> implement a more sophisticated <a id="_idIndexMarker404"/>random <a id="_idIndexMarker405"/>search that also includes <span class="No-Break">optimization hyperparameters:</span></p>
			<ol>
				<li>Add the <strong class="source-inline">import</strong> statement and set up the <span class="No-Break"><strong class="source-inline">advanced_random_search</strong></span><span class="No-Break"> hyperparameters:</span><pre class="source-code">
import random
def advanced_random_search(num_trials=20):
    best_eval_loss = float('inf')
    best_hp = None
    for _ in range(num_trials):
        hp = {
            "num_layers": random.choice([6, 12, 24]),
            "hidden_size": random.choice([512, 768, 1024]),
            "num_heads": random.choice([8, 12, 16]),
            "ff_dim": random.choice([2048, 3072, 4096]),
            "learning_rate": 10random.uniform(-5, -3),
            "batch_size": random.choice([8, 16, 32]),
            "num_epochs": random.randint(2, 5),
            "warmup_steps": random.randint(100, 1000),
            "weight_decay": random.uniform(0, 0.2)
        }</pre></li>				<li>Conduct<a id="_idIndexMarker406"/> <span class="No-Break">the </span><span class="No-Break"><a id="_idIndexMarker407"/></span><span class="No-Break">training:</span><pre class="source-code">
        model = create_llm(
            num_layers=hp['num_layers'],
                hidden_size=hp['hidden_size'],
            num_heads=hp['num_heads'], ff_dim=hp['ff_dim'],
                vocab_size=50257)
        training_args = TrainingArguments(
            output_dir=f"./results_advanced_random_{_}",
            num_train_epochs=hp['num_epochs'],
            per_device_train_batch_size=hp['batch_size'],
            learning_rate=hp['learning_rate'],
            warmup_steps=hp['warmup_steps'],
            weight_decay=hp['weight_decay'],
            logging_dir=f"./logs_advanced_random_{_}",
        )
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
        )
        trainer.train()</pre></li>				<li>Evaluate and print out <span class="No-Break">the results:</span><pre class="source-code">
        eval_results = trainer.evaluate()
        eval_loss = eval_results['eval_loss']
        if eval_loss &lt; best_eval_loss:
            best_eval_loss = eval_loss
            best_hp = hp
        print(
            f"Trial {_ + 1}: Hyperparameters: {hp}, "
            f"Eval Loss: {eval_loss}"
        )
        
    print(
        f"Best Hyperparameters: {best_hp}, "
        f"Best Eval Loss: {best_eval_loss}"
    )</pre></li>			</ol>
			<p>This <a id="_idIndexMarker408"/>advanced <a id="_idIndexMarker409"/>random <a id="_idIndexMarker410"/>search includes both architectural and optimization hyperparameters. We use <strong class="source-inline">random.uniform</strong> for <strong class="bold">continuous hyperparameters </strong>(parameters that can take any real number value within a range, such<a id="_idIndexMarker411"/> as <strong class="source-inline">0.001</strong>, <strong class="source-inline">0.0015</strong>, or <strong class="source-inline">0.002</strong>) such as learning rate and weight decay, and <strong class="source-inline">random.choice</strong> or <strong class="source-inline">random.randint</strong> for <strong class="bold">discrete hyperparameters</strong> (parameters that can only take specific<a id="_idIndexMarker412"/> predefined values or integers, such as choosing between <strong class="source-inline">32</strong>, <strong class="source-inline">64</strong>, and <strong class="source-inline">128</strong> for batch size or selecting from a fixed set <span class="No-Break">of options).</span></p>
			<p>The ranges for each hyperparameter are chosen based on common practices in LLM training (see also <a href="B31249_07.xhtml#_idTextAnchor108"><span class="No-Break"><em class="italic">Chapter 7</em></span></a><span class="No-Break">):</span></p>
			<ul>
				<li><strong class="bold">Learning rate</strong>: We use a log-uniform distribution between <strong class="source-inline">1e-5</strong> and <strong class="source-inline">1e-3</strong>, as learning rates for LLMs are typically in <span class="No-Break">this range</span></li>
				<li><strong class="bold">Batch size</strong>: We choose from <strong class="source-inline">8</strong>, <strong class="source-inline">16</strong>, and <strong class="source-inline">32</strong>, which are common batch sizes that balance between computational efficiency <span class="No-Break">and stability</span></li>
				<li><strong class="bold">Number of epochs</strong>: We allow <strong class="source-inline">2</strong> to <strong class="source-inline">5</strong> epochs, as LLMs often converge within a few epochs on <span class="No-Break">large datasets</span></li>
				<li><strong class="bold">Warmup steps</strong>: We choose between <strong class="source-inline">100</strong> and <strong class="source-inline">1,000</strong> steps, which can help stabilize <span class="No-Break">early training</span></li>
				<li><strong class="bold">Weight decay</strong>: We use a uniform distribution between <strong class="source-inline">0</strong> and <strong class="source-inline">0.2</strong>, as small amounts of weight decay can help <span class="No-Break">prevent overfitting</span></li>
			</ul>
			<p>Advanced random search is better than grid search because it explores the hyperparameter space more efficiently by sampling randomly instead of exhaustively evaluating every possible combination. This flexibility allows it to focus on key hyperparameters that significantly impact performance, preventing redundant evaluations of less impactful ones. It can handle continuous parameters directly by sampling from distributions, unlike grid search, which requires discretization and exponentially <a id="_idIndexMarker413"/>grows in computational <a id="_idIndexMarker414"/>cost as the parameter space increases. By limiting the<a id="_idIndexMarker415"/> number of trials to a predefined budget, advanced random search can discover effective configurations faster and with less computational expense, making it more practical for large and <span class="No-Break">complex models.</span></p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor127"/>Bayesian optimization</h1>
			<p><strong class="bold">Bayesian optimization</strong> is a more<a id="_idIndexMarker416"/> sophisticated<a id="_idIndexMarker417"/> approach to hyperparameter tuning that can be particularly effective for LLMs. It uses a probabilistic model to predict the performance of different hyperparameter configurations and intelligently selects the next configuration <span class="No-Break">to try.</span></p>
			<p>Let’s implement Bayesian optimization using the <strong class="source-inline">optuna</strong> library. <strong class="bold">Optuna</strong> is an open source hyperparameter <a id="_idIndexMarker418"/>optimization framework for automating the process of finding optimal parameters for algorithms and models. It employs advanced Bayesian optimization techniques, primarily the <strong class="bold">Tree-structured Parzen Estimator</strong> (<strong class="bold">TPE</strong>) algorithm, to<a id="_idIndexMarker419"/> efficiently search complex <span class="No-Break">parameter spaces:</span></p>
			<ol>
				<li>Import optuna and set up <span class="No-Break">the hyperparameters:</span><pre class="source-code">
import optuna
from transformers import Trainer, TrainingArguments
import torch
def objective(trial):
    # Define the hyperparameters to optimize
    hp = {
        "num_layers": trial.suggest_int("num_layers", 6, 24),
        "hidden_size": trial.suggest_categorical(
            "hidden_size", [512, 768, 1024]
        ,
        "num_heads": trial.suggest_categorical(
            "num_heads", [8, 12, 16]
        ),
        "ff_dim": trial.suggest_categorical(
            "ff_dim", [2048, 3072, 4096]
        ),
        "learning_rate": trial.suggest_loguniform(
            "learning_rate", 1e-5, 1e-3
        ),
        "batch_size": trial.suggest_categorical(
            "batch_size", [8, 16, 32]
        ),
        "num_epochs": trial.suggest_int("num_epochs", 2, 5),
        "warmup_steps": trial.suggest_int(
        "warmup_steps", 100, 1000),
        "weight_decay": trial.suggest_uniform(        "weight_decay", 0, 0.2)
    }
    model = create_llm(
        num_layers=hp['num_layers'],
        hidden_size=hp['hidden_size'],
        num_heads=hp['num_heads'], ff_dim=hp['ff_dim'],
        vocab_size=50257
    )</pre></li>				<li><span class="No-Break">Conduct </span><span class="No-Break"><a id="_idIndexMarker420"/></span><span class="No-Break">training:</span><pre class="source-code">
    training_args = TrainingArguments(
        output_dir=f"./results_bayesian_{trial.number}",
        num_train_epochs=hp['num_epochs'],
        per_device_train_batch_size=hp['batch_size'],
        learning_rate=hp['learning_rate'],
        warmup_steps=hp['warmup_steps'],
        weight_decay=hp['weight_decay'],
        logging_dir=f"./logs_bayesian_{trial.number}",
    )
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
    )
    trainer.train()
    eval_results = trainer.evaluate()
    return eval_results['eval_loss']</pre></li>				<li>Run <span class="No-Break">the optimization:</span><pre class="source-code">
study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=20)
print("Best trial:")
trial = study.best_trial
print(f"Value: {trial.value}")
print("Params: ")
for key, value in trial.params.items():
    print(f"    {key}: {value}")</pre></li>			</ol>
			<p>In this implementation, we<a id="_idIndexMarker421"/> define an <strong class="source-inline">objective</strong> function<a id="_idIndexMarker422"/> that Optuna will optimize. The function creates and trains a model with the hyperparameters suggested by Optuna and then returns the <span class="No-Break">evaluation loss.</span></p>
			<p>We use Optuna’s suggestion methods to define the <span class="No-Break">search space:</span></p>
			<ul>
				<li><strong class="source-inline">suggest_int</strong> for integer hyperparameters such as <strong class="source-inline">num_layers</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">num_epochs</strong></span></li>
				<li><strong class="source-inline">suggest_categorical</strong> for hyperparameters with discrete options such as <strong class="source-inline">hidden_size</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">num_heads</strong></span></li>
				<li><strong class="source-inline">suggest_loguniform</strong> for the learning rate, as we want to search this <span class="No-Break">space logarithmically</span></li>
				<li><strong class="source-inline">suggest_uniform</strong> for weight decay, as we want to search this <span class="No-Break">space uniformly</span></li>
			</ul>
			<p>The ranges for each hyperparameter are similar to those in our random search implementation based on common practices in <span class="No-Break">LLM training.</span></p>
			<p>Bayesian optimization can be more efficient than grid or random search, especially for expensive-to-evaluate functions such as training LLMs. It uses the results of previous trials to inform the<a id="_idIndexMarker423"/> selection of future trials, potentially<a id="_idIndexMarker424"/> finding good configurations <span class="No-Break">more quickly.</span></p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor128"/>Population-based methods</h1>
			<p><strong class="bold">Population-based training</strong> (<strong class="bold">PBT</strong>) is a powerful technique that combines parallel search with adaptive<a id="_idIndexMarker425"/> hyperparameter<a id="_idIndexMarker426"/> tuning during the training process. PBT is particularly effective for problems where training can be paused and resumed efficiently. This is because PBT periodically evaluates and updates hyperparameters and model weights across a population, requiring seamless pause-and-resume capabilities. This adaptability ensures optimal use of computational resources and makes PBT ideal for tasks such as neural architecture search, reinforcement learning, and hyperparameter tuning, where iterative optimization is <span class="No-Break">computationally intensive.</span></p>
			<p>Here, we’ll implement a simplified version of PB<a id="_idTextAnchor129"/>T to illustrate its core concepts <span class="No-Break">and functionality.</span></p>
			<p>We’ll start by creating a <strong class="source-inline">SimplePBT</strong> class that encapsulates the core functionality of the<a id="_idTextAnchor130"/> PBT algorithm. Let’s break down <span class="No-Break">the implementation:</span></p>
			<ol>
				<li>First, initialize <span class="No-Break">the class:</span><pre class="source-code">
import random
import copy
class SimplePBT:
    def __init__(self, population_size=4, num_generations=5):
        self.population_size = population_size
        self.num_generations = num_generations
        self.population = []</pre><p class="list-inset">The <strong class="source-inline">SimplePBT</strong> class is initialized with two <span class="No-Break">main parameters:</span></p><ul><li><strong class="source-inline">population_size</strong>: The number of different hyperparameter configurations to maintain (the default <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">4</strong></span><span class="No-Break">)</span></li><li><strong class="source-inline">num_generations</strong>: The number of iterations the PBT algorithm will run (the default <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">5</strong></span><span class="No-Break">)</span></li></ul><p class="list-inset">The <strong class="source-inline">population</strong> list will store dictionaries representing each individual in the population, containing <a id="_idIndexMarker427"/>hyperp<a id="_idTextAnchor131"/>arameters <a id="_idIndexMarker428"/>and their corresponding <span class="No-Break">performance scores.</span></p></li>				<li>Initialize the population: The <strong class="source-inline">initialize_population</strong> method creates the initial set of <span class="No-Break">hyperparameter configurations:</span><pre class="source-code">
def initialize_population(self):
    for _ in range(self.population_size):
        hp = {
            "num_layers": random.choice([6, 12, 24]),
            "hidden_size": random.choice([512, 768, 1024]),
            "num_heads": random.choice([8, 12, 16]),
            "ff_dim": random.choice([2048, 3072, 4096]),
            "learning_rate": 10random.uniform(-5, -3),
            "batch_size": random.choice([8, 16, 32]),
            "weight_decay": random.uniform(0, 0.2)
        }
        self.population.append({"hp": hp, "score": None})</pre><p class="list-inset">For each individual in the population, do <span class="No-Break">the following:</span></p><ul><li><strong class="bold">Categorical hyperparameters</strong>, which<a id="_idIndexMarker429"/> are a subset of <strong class="bold">discrete hyperparameters</strong> (e.g., <strong class="source-inline">num_layers</strong>, <strong class="source-inline">hidden_size</strong>), are randomly selected from<a id="_idIndexMarker430"/> predefined options. These hyperparameters are categorical because they represent distinct, individual choices rather than values along <span class="No-Break">a continuum.</span></li><li><strong class="bold">Continuous hyperparameters</strong> (e.g., <strong class="source-inline">learning_rate</strong>, <strong class="source-inline">weight_decay</strong>) are <a id="_idIndexMarker431"/>sampled from <span class="No-Break">specified ranges.</span></li></ul><p class="list-inset">Each configuration is ad<a id="_idTextAnchor132"/>ded to the <strong class="source-inline">population</strong> list with an initial score <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">None</strong></span><span class="No-Break">.</span></p></li>				<li>Train and evaluate: The <strong class="source-inline">train_and_evaluate</strong> method is responsible for creating an LLM with the given hyperparameters, setting up training arguments, initializing a <a id="_idIndexMarker432"/>trainer with the <a id="_idIndexMarker433"/>model and arguments, training the model, evaluating the model, and returning the <span class="No-Break">evaluation loss:</span><pre class="source-code">
def train_and_evaluate(self, hp):
    model = create_llm(num_layers=hp['num_layers'],
        hidden_size=hp['hidden_size'],
        num_heads=hp['num_heads'],
        ff_dim=hp['ff_dim'], vocab_size=50257)
    training_args = TrainingArguments(
        output_dir=f"./results_pbt_{random.randint(0, 1000)}",
        num_train_epochs=3,
        per_device_train_batch_size=hp['batch_size'],
        learning_rate=hp['learning_rate'],
        weight_decay=hp['weight_decay'],
        logging_dir=f"./logs_pbt_{random.randint(0, 1000)}",
    )
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
    )
    trainer.train()
    eval_results = trainer.evaluate()
    return eval_results['eval_loss']</pre><p class="list-inset">This method assumes the existence of <strong class="source-inline">create_llm</strong>, <strong class="source-inline">TrainingArguments</strong>, and <strong class="source-inline">Trainer</strong> classes, which would typically be provided by a<a id="_idTextAnchor133"/> deep learning framework such as Hugging <span class="No-Break">Face Transformers.</span></p></li>				<li>Exploit and <a id="_idIndexMarker434"/>explore: The <strong class="source-inline">exploit_and_explore</strong> method<a id="_idIndexMarker435"/> implements the core <span class="No-Break">PBT algorithm:</span><pre class="source-code">
def exploit_and_explore(self):
    # Sort population by score
    self.population.sort(key=lambda x: x['score'])
    # Replace bottom half with mutated versions of top half
    for i in range(self.population_size // 2):
        self.population[i + self.population_size // 2]['hp'] =\
            self.mutate(
                copy.deepcopy(self.population[i]['hp'])
            )</pre><p class="list-inset">It sorts the population based on their scores (a lower score indicates less loss). The bottom-performing half of the population is replaced with mutated versions of the top-performing half. This approach<a id="_idIndexMarker436"/> balances <strong class="bold">exploitation</strong> (keeping <a id="_idTextAnchor134"/>good<a id="_idIndexMarker437"/> configurations) with <strong class="bold">exploration</strong> (trying <span class="No-Break">new variations).</span></p></li>				<li>Mutate: The <strong class="source-inline">mutate</strong> method <a id="_idIndexMarker438"/>introduces<a id="_idIndexMarker439"/> variations in <span class="No-Break">the hyperparameters:</span><pre class="source-code">
def mutate(self, hp):
    # Randomly mutate one hyperparameter
    param_to_mutate = random.choice(list(hp.keys()))
    if param_to_mutate in [
        'num_layers', 'hidden_size', 'num_heads', 'ff_dim',
        'batch_size'
    ]:
        hp[param_to_mutate] = random.choice(
            [6, 12, 24] 
            if param_to_mutate == "num_layers" else
            [512, 768, 1024] 
            if param_to_mutate == "hidden_size" else
            [8, 12, 16] 
            if param_to_mutate == "num_heads" else
            [2048, 3072, 4096] 
            if param_to_mutate == "ff_dim" else
            [8, 16, 32]
        )
    elif param_to_mutate == 'learning_rate':
        hp[param_to_mutate] *= random.uniform(0.8, 1.2)
    elif param_to_mutate == 'weight_decay':
        hp[param_to_mutate] = min(
            max(hp[param_to_mutate]
                + random.uniform(-0.05, 0.05), 0), 0.2
            )
    return hp</pre><p class="list-inset">It randomly selects one hyperparameter to mutate. For categorical parameters, it chooses a new value from predefined options. For continuous parameters like learning rate, it perturbs the current value within a certain range. For weight decay, it adds a small random value while keeping it within [<span class="No-Break"><strong class="source-inline">0, 0.2</strong></span><span class="No-Break">].</span></p><p class="list-inset">This mutation strategy allows both small and large changes in the hyperparam<a id="_idTextAnchor135"/>eters, promoting<a id="_idIndexMarker440"/> diverse <a id="_idIndexMarker441"/>exploration of the <span class="No-Break">hyperparameter space.</span></p></li>				<li>Run the <span class="No-Break">PBT process:</span><pre class="source-code">
def run(self):
    self.initialize_population()
    for generation in range(self.num_generations):
        print(f"Generation {generation + 1}")
        for i, individual in enumerate(self.population):
            individual['score'] = \
                self.train_and_evaluate(individual['hp'])
            print(
                f"Individual {i + 1}:
                Score = {individual['score']}"
            )
        self.exploit_and_explore()
    best_individual = min(self.population,
        key=lambda x: x['score'])
    print("\nBest Hyperparameters:")
    print(best_individual['hp'])
    print(f"Best Score: {best_individual['score']}")</pre><p class="list-inset">The <strong class="source-inline">run</strong> method orchestrates the entire <span class="No-Break">PBT process:</span></p><ol><li class="upper-roman">It initializes <span class="No-Break">the population.</span></li><li class="upper-roman">For each generation, it trains and evaluates each individual in the population and it performs exploitation and exploration to update <span class="No-Break">the population.</span></li><li class="upper-roman">After all g<a id="_idTextAnchor136"/>enerations, it prints the best hyperparameters and the <span class="No-Break">score found.</span></li></ol></li>				<li>Use the <strong class="source-inline">SimplePBT</strong> class: To use the <strong class="source-inline">SimplePBT</strong> class, you can simply create an instance and <span class="No-Break">run it:</span><pre class="source-code">
# Run PBT
pbt = SimplePBT()
pbt.run()</pre></li>			</ol>
			<p>This will start the PBT process with the default population size of <strong class="source-inline">4</strong> and <strong class="source-inline">5</strong> generations. You can <a id="_idIndexMarker442"/>adjust<a id="_idIndexMarker443"/> these parameters when creating the <strong class="source-inline">SimplePBT</strong> instance to suit your <span class="No-Break">specific needs.</span></p>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor137"/>Multi-objective hyperparameter optimization</h1>
			<p>In LLM development, we <a id="_idIndexMarker444"/>often need to balance <a id="_idIndexMarker445"/>multiple objectives, such <a id="_idIndexMarker446"/>as model performance, inference speed, and model size. Let’s implement multi-objective optimization <span class="No-Break">using Optuna:</span></p>
			<ol>
				<li>Add the <strong class="source-inline">import</strong> statement and set up <span class="No-Break">the hyperparameters:</span><pre class="source-code">
import optuna
def objective(trial):
    hp = {
        "num_layers": trial.suggest_int("num_layers", 6, 24),
        "hidden_size": trial.suggest_categorical(
            "hidden_size", [512, 768, 1024]),
        "num_heads": trial.suggest_categorical(
            "num_heads", [8, 12, 16]),
        "ff_dim": trial.suggest_categorical(
            "ff_dim", [2048, 3072, 4096]),
        "learning_rate": trial.suggest_loguniform(
            "learning_rate", 1e-5, 1e-3),
        "batch_size": trial.suggest_categorical(
            "batch_size", [8, 16, 32]),
        "weight_decay": trial.suggest_uniform(
            "weight_decay", 0, 0.2)
    }
    model = create_llm(
        num_layers=hp['num_layers'],
        hidden_size=hp['hidden_size'],
        num_heads=hp['num_heads'],
        ff_dim=hp['ff_dim'],
        vocab_size=50257
    )</pre></li>				<li>Conduct<a id="_idIndexMarker447"/> <span class="No-Break">the training:</span><pre class="source-code">
    training_args = TrainingArguments(
        output_dir=f"./results_multi_objective_{trial.number}",
        num_train_epochs=3,
        per_device_train_batch_size=hp['batch_size'],
        learning_rate=hp['learning_rate'],
        weight_decay=hp['weight_decay'],
        logging_dir=f"./logs_multi_objective_{trial.number}",
    )
       trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
    )
    trainer.train()</pre></li>				<li>Carry <a id="_idIndexMarker448"/>out <a id="_idIndexMarker449"/><span class="No-Break">the</span><span class="No-Break"><a id="_idIndexMarker450"/></span><span class="No-Break"> evaluation:</span><pre class="source-code">
    eval_results = trainer.evaluate()
    eval_loss = eval_results['eval_loss']
    # Calculate model size in MB
    model_size = sum(p.numel() for p in model.parameters())
        * 4 / 1024 / 1024  # assuming float32
    # Simulate inference time (this would be more accurate if actually measured)
    inference_time = 0.001 * hp['num_layers']
        * (hp['hidden_size'] / 512)  2
    return eval_loss, model_size, inference_time</pre></li>				<li>Run<a id="_idIndexMarker451"/> the<a id="_idIndexMarker452"/> <span class="No-Break">multi-objective optimization:</span><pre class="source-code">
study = optuna.create_study(
    directions=["minimize", "minimize", "minimize"])
study.optimize(objective, n_trials=50)
print("Pareto front:")
for trial in study.best_trials:
    print(f"Trial {trial.number}")
    print(f"  Value: Loss={trial.values[0]:.4f},
        Size={trial.values[1]:.2f}MB,
        Inference Time={trial.values[2]:.4f}s")
    print("  Params:")
    for key, value in trial.params.items():
        print(f"    {key}: {value}")</pre></li>			</ol>
			<p>In this multi-objective <a id="_idIndexMarker453"/>optimization, we’re trying to minimize three <span class="No-Break">objectives simultaneously:</span></p>
			<ul>
				<li>Evaluation loss (<span class="No-Break">model performance)</span></li>
				<li>Model size (<span class="No-Break">in MB)</span></li>
				<li>Inference time (simulated based on <span class="No-Break">model architecture)</span></li>
			</ul>
			<p>We use Optuna’s multi-objective optimization capability by specifying multiple directions in <strong class="source-inline">create_study</strong>. The optimization process will try to find the <strong class="bold">Pareto front</strong> (the set <a id="_idIndexMarker454"/>of solutions where improving any one objective necessitates degrading at least one other objective) of these objectives’ configurations, where improving one objective would necessarily <span class="No-Break">worsen another.</span></p>
			<p>The <strong class="source-inline">objective</strong> function now returns three values corresponding to our three objectives. For model size, we calculate the total number of parameters and convert it to MB. For inference time, we use a simple heuristic based on the model’s architecture in a real scenario, you would want to <span class="No-Break">measure this.</span></p>
			<p>This approach allows us to explore trade-offs between model performance, size, and speed. It’s <a id="_idIndexMarker455"/>particularly useful for <a id="_idIndexMarker456"/>LLM<a id="_idIndexMarker457"/> development, where we often need to balance these factors for different <span class="No-Break">deployment scenarios.</span></p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor138"/>Hyperparameter tuning at scale – challenges and solutions</h1>
			<p>When tuning<a id="_idIndexMarker458"/> hyperparameters for LLMs, we face <span class="No-Break">several challenges:</span></p>
			<ul>
				<li><strong class="bold">Computational cost</strong>: Training LLMs is expensive, limiting the number of trials we <span class="No-Break">can run</span></li>
				<li><strong class="bold">Long training times</strong>: Each trial can take days or weeks, making the entire process <span class="No-Break">very time-consuming</span></li>
				<li><strong class="bold">Large search space</strong>: LLMs have many hyperparameters, creating a vast <span class="No-Break">search space</span></li>
				<li><strong class="bold">Sensitivity to initialization</strong>: LLM performance can vary significantly with different <span class="No-Break">random seeds</span></li>
			</ul>
			<p>To address these challenges, we can employ <span class="No-Break">several strategies:</span></p>
			<ul>
				<li><strong class="bold">Use smaller proxy tasks</strong>: Instead of tuning on the full task, use a smaller dataset or fewer training steps to get a quick estimate <span class="No-Break">of performance</span></li>
				<li><strong class="bold">Leverage pre-trained models</strong>: Start from pre-trained weights and focus on tuning <span class="No-Break">fine-tuning hyperparameters</span></li>
				<li><strong class="bold">Use multi-fidelity optimization</strong>: Start with low-fidelity evaluations (e.g., few training steps) and gradually increase fidelity for <span class="No-Break">promising configurations</span></li>
				<li><strong class="bold">Distributed hyperparameter tuning</strong>: Use multiple machines to explore different hyperparameters <span class="No-Break">in parallel</span></li>
			</ul>
			<p>Let’s implement a simple multi-fidelity <span class="No-Break">optimization approach:</span></p>
			<ol>
				<li>Add<a id="_idIndexMarker459"/> the <strong class="source-inline">import</strong> statement and set up <span class="No-Break">the hyperparameters:</span><pre class="source-code">
import optuna
def objective(trial):
    hp = {
        "num_layers": trial.suggest_int("num_layers", 6, 24),
        "hidden_size": trial.suggest_categorical(
            "hidden_size", [512, 768, 1024]),
        "num_heads": trial.suggest_categorical(
            "num_heads", [8, 12, 16]),
        "ff_dim": trial.suggest_categorical(
            "ff_dim", [2048, 3072, 4096]),
        "learning_rate": trial.suggest_loguniform(
            "learning_rate", 1e-5, 1e-3),
        "batch_size": trial.suggest_categorical(
            "batch_size", [8, 16, 32]),
        "weight_decay": trial.suggest_uniform(
            "weight_decay", 0, 0.2)
    }
    model = create_llm(
        num_layers=hp['num_layers'],
        hidden_size=hp['hidden_size'],
        num_heads=hp['num_heads'], ff_dim=hp['ff_dim'],
        vocab_size=50257)</pre></li>				<li>Use a multi-fidelity strategy to train, starting with a small number <span class="No-Break">of steps:</span><pre class="source-code">
    for steps in [100, 500, 2000]:
        training_args = TrainingArguments(
            output_dir= \
                f"./results_multi_fidelity_{trial.number}_
                {steps}",
            max_steps=steps,
            per_device_train_batch_size=hp['batch_size'],
            learning_rate=hp['learning_rate'],
            weight_decay=hp['weight_decay'],
            logging_dir=\
                f"./logs_multi_fidelity_{trial.number}_{steps}",
        )
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
        )
        trainer.train()</pre></li>				<li><span class="No-Break">Conduct </span><span class="No-Break"><a id="_idIndexMarker460"/></span><span class="No-Break">evaluation:</span><pre class="source-code">
        eval_results = trainer.evaluate()
        eval_loss =
        eval_results = trainer.evaluate()
        eval_loss = eval_results['eval_loss']
        trial.report(eval_loss, step=steps)</pre></li>				<li>Prune the <span class="No-Break">unpromising trials:</span><pre class="source-code">
        if trial.should_prune():
            raise optuna.TrialPruned()
    return eval_loss</pre></li>				<li>Run the <span class="No-Break">multi-fidelity optimization:</span><pre class="source-code">
study = optuna.create_study(
    pruner=optuna.pruners.MedianPruner())
study.optimize(objective, n_trials=30)
print("Best trial:")
trial = study.best_trial
print(f"Value: {trial.value}")
print("Params: ")
for key, value in trial.params.items():
    print(f"    {key}: {value}")</pre></li>			</ol>
			<p>There are a few<a id="_idIndexMarker461"/> aspects of this multi-fidelity approach that we should <span class="No-Break">look at:</span></p>
			<ul>
				<li>We start by training each model configuration for only <strong class="source-inline">100</strong> steps, which gives a quick initial estimate <span class="No-Break">of performance</span></li>
				<li>We then increase the number of training steps to <strong class="source-inline">500</strong> and then <strong class="source-inline">2,000</strong> for <span class="No-Break">promising configurations</span></li>
				<li>We use Optuna’s pruning mechanism to early-stop unpromising trials, saving <span class="No-Break">computational resources</span></li>
			</ul>
			<p><strong class="source-inline">MedianPruner</strong> stops a trial if its performance is worse than the median of previous trials at the same step. This allows us to focus our computational resources on the most promising <span class="No-Break">hyperparameter configurations.</span></p>
			<p>This approach helps to address the challenges of hyperparameter tuning <span class="No-Break">at scale:</span></p>
			<ul>
				<li>It reduces the computational cost by quickly eliminating <span class="No-Break">poor configurations</span></li>
				<li>It shortens the overall tuning time by using shorter training runs for <span class="No-Break">initial evaluation</span></li>
				<li>It allows us to explore a larger search space by running more trials in the same amount <span class="No-Break">of time</span></li>
			</ul>
			<p>However, there are still limitations to this approach. The performance after a small number of steps may not always correlate well with the final performance, especially for LLMs that often require long training times <span class="No-Break">to converge.</span></p>
			<p>To further <a id="_idIndexMarker462"/>improve hyperparameter tuning at scale, consider the following <span class="No-Break">advanced techniques:</span></p>
			<ul>
				<li><strong class="bold">Distributed hyperparameter tuning</strong>: This setup allows multiple machines to contribute <a id="_idIndexMarker463"/>to the same hyperparameter search, greatly speeding up <span class="No-Break">the process:</span><pre class="source-code">
import optuna
def objective(trial):
    # ... (same as before) ...
# Create a study object with MySQL storage for distributed optimization
storage = optuna.storages.RDBStorage(
    "mysql://user:password@host/database",
    engine_kwargs={"pool_size": 20, "max_overflow": 0}
)
study = optuna.create_study(
    storage=storage, pruner=optuna.pruners.MedianPruner())
# This can be run on multiple machines
study.optimize(objective, n_trials=10)</pre></li>				<li><strong class="bold">Leveraging pre-trained models</strong>: This approach starts from pre-trained models and <a id="_idIndexMarker464"/>focuses on tuning the fine-tuning hyperparameters and model size, which can be<a id="_idIndexMarker465"/> more efficient than training <span class="No-Break">from scratch:</span><pre class="source-code">
from transformers import AutoModelForCausalLM, AutoTokenizer
def create_pretrained_llm(model_name, num_layers=None):
    model = AutoModelForCausalLM.from_pretrained(model_name)
    if num_layers is not None:
        # Adjust the number of layers (this is a simplified approach)
        model.transformer.h = model.transformer.h[:num_layers]
    return model
def objective(trial):
    hp = {
        "model_name": trial.suggest_categorical(
            "model_name",
            ["gpt2", "gpt2-medium", "gpt2-large"]),
        "num_layers": trial.suggest_int("num_layers", 6, 24),
        "learning_rate": trial.suggest_loguniform(
            "learning_rate", 1e-5, 1e-3),
        "batch_size": trial.suggest_categorical(
            "batch_size", [8, 16, 32]),
        "weight_decay": trial.suggest_uniform(
            "weight_decay", 0, 0.2)
    }
    model = create_pretrained_llm(
        hp['model_name'], hp['num_layers'])
    # ... (rest of the objective function) ...
study = optuna.create_study(
        pruner=optuna.pruners.MedianPruner())
study.optimize(objective, n_trials=30)</pre></li>				<li><strong class="bold">Bayesian optimization with Gaussian processes</strong>: For problems where we can only <a id="_idIndexMarker466"/>afford a small number <a id="_idIndexMarker467"/>of trials, Gaussian process-based Bayesian optimization can be more sample-efficient than tree-based methods such as TPE (which is <span class="No-Break">Optuna’s default):</span><pre class="source-code">
import optuna
sampler = optuna.samplers.GPSampler()
study = optuna.create_study(sampler=sampler)
study.optimize(objective, n_trials=50)</pre><p class="list-inset">This approach can be particularly useful for LLM tuning where each trial is <span class="No-Break">very expensive.</span></p></li>				<li><strong class="bold">Asynchronous Successive Halving Algorithm</strong> (<strong class="bold">ASHA</strong>): ASHA is a bandit-based algorithm <a id="_idIndexMarker468"/>that can be more efficient than simple <span class="No-Break">pruning methods:</span><pre class="source-code">
from optuna.pruners import SuccessiveHalvingPruner
pruner = SuccessiveHalvingPruner(
    min_resource=100, reduction_factor=3,
        min_early_stopping_rate=0)
study = optuna.create_study(pruner=pruner)
study.optimize(objective, n_trials=100)</pre><p class="list-inset">ASHA is particularly well-suited for large-scal<a id="_idTextAnchor139"/>e hyperparameter optimization as it can <a id="_idIndexMarker469"/>handle asynchronous parallel <span class="No-Break">optimization efficiently.</span></p></li>			</ul>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor140"/>Summary</h1>
			<p>Hyperparameter tuning for LLMs presents unique challenges due to the scale and complexity of these models. By leveraging techniques such as multi-fidelity optimization, distributed tuning, and advanced algorithms such as Bayesian optimization and ASHA, we can make this process more efficient and effective. However, it’s important to remember that there’s often no one-size-fits-all solution, and the best approach may depend on your specific use case, available resources, and the characteristics of your <span class="No-Break">LLM task.</span></p>
			<p>In the next chapter, we’ll focus on <span class="No-Break">LLM regularization.</span></p>
		</div>
	</div></div></body></html>